{"id": "8038175", "url": "https://en.wikipedia.org/wiki?curid=8038175", "title": "AY-3-8500", "text": "AY-3-8500\n\nThe AY-3-8500 \"Ball & Paddle\" integrated circuit was the first in a series of ICs from General Instrument designed for the consumer video game market. These chips were designed to output video to an RF modulator, which would then display the game on a domestic television set. The AY-3-8500 contained six selectable games — tennis (a.k.a. \"Pong\"), soccer, squash, practice, and two rifle shooting games. The AY-3-8500 was the 625-line PAL version and the AY-3-8500-1 was the 525-line NTSC version. It was introduced in 1976, Coleco becoming the first customer having been introduced to the IC development by Ralph H. Baer. A minimum number of external components were needed to build a complete system.\nThe AY-3-8500 was the first version. It played seven \"Pong\" variations. The video was in black-and-white, although it was possible to colorize the game by using an additional chip, such as the AY-3-8515.\n\nSix selectable games for one or two players were included:\nIn addition, a seventh undocumented game could be played when none of the previous six was selected: Handicap, a soccer variant where the player on the right has a third paddle. This game was implemented on very few systems.\n\nThe AY-3-8500 was designed to be powered by six 1.5 V cells (9 V). Its specified operation is at 6...7 V and a maximum of 12 V instead of the 5 V standard for logic. The nominal clock was 2.0 MHz, yielding a 500 ns pixel width. One way to generate such a clock is to divide a 14.31818 MHz 4×colorburst clock by 7, producing 2.04545 MHz. It featured independent video outputs for left player, right player, ball, and playground+counter, that were summed using resistors, allowing designers to use a different luminance for each one. It was housed in a standard 28-pin DIP.\n\nSome of the dedicated consoles employing the AY-3-8500 (there are at least two hundred different consoles using this chip):\n\n\nThe AY-3-8550 was the next chip released by General Instruments. It featured horizontal player motion, and a composite video output. It was pin compatible with the AY-3-8500. It needed an additional AY-3-8515 chip to output video in color.\n\nSix selectable games for one or two players were included:\nThe AY-3-8550 used the No Connect pins from the AY-3-8500, so it was possible to put an AY-3-8550 on an AY-3-8500 (without horizontal movement), and vice versa.\n\nThis is a list of consoles that use this chip:\n\n\nThe AY-3-8610 was a major update from General Instruments. It played more games (10), like basketball or hockey, with higher-quality graphics. It was nicknamed \"Superstar\" by GI. It was in black and white, although it was possible to add color by using an additional AY-3-8615 chip.\n\nPrior to producing the 8610, GI created the AY-3-8600. The pin configuration was the same as the 8610, but it was missing the two rifle/target games, bringing the total number of games down to 8.\n\nThe 10 selectable games for this chip included:\nThe AY-3-8610 featured a completely different pinout. It, too, required an external crystal oscillator. It still had separate video output pins, and removed the dedicated sync pin.\n\nThis is a list of consoles that use the AY-3-8610:\n\nSome consoles that use the AY-3-8600 chip:\n\n\n"}
{"id": "55203348", "url": "https://en.wikipedia.org/wiki?curid=55203348", "title": "Australian Engineering Heritage Register", "text": "Australian Engineering Heritage Register\n\nThe Australian Engineering Heritage Register is a heritage register maintained by Engineers Australia to recognise and preserve Australia’s engineering and industrial heritage by recording the history of significant engineering works and by placing markers and interpretative panels at heritage sites. The register has no legislative standing.\n\nThe register was first established in 1984 and, by the end of 2016, had recognised 212 engineering heritage works.\n\n"}
{"id": "8724110", "url": "https://en.wikipedia.org/wiki?curid=8724110", "title": "Automatic Independent Surveillance-Privacy", "text": "Automatic Independent Surveillance-Privacy\n\nAutomatic Independent Surveillance - Privacy (AIS-P) is a data packet protocol for the TailLight system of aircraft Traffic Collision Avoidance System (TCAS), wherein a single Mode S 64 microsecond message is transmitted by an aircraft ATCRBS or Mode S transponder, and received by aircraft and Air Traffic Control on the ground. This is an augmentation to aircraft transponders, which report aircraft position and velocity in such a way as to minimize interference with any other avionics system, maximize the possible number of participating aircraft, while not relying on any equipment on the ground, and protecting aircraft from potential attack. AIS-P and ADS-B are competing protocols for aircraft based surveillance of traffic, a replacement technology for Mode S radar and TCAS. \n\nTailLight implemented as a free addition inside General Aviation ATCRBS transponders, such as the AT-155, utilizes the AIS-P protocol to achieve all intended ADS-B advertised collision avoidance benefits in the airport terminal and en route airspace. It does not interfere with any other avionic systems, produces a system capacity of up to 335,000 aircraft within line-of-sight of each other, and provides interoperability with the other collision avoidance systems, without exposing the aircraft to potential attack. \n\nThe AIS-P protocol is an alternative to the ADS-B and Mode S based TCAS protocols, and solves the problems of frequency congestion, by eliminating a requirement for multiple packet messages, or new longer packet definitions for ADS-B not established by international treaty, and by eliminating the 24 bit overhead for named identity in each packet of the message (required to tie multiple packets together into a message). One packet encodes latitude and longitude, altitude, direction, and speed (full position and velocity), handles error detection and recovery, along with channel use arbitration, in the AIS-P protocol. This reduces verbose overhead unnecessary for collision avoidance purposes.\n\nThe AIS-P protocol is not meant for purposes of billing and targeting. Additionally, one of the requirements satisfied by the AIS-P protocol is that a missile with an ADS-B type target homer aimed at the unnamed aircraft alone in the sky would miss.\n\n\n"}
{"id": "41871699", "url": "https://en.wikipedia.org/wiki?curid=41871699", "title": "Billon (company)", "text": "Billon (company)\n\nBillon is a fintech company operating in UK and Poland which developed a technology to store and transfer regulated currencies and other data using a proprietary blockchain.\n\nBillon designed a light-weight permissioned blockchain, with ordinary consumer electronic devices such as desktop computers, laptops, smartphones, and others, acting as nodes after installing a dedicated application. In comparison with other blockchains, Billon's technology does not use cryptocurrencies., but regulated currencies, such as Polish zloty and British pound, in compliance with e-money regulations.\n\nBillon's technology can be used for incentives, loyalty programs, market research, e-commerce, or content monetisation.\n\nBillon claims to reach the speed of transactions orders of magnitude faster than other blockchains. According to the company, throughput tests proved the number of 130 million transactions per day, i.e. couple of thousands per second. The resulting speed of transactions is comparable with contactless payment cards, achieving much lower fraud levels.\n\nThere is no mining process in Billon architecture. Instead, genesis blocks in Billon's blockchain are created by authorised issuers, primarily licensed banks that exchange paper or electronic money into their digital equivalents. Money has a form of a file with Billon specified crypto-protocol standard. This allows interoperability between multiple blockchains storing money issued by different banks. Once a blockchain is created and transferred by the bank to the first user, further circulation is done entirely on peer-to-peer basis without any further involvement from the issuing bank.\n\nEach entry has a serial number embedded, together with denomination, currency, identifier of the issuing bank as well as layer of digital watermarks and digital signatures. Signatures are both in standard RSA format and on elliptic curves created by bank's HSM.\n\nIn order to access the blockchain, users need a dedicated application that contains public keys of the issuers. The application, just like bitcoin cryptocurrency wallet, is capable of parsing Billon's digital blockchain format and verifying signatures on incoming and outgoing money. Both the sending and receiving party have to register their public keys at the issuing bank. This is done automatically during registration process.\n\nMerchant or user wishing to convert money from Billon's blockchain to electronic or paper money can do so via bank transfer or a cardless withdrawal at the automated teller machine (ATM).\n\nBillon was established in 2012 in Poland and incorporated as a UK company in 2015. In 2014, David Putts, US investor and founder of Inteligo bank, joined the company as its chairman.\n\nIn 2016, Billon introduced its solutions in a live environment in the FCA's \"regulatory sandbox\" programme. In June 2017, Billon Financial, a subsidiary company, received a FCA registration as an e-money provider for the UK.\n\nThe company received grants from the EU's Horizon 2020 research program, as well as from Ohio-based Fintech 71 startup accelerator.\n\nIn November 2017, the company was recognized as one of 40 winners of the World Summit Award for providing solutions which facilitate achieving UN's Sustainable Development Goals, such as ending poverty, providing decent work and economic growth worldwide, and empowering women with economic independence. In the same month the company signed an agreement with Mitsui Knowledge Industry to introduce Billon’s technology to Asian markets, including Japan.\n\n"}
{"id": "12861486", "url": "https://en.wikipedia.org/wiki?curid=12861486", "title": "Biosand filter", "text": "Biosand filter\n\nA biosand filter (BSF) is a point-of-use water treatment system adapted from traditional slow sand filters. Biosand filters remove pathogens and suspended solids from water using biological and physical processes that take place in a sand column covered with a biofilm. BSFs have been shown to remove heavy metals, turbidity, bacteria, viruses and protozoa. BSFs also reduce discoloration, odor and unpleasant taste. Studies have shown a correlation between use of BSFs and a decrease in the occurrence of diarrhea. Because of their effectiveness, ease of use, and lack of recurring costs, biosand filters are often considered appropriate technology in developing countries. It is estimated that over 200,000 BSFs are in use worldwide.\n\nThe household biosand filter was proposed by Dr. David Manz in the late 1980s at the University of Calgary, Canada. The system was developed from the slow sand filter, a technology that has been used for drinking water purification since the 1800s. Initial lab and field tests were conducted in 1991; the system was patented in 1993 and was implemented in the field in Nicaragua. The Canadian non-profit company Center for Affordable Water and Sanitation Technology (CAWST) was co-founded in 2001 by David Manz and Camille Dow Baker to promote education and training in water purification and sanitation including using this technology, and to continue developing it. A privately owned company, Hydraid Biosand Water Filter produces and distributes plans for filters.\n\nBiosand filters are typically constructed from concrete or plastic. At the top of the filter, a tightly fitted lid prevents contamination and unwanted pests from entering the filter. Below this, the diffuser plate prevents disturbance of the biofilm when water is poured into the filter. Water then travels through the sand column, which removes pathogens and suspended solids. Below the sand column, a layer of gravel prevents sand from entering the drainage layer and clogging the outlet tube. Below the separating layer is the drainage layer consisting of coarser gravel that prevents clogging near the base of the outlet tube.\n\nPathogens and suspended solids are removed by biological and physical processes that take place in the biolayer and the sand layer. These processes include:\n\nThe high water level (hydraulic head) in the inlet reservoir zone pushes the water through the diffuser and filter, then decreases as water flows evenly through the sand. The flow rate slows because there is less pressure to force the water through the filter. The inlet water contains dissolved oxygen, nutrients, and contaminants. It provides the oxygen required by the microorganisms in the biofilm. Large suspended particles and pathogens are trapped in the top of the sand and partially plug the pore spaces between the sand grains. This causes the flow rate to decrease.\n\nIdle time typically comprises greater than 80% of the daily cycle; during this time, microbial attenuation processes are likely to be significant. Most removal occurs where water is in contact with the biofilm. The processes that occur in the biofilm have not been identified. When the standing water layer reaches the level of the outlet tube, the flow stops. Ideally, this should be high enough to keep the biofilm in the sand layer wet and allow oxygen to diffuse through the standing water to the biolayer. The pause period allows microorganisms in the biolayer to consume the pathogens and nutrients in the water. The rate of flow through the filter is restored as they are consumed. If the pause period is too long, the biolayer will consume all of the pathogens and nutrients and will die, reducing the efficiency of the filter when it is used again. The pause period should be between 1 and 48 hours. Pathogens in the non-biological zone die from a lack of nutrients and oxygen.\n\nOver time, particles accumulate between the filter's sand grains. As more water is poured, a biofilm forms along the top of the diffuser plate. Both of these occurrences cause a decrease in flow rate (clogging and bioclogging). Although slower flow rates generally improve water filtration due to idle time [APS1], it may become too slow for the users’ convenience. If flow rates fall below 0.1 litres/minute, it is recommended by CAWST to perform maintenance. The \"swirl and dump\", or wet harrowing cleaning technique, is used to restore flow rate. About is poured into the filter before cleaning (assuming the filter is empty). The upper layer of sand is then swirled in a circular motion. Dirty water from the swirling is dumped out and the sand is smoothed out at the top. This process is repeated until flow rate is restored. Cleaning the diffuser plate, outlet tube, lid, and outside surfaces of the filters regularly is also recommended. Long-term sustainability and efficacy of biosand filters depends on education and support from knowledgeable support personnel.\n\nClean Water for Haiti, a non-profit in Haiti implements an education and follow-up program post-installation of the biosand filter. The program includes visits to the beneficiary homes after one, three and twelve months and another at 5 years from installation date. During each visit, beneficiaries receive repeated instruction about safe water practices and how to take care of the filter. Based on data collected since 2010, between 94% and 99% of filters are still being used regularly 12 months after installation. \n\nResults for turbidity reductions vary depending on the turbidity of the influent water. Turbid water contains sand, silt and clay. Feed turbidity in one study ranged from 1.86 to 3.9 NTU. In a study water was obtained from sample taps of water treatment plants from three local reservoirs. It poured through a slow sand filter and results showed that turbidity decreased to a mean of 1.45 NTU. In another study using surface water a 93% reduction in turbidity was observed. As the biofilm above the sand ripens, turbidity removal increases. Although biosand filters remove much turbidity, slow sand filters, which have a slower filtration rate, remove more.\n\nThere is limited research on the removal of heavy metals by biosand filters. In a study conducted in South Africa, the filter removed about 64% of iron and 5% of magnesium.\n\nIn laboratory studies, the biosand filter has been found to remove about 98-99% of bacteria. In removal of \"Escherichia coli\" it was found that the biosand filter may increase due to biofilm formation over about two months. The removal after this time ranged from 97-99.99% depending on the daily water volume and percent primary effluent added. The addition of primary effluent or waste water facilitates growth of the biofilm which aids bacterial die-off. Research shows that biosand filters in use in the field remove fewer bacteria than ones in a controlled environment. In research conducted in 55 households of Bonao, Dominican Republic, the average E. coli reduction was about 93 percent.\n\nLab tests have shown that while the filters reduce significant quantities of E. coli, they remove significantly fewer viruses because viruses are smaller. In a study using bacteriophages, virus removal ranged between 85% and 95% after 45 days of usage. A recent study has suggested that virus removal increases significantly over time, reaching 99.99% after approximately 150 days.\n\nIn one lab test the biosand filter removed more than 99.9% of protozoa. In tests for one type of protozoa, \"Giardia lamblia\", the filter removed 100% over 29 days of use. It removed 99.98% of the oocysts of another protozoa, \"Cryptosporidium sp.\", possibly due to their smaller size. This removal was comparable with that of the slow sand filter.\n\nStudies in the Dominican Republic and Cambodia conducted by the University of North Carolina and the University of Nevada show that BSF use the reduced occurrence of diarrheal diseases by 47% in all age groups. In a study conducted by CAWST in Haiti, 95% of 187 households believed their water quality had improved since using biosand filters to clean it. 80% of users stated that their families’ health had improved since implementation. Such health perceptions on the use of biosand filter has shown to be more positive in long-term users.\n\nConcrete filters, of concrete, are the most widespread type of biosand filter. Concrete is generally preferable to other materials because of the low cost, wide availability and the ability to be constructed on-site. The plans for the concrete filter are distributed openly by CAWST. Several versions have been developed. The CAWST Version 9 biosand filter is constructed with a higher maximum loading rate. Although the filtered water passes EPA water quality standards, it is not optimal. Recent research establishes that contact time between the water and the granular material is the leading determinant in purifying water. The CAWST Version 10 biosand filter takes this into account; the volume of the water reservoir is equal to the pore space volume of the sand layer. The maximum loading rate was decreased by 33% to ensure stagnant water is in constant contact with granular material.\n\nConcrete BioSand filters are typically manufactured using steel molds. The plans for a steel mold are openly distributed by CAWST.\n\nClean Water for Haiti, a non-profit organization based in Camp Marie, Haiti manufactures the biosand filters using an adaptation of the steel mold. \n\nThe non-profit organization OHorizons has designed a Wood Mold, based on CAWST’s Version 10 filter, which can function as low-cost alternative. The plans for a Wood Mold are openly available on the OHorizons website.\n\nPlastic filters are constructed from plastic barrels, usually formed offsite. Hydraid biosand filters are constructed from medical grade plastic with ultraviolet resistance. TivaWater is the newest version of the plastic biosand filter and has several important improvements.\n\nA stainless steel biosand filter developed by engineers at S M Sehgal Foundation, an NGO based in Gurugram (formerly Gurgaon), India, has been found to perform better than its concrete counterparts and with a wider opportunity for application and adoption in different geographical conditions. The high cost of plastic prevents its use in rural India. The stainless steel filter, called JalKalp, offers increased filtration rate and better portability (than concrete models) and better production quality control. Concrete filters are prone to breakage and can be difficult to transport due to weight (65 kg), make it unsuitable especially in remote rural or hilly locations. Common quality issues are variations in construction material and manufacturing flaws. Further, the efflorescence due to salts in water reduces the life of the concrete filter. The newly developed lightweight (4.5 kg) stainless steel biosand filter has an edge over concrete filters, overcoming each of those shortcomings and providing better quality control. Besides improving its appearance, stainless steel adds to the strength, reliability, durability, and portability of the filter. Water quality tests demonstrate JalKalp's effectiveness against E coli, total coliforms, turbidity, and iron contamination. This filter integrates the germicidal properties of copper with the conventional filtration. Introduction of copper foil in the drainage zone of JalKalp filter has increased the removal of total coliform and E coli to 100% from contaminated water. S M Sehgal Foundation promotes the model, which required no electricity, across India through partnerships with like-minded organizations to benefit as many rural families as possible.\n\nThere are challenges to creating biosand water filters in developing countries. Many lack the professional capability of constructing the metal forms to pour the concrete into. Finding proper mesh sizes to sift the sand layers may also be absent. In Nicaragua you may find metal workers capable of welding rebar for home construction, however, you will not find sheet metal bending equipment to create the metal molds. Sand is not sold in hardware stores like in the United States. It is most likely purchased by the pickup load from streambeds or pits and the only mesh available is 1/4 inch which is too large. When traveling to a third world country it might be best to take with you the proper mesh screens.\n\nAnother problem facing the use of the filters is adoption. Many projects may provide assistance in building the water filters and some may even distribute them but getting the host country nationals to use the filters requires much more dedication. People need to be connected to water filter owners to insist they use the devices and get them into the habit of using them. Otherwise several of the filters are abandoned and left unattended in the roads. Simply handing the filters out is insufficient for adoption.\n\n"}
{"id": "333625", "url": "https://en.wikipedia.org/wiki?curid=333625", "title": "Bus rapid transit", "text": "Bus rapid transit\n\nBus rapid transit (BRT), also called a busway or transitway, is a bus-based public transport system designed to improve capacity and reliability relative to a conventional bus system. Typically, a BRT system includes roadways that are dedicated to buses, and gives priority to buses at intersections where buses may interact with other traffic; alongside design features to reduce delays caused by passengers boarding or leaving buses, or purchasing fares. BRT aims to combine the capacity and speed of a metro with the flexibility, lower cost and simplicity of a bus system.\n\nThe first BRT system was the Rede Integrada de Transporte ('Integrated Transportation Network') in Curitiba, Brazil, which entered service in 1974.\n\n, a total of 166 cities in six continents have implemented BRT systems, accounting for of BRT lanes and about 32.2 million passengers every day, of which about 19.6 million passengers ride daily in Latin America, which has the most cities with BRT systems, with 54, led by Brazil with 21 cities. The Latin American countries with the most daily ridership are Brazil (10.7M), Colombia (3.06M), and Mexico (2.5M). In the other regions, China (4.3M) and Iran (2.1M) also stand out. Currently, TransJakarta is considered as the largest BRT network in the world with approximately of corridors connecting the Indonesian capital city.\n\nBus rapid transit takes its name from rail rapid transit, which describes a high-capacity urban public-transit system with its own right of way, multiple-car vehicles at short headways, and longer stop spacing than traditional streetcars and buses. BRT uses buses on a wide variety of rights-of-way, including mixed traffic, dedicated lanes on surface streets, and busways separated from traffic.\n\nThe expression \"BRT\" is mainly used in the Americas and China; in India, it is called \"BRTS\" (BRT System); in Europe and Indonesia, it is often called a \"busway\"; in Australia it is often called a \"T-Way\" (short for Transit Way); while in the British Isles, it may be called a \"quality bus\".\n\nCritics have charged that the term \"bus rapid transit\" has sometimes been misapplied to systems that lack most or all the essential features which differentiate it from conventional bus services. The term \"bus rapid transit creep\" has been used to describe severely degraded levels of bus service which fall far short of the BRT Standard promoted by the Institute for Transportation and Development Policy and other organizations.\n\nThe first use of a protected busway was the East Side Trolley Tunnel in Providence, Rhode Island. It was converted from trolley to bus use in 1948. However, the first BRT system in the world was the OC Transpo system in Ottawa, Canada. Introduced in 1973, the first element of its BRT system was dedicated bus lanes through the city centre, with platformed stops. The introduction of the first exclusive separate busways (termed 'Transitway') occurred in 1983. By 1996, all of the originally envisioned 31 km Transitway system was in operation; further expansions were opened in 2009, 2011, and 2014. As of 2017, the central part of the Transitway is being converted to a Light Rail Transit, due to the downtown section being operated beyond its designed capacity.\nThe second BRT system in the world was the Rede Integrada de Transporte (RIT, \"integrated transportation network\"), implemented in Curitiba, Brazil, in 1974. Most of the elements that have become associated with BRT were innovations first suggested by Curitiba Mayor Architect Jaime Lerner. Initially just dedicated bus lanes in the center of major arterial roads, in 1980 the Curitiba system added a feeder bus network and inter-zone connections, and in 1992 introduced off-board fare collection, enclosed stations, and platform-level boarding. Other systems made further innovations, including platooning (three buses entering and leaving bus stops and traffic signals at once) in Porto Alegre, and passing lanes and express service in São Paulo.\n\nIn the United States, BRT began in 1977, with Pittsburgh's South Busway, operating on of exclusive lanes. Its success led to the Martin Luther King Jr. East Busway in 1983, a fuller BRT deployment including a dedicated busway of , traffic signal preemption, and peak service headway as low as two minutes. After the opening of the West Busway, in length in 1990, Pittsburgh’s Busway system is today over 18.5 miles long.\n\nIn 1995, Quito, Ecuador, opened trolleybus BRT. The TransMilenio in Bogotá, Colombia, opening in 2000, was the first BRT system to combine the best elements of Curitiba's BRT with other BRT advances, and achieved the highest capacity and highest speed BRT system in the world. The success of TransMilenio spurred other cities to develop high quality BRT systems.\n\nIn January 2004 the first BRT in Asia, TransJakarta, opened in Jakarta, Indonesia. , at , it is the longest BRT system in the world.\n\nAfrica's first BRT system was opened in Lagos, Nigeria, in March 2008 but is considered as a light BRT system by many people. Johannesburg’s BRT, Rea Vaya, was the first true BRT in Africa, in August 2009, carrying 16,000 daily passengers. Rea Vaya and MIO (BRT in Cali, Colombia, opened 2009) were the first two systems to combine full BRT with some services that also operated in mixed traffic, then joined the BRT trunk infrastructure.\n\nBRT systems normally include most of the following features:\n\nBus-only lanes make for faster travel and ensure that buses are not delayed by mixed traffic congestion. A median alignment bus-only keeps buses away from busy curb-side side conflicts, where cars and trucks are parking, standing and turning. Separate rights of way may be used such as the completely elevated Xiamen BRT. Transit malls or 'bus streets' may also be created in city centers.\n\nFare prepayment at the station, instead of on board the bus, eliminates the delay caused by passengers paying on board.\n\nProhibiting turns for traffic across the bus lane significantly reduces delays to the buses. Bus priority will often be provided at signalized intersections to reduce delays by extending the green phase or reducing the red phase in the required direction compared to the normal sequence. Prohibiting turns may be the most important measure for moving buses through intersections.\n\nStation platforms should be level with the bus floor for quick and easy boarding, making it fully accessible for wheelchairs, disabled passengers and baby strollers, with minimal delays.\n\nHigh-level platforms for high-floored buses makes it difficult to have stops outside dedicated platforms, or to have conventional buses stop at high-level platforms, so these BRT stops are distinct from street-level bus stops. Similar to rail vehicles, there is a risk of a dangerous gap between bus and platform, and is even greater due to the nature of bus operations. Kassel curbs or other methods may be used to ease quick and safe alignment of the BRT vehicle with a platform.\n\nA popular compromise is low-floor buses with a low step at the door, which can allow easy boarding at low-platform stops compatible with other buses. This intermediate design may be used with some low- or medium-capacity BRT systems.\n\nThe MIO system in Cali pioneered in 2009 the use of dual buses, with doors on the left side of the bus that are located at the height of high-level platforms, and doors on the right side that are located at curb height. This buses can use the main line with its exclusive lanes and high level platforms, located on the center of the street and thus, boarding and leaving passengers on the left side. These buses can exit the main line and use normal lanes that share with other vehicles and stop at regular stations located on sidewalks, located to the right side of the street. For the system to work, users have the right to receive \"credit\" on the electronic cards: in this manner, passengers that have no money left on the cards can take the bus on sidewalk stops where there is no possibility to recharge these cards. This means that the balance in the card can be negative, up to two ticket fares, so passengers can take the bus in the street and recharge the card once they reach a main line station. As the card itself costs more than the maximum negative balance, the passenger has no incentive to default on his negative credit. Transmilenio in Bogotá followed suit in 2014 also creating routes that can use main line stations and regular sidewalk stations, but instead of giving credit to passengers to allow boarding the bus on sidewalks, published a map readable in smart phones giving the location of a dense network of 4,000 recharging points, located in internet cafes and other business, that use a swipe-card terminal for recharging. This system has the additional benefit of diminishing queues on main line stations.\n\nHigh-capacity vehicles such as articulated or even bi-articulated buses may be used, typically with multiple doors for fast entry and exit. Double-decker buses or guided buses may also be used. Advanced powertrain control may be used for a smoother ride.\n\nBRT systems typically feature significant investment in enclosed stations which may incorporate attractive sliding glass doors, staffed ticket booths, information booths, and other more standard features listed above. They will often include level boarding, using either low-floor buses or higher boarding platforms level, and multiple doors to speed passenger boardings and enhance accessibility to disabled passengers. Fare validation upon entry to the station in a similar manner to that used on entry to a subway system is also common, particularly at busy stations. An example of high-quality stations include those used on TransMilenio in Bogotá since December 2000, the MIO in Cali since November, 2008, Metrolinea in Bucaramanga since December, 2009, Megabús in Pereira since May, 2009. This design is also used in Johannesburg's Rea Vaya. The term \"station\" is more flexibly applied in North America and ranges from enclosed waiting areas (Ottawa and Cleveland) to large open-sided shelters (Los Angeles and San Bernardino).\n\nA unique and distinctive identity can contribute to BRT's attractiveness as an alternative to driving cars, (such as Viva, Max, TransMilenio, Metropolitano, Metronit, Select) marking stops and stations as well as the buses.\n\nLarge cities usually have big bus networks. A map showing all bus lines might be incomprehensible, and cause people to wait for low-frequency buses that may not even be running at the time they are needed. By identifying the main bus lines having high-frequency service, with a special brand and separate maps, it is easier to understand the entire network.\n\nPublic transit apps are more convenient than a static map, featuring services like trip planning, live arrival and departure times, up-to-date line schedules, local station maps, service alerts, and advisories that may affect one's current trip. Transit and Moovit are examples of apps that are available in many cities around the world. Some operators of bus rapid transit systems have developed their own apps, like Transmilenio. These apps even include all the schedules and live arrival times and stations for buses that feed the BRT, like the SITP (Sistema Integrado de Transporte Público or Public Transit Integrated System) in Bogotá.\n\nA special issue arises in the use of buses in metro transit structures. Since the areas where the demand for an exclusive bus right-of-way are apt to be in dense downtown areas where an above-ground structure may be unacceptable on historic, logistic, or environmental grounds, use of BRT in tunnels may not be avoidable.\n\nSince buses are usually powered by internal combustion engines, bus metros raise ventilation issues similar to those of motor vehicle tunnels. Powerful fans typically exchange air through ventilation shafts to the surface; these are usually as remote as possible from occupied areas, to minimize the effects of noise and concentrated pollution.\n\nA straightforward way to reduce air quality problems is to use internal combustion engines with lower emissions. The 2008 Euro V European emission standards set a limit on carbon monoxide from heavy-duty diesel engines of 1.5 g/kWh, one third of the 1992 Euro I standard. As a result, less forced ventilation will be required in tunnels to achieve the same air quality.\n\nA different alternative is to use electric propulsion, Seattle's Metro Bus Tunnel, and Boston's Silver Line Phase II are using this method in their BRTs. In Seattle, dual-mode (electric/diesel electric) buses manufactured by Breda were used until 2004, with the center axle driven by electric motors obtaining power from trolley wires through trolley poles in the subway, and with the rear axle driven by a conventional diesel powertrain on freeways and streets. Boston is using a similar approach, after initially using trolleybuses pending delivery of the dual-mode vehicles in 2005. In 2004, Seattle replaced its \"Transit Tunnel\" fleet with diesel-electric hybrid buses, which operate similarly to hybrid cars outside the tunnel and in a low-noise, low-emissions \"hush mode\" (in which the diesel engine operates but does not exceed idle speed) when underground. The need to provide electric power in underground environments brings the capital and maintenance costs of such routes closer to those of light rail, and raises the question of building or eventually converting to light rail. In Seattle, the downtown transit tunnel was retrofitted for conversion to a shared hybrid-bus and light-rail facility in preparation for Seattle's Central Link Light Rail line, which opened in July 2009.\n\nA BRT system can be measured by a number of factors. The BRT Standard was developed by the Institute for Transportation and Development Policy (ITDP) to score BRT corridors, producing a list of rated BRT corridors meeting the minimum definition of BRT. The highest rated systems received a \"gold\" ranking. The latest edition of the standard was published in 2014.\n\nOther metrics used to evaluate BRT performance include:\n\nBased on this data, the minimum headway and maximum current vehicle capacities, the theoretical maximum throughput measured in passengers per hour per direction (PPHPD) for a single traffic lane is some 90,000 passengers per hour (250 passengers per vehicle, one vehicles every 10 seconds). In real world conditions TransMilenio holds the record, with 35,000 – 40,000 PPHPD with most other busy systems operating in the 15,000 to 25,000 range.\n\nAfter the first BRT system opened in 1974, cities were slow to adopt BRT because they believed that the capacity of BRT was limited to about 12,000 passengers per hour traveling in a given direction during peak demand. While this is a capacity rarely needed in the US (12,000 is more typical as a total daily ridership), in the developing world this capacity constraint was a significant argument in favor of heavy rail metro investments in some venues.\n\nWhen TransMilenio opened in 2000, it changed the paradigm by giving buses a passing lane at each station stop and introducing express services within the BRT infrastructure. These innovations increased the maximum achieved capacity of a BRT system to 35,000 passengers per hour. Light rail, by comparison, has reported passenger capacities between 3,500pph (mainly street running) to 19,000pph (fully Grade-separated). \"From these findings … there is little evidence to support the view that [light rail] can carry more than busways.\". There are conditions that favor light over BRT, but they are fairly narrow. To meet these conditions you would need a corridor with only one available lane in each direction, more than 16,000 passengers per direction per hour but less than 20,000, and a long block length, because the train cannot block intersections. These conditions are rare, but in that specific instance, light rail would have a significant operational advantage. However, \"... any perceived advantages of [light rail] over BRT are primarily aesthetic and political rather than technical … due to the perceived capacity constraint of BRT there are currently no cases in the US where [light rail] should be favored over BRT.\"\n\nConventional scheduled bus services use general traffic lanes, which can be slow due to traffic congestion, and the speed of bus services is further reduced by long dwell times. \n\nIn 2013, the New York City authorities noted that buses on 34th Street, which carried 33,000 bus riders a day on local and express routes, traveled at , only slightly faster than walking pace. Even despite the implementation of Select Bus Service (New York City's version of a bus rapid transit system), dedicated bus lanes, and traffic cameras on the 34th Street corridor, buses on the corridor were still found to travel at an average of 4.5 mph.\n\nIn the 1960s, Reuben Smeed predicted that the average speed of traffic in central London would be without other disincentives such as road pricing, based on the theory that this was the minimum speed that people will tolerate. When the London congestion charge was introduced in 2003, the average traffic speed was indeed which was the highest speed since the 1970s. By way of contrast, typical speeds of BRT systems range from .\n\nThe capital costs of implementing BRT are lower than for light rail. A study by the United States Government Accountability Office from 2000 found that the average capital cost per mile for busways was $13.5 million while light rail average costs were $34.8 million. However, the total investment varies considerably due to factors such as cost of the roadway, amount of grade separation, station structures, traffic signal systems and vehicles.\n\nOperational costs of running a BRT system are generally lower than light rail, though the exact comparison varies, and labor costs depend heavily on the wages which vary between countries. For the same level of ridership and demand, higher labor costs in the developed world relative to developing countries will tend to encourage developed world transit operators to prefer operate services with larger but less frequent vehicles. This will allow the service to achieve the same capacity while minimizing the number of drivers. This may come as a hidden cost to passengers in lower demand routes who experience a significantly lower frequencies and longer waiting times. In the developing world the operating cost advantages of BRT over light rail or streetcar are much greater due to lower wages. In the study done by the GAO, BRT systems usually had lower costs based on \"operating cost per vehicle hour\", \"operating cost per revenue mile\", and \"operating cost per passenger trip\", mainly because of lower vehicle cost and lower infrastructure cost. The initial capital costs of diesel BRT are also much less lower than a trolleybus system.\n\nProponents of light rail argue that the operating costs of BRT are not necessarily lower than light rail. The typically larger light rail vehicles enjoy reduced labor costs per passenger, and the unit capital cost per passenger can be lower than BRT. Furthermore, light rail vehicles have proven useful lifespans of forty years or more, as opposed to buses that often have to be replaced after less than twenty years.\n\nAn ambitious light rail system runs partly underground, which gives free right-of-way and much faster traffic compared to passing the traffic signals needed in a surface level system. Underground BRT is rare and expensive. As most buses run on diesel, air quality can become a significant concern in tunnels, but the Downtown Seattle Transit Tunnel is an example of using hybrid buses, which switch to overhead electric propulsion while they are underground, eliminating diesel emissions and reducing fuel usage. An alternative is an elevated busway, which is also costly. A desire for grade separation indicates that a rail alternative may be better.\n\nBRT systems have been widely promoted by non-governmental organizations such as the Shell-funded EMBARQ program, Rockefeller Foundation and Institute for Transportation and Development Policy (ITDP), whose consultant pool includes the former mayor of Bogota (Colombia), Enrique Penalosa (former president of ITDP).\n\nSupported by contributions of bus-producing companies such as Volvo, the ITDP not only established a proposed \"standard\" for BRT system implementation, but developed intensive lobby activities around the world to convince local governments to select BRT systems over rail-based transportation models (subways, light trains, etc.).\n\nUnlike electric-powered trains commonly used in rapid transit and light rail systems, bus rapid transit often uses diesel- or gasoline-fueled engines. The typical bus diesel engine causes noticeable levels of air pollution, noise and vibration. It is noted however that BRT can still provide significant environmental benefits over private cars. In addition, BRT systems can replace an inefficient conventional bus network for more efficient, faster and less polluting BRT buses. For example, Bogotá previously used 2,700 conventional buses providing transportation to 1.6 million passengers daily, while in 2013 TransMilenio transported 1.9 million passengers using only 630 BRT buses, A fleet less than a quarter in size of the old fleet, that circulates at twice the speed, with a huge reduction in air pollution.\n\nTo reduce direct emissions some systems use alternative forms of traction such as electric or hybrid engines. BRT systems can use trolleybuses to lower air pollution and noise emissions such as those in Beijing and Quito. The price penalty of installing overhead lines could be offset by the environmental benefits and potential for savings from centrally generated electricity, especially in cities where electricity is less expensive than other fuel sources. Trolleybus electrical systems can be potentially reused for future light rail conversion. TransJakarta buses, uses cleaner compressed natural gas-fueled engines. While Bogotá started to use hybrid buses in 2012: they use regenerative braking to charge batteries when the bus stops and then use electric motors to propel the bus up to 40 km/h, speed at which the regular diesel engine starts automatically, with considerable savings in fuel consumption and pollutant dispersion. Furthermore, the lifetime of individual buses is generally shorter than their rail-based counterparts, potentially making the BRT system more expensive to operate in the long term.\n\nMany BRT systems suffer from overcrowding in buses and stations as well as long wait times for buses. In Santiago de Chile the average of the system is six passengers per square meter inside vehicles. Users have reported days where the buses take too long to arrive, and are usually too overcrowded to accept new passengers. As of June 2017 the system has 15% of approval from the users and has lost 27% of its passengers, who have turned mostly to cars.\n\nIn Bogotá the overcrowding is even worse; the average of TransMilenio is eight passengers per square meter. Only 29% feel satisfied with the system. The data also show that 23% of the citizens agree with building more TransMilenio lines, in contrast of the 42% who consider that a rapid transit system should be built. \nSeveral cases of sexual assault have been reported by female users in TransMilenio. According to a 2012 survey made by the secretary of the woman of Bogota, 64% of women said they had been victims of sexual assault in the system. The system has even been ranked as the most dangerous transport for women. \nThe bad quality of the system has occasioned an increment in the number of cars and motorcycles in the city, the citizens prefer these transport means over TransMilenio. According to official data the number of cars increased from approximately 666.000 in 2005 to 1.586.700 in 2016; the number of motorcycles is also growing, 660.000 were sold in Bogota in 2013, twice the number of cars sold.\n\nIn Jakarta there have been also reports of overcrowding in buses and stations, low frequency of the routes and many reports of sexual harassment cases as well as problems with buses that burn on their own. The quality of the service is so bad that in 2015 the Jakarta governor apologized for the bad service.\n\nA principal criticism of BRT systems is that they may not accomplish their promise of an efficient, rapid flow of passengers along their dedicated bus lanes. The remarkable fiasco of Delhi's BRT and the increasing riots and spontaneous user demonstrations in Bogotá raise doubts about the ability of BRTs to tackle issues such as the traffic jams induced by dedicated lanes. Overcrowded stations and BRT vehicles may fail to keep pace with increased ridership, and may eventually need to be replaced with high-capacity rail systems.\n\nThe lack of permanence of BRT has also been criticized, with some arguing that BRT systems can be used as an excuse to build roads that others later try to convert for use by non-BRT vehicles. Examples of this can be found in Delhi, where a BRT system was scrapped, and in Aspen, Colorado, where drivers are lobbying the government to allow mixed-use traffic in former BRT lanes as of 2017. Similarly, the Belfast Chamber of Trade and Commerce has called for bus lanes to be scrapped in certain areas of the city \"as an experiment.\" Bangkok was also making plans to scrap a set of bus-only lanes, as of early 2017. New Orleans ran buses on Canal Street in a dedicated right of way beginning in the 1960s. This style of service was maintained until 2004 when streetcar service was restored on this route segment. This perceived lack of permanence has made BRT lines significantly less attractive to real estate developers than rail lines.\n\nExperts also attribute the failure of BRT to land use structure. Cities that are sprawled and have no mixed use have poor ridership to make BRT economically viable. In Africa, the African Urban Institute criticized the viability of ongoing BRTs across the continent.\n\nWhile the Los Angeles Metro Orange Line is sometimes cured as an example of a successful North American BRT line, its very success has led to overcrowding and long term plans to replace it with a rail line. The problem in replacing an existing BRT corridor with a rail line is that service disruptions are unavoidable during construction and this lack of reliability may hurt ridership.\n\nA 2018 study found that the introduction of a BRT network in Mexico City reduced air pollution (emissions of CO, NOX, and PM10).\n\n\n\n\n"}
{"id": "54551323", "url": "https://en.wikipedia.org/wiki?curid=54551323", "title": "ByHours", "text": "ByHours\n\nByHours is a Spanish company and first microstays booking platform where hotels can be booked on the hourly basis.\n\nThe company was founded in March 2012 by Christian Rodriguez and Guillermo Gaspart in Barcelona, Catalonia, Spain. Byhours currently has more than 3,000 hotels across Europe (971 hotels in Spain, 424 in Germany, 133 in France, 100 in Colombia and 869 in Italy etc.). The Company represents hotels in 20 countries in Europe, America, and Asia. The app is available on Android and iOS. The company raised €600K seed funding from Caixa Capital and Cabiedes Partners in July 2013. In 2014, according to Travelmole, over 150,000 bookings were made through ByHours at over 1,500 hotels in Spain. and in 2018, the company reached the first million hours booked in their platform.\n\nIn July 2013, ByHours raised €600K seed funding from Caixa Capital and Cabiedes Partners. In April 2014, the company received a funding 2.6 million from Spanish investors. In August 2016, Byhours has also received 1.5 million from European funds and in November 2017, the company was announced a of funding 3 million from new international investors as well as existing ones.\n\n\n"}
{"id": "52937983", "url": "https://en.wikipedia.org/wiki?curid=52937983", "title": "Chemosynthesis (nanotechnology)", "text": "Chemosynthesis (nanotechnology)\n\nIn molecular nanotechnology, chemosynthesis is any chemical synthesis where reactions occur due to random thermal motion, a class which encompasses almost all of modern synthetic chemistry. The human-authored processes of chemical engineering are accordingly represented as biomimicry of the natural phenomena above, and the entire class of non-photosynthetic chains by which complex molecules are constructed is described as chemo-.\n\nThis form of engineering is then contrasted with mechanosynthesis, a hypothetical process where individual molecules are mechanically manipulated to control reactions to human specification. Since photosynthesis and other natural processes create extremely complex molecules to the specifications contained in RNA and stored long-term in DNA form, advocates of molecular engineering claim that an artificial process can likewise exploit a chain of long-term storage, short-term storage, enzyme-like copying mechanisms similar to those in the cell, and ultimately produce complex molecules which need not be proteins. For instance, sheet diamond or carbon nanotubes could be produced by a chain of non-biological reactions that have been designed using the basic model of biology.\n\nUse of the term \"chemosynthesis\" reinforces the view that this is feasible by pointing out that several alternate means of creating complex proteins, mineral shells of mollusks and crustaceans, etc., evolved naturally, not all of them dependent on photosynthesis and a food chain from the sun via chlorophyll. Since more than one such pathway exists to creating complex molecules, even extremely specific ones such as proteins edible to fish, the likelihood of humans being able to design an entirely new one is considered (by these advocates) to be near certainty in the long run, and possible within a generation.\n"}
{"id": "17231768", "url": "https://en.wikipedia.org/wiki?curid=17231768", "title": "Clesh", "text": "Clesh\n\nClesh (clip load edit share) is a cloud-based video editing platform designed for the consumers, prosumers, and online communities to integrate user generated content. The core technology is based on FORscene which is geared towards professionals working for example in broadcasting, news media, post production.\n\nVideo, audio, and graphical content is uploaded to Clesh via a standard web browser, a mobile device such as a phone / tablet, or desktop software for DV capture over Firewire. The hosted material can then be reviewed, searched, edited, and published online by anyone with a standard web browser or compatible mobile device.\n\nClesh supports storyboard shot selection, frame-accurate editing, transitions and various other functions such as; pan, zoom, colour and light correction, and audio levels. Content can be published in formats for example; Podcast, Mpeg2, HTML5 video or in a proprietary Java format.\n\nCloud-based software provides greater scope for sharing information and collaborating compared to LAN or desktop based systems. Users of cloud-based software rely on the cloud's owner for adequate security, performance and resilience.\n\nClesh does not assert any rights over uploaded content in contrast to other platforms (such as YouTube). All rights to any content uploaded to Clesh remain with the Author.\n\nSome of the services available to Clesh users:\n\nClesh is based on the same technology as FORscene. An array of servers on the internet backbone provide the cloud computing platform to host Clesh. As a white-label solution Clesh would be branded and hosted per the client requirement.\n\nEnd-users access Clesh with a rich user interface on clients such as standard Java-enabled Web Browsers and / or Android enabled mobile devices such as tablets and smartphones.\n\nClesh was launched January 2006 and subject to several upgrades during the year to extend functionality including; storyboard, podcasting, moderation, chat and a showreel. During 2007 consumers are offered Clesh via a subscription model. Upgrades include Web Start and graphics upload. Mr Paparazzi selects Clesh as the platform to host its video offering and TrueTube does the same in 2008 by choosing to use Clesh to manage its video portal. Several further upgrades are applied and include; better audio quality, image enhancement controls, transitions, fades, titles, and additional publishing options such as JPEG. In 2010 a version of Clesh is demonstrated on an Android OS tablet device (Samsung Galaxy S Tab), and several upgrades are applied including; HTML5 publishing, pan, zoom, and overlays.\n\n"}
{"id": "45589261", "url": "https://en.wikipedia.org/wiki?curid=45589261", "title": "Combination plate", "text": "Combination plate\n\nA combination plate can refer to several things, including:\n\nA combination plate may refer to a meal or plate with a combination of foods.\n\nA combination plate may refer to a type of tableware plate, dish or platter that is designed with separate compartments for foods to be placed in. This has also been referred to as a compartment plate and a partition plate. Combination plate meals are sometimes served on this type of plate. In Nepal, this type of plate is called a \"thaali\", and is typically made of metal. In Nepalese cuisine, the dish daal bhaat is often served on a thaali. In the United States, compartment plates have been used to serve table d'hôte dinners. In the United States, combination plates have been used as a part of U.S. army mess kits.\n\nIn dentistry, the term has referred to dentures prepared and cast with a combination of materials, such as gold and rubber, plastic and metallic material, and gold and porcelain.\n\nIn gemology, a combination plate refers to two or more crystals and/or minerals that have formed in a combination.\nIn printing and graphic arts, a printing plate that has \"both halftones and line drawings, often combined\"\n\nA combination plate can refer to a combination wall plate that has a combination of ports for switches, plugs, etc.\n\n"}
{"id": "12234623", "url": "https://en.wikipedia.org/wiki?curid=12234623", "title": "Constant phase element", "text": "Constant phase element\n\nA constant phase element is an equivalent electrical circuit component that models the behaviour of a double layer, that is an imperfect capacitor.\n\nConstant phase elements are also used in equivalent circuit modelling and data fitting of electrochemical impedance spectroscopy data.\n\nA constant phase element also appears currently in modeling the behaviour of the imperfect dielectrics. The generalization in the fields of imperfect electrical resistances, capacitances and inductances leads to the general \"Phasance\" concept: http://fr.scribd.com/doc/71923015/The-Phasance-Concept\n\nThe electrical impedance can be calculated:\nwhere the CPE admittance is: formula_2 and Q and n (0<n<1) are frequency independent.\n\nQ = 1/|Z| at ω = 1 rad/s\n\nThe constant phase is always −(90*n)°, also with n from 0 to 1. The case n = 1 describes an ideal capacitor while the case n = 0 describes a pure resistor.\n"}
{"id": "11718641", "url": "https://en.wikipedia.org/wiki?curid=11718641", "title": "DailyTech", "text": "DailyTech\n\nDailyTech was an online daily publication of technology news, founded by ex-AnandTech editor Kristopher Kubicki on January 1, 2005. The site features a prominent \"comments\" section that acts as the forums for the publication. Users are able to moderate or respond to each post, a template the editor admits borrowing from Slashdot. The operating revenue for DailyTech is primarily dependent on advertising, with syndication of their news feed also providing some revenue. As of early December 2015 the website seems inactive without any notice. In mid July 2016, the web address quit functioning but has since resumed and new articles are being published regularly again.\n\nThe website is split up into two sections: \"news\" and \"blogs.\" Both appear on the front page, though blogs are sectioned off and declared differently in the title. News content on the site primarily consists of computer-related hardware news, but also includes a variety of science, defense and consumer-tech information. \n\nThe schism between DailyTech and AnandTech occurred in goodwill, with the goal of establishing DailyTech as a news site that would not be bound by the NDAs that AnandTech has signed. Anand Lal Shimpi is frequently quoted and featured on DailyTech; however, the two publications compete against each other for readership. The DailyTech news feed is also used by other technology and science websites.\n\nDailyTech combines blog-style news with industry interviews and frequent roadmap leaks. The DailyTech editor has a frequent history of run-ins with writers from other publications. He has publicly denounced the writings from competitor Tom's Hardware, Gizmodo, HardOCP, The Inquirer and DigiTimes. However, the site owners do not censor comments.\n\nDailyTech has consistently leaked several generations of GPUs and CPUs. The company attributes this to the standing instruction that DailyTech writers are not allowed to sign disclosure agreements or embargoes.\n\nOn June 5, 2007, the site published a report on the levels of corruption present at other technology news and review websites. 7 out of 35 site polled accepted some kind of advertising-for-content exchange.\n"}
{"id": "4604806", "url": "https://en.wikipedia.org/wiki?curid=4604806", "title": "Danish Food and Allied Workers' Union", "text": "Danish Food and Allied Workers' Union\n\nThe Danish Food and Allied Workers' Union (NNF) (in Danish: \"Nærings- og Nydelsesmiddelarbejder Forbundet\") is a trade union in Denmark. It is an affiliate of the Danish Confederation of Trade Unions.\n\n"}
{"id": "43793768", "url": "https://en.wikipedia.org/wiki?curid=43793768", "title": "Defect criticality", "text": "Defect criticality\n\nIn the context of software quality, defect criticality is a measure of the impact of a software defect. It is defined as the product of severity, likelihood, and class.\n\nDefects are different from user stories, and therefore the priority (severity) should be calculated as follows.\n\n\n\n\n\n\n\n\n"}
{"id": "251975", "url": "https://en.wikipedia.org/wiki?curid=251975", "title": "Dehumidifier", "text": "Dehumidifier\n\nA dehumidifier is an electrical appliance which reduces and maintains the level of humidity in the air, usually for health or comfort reasons, or to eliminate musty odor and to prevent the growth of mildew by extracting water from the air. It can be used for household, commercial, or industrial applications. Large dehumidifiers are used in commercial buildings such as indoor ice rinks and swimming pools, as well as manufacturing plants or storage warehouses.\n\nDehumidifiers extract water from air that passes through the unit. There are two types of dehumidifiers - condensate dehumidifiers and desiccant dehumidifiers.\n\nCondensate dehumidifiers use a refrigerator to collect water known as condensate, which is normally greywater but may at times be reused for industrial purposes. Some manufacturers offer reverse osmosis filters to turn the condensate into potable water. Some designs, such as the ionic membrane dehumidifier, dispose of water as a vapor rather than liquid.\n\nDesiccant dehumidifiers (known also as absorption dehumidifiers) bond moisture with hydrophilic materials such as silica gel. Cheap domestic units contain single-use hydrophilic substance cartridges, gel, and powder. Larger commercial units contain hot air recovery systems in order to remove humid air from outside the room.\n\nThe energy efficiency of dehumidifiers can vary widely.\n\nThese methods rely on drawing air across a cold surface. Since the saturation vapor pressure of water decreases with decreasing temperature, the water in the air condenses on the surface, separating the water from the air.\n\nElectric refrigeration dehumidifiers are the most common type of dehumidifiers. They work by drawing moist air over a refrigerated evaporator with a fan. There are 3 main types of evaporators. They are coiled tube, fin and tube, and microchannel technology.\n\nThe cold evaporator coil of the refrigeration device condenses the water, which is removed, and then the air is reheated by the condenser coil. The now dehumidified, re-warmed air is released into the room. This process works most effectively at higher ambient temperatures with a high dew point temperature. In cold climates, the process is less effective. Highest efficiency is reached above and 45% relative humidity. This relative humidity value is higher if the temperature of the air is lower. .\n\nThis type of dehumidifier differs from a standard air conditioner in that both the evaporator and the condenser are placed in the same air path. A standard air conditioner transfers heat energy out of the room because its condenser coil releases heat outside. However, since all components of the dehumidifier are in the \"same\" room, no heat energy is removed. Instead, the electric power consumed by the dehumidifier remains in the room as heat, so the room is actually \"heated\", just as by an electric heater that draws the same amount of power.\n\nIn addition, if water is condensed in the room, the amount of heat previously needed to evaporate that water also is re-released in the room (the latent heat of vaporization). The dehumidification process is the inverse of adding water to the room with an evaporative cooler, and instead releases heat. Therefore, an in-room dehumidifier will always warm the room and reduce the relative humidity indirectly, as well as reducing the humidity more directly, by condensing and removing water.\n\nWarm, moist air is drawn into the unit at A in the diagram above. This air passes into a crossflow plate heat exchanger (B) where a substantial proportion of the sensible heat is transferred to a cool supply air stream. This process brings the extracted air close to saturation. The air then passes to the plenum chamber of the extract fan (C) where a portion of it may be rejected to outside. The amount that is rejected can be varied and is\ndetermined either by legislation on fresh air requirements, or by the requirement to maintain a fresh, odour free environment. The balance of the air then passes into the evaporator coil of the heat pump where it is cooled and the moisture is condensed. This process yields substantial amounts of latent energy to the refrigeration circuit. Fresh air is then introduced to replace the amount that was extracted and the mix is discharged by the supply fan (G) to the crossflow plate exchanger (B) where it is heated by the extract air from the pool. This pre-warmed air then passes through the heat pump condenser (F) where it is heated by the latent energy removed during the condensation process as well as the energy input to the compressor. The warm dry air is then discharged to the room.\n\nA conventional air conditioner is very similar to an electric dehumidifier and inherently acts as a dehumidifier when chilling the air. In an air conditioner, however, the air passes over the cold evaporator coils and then directly into the room. It is not re-heated by passing over the condenser, as in a refrigeration dehumidifier. Instead, the refrigerant is pumped by the compressor to a condenser which is located outside the room to be conditioned, and the heat is then released to the outside air. Conventional air conditioners use additional energy exhausting air outside, and new air can have more moisture than the room needs, such as a pool room that already holds a high amount of moisture in the air.\n\nThe water that condenses on the evaporator in an air conditioner is usually routed to remove extracted water from the conditioned space. Newer high-efficiency window units use the condensed water to help cool the condenser coil by evaporating the water into the outdoor air, while older units simply allowed the water to drip outside.\n\nWhen water is chilled below the atmospheric dewpoint, atmospheric water will condense onto it faster than water evaporates from it. Spray dehumidifiers mix sprays of chilled water and air to capture atmospheric moisture. They also capture pollutants and contaminants like pollen, for which purpose they are sometimes called \"air washers\".\n\nBecause window air conditioner units have condensers and expansion units, some of them can be used as makeshift dehumidifiers by sending their heat exhaust back into the same room as the cooled air, instead of the outside environment. If the condensate from the cooling coils is drained away from the room as it drips off the cooling coils, the result will be room air that is drier but slightly warmer.\n\nHowever, many window air conditioners are designed to dispose of condensate water by re-evaporating it into the exhaust air stream, which cancels out the air humidity decrease caused by the condensation of moisture on the cooling coils. To be effective as a dehumidifier, an air conditioner must be designed or modified so that most or all of the water that condenses is drained away in liquid form, rather than re-evaporated. Even if condensate is drained, a modified air conditioner is still less efficient than a single-purpose appliance with a design optimized for dehumidification. Dehumidifiers are designed to pass air directly over the cooling coils and then the heating coils in a single efficient pass through the device.\n\nIn addition, most air conditioners are controlled by a thermostat which senses temperature, rather than a humidistat that senses humidity and is typically used to control a dehumidifier. A thermostat is not designed for the control of humidity, and controls it poorly if at all.\n\nUnder certain conditions of temperature and humidity, ice can form on a refrigeration dehumidifier's evaporator coils. The ice buildup can impede airflow and eventually form a solid block encasing the coils. This buildup prevents the dehumidifier from operating effectively, and can cause water damage if condensed water drips off the accumulated ice and not into the collection tray. In extreme cases, the ice can deform or distort mechanical elements, causing permanent damage.\n\nBetter-quality dehumidifiers may have a frost or ice sensor. These will turn off the machine and allow the ice-covered coils to warm and defrost. Once defrosted, the machine will automatically restart. Most ice sensors are simple thermal switches and do not directly sense the presence or absence of ice buildup. An alternative design senses the impeded airflow and shuts off the cooling coils in a similar manner.\n\nThermoelectric dehumidifiers use a Peltier heat pump to cool a surface and condense water vapor from the air. The design is simpler and has the benefit of being quieter compared to a dehumidifier with a mechanical compressor. However, because of its relatively poor Coefficient of Performance, this design is mainly used for small dehumidifiers. Ice buildup may be a problem, similar to problems with refrigeration dehumidifiers.\n\nThis process uses a special humidity-absorbing material called a desiccant, which is exposed to the air to be conditioned. The humidity-saturated material is then moved to a different location, where it is \"recharged\" to drive off the humidity, typically by heating it. The desiccant can be mounted on a belt or other means of transporting it during a cycle of operation.\n\nDehumidifiers which work according to the absorption principle are especially suited for high humidity levels at low temperatures. They are often used in various sectors in industry because humidity levels below 35% can be achieved.\n\nBecause of the lack of compressor parts desiccant dehumidifiers are often lighter and quieter than compressor dehumidifiers. Desiccant dehumidifiers can also operate at lower temperatures than compressor dehumidifiers as the unit lacks coils which are unable to extract moisture from the air at lower temperatures.\n\nAn ionic membrane can be used to move humidity into or out of a sealed enclosure, operating at a molecular level without involving visible liquid water.\n\nThe solid polymer electrolyte (SPE) membrane is a low power, steady-state dehumidifier for enclosed areas where maintenance is difficult. The electrolytic process delivers dehumidifying capacities up to 0.2 grams/day from a 0.2m³ (7 cu ft) space to 58 grams/day from an 8m³ (280 cu ft). SPE systems generally do not have high dehydration capacities, but because the water vapor is removed through electrolysis, the process is maintenance free. The process also requires very little electrical energy to operate, using no moving parts, making the ionic membranes silent in operation and very reliable over long periods of time. SPE dehumidifiers are typically used to protect sensitive electrical components, medical equipment, museum specimens, or scientific apparatus from humid environments.\n\nThe SPE consists of a proton-conductive solid polymer electrolyte and porous electrodes with a catalytic layer composed of noble metal particles. When a voltage is applied to the porous electrode attached to the membrane, the moisture on the anode side (dehumidifying side) dissociates into hydrogen ions (H+) and oxygen. The hydrogen ions migrate through membrane to be discharged on the cathode (moisture discharging) side where they react with oxygen in the air, resulting in water molecules (vapor), being discharged.\n\nProducts using condensation technology has traditionally been using a cold surface where humidity in warm air is condensated. Today the warm condensation technology based on the concept of oversaturated steam inside a closed environment makes it possible to dehumidify air at sub zero temperatures. This is a very energy efficient technology and equally efficient in all temperatures.\n\nMost portable dehumidifiers are equipped with a condensate collection receptacle, typically with a float sensor that detects when the collection vessel is full, to shut off the dehumidifier and prevent an overflow of collected water. In a warm humid environments, these buckets will generally fill with water in 8–12 hours, and may need to be manually emptied and replaced several times per day to ensure continued operation.\n\nMany portable dehumidifiers can also be adapted to connect the condensate drip output directly to a drain via a hose. Some dehumidifier models can tie into plumbing drains or use a built-in water pump to empty themselves as they collect moisture. Alternatively, a separate condensate pump may be used to move collected water to a disposal location when gravity drainage is not possible.\n\nCentral air conditioning units typically need to be connected to a drain, because the quantity of condensate water generated by such systems can be quite large over time. If the condensate water is directed into the sewer system, it should be suitably trapped. Otherwise, back pressure can allow smells or sewer gases to enter the building. The condensate should not be directed into a septic system of a house, because large central air conditioning systems discharge water that does not need to be treated by septic systems. If the height of the air handler (containing the evaporator) is above the ground level or in the attic of a house, condensate lines can also often be routed into rain gutters. Air handlers located in the basement of a house require condensate pumps to pump the water up to ground level.\n\nGenerally, dehumidifier water is considered a rather clean kind of greywater: not suitable for drinking, but acceptable for watering plants, though not garden vegetables. The health concerns are:\n\nFood-grade dehumidifiers, also called atmospheric water generators, are designed to avoid toxic metal contamination and to keep all water contact surfaces clean. The devices are primarily intended to produce pure water, and the dehumidifying effect is viewed as secondary to their operation.\n\nIf condensate water is handled automatically, most dehumidifiers require very little maintenance. Because of the volume of airflow through the appliance, dust buildup needs to be removed so it does not impede airflow; many designs feature removable and washable air filters. Condensate collection trays and containers may need occasional cleaning to remove debris buildup and prevent clogging of drainage passages, which can cause water leakage and overflow.\n\nRelative humidity in dwellings should preferably range from 30 to 50%.\n\nDehumidification within buildings can control:\nBasements,\nCrawl Spaces,\nKitchens,\nBedrooms,\nBathrooms,\nSpas or Indoor Pool Areas,\nWarehouses,\nWorkshops\n\nDehumidifiers are used in industrial climatic chambers, to reduce relative humidity and the dew point in many industrial applications from waste and fresh water treatment plants to indoor grow rooms where the control of moisture is essential.\n\nSome industries include:\n\n\n\n"}
{"id": "102490", "url": "https://en.wikipedia.org/wiki?curid=102490", "title": "Dell", "text": "Dell\n\nDell is an American multinational computer technology company based in Round Rock, Texas, United States, that develops, sells, repairs, and supports computers and related products and services. Named after its founder, Michael Dell, the company is one of the largest technological corporations in the world, employing more than 103,300 people in the U.S. and around the world.\n\nDell sells personal computers (PCs), servers, data storage devices, network switches, software, computer peripherals, HDTVs, cameras, printers, MP3 players, and electronics built by other manufacturers. The company is well known for its innovations in supply chain management and electronic commerce, particularly its direct-sales model and its \"build-to-order\" or \"configure to order\" approach to manufacturing—delivering individual PCs configured to customer specifications. Dell was a pure hardware vendor for much of its existence, but with the acquisition in 2009 of Perot Systems, Dell entered the market for IT services. The company has since made additional acquisitions in storage and networking systems, with the aim of expanding their portfolio from offering computers only to delivering complete solutions for enterprise customers.\n\nDell was listed at number 51 in the \"Fortune 500\" list, until 2014. After going private in 2013, the newly confidential nature of its financial information prevents the company from being ranked by Fortune. In 2015, it was the third largest PC vendor in the world after Lenovo and HP. Dell is currently the #1 shipper of PC monitors in the world. Dell is the sixth largest company in Texas by total revenue, according to \"Fortune\" magazine. It is the second largest non-oil company in Texas – behind AT&T – and the largest company in the Greater Austin area. It was a publicly traded company (), as well as a component of the NASDAQ-100 and S&P 500, until it was taken private in a leveraged buyout which closed on October 30, 2013.\n\nIn 2016, Dell acquired the enterprise technology firm EMC Corporation; following the completion of the purchase, Dell and EMC became divisions of Dell Technologies.\n\nDell traces its origins to 1984, when Michael Dell created Dell Computer Corporation, which at the time did business as \"PC's Limited\", while a student of the University of Texas at Austin. The dorm-room headquartered company sold IBM PC-compatible computers built from stock components. Dell dropped out of school to focus full-time on his fledgling business, after getting $1,000 in expansion-capital from his family.\nIn 1985, the company produced the first computer of its own design, the \"Turbo PC\", which sold for $795. PC's Limited advertised its systems in national computer magazines for sale directly to consumers and custom assembled each ordered unit according to a selection of options. The company grossed more than $73 million in its first year of operation.\n\nIn 1986, Michael Dell brought in Lee Walker, a 51-year-old venture capitalist, as president and chief operating officer, to serve as Dell's mentor and implement Dell's ideas for growing the company. Walker was also instrumental in recruiting members to the board of directors when the company went public in 1988. Walker retired in 1990 due to health, and Michael Dell hired Morton Meyerson, former CEO and president of Electronic Data Systems to transform the company from a fast-growing medium-sized firm into a billion-dollar enterprise.\n\nThe company dropped the PC's Limited name in 1987 to become Dell Computer Corporation and began expanding globally. In June 1988, Dell's market capitalization grew by $30 million to $80 million from its June 22 initial public offering of 3.5 million shares at $8.50 a share. In 1992, \"Fortune\" magazine included Dell Computer Corporation in its list of the world's 500 largest companies, making Michael Dell the youngest CEO of a Fortune 500 company ever.\n\nIn 1993, to complement its own direct sales channel, Dell planned to sell PCs at big-box retail outlets such as Wal-Mart, which would have brought in an additional $125 million in annual revenue. Bain consultant Kevin Rollins persuaded Michael Dell to pull out of these deals, believing they would be money losers in the long run. Margins at retail were thin at best and Dell left the reseller channel in 1994. Rollins would soon join Dell full-time and eventually become the company President and CEO.\n\nOriginally, Dell did not emphasize the consumer market, due to the higher costs and unacceptably low-profit margins in selling to individuals and households; this changed when the company's Internet site took off in 1996 and 1997. While the industry's average selling price to individuals was going down, Dell's was going up, as second- and third-time computer buyers who wanted powerful computers with multiple features and did not need much technical support were choosing Dell. Dell found an opportunity among PC-savvy individuals who liked the convenience of buying direct, customizing their PC to their means, and having it delivered in days. In early 1997, Dell created an internal sales and marketing group dedicated to serving the home market and introduced a product line designed especially for individual users.\n\nFrom 1997 to 2004, Dell enjoyed steady growth and it gained market share from competitors even during industry slumps. During the same period, rival PC vendors such as Compaq, Gateway, IBM, Packard Bell, and AST Research struggled and eventually left the market or were bought out. Dell surpassed Compaq to become the largest PC manufacturer in 1999. Operating costs made up only 10 percent of Dell's $35 billion in revenue in 2002, compared with 21 percent of revenue at Hewlett-Packard, 25 percent at Gateway, and 46 percent at Cisco. In 2002, when Compaq merged with Hewlett Packard (the fourth-place PC maker), the newly combined Hewlett Packard took the top spot but struggled and Dell soon regained its lead. Dell grew the fastest in the early 2000s.\n\nDell attained and maintained the top rating in PC reliability and customer service/technical support, according to \"Consumer Reports\", year after year, during the mid-to-late 90s through 2001 right before Windows XP was released.\n\nIn 1996, Dell began selling computers through its website.\n\nIn the mid-1990s, Dell expanded beyond desktop computers and laptops by selling servers, starting with low-end servers. The major three providers of servers at the time were IBM, Hewlett Packard, and Compaq, many of which were based on proprietary technology, such as IBM's Power4 microprocessors or various proprietary versions of the Unix operating system. Dell's new PowerEdge servers did not require a major investment in proprietary technologies, as they ran Microsoft Windows NT on Intel chips, and could be built cheaper than its competitors. Consequently, Dell's enterprise revenues, almost nonexistent in 1994, accounted for 13 percent of the company's total intake by 1998. Three years later, Dell passed Compaq as the top provider of Intel-based servers, with 31 percent of the market. Dell's first acquisition occurred in 1999 with the purchase of ConvergeNet Technologies for $332 million, after Dell had failed to develop an enterprise storage system in-house; ConvergeNet's elegant but complex technology did not fit in with Dell's commodity-producer business model, forcing Dell to write down the entire value of the acquisition.\n\nIn 2002, Dell expanded its product line to include televisions, handhelds, digital audio players, and printers. Chairman and CEO Michael Dell had repeatedly blocked President and COO Kevin Rollins's attempt to lessen the company's heavy dependency on PCs, which Rollins wanted to fix by acquiring EMC Corporation.\n\nIn 2003, the company was rebranded as simply \"Dell Inc.\" to recognize the company's expansion beyond computers.\n\nIn 2004, Michael Dell resigned as CEO while retaining the position of Chairman, handing the CEO title to Kevin Rollins, who had been President and COO since 2001. Despite no longer holding the CEO title, Dell essentially acted as a de facto co-CEO with Rollins.\n\nUnder Rollins, Dell acquired Alienware, a manufacturer of high-end PCs targeted mainly towards the gaming market.\n\nIn 2005, while earnings and sales continued to rise, sales growth slowed considerably, and the company stock lost 25% of its value that year. By June 2006, the stock traded around $25 USD which was 40% down from July 2005—the high-water mark of the company in the post-dotcom era.\n\nThe slowing sales growth has been attributed to the maturing PC market, which constituted 66% of Dell's sales, and analysts suggested that Dell needed to make inroads into non-PC businesses segments such as storage, services, and servers. Dell's price advantage was tied to its ultra-lean manufacturing for desktop PCs, but this became less important as savings became harder to find inside the company's supply chain, and as competitors such as Hewlett-Packard and Acer made their PC manufacturing operations more efficient to match Dell, weakening Dell's traditional price differentiation. Throughout the entire PC industry, declines in prices along with commensurate increases in performance meant that Dell had fewer opportunities to upsell to their customers (a lucrative strategy of encouraging buyers to upgrade the processor or memory). As a result, the company was selling a greater proportion of inexpensive PCs than before, which eroded profit margins. The laptop segment had become the fastest-growing of the PC market, but Dell produced low-cost notebooks in China like other PC manufacturers which eliminated Dell's manufacturing cost advantages, plus Dell's reliance on Internet sales meant that it missed out on growing notebook sales in big box stores. \"CNET\" has suggested that Dell was getting trapped in the increasing commoditization of high volume low margin computers, which prevented it from offering more exciting devices that consumers demanded.\n\nDespite plans of expanding into other global regions and product segments, Dell was heavily dependent on U.S. corporate PC market, as desktop PCs sold to both commercial and corporate customers accounted for 32 percent of its revenue, 85 percent of its revenue comes from businesses, and Sixty-four percent of its revenue comes from North and South America, according to its 2006 third-quarter results. U.S. shipments of desktop PCs were shrinking, and the corporate PC market which purchases PCs in upgrade cycles had largely decided to take a break from buying new systems. The last cycle started around 2002, three or so years after companies started buying PCs ahead of the perceived Y2K problems, and corporate clients were not expected to upgrade again until extensive testing of Microsoft's Windows Vista (expected in early 2007), putting the next upgrade cycle around 2008. Heavily depending on PCs, Dell had to slash prices to boost sales volumes, while demanding deep cuts from suppliers.\n\nDell had long stuck by its direct sales model. Consumers had become the main drivers of PC sales in recent years, yet there had a decline in consumers purchasing PCs through the Web or on the phone, as increasing numbers were visiting consumer electronics retail stores to try out the devices first. Dell's rivals in the PC industry, HP, Gateway and Acer, had a long retail presence and so were well poised to take advantage of the consumer shift. The lack of a retail presence stymied Dell's attempts to offer consumer electronics such as flat-panel TVs and MP3 players. Dell responded by experimenting with mall kiosks, plus quasi-retail stores in Texas and New York.\n\nDell had a reputation as a company that relied upon supply chain efficiencies to sell established technologies at low prices, instead of being an innovator. By the mid-2000s many analysts were looking to innovating companies as the next source of growth in the technology sector. Dell's low spending on R&D relative to its revenue (compared to IBM, Hewlett Packard, and Apple Inc.)—which worked well in the commoditized PC market—prevented it from making inroads into more lucrative segments, such as MP3 players and later mobile devices. Increasing spending on R&D would have cut into the operating margins that the company emphasized. Dell had done well with a horizontal organization that focused on PCs when the computing industry moved to horizontal mix-and-match layers in the 1980s, but by the mid-2000 the industry shifted to vertically integrated stacks to deliver complete IT solutions and Dell lagged far behind competitors like Hewlett Packard and Oracle.\n\nDell's reputation for poor customer service, since 2002, which was exacerbated as it moved call centres offshore and as its growth outstripped its technical support infrastructure, came under increasing scrutiny on the Web. The original Dell model was known for high customer satisfaction when PCs sold for thousands but by the 2000s, the company could not justify that level of service when computers in the same lineup sold for hundreds. Rollins responded by shifting Dick Hunter from the head of manufacturing to head of customer service. Hunter, who noted that Dell's DNA of cost-cutting \"got in the way,\" aimed to reduce call transfer times and have call center representatives resolve inquiries in one call. By 2006, Dell had spent $100 million in just a few months to improve on this and rolled out \"DellConnect\" to answer customer inquiries more quickly. In July 2006, the company started its Direct2Dell blog, and then in February 2007, Michael Dell launched IdeaStorm.com, asking customers for advice including selling Linux computers and reducing the promotional \"bloatware\" on PCs. These initiatives did manage to cut the negative blog posts from 49% to 22%, as well as reduce the \"Dell Hell\" prominent on Internet search engines.\n\nThere was also criticism that Dell used faulty components for its PCs, particularly the 11.8 million OptiPlex desktop computers sold to businesses and governments from May 2003 to July 2005, that suffered from bad capacitors made by a company called Nichicon. A battery recall in August 2006, as a result of a Dell laptop catching fire caused much negative attention for the company though later, Sony was found responsible for the faulty batteries.\n\n2006 marked the first year that Dell's growth was slower than the PC industry as a whole. By the fourth quarter of 2006, Dell lost its title of the largest PC manufacturer to rival Hewlett Packard whose Personal Systems Group was invigorated thanks to a restructuring initiated by their CEO Mark Hurd.\n\nAfter four out of five quarterly earnings reports were below expectations, Rollins resigned as President and CEO on January 31, 2007, and founder Michael Dell assumed the role of CEO again.\n\nDell announced a change campaign called \"Dell 2.0,\" reducing the number of employees and diversifying the company's products. While chairman of the board after relinquishing his CEO position, Michael Dell still had significant input in the company during Rollins' years as CEO. With the return of Michael Dell as CEO, the company saw immediate changes in operations, the exodus of many senior vice-presidents and new personnel brought in from outside the company. Michael Dell announced a number of initiatives and plans (part of the \"Dell 2.0\" initiative) to improve the company's financial performance. These include elimination of 2006 bonuses for employees with some discretionary awards, reduction in the number of managers reporting directly to Michael Dell from 20 to 12, and reduction of \"bureaucracy\". Jim Schneider retired as CFO and was replaced by Donald Carty, as the company came under an SEC probe for its accounting practices.\n\nOn April 23, 2008, Dell announced the closure of one of its biggest Canadian call-centers in Kanata, Ontario, terminating approximately 1100 employees, with 500 of those redundancies effective on the spot, and with the official closure of the center scheduled for the summer. The call-center had opened in 2006 after the city of Ottawa won a bid to host it. Less than a year later, Dell planned to double its workforce to nearly 3,000 workers add a new building. These plans were reversed, due to a high Canadian dollar that made the Ottawa staff relatively expensive, and also as part of Dell's turnaround, which involved moving these call-center jobs offshore to cut costs.\nThe company had also announced the shutdown of its Edmonton, Alberta office, losing 900 jobs. In total, Dell announced the ending of about 8,800 jobs in 2007–2008 — 10% of its workforce.\n\nBy the late 2000s, Dell's \"configure to order\" approach of manufacturing—delivering individual PCs configured to customer specifications from its US facilities was no longer as efficient or competitive with high-volume Asian contract manufacturers as PCs became powerful low-cost commodities. Dell closed plants that produced desktop computers for the North American market, including the Mort Topfer Manufacturing Center in Austin, Texas (original location) and Lebanon, Tennessee (opened in 1999) in 2008 and early 2009, respectively. The desktop production plant in Winston-Salem, North Carolina, received US$280 million in incentives from the state and opened in 2005, but ceased operations in November 2010. Dell's contract with the state required them to repay the incentives for failing to meet the conditions, and they sold the North Carolina plant to Herbalife. Most of the work that used to take place in Dell's U.S. plants was transferred to contract manufacturers in Asia and Mexico, or some of Dell's own factories overseas. The Miami, Florida, facility of its Alienware subsidiary remains in operation, while Dell continues to produce its servers (its most profitable products) in Austin, Texas. On January 8, 2009, Dell announced the closure of its manufacturing plant in Limerick, Ireland, with the loss of 1,900 jobs and the transfer of production to its plant in Łodź in Poland.\n\nThe release of Apple's iPad tablet computer had a negative impact on Dell and other major PC vendors, as consumers switched away from desktop and laptop PCs. Dell's own mobility division has not managed success with developing smartphones or tablets, whether running Windows or Google Android. The Dell Streak was a failure commercially and critically due to its outdated OS, numerous bugs, and low resolution screen. \"InfoWorld\" suggested that Dell and other OEMs saw tablets as a short-term, low-investment opportunity running Google Android, an approach that neglected user interface and failed to gain long term market traction with consumers. Dell has responded by pushing higher-end PCs, such as the XPS line of notebooks, which do not compete with the Apple iPad and Kindle Fire tablets. The growing popularity of smartphones and tablet computers instead of PCs drove Dell's consumer segment to an operating loss in Q3 2012. In December 2012, Dell suffered its first decline in holiday sales in five years, despite the introduction of Windows 8.\n\nIn the shrinking PC industry, Dell continued to lose market share, as it dropped below Lenovo in 2011 to fall to number three in the world. Dell and fellow American contemporary Hewlett Packard came under pressure from Asian PC manufacturers Lenovo, Asus, and Acer, all of which had lower production costs and willing to accept lower profit margins. In addition, while the Asian PC vendors had been improving their quality and design, for instance Lenovo's ThinkPad series was winning corporate customers away from Dell's laptops, Dell's customer service and reputation had been slipping. Dell remained the second-most profitable PC vendor, as it took 13 percent of operating profits in the PC industry during Q4 2012, behind Apple Inc.'s Macintosh that took 45 percent, seven percent at Hewlett Packard, six percent at Lenovo and Asus, and one percent for Acer.\n\nDell has been attempting to offset its declining PC business, which still accounted for half of its revenue and generates steady cash flow, by expanding into the enterprise market with servers, networking, software, and services. It avoided many of the acquisition writedowns and management turnover that plagued its chief rival Hewlett Packard. Dell also managed some success in taking advantage of its high-touch direct sales heritage to establish close relationships and design solutions for clients. Despite spending $13 billion on acquisitions to diversify its portfolio beyond hardware, the company was unable to convince the market that it could thrive or made the transformation in the post-PC world, as it suffered continued declines in revenue and share price. Dell's market share in the corporate segment was previously a \"moat\" against rivals but this has no longer been the case as sales and profits have fallen precipitously.\n\nAfter several weeks of rumors, which started around January 11, 2013, Dell announced on February 5, 2013 that it had struck a $24.4 billion leveraged buyout deal, that would have delisted its shares from the NASDAQ and Hong Kong Stock Exchange and taken it private. Reuters reported that Michael Dell and Silver Lake Partners, aided by a $2 billion loan from Microsoft, would acquire the public shares at $13.65 apiece. The $24.4 billion buyout was projected to be the largest leveraged buyout backed by private equity since the 2007 financial crisis. It is also the largest technology buyout ever, surpassing the 2006 buyout of Freescale Semiconductor for $17.5 billion.\n\nThe founder of Dell, Michael Dell, said of the February offer \"I believe this transaction will open an exciting new chapter for Dell, our customers and team members\". Dell rival Lenovo reacted to the buyout, saying \"the financial actions of some of our traditional competitors will not substantially change our outlook\".\nIn March 2013, the Blackstone Group and Carl Icahn expressed interest in purchasing Dell. In April 2013, Blackstone withdrew their offer, citing deteriorating business. Other private equity firms such as KKR & Co. and TPG Capital declined to submit alternative bids for Dell, citing the uncertain market for personal computers and competitive pressures, so the \"wide-open bidding war\" never materialized. Analysts said that the biggest challenge facing Silver Lake would be to find an “exit strategy” to profit from its investment, which would be when the company would hold an IPO to go public again, and one warned “But even if you can get a $25bn enterprise value for Dell, it will take years to get out.”\n\nIn May 2013, Dell joined his board in voting for his offer. The following August he reached a deal with the special committee on the board for $13.88 (a raised price of $13.75 plus a special dividend of 13 cents per share), as well as a change to the voting rules. The $13.88 cash offer (plus a $.08 per share dividend for the third fiscal quarter) was accepted on September 12 and closed on October 30, 2013, ending Dell's 25-year run as a publicly traded company.\n\nAfter the buyout, the newly private Dell offered a Voluntary Separation Programme that they expected to reduce their workforce by up to seven percent. The reception to the program so exceeded the expectations that Dell may be forced to hire new staff to make up for the losses.\n\nOn November 19, 2015, Dell, alongside ARM Holdings, Cisco Systems, Intel, Microsoft, and Princeton University, founded the OpenFog Consortium, to promote interests and development in fog computing.\n\nIn July 2018, Dell announced intentions to become a publicly-traded company again by paying $21.7 billion in both cash and stock to buy back shares from its stake in VMware.\n\nIn November 2018, Carl Icahn (9.3% owner of Dell) sues the company over plans to go public.\n\nOn October 12, 2015, Dell announced its intent to acquire the enterprise software and storage company EMC Corporation. At $67 billion, it has been labeled the \"highest-valued tech acquisition in history\".\n\nThe announcement came two years after Dell Inc. returned to private ownership, claiming that it faced bleak prospects and would need several years out of the public eye to rebuild its business. It's thought that the company's value has roughly doubled since then. EMC was being pressured by Elliott Management, a hedge fund holding 2.2% of EMC's stock, to reorganize their unusual \"Federation\" structure, in which EMC's divisions were effectively being run as independent companies. Elliott argued this structure deeply undervalued EMC's core \"EMC II\" data storage business, and that increasing competition between EMC II and VMware products was confusing the market and hindering both companies. \"The Wall Street Journal\" estimated that in 2014 Dell had revenue of $27.3billion from personal computers and $8.9bn from servers, while EMC had $16.5bn from EMC II, $1bn from RSA Security, $6bn from VMware, and $230million from Pivotal Software. EMC owns around 80 percent of the stock of VMware. The proposed acquisition will maintain VMware as a separate company, held via a new tracking stock, while the other parts of EMC will be rolled into Dell. Once the acquisition closes Dell will again publish quarterly financial results, having ceased these on going private in 2013.\n\nThe combined business was expected to address the markets for scale-out architecture, converged infrastructure and private cloud computing, playing to the strengths of both EMC and Dell. Commentators have questioned the deal, with FBR Capital Markets saying that though it makes a \"ton of sense\" for Dell, it's a \"nightmare scenario that would lack strategic synergies\" for EMC. \"Fortune\" said there was a lot for Dell to like in EMC's portfolio, but \"does it all add up enough to justify tens of billions of dollars for the entire package? Probably not.\" \"The Register\" reported the view of William Blair & Company that the merger would \"blow up the current IT chess board\", forcing other IT infrastructure vendors to restructure to achieve scale and vertical integration. The value of VMware stock fell 10% after the announcement, valuing the deal at around $63–64bn rather than the $67bn originally reported. Key investors backing the deal besides Dell were Singapore's Temasek Holdings and Silver Lake Partners.\n\nOn September 7, 2016, Dell completed its acquisition of EMC. Post-acquisition, Dell was re-organized with a new parent company, Dell Technologies; Dell's consumer and workstation businesses are internally referred to as the Dell Client Solutions Group, and is one of the company's three main business divisions alongside Dell EMC and VMware.\n\nDell's headquarters is located in Round Rock, Texas. the company employed about 14,000 people in central Texas and was the region's largest private employer, which has of space. As of 1999 almost half of the general fund of the city of Round Rock originated from sales taxes generated from the Dell headquarters.\n\nDell previously had its headquarters in the Arboretum complex in northern Austin, Texas. In 1989 Dell occupied in the Arboretum complex. In 1990, Dell had 1,200 employees in its headquarters. In 1993, Dell submitted a document to Round Rock officials, titled \"Dell Computer Corporate Headquarters, Round Rock, Texas, May 1993 Schematic Design.\" Despite the filing, during that year the company said that it was not going to move its headquarters. In 1994, Dell announced that it was moving most of its employees out of the Arboretum, but that it was going to continue to occupy the top floor of the Arboretum and that the company's official headquarters address would continue to be the Arboretum. The top floor continued to hold Dell's board room, demonstration center, and visitor meeting room. Less than one month prior to August 29, 1994, Dell moved 1,100 customer support and telephone sales employees to Round Rock. Dell's lease in the Arboretum had been scheduled to expire in 1994.\nBy 1996, Dell was moving its headquarters to Round Rock. As of January 1996, 3,500 people still worked at the current Dell headquarters. One building of the Round Rock headquarters, Round Rock 3, had space for 6,400 employees and was scheduled to be completed in November 1996. In 1998 Dell announced that it was going to add two buildings to its Round Rock complex, adding of office space to the complex.\n\nIn 2000, Dell announced that it would lease of space in the Las Cimas office complex in unincorporated Travis County, Texas, between Austin and West Lake Hills, to house the company's executive offices and corporate headquarters. 100 senior executives were scheduled to work in the building by the end of 2000. In January 2001, the company leased the space in Las Cimas 2, located along Loop 360. Las Cimas 2 housed Dell's executives, the investment operations, and some corporate functions. Dell also had an option for of space in Las Cimas 3. After a slowdown in business required reducing employees and production capacity, Dell decided to sublease its offices in two buildings in the Las Cimas office complex. In 2002 Dell announced that it planned to sublease its space to another tenant; the company planned to move its headquarters back to Round Rock once a tenant was secured. By 2003, Dell moved its headquarters back to Round Rock. It leased all of Las Cimas I and II, with a total of , for about a seven-year period after 2003. By that year roughly of that space was absorbed by new subtenants.\n\nIn 2008, Dell switched the power sources of the Round Rock headquarters to more environmentally friendly ones, with 60% of the total power coming from TXU Energy wind farms and 40% coming from the Austin Community Landfill gas-to-energy plant operated by Waste Management, Inc.\n\nDell facilities in the United States are located in Austin, Texas; Nashua, New Hampshire; Nashville, Tennessee; Oklahoma City, Oklahoma; Peoria, Illinois; Hillsboro, Oregon (Portland area); Winston-Salem, North Carolina; Eden Prairie, Minnesota (Dell Compellent); Bowling Green, Kentucky; Lincoln, Nebraska; and Miami, Florida. Facilities located abroad include Penang, Malaysia; Xiamen, China; Bracknell, UK; Manila, Philippines Chennai, India; Hyderabad, India; Noida, India; Hortolandia and Porto Alegre, Brazil; Bratislava, Slovakia; Łódź, Poland; Panama City, Panama; Dublin and Limerick, Ireland; Casablanca, Morocco and Montpellier, France.\n\nThe US and India are the only countries that have all Dell's business functions and provide support globally: research and development, manufacturing, finance, analysis, and customer care.\n\nFrom its early beginnings, Dell operated as a pioneer in the \"configure to order\" approach to manufacturing—delivering individual PCs configured to customer specifications. In contrast, most PC manufacturers in those times delivered large orders to intermediaries on a quarterly basis.\n\nTo minimize the delay between purchase and delivery, Dell has a general policy of manufacturing its products close to its customers. This also allows for implementing a just-in-time (JIT) manufacturing approach, which minimizes inventory costs. Low inventory is another signature of the Dell business model—a critical consideration in an industry where components depreciate very rapidly.\n\nDell's manufacturing process covers assembly, software installation, functional testing (including \"burn-in\"), and quality control. Throughout most of the company's history, Dell manufactured desktop machines in-house and contracted out manufacturing of base notebooks for configuration in-house. The company's approach has changed, as cited in the 2006 Annual Report, which states, \"We are continuing to expand our use of original design manufacturing partnerships and manufacturing outsourcing relationships.\" \"The Wall Street Journal\" reported in September 2008 that \"Dell has approached contract computer manufacturers with offers to sell\" their plants. By the late 2000s, Dell's \"configure to order\" approach of manufacturing—delivering individual PCs configured to customer specifications from its US facilities was no longer as efficient or competitive with high-volume Asian contract manufacturers as PCs became powerful low-cost commodities.\n\nAssembly of desktop computers for the North American market formerly took place at Dell plants in Austin, Texas (original location) and Lebanon, Tennessee (opened in 1999), which have been closed in 2008 and early 2009, respectively. The plant in Winston-Salem, North Carolina received $280 million USD in incentives from the state and opened in 2005, but ceased operations in November 2010, and Dell's contract with the state requires them to repay the incentives for failing to meet the conditions. Most of the work that used to take place in Dell's U.S. plants was transferred to contract manufacturers in Asia and Mexico, or some of Dell's own factories overseas. The Miami, Florida facility of its Alienware subsidiary remains in operation, while Dell continues to produce its servers (its most profitable products) in Austin, Texas.\n\nDell assembled computers for the EMEA market at the Limerick facility in the Republic of Ireland, and once employed about 4,500 people in that country. Dell began manufacturing in Limerick in 1991 and went on to become Ireland's largest exporter of goods and its second-largest company and foreign investor. On January 8, 2009, Dell announced that it would move all Dell manufacturing in Limerick to Dell's new plant in the Polish city of Łódź by January 2010.\nEuropean Union officials said they would investigate a €52.7million aid package the Polish government used to attract Dell away from Ireland. European Manufacturing Facility 1 (EMF1, opened in 1990) and EMF3 form part of the Raheen Industrial Estate near Limerick. EMF2 (previously a Wang facility, later occupied by Flextronics, situated in Castletroy) closed in 2002, and Dell Inc. has consolidated production into EMF3 (EMF1 now contains only offices). Subsidies from the Polish government did keep Dell for a long time. After ending assembly in the Limerick plant the Cherrywood Technology Campus in Dublin was the largest Dell office in the republic with over 1200 people in sales (mainly UK & Ireland), support (enterprise support for EMEA) and research and development for cloud computing, but no more manufacturing except Dell's Alienware subsidiary, which manufactures PCs in an Athlone, Ireland plant. Whether this facility will remain in Ireland is not certain. Construction of EMF4 in Łódź, Poland has : Dell started production there in autumn 2007.\n\nDell opened plants in Penang, Malaysia in 1995, and in Xiamen, China in 1999. These facilities serve the Asian market and assemble 95% of Dell notebooks. Dell Inc. has invested an estimated $60 million in a new manufacturing unit in Chennai, India, to support the sales of its products in the Indian subcontinent. Indian-made products bear the \"Made in India\" mark. In 2007 the Chennai facility had the target of producing 400,000 desktop PCs, and plans envisaged it starting to produce notebook PCs and other products in the second half of 2007.\n\nDell moved desktop, notebook and PowerEdge server manufacturing for the South American market from the Eldorado do Sul plant opened in 1999, to a new plant in Hortolandia, Brazil in 2007.\n\nThe corporation markets specific brand names to different market segments.\n\nIts Business/Corporate class represent brands where the company advertising emphasizes long life-cycles, reliability, and serviceability. Such brands include:\n\nDell's Home Office/Consumer class emphasizes value, performance, and expandability. These brands include:\n\n\n\n\nDell's Peripherals class includes USB keydrives, LCD televisions, and printers; Dell monitors includes LCD TVs, plasma TVs and projectors for HDTV and monitors. Dell UltraSharp is further a high-end brand of monitors.\n\nDell service and support brands include the \"Dell Solution Station\" (extended domestic support services, previously \"Dell on Call\"), \"Dell Support Center\" (extended support services abroad), \"Dell Business Support\" (a commercial service-contract that provides an industry-certified technician with a lower call-volume than in normal queues), \"Dell Everdream Desktop Management\" (\"Software as a Service\" remote-desktop management, originally a SaaS company founded by Elon Musk's cousin, Lyndon Rive, which Dell bought in 2007), and \"Your Tech Team\" (a support-queue available to home users who purchased their systems either through Dell's website or through Dell phone-centers).\n\nDiscontinued products and brands include Axim (PDA; discontinued April 9, 2007), Dimension (home and small office desktop computers; discontinued July 2007), Dell Digital Jukebox (MP3 player; discontinued August 2006), Dell PowerApp (application-based servers), and Dell Optiplex (desktop and tower computers previously supported to run server and desktop operating systems).\n\nIn November 2015 it emerged that several Dell computers had shipped with an identical pre-installed root certificate known as \"eDellRoot\". This raised such security risks as attackers impersonating HTTPS-protected websites such as Google and Bank of America and malware being signed with the certificate to bypass Microsoft software filtering. Dell apologised and offered a removal tool.\n\nAlso in November 2015, a researcher discovered that customers with diagnostic program Dell Foundation Services could be digitally tracked using the unique service tag number assigned to them by the program. This was possible even if a customer enabled private browsing and deleted their browser cookies. \"Ars Technica\" recommended that Dell customers uninstall the program until the issue was addressed.\n\nThe board consists of nine directors. Michael Dell, the founder of the company, serves as chairman of the board and chief executive officer. Other board members include Don Carty, Judy Lewent, Klaus Luft, Alex Mandl, and Sam Nunn. Shareholders elect the nine board members at meetings, and those board members who do not get a majority of votes must submit a resignation to the board, which will subsequently choose whether or not to accept the resignation. The board of directors usually sets up five committees having oversight over specific matters. These committees include the Audit Committee, which handles accounting issues, including auditing and reporting; the Compensation Committee, which approves compensation for the CEO and other employees of the company; the Finance Committee, which handles financial matters such as proposed mergers and acquisitions; the Governance and Nominating Committee, which handles various corporate matters (including nomination of the board); and the Antitrust Compliance Committee, which attempts to prevent company practices from violating antitrust laws. \n\nDay-to-day operations of the company are run by the Global Executive Management Committee, which sets strategic direction. Dell has regional senior vice-presidents for countries other than the United States, including David Marmonti for EMEA and Stephen J. Felice for Asia/Japan. , other officers included Martin Garvin (senior vice president for worldwide procurement) and Susan Sheskey (vice president and Chief Information Officer). \n\nDell advertisements have appeared in several types of media including television, the Internet, magazines, catalogs and newspapers. Some of Dell Inc's marketing strategies include lowering prices at all times of the year, free bonus products (such as Dell printers), and free shipping to encourage more sales and stave off competitors. In 2006, Dell cut its prices in an effort to maintain its 19.2% market share. This also cut profit margins by more than half, from 8.7 to 4.3 percent. To maintain its low prices, Dell continues to accept most purchases of its products via the Internet and through the telephone network, and to move its customer-care division to India and El Salvador.\n\nA popular United States television and print ad campaign in the early 2000s featured the actor Ben Curtis playing the part of \"Steven\", a lightly mischievous blond-haired youth who came to the assistance of bereft computer purchasers. Each television advertisement usually ended with Steven's catch-phrase: \"Dude, you're gettin' a Dell!\"\n\nA subsequent advertising campaign featured interns at Dell headquarters (with Curtis' character appearing in a small cameo at the end of one of the first commercials in this particular campaign).\n\nIn 2007, Dell switched advertising agencies in the US from BBDO to Working Mother Media. In July 2007, Dell released new advertising created by Working Mother to support the Inspiron and XPS lines. The ads featured music from the Flaming Lips and Devo who re-formed especially to record the song in the ad \"Work it Out\". Also in 2007, Dell began using the slogan \"Yours is here\" to say that it customizes computers to fit customers' requirements.\n\nBeginning in 2011, Dell began hosting a conference in Austin, Texas at the Austin Convention Center titled \"Dell World\". The event featured new technology and services provided by Dell and Dell's partners. In 2011, the event was held October 12–14. In 2012, the event was held December 11–13. In 2013, the event was held December 11–13. In 2014, the event was held November 4–6. In 2015, the event was held October 20–23.\n\nIn late 2007, Dell Inc. announced that it planned to expand its program to value-added resellers (VARs), giving it the official name of \"Dell Partner Direct\" and a new Website.\n\nDell India has started Online Ecommerce website with its Dell Partner www.compuindia.com GNG Electronics Pvt Ltd termed as Dell Express Ship Affiliate(DESA).\nThe main objective was to reduce the delivery time. Customers who visit Dell India official site are given the option to buy online which then will be redirected to Dell affiliate website compuindia.com.\n\nDell also operates a captive analytics division which supports pricing, web analytics, and supply chain operations. DGA operates as a single, centralized entity with a global view of Dell's business activities. The firm supports over 500 internal customers worldwide and has created a quantified impact of over $500 million.\n\nIn 2008, Dell received press coverage over its claim of having the world's most secure laptops, specifically, its Latitude D630 and Latitude D830. At Lenovo's request, the (U.S.) National Advertising Division (NAD) evaluated the claim, and reported that Dell did not have enough evidence to support it.\n\nDell first opened their retail stores in India.\n\nIn the early 1990s, Dell sold its products through Best Buy, Costco and Sam's Club stores in the United States. Dell stopped this practice in 1994, citing low profit margins on the business, exclusively distributing through a direct-sales model for the next decade. In 2003, Dell briefly sold products in Sears stores in the U.S. In 2007, Dell started shipping its products to major retailers in the U.S. once again, starting with Sam's Club and Wal-Mart. Staples, the largest office-supply retailer in the U.S., and Best Buy, the largest electronics retailer in the U.S., became Dell retail partners later that same year.\n\nStarting in 2002, Dell opened kiosk locations in the United States to allow customers to examine products before buying them directly from the company. Starting in 2005, Dell expanded kiosk locations to include shopping malls across Australia, Canada, Singapore and Hong Kong. On January 30, 2008, Dell announced it would shut down all 140 kiosks in the U.S. due to expansion into retail stores.\n\nBy June 3, 2010, Dell had also shut down all of its mall kiosks in Australia.\n\nIn 2006, Dell Inc. opened one full store, in area, at NorthPark Center in Dallas, Texas. It operates the retail outlet seven days a week to display about 36 models, including PCs and televisions. As at the kiosks, customers can only see demonstration-computers and place orders through agents. Dell then delivers purchased items just as if the customer had placed the order by phone or over the Internet.\n\nIn addition to showcasing products, the stores also support on-site warranties and non-warranty service (\"Dell Solution Station\"). Services offered include repairing computer video-cards and removing spyware from hard drives.\n\nOn February 14, 2008, Dell closed the Service Center in its Dallas NorthPark store and laid off all the technical staff there.\n\n, Dell products shipped to one of the largest office-supply retailers in Canada, Staples Business Depot. In April 2008, Future Shop and Best Buy began carrying a subset of Dell products, such as certain desktops, laptops, printers, and monitors.\n\nSince some shoppers in certain markets show reluctance to purchase technological products through the phone or the Internet, Dell has looked into opening retail operations in some countries in Central Europe and Russia. In April 2007, Dell opened a retail store in Budapest. In October of the same year, Dell opened a retail store in Moscow.\n\nIn the UK, HMV's flagship Trocadero store has sold Dell XPS PCs since December 2007. From January 2008 the UK stores of DSGi have sold Dell products (in particular, through Currys and PC World stores). As of 2008, the large supermarket-chain Tesco has sold Dell laptops and desktops in outlets throughout the UK.\n\nIn May 2008, Dell reached an agreement with office supply chain, Officeworks (part of Coles Group), to stock a few modified models in the Inspiron desktop and notebook range. These models have slightly different model numbers, but almost replicate the ones available from the Dell Store. Dell continued its retail push in the Australian market with its partnership with Harris Technology (another part of Coles Group) in November of the same year. In addition, Dell expanded its retail distributions in Australia through an agreement with the discount electrical retailer, The Good Guys, known for \"Slashing Prices\". Dell agreed to distribute a variety of makes of both desktops and notebooks, including Studio and XPS systems in late 2008. Dell and Dick Smith Electronics (owned by Woolworths Limited) reached an agreement to expand within Dick Smith's 400 stores throughout Australia and New Zealand in May 2009 (1 year since Officeworks — owned by Coles Group — reached a deal). The retailer has agreed to distribute a variety of Inspiron and Studio notebooks, with minimal Studio desktops from the Dell range. , Dell continues to run and operate its various kiosks in 18 shopping centres throughout Australia. On March 31, 2010, Dell announced to Australian Kiosk employees that they were shutting down the Australian/New Zealand Dell kiosk program.\n\nIn Germany, Dell is selling selected smartphones and notebooks via Media Markt and Saturn, as well as some shopping websites.\n\nDell's major competitors include Hewlett-Packard (HP), Hasee, Acer, Fujitsu, Toshiba, Gateway, Sony, Asus, Lenovo, IBM, MSI, Panasonic with its Toughbook series, Samsung and Apple. Dell and its subsidiary, Alienware, compete in the enthusiast market against AVADirect, Falcon Northwest, VoodooPC (a subsidiary of HP), and other manufacturers. In the second quarter of 2006, Dell had between 18% and 19% share of the worldwide personal computer market, compared to HP with roughly 15%.\n\n, Dell lost its lead in the PC-business to Hewlett-Packard. Both Gartner and IDC estimated that in the third quarter of 2006, HP shipped more units worldwide than Dell did. Dell's 3.6% growth paled in comparison to HP's 15% growth during the same period. The problem got worse in the fourth quarter, when Gartner estimated that Dell PC shipments declined 8.9% (versus HP's 23.9% growth). As a result, at the end of 2006 Dell's overall PC market-share stood at 13.9% (versus HP's 17.4%).\n\nIDC reported that Dell lost more server market share than any of the top four competitors in that arena. IDC's Q4 2006 estimates show Dell's share of the server market at 8.1%, down from 9.5% in the previous year. This represents an 8.8% loss year-over-year, primarily to competitors EMC and IBM.\n\nIn 2001, Dell and EMC entered into a partnership whereby both companies jointly design products and Dell provided support for certain EMC products including midrange storage systems, such as fibre channel and iSCSI storage area networks. The relationship also promotes and sells OEM versions of backup, recovery, replication and archiving software. On December 9, 2008, Dell and EMC announced the multi-year extension, through 2013, of the strategic partnership with EMC. In addition, Dell expanded its product lineup by adding the EMC Celerra NX4 storage system to the portfolio of Dell/EMC family of networked storage systems and partnered on a new line of data deduplication products as part of its TierDisk family of data storage devices.\n\nOn October 17, 2011, Dell discontinued reselling all EMC storage products, ending the partnership 2 years early.\n\nDell committed to reducing greenhouse gas emissions from its global activities by 40% by 2015, with the 2008 fiscal year as the baseline year. It is listed in Greenpeace’s Guide to Greener Electronics that scores leading electronics manufacturers according to their policies on sustainability, climate and energy and how green their products are. In November 2011, Dell ranked 2nd out of 15 listed electronics makers (increasing its score to 5.1 from 4.9, which it gained in the previous ranking from October 2010).\n\nDell was the first company to publicly state a timeline for the elimination of toxic polyvinyl chloride (PVC) and brominated flame retardants (BFRs), which it planned to phase out by the end of 2009. It revised this commitment and now aims to remove toxics by the end of 2011 but only in its computing products.\nIn March 2010, Greenpeace activists protested at Dell offices in Bangalore, Amsterdam and Copenhagen calling for Dell’s founder and CEO Michael Dell to ‘drop the toxics’ and claiming that Dell's aspiration to be ‘the greenest technology company on the planet’ was ‘hypocritical’. Dell has launched its first products completely free of PVC and BFRs with the G-Series monitors (G2210 and G2410) in 2009.\n\nIn its 2012 report on progress relating to conflict minerals, the Enough Project rated Dell the eighth highest of 24 consumer electronics companies.\n\nDell became the first company in the information technology industry to establish a product-recycling goal (in 2004) and completed the implementation of its global consumer recycling-program in 2006.\nOn February 6, 2007, the National Recycling Coalition awarded Dell its \"Recycling Works\" award for efforts to promote producer responsibility.\nOn July 19, 2007, Dell announced that it had exceeded targets in working to achieve a multi-year goal of recovering 275 million pounds of computer equipment by 2009. The company reported the recovery of 78 million pounds (nearly 40,000 tons) of IT equipment from customers in 2006, a 93-percent increase over 2005; and 12.4% of the equipment Dell sold seven years earlier.\n\nOn June 5, 2007, Dell set a goal of becoming the greenest technology company on Earth for the long term. The company launched a zero-carbon initiative that includes:\n\nThe company introduced the term \"The Re-Generation\" during a round table in London commemorating 2007 World Environment Day. \"The Re-Generation\" refers to people of all ages throughout the world who want to make a difference in improving the world's environment. Dell also talked about plans to take the lead in setting an environmental standard for the technology industry and maintaining that leadership in the future.\n\nDell reports its environmental performance in an annual Corporate Social Responsibility (CSR) Report that follows the Global Reporting Initiative (GRI) protocol. Dell's 2008 CSR report ranked as \"Application Level B\" as \"checked by GRI\".\n\nThe company aims to reduce its external environmental impact through an energy-efficient evolution of products, and also reduce its direct operational impact through energy-efficiency programs. Internal energy-efficiency programs reportedly save the company more than $3 million annually in energy-cost savings. The largest component of the company's internal energy-efficiency savings comes through PC power management: the company expects to save $1.8 million in energy costs through using specialized energy-management software on a network of 50,000 PCs.\n\nIn the 1990s, Dell switched from using primarily ATX motherboards and PSU to using boards and power supplies with mechanically identical but differently wired connectors. This meant customers wishing to upgrade their hardware would have to replace parts with scarce Dell-compatible parts instead of commonly available parts. While motherboard power connections reverted to the industry standard in 2003, Dell continues to remain secretive about their motherboard pin-outs for peripherals (such as MMC readers and power on/off switches and LEDs).\n\nIn 2005, complaints about Dell more than doubled to 1,533, after earnings grew 52% that year.\n\nIn 2006, Dell acknowledged that it had problems with customer service. Issues included call transfers\nof more than 45% of calls and long wait times. Dell's blog detailed the response: \"We're spending more than a $100 million — and a lot of blood, sweat, and tears of talented people — to fix this.\" Later in the year, the company increased its spending on customer service to $150 million. Despite significant investment in this space, Dell continues to face public scrutiny with even the company's own website littered with complaints regarding the issue escalation process.\n\nOn August 17, 2007, Dell Inc. announced that after an internal investigation into its accounting practices it would restate and reduce earnings from 2003 through to the first quarter of 2007 by a total amount of between $50 million and $150 million, or 2 cents to 7 cents per share. The investigation, begun in November 2006, resulted from concerns raised by the U.S. Securities and Exchange Commission over some documents and information that Dell Inc. had submitted. It was alleged that Dell had not disclosed large exclusivity payments received from Intel for agreeing not to buy processors from rival manufacturer AMD. In 2010 Dell finally paid $100 million to settle the SEC's charges of fraud. Michael Dell and other executives also paid penalties and suffered other sanctions, without admitting or denying the charges.\n\nIn July 2009, Dell apologized after drawing the ire of the Taiwanese Consumer Protection Commission for twice refusing to honour a flood of orders against unusually low prices offered on its Taiwanese website. In the first instance, Dell offered a 19\" LCD panel for $15. In the second instance, Dell offered its Latitude E4300 notebook at NT$18,558 (US$580), 70% lower than the usual price of NT$60,900 (US$1900). Concerning the E4300, rather than honour the discount taking a significant loss, the firm withdrew orders and offered a voucher of up to NT$20,000 (US$625) a customer in compensation. The consumer rights authorities in Taiwan fined Dell NT$1 million (US$31250) for customer rights infringements. Many consumers sued the firm for the unfair compensation. A court in southern Taiwan ordered the firm to deliver 18 laptops and 76 flat-panel monitors to 31 consumers for NT$490,000 (US$15,120), less than a third of the normal price. The court said the event could hardly be regarded as mistakes, as the prestigious firm said the company mispriced its products twice in Taiwanese website within 3 weeks.\n\nAfter Michael Dell made a $24.4 billion buyout bid in August 2013, activist shareholder Carl Icahn sued the company and its board in an attempt to derail the bid and promote his own forthcoming offer.\n\n"}
{"id": "1905405", "url": "https://en.wikipedia.org/wiki?curid=1905405", "title": "Differential GPS", "text": "Differential GPS\n\nDifferential Global Positioning Systems (DGPS) are enhancements to the Global Positioning System (GPS) which provide improved location accuracy, in the range of operations of each system, from the 15-meter nominal GPS accuracy to about 10 cm in case of the best implementations.\n\nEach DGPS uses a network of fixed ground-based reference stations to broadcast the difference between the positions indicated by the GPS satellite system and known fixed positions. These stations broadcast the difference between the measured satellite pseudoranges and actual (internally computed) pseudoranges, and receiver stations may correct their pseudoranges by the same amount. The digital correction signal is typically broadcast locally over ground-based transmitters of shorter range.\n\nThe United States Coast Guard (USCG) and the Canadian Coast Guard (CCG) each run DGPS systems in the United States and Canada on longwave radio frequencies between 285 kHz and 325 kHz near major waterways and harbors. The USCG's DGPS system was named NDGPS (Nationwide DGPS) and was jointly administered by the Coast Guard and the U.S. Department of Defense's Army Corps of Engineers (USACE). It consisted of broadcast sites located throughout the inland and coastal portions of the United States including Alaska, Hawaii and Puerto Rico. Other countries have their own DGPS system.\n\nA similar system which transmits corrections from orbiting satellites instead of ground-based transmitters is called a Wide-Area DGPS (WADGPS) or Satellite Based Augmentation System.\n\nWhen GPS was first being put into service, the US military was concerned about the possibility of enemy forces using the globally available GPS signals to guide their own weapon systems. Originally, the government thought the \"coarse acquisition\" (C/A) signal would give only about 100-meter accuracy, but with improved receiver designs, the actual accuracy was 20 to 30 meters. Starting in March 1990, to avoid providing such unexpected accuracy, the C/A signal transmitted on the L1 frequency (1575.42 MHz) was deliberately degraded by offsetting its clock signal by a random amount, equivalent to about 100 meters of distance. This technique, known as \"Selective Availability\", or SA for short, seriously degraded the usefulness of the GPS signal for non-military users. More accurate guidance was possible for users of dual-frequency GPS receivers which also received the L2 frequency (1227.6 MHz), but the L2 transmission, intended for military use, was encrypted and was available only to authorized users with the decryption keys.\n\nThis presented a problem for civilian users who relied upon ground-based radio navigation systems such as LORAN, VOR and NDB systems costing millions of dollars each year to maintain. The advent of a global navigation satellite system (GNSS) could provide greatly improved accuracy and performance at a fraction of the cost. The accuracy inherent in the S/A signal was however too poor to make this realistic. The military received multiple requests from the Federal Aviation Administration (FAA), United States Coast Guard (USCG) and United States Department of Transportation (DOT) to set S/A aside to enable civilian use of GNSS, but remained steadfast in its objection on grounds of security.\n\nThrough the early to mid 1980s, a number of agencies developed a solution to the SA \"problem\". Since the SA signal was changed slowly, the effect of its offset on positioning was relatively fixed – that is, if the offset was \"100 meters to the east\", that offset would be true over a relatively wide area. This suggested that broadcasting this offset to local GPS receivers could eliminate the effects of SA, resulting in measurements closer to GPS's theoretical performance, around 15 meters. Additionally, another major source of errors in a GPS fix is due to transmission delays in the ionosphere, which could also be measured and corrected for in the broadcast. This offered an improvement to about 5 meters accuracy, more than enough for most civilian needs.\n\nThe US Coast Guard was one of the more aggressive proponents of the DGPS system, experimenting with the system on an ever-wider basis through the late 1980s and early 1990s. These signals are broadcast on marine longwave frequencies, which could be received on existing radiotelephones and fed into suitably equipped GPS receivers. Almost all major GPS vendors offered units with DGPS inputs, not only for the USCG signals, but also aviation units on either VHF or commercial AM radio bands.\n\nThey started sending out \"production quality\" DGPS signals on a limited basis in 1996, and rapidly expanded the network to cover most US ports of call, as well as the Saint Lawrence Seaway in partnership with the Canadian Coast Guard. Plans were put into place to expand the system across the US, but this would not be easy. The quality of the DGPS corrections generally fell with distance, and large transmitters capable of covering large areas tend to cluster near cities. This meant that lower-population areas, notably in the midwest and Alaska, would have little coverage by ground-based GPS. As of November 2013 the USCG's national DGPS system consisted of 85 broadcast sites which provide dual coverage to almost the entire US coastline and inland navigable waterways including Alaska, Hawaii, and Puerto Rico. In addition the system provided single or dual coverage to a majority of the inland portion of United States. Instead, the FAA (and others) started studying broadcasting the signals across the entire hemisphere from communications satellites in geostationary orbit. This led to the Wide Area Augmentation System (WAAS) and similar systems, although these are generally not referred to as DGPS, or alternatively, \"wide-area DGPS\". WAAS offers accuracy similar to the USCG's ground-based DGPS networks, and there has been some argument that the latter will be turned off as WAAS becomes fully operational.\n\nBy the mid-1990s it was clear that the SA system was no longer useful in its intended role. DGPS would render it ineffective over the US, precisely where it was considered most needed. Additionally, experience during the Gulf War demonstrated that the widespread use of civilian receivers by U.S. forces meant that leaving SA turned on was thought to harm the U.S. more than if it were turned off. After many years of pressure, it took an executive order by President Bill Clinton to get SA turned off permanently in 2000.\n\nNevertheless, by this point DGPS had evolved into a system for providing more accuracy than even a non-SA GPS signal could provide on its own. There are several other sources of error which share the same characteristics as SA in that they are the same over large areas and for \"reasonable\" amounts of time. These include the ionospheric effects mentioned earlier, as well as errors in the satellite position ephemeris data and clock drift on the satellites. Depending on the amount of data being sent in the DGPS correction signal, correcting for these effects can reduce the error significantly, the best implementations offering accuracies of under 10 cm.\n\nIn addition to continued deployments of the USCG and FAA sponsored systems, a number of vendors have created commercial DGPS services, selling their signal (or receivers for it) to users who require better accuracy than the nominal 15 meters GPS offers. Almost all commercial GPS units, even hand-held units, now offer DGPS data inputs, and many also support WAAS directly. To some degree, a form of DGPS is now a natural part of most GPS operations.\n\nA reference station calculates differential corrections for its own location and time. Users may be up to 200 nautical miles (370 km) from the station, however, and some of the compensated errors vary with space: specifically, satellite ephemeris errors and those introduced by ionospheric and tropospheric distortions. For this reason, the accuracy of DGPS decreases with distance from the reference station. The problem can be aggravated if the user and the station lack \"inter visibility\"—when they are unable to see the same satellites.\n\nThe United States \"Federal Radionavigation Plan\" and the IALA \"Recommendation on the Performance and Monitoring of DGNSS Services in the Band 283.5–325 kHz\" cite the United States Department of Transportation's 1993 estimated error growth of 0.67 m per 100 km from the broadcast site but measurements of accuracy across the Atlantic, in Portugal, suggest a degradation of just 0.22 m per 100 km.\n\nDGPS can refer to any type of Ground-Based Augmentation System (GBAS). There are many operational systems in use throughout the world, according to the US Coast Guard, 47 countries operate systems similar to the US NDGPS (Nationwide Differential Global Positioning System).\n\nA list can be found at World DGPS Database for Dxers\n\nEuropean DGPS network has been developed mainly by the Finnish and Swedish maritime administrations in order to improve safety in the archipelago between the two countries.\n\nIn the UK and Ireland, the system was implemented as a maritime navigation aid to fill the gap left by the demise of the Decca Navigator System in 2000. With a network of 12 transmitters sited around the coastline and three control stations, it was set up in 1998 by the countries' respective General Lighthouse Authorities (GLA) — Trinity House covering England, Wales and the Channel Islands, the Northern Lighthouse Board covering Scotland and the Isle of Man and the Commissioners of Irish Lights, covering the whole of Ireland. Transmitting on the 300-kHz band, the system underwent testing and two additional transmitters were added before the system was declared operational in 2002.\n\nTrinity House - DGNSS Stations: UK and Ireland\n\nEffective Solutions (Data Products) - European Differential Beacon Transmitters - Details and map\n\nThe United States Department of Transportation, in conjunction with the Federal Highway Administration, the Federal Railroad Administration and the National Geodetic Survey appointed the Coast Guard as the maintaining agency for the U.S. Nationwide DGPS network (NDGPS). The system is an expansion of the previous Maritime Differential GPS (MDGPS), which the Coast Guard began in the late 1980s and completed in March 1999. MDGPS covered only coastal waters, the Great Lakes, and the Mississippi River inland waterways, while NDGPS expands this to include complete coverage of the continental United States. The centralized Command and Control unit is the USCG Navigation Center, based in Alexandria, VA. There are currently 85 NDGPS sites in the US network, administered by the U.S. Department of Homeland Security Navigation Center.\n\nIn 2015, the USCG and USACE sought comments on a planned phasing-out of the U.S. DGPS system. In response to the comments received, a subsequent 2016 Federal Register notice announced that 46 stations would remain in service and \"available to users in the maritime and coastal regions\". In spite of this decision, USACE decommissioned its remaining 7 sites and, in March 2018, the USCG announced that it would decommission its remaining stations by 2020.\n\nThe Canadian system is similar to the US system and is primarily for maritime usage covering the Atlantic and Pacific coast as well as the Great Lakes and Saint Lawrence Seaway.\nAustralia runs three DGPS systems: one is mainly for marine navigation, broadcasting its signal on the long-wave band; another is used for land surveys and land navigation, and has corrections broadcast on the Commercial FM radio band. The third at Sydney airport is currently undergoing testing for precision landing of aircraft (2011), as a backup to the Instrument Landing System at least until 2015. It is called the Ground Based Augmentation System. Corrections to aircraft position are broadcast via the aviation VHF band.\n\nPost-processing is used in Differential GPS to obtain precise positions of unknown points by relating them to known points such as survey markers.\n\nThe GPS measurements are usually stored in computer memory in the GPS receivers, and are subsequently transferred to a computer running the GPS post-processing software. The software computes baselines using simultaneous measurement data from two or more GPS receivers.\n\nThe baselines represent a three-dimensional line drawn between the two points occupied by each pair of GPS antennas. The post-processed measurements allow more precise positioning, because most GPS errors affect each receiver nearly equally, and therefore can be cancelled out in the calculations.\n\nDifferential GPS measurements can also be computed in real time by some GPS receivers if they receive a correction signal using a separate radio receiver, for example in Real Time Kinematic (RTK) surveying or navigation.\n\nThe improvement of GPS positioning doesn't require simultaneous measurements of two or more receivers in any case, but can also be done by special use of a \"single\" device. In the 1990s when even handheld receivers were quite expensive, some methods of quasi-differential GPS were developed, using the receiver by quick turns of positions or loops of 3-10 survey points.\n\n\n"}
{"id": "55890636", "url": "https://en.wikipedia.org/wiki?curid=55890636", "title": "DingDong", "text": "DingDong\n\nDingDong is a line of smart speakers created by Chinese company LingLong (a partnership between JD.com and iFlytek). The metal A1 was released in 2016, followed by the release of the cheaper A3 and Q1 the following year. The A1 has a height of 9.5 inches (24 centimeters) and has a square base which transitions to a cylinder at the top. The devices are capable of reading the news, music streaming, weather updates, and online shopping. Versions are available that speak either Mandarin or Cantonese.\n"}
{"id": "27320105", "url": "https://en.wikipedia.org/wiki?curid=27320105", "title": "Ericus Verkade", "text": "Ericus Verkade\n\nEricus Gerhardus Verkade (20 November 1835 – 8 February 1907) was a Dutch businessman and the founder of the Verkade manufacturing company in 1886. \n\nEricus Gerhardus Verkade was born on 20 November 1835 in Vlaardingen in the Netherlands. He was named after his father, the notary Ericus Gerhardus Verkade Sr (1801-1835), who died one month before he was born. Verkade's mother, Geertruida van Gelder, was from Wormerveer and moved her family back to her birthplace shortly after Ericus Jr's birth.\n\nSoon after school, Verkade started a vegetable oil factory, which burned out in 1875. The next eight years he and his brother-in-law traded in grains. On May 2, 1886, he started a steam-powered bakery, which company he first named \"De Ruyter\" for the first mill in Zaandam that milled flour--the original \"ruiter\" (\"chevalier\") was on the company logo until 1994. Verkade originally baked bread and rusk, but soon expanded to make cookies, chocolate, and snacks. Verkade became a household name in the Netherlands.\n\nIn 1857 in Zaandam, Verkade married Trijntje Smit (1835-1863) in 1857, with whom he had a daughter. In 1865 he married Eduarda Thalia Koning (1841-1917), with whom he had 7 children. His son Jan Verkade (1868-1948) became a well-known Post-Impressionist artist. Jan was the twin brother of Ericus Gerhardus (1868-1927) who would take over his father's business.\n\nVerkade died on 8 February 1907 in Hilversum in the Netherlands.\n"}
{"id": "22512216", "url": "https://en.wikipedia.org/wiki?curid=22512216", "title": "Fender pier", "text": "Fender pier\n\nA fender pier is a structure built to protect another structure from damage, such as by ship collision.\nThe structure in the river around the swing span is a fender pier\n"}
{"id": "53550860", "url": "https://en.wikipedia.org/wiki?curid=53550860", "title": "Ford Instrument Company", "text": "Ford Instrument Company\n\nThe Ford Instrument Company was a U.S. corporation known for being the primary supplier of fire control Rangekeepers and analog computers for the United States Navy before and during World War Two.\n\nA personal blog, Doug Coward's Analog History Museum includes a page with details for the Ford Instrument Company Computer Mark I that was used after 1939 on WW II naval guns up to 5 inch and anti-aircraft guns . This page has a background stating that the Ford Instrument Company is a subsidiary of Sperry Rand, indicating that the displayed page was supplied by Sperry while operating as Sperry Rand, 1955 and 1978.\n"}
{"id": "332079", "url": "https://en.wikipedia.org/wiki?curid=332079", "title": "Ground truth", "text": "Ground truth\n\nGround truth is a term used in various fields to refer to information provided by direct observation (i.e. empirical evidence) as opposed to information provided by inference. \n\nIn machine learning, the term \"ground truth\" refers to the accuracy of the training set's classification for supervised learning techniques. This is used in statistical models to prove or disprove research hypotheses. The term \"ground truthing\" refers to the process of gathering the proper objective (provable) data for this test. Compare with gold standard.\n\nBayesian spam filtering is a common example of supervised learning. In this system, the algorithm is manually taught the differences between spam and non-spam. This depends on the \"ground truth\" of the messages used to train the algorithm – inaccuracies in the ground truth will correlate to inaccuracies in the resulting spam/non-spam verdicts.\n\nIn remote sensing, \"ground truth\" refers to information collected on location. Ground truth allows image data to be related to real features and materials on the ground. The collection of ground-truth data enables calibration of remote-sensing data, and aids in the interpretation and analysis of what is being sensed. Examples include cartography, meteorology, analysis of aerial photographs, satellite imagery and other techniques in which data are gathered at a distance.\n\nMore specifically, ground truth may refer to a process in which a \"pixel\" on a satellite image is compared to what is there in reality (at the present time) in order to verify the contents of the \"pixel\" on the image (noting that the concept of a \"pixel\" is somewhat ill-defined). In the case of a classified image, it allows supervised classification to help determine the accuracy of the classification performed by the remote sensing software and therefore minimize errors in the classification such as errors of commission and errors of omission.\n\nGround truth is usually done on site, performing surface observations and measurements of various properties of the features of the ground resolution cells that are being studied on the remotely sensed digital image. It also involves taking geographic coordinates of the ground resolution cell with GPS technology and comparing those with the coordinates of the \"pixel\" being studied provided by the remote sensing software to understand and analyze the location errors and how it may affect a particular study.\n\nGround truth is important in the initial supervised classification of an image. When the identity and location of land cover types are known through a combination of field work, maps, and personal experience these areas are known as training sites. The spectral characteristics of these areas are used to train the remote sensing software using decision rules for classifying the rest of the image. These decision rules such as Maximum Likelihood Classification, Parallelepiped Classification, and Minimum Distance Classification offer different techniques to classify an image. Additional ground truth sites allow the remote sensor to establish an error matrix which validates the accuracy of the classification method used. Different classification methods may have different percentages of error for a given classification project. It is important that the remote sensor chooses a classification method that works best with the number of classifications used while providing the least amount of error.\n\nGround truth also helps with atmospheric correction. Since images from satellites obviously have to pass through the atmosphere, they can get distorted because of absorption in the atmosphere. So ground truth can help fully identify objects in satellite photos.\n\nAn example of an error of commission is when a pixel reports the presence of a feature (such as trees) that, in reality, is absent (no trees are actually present). Ground truthing ensures that the error matrices have a higher accuracy percentage than would be the case if no pixels were ground truthed. This value is the inverse of the user's accuracy, i.e. Commission Error = 1 - user's accuracy.\n\nAn example of an error of omission is when pixels of a certain thing, for example maple trees, are not classified as maple trees. The process of ground truthing helps to ensure that the pixel is classified correctly and the error matrices are more accurate. This value is the inverse of the producer's accuracy, i.e. Omission Error = 1 - producer's accuracy\n\nGeographic information systems such as GIS, GPS, and GNSS, have become so widespread that the term \"ground truth\" has taken on special meaning in that context. If the location coordinates returned by a location method such as GPS are an estimate of a location, then the \"ground truth\" is the actual location on earth. A smart phone might return a set of estimated location coordinates such as 43.87870,-103.45901. The ground truth being estimated by those coordinates is the tip of George Washington's nose on Mt. Rushmore. The accuracy of the estimate is the maximum distance between the location coordinates and the ground truth. We could say in this case that the estimate accuracy is 10 meters, meaning that the point on earth represented by the location coordinates is thought to be within 10 meters of George's nose—the ground truth. In slang, the coordinates indicate where we think George Washington's nose is located, and the ground truth is where it's really at. In practice a smart phone or hand-held GPS unit is routinely able to estimate the ground truth within 6–10 meters. Specialized instruments can reduce GPS measurement error to under a centimeter.\n\nUS military slang uses \"ground truth\" to describe the reality of a tactical situation - as opposed to intelligence reports and mission plans. The term appears in the title of the Iraq War documentary film \"The Ground Truth\" (2006), and also in military publications, for example \"Stars and Stripes\" saying: \"Stripes decided to figure out what the ground truth was in Iraq.\"\n\nThe \"Oxford English Dictionary\" (s.v. \"ground truth\") records the use of the word \"Groundtruth\" in the sense of a \"fundamental truth\" from Henry Ellison's poem \"The Siberian Exile's Tale\", published in 1833.\n\n"}
{"id": "3420503", "url": "https://en.wikipedia.org/wiki?curid=3420503", "title": "Hand coding", "text": "Hand coding\n\nIn computing, hand coding means editing the underlying representation of a document or a computer program, when tools that allow working on a higher level representation also exist. Typically this means editing the source code, or the textual representation of a document or program, instead of using a WYSIWYG editor that always displays an approximation of the final product. It may also mean translating the whole or parts of the source code into machine language manually instead of using a compiler or an automatic translator.\n\nMost commonly, it refers to directly writing HTML documents for the web (rather than in a specialized editor), or to writing a program or portion of a program in assembly language (more rarely raw machine code) rather than in a higher level language. It can also include other markup languages, such as wikitext.\n\nThe reasons to use hand coding include the ability to:\n\nHand coding may require more expertise and time than using automatic tools.\n\nHand code is source code which does not have tools that can edit it at a more abstract level. Hand code must, by definition, be edited and maintained entirely by hand. Some code can be edited either using an editor/IDE or by hand, but hand code is differentiated from derived code in that it requires human involvement to create and maintain it over time. Projects may include both hand code and derivative code.\n\nThe automatic tools responsible for creating derivative code are themselves usually made up entirely, or at least in part, of hand code.\n"}
{"id": "37384533", "url": "https://en.wikipedia.org/wiki?curid=37384533", "title": "IEEE P1906.1", "text": "IEEE P1906.1\n\nThe IEEE P1906.1 - Recommended Practice for Nanoscale and Molecular Communication Framework is a standards working group sponsored by the IEEE Communications Society Standards Development Board whose goal is to develop a common framework for nanoscale and molecular communication. Because this is an emerging technology, the standard is designed to encourage innovation by reaching consensus on a common definition, terminology, framework, goals, metrics, and use-cases that encourage innovation and enable the technology to advance at a faster rate. The draft passed an initial sponsor balloting with comments on January 2, 2015. The comments were addressed by the working group and the resulting draft ballot passed again on August 17, 2015. Finally, additional material regarding SBML was contributed and the final draft passed again on October 15, 2015. The draft standard was approved by IEEE RevCom in the final quarter of 2015.\n\nWorking group membership includes experts in industry and academia with strong backgrounds in mathematical modeling, engineering, physics, economics and biological sciences.\n\nElectronic components such as transistors, or electrical/electromagnetic message carriers whose operation is similar at the macroscale and nanoscale are excluded from the definition. A human-engineered, synthetic component must form part of the system because it is important to avoid standardizing nature or physical processes. The definition of communication, particularly in the area of cell-surface interactions as viewed by biologists versus non-biologists has been a topic of debate. The interface is viewed as a communication channel, whereas the 'receptor-signaling-gene expression' events are the network.\nThe draft currently comprises: definition, terminology, framework, metrics, use-cases, and reference code (ns-3).\n\nThe standard provides a very broad foundation and encompasses all approaches to nanoscale communication. While there have been many superficial academic attempts to classify nanoscale communication approaches, the standard considers two fundamental approaches: waves and particles. This includes any hybrid of the two as well as quasiparticles.\n\nA unique contribution of the standard is an ns-3 reference model that enables users to build upon the standard components.\n\n\n\n\n\n\n\nApplications are numerous, however, there appears to be strong emphasis on medical and biological use-cases in nanomedicine.\n\nThe IEEE P1906.1 working group is developing ns-3 nanoscale simulation software that implements the IEEE 1906.1 standard and serves as a reference model and base for development of a wide-variety of interoperable small-scale communication physical layer models.\n\nThe Best Readings on nanoscale communication networks provides good background information related to the standard. The Topics section breaks down the information using the standard approach.\n\nIEEE 1906.1 is the foundation for nanoscale communication. Additional standards are expected to build upon it. \n\nIEEE 1906.1.1 Standard Data Model for Nanoscale Communication Systems \nThe Standard Data Model for Nanoscale Communication Systems defines a network management and configuration data model for nanoscale communication.\nThis data model has several goals:\n\nThe data model is written in YANG and will enable remote remote configuration and operation of nanoscale communication over the Internet using NETCONF.\n\n\n"}
{"id": "55893053", "url": "https://en.wikipedia.org/wiki?curid=55893053", "title": "Instant Pot", "text": "Instant Pot\n\nThe Instant Pot is a brand of kitchen appliances. The brand's original and primary products are electronically controlled, combined pressure cookers and slow cookers. The original cookers are marketed as 7-in-1 appliances designed to consolidate the cooking and preparing of food to one device (multicooker). The brand has since expanded to include non-pressure slow cookers, sous-vide immersion circulators, and blenders.\n\nThe concept of a multi-purpose cooker has been around since 2006 with the patent from the Midea Group, which would go on to manufacture the Instant Pot, detailing the function and simple structure of what would become the Instant Pot.\nIn 2008, Robert Wang, Yi Quin, and one other friend, all former employees of Nortel in Ottawa, Canada, started working on designs for the Instant Pot. Wang is credited as the inventor of the Instant Pot. This beginning model operated as a pressure cooker, slow cooker, rice / porridge cooker, yogurt maker, sauté / searing pan, steamer, and food warmer. This model was under development for 18 months.\n\nDouble Insight Inc was founded by Robert Wang, Yi Quin, and three other partners in 2009. They are the distributor and designers of the Instant Pot. Wang has a PhD in computer science with a specialty in artificial intelligence.\n\nThe company became profitable in 2012, with the Instant Pot as their main product. In 2016, Double Insight sold more than 215,000 Instant Pots on Amazon's Prime Day.\n\nIn July 2015 the Instant Pot Smart-60 cooker was recalled, affecting about 1140 units in the United States and Canada. The defect caused electric current to leak, which could potentially shock the product's user. There were four reported instances of this shock before it was recalled.\n\nIn February 2018, five production runs of Instant Pot Gem 65 8-in-1 Multicookers were recalled because they were overheating and subsequently melting due to a manufacturing issue.\nInstant Pots are manufactured by the Midea Group in GuangDong, China. \n\n"}
{"id": "20212441", "url": "https://en.wikipedia.org/wiki?curid=20212441", "title": "Institute for Agriculture and Trade Policy", "text": "Institute for Agriculture and Trade Policy\n\nThe Institute for Agriculture and Trade Policy (IATP) is a non-profit research and advocacy organization that promotes sustainable food, farm, and trade systems. IATP has offices in Minneapolis, Minnesota and Geneva, Switzerland, and operates both locally and internationally.\n\nIATP works to integrate sustainability throughout the food and farm system, from supporting farmers and the environment to securing universal access to healthy food. IATP identifies the impact trade agreements have on farmers, consumers and the environment, while promoting a fair trade system that supports locally based development, labor and human rights, and democratic institutions. IATP develops alternative economic models that integrate environmental sustainability into rural development.\n\n1987: IATP launches the Sustainable Agriculture Computer Network with email, access to shared data and research, news wire services, electronic conferencing and bulletin boards.\n\n1990: IATP distributes \"Trading Away Our Future\", a video on how General Agreement on Tariffs and Trade (GATT) affects American agriculture policy, to over 1,000 local organizers, opinion leaders and teachers.\n\n1991: In New York, IATP conducts trainings and strategy coordination for 20 coalition partners in preparation for the United Nations Conference on Environment and Development.\n\n1992: United Nations Conference on Environment and Development, more commonly known as the Earth Summit, meets in Rio de Janeiro; IATP co-hosts the Global Forum on the GATT at the summit.\n\n1995: IATP holds a series of events nationwide with surviving founders of Bretton Woods and United Nations institutions, resulting in a [book of twenty essays] by the participants.\n\n1996: IATP forms Peace Coffee in partnership with Guatemala’s Nobel Peace Prize winner Rigoberta Menchú.\n\n1997: IATP incorporates TransFair USA, the first U.S. fair-trade certification body.\n\n1998: IATP marks the 50th anniversary of the Universal Declaration of Human Rights with an event in New York City with Jesse Jackson, John Sweeney and others.\n\n2000: With six other groups, IATP launches Genetically Engineered Food Alert to challenge the use of genetically engineered crops in food. IATP launches Trade Information Project in Geneva to serve as an information clearinghouse on the World Trade Organization (WTO) for NGOs around the world.\n\n2001: IATP created the Eat Well Guide, an online directory of sustainably raised meat, poultry, dairy and eggs available from North American farms, stores, restaurants and online merchants.\n\n2005: IATP publishes a critique of the U.S. food aid system for undercutting local food systems in poor countries. The WTO holds its sixth ministerial in Hong Kong. IATP helps to organize the Fair Trade Fair and Symposium near the WTO meeting, to highlight the fair trade system for WTO delegates.\n\n2006: IATP publishes a report, summarized in \"The New York Times\", on the incidence of arsenic in U.S. poultry products.\n\n2008: John Nichols of The Nation designated IATP the \"Most Valuable Policy Group\" in his Most Valuable Progressives of 2008 list. The Food and Society Fellows program moves to IATP, bringing together chefs, farmers, nutritionists, activists, public health professionals, fishers, policy experts and academics who work to create sustainable food systems.\n\n2009: IATP released a report, Not So Sweet: Missing Mercury and High Fructose Corn Syrup, that examined the presence of mercury in high fructose corn syrup.\n\nMinnesota Secretary of State Mark Ritchie returned to the United States from a 1986 Geneva meeting and incorporated the Institute for Agriculture and Trade Policy as a nonprofit, tax-exempt organization, with the mission of fostering sustainable rural communities. In 1987, IATP began to organize and report on the newly launched round of international trade negotiations being conducted by the GATT, which eventually became the WTO. The rules of agricultural trade set in the GATT and implemented at the WTO have deeply influenced national and local farm policies around the globe over the last two decades.\n\nIn the 1990s, IATP expanded beyond its initial focus on international policymaking institutions like the WTO, and regional free trade agreements like the North American Free Trade Agreement, to include the promotion of positive alternatives to economically, socially and environmentally destructive agricultural and trade practices. For example, with the Center for Agriculture and the Environment in the Netherlands, IATP developed tools to help U.S. farmers increase their income by reducing on-farm fertilizer use.\n\nJim Harkness became IATP’s new president in 2006, after spending the previous 16 years in China, most recently as the Executive Director for the World Wildlife Fund in China. In 2014, Juliette Majot became the organization's third President, changing the title to Executive Director.\n\n\n"}
{"id": "511584", "url": "https://en.wikipedia.org/wiki?curid=511584", "title": "J. Lyons and Co.", "text": "J. Lyons and Co.\n\nJ. Lyons & Co. was a British restaurant chain, food manufacturing, and hotel conglomerate founded in 1884.\n\nThe company began as collaboration between the professional artist Joseph Lyons and his brothers in law, Isidore and Montague Gluckstein, as a spin off from the Salmon & Gluckstein tobacco company. In 1894 the company started a teashop in Piccadilly, London, and from 1909 developed this into a chain of teashops known as Lyons' Corner Houses. The company also ran high class restaurants, founding the Trocadero in 1895, and hotels including the Strand Palace, opened in 1909, the Regent Palace, opened in 1915, and the Cumberland Hotel, opened in 1933, all in London. From the 1930s Lyons began to develop a pioneering range of teas, biscuits and cakes that were sold in grocery stores across the world.\n\nLyons was appointed to run the company, and it was named after him.\n\nJ. Lyons & Co. was a pioneer in introducing computers to business. Between 1951 and 1963, the company manufactured and sold a range of LEO (Lyons Electronic Office) computers.\n\nThe company was a substantial food manufacturer, with factories at Cadby Hall in Hammersmith, and from 1921 at Greenford, producing bread, cakes, pies, tea, coffee and ice cream.\n\nTo the public, J. Lyons & Co. were best known for their chain of tea shops which opened from 1894 and finally closed in 1981, and for the Lyons Corner Houses in the West End of London. The tea shops were slightly more up market than their ABC (Aerated Bread Company) counterparts. They were notable for their interior design, from the 1920s Oliver P. Bernard being consultant artistic director. Until the 1940s they had a certain working-class chic, but by the 1950s and '60s they were quick stops for busy shoppers where one could drink a cup of tea and eat a snack or an inexpensive meal. The tea shops always had a bakery counter at the front, and their signs, art nouveau gold lettering on white, were a familiar landmark. Before the Second World War service was to the table by uniformed waitresses, known as 'Nippies', after the War the tea shops converted to cafeteria service.\n\nLyons' Corner Houses, which first appeared in 1909 and remained until 1977, were noted for their art deco style. Situated on or near the corners of Coventry Street, Strand and Tottenham Court Road, they and the Maison Lyonses at Marble Arch and in Shaftesbury Avenue were large buildings on four or five floors, the ground floor of which was a food hall with counters for delicatessen, sweets and chocolates, cakes, fruit, flowers and other products. In addition, they possessed hairdressing salons, telephone booths, theatre booking agencies and at one period a twice-a-day food delivery service. On the other floors were several restaurants, each with a different theme and all with their own musicians. For a time the Corner Houses were open 24 hours a day, and at their peak each branch employed around 400 staff. They featured window displays, and, in the post-war period, the Corner Houses were smarter and grander than the local tea shops. Between 1896 and 1965 Lyons owned the Trocadero, which was similar in size and style to the Corner Houses.\n\nAs well as the tea shops and Corner Houses, Lyons ran other large restaurants such as the Angel Cafe Restaurant in Islington and the Throgmorton in Throgmorton Street. Its chains have included Steak Houses (1961–1988), Wimpy Bars (1953–1976), Baskin-Robbins (1974-) and Dunkin' Donuts (1989-). The artist Kay Lipton designed all the windows for the Corner Houses under the jurisdiction of Norman Joseph, the director post-war.\n\nThe Regent Palace Hotel, Glasshouse Street, London was operated by Strand Hotels Limited, a subsidiary of J. Lyons and Company and opened on 16 May 1915. Strand Hotels also operated the Cumberland Hotel (Marble Arch, London), Kingsley Hotel, Park Court Hotel, Windsor Hotel, White's Hotel and the Strand Palace Hotel after the inception of Strand Hotels Limited. The last London hotel that they operated until the demise of the group in the mid-70s was the Tower Hotel situated by Tower Bridge in London.\n\nIn 1938, Lyons purchased the Bee Bee Biscuit Company, which manufactured biscuits from its factories in Blackpool. Six years later, Lyons changed the company's name to Symbol Biscuits Ltd. and began selling biscuits under the Symbol and Lyons brand names: one of their innovations was Maryland Cookies in 1956. In 1990, Lyons changed the Symbol Biscuits name to Lyons Biscuits Ltd.\n\nThe rearmament period just before World War II saw a big expansion in the number of Royal Ordnance Factories (ROFs), which were British government-owned. However, due to shortages of management resources some ROFs were run as agency factories; and J. Lyons and Co. ran at least one, ROF Elstow. The management and stock control systems needed in the ROFs, in respect of control of raw materials and \"perishable\" finished products, were somewhat similar to those used in the catering business; and J. Lyons was ideally suited to this task. They do not appear to have any involvement in managing these after 1945, when the ROFs started to run down.\n\nThe top management of Lyons, with its background in the use of mechanical adding machines, saw the necessity of new electrical computers for organising the distribution of cakes and other highly perishable goods. They, therefore, substantially financed the University of Cambridge's Electronic Delay Storage Automatic Calculator (EDSAC) which was the second electronic digital stored-program computer to go into regular service, and built their own programmable digital computers and became the first user of these in businesses, with the LEO I digital computer: the \"Lyons Electronic Office I\", designed and built by Dr John Pinkerton under the able leadership of John Simmons. It handled the company's accounts and logistics. Lyons also included the weather forecast to ensure goods carried by their \"fresh produce\" delivery vans were not wasted in large quantities. Google chairman Eric Schmidt called this \"the world's first office computer\", built in 1951. A subsidiary LEO Computers Ltd was formed in 1954 and went on to build 11 Leo II and 94 Leo III computers that were sold worldwide.\n\nThe company was losing money in the 1960s but remained under the control of the Salmon family, descended from a founding partner. Lyons began to close some of its London tea shops and hotels; in 1963 it also merged its LEO Computers business with English Electric's computer interests to form the jointly owned English Electric LEO.\n\nIn 1964, Lyons sold their half-stake; and English Electric merged the company with Marconi's computer interests to form English Electric LEO Marconi Computers. A continuing problem in the British computer industry was both lack of investment capital and competition with the much larger U.S. computer companies, such as IBM. English Electric LEO Marconi Computers merged with other companies to form International Computers Limited (ICL) which was bought by Fujitsu in 1990.\n\nIn 1978, Lyons was acquired by Allied Breweries and became part of the resulting Allied Lyons. It fell on hard economic times in the late 1980s; and was sold, eventually being broken up with its ice cream and ice lolly products, which were branded as Lyons Maid, being sold to Nestlé. Other parts that were sold off included Lyons Cakes (sold to RHM and ending up as part of their Manor Bakeries subsidiary which also makes Mr Kipling's Cakes) and Ready Brek cereal (ending up being owned by Weetabix Limited). At the end of 1994, Lyons sold Lyons Biscuits to Hillsdown Holdings, which later sold it to a U.S. investment firm which subsequently sold it to large biscuit manufacturer Burton's Foods.\n\nThe J. Lyons & Co. papers are now stored in the London Metropolitan Archives.\n\nThe niece and nephew of the Gluckstein brothers were Hannah Gluckstein, a painter; and Louis Gluckstein, a Conservative politician. A descendant of the Salmon side of the original partnership is Nigella Lawson.\n\nFormer British Prime Minister Margaret Thatcher worked as a chemist for the company prior to becoming a barrister and then a Conservative Party MP. While working for the company she helped develop methods for preserving ice cream.\n\nThe chairmen of J. Lyons were:\n\n\n\n"}
{"id": "116748", "url": "https://en.wikipedia.org/wiki?curid=116748", "title": "Lawrence, Massachusetts", "text": "Lawrence, Massachusetts\n\nLawrence is a city in Essex County, Massachusetts, United States, on the Merrimack River. As of the 2010 census, the city had a population of 76,377, which had risen to an estimated 78,197 as of 2014. Surrounding communities include Methuen to the north, Andover to the southwest, and North Andover to the southeast. Lawrence and Salem were the county seats of Essex County, until the Commonwealth abolished county government in 1999. Lawrence is part of the Merrimack Valley.\n\nManufacturing products of the city include electronic equipment, textiles, footwear, paper products, computers, and foodstuffs. Lawrence was the residence of poet Robert Frost for his early school years; his essays and poems were first published in the Lawrence High School newspaper.\n\nNative Americans, namely the Pennacook or Pentucket tribe, had a presence in this area. Evidence of farming at Den Rock Park and arrowhead manufacturing on the site of where the Wood Mill now sits have been discovered.\n\nEuropeans first settled the Haverhill area in 1640, colonists from Newbury following the Merrimack River in from the coast. The area that would become Lawrence was then part of Methuen and Andover. The first settlement came in 1655 with the establishment of a blockhouse in Shawsheen Fields, now South Lawrence.\n\nThe future site of the city (formerly parts of Andover and Methuen), was purchased by a consortium of local industrialists. The Water Power Association members: Abbott Lawrence, Edmund Bartlett, Thomas Hopkinson of Lowell, John Nesmith and Daniel Saunders, had purchased control of Peter's Falls on the Merrimack River and hence controlled Bodwell's Falls the site of the present Great Stone Dam. The group allotted fifty thousand dollars to buy land along the river to develop. In 1844, the group petitioned the legislature to act as a corporation, known as the Essex Company, which incorporated on April 16, 1845. The first excavations for the Great Stone Dam to harness the Merrimack River's water power were done on August 1, 1845. The Essex Company would sell the water power to corporations such as the Arlington Mills, as well as organize construction of mills and build to suit. Until 1847, when the state legislature recognized the community as a town, it was called interchangeably the \"New City\", \"Essex\" or \"Merrimac\". The post office, built in 1846, used the designation \"Merrimac\". Incorporation as a city would come in 1853, and the name \"Lawrence\", merely chosen as a token of respect to Abbott Lawrence, who it cannot be verified ever saw the city named after him.\n\nCanals were dug on both the north and the south banks to provide power to the factories that would soon be built on its banks as both mill owners and workers from across the city and the world flocked to the city in droves; many were Irish laborers who had experience with similar building work. The work was dangerous: injuries and even death were not uncommon.\n\nWorking conditions in the mills were unsafe and in 1860 the Pemberton Mill collapsed, killing 145 workers. As immigrants flooded into the United States in the mid to late 19th century, the population of Lawrence abounded with skilled and unskilled workers from several countries.\n\nLawrence was the scene of the infamous Bread and Roses Strike, also known as the Lawrence Textile Strike, one of the more important labor actions in American history.\n\nLawrence was a great wool-processing center until that industry declined in the 1950s. The decline left Lawrence a struggling city. The population of Lawrence declined from over 80,000 residents in 1950 (and a high of 94,270 in 1920) to approximately 64,000 residents in 1980, the low point of Lawrence's population.\n\nLike other northeastern cities suffering from the effects of post-World War II industrial decline, Lawrence has often made efforts at revitalization, some of them controversial. For example, half of the enormous Wood Mill, powered by the Great Stone Dam and once the largest mills in the world, was knocked down in the 1950s. The Lawrence Redevelopment Authority and city officials utilized eminent domain for a perceived public benefit, via a top down approach, to revitalize the city throughout the 1960s. Known first as urban redevelopment, and then urban renewal, Lawrence's local government's actions towards vulnerable immigrant and poor communities, contained an undercurrent of gentrification which lies beneath the goals to revitalize Lawrence. There was a clash of differing ideals and perceptions of blight, growth, and what constituted a desirable community. Ultimately the discussion left out those members of the community who would be directly impacted by urban redevelopment.\n\nUnder the guise of urban renewal, large tracts of downtown Lawrence were razed in the 1970s, and replaced with parking lots and a three-story parking garage connected to a new Intown Mall intended to compete with newly constructed suburban malls. The historic Theater Row along Broadway was also razed, destroying ornate movie palaces of the 1920s and 1930s that entertained mill workers through the Great Depression and the Second World War. The city's main post office, an ornate federalist style building at the corner of Broadway and Essex Street, was razed. Most of the structures were replaced with one-story, steel-frame structures with large parking lots, housing such establishments as fast food restaurants and chain drug stores, fundamentally changing the character of the center of Lawrence.\n\nLawrence also attempted to increase its employment base by attracting industries unwanted in other communities, such as waste treatment facilities and incinerators. From 1980 until 1998, private corporations operated two trash incinerators in Lawrence. Activist residents successfully blocked the approval of a waste treatment center on the banks of the Merrimack River near the current site of Salvatore's Pizza on Merrimack Street.\n\nRecently the focus of Lawrence's urban renewal has shifted to preservation rather than sprawl.\n\nImmigrants from the Dominican Republic and migrants from Puerto Rico began arriving in Lawrence in significant numbers in the late 1960s, attracted by cheap housing and a history of tolerance toward immigrants. In 1984, tensions between remaining working class whites and increasing numbers of Hispanic youth flared into a riot, centered at the intersection of Haverhill Street and Oxford Street, where a number of buildings were destroyed by Molotov cocktails and over 300 people were arrested.\n\nLawrence saw further setbacks during the recession of the early 1990s as a wave of arson plagued the city. Over 200 buildings were set alight in an eighteen-month period in 1991–92, many of them abandoned residences and industrial sites. The Malden Mills factory burned down on December 11, 1995. CEO Aaron Feuerstein decided to continue paying the salaries of all the now-unemployed workers while the factory was being rebuilt.\n\nA sharp reduction in violent crime starting in 2004 and massive private investment in former mill buildings along the Merrimack River, including the remaining section of the historic Wood Mill – to be converted into commercial, residential and education uses – have lent encouragement to boosters of the city. One of the final remaining mills in the city is Malden Mills. Lawrence's downtown has seen a resurgence of business activity as Hispanic-owned businesses have opened along Essex Street, the historic shopping street of Lawrence that remained largely shuttered since the 1970s. In June 2007, the city approved the sale of the Intown Mall, largely abandoned since the early 1990s recession, to Northern Essex Community College for the development of a medical sciences center, the construction of which commenced in 2012 when the InTown Mall was finally removed. A large multi-structure fire in January 2008 destroyed many wooden structures just south of downtown. A poor financial situation that has worsened with the recent global recession and has led to multiple municipal layoffs had Lawrence contemplating receivership. \n\nOn September 13, 2018, a series of gas explosions and fires broke out in as many as 40 homes in Lawrence, Andover, and North Andover. The disaster killed one resident and caused over 30,000 customers to evacuate their homes.\n\n\nLawrence has been aptly nicknamed the \"Immigrant City\". It has been home to numerous different immigrant communities, most of whom arrived during the great wave of European immigration to America that ended in the 1920s.\n\nLawrence became home to large groups of immigrants from Europe, beginning with the Irish in 1845, Germans after the social upheaval in Germany in 1848, Swedes fleeing an overcrowded Sweden, and French Canadians seeking to escape hard northern farm life from the 1850s onward. A second wave began arriving after 1900, as part of the great mass of Italian and Eastern European immigrants, including Jews from Russia, Poland, Lithuania and neighboring regions. Immigration to the United States was severely curtailed in the 1920s with the Immigration Act of 1924, when foreign born immigration to Lawrence virtually ceased for over 40 years.\n\nIn 1890, the foreign-born population of 28,577 was divided as follows, with the significant remainder of the population being children of foreign born residents: 7,058 Irish; 6,999 French Canadians; 5,131 English; 2,465 German; 1,683 English Canadian. In 1920, toward the end of the first wave of immigration, most ethnic groups had numerous social clubs in the city. The Portuguese had 2; the English had 2; the Jews had 3; the Armenians, 5; the Lebanese and Syrians, 6; the Irish, 8; the Polish, 9; the French Canadians and Belgian-French, 14; the Lithuanians, 18; the Italians, 32; and the Germans, 47. However, the center of social life, even more than clubs or fraternal organizations, was churches. Lawrence is dotted with churches, many now closed, torn down or converted into other uses. These churches signify, more than any other artifacts, the immigrant communities that once lived within walking distance of each church.\n\nIrish immigrants arrived in Lawrence at its birth, which nearly coincided with the Great Potato Famine of the 1840s, the event that drove great numbers of Irish out of Ireland. The Great Stone Dam, constructed in 1845–1848 to power the nascent textile mills, was largely built by Irish laborers. The first Irish immigrants settled in the area south of the Merrimack River near the intersection of Kingston Street and South Broadway. Their shantytown settlement put them close to the dam being constructed, but away from the Essex Corporation row houses built north of the river to attract New England farm girls as millworkers. The religious needs of the Irish were initially met by the Immaculate Conception church, originally erected near the corner of Chestnut and White Street in 1846, the first Roman Catholic church in Lawrence. In December, 1848, the Reverend James O'Donnell erected \"old\" St. Mary's Church. By 1847, observers counted over ninety shanties in the Irish shantytown. In 1869, the Irish were able to collect sufficient funds form their own church, St. Patrick's, on South Broadway.\n\nThe first sizable German community arrived following the revolutions of 1848. However, a larger German community was formed after 1871, when industrial workers from Saxony were displaced by economic competition from new industrial areas like the Ruhr. The German community was characterized by numerous school clubs, shooting clubs, national and regional clubs, as well as men's choirs and mutual aid societies, many of which were clustered around the Turn Verein, a major social club on Park Street. Germans had a considerable number of churches in Lawrence, including Church of the Assumption of Mary (German Catholic) parish formed in 1887 on Lawrence Street, as well as a number of Protestant churches including The German Methodist Episcopal Church, Vine street, organized in 1878; and the German Presbyterian, East Haverhill street, organized 1872 from which the Methodist church split in 1878.\n\nSome Italian immigrants celebrated Mass in the basement chapel of the largely Irish St. Laurence O'Toole Church, at the intersection of East Haverhill Street and Newbury Street, until they had collected sufficient funds to erect the Holy Rosary Church in 1909 nearby at the intersection of Union Street and Essex Street. Immigrants from Lentini (a \"comune\" in the Sicilian province of Syracuse) and from the Sicilian province of Catania maintained a particular devotion to three Catholic martyrs, Saint Alfio, Saint Filadelfo and Saint Cirino, and in 1923 began celebrating a procession on their feast day. Although most of the participants live in neighboring towns, the Feast of Three Saints festival continues in Lawrence today. Many of the Italians who lived in the Newbury Street area had immigrated from Trecastagni, Viagrande, Acireale, and Nicolosi, Italy.\n\nFrench Canadians were the second major immigrant group to settle in Lawrence. In 1872, they erected their first church, St. Anne's, at the corner of Haverhill and Franklin streets. Within decades, St. Anne's established a \"missionary church\", Sacred Heart on South Broadway, to serve the burgeoning Québécois community in South Lawrence. Later it would also establish the \"missionary\" parishes in Methuen: Our Lady of Mount Carmel and St. Theresa's (Notre-Dame du Mont Carmel et St-Thérèse). The French-Canadians arrived from various farming areas of Quebec where the old parishes were overpopulated: some people moved up north (Abitibi and Saguenay–Lac-Saint-Jean), while others moved to industrial towns to find work (Montreal, Quebec; but also in the United States). Others who integrated themselves into these French-Canadian communities were actually Acadians who had left the Canadian Maritimes of New Brunswick and Nova Scotia also in search of work.\n\nLawrence residents frequently referred to their Arabic-speaking Middle Eastern community as \"Syrian\". In fact, most so-called Syrians in Lawrence were from present-day Lebanon and were largely Maronite Christian. Lebanese immigrants organized St. Anthony's Maronite Church in 1903, and St. Joseph's Melkite Greek-Catholic Church, as well as St. George's Antiochian Orthodox Church.\n\nJewish merchants became increasingly numerous in Lawrence and specialized in dry goods and retail shops. The fanciest men's clothing store in Lawrence, Kap's, established in 1902 and closed in the early 1990s, was founded by Elias Kapelson, born in Lithuania. Jacob Sandler and two brothers also immigrated from Lithuania in approximately 1900 and established Sandlers Department Store, which continued in business until 1978. In the 1880s, the first Jewish arrivals established a community around Common, Valley, Concord and Lowell streets. As of 1922, there were at least two noteworthy congregations, both on Concord Street: Congregation of Sons of Israel (Jewish), organized October 3, 1894. Synagogue on Concord street built in 1913; and Congregation of Anshea Sfard (Jewish), organized April 6, 1900. Synagogue on Concord street built in the autumn of 1907. In the 1920s, the Jews of Lawrence began congregating further up Tower Hill, where they erected two synagogues on Lowell Street above Milton Street, as well as a Jewish Community Center on nearby Haverhill Street. All three institutions had closed their doors by 1990 as the remaining elderly members of the community died out or moved away.\n\nThe Polish community of Lawrence was estimated to be only 600–800 persons in 1900. However, by 1905, the community had expanded sufficiently to fund the construction of the Holy Trinity Church at the corner of Avon and Trinity streets. Their numbers grew to 2,100 Poles in 1910. Like many of their immigrant brethren from other nations, most of the Poles were employed in woolen and worsted goods manufacturing.\n\nLawrence had a sizable enough Lithuanian community to warrant the formation of both Lithuanian Catholic and Lithuanian National Catholic churches. St. Francis (Lithuanian Catholic Church) on Bradford Street was formed in 1903 by Rev. James T. O'Reilly of St. Mary's, in a building previously occupied by St. John's Episcopal Church. The church closed in 2002, merging with Holy Trinity (Polish) and SS. Peter and Paul (Portuguese). Sacred Heart Lithuanian National Catholic Church was established about 1917 and located on Garden Street until its closure and sale in 2001.\n\nA sizable English community, composed mainly of unskilled laborers who arrived after 1880, sought work in the textile mills where they were given choice jobs by the Yankee overseers on account of their shared linguistic heritage and close cultural links.\n\nNot all immigrants to Lawrence were foreign-born or their children. Yankee farmers, unable to compete against the cheaper farmlands of the Midwest that had been linked to the East coast by rail, settled in corners of Lawrence. Congregationalists were the second Protestant denomination to begin worship in Lawrence after the Episcopalians, with the formation of the Lawrence Street Congregational Church in 1847, and the first in South Lawrence, with the erection in 1852 of the first South Congregational Church on South Broadway, near the corner of Andover Street. Baptist churches included The First Baptist Church, one of the first churches in Lawrence, which was organized in the spring of 1947 and was known as Amesbury Street Baptist Church. Second Baptist was organized September 6, 1860; its building dedicated in 1874.\n\nImmigration of foreign born workers to Lawrence largely ceased in 1921 with the passage of strict quotas against immigrants from the countries that had supplied the cheap, unskilled workers. Although many quotas were lifted after the Second World War, foreign immigration to Lawrence only picked up again in the early 1960s with Hispanic immigrants from Cuba, Puerto Rico, the Dominican Republic and other Latin American countries. Immigrants from Southeast Asia, particularly Vietnam, have also settled in Lawrence.\n\nIndicative of immigration trends, several Catholic churches now conduct masses in two or more languages. St. Patrick's Church, a Catholic church in Lawrence and once an Irish bastion, has celebrated Spanish masses on Sundays since 1999. A mass in Vietnamese is also offered every other week. St. Mary's of the Assumption Parish is the largest Catholic parish in Lawrence by Mass attendance and number of registered parishioners. It has the largest multi-lingual congregation in the city and has been offering Spanish masses since the early 1990s.\n\nSince the 1990s, increasing numbers of former Catholic churches, closed since the 1980s when their Irish or Italian congregations died out, have been bought by Hispanic evangelical churches.\n\nThe 2000 Census revealed the following population breakdown, illustrating the shift toward newer immigrant groups:\n\nDominican Republic, 22%; Puerto Rican, 22%; other Hispanic or Latino, 12%; Irish, 7%; Italian, 7%, French (except Basque), 5%; Black or African American, 5%; French Canadian, 5%; English, 3%; Arab, 2%; German, 2%; Lebanese, 2%; Central American, 1%; Polish, 1%; Portuguese, 1%; Guatemalan, 1%; Vietnamese, 1%; South American, 1%; Spanish, 1%; Cambodian, 1%; Scottish, 1%; Cuban, 1%; Scotch-Irish, 1%; Ecuadoran, 1%.\n\nAccording to the United States Census Bureau, the city has a total area of , of which is land and (6.07%) is water. Lawrence is on both sides of the Merrimack River, approximately upstream from the Atlantic Ocean. On the north side of the river, it is surrounded by Methuen. On the south side of the river, the town is bordered by North Andover to the east, and Andover to the south and southwest. Lawrence is approximately north-northwest of Boston and southeast of Manchester, New Hampshire.\n\nAside from the Merrimack River, other water features include the Spicket River, which flows into the Merrimack from Methuen, and the Shawsheen River, which forms the southeastern border of the city. Lawrence has two power canals that were formerly used to provide hydropower to the mills - one on the north bank of the river, the other on the south. Channeling water into these canals is the Great Stone Dam, which lies across the entire Merrimack and was, at the time of its construction in the 1840s, the largest dam in the world. The highest point in Lawrence is the top of Tower Hill in the northwest corner of the city, rising approximately above sea level. Other prominent hills include Prospect Hill, in the northeast corner of the city, and Mount Vernon, along the southern edge of the city. Most industrial activity was concentrated in the flatlands along the rivers. Den Rock Park, a wooded conservation district on the southern edge of Lawrence that spans the Lawrence-Andover town line, provides recreation for nature lovers and rock-climbers alike. There are also several small parks throughout town.\n\nLawrence lies along Interstate 495, which passes through the eastern portion of the city. There are three exits entirely within the city, though two more provide access from just outside the city limits. The town is also served by Route 28 passing from south to north through the city, and Route 110, which passes from east to west through the northern half of the city. Route 114 also has its western terminus at Route 28 at the Merrimack River. Lawrence is the site of four road crossings and a railroad crossing over the Merrimack, including the O'Leary Bridge (Route 28), a railroad bridge, the Casey Bridge (bringing Parker Street and access to Route 114 and the Lawrence MBTA station to the north shore), the Duck Bridge (which brings Union Street across the river), and the double-decked O'Reilly Bridge, bringing I-495 across the river.\n\nLawrence is the western hub of the Merrimack Valley Regional Transit Authority's bus service. It is also home to the Senator Patricia McGovern Transportation Center, home to regional bus service and the Lawrence stop along the Haverhill/Reading Line of the MBTA Commuter Rail system, providing service from Haverhill to Boston's North Station. Lawrence Municipal Airport provides small plane service, though it is actually in neighboring North Andover. Lawrence is approximately equidistant from Manchester-Boston Regional Airport and Logan International Airport. Future plans to revitalize the Manchester and Lawrence branch to the north, leading to Manchester, New Hampshire, will allow the MBTA to operate rail service up to Manchester from Lawrence, in conjunction with Pan Am Freights.\n\nLawrence has a humid continental climate (Köppen climate classification Dfa), which is typical for the southern Merrimack valley region in eastern Massachusetts.\n\nAccording to the U.S. Census Bureau 2010 Census, the city's population is 76,377, the population density is 10,973.7 per square mile (4237/km²), and there are 27,137 households (25,181 occupied).\n\nThe racial makeup of the city in 2016 was 16.6% non-Hispanic white, 7.8% Black or African American, 2.8% Asian (1.2% Cambodian, 0.7% Vietnamese, 0.3% Pakistani, 0.2% Indian, 0.2% Chinese, 0.1% Korean), 0.4% American Indian or Alaskan Native, 0.0% Pacific Islander, 39.3% some other race, 2.7% two or more races, and 77.1% of the population is Hispanic or Latino (of any race) (47.0% Dominican, 21.7% Puerto Rican, 3.0% Guatemalan, 0.7% Salvadoran, 0.7% Spanish, 0.6% Cuban, 0.5% Ecuadorian, 0.5% Mexican, 0.2% Honduran, 0.2% Colombian, 0.1% Venezuelan, 0.1% Nicaraguan, 0.1% Peruvian).\n\nAs of the census of 2000, there were 72,043 people, 24,463 households, and 16,903 families residing in the city. The population density was 10,351.4 people per square mile (3,996.5/km²). There were 25,601 housing units at an average density of 3,678.4 per square mile (1,420.2/km²). The racial makeup of the city was 48.64% White (U.S. Average: 72.4%), 4.88% African American (U.S. Average: 12.3%), 2.65% Asian (U.S. Average: 3.6%), 0.81% Native American (U.S. Average: 0.1%), 0.10% Pacific Islander (U.S. Average: 0.1%), 36.67% from other races (U.S. Average: 5.5%), 6.25% from two or more races (U.S. Average: 2.4%).\n\nThere were 24,463 households where the average household size was 2.90 and the average family size was 3.46.\n\nIn the city, the population had a median age was 30.0 years (U.S. Average: 35.3):\n\nFor every 100 females, there were 91.6 males. For every 100 females age 18 and over, there were 86.8 males.\n\nThe median income for a household in the city was $25,983 (U.S. Average: $41,994), and the median income for a family was $29,809 (U.S. Average: $50,046). Males had a median income of $27,772 versus $23,137 for females. The per capita income for the city was $11,360. About 21.2% of families (U.S. Average: 9.2%) and 34.3% (U.S. Average: 12.4%) of the population were below the poverty line, including 31.7% of those under age 18 and 20.1% of those age 65 or over.\n\nThe Mayor of Lawrence, Daniel Rivera, said the city was \"approximately 75% Spanish\" following an incident where non English speaking callers were allegedly hung up on by a 911 operator.\n\nForm of government:<br>\nPlan B - \"Strong mayor\" - Mayor and city council, the councilors being elected partly at large and partly from districts or wards of the city. Party primaries prohibited.\n\nLawrence has an established City Charter and with a Mayor-council government. There are nine city councilors and six school committee members; most are elected by district; three city council members are elected at large. There are six districts in Lawrence and all elections are non-partisan. The Mayor serves as the seventh member and chair of the school committee. The city council chooses one of its number as president who serves as chair of the council. The city of Lawrence also elects three members to the Greater Lawrence Technical School Committee these members are elected at-large. City Council and Mayoral terms of office begin in the month of January.\n\n\"*\" = President/Chair <br>** = Vice President/Vice Chair\n\nLawrence has its own police and fire departments, and Lawrence General Hospital provides ambulance services to the city. The city also has its own public works and trash pickup departments.\n\nLawrence is one of Essex County's two county seats, along with Salem. As such, it is home to a juvenile, district and superior court, as well as a regional office of the Massachusetts Registry of Motor Vehicles. It is also home to the Lawrence Correctional Alternative Center, a regional alternative jail for low-risk offenders. It is not home to the county's sheriff or district attorney; they are in Middleton (home to the county's correctional facility) and Salem, respectively. The city is also covered by the Andover barracks of Troop A of the Massachusetts State Police, which serves much of the western Merrimack Valley and several towns just south of Andover.\n\nLawrence General Hospital is the city's main hospital, providing service to much of the area south of the city. Other nearby hospitals are in Methuen, Haverhill and Lowell. The city also is served by the Greater Lawrence Family Health Center.\nGuardian Ambulance was established in 1990 and incorporated in 1991 by local EMTs to serve the city during a downturn in the economy at that time. The station moved from the Tower Hill section to its current location on Marston Street in 1993.\n\nThe city has a public school system managed by Lawrence Public Schools. In November 2011, the Lawrence Public Schools was placed into state receivership by the Massachusetts Board of Elementary & Secondary Education.\n\nHigh schools\n\n\nElementary schools\n\nHigh schools\n\nPublic\n\nPrivate\n\nThe Lawrence public library was established in 1872. In fiscal year 2008, the city of Lawrence spent 0.55% ($1,155,597) of its budget on its public library—some $16 per person.\n\nLawrence's main newspaper is \"The Eagle-Tribune\", one of the major newspapers for the Merrimack Valley that was founded in Lawrence in 1890 but later moved its facilities to the town of North Andover on Route 114. Lawrence is home to \"Rumbo\" (a bilingual English/Spanish paper) and \"Siglo 21\" (a Spanish paper). Another newspaper closely covering Lawrence news is \"The Valley Patriot\", a monthly paper published in North Andover. The city has three AM stations, WNNW/800, WCAP/980, and WLLH/1400 (which is also dually licensed to Lowell, Massachusetts with a synchronous transmitter in that city); along with one FM station: WEEI-FM/93.7. WMFP is the only television station operating out of the city, and the city is considered part of the Boston television market.\n\nLawrence is served by Area codes 978 and 351. Originally a part of area code 617, it became part of area code 508 in 1988 before that, too, was split, with 978 covering the northern half of the old area code. Area code 351 is considered an overlay code.\n\nNew Balance has a shoe manufacturing plant in Lawrence, one of five plants operating in the US.\n\nCharm Sciences, which manufactures test kits and systems for antibiotic, veterinary drugs, mycotoxins, pesticides, alkaline phosphatase, pathogens, end-product microbial assessment, allergen control, and ATP hygiene, has a laboratory in Lawrence.\n\nLawrence General Hospital, founded in 1875, is located near downtown.\n\n\n\n"}
{"id": "17721100", "url": "https://en.wikipedia.org/wiki?curid=17721100", "title": "Leister", "text": "Leister\n\nA leister is a type of spear used for fishing.\n\nLeisters have been used by hunter-gatherer cultures throughout the world since the Stone Age and are still used for fishing by indigenous tribes and cultures today.\n\n\n"}
{"id": "33978666", "url": "https://en.wikipedia.org/wiki?curid=33978666", "title": "Life Sciences Foundation", "text": "Life Sciences Foundation\n\nLife Sciences Foundation (LSF) was a San Francisco-based nonprofit organization that was established in 2011 to collect, preserve, interpret, and promote the history of biotechnology. LSF conducted historical research, maintained archives and published historically relevant materials and information. Their public support of climate-science denier Rep. Dana Rohrabacher in 2018 caused a backlash against the foundation and eroded their scientific credibility.\n\nOn December 1, 2015, the LSF and the Chemical Heritage Foundation finalized a merger, creating one organization that covers \"the history of the life sciences and biotechnology together with the history of the chemical sciences and engineering.\"\nAs of February 1, 2018, the organization was renamed the Science History Institute, to reflect its wider range of historical interests, from chemical sciences and engineering to the life sciences and biotechnology.\nThe organization is headquartered in Philadelphia but retains offices in the San Francisco Bay area.\n\nThe LSF mandate was to collect and promote the history of biotechnology. This includes telling the stories of \"scientists, inventors, entrepreneurs, managers, executives, and financiers\" in order to \"humanize\" biotechnology to a lay audience. The history of the biotechnology industry includes examining the complex relationships and socio-political dynamics that occur when science and entrepreneurship come together.\n\nThe idea for a foundation that would collect and share the history of biotechnology came about at a meeting in early January 2009 in San Francisco attended by G. Steven Burrill of Burrill & Company, Dennis Gillings of Quintiles in Durham, NC, John Lechleiter of Eli Lilly & Co., Henri Termeer, then CEO of Genzyme and Arnold Thackray, founding President and CEO of the Chemical Heritage Foundation (CHF)\n\nThackray had shaped Chemical Heritage Foundation—\"the premier institution preserving the history of chemistry, chemical engineering, and related sciences and technologies.\" Oral history was one component of the CHF mandate of preserving interpreting, and promoting the history of science. In 1982 the University of Pennsylvania and the American Chemical Society had launched the Center for the History of Chemistry which was renamed the Chemical Heritage Foundation (CHF) in 1992. Thackray, a Fellow of American Academy of Arts and Sciences, the Royal Historical Society and the Royal Society of Chemistry, Thackray received his M.A. and Ph.D. degrees in the history of science from Cambridge University.\n\nThackray argued that before LSF was founded, the recorded history of biotechnology was \"fragmented, uneven, and rather paltry.\" He observed that, \"If you don't write your own history, somebody else will do it for you, and they may be hostile.\"\n\nBy the end of 2011, LSF's steering committee of industry leaders— Joshua Boger, Robert Carpenter, Bob Coughlin, Henri Termeer and Peter Wirth— were promoting the foundation's work by encouraging scientists and industrialists who were members of the Massachusetts Biotechnology Council, to contribute potential stories and materials to the archival record of the history of biotechnology in Boston and the surrounding region.\n\nThe Life Sciences Foundation conducted oral history interviews with scientists, entrepreneurs, executives, policy makers, and leaders of thought in the biotechnology industry. LSF's hosts timelines, transcripts and audio recordings and provides links to existing oral histories housed at institutions across the globe.\n\nOriginal documentary materials pertinent to the history of biotechnology and the life sciences are being collected. The materials include personal papers and correspondence, donated company records, laboratory notebooks, photographs, video and audio recordings. Collected materials will be guided to permanent repositories in appropriate institutional settings. Electronic reproductions will be made available to scholars, journalists, educators, and the general public in a digital archive.\n\nLSF historians work on a range of publications including a quarterly magazine, scholarly articles, white papers, and books. These works are intended for multiple audiences and focused on the emergence and evolution of biotechnologies in pharmaceutical discovery and development, agriculture, energy production, and environmental remediation. In October 2011, the University of Chicago Press released \"Genentech: The Beginnings of Biotech\" by Life Sciences Foundation historian Sally Smith Hughes.\n\nFounding partners of the Life Sciences Foundation include Burrill, Celgene, John Lechleiter, Genentech, Henri Termeer, Merck & Co., Millennium, Pfizer, Quintiles, and Thermo Fisher. MIT professor, Phillip Sharp, serves as LSF's academic advisor. Its executive and advisory board members are leaders from biotech, venture capital, academic institutions and trade associations.\n\nWhen Thackray retired in 2012, Heather R. Erickson, 34, was appointed as LSF President and CEO and member of the Board of Directors. Thackray remained as LSF advisor to its scholarly activities. The Board also includes Brook Byers of Kleiner Perkins Caufield & Byers in Menlo Park, California, Carl B. Feldbaum of Biotechnology Industry Organization (BIO) in Washington, DC who replaced Burrill, Frederick Frank of EVOLUTION Life Science Partners in New York, NY, Gillings in Durham, NC, Lechleiter in Indianapolis, IN, Scott Morrison from San Francisco, CA, Ivor Royston, MD, of Forward Ventures in San Diego, CA, Phillip Sharp from Massachusetts Institute of Technology in Cambridge, MA and Henri Termeer in Cambridge, MA. The first board of directors also included G. Steven Burrill, CEO of Burrill & Company— who also published The Journal of Life Sciences and Joshua Boger, former chairman and CEO of Vertex Pharmaceuticals.\n\n"}
{"id": "56896664", "url": "https://en.wikipedia.org/wiki?curid=56896664", "title": "List of Fujitsu image scanners", "text": "List of Fujitsu image scanners\n\nThe following is a partial list of image scanners manufactured under the Fujitsu brand.\n\n\n\n"}
{"id": "51768040", "url": "https://en.wikipedia.org/wiki?curid=51768040", "title": "Magnetic drilling machine", "text": "Magnetic drilling machine\n\nA magnetic drilling machine is a portable drilling machine with a magnetic base (either electromagnetic or permanent magnet). It can use twist drill bits, annular cutters, milling cutters, and other rotary cutters. With suitable bits it can also tap threads, ream, and countersink. Its combination of a stable magnetic base and low RPMs help resist or reduce torque forces created by large diameter bits.\n\nA portable magnetic drilling machine is faster and more portable alternative to hole making machines such as the drill press, and is more accurate than a hand drill. \n\nA portable magnetic drilling machine is used on steel or other magnetic materials. It gives an accuracy of 0.01 mm to 0.05mm in steel or other magnetic materiel. The drill bits used for this machines are generally made from high-speed steel(HSS) or are tungsten carbide tipped(TCT).\n\nThe base of a magnetic drill is equipped with a powerful electromagnet to easily clamp the machine on the work piece to be drilled. When energized this magnet is held on the metal work piece locking the machine base to the surface. The electromagnet plays a very important role in a portable magnetic drill, as it helps the machine to be steady, does not let the machine dismount during drilling, can work with the machine overhead, horizontal or vertical. Generally, a magnetic core drilling machine is used on a ferrous material directly, but it can also be used on no ferrous material like stainless steel with the help of clamping devices.\n\nA drill stand is the main body of the magnetic drill where the electric switches for motor and magnet are mounted, magnet indicator is mounted and also the clock-anticlockwise direction switches are mounted. The body of the magnetic drill holds together the motor and the magnet base. The feed handle is also attached to the body. The body of the magnetic drill helps the motor slide on it to get an upward and downward feed. The body of the magnetic drill also plays the role of a handle to lift and move the machine from one place to other. The material used for the body is generally cast iron.\n\nAn arbor or chuck on a magnetic drill is attached to the motor. it is a type of clamp used to attach the core drills. There are mainly two types of chuck available for the magnetic drill, industrial arbor (manual tightening) and quick change drill chucks. The quick change drill chucks are easy and fast option to attach the core drills. They do not need to tighten the screws/jaws manually. The arbor or chucks have different types of spindle holder (machine taper) like Morse taper MT 2, MT 3, and MT 4.The BDS MAB 1300 core drilling machine is the only machine in the world with MT 4 tool holder. The chuck allows different types of core drill shafts (shanks) to fit in it.\n\nPopular forms of magnetic drilling machine include: \n\nVery light weight types magnetic drills are very popular to perform several operations where the weight of a machine to carry is a great concern like working on an electric pole, mobile tower, TV tower, Bridges, etc.\nMagnetic core drilling machines with fully and semi-automatic drill feed are very popular these days. These machines help in saving time and energy and resulting in more production.\n\nPneumatic core drilling machines are specially used where there is a danger of fire due to electricity. This model used for the more secure purpose. The motor is driven by pneumatic medium and the magnetic is a permanent magnet instead of an electromagnet.\n\nBattery operated magnetic core drilling machines are used for a work place where there is no electricity. The motor is driven by a rechargeable battery. The magnet for these machines is either electromagnet of the permanent magnet. \nHorizontal magnetic core drilling machines with angular gears are made for confined drilling situations.\n\nThe magnetic core drilling machine utilizes core drill bits or annular cutters. \n\nWith a cutter wall thickness of approximately 5 mm only a small amount of material around the edge of a hole is removed by an annular cutter. Numerous teeth remain sharper longer than the single pointed tip of a spiral drill. Holes produced are smooth and burr-free - no reaming is required. \n\nDrilling holes with a magnetic drilling machine is a three-step process:\n\n1. The pilot pin accurately centers the cutter over the area to be drilled.\n\n2. During drilling, the pilot pin retracts and allows the internal lubrication to reach the cutting teeth.\n\n3. When the hole is complete, the slug/core is automatically ejected from the cutter, leaving an accurate, finished hole.\n\nHigh-quality precision-engineered cutters may have tapered inner walls. These hellp offset the effect of frictional heat build-up that causes expansion of both the cutter and waste slug being produced by the cutting action, allowing ready ejection of the slug upon completion.\n"}
{"id": "51689971", "url": "https://en.wikipedia.org/wiki?curid=51689971", "title": "Malaysian Industry-Government Group for High Technology", "text": "Malaysian Industry-Government Group for High Technology\n\nThe Malaysian Industry-Government Group for High Technology or better known as MIGHT is an independent non-profit technology think tank under the purview of the Prime Minister's Department. It was established in 1993 to support the Science Advisor to the Prime Minister and leverage on the multi-disciplinary and inter-ministerial synergies from both the industry and Government.\n\nMIGHT was tasked to be forward looking, utilizing foresight & futures (then known as prospecting) to help drive the advancement of high technology competency and capacity in Malaysia. A public-private partnership organization in nature, it provides a consensus building platform for collaboration in developing policies and strategic advice to the government.\n\nThrough its platform and works, MIGHT gave birth to notable and strategic national initiatives such Malaysia's Formula 1, Kulim High-Tech Park, Malaysian Automotive Institute, Technology Depository Agency and many others. Remaining in the background, MIGHT is known in certain circles to be a \"surrogate mother\" or the \"nation's kitchen crew\".\n\nMalaysia's emphasis on development of science and technology (S&T) is nothing new. The government has long initiated active measures to promote and develop techno-business opportunities by harnessing science and technology (S&T). In 1984, under then Prime Minister Dr. Mahathir Mohamad, a Science Advisor’s post was created in the Prime Minister’s Department to create a conducive ecosystem where S&T and its uptake could flourish. The move is seen as complementary as well as to provide a second opinion to those of the relevant Ministries. Dr. Omar Abdul Rahman was appointed to the post of Science Advisor to the Prime Minister and held the post until he retired in 2001.\nThe seed of MIGHT was sown when a Unit under the Office of Science Advisor was created aptly named 'High Technology Special Unit' \"(Unit Khas Teknologi Tinggi)\" . This unit then grew to become what MIGHT is today.\nMIGHT's focus and emphasis has been very dynamic throughout the years but has always been in the areas of high technology and heavy engineering. The focus emphasis was dependent on the maturity of the industry as well as timing of the intervention. Throughout the years, MIGHT focus areas includes but not limited to the following:-\nMIGHT is chaired jointly by the Science advisor as well as a senior captain of the industry appointed by the Prime Minister.\n2011 – present\n\nMIGHT's board is represented by both senior government officials and captains of the industry. This includes representatives from the following organizations.\nGovernment representation\n\nIndustry representation\n\nMIGHT is helmed by a President & Chief Executive Officer and supported by Senior Vice Presidents and Vice Presidents. The make up of the senior management changes with the growth of the organization as well as changes in emphasis to reflect the dynamic nature of MIGHT's focus areas. The following is the current senior management of MIGHT\nPresident & CEO\n\nSenior Vice Presidents\n\nVice Presidents\n\nMIGHT's programs and activities revolves around the following\n\nForesight & futures thinking is a core competency and activity of MIGHT. Known as technology prospecting in its early days, MIGHT has been conducting technology foresight and future studies work to support its other activities though there are evolution and changes to the methods and processes. To expand foresight beyond technology, MIGHT created myForesight - Malaysia Foresight Institute in 2012.\n\nmyForesight® was created in 2012 with the following objectives:-\n\nOutcome of MIGHT's foresight & future studies are used to prioritize technology and industry development in Malaysia. To date, MIGHT has produced more than twenty (20) industry/sector blueprints and roadmaps. These documents were used as references to chart the development of various industry and technology in Malaysia. Various white papers and proposals by MIGHT are also used for these purposes. \nIn continuous search of new areas, some of these programs has since been passed to other government agencies or machineries to undertake. The following are amongst notable programs conducted by MIGHT:-\n\nThe following are highlights of Publications produced by MIGHT:-\n\n\nSince its inception, MIGHT has actively been leveraging its global network as part of a strategy to built national capacity as well as to disseminate knowledge and expertise. Notable past activities includes Langkawi International Dialogue, various Smart Partnership program with CPTM. Nowadays the program of choice are the following:-\n\nGSIAC is chaired by the Prime Minister of Malaysia, YAB Dato’ Sri Mohd Najib Tun Razak, The secretary is the Science Advisor to the Prime Minister of Malaysia. The council consists of selected Malaysian Ministers, national and global corporate leaders, Nobel Laureates, eminent global academicians and researchers. The council meets once a year to deliberate on strategic and future matters that will benefit Malaysia in the long run\n\nmyKOR or Malaysia Korea Technology Center was launched by Prime Minister Abdullah Ahmad Badawi on 20 October 2008. The center purpose is to serve as a gateway for Malaysian organizations and businesses to capitalize and gain access to the pool of Korean IPs and technologies, for the purpose of enhancing and increasing the value of Malaysian made products and services.\n\nRecognizing that technology and industry development will require the necessary human capital to support them, MIGHT has been involved in various human capital development programs. This is done through partnership with selected educational institutions as well as industry collaborators. These includes programs that aims to promote the uptake of Science, Technology, Engineering & Mathematics (STEM) amongst students as well as industry bridging programs.\n\nKLESF is an annual program jointly organized by MIGHT, Akademi Sains Malaysia, Universiti Teknologi MARA and Universiti Tunku Abdul Rahman. The program objectives is to promote STEM (Science, Technology, Engineering & Mathematics) to students, parents, teachers and public alike.\n\nFame Lab is a science communication competition co-organized by MIGHT & British Council in search of the best science communicator in the country. In 2016, Dr Abhimanyu Veerakumarasivam, representing Malaysia won the ‘Best Science Communicator award at Fame Lab International 2016. Fame Lab International is the world’s biggest science communication competitions organized in the United Kingdom attracting participants from 27 countries.\n\nSchool Lab Malaysia A science communication competition that aims to help students understand the exciting challenges of science, develop critical and creative thinking skills and, at the same time, gain confidence to present their understanding of scientific concepts.\n\nMIGHT's foray into entrepreneurship is due to its role in encouraging the uptake of technology business. To date MIGHT's venture into this includes technology advice and coaching, market identification and access.\n\nGlobal Cleantech Innovation Program (GCIP) is a program conducted in collaboration with United Nations Industrial Development Organization (UNIDO) and Cleantech Open to assist Malaysian entrepreneurs in the area of Green & Clean Technology. The program started in 2014 and the winners of the program are given opportunities to pitch in Silicon Valley as well as access to funding provided by Platcom Ventures.\n\n"}
{"id": "55978210", "url": "https://en.wikipedia.org/wiki?curid=55978210", "title": "Manel Muñoz", "text": "Manel Muñoz\n\nManel Muñoz (Barcelona,1996) is a Catalan cyborg artist based in Barcelona, best known for developing and installing barometric sensors in his body. The sensors allow him to feel atmospheric pressure changes through vibrations in his ears. Depending on the changes he feels, he can predict weather changes as well as feel at what altitude he is in.\n\nMuñoz studied contemporary photography in Barcelona and became Cyborg Foundation's artist in residence in 2016. In 2017, he co-founded the Transpecies Society, an association that gives voice to people who do not identify as being 100% human and raises awareness on issues they face. The association, based in Barcelona, offers workshops specialized in the design and creation of new senses and organs.\n\nMuñoz has shared his experience as a cyborg artist by performing and speaking in conferences and festivals in Germany, UK, Romania, Spain and The Netherlands among others.\n\n\n"}
{"id": "35670365", "url": "https://en.wikipedia.org/wiki?curid=35670365", "title": "Mexican barbasco trade", "text": "Mexican barbasco trade\n\nThe Mexican barbasco trade was the trade of the diosgenin-rich yam species \"Dioscorea mexicana\", \"Dioscorea floribunda\" and \"Dioscorea composita\" which emerged in Mexico in the 1950s as part of the Mexican steroid industry. The trade consisted in Mexican campesinos harvesting the root in the jungle, selling it to middlemen who brought it to processing plants where the root was fermented and the diosgenin extracted and sold to pharmaceutical companies such as Syntex who used it to produce synthetic hormones.\n\nThe trade started when Russell Marker, a chemist looking for a plant source from which to extract diosgenin and saponin, traveled to Veracruz looking for the yam \"Dioscorea mexicana\" which he suspected might be suitable. He hired two Mexican campesinos to bring him exemplars of the tuber. When he discovered that the root was indeed a significant source of diosgenin he established Syntex, the first Mexican fine chemical company dedicated to producing semisynthetic hormones from Barbasco. Before this development, natural hormones were extracted from animal sources, such as urine from pregnant mares or women, or from bull testes; prices were consequently very high. With the development of the process of Marker degradation which allowed the production of hormones from vegetable saponin sources, Marker began a search for a plant steroid of the sapogenin class with a ring structure more like progesterone. With the discovery of the chemical properties of the barbasco root, world market prices for steroids and other synthetic hormones plummeted – making them feasible for large scale production of medicines for common ailments such as arthritis or Addison's disease, and eventually as the basis for the combined oral contraceptive pill.\n\nThis development sparked a barbasco extraction industry centered on the barbasco-rich areas of southeastern Mexico, in Northern Oaxaca, Southern Veracruz and Puebla states. Especially the area called Chinantla in Northern Oaxaca, around the cities of Tuxtepec and Valle Nacional. The root was extracted in the wild by \"barbasqueros\", often poor Indigenous Chinantecs, who ventured into the jungle to dig out the tuber with digging sticks or with their bare hands. Before becoming used industrially, the tuber was used by Chinantec healers in northern Oaxaca as an abortifacient and in cures for aching joints. It was also used by Chinantecs as a poison for fishing in the Papaloapan river.\n\nBy the mid 1970s 125,000 Mexican peasants depended on the barbasco trade for their livelihood, and ten tons of Barbasco per week were extracted from the wild.\n\nQuickly a system of middlemen appeared, as those who had enough means to pay \"barbasqueros\", started buying large quantities, often using a system of debt peonage. They would start by giving the \"barbasquero\" a loan which he or she would then have to pay off with barbasco. These middlemen would eventually establish \"acopios\", recollection and distribution centrals where large quantities of barbasco are gathered and shipped on to the \"beneficios\", the processing plants.\n\nAt the processing plants the tuber is inspected, washed and chopped up, and mixed with water to produce a thick paste. The paste is then put into fermentation vats where it remains several days, after which it is taken out and sun dried on a concrete floor where it is turned by workers using rakes. In the process of drying the paste crystallizes into diosgenin granules, also called flour. The flour is then bagged and sent to laboratories where the diosgenin content is measured and the price is calculated based on the diosgenin percentage, which varies from 4–6%.\n\nKnowledge of the uses and purposes of the barbasco tuber was highly stratified and barbasqueros often did not know the true purpose of the root they were gathering: they were frequently told that it was used for soap. The acopio owners knew more about the process and eventually invented ways of improving the diosgenin concentration in roots collected by adding different solvents to the tubers before shipping them to the beneficios.\n\nIn the late 1970s, populist President Luis Echeverría sought to organize and nationalize the barbasco trade in order to derive more benefit to the barbasqueros and to the Mexican state. He established the organization PROQUIVEMEX (Productos Químicos Vegetales de México). However, at this point Mexico had lost its status as a world leader of the synthetic hormone market and the barbasco trade was declining, just as the root was becoming depleted in the wild.\n\nAlso during the 1970s it became possible to produce steroids from soy phytosterols, including progesterone. This meant that barbasco was no longer necessary as a base product, and international reliance on Mexican yam stopped. Today only a few communities in Northern Oaxaca continue to produce barbasco, and the few existing beneficios process only a few tons per year. In 1999 65% of families in the municipio of Santiago Jocotepec depended on barbasco production, whereas in the municipios of San Juan Lalana it was 29.2%, in San Felipe Usila 28.3 and in San Lucas Ojitlan 24.4%.\n"}
{"id": "1705010", "url": "https://en.wikipedia.org/wiki?curid=1705010", "title": "Minerals Management Service", "text": "Minerals Management Service\n\nThe Minerals Management Service (MMS) was an agency of the United States Department of the Interior that managed the nation's natural gas, oil and other mineral resources on the outer continental shelf (OCS).\n\nDue to perceived conflict of interest and poor regulatory oversight following the Deepwater Horizon oil spill and Inspector General investigations, Secretary of the Interior Ken Salazar issued a secretarial order on May 19, 2010 splitting MMS into three new federal agencies: the Bureau of Ocean Energy Management, the Bureau of Safety and Environmental Enforcement, and the Office of Natural Resources Revenue. MMS was temporarily renamed the Bureau of Ocean Energy Management, Regulation and Enforcement (BOEMRE) during this reorganization before being formally dissolved on October 1, 2011.\n\nHeadquartered in Washington, DC, the Agency received most of its revenue from leasing federal lands and waters to oil and natural gas companies with a profit margin of 98%. It was among the top five revenue sources to the federal government, the IRS being number one. As the MMS (before transition to BOEMRE), the Agency's signature feature according to an informational trifold was that it had \"become our Nation's leader in offshore energy development and the collection of royalties on behalf of the American Public.\" With respect to enforcement of regulations and safety, this same publication indicated that the \"MMS also funds advanced scientific studies and enforces the highest safety and environmental standards.\" The Agency's mission statement was put more formally in its 2010 Budget Proposal:\n\nThe Minerals Management Service was created on January 19, 1982. In January 1983, Congress passed the Federal Oil and Gas Royalty Management Act with the stated purpose:\n\nThe Secretary of the Interior at the time, James G. Watt, designated MMS as the administrative agency responsible for execution of activities under the Act.\n\nWith the passage of the Energy Policy Act of 2005, MMS was given authority to develop renewable energy projects such as wave, wind and current energy on the Outer Continental Shelf. As of 2010, the Agency was composed of two operating units, the MRM and OEMM.\nThe Agency's offshore renewable energy program included development of renewable energy such as wind, wave, and solar.\n\nSince its inception in 1982 through FY2008, the Agency had disbursed approximately $200 million to Federal, state, and American Indian accounts.\n\nOn June 21, 2010, the Minerals Management Service was renamed the Bureau of Ocean Energy Management, Regulation and Enforcement and reorganized.\n\nAs of 2009, the Agency employed about 1,600 people, which was proposed to grow by less than one hundred in 2010.\n\nThe BOEMRE was reorganized in May 2010 under the direction of Secretary of the Interior Ken Salazar following the Deepwater Horizon disaster. The bureau is organized into these three newly created agencies:\n\n\nSince the inception of the MMS, and in particular since the 1990s, the Agency has been embroiled or implicated in numerous scandals. For example, in 1990 MMS employees were linked to prostitution, and in 2008 the Department of Interior's Inspector General reported that MMS employees had participated in drug use and sexual activity with employees from the very energy firms they were to be regulating.\n\nFrom the 1950s to at least 2002, drilling for oil and gas on federal lands and waters produced the second largest source of revenue for the federal government other than taxes. The Minerals Revenue Management (MRM) division of MMS was responsible for managing all royalties associated with both onshore and offshore oil and gas production from federal mineral leases. In 1997, in light of evidence that industry was getting around royalty regulations and underpaying royalties to the tune of billions of dollars, MMS proposed a more stringent rule to collect royalty payments in value (RIV), meaning in the form of cash payments from companies producing from federal leases. In response to that rule-making, industry proposed an alternative—\"royalty-in-kind\" (RIK) meaning in the form of actual oil or gas production. In fact, the industry opposed cash payments (RIV) and planned legal challenges to government efforts to establish regulations for fair market-based royalty payments. A pilot test of the RIK concept was conducted. The Bush administration allowed the pilot to expand to a full program, with industry support, even though the bill authorizing the program failed to pass in Congress. In FY2008, the RIK program accounted for more than 50% of the Agency's revenue collections. When MRM collected royalties-in-kind, the oil or gas received from producers was offered for sale by the U.S. Government on the open market and the proceeds from these sales were taken as revenues. The RIK program within MRM was responsible for managing these in-kind sales.\n\nIn 2003, the General Accounting Office (GAO) noted that the MMS had failed to develop \"clear strategic objectives linked to statutory requirements nor collected the necessary information to effectively monitor and evaluate the Royalty-in-Kind Program\". From 2003 to 2008, the GAO consistently challenged the legitimacy of the statistics published by the MMS that it used to support its claims that the RIK program was a success and justify its expansion. Deficits in accounting practices, policies and procedures, and information systems used by the MMS led to concerns that the industry was significantly underpaying on their royalty obligations. Computer systems in use by the Agency were considered to be sufficiently inadequate that a failure to report revenue or provide RIK by an industry member could not be reliably detected. For instance, the GAO estimated that underpayments amounted to ~$160 million USD in 2006. The GAO also disputed the practice of tracking oil and gas RIK deliveries on a monthly rather than daily basis, a practice used by the MMS and supported by the Department of the Interior, but potentially prone to abuse by producers. Other contributing factors to the placing the accuracy of revenue streams at risk were insufficiently trained personnel and insufficient numbers of personnel working in the RIK program and a lack of standard reporting method by industry members, leading to manual rather than computer-based processing of more than half of the data required for RIK data inputs.\n\nCiting its scandals and the persistent incapacity of the RIK program to fulfill its statutory obligations, Interior Secretary Salazar announced in September 2009 that the RIK program would be shut down. Due to existing lease contracts with RIK provisions, the program as of 2010 is still winding down. On October 7, 2009, the U.S. House Oversight Committee reported the loss of billions in revenue resulting from MMS mismanagement and cozy relationships with industry officials. According to Darrell Issa, the top Republican on the United States House Committee on Oversight and Government Reform, there may be a conflict of interest for the Minerals Management Service to collect revenue and also oversee safety.\n\nIn September 2008, reports by the Inspector General of the Interior Department, Earl E. Devaney, were released that implicated over a dozen officials of the MMS of unethical and criminal conduct in the performance of their duties. The investigation found MMS employees had used cocaine and marijuana, and had sex with energy company representatives. MMS staff had also accepted gifts and free holidays amid \"a culture of ethical failure\", according to the investigation. The New York Times's summary states the investigation revealed \"a dysfunctional organization that has been riddled with conflicts of interest, unprofessional behavior and a free-for-all atmosphere for much of the Bush administration's watch.\"\n\nA May 2010 inspector general investigation revealed that MMS regulators in the Gulf region had allowed industry officials to fill in their own inspection reports in pencil and then turned them over to the regulators, who traced over them in pen before submitting the reports to the agency. MMS staff had routinely accepted meals, tickets to sporting events, and gifts from oil companies. Staffers also used government computers to view pornography. In 2009 the regional supervisor of the Gulf region for MMS pleaded guilty and was sentenced to a year's probation in federal court for lying about receiving gifts from an offshore drilling contractor. \"This deeply disturbing report is further evidence of the cozy relationship between MMS and the oil and gas industry,\" Salazar said.\n\nThe Project On Government Oversight (POGO) alleges that MMS has suffered from a systemic revolving door problem between the Department of Interior and the oil and gas industries. For example, thirteen months after departing as MMS director, Bush appointee Randall Luthi became president of the National Oceans Industries Association (NOIA) whose mission is \"to secure reliable access and a favorable regulatory and economic environment for the companies that develop the nation's valuable offshore energy resources in an environmentally responsible manner.\" Luthi succeeded Tom Fry, who was MMS director under the Clinton administration. Luthi and Fry represented precisely the industries their agency was tasked with being a watchdog over. Lower level administrators influencing MMS have also gone on to work for the companies they once regulated: In addition, Jimmy Mayberry served as Special Assistant to the Associate Director of Minerals Revenue Management (MRM), managed by MMS, from 2000 to January 2003. After he left, he created an energy consulting company that was awarded an MMS contract via a rigged bid. He was convicted along with a former MMS coworker Milton Dial who also came to work at the company. Both were found guilty of felony violation of conflict of interest law.\n\nOn May 11, 2010, in response to the Deepwater Horizon oil spill, Secretary of the Interior Ken Salazar announced that MMS would be restructured so that the safety and environmental functions are carried out by a unit with full independence from MMS in order to ensure that federal inspectors will have more tools, resources, and greater authority to enforce laws and regulations that apply to oil and gas companies operating on the Outer Continental Shelf. Another outcome of the spill was the retirement of the associate director for offshore energy and minerals management at the time of the spill, Chris Oynes.\n\nMMS's regulatory decisions contributing to the 2010 oil spill included, in negligence, the decision that an acoustically controlled shut-off valve (BOP) would not be required as a last resort against underwater spills at the site, MMS's failure to suggest other \"fail-safe\" mechanisms after a 2004 report raised questions about the reliability of the electrical remote-control devices., and the fact that MMS gave permission to dozens of oil companies to drill in the Gulf of Mexico without first getting required permits from the National Oceanic and Atmospheric Administration that assess threats to endangered species and to assess the impact the drilling was likely to have on the gulf.\n\nOn May 19, 2010 Salazar announced that MMS will be broken up into three separate divisions, the Bureau of Ocean Energy Management, the Bureau of Safety and Environmental Enforcement, and the Office of Natural Resources Revenue, which will separately oversee energy leasing, safety enforcement, and revenue collection.\n\nS. Elizabeth (Liz) Birnbaum served as the Director of the then named Minerals Management Service from July 15, 2009 to her resignation on May 27, 2010 amidst the Deepwater Horizon oil spill. On June 15, 2010 President Obama named Michael R. Bromwich, a former federal prosecutor and inspector general for the Justice Department, to head up efforts to restructure BOEMRE. Bob Abbey, then director of the Bureau of Land Management, took over as Acting Director of BOEMRE until his replacement could be confirmed. Amidst efforts to reorganize the beleaguered agency, on June 21, 2010, Bromwich was sworn in as BOEMRE's new director, and Secretary of the Interior Ken Salazar issued a Secretarial Order that renamed the Minerals Management Service the Bureau of Ocean Energy Management, Regulation and Enforcement. Almost a year later, William K. Reilly, who co-chaired the commission charged with investigating the Horizon blowout, was quoted as saying \"they changed the name, but all the people are the same\" and \"it's embarrassing\" in reference to the current situation.\n\n\n"}
{"id": "604572", "url": "https://en.wikipedia.org/wiki?curid=604572", "title": "Nariman Point", "text": "Nariman Point\n\nNariman Point is a business district in Downtown Mumbai. Formerly the prominent business district on India's west coast, Nariman Point yielded that status to Mumbai's Bandra-Kurla Complex in 2010. Prior to Nariman Point's development, Mumbai's business centre was at Ballard Estate, which like Nariman Point also was built on land reclaimed from the sea.\n\nLocated on the southern tip of the Mumbai peninsula, at the end of the Mumbai's Marine Drive, Nariman Point is named after Khursheed Framji Nariman, a municipal corporator who had initiated the area's development as an extension to the Back Bay reclamation. Nariman Point houses some of India's prestigious business headquarters, and despite its decline (see below), it remains one of the more expensive business districts in India, exceeded only by Delhi's Connaught Place and since April 2012 by Mumbai's own Bandra-Kurla Complex.\n\nPrior to 1940, the area was part of the Arabian sea. A popular leader of the Congress, Khurshed Nariman (affectionately called \"Veer\" Nariman), a Bombay Municipal Corporation corporator, proposed to reclaim the area from the sea near Churchgate. To accomplish this task, the shallow seafront was filled with debris from various parts of the city. Reinforced concrete cement was also used, the steel for which had to be purchased on the black market at higher prices due to World War II.The entire cost was estimated to be (equivalent to about in 2018). Additional reclamations were carried out in the 1970s. A construction boom in that decade also led to the development of commercial high-rises in the area.\n\nIn 2006, prior to the financial crisis of 2007–08, Nariman point was the 7th most expensive location in the world for office space. However, by December 2012 Nariman Point had fallen to 25th place while Delhi's Connaught Place remained the 5th most expensive location despite many offices moving to Gurgaon and Noida. During the same period, Nariman Point also dropped from 7th to 15th most expensive location for office rentals. The reasons for the decline were the high prices, lower quality and age of construction, and increasing distances from residential hubs which have now moved northwards and to the suburbs. In the first three quarters of 2012, Nariman Point had a vacancy rate of almost 25%, compared with 18% in the rest of the Mumbai city.\n\nAir India had its headquarters in the Air India Building for many years.\n\nRenewable Energy and Power companies Ind Renewable Enercy Ltd and Vakharia Power Infrastructure Ltd are headquartered at Regent Chambers, 208 Nariman Point, Mumbai.\n\nAt one time All Nippon Airways maintained its Mumbai sales office in the Oberoi Trident Towers in Nariman Point.\n"}
{"id": "27934732", "url": "https://en.wikipedia.org/wiki?curid=27934732", "title": "Oak Ridge School of Reactor Technology", "text": "Oak Ridge School of Reactor Technology\n\nOak Ridge School of Reactor Technology (ORSORT) was the successor of the school known locally as the Clinch College of Nuclear Knowledge, later shorten to Clinch College. ORSORT was authorized and financed by the U.S. government and founded in 1950 by Admiral Hyman G. Rickover and Alvin Weinberg. During its existence, the school was the only educational venue in the U.S. from where a comprehensive twelve-month education and training in either \"Reactor Hazards Analysis\" or \"Reactor Operations\" could be obtained, with accompanying certificates. Funding ended and the school was closed in 1965, shortly after authorization was extended to select U.S. universities to develop their own Nuclear Engineering curricula. Housed at the Oak Ridge National Laboratory, this unique venue and its renowned instructors offered its students the highest level of education of practical applications of atomic energy available at the time, and first-hand exposure to a variety of nuclear reactor designs including the legendary first graphite reactor, pool reactor, high temperature gas reactor, molten salt reactor, fast reactor and high flux reactor. \n\nThe school was made known first nationally and eventually worldwide to U.S. enterprises and to U.S. allies involved in the development of peaceful uses of atomic energy, and who were interested in educating and training designated scientific and engineering personnel at its unique venue. In 1959, ORSORT accepted its first international enrollments. Applications to enroll required strict clearance from the Atomic Energy Commission. Tuition fees partially offset school operating costs. Courses listed in their 1965 curricula included Analysis, Chemical Technology, Economics of Nuclear Power, Engineering Science, Experimental Physics, Nuclear Systems Laboratory, Hazards Study, Health Physics, Instrumentation and Controls, Materials, Mathematics, Meteorology, Physics, Reactor Operating Experience and Shielding. Scientific and engineering graduates of the one-year program earned certificates of completion and were awarded the degree of Doctor of Pile Engineering (D.O.P.E.). ORSORT turned out up to 100 graduates a year, many of whom became leaders in the nuclear industry, such as a former Secretary of Energy, James D. Watkins. The total number of ORSORT graduates was 976. In addition to 19 US students from the Atomic Energy Commission, the Oak Ridge National Laboratory and US utilities, the last graduating class of 1965 included engineering and scientific personnel sponsored by their governments in Australia, India, Israel, Japan, Netherlands, Pakistan, Philippines and South Africa.\n\n\n"}
{"id": "3774655", "url": "https://en.wikipedia.org/wiki?curid=3774655", "title": "Pakistan Council of Scientific and Industrial Research", "text": "Pakistan Council of Scientific and Industrial Research\n\nThe Pakistan Council of Scientific and Industrial Research (PCSIR) is a government-owned science and industrialisation research organisation which mainly focuses on the development of industrial research. Initially established as Pakistan Department of Research in 1951, it was reformulated in its current state in 1953.\n\nThe PCSIR was established by Prof. Dr. Salimuzzaman Siddiqui in 1953 for the development of scientific and technical Research and Development and to provide infrastructure for industrial development in Pakistan. The organisation was founded under the Societies Act to promote the cause of Science and Technology in the country. Since 1973, it has functioned under the Act of Parliament. The organisation was under control of Pakistan's Ministry of Science and Technology, and was given autonomous control in 1984.\n\nAs of today, the PCSIR has several geographically dispersed research centres. including four regional offices in each of the provincial capitals, with the head office in Islamabad. There are eleven laboratories and units and five HRD centres established throughout the country, headed by director generals or directors who directly report to the chairman. The chairman of PCSIR is appointed by the Government of Pakistan. There are 150 officers and technical staff in the head office, including seven directors working in different divisions and departments. There are 681 scientists, engineers, and technologists working in different laboratories, of whom eighty have Ph.D. degrees and others have M.Sc./MS/M.Phil./B.E. degrees in multidisciplinary fields. They are supported by 1,656 technical and skilled staff and 178 administrative staff.\n\nIn January 2017, the council conducted tests on 16 brands of packaged milk in the interest of public safety and found that only 6 of 16 brands were safe for public consumption. This report was presented to the National Assembly of Pakistan.\n\n\n"}
{"id": "42562996", "url": "https://en.wikipedia.org/wiki?curid=42562996", "title": "Plath GmbH", "text": "Plath GmbH\n\nPLATH GmbH is a German company specialising in military radio monitoring and radiolocation, active internationally and with headquarters in Hamburg.\n\nThe company specialises in communication intelligence for tactical (COMMS ESM) and strategic applications (COMINT) on the one hand, as well as applications for maritime routing and search and rescue support on the other. With its products and systems, and in association with various subsidiaries, PLATH GmbH covers the entire process chain for communication intelligence, from sensor systems to analysis and the evaluation of mass data. PLATH GmbH has ca. 180 employees and achieved revenues of €45 million in the 2016 business year. In 2009 PLATH GmbH was both ranked 49th. amongst the hundred global market leaders, as well as being named ‘Hidden Champion’ by VDI Nachrichten.\n\nIn 1837 David Filby, an instrument-maker from Husum, founded a trading house for nautical instruments and maps, which was acquired in 1862 by Hamburg citizen Carl Christian Plath. Further reorganisations and investment followed, for example in the company Cassens & Bennecke, which, operating from 1909 under the name Cassens & Plath, was engaged in the sale of navigation devices in Bremerhaven, or Weems und Plath in Annapolis, USA, until C. Plath KG was founded in 1939. In 1950 C. Plath KG set up a department for the development of radio navigation in its so-called Compass House, a symbol of the port of Hamburg for decades. This department was headed by Maximilian Wächtler, regarded as a pioneer in radiolocation and radio and remote intelligence (recipient of the Rudolf-Diesel-Medaille), holding more than sixty patents in this field.\nFollowing the incorporation of parts of the signalling company founded in Kiel in 1911, this department eventually became C. Plath GmbH – what is now PLATH GmbH. The PLATH group now encompasses PLATH and its subsidiaries innoSysTec, PROCITEC, PLATH EFT and PLATH AG. The group is now majority-owned by the Handelsgesellschaft Scharfe mbH & Co. KG family business.\n\nC. Plath KG, on the other hand, became part of LITEF GmbH, now known as Northrop Grumman LITEF. Due to its origins and the many similar-sounding names, PLATH GmbH is often wrongly regarded as the successor of C. Plath KG and associated with compasses and other navigation devices. Previous managing directors of C. Plath GmbH included Mr. Pfaff, who was CEO from 1989 to 1997 and who, together with Oberst and D. Grabau, wrote some of the basic literature on communication intelligence, and which is still used today in the training of the German Bundeswehr’s electronic warfare section and are standard intelligence literature.\n\nPLATH GmbH is renowned primarily for the production of visual direction finders for navigation and the location of ships in maritime emergencies. In the days before GPS was invented, these devices were vital for finding one’s position in maritime navigation. Together with Telefunken/DEBEG, who utilised components manufactured by PLATH, PLATH GmbH dominated direction-finding technology field in the 1950s and ‘60s. This dominance in the market was so great that radio direction-finding was explained in terms of the SFP7000 in the standard manual for maritime officers\n\n\nAmongst the other innovations introduced by PLATH GmbH are e.g.: the maximum principle of the sight direction finder as well as patents for the first double-channel sight direction finder, procedures for the direction-finding and location of emitters in the event of frequency hopping or, more recently, procedures for the camouflaging of satellite navigation (GPS-Spoofing). More than 200 patents have been registered since the company was founded in 1954.\n\n\n"}
{"id": "4119246", "url": "https://en.wikipedia.org/wiki?curid=4119246", "title": "Psychology of programming", "text": "Psychology of programming\n\nThe psychology of programming (PoP) is the field of research that deals with the psychological aspects of writing programs (often computer programs). The field has also been called the empirical studies of programming (ESP). It covers research into computer programmers' cognition; tools and methods for programming-related activities; and programming education.\n\nPsychologically, computer programming is a human activity which involves cognitions such as reading and writing computer language, learning, problem solving, and reasoning.\n\nThe history of psychology of programming dates back to late 1970s and early 1980s, when researchers realized that computational power should not be the only thing to be evaluated in programming tools and technologies, but also the usability from the users. In the first Workshop on Empirical Studies of Programmers, Ben Shneiderman listed several important destinations for researchers. These destinations include refining the use of current languages, improving present and future languages, developing special purpose languages, and improving tools and methods. Two important workshop series have been devoted to psychology of programming in the last two decades: the Workshop on Empirical Studies of Programmers (ESP), based primarily in the US, and the Psychology of Programming Interest Group Workshop (PPIG), having a European character. ESP has a broader scope than pure psychology in programming, and on the other hand, PPIG is more focused in the field of PoP. However, PPIG workshops and the organization PPIG itself is informal in nature, It is group of people who are interested in PoP that comes together and publish their discussions.\n\nIt is desirable to achieve a programming performance such that creating a program meets its specifications, is on schedule, is adaptable for the future and runs efficiently. Being able to satisfy all these goals at a low cost is a difficult and common problem in software engineering and project management. By understanding the psychological aspects of computer programming, we can better understand how to achieve a higher programming performance, and to assist programmers to produce better software with less error.\n\nSome methods which one can use to study the psychological aspects of computer programming include introspection, observation, experiment, and qualitative research.\n\n\n"}
{"id": "6987057", "url": "https://en.wikipedia.org/wiki?curid=6987057", "title": "Sinosteel", "text": "Sinosteel\n\nSinosteel Corporation (S: 中国中钢集团公司, T: 中國中鋼集團公司, P: \"Zhōngguó Zhōnggāng Jítuán Gōngsī\") is a central state owned enterprise, primarily in mining, trading, equipment manufacturing and engineering, under the supervision of the State-owned Assets Supervision and Administration Commission. Founded in 1993 and based in the People's Republic of China, it is the country's second largest importer of iron ore. \n\nThere are 86 subsidiaries under the administration of Sinosteel, among which 63 are in China and 23 abroad. Sinosteel is mainly engaged in mining and processing of minerals; related trading and logistics; construction, engineering, and technical service; and equipment manufacture. It operates in Algeria, Australia, South Africa, India, Singapore, Brazil, Germany, Gabon, Cambodia, Indonesia, Vietnam, Turkey, Hong Kong, and Macao. \n\nThe company has iron and chrome mines in Australia and South Africa. \n\nSinosteel possesses a global-running sales network and logistic service system. It is the raw material supplier and sales-agent for a number of Chinese steel mills, some of which Sinosteel has entered into long-term strategic partnership. The company offers ferrous metal minerals, non-metal minerals, and coke and coal; feedstock, such as scrap, pig iron, and heavy crude oil; steel/finished steel products, including steel/semi-finished steel products, steel wire products, and steel grating; and ferroalloys. \n\nIt acts an agent for many Chinese and international equipment and technology suppliers, providing investment development, future brokerage, bidding services for Chinese steel mills, goods transportation, insurance, cargo agent, storage and port and wharf services.\n\nIt also provides manufacturing equipment, such as mining, rotating and sintering, iron making, and steel making and refining equipment; submerged-arch furnace; and continuous casting, rolling, carbon mechanical and electrical, and refractory manufacturing equipment. In addition, the company offers refractories for steel making industry, petrochemical industry, non-ferrous industry, cement industry, and glass industry. Further, it provides carbon products, electro-magnetic material, and non-ferrous metals. \n\nThe scientific and technological companies under Sinosteel operate in the areas of geological exploration, beneficiation, heat engineering, environmental protection, refractory materials, metal products and engineering design. Its engineering and construction arm is Sinosteel Equipment & Engineering Co. Ltd., which in 2012 had revenue of $1.09 billion, making it the 188th largest construction and engineering firm in the world. International revenue accounted for $271.3 million of the overall total.\n\nThe company was founded in 1993 as China Iron & Steel Trade & Industry Group Corporation and changed its name to Sinosteel Corporation in August 2004. Sinosteel Corporation is based in Beijing, China.\n\nSinosteel in 2008 completed a hostile takeover of Australian iron ore producer, Midwest Corporation, cornering 51% of the shares of the company in a A$1.36 billion acquisition. It was the first successful hostile takeover of an Australian company by a Chinese company.\n\nIn 2016, due to heavy debt of 60 million yuan owed to financial institutions, Sinosteel was greenlit for swapping 27 billion yuan of its debt for equity convertible bonds under a new government policy designed to curb run-away corporate debt in the economy.\n\n"}
{"id": "4605995", "url": "https://en.wikipedia.org/wiki?curid=4605995", "title": "Sord IS-11", "text": "Sord IS-11\n\nThe Sord IS-11 was an A4-size, lightweight, portable Z80-based computer. The IS-11 ('IS' stands for 'Integrated Software') had no operating system, but came with built-in word processing, spreadsheet, file manager and communication software.\n\nThe machine was manufactured by Sord Computer Corporation and released in 1983. It was later followed by the IS-11B and IS-11C.\n\nThe IS-11 had a CMOS version of the Z80A running at 3.4 MHz with 32-64 KiB NVRAM and 64 KiB ROM. The monochrome non-back-lit LCD screen allowed for 40 characters × 8 lines or 256 × 64 pixels. Data was stored on built-in microcassette recorder (128kb, 2000 baud).\n\n\n"}
{"id": "4723892", "url": "https://en.wikipedia.org/wiki?curid=4723892", "title": "Stamp mill", "text": "Stamp mill\n\nA stamp mill (or stamp battery or stamping mill) is a type of mill machine that crushes material by pounding rather than grinding, either for further processing or for extraction of metallic ores. Breaking material down is a type of unit operation.\n\nA stamp mill consists of a set of heavy steel (iron-shod wood in some cases) stamps, loosely held vertically in a frame, in which the stamps can slide up and down. They are lifted by cams on a horizontal rotating shaft. As the cam moves from under the stamp, the stamp falls onto the ore below, crushing the rock, and the lifting process is repeated at the next pass of the cam.\n\nEach one frame and stamp set is sometimes called a \"battery\" or, confusingly, a \"stamp\" and mills are sometimes categorised by how many stamps they have, i.e. a \"10 stamp mill\" has 10 sets. They usually are arranged linearly, but when a mill is enlarged, a new line of them may be constructed rather than extending the line. Abandoned mill sites (as documented by industrial archaeologists) will usually have linear rows of foundation sets as their most prominent visible feature as the overall apparatus can exceed 20 feet in height, requiring large foundations. Stamps are usually arranged in sets of five.\n\nSome ore processing applications used large quantities of water so some stamp mills are located near natural or artificial bodies of water. For example, the Redridge Steel Dam was built to supply stamp mills with process water.\n\nThe main components for water-powered stamp mills – water wheels, cams, and hammers – were known in the Hellenistic era Eastern Mediterranean region. Ancient cams are in evidence in early water-powered automata from the third century BC. A passage in the \"Natural History\" of the Roman scholar Pliny (NH 18.23) indicates that water-driven pestles had become fairly widespread in Italy by the first century AD: \"The greater part of Italy uses an unshod pestle and also wheels which water turns as it flows past, and a trip-hammer [mola]\". These trip-hammers were used for the pounding and hulling of grain. Grain-pounders with pestles, as well as ordinary watermills, are also attested as late as the middle of the fifth century in a monastery founded by Romanus of Condat in the remote Jura region, indicating that the knowledge of trip hammers continued into the early Middle Ages. Apart from agricultural processing, archaeological evidence also strongly suggests the existence of trip hammers in Roman metal working. In Ickham in Kent, a large metal hammer-head with mechanical deformations was excavated in an area where several Roman water-mills and metal waste dumps have also been traced.\n\nThe widest application of stamp mills, however, seems to have occurred in Roman mining, where ore from deep veins was first crushed into small pieces for further processing. Here, the regularity and spacing of large indentations on stone anvils indicate the use of cam-operated ore stamps, much like the devices of later medieval mining. Such mechanically deformed anvils have been found at numerous Roman silver and gold mining sites in Western Europe, including at Dolaucothi (Wales), and on the Iberian peninsula, where the datable examples are from the 1st and 2nd century AD. At Dolaucothi, these stamp mills were hydraulic-driven and possibly also at other Roman mining sites, where the large scale use of the hushing and ground sluicing technique meant that large amounts of water were directly available for powering the machines.\n\nStamp mills were used by miners in Samarkand as early as 973. They were used in medieval Persia to crush mineral ores. By the 11th century, stamp mills were in widespread use throughout the medieval Islamic world, from Islamic Spain and North Africa in the west to Central Asia in the east.\nWater-powered and mechanised trip hammers reappeared in medieval Europe by the 12th century. Their use was described in medieval written sources of Styria (in modern-day Austria), written in 1135 and another in 1175 AD. Both texts mentioned the use of vertical stamp mills for ore-crushing. Medieval French sources of the years 1116 and 1249 both record the use of mechanised trip hammers used in the forging of wrought iron. Medieval European trip hammers by the 15th century were most often in the shape of the vertical pestle stamp-mill. The well-known Renaissance artist and inventor Leonardo da Vinci often sketched trip hammers for use in forges and even file-cutting machinery, those of the vertical pestle stamp-mill type. The oldest depicted European illustration of a \"martinet\" forge-hammer is perhaps the Historia de Gentibus Septentrionalibus of Olaus Magnus, dated to 1565 AD. In this woodcut image, there is the scene of three martinets and a waterwheel working wood and leather bellows of an osmund () bloomery furnace. The recumbent trip hammer was first depicted in European artwork in an illustration by Sandrart and Zonca (dated 1621 AD).\n\nWater-powered stamp mills are illustrated in book 8 of Georg Agricola's De Re Metallica, published in 1556. The mills Agricola shows were largely wooden construction, excepting the use of iron shoes on the end of each stamp. The camshaft was set directly on the axle of the waterwheel, and stamps were typically arranged in gangs of three, with each wheel driving one or two gangs.\n\nThe first stamp mill in the U.S. was built in 1829 at the Capps mine near Charlotte, North Carolina. They were common in gold, silver and copper mining regions of the US in the latter 19th and early 20th centuries, in operations where the ore was crushed as a prelude to extracting the metals. They were superseded in the second half of the 19th century in many applications by more efficient methods. However their simplicity meant that they were used in remote areas for ore processing well into the 20th century (19th century advertisements for some mills highlighted that they could be broken down, packed in by mule in pieces, and assembled on site with only simple tools). Stamp mills are still in use in Colombia by artisanal miners, powered by electric motors. \n\nCornish stamps are stamp mills that were developed in Cornwall for use in tin mining in around 1850. Cornish stamps were used to crush small lumps of ore into sand-like material. Constructed from heavy timber or iron lifters with iron \"heads\" at the bottom were raised by cams on a rotating axle, and fell on the ore and water mixture, fed into a box beneath. The heads normally weighed between 4 and 8 cwt (about 200 to 400 kg) each, and were usually arranged in sets of four, in timber frames. Small stamps were commonly powered by water wheels and larger ones by steam engines.\n\nCalifornian stamps were based on Cornish stamps and were used in the Californian gold mines. In these stamps the cam is arranged to lift the stamp from the side, so that it causes the stamp to rotate. This evens the wear on the shoe at the foot of the stamp. They were more rapid in action and a single head could crush 1.5 tons of ore as opposed to the Cornish stamps which could only crush 1 ton.\n\nStamp mills were used in early paper making for preparing the paper-stuff (pulp), before the invention of the Hollander beater, and might have derived from those used in fulling wool. They were used in oil-seed processing prior to pressing the oil from the milled seeds. Early mills were water-powered but mills can also be steam or electric powered.\n\nA stamping mill may refer to a factory that performs stamping.\n\n\n\n"}
{"id": "8726659", "url": "https://en.wikipedia.org/wiki?curid=8726659", "title": "Structural Engineering exam", "text": "Structural Engineering exam\n\nThe Structural Engineering exam is a written examination given by state licensing boards in the United States as part of the testing for licensing structural engineers. This exam is written by the National Council of Examiners for Engineering and Surveying. It is given in eight-hour segments over two days, with the first day covering vertical forces. Problems involving lateral forces are covered on the second day. Each day's morning session features multiple-choice questions, while the afternoon sessions are devoted to essay questions.\n"}
{"id": "10667280", "url": "https://en.wikipedia.org/wiki?curid=10667280", "title": "Treethanol", "text": "Treethanol\n\nTreethanol is an ethanol fuel (more precisely cellulosic ethanol) made from trees.\n\nThe biofuel is a contender in the race to find an energy alternative to fossil fuels. Proponents of Treethanol claim that its energy yield is higher compared to the energy required for production when compared with more common sources of ethanol i.e. sugar cane and corn.\n\nCellulosic ethanol is produced using the lignocellulose biomass that comprises much of the mass of plants. Essentially at the core of the plant material is cellulose, which can be broken down into simple carbohydrate sugars. After these sugars have been extracted, they can be then be fermented into an alcohol, which is known as ethanol. The most widely used and promising means of creating cellulosic ethanol is called the cellulolysis process. The process consists of hydrolysis on pretreated lignocellulosic materials. Then enzymes are used to break down cellulose into glucose. This glucose is then fermented and distilled. The pretreatment step mentioned above is necessary when processing cellulosic ethanol because the glucose (sugars) are not readily accessible as they are with other ethanol sources such as corn or sugar cane. Rather, the cellulose in wood must be separated from the encapsulating hemicellulose and lignin.\n\nThere are three types of pretreatment: physical, chemical, and biological. Physical treatment involves physically reducing wood particle size. This can be accomplished through chipping, grinding, etc. Biological treatments involve the use of microorganisms to break down the wood. This type is considered favorable to physical pretreatments because it consumes far less energy in comparison, but the biological method has not proven to be scalable to an industrial level. The chemical method utilizes an alkaline or otherwise acidic medium to make the cellulose within wood fibers more accessible. This has shown to be the most efficient and has the lowest energy cost.\n\nForest trees make up more than 90% of the total terrestrial biomass while performing functions such as carbon sequestration, producing oxygen, and promoting biodiversity. Trees are a promising source of ethanol because they grow all year round, require significantly less fertilizer and water and contain far more carbohydrates (the chemical precursors of ethanol) than food crops (like corn) do.\n\nPoplar, Willow, and Eucalyptus tree are emerging as favorites in the process of creating Treethanol. This is due to their ability to grow at a fast rate in many parts of the world.\n\nTreethanol is not an energy source that promises to power houses in the future, but rather it can be taken advantage of in applications where combustion engines are used. Approximately 85% of US energy consumption is produced from fossil fuels such as natural gas, coal, and oil. With China, India, and other rapidly developing nations increasing their demand for fossil fuels, the world’s total energy use is expected to grow by 57% over the next 20 years. It is estimated that the U.S. alone uses 140 billion gallons of fuel per year for transportation alone. Not only can Treethanol be mixed with ordinary fuels, it can also be burned directly in modified engines to greatly reduce greenhouse gas emissions.\n\nCellulosic ethanol is an environmentally friendly and renewable transportation fuel that can be produced from forest biomass. Trees are a particularly promising feedstock because they grow all year round, require vastly less fertiliser and water and contain far more carbohydrates (the chemical precursors of ethanol) than food crops do. Also, compared to corn ethanol, cellulosic biofuel does not require the same quantity of fertilizers, pesticides, energy, or water to grow. The most important attribute of this type of ethanol is, like all types of ethanol, it is renewable. If you want or need to make more of it, you just grow more trees.\n\nThe development of all types of biofuel, including Treethanol can be of importance for countries looking to decrease their dependence on petroleum, especially those countries that import most of their petroleum and also have plenty of crop/forest land such as New Zealand and Sweden.\n\nAn important issue is whether Treethanol is a superior alternative to more common forms of ethanol like corn based. The general consensus in an article by Hoover, F., & Abraham, J. (2009), is that most forms of cellulosic ethanol have the potential to yield higher energy outputs and be more sustainable than corn ethanol. They also note that while cellulosic ethanol does not necessarily yield more energy than say, corn based ethanol per unit of measurement, it requires far less energy inputs to produce which could give it a far higher net energy yield at the end of processing. The findings that lignocellulosic biomass has a far greater productivity yield than traditional biofuel sources is supported by Papini, A., & Simeone, M. (2010).\n\nResponsible forestry practices do not contribute to greenhouse gases because the forest is allowed to regenerate following fiber harvesting. For this reason wood can be considered to be an essentially carbon-neutral source of energy.\n\nWhile it seems reasonable that Treethanol could be an alternative to current ethanol types, it has one flaw, which is the extra processing needed to break down the tough cellulose and hemicellulose within the walls of the cell to isolate the sugars. As discussed above in the production section, creating ethanol from the lignocellulose found in tree biomass requires the extra step of “pre-treatment”. It is this pre-treatment that still requires too much energy to make the Treethanol worth the effort.\n\nThat being said, many believe that the potential pros far out-weigh the short-term cons. The process of growing the tree biomass is energy efficient compared with growing corn or sugar cane for ethanol. However, it also takes longer to grow trees than to grow corn, and so any accurate research on sustainability and crop rotation (even for fast growing trees) requires a long time commitment, which up to now has been hard to find. It has been estimated that this process, including the building of processing plants and then refining of the growing and processing stage could take at least a decade.\n\nAnother drawback to the processing of cellulosic ethanol is that there is little known about the waste/by products from the processing. Of particular concern to some is the biological method of pre-treatment. It is estimated that there is the possibility of producing almost as much (if not more) waste than usable ethanol, with waste products including mold, bacteria, yeast, biological toxins and allergens produced by these microorganisms, enzymes, and other chemicals.\n\n\n"}
{"id": "19110714", "url": "https://en.wikipedia.org/wiki?curid=19110714", "title": "Trillium Digital Systems", "text": "Trillium Digital Systems\n\nTrillium Digital Systems developed and licensed standards-based communications source code software to telecommunications equipment manufacturers for the wireless, broadband, Internet and telephone network infrastructure. Trillium was an early company to license source code. The Trillium Digital Systems business entity no longer exists, but the Trillium communications software is still developed and licensed by RadiSys. Trillium software is used in the network infrastructure as well as associated service platforms, clients and devices.\n\nTrillium was founded in February 1988 in Los Angeles, California. The co-founders were Jeff Lawrence and Larisa Chistyakov. Giorgio Propersi joined in September 1989. The initial capitalization of Trillium when it was incorporated was $1,000. The name Trillium came about because of a mistake. Jeff and Larisa asked for company name suggestions from family and friends. Someone suggested a character named Trillian from the book \"Hitchhiker's Guide to the Galaxy\" by Douglas Adams. They thought the suggestion was supposed to be trillium, a flower in the lily family. They liked the sound and symbolism of the name Trillium so they used it.\n\nTrillium was started as a consulting company. Its first consulting jobs were to develop communications software for bisynchronous, asynchronous and multiprotocol PAD products. Consulting continued through the end of 1990. While consulting, the co-founders decided there was an opportunity to develop and license portable source code software for communications protocols. Towards the end of 1990 Trillium became focused on developing its own products.\n\nSource code is a symbolic language (e.g., the C programming language) which is run through a compiler to generate binary code which can run on a particular microprocessor. Communications systems have a variety of hardware and software architectures, use a variety of microprocessors and use a variety of software development environments. It wasn’t technically possible to develop a single piece of binary code that could run on many different systems. Source code, if properly designed and supported, can provide a highly leveragable solution that can be integrated and used in many different systems. The proper way to test source code is to compile it for all possible environments it might run in and then run and test it in those environments. There were as many environments as there were pieces of communications equipment. That testing approach was difficult. To overcome this difficulty Trillium developed an operating system called the Multiprocessor Operating System (MOS) that could run under commercially available operating systems such as DOS, Windows, Solaris and Linux and provide a simulation and testing environment for its software products.\n\nTrillium's first software product supported the X.25 communications protocol. Subsequent products were developed for a number of data communications and voice communications protocols. Trillium's primary focus in its early years was on control plane and signalling plane protocols. In later years Trillium also developed some data plane protocols. A more comprehensive list of the software products developed by Trillium is listed in the Product History section. Trillium is currently developing software products to support the Femtocell communications protocols. Throughout its history Trillium was also very active in standards setting bodies including the CCITT/ITU, IETF, ATM Forum, Frame Relay Forum and others.\n\nTrillium software products were used in communications and networking products designed for the PDN, PSTN, Internet, enterprise networks and home networks.\nTrillium's evolution and development, paralleled the evolution and development of the communications industry. In 1988 there were less than 1/2 million Internet users, about 4 million cell phone users and no broadband (DSL, cable) users. The industry went through significant transitions from the mid-1980s through the early 2000s, as described in the market history section. By 2008 there were over 1.4 billion Internet users, almost 3.3 billion mobile phone users and over 1 billion broadband users.\n\nDuring this period, communications equipment manufacturers licensed source code software to reduce their time to market, decrease their development risk and reduce their costs. By 1999 many companies offered source code software products. Some of these included:\n\nTrillium was funded entirely by its cash flow from its founding through 1999. In late 1998, Trillium decided that to provide liquidity for its shareholders and accelerate its growth it should raise money through an initial public offering. After discussions with investment bankers, it was decided that to improve its initial public offering valuation, it would be necessary to first raise some private equity money to fund organizational expansion, revenue growth and revenue rebalancing. In early 1999, Trillium entered discussions with various venture capital and private equity firms. It closed two private equity deals, one with Rader Reinfrank & Co. in July 1999 for $10 million and the other with Intel Capital and its Intel Communications Fund in September 1999 for $4 million. Rader Reinfrank & Co. was the lead investor and Intel Capital was a co-investor. Trillium used the funds to accelerate the growth of its organization and product line in preparation for a planned initial public offering in either 2000 or 2001. Trillium received ISO 9001 certification in February 2000 and SEI -CMM Level 2 certification in December 2001.\n\nDuring 1999 and 2000, a number of Trillium's competitors either went public, or were acquired.\n\nIn March 2000, following inquiries from potential acquirers, Trillium decided to explore its sale. Trillium created a list of potential acquirers that included communications equipment manufacturers, communications semiconductor companies and a few other companies. Intel Corporation was part of this list since they were already an investor in Trillium. Craig Barrett, the CEO of Intel, felt it was important for Intel to be involved in the communications and networking business. Starting in 1997, and over a 5-year period, Intel spent over $10 billion acquiring communications chip, hardware and software companies. Intel acquired Trillium in a deal for $300 million that closed on August 24, 2000. Intel’s objectives in acquiring Trillium were to expand the networking software available to its network processor, establish a viable entry into the networking software business to complement the network processor business as they moved to sell platform level solutions, and validate and optimize software designs to address high growth communications market segments including voice over IP and wireless.\n\nAs part of the closing, Intel certified there were no material adverse effects; any changes reasonably likely in the future to be materially adverse on the operations, assets, liabilities or earnings of Intel. On August 28, 2000 Intel stopped selling and recalled its Pentium III due to design defects and performance problems, on September 21, 2000 it issued an earnings warning and on September 28 it cancelled its Timna chip and delayed its Pentium 4 and Itanium chips due to design defects and performance problems. In just a little over a month (from August 24, 2000 to September 29, 2000) Intel stock plummeted from over $70 per share to $40 per share.\n\nTrillium became a wholly owned subsidiary of Intel. It was renamed for external purposes as “Trillium, an Intel Company” and for internal purposes as the \"Networking Software Division (NSD)\". In 2002, NSD was renamed as the “Control Plane Processing Division (CPPD)”. The division was initially part of Intel's Network Communications Group, which later became the Intel Communications Group. One of the co-founders moved into Intel when the deal closed, and the other stayed with Trillium. The functional integration of Trillium into Intel was considered successful, but the strategic and value integration was considered less successful. After the deal closed Trillium continued to focus on offering its communications software products and professional services to external customers and also started to develop cross divisional and business group customers within Intel. Trillium focused significant resources on integrating its software products into Intel network processors and related products.\n\nThe co-founders of Trillium left Intel in March and September 2002. Larisa died on December 22, 2008.\n\nAs the dot-com bubble burst in 2000 and 2001, Intel started selling many of its communications businesses. Continuous Computing, based in San Diego, California, acquired Trillium's intellectual property, customers and also hired some Trillium engineering, sales and marketing staff from Intel in February 2003.\n\nContinuous Computing continued to license Trillium software, develop additional software Trillium software products and also bundle the software with its products. Trillium software products celebrated their 20th anniversary in 2008.\n\nRadiSys which provides hardware and software for Internet-based telecommunications, announced on May 3, 2011 that it was going to acquire Continuous Computing and its Trillium software products in a deal valued at $120 million. RadiSys and Continuous Computing are focused on complementary areas of networking technology.\n\nThere were a number of regulatory, financial and technology events that drove the telecommunications industry, and subsequently drove Trillium's growth and development.\n\n\nAll Trillium products are based on the Trillium Advanced Portability Architecture (TAPA), a set of architectural and coding standards designed to ensure that the individual source code software products are portable and independent of the target system's compiler, processor, operating system and architecture. Each software product is provided as C source code and has four programming interfaces: the system services interface, the layer management interface, the upper interface and the lower interface. TAPA describes the parameters and expected behavior across each interface.\n\nIndividual Trillium software products consist of 10,000’s to 100,000’s lines of source code. Each Trillium software product could be used separately, or in conjunction with other software products to build complete protocol stacks. Trillium software products were also able to support different national and industry variants of specific protocols. In later years, as the different network infrastructure technologies converged, Trillium software products were able to support interworking and translation between the different network infrastructure technologies (e.g. telephony to Internet Protocols). They were used in a wide range of network equipment, products and devices.\n\nDuring its history, Trillium has developed over 150 software products, which parallel the evolution and development of the network infrastructure. These software products support communications protocols specified in international (e.g. ITU), national (e.g. ANSI) and industry (e.g. IETF) standards. These products are licensed primarily to telecommunications equipment manufacturers and include:\n\nTrillium software has been used in over 500 communications and networking products.\n\nTrillium conceived of and published a poster that provided detailed technical information about the network infrastructure and protocols in an attractive format that was easy to understand. It became an indispensable tool for the communications industry, and was displayed on the office walls and conference rooms of tens of thousands of engineers, venture capitalists and financial analysts around the world. The poster is periodically updated to reflect changes in the network infrastructure and protocols. The 1st generation poster was published in 1997 and was inspired by the \"ISO and CCITT Data Communication Standards\" poster published by Retix. The 5th generation of the poster was published in 2008.\n\n"}
{"id": "28174565", "url": "https://en.wikipedia.org/wiki?curid=28174565", "title": "UK Academy for Information Systems", "text": "UK Academy for Information Systems\n\nThe UK Academy for Information Systems (UKAIS) is an active combination of a traditional learned society, communications channel and pressure group. It is a conduit for communication between industry and academia to ensure that relevant courses can be designed and research initiatives established throughout the UK.\n\nBy continually improving the quality and relevance of teaching through innovative and rigorous research the society contributes to both academic development and excellence in IS practice throughout the UK. Part of their role has been to establish links between commercial, government and academic organisations. Teaching, research and practice in the field are supported by UKAIS through its annual conference, PhD consortia, workshops, regional groups and quarterly newsletter.\n\nThe society arose from a meeting in 1993 of leading UK academics in information systems. Concerns were expressed at the meeting about the way IS teaching and research were funded which stemmed from a lack of recognition of IS as a growing and important academic discipline. The UKAIS was established in 1994 to remedy this situation as a charity, whose aims are to provide a better knowledge of IS within the UK and to provide a forum for discussing issues in IS teaching and research. It also aims to be influential in obtaining better understanding of the uniqueness of the subject by HEFCE, the UK Research Councils, professional bodies, UK business and Government. They have successfully achieved recognition that IS is not being dealt with sufficiently by such bodies and have suggested ways of improving the situation.\n\nThe UKAIS has attempted to create a 'uniform' definition of Information Systems.\n\nBeyond this the work of the society is within the domain involving the study of theories and practices related to the social and technological phenomena, which determine the development, use and effects of information systems in organisations and society.\n\n"}
{"id": "34599617", "url": "https://en.wikipedia.org/wiki?curid=34599617", "title": "Walter Percy Day", "text": "Walter Percy Day\n\nWalter Percy Day O.B.E. (1878–1965) was a British painter best remembered for his work as a matte artist and special effects technician in the film industry.\nProfessional names include W. Percy Day; Percy Day; “Pop” or “Poppa” Day, owing to his collaboration with sons Arthur George Day (1909–1952) draughtsman, Thomas Sydney Day (1912–1985), stills photographer and cameraman, and stepson, Peter Ellenshaw, who also worked in this field.\n\nWalter Percy Day was born in Luton (Bedfordshire) to Eli Day and Lucy Day, née Crawley, the second of three children. From 1908 to 1912, he resided in Tunisia, at Sidi Bou Saïd and Tunis, where he pursued a career as a painter of portraits and Orientalist scenes. The dramatic consequences of the “affaire du Jellaz” uprising obliged the family to return to Britain early in 1912.\n\nIn 1919, at Ideal Films Studios in Borehamwood, near Elstree Day mastered the art of illusionist techniques. Special effects such as those produced by Day enabled directors to enlarge their repertoire and to tackle subjects which might otherwise have been too costly to produce. In 1922, he relocated to France to its more vibrant cinema. There he introduced the use of the glass shot into French cinema. Used for the first time in Henry Roussel's \"Les Opprimés\", released in 1923, the process was hailed by a critic as a revolution in cinematography. Among the directors with whom Day collaborated during the twenties were Jean Renoir, Raymond Bernard, Julien Duvivier, and Abel Gance. In addition to creating visual effects for \"Napoléon\" (1927), Day also played the role of the British Admiral Hood in the film. From 1928, Day's studio became a team, when sons Arthur George Day (1909–1912) and Thomas Sydney Day (1912–1985) began to work for their father, the former as draughtsman and the latter as cameraman and stills photographer, starting with Léon Poirier's \"\" (1928).\n\nIn the late 1920s, he learned a new technique while working at the Elstree studio on Alfred Hitchcock's \"The Ring\", that used mirrors and angling to superimpose a miniature over a scene. The inventor, Eugen Schüfftan, whose office was opposite the studios, taught him the process directly.\n\nWhen shooting the façade of the department store in Julien Duvivier's film \"Au Bonheur des Dames\" (1929) proved to be an insurmountable difficulty, Day utilised the stationary matte, a process similar to that patented by Norman Dawn on 11 June 1918.\n\nA meeting with Alexander Korda opened up new perspectives for the Day studio. Day worked with Korda on \"The Private Life of Henry VIII\" (1933) a film starring Charles Laughton. Day accordingly established a studio in Iver (Buckinghamshire) and from 1936, directed the matte department at Denham Studios. The artist painted mattes and created trick shots for numerous films by Korda and his stable of directors, who included his brother Zoltan Korda, Anthony Asquith, William Cameron Menzies (\"Things to Come\", 1936), Michael Powell, Lothar Mendes and David Lean. In 1946 Day joined the Korda group as Director of Special Effects at Korda's new premises at Shepperton Studios where he remained until his retirement in 1954.\n\nPoppa Day's team disbanded once World War II began as all three sons enlisted. Pop Day trained some promising young matte painters, including Wally Veevers, who took over the matte department at Shepperton Studios when Pop Day retired in 1952.\n\nDuring the war, the film studios made a series of heroic war films, aimed at boosting the morale of the beleaguered British, including Powell and Pressburger's \"49th Parallel\" (US: \"The Invaders\", 1941), Noël Coward and David Lean's \"In Which We Serve\" (1942) and Leslie Howard's \"The First of the Few\" (US: \"Spitfire\", also 1942]. In addition to designing special effects for these films, Day created trick photography for many other British classics released during the forties, including \"The Life and Death of Colonel Blimp\" (1943), \"A Matter of Life and Death\" [US: Stairway to Heaven, 1946], \"Anna Karenina\" (1948), and \"The Third Man\" (1949). In Laurence Olivier's production of \"Henry V\" (1944), many of the Agincourt battle scenes were painted on glass by Day, who contrived to make the horses' heads move, the pennants flutter and whirring motion of a flight of arrows in the completed shots. The Powell and Pressburger production of \"I Know Where I'm Going!\" (1945) contains a sequence in which the hero and heroine's boat gets sucked into the Corryvreckan whirlpool. \"Black Narcissus\" (1947) was shot entirely on the Pinewood Studios back lot with matte of the Himalayan mountain range painted by Day and his assistants.\n\nIn 1948, Day was awarded the O.B.E. for his services to British cinema. Cameraman Christopher Challis, has rendered homage to Percy Day's achievements: \"Being able to marry painted backgrounds on glass to real action foregrounds opened up a new world to film makers… To appreciate the magnitude of his achievement, one has to understand the complexity of the work. Hours of painstaking labour with many retakes to obtain perfection. Now it is all too easy with computers and electronics and few people remain who can understand just how complicated it was. [Day’s] name should certainly be numbered among the great film pioneers, alongside Gaumont, Lumière, etc\". Michael Powell, for his part, hailed Percy Day as \"the greatest trick-man and film wizard that I have ever known…\" Percy Day’s legacy was ranked by the British daily \"The Independent\" in 2008 as on a par with the great French special effects pioneer Georges Méliès.\n\nDay died in Los Angeles.\n\n\"Date of shooting followed by date of release\"\n\n\n\n"}
