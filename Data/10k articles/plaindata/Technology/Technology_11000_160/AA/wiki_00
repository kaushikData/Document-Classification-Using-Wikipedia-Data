{"id": "19425312", "url": "https://en.wikipedia.org/wiki?curid=19425312", "title": "Asymmetric hydrogenation", "text": "Asymmetric hydrogenation\n\nAsymmetric hydrogenation is a chemical reaction that adds two atoms of hydrogen preferentially to one of two faces of an unsaturated substrate molecule, such as an alkene or ketone. The selectivity derives from the manner that the substrate binds to the chiral catalysts. In jargon, this binding transmits spatial information (what chemists refer to as chirality) from the catalyst to the target, favoring the product as a single enantiomer. This enzyme-like selectivity is particularly applied to bioactive products such as pharmaceutical agents and agrochemicals.\n\nIn 1956 a heterogeneous catalyst made of palladium deposited on silk was shown to effect asymmetric hydrogenation. Later, in 1968, the groups of William Knowles and Leopold Horner independently published the examples of asymmetric hydrogenation using a homogeneous catalysts. While exhibiting only modest enantiomeric excesses, these early reactions demonstrated feasibility. By 1972, enantiomeric excess of 90% was achieved, and the first industrial synthesis of the Parkinson's drug L-DOPA commenced using this technology.\n\nThe field of asymmetric hydrogenation continued to experience a number of notable advances. Henri Kagan developed DIOP, an easily prepared C-symmetric diphosphine that gave high ee's in certain reactions. Ryōji Noyori introduced the ruthenium-based catalysts for the asymmetric hydrogenated polar substrates, such as ketones and aldehydes. The introduction of P,N ligands then further expanded the scope of the C-symmetric ligands, although they are not fundamentally superior to chiral ligands lacking rotational symmetry. Today, asymmetric hydrogenation is a routine methodology in laboratory and industrial scale organic chemistry.\n\nThe importance of asymmetric hydrogenation was recognized by the 2001 Nobel Prize in Chemistry awarded to William Standish Knowles and Ryōji Noyori.\n\nTwo major mechanisms have been proposed for catalytic hydrogenation with rhodium complexes: the unsaturated mechanism and the dihydride mechanism. While distinguishing between the two mechanisms is difficult, the difference between the two for asymmetric hydrogenation is relatively unimportant since both converge to a common intermediate before any stereochemical information is transferred to the product molecule.\n\nThe preference for producing one enantiomer instead of another in these reactions is often explained in terms of steric interactions between the ligand and the prochiral substrate. Consideration of these interactions has led to the development of quadrant diagrams where \"blocked\" areas are denoted with a shaded box, while \"open\" areas are left unfilled. In the modeled reaction, large groups on an incoming olefin will tend to orient to fill the open areas of the diagram, while smaller groups will be directed to the blocked areas and hydrogen delivery will then occur to the back face of the olefin, fixing the stereochemistry. Note that only part of the chiral phosphine ligand is shown for the sake of clarity.\n\nRhodium, the first metal to be used in a homogeneous asymmetric hydrogenation, continues to be widely used. Targets for asymmetric hydrogenation with rhodium generally require a coordinating group close to the olefin. While this requirement is a limitation, many classes of substrates possess such functionalization, e.g. unsaturated amides.\n\nThe Noyori asymmetric hydrogenation is based on ruthenium. Subsequent work has expanded upon Noyori's original catalyst template, leading to the inclusion of traditionally difficult substrates like \"t\"-butyl ketones and 1-tetralones as viable substrates for hydrogenation with ruthenium catalysts. Transfer hydrogenation based on the Ru and TsDPEN has also enjoyed commercial success.\n\nIridium catalysts are useful for a number of \"non-traditional\" substrates for which good catalysts had not been found with Ru and Rh. Unfunctionalized olefins are the archetypal case, but other examples including ketones exist. A common difficulty with iridium-based catalyst is their tendency to trimerize in solution. The use of a BAr anions has proven to be the most widely applicable solution to the aggregation problem. Other strategies to enhance catalyst stability include the addition of an additional coordinating arm to the chiral ligand, increasing the steric bulk of the ligand, using a dendrimeric ligand, increasing the rigidity of the ligand, immobilizing the ligand, and using heterobimetallic systems (with iridium as one of the metals).\n\nIron is a popular research target for many catalytic processes, owing largely to its low cost and low toxicity relative to other transition metals. Asymmetric hydrogenation methods using iron have been realized, although in terms of rates and selectivity, they are inferior to catalysts based on precious metals. In some cases, structurally ill-defined nanoparticles have proven to be the active species \"in situ\" and the modest selectivity observed may result from their uncontrolled geometries.\n\nChiral phosphine ligands, especially C-symmetric ligands, are the source of chirality in most asymmetric hydrogenation catalysts. Of these the BINAP ligand is well-known, as a result of its Nobel Prize-winning application in the Noyori asymmetric hydrogenation.\n\nChiral phosphine ligands can be generally classified as mono- or bidentate. They can be further classified according to the location of the stereogenic centre – phosphorus vs the organic substituents. Ligands with a C symmetry element have been particularly popular, in part because the presence of such an element reduces the possible binding conformations of a substrate to a metal-ligand complex dramatically (often resulting in exceptional enantioselectivity).\n\nMonophosphine-type ligands were among the first to appear in asymmetric hydrogenation, e.g., the ligand CAMP. Continued research into these types of ligands has explored both P-alkyl and P-heteroatom bonded ligands, with P-heteroatom ligands like the phosphites and phosphoramidites generally achieving more impressive results. Structural classes of ligands that have been successful include those based on the binapthyl structure of MonoPHOS or the spiro ring system of SiPHOS. Notably, these monodentate ligands can be used in combination with each other to achieve a synergistic improvement in enantioselectivity; something that is not possible with the diphosphine ligands.\n\nThe diphosphine ligands have received considerably more attention than the monophosphines and, perhaps as a consequence, have a much longer list of achievement. This class includes the first ligand to achieve high selectivity (DIOP), the first ligand to be used in industrial asymmetric synthesis (DIPAMP) and what is likely the best known chiral ligand (BINAP). Chiral diphosphine ligands are now ubiquitous in asymmetric hydrogenation.\n\nThe use of P,N ligands in asymmetric hydrogenation can be traced to the C symmetric bisoxazoline ligand. However, these symmetric ligands were soon superseded by mono(oxazoline) ligands whose lack of C symmetry has in no way limits their efficacy in asymmetric catalysis. Such ligands generally consist of an achiral nitrogen-containing heterocycle that is functionalized with a pendant phosphorus-containing arm, although both the exact nature of the heterocycle and the chemical environment phosphorus center has varied widely. No single structure has emerged as consistently effective with a broad range of substrates, although certain privileged structures (like the phosphine-oxazoline or PHOX architecture) have been established. Moreover, within a narrowly defined substrate class the performance of metallic complexes with chiral P,N ligands can closely approach perfect conversion and selectivity in systems otherwise very difficult to target. Certain complexes derived from chelating P-O ligands have shown promising results in the hydrogenation of α,β-unsaturated ketones and esters.\n\nSimple N-heterocyclic carbene (NHC)-based ligands have proven impractical for asymmetrical hydrogenation.\n\nSome C,N ligands combine an NHC with a chiral oxazoline to give a chelating ligand. NHC-based ligands of the first type have been generated as large libraries from the reaction of smaller libraries of individual NHCs and oxazolines. NHC-based catalysts featuring a bulky seven-membered metallocycle on iridium have been applied to the catalytic hydrogenation of unfunctionalized olefins and vinyl ether alcohols with conversions and ee's in the high 80s or 90s. The same system has been applied to the synthesis of a number of aldol, vicinal dimethyl and deoxypolyketide motifs, and to the deoxypolyketides themselves.\n\nC-symmetric NHCs have shown themselves to be highly useful ligands for the asymmetric hydrogenation.\n\nAcyclic unsaturated substrates (olefins, ketones, enamines imines) represents the most common prochiral substrates. Substrates that are particularly amenable to asymmetric hydrogenation often feature a polar functional group adjacent to the site to be hydrogenates. In the absence of this functional) group, catalysis often results in low ee's. For unfunctionalized olefins, iridium with P,N-based ligands) have proven successful catalysts. Catalyst utility within this category is unusually narrow; consequently, many different categories of solved and unsolved catalytic problems have developed. 1,1-disubstituted, 1,2-diaryl trisubstituted, 1,1,2-trialkyl and tetrasubstituted olefins represent classes that have been investigated separately, and even within these classes variations may exist that make different solutions optimal.\n\nConversely to the case of olefins, asymmetric hydrogenation of enamines has favoured diphosphine-type ligands; excellent results have been achieved with both iridium- and rhodium-based systems. However, even the best systems often suffer from low ee's and a lack of generality. Certain pyrrolidine-derived enamines of aromatic ketones are amenable to asymmetrically hydrogenation with cationic rhodium(I) phosphonite systems, and I and acetic acid system with ee values usually above 90% and potentially as high as 99.9%. A similar system using iridium(I) and a very closely related phosphoramidite ligand is effective for the asymmetric hydrogenation of pyrrolidine-type enamines where the double bond was inside the ring: in other words, of dihydropyrroles. In both cases, the enantioselectivity dropped substantially when the ring size was increased from five to six.\n\nKetones and imines are related functional groups, and effective technologies for the asymmetric hydrogenation of each are also closely related. Of these, Noyori's ruthenium-chiral diphosphine-diamine system is perhaps one of the best known. It can be employed in conjunction with a wide range of phosphines and amines (where the amine may be, but need not be, chiral) and can be easily adjusted for an optimal match with the target substrate, generally achieving enantiomeric excesses (ee's) above 90%.\n\nFor carbonyl and imine substrates, end-on, η coordination can compete with η mode. For η-bound substrates, the hydrogen-accepting carbon is removed from the catalyst and resists hydrogenation.\n\nIridium/P,N ligand-based systems are also commonly used for the asymmetric hydrogenation of ketones and imines. For example, a consistent system for benzylic aryl imines uses the P,N ligand SIPHOX in conjunction with iridium(I) in a cationic complex to achieve asymmetric hydrogenation with ee >90%. One of the most efficient and effective catalysts ever developed for the asymmetric hydrogenation of ketones, with a turnover number (TON) up to 4,550,000 and ee up to 99.9%, uses another iridium(I) system with a closely related tridentate ligand.\n\nDespite their similarities, the two functional groups are not identical; there are many areas where they diverge significantly. One of these is in the asymmetric hydrogenation of N-unfunctionalized imines to give primary amines. Such species can be difficult to selectively reduce because they tend to exist in complex equilibria of imine and enamine tautomers, as well as (E) and (Z) isomers. One approach to this problem has been to use ketimines as their hydrochloride salt and rely on the steric properties of the adjacent alkyl or aryl groups to allow the catalyst to differentiate between the two enantiotopic faces of the ketimine.\n\nThe asymmetric hydrogenation of aromatic (especially heteroaromatic), substrates is a very active field of ongoing research. Catalysts in this field must contend with a number of complicating factors, including the tendency of highly stable aromatic compounds to resist hydrogenation, the potential coordinating (and therefore catalyst-poisoning) abilities of both substrate and product, and the great diversity in substitution patterns that may be present on any one aromatic ring. Of these substrates the most consistent success has been seen with nitrogen-containing heterocycles, where the aromatic ring is often activated either by protonation or by further functionalization of the nitrogen (generally with an electron-withdrawing protecting group). Such strategies are less applicable to oxygen- and sulfur-containing heterocycles, since they are both less basic and less nucleophilic; this additional difficulty may help to explain why few effective methods exist for their asymmetric hydrogenation.\n\nTwo systems exist for the asymmetric hydrogenation of 2-substituted quinolines with isolated yields generally greater than 80% and ee values generally greater than 90%. The first is an iridium(I)/chiral phosphine/I system, first reported by Zhou \"et al\". While the first chiral phosphine used in this system was MeOBiPhep, newer iterations have focused on improving the performance of this ligand. To this end, systems use phosphines (or related ligands) with improved air stability, recyclability, ease of preparation, lower catalyst loading and the potential role of achiral phosphine additives. As of October 2012 no mechanism appears to have been proposed, although both the necessity of I or a halogen surrogate and the possible role of the heteroaromatic N in assisting reactivity have been documented.\n\nThe second is an organocatalytic transfer hydrogenation system based on Hantzsch esters and a chiral Brønsted acid. In this case, the authors envision a mechanism where the isoquinoline is alternately protonated in an activating step, then reduced by conjugate addition of hydride from the Hantzsch ester.\n\nMuch of the asymmetric hydrogenation chemistry of quinoxalines is closely related to that of the structurally similar quinolines. Effective (and efficient) results can be obtained with an Ir(I)/phophinite/I system and a Hantzsh ester-based organocatalytic system, both of which are similar to the systems discussed earlier with regards to quinolines.\n\nPyridines are highly variable substrates for asymmetric reduction (even compared to other heteroaromatics), in that five carbon centers are available for differential substitution on the initial ring. As of October 2012 no method seems to exist that can control all five, although at least one reasonably general method exists.\n\nThe most-general method of asymmetric pyridine hydrogenation is actually a heterogeneous method, where asymmetry is generated from a chiral oxazolidinone bound to the C2 position of the pyridine. Hydrogenating such functionalized pyridines over a number of different heterogeneous metal catalysts gave the corresponding piperidine with the substituents at C3, C4, and C5 positions in an all-\"cis\" geometry, in high yield and excellent enantioselectivity. The oxazolidinone auxiliary is also conveniently cleaved under the hydrogenation conditions.\n\nMethods designed specifically for 2-substituted pyridine hydrogenation can involve asymmetric systems developed for related substrates like 2-substituted quinolines and quinoxalines. For example, an iridium(I)\\chiral phosphine\\I system is effective in the asymmetric hydrogenation of activated (alkylated) 2-pyridiniums or certain cyclohexanone-fused pyridines. Similarly, chiral Brønsted acid catalysis with a Hantzsh ester as a hydride source is effective for some 2-alkyl pyridines with additional activating substitution.\n\nThe asymmetric hydrogenation of indoles initially focused on N-protected indoles, where the protecting group could serve both to activate the heterocycle to hydrogenation and as a secondary coordination site for the metal. Later work allowed unprotected indoles to be targeted through Brønsted acid activation of the indole.\n\nIn the initial report on asymmetric indole hydrogenation, N-acetyl 2-substituted indoles could be protected with high yields and ee of 87-95%. 3-substituted indoles were less successful, with hydrolysis of the protecting group outcompeting the hydrogenation of the indole. Switching to an N-tosyl protecting group inhibited the hydrolysis reaction and allowed both 2- and 3-substituted indoles to be hydrogenated in high yield and ee. The problem with both methods, however, is that N-acetyl and N-tosyl groups require harsh cleavage conditions that might be incompatible with complex substrates. Using an easily cleaved N-Boc group would relieve this problem, and highly effective methods for the asymmetric hydrogenation of such indoles (both 2- and 3-substituted) were soon developed.\n\nDespite these advances in the asymmetric hydrogenation of protected indoles, considerable operational simplicity can be gained by removing the protecting group altogether. This has been achieved with catalytic systems utilizing Brønsted acids to activate the indole. The initial system used a Pd(TFA)/H8-BINAP system to achieve the enantioselective \"cis\"-hydrogenation of 2,3- and 2-substituted indoles with high yield and excellent ee. A similar process, where sequential Friedel-Crafts alkylation and asymmetric hydrogenation occur in one pot, allow asymmetric 2,3-substituted indolines to be selectively prepared from 2-substituted indoles in similarly high yields and ee.\n\nA promising organocatalytic method for the asymmetric hydrogenation of 2,3-substituted indoles utilizing a chiral Lewis base also exists, although the observed ee's are not quite equivalent to those of the metal-based hydrogenations.\n\nAchieving complete conversion of pyrroles to pyrrolidines by asymmetric hydrogenation has so far proven difficult, with partial-hydrogenation products often being observed. Complete enantioselective reduction is possible, with the outcome depending on both the starting substrate and the method.\n\nThe asymmetric hydrogenation of 2,3,5-substituted pyrroles was achieved by the recognition that such substrates bear the same substitution pattern as 2-substituted indoles, and an asymmetric hydrogenation system that is effective for one of these substrates might be effective for both. Such an analysis led to the development of a ruthenium(I)/phosphine/amine base system for 2,3,5-substituted N-Boc pyrroles that can give either dihydro or tetrahydropyrroles (pyrrolidines), depending on the nature of the pyrrole substituents. An all-phenyl substitution pattern leads to dihydropyrroles in very high yield (>96%) and essentially perfect enantioselectivity. Access to the fully hydrogenated, all-\"cis\" dihydropyrrole may then be accessible through diastereoselective heterogeneous hydrogenation. Alkyl substitution may lead to either the dihydro or tetrahydropyrrole, although the yields (>70%) and enantioselectivities (often >90%) generally remain high. The regioselectivity in both cases appears to be governed by sterics, with the less-substituted double being preferentially hydrogenated.\n\nUnprotected 2,5-pyrroles may also be hydrogenated asymmetrically by a Brønsted acid/Pd(II)/chiral phosphine-catalyzed method, to give the corresponding 2,5-disubstituted 1-pyrrolines in roughly 70-80% yield and 80-90% ee.\n\nThe asymmetric hydrogenation of furans and benzofurans has so far proven challenging. Some Ru-NHC complex catalyze asymmetric hydrogenations of benzofurans and furans. with high levels of enantioinduction.\n\nAs is the case with oxygen-containing heterocycles, the asymmetric hydrogenation of compounds where sulfur is part of the initial unsaturated pi-bonding system so far appears to be limited to thiophenes and benzothiophenes. The key approach to the asymmetric hydrogenation of these heterocycles involves a ruthenium(II) catalyst and chiral, C symmetric N-heterocyclic carbene (NHC). This system appears to possess superb selectivity (ee > 90%) and perfect diastereoselectivity (all \"cis\") if the substrate has a fused (or directly bound) phenyl ring but yields only racemic product in all other tested cases.\n\nResearch into asymmetric hydrogenation with heterogeneous catalysts has generally focused on three areas. The oldest, dating back to the first asymmetric hydrogenation with palladium deposited on a silk support, involves modifying a metal surface with a chiral molecule, usually one that can be harvested from nature. Alternatively, researchers have used various techniques to attempt to immobilize what would otherwise be homogeneous catalysts on heterogeneous supports or have used synthetic organic ligands and metal sources to build chiral metal-organic frameworks (MOFs).\nThe greatest successes in chiral modification of metal surfaces have come from the use of cinchona alkaloids, though numerous other classes of natural products have been evaluated. These alkaloids have been shown to enhance the rate of substrate hydrogenation by 10–100 times, such that less than one molecule of cinchona alkaloid is needed for every reactive site on the metal and, in fact, the presence of too much of the chiral modifier can cause a decrease in the enantioselectivity of the reaction.\n\nAn alternative technique and one that allows more control over the structural and electronic properties of active catalytic sites is the immobilization of catalysts that have been developed for homogeneous catalyis on a heterogeneous support. Covalent bonding of the catalyst to a polymer or other solid support is perhaps most common, though immobilization of the catalyst may also be achieved by adsorption onto a surface, ion exchange, or even physical encapsulation. One drawback of this approach is the potential for the proximity of the support to change the behaviour of the catalyst, lowering the enantioselectivity of the reaction. To avoid this, the catalyst is often bound to the support by a long linker though cases are known where the proximity of the support can actually enhance the performance of the catalyst.\n\nThe final approach involves the construction of MOFs that incorporate chiral reaction sites from a number of different components, potentially including chiral and achiral organic ligands, structural metal ions, catalytically active metal ions, and/or preassembled catalytically active organometallic cores. This field is relatively new, and few examples exist of chiral asymmetric hydrogenation using these frameworks. One of these was reported in 2003, when a heterogeneous catalyst was reported that included structural zirconium, catalytically active ruthenium, and a BINAP-derived phosphonate as both chiral ligand and structural linker. As little as 0.005 mol% of this catalyst proved sufficient to achieve the asymmetric hydrogenation of aryl ketones, though the usual conditions featured 0.1 mol % of catalyst and resulted in an enantiomeric excess of 90.6–99.2%.\n\nKnowles' research into asymmetric hydrogenation and its application to the production scale synthesis of L-Dopa gave asymmetric hydrogenation a strong start in the industrial world. More recently, a 2001 review indicated that asymmetric hydrogenation accounted for 50% of production scale, 90% of pilot scale, and 74% of bench scale catalytic, enantioselective processes in industry, with the caveat that asymmetric catalytic methods in general were not yet widely used.\n\nThe success of asymmetric hydrogenation in industry can be seen in a number of specific cases where the replacement of kinetic resolution based methods has resulted in substantial improvements in the process's efficiency. For example, Roche's Catalysis Group was able to achieve the synthesis of (S,S)-Ro 67-8867 in 53% overall yield, a dramatic increase above the 3.5% that was achieved in the resolution based synthesis. Roche's synthesis of mibefradil was likewise improved by replacing resolution with asymmetric hydrogenation, reducing the step count by three and increasing the yield of a key intermediate to 80% from the original 70%.\n"}
{"id": "26011932", "url": "https://en.wikipedia.org/wiki?curid=26011932", "title": "Barsanti-Matteucci engine", "text": "Barsanti-Matteucci engine\n\nThe Barsanti-Matteucci engine was the first internal combustion engine. In late 1851 or early 1852 Eugenio Barsanti, a professor of mathematics, and Felice Matteucci, an engineer and expert in mechanics and hydraulics, joined forces on a project to exploit the explosion and expansion of a gaseous mix of hydrogen and atmospheric air to transform part of the energy of such explosions into mechanical energy.\n\nThe idea originated almost ten years earlier with Barsanti when, as a young man, he was teaching at St. Michael's College in Volterra, Italy. An engineer from Milan Italy, Luigi de Cristoforis, described in a paper published in the acts of the Lombard Royal Institute of Science, Literature and Art, a pneumatic machine (later built and shown to work) that ran on naphtha and an air mixture, and which constituted the first liquid fuel engine.\n\nDuring the twelve years of collaboration between Barsanti and Matteucci several prototypes of internal combustion engines were realized. It was the first real internal combustion engine, constituted in its simplest realization by a vertical cylinder in which an explosion of a mixture of air and hydrogen or an illuminating gas shot a piston upwards thereby creating a vacuum in the space underneath. When the piston returned to its original position, due to the action of the atmospheric pressure, it turned a toothed rod connected to a sprocket wheel and transmitted movement to the driving shaft.\nNumerous patents were obtained by the two inventors: the 1854 English and Piedmont patents, the 1861 Piedmont patent of Barsanti, Matteucci and Babacci which was then used as a base to construct the engine of the Escher Wyss company of Zurich and put on exhibit during the first National Expo of Florence in 1861, and the 1861 English patent.\n\n"}
{"id": "16489419", "url": "https://en.wikipedia.org/wiki?curid=16489419", "title": "Bi-fuel vehicle", "text": "Bi-fuel vehicle\n\nBi-fuel vehicles are vehicles with multifuel engines capable of running on two fuels. On internal combustion engines one fuel is gasoline or diesel, and the other is an alternate fuel such as natural gas (CNG), LPG, or hydrogen. The two fuels are stored in separate tanks and the engine runs on one fuel at a time in some cases, in others both fuels are used in unison. Bi-fuel vehicles have the capability to switch back and forth from gasoline or diesel to the other fuel, manually or automatically.\n\nThe most common technology and alternate fuel available in the market for bi-fuel gasoline cars is Autogas (LPG), followed by natural gas (CNG), and it is used mainly in Europe. The Netherlands and the Baltic states have a large number of cars running with LPG. Italy currently has the largest number of CNG vehicles, followed by Sweden. They are also used in South America, where these vehicles are mainly used as taxicabs in main cities of Brazil and Argentina. Normally, standard gasoline vehicles are retrofitted in specialized shops, which involve installing the gas cylinder in the trunk and the LPG or CNG injection system and electronics.\n\nDiesel engine is compression ignition engine and does not have spark plug. To operate a diesel engine with an alternate combustible fuel source such as Natural gas, Dual-Fuel system used. Natural gas as the main fuel while diesel fuel is used for the ignition of the gas/air mixture inside the cylinder (a portion of diesel is injected at the end of the compression stroke, thereby maintaining the original diesel operation principle)\n\nDual fuel operation means the engine uses two fuels (gas and diesel) at the same time, as opposed to Bi Fuel which would mean the engine could have the option of using either fuel separately.\n\nThere usually two type of conversions - low speed (below 1000 RPM) and high speed (between 1200 and 1800 RPM).\n\nGas is injected into the cylinder inlet manifold by individual gas electromagnetic valves installed as close to the suction valves as possible. The valves are separately timed and controlled by injection control unit. This system interrupts the gas supply to the cylinder during the long overlap of the suction and exhaust valves (just typical for slow-speed and medium-speed engines – within the valve overlap cylinder scavenging is performed). This avoids substantial gas losses and prevents dangerous gas flow to the exhaust manifold.\n\n\nGas is mixed with air by a common mixer installed before turbocharger(s). Gas flow is controlled by a throttle valve, which is electronically operated by the special control system according to the required engine output and speed. In order to avoid knocking of the engine, knocking detector/controller is installed, thus enabling engine operation at the most efficient gas/diesel ratio.\n\n\nIt is common to use CNG (Compressed Natural Gas) or LNG (Liquid Natural Gas) for bi-fuel operations. Both are also mostly used for Generator sets conversions, because the engine does not lose the output power.\n\nIn recent years biogas is being used. The biogas composition and calorific value must be known in order to evaluate if the particular biogas type is suitable. Calorific value may be an issue as biogas is derived from different sources and there is low calorific value in many cases. You can imagine you have to inject sufficient volume of gas into the cylinder to substitute diesel oil (or, better to say, substitute energy delivered by diesel oil). If the calorific value (energy) of the biogas was very low, there is a need to inject really big volume of biogas into the cylinder, which might be technically impossible. Additionally, the composition of the biogas has to lean towards ignitable gases and be filtered as much as possible of non-combustible compounds such as carbon dioxide.\n\nAssociated gas is the last type of gas which is commonly used for bi-fuel conversions of generator sets. Associated gas is a natural gas found in association with oil, either dissolved in the oil or as a cap of free gas above the oil. It means it has almost the same quality as CNG or LNG.\nIt depends on the technical state of the engine, especially of the injection system. The typical Diesel / Gas ratio is 40/60% for the high-speed engines. If the operating output of the engine is constant and between 70-80% of nominal output, than it is possible to reach up to 30/70% ratio. If the operating output is lower (for example 50% of the nominal output) or if there are variations, the rate is about 45/55% (more of diesel is used).For Low Speed conversions it is possible to reach the Diesel/gas ratio up to 10/90%. Generally, it is not possible to guarantee an exact diesel/gas ratio without a test being done after the conversion.\n\nAftermarket 'bi-fuel' and even 'tri-fuel' conversions are also available.\n\n\n\n\n"}
{"id": "2929434", "url": "https://en.wikipedia.org/wiki?curid=2929434", "title": "Carlos Glidden", "text": "Carlos Glidden\n\nCarlos Glidden (November 8, 1834 – March 11, 1877), along with Christopher Sholes, Frank Haven Hall, and Samuel W. Soule, invented the first practical typewriter at a machine shop in Milwaukee, Wisconsin. He kept on improving the typewriter until he died.\n"}
{"id": "5076875", "url": "https://en.wikipedia.org/wiki?curid=5076875", "title": "Chip PC Technologies", "text": "Chip PC Technologies\n\nChip PC Technologies is a developer and manufacturer of thin client solutions and management software for server-based computing; where in a network architecture applications are deployed, managed and can be fully executed on the server.\n\nChip PC was founded in 2000 by Aviv Soffer and Ora Meir Soffer and raised its first round of financing from R.H. Technologies Ltd. (), an electronics contract manufacturing group.\n\nIn 2005 Elbit Systems acquired 20% of the company.\n\nThe company grew rapidly and established partnerships with Dell, which distributes its products, and Microsoft. In June 2007, it raised NIS 26 million in stocks, bonds, and warrants in an IPO on the Tel Aviv Stock Exchange.\n\nIn November 2007, the company won Europe's largest Thin client tender thus far, to supply 20,000 Thin client PC's and management software to RZF, the tax authority of the State of North Rhine-Westphalia in Germany.\n\nChip PC supplies thin clients to Multinational & Public sector organizations, recently winning 1st place in an independent Thin-Clients Evaluation among 26 thin clients from 9 vendors worldwide. Among Chip PC customers are top organizations from various verticals, such as Healthcare, Finance, Defense (Israeli Navy), Government (US Police), and Education.\n\nAlthough the company's main target markets are enterprises and large organizations, it modifies and customizes models to fit other markets; such as the Networked home, SOHO (Small-Office-Home-Office), Point of sale and others.\n\n"}
{"id": "1161624", "url": "https://en.wikipedia.org/wiki?curid=1161624", "title": "Crystal Decisions", "text": "Crystal Decisions\n\nCrystal Decisions (previously known as \"Seagate Software Information Management Group\") was a company that was known for its business intelligence products.\n\nThe company was formed when hard disk drive manufacturer Seagate Technology acquired Holistic Systems and Crystal Services with the intention of pursuing better profit margins in the software market. Holistic Systems had a wide range of sales offices, infrastructure and the Holos OLAP product, whilst Crystal Services had good OEM deals for the Crystal Reports database reporting product that they had written. The first new product from the combined company was Seagate Info, which later evolved to become known as Crystal Enterprise. Crystal Analysis followed as an OLAP client.\n\nThe company's structure reflected its heritage, with OLAP technologies being developed out of the former Holistic Systems R&D site in Ipswich, Suffolk, England and Relational Database technologies being developed out of the former Crystal Services R&D centre in Vancouver, British Columbia, Canada.\n\nCrystal Decisions was acquired by Business Objects in December 2003. As part of this acquisition, the former research and development site in Ipswich, Suffolk, England, was closed with a loss of about 80 jobs in order to centralize development in Vancouver and Paris, with the support for the Holos product being outsourced to Raspberry Software based near Ipswich.\n\nThe Holos product line has now ceased.\n\nBusiness Objects was purchased by SAP in March 2008.\n\n\n"}
{"id": "2767904", "url": "https://en.wikipedia.org/wiki?curid=2767904", "title": "Design Council", "text": "Design Council\n\nThe Design Council, formerly the Council of Industrial Design, is a United Kingdom charity incorporated by Royal Charter. Its stated mission is \"to champion great design that improves lives and makes things better\".\nIt was instrumental in the promoting of the concept of inclusive design.\n\nThe Design Council's archive is located at the University of Brighton Design Archives.\n\nThe Design Council operates two subsidiaries, the Design Council Commission for Architecture and the Built Environment (Design Council CABE) and Design Council Enterprises Limited.\n\nThe Design Council Commission for Architecture and the Built Environment (DC CABE, alternatively Design Council CABE, CABE at the Design Council, or simply CABE), is one of Design Council’s two subsidiaries. It supports communities, local authorities and developers involved in built environment projects by providing services in three areas: design review, customised expert support, and training and continued professional development (CPD). These services are supported by a network of Built Environment Experts (BEEs), a multidisciplinary team of 250 experts from “architecture, planning and infrastructure backgrounds, as well as academics, health specialists, and community engagement workers”.\n\nDesign Council CABE, which is intended to operate as a self-sustaining business, was formed on 1 April 2011 with about 20 staff from the original CABE after it was merged with the Design Council. The BEE network was formed in 2012.\n\nThe Design Council began on 19 December 1944 as the \"Council of Industrial Design\" (COID), founded by Hugh Dalton, President of the Board of Trade in the wartime Government. Its objective was 'to promote by all practicable means the improvement of design in the products of British industry'.\n\nS. C. Leslie, the Council's first director, played an important part in the \"Britain Can Make It\" exhibition of 1946. His 1947 successor Sir Gordon Russell established the organisational model for the next 40 years. Under Sir Paul Reilly the organisation changed its name to the \"Design Council\" in 1972.\n\nThe Design Council was incorporated as a registered charity by Royal Charter in 1976, although it continued to operate as a non-departmental public body.\n\nIn December 1994 it was restructured, resulting in a functional change from being both an advisory body and a provider of goods and services to a primarily strategic mission “to inspire the best use of design by the United Kingdom in the world context, in order to improve prosperity and wellbeing”.\n\nOn 1 April 2010 it incorporated a subsidiary trading company called Design Council Enterprises Limited to transact “fundraising activities that are not primary-purpose charitable activity.”\n\nOn 1 April 2011, it ceased to be a non-departmental public body of the Department for Business, Innovation and Skills and became an independent registered charity, although it continued to receive grants from the Department. It also officially merged with the Commission for Architecture and the Built Environment (CABE) on the same day although Design Council CABE was incorporated four days earlier.\n\nIn 2017, Design Council appointed Sarah Weir (OBE) as their CEO.\n\nSir Gordon Russell, who was heavily involved in the 1951 \"Festival of Britain\", examined ways to reform the education and training of new industrial designers. The Design Centre, in London's Haymarket, was officially opened on 26 April 1956.\n\nThe Council under Russell combined exhibitions with product endorsements, direct services to industry, commercial publishing and retail.\n\nAfter the Design Council’s restructuring in 1994, the Design Centre was closed to the public. The Design Council continued to operate from the Design Centre until 1998.\n\nBetween 1949 and 1999, the Design Council published \"Design\" (), a “well-regarded magazine of its own” The journal ceased publication after the summer issue of 1999.\n\nThe Council has hosted the \"British Design Awards\", with the 1987 logo rights co-owned with Manchester Metropolitan University. It was suggested in 1995 in \"Business Strategy Review\" magazine that the awards made suitable benchmarks, contributing to industrial competitiveness.\n\n\n"}
{"id": "30122659", "url": "https://en.wikipedia.org/wiki?curid=30122659", "title": "Diacetyl peroxide", "text": "Diacetyl peroxide\n\nDiacetyl peroxide is the organic peroxide with the formula (CHCO). It is a white solid or oily liquid with a sharp odor. Since the pure material poses an explosion hazard, it is often used as a solution, e.g., in dimethyl phthalate as a solvent.\n\nDiacetyl peroxide forms upon combining hydrogen peroxide and excess acetic anhydride. Peracetic acid is an intermediate.\n\nOrganic peroxides are typically explosive since they contain both the oxidizer, the O-O bond, and reducing agents, the C-C and C-H bonds.\n\nIt is shock sensitive and explosive.\n\nThe threshold quantity for Process Safety Management per Occupational Safety and Health Administration 1910.119 is if the concentration of the diacetyl peroxide solution is greater than 70%.\n\nThere have been reports of detonation of the pure material. The 25% solution also has explosive potential. The crystalline peroxide is especially shock sensitive and a high explosion risk.\n\nContact with liquid causes irritation of eyes and skin. If ingested, irritates mouth and stomach.\n\n"}
{"id": "58011041", "url": "https://en.wikipedia.org/wiki?curid=58011041", "title": "Exit Intent Popup", "text": "Exit Intent Popup\n\nAn exit intent popup is a technique used in online shops and websites to retain visitors that are going to leave the site.\n\nWith an exit intent popup, a visitor's mouse movements are tracked, and when the cursor moves outside the upper page boundary (because the tab bar is usually there), a popup window is shown. The ad is triggered by using a Javascript snippet that measures the speed and direction of the mouse. Often, the popup contains a discount code to convince the user to finish the sale. Exit intent advertisements can also be used to offer a discount, push to a demo, collect emails, or segment to a newsletter list.\n\nThe exit-intent technology works differently on mobile devices than on a desktop. There is no mouse movement to track, so advanced exit intent popup software will show a popup once a visitor tries to click \"back\" or close the mobile browser tab.\n\n"}
{"id": "4766766", "url": "https://en.wikipedia.org/wiki?curid=4766766", "title": "Extension tube", "text": "Extension tube\n\nAn extension tube - also called extension ring - is used with interchangeable lenses to focus closer, useful in macro photography.\n\nThe tube contains no optical elements; its sole purpose is to move the lens farther from the image plane. The farther away the lens is, the closer the focus, the greater the magnification, and also the greater the loss of light (requiring a longer exposure time). Lenses classically focus closer than infinity by moving all optical elements farther from the film or sensor; an extension tube simply enables this movement. \n\nExtension tubes without electrical contacts will not allow an electronic automatic camera to control the lens, thus disabling autofocus and in some cases forcing a user to shoot wide open unless the lens offers manual aperture control. More expensive extension tubes contain electrical contacts allowing the user to use autofocus and electronically control the aperture of the attached lens. An advantage to the non-electrical tubes is their lower price.\n\nOther items like lens adapters may unintentionally have an effect similar to an extension tube. A lens designed for a small flange focal distance may not be able to focus to infinity when a lens adapter places the sensor too far away.\n\nExtension tubes are sometimes confused with teleconverters, an optical component (i.e., containing lenses) designed to increase effective focal length.\n\nA close-up lens also enables focusing closer for macro photography but, unlike an extension tube, a close-up lens actually is an optical element.\n\n"}
{"id": "23542979", "url": "https://en.wikipedia.org/wiki?curid=23542979", "title": "Flash-lamp", "text": "Flash-lamp\n\nThe electric flash-lamp uses electric current to start flash powder burning, to provide a brief sudden burst of bright light. It was principally used for flash photography in the early 20th century but had other uses as well. Previously, photographers' flash powder, introduced in 1887 by Adolf Miethe and Johannes Gaedicke, had to be ignited manually, exposing the user to greater risk.\n\nThe electric flash-lamp was invented by Joshua Cohen (a.k.a. Joshua Lionel Cowen of the Lionel toy train fame) in 1899, and by Paul Boyer in France. It was granted U.S. patent number 636,492. This flash of bright light from the flash-lamp was used for indoor photography in the late nineteenth century and the early part of the twentieth century.\n\nJoshua Lionel Cowen's flash-lamp patent 636,492 reads in part, \n\nThe principle of operation of the electrical flash-lamp is linked to the shutter of an early box camera: tripping the shutter ignites the flash powder and releases the potential energy of the exploding powder causing a bright flash for indoor photography.\n\nThe main purpose of Cowen's invention was as a fuse to ignite explosive powder to get a photographer's flash. One of the first practical applications, however, for Cowen's flash-lamp was as underwater mine detonator fuses for the U.S. Navy. In 1899, the year the invention was patented, the government awarded Cowen a $12,000 contract for 24,000 naval mine detonator fuses. The use of the flash for photography was dangerous, and photographers could get burned hands from the flash.\n\nA 1910 brochure for the Nesbit High Speed Flashlight Apparatus says,\n\n"}
{"id": "7986973", "url": "https://en.wikipedia.org/wiki?curid=7986973", "title": "Fuel card", "text": "Fuel card\n\nA fuel card or fleet card is used as a payment card most commonly for gasoline, diesel, and other fuels at gas stations. Fleet cards can also be used to pay for vehicle maintenance and expenses at the discretion of the fleet owner or manager.\n\nFleet cards are unique due to the convenient and comprehensive reporting that accompanies their use. Fleet cards enable fleet owners/ managers to receive real time reports and set purchase controls with their cards helping them to stay informed of all business related expenses. \n\nFleet cards can be provided not only by oil brands like Shell, Chevron, ExxonMobil, but also by companies that only specialize in providing fuel cards such as Edenred, Fleetcor, Petrol Plus Region, Fuelman, Fleetcard and others. Additionally, rideshare companies like Uber have fleet cards for their drivers, which allow the drivers to have gas money deducted from their earnings.\n\nIn its infancy, fuel cards were only printed with the company name, vehicle registration and a signature strip on the reverse. No electronic data was stored. Fuelling sites would verify the company, vehicle registration (on the forecourt) against the card and also the signature written on the back. The site would allow access to the fuel once the retailer's receipt had been signed for and cross checked against the signature written on the back of the card.\n\nInitially, fuel card networks were very small and based around truck roads and main haulage routes. For example, in 1983, the Keyfuels site network consisted of only seven stations. Therefore, they were initially targeted at haulage or delivery companies. A few years later, cards became embossed rather than printed. This was due to provide the cards with a greater longevity — frequent use would rub off the printed information.\n\nDue to the lack of electronic data on the fuel cards at this stage, transactions would be recorded by the card being 'stamped' onto a manual transaction sheet. Further details detailing date, time, volume, grade of fuel and registration would be hand-written.\n\nDuring the mid to late 1980s, fuel cards began to use magnetic strip technology. This meant fuel cards could be processed by a retailer electronically and reduced the risk of human error when recording transaction details.\n\nMagnetic strips also enabled fuel card providers to increase fuel card security by ensuring PINs were encoded into the card. Although it should be noted that when the magnetic strip is swiped though a fuel card reader, the transaction is still only verified by checking signatories to this day.\n\nIn the advent of outdoor terminals, these PINs became compulsory in order to re-fuel.\n\nThe reasoning behind moving from the magnetic strip to smartchip technology was down to the fact that the magnetic strip could be cloned and the data written onto a dummy card. Also, the use of fuel cards was far heavier than that of debit or credit cards, and therefore it became apparent that the magnetic strip began to wear out far quicker.\n\nSmartchip technology (similar to Chip and PIN) is the largest development in the fuel card industry in recent years. (See Smartchip benefits)\n\nIncreasingly, supermarkets are exploring opportunities in fuel cards. While Tesco & Sainsbury's have the ability to collect loyalty points (Clubcard & Nectar points) and pay with credit, Morrisons has a dedicated fuel card offering. Asda has none at all, In spite of being the cheapest seller of fuel on average in 2012.\n\nDuring 2008, market maturity has led to users increasingly expecting more from fuel cards than discount pricing, with the demand for service, savings and security leading to the appearance of dedicated account management. While most fuel card suppliers handle customer queries via random-operator call centres, customer preference is increasingly for a named individual to handle their business. Respected publication \"Fleet News\" reported in July 2008 that more than a quarter of fleet managers are unhappy with the level of service offered by their fuel card supplier.\n\nThe advent of fleet cards can be traced back to the 1960s and 70s when key stops/key locks and stand alone card locks were used by independent marketers and filling station owners. A Key Stop or Key Lock fuel control system was a system where a group of commercial fleets could access a fuel pump with a unique key that tracked solely their gallons through that pump. This technology is obsolete and no longer available commercially. A card lock control system uses a punch card or magnetic stripe card to uniquely identify a fuel buyer on a private server. This was done to avoid a credit card interchange fee which raised the cost of fuel considerably. The first commercial fuel cards resembled a credit card with a name and a company logo on them. When a customer entered a fueling station, the cashier would take down the customer’s name and company information to authenticate ownership of the card. This process was time consuming and was vulnerable to fraudulent transactions. With the advent of computers and computer software in the 1980s, the development of the fleet card industry quickly expanded. The invention of the magnetic stripe and magnetic card reader allowed petroleum marketers to control fuel pump transactions, leading to today’s wide range of fleet card security features and state of the art reporting systems to track all fleet expenses. These “intelligent” systems make fleet management convenient and secure, as fleet card owners are able to track fleet fuel use with increased accuracy, receiving reports in real time on the fueling habits. Business owners are able to limit employee fueling by time-of-day and day-of-week.\n\nIndian consumers saw their first fuel card during 2001 from Bharat Petroleum, the card was mainly aimed at retail customers for personal vehicles. Subsequently various products aimed at various customer segments were launched by all Oil Companies - Smart fleet by Bharat Petroleum, XTRAPOWER by Indian Oil, Drive Track by Hindustan Petroleum, Transconnect by Reliance Industries. While Transconnect found favours during its launch, the same withered away once Reliance shut its outlets due to huge price difference. Indian Oil's XTRAPOWER Fleet Card Program has grown to be the largest fuel card in India, followed by Smart Fleet of Bharat Petroleum and revived Drivetrack Plus of Hindustan Petroleum.\n\nAlthough fuel cards effectively 'look' like credit cards and use very similar technology, their use and implementation is significantly dissimilar, enough to differentiate them as an alternative payment method. The main differences from credit cards are:\n\n\nDepending upon the individual fuel card and the supplier, security benefits of fuel cards can include:\n\nFuel card providers realised there were many benefits from moving over to the smartcard from the magnetic strip:\n\n\nAs of 2007, only 50% or so of fuel cards on offer utilise the smartchip technology.\n\nFuelling limits can also be programmed into a fuel card using smartchip technology to specify the following:\n\n\nCommercial fuel cards are designed with fleets in mind and seek to avoid both the percentage up-charges of credit card companies as well as the theft risk retail credit cards expose a business to due to their focus on convenience over financial security. \n\nTypically, the majority of businesses using fuel cards are those which heavily rely on motor vehicles on a day-to-day basis e.g. transport, haulage, courier services. One of the primary reasons a business will use a fuel card is to obtain (potentially) significant savings both on the current price of fuel and on administrative costs. It would be normal for the business to receive a single weekly invoice, payable by direct debit; this replaces the manual reconciliation of individual paper receipts which could, for larger organizations, number in the hundreds each week. A number of additional benefits are available for users of fuel cards from a supplier offering an e-business capability.\n\nIn most cases, fuel cards can provide fuel at a wholesale price as opposed to standard retail. This way, discount fuel can be purchased without needing to buy in bulk.\n\nFurthermore, the management and security concerning fuel purchases is greatly improved via the use of fuel cards. These features often prove themselves attractive to businesses, especially with those operating large fleets which can sometimes be in the thousands of vehicles.\n\nFor example, a business may obtain anything from a one to four pence per litre reduction (PPL) on diesel, which in real terms can be translated into the following (UK based) example:\n\nWhile most fuel cards are for use in a particular country, there are some companies who offer international fuel cards themselves and some via a third party. International site networks often use fully automatic fuel pumps to avoid possible language difficulties and are specially designed to account for different taxation regimes e.g. producing separate invoices for each country which fuel was purchased in a particular month to account for different rates of VAT charged. These site networks sometimes offer the ability to reclaim VAT paid in each country, for a small percentage of the amount reclaimed. Some international fuel card providers solely offer fuel cards for business fleets, others also for individuals. However, the system of fuel cards has meanwhile spread throughout Europe, where they are either called cartões de combustivel, cartecarburanti, tankpas zakelijk tankkaart or fleetpass, cartes de carburants, and Tankkarten.\n\nFuel card providers which operate on a bunkering basis aim to achieve a fuel reserve on a particular network in order to achieve a discounted price, therefore taking advantage of economies of scale.\n\nFor example, a company may purchase one million litres of diesel at 72.50 and aim to sell this on to the customer with a mark up of 2.5 pence per litre — 75.00 PPL.\n\nBunkered fuel card companies sometimes also offer customers their own fuel bunker to under the premise of further benefiting from a discounted price. Furthermore, a customer can also hope to achieve a saving by way of avoiding any market increases in the standard market price for that particular fuel. In short, customer fuel bunkering has many pros & cons:\n\nPros:\n\nCons:\n\nIn contrast to bunkered, retail fuel cards operate by way of allowing the customer to draw fuel at almost any fuelling station (in same method as credit card). Often providers will levy a surcharge in addition to the retail price as advertised at the fuelling station. The retail price given is often considerably higher than that of the bunkered. The majority of fuel cards provide weekly (advance) notification of fuel price generally applicable nationwide.\n\nAlthough retail is generally more expensive, there are a number of exceptions depending on the region, particular brand/site and timing of fuel purchased. Retail fuel can be cheaper in certain regions, particularly those near to a major port. Further reasons for the difference in price may be due to local economy (e.g. north / south of England) and whether the site is close to any main transport links i.e. the fuel costs more to deliver into the site. As for timing, the supermarkets or large providers often have a great deal of fuel in their stock reserves, so if the market increases rapidly, they would generally take longer than smaller providers to reflect this change.\n\nFuel cards are not all the same and 'shopping around' is advised. Typically, a supplier will offer just one or two cards. The user should seek an independent supplier offering a range of cards from major brands, so that the most appropriate fuel card for their individual needs can be chosen. A fuel card with little or no motorway coverage but extensive coverage of metropolitan areas, for example, could be of limited use to a national haulier but ideal for a taxi company. A supplier offering only diesel cards will be of minimal appeal to a fleet manager responsible for a petrol-only or mixed-fuel company car fleet.\n\nFurthermore, retail fuel prices have decreased over the past 15 or so years largely due to supermarkets providing fuel at their superstores at hugely discounted prices in order to entice users to the store. Supermarket prices are an irrelevance to many diesel users, as very few supermarket forecourts are accessible by heavy goods vehicles or coaches.\n\n"}
{"id": "33323229", "url": "https://en.wikipedia.org/wiki?curid=33323229", "title": "Grouptime", "text": "Grouptime\n\nGrouptime is a technology start-up based in Munich, Germany, and focused on mobile messaging apps. The core product is Teamwire, an encrypted instant messaging app for enterprises and the public sector. In addition to text messaging, users can send photos, videos, locations, voice messages and files with Teamwire.\n\nThe company was founded in August 2010 in Munich (Germany) by Tobias Stepan. In September 2011 the grouptime app was officially launched for devices with Apple`s iOS. In 2012 several updates of the app were released to improve group messaging and sharing of digital content.\n\nIn March 2014 grouptime launched a secure enterprise messaging app called Teamwire to simplify and improve the internal communication. The idea was to provide a messenger for businesses, to replace the email as the dominant channel for team communication. By mid of 2014 grouptime abandoned its consumer product and completely focussed on the enterprise messaging market. In January 2015 in addition to the German cloud Teamwire became also available as an on-premise and private cloud deployment in order to fulfill the strong data protection requirements of enterprises, large corporations and the public sector. In March 2016 Teamwire became a cross-platform enterprise messaging app with the release of apps for desktop devices like Windows, Mac and Linux in addition to the existing iOS and Android apps.\n\nIn January 2017 the largest German financial services group deployed Teamwire as a secure messenger for the bank. In May 2017 it became public that the police in the state of Bavaria in Germany uses Teamwire as a secure alternative to WhatsApp.\n"}
{"id": "1488293", "url": "https://en.wikipedia.org/wiki?curid=1488293", "title": "Heating pad", "text": "Heating pad\n\nA heating pad is a pad used for warming of parts of the body in order to manage pain. Localized application of heat causes the blood vessels in that area to dilate, enhancing perfusion to the targeted tissue. Types of heating pads include electrical, chemical and hot water bottles.\n\nSpecialized heating pads (mats) are also used in other settings. Heat mats in plant propagation stimulates seed germination and root development; they operate at cooler temperatures. Heat mats also are available in the pet trade, especially as warming spots for reptiles such as lizards and snakes.\n\nElectric pads usually operate from household current and must have protection against overheating.\n\nA moist heating pad is used dry on the user's skin. These pads register temperatures from and are intended for deep tissue treatment and can be dangerous if left on unattended. Moist heating pads are used mainly by physical therapists but can be found for home use. A moist cloth can be added with a stupe cover to add more moisture to the treatment.\n\nDisposable chemical pads employ a one-time exothermic chemical reaction. One type, frequently used for hand warmers, is triggered by unwrapping an air-tight packet containing slightly moist iron powder and salt or catalysts which rusts over a period of hours after being exposed to oxygen in the air. Another type contains separate compartments within the pad; when the user squeezes the pad, a barrier ruptures and the compartments mix, producing heat such as the enthalpy change of solution of calcium chloride dissolving.\n\nThe most common reusable heat pads contain a supersaturated solution of sodium acetate in water. Crystallization is triggered by flexing a small flat disc of notched ferrous metal embedded in the liquid. Pressing the disc releases very tiny adhered crystals of sodium acetate into the solution which then act as nucleation sites for the crystallization of the sodium acetate into the hydrated salt (sodium acetate trihydrate, CHCOONa·3HO). Because the liquid is supersaturated, this makes the solution crystallize suddenly, thereby releasing the energy of the crystal lattice. The use of the metal disc was invented in 1978.\n\nThe pad can be reused by placing it in boiling water for 10–15 minutes, which redissolves the sodium acetate trihydrate in the contained water and recreates a supersaturated solution. Once the pad has returned to room temperature it can be triggered again. Triggering the pad before it has reached room temperature results in the pad reaching a lower peak temperature, as compared to waiting until it had completely cooled.\n\nHeating packs can also be made by filling a container with a material that has a high specific heat capacity, which then gradually releases the heat over time. A hot water bottle is the most familiar example of this type of heating pad.\n\nA microwavable heating pad is a heating pad that is warmed by placing it in a microwave oven before use. Microwavable heating pads are typically made out of a thick insulative fabric such as flannel and filled with grains such as wheat, buckwheat or flax seed. Due to their relative simplicity to make, they are frequently sewn by hand, often with a custom shape to fit the intended area of use.\n\nOften, aromatic compounds will also be added to the filler mixture to create a pleasant or soothing smell when heated. The source of these can vary significantly, ranging from adding essential oils to ground up spices such as cloves and nutmeg, or even dried rose petals.\n\nPhase change materials can be used for heating pads intended to operate at a fixed temperature. The heat of fusion is used to release the thermal energy. This results in the pad heating up.\n\nMany episodes of pain come from muscle exertion or strain, which creates tension in the muscles and soft tissues. This tension can constrict circulation, sending pain signals to the brain. Heat application eases pain by:\n\n\nAs many heating pads are portable, heat may be applied as needed at home, at work, or while traveling. Some physicians recommend alternating heat and ice for pain relief. \"As with any pain treatment, a physician should be consulted prior to beginning treatment.\"\n\n"}
{"id": "5312299", "url": "https://en.wikipedia.org/wiki?curid=5312299", "title": "Impedance cardiography", "text": "Impedance cardiography\n\nImpedance cardiography (ICG) is a noninvasive technology measuring total electrical conductivity of the thorax and its changes in time to process continuously a number of cardiodynamic parameters, such as Stroke Volume, SV, Heart Rate, HR, Cardiac Output, CO, Ventricular Ejection Time, VET, Pre-ejection Period and used to detect the impedance changes caused by a high-frequency, low magnitude current flowing through the thorax between additional two pairs of electrodes located outside of the measured segment. The sensing electrodes also detect the ECG signal, which is used as a timing clock of the system.\n\nImpedance cardiography (ICG), also referred to as electrical impedance plethysmography (EIP) or Thoracic Electrical Bioimpedance (TEB) has been researched since the 1940s. NASA helped develop the technology in the 1960s. The use of impedance cardiography in psychophysiological research was pioneered by the publication of an article by Miller and Horvath in 1978. Subsequently, the recommendations of Miller and Horvath were confirmed by a standards group in 1990. A comprehensive list of references is available at ICG Publications. With ICG, the placement of four dual disposable sensors on the neck and chest are used to transmit and detect electrical and impedance changes in the thorax, which are used to measure and calculate cardiodynamic parameters.\n\n\nHemodynamics is a subchapter of cardiovascular physiology, which is concerned with the forces generated by the heart and the resulting motion of blood through the cardiovascular system. These forces demonstrate themselves to the clinician as paired values of blood flow and blood pressure measured simultaneously at the output node of the left heart. \n\"Hemodynamics is a fluidic counterpart to the Ohm's Law in electronics: pressure is equivalent to voltage, flow to current, vascular resistance to electrical resistance and myocardial work to power.\"\nThe relationship between the instantaneous values of aortic blood pressure and blood flow through the aortic valve over one heartbeat interval and their mean values are depicted in Fig.1. Their instantaneous values may be used in research; in clinical practice, their mean values, MAP and SV, are adequate.\n\nSystemic (global) blood flow parameters are (a) the blood flow per heartbeat, the Stroke Volume, SV [ml/beat], and (b) the blood flow per minute, the Cardiac Output, CO [l/min]. \nThere is clear relationship between these blood flow parameters:\nwhere HR is the Heart Rate frequency (beats per minute, bpm).\n\nSince the normal value of CO is proportional to body mass it has to perfuse, one \"normal\" value of SV and CO for all adults cannot exist. All blood flow parameters have to be indexed. The accepted convention is to index them by the Body Surface Area, BSA [m²], by DuBois & DuBois Formula, a function of height and weight:\n\nThe resulting indexed parameters are Stroke Index, SI (ml/beat/m²) defined as\n\nand Cardiac Index, CI (l/min/m²), defined as \n\nThese indexed blood flow parameters exhibit typical ranges:\n\nFor the Stroke Index: 35 < SI < 65 ml/beat/m²; for the Cardiac Index: 2.8 < CI < 4.2 l/min/m².\n\nEq.1 for indexed parameters then changes to\n\nThe primary function of the cardiovascular system is transport of oxygen: blood is the vehicle, oxygen is the cargo. The task of the healthy cardiovascular system is to provide adequate perfusion to all organs and to maintain a dynamic equilibrium between oxygen demand and oxygen delivery. In a healthy patient, his or her cardiovascular system always increases blood flow in response to increased oxygen demand. However, in a hemodynamically compromised patient, when the system is unable to satisfy increased oxygen demand, the blood flow to organs lower on the oxygen delivery priority list is reduced and these organs may, eventually, fail. Digestive disorders, male impotence, tiredness, sleepwalking, environmental temperature intolerance, etc. are classical examples of a low-flow-state, resulting in reduced blood flow to the gut, sexual organs, skeletal muscles, skin, etc.\n\nSI variability and MAP variability are accomplished through activity of hemodynamic modulators.\nThe conventional cardiovascular physiology terms for the hemodynamic modulators are preload, contractility and afterload. They deal with (a) the inertial filling forces of blood return into the atrium (preload), which stretch the myocardial fibers, thus storing energy in them, (b) the force by which the heart muscle fibers shorten thus releasing the energy stored in them in order to expel part of blood in the ventricle into the vasculature (contractility), and (c) the forces the pump has to overcome in order to deliver a bolus of blood into the aorta per each contraction (afterload). \nThe level of preload is currently assessed either from the PAOP (pulmonary artery occluded pressure) in a catheterized patient, or from EDI (end-diastolic index) by use of ultrasound. Contractility is not routinely assessed; quite often inotropy and contractility are interchanged as equal terms. Afterload is assessed from the SVRI value.\nRather than using the terms preload, contractility and afterload, the preferential terminology and methodology in per-beat hemodynamics is to use the terms for actual hemodynamic modulating tools, which either the body utilizes or the clinician has in his toolbox to control the hemodynamic state:\n\nThe preload and the Frank-Starling (mechanically)-induced level of contractility is modulated by variation of intravascular volume (volume expansion or volume reduction/diuresis).\n\nPharmacological modulation of contractility is performed with cardioactive inotropic agents (positive or negative inotropes) being present in the blood stream and affecting the rate of contraction of myocardial fibers.\n\nThe afterload is modulated by varying the caliber of sphincters at the input and output of each organ, thus the vascular resistance, with the vasoactive pharmacological agents (vasoconstrictors or vasodilators and/or ACE Inhibitors and/or ARBs)(ACE = Angiotensin-converting-enzyme; ARB = Angiotensin-receptor-blocker). \"Afterload also increases with increasing blood viscosity, however, with the exception of extremely hemodiluted or hemoconcentrated patients, this parameter is not routinely considered in clinical practice.\"\n\n\"Please note that with the exception of volume expansion, which can be accomplished only by physical means (intravenous or oral intake of fluids), all other hemodynamic modulating tools are pharmacological, cardioactive or vasoactive agents.\"\n\nThe measurement of CI and its derivatives allow clinicians to make timely patient assessment, diagnosis, prognosis, and treatment decisions. It has been well established that both trained and untrained physicians alike are unable to estimate cardiac output through physical assessment alone.\n\nClinical measurement of cardiac output has been available since the 1970s. However, this blood flow measurement is highly invasive, utilizing a flow-directed, thermodilution catheter (also known as the Swan-Ganz catheter), which represents significant risks to the patient. In addition, this technique is costly (several hundred dollars per procedure) and requires a skilled physician and a sterile environment for catheter insertion. As a result, it has been used only in very narrow strata (less than 2%) of critically ill and high-risk patients in whom the knowledge of blood flow and oxygen transport outweighed the risks of the method. In the United States, it is estimated that at least two million pulmonary artery catheter monitoring procedures are performed annually, most often in peri-operative cardiac and vascular surgical patients, decompensated heart failure, multi-organ failure, and trauma.\n\nIn theory, a noninvasive way to monitor hemodynamics would provide exceptional clinical value because data similar to invasive hemodynamic monitoring methods could be obtained with much lower cost and no risk. While noninvasive hemodynamic monitoring can be used in patients who previously required an invasive procedure, the largest impact can be made in patients and care environments where invasive hemodynamic monitoring was neither possible nor worth the risk or cost. Because of its safety and low cost, the applicability of vital hemodynamic measurements could be extended to significantly more patients, including outpatients with chronic diseases. ICG has even been used in extreme conditions such as outer space and a Mt. Everest expedition. Heart failure, hypertension, pacemaker, and dyspnea patients are four conditions in which outpatient noninvasive hemodynamic monitoring can play an important role in the assessment, diagnosis, prognosis, and treatment. Some studies have shown ICG cardiac output is accurate, while other studies have shown it is inaccurate. Use of ICG has been shown to improve blood pressure control in resistant hypertension when used by both specialists and general practitioners. ICG has also been shown to predict worsening status in heart failure.\n\nThe electrical and impedance signals are processed to determine fiducial points, which are then utilized to measure and calculate hemodynamic parameters, such as cardiac output, stroke volume, systemic vascular resistance, thoracic fluid content, acceleration index, and systolic time ratio.\n\n"}
{"id": "50961891", "url": "https://en.wikipedia.org/wiki?curid=50961891", "title": "Individual Quick Freezing", "text": "Individual Quick Freezing\n\nIndividual Quick Freezing usually abbreviated IQF is a freezing method used in food processing industry. Products commonly frozen with IQF technologies are typically smaller pieces of food products and can range from all types of berries, fruits and vegetables diced or sliced, seafood such as shrimps and small fish, meat, poultry and even pasta, cheese and grains. Products that have been subjected to IQF are referred to as Individually Quick Frozen or IQF'd\nOne of the main advantages of this method of preparing frozen food is that the freezing process takes only a few minutes. The exact time depends on the type of IQF freezer and the product. The short freezing prevents formation of large ice crystals in the product’s cells, which destroys the membrane structures at the molecular level. This makes the product keep its shape, colour, smell and taste after defrost, at a far greater extent. \n\nAnother significant advantage of IQF technology is its ability to separate units of the products during freezing, which produces higher quality product compared to block freezing. This advantage is also important for food sustainability, as the consumer can defrost and use the exact needed quantity. \nThere is a range of IQF technologies, but the main concept is to transport the product into the freezer with the help of a processing line belt or infeed shaker. Inside the freezer, the product travels through the freezing zone and comes out on the other side. Product transportation inside the freezer is by different technologies. Some freezers use transportation belts similar to a conveyor belt. Others use bed plates that hold the product, and an asymmetrical movement makes the plate advance by itself through the freezer.\nThere are 2 main IQF technologies: mechanical IQF freezers and cryogenic IQF freezers. \n\nMechanical IQF freezers work on the principle of cold air circulation, which flows from underneath the bed plate or transportation belt with the help of fans. The cold airflow keeps passing through the pieces of product in circular motions while the product is also advancing through the freezer towards the exit end. The design and efficiency of this type of IQF freezers varies among manufacturers who are trying to find the perfect balance of aerodynamics for an optimal freezing result. This technology has seen impressive improvements and developments during the past 20 years, being suited for an increasing range of products.\n\nCryogenic IQF freezers immerse the product in liquid nitrogen at very low temperatures, freezing it very rapidly while continuously moving the product to avoid block or lump formation. Even though this method shows good freezing results, it might lead to higher processing costs per kg of product due to the large amount of higher cost liquid nitrogen required. \n\nA growing demand and interest in IQF products is registered at global level due to the higher quality of these products and to the practical benefits of having separately frozen pieces. IQF is also a common pre-treatment for freeze-drying food because both processes preserve the size, taste and cell structure of the food better than methods like traditional block freezing or air drying respectively.\n"}
{"id": "16093841", "url": "https://en.wikipedia.org/wiki?curid=16093841", "title": "Instron", "text": "Instron\n\nInstron (an ITW company) is a manufacturer of test equipment designed to evaluate the mechanical properties of materials and components, such as universal testing machines.\n\nIn 1946, Harold Hindman and George Burr, who worked together at Massachusetts Institute of Technology (MIT), teamed up to determine the properties of new materials to be used in parachutes. Together, they designed a material testing machine based on strain gauge load cells and servo-control systems. This led to the formation of Instron Engineering Corporation.\n\nCEAST (Now operating within Instron) was founded in 1953 by Dr. Mario Grosso and focused initially on refurbishing instruments shipped from America through the Marshall plan. Later, the company used its experience to develop polymer testing instruments. Today the company is based in Pianezza, Turin, and specializes in impact, rheology and HDT/Vicat testing of polymers, typically thermoplastics. In 2008 CEAST was acquired by Instron but continues to operate semi-autonomously.\n\n\n\nIn 2015, customers recognized Instron as the Best Mechanical Testers \n\n"}
{"id": "5720202", "url": "https://en.wikipedia.org/wiki?curid=5720202", "title": "Joint application design", "text": "Joint application design\n\nJoint application design (JAD) is a process used in the life cycle area of the dynamic systems development method (DSDM) to collect business requirements while developing new information systems for a company. \"The JAD process also includes approaches for enhancing user participation, expediting development, and improving the quality of specifications.\" It consists of a workshop where \"knowledge workers and IT specialists meet, sometimes for several days, to define and review the business requirements for the system.\" The attendees include high level management officials who will ensure the product provides the needed reports and information at the end. This acts as \"a management process which allows Corporate Information Services (IS) departments to work more effectively with users in a shorter time frame\".\n\nThrough JAD workshops the knowledge workers and IT specialists are able to resolve any difficulties or differences between the two parties regarding the new information system. The workshop follows a detailed agenda in order to guarantee that all uncertainties between parties are covered and to help prevent any miscommunications. Miscommunications can carry far more serious repercussions if not addressed until later on in the process. (See below for Key Participants and Key Steps to an Effective JAD). In the end, this process will result in a new information system that is feasible and appealing to both the designers and end users. \n\n\"Although the JAD design is widely acclaimed, little is actually known about its effectiveness in practice.\" According to the \"Journal of Systems and Software\", a field study was done at three organizations using JAD practices to determine how JAD influenced system development outcomes. The results of the study suggest that organizations realized modest improvement in systems development outcomes by using the JAD method. JAD use was most effective in small, clearly focused projects and less effective in large complex projects. Since 2010, the International Association of Facilitators (IAF) has measured the significance of facilitated workshops, a la JAD, and found significant value.\n\nJoint application is term originally used to describe a software development process pioneered and successfully deployed during the mid-1970s by the New York Telephone Company's Systems Development Center under the direction of Dan Gielan. Following a series of remarkably successful implementations of this methodology, Gielan lectured extensively in various forums on the methodology, its benefits and best practices. of IBM Canada created and named JAD in 1974, or joint application design, as it is currently used in software development. While working at IBM in Regina, Saskatchewan, Arnie Lind, a Senior Systems Engineer at the time, was searching for a better way to implement applications at IBM's customers. The existing method entailed application developers spending months learning the specifics of a particular department or job function, and then developing an application for the function or department. In addition to significant development backlog delays, this process resulted in applications taking years to develop, and often not being fully accepted by the application users.\n\nArnie Lind's idea was simple: rather than have application developers learn about people's jobs, why not teach the people doing the work how to write an application? Arnie pitched the concept to IBM Canada's Vice President Carl Corcoran (later President of IBM Canada), and Carl approved a pilot project. Arnie and Carl together named the methodology JAD, an acronym for joint application design, after Carl Corcoran rejected the acronym JAL, or joint application logistics, upon realizing that Arnie Lind's initials were JAL (John Arnold Lind).\n\nThe pilot project was an emergency room project for the Saskatchewan Government. Arnie developed the JAD methodology, and put together a one-week seminar, involving primarily nurses and administrators from the emergency room, but also including some application development personnel. The project was a huge success, as the one-week seminar produced a detailed application framework, which was then coded and implemented in less than one month, versus an average of 18 months for traditional application development. And because the users themselves designed the system, they immediately adopted and liked the application. After the pilot project, IBM was very supportive of the JAD methodology, as they saw it as a way to more quickly implement computing applications, running on IBM hardware.\n\nArnie Lind spent the next 13 years at IBM Canada continuing to develop the JAD methodology, and traveling around the world performing JAD seminars, and training IBM employees in the methods and techniques of JAD. JADs were performed extensively throughout IBM Canada, and the technique also spread to IBM in the United States. Arnie Lind trained several people at IBM Canada to perform JADs, including Tony Crawford and Chuck Morris. Arnie Lind retired from IBM in 1987, and continued to teach and perform JADs on a consulting basis, throughout Canada, the United States, and Asia.\n\nThe JAD process was formalized by Tony Crawford and Chuck Morris of IBM in the late 1970s. It was then deployed at Canadian International Paper. JAD was used in IBM Canada for a while before being brought back to the US. Initially, IBM used JAD to help sell and implement a software program they sold, called COPICS. It was widely adapted to many uses (system requirements, grain elevator design, problem-solving, etc.). Tony Crawford later developed JAD-Plan and then JAR (joint application requirements). In 1985, Gary Rush wrote about JAD and its derivations – Facilitated Application Specification Techniques (FAST) – in Computerworld.\n\nOriginally, JAD was designed to bring system developers and users of varying backgrounds and opinions together in a productive as well as creative environment. The meetings were a way of obtaining quality requirements and specifications. The structured approach provides a good alternative to traditional serial interviews by system analysts. JAD has since expanded to cover broader IT work as well as non-IT work (read about Facilitated Application Specification Techniques – FAST – created by Gary Rush in 1985 to expand JAD applicability.\n\nExecutive Sponsor: The executive who charters the project, the system owner. They must be high enough in the organization to be able to make decisions and provide the necessary strategy, planning, and direction.\n\nSubject Matter Experts: These are the business users, the IS professionals, and the outside experts that will be needed for a successful workshop. This group is the backbone of the meeting; they will drive the changes.\n\nFacilitator/Session Leader: meeting and directs traffic by keeping the group on the meeting agenda. The facilitator is responsible for identifying those issues that can be solved as part of the meeting and those which need to be assigned at the end of the meeting for follow-up investigation and resolution. The facilitator serves the participants and does not contribute information to the meeting.\n\nScribe/Modeller/Recorder/Documentation Expert: Records and publish the proceedings of the meeting and does not contribute information to the meeting. \n\nObservers: Generally members of the application development team assigned to the project. They are to sit behind the participants and are to silently observe the proceedings.\n\n\n\n\n \n"}
{"id": "3181724", "url": "https://en.wikipedia.org/wiki?curid=3181724", "title": "League of Professional System Administrators", "text": "League of Professional System Administrators\n\nThe League of Professional System Administrators (LOPSA), founded in 2004, is a professional association for IT Administrators.\n\nOriginally, the corporation was created as \"The System Administrators Guild, Inc\" in July 2004 by volunteers of the USENIX Association as part of a plan to spin off its SAGE Special Technical Group into a separate organization. After the spin-off from the USENIX Association was halted in November 2005, the volunteers involved in the spin-off opted to move forward as a new organization which was renamed LOPSA, and began reorganizing itself into an independent entity.\n\nThe organization's mission is \"to advance the practice of system administration; to support, recognize, educate, and encourage its practitioners; and to serve the public through education and outreach on system administration issues\".\n\nLOPSA has several ongoing programs that it uses to further its mission.\n\nThe LOPSA mentorship program was conceived in 2010 as a way to network inexperienced system administrators with senior members of the organization who could help them on project-related topics. As of January 2013, the mentorship program accepts ongoing, rather than project-based, projects. Protege status is open to any system administrator irrespective of membership in LOPSA, while mentor status requires an active membership in the organization.\n\nThere is a well-recognized disparity between men and women in STEM fields. LOPSA provides an annual stipend for one woman to attend the USENIX Women in Advanced Computing (WiAC) Summit. The recipient is chosen through an essay contest.\n\nLOPSA has formed multiple local chapters in cities in throughout the United States. Two LOPSA chapters have formed regionally-targeted system administration conferences.\n\nThe organization is a 501c3 not-for-profit group, and is governed by a nine-member Board of Directors. The first board was elected in July 2005 by the membership of SAGE. Elections are held each year for either four or five Board members, with each election term being for two years.\n\nLOPSA is headquartered in Mount Laurel, New Jersey.\n\nLOPSA fosters community through local chapters and affiliation with and support for other local groups.\n\n\n\n\n"}
{"id": "953193", "url": "https://en.wikipedia.org/wiki?curid=953193", "title": "List of communications satellite firsts", "text": "List of communications satellite firsts\n\nMilestones in the history of communications satellites.\n"}
{"id": "20783445", "url": "https://en.wikipedia.org/wiki?curid=20783445", "title": "Localizer performance with vertical guidance", "text": "Localizer performance with vertical guidance\n\nLocalizer performance with vertical guidance (LPV) are the highest precision GPS (WAAS enabled) aviation instrument approach procedures currently available without specialized aircrew training requirements, such as required navigation performance (RNP). Landing minima are usually similar to those of a Cat I instrument landing system (ILS), that is, a decision height of and visibility of 800 m. Lateral guidance is equivalent to a localizer, and uses a ground-independent electronic glide path. Thus, the Decision Altitude, DA, can be as low as 200 feet. An LPV approach is an approach with vertical guidance, APV, to distinguish it from a precision approach, PA, or a non-precision approach, NPA. WAAS criteria includes a vertical alarm limit more than 12 m, but less than 50 m, yet an LPV does not meet the ICAO Annex 10 precision approach standard. \n\nExamples of receivers providing LPV capability include (from Garmin) the GTN 7xx & 6xx, GNS 480, GNS 430W & 530W, and the post 2007 Garmin G1000 with GIA 63W.\nVarious FMS models, GNSS receivers and FMS upgrades are available from Rockwell Collins (e.g.). Most new aircraft and helicopters equipped with integrated flight decks such as Rockwell Collins ProLine (TM) 21 and ProLine Fusion (TM) are LPV-capable. In 2014, Avidyne began equipping general aviation and business aircraft with the IFD540 and IFD440 Navigators incorporating a touch-screen Flight Management System with full LPV capability.\n\nLPV is designed to provide lateral and vertical accuracy 95 percent of the time. Actual performance has exceeded these levels. WAAS has never been observed to have a vertical error greater than 12 metres in its operational history.\nAs of September 17, 2015 the Federal Aviation Administration has published 3,567 LPV approaches at 1,739 airports. This is greater than the number of published Category I ILS procedures.\n\n\n"}
{"id": "4294169", "url": "https://en.wikipedia.org/wiki?curid=4294169", "title": "Magnetic detector", "text": "Magnetic detector\n\nThe magnetic detector or Marconi magnetic detector, sometimes called the \"Maggie\", was an early radio wave detector used in some of the first radio receivers to receive Morse code messages during the wireless telegraphy era around the turn of the 20th century. Developed in 1902 by radio pioneer Guglielmo Marconi from a method invented in 1895 by New Zealand physicist Ernest Rutherford it was used in Marconi wireless stations until around 1912, when it was superseded by vacuum tubes. It was widely used on ships because of its reliability and insensitivity to vibration. A magnetic detector was part of the wireless apparatus in the radio room of the RMS \"Titanic\" which was used to summon help during its famous 15 April 1912 sinking.\n\nThe primitive spark gap radio transmitters used during the first three decades of radio (1886-1916) could not transmit audio (sound) and instead transmitted information by wireless telegraphy; the operator switched the transmitter on and off with a telegraph key, creating pulses of radio waves to spell out text messages in Morse code. So the radio receiving equipment of the time did not have to convert the radio waves into sound like modern receivers, but merely detect the presence or absence of the radio signal. The device that did this was called a detector. The first widely used detector was the coherer, invented in 1890.\n\nErnest Rutherford had first used the hysteresis of iron to detect Hertzian waves in 1896 by the demagnetization of an iron needle when a radio signal passed through a coil around the needle, however the needle had to be remagnetized so this was not suitable for a continuous detector. Many other wireless researchers such as E. Wilson, C. Tissot, Reginald Fessenden, John Ambrose Fleming, Lee De Forest, J.C. Balsillie, and L. Tieri had subsequently devised detectors based on hysteresis, but none had become widely used due to various drawbacks. Many earlier versions had a rotating magnet above a stationary iron band with coils on it. This type was only periodically sensitive, when the magnetic field was changing, which occurred as the magnetic poles passed the iron.\n\nDuring his transatlantic radio communication experiments in December 1902 Marconi found the coherer to be too unreliable and insensitive for detecting the very weak radio signals from long distance transmissions. It was this need that drove him to develop his magnetic detector. Marconi devised a more effective configuration with a moving iron band driven by a clockwork motor passing by stationary magnets and coils, resulting in a continuous supply of iron that was changing magnetization, and thus continuous sensitivity (Rutherford claimed he had also invented this configuration). The Marconi magnetic detector was the \"official\" detector used by the Marconi Company from 1902 through 1912, when the company began converting to the Fleming valve and Audion-type vacuum tubes. It was used through 1918.\n\nSee drawing at right. The Marconi version consisted of an endless iron band (\"B\") built up of 70 strands of number 40 gage silk-covered iron wire. In operation, the band passes over two grooved pulleys rotated by a wind-up clockwork motor. The iron band passes through the center of a glass tube which is close wound with a single layer along several millimeters with number 36 gage silk-covered copper wire. This coil (\"C\") functions as the radio frequency excitation coil. Over this winding is a small bobbin wound with wire of the same gauge to a resistance of about 140 ohms. This coil (\"D\") functions as the audio pickup coil. Around these coils two permanent horseshoe magnets are arranged to magnetize the iron band as it passes through the glass tube.\n\nThe device works by hysteresis of the magnetization in the iron wires. The permanent magnets are arranged to create two opposite magnetic fields each directed toward (or away) from the center of the coils in opposite directions along the wire. This functions to magnetize the iron band along its axis, first in one direction as it approaches the center of the coils, then reverse its magnetism to the opposite direction as it leaves from the other side of the coil. Due to the hysteresis (coercivity) of the iron, a certain threshold magnetic field (the coercive field, \"H\") is required to reverse the magnetization. So the magnetization in the moving wires does not reverse in the center of the device where the field reverses, but some way toward the departing side of the wires, when the field of the second magnet reaches \"H\". Although the wire itself is moving through the coil, in the absence of a radio signal the location where the magnetization \"flips\" is stationary with respect to the pickup coil, so there is no flux change and no voltage is induced in the pickup coil.\n\nThe radio signal from the antenna (\"A\") is received by a tuner (\"not shown\") and passed through the excitation coil \"C\", the other end of which is connected to ground (\"E\"). The rapidly reversing magnetic field from the coil exceeds the coercivity \"H\" and cancels the hysteresis of the iron, causing the magnetization change to suddenly move up the wire to the center, between the magnets, where the field reverses. This had an effect similar to thrusting a magnet into the coil, causing the magnetic flux through the pickup coil \"D\" to change, inducing a current pulse in the pickup coil. The audio pickup coil is connected to a telephone receiver (earphone) (\"T\") which converts the current pulse to sound.\n\nThe radio signal from a spark gap transmitter consisted of pulses of radio waves (damped waves) which repeated at an audio rate, around several hundred per second. Each pulse of radio waves produced a pulse of current in the earphone, so the signal sounded like a musical tone or buzz in the earphone.\n\nThe iron band was turned by a mainspring and clockwork mechanism inside the case. Differing values have been given for the speed of the band, from 1.6 to 7.5 cm per second; the device could probably function over a wide range of band speeds. The operator had to keep the mainspring wound up, using a crank on the side. Operators would sometimes forget to wind it, so the band would stop turning and the detector stop working, sometimes in the middle of a radio message.\n\nThe detector produced electronic noise that was heard in the earphone as a \"hissing\" or \"roaring\" sound in the background, somewhat fatiguing to listen to. This was Barkhausen noise due to the Barkhausen effect in the iron. As the magnetic field in a given area of the iron wire changed as it moved through the detector, the microscopic domain walls between magnetic domains in the iron moved in a series of jerks, as they got hung up on defects in the iron crystal lattice, then pulled free. Each jerk produced a tiny change in the magnetic field through the coil, and induced a pulse of noise.\n\nBecause the output was an audio alternating current and not a direct current, the detector could only be used with earphones and not with the common recording instrument used in coherer radiotelegraphy receivers, the siphon paper tape recorder.\n\nFrom a technical standpoint, several subtle prerequisites are necessary for operation. The strength of the magnetic field of the permanent magnets at the iron band must be of the same order of magnitude as the strength of the field generated by the radio frequency excitation coil, allowing the radio frequency signal to exceed the threshold hysteresis (coercivity) of the iron. Also, the impedance of the tuner that supplies the radio signal must be low to match the low impedance of the excitation coil, requiring special tuner design considerations. The impedance of the telephone earphone must roughly match the impedance of the audio pickup coil, which is a few hundred ohms. The iron band moves a few millimeters per second. The magnetic detector was much more sensitive than the coherers commonly in use at the time, although not as sensitive as the Fleming valve, which began to replace it around 1912.\n\nIn the \"Handbook Of Technical Instruction For Wireless Telegraphists\" by: J. C. Hawkhead (Second Edition Revised by H. M. Dowsett) on pp 175 are detailed instructions and specifications for operation and maintenance of Marconi's magnetic detector.\n\n"}
{"id": "39726497", "url": "https://en.wikipedia.org/wiki?curid=39726497", "title": "Makeup brush", "text": "Makeup brush\n\nA makeup brush is a tool with bristles, used for the application of makeup or face painting. The bristles may be made out of natural or synthetic materials, while the handle is usually made out of plastic or wood. When cosmetics are applied using the appropriate brush, they blend better into the skin.\n\nThere is a large variety of shapes and sizes of makeup brushes, depending on the face area where makeup will be applied, the cosmetic product and the wanted result. For example, the shape of the brush tip can be chiseled, straight, angular, round, flat or tapered.\n\n\n\n\nMakeup application can alter due to the materials used to create a brush. The bristles of a makeup brush can either be synthetic or natural. The brush itself is important to the application of makeup as it can densely pack on product or loosely pack on product.\n\nSynthetic bristles are the most common material used in makeup brushes. These brushes are widely found in drugstores and makeup-specific stores. The bristles are made out of plastic, nylon, or other synthetic fibers, and may be dyed. Synthetic bristles are often used with liquid and cream products, as they tend to blend out products more easily and will not absorb product as much as a natural bristle brush. Synthetic brushes are cruelty-free. Synthetic brushes usually last longer than natural haired bristles as they do not degrade and are not as fragile.\n\nThe natural bristles are often dark in color, not super soft, sheds, hold pigments better and difficult to wash. As the natural bristles are very porous they pick up more pigments and distributes them evenly. The natural bristled brushes best applies powder products and it is best to avoid liquid or cream products as they will drink up most of the products. Although natural bristles are more preferred in the cosmetic industry, the bristles themselves can cause allergic reactions to the animal hair. \n\nNatural bristles may more easily control the product when blending out or even packing on a product, because they have hair cuticles.\n\n"}
{"id": "3625943", "url": "https://en.wikipedia.org/wiki?curid=3625943", "title": "Moondial", "text": "Moondial\n\nMoondials are time pieces similar to a sundial. The most basic moondial, which is identical to a sundial, is only accurate on the night of the full moon. Every night after it becomes an additional (on average) 48 minutes slow, while every night preceding the full moon it is (again on average) 49 minutes fast, assuming there is even enough light to take a reading by. Thus, one week to either side of the full moon the moondial will read 5 hours and 36 minutes before or after the proper time. \n\nMore advanced moondials can include charts showing the exact calculations to get the correct time, as well as dials designed with latitude and longitude in mind.\n\nMoondials are very closely associated with lunar gardening (night-blooming plants) and some comprehensive gardening books may mention them.\n\n\n"}
{"id": "49974099", "url": "https://en.wikipedia.org/wiki?curid=49974099", "title": "Moving bed biofilm reactor", "text": "Moving bed biofilm reactor\n\nMoving bed biofilm reactor (MBBR) is a type of wastewater treatment process that was first invented by Prof. Hallvard Ødegaard at Norwegian University of Science and Technology in the late 1980s. It was commercialized by Kaldnes Miljöteknologi (now called AnoxKaldnes and owned by Veolia Water Technologies). There are over 700 wastewater treatment systems (both municipal and industrial) installed in over 50 countries. Currently, there are various suppliers of MBBR systems.\n\nThe MBBR system consists of an aeration tank (similar to an activated sludge tank) with special plastic carriers that provide a surface where a biofilm can grow. The carriers are made of a material with a density close to the density of water (1 g/cm). An example is high-density polyethylene (HDPE) which has a density close to 0.95 g/cm. The carriers will be mixed in the tank by the aeration system and thus will have good contact between the substrate in the influent wastewater and the biomass on the carriers.\n\nTo prevent the plastic carriers from escaping the aeration it is necessary to have a sieve on the outlet of the tank.\n\nThe MBBR system is considered a biofilm process. Other conventional biofilm processes for wastewater treatment are called trickling filter, rotating biological contactor (RBC) and biological aerated filter (BAF). Biofilm processes in general require less space than activated sludge systems because the biomass is more concentrated, and the efficiency of the system is less dependent on the final sludge separation. A disadvantage with other biofilm processes is that they experience bioclogging and build-up of headloss. \n\nMBBR systems don't need a recycling of the sludge, which is the case with activated sludge systems.\n\nThe MBBR system is often installed as a retrofit of existing activated sludge tanks to increase the capacity of the existing system. The degree of filling of carriers can be adapted to the specific situation and the desired capacity. Thus an existing treatment plant can increase its capacity without increasing the footprint by constructing new tanks.\n\nWhen constructing the filling degree can be set to, for example, 40% in the beginning, and later be increased to 70% by filling more carriers. Examples of situations can be population increase in a city for a municipal wastewater treatment plant or increased wastewater production from an industrial factory.\n\nSome other advantages compared to activated sludge systems are:\n\n\n"}
{"id": "44801986", "url": "https://en.wikipedia.org/wiki?curid=44801986", "title": "Mr. Robot", "text": "Mr. Robot\n\nMr. Robot is an American drama thriller television series created by Sam Esmail. It stars Rami Malek as Elliot Alderson, a cybersecurity engineer and hacker who has social anxiety disorder and clinical depression. Elliot is recruited by an insurrectionary anarchist known as \"Mr. Robot\", played by Christian Slater, to join a group of hacktivists called \"fsociety\". The group aims to destroy all debt records by encrypting the financial data of the largest conglomerate in the world, E Corp.\n\nThe pilot premiered on multiple online and video on demand services on May 27, 2015. The first season premiered on USA Network on June 24, 2015, and the second on July 13, 2016. The 10-episode third season premiered on October 11, 2017. In December 2017, \"Mr. Robot\" was renewed for a fourth season. In August 2018, it was confirmed that the fourth season, to air in 2019 and consisting of 12 episodes, will be the final season.\n\n\"Mr. Robot\" has received critical acclaim and has been nominated for and won multiple awards, including the Golden Globe for Best Television Drama Series and was recognized with a Peabody Award. In 2016, the series received six Emmy nominations, including Outstanding Drama Series with Malek winning for Outstanding Lead Actor in a Drama Series.\n\nThe series follows Elliot Alderson, a young man living in New York City, who works at the cyber security company Allsafe as a security engineer. Constantly struggling with social anxiety disorder, dissociative identity disorder and clinical depression, Elliot's thought process seems heavily influenced by paranoia and delusion. He connects to people by hacking them, which often leads him to act as a cyber-vigilante. He is recruited by a mysterious insurrectionary anarchist known as Mr. Robot and joins his team of hacktivists known as \"fsociety\". One of their missions is to cancel all consumer debt by destroying the data of one of the largest corporations in the world, E Corp (which Elliot perceives as Evil Corp), which also happens to be Allsafe's biggest client.\n\n\n\n\n\n\nEsmail, the show’s creator and showrunner, is the credited writer on the vast majority of the episodes. According to Sam Esmail, he is fascinated by the hacker culture and wanted to make a film about it for around 15 years. In the production, Esmail consulted experts to give a realistic picture of hacking activities. Another inspiration for Esmail, who is of Egyptian descent, was the Arab Spring, where young people who were angry at society used social media to bring about a change.\n\nSam Esmail had originally intended \"Mr. Robot\" to be a feature film, with the end of the first act being someone finding out that he had a mental disorder while enacting a greater scheme. However, midway through writing the first act, he found that the script had expanded considerably, and that it had become a script more suited for a television show. He removed 20 of around 89 pages of the script then written, and used it as the pilot for the series, and what was to have been the end of the first act became the finale of the first season. Esmail took the script to film and television production company Anonymous Content to see if it could be developed into a television series, which was then picked up by USA Network. USA Network gave a pilot order to \"Mr. Robot\" in July 2014, and picked it up to series with a 10-episode order in December 2014. Production began in New York on April 13, 2015. The pilot premiered on multiple online and video on demand services on May 27, 2015, and the series was renewed for a second season before the first season premiered on USA on June 24, 2015. In December 2015, it was announced that Esmail would direct all episodes in the second season. In June 2016, it was announced that the second season's episode order was increased from 10 to 12 episodes. The 12-episode second season premiered on July 13, 2016. On August 16, 2016, USA renewed \"Mr. Robot\" for a third season to air in 2017. The third season debuted in October 2017 and consisted of 10 episodes. All episodes were directed by Sam Esmail, just as in the second season. On December 13, 2017, USA renewed \"Mr. Robot\" for a fourth season.\n\nTo portray the unusual, often confused worldview of lead character Elliot Alderson, Franklin Peterson, who edited three \"Mr. Robot\" episodes in the first season and six in the second season, used creative editing styles that included jump cuts, varied lengths of takes and shuffling scenes around within an episode and sometimes even between episodes. Esmail encouraged the experimentation as Peterson and his team explored the personality of each character in the editing suite, finding creative ways to tell their stories and maintain their humanity.\n\n\"Mr. Robot\" has been widely praised for its technical accuracy by numerous cyber security firms and services such as Avast, Panda Security, Avira, Kaspersky, ProtonMail, and bloggers who dissect and comment on the technology and the technical aspects of the show after every episode. Aside from the pilot episode, Esmail hired Kor Adana (former network security analyst and forensics manager for Toyota Motor Sales), Michael Bazzell (Security consultant and former FBI Cyber Crimes Task Force agent and investigator) and James Plouffe (lead solutions architect at MobileIron) as his advisors to oversee the technical accuracy of the show. By the second season, Adana assembled a team of hackers and cyber security experts including Jeff Moss (founder and director of Black Hat and DEF CON computer security conferences), Marc Rogers (Principal security researcher for Cloudflare and head of security for DEF CON), Ryan Kazanciyan (Chief security architect for Tanium) and Andre McGregor (Director of security for Tanium and former FBI Cyber Special agent) to assist him with the authenticity of the hacks and the technology being used.\nHacking scenes are then performed by members of the technical team in real life, recorded and rebuilt using Flash animation. Animation process is carried out by animator Adam Brustein under the direct supervision of Kor Adana himself.\n\nSam Esmail has acknowledged several major influences on the show, such as \"American Psycho\", \"Taxi Driver\", \"A Clockwork Orange\", and \"The Matrix\". In particular, Esmail credited \"Fight Club\" as the inspiration for a main character who has dissociative identity disorder creating a new manifestation of his deceased father in the form of a hacker, as well as for the anti-consumerist, anti-establishment, and anti-capitalism spirit of its characters. Commentators have also noted the parallel in its plot on the erasing of consumer debt records to the film. In an interview, Esmail explains how playing the song that David Fincher used to underscore the climax of \"Fight Club\" (\"Where Is My Mind?\") when Elliot initiates the hack in episode nine is intended as a message to the audience that he is aware of the inspiration they took from the film. The narration by the protagonist was influenced by \"Taxi Driver\", and other influences mentioned included \"Risky Business\" in its music score, \"Blade Runner\" for the character development, and the television series \"Breaking Bad\" for the story arc.\n\nThe series is filmed in New York. Filming locations include Silvercup Studios and Coney Island, which serves as the exterior of the base of operations for the hacking group fsociety. As the production crew was unable to shut down Times Square for filming, the scenes at Times Square in the first-season finale were shot late at night just before the 4th of July holiday weekend to catch the area at its emptiest while other shots were done on sets. Production on the second season began on March 7, 2016, resuming filming in New York City.\n\nIn June 2016, USA Network announced \"Hacking Robot\", a live aftershow hosted by Andy Greenwald to air during the second season. \"Hacking Robot\" debuted after the second-season premiere and aired again following the tenth episode. In addition, a weekly web-only aftershow titled \"Mr. Robot Digital After Show\" premiered on \"The Verge\" and USA Network's websites after the third episode, and has continued through the third season.\n\nThe first season of \"Mr. Robot\" received critical acclaim. On Rotten Tomatoes, it has a rating of 98%, based on 57 reviews, with an average rating of 8.36/10. The site's consensus reads, \"\"Mr. Robot\" is a suspenseful cyber-thriller with timely stories and an intriguing, provocative premise.\" It set a record on Rotten Tomatoes as the only show to earn perfect episode scores for an entire season since the site began tracking television episodes. On Metacritic, the first season scored 79 out of 100, based on 24 critics, indicating \"generally favorable reviews\".\n\nMerrill Barr of \"Forbes\" gave it a very positive review, writing, \"\"Mr. Robot\" has one of the best kick-offs to any series in a while\" and that it \"could be the series that finally, after years of ignorance, puts a deserving network among the likes of HBO, AMC and FX in terms of acclaim.\"\n\nIn \"The New York Times\", Alessandra Stanley noted that \"Occupy Wall Street, the protest movement that erupted in 2011, didn’t do much to curb the financial industry. It didn’t die out, though. It went Hollywood\", before finding \"Mr. Robot\" to be, \"an intriguing new series ... a cyber-age thriller infused with a dark, almost nihilistic pessimism about the Internet, capitalism and income inequality. And that makes it kind of fun\". The UK \"The Daily Telegraph\" reviewer Michael Hogan gave the show five stars, finding it to be \"\"The Matrix\" meets \"Fight Club\" meets \"Robin Hood\"\", noting that, \"bafflingly, it took months for a UK broadcaster to snap up the rights\". Although Hogan found too much attention was devoted to Elliot's social anxiety, he eventually decided that \"this alienated anti-hero was a brilliant, boldly complex character.\" Overall, Hogan concluded that the show deserved to find an audience in the UK.\n\n\"Mr. Robot\" made several critics' list for the best TV shows of 2015. Three critics, Jeff Jensen of \"Entertainment Weekly\", Rob Sheffield of \"Rolling Stone\", and the staff of \"TV Guide\", named it the best show of the year. The series also placed second on the list from three other critics, and was named among the best of the year from four other critics.\n\nThe second season also received critical acclaim. On Rotten Tomatoes, it has a score of 92%, based on 35 reviews, with an average rating of 8.05/10. The site's consensus reads: \"Unique storytelling, a darker tone, and challenging opportunities for its tight cast push \"Mr. Robot\" even further into uncharted television territory.\" On Metacritic, it has a score of 81 out of 100, based on 28 critics, indicating \"universal acclaim\".\n\nSonia Saraiya of \"Variety\" praised Rami Malek's performance and wrote, \"it's Malek's soulful eyes and silent pathos that give \"Mr. Robot\" its unexpected warmth, as the viewer is lured into Elliot's chaos and confusion. Tim Goodman of \"The Hollywood Reporter\" lauded Sam Esmail's direction, writing \"Esmail's camerawork—characters tucked into corners of the frame, among other nontraditional compositions—continues to give the sense of disorientation and never feels tired\" and \"there are some flourishes in the first two hours that are brilliantly conceived and [...] contribute to what is one of the most visually remarkable hours on television.\"\n\nThe third season also received critical acclaim. On Rotten Tomatoes, it has a score of 92%, based on 17 reviews, with an average rating of 8.25/10. On Metacritic, it has a score of 82 out of 100, based on 9 critics, indicating \"universal acclaim\".\n\nBased on six episodes for review, Darren Franich of \"Entertainment Weekly\" gave it an \"A\" grade, calling it a \"noir masterpiece\", and overall, wrote that \"Season 3 of \"Mr. Robot\" is a masterpiece, ballasting the global ambitions of season 2 while sharpening back to the meticulous build of season 1.\"\n\n\"Mr. Robot\" has spawned a variety of video games. Its mobile game, titled: \"Mr. Robot:1.51exfiltrati0n.apk\" is set during the first season of the series and was published by Telltale Games. Users play as a fictional character who has stumbled upon the phone of an important member of fsociety and it's up to the player to assist them in bringing down E Corp. The \"Mr. Robot\" Virtual Reality Experience, written and directed by Sam Esmail, is a 13-minute video viewable using virtual reality headsets that explores Elliot's past. \"Mr. Robot\" also features several Easter eggs, including websites related to the show or IP addresses used within the series that redirect to real websites.\n\nA book tie-in, \"Mr. Robot: Red Wheelbarrow (eps1.91_redwheelbarr0w.txt)\" written by Sam Esmail and Courtney Looney, was released on November 1, 2016.\n\n\"Mr. Robot\" premiered in the United States on USA Network on June 24, 2015, and in Canada on Showcase on September 4, 2015. Amazon.com secured broadcasting rights in the United Kingdom, with the first season added on Amazon Prime on October 16, 2015, and second-season episodes to be released immediately after initial broadcast in the United States. In Australia, the series premiered on Presto on August 14, 2015.\n\nUniversal Studios Home Entertainment released the first three seasons on DVD and Blu-ray. The first season was released on January 12, 2016. It contains all 10 episodes, plus deleted scenes, gag reel, making-of featurette, and UltraViolet digital copies of the episodes. The second season was released on January 10, 2017. The third season was released on March 27, 2018.\n\nIn September 2015, Amazon.com acquired exclusive streaming VOD rights to \"Mr. Robot\" in several countries, with the first season becoming available to stream in June 2016 for U.S. Amazon Prime subscribers.\n\n"}
{"id": "3792964", "url": "https://en.wikipedia.org/wiki?curid=3792964", "title": "NOx adsorber", "text": "NOx adsorber\n\nA adsorber or trap (also called Lean trap, abbr. LNT) is a device that is used to reduce oxides of nitrogen (NO and NO) emissions from a lean burn internal combustion engine by means of adsorption.\n\nA adsorber is designed to reduce oxides of nitrogen emitted in the exhaust gas of a lean burn internal combustion engine. Lean burn engines, particularly diesels, present a special challenge to emission control system designers because of the relatively high levels of O (atmospheric oxygen) in the exhaust gas. The 3-way catalytic converter that has been successfully used since the 1980s on stoichiometric engines (such as fueled by petrol, LPG, CNG, or ethanol) will not function at O levels in excess of 1.0%, and does not function well at levels above 0.5%. Because of the increasing need to limit emissions from diesel engines, technologies such as exhaust gas recirculation (EGR) and selective catalytic reduction (SCR) have been used, however EGR is of limited effectiveness and SCR requires a continuous supply of reductant to the exhaust.\n\nThe adsorber was designed to avoid the problems that EGR and SCR experienced. An adsorbant such as zeolite traps the NO and NO molecules — acting as a molecular sponge. Once the trap is full (like a sponge full of water) no more can be absorbed. Various schemes have been designed to \"purge\" or \"regenerate\" the trap. One of possible reactants used to this aim is diesel fuel. Injection of diesel fuel before the adsorber can purge it — the is made to desorb and react with hydrocarbons under rich conditions to produce water and nitrogen. Also hydrogen is a good reductant, but is dangerous and difficult to store. Some experimental systems have used fuel reformers for on-board hydrogen generation.\n\nA trap is used on the Volkswagen Jetta TDI and the Volkswagen Tiguan concepts. Both are projected to be introduced into the American market by 2008. They were to be marketed as part of the BlueTec program from Audi, Daimler-Chrysler, and Volkswagen.\n\nIn 2015 an ADAC study (ordered by ICCT) of 32 Euro6 cars showed that few complied with on-road emission limits, and LNT cars had the highest emissions.\n\nThe adsorber is based on a monolithic catalyst support that has been coated with a adsorbing washcoat such as one containing zeolites. Alkali/alkaline oxide (carbonate) can also be used as the adsorbant.\n\nTraps are gradually poisoned by SO which adsorbs more strongly than . It necessitates a periodic high temperature regeneration that tends to reduce the adsorber's operating life.\n"}
{"id": "9921347", "url": "https://en.wikipedia.org/wiki?curid=9921347", "title": "Pointer aliasing", "text": "Pointer aliasing\n\nIn computer programming, aliasing refers to the situation where the same memory location can be accessed using different names.\n\nFor instance, if a function takes two pointers codice_1 and codice_2 which have the same value, then the name codice_3 aliases the name codice_4. In this case we say the pointers codice_1 and codice_2 alias each other. Note that the concept of pointer aliasing is not very well-defined – two pointers codice_1 and codice_2 may or may not alias each other, depending on what operations are performed in the function using codice_1 and codice_2.\n\nAliasing introduces strong constraints on program execution order. If two write accesses which alias occur in sequence in a program text, they must occur in sequence in the final machine code. Re-ordering the accesses will produce an incorrect program result (in the general case). The same is true for a write access and a read access.\n\nHowever, if two read accesses which alias occur in sequence in a program text, they need not occur in the same sequence in the machine code - aliasing read accesses are safe to re-order. This is because the underlying data is not changed by the read operation, so the same value is always read.\n\nIt is vitally important that a compiler can detect which accesses may alias each other, so that re-ordering optimizations can be performed correctly.\n\nSeveral strategies are possible:\n\nIn C or C++, as mandated by the strict aliasing rule, pointer arguments in a function are assumed not to alias if they point to fundamentally different types, except for codice_11 and codice_12, which may alias to any other type. Some compilers allow the strict aliasing rule to be turned off, so that any pointer argument may alias any other pointer arguments. In this case, the compiler must assume that any accesses through these pointers can alias. This can prevent some optimizations.\n\nIn C99, the codice_13 keyword was added, which specifies that a pointer argument does not alias any other pointer argument.\n\nIn Fortran, procedure arguments and other variables may not alias each other (unless they are pointers or have the target attribute), and the compiler assumes they do not. This enables excellent optimization, and is one major reason for Fortran's reputation as a fast language. (Note that aliasing may still occur within a Fortran function. For instance, if codice_1 is an array and codice_15 and codice_16 are indices which happen to have the same value, then codice_17 and codice_18 are two different names for the same memory location. Fortunately, since the base array must have the same name, index analysis can be done to determine cases where codice_17 and codice_18 cannot alias.)\n\nIn pure functional languages, function arguments may usually alias each other, but all pointers are read-only. Thus, no alias analysis needs to be done.\n\nIf multiple threads have names which alias the same memory location, two problems arise.\n\nFirst, unless the memory location is protected in some way, then read-modify-write operations which are non-atomic might be interleaved with similar operations on another thread, producing incorrect results. In this case, some form of synchronization primitive must be used (e.g. a critical section).\n\nSecond, even with synchronization, if two aliasing accesses occur in different threads in a non-ordered way, the program behaviour may become non-deterministic - different runs of the same program on the same data may produce different results. Thus, it is often important to impose an explicit ordering between threads which have aliases.\n\n"}
{"id": "4578757", "url": "https://en.wikipedia.org/wiki?curid=4578757", "title": "Power module", "text": "Power module\n\nA power module or power electronic module provides the physical containment for several power components, usually power semiconductor devices. These power semiconductors (so-called dies) are typically soldered or sintered on a power electronic substrate that carries the power semiconductors, provides electrical and thermal contact and electrical insulation where needed. Compared to discrete power semiconductors in plastic housings as TO-247 or TO-220, power packages provide a higher power density and are in many cases more reliable.\n\nBesides modules that contain a single power electronic switch (as MOSFET, IGBT, BJT, Thyristor, GTO or JFET) or diode, classical power modules contain multiple semiconductor dies that are connected to form an electrical circuit of a certain structure, called topology. Modules also contain other components such as ceramic capacitors to minimize switching voltage overshoots and NTC thermistors to monitor the module's substrate temperature. Examples of broadly available topologies implemented in modules are:\n\nAdditional to the traditional screw contacts the electrical connection between the module and other parts of the power electronic system can also be achieved by pin contacts (soldered onto a PCB), press-fit contacts pressed into PCB vias, spring contacts that inherently press on contact areas of a PCB or by pure pressure contact where corrosion-proof surface areas are directly pressed together.\nPress-fit pins achieve a very high reliability and ease the mounting process without the need for soldering. Compared to press-fit connections, spring contacts have the benefit of allowing easy and non-destructive removal of the connection several times, as for inspection or replacement of an module, for instance. Both contact types have rather limited current-carrying capability due to their comparatively low cross-sectional area and small contact surface. Therefore, modules often contain multiple pins or springs for each of the electrical power connections.\n\nThe current focus in R&D is on cost reduction, increase of power density, increase of reliability and reduction of parasitic lumped elements. These parasitics are unwanted capacitances between circuit parts and inductances of circuit traces. Both can have negative effects on the electromagnetic radiation (EMR) of the module if it is operated as an inverter, for instance. Another problem connected to parasitics is their negative impact on the switching behavior and the switching loss of the power semiconductors. Therefore, manufacturers work on minimizing the parasitic elements of their modules while keeping cost low and maintain a high degree of interchangeability of their modules with those of a second source (other manufacturer).\nA further aspect for optimization is the so-called thermal path between the heat source (the dies) and the heat-sink. The heat has to pass through different physical layers as solder, DCB, baseplate, thermal interface material (TIM) and the bulk of the heat-sink, until it is transferred to a gaseous medium as air or a fluid medium as water or oil. Since modern silicon-carbide power semiconductors show a larger power density, the requirements for heat transfer are rising.\n\nPower modules are used for power conversion equipment such as industrial motor drives, embedded motor drives, uninterruptible power supplies, AC-DC power supplies and in welder power supplies.\n\nPower modules are also widely found in inverters for renewable energies as wind turbines, solar power panels, tidal power plants and electric vehicles (EVs).\n\nThe first potential-free power module was introduced into the market by Semikron in 1975. It is still in production, which gives an idea about the lifecycles of power modules.\n\n\n"}
{"id": "4248773", "url": "https://en.wikipedia.org/wiki?curid=4248773", "title": "Programmable system-on-chip", "text": "Programmable system-on-chip\n\nPSoC (programmable system-on-chip) is a family of microcontroller integrated circuits by Cypress Semiconductor. These chips include a CPU core and mixed-signal arrays of configurable integrated analog and digital peripherals.\n\nIn 2002, Cypress began shipping commercial quantities of the PSoC 1. To promote the PSoC, Cypress sponsored a \"PSoC Design Challenge\" in \"Circuit Cellar\" magazine in 2002 and 2004.\n\nIn April 2013, Cypress released the fourth generation, PSoC 4. The PSoC 4 features a 32-bit ARM Cortex-M0 CPU, with programmable analog blocks (operational amplifiers and comparators), programmable digital blocks (PLD-based UDBs), programmable routing and flexible GPIO (route any function to any pin), a serial communication block (for SPI, UART, I²C), a timer/counter/PWM block and more.\n\nPSoC is used in devices as simple as Sonicare toothbrushes and Adidas sneakers, and as complex as the TiVo set-top box. One PSoC, using CapSense, controls the touch-sensitive scroll wheel on the Apple iPod click wheel.\n\nIn 2014, Cypress extended the PSoC 4 family by integrating a Bluetooth Low Energy radio along with a PSoC 4 Cortex-M0-based SoC in a single, monolithic die.\n\nIn 2016, Cypress released PSoC 4 S-Series, featuring ARM Cortex-M0+ CPU.\n\nA PSoC integrated circuit is composed of a core, configurable analog and digital blocks, and programmable routing and interconnect. The configurable blocks in a PSoC are the biggest difference from other microcontrollers.\n\nPSoC has three separate memory spaces: paged SRAM for data, Flash memory for instructions and fixed data, and I/O registers for controlling and accessing the configurable logic blocks and functions. The device is created using SONOS technology.\n\nPSoC resembles an ASIC: blocks can be assigned a wide range of functions and interconnected on-chip. Unlike an ASIC, there is no special manufacturing process required to create the custom configuration — only startup code that is created by Cypress' \"PSoC Designer\" (for PSoC 1) or \"PSoC Creator\" (for PSoC 3 / 4 / 5) IDE.\n\nPSoC resembles an FPGA in that at power up it must be configured, but this configuration occurs by loading instructions from the built-in Flash memory.\n\nPSoC most closely resembles a microcontroller combined with a PLD and programmable analog. Code is executed to interact with the user-specified peripheral functions (called \"Components\"), using automatically generated APIs and interrupt routines. \"PSoC Designer\" or \"PSoC Creator\" generate the startup configuration code. Both integrate APIs that initialize the user selected components upon the users needs in a Visual-Studio-like GUI.\n\nUsing configurable analog and digital blocks, designers can create and change mixed-signal embedded applications. The digital blocks are state machines that are configured using the blocks registers. There are two types of digital blocks, Digital Building Blocks (DBBxx) and Digital Communication Blocks (DCBxx). Only the communication blocks can contain serial I/O user modules, such as SPI, UART, etc.\n\nEach digital block is considered an 8-bit resource that designers can configure using pre-built digital functions or user modules (UM), or, by combining blocks, turn them into 16-, 24-, or 32-bit resources. Concatenating UMs together is how 16-bit PWMs and timers are created.\n\nThere are two types of analog blocks. The continuous time (CT) blocks are composed of an op-amp circuit and designated as ACBxx where xx is 00-03. The other type is the switch cap (SC) blocks, which allow complex analog signal flows and are designated by ASCxy where x is the row and y is the column of the analog block. Designers can modify and personalize each module to any design.\n\nPSoC mixed-signal arrays' flexible routing allows designers to route signals to and from I/O pins more freely than with many competing microcontrollers. Global buses allow for signal multiplexing and for performing logic operations. Cypress suggests that this allows designers to configure a design and make improvements more easily and faster and with fewer PCB redesigns than a digital logic gate approach or competing microcontrollers with more fixed function pins.\n\nThere are five different families of devices, each based around a different microcontroller core:\n\nStarting in 2014, Cypress began offering PSoC 4 BLE devices with integrated Bluetooth Low Energy (Bluetooth Smart). This can be used to create connected products leveraging the analog and digital blocks. Users can add and configure the BLE module directly in PSoC creator. Cypress also provides a complete Bluetooth Low Energy stack licensed from Mindtree with both Peripheral and Central functionality. The PSoC 6 series includes versions with BLE including Bluetooth 5 features including extended range or higher speed.\n\nThis is the first generation software IDE to design and debug and program the PSoC 1 devices. It introduced unique features including a library of pre-characterized analog and digital peripherals in a drag-and-drop design environment which could then be customized to specific design needs by leveraging the dynamically generated API libraries of code.\n\nPSoC Creator is the second generation software IDE to design debug and program the PSoC 3 / 4 / 5 devices. The development IDE is combined with an easy to use graphical design editor to form a powerful hardware/software co-design environment. PSoC Creator consists of two basic building blocks. The program that allows the user to select, configure and connect existing circuits on the chip and the components which are the equivalent of peripherals on MCUs. What makes PSoC intriguing is the possibility to create own application specific peripherals in hardware. Cypress publishes component packs several times a year. PSoC users get new peripherals for their existing hardware without being charged or having to buy new hardware. PSoC Creator also allows much freedom in assignment of peripherals to I/O pins.\n\nGeneric ARM development tools for PSoC 4 and PSoC 5.\n\nThe amount of documentation for all ARM chips is daunting, especially for newcomers. The documentation for microcontrollers from past decades would easily be inclusive in a single document, but as chips have evolved so has the documentation grown. The total documentation is especially hard to grasp for all ARM chips since it consists of documents from the IC manufacturer (Cypress Semiconductor) and documents from CPU core vendor (ARM Holdings).\n\nA typical top-down documentation tree is: manufacturer website, manufacturer marketing slides, manufacturer datasheet for the exact physical chip, manufacturer detailed reference manual that describes common peripherals and aspects of a physical chip family, ARM core generic user guide, ARM core technical reference manual, ARM architecture reference manual that describes the instruction set(s).\n\n\nCypress Semiconductor has additional documents, such as: evaluation board user manuals, application notes, getting started guides, software library documents, errata, and more. See External Links section for links to official PSoC and ARM documents.\n\n\n\n"}
{"id": "13108226", "url": "https://en.wikipedia.org/wiki?curid=13108226", "title": "Radio occultation", "text": "Radio occultation\n\nRadio occultation (RO) is a remote sensing technique used for measuring the physical properties of a planetary atmosphere or ring system.\n\nAtmospheric radio occultation relies on the detection of a change in a radio signal as it passes through a planet's atmosphere, i.e. as it is occulted by the atmosphere. When electromagnetic radiation (light) passes through the atmosphere, it is refracted (or bent). The magnitude of the refraction depends on the gradient of refractivity normal to the path, which in turn depends on the density gradient. The effect is most pronounced when the radiation traverses a long atmospheric limb path. At radio frequencies the amount of bending cannot be measured directly; instead the bending can be calculated using the Doppler shift of the signal given the geometry of the emitter and receiver. The amount of bending can be related to the refractive index by using an Abel transform on the formula relating bending angle to refractivity. In the case of the neutral atmosphere (below the ionosphere) information on the atmosphere's temperature, pressure and water vapour content can be derived giving radio occultation data applications in meteorology.\n\nGNSS or GPS radio occultation (GNSS-RO, GPS-RO, GPSRO) is a type of radio occultation that relies on radio transmissions from GPS (Global Positioning System), or more generally from GNSS (Global Navigation Satellite System), satellites. This is a relatively new technique (first applied in 1995) for performing atmospheric measurements. It is used as a weather forecasting tool, and could also be harnessed in monitoring climate change. The technique involves a low-Earth orbit satellite receiving a signal from a GPS satellite. The signal has to pass through the atmosphere and gets refracted along the way. The magnitude of the refraction depends on the temperature and water vapor concentration in the atmosphere.\n\nGPS Radio occultation amounts to an almost instantaneous depiction of the atmospheric state. The relative position between the GPS satellite and the low-Earth orbit satellite changes over time, allowing for a vertical scanning of successive layers of the atmosphere.\n\nGPSRO observations can also be conducted from aircraft. or on high mountaintops.\n\nCurrent missions include:\n\n\n\n"}
{"id": "45232469", "url": "https://en.wikipedia.org/wiki?curid=45232469", "title": "Rivers State Ministry of Power", "text": "Rivers State Ministry of Power\n\nThe Rivers State Ministry of Power is a government ministry of Rivers State, Nigeria created in July 1999 to serve as the policy-formulating and implementing body for the state's electrical energy and power sector. The ministry's mission is to \"ensure that Rivers State meets its energy needs through a sustainable framework that will support the state’s economic growth and provide its citizenry with services that meets their expectations as a fast growing economy\". The ministry is headed by the Commissioner of Power, currently Augustine Wokocha.\n\nThe mandate of the Ministry of Power is: \"Create an enabling environment for rapid industrial and economic regeneration through quality and uninterrupted electricity supply.\"\n\n\n"}
{"id": "55334092", "url": "https://en.wikipedia.org/wiki?curid=55334092", "title": "Roger Kidner", "text": "Roger Kidner\n\nRoger Wakeley Kidner was a railway enthusiast and noted publisher whose imprint, The Oakwood Press, published many of the earliest books on British narrow-gauge railways.\n\nKidner was born on 16 March 1914, the son of civil servant Arthur, and Mabel. His love of railways stemmed from being given a few Locomotive Publishing Company postcards in primary school. He attended Westminster School where he struck up a friendship with Michael Robbins. The two bonded over a shared interest in railways, and in 1931, they founded The Four Os to publish a newsletter called \"Locomotion\". Both were still at school, and the company operated out of Kidner's parents' garage. \\\n\nIn 1935, Kidner and Robbins changed the name of their nascent publishing house to The Oakwood Press and published their first book, \"Railway Bibliography\" by Canon Fellows. This was followed in 1936 by L.T. Catchpole's \"The Lynton and Barnstaple Railway\" which is still in print in its 9th. edition. Meanwhile, after a year at the London School of Economics Kidner was working as an editor of travel guides for Benn Brothers. In 1938, Oakwood published the first train spotter's guide, called \"How to Recognise Southern Railway Locomotives\" written by Kidner.\n\nKidner travelled widely to research the railways that his authors wrote about. He visited the Lynton and Barnstaple Railway in 1935 with Catchpole, and the Welsh Highland Railway in 1926 and 1934.\n\nThe Oakwood Press suspended publication during the Second World War, and Kidner served in the Queen's Own Royal West Kent Regiment where he rose to the rank of Major. He also married Beryl Walton in 1943. After the war he resumed publishing, though was initially restricted by paper rationing. He published James I. C. Boyd's seminal series on the narrow-gauge railways of north Wales, starting in 1949 with \"Narrow Gauge Rails to Portmadoc\" which drew attention to the then-closed Ffestiniog Railway and was instrumental in its eventual restoration. Michael Robbins dropped out of the business in the 1950s.\n\nIn 1972, Kidner retired from his work in public relations to focus full-time on The Oakwood Press. He broadened the range of subjects covered, to include biographies of railwaymen and books about trams, traction engines, buses and canals. He sold The Oakwood Press in 1984, but kept in close contact with the new owner, writing and editing books until 2015. He died of cancer in 2007.\n\n"}
{"id": "13168213", "url": "https://en.wikipedia.org/wiki?curid=13168213", "title": "Rowan Companies", "text": "Rowan Companies\n\nRowan Companies plc, is an offshore drilling contractor that provides well drilling services to the petroleum industry. The company is incorporated in the United Kingdom and headquartered in Houston, Texas, in the United States.\n\nThe company operates a fleet 23 offshore drilling jackup rigs and 4 ultra-deepwater drillships and has 5 jackup rigs in its joint venture with Saudi Aramco. Its offshore jackup rigs include cantilever jackup rigs that are designated as Gorilla Class rigs, Super Gorilla Class rigs, Tarzan Class rigs, 240C Class Rigs, EXL class rigs and N-class rigs, as well as conventional jackup rigs. Its ultra-deepwater drillships are based on the Gusto MSC P10,000 class with capacity for 12,000’ of riser and a variable deck load capacity of 20,000 tons.\n\nIn 2017, 29% of the company's revenues came from Saudi Aramco, 17% of its revenues came from Anadarko Petroleum, 14% of its revenues came from Cobalt International Energy, 7% of its revenues came from each of Repsol and ConocoPhillips.\n\nRowan Drilling Company, Inc. was founded in 1923 by brothers Charles and Arch Rowan as a contract drilling business.\n\nThe company went public in 1967, issuing 323,000 shares.\n\nIn 1971, the company changed its name to Rowan Companies, Inc.\n\nIn 1994, the company acquired the assets of Marathon LeTourneau Company from General Cable for $52 million.\n\nIn 2000, Rowan subsidiary LeTourneau, Inc. acquired The Ellis Williams Company, Inc., a manufacturer of mud pumps, for $9 million.\n\nIn 2002, Rowan Electric, Inc. acquired Oilfield-Electric-Marine, a manufacturer of variable speed AC motors and variable frequency drive systems, for $8 million in stock.\n\nIn 2004, the company sold its aviation subsidiary business, Era Aviation, which chartered helicopter and fixed-wing services, for $118.1 million in cash to SEACOR Holdings.\n\nIn 2007, the company pleaded guilty to environmental violations caused by the Rowan-Midland drilling rig between 2002 and 2004 and agreed to pay $9 million in fines.\n\nIn 2009, W. Matt Ralls was named chief executive officer & president of the company.\n\nIn 2010, the company acquired Skeie Drilling & Production ASA, including its three North Sea compliant (N-class) jack-up rigs under construction.\n\nIn April 2012, the company changes its corporate structure to become a subsidiary of a company based in the United Kingdom.\n\nIn 2013, the company sold its manufacturing subsidiary, LeTourneau Technologies, Inc., and its land drilling operations to Joy Global in a $1.1 billion transaction. Joy Global subsequently sold LeTourneau's Drilling, Marine, and Power divisions to Cameron International for $375 million.\n\nIn April 2014, Thomas P. Burke was named chief executive officer & president of the company. Matt Ralls became Executive Chairman.\n\nIn October 2017, the company launched ARO Drilling, a 50/50 joint venture with Saudi Aramco.\n\nIn January 2018, the company acquired 2 rigs from Petrobras.\n"}
{"id": "5303809", "url": "https://en.wikipedia.org/wiki?curid=5303809", "title": "SHAZAM (interbank network)", "text": "SHAZAM (interbank network)\n\nThe SHAZAM Network, founded in 1976, is a United States national member-owned financial services and payments processing company.\n\nSHAZAM is a single-source provider of debit card, core, fraud, ATM, merchant, marketing, training, risk, and automated clearing house (ACH) services.\n\nThey are partnered with Presto!, a similar network covering the southeastern part of the US.\n\n\n"}
{"id": "18755320", "url": "https://en.wikipedia.org/wiki?curid=18755320", "title": "Shadow IT", "text": "Shadow IT\n\nShadow IT, also known as Stealth IT or Client IT, are Information technology (IT) systems built and used within organizations without explicit organizational approval, for example, systems specified and deployed by departments other than the IT department.\n\nMany people consider shadow IT an important source of innovation, and such systems may become prototypes for future approved IT solutions. On the other hand, shadow IT solutions are not often in line with organizational requirements for control, documentation, security, reliability, etc. \n\nShadow IT is any application or transmission of data, relied upon for business processes, that is not under the jurisdiction of a centralized IT or IS department. The IT department did not develop it or was not aware of it, and does not support it. This increases the likelihood of ‘unofficial’ and uncontrolled data flows, making it more difficult to comply with the Sarbanes-Oxley Act (USA) and many other compliance-centric initiatives, such as:\n\nExamples of these unofficial data flows include USB flash drives or other portable data storage devices, MSN Messenger or other online messaging software, Gmail or other online e-mail services, Google Docs or other online document sharing and Skype or other online VOIP software—and other less straightforward products: self-developed Access databases and self-developed Excel spreadsheets and macros. Security risks arise when data or applications move outside protected systems, networks, physical location, or security domains.\n\nA 2012 French survey of 129 IT managers revealed some examples of shadow IT :\n\nAnother form of shadow IT comes by way of OAuth connected applications, where a user authorizes access to a third-party application via a sanctioned application. For example, the user can use their Facebook credentials to log into Spotify, or another 3rd party application via their corporate cloud app (Google G Suite or Microsoft Office 365). With this access, the 3rd party app may have excessive access to the sanctioned app, thereby introducing up unintended risk.\n\nIncumbent IT management dealing with legacy infrastructure and data management challenges cannot easily provision data as a service either because they are unaware of its advantages, or cannot acquire the budget for its successful implementation. Against this background, neither can the IT department ever deliver against all business requirements at a low enough cost relative to a true DaaS IT department. These deficiencies lead the business to implement IT solutions that may be perceived to cost less to execute, albeit whilst introducing risks a formal IT project could avoid.\n\nFor example, with the rise of powerful desktop CPUs, business subject matter experts can use shadow IT systems to extract and manipulate complex datasets without having to request work from the IT department. The challenge for IT is to recognize this activity and improve the technical control environment, or to guide the business in selecting enterprise-class data analysis tools.\n\nA further barrier to adopting DaaS is the legacy IT bulk provisioning of only the 'Read' element of the CRUD model (Create, Read, Update, Delete). This leads IT into neglecting the need to 'write back' into the original dataset, because this is complex to achieve. It is the need of shadow IT users to then store this changed data separately (I.E. 'siloeing') that results in a loss of organisational data integrity.\n\nPlacing barriers to shadow IT can be the equivalent of stifling organizational innovation and cost reduction.\nA study confirms that 35% of employees feel they need to work around a security measure or protocol to work efficiently. 63% send documents to their home e-mail address to continue work from home, even when they are aware that this is probably not allowed.\n\nBesides security risks, some of the implications of Shadow IT are:\n\nShadow IT adds hidden costs to organizations, consisting largely of non-IT workers in finance, marketing, HR, etc., who spend a significant amount of time discussing and re-checking the validity of certain data, setting up and managing systems and software without experience.\n\nIf a ‘shadow IT’ spreadsheet application encapsulates its own definitions and calculations, it is likely that over time inconsistencies will arise from the accumulation of small differences from one version to another and from one group to another, as spreadsheets are often copied and modified. In addition, many errors that occur from either lack of understanding of the concepts or incorrect use of the spreadsheet frequently go undetected due to a lack of rigorous testing and version control.\n\nEven when the definitions and formulas are correct, the methodology for doing analysis can be distorted by the arrangement and flow of linked spreadsheets, or the process itself can be wrong.\n\nShadow IT applications sometimes prevent full Return on investment (ROI) from investments in systems that are designed to perform the functions now replaced by Shadow IT. This is often seen in Data warehousing (DW) and Business informatics (BI) projects, which are initiated with good intentions, where the broader and consistent usage of DW and BI in the organization never really starts off. This can also be caused by management failure to anticipate deployment, licensing and system capacity costs when attempting to deliver DW & BI solutions. Adopting an internal cost model that forces potential new users of the DW/BI system to choose cheaper (shadow) alternatives, also plays a part in preventing successful enterprise implementation.\n\nShadow IT can be a barrier to innovation by blocking the establishment of more efficient work processes. Additional performance bottlenecks and new single points of failure may be introduced when Shadow IT systems layer on top of existing systems. Data might be exported from a shared system to a spreadsheet to perform the critical tasks or analysis.\n\nShadow IT data backup procedures may not be provided or audited. Personnel and contractors in Shadow IT operations may not be put through normal education, procedures or vetting processes. Originators of Shadow IT systems may leave the organization often leaving with proprietary data or leaving behind complicated systems the remainder of staff cannot manage.\n\nShadow IT can act as a brake on the adoption of new technology. Because IT artifacts, e.g., spreadsheets, are deployed to fill critical needs, they must be replaced carefully. But lacking adequate documentation, controls and standards, that process is slow and error-prone.\n\nShadow IT creates a dysfunctional environment leading to animosity between IT and non-IT related groups within an organization. Improper motivations behind Shadow IT efforts such as seeking job-security (i.e., \"Bob is the only person with this data,\" or \"What will happen if he leaves?\"), data hoarding, self-promotion, favor trading, etc. can lead to significant management issues.\n\nA 2015 survey of over 400 global CIOs showed 90% of CIOs worldwide find themselves by-passed by line of business at least sometimes. One third (31%) of CIOs globally are routinely side-lined when it comes to making IT purchasing decisions.\n\nAccording to Gartner, by 2015, 35 percent of enterprise IT expenditures for most organizations will be managed outside the IT department's budget.\n\n"}
{"id": "29153757", "url": "https://en.wikipedia.org/wiki?curid=29153757", "title": "Shiply", "text": "Shiply\n\nShiply is a UK-based limited company providing an internet marketplace where transport service requesters may list items they need to move, and where providers of transport services can bid in a reverse auction format.\n\nThe concept employed by Shiply aims to utilise transport capacity that may be wasted in inefficient transport operations and as a consequence enables transport service providers to offer marginal prices to transport service requesters.\n\nService requesters create accounts and list the items to be transported, stipulating the maximum they are prepared to pay and specifying whether they can be flexible on delivery/pickup dates or need the listed items moved subject to specified date parameters. The transport service providers bid the lowest amount they would accept to perform the transport service. A user feedback system is enabled in order to keep track of the reputation of the transport service providers and transport service requesters, this allows the transport service requesters the ability to frame their procurement decision on factors other than solely price.\n\nRegistration is free for both transport service providers and transport service requesters. Transport service providers' bids are subject to a transaction based tiered fee structure, (in the UK this is between 3.9% and 9.9% -dependent on the amount of the transaction-with a minimum fee of £6) the cost of which is borne by the transport service requesters. Transport service requesters may also be subject to an auction service fee. This fee amount is dependent on the shipment in question and is displayed before the transport service requester accepts a bid.\n\nIn September 2010 Shiply had 17,500 transport service provider accounts, and helped voice their concerns over trade-related issues.\n\nShiply was founded in March 2008 by Robert Matthams, in Manchester. He has said he had the idea of this business concept when a pool table he ordered was delivered to his university in Manchester. The driver complained that he would have to make the return journey to London empty, thereby wasting fuel and his own resources.\n\nIn March 2009 Shiply launched an eBay widget to be inserted by sellers into eBay auction listings. This - now part of the item listing process - gives prospective eBay bidders the option to import items they have won - or on which they are currently bidding - into Shiply.com. The widget allows them to enter their delivery post code and then Shiply imports other pertinent information into their listing. In January 2011, Shiply signed an exclusive agreement with eBay Motors UK, becoming the site’s transport partner. At that time, 50% of its business came from eBay users.\n\nInitially, only haulage companies from UK participated in the marketplace. In October 2009, the company expanded in Germany, opening a German website and allowing German haulage companies. In August 2010, when the company expanded to France, Italy, the Netherlands and Spain, 15% of shipments on Shiply already crossed national borders and 10% of its business came from the German website.\n\nOn 24 February 2011 uShip and Shiply jointly announced a settlement regarding uShip's claims of trademark and copyright infringement. As part of the settlement, Shiply has agreed to redesign some parts of its web site, and pay an amount in compensation to uShip.\n\nOn 19 December 2012, UK's ASA published an adjudication against Shiply Ltd about misleading advertising on www.shiply.com.\nThe ASA investigated whether www.shiply.com made it sufficiently clear that a credit policy applied to cancellations. The ASA concluded:\n\n\"The home page breached CAP Code (Edition 12) rules 3.1 and 3.3 (Misleading advertising), 3.9 and 3.10 (Qualification).\"\n\"Because of Shiply Ltd’s continued non compliance we took the decision to place the details for www.shiply.com on the ASA section of the website on 14 October 2013. Shiply Ltd’s details shall remain in place until such time as www.shiply.com clearly and prominently states the significant condition of its cancellation policy on its home page.\"\n\nShiply Ltd's details are no longer on the ASA section of the website.\n\nShiply has been covered in the UK media as successful start-up business during recession times, as a provider of cheap shipping alternatives, and as an innovative idea for tackling climate change.\n\nIn 2010 Shiply was awarded the Shell Springboard \"Climate Change Innovation Prize\", and was runner-up in the Dutch Postcode Lottery’s 2010 Green Challenge competition. In 2012 Shiply won £50,000 in HSBC's Business Growth Grant Competition\n\n"}
{"id": "4160674", "url": "https://en.wikipedia.org/wiki?curid=4160674", "title": "Sight glass", "text": "Sight glass\n\nA sight glass or water gauge is a type of level sensor, a transparent tube through which the operator of a tank or boiler can observe the level of liquid contained within.\n\nSimple sight glasses may be just a plastic or glass tube connected to the bottom of the tank at one end and the top of the tank at the other. The level of liquid in the sight glass will be the same as the level of liquid in the tank. Today, however, sophisticated float switches have replaced sight glasses in many such applications.\n\nIf the liquid is hazardous or under pressure, more sophisticated arrangements must be made. In the case of a boiler, the pressure of the water below and the steam above is equal, so any change in the water level will be seen in the gauge. The transparent tube (the “glass” itself) may be mostly enclosed within a metal or toughened glass shroud to prevent it from being damaged through scratching or impact and offering protection to the operators in the case of breakage. This usually has a patterned backplate to make the magnifying effect of the water in the tube more obvious and so allow for easier reading. In some locomotives where the boiler is operated at very high pressures, the tube itself would be made of metal-reinforced toughened glass. It is important to keep the water at the specified level, otherwise the top of the firebox will be exposed, creating an overheat hazard and causing damage and possibly catastrophic failure.\n\nTo check that the device is offering a correct reading and the connecting pipes to the boiler are not blocked by scale, the water level needs to be “bobbed” by quickly opening the taps in turn and allowing a brief spurt of water through the drain cock.\n\nThe National Board of Boiler and Pressure Vessel Inspectors recommends a daily testing procedure described by the American National Standards Institute, chapter 2 part I-204.3 water level gauge. While not strictly required, this procedure is designed to allow an operator to safely verify that all parts of the sight glass are operating correctly and have free flowing connections to the boiler necessary for proper operation.\n\nThe gauge glass on a boiler needs to be inspected periodically and replaced if it is seen to have worn thin in the vicinity of the gland nuts, but a failure in service can still occur. Drivers are expected to carry two or three glass tubes, pre-cut to the required length, together with hemp or rubber seals, to replace the tubes on the road. Familiarity with this disquieting occurrence was considered so important that a glass would often be smashed deliberately while a trainee driver was on the footplate, to give him practice in fitting a new tube. Although automatic ball valves are fitted in the mounts to limit the release of steam and scalding water, these can fail through accumulation of limescale. It was standard procedure to hold the coal scoop in front of the face while the other hand, holding the cap for protection, reached to turn off the valves at both ends of the glass.\n\nA reflex gauge is more complex in construction but can give a clearer distinction between gas (steam) and liquid (water). Instead of containing the media in a glass tube, the gauge consists of a vertically oriented slotted metal body with a strong glass plate mounted on the open side of the slot facing the operator. The rear of the glass, in contact with the media, has grooves moulded into its surface, running vertically. The grooves form a zig-zag pattern with 90° angles. Incident light entering the glass is refracted at the rear surface in contact with the media. In the region that is contact with the gas, most of the light is reflected from the surface of one groove to the next and back towards the operator, appearing silvery white. In the region that is in contact with the liquid, most of the light is refracted into the liquid causing this region to appear almost black to the operator. Well-known makes of reflex gauge are IGEMA,TGI Ilmadur,Penberthy, Jerguson, Klinger, Cesare-Bonetti and Kenco. Due to the caustic nature of boiler anti-scaling treatments (\"water softeners\"), reflex gauges tend to become relatively rapidly etched by the water and lose their effectivess at displaying the liquid level. Therefore, bi-colour gauges are recommended for certain types of boiler, particularly those operating at pressure above 60 bar.\n\nA bi-colour gauge is generally preferred for caustic media in order to afford protection to the glass. The gauge consists of a vertically oriented slotted metal body with a strong plain glass to the front and the rear. The front and rear body surfaces are in non-parallel vertical planes. Behind the gauge body are light sources with two quite different wavelengths, typically red and green. Due to the different refraction of the red and green light, the liquid region appears green to the operator, while the gas region appears red. Unlike the reflex gauge, the glass has a plane surface which it does not need to be in direct contact with the media and can be protected with a layer of a caustic-resistant transparent material such as silica. Well-known manufacturers of the highest quality Bi-Colour Level Gauges are FPS-Aquarian, IGEMA and Quest-Tec\n\nIn a magnetic indicator is a float on the surface of the liquid contains a permanent magnet. The liquid is contained in a chamber of strong, non-magnetic material, avoiding the use of glass. The level indicator consists of a number of pivoting magnetic vanes arranged one above the other and placed close to the chamber containing the float. The two faces of the vanes are differently coloured. As the magnet passes up and down behind the vanes it cause them to rotate, displaying one colour for the region containing the liquid and another for the region containing gas. Magnetic indicators are stated in various manufacturers' literature to be most suitable for very high pressure and / or temperature and for aggressive liquids.\n\nThe first locomotive to be fitted with the device was built in 1829 by John Rastrick at his Stourbridge works.\n\nIndustrial observational instruments have changed with industry itself. More structurally sophisticated than the water gauge, the contemporary sight glass — also called the sight window or sight port — can be found on the media vessel at chemical plants and in other industrial settings, including pharmaceutical, food, beverage and bio gas plants. Sight glasses enable operators to visually observe processes inside tanks, pipes, reactors and vessels. \n\nThe modern industrial sight glass is a glass disk held between two metal frames, which are secured by bolts and gaskets, or the glass disc is fused to the metal frame during manufacture. The glass used for this purpose is either soda lime glass or borosilicate glass, and the metal, usually a type of stainless steel, is chosen for desired properties of strength. Borosilicate glass is superior to other formulations in terms of chemical corrosion resistance and temperature tolerance, as well as transparency. \n\nFused sight glasses are also called mechanically prestressed glass, because the glass is strengthened by compression of the metal ring. Heat is applied to a glass disc and its surrounding steel ring, causing a fusion of the materials. As the steel cools, it contracts, compressing the glass and making it resistant to tension. Because glass typically breaks under tension, mechanically prestressed glass is unlikely to break and endanger workers. The strongest sight glasses are made with borosilicate glass, because of the greater difference in its coefficient of expansions.\n\n\n"}
{"id": "42141799", "url": "https://en.wikipedia.org/wiki?curid=42141799", "title": "Smart onboard data interface module", "text": "Smart onboard data interface module\n\nThe Smart Onboard Data Interface Module (SMODIM) is used by the United States Army and foreign militaries for live simulated weapons training on military platforms. The SMODIM is the primary component of the Longbow Apache Tactical Engagement Simulation System (LBA TESS) that provides weapons systems training and collective Force-on-Force live training participation.\n\nTESS is an advanced weapons training system developed for the AH-64 Apache to support force-on-force and force-on-target live training at U.S. Army Combat Training Centers (CTCs), Aviation Home Stations, and deployed locations. TESS integrates with aircraft and ground vehicles to provide collective opposing force participation in live training.\n\nThe Aerial Weapons Scoring System (AWSS) integration with LBA TESS provides the ability to conduct force-on-target engagements using live ammunition for 30 mm gun, rockets, and simulated Hellfire missiles. Gunnery scoring is supported by the SMODIM, which transmits aviation data from the LBA to the Exercise Control (EXCON). AWSS scores the pilot’s live-fire gunnery performance and provides constructive After Action Review (AAR) feedback.\n\nDesigned and manufactured by Inter-Coastal Electronics, Inc (ICE), \"The SMODIM is fully compatible with the multiple integrated laser engagement system (MILES) and laser-based direct fire weapons. Additionally, it eliminates the requirement for MILES-type lasers by providing the capability to geometrically pair weapon engagements\". \n\nIn 1992, Inter-Coastal Electronics was selected by U.S. Army Communications Electronics Command (CECOM) to provide the real-time casualty assessment (RTCA) interface for the AH-64 Apache Attack Helicopter. This component was called the onboard data interface module (ODIM). As the Army incorporated new technologies, the SMODIM added GPS and telemetry and became the core component of live helicopter training systems at all U.S. Army Combat Training Centers (CTCs) with a fielding of over 240 systems. In 1997, the SMODIM was modified to provide a proof of concept for the upgraded AH-64 (D model) Longbow Apache. In 1998 the \"Modular\" SMODIM and the longbow tactical engagement simulation system (TESS) training system was fielded to all three CTCs and Aviation Home Stations, and became the U.S. Army's first fully integrated live aviation training system. Over the following 15 years, the SMODIM has deployed in multiple systems and platforms with over one thousand SMODIMs fielded in the U.S. and abroad. An \"Advanced\" SMODIM or \"ASMODIM\" is currently in development due to parts obsolescence and will provide an 80% increased processing performance. Security encryption is in accordance with FIPS 140-2 level 2. Advanced weapon simulation is augmented by digital terrain elevation data (DTED) and geometric ranging. Data communication and data transmission upgrades utilize RS-422 and RS-485 full duplex channels, and ARINC 429 technical standards.\n\nThe SMODIM includes a built-in GPS receiver, telemetry radio, data recorder, and MIL-STD-1553 mux bus processors. The SMODIM interfaces with the weapons systems and actively tracks, records and transmits data to the ground station or exercise control (EXCON). RTCA feedback is passed directly to the weapons processor and the ground station through the onboard telemetry radio. The SMODIM processes area weapons effects (AWE) data received from the EXCON and computes geometric pairing solutions for weapon engagements using the RF Hellfire, semi-automated laser (SAL) Hellfire, 30 mm chain gun, and rockets. It selects a target from its onboard player/position database in the appropriate weapons impact footprint, then uses the probability-of-hit (Ph) factor to determine the assessment (i.e., ‘hit’/‘miss’). Via the data link, it then informs the target it has been selected for assessment.\n\nThe SMODIM maintains a dynamic position database through player-to-player network communications. The onboard telemetry radio supports simultaneous distribution to multiple locations. The radio acts as a message repeater to overcome line of sight (LOS) interruptions. The SMODIM interfaces with distributed interactive simulation (DIS) networks using SMODIM tracking analysis and recording (SMOTAR), an advanced software suite that provides visual display and tracking of SMODIM instrumented players. GPS provides real-time position data as players are dynamically simulated, tracked and recorded over tactical maps and aerial photos for after-action review (AAR).\n\nThe TESS Aircraft System consists of an “A” kit that becomes part of the aircraft, and a “B” kit that is added to the aircraft for training exercises. The “A” kit includes the SMODIM Tray Assembly, modified software in the weapons display and systems processors, with cable connection provisions. The “B” kit includes the SMODIM, Eye-Safe Laser Range Finder/Designator (ESLRF/D), TESS Gun Control Unit (TGCU), Aircraft Internal Boresight Subassembly (AIBS), TESS Training Missile (TTM) with Flash Weapon Effects Signature Simulator (FlashWESS), GPS and telemetry antennas. \n\n\nThe SMODIM is qualified with an airworthiness release (AWR) through the Aviation Engineering Directorate (AED). Environmental and Electromagnetic Interference (EMI) compliance tests include DO-160, MIL-STD-810E/F and ADS-37A-PRF.\n\nThe SMODIM is used on the following platforms with applicable current contract information:\n\n\nOther platforms instrumented with the SMODIM include:\n\nThe TESS training system and SMODIM are used by the U.S. Army at Aviation Homestations and CTCs (NTC, JRTC, JMRC). Permanent TESS training support is provided at Ft Hood, TX since 1998 for the 21st Cavalry Brigade \n, and is ongoing through the 166th Aviation Brigade and Foreign Military customers that train there. Permanent field support for TESS is also provided at the CTCs.\n\nThe UH-72A LUH Lakota is recently instrumented with the SMODIM and will deploy to Germany at the Joint Multinational Command Training Center (JMTC) to train pilots in combat engagements.\n\nTESS provides the capability to track all LUH aircraft and provide OPFOR aircraft status (alive or killed) to the CTC EXCON. Simulated weapons capability allows Force-on-Force and Force-on-Target training engagements.\n\nThe SMODIM supports U.S. Army Homestation gunnery on digital range training system (DRTS), Digital Air Ground Integration Range (DAGIR), and Aviation Homestation Interim Package (AHIP) ranges. Upon proven success, the SMODIM has become a key component of the Tank-Automotive and Armaments Command (TACOM) AHIP modular After Action Review capability. Beginning in September 2013, five AHIP AAR systems that use SMOTAR software suite are being fielded to Ft Knox, KY, Ft Drum, NY, Ft Stewart, GA, Ft Hood, TX, and Grafenwoehr Germany. The objective solution is full integration of ground/air manned platforms and Unmanned Aircraft Systems (UAS) including Gray Eagle, Shadow, Raven, and Puma.\n\nThese programs are contracted by the U.S. Army Training and Doctrine Command (TRADOC), Program Executive Office for Simulation, Training and Instrumentation (PEO STRI), and Program Manager Training Devices (PM TRADE).\n\nDirect and foreign military sales (FMS) of the aviation TESS are currently in use by: Netherlands, Taiwan, Kuwait, Egypt, United Kingdom, Singapore, and United Arab Emirates.\n\n"}
{"id": "41658217", "url": "https://en.wikipedia.org/wiki?curid=41658217", "title": "Sri Lanka Sustainable Energy Authority", "text": "Sri Lanka Sustainable Energy Authority\n\nThe Sri Lanka Sustainable Energy Authority (or SLSEA) is the primary body responsible for the issuance of licenses for sustainable energy developments in Sri Lanka. In addition to being the key licence provider, it is also the organization responsible for promoting renewable energy and sustainable developments in the country.\n\n"}
{"id": "1954771", "url": "https://en.wikipedia.org/wiki?curid=1954771", "title": "Swaddling", "text": "Swaddling\n\nSwaddling is an age-old practice of wrapping infants in blankets or similar cloths so that movement of the limbs is tightly restricted. Swaddling bands were often used to further restrict the infant. Swaddling fell out of favor in the 17th century.\n\nSome authors are of the opinion that swaddling is becoming popular again, although medical and psychological opinion on the effects of swaddling is divided. Some modern medical studies indicate that swaddling helps babies fall asleep and to remain asleep and helps to keep the baby in a supine position, which lowers the risk of sudden infant death syndrome (SIDS). However, one recent study indicated that swaddling increased the risk of SIDS.Additionally emerging evidence is showing that certain swaddling techniques may increase the risk of developmental dysplasia of the hip.\n\nSeveral authors presume that swaddling was invented in the paleolithic period.\nThe earliest depictions of swaddled babies are votive offerings and grave goods from Crete and Cyprus, 4000 to 4500 years old.\n\nVotive statuettes have been found in the tombs of Ancient Greek and Roman women who died in childbirth, displaying babies in swaddling clothes. In shrines dedicated to Amphiaraus, models representing babies wrapped in swaddling clothes have been excavated. Apparently, these were frequently given as thank-offerings by anxious mothers when their infants had recovered from sickness.\n\nProbably the most famous record of swaddling is found in the New Testament concerning the birth of Jesus in :\nSwaddling clothes described in the Bible consisted of a cloth tied together by bandage-like strips. After an infant was born, the umbilical cord was cut and tied, and then the baby was washed, rubbed with salt and oil, and wrapped with strips of cloth. These strips kept the newborn child warm and also ensured that the child's limbs would grow straight. describes Israel as unswaddled, a metaphor for abandonment.\n\nDuring Tudor times, swaddling involved wrapping the new baby in linen bands from head to foot to ensure the baby would grow up without physical deformity. A stay band would be attached to the forehead and the shoulders to secure the head. Babies would be swaddled like this until about 8 or 9 months.\n\nThe Swiss surgeon Felix Würtz (approx. 1500 to approx. 1598) was the first who criticized aspects of swaddling openly.\n\nIn the seventeenth century, the scientific opinion towards swaddling began to change. There was an association of neglect with swaddling, especially regarding wetnurses who would leave babies in their care swaddled for long periods without washing or comforting them. More than a hundred years after Würtz, physicians and philosophers from England began to openly criticize swaddling and finally demanded its complete abolishment. The British philosopher John Locke (1632–1704) rejected swaddling in his 1693 publication \"Some Thoughts Concerning Education\", becoming a lobbyist for not binding babies at all. This thought was very controversial during the time, but slowly gained ground, first in England and later elsewhere in Western Europe.\n\nWilliam Cadogan (1711–1797) seems to have been the first physician, who pleaded for the complete abolition of swaddling. In his \"Essay upon Nursing\" of 1748, he expressed his view of contemporary child care, swaddling, the topic of too much clothing for infants and overfeeding. He wrote:\n\nPhilosophers and physicians more and more began to reject swaddling in the 18th century. Jean Jacques Rousseau wrote in his book \"\" in 1762:\n\nAlthough this form of swaddling has fallen out of favour in the Western world, many Eastern cultures and tribal people still use it.\n\nThe swaddling clothes of mediaeval Madonna and Child paintings are now replaced with cotton receiving blankets, cotton muslin wraps, or specialised \"winged\" baby swaddles. Modern swaddling is becoming increasingly popular today as a means of settling and soothing irritable infants and helping babies sleep longer with fewer awakenings. Since the early 1990s, the medical community has recommended placing babies on their back to sleep to reduce the risk of SIDS. As studies proved swaddled babies sleep better in the back sleeping position, swaddling has become increasingly popular and recommended so parents avoid the dangerous stomach sleeping position. \nSwaddling also prevents newborns waking themselves with their Moro reflex.\n\nLoose and ineffective swaddling techniques made while using an undersized blanket can generally be kicked off by a wakeful baby. It is important for caregivers to accomplish a secure swaddle to ensure the blanket does not become loose and the baby remains wrapped during the sleep period. The act of swaddling does carry a risk of the baby overheating if the caregiver uses multiple blankets that are too thick or uses thick fluffy fabric that creates excessive thermal insulation.\n\nModern specialized baby swaddles are designed to make it easier to swaddle a baby than with traditional square blanket. They are typically fabric blankets in a triangle, 'T' or 'Y' shape, with 'wings' that fold around the baby's torso or down over the baby's shoulders and around underneath the infant. Some of these products employ Velcro patches or other fasteners. Some parents prefer a specialized device because of the relative ease of use, and many parents prefer a large square receiving blanket or wrap because they can get a tighter and custom fit and the baby will not outgrow the blanket.\n\nTo avoid hip dysplasia risk, the swaddle should be done in such a way that the baby is able to move his or her legs freely at the hip. This is more easily done with a large blanket that can keep the arms in place while allowing the legs flexibility, all while allowing for proper hip development.\n\nBy the time the baby is learning to roll over, often around 4–5 months, parents and caregivers should transition the baby from swaddling to a less restrictive covering for sleep. If the baby can roll over, then it is important for the baby to have use of its hands and arms to adjust his or her head position after rolling over.\nThe traditional swaddling uses flat strings for babies to be tied; care is needed not to tie them too hard or blood flow would be restricted.\n\nSwaddling still is distributed worldwide. In some countries, swaddling is the standard treatment of babies. In Turkey, for instance, 93.1% of all babies become swaddled in the traditional way. According to the Human Relations Area Files (HRAF), 39% of all documented contemporary non-industrialized cultures show swaddling practices; further 19% use other methods of movement restriction for infants. Some authors assume that the popularity of swaddling is growing in the U.S., Great Britain and the Netherlands. A British sample showed up 19.4% of the babies are swaddled at night. In Germany, swaddling is not used as routine care measure and experiences relatively little acceptance, as the missing mentioning of this practice in the standard work on regulatory disturbances of Papusek shows.\n\nSwaddling as a medical intervention with a clearly limited indication range is used in the care practices of premature babies or crybabies with brain-organically provable damage. Also swaddling is used for reducing pain in such care actions as collecting blood at the heel. The swaddling of these premature babies (very low birth weight infants, VLBW infants) takes place only very loosely. It is meant to hold the weak arms at the body and make certain movements possible. This \"swaddling\" is something completely different from traditional swaddling in the stretched position.\n\nModern medical studies of swaddling use a form that is considerably shorter and less severe than the historical forms. The results of such studies are therefore to be understood only as assessments of historical practices. The classical study by Lipton \"et al.\" of 1965 dealt with a modern swaddling form. The researchers described the two main effects of tightly wrapping babies: they are motorically calm and sleep much. These effects are detected by means of various psycho-physiological parameters, such as heart rate, sleep duration and duration of crying. The research group around the Dutch biologist van Sleuwen in 2007 confirms this picture in their latest meta-analysis of medical studies on swaddling and its effects.\n\nHowever, severe restrictions on the scope of these studies should be kept in mind, because most of the positive effects mentioned by van Sleuwen \"et al.\" are not related to normally developed newborns, but to impaired babies, namely premature babies and babies with detectable organic brain damage.\nSwaddling enhances the REM sleep (active sleep) and also the whole sleep duration.\nThe effect of swaddling on the regulatory disturbance excessive crying is not very convincing: By adding the swaddling there is an immediate \"calming\" effect on children, but after a few days the effect of the introduction of regularity with swaddling is exactly the same as the regularity on its own. In other words: after a few days swaddling is completely unnecessary. It is therefore contraindicated to address the potential risk of swaddling, because the effect is only for a short term available, but after a little while is negligible.\n\nTwo studies based on indigenous peoples of the Americas did not show a delay in the onset of walking caused by the restraint of the use of the cradleboard. In other areas of the motor development, clear delays of the development show up even when mild restrictions take place. Skepticism concerning the allegedly missing effect of swaddling on the onset of walking delivers a Japanese study: the application of the basket cradle (ejiko) leads to a delayed onset of walking. An older Austrian study showed that swaddled Albanian babies showed a delayed ability to crawl and reach things with their hands. This shows the need for further substantial scientific clarifying regarding the impairment of motor skills by swaddling.\n\nThe effects of swaddling on the sudden infant death syndrome (SIDS) are unclear. A 2016 review found tentative evidence that swaddling increases risk of SIDS, especially among babies placed on their stomachs or side while sleeping.\n\nSwaddling was supposed to keep babies on their back, in order to prevent SIDS. Swaddling itself is not seen as a protective factor for SIDS. Swaddling may even increases the risk when babies sleep in the prone position; it reduces the risk if they sleep in the supine position. A recent study demonstrated now, that swaddling is apparently a risk factor for SIDS, although the opposite was often previously assumed: Of the babies who died of SIDS, 24% were swaddled; in the control-groups only 6% were swaddled.\n\nSeveral empirical studies show evidence of negative effects of swaddling. \n\n\n"}
{"id": "43944449", "url": "https://en.wikipedia.org/wiki?curid=43944449", "title": "SwitchUp", "text": "SwitchUp\n\nSwitchUp is an online platform that helps students find a technology, data science or coding bootcamp for their transition into a technology career. Students use the website to research online and offline programming courses by reading alumni reviews, connecting with mentors in the forum, taking an online quiz, and reading industry studies. SwitchUp only accepts reviews from verified alumni and has a very strict verification process. They have no affiliation with\n\nMission: SwitchUp wants potential bootcamp attendees, current bootcamp attendees and graduates to have access to everything they need to begin a tech career. As tech continues to evolve, SwitchUp hopes to be at the forefront of student searches for the best training programs.\nSwitchUp, was started after Jonathan Lau, an MIT alum, attended a Coding Bootcamp in Boston. He left feeling he could have gained more from his bootcamp experience. He noticed a quickly growing industry with little oversight, where it was difficult for students to identify high-quality programs. This industry was becoming saturated, and he wanted to find a way to help students easily access information and advice about tech bootcamps. He launched SwitchUp with a small team, and left the very first code bootcamp review on the site.\n\nSince SwitchUp's founding, the industry has only grown more quickly. SwitchUp aims to add transparency to the technology education industry. It is imperative now, more than ever, to have a site like SwitchUp available to prospective students, free-of-charge. The website was launched in August 2014.\n\nAs of January 2018, the site had over 10,000 reviews of 1000 different programming bootcamps and courses across 30 different countries. The platform helps connect potential students with mentors, find jobs, and choose an appropriate educational program. Switch guides students on the right career path, recommends bootcamps, and aggregates alumni reviews.\n\nSwitchUp regularly publishes industry research and bootcamp rankings. They also put out data science, cyber security, and web design rankings.\n\nThey also offer scholarship information and listings for bootcamps that accept the GI Bill.\n\nIn a job outcomes study conducted by researchers published on Dec 1, 2016, the following trends were found:\n\n\nSwitchUp also published the 2018 best coding bootcamp rankings on December 31, 2018.\n\nSwitchUp has received significant media coverage upon its launch, including articles in the Wired, TechCrunch, VentureBeat, FastCompany, and LifeHacker.\n\n"}
{"id": "1918397", "url": "https://en.wikipedia.org/wiki?curid=1918397", "title": "Synthetic aperture sonar", "text": "Synthetic aperture sonar\n\nSynthetic aperture sonar (SAS) is a form of sonar in which sophisticated post-processing of sonar data are used in ways closely analogous to synthetic aperture radar. Synthetic aperture sonars combine a number of acoustic pings to form an image with much higher along-track resolution than conventional sonars. The along-track resolution can approach half the length of one sonar element, though is downward limited by 1/4 wavelength. The principle of synthetic aperture sonar is to move the sonar while illuminating the same spot on the sea floor with several pings. When moving along a straight line, those pings that have the image position within the beamwidth constitutes the synthetic array. By coherent reorganization of the data from all the pings, a synthetic aperture image is produced with improved along-track resolution. In contrast to conventional side-scan sonar, SAS processing provides range-independent along-track resolution. At maximum range the resolution can be magnitudes better than that of side-scan sonars.\n\nFor further reading, the Open Access Article: \"Introduction to Synthetic Aperture Sonar\" can be recommended as introduction for people familiar with Sonar Systems. A 2013 technology review with examples and future trends is also available. For academics, the IEEE Journal of Oceanic Engineering article: \"Synthetic Aperture Sonar, A Review of Current Status\" gives an overview of the history and an extensive list of references for the community achievements up to 2009.\n\n\n"}
{"id": "29637280", "url": "https://en.wikipedia.org/wiki?curid=29637280", "title": "TPS Pakistan", "text": "TPS Pakistan\n\nTPS Pakistan is a Pakistani technology company established in 1996 by two entrepreneurs Mohammed Sohail and Mubashir Rahim. \nTPS Pakistan's two founders, Mohammad Sohail and Mubashir Rahim completed their higher education in the US in engineering and returned to Pakistan in the early 1990s. Before returning, they worked for a short period at NCR Corporation, a software and electronics company which is primarily involved in selling computer servers and ATMs. They returned to Pakistan and decided to form their own software company in Karachi. They both worked very hard and mainly looked for opportunities in networking connectivity especially in the banking and financial sectors. In 2015, CEO of TPS Pakistan, Shahzad Shahid compared Pakistan with the neighboring countries like Sri Lanka, India and Bangladesh where more people had bank accounts and emphasized the need for Pakistan to catch up with them.\nTPS Pakistan's headquarters are located at TPS Tower in Karachi, Pakistan\n\nTPS Pakistan's headquarters were located in Business Avenue Karachi, Pakistan. In 2008 TPS Pakistan announced that it was moving its headquarters from Business Avenue to a bigger and spacious building named as TPS Tower in Karachi, Pakistan.\n\nTPS Pakistan started off as a two-man operation in 1996 by the name of Transaction Processing Systems, it gradually grew and eventually was incorporated with the Securities and Exchange Commission of Pakistan as \"TPS Pakistan (Private) Limited company\" in 2003.\n\n\n"}
{"id": "2906805", "url": "https://en.wikipedia.org/wiki?curid=2906805", "title": "Technology strategy", "text": "Technology strategy\n\nTechnology strategy (information technology strategy or IT strategy) is the overall plan which consists of objectives, principles and tactics relating to use of technologies within a particular organization. Such strategies primarily focus on the technologies themselves and in some cases the people who directly manage those technologies. The strategy can be implied from the organization's behaviors towards technology decisions, and may be written down in a document. The strategy includes the formal vision that guide the acquisition, allocation, and management of IT resources so it can help fulfill the organizational objectives.\n\nOther generations of technology-related strategies primarily focus on: the efficiency of the company's spending on technology; how people, for example the organization's customers and employees, exploit technologies in ways that create value for the organization; on the full integration of technology-related decisions with the company's strategies and operating plans, such that no separate technology strategy exists other than the de facto strategic principle that the organization does not need or have a discrete 'technology strategy'.\n\nA technology strategy has traditionally been expressed in a document that explains how technology should be utilized as part of an organization's overall corporate strategy and each business strategy. In the case of IT, the strategy is usually formulated by a group of representatives from both the business and from IT. Often the Information Technology Strategy is led by an organization's Chief Technology Officer (CTO) or equivalent. Accountability varies for an organization's strategies for other classes of technology. Although many companies write an overall business plan each year, a technology strategy may cover developments somewhere between 3 and 5 years into the future.\n\nThe United States identified the need to implement a technology strategy in order to restore the country's competitive edge. In 1983 Project Socrates, a US Defense Intelligence Agency program, was established to develop a national technology strategy policy.\n\nA successful technology strategy involves the documentation of planning assumptions and the development of success metrics. These establish a mission-driven strategy, which ensures that initiatives are aligned with the organization's goals and objectives. This aspect underscores that the primary objective of designing technology strategy is to make sure that the business strategy can be realized through technology and that technology investments are aligned with business. Some experts underscore the successful technology strategy is one that is integrated within the organization's overall business strategy not just to contribute to the mission and vision of the company but also get support from it.\n\nThere are frameworks (e.g. ASSIMPLER) available that provide insights into the current and future business strategy, assess business-IT alignment on various parameters, identify gaps, and define technology roadmaps and budgets. These highlight key information, which include the following: \n\n\nFor a strategy to be effective, it should also answer questions of how to create value, deliver value, and capture value. In order to create value one needs to trace back the technology and forecast on how the technology evolves, how the market penetration changes, and how to organize effectively. Capturing value requires knowledge how to gain competitive advantage and sustain it, and how to compete in case that standards of technology is important. The final step is delivering the value, where firms defines how to execute the strategy, make strategic decisions and take decisive actions. The Strategic Alignment Process is a step by step process that helps managers stay focused on specific task in order to execute the task and deliver value.\n\nAligned with Statement Of Applicability (SOA) approach, IT strategy is composed of IT Capability Model (ITCM) and IT Operating Model (IT-OM) as proposed by Haloedscape IT Strategy Model.\n\nProcess of IT Strategy is simplified with framework constituted of IT Service Management (ITSM), Enterprise Architecture Development (TOGAF) and Governance (COBIT). IT Strategy is modeled as vertical IT service applied to and supported by each horizontal layers of SOA architecture. For details, refer Haloedscape IT Strategy Framework.\n\nThe following are typically sections of a technology strategy:\nIncludes a SWOT Analysis SWOT_analysis\n\nA technology strategy document is usually designed to be read by non-technical stakeholders involved in business planning within an organization. It should be free of technical jargon and information technology acronyms.\n\nThe IT strategy should also be presented or read by internal IT staff members. Many organizations will circulate prior year versions to internal IT department for feedback. The feedback is used to create new annual IT strategy plans.\n\nOne critical integration point is the interface with an organization's marketing plan. The marketing plan frequently requires the support of a web site to create an appropriate on-line presence. Large organizations frequently have complex web site requirements such as web content management.\n\nThe implementation of technology strategy will likely follow the conventional procedure taken when implementing a business strategy or an organization's planned changes within the so-called change management framework. Fundamentally, it is directed by a manager who oversees the process, which could include gaining targeted organizational members' commitment to an innovation. There are a number of strategic options available when implementing a technology strategy and one of these involves the structured approach. For instance, in the area of systematic exploration of emerging technologies, this approach help determine the relevance and opportunities offered by new technologies to business through its well-defined assessment mechanisms that can effectively justify adoption. \n\nA technology strategy document typically refers to but does not duplicate an overall enterprise architecture. The technology strategy may refer to:\n\n\n"}
{"id": "29831", "url": "https://en.wikipedia.org/wiki?curid=29831", "title": "Television", "text": "Television\n\nTelevision (TV), sometimes shortened to tele or telly, is a telecommunication medium used for transmitting moving images in monochrome (black and white), or in colour, and in two or three dimensions and sound. The term can refer to a television set, a television program (\"TV show\"), or the medium of television transmission. Television is a mass medium for advertising, entertainment and news. \n\nTelevision became available in crude experimental forms in the late 1920s, but it would still be several years before the new technology would be marketed to consumers. After World War II, an improved form of black-and-white TV broadcasting became popular in the United States and Britain, and television sets became commonplace in homes, businesses, and institutions. During the 1950s, television was the primary medium for influencing public opinion. In the mid-1960s, color broadcasting was introduced in the US and most other developed countries. The availability of multiple types of archival storage media such as Betamax, VHS tape, local disks, DVDs, flash drives, high-definition Blu-ray Discs, and cloud digital video recorders has enabled viewers to watch pre-recorded material—such as movies— at home on their own time schedule. For many reasons, especially the convenience of remote retrieval, the storage of television and video programming now occurs on the cloud. At the end of the first decade of the 2000s, digital television transmissions greatly increased in popularity. Another development was the move from standard-definition television (SDTV) (576i, with 576 interlaced lines of resolution and 480i) to high-definition television (HDTV), which provides a resolution that is substantially higher. HDTV may be transmitted in various formats: 1080p, 1080i and 720p. Since 2010, with the invention of smart television, Internet television has increased the availability of television programs and movies via the Internet through streaming video services such as Netflix, Amazon Video, iPlayer, Hulu, Roku and Chromecast.\n\nIn 2013, 79% of the world's households owned a television set. The replacement of early bulky, high-voltage cathode ray tube (CRT) screen displays with compact, energy-efficient, flat-panel alternative technologies such as LCDs (both fluorescent-backlit and LED), OLED displays, and plasma displays was a hardware revolution that began with computer monitors in the late 1990s. Most TV sets sold in the 2000s were flat-panel, mainly LEDs. Major manufacturers announced the discontinuation of CRT, DLP, plasma, and even fluorescent-backlit LCDs by the mid-2010s. In the near future, LEDs are expected to be gradually replaced by OLEDs. Also, major manufacturers have announced that they will increasingly produce smart TVs in the mid-2010s. Smart TVs with integrated Internet and Web 2.0 functions became the dominant form of television by the late 2010s.\n\nTelevision signals were initially distributed only as terrestrial television using high-powered radio-frequency transmitters to broadcast the signal to individual television receivers. Alternatively television signals are distributed by coaxial cable or optical fiber, satellite systems and, since the 2000s via the Internet. Until the early 2000s, these were transmitted as analog signals, but a transition to digital television is expected to be completed worldwide by the late 2010s. A standard television set is composed of multiple internal electronic circuits, including a tuner for receiving and decoding broadcast signals. A visual display device which lacks a tuner is correctly called a video monitor rather than a television.\n\nThe word \"television\" comes . The first documented usage of the term dates back to 1900, when the Russian scientist Constantin Perskyi used it in a paper that he presented in French at the 1st International Congress of Electricity, which ran from 18 to 25 August 1900 during the International World Fair in Paris. The Anglicised version of the term is first attested in 1907, when it was still \"...a theoretical system to transmit moving images over telegraph or telephone wires\". It was \"...formed in English or borrowed from French télévision.\" In the 19th century and early 20th century, other \"...proposals for the name of a then-hypothetical technology for sending pictures over distance were telephote (1880) and televista (1904).\" The abbreviation \"TV\" is from 1948. The use of the term to mean \"a television set\" dates from 1941. The use of the term to mean \"television as a medium\" dates from 1927. The slang term \"telly\" is more common in the UK. The slang term \"the tube\" or the \"boob tube\" refers to the bulky cathode ray tube used on most TVs until the advent of flat-screen TVs. Another slang term for the TV is \"idiot box\". Also, in the 1940s and throughout the 1950s, during the early rapid growth of television programming and television-set ownership in the United States, another slang term became widely used in that period and continues to be used today to distinguish productions originally created for broadcast on television from films developed for presentation in movie theaters. The “small screen”, as both a compound adjective and noun, became specific references to television, while the “big screen” was used to identify productions made for theatrical release.\n\nFacsimile transmission systems for still photographs pioneered methods of mechanical scanning of images in the early 19th century. Alexander Bain introduced the facsimile machine between 1843 and 1846. Frederick Bakewell demonstrated a working laboratory version in 1851. Willoughby Smith discovered the photoconductivity of the element selenium in 1873. As a 23-year-old German university student, Paul Julius Gottlieb Nipkow proposed and patented the Nipkow disk in 1884. This was a spinning disk with a spiral pattern of holes in it, so each hole scanned a line of the image. Although he never built a working model of the system, variations of Nipkow's spinning-disk \"image rasterizer\" became exceedingly common. Constantin Perskyi had coined the word \"television\" in a paper read to the International Electricity Congress at the International World Fair in Paris on 24 August 1900. Perskyi's paper reviewed the existing electromechanical technologies, mentioning the work of Nipkow and others. However, it was not until 1907 that developments in amplification tube technology by Lee de Forest and Arthur Korn, among others, made the design practical.\n\nThe first demonstration of the live transmission of images was by Georges Rignoux and A. Fournier in Paris in 1909. A matrix of 64 selenium cells, individually wired to a mechanical commutator, served as an electronic retina. In the receiver, a type of Kerr cell modulated the light and a series of variously angled mirrors attached to the edge of a rotating disc scanned the modulated beam onto the display screen. A separate circuit regulated synchronization. The 8x8 pixel resolution in this proof-of-concept demonstration was just sufficient to clearly transmit individual letters of the alphabet. An updated image was transmitted \"several times\" each second. In 1921 Edouard Belin sent the first image via radio waves with his belinograph.\n\nIn 1911, Boris Rosing and his student Vladimir Zworykin created a system that used a mechanical mirror-drum scanner to transmit, in Zworykin's words, \"very crude images\" over wires to the \"Braun tube\" (cathode ray tube or \"CRT\") in the receiver. Moving images were not possible because, in the scanner: \"the sensitivity was not enough and the selenium cell was very laggy\".\n\nBy the 1920s, when amplification made television practical, Scottish inventor John Logie Baird employed the Nipkow disk in his prototype video systems. On 25 March 1925, Baird gave the first public demonstration of televised silhouette images in motion, at Selfridge's Department Store in London. Since human faces had inadequate contrast to show up on his primitive system, he televised a ventriloquist's dummy named \"Stooky Bill\", whose painted face had higher contrast, talking and moving. By 26 January 1926, he demonstrated the transmission of the image of a face in motion by radio. This is widely regarded as the first television demonstration. The subject was Baird's business partner Oliver Hutchinson. Baird's system used the Nipkow disk for both scanning the image and displaying it. A bright light shining through a spinning Nipkow disk set with lenses projected a bright spot of light which swept across the subject. A Selenium photoelectric tube detected the light reflected from the subject and converted it into a proportional electrical signal. This was transmitted by AM radio waves to a receiver unit, where the video signal was applied to a neon light behind a second Nipkow disk rotating synchronized with the first. The brightness of the neon lamp was varied in proportion to the brightness of each spot on the image. As each hole in the disk passed by, one scan line of the image was reproduced. Baird's disk had 30 holes, producing an image with only 30 scan lines, just enough to recognize a human face. In 1927, Baird transmitted a signal over of telephone line between London and Glasgow.\n\nIn 1928, Baird's company (Baird Television Development Company/Cinema Television) broadcast the first transatlantic television signal, between London and New York, and the first shore-to-ship transmission. In 1929, he became involved in the first experimental mechanical television service in Germany. In November of the same year, Baird and Bernard Natan of Pathé established France's first television company, Télévision-Baird-Natan. In 1931, he made the first outdoor remote broadcast, of The Derby. In 1932, he demonstrated ultra-short wave television. Baird's mechanical system reached a peak of 240-lines of resolution on BBC television broadcasts in 1936, though the mechanical system did not scan the televised scene directly. Instead a 17.5mm film was shot, rapidly developed and then scanned while the film was still wet.\n\nAn American inventor, Charles Francis Jenkins, also pioneered the television. He published an article on \"Motion Pictures by Wireless\" in 1913, but it was not until December 1923 that he transmitted moving silhouette images for witnesses; and it was on 13 June 1925, that he publicly demonstrated synchronized transmission of silhouette pictures. In 1925 Jenkins used the Nipkow disk and transmitted the silhouette image of a toy windmill in motion, over a distance of five miles, from a naval radio station in Maryland to his laboratory in Washington, D.C., using a lensed disk scanner with a 48-line resolution. He was granted U.S. Patent No. 1,544,156 (Transmitting Pictures over Wireless) on 30 June 1925 (filed 13 March 1922).\n\nHerbert E. Ives and Frank Gray of Bell Telephone Laboratories gave a dramatic demonstration of mechanical television on 7 April 1927. Their reflected-light television system included both small and large viewing screens. The small receiver had a 2-inch-wide by 2.5-inch-high screen. The large receiver had a screen 24 inches wide by 30 inches high. Both sets were capable of reproducing reasonably accurate, monochromatic, moving images. Along with the pictures, the sets received synchronized sound. The system transmitted images over two paths: first, a copper wire link from Washington to New York City, then a radio link from Whippany, New Jersey. Comparing the two transmission methods, viewers noted no difference in quality. Subjects of the telecast included Secretary of Commerce Herbert Hoover. A flying-spot scanner beam illuminated these subjects. The scanner that produced the beam had a 50-aperture disk. The disc revolved at a rate of 18 frames per second, capturing one frame about every 56 milliseconds. (Today's systems typically transmit 30 or 60 frames per second, or one frame every 33.3 or 16.7 milliseconds respectively.) Television historian Albert Abramson underscored the significance of the Bell Labs demonstration: \"It was in fact the best demonstration of a mechanical television system ever made to this time. It would be several years before any other system could even begin to compare with it in picture quality.\"\n\nIn 1928, WRGB, then W2XB, was started as the world's first television station. It broadcast from the General Electric facility in Schenectady, NY. It was popularly known as \"WGY Television\". Meanwhile, in the Soviet Union, Léon Theremin had been developing a mirror drum-based television, starting with 16 lines resolution in 1925, then 32 lines and eventually 64 using interlacing in 1926. As part of his thesis, on 7 May 1926, he electrically transmitted, and then projected, near-simultaneous moving images on a five-foot square screen. By 1927 he achieved an image of 100 lines, a resolution that was not surpassed until May 1932 by RCA, with 120 lines. On 25 December 1926, Kenjiro Takayanagi demonstrated a television system with a 40-line resolution that employed a Nipkow disk scanner and CRT display at Hamamatsu Industrial High School in Japan. This prototype is still on display at the Takayanagi Memorial Museum in Shizuoka University, Hamamatsu Campus. His research in creating a production model was halted by the United States after Japan lost World War II.\n\nBecause only a limited number of holes could be made in the disks, and disks beyond a certain diameter became impractical, image resolution on mechanical television broadcasts was relatively low, ranging from about 30 lines up to 120 or so. Nevertheless, the image quality of 30-line transmissions steadily improved with technical advances, and by 1933 the UK broadcasts using the Baird system were remarkably clear. A few systems ranging into the 200-line region also went on the air. Two of these were the 180-line system that Compagnie des Compteurs (CDC) installed in Paris in 1935, and the 180-line system that Peck Television Corp. started in 1935 at station VE9AK in Montreal. The advancement of all-electronic television (including image dissectors and other camera tubes and cathode ray tubes for the reproducer) marked the beginning of the end for mechanical systems as the dominant form of television. Mechanical television, despite its inferior image quality and generally smaller picture, would remain the primary television technology until the 1930s. The last mechanical television broadcasts ended in 1939 at stations run by a handful of public universities in the United States.\n\nIn 1897, English physicist J. J. Thomson was able, in his three famous experiments, to deflect cathode rays, a fundamental function of the modern cathode ray tube (CRT). The earliest version of the CRT was invented by the German physicist Ferdinand Braun in 1897 and is also known as the \"Braun\" tube. It was a cold-cathode diode, a modification of the Crookes tube, with a phosphor-coated screen. In 1906 the Germans Max Dieckmann and Gustav Glage produced raster images for the first time in a CRT. In 1907, Russian scientist Boris Rosing used a CRT in the receiving end of an experimental video signal to form a picture. He managed to display simple geometric shapes onto the screen.\n\nIn 1908 Alan Archibald Campbell-Swinton, fellow of the Royal Society (UK), published a letter in the scientific journal \"Nature\" in which he described how \"distant electric vision\" could be achieved by using a cathode ray tube, or Braun tube, as both a transmitting and receiving device, He expanded on his vision in a speech given in London in 1911 and reported in \"The Times\" and the Journal of the Röntgen Society. In a letter to \"Nature\" published in October 1926, Campbell-Swinton also announced the results of some \"not very successful experiments\" he had conducted with G. M. Minchin and J. C. M. Stanton. They had attempted to generate an electrical signal by projecting an image onto a selenium-coated metal plate that was simultaneously scanned by a cathode ray beam. These experiments were conducted before March 1914, when Minchin died, but they were later repeated by two different teams in 1937, by H. Miller and J. W. Strange from EMI, and by H. Iams and A. Rose from RCA. Both teams succeeded in transmitting \"very faint\" images with the original Campbell-Swinton's selenium-coated plate. Although others had experimented with using a cathode ray tube as a receiver, the concept of using one as a transmitter was novel. The first cathode ray tube to use a hot cathode was developed by John B. Johnson (who gave his name to the term Johnson noise) and Harry Weiner Weinhart of Western Electric, and became a commercial product in 1922.\n\nIn 1926, Hungarian engineer Kálmán Tihanyi designed a television system utilizing fully electronic scanning and display elements and employing the principle of \"charge storage\" within the scanning (or \"camera\") tube. The problem of low sensitivity to light resulting in low electrical output from transmitting or \"camera\" tubes would be solved with the introduction of charge-storage technology by Kálmán Tihanyi beginning in 1924. His solution was a camera tube that accumulated and stored electrical charges (\"photoelectrons\") within the tube throughout each scanning cycle. The device was first described in a patent application he filed in Hungary in March 1926 for a television system he dubbed \"Radioskop\". After further refinements included in a 1928 patent application, Tihanyi's patent was declared void in Great Britain in 1930, so he applied for patents in the United States. Although his breakthrough would be incorporated into the design of RCA's \"iconoscope\" in 1931, the U.S. patent for Tihanyi's transmitting tube would not be granted until May 1939. The patent for his receiving tube had been granted the previous October. Both patents had been purchased by RCA prior to their approval. Charge storage remains a basic principle in the design of imaging devices for television to the present day. On 25 December 1926, at Hamamatsu Industrial High School in Japan, Japanese inventor Kenjiro Takayanagi demonstrated a TV system with a 40-line resolution that employed a CRT display. This was the first working example of a fully electronic television receiver. Takayanagi did not apply for a patent.\n\nOn 7 September 1927, American inventor Philo Farnsworth's image dissector camera tube transmitted its first image, a simple straight line, at his laboratory at 202 Green Street in San Francisco. By 3 September 1928, Farnsworth had developed the system sufficiently to hold a demonstration for the press. This is widely regarded as the first electronic television demonstration. In 1929, the system was improved further by the elimination of a motor generator, so that his television system now had no mechanical parts. That year, Farnsworth transmitted the first live human images with his system, including a three and a half-inch image of his wife Elma (\"Pem\") with her eyes closed (possibly due to the bright lighting required).\nMeanwhile, Vladimir Zworykin was also experimenting with the cathode ray tube to create and show images. While working for Westinghouse Electric in 1923, he began to develop an electronic camera tube. But in a 1925 demonstration, the image was dim, had low contrast, and poor definition, and was stationary. Zworykin's imaging tube never got beyond the laboratory stage. But RCA, which acquired the Westinghouse patent, asserted that the patent for Farnsworth's 1927 image dissector was written so broadly that it would exclude any other electronic imaging device. Thus RCA, on the basis of Zworykin's 1923 patent application, filed a patent interference suit against Farnsworth. The U.S. Patent Office examiner disagreed in a 1935 decision, finding priority of invention for Farnsworth against Zworykin. Farnsworth claimed that Zworykin's 1923 system would be unable to produce an electrical image of the type to challenge his patent. Zworykin received a patent in 1928 for a color transmission version of his 1923 patent application; he also divided his original application in 1931. Zworykin was unable or unwilling to introduce evidence of a working model of his tube that was based on his 1923 patent application. In September 1939, after losing an appeal in the courts, and determined to go forward with the commercial manufacturing of television equipment, RCA agreed to pay Farnsworth US$1 million over a ten-year period, in addition to license payments, to use his patents.\n\nIn 1933, RCA introduced an improved camera tube that relied on Tihanyi's charge storage principle. Dubbed the \"Iconoscope\" by Zworykin, the new tube had a light sensitivity of about 75,000 lux, and thus was claimed to be much more sensitive than Farnsworth's image dissector. However, Farnsworth had overcome his power problems with his Image Dissector through the invention of a completely unique \"multipactor\" device that he began work on in 1930, and demonstrated in 1931. This small tube could amplify a signal reportedly to the 60th power or better and showed great promise in all fields of electronics. Unfortunately, a problem with the multipactor was that it wore out at an unsatisfactory rate.\n\nAt the Berlin Radio Show in August 1931, Manfred von Ardenne gave a public demonstration of a television system using a CRT for both transmission and reception. However, Ardenne had not developed a camera tube, using the CRT instead as a flying-spot scanner to scan slides and film. Philo Farnsworth gave the world's first public demonstration of an all-electronic television system, using a live camera, at the Franklin Institute of Philadelphia on 25 August 1934, and for ten days afterwards. Mexican inventor Guillermo González Camarena also played an important role in early TV. His experiments with TV (known as telectroescopía at first) began in 1931 and led to a patent for the \"trichromatic field sequential system\" color television in 1940. In Britain, the EMI engineering team led by Isaac Shoenberg applied in 1932 for a patent for a new device they dubbed \"the Emitron\", which formed the heart of the cameras they designed for the BBC. On 2 November 1936, a 405-line broadcasting service employing the Emitron began at studios in Alexandra Palace, and transmitted from a specially built mast atop one of the Victorian building's towers. It alternated for a short time with Baird's mechanical system in adjoining studios, but was more reliable and visibly superior. This was the world's first regular \"high-definition\" television service.\n\nThe original American iconoscope was noisy, had a high ratio of interference to signal, and ultimately gave disappointing results, especially when compared to the high definition mechanical scanning systems then becoming available. The EMI team, under the supervision of Isaac Shoenberg, analyzed how the iconoscope (or Emitron) produces an electronic signal and concluded that its real efficiency was only about 5% of the theoretical maximum. They solved this problem by developing, and patenting in 1934, two new camera tubes dubbed super-Emitron and CPS Emitron. The super-Emitron was between ten and fifteen times more sensitive than the original Emitron and iconoscope tubes and, in some cases, this ratio was considerably greater. It was used for outside broadcasting by the BBC, for the first time, on Armistice Day 1937, when the general public could watch on a television set as the King laid a wreath at the Cenotaph. This was the first time that anyone had broadcast a live street scene from cameras installed on the roof of neighboring buildings, because neither Farnsworth nor RCA would do the same until the 1939 New York World's Fair.\nOn the other hand, in 1934, Zworykin shared some patent rights with the German licensee company Telefunken. The \"image iconoscope\" (\"Superikonoskop\" in Germany) was produced as a result of the collaboration. This tube is essentially identical to the super-Emitron. The production and commercialization of the super-Emitron and image iconoscope in Europe were not affected by the patent war between Zworykin and Farnsworth, because Dieckmann and Hell had priority in Germany for the invention of the image dissector, having submitted a patent application for their \"Lichtelektrische Bildzerlegerröhre für Fernseher\" (\"Photoelectric Image Dissector Tube for Television\") in Germany in 1925, two years before Farnsworth did the same in the United States. The image iconoscope (Superikonoskop) became the industrial standard for public broadcasting in Europe from 1936 until 1960, when it was replaced by the vidicon and plumbicon tubes. Indeed, it was the representative of the European tradition in electronic tubes competing against the American tradition represented by the image orthicon. The German company Heimann produced the Superikonoskop for the 1936 Berlin Olympic Games, later Heimann also produced and commercialized it from 1940 to 1955; finally the Dutch company Philips produced and commercialized the image iconoscope and multicon from 1952 to 1958.\n\nAmerican television broadcasting, at the time, consisted of a variety of markets in a wide range of sizes, each competing for programming and dominance with separate technology, until deals were made and standards agreed upon in 1941. RCA, for example, used only Iconoscopes in the New York area, but Farnsworth Image Dissectors in Philadelphia and San Francisco. In September 1939, RCA agreed to pay the Farnsworth Television and Radio Corporation royalties over the next ten years for access to Farnsworth's patents. With this historic agreement in place, RCA integrated much of what was best about the Farnsworth Technology into their systems. In 1941, the United States implemented 525-line television. Electrical engineer Benjamin Adler played a prominent role in the development of television.\n\nThe world's first 625-line television standard was designed in the Soviet Union in 1944 and became a national standard in 1946. The first broadcast in 625-line standard occurred in Moscow in 1948. The concept of 625 lines per frame was subsequently implemented in the European CCIR standard. In 1936, Kálmán Tihanyi described the principle of plasma display, the first flat panel display system.\nThe basic idea of using three monochrome images to produce a color image had been experimented with almost as soon as black-and-white televisions had first been built. Although he gave no practical details, among the earliest published proposals for television was one by Maurice Le Blanc, in 1880, for a color system, including the first mentions in television literature of line and frame scanning. Polish inventor Jan Szczepanik patented a color television system in 1897, using a selenium photoelectric cell at the transmitter and an electromagnet controlling an oscillating mirror and a moving prism at the receiver. But his system contained no means of analyzing the spectrum of colors at the transmitting end, and could not have worked as he described it. Another inventor, Hovannes Adamian, also experimented with color television as early as 1907. The first color television project is claimed by him, and was patented in Germany on 31 March 1908, patent № 197183, then in Britain, on 1 April 1908, patent № 7219, in France (patent № 390326) and in Russia in 1910 (patent № 17912).\n\nScottish inventor John Logie Baird demonstrated the world's first color transmission on 3 July 1928, using scanning discs at the transmitting and receiving ends with three spirals of apertures, each spiral with filters of a different primary color; and three light sources at the receiving end, with a commutator to alternate their illumination. Baird also made the world's first color broadcast on 4 February 1938, sending a mechanically scanned 120-line image from Baird's Crystal Palace studios to a projection screen at London's Dominion Theatre. Mechanically scanned color television was also demonstrated by Bell Laboratories in June 1929 using three complete systems of photoelectric cells, amplifiers, glow-tubes, and color filters, with a series of mirrors to superimpose the red, green, and blue images into one full color image.\n\nThe first practical hybrid system was again pioneered by John Logie Baird. In 1940 he publicly demonstrated a color television combining a traditional black-and-white display with a rotating colored disk. This device was very \"deep\", but was later improved with a mirror folding the light path into an entirely practical device resembling a large conventional console. However, Baird was not happy with the design, and, as early as 1944, had commented to a British government committee that a fully electronic device would be better.\n\nIn 1939, Hungarian engineer Peter Carl Goldmark introduced an electro-mechanical system while at CBS, which contained an Iconoscope sensor. The CBS field-sequential color system was partly mechanical, with a disc made of red, blue, and green filters spinning inside the television camera at 1,200 rpm, and a similar disc spinning in synchronization in front of the cathode ray tube inside the receiver set. The system was first demonstrated to the Federal Communications Commission (FCC) on 29 August 1940, and shown to the press on 4 September.\n\nCBS began experimental color field tests using film as early as 28 August 1940, and live cameras by 12 November. NBC (owned by RCA) made its first field test of color television on 20 February 1941. CBS began daily color field tests on 1 June 1941. These color systems were not compatible with existing black-and-white television sets, and, as no color television sets were available to the public at this time, viewing of the color field tests was restricted to RCA and CBS engineers and the invited press. The War Production Board halted the manufacture of television and radio equipment for civilian use from 22 April 1942 to 20 August 1945, limiting any opportunity to introduce color television to the general public.\n\nAs early as 1940, Baird had started work on a fully electronic system he called Telechrome. Early Telechrome devices used two electron guns aimed at either side of a phosphor plate. The phosphor was patterned so the electrons from the guns only fell on one side of the patterning or the other. Using cyan and magenta phosphors, a reasonable limited-color image could be obtained. He also demonstrated the same system using monochrome signals to produce a 3D image (called \"stereoscopic\" at the time). A demonstration on 16 August 1944 was the first example of a practical color television system. Work on the Telechrome continued and plans were made to introduce a three-gun version for full color. However, Baird's untimely death in 1946 ended development of the Telechrome system.\nSimilar concepts were common through the 1940s and 1950s, differing primarily in the way they re-combined the colors generated by the three guns. The Geer tube was similar to Baird's concept, but used small pyramids with the phosphors deposited on their outside faces, instead of Baird's 3D patterning on a flat surface. The Penetron used three layers of phosphor on top of each other and increased the power of the beam to reach the upper layers when drawing those colors. The Chromatron used a set of focusing wires to select the colored phosphors arranged in vertical stripes on the tube.\n\nOne of the great technical challenges of introducing color broadcast television was the desire to conserve bandwidth, potentially three times that of the existing black-and-white standards, and not use an excessive amount of radio spectrum. In the United States, after considerable research, the National Television Systems Committee approved an all-electronic system developed by RCA, which encoded the color information separately from the brightness information and greatly reduced the resolution of the color information in order to conserve bandwidth. As black-and-white TVs could receive the same transmission and display it in black-and-white, the color system adopted is [backwards] \"compatible\". (\"Compatible Color\", featured in RCA advertisements of the period, is mentioned in the song \"America\", of West Side Story, 1957.) The brightness image remained compatible with existing black-and-white television sets at slightly reduced resolution, while color televisions could decode the extra information in the signal and produce a limited-resolution color display. The higher resolution black-and-white and lower resolution color images combine in the brain to produce a seemingly high-resolution color image. The NTSC standard represented a major technical achievement.\nAlthough all-electronic color was introduced in the U.S. in 1953, high prices, and the scarcity of color programming, greatly slowed its acceptance in the marketplace. The first national color broadcast (the 1954 Tournament of Roses Parade) occurred on 1 January 1954, but during the following ten years most network broadcasts, and nearly all local programming, continued to be in black-and-white. It was not until the mid-1960s that color sets started selling in large numbers, due in part to the color transition of 1965 in which it was announced that over half of all network prime-time programming would be broadcast in color that fall. The first all-color prime-time season came just one year later. In 1972, the last holdout among daytime network programs converted to color, resulting in the first completely all-color network season.\n\nEarly color sets were either floor-standing console models or tabletop versions nearly as bulky and heavy; so in practice they remained firmly anchored in one place. The introduction of GE's relatively compact and lightweight Porta-Color set in the spring of 1966 made watching color television a more flexible and convenient proposition. In 1972, sales of color sets finally surpassed sales of black-and-white sets. Color broadcasting in Europe was not standardized on the PAL format until the 1960s, and broadcasts did not start until 1967. By this point many of the technical problems in the early sets had been worked out, and the spread of color sets in Europe was fairly rapid. By the mid-1970s, the only stations broadcasting in black-and-white were a few high-numbered UHF stations in small markets, and a handful of low-power repeater stations in even smaller markets such as vacation spots. By 1979, even the last of these had converted to color and, by the early 1980s, B&W sets had been pushed into niche markets, notably low-power uses, small portable sets, or for use as video monitor screens in lower-cost consumer equipment. By the late 1980s even these areas switched to color sets.\n\nDigital television (DTV) is the transmission of audio and video by digitally processed and multiplexed signals, in contrast to the totally analog and channel separated signals used by analog television. Due to data compression digital TV can support more than one program in the same channel bandwidth. It is an innovative service that represents the first significant evolution in television technology since color television in the 1950s. Digital TV's roots have been tied very closely to the availability of inexpensive, high performance computers. It was not until the 1990s that digital TV became feasible.\n\nIn the mid-1980s, as Japanese consumer electronics firms forged ahead with the development of HDTV technology, the MUSE analog format proposed by NHK, a Japanese company, was seen as a pacesetter that threatened to eclipse U.S. electronics companies' technologies. Until June 1990, the Japanese MUSE standard, based on an analog system, was the front-runner among the more than 23 different technical concepts under consideration. Then, an American company, General Instrument, demonstrated the feasibility of a digital television signal. This breakthrough was of such significance that the FCC was persuaded to delay its decision on an ATV standard until a digitally based standard could be developed.\n\nIn March 1990, when it became clear that a digital standard was feasible, the FCC made a number of critical decisions. First, the Commission declared that the new ATV standard must be more than an enhanced analog signal, but be able to provide a genuine HDTV signal with at least twice the resolution of existing television images.(7) Then, to ensure that viewers who did not wish to buy a new digital television set could continue to receive conventional television broadcasts, it dictated that the new ATV standard must be capable of being \"simulcast\" on different channels.(8)The new ATV standard also allowed the new DTV signal to be based on entirely new design principles. Although incompatible with the existing NTSC standard, the new DTV standard would be able to incorporate many improvements.\n\nThe final standards adopted by the FCC did not require a single standard for scanning formats, aspect ratios, or lines of resolution. This compromise resulted from a dispute between the consumer electronics industry (joined by some broadcasters) and the computer industry (joined by the film industry and some public interest groups) over which of the two scanning processes—interlaced or progressive—would be best suited for the newer digital HDTV compatible display devices. Interlaced scanning, which had been specifically designed for older analogue CRT display technologies, scans even-numbered lines first, then odd-numbered ones. In fact, interlaced scanning can be looked at as the first video compression model as it was partly designed in the 1940s to double the image resolution to exceed the limitations of the television broadcast bandwidth. Another reason for its adoption was to limit the flickering on early CRT screens whose phosphor coated screens could only retain the image from the electron scanning gun for a relatively short duration. However interlaced scanning does not work as efficiently on newer display devices such as Liquid-crystal (LCD), for example, which are better suited to a more frequent progressive refresh rate.\n\nProgressive scanning, the format that the computer industry had long adopted for computer display monitors, scans every line in sequence, from top to bottom. Progressive scanning in effect doubles the amount of data generated for every full screen displayed in comparison to interlaced scanning by painting the screen in one pass in 1/60-second, instead of two passes in 1/30-second. The computer industry argued that progressive scanning is superior because it does not \"flicker\" on the new standard of display devices in the manner of interlaced scanning. It also argued that progressive scanning enables easier connections with the Internet, and is more cheaply converted to interlaced formats than vice versa. The film industry also supported progressive scanning because it offered a more efficient means of converting filmed programming into digital formats. For their part, the consumer electronics industry and broadcasters argued that interlaced scanning was the only technology that could transmit the highest quality pictures then (and currently) feasible, i.e., 1,080 lines per picture and 1,920 pixels per line. Broadcasters also favored interlaced scanning because their vast archive of interlaced programming is not readily compatible with a progressive format. William F. Schreiber, who was director of the Advanced Television Research Program at the Massachusetts Institute of Technology from 1983 until his retirement in 1990, thought that the continued advocacy of interlaced equipment originated from consumer electronics companies that were trying to get back the substantial investments they made in the interlaced technology.\n\nDigital television transition started in late 2000s. All governments across the world set the deadline for analog shutdown by 2010s. Initially the adoption rate was low, as the first digital tuner-equipped TVs were costly. But soon, as the price of digital-capable TVs dropped, more and more households were converting to digital televisions. The transition is expected to be completed worldwide by mid to late 2010s.\n\nThe advent of digital television allowed innovations like smart TVs. A smart television, sometimes referred to as \"connected TV\" or \"hybrid TV\", is a television set or set-top box with integrated Internet and Web 2.0 features, and is an example of technological convergence between computers, television sets and set-top boxes. Besides the traditional functions of television sets and set-top boxes provided through traditional broadcasting media, these devices can also provide Internet TV, online interactive media, over-the-top content, as well as on-demand streaming media, and home networking access. These TVs come pre-loaded with an operating system.\n\nSmart TV should not to be confused with Internet TV, Internet Protocol television (IPTV) or with Web TV. Internet television refers to the receiving of television content over the Internet instead of by traditional systems – terrestrial, cable and satellite (although internet itself is received by these methods). IPTV is one of the emerging Internet television technology standards for use by television broadcasters. Web television (WebTV) is a term used for programs created by a wide variety of companies and individuals for broadcast on Internet TV. A first patent was filed in 1994 (and extended the following year) for an \"intelligent\" television system, linked with data processing systems, by means of a digital or analog network. Apart from being linked to data networks, one key point is its ability to automatically download necessary software routines, according to a user's demand, and process their needs. Major TV manufacturers have announced production of smart TVs only, for middle-end and high-end TVs in 2015. Smart TVs are expected to become dominant form of television by late 2010s.\n\n3D television conveys depth perception to the viewer by employing techniques such as stereoscopic display, multi-view display, 2D-plus-depth, or any other form of 3D display. Most modern 3D television sets use an active shutter 3D system or a polarized 3D system, and some are autostereoscopic without the need of glasses. Stereoscopic 3D television was demonstrated for the first time on 10 August 1928, by John Logie Baird in his company's premises at 133 Long Acre, London. Baird pioneered a variety of 3D television systems using electromechanical and cathode-ray tube techniques. The first 3D TV was produced in 1935. The advent of digital television in the 2000s greatly improved 3D TVs. Although 3D TV sets are quite popular for watching 3D home media such as on Blu-ray discs, 3D programming has largely failed to make inroads with the public. Many 3D television channels which started in the early 2010s were shut down by the mid-2010s.According to DisplaySearch 3D televisions shipments totaled 41.45 million units in 2012, compared with 24.14 in 2011 and 2.26 in 2010. As of late 2013, the number of 3D TV viewers started to decline.\n\nProgramming is broadcast by television stations, sometimes called \"channels\", as stations are licensed by their governments to broadcast only over assigned channels in the television band. At first, terrestrial broadcasting was the only way television could be widely distributed, and because bandwidth was limited, i.e., there were only a small number of channels available, government regulation was the norm. In the U.S., the Federal Communications Commission (FCC) allowed stations to broadcast advertisements beginning in July 1941, but required public service programming commitments as a requirement for a license. By contrast, the United Kingdom chose a different route, imposing a television license fee on owners of television reception equipment to fund the British Broadcasting Corporation (BBC), which had public service as part of its Royal Charter.\n\nWRGB claims to be the world's oldest television station, tracing its roots to an experimental station founded on 13 January 1928, broadcasting from the General Electric factory in Schenectady, NY, under the call letters W2XB. It was popularly known as \"WGY Television\" after its sister radio station. Later in 1928, General Electric started a second facility, this one in New York City, which had the call letters W2XBS and which today is known as WNBC. The two stations were experimental in nature and had no regular programming, as receivers were operated by engineers within the company. The image of a Felix the Cat doll rotating on a turntable was broadcast for 2 hours every day for several years as new technology was being tested by the engineers. On 2 November 1936, the BBC began transmitting the world's first public regular high-definition service from the Victorian Alexandra Palace in north London. It therefore claims to be the birthplace of TV broadcasting as we know it today.\n\nWith the widespread adoption of cable across the United States in the 1970s and 80s, terrestrial television broadcasts have been in decline; in 2013 it was estimated that about 7% of US households used an antenna. A slight increase in use began around 2010 due to switchover to digital terrestrial television broadcasts, which offered pristine image quality over very large areas, and offered an alternate to cable television (CATV) for cord cutters. All other countries around the world are also in the process of either shutting down analog terrestrial television or switching over to digital terrestrial television.\n\nCable television is a system of broadcasting television programming to paying subscribers via radio frequency (RF) signals transmitted through coaxial cables or light pulses through fiber-optic cables. This contrasts with traditional terrestrial television, in which the television signal is transmitted over the air by radio waves and received by a television antenna attached to the television. In the 2000s, FM radio programming, high-speed Internet, telephone service, and similar non-television services may also be provided through these cables. The abbreviation CATV is often used for cable television. It originally stood for Community Access Television or Community Antenna Television, from cable television's origins in 1948: in areas where over-the-air reception was limited by distance from transmitters or mountainous terrain, large \"community antennas\" were constructed, and cable was run from them to individual homes. The origins of cable broadcasting are even older as radio programming was distributed by cable in some European cities as far back as 1924. Earlier cable television was analog, but since the 2000s, all cable operators have switched to, or are in the process of switching to, digital cable television.\n\nSatellite television is a system of supplying television programming using broadcast signals relayed from communication satellites. The signals are received via an outdoor parabolic reflector antenna usually referred to as a satellite dish and a low-noise block downconverter (LNB). A satellite receiver then decodes the desired television program for viewing on a television set. Receivers can be external set-top boxes, or a built-in television tuner. Satellite television provides a wide range of channels and services, especially to geographic areas without terrestrial television or cable television.\n\nThe most common method of reception is direct-broadcast satellite television (DBSTV), also known as \"direct to home\" (DTH). In DBSTV systems, signals are relayed from a direct broadcast satellite on the K wavelength and are completely digital. Satellite TV systems formerly used systems known as television receive-only. These systems received analog signals transmitted in the C-band spectrum from FSS type satellites, and required the use of large dishes. Consequently, these systems were nicknamed \"big dish\" systems, and were more expensive and less popular.\n\nThe direct-broadcast satellite television signals were earlier analog signals and later digital signals, both of which require a compatible receiver. Digital signals may include high-definition television (HDTV). Some transmissions and channels are free-to-air or free-to-view, while many other channels are pay television requiring a subscription.\nIn 1945, British science fiction writer Arthur C. Clarke proposed a worldwide communications system which would function by means of three satellites equally spaced apart in earth orbit. This was published in the October 1945 issue of the \"Wireless World\" magazine and won him the Franklin Institute's Stuart Ballantine Medal in 1963.\n\nThe first satellite television signals from Europe to North America were relayed via the Telstar satellite over the Atlantic ocean on 23 July 1962. The signals were received and broadcast in North American and European countries and watched by over 100 million. Launched in 1962, the \"Relay 1\" satellite was the first satellite to transmit television signals from the US to Japan. The first geosynchronous communication satellite, Syncom 2, was launched on 26 July 1963.\n\nThe world's first commercial communications satellite, called Intelsat I and nicknamed \"Early Bird\", was launched into geosynchronous orbit on 6 April 1965. The first national network of television satellites, called Orbita, was created by the Soviet Union in October 1967, and was based on the principle of using the highly elliptical Molniya satellite for rebroadcasting and delivering of television signals to ground downlink stations. The first commercial North American satellite to carry television transmissions was Canada's geostationary Anik 1, which was launched on 9 November 1972. ATS-6, the world's first experimental educational and Direct Broadcast Satellite (DBS), was launched on 30 May 1974. It transmitted at 860 MHz using wideband FM modulation and had two sound channels. The transmissions were focused on the Indian subcontinent but experimenters were able to receive the signal in Western Europe using home constructed equipment that drew on UHF television design techniques already in use.\n\nThe first in a series of Soviet geostationary satellites to carry Direct-To-Home television, Ekran 1, was launched on 26 October 1976. It used a 714 MHz UHF downlink frequency so that the transmissions could be received with existing UHF television technology rather than microwave technology.\n\nInternet television (Internet TV) (or online television) is the digital distribution of television content via the Internet as opposed to traditional systems like terrestrial, cable, and satellite, although the Internet itself is received by terrestrial, cable, or satellite methods. Internet television is a general term that covers the delivery of television shows, and other video content, over the Internet by video streaming technology, typically by major traditional television broadcasters. Internet television should not be confused with Smart TV, IPTV or with Web TV. Smart television refers to the TV set which has a built-in operating system. Internet Protocol television (IPTV) is one of the emerging Internet television technology standards for use by television broadcasters. Web television is a term used for programs created by a wide variety of companies and individuals for broadcast on Internet TV.\n\nA television set, also called a television receiver, television, TV set, TV, or \"telly\", is a device that combines a tuner, display, an amplifier, and speakers for the purpose of viewing television and hearing its audio components. Introduced in late 1920's in mechanical form, television sets became a popular consumer product after World War II in electronic form, using cathode ray tubes. The addition of color to broadcast television after 1953 further increased the popularity of television sets and an outdoor antenna became a common feature of suburban homes. The ubiquitous television set became the display device for recorded media in the 1970s, such as Betamax and VHS, which enabled viewers to record TV shows and watch prerecorded movies. In the subsequent decades, TVs were used to watch DVDs and Blu-ray Discs of movies and other content. Major TV manufacturers announced the discontinuation of CRT, DLP, plasma and fluorescent-backlit LCDs by the mid-2010s. Televisions since 2010s mostly use LEDs. LEDs are expected to be gradually replaced by OLEDs in near future.\n\nThe earliest systems employed a spinning disk to create and reproduce images. These usually had a low resolution and screen size and never became popular with the public.\n\nThe cathode ray tube (CRT) is a vacuum tube containing one or more electron guns (a source of electrons or electron emitter) and a fluorescent screen used to view images.\nIt has a means to accelerate and deflect the electron beam(s) onto the screen to create the images. The images may represent electrical waveforms (oscilloscope), pictures (television, computer monitor), radar targets or others. The CRT uses an evacuated glass envelope which is large, deep (i.e. long from front screen face to rear end), fairly heavy, and relatively fragile. As a matter of safety, the face is typically made of thick lead glass so as to be highly shatter-resistant and to block most X-ray emissions, particularly if the CRT is used in a consumer product.\n\nIn television sets and computer monitors, the entire front area of the tube is scanned repetitively and systematically in a fixed pattern called a raster. An image is produced by controlling the intensity of each of the three electron beams, one for each additive primary color (red, green, and blue) with a video signal as a reference. In all modern CRT monitors and televisions, the beams are bent by \"magnetic deflection\", a varying magnetic field generated by coils and driven by electronic circuits around the neck of the tube, although electrostatic deflection is commonly used in oscilloscopes, a type of diagnostic instrument.\n\nDigital Light Processing (DLP) is a type of video projector technology that uses a digital micromirror device. Some DLPs have a TV tuner, which makes them a type of TV display. It was originally developed in 1987 by Dr. Larry Hornbeck of Texas Instruments. While the DLP imaging device was invented by Texas Instruments, the first DLP based projector was introduced by Digital Projection Ltd in 1997. Digital Projection and Texas Instruments were both awarded Emmy Awards in 1998 for invention of the DLP projector technology. DLP is used in a variety of display applications from traditional static displays to interactive displays and also non-traditional embedded applications including medical, security, and industrial uses. DLP technology is used in DLP front projectors (standalone projection units for classrooms and business primarily), but also in private homes; in these cases, the image is projected onto a projection screen. DLP is also used in DLP rear projection television sets and digital signs. It is also used in about 85% of digital cinema projection.\n\nA plasma display panel (PDP) is a type of flat panel display common to large TV displays or larger. They are called \"plasma\" displays because the technology utilizes small cells containing electrically charged ionized gases, or what are in essence chambers more commonly known as fluorescent lamps.\nLiquid-crystal-display televisions (LCD TV) are television sets that use LCD display technology to produce images. LCD televisions are much thinner and lighter than cathode ray tube (CRTs) of similar display size, and are available in much larger sizes (e.g., 90-inch diagonal). When manufacturing costs fell, this combination of features made LCDs practical for television receivers. LCD's come in two types: those using cold cathode fluorescent lamps, simply called LCDs and those using LED as backlight called as LEDs.\n\nIn 2007, LCD televisions surpassed sales of CRT-based televisions worldwide for the first time, and their sales figures relative to other technologies accelerated. LCD TVs have quickly displaced the only major competitors in the large-screen market, the plasma display panel and rear-projection television. In mid 2010s LCDs especially LEDs became, by far, the most widely produced and sold television display type. LCDs also have disadvantages. Other technologies address these weaknesses, including OLEDs, FED and SED, but none of these have entered widespread production.\n\nAn OLED (organic light-emitting diode) is a light-emitting diode (LED) in which the emissive electroluminescent layer is a film of organic compound which emits light in response to an electric current. This layer of organic semiconductor is situated between two electrodes. Generally, at least one of these electrodes is transparent. OLEDs are used to create digital displays in devices such as television screens. It is also used for computer monitors, portable systems such as mobile phones, handheld games consoles and PDAs.\n\nThere are two main families of OLED: those based on small molecules and those employing polymers. Adding mobile ions to an OLED creates a light-emitting electrochemical cell or LEC, which has a slightly different mode of operation. OLED displays can use either passive-matrix (PMOLED) or active-matrix (AMOLED) addressing schemes. Active-matrix OLEDs require a thin-film transistor backplane to switch each individual pixel on or off, but allow for higher resolution and larger display sizes.\n\nAn OLED display works without a backlight. Thus, it can display deep black levels and can be thinner and lighter than a liquid crystal display (LCD). In low ambient light conditions such as a dark room an OLED screen can achieve a higher contrast ratio than an LCD, whether the LCD uses cold cathode fluorescent lamps or LED backlight. OLEDs are expected to replace other forms of display in near future.\n\nLow-definition television or LDTV refers to television systems that have a lower screen resolution than standard-definition television systems such 240p (320*240). It is used in handheld television. The most common source of LDTV programming is the Internet, where mass distribution of higher-resolution video files could overwhelm computer servers and take too long to download. Many mobile phones and portable devices such as Apple's iPod Nano, or Sony's PlayStation Portable use LDTV video, as higher-resolution files would be excessive to the needs of their small screens (320×240 and 480×272 pixels respectively). The current generation of iPod Nanos have LDTV screens, as do the first three generations of iPod Touch and iPhone (480×320). For the first years of its existence, YouTube offered only one, low-definition resolution of 320x240p at 30fps or less. A standard, consumer grade VHS videotape can be considered SDTV due to its resolution (approximately 360 × 480i/576i).\n\nStandard-definition television or SDTV refers to two different resolutions: 576i, with 576 interlaced lines of resolution, derived from the European-developed PAL and SECAM systems; and 480i based on the American National Television System Committee NTSC system. SDTV is a television system that uses a resolution that is not considered to be either high-definition television (720p, 1080i, 1080p, 1440p, 4K UHDTV, and 8K UHD) or enhanced-definition television (EDTV 480p). In North America, digital SDTV is broadcast in the same aspect ratio as NTSC signals with widescreen content being center cut. However, in other parts of the world that used the PAL or SECAM color systems, standard-definition television is now usually shown with a aspect ratio, with the transition occurring between the mid-1990s and mid-2000s. Older programs with a 4:3 aspect ratio are shown in the US as 4:3 with non-ATSC countries preferring to reduce the horizontal resolution by anamorphically scaling a pillarboxed image.\n\nHigh-definition television (HDTV) provides a resolution that is substantially higher than that of standard-definition television.\n\nHDTV may be transmitted in various formats:\n\nUltra-high-definition television (also known as Super Hi-Vision, Ultra HD television, UltraHD, UHDTV, or UHD) includes 4K UHD (2160p) and 8K UHD (4320p), which are two digital video formats proposed by NHK Science & Technology Research Laboratories and defined and approved by the International Telecommunication Union (ITU). The Consumer Electronics Association announced on 17 October 2012, that \"Ultra High Definition\", or \"Ultra HD\", would be used for displays that have an aspect ratio of at least 16:9 and at least one digital input capable of carrying and presenting native video at a minimum resolution of 3840×2160 pixels.\n\nNorth American consumers purchase a new television set on average every seven years, and the average household owns 2.8 televisions. , 48 million are sold each year at an average price of $460 and size of .\n\nGetting TV programming shown to the public can happen in many different ways. After production, the next step is to market and deliver the product to whichever markets are open to using it. This typically happens on two levels:\n\n\nFirst-run programming is increasing on subscription services outside the US, but few domestically produced programs are syndicated on domestic free-to-air (FTA) elsewhere. This practice is increasing, however, generally on digital-only FTA channels or with subscriber-only, first-run material appearing on FTA. Unlike the US, repeat FTA screenings of an FTA network program usually only occur on that network. Also, affiliates rarely buy or produce non-network programming that is not centered on local programming.\n\nTelevision genres include a broad range of programming types that entertain, inform, and educate viewers. The most expensive entertainment genres to produce are usually dramas and dramatic miniseries. However, other genres, such as historical Western genres, may also have high production costs.\n\nPopular culture entertainment genres include action-oriented shows such as police, crime, detective dramas, horror, or thriller shows. As well, there are also other variants of the drama genre, such as medical dramas and daytime soap operas. Science fiction shows can fall into either the drama or action category, depending on whether they emphasize philosophical questions or high adventure. Comedy is a popular genre which includes situation comedy (sitcom) and animated shows for the adult demographic such as \"South Park\".\n\nThe least expensive forms of entertainment programming genres are game shows, talk shows, variety shows, and reality television. Game shows feature contestants answering questions and solving puzzles to win prizes. Talk shows contain interviews with film, television, music and sports celebrities and public figures. Variety shows feature a range of musical performers and other entertainers, such as comedians and magicians, introduced by a host or Master of Ceremonies. There is some crossover between some talk shows and variety shows because leading talk shows often feature performances by bands, singers, comedians, and other performers in between the interview segments. \"Reality TV\" shows \"regular\" people (i.e., not actors) facing unusual challenges or experiences ranging from arrest by police officers (\"COPS\") to significant weight loss (\"The Biggest Loser\"). A variant version of reality shows depicts celebrities doing mundane activities such as going about their everyday life (\"The Osbournes\", \"Snoop Dogg's Father Hood\") or doing regular jobs (\"The Simple Life\").\n\nFictional television programs that some television scholars and broadcasting advocacy groups argue are \"quality television\", include series such as \"Twin Peaks\" and \"The Sopranos\". Kristin Thompson argues that some of these television series exhibit traits also found in art films, such as psychological realism, narrative complexity, and ambiguous plotlines. Nonfiction television programs that some television scholars and broadcasting advocacy groups argue are \"quality television\", include a range of serious, noncommercial, programming aimed at a niche audience, such as documentaries and public affairs shows.\n\nAround the globe, broadcast TV is financed by government, advertising, licensing (a form of tax), subscription, or any combination of these. To protect revenues, subscription TV channels are usually encrypted to ensure that only subscribers receive the decryption codes to see the signal. Unencrypted channels are known as free to air or FTA. In 2009, the global TV market represented 1,217.2 million TV households with at least one TV and total revenues of 268.9 billion EUR (declining 1.2% compared to 2008). North America had the biggest TV revenue market share with 39% followed by Europe (31%), Asia-Pacific (21%), Latin America (8%), and Africa and the Middle East (2%). Globally, the different TV revenue sources divide into 45%–50% TV advertising revenues, 40%–45% subscription fees and 10% public funding.\n\nTV's broad reach makes it a powerful and attractive medium for advertisers. Many TV networks and stations sell blocks of broadcast time to advertisers (\"sponsors\") to fund their programming. Television advertisements (variously called a television commercial, commercial or ad in American English, and known in British English as an advert) is a span of television programming produced and paid for by an organization, which conveys a message, typically to market a product or service. Advertising revenue provides a significant portion of the funding for most privately owned television networks. The vast majority of television advertisements today consist of brief advertising spots, ranging in length from a few seconds to several minutes (as well as program-length infomercials). Advertisements of this sort have been used to promote a wide variety of goods, services and ideas since the beginning of television.\n\nThe effects of television advertising upon the viewing public (and the effects of mass media in general) have been the subject of philosophical discourse by such luminaries as Marshall McLuhan. The viewership of television programming, as measured by companies such as Nielsen Media Research, is often used as a metric for television advertisement placement, and consequently, for the rates charged to advertisers to air within a given network, television program, or time of day (called a \"daypart\"). In many countries, including the United States, television campaign advertisements are considered indispensable for a political campaign. In other countries, such as France, political advertising on television is heavily restricted, while some countries, such as Norway, completely ban political advertisements.\n\nThe first official, paid television advertisement was broadcast in the United States on 1 July 1941 over New York station WNBT (now WNBC) before a baseball game between the Brooklyn Dodgers and Philadelphia Phillies. The announcement for Bulova watches, for which the company paid anywhere from $4.00 to $9.00 (reports vary), displayed a WNBT test pattern modified to look like a clock with the hands showing the time. The Bulova logo, with the phrase \"Bulova Watch Time\", was shown in the lower right-hand quadrant of the test pattern while the second hand swept around the dial for one minute. The first TV ad broadcast in the UK was on ITV on 22 September 1955, advertising Gibbs SR toothpaste. The first TV ad broadcast in Asia was on Nippon Television in Tokyo on 28 August 1953, advertising Seikosha (now Seiko), which also displayed a clock with the current time.\n\nSince inception in the US in 1941, television commercials have become one of the most effective, persuasive, and popular methods of selling products of many sorts, especially consumer goods. During the 1940s and into the 1950s, programs were hosted by single advertisers. This, in turn, gave great creative license to the advertisers over the content of the show. Perhaps due to the quiz show scandals in the 1950s, networks shifted to the magazine concept, introducing advertising breaks with multiple advertisers.\n\nUS advertising rates are determined primarily by Nielsen ratings. The time of the day and popularity of the channel determine how much a TV commercial can cost. For example, it can cost approximately $750,000 for a 30-second block of commercial time during the highly popular American Idol, while the same amount of time for the Super Bowl can cost several million dollars. Conversely, lesser-viewed time slots, such as early mornings and weekday afternoons, are often sold in bulk to producers of infomercials at far lower rates. In recent years, the paid program or infomercial has become common, usually in lengths of 30 minutes or one hour. Some drug companies and other businesses have even created \"news\" items for broadcast, known in the industry as video news releases, paying program directors to use them.\n\nSome TV programs also deliberately place products into their shows as advertisements, a practice started in feature films and known as product placement. For example, a character could be drinking a certain kind of soda, going to a particular chain restaurant, or driving a certain make of car. (This is sometimes very subtle, with shows having vehicles provided by manufacturers for low cost in exchange as a product placement). Sometimes, a specific brand or trade mark, or music from a certain artist or group, is used. (This excludes guest appearances by artists who perform on the show.)\n\nThe TV regulator oversees TV advertising in the United Kingdom. Its restrictions have applied since the early days of commercially funded TV. Despite this, an early TV mogul, Roy Thomson, likened the broadcasting licence as being a \"licence to print money\". Restrictions mean that the big three national commercial TV channels: ITV, Channel 4, and Channel 5 can show an average of only seven minutes of advertising per hour (eight minutes in the peak period). Other broadcasters must average no more than nine minutes (twelve in the peak). This means that many imported TV shows from the US have unnatural pauses where the UK company does not utilize the narrative breaks intended for more frequent US advertising. Advertisements must not be inserted in the course of certain specific proscribed types of programs which last less than half an hour in scheduled duration; this list includes any news or current affairs programs, documentaries, and programs for children; additionally, advertisements may not be carried in a program designed and broadcast for reception in schools or in any religious broadcasting service or other devotional program or during a formal Royal ceremony or occasion. There also must be clear demarcations in time between the programs and the advertisements. The BBC, being strictly non-commercial, is not allowed to show advertisements on television in the UK, although it has many advertising-funded channels abroad. The majority of its budget comes from television license fees (see below) and broadcast syndication, the sale of content to other broadcasters.\n\nBroadcast advertising is regulated by the Broadcasting Authority of Ireland,\n\nSome TV channels are partly funded from subscriptions; therefore, the signals are encrypted during broadcast to ensure that only the paying subscribers have access to the decryption codes to watch pay television or specialty channels. Most subscription services are also funded by advertising.\n\nTelevision services in some countries may be funded by a television licence or a form of taxation, which means that advertising plays a lesser role or no role at all. For example, some channels may carry no advertising at all and some very little, including:\n\nThe BBC carries no television advertising on its UK channels and is funded by an annual television licence paid by premises receiving live TV broadcasts. Currently, it is estimated that approximately 26.8 million UK private domestic households own televisions, with approximately 25 million TV licences in all premises in force as of 2010. This television license fee is set by the government, but the BBC is not answerable to or controlled by the government.\n\nThe two main BBC TV channels are watched by almost 90% of the population each week and overall have 27% share of total viewing, despite the fact that 85% of homes are multichannel, with 42% of these having access to 200 free to air channels via satellite and another 43% having access to 30 or more channels via Freeview. The licence that funds the seven advertising-free BBC TV channels costs £147 a year (about US$200) as of 2018 regardless of the number of TV sets owned; the price is reduced by two thirds if only black and white television is received. When the same sporting event has been presented on both BBC and commercial channels, the BBC always attracts the lion's share of the audience, indicating that viewers prefer to watch TV uninterrupted by advertising.\n\nOther than internal promotional material, the Australian Broadcasting Corporation (ABC) carries no advertising; it is banned under the ABC Act 1983. The ABC receives its funding from the Australian government every three years. In the 2014/15 federal budget, the ABC received A$1.11 billion. The funds provide for the ABC's television, radio, online, and international outputs. The ABC also receives funds from its many ABC shops across Australia. Although funded by the Australian government, the editorial independence of the ABC is ensured through law.\n\nIn France, government-funded channels carry advertisements, yet those who own television sets have to pay an annual tax (\"la redevance audiovisuelle\").\n\nIn Japan, NHK is paid for by license fees (known in Japanese as ). The broadcast law that governs NHK's funding stipulates that any television equipped to receive NHK is required to pay. The fee is standardized, with discounts for office workers and students who commute, as well a general discount for residents of Okinawa prefecture.\n\nBroadcast programming, or TV listings in the United Kingdom, is the practice of organizing television programs in a schedule, with broadcast automation used to regularly change the scheduling of TV programs to build an audience for a new show, retain that audience, or compete with other broadcasters' programs.\n\nTelevision has played a pivotal role in the socialization of the 20th and 21st centuries. There are many aspects of television that can be addressed, including negative issues such as media violence. Current research is discovering that individuals suffering from social isolation can employ television to create what is termed a parasocial or faux relationship with characters from their favorite television shows and movies as a way of deflecting feelings of loneliness and social deprivation. Several studies have found that educational television has many advantages. The article \"The Good Things about Television\" argues that television can be a very powerful and effective learning tool for children if used wisely.\n\nWith high lead content in CRTs and the rapid diffusion of new flat-panel display technologies, some of which (LCDs) use lamps which contain mercury, there is growing concern about electronic waste from discarded televisions. Related occupational health concerns exist, as well, for disassemblers removing copper wiring and other materials from CRTs. Further environmental concerns related to television design and use relate to the devices' increasing electrical energy requirements.\n\nA 2017 study in \"The Journal of Human Resources\" found that exposure to cable television reduced cognitive ability and high school graduation rates for boys. This effect was stronger for boys from more educated families. The article suggests a mechanism where light television entertainment crowds out more cognitively stimulating activities.\n\n\n"}
{"id": "8929944", "url": "https://en.wikipedia.org/wiki?curid=8929944", "title": "VisionPLUS", "text": "VisionPLUS\n\nVisionPLUS is a financial software application from First Data Corporation. Originally developed by the Paysys Research and Development Group, this application is mainly used for credit card transaction processing by banks and transaction processing companies, storing and processing credit card, debit card, prepaid, closed end loan accounts and similar financial transactions such as Visa, MasterCard, American Express, Europay, and private label transactions against those accounts. More than 600 million cards around the world are processed on different versions of this application software.\n\nVisionPLUS consists of various modules that work together to manage the life cycle of accounts. VisionPLUS allows financial institutions to configure their own product features and functionality. The main functional modules of VisionPLUS include:\n\nCMS is the core module and plays an important part, as all account-related activities are posted in the CMS module.\n\nVisionPLUS Software was introduced by Paysys International Inc. in 1996. In 2001, First Data Coroporation acquired Paysys and since then VisionPLUS has been marketed by FirstData in Europe and elsewhere.\n\n\nDue to software feature upgrades and compliance mandates from payments schemes like Visa, MasterCard, American Express, and JCB, First Data releases updates twice a year. There are also incremental functional releases and brand new products (modules) released from time to time. These are referred to as VisionPLUS 8.nn (where nn stands for a number between 01 and 99).\n\n"}
{"id": "78486", "url": "https://en.wikipedia.org/wiki?curid=78486", "title": "W. E. Shewell-Cooper", "text": "W. E. Shewell-Cooper\n\nDr. Wilfred Edward Shewell-Cooper, M.B.E., N.D.H., F.L.S., F.R.S.L., F.R.H.S., Dip. Hort. (Wye) (1900–1982) was a British organic gardener and pioneer of no dig gardening. He was the author of \"Soil, Humus and Health (1975)\", \"The Royal Gardeners (1952)\", \"Grow your own food supply (1939)\", \"The ABC of Vegetable Gardening (1937)\" and many other books on gardening. He was the founder in 1966 of the Good Gardeners Association. For many years his gardens at Arkley Manor were open to the public so his no dig methods, symbolised by a robin resting on a spade handle, could be seen first hand.\n\nHe married Irene, with whom he wrote a cookery book \"Cook what you grow (1940)\". He had two sons Ramsay and Jeremy. His son Ramsay continues to promote his no-dig gardening approach, and a demonstration plot may be seen at Capel Manor College in Enfield in conjunction with the Good Gardeners' Association as of 2008.\n\nHe was born at Waltham Abbey, Essex in 1900 where his father was a major in the Royal Artillery and at the time the assistant superintendent of the gunpowder factory there. Moving from there to Blackheath then Penarth. Then before the outbreak of the first world war the family set sail on the Galaka for South Africa where they lived in Rondesbosch. While there he went to school at Diocesan College, Rondesbosch then to Monkton Combe School outside Bath in England.\n\nIn 1960 he moved to Arkley Manor, as recommended by Sir John Laing, which was to be his home for the rest of his life.\n\nOver the course of his life, Shewell-Cooper held a number of positions, some of which are listed below:\n\nHis published works include: \nThe book of the Tomato (1948)\n\n"}
{"id": "4766438", "url": "https://en.wikipedia.org/wiki?curid=4766438", "title": "W67", "text": "W67\n\nThe W67 was an American thermonuclear warhead which was developed in the mid-1960s but then cancelled prior to any production or service use in 1967.\n\nThe W67 was intended to be a new warhead for both land and sea-based ballistic missiles, specifically the UGM-73 Poseidon and Minuteman III (LGM-30G) missiles.\n\nThe design was developed by Lawrence Radiation Laboratory (now Lawrence Livermore National Laboratory).\n\nAfter its cancellation, the W68 warhead was used for the Poseidon and the W78 for the Minuteman III.\n\n\n"}
{"id": "10554676", "url": "https://en.wikipedia.org/wiki?curid=10554676", "title": "Zolotaya Korona", "text": "Zolotaya Korona\n\nZolotaya Korona (, literally - \"Golden Crown\") is a Russian payment system that provides a range of consumer payment services, including Online Card-2-Cash RUB-USD/EUR exchange service, Cash-2-Cash, Online Card-2-Cash, Online Card-2-Card money transfers, loan repayments as well as b2b payment services for banks and organizations. Zolotaya Korona is officially recognized as socially important payment system in Russia and Kazakhstan.\n\nParticipants of payment services are 550 banks, geography of operations includes 32 countries. Zolotaya Korona was created in 1994 by CFT Group. Total turnover in 2016 was 745 billion rubles.\n\nPayment system Zolotaya Korona was founded in 1994 in Novosibirsk. By 1999 it united 144 participants in 58 regions of Russian Federation. In 2001 it implemented support of emission and acquiring of chip Debit/Credit cards according to EMV specifications.\n\nIn 2006 Russian payment system Zolotaya Korona won open tender of Novosibirsk city hall for supply of Social cards. \n\nIn 2008 Zolotaya Korona became a laureate of biggest national award in financial sphere \"Financial elite of the South\" in nomination \"Most reliable bank card\".\n\nIn October 2009, Zolotaya Korona became a member of \"Sibirian Transport Union\". \n\nZolotaya Korona technology is used in Turkmenistan (Turkmencard), Kyrgyzstan (AlayCard), and national credit organization \"ElBank-Lucas\" (Poland).\n\nIn 2012 operation of Zolotaya Korona was prohibited in the territory of Armenia because of the violation of law and licence. \n\nIn September 2013 Zolotaya Korona was officially recognized as socially important payment system by the Central Bank of Russia.\n\nIn October 2016 Ukraine banned operations of Zolotaya Korona and all other Russian payment systems as part of its sanction policy against the Russian military intervention in Ukraine.\n\nIn February 2017 Zolotaya Korona was officially recognized as socially important payment system by the National Bank of Kazakhstan.\n\nMoney transfer - Cash-2-Cash, Online Card-2-Cash, Online Card-2-Card money transfers.\n\nOnline RUB-USD/EUR Exchange - 24/7 Card-2-Cash service.\n\nBank cards - debit, credit, prepaid, corporate, etc.\n\nLoan repayments - Cash-beased payments in bank and retail POS network.\n\nTransport and social cards - Zolotaya Korona projects in Chelyabinsk, Novosibirsk, Samara, Omsk, Orenburg, Gorno-Altaysk. \n\nLoyalty cards - bonus, gift, discount, prepaid, etc. \n\nOn 17 November 2016 Russian investigators searched the offices of CFT, owner of Zolotaya Korona, in connection to the ongoing investigation of the criminal case against Group Board Member and Director of Zolotaya Korona Nikolai Smirnov and others under article 127.1 of the Criminal Code ( \"human trafficking\"). \n\n"}
