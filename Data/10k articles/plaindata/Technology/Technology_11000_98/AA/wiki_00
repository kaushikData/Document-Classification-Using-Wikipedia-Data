{"id": "1048693", "url": "https://en.wikipedia.org/wiki?curid=1048693", "title": "Active camouflage", "text": "Active camouflage\n\nActive camouflage or adaptive camouflage is camouflage that adapts, often rapidly, to the surroundings of an object such as an animal or military vehicle. In theory, active camouflage could provide perfect concealment from visual detection.\n\nActive camouflage is used in several groups of animals, including reptiles on land, and cephalopod molluscs and flatfish in the sea. Animals achieve active camouflage both by color change and (among marine animals such as squid) by counter-illumination, with the use of bioluminescence.\n\nMilitary counter-illumination camouflage was first investigated during World War II for marine use. More recent research has aimed to achieve crypsis by using cameras to sense the visible background, and by controlling Peltier panels or coatings that can vary their appearance.\n\nActive camouflage provides concealment by making an object not merely generally similar to its surroundings, but effectively invisible with \"illusory transparency\" through accurate mimicry, and by changing the appearance of the object as changes occur in its background.\n\nMilitary interest in active camouflage has its origins in Second World War studies of counter-illumination. The first of these was the so-called diffused lighting camouflage tested on Canadian Navy corvettes including HMCS Rimouski. This was followed in the armed forces of the United States of America with the airborne Yehudi lights project, and trials in ships of the Royal Navy and the US Navy. The Yehudi lights project placed low-intensity blue lights on aircraft. As skies are bright, an unilluminated aircraft (of any color) might be rendered visible. By emitting a small, measured amount of blue light, the aircraft's average brightness better matches that of the sky, and the aircraft is able to fly closer to its target before being detected.\n\nActive camouflage may now develop using organic light-emitting diodes (OLEDs) and other technologies which allow for images to be projected onto irregularly shaped surfaces. Using visual data from a camera, an object could perhaps be camouflaged well enough to avoid detection by the human eye and optical sensors when stationary. Camouflage is weakened by motion, but active camouflage could still make moving targets more difficult to see. However, active camouflage works best in one direction at a time, requiring knowledge of the relative positions of the observer and the concealed object.\n\nActive camouflage technology exists only in theory and proof-of-concept prototypes. In 2003 researchers at the University of Tokyo under Susumu Tachi created a prototype active camouflage system using material impregnated with retroreflective glass beads. The viewer stands in front of the cloth viewing the cloth through a transparent glass plate. A video camera behind the cloth captures the background behind the cloth. A video projector projects this image on to the glass plate which is angled so that it acts as a partial mirror reflecting a small portion of the projected light onto the cloth. The retroreflectors in the cloth reflect the image back towards the glass plate which being only weakly reflecting allows most of the retroreflected light to pass through to be seen by the viewer. The system only works when seen from a certain angle.\n\nPhased-array optics (PAO) would implement active camouflage, not by producing a two-dimensional image of background scenery on an object, but by computational holography to produce a three-dimensional hologram of background scenery on an object to be concealed. Unlike a two-dimensional image, the holographic image would appear to be the actual scenery behind the object independent of viewer distance or view angle.\n\nIn 2010, the Israeli company Eltics created an early prototype of a system of tiles for infrared camouflage of vehicles. In 2011, BAE Systems announced their \"Adaptiv\" infrared camouflage technology. It uses about 1000 hexagonal Peltier panels to cover the sides of a tank. The panels are rapidly heated and cooled to match either the temperature of the vehicle's surroundings, or one of the objects in the thermal cloaking system's \"library\" such as a truck, car or large rock.\n\nActive camouflage is used in several groups of animals including cephalopod molluscs, fish, and reptiles. There are two mechanisms of active camouflage in animals: Counterillumination camouflage, and color change.\n\nCounterillumination is camouflage using the production of light to blend in against a lit background. In the sea, light comes down from the surface, so when marine animals are seen from below, they appear darker than the background. Some species of cephalopod, such as the eye-flash squid and the firefly squid, produce light in photophores on their undersides to match the background. Bioluminescence is common among marine animals, so counterillumination may be widespread, though light has other functions, including attracting prey and signalling.\n\nColor change permits camouflage against different backgrounds. Many cephalopods including octopuses, cuttlefish, and squids, and some terrestrial amphibians and reptiles including chameleons and anoles can rapidly change color and pattern, though the major reasons for this include signalling, not only camouflage. Cephalopod active camouflage has stimulated military research in the United States.\n\nActive camouflage by color change is used by many bottom-living flatfish such as plaice, sole, and flounder that actively copy the patterns and colors of the seafloor below them. For example, the tropical flounder \"Bothus ocellatus\" can match its pattern to \"a wide range of background textures\" in 2–8 seconds. Similarly, the coral reef fish, the seaweed blenny can match its coloration to its surroundings.\n\nThe eponymous antagonists in the \"Predator\" films use active camouflage. Many video games, such as\nthe \"Halo\" series, \n\"\", and the \"Crysis\" series, players can obtain and use cloaking devices.\n\n"}
{"id": "25417487", "url": "https://en.wikipedia.org/wiki?curid=25417487", "title": "Alpenstock", "text": "Alpenstock\n\nAn alpenstock is a long wooden pole with an iron spike tip, used by shepherds for travel on snowfields and glaciers in the Alps since the Middle Ages. It is the antecedent of the modern ice axe. \n\nFrench-speaking climbers called this item a \"baton\". Josias Simler, a Swiss professor of theology at what later became the University of Zurich, published the first treatise on the Alps, entitled \"De Alpibus commentarius\". T. Graham Brown described Simler's observations on gear for travel over ice and snow in the mountains: \"In 1574, Simler published a commentary on the Alps which is remarkable for its description of the technique of glacier travel and for its proof that Simler himself had practical experience. He describes the alpenstock, crampons, the use of the rope, the necessity of protecting the eyes on snow by veils or spectacles; and he mentions that the leader on snow covered glaciers sounds for hidden crevasses with a pole.\" \n\nYvon Chouinard quotes Simler as writing, \"To counteract the slipperiness of the ice, they firmly attach to their feet shoes resembling the shoes of horses, with three sharp spikes in them, so that they may be able to stand firmly. In some places they use sticks tipped with iron, by leaning upon which they climb steep slopes. These are called alpine sticks, and are principally in use among the shepherds.\"\n\nOn August 8, 1786, Jacques Balmat and Michel-Gabriel Paccard made the first ascent of Mont Blanc. Balmat, a chamois hunter and crystal collector, had experience with high mountain travel, and Paccard had made previous attempts to climb the peak. Illustrations show Balmat carrying two separate tools (whose respective functions would later be re-assigned to the ice axe): an alpenstock (or baton), and a small axe that could be used to chop steps on icy slopes.\n\nIn the second half of the nineteenth century, seeing that the traditional but unwieldy alpenstock might be a useful aid to climb steep slopes of snow or ice, Victorian alpinists fastened a sharpened blade (the pick) to the top of the alpenstock; this was used to provide stability. On the opposite end, a flattened blade was placed (the adze), which was used for cutting steps in the snow or ice, an essential technique for moving over steep icy slopes before the advent of the crampon. Gradually, the alpenstock evolved into the ice axe.\n\n\n\n\"Most of the people, both male and female, are in walking costume, and carry alpenstocks. Evidently, it is not considered safe to go about in Switzerland, even in town, without an alpenstock. If the tourist forgets and comes down to breakfast without his alpenstock, he goes back and gets it, and stands it up in the corner. When his touring in Switzerland is finished, he does not throw that broomstick away, but lugs it home with him, to the far corners of the earth, although this costs him more trouble and bother than a baby or a courier could. You see, the alpenstock is his trophy; his name is burned upon it; and if he has climbed a hill, or jumped a brook, or traversed a brickyard with it, he has the names of those places burned upon it, too.\"\"Thus it is his regimental flag, so to speak, and bears the record of his achievements. It is worth three francs when he buys it, but a bonanza could not purchase it after his great deeds have been inscribed upon it. There are artisans all about Switzerland whose trade it is to burn these things upon the alpenstock of the tourist. And observe, a man is respected in Switzerland according to his alpenstock. I found I could get no attention there, while I carried an unbranded one. However, branding is not expensive, so I soon remedied that. The effect upon the next detachment of tourists was very marked. I felt repaid for my trouble.\" \n\n\n"}
{"id": "10405822", "url": "https://en.wikipedia.org/wiki?curid=10405822", "title": "Analogic Corporation", "text": "Analogic Corporation\n\nAnalogic is an American multinational corporation. Analogic Corporation currently employs 1,500 employees worldwide with approximately 900 working at the main facility and headquarters in Peabody, Massachusetts.\n\nBernard M. Gordon, chairman emeritus, founded Analogic in 1967.\n\nAnalogic provides imaging systems and technology that enable computed tomography (CT), ultrasound, digital mammography, and magnetic resonance imaging (MRI), as well as automated threat detection for aviation security. The company's CT, MRI, digital mammography, and ultrasound transducer products are sold to original equipment manufacturers (OEMs). Its BK Ultrasound branded ultrasound systems, used in procedure-driven markets such as urology, guided surgery, and anesthesia, are sold to clinical end users through Analogic's direct sales force.\n"}
{"id": "43379", "url": "https://en.wikipedia.org/wiki?curid=43379", "title": "Ballista", "text": "Ballista\n\nThe ballista (Latin, from Greek βαλλίστρα \"ballistra\" and that from βάλλω \"ballō\", \"throw\"), plural ballistae, sometimes called bolt thrower, was an ancient missile weapon that launched a large projectile at a distant target.\n\nDeveloped from earlier Greek weapons, it relied upon different mechanics, using two levers with torsion springs instead of a tension prod (the bow part of a modern crossbow), offering much greater efficiency over tension-based weaponry. The springs consisting of several loops of twisted skeins. Early versions projected heavy darts or spherical stone projectiles of various sizes for siege warfare. It developed into a smaller precision weapon, the scorpio, and possibly the polybolos.\n\nThe early ballista in Ancient Greece was developed from two weapons called oxybeles and gastraphetes. The gastraphetes ('belly-bow') was a handheld crossbow. It had a composite prod and was spanned by bracing the front end of the weapon against the ground while placing the end of a slider mechanism against the stomach. The operator would then walk forward to arm the weapon while a ratchet prevented it from shooting during loading. This produced a weapon which, it was claimed, could be operated by a person of average strength but which had a power that allowed it to be successfully used against armoured troops. The oxybeles was a bigger and heavier construction employing a winch, and was mounted on a tripod. It had a lower rate of fire and was used as a siege engine.\nWith the invention of torsion spring bundle technology, the first ballista was built. The advantage of this new technology was the fast relaxation time of this system. Thus it was possible to shoot lighter projectiles with higher velocities over a longer distance.\n\nFor an oxybeles, the rules of a torsion weapon demanded that the more energy could be stored, the thicker the prod and the heavier the projectile had to be to increase the amount of stored energy delivered to the projectile. The earliest form of the ballista is thought to have been developed for Dionysius of Syracuse, circa 400 BCE.\n\nThe Greek ballista was a siege weapon. All components that were not made of wood were transported in the baggage train. It would be assembled with local wood, if necessary. Some were positioned inside large, armoured, mobile siege towers or even on the edge of a battlefield. For all of the tactical advantages offered, it was only under Philip II of Macedon, and even more so under his son Alexander, that the ballista began to develop and gain recognition as both a siege engine and field artillery. Historical accounts, for instance, cited that Philip II employed a group of engineers within his army to design and build catapults for his military campaigns. There is even a claim that it was Philip II - with his team of engineers - who invented the ballista after improving Dionysius's device, which was merely an oversized slingshot. It was further perfected by Alexander, whose own team of engineers introduced innovations such as the idea of using springs made from tightly strung coils of rope instead of a bow to achieve more energy and power when throwing projectiles. Polybius reported about the usage of smaller, more portable ballistae, called scorpions, during the Second Punic War.\n\nBallistae could be easily modified to shoot both spherical and shaft projectiles, allowing their crews to adapt easily to prevailing battlefield situations in real time.\n\nAs the role of battlefield artillery became more sophisticated, a universal joint (which was invented just for this function) was integrated into the ballista's stand, allowing the operators to alter the trajectory and firing direction of the ballista as required without a lengthy disassembly of the machine.\n\nAfter the absorption of the Ancient Greek city-states into the Roman Republic in 146 BCE, the highly advanced Greek technology began to spread across many areas of Roman influence. This included the great military machine advances the Greeks had made (most notably by Dionysus of Syracuse), as well as all the scientific, mathematical, political and artistic developments.\n\nThe Romans 'inherited' the torsion-powered ballista, which had by now spread to several cities around the Mediterranean, all of which became Roman spoils of war, including one from Pergamum, which was depicted among a pile of trophy weapons in relief on a balustrade.\n\nThe torsion ballista, developed by Alexander, was a far more complicated weapon than its predecessor and the Romans developed it even further, especially into much smaller versions, that could be easily carried.\n\nThe early Roman ballistae were made of wood, and held together with iron plates around the frames and iron nails in the stand. The main stand had a slider on the top, into which were loaded the bolts or stone \"shot\". Attached to this, at the back, was a pair of 'winches' and a 'claw', used to ratchet the bowstring back to the armed firing position.\n\nThe slider passed through the \"field frames\" of the weapon, in which were located the \"torsion springs\" (rope made of animal sinew), which were twisted around the bow arms, which in turn, were attached to the bowstring.\n\nDrawing the bowstring back with the winches twisted the already taut springs, storing the energy to fire the projectiles. The bronze or iron caps, which secured the torsion-bundles were adjustable by means of pins and peripheral holes, which allowed the weapon to be tuned for symmetrical power and for changing weather conditions.\n\nThe ballista was a highly accurate weapon (there are many accounts of single soldiers being picked off by ballista operators), but some design aspects meant it could compromise its accuracy for range. The maximum range was over , but effective combat range for many targets was far shorter.\n\nThe Romans continued the development of the ballista, and it became a highly prized and valued weapon in the army of the Roman Empire.\n\nIt was used, just before the start of the Empire, by Julius Caesar during his conquest of Gaul and on both of his campaigns in subduing Britain.\n\nThe first of Caesar's invasions of Britain took place in 55 BCE, after a rapid and successful initial conquest of Gaul, in part as an expedition, and more practically to try to put an end to the reinforcements sent by the native Britons to fight the Romans in Gaul.\n\nA total of eighty transports, carrying two legions, attempted to land on the British shore, only to be driven back by the many British warriors assembled along the shoreline. The ships had to unload their troops on the beach, as it was the only one suitable for many miles, yet the massed ranks of British charioteers and javeliners were making it difficult.\n\nSeeing this, Caesar ordered the warships – which were swifter and easier to handle than the transports, and likely to impress the natives more by their unfamiliar appearance – to be removed a short distance from the others, and then be rowed hard and run ashore on the enemy’s right flank, from which position the slings, bows and artillery could be used by men on deck to drive them back. This manoeuvre was highly successful.<br> Scared by the strange shape of the warships, the motion of the oars, and the unfamiliar machines, the natives halted and retreated. \"(Caesar, The Conquest of Gaul, p.99)\n\nIn Gaul, the stronghold of Alesia was under a Roman siege in 52 BCE, and was completely surrounded by a Roman fortification including a wooden palisade and towers. As was standard siege technique at the time, small ballistae were placed in the towers with other troops armed with bows or slings. The use of the ballista in the Roman siege strategy was also demonstrated in the case of the Siege of Masada.\n\nDuring the conquest of the Empire, the ballista proved its worth many times in sieges and battles, both at sea and on land. It was even used to quell riots. It is from the time of the Roman Empire that many of the archaeological finds of ballistae date. Accounts by the finders, including technical manuals and journals, are used today by archaeologists to reconstruct these weapons.\n\nAfter Julius Caesar, the ballista was a permanent fixture in the Roman army and, over time, modifications and improvements were made by successive engineers. This included replacing the remaining wooden parts of the machine with metal, creating a much smaller, lighter and more powerful machine than the wooden version, which required less maintenance (though the vital torsion springs were still vulnerable to the strain). The largest ballistae of the 4th century could throw a dart further than 1200 yards (1,100 m). The weapon was named \"ballista fulminalis\" in De Rebus Bellicis: \"From this ballista, darts were projected not only in great number but also at a large size over a considerable distance, such as across the width of the Danube River.\" Ballistae were not only used in laying siege: after  350, at least 22 semi-circular towers were erected around the walls of Londinium (London) to provide platforms for permanently mounted defensive devices.\n\nDuring the 6th century, Procopius described the effects of this weapon:\n\nBut Belisarius placed upon the towers engines which they call \"ballistae\". Now these engines have the form of a bow, but on the under side of them a grooved wooden shaft projects; this shaft is so fitted to the bow that it is free to move, and rests upon a straight iron bed. So when men wish to shoot at the enemy with this, they make the parts of the bow which form the ends bend toward one another by means of a short rope fastened to them, and they place in the grooved shaft the arrow, which is about one half the length of the ordinary missiles which they shoot from bows, but about four times as wide...but the missile is discharged from the shaft, and with such force that it attains the distance of not less than two bow-shots, and that, when it hits a tree or a rock, it pierces it easily. Such is the engine which bears this name, being so called because it shoots with very great force\"\n\nThe missiles were able to penetrate body-armour:\n\nAnd at the Salarian Gate a Goth of goodly stature and a capable warrior, wearing a corselet and having a helmet on his head, a man who was of no mean station in the Gothic nation, refused to remain in the ranks with his comrades, but stood by a tree and kept shooting many missiles at the parapet. But this man by some chance was hit by a missile from an engine which was on a tower at his left. And passing through the corselet and the body of the man, the missile sank more than half its length into the tree, and pinning him to the spot where it entered the tree, it suspended him there a corpse.\n\nThe cheiroballistra and the manuballista are held by many archaeologists to be the same weapon. The difference in name may be attributable to the different languages spoken in the Empire. Latin remained the official language in the Western Empire, but the Eastern Empire predominantly used Greek, which added an extra 'r' to the word ballista.\n\nThe manuballista was a handheld version of the traditional ballista. This new version was made entirely of iron, which conferred greater power to the weapon, since it was smaller, and less iron (an expensive material before the 19th century), was used in its production. It was not the ancient gastraphetes, but the Roman weapon. However, the same physical limitations applied as with the gastraphetes.\n\nThe carroballista was a cart-mounted version of the weapon. There were probably different models of ballista under the \"cheiroballistra\" class, at least two different two-wheeled models and one model with four wheels. Their probable size was roughly 1.47 m width, i.e., 5 Roman feet. The cart system and structure gave it a great deal of flexibility and capability as a battlefield weapon, since the increased maneuverability allowed it to be moved with the flow of the battle. This weapon features several times on Trajan's Column.\n\nIt has been speculated that the Roman military may have also fielded a 'repeating' ballista, also known as a \"polybolos\". Reconstruction and trials of such a weapon carried out in a BBC documentary, \"What the Romans Did For Us\", showed that they \"were able to shoot eleven bolts a minute, which is almost four times the rate at which an ordinary ballista can be operated\". However, no example of such a weapon has been found by archaeologists.\n\nArchaeology, and in particular experimental archaeology has been influential on this subject. Although several ancient authors (such as Vegetius) wrote very detailed technical treatises, providing us with all the information necessary to reconstruct the weapons, all their measurements were in their native language and therefore highly difficult to translate.\n\nAttempts to reconstruct these ancient weapons began at the end of the 19th century, based on rough translations of ancient authors. It was only during the 20th century, however, that many of the reconstructions began to make any sense as a weapon. By bringing in modern engineers, progress was made with the ancient systems of measurement. By redesigning the reconstructions using the new information, archaeologists in that specialty were able to recognize certain finds from Roman military sites, and identify them as ballistae. The information gained from the excavations was fed into the next generation of reconstructions and so on.\n\nSites across the empire have yielded information on ballistae, from Spain (the Ampurias Catapult), to Italy (the Cremona Battleshield, which proved that the weapons had decorative metal plates to shield the operators), to Iraq (the Hatra Machine) and even Scotland (Burnswark siege tactics training camp), and many other sites between.\n\nThe most influential archaeologists in this area have been Peter Connolly and Eric Marsden, who have not only written extensively on the subject but have also made many reconstructions themselves and have refined the designs over many years of work.\n\nWith the decline of the Roman Empire, resources to build and maintain these complex machines became very scarce, so the ballista was supplanted initially by the simpler and cheaper onager and the more efficient springald.\n\nThough the weapon continued to be used in the Middle Ages, it faded from popular use with the advent of the trebuchet and mangonel in siege warfare.\n\n\n"}
{"id": "21090792", "url": "https://en.wikipedia.org/wiki?curid=21090792", "title": "Bango plc", "text": "Bango plc\n\nBango is a mobile payment company for mobile app stores to power direct carrier billing for their customers. Bango enables app store customers to click and buy apps or in-app content, placing the charge directly onto their mobile phone bill. The company is listed on the London Stock Exchange (ticker symbol BGO.L).\n\nBango app store partners include Google for Google Play, Amazon for Amazon Appstore, Samsung for GALAXY Apps, BlackBerry for BlackBerry World and BlackBerry Messenger (BBM), Mozilla for Firefox Marketplace and Microsoft for Windows 10 and Windows Phone Store.\n\nBango has direct billing relationships with hundreds of mobile operators worldwide. According to Progressive Equity Research, \"Bango dominates the third party carrier billing marketplace with over 40% of the total app store direct carrier billing connections\"\n\nIn November 2015, it announced its $17 million raised in funding, valuing the company now at $91 million. In January 2018, Bango plc partnered with Netflix to launch carrier billing for Netflix subscriptions in Mexico. Bango and 9mobile became partners to launch operator-billed payments for Google Play users. In January 2018, Bango plc launched carrier billing for Netflix subscriptions.\n\n"}
{"id": "8098322", "url": "https://en.wikipedia.org/wiki?curid=8098322", "title": "Biodrying", "text": "Biodrying\n\nBiodrying is the process by which biodegradable waste is rapidly heated through initial stages of composting to remove moisture from a waste stream and hence reduce its overall weight. \nIn biodrying processes, the drying rates are augmented by biological heat in addition to forced aeration. The major portion of biological heat, naturally available through the aerobic degradation of organic matter, is utilized to evaporate surface and bound water associated with the mixed sludge. This heat generation assists in reducing the moisture content of the biomass without the need for supplementary fossil fuels, and with minimal electricity consumption. It can take as little as 8 days to dry waste in this manner. This enables reduced costs of disposal if landfill is charged on a cost per tonne basis. Biodrying may be used as part of the production process for refuse-derived fuels. \nBiodrying does not however greatly affect the biodegradability of the waste and hence is not stabilised. Biodried waste will still break down in a landfill to produce landfill gas and hence potentially contribute to climate change. In the UK this waste will still impact upon councils LATS allowances. Whilst biodrying is increasingly applied within commercial mechanical biological treatment (MBT) plants, it is also still subject to on-going research and development.\n\n"}
{"id": "33735176", "url": "https://en.wikipedia.org/wiki?curid=33735176", "title": "Bridging fault", "text": "Bridging fault\n\nIn electronic engineering, a bridging fault consists of two signals that are connected when they should not be. Depending on the logic circuitry employed, this may result in a wired-OR or wired-AND logic function. Since there are O(n^2) potential bridging faults, they are normally restricted to signals that are physically adjacent in the design.\n\nBridging to VDD or Vss is equivalent to stuck at fault model. Traditionally bridged signals were modeled with logic AND or OR of signals. If one driver dominates the other driver in a bridging situation, the dominant driver forces the logic to the other one, in such case a \"dominant bridging fault\" is used. To better reflect the reality of CMOS VLSI devices, a \"dominant AND or dominant OR bridging fault model\" is used where dominant driver keeps its value, while the other signal value is the result of AND (or OR) of its own value with the dominant driver.\n\n"}
{"id": "6879785", "url": "https://en.wikipedia.org/wiki?curid=6879785", "title": "Bullwheel", "text": "Bullwheel\n\nA bullwheel or bull wheel is a large wheel on which a rope turns, such as in a chairlift. In that application, the bullwheel that is attached to the is called the drive bullwheel, and the other is the return bullwheel.\n\nOriginally, \"bullwheel\" was an oil field term applied to the large wheel that turns the drum upon which the drilling line is wound in percussion drilling.\n\nThe bullwheel began use in farm implements with the reaper. The term described the traveling wheel, traction wheel, drive wheel, or harvester wheel. The bullwheel powered all the moving parts of these farm machines including the reciprocating knives, reel, rake, and self binder. The bullwheel's outer surface provided traction against the ground and turned when the draft animals or tractor pulled the implement forward. Cyrus McCormick used the bullwheel to power his 1834 reaper and until the early 1920s when small internal combustion engine gasoline engines like the Cushman Motor began to be favored.\n"}
{"id": "10147369", "url": "https://en.wikipedia.org/wiki?curid=10147369", "title": "Business analytics", "text": "Business analytics\n\nBusiness analytics (BA) refers to the skills, technologies, practices for continuous iterative exploration and investigation of past business performance to gain insight and drive business planning. Business analytics focuses on developing new insights and understanding of business performance based on data and statistical methods. In contrast, business intelligence traditionally focuses on using a consistent set of metrics to both measure past performance and guide business planning, which is also based on data and statistical methods.\n\nBusiness analytics makes extensive use of statistical analysis, including explanatory and predictive modeling, and fact-based management to drive decision making. It is therefore closely related to management science. Analytics may be used as input for human decisions or may drive fully automated decisions. Business intelligence is querying, reporting, online analytical processing (OLAP), and \"alerts.\"\n\nIn other words, querying, reporting, OLAP, and alert tools can answer questions such as what happened, how many, how often, where the problem is, and what actions are needed. Business analytics can answer questions like why is this happening, what if these trends continue, what will happen next (predict), and what is the best outcome that can happen (optimize).\n\nBanks, such as Capital One, use data analysis (or \"analytics\", as it is also called in the business setting), to differentiate among customers based on credit risk, usage and other characteristics and then to match customer characteristics with appropriate product offerings. Harrah’s, the gaming firm, uses analytics in its customer loyalty programs. E & J Gallo Winery quantitatively analyses and predicts the appeal of its wines. Between 2002 and 2005, Deere & Company saved more than $1 billion by employing a new analytical tool to better optimize inventory. A telecoms company that pursues efficient call center usage over customer service may save money as well.\n\n\n\nAnalytics have been used in business since the management exercises were put into place by Frederick Winslow Taylor in the late 19th century. Henry Ford measured the time of each component in his newly established assembly line. But analytics began to command more attention in the late 1960s when computers were used in decision support systems. Since then, analytics have changed and formed with the development of enterprise resource planning (ERP) systems, data warehouses, and a large number of other software tools and processes.\n\nIn later years the business analytics have exploded with the introduction to computers. This change has brought analytics to a whole new level and has brought about endless possibilities. As far as analytics has come in history, and what the current field of analytics is today, many people would never think that analytics started in the early 1900s with Mr. Ford himself.\n\nBusiness analytics depends on sufficient volumes of high quality data. The difficulty in ensuring data quality is integrating and reconciling data across different systems, and then deciding what subsets of data to make available.\n\nPreviously, analytics was considered a type of after-the-fact method of forecasting consumer behavior by examining the number of units sold in the last quarter or the last year. This type of data warehousing required a lot more storage space than it did speed. Now business analytics is becoming a tool that can influence the outcome of customer interactions. When a specific customer type is considering a purchase, an analytics-enabled enterprise can modify the sales pitch to appeal to that consumer. This means the storage space for all that data must react extremely fast to provide the necessary data in real-time.\n\nThomas Davenport, professor of information technology and management at Babson College argues that businesses can optimize a distinct business capability via analytics and thus better compete. He identifies these characteristics of an organization that are apt to compete on analytics:\n\n"}
{"id": "2218995", "url": "https://en.wikipedia.org/wiki?curid=2218995", "title": "Catherine wheel (firework)", "text": "Catherine wheel (firework)\n\nThe Catherine wheel or pinwheel is a type of firework consisting of a powder-filled spiral tube, or an angled rocket mounted with a pin through its center. When ignited, it rotates quickly, producing a display of sparks and coloured flame.\n\nThe firework is named after Saint Catherine of Alexandria who, according to Christian tradition, was condemned to death by “breaking on the wheel”. When she touched the wheel it miraculously flew to pieces.\n\nThe largest Catherine wheel ever made was designed by the Lily Fireworks Factory of Mqabba, Malta. The Catherine wheel had a diameter of , and was lit on 18 June 2011, the eve of the annual feast of \"Our Lady of the Lilies\". \n\nIn Malta, Catherine wheels are a traditional fixture during every village 'festa'. Some villages even hold competitions on the eve of the parish feast, while others display the vast work of one firework. Entrants display a variety of moving shapes and include various colours year after year as the technology progresses. These displays are only a small part of the firework catalogue planned throughout the week preceding the feast and on the feast day itself. The Catherine wheel displays typically end with the burning of what is called 'the carpet': the largest Catherine wheel in the display on the night. \n\nIn the Philippines, Catherine wheel is also known as trompillo and according to Republic Act 7183, it is a legal firework. \n"}
{"id": "16107311", "url": "https://en.wikipedia.org/wiki?curid=16107311", "title": "Cell (EDA)", "text": "Cell (EDA)\n\nA cell in the context of electronic design automation (EDA) is an abstract representation of a component within a schematic diagram or physical layout of an electronic circuit in software. \n\nA cell-based design methodology is a technique that enables designers to analyze chip designs at varying levels of abstraction. For example, one designer may focus on the logical function (high-level) and another may concentrate on physical implementation (low-level). The technique also enables designers to reuse components in more complex designs without understanding all of the implementation details.\n\n"}
{"id": "32717006", "url": "https://en.wikipedia.org/wiki?curid=32717006", "title": "ClearCheckbook.com", "text": "ClearCheckbook.com\n\nClearCheckbook.com is a freemium, web-based personal money management tool created by Brandon O'Brien.\n\nClearCheckbook was originally conceived as a way to reconcile bank account transactions with \"actual\" spending (a process called \"jiving\" in ClearCheckbook lingo). Since its launch in May 2006, the application has undergone three major revisions and now allows users to track bills, setup reminders and recurring transactions, create spending reports, set budgets and more. According to the reports, It is said that ClearCheckbook manages two million transactions for over 17,800 active users.\n\n"}
{"id": "25126213", "url": "https://en.wikipedia.org/wiki?curid=25126213", "title": "Controlled mines", "text": "Controlled mines\n\nA Controlled Mine was a circuit fired weapon used in coastal defenses with ancestry going back to 1805 when Robert Fulton termed his underwater explosive device a torpedo:\nRobert Fulton invented the word torpedo to describe his underwater explosive device and successfully destroyed a ship in 1805. In the 1840s Samuel Colt began experimenting with underwater mines fired by electric current and in 1842, he blew up an old schooner in the Potomac River from a shore station five miles away.\n\"Torpedoes\" were in use during the American Civil War when such devices were made famous with the order given by David Farragut at Mobile Bay. After that war similar mines were being contemplated or put into use by other nations.\n\nIn 1869 the United States Army Corps of Engineers was directed by Secretary of War William Belknap to assume responsibility for torpedoes for coastal defense. That responsibility continued through the formation of the U.S. Torpedo Service as part of the Seacoast defense in the United States. In the United States, modern naval mine development began in 1869 at the Engineer School of Application under Major Henry Larcom Abbot at Willets Point, New York. Eventually, after calls for \"rifled cannon\" to cover the torpedo fields became reality, that service and the Corps of Engineers turned over responsibility to the newly formed U.S. Army Coast Artillery Corps in 1901.\n\nThe terms \"mine\" and \"torpedo\" were used interchangeably until modern usage began separating the term with \"mine\" applied to static explosive devices and \"torpedo\" to self-propelled or \"locomotive torpedo\" weapons. Even during the Spanish–American War the interchangeable terms caused confusion.\n\nIn Britain, the term 'Submarine Mine' was used. Fixed minefields to defend harbours were the responsibility of the Royal Engineers (RE), which formed special companies of Submarine Miners to maintain them. Lieutenant-General Sir Andrew Clarke, Inspector-General of Fortifications 1882–86, found that he did not have enough Regular Army engineers to man all the minefields being installed so he decided to utilise the part-time soldiers of the Volunteer Force. After successful trials the system was rolled out to ports around the country, where the Submarine Miners might be drawn from the Regular RE, the Militia, or the Volunteers. The Submarine Miners were also to the fore in developing searchlights to illuminate the minefields. By 1907 the War Office had decided to hand responsibility for the minefields to the Militia, but several Volunteer units were converted to Electrical Engineer Companies employing their lights for coastal artillery control and, eventually, anti-aircraft defences.\n\nUnlike naval mines that are dispersed at sea, the controlled mine field location is chosen so that it could be under observation. The exact location of the mines was required so that they could be fired from the mine casemate when a target vessel was plotted by observers to be within the mine's effective range. For this reason the mines were \"planted\" in predetermined locations with electrical connection through cables to the firing location. The complex of mines, cables and junction boxes required maintenance. Specialized vessels to undertake the hazards of planting mines and maintaining the electrical cables were used.\n\nIn the United States a type of vessel termed mine planter was developed, built and deployed in 1904. By 1909 more mine planters were under construction and deployment had reached the San Francisco fortifications. These were assisted by smaller vessels. In the last stages of such coastal defenses during the Second World War the U.S. Army Mine Planter Service (USAMPS) mine flotilla usually consisted of two planters, four Distribution Box Boats and a small fleet of yawls and launches.\n\nIn the Royal Navy controlled mines were often laid alongside anti-submarine indicator loops during both World Wars; the US Navy used a similar strategy in at least World War II. A dozen specialized vessels known as \"Indicator Loop Mine Layers\"—including three \"Linnet\"-class minelayers and nine smaller vessels—much like the U.S. mine planters, were built for the Royal Navy immediately before and during WWII. Similarly in Japan four Hashima-class cable layers were built between 1939 and 1941 for mine planting duties.\n\n\n\n"}
{"id": "56449548", "url": "https://en.wikipedia.org/wiki?curid=56449548", "title": "Cosmos (standard)", "text": "Cosmos (standard)\n\nCOSMOS stands for \"COSMetic Organic and Natural Standard\", which sets certification requirements for organic and natural cosmetics products in the Europe. The standard is recognized globally by the cosmetic industry. By adhering to specific guidelines, cosmetics marketers can use COSMOS signatures, which are registered trademarks, on packaging to confirm the products meet minimum industry requirements to be considered organic or natural.\n\nIn 2002 five European organisations responsible for setting organic and natural cosmetics standards met at a trade show to share ideas for broader standards to be used globally. These five COSMOS members are:\n\nOver 1,600 manufacturers who sell over 25,000 products across over 45 countries follow the standard, according to Cosmos-standard.org. About 85% of the certified cosmetics industry uses COSMOS signatures on its products.\n\nAlthough the five members differed on certain standards separately, they were able to smooth out differences to create a harmonised international standard that was first published in 2010. At this time the five members formed a non-profit international association overseeing the standard. In June 2010 the COSMOS-standard AISBL was awarded Royal Assent from Belgian authorities. The documents published with the standard include:\n\n\nThere are four main certification signatures that comprise the COSMOS-standard, which are for ORGANIC, NATURAL, COSMOS CERTIFIED and COSMOS APPROVED products. Here are six steps to gaining approval of product labeling within the certification process:\n\n\nManufacturers and marketers are only allowed to use COSMOS terms and signatures for products authorized by the certification body. The certification body must be identified on product labels if it is not clearly mentioned anywhere else on the product. In cases in which the label size restricts product labeling, the certification body may allow flexibility as long as the product maintains the general principles of the Labeling Guide. The firm must at least mention the nature of the certification (such as organic or natural) and the identity of the certification body.\n\nMarketers are allowed to use the COSMOS terms and signatures on company letterhead and websites under certain conditions. All the products of a brand must be certified organic in order for the company to make the claim they are \"COSMOS ORGANIC certified.\" Otherwise, the firm must be clear that only specific products have been certified organic. In other words, the use of the terms and signatures must not be misleading to the consumer.\n\nEssentially, the labelling must clearly and accurately describe the product, which must comply with the standard. The marketer must avoid listing ingredients or naming the product in a way that implies it contains certain ingredients that are not present. Any use or branding of the term \"organic,\" for example, must comply with the organic standard and not be confusing to the consumer.\n\nThe labeling must not confuse the terms \"organic\" and \"natural,\" which have separate definitions and certifications based on the way the products and ingredients are processed. If a brand sells several organic products and a few natural products, they must make it clear in their labeling and marketing the differences. The firm must also be clear if some of its products have no certification at all. In other words, in order for a company to promote itself as \"COSMOS ORGANIC certified,\" its entire range of products must meet the organic standard and be certified.\n\nCompanies are not allowed to use logos or seals that may mislead customers into believing the products are COSMOS certified.\n\n"}
{"id": "49423960", "url": "https://en.wikipedia.org/wiki?curid=49423960", "title": "Demet Mutlu", "text": "Demet Mutlu\n\nDemet Mutlu is a tech entrepreneur, founder and CEO of Trendyol group, the largest mobile e-commerce company based in Turkey. She is an angel investor in Peak Games, a gaming platform founded by Sidar Sahin and Rina Onur, that has more than 30 million monthly users. She has also been a speaker at several conferences including TechCrunch, Davos World Economic Forum, Financial Times Summit, Noah and Fortune. She is married to Evren Ucok, who is the managing partner of Earlybird VC for EMEA region.\n\nDemet Mutlu graduated \"cum laude\" in Economics from New York University. In 2008, she attended Harvard Business School. During her 1st year at Harvard MBA she founded Trendyol.com and dropped out to build her company.\n\nPrior to founding Trendyol, Demet Mutlu worked as a consultant, product manager in several countries including Switzerland, Japan, Hong Kong, Turkey and US.\n\nIn 2009, Demet founded Trendyol.com, the leading mobile e-commerce company in Turkey. Trendyol, where Demet is CEO has received strategic investment from Alibaba Group in 2018.\n\nShe has also been named one of the Fortune's Most Powerful Women Entrepreneurs, Fortune 40 under 40, Business of Fashion 500 Award- top 500 shaping Fashion Industry, 50 Most Inspiring Women in Tech, Fortune Powerful Top 50 Women, 100 Tech in EU and the Entrepreneur of the year of 2013 by Economist.\n\nIn 2011 she was selected as a Global Shaper by WEF, and was the founding member of the Global Shapers in Turkey, she also attend Davos in 2012. \nShe has been named as a Young Global Leader by the World Economic Forum in 2016. The Young Global leaders include Mark Zuckerberg, Sergei Brin, Ashton Kutcher, Lauren Bush, Chelsea Clinton, Marissa Mayer.\n"}
{"id": "4253861", "url": "https://en.wikipedia.org/wiki?curid=4253861", "title": "Double subscript notation", "text": "Double subscript notation\n\nIn engineering, double-subscript notation is notation used to indicate some variable \"between\" two points (each point being represented by one of the subscripts). In electronics, the notation is usually used to indicate the direction of current or voltage, while in mechanical engineering it is sometimes used to describe the force or stress between two points, and sometimes even a component that spans between two points (like a beam on a bridge or truss). Note that, although there are many cases where multiple subscripts are used, they are not necessarily called \"double subscript notation\" specifically.\n\nIEEE standard 255-1963, \"Letter Symbols for Semiconductor Devices\", defined eleven original quantity symbols expressed as abbreviations.\nThis is the basis for a convention to standardize the directions of double-subscript labels. The following uses transistors as an example, but shows how the direction is read generally. The convention works like this:\n\nformula_1 represents the voltage from C to B. In this case, C would denote the collector end of a transistor, and B would denote the base end of the same transistor. This is the same as saying \"the voltage drop from C to B\". Though this applies the standard definitions of the letters C and B,\n\nformula_2 would in turn represent the current from C to E. In this case, C would again denote the collector end of a transistor, and E would denote the emitter end of the transistor. This is the same as saying \"the current in the direction going from C to E\".\n\nPower supply pins on integrated circuits utilize the same letters for denoting what kind of voltage the pin would receive. For example, a power input labeled V would be a positive input that would presumably connect to the collector pin of a BJT transistor in the circuit, and likewise respectively with other subscripted letters. The format used is the same as for notations described above, though without the connotation of V meaning the voltage from a collector pin to collector pin; the repetition avoids confusion as such an expression would not exist.\n\nThe table above shows only the originally denoted letters; others have found their way into use over time, such as S and D for the Source and Drain of a FET, respectively.\n"}
{"id": "9611", "url": "https://en.wikipedia.org/wiki?curid=9611", "title": "E-commerce", "text": "E-commerce\n\nE-commerce is the activity of buying or selling of products on online services or over the Internet. Electronic commerce draws on technologies such as mobile commerce, electronic funds transfer, supply chain management, Internet marketing, online transaction processing, electronic data interchange (EDI), inventory management systems, and automated data collection systems.\n\nModern electronic commerce typically uses the World Wide Web for at least one part of the transaction's life cycle although it may also use other technologies such as e-mail. Typical e-commerce transactions include the purchase of online books (such as Amazon) and music purchases (music download in the form of digital distribution such as iTunes Store), and to a less extent, customized/personalized online liquor store inventory services. There are three areas of e-commerce: online retailing, electric markets, and online auctions. E-commerce is supported by electronic business.\n\nE-commerce businesses may also employ some or all of the followings:\n\nA timeline for the development of e-commerce:\n\nSome common applications related to electronic commerce are:\nIn the United States, certain electronic commerce activities are regulated by the Federal Trade Commission (FTC). These activities include the use of commercial e-mails, online advertising and consumer privacy. The CAN-SPAM Act of 2003 establishes national standards for direct marketing over e-mail. The Federal Trade Commission Act regulates all forms of advertising, including online advertising, and states that advertising must be truthful and non-deceptive. Using its authority under Section 5 of the FTC Act, which prohibits unfair or deceptive practices, the FTC has brought a number of cases to enforce the promises in corporate privacy statements, including promises about the security of consumers' personal information. As a result, any corporate privacy policy related to e-commerce activity may be subject to enforcement by the FTC.\n\nThe Ryan Haight Online Pharmacy Consumer Protection Act of 2008, which came into law in 2008, amends the Controlled Substances Act to address online pharmacies.\n\nConflict of laws in cyberspace is a major hurdle for harmonization of legal framework for e-commerce around the world. In order to give a uniformity to e-commerce law around the world, many countries adopted the UNCITRAL Model Law on Electronic Commerce (1996).<ref name=\"http://www.uncitral.org/uncitral/en/uncitral_texts/electronic_commerce/1996Model.html\"></ref>\n\nInternationally there is the International Consumer Protection and Enforcement Network (ICPEN), which was formed in 1991 from an informal network of government customer fair trade organisations. The purpose was stated as being to find ways of co-operating on tackling consumer problems connected with cross-border transactions in both goods and services, and to help ensure exchanges of information among the participants for mutual benefit and understanding. From this came Econsumer.gov, an ICPEN initiative since April 2001. It is a portal to report complaints about online and related transactions with foreign companies.\n\nThere is also Asia Pacific Economic Cooperation (APEC) was established in 1989 with the vision of achieving stability, security and prosperity for the region through free and open trade and investment. APEC has an Electronic Commerce Steering Group as well as working on common privacy regulations throughout the APEC region.\n\nIn Australia, Trade is covered under Australian Treasury Guidelines for electronic commerce and the Australian Competition and Consumer Commission regulates and offers advice on how to deal with businesses online, and offers specific advice on what happens if things go wrong.\n\nIn the United Kingdom, The Financial Services Authority (FSA) was formerly the regulating authority for most aspects of the EU's Payment Services Directive (PSD), until its replacement in 2013 by the Prudential Regulation Authority and the Financial Conduct Authority. The UK implemented the PSD through the Payment Services Regulations 2009 (PSRs), which came into effect on 1 November 2009. The PSR affects firms providing payment services and their customers. These firms include banks, non-bank credit card issuers and non-bank merchant acquirers, e-money issuers, etc. The PSRs created a new class of regulated firms known as payment institutions (PIs), who are subject to prudential requirements. Article 87 of the PSD requires the European Commission to report on the implementation and impact of the PSD by 1 November 2012.\n\nIn India, the Information Technology Act 2000 governs the basic applicability of e-commerce.\n\nIn China, the Telecommunications Regulations of the People's Republic of China (promulgated on 25 September 2000), stipulated the Ministry of Industry and Information Technology (MIIT) as the government department regulating all telecommunications related activities, including electronic commerce. On the same day, The Administrative Measures on Internet Information Services released, is the first administrative regulation to address profit-generating activities conducted through the Internet, and lay the foundation for future regulations governing e-commerce in China. On 28 August 2004, the eleventh session of the tenth NPC Standing Committee adopted The Electronic Signature Law, which regulates data message, electronic signature authentication and legal liability issues. It is considered the first law in China's e-commerce legislation. It was a milestone in the course of improving China's electronic commerce legislation, and also marks the entering of China's rapid development stage for electronic commerce legislation.\n\nContemporary electronic commerce can be classified into two categories. The first category is business based on types of goods sold (involves everything from ordering \"digital\" content for immediate online consumption, to ordering conventional goods and services, to \"meta\" services to facilitate other types of electronic commerce). The second category is based on the nature of the participant (B2B, B2C, C2B and C2C);\n\nOn the institutional level, big corporations and financial institutions use the internet to exchange financial data to facilitate domestic and international business. Data integrity and security are pressing issues for electronic commerce.\n\nAside from traditional e-commerce, the terms m-Commerce (mobile commerce) as well (around 2013) t-Commerce have also been used.\n\nIn 2010, the United Kingdom had the highest per capita e-commerce spending in the world. As of 2013, the Czech Republic was the European country where e-commerce delivers the biggest contribution to the enterprises´ total revenue. Almost a quarter (24%) of the country's total turnover is generated via the online channel.\n\nAmong emerging economies, China's e-commerce presence continues to expand every year. With 668 million Internet users, China's online shopping sales reached $253 billion in the first half of 2015, accounting for 10% of total Chinese consumer retail sales in that period. The Chinese retailers have been able to help consumers feel more comfortable shopping online. e-commerce transactions between China and other countries increased 32% to 2.3 trillion yuan ($375.8 billion) in 2012 and accounted for 9.6% of China's total international trade. In 2013, Alibaba had an e-commerce market share of 80% in China. In 2014, there were 600 million Internet users in China (twice as many as in the US), making it the world's biggest online market. China is also the largest e-commerce market in the world by value of sales, with an estimated in 2016.\n\nRecent research clearly indicates that electronic commerce, commonly referred to as e-commerce, presently shapes the manner in which people shop for products. The GCC countries have a rapidly growing market and characterized by a population that becomes wealthier (Yuldashev). As such, retailers have launched Arabic-language websites as a means to target this population. Secondly, there are predictions of increased mobile purchases and an expanding internet audience (Yuldashev). The growth and development of the two aspects make the GCC countries to become larger players in the electronic commerce market with time progress. Specifically, research shows that e-commerce market is expected to grow to over $20 billion by the year 2020 among these GCC countries (Yuldashev). \nThe e-commerce market has also gained much popularity among the western countries, and in particular Europe and the U.S. These countries have been highly characterized with consumer-packaged-goods (CPG) (Geisler, 34). However, trends show that there are future signs of a reverse. Similar to the GCC countries, there has been increased purchase of goods and services in online channels rather than offline channels. Activist investors are trying hard to consolidate and slash their overall cost and the governments in western countries continue to impose more regulation on CPG manufacturers (Geisler, 36). In these senses, CPG investors are being forced to adapt e-commerce as it is effective as a well as a means for them to thrive.\n\nIn 2013, Brazil's e-commerce was growing quickly with retail e-commerce sales expected to grow at a double-digit pace through 2014. By 2016, eMarketer expected retail e-commerce sales in Brazil to reach $17.3 billion. India has an Internet user base of about 460 million as of December 2017. Despite being third largest user base in world, the penetration of Internet is low compared to markets like the United States, United Kingdom or France but is growing at a much faster rate, adding around 6 million new entrants every month. In India, cash on delivery is the most preferred payment method, accumulating 75% of the e-retail activities. The India retail market is expected to rise from 2.5% in 2016 to 5% in 2020.\n\nThe future trends in the GCC countries will be similar with that of the western countries. Despite the forces that push business to adapt e-commerce as a means to sell goods and products, the manner in which customers make purchases is similar in countries from these two regions. For instance, there has been an increased usage of smartphones which comes in conjunction with an increase in the overall internet audience from the regions. Yuldashev writes that consumers are scaling up to more modern technology that allows for mobile marketing.\nHowever, the percentage of smartphone and internet users who make online purchases is expected to vary in the first few years. It will be independent on the willingness of the people to adopt this new trend (The Statistics Portal). For example, UAE has the greatest smartphone penetration of 73.8 percent and has 91.9 percent of its population has access to the internet. On the other hand, smartphone penetration in Europe has been reported to be at 64.7 percent (The Statistics Portal). Regardless, the disparity in percentage between these regions is expected to level out in future because e-commerce technology is expected to grow allowing for more users. \nThe e-commerce business within these two regions will result in a competition. Government bodies at country level will enhance their measures and strategies to ensure sustainability and consumer protection (Krings, et al.). These increased measures will raise the environmental and social standards in the countries, factors that will determine the success of e-commerce market in these countries. For example, an adoption of tough sanctions will make it difficult for companies to enter the e-commerce market while lenient sanctions will allow ease of companies. As such, the future trends between GCC countries and the Western countries will be independent of these sanctions (Krings, et al.). These countries need to make rational conclusions in coming up with effective sanctions.\n\nThe rate of growth of the number of internet users in the Arab countries has been rapid – 13.1% in 2015. A significant portion of the e-commerce market in the Middle East comprises people in the 30–34 year age group. Egypt has the largest number of internet users in the region, followed by Saudi Arabia and Morocco; these constitute 3/4th of the region’s share. Yet, internet penetration is low: 35% in Egypt and 65% in Saudi Arabia.\n\nE-commerce has become an important tool for small and large businesses worldwide, not only to sell to customers, but also to engage them.\n\nIn 2012, e-commerce sales topped $1 trillion for the first time in history.\n\nMobile devices are playing an increasing role in the mix of e-commerce, this is also commonly called mobile commerce, or m-commerce. In 2014, one estimate saw purchases made on mobile devices making up 25% of the market by 2017.\n\nFor traditional businesses, one research stated that information technology and cross-border e-commerce is a good opportunity for the rapid development and growth of enterprises. Many companies have invested enormous volume of investment in mobile applications. The DeLone and McLean Model stated that three perspectives contribute to a successful e-business: information system quality, service quality and users' satisfaction. There is no limit of time and space, there are more opportunities to reach out to customers around the world, and to cut down unnecessary intermediate links, thereby reducing the cost price, and can benefit from one on one large customer data analysis, to achieve a high degree of personal customization strategic plan, in order to fully enhance the core competitiveness of the products in company.\n\nModern 3D graphics technologies, such as Facebook 3D Posts, are considered by some social media marketers and advertisers as a more preferable way to promote consumer goods than static photos, and some brands like Sony are already paving the way for augmented reality commerce. Wayfair now lets you inspect a 3D version of its furniture in a home setting before buying.\n\nLogistics in e-commerce mainly concerns fulfillment. Online markets and retailers have to find the best possible way to fill orders and deliver products. Small companies usually control their own logistic operation because they do not have the ability to hire an outside company. Most large companies hire a fulfillment service that takes care of a company's logistic needs.\n\nContrary to common misconception, there are significant barriers to entry in e-commerce.\n\nE-commerce markets are growing at noticeable rates. The online market is expected to grow by 56% in 2015–2020. In 2017, retail e-commerce sales worldwide amounted to 2.3 trillion US dollars and e-retail revenues are projected to grow to 4.88 trillion US dollars in 2021. Traditional markets are only expected 2% growth during the same time. Brick and mortar retailers are struggling because of online retailer's ability to offer lower prices and higher efficiency. Many larger retailers are able to maintain a presence offline and online by linking physical and online offerings.\n\nE-commerce allows customers to overcome geographical barriers and allows them to purchase products anytime and from anywhere. Online and traditional markets have different strategies for conducting business. Traditional retailers offer fewer assortment of products because of shelf space where, online retailers often hold no inventory but send customer orders directly to the manufacture. The pricing strategies are also different for traditional and online retailers. Traditional retailers base their prices on store traffic and the cost to keep inventory. Online retailers base prices on the speed of delivery.\n\nThere are two ways for marketers to conduct business through e-commerce: fully online or online along with a brick and mortar store. Online marketers can offer lower prices, greater product selection, and high efficiency rates. Many customers prefer online markets if the products can be delivered quickly at relatively low price. However, online retailers cannot offer the physical experience that traditional retailers can. It can be difficult to judge the quality of a product without the physical experience, which may cause customers to experience product or seller uncertainty. Another issue regarding the online market is concerns about the security of online transactions. Many customers remain loyal to well-known retailers because of this issue.\n\nSecurity is a primary problem for e-commerce in developed and developing countries. E-commerce security is protecting business' websites and costumers from unauthorized access, use, alteration, or destruction. The type of threats include: malicious codes, unwanted programs (ad ware, spyware), phishing, hacking, and cyber vandalism. E-commerce websites use different tools to avert security threats. These tools include firewalls, encryption software, digital certificates, and passwords.\n\nFor a long time, companies had been troubled by the gap between the benefits which supply chain technology has and the solutions to deliver those benefits. However, the emergence of e-commerce has provided a more practical and effective way of delivering the benefits of the new supply chain technologies.\n\nE-commerce has the capability to integrate all inter-company and intra-company functions, meaning that the three flows (physical flow, financial flow and information flow) of the supply chain could be also affected by e-commerce. The affections on physical flows improved the way of product and inventory movement level for companies. For the information flows, e-commerce optimised the capacity of information processing than companies used to have, and for the financial flows, e-commerce allows companies to have more efficient payment and settlement solutions.\n\nIn addition, e-commerce has a more sophisticated level of impact on supply chains: Firstly, the performance gap will be eliminated since companies can identify gaps between different levels of supply chains by electronic means of solutions; Secondly, as a result of e-commerce emergence, new capabilities such implementing ERP systems, like SAP ERP, Xero, or Megaventory, have helped companies to manage operations with customers and suppliers. Yet these new capabilities are still not fully exploited. Thirdly, technology companies would keep investing on new e-commerce software solutions as they are expecting investment return. Fourthly, e-commerce would help to solve many aspects of issues that companies may feel difficult to cope with, such as political barriers or cross-country changes. Finally, e-commerce provides companies a more efficient and effective way to collaborate with each other within the supply chain.\n\nE-commerce helps create new job opportunities due to information related services, software app and digital products. It also causes job losses. The areas with the greatest predicted job-loss are retail, postal, and travel agencies. The development of e-commerce will create jobs that require highly skilled workers to manage large amounts of information, customer demands, and production processes. In contrast, people with poor technical skills cannot enjoy the wages welfare. On the other hand, because e-commerce requires sufficient stocks that could be delivered to customers in time, the warehouse becomes an important element. Warehouse needs more staff to manage, supervise and organize, thus the condition of warehouse environment will be concerned by employees.\n\nE-commerce brings convenience for customers as they do not have to leave home and only need to browse website online, especially for buying the products which are not sold in nearby shops. It could help customers buy wider range of products and save customers’ time. Consumers also gain power through online shopping. They are able to research products and compare prices among retailers. Also, online shopping often provides sales promotion or discounts code, thus it is more price effective for customers. Moreover, e-commerce provides products’ detailed information; even the in-store staff cannot offer such detailed explanation. Customers can also review and track the order history online.\n\nE-commerce technologies cut transaction costs by allowing both manufactures and consumers to skip through the intermediaries. This is achieved through by extending the search area best price deals and by group purchase. The success of e-commerce in urban and regional levels depend on how the local firms and consumers have adopted to e-commerce. \nHowever, e-commerce lacks human interaction for customers, especially who prefer face-to-face connection. Customers are also concerned with the security of online transactions and tend to remain loyal to well-known retailers. In recent years, clothing retailers such as Tommy Hilfiger have started adding Virtual Fit platforms to their e-commerce sites to reduce the risk of customers buying the wrong sized clothes, although these vary greatly in their fit for purpose. When the customer regret the purchase of a product, it involves returning goods and refunding process. This process is inconvenient as customers need to pack and post the goods. If the products are expensive, large or fragile, it refers to safety issues.\n\nE-commerce has grown in importance as companies have adopted pure-click and brick-and-click channel systems. We can distinguish pure-click and brick-and-click channel system adopted by companies.\n\nE-commerce may take place on retailers' Web sites or mobile apps, or those of e-commerce marketplaces such as on Amazon, or Tmall from AliBaba. Those channels may also be supported by conversational commerce, e.g. live chat or chatbots on Web sites. Conversational commerce may also be standalone such as live chat or chatbots on messaging apps and via voice assistants.\n\nThe contemporary e-commerce trend recommends companies to shift the traditional business model where focus on \"standardized products, homogeneous market and long product life cycle\" to the new business model where focus on \"varied and customized products\". E-commerce requires the company to have the ability to satisfy multiple needs of different customers and provide them with wider range of products.\n\nWith more choices of products, the information of products for customers to select and meet their needs become crucial. In order to address the mass customization principle to the company, the use of recommender system is suggested. This system helps recommend the proper products to the customers and helps customers make the decision during the purchasing process. The recommender system could be operated through the top sellers on the website, the demographics of customers or the consumers' buying behavior. However, there are 3 main ways of recommendations: recommending products to customers directly, providing detailed products' information and showing other buyers' opinions or critiques. It is benefit for consumer experience without physical shopping. In general, recommender system is used to contact customers online and assist finding the right products they want effectively and directly.\n\n\n\n"}
{"id": "7726042", "url": "https://en.wikipedia.org/wiki?curid=7726042", "title": "Electronic performance support systems", "text": "Electronic performance support systems\n\nAn electronic performance support system (EPSS) is any computer software program or component that improves user performance.\n\nEPSSs can help an organization to reduce the cost of training staff while increasing productivity and performance. They can empower employees to perform tasks with a minimum amount of external intervention or training. By using this type of system an employee, especially a new employee, will often not only be able to complete his or her work more quickly and accurately, but, as a secondary benefit, will also learn more about the job and the employer's business.\n\nAn EPSS is best considered when\nThese situations often occur when new systems (e.g. customer relationship management, enterprise resource planning) are introduced, upgraded or consolidated, and in certain call centres when agents must perform using complex systems, processes or products.\n\nThere are different views about the components and characteristics of EPSS. For example, from Barker and Banerji’s (1995) point of view, an EPSS has four functional levels, which should be brought together:\n\n\nIn \"Electronic Performance Support Systems\", Gloria Gery defined EPSS as:\nan integrated electronic environment that is available to and easily \naccessible by each employee and is structured to provide immediate, individualized \non-line access to the full range of information, software, guidance, advice and \nassistance, data, images, tools, and assessment and monitoring systems to permit \njob performance with minimal support and intervention by others.\nAlso, in 1991, Barry Raybould gave a shorter definition:\na computer-based system that improves worker productivity by providing on-the-job access to integrated information, advice, and learning experiences.\n\nFrom a business perspective, a former Nortel Networks executive, William Bezanson (2002) provided a definition linked to application usability and organizational results: A performance support system provides just-in-time, just enough training, information, tools, and help for users of a product or work environment, to enable optimum performance by those users when and where needed, thereby also enhancing the performance of the overall business.\n\nAn EPSS must be distinguished from a traditional online help system. Online help usually supports a single software application and is not necessarily focused on the entire range of job tasks (which may involve multiple applications), but just that specific software. With online help, cross-referencing is often not available and the information provided is limited and rarely combined with procedures or complex tasks. Perhaps most critically, on-line help cannot be customized to the user or the job task; in fact, the same software screen may require different inputs depending on the user and job task. Online help is also not contextual to the user's current situation and requires users to search through for the solution to their problem. \n\nEPSS must also be differentiated from e-learning simulations that replay a series of steps on-demand within a software application. Simulations are more closely associated with on-demand training, not just-in-time support, because of the longer time considerations, complexity, and media restrictions for playing a simulation. An EPSS can be considered a part of the e-learning category, as it is on-demand learning, and notes that the EPSS modality fits more within the informal learning definition.\n\nIn his book, Bezanson points out that \"knowledge management\" is the noun corresponding to the verb of \"performance support\". The knowledge documented in the system plays a critical role in any EPSS system. This concept was originated by Raybould (1997) who separates out the repository, delivery and infrastructure aspects of the EPSS from the knowledge base it contains. In fact, Bezanson emphasizes the advantage that an LMS (learning management system) will interface with the EPSS to supply the knowledge base, content-courseware, or other tracking capabilities that an EPSS may require if the LMS precedes the implementation of the EPSS.\n\nEPSS's role in the future of training and work have been noted by followers of the trend towards more informal learning systems driven by knowledge management systems.\nTony O'Driscoll (1999) summarizes: \n\nTo this day, analysts such as Forrester's Claire Schooley and Bersin & Associates' Chris Howard write similarly themed articles based on their research in informal learning, technology and training.\n\n\n"}
{"id": "16015192", "url": "https://en.wikipedia.org/wiki?curid=16015192", "title": "Fish preservation", "text": "Fish preservation\n\nAncient methods of preserving fish included drying, salting, pickling and smoking. All of these techniques are still used today but the more modern techniques of freezing and canning have taken on a large importance.\n\nFish curing includes and of curing fish by drying, salting, smoking, and pickling, or by combinations of these processes have been employed since ancient times. On sailing vessels fish were usually salted down immediately to prevent spoilage; the swifter boats of today commonly bring in unsalted fish. Modern freezing and canning methods have largely supplanted older methods of preservation. Fish to be cured are usually first cleaned, scaled, and eviscerated. Fish are salted by packing them between layers of salt or by immersion in brine. The fish most extensively salted are cod, herring, mackerel, and haddock. Smoking preserves fish by drying, by deposition of creosote ingredients, and, when the fish are near the source of heat, by heat penetration. Herring and haddock (finnan haddie) are commonly smoked. Kippers are split herring, and bloaters are whole herring, salted and smoked. Sardines, pilchards, and anchovies are small fish of the herring family, often salted and smoked and then preserved in oil. Fish are dried under controlled conditions of temperature, humidity, and air velocity. Since the dried product is relatively unappetizing and rehydrating slow, other preservation methods are common.\n\nIn the past, fishing vessels were restricted in range by the simple consideration that the catch must be returned to port before it spoils and becomes worthless. The development of refrigeration and freezing technologies transformed the commercial fishing industry: fishing vessels could be larger, spending more time away from port and therefore accessing fish stocks at a much greater distance. Refrigeration and freezing also allow the catch to be distributed to markets further inland, reaching customers who previously would have had access only to dried or salted sea fish.\n\nCanning, developed during the 19th century has also had a significant impact on fishing by allowing seasonal catches of fish that are possibly far from large centres of population to be exploited. For example: canned sardines.\n\nPreservation techniques are needed to prevent fish spoilage and lengthen shelf life. They are designed to inhibit the activity of spoilage bacteria and the metabolic changes that result in the loss of fish quality. Spoilage bacteria are the specific bacteria that produce the unpleasant odours and flavours associated with spoiled fish. Fish normally host many bacteria that are not spoilage bacteria, and most of the bacteria present on spoiled fish played no role in the spoilage. To flourish, bacteria need the right temperature, sufficient water and oxygen, and surroundings that are not too acidic. Preservation techniques work by interrupting one or more of these needs. Preservation techniques can be classified as follows.\n\nIf the temperature is decreased, the metabolic activity in the fish from microbial or autolytic processes can be reduced or stopped. This is achieved by refrigeration where the temperature is dropped to about 0 °C, or freezing where the temperature is dropped below -18 °C. On fishing vessels, the fish are refrigerated mechanically by circulating cold air or by packing the fish in boxes with ice. Forage fish, which are often caught in large numbers, are usually chilled with refrigerated or chilled seawater. Once chilled or frozen, the fish need further cooling to maintain the low temperature. There are key issues with fish cold store design and management, such as how large and energy efficient they are, and the way they are insulated and palletized.\n\nAn effective method of preserving the freshness of fish is to chill with ice by distributing ice uniformly around the fish. It is a safe cooling method that keeps the fish moist and in an easily stored form suitable for transport. It has become widely used since the development of mechanical refrigeration, which makes ice easy and cheap to produce. Ice is produced in various shapes; crushed ice and ice flakes, plates, tubes and blocks are commonly used to cool fish. Particularly effective is slurry ice, made from micro crystals of ice formed and suspended within a solution of water and a freezing point depressant, such as common salt.\n\nA more recent development is pumpable ice technology. Pumpable ice flows like water, and because it is homogeneous, it cools fish faster than fresh water solid ice methods and eliminates freeze burns. It complies with HACCP and ISO food safety and public health standards, and uses less energy than conventional fresh water solid ice technologies.\n\nThe water activity, a, in a fish is defined as the ratio of the water vapour pressure in the flesh of the fish to the vapour pressure of pure water at the same temperature and pressure. It ranges between 0 and 1, and is a parameter that measures how available the water is in the flesh of the fish. Available water is necessary for the microbial and enzymatic reactions involved in spoilage. There are a number of techniques that have been or are used to tie up the available water or remove it by reducing the a. Traditionally, techniques such as drying, salting and smoking have been used, and have been used for thousands of years. These techniques can be very simple, for example, by using solar drying. In more recent times, freeze-drying, water-binding humectants, and fully automated equipment with temperature and humidity control have been added. Often a combination of these techniques is used.\n\nHeat or ionizing irradiation can be used to kill the bacteria that cause decomposition. Heat is applied by cooking, blanching or microwave heating in a manner that pasteurizes or sterilizes fish products. Cooking or pasteurizing does not completely inactivate microorganisms and may need to be followed with refrigeration to preserve fish products and increase their shelf life. Sterilised products are stable at ambient temperatures up to 40 °C, but to ensure they remain sterilized they need packaging in metal cans or retortable pouches before the heat treatment.\n\nMicrobial growth and proliferation can be inhibited by a technique called biopreservation. Biopreservation is achieved by adding antimicrobials or by increasing the acidity of the fish muscle. Most bacteria stop multiplying when the pH is less than 4.5. Acidity is increased by fermentation, marination or by directly adding acids (acetic, citric, lactic) to fish products. Lactic acid bacteria produce the antimicrobial nisin which further enhances preservation. Other preservatives include nitrites, sulphites, sorbates, benzoates and essential oils.\n\nSpoilage bacteria and lipid oxidation usually need oxygen, so reducing the oxygen around fish can increase shelf life. This is done by controlling or modifying the atmosphere around the fish, or by vacuum packaging. Controlled or modified atmospheres have specific combinations of oxygen, carbon dioxide and nitrogen, and the method is often combined with refrigeration for more effective fish preservation.\n\nTwo or more of these techniques are often combined. This can improve preservation and reduce unwanted side effects such as the denaturation of nutrients by severe heat treatments. Common combinations are salting/drying, salting/marinating, salting/smoking, drying/smoking, pasteurization/refrigeration and controlled atmosphere/refrigeration. Other process combinations are currently being developed along the multiple hurdle theory.\n\nSee:\n\n"}
{"id": "209963", "url": "https://en.wikipedia.org/wiki?curid=209963", "title": "Fluid bearing", "text": "Fluid bearing\n\nFluid bearings are bearings in which the load is supported by a thin layer of rapidly moving pressurized liquid or gas between the bearing surfaces. Since there is no contact between the moving parts, there is no sliding friction, allowing fluid bearings to have lower friction, wear and vibration than many other types of bearings.\n\nThey can be broadly classified into two types: fluid dynamic bearings (also known as hydrodynamic bearings) and hydrostatic bearings. Hydrostatic bearings are externally pressurized fluid bearings, where the fluid is usually oil, water or air, and the pressurization is done by a pump. Hydrodynamic bearings rely on the high speed of the journal (the part of the shaft resting on the fluid) to pressurize the fluid in a wedge between the faces. Fluid bearings are frequently used in high load, high speed or high precision applications where ordinary ball bearings would have short life or cause high noise and vibration. They are also used increasingly to reduce cost. For example, hard disk drive motor fluid bearings are both quieter and cheaper than the ball bearings they replace.\n\nThe fluid bearing may have been invented by French civil engineer L. D. Girard, who in 1852 proposed a system of railway propulsion incorporating water-fed hydraulic bearings.\n\nFluid bearings are noncontact bearings that use a thin layer of rapidly moving pressurized liquid or gas fluid between the moving bearing faces, typically sealed around or under the rotating shaft. The moving parts do not come into contact, so there is no sliding friction; the load force is supported solely by the pressure of the moving fluid. There are two principal ways of getting the fluid into the bearing:\n\n\nHydrostatic bearings rely on an external pump. The power required by that pump contributes to system energy loss, just as bearing friction otherwise would. Better seals can reduce leak rates and pumping power, but may increase friction.\n\nHydrodynamic bearings rely on bearing motion to suck fluid into the bearing, and may have high friction and short life at speeds lower than design, or during starts and stops. An external pump or secondary bearing may be used for startup and shutdown to prevent damage to the hydrodynamic bearing. A secondary bearing may have high friction and short operating life, but good overall service life if bearing starts and stops are infrequent.\n\n\"Hydrodynamic\" (HD) \"lubrication\", also known as \"fluid film lubrication\" has essential elements:\n\n\nHydrodynamic (Full Film) Lubrication is obtained when two mating surfaces are completely separated by a cohesive film of lubricant.\n\nThe thickness of the film thus exceeds the combined roughness of the surfaces. The coefficient of friction is lower than with boundary-layer lubrication. Hydrodynamic lubrication prevents wear in moving parts, and metal to metal contact is prevented.\n\nHydrodynamic lubrication requires thin, converging fluid films. These fluids can be liquid or gas, so long as they exhibit viscosity. In computer components, like a hard disk, heads are supported by hydrodynamic lubrication in which the fluid film is the atmosphere.\n\nThe scale of these films is on the order of micrometers. Their convergence creates pressures normal to the surfaces they contact, forcing them apart.\n\nThree types of bearings include:\n\n\nConceptually the bearings can be thought of as two major geometric classes: bearing-journal (anti-friction), and plane-slider (friction).\n\nThe Reynolds equations can be used to derive the governing principles for the fluids. Note that when gases are used, their derivation is much more involved.\n\nThe thin films can be thought to have pressure and viscous forces acting on them. Because there is a difference in velocity there will be a difference in the surface traction vectors. Because of mass conservation we can also assume an increase in pressure, making the body forces different.\n\n\n\n\n\n\n\n\nSince viscosity, velocity, and load determine the characteristics of a hydrodynamic condition, a bearing characteristic number was developed based on the effects of these on film thickness.\n\nTherefore,\n\n\"C\" is known as the bearing characteristic number.\n\nThe value of \"C\", to some extent, gives an indication of whether there will be hydrodynamic lubrication or not\n\nFluid bearings can be relatively cheap compared to other bearings with a similar load rating. The bearing can be as simple as two smooth surfaces with seals to keep in the working fluid. In contrast, a conventional rolling-element bearing may require many high-precision rollers with complicated shapes. Hydrostatic and many gas bearings do have the complication and expense of external pumps.\n\nMost fluid bearings require little or no maintenance, and have almost unlimited life. Conventional rolling-element bearings usually have shorter life and require regular maintenance. Pumped hydrostatic and aerostatic (gas) bearing designs retain low friction down to zero speed and need not suffer start/stop wear, provided the pump does not fail.\n\nFluid bearings generally have very low friction—far better than mechanical bearings. One source of friction in a fluid bearing is the viscosity of the fluid. Hydrostatic gas bearings are among the lowest friction bearings. However, lower fluid viscosity also typically means fluid leaks faster from the bearing surfaces, thus requiring increased power for pumps or friction from seals.\n\nWhen a roller or ball is heavily loaded, fluid bearings have clearances that change less under load (are \"stiffer\") than mechanical bearings. It might seem that bearing stiffness, as with maximum design load, would be a simple function of average fluid pressure and the bearing surface area. In practice, when bearing surfaces are pressed together, the fluid outflow is constricted. This significantly increases the pressure of the fluid between the bearing faces. As fluid bearing faces can be comparatively larger than rolling surfaces, even small fluid pressure differences cause large restoring forces, maintaining the gap.\n\nHowever, in lightly loaded bearings, such as disk drives, the typical ball bearing stiffnesses are ~10^7 MN/m. Comparable fluid bearings have stiffness of ~10^6 MN/m. Because of this, some fluid bearings, particularly hydrostatic bearings, are deliberately designed to pre-load the bearing to increase the stiffness.\n\nFluid bearings often inherently add significant damping. This helps attenuate resonances at the gyroscopic frequencies of journal bearings (sometimes called conical or rocking modes).\n\nIt is very difficult to make a mechanical bearing which is atomically smooth and round; and mechanical bearings deform in high-speed operation due to centripetal force. In contrast, fluid bearings self-correct for minor imperfections.\n\nFluid bearings are typically quieter and smoother (more consistent friction) than rolling-element bearings. For example, hard disks manufactured with fluid bearings have noise ratings for bearings/motors on the order of 20–24 dB, which is a little more than the background noise of a quiet room. Drives based on rolling-element bearings are typically at least 4 dB noisier.\n\nFluid bearings can be made with a lower NRRO (non repeatable run out) than a ball or rolling element bearing. This can be critical in modern hard disk drive and ultra precision spindles.\n\nTilting pad bearings are used as radial bearings for supporting and locating shafts in compressors.\n\n\nFoil bearings are a type of fluid dynamic air bearing that was introduced in high speed turbine applications in the 1960s by Garrett AiResearch. They use a gas as the working fluid, usually air and require no external pressurisation system.\n\nJournal bearings are lubricated with fluid. The working part of the bearing operates by carrying oil at a low pressure and is compressed to allow the bearing to spin around the shaft without any contact.\n\nWater lubricated rubber bearings have long cylindrical metal shell that hosts multiple rubber staves separated by axial grooves. The usage of the bearing has three major advantages: (i) pumped water going through the bearing is conveniently used as a lubricant, which reduces pump operation cost; (ii) water flow takes away heat and fine particles through the bearing grooves; and (iii) the natural resilience of rubber gives the bearing good properties for shock and vibration absorption and wear resistance. Water lubricated rubber bearings operate under the condition of mixed-lubrication. \n\nUnlike contact-roller bearings, an air bearing (or air caster) utilizes a thin film of pressurized air to provide an exceedingly low friction load-bearing interface between surfaces. The two surfaces don't touch. Being non-contact, air bearings avoid the traditional bearing-related problems of friction, wear, particulates, and lubricant handling, and offer distinct advantages in precision positioning, such as lacking backlash and stiction, as well as in high-speed applications.\n\nThe fluid film of the bearing is air that flows through the bearing itself to the bearing surface. The design of the air bearing is such that, although the air constantly escapes from the bearing gap, the pressure between the faces of the bearing is enough to support the working loads.\n\nAir-lubricated bearings require highly finished surfaces and precise manufacturing, and can only be operated in high-speed applications.\n\nAir hockey is a game based on an aerostatic bearing which suspends the puck and players' paddles to provide low friction and thus sustain high puck speeds. The bearing uses a flat plane with periodic orifices which deliver air just over ambient pressure. The puck and paddles rest on air.\n\nAnother example of a fluid bearing is ice skating. Ice skates form a hydrodynamic fluid bearing where the skate and ice are separated by a layer of water caused by entropy (formerly thought to be caused by pressure-induced melting; see ice skating for details.)\n\nMichell/Kingsbury fluid dynamic tilting-pad bearings were invented independently and almost simultaneously by both British-born Australian, Anthony George Maldon Michell and American tribologist Albert Kingsbury. Both designs were near-identical except for differences in the approach used for pivoting the pads. Michell mathematically derived the pressure distribution where a span-wise line pivot was placed, allowing the load to act through the point of maximum fluid pressure. The Kingsbury patent lacked this mathematical approach, and the pad's pivot point was placed in the geometric centre of the bearing. Michell's patent (in Britain and Australia) was granted in 1905, while Kingsbury's first patent attempt was 1907. Kingsbury's U.S. patent was eventually granted in 1911 after he demonstrated that he had been working on the concept for many years. As stated by Sydney Walker, a long-time employee of Michell's, the granting of Kingsbury's patent was \"a blow which Michell found hard to accept\".\n\nThe bearing has sectional \"shoes\", or \"pads\" on pivots. When the bearing is in operation, the rotating part of the bearing carries fresh oil in to the pad area through viscous drag. Fluid pressure causes the pad to tilt slightly, creating a narrow constriction between the shoe and the other bearing surface. A wedge of pressurised fluid builds behind this constriction, separating the moving parts. The tilt of the pad adaptively changes with bearing load and speed. Various design details ensure continued replenishment of the oil to avoid overheating and pad damage.\n\nMichell/Kingsbury fluid bearings are used in a wider variety of heavy-duty rotating equipment, including in hydroelectric plants to support turbines and generators weighing hundreds of tons. They are also used in very heavy machinery, such as marine propeller shafts.\n\nThe first tilting pad bearing in service was probably built under A.G.M. Michell's guidance by George Weymoth (Pty) Ltd, for a centrifugal pump at Cohuna on the Murray River, Victoria, Australia, in 1907, just two years after Michell had published and patented his three-dimensional solution to Reynold's equation. By 1913, the great merits of the tilting-pad bearing had been recognised for marine applications. The first British ship to be fitted out with the bearing was the cross-channel steamboat the \"Paris\", but many naval vessels were similarly equipped during the First World War. The practical results were spectacular – the troublesome thrust block became dramatically smaller and lighter, significantly more efficient, and remarkably free from maintenance troubles. It was estimated that the Royal Navy saved coal to a value of £500,000 in 1918 alone as a result of fitting Michell's tilting-pad bearings.\n\nAccording to the ASME (see reference link), the first Michell/Kingsbury fluid bearing in the US was installed in the Holtwood Hydroelectric Power Plant (on the Susquehanna River, near Lancaster, Pennsylvania, US) in 1912. The 2.25-tonne bearing supports a water turbine and electric generator with a rotating mass of about 165 tonnes and water turbine pressure adding another 40 tonnes. The bearing has been in nearly continuous service since 1912, with no parts replaced. The ASME reported it was still in service as of 2000. As of 2002, the manufacturer estimated the bearings at Holtwood should have a maintenance-free life of about 1,300 years.\n\n"}
{"id": "4161821", "url": "https://en.wikipedia.org/wiki?curid=4161821", "title": "Full custom", "text": "Full custom\n\nFull-custom design is a methodology for designing integrated circuits by specifying the layout of each individual transistor and the interconnections between them. Alternatives to full-custom design include various forms of semi-custom design, such as the repetition of small transistor subcircuits; one such methodology is the use of standard cell libraries (standard cell libraries are themselves designed using full-custom design techniques). \n\nFull-custom design potentially maximizes the performance of the chip, and minimizes its area, but is extremely labor-intensive to implement. Full-custom design is limited to ICs that are to be fabricated in extremely high volumes, notably certain microprocessors and a small number of ASICs. \n\nThe main factor affecting the design and production of ASICs is the high cost of mask sets and the requisite EDA design tools. The mask sets are required in order to transfer the ASIC designs onto the wafer.\n\n"}
{"id": "7166412", "url": "https://en.wikipedia.org/wiki?curid=7166412", "title": "Global Area Reference System", "text": "Global Area Reference System\n\nThe Global Area Reference System (GARS) is a standardized geospatial reference system developed by the National Geospatial-Intelligence Agency (NGA) for use across the United States Department of Defense. Under the Chairman of the Joint Chiefs of Staff Instruction CJCSI 3900.01C dated 30 June 2007, GARS was adopted for use by the US DoD as \"the “area-centric” counterpart to the “point-centric” MGRS\". It uses the WGS 1984 Datum and is based on lines of longitude (LONG) and latitude (LAT). It is intended to provide an integrated common frame of reference for joint force situational awareness to facilitate air-to-ground coordination, deconfliction, integration, and synchronization. This area reference system provides a common language between the components and simplifies communications. GARS is primarily designed as a battlespace management tool and not to be used for navigation or targeting.\n\n\n\n"}
{"id": "50003413", "url": "https://en.wikipedia.org/wiki?curid=50003413", "title": "Golden bull (broker award)", "text": "Golden bull (broker award)\n\nThe Golden Bull is a Dutch prize in the world of brokers since 2007. The choice for the bull as a symbol is easy, since it also represents a rising exchange, which therefore is also called a bullmarket.\nSince the adding of a jury, the price has got more esteem with professional traders. The press also noticed and on special broker and trading websites there has been more and more attention towards the issue of the \"Golden Bull\" award.\n\nOn annual basis, there are prizes in different categories. In 2015 these categories were: \"best broker, best trade innovation, best fund, best ETF, Green Bull (best sustainable fund), best mixfund-range\", \"best asset\" en \"best trade-expert\". The winners of the categories are chosen by a jury, except for the people choice award for \"best trade-expert\".\n\nSince 2015, the Golden Bull award has been hosted by the biggest broker websites in The Netherlands: IEX.nl, Belegger.nl, DeBeurs.nl, EuroBench.com en BeursOnline. Before that the organization of the event was in hands of Belegger.nl. This website was in hands of the Dutch company Sanoma Media until 2015 and is now in hands of Value8 of former chairman of the Vereniging van Effectenbezitters, Peter Paul de Vries.\nIn the first few years the winners of the award were determined by public vote. The last few years the public vote made place for a jury with specialists from the trading world.\n"}
{"id": "12193926", "url": "https://en.wikipedia.org/wiki?curid=12193926", "title": "IMUnified", "text": "IMUnified\n\nIMUnified, formed in 2000, is a coalition of companies that intend to develop open standards for instant messaging (IM). The founding members are AT&T, Excite@Home, iCAST, Microsoft, Odigo, Phone.com, Prodigy, Tribal Voice and Yahoo!. Notably absent from the list of members is AOL, who was not invited to join the coalition. Some analysts believe the goal of the coalition was to try to force AOL toward a more open IM standard.\n\n\n"}
{"id": "44364819", "url": "https://en.wikipedia.org/wiki?curid=44364819", "title": "Joseph Glickauf", "text": "Joseph Glickauf\n\nJoseph Glickauf Jr. (January 15, 1912 – July 9, 2005), was an American-born engineer, inventor and corporate executive known as one of the first advocates of the use of computers in business and industry and the “father” of the computer consulting industry. \nJoseph Glickauf was hired by Arthur Andersen Co. immediately after serving in the US Navy and was tasked with initiating the use of the freshly invented computer for his employer. Glickauf became familiar with the capabilities of the UNIVAC and immediately saw the far-reaching implications of computers for business. To demonstrate the computer to Arthur Andersen’s employees he built the Arthur Andersen Demonstration Computer known as “Glickiac”. The company management was quick to see the potential and made resources available for future development. \nIn 1953 General Electric Appliance Park hired Arthur Andersen to automate GE’s payroll. Glickauf lead the effort and recommended GE the installation of a UNIVAC I computer and printer. The project was initially a failure but it started what is now known as the “computer consulting”.\n"}
{"id": "22348485", "url": "https://en.wikipedia.org/wiki?curid=22348485", "title": "Level spreader", "text": "Level spreader\n\nA level spreader is an erosion control device designed to reduce water pollution by mitigating the impact of high-velocity stormwater surface runoff. It is used both on construction sites and for permanent applications such as drainage for roads and highways. The device reduces the energy level in high-velocity flow by converting it into sheet flow, and disperses the discharged water so that it may be infiltrated into soil.\n\nLevel spreaders may be used in conjunction with runoff infiltration devices such as bioretention systems, infiltration basins and percolation trenches.\n\n"}
{"id": "22409558", "url": "https://en.wikipedia.org/wiki?curid=22409558", "title": "Livewell", "text": "Livewell\n\nA livewell is a tank found on many fishing boats that is used to keep bait and caught fish alive. It works by pumping fresh water from the surrounding body into the tank, as well as keeping the water aerated. A rule of thumb for determining the necessitated size of a livewell is that every one inch of fish needs a gallon of water if it is desired to keep the fish alive for a prolonged period of time.\n\nBesides size and water circulation, two other key factors of the functionality of a livewell are maintaining proper temperature and removing metabolic waste. Water temperatures should be below ; ice can be used as a coolant.\n\nIt is very important to maintain adequate oxygen levels in the livewell. It should be above 5 parts per million. Electric aeration systems are often used to do this. These often work by creating a spray that increases the surface area of the water, or by passing it through a Venturi.\n\nA livewell is a box used to transport live aquatic animals; shrimp, baitfish and mature fish, saltwater or freshwater species. A livewell should be made of materials that are non- toxic to aquatic animals. The shape may be square, rectangular, oval or round. The box may be insulated, portable, have a drain and lid.\nMaintaining minimal safe water quality standards in livewell water is essential and necessary to insure a safe habitat for all the captive animals during transport.\nWhen transporting baitfish, shrimp or mature fish, maintaining dissolved oxygen saturation (DO Sat) is the single most important water quality parameter that must be controlled.\nLivewell oxygen–injection systems and LOX systems insure O2 enriched livewell water. Pure 100% compressed welding oxygen is injected into the water with a precision dose adjustable high-pressure oxygen regulator, oxygen tube and diffuser. Commercial and sport fishing oxygen-injection systems are designed to insure 100% DO Saturation or greater whether the bait or fish load is 1 lb or >1000 lbs.\nMinimal safe EPA water quality standards for steady state environments (rivers, lakes, ponds, etc.) is 5 ppm DO.\n\nDissolved oxygen is the single most important factor for keeping bass alive, and an understanding of factors that affect oxygen levels will better enable anglers to keep their fish [aquatic animals] alive. At a moderate water temperature of 70˚F, 100 percent oxygen saturation is 8.8 mg/l of oxygen, whereas at the higher temperature of 80˚F, 100 percent saturation is 7.9 mg/l. Both of these 100 percent saturation oxygen levels are suitable for keeping bass alive.\nWithout injecting oxygen into the livewell, it is very difficult to supply enough oxygen to keep alive heavier tournament limits.\nOxygen injection has long been used by Texas Parks and Wildlife Department (TPWD) hatcheries to maintain the health of fish being stocked into reservoirs. Fisheries staff regularly transport or hold fish in ratios equal to or greater than one pound of fish to a gallon of water. However, boat manufactures do not offer oxygen injection system options…\nProper installation and operation of an oxygen injection system will ensure oxygen levels remain above the preferred level of 7 mg/l even when livewells contain heavy limits.\n"}
{"id": "23031100", "url": "https://en.wikipedia.org/wiki?curid=23031100", "title": "Malaysian Biotechnology Information Centre", "text": "Malaysian Biotechnology Information Centre\n\n•\n\nMalaysian Biotechnology Information Centre fondly known as MABIC is a Non-Governmental Organization which is also one of many sister organisations under the umbrella of the Global Knowledge Center on Crop Biotechnology (KC) by the International Service for the Acquisition of Agri-biotech Applications (ISAAA).\nIt has been entrusted to communicate the vast and numerous aspects of biotechnology to its various stakeholder groups.The organization's office is located in Monash University, Sunway Campus.\n\nFormed in year 2000, MABIC has been communicating the various aspects of biotechnology and has been providing support to its various stakeholders through the dissemination of accurate information on biotechnology development in Malaysia.\n\nMABIC, as one-stop biotechnology resource centre, functions as a prime mover of biotechnological related news in Malaysia and acts as a catalyst in bringing about biotechnology knowledge dynamism through excellence in publishing biotechnology related articles in BicAlert newsletter which is published on the 15th of every month. Their mission is to provide scientifically accurate and fact-based resources to all stakeholders, to provide a platform for discussion of issues in biotechnology, and ultimately to support the Government’s efforts in developing biotechnology as a tool for national development.\n\nAs such its role is to act as:\n\n• Information Disseminator\n\n• Organising Outreach Programmes\n\n• Connecting Biotechnology Players\n\n• Newsletter Publication\n\n• Building Strong Database of Stakeholders\n\nTo be a gateway to public understanding of biotechnology\n\n• To serve as a Biotechnology Information Centre which provides accurate and factual resources to stakeholders\n\n• To increase public awareness on biotechnology\n\n• To support the government’s efforts and assist in developing biotechnology as a key driver of economic growth\n\n• To promote active discussions in issues which hinge heavily on biotechnology which are important to the nation\n\n• To serve as a platform for dynamic discussions among stakeholders on issues related to biotechnology.\n\n• To make information on biotechnology accessible to all stakeholders\n\n"}
{"id": "10489186", "url": "https://en.wikipedia.org/wiki?curid=10489186", "title": "Melamine resin", "text": "Melamine resin\n\nMelamine resin or melamine formaldehyde (also shortened to melamine) is a hard, thermosetting plastic material made from melamine and formaldehyde by polymerization. \n\nIn its butylated form, it is dissolved in \"n\"-butanol and xylene. It is then used to cross-link with alkyd, epoxy, acrylic, and polyester resins, used in surface coatings. There are many types, varying from very slow to very fast curing. \n\nIt was discovered by William F. Talbot, who applied to patent it on December 12, 1936.\n\nThe principal use of melamine resin is as the main constituent of high-pressure laminates, such as Formica and Arborite, and of laminate flooring. Melamine-resin tile wall panels can also be used as whiteboards. Melamine formaldehyde is used in plastic laminate and overlay materials. Formaldehyde is more tightly bound in melamine-formaldehyde than it is in urea-formaldehyde, reducing emissions.\n\nMelamine resin is often used in kitchen utensils and plates (such as Melmac). Melamine resin utensils and bowls are not microwave safe.\n\nDuring the late 1950s and 1960s melamine tableware became fashionable. Aided by the stylish modern designs of A. H. Woodfull and the Product Design Unit of British Industrial Plastics, it was thought to threaten the dominant position of ceramics in the market. The tendency of melamine cups and plates to stain and scratch led sales to decline in the late 1960s, however, and eventually the material became largely restricted to the camping and nursery market.\n\nMelamine resin is often used to saturate decorative paper that is laminated under heat and pressure and then pasted onto particle board; the resulting panel, often called \"melamine\", is commonly used in ready-to-assemble furniture and kitchen cabinets.\n\nMelamine is available in diverse sizes and thicknesses, as well as a large number of colors and patterns. The sheets are heavy; the resin is prone to chipping when being cut with conventional table saws.\n\nMelamine-formaldehyde resin forms via the condensation of formaldehyde with melamine to give, under idealized conditions, the hexa-hydroxymethyl derivative. Upon heating in the presence of acid, this or similar hydroxymethylated species undergoes further condensation and crosslinking. Linkages between the heterocycles include mono-, di-, and polyethers. The microstructure of the material can be analyzed by NMR spectroscopy. The crosslinking density of melamine resins can be controlled by co-condensation with bifunctional analogues of melamine, such as benzoguanamine and acetoguanamine. \n\n"}
{"id": "4213869", "url": "https://en.wikipedia.org/wiki?curid=4213869", "title": "Ministry of Petroleum (Egypt)", "text": "Ministry of Petroleum (Egypt)\n\nThe Ministry of Petroleum is the Egyptian authority that supervises exploration, production, marketing and distribution of oil, gas and other natural resources. The ministry was established in 1972. The current minister is Tarek El-Molla.\n\nThe petroleum sector in Egypt consists of 6 state-owned entities. These are: Egyptian General Petroleum Corporation (EGPC), Egyptian Natural Gas Holding Company(EGAS), Egyptian Petrochemicals Holding Company (ECHEM), Ganoub El Wadi Petroleum Holding Company (GANOPE), and Egyptian General Authority for Mineral Resources.\n\n\n"}
{"id": "1600474", "url": "https://en.wikipedia.org/wiki?curid=1600474", "title": "Momentum exchange tether", "text": "Momentum exchange tether\n\nA momentum exchange tether is a kind of space tether that could theoretically be used as a launch system, or to change spacecraft orbits. Momentum exchange tethers create a controlled force on the end-masses of the system due to the pseudo-force known as centrifugal force. While the tether system rotates, the objects on either end of the tether will experience continuous acceleration; the magnitude of the acceleration depends on the length of the tether and the rotation rate. Momentum exchange occurs when an end body is released during the rotation. The transfer of momentum to the released object will cause the rotating tether to lose energy, and thus lose velocity and altitude. However, using electrodynamic tether thrusting, or ion propulsion the system can then re-boost itself with little or no expenditure of consumable reaction mass.\n\nA non-rotating tether is a rotating tether that rotates exactly once per orbit so that it always has a vertical orientation relative to the parent body. A spacecraft arriving at the lower end of this tether, or departing from the upper end, will take momentum from the tether, while a spacecraft departing from the lower end of the tether, or arriving at the upper end, will add momentum to the tether. \n\nIn some cases momentum exchange systems are intended to run as balanced transportation schemes where an arriving spacecraft or payload is exchanged with one leaving with the same speed and mass, and then no net change in momentum or angular momentum occurs.\n\nGravity-gradient stabilization, also called \"gravity stabilization\" and \"tidal stabilization\", is a simple and reliable method for controlling the attitude of a satellite that requires no electronic control systems, rocket motors or propellant.\n\nThis type of attitude control tether has a small mass on one end, and a satellite on the other. Tidal forces stretch the tether between the two masses. There are two ways of explaining tidal forces. In one, the upper end mass of the system is moving faster than orbital velocity for its altitude, so centrifugal force makes it want to move further away from the planet it is orbiting. At the same time, the lower end mass of the system is moving at less than orbital speed for its altitude, so it wants to move closer to the planet. The end result is that the tether is under constant tension and wants to hang in a vertical orientation. The other way to explain tidal force is that the top of a tall object weighs less than the bottom, so they are pulled by different amounts. The \"extra\" pull on the \"bottom\" of the object stretches it out. On Earth, these are small effects, but in space, nothing opposes them. Either way, the end result is that the tidal forces stabilize the satellite so that its long dimension points towards the planet it is orbiting. Simple satellites have often been stabilized this way; either with tethers, or with how the mass is distributed within the satellite.\n\nAs with any freely hanging object, it can be disturbed and start to swing. Since there is no atmospheric drag in space to slow the swing, a small bottle of fluid with baffles may be mounted in the spacecraft to damp the pendulum vibrations via the viscous friction of the fluid.\n\nA sky-hook is a theoretical class of orbiting tether propulsion intended to lift payloads to high altitudes and speeds. Proposals for sky-hooks include designs that employ tethers spinning at hyper-sonic speed for catching high speed payloads or high altitude aircraft and placing them in orbit.\n\nIn a strong planetary magnetic field such as around the Earth, a conducting tether can be configured as an electrodynamic tether. This can either be used as a dynamo to generate power for the satellite at the cost of slowing its orbital velocity, or it can be used to increase the orbital velocity of the satellite by putting power into the tether from the satellite's power system. Thus the tether can be used to either accelerate or to slow an orbiting spacecraft without using any rocket propellant.\n\nWhen using this techniques with a rotating tether, the current through the tether must alternate in phase with the rotation rate of the tether in order to produce either a consistent slowing force or a consistent accelerating force.\n\nWhether slowing or accelerating the satellite, the electrodynamic tether pushes against the planet's magnetic field, and thus the momentum gained or lost ultimately comes from the planet.\n\nA Bolo, or rotating tether, is a tether that rotates more than once per orbit and whose endpoints have a significant tip speed (~ 1 – 3 km/s). The maximum speed of the endpoints is limited by the strength of the cable material and the safety factor it is designed for.\n\nThe purpose of the Bolo is to either speed up, or slow down, a spacecraft that docks with it without using any of the spacecraft's on-board propellant and to change the spacecraft's orbital flight path. Effectively, the Bolo acts as a reusable upper stage for any spacecraft that docks with it.\n\nThe momentum imparted to the spacecraft by the Bolo is not free. In the same way that the Bolo changes the spacecraft's momentum and direction of travel, the Bolo's orbital momentum and rotational momentum is also changed, and this costs energy that must be replaced. The idea is that the replacement energy would come from a more efficient and lower cost source than a chemical rocket motor. Two possible lower cost sources for this replacement energy are an ion propulsion system, or an electrodynamic tether propulsion system that would be part of the Bolo. An essentially free source of replacement energy is momentum gathered from payloads to be accelerated in the other direction, suggesting that the need for adding energy from propulsion systems will be quite minimal with balanced, two-way, space commerce.\n\nRotovators are rotating tethers with a rotational direction such that the lower endpoint of the tether is moving slower than the orbital velocity of the tether and the upper endpoint is moving faster. The word is a portmanteau derived from the words \"rotor\" and \"elevator\".\n\nIf the tether is long enough and the rotation rate high enough, it is possible for the lower endpoint to completely cancel the orbital speed of the tether such that the lower endpoint is stationary with respect to the planetary surface that the tether is orbiting. As described by Moravec, this is \"a satellite that rotates like a wheel\". The tip of the tether moves in approximately a cycloid, in which it is momentarily stationary with respect to the ground. In this case, a payload that is \"grabbed\" by a capture mechanism on the rotating tether during the moment when it is stationary would be picked up and lifted into orbit; and potentially could be released at the top of the rotation, at which point it is moving with a speed significantly greater than the escape velocity and thus could be released onto an interplanetary trajectory. (As with the bolo, discussed above, the momentum and energy given to the payload must be made up, either with a high-efficiency rocket engine, or with momentum gathered from payload moving the other direction.)\n\nOn bodies with an atmosphere, such as the Earth, the tether tip must stay above the dense atmosphere. On bodies with reasonably low orbital speed (such as the Moon and possibly Mars), a rotovator in low orbit can potentially touch the ground, thereby providing cheap surface transport as well as launching materials into cislunar space. In January 2000, The Boeing Company completed a study of tether launch systems including two-stage tethers that had been commissioned by the NASA Institute for Advanced Concepts.\n\nUnfortunately an Earth-to-orbit rotovator cannot be built from currently available materials since the thickness and tether mass to handle the loads on the rotovator would be uneconomically large. A \"watered down\" rotovator with two-thirds the rotational speed, however, would halve the centripetal acceleration stresses.\n\nTherefore, another trick to achieve lower stresses is that rather than picking up a cargo from the ground at zero velocity, a rotovator could pick up a moving vehicle and sling it into orbit. For example, a rotovator could pick up a Mach 12 aircraft from the upper atmosphere of the Earth and move it into orbit without using rockets, and could likewise catch such a vehicle and lower it into atmospheric flight. It is easier for a rocket to achieve the lower tip speed, so \"single stage to tether\" has been proposed. One such is called the Hyper-sonic Airplane Space Tether Orbital Launch (HASTOL). Either air breathing or rocket to tether could save a great deal of fuel per flight, and would permit for both a simpler vehicle and more cargo.\n\nThe company Tethers Unlimited, Inc. (founded by Robert Forward and Robert P. Hoyt) has called this approach \"Tether Launch Assist\". It has also been referred to as a space bolas.\n\nA space elevator is a space tether that is attached to a planetary body. For example, on Earth, a space elevator would go from the equator to well above geosynchronous orbit.\n\nA space elevator does not need to be powered as a rotovator does, because it gets any required angular momentum from the planetary body. The disadvantage is that it is much longer, and for many planets a space elevator cannot be constructed from known materials. A space elevator on Earth would require material strengths outside current technological limits (2014). Martian and lunar space elevators could be built with modern-day materials however. A space elevator on Phobos has also been proposed.\n\nSpace elevators also have larger amounts of potential energy than a rotovator, and if heavy parts (like a \"dropped wrench\") should fall they would reenter at a steep angle and impact the surface at near orbital speeds. On most anticipated designs, if the cable component itself fell, it would burn up before hitting the ground.\n\nAlthough it might be thought that this requires constant energy input, it can in fact be shown to be energetically favorable to lift cargo off the surface of the Moon and drop it into a lower Earth orbit, and thus it can be achieved without any significant use of propellant, since the Moon's surface is in a comparatively higher potential energy state. Also, this system could be built with a total mass of less than 28 times the mass of the payloads.\n\nRotovators can thus be charged by momentum exchange. Momentum charging uses the rotovator to move mass from a place that is \"higher\" in a gravity field to a place that is \"lower\". The technique to do this uses the Oberth effect, where releasing the payload when the tether is moving with higher linear speed, lower in a gravitational potential gives more specific energy, and ultimately more speed than the energy lost picking up the payload at a higher gravitational potential, even if the rotation rate is the same. For example, it is possible to use a system of two or three rotovators to implement trade between the Moon and Earth. The rotovators are charged by lunar mass (dirt, if exports are not available) dumped on or near the Earth, and can use the momentum so gained to boost Earth goods to the Moon. The momentum and energy exchange can be balanced with equal flows in either direction, or can increase over time.\n\nSimilar systems of rotovators could theoretically open up inexpensive transportation throughout the solar system.\n\nA tether cable catapult system is a system where two or more long conducting tethers are held rigidly in a straight line, attached to a heavy mass. Power is applied to the tethers and is picked up by a vehicle that has linear magnet motors on it, which it uses to push itself along the length of the cable. Near the end of the cable the vehicle releases a payload and slows and stops itself and the payload carries on at very high velocity. The calculated maximum speed for this system is extremely high, more than 30 times the speed of sound in the cable; and velocities of more than 30 km/s seem to be possible.\n"}
{"id": "34955137", "url": "https://en.wikipedia.org/wiki?curid=34955137", "title": "Mortice Gauge", "text": "Mortice Gauge\n\nA mortise gauge is a woodworking tool used by a carpenter or joiner to scribe mortise and tenon joints on wood prior to cutting. Mortise gauges are commonly made of hardwood with brass fittings.\n\nLike the simpler marking gauge, a mortise gauge has a locking thumb screw slide for adjusting the distance of the scribe from the edge of the wood. It has two protruding pins, often called \"spurs\", which are designed to scribe parallel lines marking both sides of a mortise at the same time. One of the pins is adjustable, attached to a sliding fence, so that mortises of different widths can be marked.\n\nSome mortise gauges are designed with one retractable spur, so that they can be used as marking gauges as well; however, because the mortise gauge is an expensive and high precision tool, many carpenters prefer to have a separate marking gauge for general use.\n\nFor complex joints, some mortise gauges have a double-beam design which allows the gauge to be wrapped around a tool such as a chisel for extra accuracy.\n"}
{"id": "20390106", "url": "https://en.wikipedia.org/wiki?curid=20390106", "title": "Museum for Old Techniques", "text": "Museum for Old Techniques\n\nThe Museum for Old Techniques (, MOT) is located in Grimbergen, Belgium. The museum has an extensive collection of hand tools, technical manuals and trade catalogues.\n\nThe MOT studies the history of techniques, more specifically natural power. The subject is limited to what is driven by muscle, water or wind power.\n\nThe MOT was founded in 1982 by Johan David, its curator. The town of Grimbergen made some historic buildings available for it. In 1999 the MOT was recognized as a museum by the Flemish Community.\n\nThe main building of the MOT is the Guldendal, situated in the shadow of the ruins of the Prinsenkasteel (Prince’s Castle). The administration and staff are accommodated in this building. Here, too, are the library, the educational service and the conference room.\n\nIn addition there are the Liermolen and Tommenmolen, two watermills, situated on the Maalbeek. The Liermolen is a working mill, which is used for milling demonstrations.\n\nThe MOT aims to encourage an understanding of the past and present of mankind, here and elsewhere, by giving a realistic picture of the origin and evolution of techniques and their impact of everyday life and the environment.\n\n"}
{"id": "27373075", "url": "https://en.wikipedia.org/wiki?curid=27373075", "title": "National Council of Science Museums", "text": "National Council of Science Museums\n\nNational Council of Science Museums (NCSM) is an autonomous organisation under Indian Ministry of Culture. It is the largest chain of science centers or museums under a single administrative umbrella in the world. There are 24 own science centers or museums and one R & D laboratory and Training centre of NCSM, located in different states in India.\n\n\n\n\n\n\n\nNCSM is now developing science centres at the following places in India with collaboration by the respective State Governments.\n\n\nA two years full-time four semester M Tech course in Science Communication in collaboration with the Birla Institute of Technology and Science, Pilani, is being offered from 2005 at NCSM campus, Kolkata.\n\n"}
{"id": "20505381", "url": "https://en.wikipedia.org/wiki?curid=20505381", "title": "Nicole Wong", "text": "Nicole Wong\n\nNicole A. Wong is an American attorney, specializing in Internet, media and intellectual property law. In May 2013, she was selected by the Barack Obama administration to be the White House deputy chief technology officer (CTO) of the United States. She earned the nickname \"the Decider\" while she was vice president and deputy general counsel at Google, where she was responsible for arbitrating issues of censorship for Google. Wong stepped down as Deputy US CTO on August 16, 2014 to return with her family to California. She currently serves as a Senior Advisor to Albright Stonebridge Group, a global business strategy firm.\n\nNicole Wong, a fourth-generation Chinese American, was born in the United States. Her great-grandfather was a Chinese immigrant who entered the United States through Canada and harvested potatoes in Idaho, worked at a laundry in Michigan and became a cook in Livermore, California. Her maternal grandmother was from Southern China. Until the 1950s, her grandparents were unable to own property in California. They helped found one of the first Chinese community banks in the country, and her grandfather became its vice president. \n\nWong grew up in Del Mar, California and initially wanted to be a journalist because her aunt was a reporter at the \"Los Angeles Times\". She attended Georgetown University where she worked as a news editor at the campus paper and radio station. She graduated \"magna cum laude\" with a B.A. degree in American Studies and a minor in English in 1990. She has a fellowship in poetry and later received her law degree from the University of California, Berkeley School of Law, and a master's degree in journalism from the University of California, Berkeley Graduate School of Journalism in 1995. At Berkeley, she co-founded the Asian Law Journal and became its first editor-in-chief.\n\nAfter graduation, Wong worked as an associate and practiced First Amendment law at Steinhart & Falconer LLP in San Francisco; her clients included Bay Area newspapers and radio stations. When the Internet boom hit, she began advising Yahoo!, Evite, and PayPal. She was an associate at Perkins Coie LLP in 1997 and was named a partner in 2000. Wong represented media clients including the \"Los Angeles Times\", The Walt Disney Company, Microsoft, and Amazon.com before joining Google as senior compliance counsel. She eventually got promoted to Vice President and Deputy General Counsel in 2004. Wong was responsible for Google’s product and regulatory matters.\n\nIn November 2012, Wong left Google and became the legal director of products at Twitter. In June 2013, she joined the Obama administration as deputy U.S. chief technology officer, working with U.S. Chief Technology Officer Todd Park. She is currently a Senior Advisor at Albright Stonebridge Group.\n\nWong has served on the governing committee of the ABA Communications Law Forum since 2001, and on the board of directors of the First Amendment Coalition since 2007. She is on the advisory board at University of California, Berkeley Graduate School of Journalism. She previously served on the Board of Governors of the National Asian Pacific American Bar Association from 1996 to 1998, and as a co-chair of the Practising Law Institute’s Internet Law Institute from 2001 to 2004. From 1997 to 1998, she was a member of the San Francisco Sunshine Task Force.\n\nWong had testified four times before the U.S. Congress regarding Internet policy. At one hearing she stated, \"First and foremost, the U.S. Government should promote Internet openness as a major plank for our foreign policy.\" She is a co-editor of \"Electronic Media and Privacy Law Handbook\" (2003). She also has taught media and Internet law courses as an adjunct professor at the University of California at Berkeley, Stanford University, and University of San Francisco School of Law in 1997 and 1999.\n\n"}
{"id": "24305905", "url": "https://en.wikipedia.org/wiki?curid=24305905", "title": "Parcel audit", "text": "Parcel audit\n\nParcel audit (also referred to as small package auditing or small parcel auditing) is the process of reviewing shipping invoices for invalid charges and other billing inaccuracies. Each shipment tendered to a major parcel carrier like UPS and FedEx come with service guarantees outlined within each carrier's tariff or service guide. An increasing number of businesses contract third-party companies to perform these services because they have the technology necessary to automate the otherwise time consuming process.\n\nParcel carriers like UPS, FedEx, and DHL often have large, long, and complicated invoices. A customer shipping a high volume of parcel boxes via one of these carriers sees invoices hundreds of pages long. The invoices show a high amount of visibility to how a package was carried and delivered. This finite data is collected by UPS and FedEx for tracking purposes and to automate the process of charging the appropriate amount to their clients.\n\nThe invoicing process is so automated that mistakes happen often. Insiders to UPS and FedEx claim that little focus is spent to prevent billing for inappropriate charges: duplicate charges, incorrect discount amounts, and phantom accessorial charges. Invoicing errors are a common phenomenon, and most large carriers provide teams and electronic forms to contest improper charges on invoices. With the proper data to dispute the charges, they are often credited in a matter of hours.\n\nOverbilling - or service failures - is a larger problem than invoicing errors. A service failure occurs when a package with a time-sensitive guarantee is delivered late and a refund is due to the client. Most carriers like UPS and FedEx place the responsibility on the customer to check the timeliness of the delivered package. Once the service failure is found, the carrier typically credits the amount due quickly. The terms and conditions for claiming a late package are stated and the act of claiming a refund is also time-sensitive.\n\nUp to 5% of a clients UPS and/or FedEx charges are due to errors or overbilled items. This results in around $2 billion of unclaimed refunds.\n\nCustomers forward their electronic billing file or approve their carriers to send the electronic billing files directly to the auditor. Most auditors accept all forms of electronic billing files. Customers may request these files from their carriers.\n\nThe parcel auditing industry, otherwise knows as small package audit, has been growing in popularity. Most auditors operate using a performance-based model.\n"}
{"id": "1732978", "url": "https://en.wikipedia.org/wiki?curid=1732978", "title": "Pascal's calculator", "text": "Pascal's calculator\n\nPascal's calculator (also known as the arithmetic machine or Pascaline) is a mechanical calculator invented by Blaise Pascal in the early 17th century. Pascal was led to develop a calculator by the laborious arithmetical calculations required by his father's work as supervisor of taxes in Rouen. He designed the machine to add and subtract two numbers directly and to perform multiplication and division through repeated addition or subtraction.\n\nPascal's calculator was especially successful in the design of its carry mechanism, which adds 1 to 9 on one dial, and when it changes from 9 to 0, carries 1 to the next dial. His innovation made each digit independent of the state of the others, which enabled multiple carries to rapidly cascade from one digit to another regardless of the machine's capacity. Pascal was also the first to shrink and adapt for his purpose a lantern gear, used in turret clocks and water wheels, which could resist the strength of any operator input with very little added friction.\n\nPascal designed the machine in 1642, and after 50 prototypes, he presented it to the public in 1645, dedicating it to Pierre Séguier, then chancellor of France. Pascal built around twenty more machines during the next decade, many of which improved on his original design. In 1649, King Louis XIV of France gave Pascal a royal privilege (similar to a patent), which gave him the exclusive right to design and manufacture calculating machines in France. Nine Pascal calculators presently exist; most are on display in European museums.\n\nMany later calculators either were directly inspired by, or shaped by the same historical influences which led to, Pascal's invention. Gottfried Leibniz invented his Leibniz wheels after 1671, after trying to add an automatic multiplication feature to the Pascaline. In 1820, Thomas de Colmar designed his arithmometer, the first mechanical calculator strong enough and reliable enough to be used daily in an office environment. It is not clear whether he ever saw Leibniz's device, but he either re-invented it or utilised Leibniz's invention of the step drum.\n\nPascal began to work on his calculator in 1642, when he was 19 years old. He had been assisting his father, who worked as a tax commissioner, and sought to produce a device which could reduce some of his workload. Pascal received a Royal Privilege in 1649 that granted him exclusive rights to make and sell calculating machines in France.\n\nBy 1654 he had sold about twenty machines, but the cost and complexity of the Pascaline was a barrier to further sales and production ceased in that year. By that time Pascal had moved on to the study of religion and philosophy, which gave us both the \"Lettres provinciales\" and the \"Pensées\".\n\nThe tercentenary celebration of Pascal's invention of the mechanical calculator occurred during WWII when France was occupied by Germany and therefore the main celebration was held in London, England. Speeches given during the event highlighted Pascal's practical achievements when he was already known in the field of pure mathematics, and his creative imagination, along with how ahead of their time both the machine and its inventor were.\n\nThe calculator had spoked metal wheel dials, with the digit 0 through 9 displayed around the circumference of each wheel. To input a digit, the user placed a stylus in the corresponding space between the spokes and turned the dial until a metal stop at the bottom was reached, similar to the way the rotary dial of a telephone is used. This displayed the number in the windows at the top of the calculator. Then, one simply redialed the second number to be added, causing the sum of both numbers to appear in the accumulator.\n\nEach dial is associated with a one-digit display window located directly above it, which displays the value of the accumulator for this position. The complement of this digit, in the base of the wheel (6, 10, 12, 20), is displayed just above this digit. A horizontal bar hides either all the complement numbers when it is slid to the top, or all the direct numbers when it is slid toward the center of the machine. It thereby displays either the content of the accumulator or the complement of its value.\n\nSince the gears of the calculator rotated in only one direction, negative numbers could not be directly summed. To subtract one number from another, the method of nine's complement was used. The only two differences between an addition and a subtraction are the position of the display bar (direct versus complement) and the way the first number is entered (direct versus complement).\n\nFor a 10-digit wheel (N), the fixed outside wheel is numbered from 0 to 9 (N-1). The numbers are inscribed in a decreasing manner clockwise going from the bottom left to the bottom right of the stop lever. To add a 5, one must insert a stylus between the spokes that surround the number 5 and rotate the wheel clockwise all the way to the stop lever. The number displayed on the corresponding display register will be increased by 5 and, if a carry transfer takes place, the display register to the left of it will be increased by 1. To add 50, use the tens input wheel (second dial from the right on a decimal machine), to add 500, use the hundreds input wheel, etc...\n\nOn all the wheels of all the known machines, except for the \"machine tardive\", two adjacent spokes are marked; these marks differ from machine to machine. On the wheel pictured on the right, they are drilled dots, on the surveying machine they are carved; some are just scratches or marks made with a bit of varnish, some were even marked with little pieces of paper.\n\nThese marks are used to set the corresponding cylinder to its maximum number, ready to be re-zeroed. To do so, the operator inserts the stylus in between these two spokes and turns the wheel all the way to the stopping lever. This works because each wheel is directly linked to its corresponding display cylinder (it automatically turns by one during a carry operation). To mark the spokes during manufacturing, one can move the cylinder so that its highest number is displayed and then mark the spoke under the stopping lever and the one to the right of it.\n\nFour of the known machines have inner wheels of complements, which were used to enter the first operand in a subtraction. They are mounted at the center of each spoked metal wheel and turn with it. The wheel displayed in the picture above has an inner wheel of complements but the numbers written on it are barely visible. On a decimal machine, the digits 0 through 9 are carved clockwise, with each digit positioned between two spokes so that the operator can directly inscribe its value in the window of complements by positioning his stylus in between them and turning the wheel clockwise all the way to the stop lever. The marks on two adjacent spokes flank the digit 0 inscribed on this wheel.\n\nOn four of the known machines, above each wheel, a small quotient wheel is mounted on the display bar. These quotient wheels, which are set by the operator, have numbers from 1 to 10 inscribed clockwise on their peripheries (even above a non-decimal wheel). Quotient wheels seem to have been used during a division to memorize the number of times the divisor was subtracted at each given index.\n\nPascal went through 50 prototypes before settling on his final design; we know that he started with some sort of calculating clock mechanism that used springs which apparently \"works by springs and which has a very simple design\", was used \"many times\" and remained in \"operating order\". Nevertheless, \"while always improving on it\" he found reason to try to make the whole system more reliable and robust. Eventually he adopted a component of very large clocks, shrinking and adapting for his purpose the robust gears that can be found in a turret clock mechanism called a lantern gear, itself derived from a water wheel mechanism. This could easily handle the strength of an operator input.\n\nPascal adapted a pawl and ratchet mechanism to his own turret wheel design; the pawl prevents the wheel from turning counterclockwise during an operator input, but it is also used to precisely position the display wheel and the carry mechanism for the next digit when it is pushed up and lands into its next position. Because of this mechanism, each number displayed is perfectly centered in the display window and each digit is precisely positioned for the next operation. This mechanism would be moved six times if the operator dialed a six on its associated input wheel.\n\nThe sautoir is the centerpiece of the pascaline's carry mechanism. In his \"\", Pascal noted that a machine with 10,000 wheels would work as well as a machine with two wheels because each wheel is independent of the other. When it is time to propagate a carry, the sautoir, under the sole influence of gravity, is thrown toward the next wheel without any contact between the wheels. During its free fall the sautoir behaves like an acrobat jumping from one trapeze to the next without the trapezes touching each other (\"sautoir\" comes from the French verb \"sauter\", which means to jump). All the wheels (including gears and sautoir) have therefore the same size and weight independently of the capacity of the machine.\n\nPascal used gravity to arm the sautoirs. One must turn the wheel five steps from 4 to 9 in order to fully arm a sautoir, but the carry transfer will move the next wheel only one step. Thus much extra energy is accumulated during the arming of a sautoir.\n\nAll the sautoirs are armed by either an operator input or a carry forward. To re-zero a 10,000-wheel machine, if one existed, the operator would have to set every wheel to its maximum and then add a 1 to the \"unit\" wheel. The carry would turn every input wheel one by one in a very rapid Domino effect fashion and all the display registers would be reset.\n\nThe carry transmission has three phases:\n\n\nThe Pascaline is a direct adding machine (it has no crank), so the value of a number is added to the accumulator as it is being dialed in. By moving a display bar, the operator can see either the number stored in the calculator or the complement of its value. Subtractions are performed like additions using some properties of 9's complement arithmetic.\n\nThe 9's complement of any one digit decimal number \"d\" is 9-\"d\". So the 9's complement of 4 is 5 and the 9's complement of 9 is 0. Similarly the 11's complement of 3 is 8.\n\nIn a decimal machine with n dials the 9's complement of a number A is:\nand therefore the 9's complement of (A-B) is:\n\nIn other words, the 9's complement of the difference of two numbers is equal to the sum of the 9's complement of the minuend added to the subtrahend. The same principle is valid and can be used with numbers composed of digits of various bases (base 6, 12, 20), like in the surveying or the accounting machines.\n\nThis can also be extended to:\n\nThis principle applied to the pascaline:\n\nThe machine has to be re-zeroed before each new operation. To reset his machine, the operator has to set all the wheels to their maximum, using the marks on two adjacent spokes, and then add 1 to the rightmost wheel.\n\nThe method of re-zeroing that Pascal chose, which propagates a carry right through the machine, is the most demanding task for a mechanical calculator and proves, before each operation, that the machine is fully functional. This is a testament to the quality of the Pascaline because none of the 18th century criticisms of the machine mentioned a problem with the carry mechanism and yet this feature was fully tested on all the machines, by their resets, all the time.\n\nAdditions are performed with the display bar moved closest to the edge of the machine, showing the direct value of the accumulator.\n\nAfter re-zeroing the machine, numbers are dialed in one after the other.\n\nThe following table shows all the steps required to compute 12,345 + 56,789 = 69,134\n\nSubtractions are performed with the display bar moved closest to the center of the machine showing the complement value of the accumulator.\n\nThe accumulator contains during the first step and after adding B. In displaying that data in the complement window, the operator sees which is A and then which is . It feels like an addition since the only two differences in between an addition and a subtraction are the position of the display bar (direct versus complement) and the way the first number is entered (direct versus complement).\n\nThe following table shows all the steps required to compute 54,321-12,345=41,976\n\nPascalines came in both decimal and non-decimal varieties, both of which can be viewed in museums today. They were designed for use by scientists, accountants and surveyors. The simplest Pascaline had five dials; later variants had up to ten dials.\n\nThe contemporary French currency system used \"livres\", \"sols\" and \"deniers\" with 20 \"sols\" to a \"livre\" and 12 \"deniers\" to a \"sol\". Length was measured in \"toises\", \"pieds\", \"pouces\" and \"lignes\" with 6 \"pieds\" to a \"toise\", 12 \"pouces\" to a \"pied\" and 12 \"lignes\" to a \"pouce\". Therefore, the pascaline needed wheels in base 6, 10, 12 and 20. Non-decimal wheels were always located before the decimal part.\n\nIn an accounting machine (..10,10,20,12), the decimal part counted the number of \"livres\" (20 \"sols\"), \"sols\" (12 \"deniers\") and \"deniers\". In a surveyor's machine (..10,10,6,12,12), the decimal part counted the number of \"toises\" (6 \"pieds\"), \"pieds\" (12 \"pouces\"), \"pouces\" (12 \"lignes\") and \"lignes\".\nScientific machines just had decimal wheels.\n\nThe decimal part of each machine is highlighted.\n\nThe metric system was adopted in France on , by which time Pascal's basic design had inspired other craftsmen, although with a similar lack of commercial success.\n\nMost of the machines that have survived the centuries are of the accounting type. Seven of them are in European museums, one belongs to the IBM corporation and one is in private hands.\n\nBesides being the first calculating machine made public during its time, the pascaline is also:\n\n\nIn 1957, Franz Hammer, a biographer of Johannes Kepler, announced the discovery of two letters that Wilhelm Schickard had written to his friend Johannes Kepler in 1623 and 1624 which contain the drawings of a previously unknown working calculating clock, predating Pascal's work by twenty years. The 1624 letter stated that the first machine to be built by a professional had been destroyed in a fire during its construction and that he was abandoning his project. After careful examination it was found, in contradistinction to Franz Hammer's understanding, that Schickard's drawings had been published at least once per century starting from 1718.\n\nBruno von Freytag Loringhoff, a mathematics professor at the University of Tübingen built the first replica of Schickard's machine but not without adding wheels and springs to finish the design. This detail is not described in Schickard's two surviving letters and drawings. A problem in the operation of the Schickard machine, based on the surviving notes, was found after the replicas were built. Schickard's machine used clock wheels which were made stronger and were therefore heavier, to prevent them from being damaged by the force of an operator input. Each digit used a display wheel, an input wheel and an intermediate wheel. During a carry transfer all these wheels meshed with the wheels of the digit receiving the carry. The cumulative friction and inertia of all these wheels could \"...potentially damage the machine if a carry needed to be propagated through the digits, for example like adding 1 to a number like 9,999\". The great innovation in Pascal's calculator was that it was designed so that each input wheel is totally independent from all the others and carries are propagated in sequence. Pascal chose, for his machine, a method of re-zeroing that propagates a carry right through the machine. It is the most demanding operation to execute for a mechanical calculator and proved, before each operation, that the carry mechanism of the Pascaline was fully functional. This could be taken as a testament to the quality of the Pascaline because none of the 18th century criticisms of the machine mentioned a problem with the carry mechanism and yet this feature was fully tested on all the machines, by their resets, all the time.\n\nGottfried Leibniz started to work on his own calculator after Pascal's death. He first tried to build a machine that could multiply automatically while sitting on top of the Pascaline, assuming (wrongly) that all the dials on Pascal's calculator could be operated at the same time. Even though this could not be done, it was the first time that a pinwheel was described and used in the drawing of a calculator.\n\nHe then devised a competing design, the Stepped Reckoner which was meant to perform additions, subtractions and multiplications automatically and division under operator control. Leibniz struggled for forty years to perfect this design and produced two machines, one in 1694 and one in 1706. Only the machine built in 1694 is known to exist; it was rediscovered at the end of the 19th century, having spent 250 years forgotten in an attic at the University of Göttingen.\n\nThe German calculating-machine inventor Arthur Burkhardt was asked to attempt to put Leibniz' machine in operating condition. His report was favorable except for the sequence in the carry. and \"therefore, especially in the case of multiple carry transfers, the operator had to check the result and manually correct the possible errors\". Leibniz had not succeeded in creating a calculator that worked properly, but he had invented the Leibniz wheel, the principle of a two-motion mechanical calculator. He was also the first to have cursors to inscribe the first operand and a movable carriage for results.\n\nThere were five additional attempts at designing \"direct entry\" calculating machines in the 17th century (including the designs of Tito Burattini, Samuel Morland and René Grillet).\n\nAround 1660 Claude Perrault designed an \"abaque rhabdologique\" that is often mistaken for a mechanical calculator because it has a carry mechanism in between the numbers. But it is actually an abacus, since it requires the operator to handle the machine differently when a carry transfer takes place.\n\nPascal's calculator was the most successful mechanical calculator developed in the 17th century for the addition and subtraction of large numbers. The stepped reckoner had a problem in the carry mechanism after more than two consecutive carries, and the other devices had carry mechanisms (one tooth wheel) that were limited in their capacity to carry across multiple digits or had no carry mechanism in between the digits of the accumulator.\n\nCalculating machines did not become commercially viable until 1851, when Thomas de Colmar released, after thirty years of development, his simplified arithmometer, the first machine strong enough to be used daily in an office environment. The Arithmometer was designed around Leibniz wheels and initially used Pascal's 9's complement method for subtractions.\n\n\n\n"}
{"id": "13976722", "url": "https://en.wikipedia.org/wiki?curid=13976722", "title": "Petrojack", "text": "Petrojack\n\nPetrojack was a Norwegian offshore drilling rig operator. The company had two jackup rigs under order from Jurong Shipyard in Singapore, while it sold its two former rigs \"Petrojack I\" and \"Petrojack III\" to Maersk Drilling. The company was founded in 2004 and was listed on the Oslo Stock Exchange the next year. The largest shareholders were Petrolia Drilling (40%), Awilco Offshore (18%) and Sinvest (18%).\n\nIn week 47 2009, Petrojack's stock value went up from 0,90 NOK per share to a maximum of 7,12 NOK per share. On the first day of this extreme rise, at 09:49, the stock was suspended because the share price rose 64% without any statement from the company itself. The stock was reopened 11 minutes later at 10:00.\n\nPetrojack reportedly went bankrupt toward the end of 2010.\n"}
{"id": "31451691", "url": "https://en.wikipedia.org/wiki?curid=31451691", "title": "RF module", "text": "RF module\n\nAn RF module (radio frequency module) is a (usually) small electronic device used to transmit and/or receive radio signals between two devices. In an embedded system it is often desirable to communicate with another device wirelessly. This wireless communication may be accomplished through optical communication or through radio frequency (RF) communication. For many applications the medium of choice is RF since it does not require line of sight. RF communications incorporate a transmitter and a receiver. They are of various types and ranges. Some can transmit up to 500 feet.\nRF modules are widely used in electronic design owing to the difficulty of designing radio circuitry. Good electronic radio design is notoriously complex because of the sensitivity of radio circuits and the accuracy of components and layouts required to achieve operation on a specific frequency. In addition, reliable RF communication circuit requires careful monitoring of the manufacturing process to ensure that the RF performance is not adversely affected. Finally, radio circuits are usually subject to limits on radiated emissions, and require Conformance testing and certification by a standardization organization such as ETSI or the U.S. Federal Communications Commission (FCC). For these reasons, design engineers will often design a circuit for an application which requires radio communication and then \"drop in\" a pre-made radio module rather than attempt a discrete design, saving time and money on development.\n\nRF modules are most often used in medium and low volume products for consumer applications such as garage door openers, wireless alarm or monitoring systems, industrial remote controls, smart sensor applications, and wireless home automation systems. They are sometimes used to replace older infra red communication designs as they have the advantage of not requiring line-of-sight operation.\n\nSeveral carrier frequencies are commonly used in commercially available RF modules, including those in the industrial, scientific and medical (ISM) radio bands such as 433.92 MHz, 915 MHz, and 2400 MHz. These frequencies are used because of national and international regulations governing the used of radio for communication. Short Range Devices may also use frequencies available for unlicensed such as 315 MHz and 868 MHz.\n\nRF modules may comply with a defined protocol for RF communications such as Zigbee, Bluetooth low energy, or Wi-Fi, or they may implement a proprietary protocol.\n\nThe term RF module can be applied to many different types, shapes and sizes of small electronic sub assembly circuit board. It can also be applied to modules across a huge variation of functionality and capability. RF modules typically incorporate a printed circuit board, transmit or receive circuit, antenna, and serial interface for communication to the host processor.\n\nMost standard, well known types are covered here:\n\nAn RF transmitter module is a small PCB sub-assembly capable of transmitting a radio wave and modulating that wave to carry data. Transmitter modules are usually implemented alongside a micro controller which will provide data to the module which can be transmitted. RF transmitters are usually subject to regulatory requirements which dictate the maximum allowable transmitter power output, harmonics, and band edge requirements.\n\nAn RF receiver module receives the modulated RF signal, and demodulates it. There are two types of RF receiver modules: superheterodyne receivers and super-regenerative receivers. Super-regenerative modules are usually low cost and low power designs using a series of amplifiers to extract modulated data from a carrier wave. Super-regenerative modules are generally imprecise as their frequency of operation varies considerably with temperature and power supply voltage. Superheterodyne receivers have a performance advantage over super-regenerative; they offer increased accuracy and stability over a large voltage and temperature range. This stability comes from a fixed crystal design which in the past tended to mean a comparatively more expensive product. However, advances in receiver chip design now mean that currently there is little price difference between superheterodyne and super-regenerative receiver modules.\n\nAn RF transceiver module incorporates both a transmitter and receiver. The circuit is typically designed for half-duplex operation, although full-duplex modules are available, typically at a higher cost due to the added complexity.\n\nAn SoC module is the same as a transceiver module, but it is often made with an onboard microcontroller. The microcontroller is typically used to handle radio data packetisation or managing a protocol such as an IEEE 802.15.4 compliant module. This type of module is typically used for designs that require additional processing for compliance with a protocol when the designer does not wish to incorporate this processing into the host microcontroller.\n\nRF modules typically communicate with an embedded system, such as a microcontroller or a microprocessor. The communication protocols include UART, used in Digi International's X-Bee modules, Serial Peripheral Interface Bus used in Anaren's AIR modules and Universal Serial Bus used in Roving Networks' modules. Although the module may use a standardized protocol for wireless communication, the commands sent over the microcontroller interface are typically not standardized as each vendor has its own proprietary communications format. The speed of the microcontroller interface depends on the speed of the underlying RF protocol used: higher speed RF protocols such as Wi-Fi require a high-speed serial interface such as USB whereas protocols with a slower data rate such as Bluetooth Low Energy may use a UART interface.\n\nThere are three types of signal modulation methods commonly used in RF transmitter and receiver modules:\nThe detailed description, advantages and disadvantages are listed in the linked articles above.\n\nAs with any other RF device, the performance of an RF module will depend on a number of factors. For example, by increasing the transmitter power, a larger communication distance will be achieved. However, this will also result in a higher electrical power drain on the transmitter device, which will cause shorter operating life for battery powered devices. Also, using a higher transmit power will make the system more prone to interference with other RF devices, and may in fact possibly cause the device to become illegal depending on the jurisdiction. Correspondingly, increasing the receiver sensitivity will also increase the effective communication range, but will also potentially cause malfunction due to interference from other RF devices.\n\nThe performance of the overall system may be improved by using matched antennas at each end of the communication link, such as those described earlier.\n\nFinally, the labeled remote distance of any particular system is normally measured in an open-air line of sight configuration without any interference, but often there will be obstacles such as walls, floors, or dense construction to absorb the radio wave signals, so the effective operational distance will in most practical instances be less than specified.\n\nA variety of methods are used to attach an RF module to a printed circuit board, either with through-hole technology or surface-mount technology. Through-hole technology allows the module to be inserted or removed without soldering. Surface-mount technology allows the module to be attached to the PCB without an additional assembly step. Surface-mount connections used in RF modules include land grid array (LGA) and Castellated Holes. The LGA package allows for small module sizes as the pads are all beneath the module but connections must be X-rayed to verify connectivity. Castellated Holes enable optical inspection of the connection but will make the module footprint physically larger to accommodate the pads.\n\nRF modules, especially SoC modules, are frequently used to communicate according to a pre-defined wireless standard, including:\nHowever, RF modules also frequently communicate using proprietary protocols, such as those used in garage door openers.\n\n\n"}
{"id": "51849942", "url": "https://en.wikipedia.org/wiki?curid=51849942", "title": "Remington-Rand Quiet-Riter.", "text": "Remington-Rand Quiet-Riter.\n\nThe Remington-Rand Quiet-Riter is a portable, mechanical typewriter manufactured in the 1950s.\n"}
{"id": "1543761", "url": "https://en.wikipedia.org/wiki?curid=1543761", "title": "Softbox", "text": "Softbox\n\nA soft box is a type of photographic lighting device, one of a number of photographic soft light devices. All the various soft light types create even and diffused light by transmitting light through some scattering material, or by reflecting light off a second surface to diffuse the light. The best known form of reflective source is the umbrella light, where the light from the bulb is \"bounced\" off the inside of a metalized umbrella to create an indirect \"soft\" light.\n\nA soft box is an enclosure around a bulb comprising reflective side and back walls and a diffusing material at the front of the light. \n\nThe sides and back of the box are lined with a bright surface - an aluminized fabric surface or an aluminum foil, to act as an efficient reflector. In some commercially available models the diffuser is removable to allow the light to be used alone as a floodlight or with an umbrella reflector. \n\nA soft box can be used with either flash or continuous light sources such as fluorescent lamps or \"hot lights\" such as quartz halogen bulbs or tungsten bulbs. If soft box lights are used with \"hot\" light sources, the user must be sure the soft box is heat rated for the wattage of the light to which it is attached in order to avoid fire hazard.\n\n"}
{"id": "36545025", "url": "https://en.wikipedia.org/wiki?curid=36545025", "title": "Stack Soap", "text": "Stack Soap\n\nStack Soap is a personal care product developed to solve the issue of soap waste when an old bar of soap wears thin and would otherwise be discarded. Stack Soap implements a grooved stack design such as that used with Pringles chips to merge an old, thin bar of soap with a new one.\n\nThe concept was successfully pitched on Kickstarter in 2012, and the product was featured in various media outlets including \"Wired\" magazine, \"The Atlantic\", Gizmodo, and market research firm Mintel.\n"}
{"id": "55122869", "url": "https://en.wikipedia.org/wiki?curid=55122869", "title": "Sticky pad", "text": "Sticky pad\n\nSticky pad is a friction device used to prevent objects from sliding on a surface, by effectively increasing the friction between the object and the surface.\n\nSticky pads are used to fix items to otherwise smooth surface that is leaned or that moves, so that objects put on that surface could off due to insufficient friction when the surface inclines or moves.\n\nThe pad has large friction coefficient both with the base surface and with the item laid on it, which prevent both the sticky pad from moving with respect to the surface, and objects laid on the pad from moving relative to the pad. Sticky pads are commonly used on car dashboards where forces caused by acceleration of the vehicle would cause objects put on dashboards slip off the otherwise smooth surface of the dashboard.\n\nContrary to fasteners, sticky pads do not affix objects to the surface. They merely prevent objects from slipping on the surface until the threshold acceleration or inclination angle is exceeded. Sticky pads also usually don't make use of adhesives. Because of this they are easily detached form the surface, and they need gravity to serve their purpose. In particular, the force acting on the object must have a component perpendicular on the surface and directed towards it. This is different from Microsuction tape where adhesion of object is achieved by microscopic bubbles on the surface that function as small suction cups. Sticky pads are made of rubber-like materials. This help dissipate kinetic energy when the base surface vibrates, such that object on the pad keep maintaining large enough contact surface with the pad and tangential friction forces keep preventing objects from slipping relative to the pad.\n\nAlthough basic principles of sticky pads are simple, physics behind may be complex due to many specific and sometimes conflicting requirements arising from practical use. Mechanisms involved in high friction materials go beyond simple Coulomb friction.\nThese can be combined by other mechanisms such as energy dissipation in viscous materials or adhesion.\n\nThe above requirements impose many design challenges. To operate well on vibrating surfaces, pads are usually made of soft, rubber like materials with very high friction coefficients. Designs seek to achieve certain level of adhesion (e.g. for use on vertical or very steep surfaces) without compromising easy detachment and continuous use without residual left-over. Some applications (such as sticking smartphones or tablets to vertical surface) require high degree of reliability, which is difficult to achieve without strong sticking to the surfaces.\n\nVarious innovative approaches and engineered materials are used to keep in line with requirements. Some designs apply sticking based on vacuum in addition to high friction and softness (see e.g. micro-suction tapes). \n\nOther development includes designs that find inspiration in nature, especially in animals that are able to climb walls and ceilings such as geckos, \n\nvarious species of insects\n\n, tree frogs\n\n, or chameleons\n\nMechanisms of insects that can scale walls and ceilings help understand how to produce surfaces with extremely high friction that don't exhibit too much sticking for practical applications. Abilities of geckos have been intensively studied to find out how sticking of vertical walls or ceilings can be joined with ability of easy and quick detachment that enables geckos quick movement. It has been discovered that Van der Waals force rather than friction or adhesion is the most important mechanism behind gecko's abilities. This implies that artificial designs mimicking geckos' feet should rely on maximizing surface contact between object and the pad, which is less practical in some situations, for example when pads are used on non-flat surfaces or when objects put on the pad don't have flat surfaces. On the other hand, mechanisms in geckos' feet help design materials with reliable sticking and easy detachment at the same time. Mechanisms used in geckos, tree frogs and some insects were also studied for self-cleaning ability, which would enable artificial materials retain the ability to prevent sliding after continuous use in dirty environments.\n\n"}
{"id": "52795867", "url": "https://en.wikipedia.org/wiki?curid=52795867", "title": "The Pee Pocket", "text": "The Pee Pocket\n\nThe Pee Pocket is a single-use urinary device that allows a person to stand while urinating, in situations where they could not normally do so. It was developed by a team of doctors and it is marketed primarily to women, athletes, travelers, the elderly, disabled, pregnancy, parents of young girls, and post-surgery patients. It can also be used to prevent the exchange of bodily fluids between people through contact with unsanitary surfaces. In 2014, when media reported an increase of Ebola cases around the world, the Pee Pocket was said to help prevent the disease.\nThe Pee Pocket has also become very popular in the LGBTQ community and is used in bathrooms world wide.\n\n"}
{"id": "15367982", "url": "https://en.wikipedia.org/wiki?curid=15367982", "title": "Three-dimensional integrated circuit", "text": "Three-dimensional integrated circuit\n\nIn microelectronics, a three-dimensional integrated circuit (3D IC) is an integrated circuit manufactured by stacking silicon wafers or dies and interconnecting them vertically using, for instance, through-silicon vias (TSVs) or Cu-Cu connections, so that they behave as a single device to achieve performance improvements at reduced power and smaller footprint than conventional two dimensional processes. 3D IC is just one of a host of 3D integration schemes that exploit the z-direction to achieve electrical performance benefits.\n\n3D integrated circuits can be classified by their level of interconnect hierarchy at the global (package), intermediate (bond pad) and local (transistor) level In general, 3D integration is a broad term that includes such technologies as 3D wafer-level packaging (3DWLP); 2.5D and 3D interposer-based integration; 3D stacked ICs (3D-SICs), monolithic 3D ICs; 3D heterogeneous integration; and 3D systems integration.\n\nInternational organizations such as the Jisso Technology Roadmap Committee (JIC) and the International Technology Roadmap for Semiconductors (ITRS) have worked to classify the various 3D integration technologies to further the establishment of standards and roadmaps of 3D integration.\n\n3D Packaging refers to 3D integration schemes that rely on traditional methods of interconnect such as wire bonding and flip chip to achieve vertical stacks. 3D packaging can be disseminated further into 3D system in package (3D SiP) and 3D wafer level package (3D WLP). Stacked memory die interconnected with wire bonds, and package on package (PoP) configurations interconnected with either wire bonds, or flip chips are 3D SiPs that have been in mainstream manufacturing for some time and have a well established infrastructure. PoP is used for vertically integrating disparate technologies such as 3D WLP uses wafer level processes such as redistribution layers (RDL) and wafer bumping processes to form interconnects.\n\n2.5D interposer is also a 3D WLP that interconnects die side-side on a silicon, glass or organic interposer using TSVs and RDL. In all types of 3D Packaging, chips in the package communicate using off-chip signaling, much as if they were mounted in separate packages on a normal circuit board.\n\n3D ICs can be divided into 3D Stacked ICs (3D SIC), which refers to stacking IC chips using TSV interconnects, and monolithic 3D ICs, which use fab processes to realize 3D interconnects at the local levels of the on-chip wiring hierarchy as set forth by the ITRS, this results in direct vertical interconnects between device layers. The first examples of a monolithic approach are seen in Samsung’s 3D VNAND devices.\n\nThe digital electronics market requires a higher density semiconductor memory chip to cater to recently released CPU components, and the multiple die stacking technique has been suggested as a solution to this problem. JEDEC disclosed the upcoming DRAM technology includes the \"3D SiC\" die stacking plan at \"Server Memory Forum\", November 1–2, 2011, Santa Clara, CA. In August 2014, Samsung started producing 64GB DRAM modules for servers based on emerging DDR4 (double-data rate 4) memory using 3D TSV package technology. Newer proposed standards for 3D stacked DRAM include Wide I/O, Wide I/O 2, Hybrid Memory Cube, High Bandwidth Memory.\n\nMonolithic 3D ICs are built in layers on a single semiconductor wafer, which is then diced into 3D ICs. There is only one substrate, hence no need for aligning, thinning, bonding, or through-silicon vias. Process temperature limitations are addressed by partitioning the transistor fabrication to two phases. A high temperature phase which is done before layer transfer follow by a layer transfer use ion-cut, also known as layer transfer, which has been used to produce Silicon on Insulator (SOI) wafers for the past two decades. Multiple thin (10s–100s nanometer scale) layers of virtually defect-free Silicon can be created by utilizing low temperature (<400℃) bond and cleave techniques, and placed on top of active transistor circuitry. Follow by finalizing the transistors using etch and deposition processes. This monolithic 3D IC technology has been researched at Stanford University under a DARPA-sponsored grant.\n\nCEA-Leti is also developing monolithic 3D IC approaches, called sequential 3D IC. In 2014, the French research institute introduced its CoolCube™, a low-temperature process flow that provides a true path to 3DVLSI. At Stanford University, researchers are designing monolithic 3D ICs using carbon nanotube (CNT) structures vs. silicon using a wafer-scale low temperature CNT transfer processes that can be done at 120℃.\n\nIn general, monolithic 3D ICs are still a developing technology and are considered by most to be several years away from production.\n\nAs of 2014, a number of memory products such as High Bandwidth Memory (HBM) and the Hybrid Memory Cube have been launched that implement 3D IC stacking with TSVs. There are a number of key stacking approaches being implemented and explored. These include die-to-die, die-to-wafer, and wafer-to-wafer.\n\n\nWhile traditional CMOS scaling processes improves signal propagation speed, scaling from current manufacturing and chip-design technologies is becoming more difficult and costly, in part because of power-density constraints, and in part because interconnects do not become faster while transistors do. 3D ICs address the scaling challenge by stacking 2D dies and connecting them in the 3rd dimension. This promises to speed up communication between layered chips, compared to planar layout. 3D ICs promise many significant benefits, including:\n\n\nBecause this technology is new it carries new challenges, including:\n\nDepending on partitioning granularity, different design styles can be distinguished. Gate-level integration faces multiple challenges and currently appears less practical than block-level integration.\n\n\nIn 2004 Tezzaron Semiconductor built working 3D devices from six different designs. The chips were built in two layers with \"via-first\" tungsten TSVs for vertical interconnection. Two wafers were stacked face-to-face and bonded with a copper process. The top wafer was thinned and the two-wafer stack was then diced into chips. The first chip tested was a simple memory register, but the most notable of the set was an 8051 processor/memory stack that exhibited much higher speed and lower power consumption than an analogous 2D assembly.\n\nIn 2004, Intel presented a 3D version of the Pentium 4 CPU. The chip was manufactured with two dies using face-to-face stacking, which allowed a dense via structure. Backside TSVs are used for I/O and power supply. For the 3D floorplan, designers manually arranged functional blocks in each die aiming for power reduction and performance improvement. Splitting large and high-power blocks and careful rearrangement allowed to limit thermal hotspots. The 3D design provides 15% performance improvement (due to eliminated pipeline stages) and 15% power saving (due to eliminated repeaters and reduced wiring) compared to the 2D Pentium 4.\n\nThe Teraflops Research Chip introduced in 2007 by Intel is an experimental 80-core design with stacked memory. Due to the high demand for memory bandwidth, a traditional I/O approach would consume 10 to 25 W. To improve upon that, Intel designers implemented a TSV-based memory bus. Each core is connected to one memory tile in the SRAM die with a link that provides 12 GB/s bandwidth, resulting in a total bandwidth of 1 TB/s while consuming only 2.2 W.\n\nAn academic implementation of a 3D processor was presented in 2008 at the University of Rochester by Professor Eby Friedman and his students. The chip runs at a 1.4 GHz and it was designed for optimized vertical processing between the stacked chips which gives the 3D processor abilities that the traditional one layered chip could not reach. One challenge in manufacturing of the three-dimensional chip was to make all of the layers work in harmony without any obstacles that would interfere with a piece of information traveling from one layer to another.\n\nIn ISSCC 2012, two 3D-IC-based multi-core designs using GlobalFoundries' 130 nm process and Tezzaron's FaStack technology were presented and demonstrated. 3D-MAPS, a 64 custom core implementation with two-logic-die stack was demonstrated by researchers from the School of Electrical and Computer Engineering at Georgia Institute of Technology. The second prototype was from the Department of Electrical Engineering and Computer Science at University of Michigan called Centip3De, a near-threshold design based on ARM Cortex-M3 cores.\n\n\n"}
{"id": "12550749", "url": "https://en.wikipedia.org/wiki?curid=12550749", "title": "Tung-Sol", "text": "Tung-Sol\n\nTung-Sol was an American manufacturer of electronics, mainly lamps and vacuum tubes.\n\nTung-Sol was founded as Tung-Sol Lamp Works Inc. in Newark, New Jersey in 1907. Their early products were mainly geared towards the automotive market and included headlamps, pilot lights, and flashlight bulbs.\n\nThe trade name was formed from the first syllable of \"tungsten\" and the Latin word \"sol\" meaning sun.\n\nThe company entered the electronics field in the 1920s. In time they established themselves as leaders in the development and production of vacuum tubes, with their main competition including RCA and Sylvania. By 1951, they began doing business as Tung-Sol Electric Inc. Soon after, they were acquired by Wagner Electric, which itself merged into Studebaker-Worthington in 1967. Tung-Sol was also active in the semiconductor industry, with its transistors being easily recognizable by their sky blue color.\n\nTung-Sol was privately held and run like a laboratory. This gave Tung-Sol vacuum tubes a reputation of having some of the best metallurgy and chemistry that has ever been pulled off in actual production. The Tung-Sol brand name is now owned by the New Sensor Corporation, the same company that owns the brands Sovtek and Electro-Harmonix.\n\nTung-Sol developed the first successful car headlight in 1907, followed in 1913 by a single bulb two filament high and low beam headlight. Other Tung-Sol inventions included the flashing turn signal. Tung-Sol created the 6550 vacuum tube, a specialized Hi-Fi audio tube in 1955.\n\nThe 6550 is a high power beam tetrode vacuum tube used mainly as an audio power amplifier.\n\nThe 6550 was developed by Tung-Sol in 1955 originally for use as a servo amplifier. It was used in audio equipment before semiconductor power amplifiers came into use, and continues in production today. It is often found in valve-driven bass amplifiers such as Marshall and Ampeg, guitar amplifiers and the Leslie speaker system.\n\nBased on the 6L6 vacuum tube, the 6550 was designed to have higher output power and better stability. It operates with a plate voltage of 600VDC, a screen voltage of 400VDC, and plate dissipation of 35 watts. The KT88, KT90, and 6550, although not identical, are often interchangeable, dependent on external circuit parameters. The 6550's glass envelope was originally wider in the middle than at the top and bottom, but a straight-sided design was later introduced by GE and Philips.\n"}
{"id": "17065113", "url": "https://en.wikipedia.org/wiki?curid=17065113", "title": "US Climate Reference Network", "text": "US Climate Reference Network\n\nThe US Climate Reference Network (USCRN) is a network of climate stations developed and maintained by the National Oceanic and Atmospheric Administration (NOAA), completed in 2008.. It has the long-term commitment of the Department of Commerce and the NOAA.\n\nThe USCRN is made up of over 143 stations in the United States. Its purpose is to maintain a sustainable high quality network which will detect, with high confidence, signals of climate change in the US. It provides the United States with a reference network that meets the requirements of the Global Climate Observing System.\n\nThe primary goal of the USCRN is to provide future long-term high-quality observations of surface air temperature and precipitation that can be coupled to past long-term observations for the detection and attribution of present and future climate change. It records data with minimal time dependent biases affecting the interpretation of decadal to centennial climate variability and change.\n\nThis program is used to collect and analyze the high-quality data on climate change. The hope is that research based on this data is thrusted to impact near and long term policy and decision plans made by senior government and business leaders.\n\nIt is being implemented and managed by the National Climatic Data Center (NCDC)\nlocated in Asheville, NC. Scientists and engineers from the Atmospheric Turbulence and Diffusion Division located in Oak Ridge, TN, are assisting the NCDC USCRN program staff. System design, test, implementation, and associated expenses are being provided by the National Environmental Satellite, Data, and Information Service's Office of Systems Development.\n\nEach station is positioned in a pristine site which is expected to remain free from development over coming decades. Each station may include the following sensors: triple redundant air temperature sensors, precipitation sensors, wind speed sensors, and ground temperature sensors. Stations have been placed in rural environments in order to avoid possible urban micro climate interference. In 2008, the NOAA announced that it expected to have the 114 contiguous US sites online that summer.\n\nEssential components of the USCRN are well-documented life cycle maintenance, modernization, and performance histories, as well as a robust science and research component. There are routine maintenance visits to the sites and regular calibration of the sensors. Research effort has focused on continual evaluation of the data, new sensors, and emerging calibration techniques. When a new type of sensor can contribute to improving the quality of the observations, there will be at least a one-year continuity overlap of current and new sensors.\n\nEvery USCRN instrument site is being equipped with the following:\n\nData from these USCRN sites are used to provide information on long-term changes in air temperature and precipitation, including means and extremes. Additional sensors may be added in the future, such as soil moisture and soil temperature. USCRN data is intended to be used in operational climate monitoring activities and for placing current climate anomalies into a historical perspective. Data is transmitted hourly via the Geostationary Operational Environmental Satellite Data Collection System and is immediately distributed by the National Weather Service to their operational sites. Observations are accessible through the Internet.\n\nThe USCRN hopes to provide the nation with a long-term (50 to 100 years) observation network that will serve as the Nation's Benchmark Climate Reference Network. When fully implemented, the network will consist of several hundred instrument suites strategically selected to capture climate trends, variations, and change across the nation.\n\n"}
{"id": "9843747", "url": "https://en.wikipedia.org/wiki?curid=9843747", "title": "United States Naval Computing Machine Laboratory", "text": "United States Naval Computing Machine Laboratory\n\nThe United States Naval Computing Machine Laboratory (NCML) was a highly secret design and manufacturing site for code-breaking machinery located in Building 26 of the National Cash Register (NCR) company in Dayton, Ohio and operated by the United States Navy during World War II. It is now on the List of IEEE Milestones, and one of its machines is on display at the National Cryptologic Museum.\n\nThe laboratory was established in 1942 by the Navy and National Cash Register Company to design and manufacture a series of code-breaking machines (\"bombes\") targeting German Enigma machines, based on earlier work by the British at Bletchley Park (which in turn owed something to pre-war Polish cryptanalytical work). Joseph Desch led the effort. Preliminary designs, approved in September 1942, called for a fully electronic machine to be delivered by year's end. However, these plans were soon judged infeasible, and revised plans were approved in January 1943 for an electromechanical machine, which became the US Navy bombe.\n\nThese designs were proceeding in parallel with, and influenced by, British attempts to build a high-speed bombe for the German 4-rotor Enigma. Indeed, Alan Turing visited Dayton in December 1942. His reaction was far from enthusiastic:\n\nThe American approach was, however, successful. The first two experimental bombes went into operation in May 1943, running in Dayton so they could be observed by their engineers. Designs for production models were completed in April, 1943, with initial operation starting in early June.\n\nAll told, the laboratory constructed 121 bombes which were then employed for code-breaking in the US Navy's signals intelligence and cryptanalysis group OP-20-G in Washington, D.C.. Construction was accomplished in three shifts per day by some 600 WAVES (Women Accepted for Volunteer Emergency Service), 100 Navy officers and enlisted men, and a large civilian workforce. Approximately 3,000 workers operated the bombes to produce \"Ultra\" decryptions of German Enigma traffic.\n\nAccording to a contemporary US Navy report (dated April 1944), the bombes were used on naval jobs until all daily keys had been run; then the machines were used for non-naval tasks. During the previous six months, about 45% of the bombe time had been devoted to non-naval problems carried out at the request of the British. British production and reliability problems with their own high-speed bombes had then recently led to construction of 50 additional Navy units for Army and Air Force keys.\n\nThe building in Dayton, called Building 26 (Dayton) was an Art Deco design of Dayton firm Schenck & Williams and was located at Patterson Blvd and Stewart Street. The building was demolished by the University of Dayton in January 2008.\n\n"}
{"id": "45167593", "url": "https://en.wikipedia.org/wiki?curid=45167593", "title": "Vessyl", "text": "Vessyl\n\nVessyl is an intelligent drinking glass announced in June 2014 by Mark One, but never released. The cup was to have embedded sensors and the capability of linking to a smartphone to provide its user with nutritional and other data on the beverage in the cup. The designers hoped to help users make better decisions about their health and overall consumption. The cup was expected to recharge on a proprietary base station, included in the product packaging. Industrial designer Yves Béhar and his design firm Fuseproject were involved in its creation.\n\nOriginally, the device was expected to ship in \"Early 2015\". However, in March 2015, the manufacturer announced that the shipping date would slip to \"Q4 2017\". once it has reached a satisfactory level of performance. , Mark One, the company behind Vessyl, is no longer operating.\n\nWith the announcement of a production delay to ensure the quality of the original Vessyl, the designers introduced a secondary product, Pryme Vessyl, released to the public in November 2015 and available to early supporters by means of a free opt-in program, orders which were in many cases not fulfilled.\n"}
{"id": "7399275", "url": "https://en.wikipedia.org/wiki?curid=7399275", "title": "Wet oxidation", "text": "Wet oxidation\n\nWet oxidation is a form of hydrothermal treatment. It is the oxidation of dissolved or suspended components in water using oxygen as the oxidizer. It is referred to as \"Wet Air Oxidation\" (WAO) when air is used. The oxidation reactions occur in superheated water at a temperature above the normal boiling point of water (100 °C), but below the critical point (374 °C).\n\nThe system must be maintained under pressure to avoid excessive evaporation of water. This is done to control energy consumption due to the latent heat of vaporization. It is also done because liquid water is necessary for most of the oxidation reactions to occur. Compounds oxidize under wet oxidation conditions that would not oxidize under dry conditions at the same temperature and pressure.\n\nWet oxidation has been used commercially for around 60 years. It is used predominantly for treating wastewater. It is often referred to as Zimpro (from ZIMmerman PROcess), after Fred T. Zimmermann who commercialized it in the mid 20th century.\n\nCommercial systems typically use a bubble column reactor, where air is bubbled through a vertical column that is liquid full of the hot and pressurized wastewater. Fresh wastewater enters the bottom of the column and oxidized wastewater exits the top. The heat released during the oxidation is used to maintain the operating temperature.\n\nWAO is a liquid phase reaction using dissolved oxygen in water to oxidize wastewater contaminants. The dissolved oxygen is typically supplied using pressurized air, but pure oxygen can also be used. The oxidation reaction generally occurs at moderate temperatures of 150°-320 °C and at pressures from 10 to 220 barg. The process converts organic contaminants to carbon dioxide, water, and biodegradable short chain organic acids. Inorganic constituents such as sulfides and cyanides are converted to non-reactive inorganic compounds.\n\nIn the WAO reaction, complex organic molecules, including biological refractory compounds, are broken into simpler organic compounds or to a complete mineralized state (CO, NH, Cl, SO, PO) Simple organic compounds such as low molecular weight carboxylic acids and mineralized reaction products may be present in the WAO effluents. Because of this, the WAO effluent generally requires post treatment prior to discharge. WAO effluents are typically readily biodegradable and exhibit high values for BOD:COD ratios. Standard treatment techniques such as activated sludge biotreatment are typically used with WAO for complete treatment.\n\nCatalyst can be used in the WAO system to enhance treatment and achieve a higher COD destruction. Heterogeneous and homogenous catalysts have been used. Heterogeneous catalysts are based on precious metals deposited on a stable substrate. Homogenous catalysts are dissolved transition metals. Several processes, such as Ciba-Geigy, LOPROX, and ATHOS utilize a homogenous catalyst. Mixed metal catalysts, such a Ce/Mn, Co/Ce, Ag/Ce, have also been effective in improving the treatment achieved in a WAO system.\n\nA special type of wet oxidation process was the so-called \"VerTech process\" system. A system of this type operated in Apeldoorn, Netherlands between 1994 and 2004. The system was installed in a below-ground pressure vessel (also called a gravity pressure vessel or GPV). The pressure was supplied by feeding the material to a reactor with a depth of . The deep shaft reactor also served as a heat exchanger, so no pre-heating was required. The operating temperature was about 270 °C with a pressure of about . The installation was eventually shut down due to operational problems.\n\nThe majority of commercial wet oxidation systems are used to treat industrial wastewater, such as sulfide laden spent caustic streams from ethylene and LPG production as well as naphthenic and cresylic spent caustics from refinery applications.\nTypical classification of WAO treatment systems.\n\nLow temperature WAO systems oxidize sulfides to thiosulfate and sulfate but high concentrations of thiosulfate are present in the treated effluent. The mid temperature systems fully oxidize sulfides to sulfate and mercaptans are oxidized to sulfonic acids. For sulfidic spent caustics, this results in a high chemical oxygen demand (COD) destruction (>90%). High temperature systems are used to oxidize organic compounds that are present in naphthenic and cresylic spent caustics.\n\nAlmost as many systems are also used for treating biosolids, in order to pasteurize and to decrease volume of material for disposal. The thermal conditioning occurs at temperatures of 210 – 240 °C. A 4% dry solid slurry can be processed in a WAO system where it is disinfected and the treated effluent can be dewatered to 55% dry solids using a filter press.\n\nWet air oxidation has also been used to treat a variety of other industrial process waters and wastewaters which include:\n\n· Hazardous Waste\n\n· Kinetic Hydrate Inhibitors (KHI) from produced water\n\n· Polyol ether/styrene monomer (POSM) wastewater\n\n· Ammonium sulfate crystallizer mother liquor\n\n· Pharmaceutical wastewater\n\n· Cyanide Wastewater\n\n· Powdered Activated Carbon regeneration\n\n\n"}
