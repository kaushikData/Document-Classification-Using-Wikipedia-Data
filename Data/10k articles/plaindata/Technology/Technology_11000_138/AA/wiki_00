{"id": "1722943", "url": "https://en.wikipedia.org/wiki?curid=1722943", "title": "A-frame", "text": "A-frame\n\nAn A-frame is a basic structure designed to bear a load in a lightweight economical manner. The simplest form of an A-frame is two similarly sized beams, arranged in an angle of 45 degrees or less, attached at the top. These materials are often wooden or steel beams attached at the top by rope, welding, gluing, or riveting. \nA-frames can be used as-is, as part of shears, or set up in a row along a longitudinal beam for added stability, as in a saw horse. More complex structures will often have crossmembers connecting the A-frames at different angles, forming a truss.\n\n\n \n"}
{"id": "17827176", "url": "https://en.wikipedia.org/wiki?curid=17827176", "title": "AdsML", "text": "AdsML\n\nAdsML is a suite of business-to-business electronic commerce standards intended to support the exchange of advertising business messages and content delivery using XML. It is supported by Peter Meirs of Time Inc..\n\nTypical users include newspapers, advertising agencies, broadcasters and others who buy or sell advertising.\n\n"}
{"id": "21036560", "url": "https://en.wikipedia.org/wiki?curid=21036560", "title": "Ammonium permanganate", "text": "Ammonium permanganate\n\nAmmonium permanganate is the chemical compound NHMnO, or NH·HMnO. It is soluble in water. It is a strong oxidizer, owing to its permanganate anion, and it is a moderately strong explosive, owing to the combination of oxidizer permanganate anion and reducing ammonium cation. Dry ammonium permanganate can detonate by heat, shock, or friction, and it may explode at temperatures above 140 °F (60 °C).\n\nAmmonium permanganate decomposes explosively to manganese dioxide, nitrogen, and water:\n\nAmmonium permanganate was first prepared by Eilhard Mitscherlich in 1824 by reaction of silver permanganate with equal molar amount of ammonium chloride, filtering the silver chloride and evaporating the water. It can be also prepared in a similar way from barium permanganate and ammonium sulfate.\n\nAmmonium permanganate decomposes slowly in storage even at normal temperatures. A sample stored for 3 months was only 96% pure, after 6 months it assumed color of iodine and had strong smell of nitrogen oxides. It emits toxic fumes when decomposed by heat.\n\nQuaternary ammonium permanganate compounds can be prepared, such as tetrabutylammonium permanganate.\n"}
{"id": "3688803", "url": "https://en.wikipedia.org/wiki?curid=3688803", "title": "Atropos scheduler", "text": "Atropos scheduler\n\nIn computer science, Atropos is a real-time scheduling algorithm developed at Cambridge University. It combines the earliest deadline first algorithm with a best effort scheduler to make use of slack time, while exercising strict admission control.\n\n"}
{"id": "12311837", "url": "https://en.wikipedia.org/wiki?curid=12311837", "title": "Bergeron diagram", "text": "Bergeron diagram\n\nThe Bergeron diagram method is a method to evaluate the effect of a reflection on an electrical signal. This graphic method—based on the real characteristic of the line—is valid for both linear and non-linear models and helps to calculate the delay of an electromagnetic signal on an electric transmission line.\n\nUsing the Bergeron method, on the I-V characteristic chart, start from the regime point before the transition, then move along a straight line with a slope of \"Z\" (\"Z\" is the line's characteristic impedance) to the new characteristic; then move along lines with −\"Z\" or +\"Z\" slope until the new regime situation is reached.\n\nThe − value is considered always the same at every reflection because the Bergeron method is used only for first reflections.\n\nThe method was originally developed by a French hydraulic engineer, L. J. B. Bergeron, for analysing water hammer effects in hydraulic systems.\n\n\n"}
{"id": "30863334", "url": "https://en.wikipedia.org/wiki?curid=30863334", "title": "Blue State Digital", "text": "Blue State Digital\n\nBlue State Digital is a digital strategy and technology firm that specializes in online fundraising, advocacy, social networking, and constituency development. It was founded by former staffers of Howard Dean's 2004 presidential campaign and provided digital strategy and technology services for the 2008 and 2012 Barack Obama presidential campaigns. The company is led by the one remaining co-founder, Joe Rospars (CEO). Co-founder Jascha Franklin-Hodge (former CTO) left in July 2014 to become the Chief Information Officer for the City of Boston.\n\nBlue State Digital has offices in New York City, Washington DC, Boston, Los Angeles, San Francisco, and London. In December 2010, Blue State Digital was acquired by WPP.\n\nHoward Dean's 2004 presidential campaign pioneered new applications of new media to engage voters and raise campaign funds. In 2004, four former Dean staffers—Jascha Franklin-Hodge, Clay Johnson, Joe Rospars, and Ben Self—founded Blue State Digital to improve technology solutions for political campaigns and provide online strategic services to complement the technology platform. The company launched with offices in Washington, DC, and Boston.\n\nThe following year, Thomas Gensemer joined Blue State Digital from America Coming Together, a Democratic-allied advocacy organization, to serve as managing partner.\n\nBlue State Digital's earliest clients included Ted Kennedy's Senate campaign, the Communications Workers of America, the Democratic National Committee (chaired by Howard Dean), Harry Reid, and AT&T.\n\nIn 2007, Barack Obama's nascent presidential campaign recruited Blue State Digital to lead its digital efforts. Blue State Digital provided technology services—including web hosting, online fundraising tools, and a custom social networking platform—while Rospars built and led the campaign's internal new media strategy team. Over the course of the campaign, this team raised more than $500 million, mobilized millions of volunteers, and built an online community of 13 million supporters.\n\nBlue State Digital grew rapidly after the Obama campaign, opening offices in New York, Los Angeles, and London. It also grew its client base, working with clients like the American Red Cross, United Way, Carnegie Hall, and Vogue Magazine.\n\nOn December 30, 2010, Blue State Digital announced that they had been wholly acquired by WPP Digital.\n\nBlue State Digital provides both strategic and technology services. Blue State Digital is also a full-service digital agency that develops and executes multiplatform engagement campaigns. Its strategic offerings include strategy, design, production, content strategy, digital communications, video, analytics, and online advertising.\n\nThe BSD Tools are an online platform that integrates constituent databases, email management, online fundraising, content management, social networking, and web analytics.\n\nAn employee of Blue State Digital, Philip de Vellis, admitted to having created a widely circulated video, \"Hillary 1984,\" that edited clips from the Hillary Clinton campaign into the famous 1984 television advertisement by Apple Computer. In a blog entry posted on \"The Huffington Post\", de Vellis indicated that he had resigned from Blue State Digital after making and publishing the video. The company later released a statement claiming that they had terminated his employment. De Vellis stated in an interview with the Associated Press that he had created the video from home and that Blue State Digital had nothing to do with its creation.\n\nThe Obama campaign released a statement stating that it had no knowledge of and had nothing to do with the creation of the ad. De Vellis did claim that he helped design the Obama website.\n\nDe Vellis lived with Senator Obama's press secretary in 2006 while they were both working on Sherrod Brown's campaign for Senate.\n\n\n"}
{"id": "28333129", "url": "https://en.wikipedia.org/wiki?curid=28333129", "title": "Changhong", "text": "Changhong\n\nSichuan Changhong Electric Co., Ltd. (Changhong) is a Chinese consumer electronics company based in Mianyang, Sichuan, and founded on October 1958 in the same location. It is the second-largest manufacturer of televisions in China. In 2004, 90% of the television sets exported from China to the United States were made by Changhong.\n\nChanghong emerged from the Changhong Machinery Factory, which was a state-owned large enterprise established in the 1950s. The company, which was part of the 156 key projects that were aided by the Soviet Union, focused on the development and production of airborne fire control radar system. By mid-1970s, Changhong began manufacturing products for civilian use when demand for military hardware declined, eventually focusing on the television product line. During the next decade, it beefed up its technological capabilities with a series of partnerships with overseas companies such as the Japanese company National, where it imported tubes and advanced production lines to drive the volume production of television. In 1980, the company already boasted the production of over 10,000 television units annually and by 1988, this number rose to almost a million units. In 1994, the company was listed as a publicly traded company and, a year later, it was recognized as China's largest television manufacturer.\n\nChanghong has a minimal presence in North America, where it sells TVs through the online retailer Newegg.\n\nAnother line of products is the manufacturing of nickel–iron batteries. Changhong is the sole manufacturer of batteries for the Chengdu J-10.\n\nIn 2008, an offer was reportedly made by AMD for a 15% stake but was blocked by the Chinese government.\n\nPresidents of Changhong: MA Zhang, 1957–61; SHI Fu, 1962-4; WANG Zhidong, 1966–74; KANG Naide, 1975–80; HU Zhengxing, 1981-2; WANG Jincheng, 1983-4; NI Runfeng, 1985-2004; ZHAO Yong, 2004-current.\n\n"}
{"id": "2503230", "url": "https://en.wikipedia.org/wiki?curid=2503230", "title": "Chronomics", "text": "Chronomics\n\nChronomics is a privately held personalised epigenetic testing and biotechnology company based in Cambridge, UK. The company tracks changes in health and wellbeing by via epigenetic information from the DNA extracted from saliva samples. Since February 2018 Chronomics has been providing online, consumer epigenetic health services with environmental and lifestyle-related health components.\n\nThe company was founded in 2017 by Cambridge University PhD researchers; Tom Stubbs, Daniel Herranz, Toby Call and Charles Ball who previously worked at the Babraham Institute and the European Bioinformatics Institute.\n\nWhere the customer grants permission, aggregate customer data is studied by scientific researchers at Chronomics for research purposes including the development of new and improved epigenetic indicators. Chronomics has made provisions to provide services related to specific medical research initiatives based on customer interest.\n"}
{"id": "195520", "url": "https://en.wikipedia.org/wiki?curid=195520", "title": "Civil engineer", "text": "Civil engineer\n\nA civil engineer is a person who practices civil engineering – the application of planning, designing, constructing, maintaining, and operating infrastructures while protecting the public and environmental health, as well as improving existing infrastructures that have been neglected.\n\nCivil engineering is one of the oldest engineering disciplines because it deals with constructed environment including planning, designing, and overseeing construction and maintenance of building structures, and facilities, such as roads, railroads, airports, bridges, harbors, channels, dams, irrigation projects, pipelines, power plants, and water and sewage systems.\n\nThe term \"civil engineer\" was established by John Smeaton in 1750 to contrast engineers working on civil projects with the military engineers, who worked on armaments and defenses. Over time, various sub-disciplines of civil engineering have become recognized and much of military engineering has been absorbed by civil engineering. Other engineering practices became recognized as independent engineering disciplines, including chemical engineering, mechanical engineering, and electrical engineering.\n\nIn some places, a civil engineer may perform land surveying; in others, surveying is limited to construction surveying, unless an additional qualification is obtained.\n\nCivil engineers usually practice in a particular specialty, such as construction engineering, geotechnical engineering, structural engineering, land development, transportation engineering, hydraulic engineering, and environmental engineering. Some civil engineers, particularly those working for government agencies, may practice across multiple specializations, particularly when involved in critical infrastructure development or maintenance.\n\nCivil engineers generally work in a variety of locations and conditions. Many spend time outdoors at construction sites so that they can monitor operations or solve problems onsite. The job is typically a blend of in-office and on-location work. Most work full-time.\n\nIn most countries, a civil engineer will have graduated from a post-secondary school with a degree in civil engineering, which requires a strong background in mathematics and the physical sciences; this degree is typically a bachelor's degree, though many civil engineers study further to obtain master's, engineer, doctoral and post doctoral degrees. In many countries, civil engineers are subject to licensure. In some jurisdictions with mandatory licensing, people who do not obtain a license may not call themselves \"civil engineers\".\n\nIn Belgium, \"Civil Engineer\" (abbreviated Ir.) (, ) is a legally protected title applicable to graduates of the five-year engineering course of one of the six universities and the Royal Military Academy. Their speciality can be all fields of engineering: civil, structural, electrical, mechanical, chemical, physics and even computer science. This use of the title may cause confusion to the English speaker as the Belgian \"civil\" engineer can have a speciality other than civil engineering. In fact, Belgians use the adjective \"civil\" in the sense of \"civilian\", as opposed to military engineers.\n\nThe formation of the civil engineer has a strong mathematical and scientific base and is more theoretical in approach than the practical oriented industrial engineer (Ing.) educated in a five-year program at a polytechnic. Traditionally, students were required to pass an entrance exam on mathematics to start civil engineering studies. This exam was abolished in 2004 for the Flemish Community, but is still organised in the French Community.\n\nIn Scandinavian countries, civil engineer (civilingenjör (Swedish), sivilingeniør (Norwegian), civilingeniør (Danish)) is a first professional degree, approximately equivalent to Master of Science in Engineering, and a protected title granted to students by selected institutes of technology. As in English the word has its origin in the distinction between civilian and military engineers, as in before the start of the 19th century only military engineers existed and the prefix \"civil\" was a way to separate those who had studied engineering in a regular University from their military counterparts. Today the degree spans over all fields within engineering, like civil engineering, mechanical engineering, computer science, electronics engineering, etc.\n\nThere is generally a slight difference between a Master of Science in Engineering degree and the Scandinavian civil engineer degree, the latter's programme having closer ties with the industry's demands. A civil engineer is the most well-known of the two; still, the area of expertise remains obfuscated for most of the public. A noteworthy difference is the mandatory courses in mathematics and physics, regardless of the equivalent master's degree, e.g. computer science.\n\nAlthough a 'college engineer' (högskoleingenjör, diplomingenjör/mellaningenjör (Swedish), høgskoleingeniør (Norwegian), diplomingeniør (Danish)) is roughly equivalent to a Bachelor of Science in Scandinavia, to become a 'civil engineer' one often has had to do up to one extra year of overlapping studies compared to attaining a B.Sc./M.Sc. combination. This is because the higher educational system is not fully adopted to the international standard graduation system, since it is treated as a professional degree. Today (2009) this is starting to change due to the Bologna process.\n\nA Scandinavian \"civilingenjör\" will in international contexts commonly call himself \"Master of Science in Engineering\" and will occasionally wear an engineering class ring. At the Norwegian Institute of Technology (now the Norwegian University of Science and Technology), the tradition with an NTH Ring goes back to 1914, before the Canadian iron ring.\n\nIn Norway, the title \"Sivilingeniør\" is no longer issued after 2007, and has been replaced with \"Master i teknologi\". In the English translation of the diploma, the title will be \"Master of Science\", since \"Master of Technology\" is not an established title in the English-speaking world. The extra overlapping year of studies have also been abolished with this change to make Norwegian degrees more equal to their international counterparts.\n\nIn Spain, a civil engineering degree can be obtained after four years of study in the various branches of mathematics, physics, mechanics, etc. The earned degree is called \"Grado en Ingeniería Civil\". Further studies at a graduate school include master's and doctoral degrees.\n\nBefore the current situation, that is, before the implementation of Bologna Process in 2010, a degree in civil engineering in Spain could be obtained after three to six years of study and was divided into two main degrees. \nIn the first case, the earned degree was called \"Ingeniero Técnico de Obras Públicas\" (\"ITOP\"), literally translated as \"Public Works Engineer\" obtained after three years of study and equivalent to a Bachelor of Civil Engineering. \nIn the second case, the academic degree was called \"Ingeniero de Caminos, Canales y Puertos\" (often shortened to \"Ingeniero de Caminos\" or \"ICCP\"), that literally means \"Highways, Canals and Harbors Engineer\", though civil engineers in Spain practice in the same fields as civil engineers do elsewhere. This degree is equivalent to a Master of Civil Engineering and is obtained after five or six years of study depending on the school granting the title.\n\nThe first Spanish Civil Engineering School was the \"Escuela Especial de Ingenieros de Caminos y Canales\" (now called \"Escuela Técnica Superior de Ingenieros de Caminos, Canales y Puertos\"), established in 1802 in Madrid, followed by the \"Escuela Especial de Ayudantes de Obras Públicas\" (now called \"Escuela Universitaria de Ingeniería Técnica de Obras Públicas de la Universidad Politécnica de Madrid\"), founded in 1854 in Madrid. Both schools now belong to the Technical University of Madrid.\n\nIn Spain, a civil engineer has the technical and legal ability to design projects of any branch, so any Spanish civil engineer can oversee projects about structures, buildings (except residential structures which are reserved for architects), foundations, hydraulics, the environment, transportation, urbanism, etc.\nIn Spain, Mechanical and Electrical engineering tasks are included under the Industrial engineering degree.\n\nA chartered civil engineer (known as certified or professional engineer in other countries) is a member of the Institution of Civil Engineers, and has also passed membership exams. However a non-chartered civil engineer may be a member of the Institution of Civil Engineers or the Institution of Civil Engineering Surveyors. The description \"Civil Engineer\" is not restricted to members of any particular professional organisation although \"Chartered Civil Engineer\" is.\n\nIn many Eastern European countries, civil engineering does not exist as a distinct degree or profession but its various sub-professions are often studied in separate university faculties and performed as separate professions, whether they are taught in civilian universities or military engineering academies. Even many polytechnic tertiary schools give out separate degrees for each field of study. Typically study in geology, geodesy, structural engineering and urban engineering allows a person to obtain a degree in construction engineering. Mechanical engineering, automotive engineering, hydraulics and even sometimes metallurgy are fields in a degree in \"Machinery Engineering\". Computer sciences, control engineering and electrical engineering are fields in a degree in electrical engineering, while security, safety, environmental engineering, transportation, hydrology and meteorology are in a category of their own, typically each with their own degrees, either in separate university faculties or at polytechnic schools.\n\nIn the United States, civil engineers are typically employed by municipalities, construction firms, consulting engineering firms, architect/engineer firms, the military, state governments, and the federal government. Each state requires engineers who offer their services to the public to be licensed by the state. Licensure is obtained by meeting specified education, examination, and work experience requirements. Specific requirements vary by state.\n\nTypically licensed engineers must graduate from an ABET-accredited university or college engineering program with a minimum of bachelor's degree, pass the Fundamentals of Engineering exam, obtain several years of engineering experience under the supervision of a licensed engineer, then pass the Principles and Practice of Engineering Exam. After completing these steps and the granting of licensure by a state board, engineers may use the title \"Professional Engineer\" or PE in advertising and documents. Most states have implemented mandatory continuing education requirements to maintain a license.\n\nThe ASCE (American Society of Civil Engineers) represents more than 140,000 members of the civil engineering profession worldwide. Official members of the ASCE must hold a bachelor's degree from an accredited civil engineering program and be a licensed professional engineer or have five years responsible charge of engineering experience.\nMost civil engineers join this organization to be updated of current news, projects, and methods (such as sustainability) related to civil engineering; as well as contribute their expertise and knowledge to other civil engineers and students obtaining their civil engineering degree.\n\nThe ICE (Institution of Civil Engineers) founded in 1818, represents, as of 2008, more than 80,000 members of the civil engineering profession worldwide. Its commercial arm, Thomas Telford Ltd, provides training, recruitment, publishing and contract services.\n\nFounded in 1887, the CSCE (Canadian Society for Civil Engineering) represents members of the Canadian civil engineering profession. Official members of the CSCE must hold a bachelor's degree from an accredited civil engineering program. Most civil engineers join this organization to be updated of current news, projects, and methods (such as sustainability) related to civil engineering; as well as contribute their expertise and knowledge to other civil engineers and students obtaining their civil engineering degree. Local sections frequently host events such as seminars, tours, and courses.\n\n"}
{"id": "13139806", "url": "https://en.wikipedia.org/wiki?curid=13139806", "title": "Codian", "text": "Codian\n\nCodian is a former supplier of video conferencing products. The company became part of Tandberg in 2007, which became part of Cisco Systems in 2010.\n\nCodian was founded in 2002 and based in Langley, Slough, UK. Its main products included Multipoint Control Units and gateways. It was acquired by Tandberg in 2007 for US$270 million and its products continued to be sold under the \"TANDBERG Codian\" brand name. Tandberg was itself acquired by Cisco Systems in 2010 for US$3.3 billion. Codian products are still supported and developed by Cisco under the same brand name, and Codian's original technology was used by Cisco to develop its own line of TelePresence servers.\n"}
{"id": "7896257", "url": "https://en.wikipedia.org/wiki?curid=7896257", "title": "Controller–pilot data link communications", "text": "Controller–pilot data link communications\n\nController–pilot data link communications (CPDLC), also referred to as controller pilot data link (CPDL), is a method by which air traffic controllers can communicate with pilots over a datalink system.\n\nThe standard method of communication between an air traffic controller and a pilot is voice radio, using either VHF bands for line-of-sight communication or HF bands for long-distance communication (such as that provided by Shanwick Oceanic Control).\n\nOne of the major problems with voice radio communications used in this manner is that all pilots being handled by a particular controller are tuned to the same frequency. As the number of flights air traffic controllers must handle is steadily increasing (for instance, Shanwick handled 414,570 flights in 2007, an increase of 5% - or 22,000 flights - from 2006), the number of pilots tuned to a particular station also increases. This increases the chances that one pilot will accidentally override another, thus requiring the transmission to be repeated. In addition, each exchange between a controller and pilot requires a certain amount of time to complete; eventually, as the number of flights being controlled reaches a saturation point, the controller will not be able to handle any further aircraft.\n\nTraditionally, this problem has been countered by dividing a saturated air traffic control sector into two smaller sectors, each with its own controller and each using a different voice communications channel. However, this strategy suffers from two problems:\n\n\nIn some cases it may not be possible or feasible to further divide down a section.\n\nA new strategy is needed to cope with increased demands on air traffic control, and data link based communications offers a possible strategy by increasing the effective capacity of the communications channel.\n\nController–pilot data link communication (CPDLC) is a means of communication between controller and pilot, using data link for ATC communication. At the highest level, the concept is simple, with the emphasis on the continued involvement of the human at either end and the flexibility of use.\n\nThe CPDLC application provides air-ground data communication for the ATC service. This includes a set of clearance/information/request message elements which correspond to voice phraseology employed by air traffic control procedures. The controller is provided with the capability to issue level assignments, crossing constraints, lateral deviations, route changes and clearances, speed assignments, radio frequency assignments, and various requests for information. The pilot is provided with the capability to respond to messages, to request clearances and information, to report information, and to declare/rescind an emergency. The pilot is, in addition, provided with the capability to request conditional clearances (downstream) and information from a downstream air traffic service unit (ATSU). A “free text” capability is also provided to exchange information not conforming to defined formats. An auxiliary capability is provided to allow a ground system to use data link to forward a CPDLC message to another ground system.\n\nThe sequence of messages between the controller and a pilot relating to a particular transaction (for example request and receipt of a clearance) is termed a ‘dialogue’. There can be several sequences of messages in the dialogue, each of which is closed by means of appropriate messages, usually of acknowledgement or acceptance. Closure of the dialogue does not necessarily terminate the link, since there can be several dialogues between controller and pilot while an aircraft transits the ATSU airspace.\n\nAll exchanges of CPDLC messages between pilot and controller can be viewed as dialogues.\n\nThe CPDLC application has three primary functions:\n\n\nSimulations carried out at the Federal Aviation Administration's William J. Hughes Technical Center have shown that the use of CPDLC meant that \"the voice channel occupancy was decreased by 75 percent during realistic operations in busy en route airspace. The net result of this decrease in voice channel occupancy is increased flight safety and efficiency through more effective communications.\"\n\nToday, there are two main implementations of CPDLC:\n\n\nThe following UACs offer CPDLC services:\n\nFollowing the PETAL I and II (Preliminary Eurocontrol Trial Air Ground Data link) trials in 1995 including NEAN (VDL Mode 4), today both ATN (VDL Mode 2) and FANS 1/A services are supported.\n\nMore than 40 major airlines participate in the CPDLC programme with Maastricht UAC. Average end to end response times (ATC-cockpit-ATC) are well below 30 seconds. More than 30,000 LOG-ONs were reported in 2007, leading to over 82,000 CPDLC uplinks, each saving precious frequency time.\n\nATC clearance (ACL), aircraft communication messages (ACM), and check mike (AMC) services are supported, including the automatic uplink of the SSR transponder code into the cockpit.\n\nCPDLC will probably be a major enabler for following on projects as monitor message, route clearance uplink, 2-4 D trajectories, continuous descent approaches, and constraint coordination also.\n\nAll CPDLC deployments must be supported by an approved safety case demonstrating that all safety objectives for the applicable airspace have been met. EUROCAE ED-120 (RTCA DO-290) is the safety and performance requirements (SPR) for continental airspace and should be consulted for the safety objectives relevant to the use of CPDLC in continental airspace.\n\nED-120 provides a hazard analysis and identifies the hazards applicable to systems implementing the ATC services that CPDLC deployments are currently providing. It then derives the safety objectives for such systems and the safety requirements with which they must comply.\n\nImplementers of both ground and airborne systems must comply with these safety requirements if their products are to be approved and/or certified for operational use.\n\nSafety objectives identified by ED-120/DO-290 include the need to ensure that messages are neither corrupted nor mis-delivered. Equally important is the need for accurate timestamping and the rejection of out-of-date messages. A consequence of these requirements is that CPDLC implementations, both on aircraft and at ATC centres, must have access to an accurate clock (to within 1 second of UTC). For aircraft, this is typically provided by GPS.\n\n\n"}
{"id": "48565132", "url": "https://en.wikipedia.org/wiki?curid=48565132", "title": "Cooling load", "text": "Cooling load\n\nCooling load is the rate at which sensible and latent heat must be removed from the space to maintain a constant space dry-bulb air temperature and humidity. Sensible heat into the space causes its air temperature to rise while latent heat is associated with the rise of the moisture content in the space. The building design, internal equipment, occupants, and outdoor weather conditions may affect the cooling load in a building using different heat transfer mechanisms. The SI units are watts.\n\nThe cooling load is calculated to select HVAC equipment that has the appropriate cooling capacity to remove heat from the zone. A zone is typically defined as an area with similar heat gains, similar temperature and humidity control requirements, or an enclosed space within a building with the purpose to monitor and control the zone's temperature and humidity with a single sensor e.g. thermostat. Cooling load calculation methodologies take into account heat transfer by conduction, convection, and radiation. Methodologies include heat balance, radiant time series, cooling load temperature difference, transfer function, and sol-air temperature. Methods calculate the cooling load in either steady state or dynamic conditions and some can be more involved than others. These methodologies and others can be found in ASHRAE handbooks, ISO Standard 11855, European Standard (EN) 15243, and EN 15255. ASHRAE recommends the heat balance method and radiant time series methods.\n\nThe cooling load of a building should not be confused with its heat gains. Heat gains refer to the rate at which heat is transferred into or generated inside a building. Just like cooling loads, heat gains can be separated into sensible and latent heat gains that can occur through conduction, convection, and radiation. Thermophysical properties of walls, floors, ceilings, and windows, lighting power density (LPD), plug load density, occupant density, and equipment efficiency play an important role in determining the magnitude of heat gains in a building. ASHRAE handbook of fundamentals refers to the following six modes of entry for heat gains:\n\n\nFurthermore, heat extraction rate is the rate at which heat is actually being removed from the space by the cooling equipment. Heat gains, heat extraction rate, and cooling loads values are often not equal due to thermal inertia effects. Heat is stored in the mass of the building and furnishings delaying the time at which it can become a heat gain and be extracted by the cooling equipment to maintain the desired indoor conditions. Another reason is that the inability of the cooling system to keep dry bulb temperature and humidity constant.\n\nIn air systems, convective heat gains are assumed to become a cooling load instantly. Radiative heat gains are absorbed by walls, floors, ceilings, and furnishings causing an increase in their temperature which will then transfer heat to the space's air by convection. Conductive heat gains are converted to convective and radiative heat gains. If the space's air temperature and humidity are kept constant then heat extraction rate and space cooling load are equal. The resulting cooling load through different air system types in the same built environment can be different.\n\nIn radiant systems, not all convective heat gains become a cooling load instantly because radiant system has limitations on how much heat can be removed from the zone through convection. Radiative heat gains are absorbed by active and non-active cooling surfaces. If absorbed by active surfaces then heat gains become an instant cooling load otherwise a temperature increase will occur in the non-active surface that will eventually cause heat transfer to the space by convection and radiation.\n"}
{"id": "8013", "url": "https://en.wikipedia.org/wiki?curid=8013", "title": "Data compression", "text": "Data compression\n\nIn signal processing, data compression, source coding, or bit-rate reduction involves encoding information using fewer bits than the original representation. Compression can be either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by removing unnecessary or less important information.\n\nThe process of reducing the size of a data file is often referred to as data compression. In the context of data transmission, it is called source coding; encoding done at the source of the data before it is stored or transmitted. Source coding should not be confused with channel coding, for error detection and correction or line coding, the means for mapping data onto a signal.\n\nCompression is useful because it reduces resources required to store and transmit data. Computational resources are consumed in the compression process and, usually, in the reversal of the process (decompression). Data compression is subject to a space–time complexity trade-off. For instance, a compression scheme for video may require expensive hardware for the video to be decompressed fast enough to be viewed as it is being decompressed, and the option to decompress the video in full before watching it may be inconvenient or require additional storage. The design of data compression schemes involves trade-offs among various factors, including the degree of compression, the amount of distortion introduced (when using lossy data compression), and the computational resources required to compress and decompress the data.\n\nLossless data compression algorithms usually exploit statistical redundancy to represent data without losing any information, so that the process is reversible. Lossless compression is possible because most real-world data exhibits statistical redundancy. For example, an image may have areas of color that do not change over several pixels; instead of coding \"red pixel, red pixel, ...\" the data may be encoded as \"279 red pixels\". This is a basic example of run-length encoding; there are many schemes to reduce file size by eliminating redundancy.\n\nThe Lempel–Ziv (LZ) compression methods are among the most popular algorithms for lossless storage. DEFLATE is a variation on LZ optimized for decompression speed and compression ratio, but compression can be slow. In the mid-1980s, following work by Terry Welch, the Lempel–Ziv–Welch (LZW) algorithm rapidly became the method of choice for most general-purpose compression systems. LZW is used in GIF images, programs such as PKZIP, and hardware devices such as modems. LZ methods use a table-based compression model where table entries are substituted for repeated strings of data. For most LZ methods, this table is generated dynamically from earlier data in the input. The table itself is often Huffman encoded. Grammar-based codes like this can compress highly repetitive input extremely effectively, for instance, a biological data collection of the same or closely related species, a huge versioned document collection, internet archival, etc. The basic task of grammar-based codes is constructing a context-free grammar deriving a single string. Other practical grammar compression algorithms include Sequitur and Re-Pair.\n\nThe strongest modern lossless compressors use probabilistic models, such as prediction by partial matching. The Burrows–Wheeler transform can also be viewed as an indirect form of statistical modelling. In a further refinement of the direct use of probabilistic modelling, statistical estimates can be coupled to an algorithm called arithmetic coding. Arithmetic coding is a more modern coding technique that uses the mathematical calculations of a finite-state machine to produce a string of encoded bits from a series of input data symbols. It can achieve superior compression compared to other techniques such as the better-known Huffman algorithm. It uses an internal memory state to avoid the need to perform a one-to-one mapping of individual input symbols to distinct representations that use an integer number of bits, and it clears out the internal memory only after encoding the entire string of data symbols. Arithmetic coding applies especially well to adaptive data compression tasks where the statistics vary and are context-dependent, as it can be easily coupled with an adaptive model of the probability distribution of the input data. An early example of the use of arithmetic coding was in an optional (but not widely used) feature of the JPEG image coding standard. It has since been applied in various other designs including H.263, H.264/MPEG-4 AVC and HEVC for video coding.\n\nLossy data compression is the converse of lossless data compression. In the late 1980s, digital images became more common, and standards for compressing them emerged. In the early 1990s, lossy compression methods began to be widely used. In these schemes, some loss of information is acceptable. Dropping nonessential detail from the data source can save storage space. Lossy data compression schemes are designed by research on how people perceive the data in question. For example, the human eye is more sensitive to subtle variations in luminance than it is to the variations in color. JPEG image compression works in part by rounding off nonessential bits of information. There is a corresponding trade-off between preserving information and reducing size. A number of popular compression formats exploit these perceptual differences, including those used in music files, images, and video.\n\nLossy image compression can be used in digital cameras, to increase storage capacities with minimal degradation of picture quality. Similarly, DVDs use the lossy MPEG-2 video coding format for video compression.\n\nIn lossy audio compression, methods of psychoacoustics are used to remove non-audible (or less audible) components of the audio signal. Compression of human speech is often performed with even more specialized techniques; speech coding, or voice coding, is sometimes distinguished as a separate discipline from \"audio compression\". Different audio and speech compression standards are listed under audio coding formats. \"Voice compression\" is used in internet telephony, for example, audio compression is used for CD ripping and is decoded by the audio players.\n\nThe theoretical background of compression is provided by information theory (which is closely related to algorithmic information theory) for lossless compression and rate–distortion theory for lossy compression. These areas of study were essentially created by Claude Shannon, who published fundamental papers on the topic in the late 1940s and early 1950s. Coding theory is also related to this. The idea of data compression is also deeply connected with statistical inference.\n\nThere is a close connection between machine learning and compression: a system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution) while an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for \"general intelligence.\"\n\nHowever a new, alternative view can show compression algorithms implicitly map strings into implicit feature space vectors, and compression-based similarity measures compute similarity within these feature spaces. For each compressor C(.) we define an associated vector space ℵ, such that C(.) maps an input string x, corresponds to the vector norm ||~x||. An exhaustive examination of the feature spaces underlying all compression algorithms is precluded by space; instead, feature vectors chooses to examine three representative lossless compression methods, LZW, LZ77, and PPM.\n\nData compression can be viewed as a special case of data differencing: Data differencing consists of producing a \"difference\" given a \"source\" and a \"target,\" with patching producing a \"target\" given a \"source\" and a \"difference,\" while data compression consists of producing a compressed file given a target, and decompression consists of producing a target given only a compressed file. Thus, one can consider data compression as data differencing with empty source data, the compressed file corresponding to a \"difference from nothing.\" This is the same as considering absolute entropy (corresponding to data compression) as a special case of relative entropy (corresponding to data differencing) with no initial data.\n\nWhen one wishes to emphasize the connection, one may use the term \"differential compression\" to refer to data differencing.\n\nAudio data compression, not to be confused with dynamic range compression, has the potential to reduce the transmission bandwidth and storage requirements of audio data. Audio compression algorithms are implemented in software as audio codecs. Lossy audio compression algorithms provide higher compression at the cost of fidelity and are used in numerous audio applications. These algorithms almost all rely on psychoacoustics to eliminate or reduce fidelity of less audible sounds, thereby reducing the space required to store or transmit them.\n\nIn both lossy and lossless compression, information redundancy is reduced, using methods such as coding, pattern recognition, and linear prediction to reduce the amount of information used to represent the uncompressed data.\n\nThe acceptable trade-off between loss of audio quality and transmission or storage size depends upon the application. For example, one 640 MB compact disc (CD) holds approximately one hour of uncompressed high fidelity music, less than 2 hours of music compressed losslessly, or 7 hours of music compressed in the MP3 format at a medium bit rate. A digital sound recorder can typically store around 200 hours of clearly intelligible speech in 640 MB.\n\nLossless audio compression produces a representation of digital data that decompress to an exact digital duplicate of the original audio stream, unlike playback from lossy compression techniques such as Vorbis and MP3. Compression ratios are around 50–60 % of original size, which is similar to those for generic lossless data compression. Lossless compression is unable to attain high compression ratios due to the complexity of waveforms and the rapid changes in sound forms. Codecs like FLAC, Shorten, and TTA use linear prediction to estimate the spectrum of the signal. Many of these algorithms use convolution with the filter [-1 1] to slightly whiten or flatten the spectrum, thereby allowing traditional lossless compression to work more efficiently. The process is reversed upon decompression.\n\nWhen audio files are to be processed, either by further compression or for editing, it is desirable to work from an unchanged original (uncompressed or losslessly compressed). Processing of a lossily compressed file for some purpose usually produces a final result inferior to the creation of the same compressed file from an uncompressed original. In addition to sound editing or mixing, lossless audio compression is often used for archival storage, or as master copies.\n\nA number of lossless audio compression formats exist. Shorten was an early lossless format. Newer ones include Free Lossless Audio Codec (FLAC), Apple's Apple Lossless (ALAC), MPEG-4 ALS, Microsoft's Windows Media Audio 9 Lossless (WMA Lossless), Monkey's Audio, TTA, and WavPack. See list of lossless codecs for a complete listing.\n\nSome audio formats feature a combination of a lossy format and a lossless correction; this allows stripping the correction to easily obtain a lossy file. Such formats include MPEG-4 SLS (Scalable to Lossless), WavPack, and OptimFROG DualStream.\n\nOther formats are associated with a distinct system, such as:\n\nLossy audio compression is used in a wide range of applications. In addition to the direct applications (MP3 players or computers), digitally compressed audio streams are used in most video DVDs, digital television, streaming media on the internet, satellite and cable radio, and increasingly in terrestrial radio broadcasts. Lossy compression typically achieves far greater compression than lossless compression (data of 5 percent to 20 percent of the original stream, rather than 50 percent to 60 percent), by discarding less-critical data.\n\nThe innovation of lossy audio compression was to use psychoacoustics to recognize that not all data in an audio stream can be perceived by the human auditory system. Most lossy compression reduces perceptual redundancy by first identifying perceptually irrelevant sounds, that is, sounds that are very hard to hear. Typical examples include high frequencies or sounds that occur at the same time as louder sounds. Those sounds are coded with decreased accuracy or not at all.\n\nDue to the nature of lossy algorithms, audio quality suffers when a file is decompressed and recompressed (digital generation loss). This makes lossy compression unsuitable for storing the intermediate results in professional audio engineering applications, such as sound editing and multitrack recording. However, they are very popular with end users (particularly MP3) as a megabyte can store about a minute's worth of music at adequate quality.\n\nTo determine what information in an audio signal is perceptually irrelevant, most lossy compression algorithms use transforms such as the modified discrete cosine transform (MDCT) to convert time domain sampled waveforms into a transform domain. Once transformed, typically into the frequency domain, component frequencies can be allocated bits according to how audible they are. Audibility of spectral components calculated using the absolute threshold of hearing and the principles of simultaneous masking—the phenomenon wherein a signal is masked by another signal separated by frequency—and, in some cases, temporal masking—where a signal is masked by another signal separated by time. Equal-loudness contours may also be used to weight the perceptual importance of components. Models of the human ear-brain combination incorporating such effects are often called psychoacoustic models.\n\nOther types of lossy compressors, such as the linear predictive coding (LPC) used with speech, are source-based coders. These coders use a model of the sound's generator (such as the human vocal tract with LPC) to whiten the audio signal (i.e., flatten its spectrum) before quantization. LPC may be thought of as a basic perceptual coding technique: reconstruction of an audio signal using a linear predictor shapes the coder's quantization noise into the spectrum of the target signal, partially masking it.\n\nLossy formats are often used for the distribution of streaming audio or interactive applications (such as the coding of speech for digital transmission in cell phone networks). In such applications, the data must be decompressed as the data flows, rather than after the entire data stream has been transmitted. Not all audio codecs can be used for streaming applications, and for such applications a codec designed to stream data effectively will usually be chosen.\n\nLatency results from the methods used to encode and decode the data. Some codecs will analyze a longer segment of the data to optimize efficiency, and then code it in a manner that requires a larger segment of data at one time to decode. (Often codecs create segments called a \"frame\" to create discrete data segments for encoding and decoding.) The inherent latency of the coding algorithm can be critical; for example, when there is a two-way transmission of data, such as with a telephone conversation, significant delays may seriously degrade the perceived quality.\n\nIn contrast to the speed of compression, which is proportional to the number of operations required by the algorithm, here latency refers to the number of samples that must be analysed before a block of audio is processed. In the minimum case, latency is zero samples (e.g., if the coder/decoder simply reduces the number of bits used to quantize the signal). Time domain algorithms such as LPC also often have low latencies, hence their popularity in speech coding for telephony. In algorithms such as MP3, however, a large number of samples have to be analyzed to implement a psychoacoustic model in the frequency domain, and latency is on the order of 23 ms (46 ms for two-way communication)).\n\nSpeech encoding is an important category of audio data compression. The perceptual models used to estimate what a human ear can hear are generally somewhat different from those used for music. The range of frequencies needed to convey the sounds of a human voice are normally far narrower than that needed for music, and the sound is normally less complex. As a result, speech can be encoded at high quality using a relatively low bit rate.\n\nIf the data to be compressed is analog (such as a voltage that varies with time), quantization is employed to digitize it into numbers (normally integers). This is referred to as analog-to-digital (A/D) conversion. If the integers generated by quantization are 8 bits each, then the entire range of the analog signal is divided into 256 intervals and all the signal values within an interval are quantized to the same number. If 16-bit integers are generated, then the range of the analog signal is divided into 65,536 intervals.\n\nThis relation illustrates the compromise between high resolution (a large number of analog intervals) and high compression (small integers generated). This application of quantization is used by several speech compression methods. This is accomplished, in general, by some combination of two approaches:\n\nPerhaps the earliest algorithms used in speech encoding (and audio data compression in general) were the A-law algorithm and the µ-law algorithm.\n\nA literature compendium for a large variety of audio coding systems was published in the IEEE Journal on Selected Areas in Communications (JSAC), February 1988. While there were some papers from before that time, this collection documented an entire variety of finished, working audio coders, nearly all of them using perceptual (i.e. masking) techniques and some kind of frequency analysis and back-end noiseless coding. Several of these papers remarked on the difficulty of obtaining good, clean digital audio for research purposes. Most, if not all, of the authors in the JSAC edition were also active in the MPEG-1 Audio committee.\n\nThe world's first commercial broadcast automation audio compression system was developed by Oscar Bonello, an engineering professor at the University of Buenos Aires. In 1983, using the psychoacoustic principle of the masking of critical bands first published in 1967, he started developing a practical application based on the recently developed IBM PC computer, and the broadcast automation system was launched in 1987 under the name Audicom. Twenty years later, almost all the radio stations in the world were using similar technology manufactured by a number of companies.\n\nVideo compression is a practical implementation of source coding in information theory. In practice, most video codecs are used alongside audio compression techniques to store the separate but complementary data streams as one combined package using so-called \"container formats\".\n\nUncompressed video requires a very high data rate. Although lossless video compression codecs perform at a compression factor of 5 to 12, a typical MPEG-4 lossy compression video has a compression factor between 20 and 200.\n\nVideo data may be represented as a series of still image frames. Such data usually contains abundant amounts of spatial and temporal redundancy. Video compression algorithms attempt to reduce redundancy and store information more compactly. \n\nMost video compression formats and codecs exploit both spatial and temporal redundancy (e.g. through difference coding with motion compensation). Similarities can be encoded by only storing differences between e.g. temporally adjacent frames (inter-frame coding) or spatially adjacent pixels (intra-frame coding).\nInter-frame compression (a temporal delta encoding) is one of the most powerful compression techniques. It (re)uses data from one or more earlier or later frames in a sequence to describe the current frame. Intra-frame coding, on the other hand, uses only data from within the current frame, effectively being still-image compression. And the intra-frame coding always uses lossy compression algorithms. \n\nA class of specialized formats used in camcorders and video editing use less complex compression schemes that restrict their prediction techniques to intra-frame prediction.\n\nUsually video compression additionally employs lossy compression techniques like quantization that reduce aspects of the source data that are (more or less) irrelevant to the human visual perception by exploiting perceptual features of human vision. For example, small differences in color are more difficult to perceive than are changes in brightness. Compression algorithms can average a color across these similar areas to reduce space, in a manner similar to those used in JPEG image compression. As in all lossy compression, there is a trade-off between video quality, cost of processing the compression and decompression, and system requirements. Highly compressed video may present visible or distracting artifacts.\n\nOther methods than the prevalent DCT-based transform formats, such as fractal compression, matching pursuit and the use of a discrete wavelet transform (DWT), have been the subject of some research, but are typically not used in practical products (except for the use of wavelet coding as still-image coders without motion compensation). Interest in fractal compression seems to be waning, due to recent theoretical analysis showing a comparative lack of effectiveness of such methods.\n\nInter-frame coding works by comparing each frame in the video with the previous one. Individual frames of a video sequence are compared from one frame to the next, and the video compression codec sends only the differences to the reference frame. If the frame contains areas where nothing has moved, the system can simply issue a short command that copies that part of the previous frame into the next one. If sections of the frame move in a simple manner, the compressor can emit a (slightly longer) command that tells the decompressor to shift, rotate, lighten, or darken the copy. This longer command still remains much shorter than intraframe compression. Usually the encoder will also transmit a residue signal which describes the remaining more subtle differences to the reference imagery. Using entropy coding, these residue signals have a more compact representation than the full signal. In areas of video with more motion, the compression must encode more data to keep up with the larger number of pixels that are changing. Commonly during explosions, flames, flocks of animals, and in some panning shots, the high-frequency detail leads to quality decreases or to increases in the variable bitrate.\n\nToday, nearly all commonly used video compression methods (e.g., those in standards approved by the ITU-T or ISO) share the same basic architecture that dates back to H.261 which was standardized in 1988 by the ITU-T. They mostly rely on the DCT, applied to rectangular blocks of neighboring pixels, and temporal prediction using motion vectors, as well as nowadays also an in-loop filtering step.\n\nIn the prediction stage, various deduplication and difference-coding techniques are applied that help decorrelate data and describe new data based on already transmitted data.\n\nThen rectangular blocks of (residue) pixel data are transformed to the frequency domain to ease targeting irrelevant information in quantization and for some spatial redundancy reduction. The discrete cosine transform (DCT) that is widely used in this regard was introduced by N. Ahmed, T. Natarajan and K. R. Rao in 1974.\n\nIn the main lossy processing stage that data gets quantized in order to reduce information that is irrelevant to human visual perception.\n\nIn the last stage statistical redundancy gets largely eliminated by an entropy coder which often applies some form of arithmetic coding.\n\nIn an additional in-loop filtering stage various filters can be applied to the reconstructed image signal. By computing these filters also inside the encoding loop they can help compression because they can be applied to reference material before it gets used in the prediction process and they can be guided using the original signal. The most popular example are deblocking filters that blur out blocking artefacts from quantization discontinuities at transform block boundaries.\n\nAll basic algorithms of today's dominant video codec architecture have been invented before 1979.\nIn 1950, the Bell Labs filed the patent on DPCM which soon was applied to video coding. Entropy coding started in the 1940s with the introduction of Shannon–Fano coding on which the widely used Huffman coding is based that was developed in 1950; the more modern context-adaptive binary arithmetic coding (CABAC) was published in the early 1990s. Transform coding (using the Hadamard transform) was introduced in 1969, the popular discrete cosine transform (DCT) appeared in 1974 in scientific literature.\nThe ITU-T's standard H.261 from 1988 introduced the prevalent basic architecture of video compression technology.\n\nGenetics compression algorithms are the latest generation of lossless algorithms that compress data (typically sequences of nucleotides) using both conventional compression algorithms and genetic algorithms adapted to the specific datatype. In 2012, a team of scientists from Johns Hopkins University published a genetic compression algorithm that does not use a reference genome for compression. HAPZIPPER was tailored for HapMap data and achieves over 20-fold compression (95% reduction in file size), providing 2- to 4-fold better compression and in much faster time than the leading general-purpose compression utilities. For this, Chanda, Elhaik, and Bader introduced MAF based encoding (MAFE), which reduces the heterogeneity of the dataset by sorting SNPs by their minor allele frequency, thus homogenizing the dataset. Other algorithms in 2009 and 2013 (DNAZip and GenomeZip) have compression ratios of up to 1200-fold—allowing 6 billion basepair diploid human genomes to be stored in 2.5 megabytes (relative to a reference genome or averaged over many genomes).. For a benchmark in genetics/genomics data compressors, see \n\nIt is estimated that the total amount of data that is stored on the world's storage devices could be further compressed with existing compression algorithms by a remaining average factor of 4.5:1. It is estimated that the combined technological capacity of the world to store information provides 1,300 exabytes of hardware digits in 2007, but when the corresponding content is optimally compressed, this only represents 295 exabytes of Shannon information.\n\n"}
{"id": "52275869", "url": "https://en.wikipedia.org/wiki?curid=52275869", "title": "Directory-based coherence", "text": "Directory-based coherence\n\nDirectory-based coherence is a mechanism to handle Cache coherence problem in Distributed shared memory (DSM) a.k.a. Non-Uniform Memory Access (NUMA). Another popular way is to use a special type of computer bus between all the nodes as a \"shared bus\" (a.k.a. System bus). Directory-based coherence uses a special directory to serve instead of the shared bus in the bus-based coherence protocols. Both of these designs use the corresponding medium (i.e. directory or bus) as tool to facilitate the communication between different nodes, and to guarantee that the coherence protocol is working properly along all the communicating nodes. In directory based cache coherence, this is done by using this directory to keep track of the status of all cache blocks, the status of each block includes in which cache coherence \"state\" that block is, and which nodes are sharing that block at that time, which can be used to eliminate the need to broadcast all the signals to all nodes, and only send it to the nodes that are interested in this single block.\n\nFollowing are a few advantages and disadvantages of the directory based cache coherence protocol:\nAccording to the above discussion, it is clear that using bus based systems seems more attractive for relatively small systems. However, directory based systems become crucial when the system scale up and the number of nodes grows. So there is a kind of trade-off between the simplicity and the scalability when comparing between Bus-based and Directory-based cache coherence designs.\n\nThe idea of Directory-based cache coherence systems begun long time ago. Although the idea of DASH (Directory Architecture for SHared-memory) was first proposed by C.K. Tang in the mid 1970s. However, applying it to cache coherence was proposed a few years later, specifically in 1978, when researchers at Stanford university proposed the first version of this coherence systems called Stanford DASH, in a paper that described the system with the difficulties and improvements that come with such designs. Beside this approach, several attempts were done to provide a scalable systems. For instance, BBN Butterfly which was introduced in 1985, and IBM PR3 which was introduced in 1987, are some examples of multiprocessor systems that are scalable. However, both of these systems have a drawback; For example, BBN Butterfly does not have caches. Similarly, IBM PR3 does not provide hardware cache coherence, which limits the performance of both of these designs, especially when employing high performance processors.\n\nThis limitation in the other competitors, made it easier for DASH based systems to get chosen for design cache coherence systems and all other systems that need scalabality in cache-based nodes. In 1985, James Archibald and Jean-Loup Baer from the University of Washington published a paper that proposes a more economical, expandable, and modular variation of the \"global directory\" approach in the term of hardware use in the design.\n\nIn 1992, Daniel Lenoski from Stanford university published a paper proposing advances in cache coherence protocols for directory-based systems. In a 1996 paper, he introduced the design of the SGI Origin 2000, a family of server computers employing directory based cache coherence. The subsequent Origin 3000 was introduced in July 2000.\n\nUnlike Snoopy coherence protocols, in a directory based coherence approach, the information about which caches have a copy of a block is maintained in a structure called \"Directory\". In a directory based scheme, participating caches do not broadcast requests to all other sharing caches of the block in order to locate cached copies, instead it queries the directory to retrieve the information about which block have cached copies and sends only to those particular processors and hence traffic saving compared to a snoopy protocol is large. In well optimized applications, most data sharing is only for data that is read only, and there is little sharing for data that is frequently read and written. A directory approach can result in a substantial traffic saving compared to broadcast/snoopy approach in such applications.\n\nAs shown in the data flow diagram, the actors involved in a distributed shared memory system implementing directory based coherence protocol are:\nRequestor and Owner nodes maintain their state transition similar to a snoopy coherence protocols like MESI protocol. However, unlike a bus based implementation where nodes communicate using a common bus, directory based implementation uses message passing model to exchange information required for maintaining cache coherence.\n\nDirectory node acts as a serializing point and all communications are directed through this node to maintain correctness.\n\nA directory node keeps track of the overall state of a cache block in the entire cache system for all processors. It can be in three states :\nExplanation of the Directory state transition Finite State Machine (refer image 1) is captured below in the table:\nIn addition to cache state, a directory must track which processors have data when in the shared state. This is required to for sending invalidation and intervention requests to the individual processor caches which have the cache block in shared state. Few of the popular implementation approaches are: \n\nPlease note that the protocol described above is the basic implementation and race conditions can occur due to the fact that directory can be out of sync with the caches and also messages between processors can be overlapping. More complex implementations are available like Scalable Coherent Interface which have multiple states.\n\nDASH cache coherence protocol is another protocol that uses directory-based coherence scheme. DASH protocol uses a clustered approach, where processors inside a cluster are kept coherent using bus based snooping scheme, while the clusters are connected in a directory approach. Even though various protocols use different implementations for tracking cache blocks, however the concept of directory remains same.\n\n"}
{"id": "726830", "url": "https://en.wikipedia.org/wiki?curid=726830", "title": "Electronic viewfinder", "text": "Electronic viewfinder\n\nAn electronic viewfinder (EVF) is a camera viewfinder where the image captured by the lens is projected electronically onto a miniature display. The image on this display is used to assist in aiming the camera at the scene to be photographed. It differs from a live preview screen in being smaller and shaded from ambient light.\n\nThe sensor records the view through the lens, the view is processed, and finally projected on a miniature display which is viewable through the eyepiece. Electronic viewfinders are used in digital still cameras and in video cameras. \n\nSome cameras (such as Panasonic, Sony, Fujifilm) have an automatic eye sensor which switches the display from screen to EVF when the viewfinder is near the eye. More modest cameras use a button to switch the display. Some have no button at all.\n\nLike the live preview screen, electronic viewfinders can show additional information, such as an image histogram, focal ratio, camera settings, battery charge, and remaining storage space. Some have a focus peaking feature that highlights areas of the frame that are in focus. They are also in several ways more accurate than an optical viewfinder (OVF):\n\n\nElectronic viewfinders have the following limitations:\n\nElectronic viewfinders have been in use with bridge cameras for some years but with limited resolution and image quality. They are used in most mirrorless modern system cameras (e.g. Panasonic Micro Four Third System, Sony NEX, Sony SLT, Nikon V1).\n\nMany professional photographers and advanced amateurs prefer DSLR cameras that have a true optical through-the-lens viewfinder (OVF). From 2006 some DSLR camera models provide both through-the-lens viewing and a \"live preview\" on the LCD (as distinct from an electronic viewfinder). These include, but are not limited to, the Olympus E-330, E-410, E-510 and E-3, the Panasonic Lumix DMC-L1 and DMC-L10, the Leica Digilux 3, the Canon EOS 40D, EOS 50D, EOS 60D, EOS 7D and EOS-1D Mark III, and the Nikon D3, D300 and D90.\n\nTo get the advantage of both optical and electronic viewfinders some cameras have hybrid viewfinders. These display the image in an optical eyepiece viewfinder, or electronically on an LCD screen. Examples include some Fujifilm X-series cameras.\n\n"}
{"id": "15125142", "url": "https://en.wikipedia.org/wiki?curid=15125142", "title": "Eufiserv", "text": "Eufiserv\n\nEUFISERV (European Savings Banks Financial Services) is a European interbank network connecting the ATMs of savings banks in Austria, Belgium, the Czech Republic, Finland, France, Germany, Italy, the Netherlands, Norway, Portugal, Spain, Sweden, and Switzerland. It is the largest and the only international credit union-owned interbank network in Europe.\n\n\nEufiserv provides Gateway Services to the Visa PLUS network, to MasterCard's Cirrus network, and to the networks of American Express and China UnionPay.\nAll ATMs in the Eufiserv network are connected to the gateways to Visa PLUS, Cirrus and American Express, and therefore accept all Visa, Visa Electron, Visa Debit, PLUS, MasterCard, Maestro, Cirrus, and American Express cards but, until now, only selected ATMs are connected to the CUP gateway. The operations of the gateways to the Visa PLUS and Cirrus networks are outsourced by EUFISERV to Confederacion Espanola de Cajas de Ahorros in Spain.\n\n\n"}
{"id": "55709846", "url": "https://en.wikipedia.org/wiki?curid=55709846", "title": "Europa Thermal Emission Imaging System", "text": "Europa Thermal Emission Imaging System\n\nThe Europa Thermal Emission Imaging System (E-THEMIS) instrument is designed to scan the surface of Europa and identify areas of geologically recent resurfacing through the detection of subtle thermal anomalies. This 'heat detector' will provide high spatial resolution, multi-spectral thermal imaging of Europa to help detect active sites such as outflows and plumes. E-THEMIS will be launched on board the planned \"Europa Clipper\" astrobiology mission to Jupiter's moon Europa in 2022-2025.\n\nThe E-THEMIS uses technology inherited from the THEMIS camera flown on board the 2001 \"Mars Odyssey\" orbiter, and the \"OSIRIS-REx\" OTES instruments.\n\nE-THEMIS will identify areas of geologically recent resurfacing through the detection of subtle thermal anomalies. E-THEMIS will be fabricated by Arizona State University with hardware contributions from Ball Aerospace Corporation, and Raytheon Vision Systems. The Principal Investigator is Philip Christensen at Arizona State University. One of the primary science objectives of the Europa Thermal Emission Imaging System (E-THEMIS) is to determine the regolith particle size, block abundance, and sub-surface layering for landing site assessment and surface process studies. The E-THEMIS investigation is designed to characterize Europa's thermal behavior and identify any thermal anomalies due to recent or ongoing activity, which include multi-spectral infrared emission, at both day and night. To accomplish this, E-THEMIS will obtain thermal infrared images in three spectral bands from 7 to 70 μm at multiple times of day.\n\nThermal anomalies on Europa may be manifestations of subsurface melting due to hot spots, shear heating on faults, and eruptions of liquid water, which can be imaged in the infrared spectrum. Europa's water is suspected to lie below the moon's ice crust.\n\nThe specific objectives of the E-THEMIS investigation are: \n\nTo achieve this, E-THEMIS will image the surface at a resolution of 5 × 22 m from 25 km altitude; it will have a precision of 0.2 K for 90 K surfaces and 0.1 K at 220 K, with an accuracy of 1-2.2 K from 220-90 K; and E-THEMIS will obtain images with up to 360 cross-track pixels with a 10.1 km wide image swath from 100 km. The instrument can identify active vents, if existing, at the 1-10 meter scale. A radiation-hardened integrated circuit will be incorporated to meet the radiation requirements.\n"}
{"id": "15875142", "url": "https://en.wikipedia.org/wiki?curid=15875142", "title": "Fit-PC", "text": "Fit-PC\n\nThe fit-PC is a small, light, fan-less nettop computer manufactured by the Israeli company CompuLab.\n\nMany fit-PC models are available. fit-PC 1.0 was introduced in July 2007, fit-PC Slim was introduced in September 2008, fit-PC 2 was introduced in May 2009, fit-PC 3 was introduced in early 2012, and fit-PC 4 was introduced spring 2014. The device is power-efficient (fit-PC 1 was about 5 W) and therefore considered to be a green computing project, capable of using open source software and creating minimal electronic waste.\n\nOn February 19, 2009, Compulab announced the fit-PC2, which is \"a major upgrade to the fit-PC product line\".\nDetailed specifications for the fit-PC2 include an Intel Atom Z5xx \"Silverthorne\" processor (1.1/1.6/2.0 GHz options), up to 2GB of RAM, 160GB SATA Hard Drive, GigaBit LAN and more. The fit-PC2 is also capable of HD video playback. Its declared power consumption is only 6W, and according to the manufacturer, it saves 96% of the power used by a standard desktop. fit-PC2 is the most power efficient PC on the Energy-Star list.\nThe fit-PC2 is based on the GMA 500 (Graphics Media Accelerator). Unfortunately the open source driver included in Linux kernel 2.6.39 does not support VA-API video or OpenGL/3D acceleration.\nThe fit-PC2 is being phased out and is being replaced by the fitlet, the fitlet was designed to replace the groundbreaking (and still popular) CompuLab fit-PC2.\n\nOn December 2, 2009, Compulab announced the fit-PC2i, a fit-PC2 variation targeting networking and industrial applications.\nfit-PC2i adds a second Gbit Ethernet port, Wake-on-LAN, S/PDIF output and RS232 port, has two fewer USB ports, and no IR.\n\nThe fit-PC3 has been released early 2012. \nSee the fit-PC3 article.\n\nThe fit-PC4 has been released spring 2014.\n\nThe fitlet has been announced January 14, 2015.\nIt has 3 CPU/SoC variations, and 5 feature variations, though only 7 models have been announced so far.\n\nOn September 16, 2008, Compulab announced the Fit-PC Slim, which at 11 x 10 x 3 cm is smaller than fit-PC 1.0.\n\nfit-PC Slim uses 500 MHz AMD Geode LX800 processor and has 512mb soldered-on RAM. The computer includes a VGA output, a serial port with a custom connector, Ethernet, b/g WLAN, and 3 USB ports (2 on the front panel). The system has an upgradeable 2.5\" 60GB ATA hard drive.\n\nfit-PC Slim has General Software BIOS supporting PXE and booting from a USB CDROM or USB thumb drive. It is pre-installed with either Windows Vista or with Ubuntu 8.10 and Gentoo Linux 2008.0 . Also Windows Embedded can be used, or pre-installed on a FlowDrive.\n\nThe fit-PC Slim end-of-life was announced on 19 June 2009 with the general availability of fit-PC2.\n\nfit-PC 1.0 is an earlier model that has the following differences\n\n\n"}
{"id": "51874122", "url": "https://en.wikipedia.org/wiki?curid=51874122", "title": "Force meter", "text": "Force meter\n\nA force meter is an instrument to measure the magnitude of a force. This is often just a spring contained within a cylinder that has a scale to mark the extension of the spring produced by the force. The unit of force is the Newton (symbol N) in the International System of Units and in that case the meter may be referred to as a newtonmeter.\n\nA spring-based force meter with uniformly marked scale intervals is relying on Hooke's law which states that the extension (or compression) of the spring is proportional to the applied force.\n\n"}
{"id": "20616573", "url": "https://en.wikipedia.org/wiki?curid=20616573", "title": "Genwi", "text": "Genwi\n\nGENWI is a privately held technology company based in San Jose, CA that provides a mobile content enablement platform.\n\nGENWI was a free web-based news reader, or aggregator, initially released in March 2007. Genwi provided a news feed service by enabling users to publish their feeds to one profile and follow others' news feeds in the feed reader – this feed reader was called \"Wire\" and was capable of reading RSS, Media RSS, iTunes RSS and ATOM feeds. Genwi offered a suite of social networking features built into the RSS reader. Users were able to add friends, send messages, leave comments and share individual feed items. The site underwent a major redesign in November 2008 and was shut down in 2009.\n\nIn January 2010, GENWI, Inc. used the same technology that built their RSS reader to launch iSites.us, a smartphone app builder and management system, which enables businesses to build applications for iPhone and Android using RSS, ATOM or social feeds. GENWI uses cloud-based technology to keep more than 1,500 native apps up-to-date and to instantly build HTML5 apps for iPhone. \n\nIn September 2011, GENWI launched Conde Nast's \"The Daily W\" app and rebranded the iSites brand back to GENWI and now helps publishers and brands create engaging native and HTML5 apps with a cloud-based mobile content management system, or mCMS. \n\nGENWI is short for \"Generation Wireless\" and is headquartered in San Jose, California.\n\n"}
{"id": "23841270", "url": "https://en.wikipedia.org/wiki?curid=23841270", "title": "GreenSun Energy", "text": "GreenSun Energy\n\nGreenSun Energy is a Jerusalem-based Israeli company that has developed a new process for producing electricity from solar energy. As The Economist points out, solar energy is a logical development since \"Israel is a country with plenty of sunshine, lots of sand and quite a few clever physicists and chemists.\"\n\nTraditional photovoltaic cells are made from thin sheets of silicon covered by glass plates. In the GreenSun process, it is only the outer edges of the glass plates that are covered by thin strips of silicon. The physicists and chemists at GreenSun, led by Renata Reisfeld, coat the glass with metallic nanoparticles and dyes to cause the sunlight falling on the glass to diffuse sideways toward the edges where the silicon strips turn it into electricity. \n\nA mixture of dyes is used to capture and absorb a wide spectrum of available light. Having absorbed the sunlight, the fluorescent dyes then re-radiate it. Interaction with the metallic nanoparticles turns light into a form of electromagnetic radiation known as surface plasmons. The surface plasmons propagate over the glass surface and are intercepted by the silicon strips at the edges. The company would not reveal which metals are used in the process.\n\nBecause the process uses less silicon, it is far less expensive than conventional photovoltaic modules. It is also more efficient. In a conventional photovoltaic cell, much of the sun's energy is lost as heat because the energy of light varies across the spectrum (red light is less energetic than blue, for example). Only a particular amount of energy is needed to knock an electron free from the silicon atoms. If the sunlight is more energetic than necessary, more energy than usual is lost as heat. Sunlight scatters its energy, but the dye/nanoparticle mix in the GreenSun process delivers plasmons and photons of the right energy to knock electrons free more efficiently.\n\nGreenEnergy CEO Amnon Leikovich claims that the process, once put into production, could deliver electricity at about double the cost of a conventional power station. Traditional photovoltaic modules deliver electricity at about five times the cost of a conventional power plant. Leikovich hopes that costs can be brought down even further. \n\nAccording to corporate spokesmen Eitan Shmueli, GreenSun's photovoltaic panels are capable of producing electricity from low-intensity sunlight on cloudy days and from sunlight that reaches them from any angle. The company claims that can turn the windows or walls of buildings into electricity-generating photovoltaic panels.\n\n"}
{"id": "48305915", "url": "https://en.wikipedia.org/wiki?curid=48305915", "title": "HASQI", "text": "HASQI\n\nHASQI, \"Hearing-Aid Speech Quality Index\", is a measure of audio quality originally designed for the evaluation of speech quality for those with a hearing aid. It has also been shown to be able to gauge audio quality for non-speech sounds and for listeners without a hearing loss.\n\nWhile the perception of audio quality can be gauged through perceptual measurements, the testing is time-consuming to undertake. Consequently, a number of metrics have been developed to allow audio quality to be evaluated without the need for human listening. Standardized examples from telephony include PESQ, POLQA, PEVQ and PEAQ. HASQI was originally developed by Kates and Arehart to evaluate how the distortions introduced by hearing aids degrade quality. They also produced a new version in 2014.\n\nKressner et al. tested a speech corpus different from the dataset used to develop HASQI and showed that the index generalizes well for listeners without a hearing loss with a performance comparable to PESQ. Kendrick et al. showed that HASQI can grade the audio quality of music and geophonic, biophonic, and anthrophonic quotidian sounds, although their study used a more limited set of degradations.\n\nHASQI and its 2014 revision are double-ended methods requiring both a clean reference and the degraded signal to allow evaluation. The index attempts to capture the effects of noise, nonlinear distortion, linear filtering and spectral changes, by computing the difference or correlation between key audio features. This is done by examining short-time signal envelopes to quantify the degradation caused by noise and nonlinear filtering, and long-time signal envelopes to quantify the effects of linear filtering. Version 2 of HASQI includes a model to capture some aspects of the peripheral auditory system for both normal and hearing impaired listeners.\n\nKendrick et al. developed a blind (single-ended) method, bHASQI, using machine learning. This enables the audio quality to be evaluated from just the degraded signal without needing the clean reference.\n\n\n"}
{"id": "3829190", "url": "https://en.wikipedia.org/wiki?curid=3829190", "title": "Hand sanitizer", "text": "Hand sanitizer\n\nHand sanitizer is a liquid generally used to decrease infectious agents on the hands. Formulations of the alcohol-based type are preferable to hand washing with soap and water in most situations in the healthcare setting. It is generally more effective at killing microorganisms and better tolerated than soap and water. Hand washing should still be carried out if contamination can be seen or following the use of the toilet. The general use of non-alcohol based versions has no recommendations. Outside the health care setting evidence to support the use of hand sanitizer over hand washing is poor. They are available as liquids, gels, and foams.\nAlcohol-based versions typically contain some combination of isopropyl alcohol, ethanol (ethyl alcohol), or \"n\"-propanol. Versions that contain 60 to 95% alcohol are most effective. Care should be taken as they are flammable. Alcohol-based hand sanitizer works against a variety of microorganisms but not spores. Some versions contain compounds such as glycerol to prevent drying of the skin. Non-alcohol based versions may contain benzalkonium chloride or triclosan.\nAlcohol has been used as an antiseptic at least as early as 1363 with evidence to support its use becoming available in the late 1800s. Alcohol-based hand sanitizer has been commonly used in Europe since at least the 1980s. The alcohol-based version is on the World Health Organization's List of Essential Medicines, the most effective and safe medicines needed in a health system. The wholesale cost in the developing world is about US$1.40–3.70 per liter bottle.\n\nThe Clean Hands campaign by the US Centers for Disease Control and Prevention (CDC) instructs the public hand washing. Alcohol-based hand sanitizer is recommended only if soap and water are not available.\n\nThe evidence for hand hygiene in school generally is of poor quality.\n\nAlcohol-based hand sanitizer is more convenient compared to hand washing with soap and water in most situations in the healthcare setting. It is generally more effective at killing microorganisms and better tolerated than soap and water. Hand washing should still be carried out if contamination can be seen or following the use of the toilet.\n\nHand sanitizer that contains at least 60 percent alcohol or contains a \"persistent antiseptic\" should be used. Alcohol rubs kill many different kinds of bacteria, including antibiotic resistant bacteria and TB bacteria. 90% alcohol rubs are highly flammable, but kill many kinds of viruses, including enveloped viruses such as the flu virus, the common cold virus, and HIV, though is notably ineffective against the rabies virus.\n\n90% alcohol rubs are more effective against viruses than most other form of hand washing. Isopropyl alcohol will kill 99.99 percent or more of all non-spore forming bacteria in less than 30 seconds, both in the laboratory and on human skin.\n\nThe alcohol in hand sanitizers may not have the 10–15 seconds exposure time required to denature proteins and lyse cells in too low quantities (0.3 mL) or concentrations (below 60%). In environments with high lipids or protein waste (such as food processing), the use of alcohol hand rubs alone may not be sufficient to ensure proper hand hygiene.\n\nFor health care settings like hospitals and clinics, optimum alcohol concentration to kill bacteria is 70% to 95%. Products with alcohol concentrations as low as 40% are available in American stores, according to researchers at East Tennessee State University.\n\nAlcohol rub sanitizers kill most bacteria, and fungi, and stop some viruses. Alcohol rub sanitizers containing at least 70% alcohol (mainly ethyl alcohol) kill 99.9% of the bacteria on hands 30 seconds after application and 99.99% to 99.999% in one minute.\n\nFor health care, optimal disinfection requires attention to all exposed surfaces such as around the fingernails, between the fingers, on the back of the thumb, and around the wrist. Hand alcohol should be thoroughly rubbed into the hands and on the lower forearm for a duration of at least 30 seconds and then allowed to air dry.\n\nUse of alcohol-based hand gels dries skin less, leaving more moisture in the epidermis, than hand washing with antiseptic/antimicrobial soap and water.\n\nThere are certain situations during which hand washing with water and soap are preferred over hand sanitizer, these include: eliminating bacterial spores of \"Clostridioides difficile\", parasites such as \"Cryptosporidium\", and certain viruses like norovirus depending on the concentration of alcohol in the sanitizer (95% alcohol was seen to be most effective in eliminating most viruses). In addition, if hands are contaminated with fluids or other visible contaminates, hand washing is preferred as well as when after using the toilet and if discomfort develops from the residue of alcohol sanitizer use. Furthermore, CDC recommends hand sanitizers are not effective in removing chemicals such as pesticides.\n\nAlcohol gel can catch fire, producing a translucent blue flame. This is due to the flammable alcohol in the gel. Some hand sanitizer gels may not produce this effect due to a high concentration of water or moisturizing agents. There have been some rare instances where alcohol has been implicated in starting fires in the operating room, including a case where alcohol used as an antiseptic pooled under the surgical drapes in an operating room and caused a fire when a cautery instrument was used. Alcohol gel was not implicated. To minimize the risk of fire, alcohol rub users are instructed to rub their hands until dry, which indicates that the flammable alcohol has evaporated. Fire departments suggest refills for the alcohol-based hand sanitizers can be stored with cleaning supplies away from heat sources or open flames.\n\nResearch shows that alcohol hand sanitizers do not pose any risk by eliminating beneficial microorganisms that are naturally present on the skin. The body quickly replenishes the beneficial microbes on the hands, often moving them in from just up the arms where there are fewer harmful microorganisms. However, alcohol may strip the skin of the outer layer of oil, which may have negative effects on barrier function of the skin. A study also shows that disinfecting hands with an antimicrobial detergent results in a greater barrier disruption of skin compared to alcohol solutions, suggesting an increased loss of skin lipids.\n\nIn the United States, the U.S. Food and Drug Administration (FDA) controls antimicrobial handsoaps and sanitizers as over-the-counter drugs (OTC) because they are intended for topical anti-microbial use to prevent disease in humans. The FDA requires strict labeling which informs consumers on proper use of this OTC drug and dangers to avoid, including warning adults not to ingest, not to use in the eyes, to keep out of the reach of children, and to allow use by children only under adult supervision. According to the American Association of Poison Control Centers, there were nearly 12,000 cases of hand sanitizer ingestion in 2006. If ingested, alcohol-based hand sanitizers can cause alcohol poisoning in small children. However, the U.S. Centers for Disease Control recommends using hand sanitizer with children to promote good hygiene, under supervision, and furthermore recommends parents pack hand sanitizer for their children when traveling, to avoid their contracting disease from dirty hands.\n\nThere have been reported incidents of people drinking the gel in prisons and hospitals, where alcohol is not allowed, to become intoxicated leading to its withdrawal from some establishments.\n\nOn April 30, 2015, the FDA announced that they were requesting more scientific data based on the safety of hand sanitizer. Emerging science also suggests that for at least some health care antiseptic active ingredients, systemic exposure (full body exposure as shown by detection of antiseptic ingredients in the blood or urine) is higher than previously thought, and existing data raise potential concerns about the effects of repeated daily human exposure to some antiseptic active ingredients. This would include hand antiseptic products containing alcohol and triclosan.\n\nConsumer alcohol-based hand sanitizers, and health care \"hand alcohol\" or \"alcohol hand antiseptic agents\", are antiseptic products used to avoid transmission of pathogens. These exist in liquid, foam, and easy-flowing gel formulations. The level of alcohol varies between 60% and 95%.\n\nAlcohol-based hand rubs are extensively used in the hospital environment as an alternative to antiseptic soaps. Hand-rubs in the hospital environment have two applications: hygienic hand rubbing and surgical hand disinfection. Alcohol based hand rubs provide a better skin tolerance as compared to antiseptic soap.\nHand rubs also prove to have more effective microbiological properties as compared to antiseptic soaps.\n\nThe same ingredients used in over-the-counter hand-rubs are also used in hospital hand-rubs: alcohols such ethanol and isopropanol, sometimes combined with quats (quaternary ammonium cations) such as benzalkonium chloride. Quats are added at levels up to 200 parts per million to increase antimicrobial effectiveness. Although allergy to alcohol-only rubs is rare, fragrances, preservatives and quats can cause contact allergies. These other ingredients do not evaporate like alcohol and accumulate leaving a \"sticky\" residue until they are removed with soap and water.\n\nThe most common brands of alcohol hand rubs include Aniosgel, Avant, Sterillium, Desderman and Allsept S. All hospital hand rubs must conform to certain regulations like EN 12054 for hygienic treatment and surgical disinfection by hand-rubbing. Products with a claim of \"99.99% reduction\" or 4Log reduction are ineffective in hospital environment, since the reduction must be more than \"99.99%\".\n\nThe hand sanitizer dosing systems for hospitals are designed to deliver a measured amount of the product for staff. They are dosing pumps screwed onto a bottle or are specially designed dispensers with refill bottles. Dispensers for surgical hand disinfection are usually equipped with elbow controlled mechanism or infrared sensors to avoid any contact with the pump.\n\nHands must be disinfected before any surgical procedure by hand washing with mild soap and then hand-rubbing with a sanitizer. Surgical disinfection requires a larger dose of the hand-rub and a longer rubbing time than is ordinarily used. It is usually done in two applications according to specific hand-rubbing techniques, EN1499 (hygienic handwash), and EN 1500 (hygienic hand disinfection) to ensure that antiseptic is applied everywhere on the surface of the hand.\n\nSome hand sanitizer products use agents other than alcohol to kill microorganisms, such as povidone-iodine, benzalkonium chloride or triclosan.\n\nThe World Health Organization (WHO) and the CDC recommends \"persistent\" antiseptics for hand sanitizers. Persistent activity is defined as the prolonged or extended antimicrobial activity that prevents or inhibits the proliferation or survival of microorganisms after application of the product. This activity may be demonstrated by sampling a site several minutes or hours after application and demonstrating bacterial antimicrobial effectiveness when compared with a baseline level. This property also has been referred to as \"residual activity.\" Both substantive and nonsubstantive active ingredients can show a persistent effect if they substantially lower the number of bacteria during the wash period.\n\nLaboratory studies have shown lingering benzalkonium chloride may be associated with antibiotic resistance in MRSA while no mechanism for resistance to alcohol has ever been described in bacteria. Where alcohol sanitizers utilize 62%, or higher, alcohol by weight, only .1 to .13% of benzalkonium chloride by weight provides equivalent antimicrobial effectiveness.\n\nTriclosan has been shown to accumulate in biosolids in the environment, one of the top seven organic contaminants in waste water according to the National Toxicology Program Triclosan leads to various problems with natural biological systems, and triclosan, when combined with chlorine e.g. from tap water, produces dioxins, a probable carcinogen in humans. However, 90–98% of triclosan in waste water biodegrades by both photolytic or natural biological processes or is removed due to sorption in waste water treatment plants. Numerous studies show that only very small traces are detectable in the effluent water that reaches rivers.\n\nA series of studies show that photodegradation of triclosan produced 2,4-dichlorophenol and 2,8-dichlorodibenzo-p-dioxin (2,8-DCDD). The 2,4-dichlorophenol itself is known to be biodegradable as well as photodegradable. For DCDD, one of the non-toxic compounds of the dioxin family, a conversion rate of 1% has been reported and estimated half-lives suggest that it is photolabile as well. The formation-decay kinetics of DCDD are also reported by Sanchez-Prado et al. (2006) who claim \"transformation of triclosan to toxic dioxins has never been shown and is highly unlikely.\"\n\nAlcohol-free hand sanitizers may be effective immediately while on the skin, but the solutions themselves can become contaminated because alcohol is an in-solution preservative and without it, the alcohol-free solution itself is susceptible to contamination. However, even alcohol-containing hand sanitizers can become contaminated if the alcohol content is not properly controlled or the sanitizer is grossly contaminated with microorganisms during manufacture. In June 2009, alcohol-free Clarcon Antimicrobial Hand Sanitizer was pulled from the US market by the FDA, which found the product contained gross contamination of extremely high levels of various bacteria, including those which can \"cause opportunistic infections of the skin and underlying tissues and could result in medical or surgical attention as well as permanent damage\". Gross contamination of any hand sanitizer by bacteria during manufacture will result in the failure of the effectiveness of that sanitizer and possible infection of the treatment site with the contaminating organisms.\n\nSome products claim to kill microorganisms naturally, although these claims are not substantiated in any FDA monograph. All hand sanitizer products require National Drug Code designation in the United States and natural product number designation in Canada. \n\n"}
{"id": "18660882", "url": "https://en.wikipedia.org/wiki?curid=18660882", "title": "Hi-Fi Digimonster", "text": "Hi-Fi Digimonster\n\nThe Hi-Fi Digimonster is an electronic device that can detonate a bomb. It was built for terrorists and designed for use in a terrorist plot in the United Kingdom. A device seized by the police in Canada from the home of Momin Khawaja had a range of in a neighbourhood, but in an open area the range could be .\n"}
{"id": "1471717", "url": "https://en.wikipedia.org/wiki?curid=1471717", "title": "Hot air engine", "text": "Hot air engine\n\nA hot air engine (historically called an air engine or caloric engine) is any heat engine that uses the expansion and contraction of air under the influence of a temperature change to convert thermal energy into mechanical work. These engines may be based on a number of thermodynamic cycles encompassing both open cycle devices such as those of Sir George Cayley and John Ericsson and the closed cycle engine of Robert Stirling. Hot air engines are distinct from the better known internal combustion based engine and steam engine.\n\nIn a typical implementation, air is repeatedly heated and cooled in a cylinder and the resulting expansion and contraction is used to move a piston and produce useful mechanical work.\n\nThe term \"hot air engine\" specifically excludes any engine performing a thermodynamic cycle in which the working fluid undergoes a phase transition, such as the Rankine cycle. Also excluded are conventional internal combustion engines, in which heat is added to the working fluid by combustion of fuel within the working cylinder. Continuous combustion types, such as George Brayton's Ready Motor and the related gas turbine, could be seen as borderline cases.\n\nThe expansive property of heated air was known to the ancients. Hero of Alexandria's \"Pneumatica\" describes devices that might be used to automatically open temple doors when a fire was lit on a sacrificial altar. Devices called hot air engines, or simply \"air engines\", have been recorded from as early as 1699. In 1699, Guillaume Amontons (1663–1705) presented, to the Royal Academy of Sciences in Paris, a report on his invention: a wheel that was made to turn by heat. The wheel was mounted vertically. Around the wheel's hub were water-filled chambers. Air-filled chambers on the wheel's rim were heated by a fire under one side of the wheel. The heated air expanded and, via tubes, forced water from one chamber to another, unbalancing the wheel and causing it to turn. \n\nSee:\n\nIt is likely that Robert Stirling's air engine of 1818, which incorporated his innovative \"Economiser\" (patented in 1816) was the first air engine put to practical work. The economiser, now known as the regenerator, stored heat from the hot portion of the engine as the air passed to the cold side, and released heat to the cooled air as it returned to the hot side. This innovation improved the efficiency of Stirling's engine and should be present in any air engine that is properly called a Stirling engine.\n\nStirling patented a second hot air engine, together with his brother James, in 1827. They inverted the design so that the hot ends of the displacers were underneath the machinery and they added a compressed air pump so the air within could be increased in pressure to around 20 atmospheres. It is stated by Chambers to have been unsuccessful, owing to mechanical defects and to “the unforeseen accumulation of heat, not fully extracted by the sieves or sin ill passages in the cool part of the regenerator, of which the external surface was not sufficiently large to throw off the unrecovered heat when the engine was working with highly compressed air.” \n\nParkinson and Crossley, English patent, 1828 came up with their own hot air engine. In this engine the air-chamber is partly exposed, by submergence in cold water, to external cold, and its upper portion is heated by steam. An internal vessel moves up and down in this chamber, and in so doing displaces the air, alternately exposing it to the hot and cold influences of the cold water and the hot steam, changing its temperature and expansive condition. The fluctuations cause the reciprocation of a piston in a cylinder to whose ends the air-chamber is alternately connected.\n\nIn 1829 Arnott patented his air expansion machine where a fire is placed on a grate near the bottom of a close cylinder, and the cylinder is full of fresh air recently admitted. A loose piston is pulled upwards so that all the air in the cylinder above will be made to pass by a tube through the fire, and will receive an increased elasticity tending to the expansion or increase of volume, which the fire is capable of giving it.\n\nHe is followed the next year (1830) by Captain Ericsson who patented his second hot air engine. The specification describes it more particularly, as consisting of a “circular chamber, in which a cone is made to revolve on a shaft or axis by means of leaves or wings, alternately exposed to the pressure of steam; these wings or leaves being made to work through slits or openings of a circular plane, which revolves obliquely to, and is thereby kept in contact with the side of the cone.”\n\nEricsson built his third hot air engine (the caloric engine) in 1833 which excited so much interest a few years ago in England; and which, if it should be brought into practical operation, will prove the most important mechanical invention ever conceived by the human mind, and one that will confer greater benefits on civilized life than any that has ever preceded it. For the object of it is the production of mechanical power by the agency of heat, at an expenditure of fuel so exceedingly small, that man will have an almost unlimited mechanical force at his command, in regions where fuel may now be said hardly to exist.\n\n1838 sees the patent of Franchot hot air engine, certainly the hot air engine that was best following the Carnot requirements.\n\nSo far all these air engines have been unsuccessful, but the technology was maturing. In 1842, James Stirling, the brother of Robert, build the famous Dundee Stirling Engine. This one at least lasted 2-3 years but then was discontinued due to improper technical contrivances.\nHot air engines is a story of trials and errors, and it took another 20 years before hot air engines could be used on an industrial scale. The first reliable hot air engines were built by Shaw, Roper, Ericsson. Several thousands of them were built. \n\nA hot air engine thermodynamic cycle can (ideally) be made out of 3 or more processes (typical 4). The processes can be any of these:\n\nSome examples (not all hot air cycles, as defined above) are as follows:\nYet another example is the Vuilleumier cycle.\n\n"}
{"id": "37470383", "url": "https://en.wikipedia.org/wiki?curid=37470383", "title": "Input lag", "text": "Input lag\n\nThe general use of the phrase ‘input lag’ is used to describe the delay between an electrical input from the point of stimulation to action, for example, pressing a button and seeing the event after as little as 1/24th of a second.\n\nIn video games, input lag is either the delay between the television or monitor receiving a signal and it being displayed on the screen (see display lag below), or the delay between pressing a button and seeing the game react.\n\nIn electronic hardware development, input lag is the delay between an electronic input signal being generated (pressing a button as above) and processed (I/O ports have been read, and memory updated to reflect the state of the input). In this field, the phenomena detailed below, are referred to as Output Lag. More formally, the terms 'Input Latency' and 'Output Latency' are also used. Due to gamers being largely unaware of the phenomenon of input lag at this low level, the terms input lag and output lag became confused over time, and the above description, where input and output lag are combined into a singular phenomenon known as input lag, has become popular.\n\nOverall, the correct terminology is clear - A delay between a physical input occurring (e.g., I/O pin voltage change) and it being processed electronically (I/O pins read by processor and memory registers updated to reflect the state of the pins) is input lag, and a delay between an electronic output being sent (e.g., memory register set to reflect the desired state of an output) and it being processed into a physically observable phenomenon (e.g., memory register read and I/O pin voltage modified accordingly), is output lag. In short, Input Lag occurs on input devices, Output Lag occurs on output devices.\n\nThe following are descriptions based on the colloquial use of the term, as used by gamers.\n\nThe potential causes for \"input lag\"- according to the second definition- are described below (steps which have negligible contributions to the input lag have been omitted). Each step in the process increases \"input lag\", however the net result may be unnoticeable if the overall \"input lag\" is low enough.\n\nFor wired controllers, this lag is negligible. For wireless controllers, opinions vary as to the significance of this lag. Some people claim to notice extra lag when using a wireless controller, while other people claim that the 4–8 milliseconds of lag is negligible.\n\nSince the game requires information on the location of other players, there is sometimes a delay as this information travels over the network. This occurs in games where the input signals are \"held\" for several frames (to allow time for the data to arrive at every player's console/PC) before being used to render the next frame. At 25 FPS, holding 4 frames adds to the overall input lag. However, very few modern online games use this method. The view angle of every modern AAA shooter game is completely unaffected by network lag, for example. In addition, lag compensating code makes classification a complex issue.\n\nA videogame console or PC will send out a new frame once it has finished performing the necessary calculations to create it. The rate at which this is achieved is measured with the frame rate. Using common 60 Hz monitor as an example, the maximum theoretical frame rate is 60 FPS (frames per second), which means the minimum theoretical input lag for the overall system is . Theoretical maximum FPS is usually limited by the video monitor, since the game cannot display more frames per second than the monitor's refresh rate (with exception of turning vertical sync (v-sync) off which in turn causes graphical artifacts). In situations where the CPU and/or GPU load is high, FPS can drop below the monitors refresh rate.\n\nThis is the lag caused by the television or monitor (which is also called \"input lag\" by the first definition above, but \"output lag' by the second definition). Image processing (such as upscaling, 100 Hz, motion smoothing, edge smoothing) takes time and therefore adds some degree of input lag. It is generally considered that input lag of a television below is not noticeable. Once the frame has been processed, the final step is the pixel response time for the pixel to display the correct colour for the new frame.\n\nTesting has found that overall \"input lag\" (from controller input to display response) times of approximately are distracting to the user. It also appears that (excluding the monitor/television display lag) is an average response time and the most sensitive games (fighting games, first person shooters and rhythm games) achieve response times of (excluding display lag).\n\nInput Lag Test: TVs from 2016 + 2017 Dein-Fernseher.de\n"}
{"id": "24027962", "url": "https://en.wikipedia.org/wiki?curid=24027962", "title": "Jet aerators", "text": "Jet aerators\n\nJet aerators are applied across a wide range of water, wastewater and biosolids treatment applications. Their primary purpose is to transfer oxygen to the liquid or sludge. A jet aerator works through aspirating technology by simultaneously introducing large volumes of high kinetic energy liquid and air through one or more jet nozzles. The high velocity liquid exits the inner, primary jet and rapidly mixes with the incoming air in the outer jet. This intense mixing and high degree of turbulence in the gas/liquid cloud travels outward from the jet along the basin floor prior to the vertical rise of the gas bubble column to the liquid surface.\n\nIn most industrial wastewater and biosolids applications jet aerators exhibit superior oxygen transfer efficiency compared to other aeration technologies. The hydrodynamic conditions within the jet and fine bubble cloud produces continuous surface renewal at the gas/liquid interface resulting in higher alpha factors. This results in superior process oxygen transfer performance in the presence of surfactants, extracelluar enzymes and high MLS concentrations.\n\nJet aerators do not require any external air source (i.e. compressor), except for the surrounding atmosphere. Jet aerators can be installed either as submersible units or piped through the tank wall using an external dry-installed chopper pump to feed the aspirating ejector(s). Jet aerators are easily configured into any basin geometry including circular, rectangular, looped reactors and sloped wall basins. Jet aerators are ideally suited for deep tank processes. The jet oxidation ditch is an example of technology innovation where the combination of a deeper basin design, bottom to top mixing and conservation of momentum combines to make a very efficient treatment process. In this and other applications the independent control of oxygen transfer and mixing is a valuable feature for both process control and energy savings.\n"}
{"id": "35085698", "url": "https://en.wikipedia.org/wiki?curid=35085698", "title": "Journal of Electronic Defense", "text": "Journal of Electronic Defense\n\nThe Journal of Electronic Defense (\"JED\") is a monthly magazine of military science covering developments in the fields of electronic warfare and signals intelligence. \n\n\"JED\" was established in 1978 and is published by the Association of Old Crows, an international aerospace and electronic warfare association based in the United States. The magazine is the official media outlet of the Association. The magazine has its headquarters in Gainesville, Florida.\n"}
{"id": "21546542", "url": "https://en.wikipedia.org/wiki?curid=21546542", "title": "Magic tee", "text": "Magic tee\n\nA magic tee (or magic T or hybrid tee) is a hybrid or 3 dB coupler used in microwave systems. It is an alternative to the rat-race coupler. In contrast to the rat-race, the three-dimensional structure of the magic tee makes it less readily constructed in planar technologies such as microstrip or stripline.\n\nThe magic tee was originally developed in World War II, and first published by W. A. Tyrell of Bell Labs in a 1947 IRE paper. Robert L. Kyhl and Bob Dicke independently created magic tees around the same time.\n\nThe magic tee is a combination of E and H plane tees. Arm 3 forms an H-plane tee with arms 1 and 2. Arm 4 forms an E-plane tee with arms 1 and 2. Arms 1 and 2 are sometimes called the \"side\" or \"collinear\" arms. Port 3 is called the \"H-plane port\", and is also called the \"Σ port\", \"sum port\" or the \"P-port\" (for \"parallel\"). Port 4 is the \"E-plane port\", and is also called the \"Δ port\", \"difference port\", or \"S-port\" (for \"series\"). There is no one single established convention regarding the numbering of the ports.\n\nTo function correctly, the magic tee must incorporate an internal matching structure. This structure typically consists of a post inside the H-plane tee and an inductive iris inside the E-plane limb, though many alternative structures have been proposed. Dependence on the matching structure means that the magic tee will only work over a limited frequency band.\n\nThe name \"magic tee\" is derived from the way in which power is divided among the various ports. A signal injected into the H-plane port will be divided equally between ports 1 and 2, and will be in phase. A signal injected into the E-plane port will also be divided equally between ports 1 and 2, but will be 180 degrees out of phase. If signals are fed in through ports 1 and 2, they are added at the H-plane port and subtracted at the E-plane port. Thus, with the ports numbered as shown, and to within a phase factor, the full scattering matrix for an ideal magic tee is\n\n(the signs of the elements in the fourth row and fourth column of this matrix may be reversed, depending on the polarity assumed for port 4).\n\nIf, by means of a suitable internal structure, the E-plane (difference) and H-plane (sum) ports are simultaneously matched, then by symmetry, reciprocity and conservation of energy it may be shown that the two collinear ports are also matched, and are 'magically' isolated from each other.\n\nThe E-field of the dominant mode in each port is perpendicular to the broad wall of the waveguide. The signals in the E-plane and H-plane ports therefore have orthogonal polarizations, and so (considering the symmetry of the structure) there can be no communication between these two ports.\n\nFor a signal entering the H-plane port, a well-designed matching structure will prevent any of the power in the signal being reflected back out of the same port. As there can be no communication with the E-plane port, and again considering the symmetry of the structure, then\nthe power in this signal must be divided equally between the two collinear ports.\n\nSimilarly for the E-plane port, if the matching structure eliminates any reflection from this port, then the power entering it must be divided equally between the two collinear ports.\n\nNow by reciprocity, the coupling between any pair of ports is the same in either direction (the scattering matrix is symmetric). So if the H-plane port is matched, then half the power entering either one of the collinear ports will leave by the H-plane port. If the E-plane port is also matched, then half power will leave by the E-plane port. In this circumstance, there is no power 'left over' either to be reflected out of the first collinear port or to be transmitted to the other collinear port. Despite apparently being in direct communication with each other, the two collinear ports are 'magically' isolated.\n\nThe isolation between the E-plane and H-plane ports is wide-band and is as perfect as is the symmetry of the device. The isolation between the collinear ports is however limited by the performance of the matching structure.\n"}
{"id": "50758127", "url": "https://en.wikipedia.org/wiki?curid=50758127", "title": "Market urbanism", "text": "Market urbanism\n\nMarket urbanism is an urban policy theory which advocates for the liberalization of urban planning and transportation policy. Market urbanists espouse a wide array of views, but generally they support ideas and programs such as loosening urban land use and zoning regulations, implementing congestion pricing on public roads, and applying classical liberal thought to urban policy issues. The term was coined by Adam Hengels, founder of the \"Market Urbanism\" blog. \n\nIn July 2017, Scott Beyer, a roving urban affairs journalist who writes for Forbes, started a media company called \"The Market Urbanism Report\". The website is designed to advance Market Urbanism policy ideas in U.S. cities.\n\nIn July 2018, Facebook group New Urbanist Memes for Transit-Oriented Teens moderator Emily Orenstein reported there was \"an uptick of 'market urbanist types' following coverage on CityLab.\"\n\nIn an article in the \"National Review\" (a conservative editorial magazine) in August 2018, Jibran Khan comments that \"...[Kamala] Harris' bill [which would give tax subsidies to renters] could compound the problems facing renters, by reducing the political pressure — currently building from both left and right in California via the “market urbanism” movement — to tackle the lack of housing.\"\n"}
{"id": "535805", "url": "https://en.wikipedia.org/wiki?curid=535805", "title": "Mat", "text": "Mat\n\nA mat is a piece of fabric material that generally is placed on a floor or other flat surface. Mats serve a range of purposes including:\n\n\nA car mat is designed to help protect a vehicle's carpeted floors. One major use of a car mat is to keep mud, sand and snow from contacting the carpeted floors. Some require fixation points to ensure they remain fixed in position.\n\nCarpet mats and rubber mats differ in a number of ways. Carpet mats are generally tufted and have a rubberised anti-slip backing. On the other hand, rubber car mats are heavy duty and higher durability. While some car mats are the plain colour of rubber, many contain branded company logos, cartoon characters or advertisements. Some are in textile form of carpet material. They can also come in a wide range of colours. The terms universal and custom fit mats differentiate between floor mats that will fit a multitude of different cars and those that are specifically designed to fit only one chassis.\n\n\"Anti-fatigue mats\" are designed to help a person who is working in a standing position for prolonged periods of time. Most anti-fatigue matting is a combination of an ergonomic pattern and a cushioning underlay. The cushioning causes constant subconscious balance checks and micro movements that stimulate blood flow through the legs and lower extremities. This results in better circulation and less fatigue. The cushioning underlay insulates the feet from the hard surface, cold floors, vibrations, moisture and sound. Their unique design encourages the user to make continual micro-movements which provides a wealth of health benefits, such as minimizing back pain, foot pain, weariness, stress, etc. Anti-fatigue mats are one of the approaches to prevent injuries, caused by working in a standing position. In a study at the Center of Ergonomics at the University of Michigan in 1987, ergonomist Mark Redfern concluded that different standing surfaces can have dramatic effects on physical fatigue. Workers who stood on anti-fatigue mats were able to reduce the level of fatigue and discomfort by as much as 50%. This type of mat is recommended by Occupational Safety and Health Administration. The range of common materials for manufacturing anti-fatigue mats includes vinyl, wood, PVC tubing, rubber, PVC closed cell foam, polypropylene, nitrile rubber. Anti-fatigue mats were initially used in factories and production lines where staff has to stand for the majority of their working shifts.\n\nAnti-fatigue mats come in various types and materials for industrial or commercial applications for a variety of workplace conditions that exist as well as the variety of workplace designs from individual work benches, to large assembly lines or complex manufacturing work stations. Work place environments can vary from dry areas to wet or extremely oily areas. Plus specialized industries may need additional properties such as fire retardant matting for welding, static dissipative matting for electrostatic discharge (ESD) protection, anti-microbial for food industry applications.\n\nToday, this type of ergonomic mat is commonly used during trade shows for floor covering, in hospitals and clinics during surgeries to cover the floor near surgical tables to minimize surgeons fatigue resulted from continuous standing. Also these mats are used in housekeeping, especially for kitchen floors to alleviate fatigue during cooking.\n\nThe purpose of a clean room mat is to keep sterile the areas that require ultimate protection from dirt, bacteria and any contamination brought from outside. Clean room mats are tacky, sticky, non slip mats that possess multiple layers of clean film that effectively capture dirt and dust from foot traffic and wheels. Peel-off mats are made up of multiple sheets of polyethylene film coated with acrylic adhesive that traps particles. Each layer peels off to reveal a new clean surface. The adhesive backing prevents microbial growth and contamination. Mats used outside clean rooms and laboratories are designed to withhold foreign pollution elements. This goal is achieved by a sticky surface that serves as a barrier for debris, dirt and dust adhered to shoe soles. Clean room sticky mats can contain two defensive barriers: the first part is a carpet itself, while the second part is sticky surface mat. Another mat type to be used to protect rooms from pollution is sanitizing foot bath floor mats. The mat itself is a small bath that contains sanitizing liquid. The foot bath bottom is covered with pliable rubber scrapers for effective cleaning of footwear soles while the liquid disinfects them.\n\nAn alternative clean room mat is one made from polymeric material. Polymeric products are made from a blend of pure polymeric compounds and have a three- to five-year life cycle. When a polymeric surface becomes dirty, operators can clean it with a sponge and a mop with detergent and dry the surface with a squeegee. This quick cleaning process can be incorporated into the facility’s regular wet-clean cycle.\n\nThe mats differ by composition:\n\n\n\nMatting or floor covering or rugs is any of many coarse woven or plaited fibrous materials used for covering floors or furniture, for hanging as screens, for wrapping up heavy merchandise and for other miscellaneous purposes. In the United Kingdom, under the name of \"coir\" matting, a large amount of a coarse kind of carpet is made from coconut fibre; and the same material, as well as strips of cane, manila hemp, various grasses and rushes, is largely employed in various forms for making doormats. Large quantities of the coconut fibre are woven in heavy looms, then cut up into various sizes, and finally bound round the edges by a kind of rope made from the same material. The mats may be of one colour only, or they may be made of different colours and in different designs. Sometimes the names of institutions are introduced into the mats.\nDue to the silky nature and tensile strength, jute mats or mattings have started being used as floor covering or doormats, runners and in different forms. Jute floor coverings consist of woven and tufted and piled carpets. Jute Mats and mattings starting from 1 m width to 6 m width and of continuous length are easily being woven in Southern parts of India, in solid and fancy shades, and in different weaves such as boucle, Panama, anlold herringbone. Jute mats and rugs are made on both powerlooms and handlooms in large volumes in Kerala, India. Indian jute mattings / rugs are being widely used in USA and European countries, due to its soft nature. Jute can be easily bleached, colored or printed, similar to textile fibres, with eco-friendly dyes & chemicals. Hand-knotted Jute carpets & mattings are also being made from Kerala, India.\n\nAnother type of mat is made exclusively from the above-mentioned coir rope by arranging alternate layers in sinuous and straight paths, and then stitching the parts together. It is also largely used for the outer covering of ships' fenders. Perforated and otherwise prepared rubber, as well as wire-woven material, are also largely utilized for door and floor mats. Matting of various kinds is very extensively employed throughout India for floor coverings, the bottoms of bedsteads, fans and fly-flaps, etc.; and a considerable export trade in such manufactures is carried on. The materials used are numerous; but the principal substances are straw, the bulrushes Typha elephantina and Typha angustifolia, leaves of the date palm (Phoenix sylvestris), of the dwarf palm (Chamaerops Ritchiana), of the Palmyra palm (Borassus flabelliformis), of the coconut palm (Cocos nucifera) and of the screw pine (Pandanus odoratissimus), the munja or munj grass (Saccharum Munja) and allied grasses, and the mat grasses \"Cyperus textilis\" and \"Cyperus pangorei.\n\nThe mats made from Cyperus pangorei (Korai in Tamil) are called \"Korai paai\" in Tamil and can be found widely in the households of Tamil Nadu, usually in the size 6 feet by 3 feet. They are usually dyed in colors of bright red, green or purple, resulting in patterns. These mats differ in their levels of flexibility, fineness and price. Pattamadai paai (named after the region Pattamadai, near Tirunelveli) is generally considered the finest \"paai\". Many of these Indian grass-mats are examples of elegant design, and the colors in which they are woven are rich, harmonious and effective. Mats made from Vandavasi are also famed and used commonly. These days, along with these natural grass mats, one can also find plastic mats, which are easier to maintain and are cheaper. This class of work obtains in India, Japan and other Eastern countries. Vast quantities of coarse matting used for packing furniture, heavy and coarse goods, flax and other plants, etc., are made in Russia from the bast or inner bark of the lime tree. This industry centres in the great forest governments of Viatka, Nizhniy-Novgorod, Kostroma, Kazan, Perm and Simbirsk.\n\nQuality floor mats improve indoor air quality (IAQ) and safety in commercial and residential applications. Studies have shown that most toxic chemicals that end up inside a home are tracked in on people's shoes. A well-used door mat can trap and hold dirt and allergens, preventing their spread into the rest of the building, significantly improving IAQ and reducing the need for extensive cleaning. Additionally many floor mats are resistant to welding sparks and can keep employees from slipping on industrial lubricants or water.\n\nFloor mats also provide safe surfaces on which to walk, preventing slips and falls that cause injury and liability damages. Anti-slip mats are now required in many areas to ensure maximum protection for both employees and customers. Specialized anti-slip mats are now available that provide extra resistance to the chemicals and grease that are sometimes found in industrial and food service settings.\n\nCustom made anti-fatigue mats are also used in work areas where employees are required to stand for long periods of time. Employers have found that much muscle strain and injury endured by workers is caused by improper flooring conditions. Non-supportive surfaces cause fatigue and foot, back and neck pain due to impaired circulation. Anti-fatigue mats were shown to improve worker productivity by reducing the number of sick-days and injuries sustained by workers whose mobility would otherwise be restricted. Mat has a good taste.\n\n"}
{"id": "4216775", "url": "https://en.wikipedia.org/wiki?curid=4216775", "title": "Michael Chorost", "text": "Michael Chorost\n\nMichael Chorost (born December 26, 1964) is an American book author, essayist, and public speaker. Born with severe loss of hearing due to rubella, his hearing was partially restored with a cochlear implant in 2001 and he had his other ear implanted in 2007. \n\nHe wrote a memoir of the experience, titled \"Rebuilt: How Becoming Part Computer Made Me More Human\" (Houghton Mifflin, 2005, ). Its paperback version has a different subtitle, \"Rebuilt: My Journey Back to the Hearing World\", . In August 2006 \"Rebuilt\" won the PEN/USA Book Award for Creative Nonfiction. \n\nHis second book, \"World Wide Mind: The Coming Integration of Humanity, Machines, and the Internet\", , was published by Free Press on February 15, 2011.\n\nDr. Chorost has published in \"Wired\", \"New Scientist\", \"Astronomy Now\", \"The Futurist\", \"The Scientist\", \"Technology Review\", the \"Chronicle of Higher Education\", and \"SKY\". He co-wrote a PBS television show titled \"The 22nd Century\" which aired in January 2007. He was a member of the San Francisco Writers Workshop.\n\nDr. Chorost is frequently interviewed as an authority on cochlear implants and neurally controlled prosthetics by national media such as \"PBS Newshour\", the \"New York Times\" and \"The Economist\". \n\nHe lectures frequently at universities, conferences, corporations, and organizations for the deaf.\n\nBorn in New Jersey and educated at Brown University and the University of Texas at Austin, he now lives in Washington, DC with his wife and two cats. \n\n"}
{"id": "31287472", "url": "https://en.wikipedia.org/wiki?curid=31287472", "title": "Multrum", "text": "Multrum\n\nA multrum is a large composting vessel, predominantly meant to decompose toilet excreta but also other organic residue. It is originally a composite word consisting of \"multna\" which means moldering or composting in Swedish and \"rum\" which is the Swedish word for room. A multrum has over several decades become a noun and has come to mean any large composting chamber connected to a toilet. This should not be confused with Clivus multrum which is a proprietary product. In Scandinavia there are many kinds of composting toilet multrums like Mullis, CompostEra besides Clivus Multrum.\n"}
{"id": "18599930", "url": "https://en.wikipedia.org/wiki?curid=18599930", "title": "Neurobioengineering", "text": "Neurobioengineering\n\nIn 1995, professor Massimo Grattarola of the Biophysics and Electrical Engineering Department (DIBE) at the University of Genoa, in Genoa, Italy, created an undergraduate and graduate program named neurobioengineering (also referred to as neuroengineering). The program was designed to amalgamate anthropomorphic robotics, artificial intelligence, bioelectronics, electrical engineering, molecular biology, physics, and medicine, into a single program with the aim of developing advanced bio-compatible neuro-prosthetic implants (man-machine interfacing) for a variety applications (e.g. nervous system interaction with artificial limbs, central and peripheral nervous system implants, directional neural grafting (neural engineering), electron harvesting from biological processes to power implanted devices, neural arrays cultured on CMOS sensors, etc.).\n\nNeurobioengineering deals with the study and application of bio-compatible neuro-prosthetic implants, neural sensors and interfaces with the nervous system.\n\nThe goal of that branch is to develop a bio-artificial brain of cultured neurons capable of replicating human behaviour in an artificial robotic system. The European Union F.E.T. funded the neurobioengineering department to pursue this ambitious project.\n\nThe neurobioengineering program spawned numerous journal publications by the departmental scientific pioneers (Dr. Marco Bove, Dr. Sergio Martinoia, Dr. Renato Zaccaria, and many others) and a university course textbook Bioelectronics, MOSFETS, Biosensors and Neurons published by Massimo Grattarola. Members of the department were collaborating with researchers throughout the European Union, Japan and North America. Students from throughout the European Union and Canada enrolled in the program.\n\nAt the time (1995), a small number of universities offered specialized bioelectronics and/or implantable neuro-prostheses-related research (but no specialized undergraduate programs) internationally for example: University of Utah, MIT, UCSF (United States); McGill University (Canada), among others.\n\nIn 2003, following professor Grattarola's death, the University of Genova announced Europe's first neurobioengineering conference in his honour. The First European School on Neuroengineering 'Massimo Grattarola' was also founded in 2004, with the goal of establishing a long-term formal educational program to foster future pioneers in neurobioengineering.\n\nIn a state of transition, the neurobioengineering department renamed itself the neuroengineering and bio-nanotechnology group in 2005. By 2008, the core researchers of the original department had formalized the educational process into a formal long-term program at the University of Genova named School of Neuroengineering, fulfilling Massimo Grattarola's original ambitions, offering degrees in Humanoid Technologies.\n\nThe current and former neurobioengineering (or neuroengineering) students continue his research interests throughout Europe and North America, some of whom have established related businesses, or hold positions of authority in neuroscience/biomedical institutions worldwide.\n\n"}
{"id": "42182598", "url": "https://en.wikipedia.org/wiki?curid=42182598", "title": "Neville Moray", "text": "Neville Moray\n\nNeville Moray (May 27, 1935 – 15 December 2017) was a British/Canadian academic and Professor at the Department of Psychology of the University of Surrey, known from his 1959 research of the cocktail party effect.\n\nMoray started studying medicine at Worcester College, Oxford in 1953. He received his BA in Philosophy, Psychology and Physiology in 1957, his MA in Psychology in 1959, and his D.Phil. in 1960 all from Oxford University in the United Kingdom.\n\nMoray started his academic career as Assistant Lecturer in Psychology at the University of Hull in 1959. The next year he moved to the University of Sheffield and became lecturer in Psychology, and senior lecturer in 1966. In 1970 he moved to Canada where he became Associate Professor at the University of Toronto, and Professor of Psychology in 1972. In 1974 he moved back to Scotland to become Professor of Psychology at the University of Stirling and Chairman of Department of Psychology since 1977. In 1981 back at University of Toronto he was appointed Professor of Industrial Engineering, and member of its Institute of Nuclear Engineering since 1984. In 1988 he moved to the United States to the University of Illinois at Urbana–Champaign, where until 1995 he had a joint appointment as Professor at the Departments of Mechanical and Industrial Engineering; the Department of Psychology; and the Institute of Aviation. From 1995 to 1997 he was a Professor at the University of Valenciennes in France, and from 1997 until his retirement in 2001 he was a Professor of Psychology at University of Surrey in the UK.\n\nMoray was elected Fellow of the Human Factors and Ergonomics Society in 1991; Fellow of the Ergonomics Society in 1998; Fellow of the Institute of Ergonomics and Human Factors; and Fellow of the International Ergonomics Association. The International Ergonomics Association awarded him the Ergonomics Development Award and the President's Award in 2000. In 2000, Stirling University inaugurated the annual prize for best Combined Honours Student, to be named the \"Neville Moray Prize.\"\n\nHe died on 15 December 2017 at the age of 82.\n\nMoray became known for his scientific contributions to the cocktail party effect, which became his major research interest for about two decades. This effect concerns the phenomenon of being able to focus one's auditory attention on a particular stimulus while filtering out a range of other stimuli. The effect was first defined and named \"the cocktail party problem\" by Colin Cherry in 1953. Cherry found that participants were able to detect their name from the unattended channel, the channel they were not shadowing. Moray build his research using Cherry's shadowing task. He was able to conclude that almost none of the rejected messages were able to penetrate the block set up, except subjectively \"important\" messages.\n\nNeville Moray used Cherry's shadowed dichotic listening task in his 1959 research and was able to conclude that almost none of the rejected messages were able to penetrate the block set up, except subjectively \"important\" messages. Personal names, taboo language, and backward language are the \"subjectively\" important messages that have been found to date. Moray's 1959 study found a 33% detection rate for personal names, which revealed that participants sometimes notice their name in an ignored auditory channel. This ability to selectively attend to one's own name has been found in infants as young as five months of age and appears to be fully developed by thirteen months of age.\n\nRochelle S. Newman in a 2005 study found that five-month-old infants listened longer to their names when the target voice was 10 dB, but not 5 dB more intense than the background noise. Nine-month-olds also failed at 5 dB, but thirteen-month-olds succeeded. This success in recognizing one's own name in the unattended channel can be explained using Cherry's initial report on dichotic shadowing. Cherry found that the verbal content of the message in the unattended channel was completely blocked, so that the words were treated as merely sounds. This allows the subject to know that something has stimulated the ear whose message is rejected. It may be thought of as a general warning signal, that a sound has occurred to which the subject might need to respond.\n\n\nArticles, a selection:\n\n"}
{"id": "168701", "url": "https://en.wikipedia.org/wiki?curid=168701", "title": "Open Database Connectivity", "text": "Open Database Connectivity\n\nIn computing, Open Database Connectivity (ODBC) is a standard application programming interface (API) for accessing database management systems (DBMS). The designers of ODBC aimed to make it independent of database systems and operating systems. An application written using ODBC can be ported to other platforms, both on the client and server side, with few changes to the data access code.\n\nODBC accomplishes DBMS independence by using an \"ODBC driver\" as a translation layer between the application and the DBMS. The application uses ODBC functions through an \"ODBC driver manager\" with which it is linked, and the driver passes the query to the DBMS. An ODBC driver can be thought of as analogous to a printer driver or other driver, providing a standard set of functions for the application to use, and implementing DBMS-specific functionality. An application that can use ODBC is referred to as \"ODBC-compliant\". Any ODBC-compliant application can access any DBMS for which a driver is installed. Drivers exist for all major DBMSs, many other data sources like address book systems and Microsoft Excel, and even for text or comma-separated values (CSV) files.\n\nODBC was originally developed by Microsoft and Simba Technologies during the early 1990s, and became the basis for the Call Level Interface (CLI) standardized by SQL Access Group in the Unix and mainframe field. ODBC retained several features that were removed as part of the CLI effort. Full ODBC was later ported back to those platforms, and became a de facto standard considerably better known than CLI. The CLI remains similar to ODBC, and applications can be ported from one platform to the other with few changes.\n\nThe introduction of the mainframe-based relational database during the 1970s led to a proliferation of data access methods. Generally these systems operated together with a simple command processor that allowed users to type in English-like commands, and receive output. The best-known examples are SQL from IBM and QUEL from the Ingres project. These systems may or may not allow other applications to access the data directly, and those that did use a wide variety of methodologies. The introduction of SQL aimed to solve the problem of \"language\" standardization, although substantial differences in implementation remained.\n\nAlso, since the SQL language had only rudimentary programming features, users often wanted to use SQL within a program written in another language, say Fortran or C. This led to the concept of Embedded SQL, which allowed SQL code to be \"embedded\" within another language. For instance, a SQL statement like codice_1 could be inserted as text within C source code, and during compiling it would be converted into a custom format that directly called a function within a library that would pass the statement into the SQL system. Results returned from the statements would be interpreted back into C data formats like codice_2 using similar library code.\n\nThere were several problems with the Embedded SQL approach. Like the different varieties of SQL, the Embedded SQLs that used them varied widely, not only from platform to platform, but even across languages on one platform – a system that allowed calls into IBM's DB2 would look very different from one that called into their own SQL/DS. Another key problem to the Embedded SQL concept was that the SQL code could only be changed in the program's source code, so that even small changes to the query required considerable programmer effort to modify. The SQL market referred to this as \"static SQL\", versus \"dynamic SQL\" which could be changed at any time, like the command-line interfaces that shipped with almost all SQL systems, or a programming interface that left the SQL as plain text until it was called. Dynamic SQL systems became a major focus for SQL vendors during the 1980s.\n\nOlder mainframe databases, and the newer microcomputer based systems that were based on them, generally did not have a SQL-like command processor between the user and the database engine. Instead, the data was accessed directly by the program – a programming library in the case of large mainframe systems, or a command line interface or interactive forms system in the case of dBASE and similar applications. Data from dBASE could not generally be accessed directly by other programs running on the machine. Those programs may be given a way to access this data, often through libraries, but it would not work with any other database engine, or even different databases in the same engine. In effect, all such systems were static, which presented considerable problems.\n\nBy the mid-1980s the rapid improvement in microcomputers, and especially the introduction of the graphical user interface and data-rich application programs like Lotus 1-2-3 led to an increasing interest in using personal computers as the client-side platform of choice in client-server computing. Under this model, large mainframes and minicomputers would be used primarily to serve up data over local area networks to microcomputers that would interpret, display and manipulate that data. For this model to work, a data access standard was a requirement – in the mainframe field it was highly likely that all of the computers in a shop were from one vendor and clients were computer terminals talking directly to them, but in the micro field there was no such standardization and any client might access any server using any networking system.\n\nBy the late 1980s there were several efforts underway to provide an abstraction layer for this purpose. Some of these were mainframe related, designed to allow programs running on those machines to translate between the variety of SQL's and provide a single common interface which could then be called by other mainframe or microcomputer programs. These solutions included IBM's Distributed Relational Database Architecture (DRDA) and Apple Computer's Data Access Language. Much more common, however, were systems that ran entirely on microcomputers, including a complete protocol stack that included any required networking or file translation support.\n\nOne of the early examples of such a system was Lotus Development's DataLens, initially known as Blueprint. Blueprint, developed for 1-2-3, supported a variety of data sources, including SQL/DS, DB2, FOCUS and a variety of similar mainframe systems, as well as microcomputer systems like dBase and the early Microsoft/Ashton-Tate efforts that would eventually develop into Microsoft SQL Server. Unlike the later ODBC, Blueprint was a purely code-based system, lacking anything approximating a command language like SQL. Instead, programmers used data structures to store the query information, constructing a query by linking many of these structures together. Lotus referred to these compound structures as \"query trees\".\n\nAround the same time, an industry team including members from Sybase (Tom Haggin), Tandem Computers (Jim Gray & Rao Yendluri) and Microsoft (Kyle G) were working on a standardized dynamic SQL concept. Much of the system was based on Sybase's DB-Library system, with the Sybase-specific sections removed and several additions to support other platforms. DB-Library was aided by an industry-wide move from library systems that were tightly linked to a specific language, to library systems that were provided by the operating system and required the languages on that platform to conform to its standards. This meant that a single library could be used with (potentially) any programming language on a given platform.\n\nThe first draft of the \"Microsoft Data Access API\" was published in April 1989, about the same time as Lotus' announcement of Blueprint. In spite of Blueprint's great lead – it was running when MSDA was still a paper project – Lotus eventually joined the MSDA efforts as it became clear that SQL would become the de facto database standard. After considerable industry input, in the summer of 1989 the standard became \"SQL Connectivity\" (\"SQLC\").\n\nIn 1988 several vendors, mostly from the Unix and database communities, formed the SQL Access Group (SAG) in an effort to produce a single basic standard for the SQL language. At the first meeting there was considerable debate over whether or not the effort should work solely on the SQL language itself, or attempt a wider standardization which included a dynamic SQL language-embedding system as well, what they called a Call Level Interface (CLI). While attending the meeting with an early draft of what was then still known as MS Data Access, Kyle Geiger of Microsoft invited Jeff Balboni and Larry Barnes of Digital Equipment Corporation (DEC) to join the SQLC meetings as well. SQLC was a potential solution to the call for the CLI, which was being led by DEC.\n\nThe new SQLC \"gang of four\", MS, Tandem, DEC and Sybase, brought an updated version of SQLC to the next SAG meeting in June 1990. The SAG responded by opening the standard effort to any competing design, but of the many proposals, only Oracle Corp had a system that presented serious competition. In the end, SQLC won the votes and became the draft standard, but only after large portions of the API were removed – the standards document was trimmed from 120 pages to 50 during this time. It was also during this period that the name Call Level Interface was formally adopted. In 1995 SQL/CLI became part of the international SQL standard, ISO/IEC 9075-3.<ref name=\"ISO/IEC 9075-3\">ISO/IEC 9075-3 – Information technology – Database languages – SQL – Part 3: Call-Level Interface (SQL/CLI)</ref> The SAG itself was taken over by the X/Open group in 1996, and, over time, became part of The Open Group's Common Application Environment.\n\nMS continued working with the original SQLC standard, retaining many of the advanced features that were removed from the CLI version. These included features like scrollable cursors, and metadata information queries. The commands in the API were split into groups; the Core group was identical to the CLI, the Level 1 extensions were commands that would be easy to implement in drivers, while Level 2 commands contained the more advanced features like cursors. A proposed standard was released in December 1991, and industry input was gathered and worked into the system through 1992, resulting in yet another name change to \"ODBC\".\n\nDuring this time, Microsoft was in the midst of developing their Jet database system. Jet combined three primary subsystems; an ISAM-based database engine (also named \"Jet\", confusingly), a C-based interface allowing applications to access that data, and a selection of driver dynamic-link libraries (DLL) that allowed the same C interface to redirect input and output to other ISAM-based databases, like Paradox and xBase. Jet allowed using one set of calls to access common microcomputer databases in a fashion similar to Blueprint, by then renamed DataLens. However, Jet did not use SQL; like DataLens, the interface was in C and consisted of data structures and function calls.\n\nThe SAG standardization efforts presented an opportunity for Microsoft to adapt their Jet system to the new CLI standard. This would not only make Windows a premier platform for CLI development, but also allow users to use SQL to access both Jet and other databases as well. What was missing was the SQL parser that could convert those calls from their text form into the C-interface used in Jet. To solve this, MS partnered with PageAhead Software to use their existing query processor, SIMBA. SIMBA was used as a parser above Jet's C library, turning Jet into an SQL database. And because Jet could forward those C-based calls to other databases, this also allowed SIMBA to query other systems. Microsoft included drivers for Excel to turn its spreadsheet documents into SQL-accessible database tables.\n\nODBC 1.0 was released in September 1992. At the time, there was little direct support for SQL databases (versus ISAM), and early drivers were noted for poor performance. Some of this was unavoidable due to the path that the calls took through the Jet-based stack; ODBC calls to SQL databases were first converted from Simba Technologies's SQL dialect to Jet's internal C-based format, then passed to a driver for conversion back into SQL calls for the database. Digital Equipment and Oracle both contracted Simba Technologies to develop drivers for their databases as well.\n\nCirca 1993, OpenLink Software shipped one of the first independently developed third-party ODBC drivers, for the PROGRESS DBMS, and soon followed with their UDBC (a cross-platform API equivalent of ODBC and the SAG/CLI) SDK and associated drivers for PROGRESS, Sybase, Oracle, and other DBMS, for use on Unix-like OS (AIX, HP-UX, Solaris, Linux, etc.), VMS, Windows NT, OS/2, and other OS.\n\nMeanwhile, the CLI standard effort dragged on, and it was not until March 1995 that the definitive version was finalized. By then, Microsoft had already granted Visigenic Software a source code license to develop ODBC on non-Windows platforms. Visigenic ported ODBC to a wide variety of Unix platforms, where ODBC quickly became the de facto standard. \"Real\" CLI is rare today. The two systems remain similar, and many applications can be ported from ODBC to CLI with few or no changes.\n\nOver time, database vendors took over the driver interfaces and provided direct links to their products. Skipping the intermediate conversions to and from Jet or similar wrappers often resulted in higher performance. However, by then Microsoft had changed focus to their OLE DB concept (recently reinstated ), which provided direct access to a wider variety of data sources from address books to text files. Several new systems followed which further turned their attention from ODBC, including ActiveX Data Objects (ADO) and ADO.net, which interacted more or less with ODBC over their lifetimes.\n\nAs Microsoft turned its attention away from working directly on ODBC, the Unix field was increasingly embracing it. This was propelled by two changes within the market, the introduction of graphical user interfaces (GUIs) like GNOME that provided a need to access these sources in non-text form, and the emergence of open software database systems like PostgreSQL and MySQL, initially under Unix. The later adoption of ODBC by Apple for using the standard Unix-side iODBC package Mac OS X 10.2 (Jaguar) (which OpenLink Software had been independently providing for Mac OS X 10.0 and even Mac OS 9 since 2001) further cemented ODBC as the standard for cross-platform data access.\n\nSun Microsystems used the ODBC system as the basis for their own open standard, Java Database Connectivity (JDBC). In most ways, JDBC can be considered a version of ODBC for the programming language Java instead of C. JDBC-to-ODBC \"bridges\" allow Java-based programs to access data sources through ODBC drivers on platforms lacking a native JDBC driver, although these are now relatively rare. Inversely, ODBC-to-JDBC bridges allow C-based programs to access data sources through JDBC drivers on platforms or from databases lacking suitable ODBC drivers.\n\nODBC remains in wide use today, with drivers available for most platforms and most databases. It is not uncommon to find ODBC drivers for database engines that are meant to be embedded, like SQLite, as a way to allow existing tools to act as front-ends to these engines for testing and debugging.\n\nHowever, the rise of thin client computing using HTML as an intermediate format has reduced the need for ODBC. Many web development platforms contain direct links to target databases – MySQL being very common. In these scenarios, there is no direct client-side access nor multiple client software systems to support; everything goes through the programmer-supplied HTML application. The virtualization that ODBC offers is no longer a strong requirement, and development of ODBC is no longer as active as it once was.\n\nVersion history:\n\n\nODBC is based on the device driver model, where the driver encapsulates the logic needed to convert a standard set of commands and functions into the specific calls required by the underlying system. For instance, a printer driver presents a standard set of printing commands, the API, to applications using the printing system. Calls made to those APIs are converted by the driver into the format used by the actual hardware, say PostScript or PCL.\n\nIn the case of ODBC, the drivers encapsulate many functions that can be broken down into several broad categories. One set of functions is primarily concerned with finding, connecting to and disconnecting from the DBMS that driver talks to. A second set is used to send SQL commands from the ODBC system to the DBMS, converting or interpreting any commands that are not supported internally. For instance, a DBMS that does not support cursors can emulate this functionality in the driver. Finally, another set of commands, mostly used internally, is used to convert data from the DBMS's internal formats to a set of standardized ODBC formats, which are based on the C language formats.\n\nAn ODBC driver enables an ODBC-compliant application to use a \"data source\", normally a DBMS. Some non-DBMS drivers exist, for such data sources as CSV files, by implementing a small DBMS inside the driver itself. ODBC drivers exist for most DBMSs, including Oracle, PostgreSQL, MySQL, Microsoft SQL Server (but not for the Compact aka CE edition), Sybase ASE, SAP HANA and DB2. Because different technologies have different capabilities, most ODBC drivers do not implement all functionality defined in the ODBC standard. Some drivers offer extra functionality not defined by the standard.\n\nDevice drivers are normally enumerated, set up and managed by a separate Manager layer, which may provide additional functionality. For instance, printing systems often include functionality to provide spooling functionality on top of the drivers, providing print spooling for any supported printer.\n\nIn ODBC the Driver Manager (DM) provides these features. The DM can enumerate the installed drivers and present this as a list, often in a GUI-based form.\n\nBut more important to the operation of the ODBC system is the DM's concept of a \"Data Source Name\" (DSN). DSNs collect additional information needed to connect to a \"specific\" data source, versus the DBMS itself. For instance, the same MySQL driver can be used to connect to any MySQL server, but the connection information to connect to a local private server is different from the information needed to connect to an internet-hosted public server. The DSN stores this information in a standardized format, and the DM provides this to the driver during connection requests. The DM also includes functionality to present a list of DSNs using human readable names, and to select them at run-time to connect to different resources.\n\nThe DM also includes the ability to save partially complete DSN's, with code and logic to ask the user for any missing information at runtime. For instance, a DSN can be created without a required password. When an ODBC application attempts to connect to the DBMS using this DSN, the system will pause and ask the user to provide the password before continuing. This frees the application developer from having to create this sort of code, as well as having to know which questions to ask. All of this is included in the driver and the DSNs.\n\nA \"bridge\" is a special kind of driver: a driver that uses another driver-based technology.\n\nAn ODBC-JDBC bridge consists of an \"ODBC\" driver which uses the services of a JDBC driver to connect to a database. This driver translates ODBC function-calls into JDBC method-calls. Programmers usually use such a bridge when they lack an ODBC driver for some database but have access to a JDBC driver. Examples: OpenLink ODBC-JDBC Bridge, SequeLink ODBC-JDBC Bridge.\n\nA JDBC-ODBC bridge consists of a JDBC driver which employs an ODBC driver to connect to a target database. This driver translates JDBC method calls into ODBC function calls. Programmers usually use such a bridge when a given database lacks a JDBC driver, but is accessible through an ODBC driver. Sun Microsystems included one such bridge in the JVM, but viewed it as a stop-gap measure while few JDBC drivers existed. (The built-in JDBC-ODBC bridge was dropped from the JVM in Java 8.) Sun never intended its bridge for production environments, and generally recommended against its use. independent data-access vendors deliver JDBC-ODBC bridges which support current standards for both mechanisms, and which far outperform the JVM built-in. Examples: OpenLink JDBC-ODBC Bridge, SequeLink JDBC-ODBC Bridge.\n\nAn OLE DB-ODBC bridge consists of an OLE DB Provider which uses the services of an ODBC driver to connect to a target database. This provider translates OLE DB method calls into ODBC function calls. Programmers usually use such a bridge when a given database lacks an OLE DB provider, but is accessible through an ODBC driver. Microsoft ships one, MSDASQL.DLL, as part of the MDAC system component bundle, together with other database drivers, to simplify development in COM-aware languages (e.g. Visual Basic). Third parties have also developed such, notably OpenLink Software whose 64-bit OLE DB Provider for ODBC Data Sources filled the gap when Microsoft initially deprecated this bridge for their 64-bit OS. (Microsoft later relented, and 64-bit Windows starting with Windows Server 2008 and Windows Vista SP1 have shipped with a 64-bit version of MSDASQL.) Examples: OpenLink OLEDB-ODBC Bridge, SequeLink OLEDB-ODBC Bridge.\n\nAn ADO.NET-ODBC bridge consists of an ADO.NET Provider which uses the services of an ODBC driver to connect to a target database. This provider translates ADO.NET method calls into ODBC function calls. Programmers usually use such a bridge when a given database lacks an ADO.NET provider, but is accessible through an ODBC driver. Microsoft ships one as part of the MDAC system component bundle, together with other database drivers, to simplify development in C#. Third parties have also developed such. Examples: OpenLink ADO.NET-ODBC Bridge, SequeLink ADO.NET-ODBC Bridge.\n\n\n\n"}
{"id": "14431229", "url": "https://en.wikipedia.org/wiki?curid=14431229", "title": "Outline of nanotechnology", "text": "Outline of nanotechnology\n\nThe following outline is provided as an overview of and topical guide to nanotechnology:\n\nNanotechnology is science, engineering, and technology conducted at the nanoscale, which is about 1 to 100 nanometers.\n\n\n\n\n\nImplications of nanotechnology\n\n\n\nFullerene – any molecule composed entirely of carbon, in the form of a hollow sphere, ellipsoid, or tube. Fullerene spheres and tubes have applications in nanotechnology.\n\nNanoparticle –\n\nNanomedicine –\n\nMolecular self-assembly – \n\nNanoelectronics –\n\nMolecular electronics –\n\nNanolithography –\n\nMolecular nanotechnology –\n\n\nMicroscopy –\n\nList of nanotechnology organizations\n\n\n\n\n"}
{"id": "860879", "url": "https://en.wikipedia.org/wiki?curid=860879", "title": "Oxford Instruments", "text": "Oxford Instruments\n\nOxford Instruments plc is a United Kingdom manufacturing and research company that designs and manufactures tools and systems for industry and research. The company is headquartered in Abingdon, Oxfordshire, England, with sites in the United Kingdom, United States, Europe, and Asia. It is listed on the London Stock Exchange.\n\nThe company was founded by Sir Martin Wood in 1959 with help from his wife Audrey Wood (Lady Wood) to manufacture superconducting magnets for use in scientific research, starting in his garden shed in Northmoor Road, Oxford, England. It was the first substantial commercial spin-out company from the University of Oxford and was first listed on the London Stock Exchange in 1983.\n\nIt had a pioneering role in the development of magnetic resonance imaging, providing the first superconducting magnets for this application. The first commercial MRI whole body scanner was manufactured at its Osney Mead factory in Oxford in 1980 for installation at Hammersmith Hospital, London. Further innovations included the development of active shielding, whereby fringe fields hazardous to pacemaker wearers, causing difficulty and expense in siting, were virtually eliminated. Oxford Instruments was not able to capitalise on these inventions itself, granting royalty-free license to Philips and General Electric whilst developing a joint venture with Siemens in 1989: this was dissolved in 2004.\n\n"}
{"id": "22036575", "url": "https://en.wikipedia.org/wiki?curid=22036575", "title": "ParAccel", "text": "ParAccel\n\nParAccel, Inc. was a California-based software company. \nIt provided a database management system designed for advanced analytics for business intelligence. ParAccel was acquired by Actian in April 2013.\n\nParAccel was a venture-backed company focused on developing software for data analysis.\nIt acquired some intellectual property from the company XPrime, which ended operations in 2005.\nIt was officially incorporated in February 2006, founded by Barry Zane who became chief technology officer, and was first funded by angel investors.\nIn August 2006 the first series of venture capital came from Mohr Davidow Ventures, Bay Partners and Tao Venture Partners.\nIn 2007 the company was based in San Diego, California, with an office in Ann Arbor, Michigan. David J. Ehrlich was chief executive, and Bruce Scott, vice president of engineering.\nIn November 2007, a second round of $20 million included previous investors and was led by Walden Ventures.\nIn December the company opened an office in Cupertino, California (part of Silicon Valley).\n\nA third round of $22 million in June 2009 was led by Menlo Ventures. In January 2010 Mark Lockareff replaced Ehrlich as interim chief executive.\nIn March 2010 the \"Wall Street Journal\" listed ParAccel in a list of venture backed companies that included Solyndra.\nA result from the TPC-H benchmark from the Transaction Processing Performance Council in April 2010 had record performance at 1 TB data size using VMware. Charles W. Berger was appointed chairman and CEO in September 2010.\n\nBy early 2011 many of its competitors had been acquired.\nDuring its July 2011 funding round, existing investors were led by Amazon.com.\nIn December 2012, the Amazon Redshift database service was announced (and generally available in early 2013) using ParAccel technology.\n\nParAccel was based in California with offices in Campbell and San Diego.\nCompetitors included Greenplum (from Pivotal), EXASOL, Vertica (from Hewlett-Packard), Netezza (from IBM), Oracle Corporation, and Teradata (including its Aster Data Systems technology).\nParAccel was acquired by Actian in April 2013.\nBerger left at that time to become CEO of Extreme Networks.\n\nIn 2008 ParAccel offered two different products: Amigo and Maverick. Amigo was designed to accelerate queries directed at an existing data warehouse while leaving the data warehouse as the database of record. In contrast Maverick was designed as a stand-alone data store.\nParAccel discontinued Amigo in favor of the stand-alone offering which evolved into the ParAccel Analytic Database (PADB).\n\nThe ParAccel Analytic Database was a parallel relational database system using a shared-nothing architecture with a columnar orientation, adaptive compression, memory-centric design.\nParAccel's DBMS engine is built for analytics, initially based on PostgreSQL. ParAccel began phasing in a new optimizer (Omne) in release 2.0 and made significant changes to Omne in subsequent releases (3.1 released in June 2011).\nParAccel implements compiled queries, and a proprietary interconnect protocol for inter-node communications. It integrated with storage area network technologies such as those from EMC Corporation.\n\nParAccel offered on-demand integration (ODI) modules for analytics and data outside of the ParAccel Analytic Platform.\n"}
{"id": "1327379", "url": "https://en.wikipedia.org/wiki?curid=1327379", "title": "Quantum wire", "text": "Quantum wire\n\nIn mesoscopic physics, a quantum wire is an electrically conducting wire in which quantum effects influence the transport properties. Usually such effects appear in the dimension of nanometers, so they are also referred to as nanowires.\n\nIf the diameter of a wire is sufficiently small, electrons will experience quantum confinement in the transverse direction. As a result, their transverse energy will be limited to a series of discrete values. One consequence of this quantization is that the classical formula for calculating the electrical resistance of a wire,\n\nformula_1,\nis not valid for quantum wires (where formula_2 is the material's resistivity, formula_3 is the length, and formula_4 is the cross-sectional area of the wire).\n\nInstead, an exact calculation of the transverse energies of the confined electrons has to be performed to calculate a wire's resistance. Following from the quantization of electron energy, the electrical conductance (the inverse of the resistance) is found to be quantized in multiples of formula_5, where formula_6 is the electron charge and formula_7 is the Planck constant. The factor of two arises from spin degeneracy. A single ballistic quantum channel (i.e. with no internal scattering) has a conductance equal to this quantum of conductance. The conductance is lower than this value in the presence of internal scattering.\n\nThe importance of the quantization is inversely proportional to the diameter of the nanowire for a given material. From material to material, it is dependent on the electronic properties, especially on the effective mass of the electrons. Physically, this means that it will depend on how conduction electrons interact with the atoms within a given material. In practice, semiconductors can show clear conductance quantization for large wire transverse dimensions (~100 nm) because the electronic modes due to confinement are spatially extended. As a result, their Fermi wavelengths are large and thus they have low energy separations. This means that they can only be resolved at cryogenic temperatures (within a few degrees of absolute zero) where the thermal energy is lower than the inter-mode energy separation.\n\nFor metals, quantization corresponding to the lowest energy states is only observed for atomic wires. Their corresponding wavelength being thus extremely small they have a very large energy separation which makes resistance quantization observable even at room temperature.\n\nThe carbon nanotube is an example of a quantum wire. A metallic single-walled carbon nanotube that is sufficiently short to exhibit no internal scattering (ballistic transport) has a conductance that approaches two times the conductance quantum, formula_8. The factor of two arises because carbon nanotubes have two spatial channels.\n\nThe structure of a nanotube strongly affects its electrical properties. For a given (\"n\",\"m\") nanotube, if \"n\" = \"m\", the nanotube is metallic; if \"n\" − \"m\" is a multiple of 3, then the nanotube is semiconducting with a very small band gap, otherwise the nanotube is a moderate semiconductor. Thus all armchair (\"n\" = \"m\") nanotubes are metallic, and nanotubes (6,4), (9,1), etc. are semiconducting.\n\nNanowires can be used for transistors. Transistors are used widely as fundamental building element in today's electronic circuits. One of the key challenges of building future transistors is ensuring good gate control over the channel. Due to the high aspect ratio, if the gate dielectric is wrapped around the nanowire channel, we can get good electrostatic control of channel potential, thereby turning the transistor on and off efficiently.\n\nIn an analogous way to field-effect transistor (FET) devices in which the modulation of conductance (flow of electrons/holes) in the device, is controlled by electrostatic potential variation (gate-electrode) of the charge density in the conduction channel, the methodology of a Bio/Chem-FET is based on the detection of the local change in charge density, or so-called “field effect”, that characterizes the recognition event between a target molecule and the surface receptor.\n\nThis change in the surface potential influences the Chem-FET device exactly as a ‘gate’ voltage does, leading to a detectable and measurable change in the device conduction.\n\n"}
{"id": "25076317", "url": "https://en.wikipedia.org/wiki?curid=25076317", "title": "Radiator reflector", "text": "Radiator reflector\n\nA radiator reflector is a thin sheet or foil applied to the wall behind, and closely spaced from, a domestic heating radiator. The intention is to reduce heat losses into the wall by reflecting radiant heat away from the wall. It is a form of radiant barrier and is intended to reduce energy losses and hence decrease fuel expenditure. \nStudies based both on modelling and experiments have demonstrated modest improvements in energy losses through the walls of houses through this method. Harris shows that plain aluminium foil was only \"marginally\" less effective than a propriety shaped foil that claimed to avoid temperature stratification. He reports that \"reductions in the overall energy consumption of the [test] room of up to 6% were recorded by installing [pain] foil behind a radiator, while the heat loss through the area of wall immediately behind the radiator fell to less than 30% of the original value\". In his 3m cubed test room with a 1 x 0.5 m radiator and walls of average U value 0.44 W/mK, he found that for a radiator temperature of 43 °C the heat flux through the wall behind the radiator reduced from 7.1 to 3.1 W/m. Note that the average heat loss in the room was not reduced by such a large percentage as only part of the surface of the room was covered by radiators. He concludes that \"in the test room used, which is the size of a small bedroom or sitting room, the total energy saved in a typical year in the UK’s climate would be of the order of 60 kWh\".\n\nBaldinelli et al. support these findings and note that their \"results show how the performance of the reflecting panel depends strictly on the insulation level of the external wall facing the radiator; more specifically, efficiency increases when the thermal resistance decreases, reaching energy savings of up to 8.8% in worst insulation conditions.\"\n\nAlthough the foils are termed \"reflectors\", they do not have much effect on radiated heat or its reflection. As radiators work at a relatively low temperature, the Stefan–Boltzmann law means that they are weak radiators of heat. Most heat from a domestic radiator is as convection currents of heated air. Where a reflector foil also has some insulating ability against conduction (i.e. losses through the wall), it may have some useful effect. This is most pronounced when the wall itself has poor insulation performance: in a wall constructed to modern standards of insulation, even this effect may be reduced to a negligible benefit.\n\nThe effect of placing a 10mm combined insulation and reflection behind radiators is about the same as that of 15mm insulation without a reflective layer. When the wall thickness behind the radiator is at minimum 1980 German standards this will reduce total heat losses of a building by about 4%. For a (by 1980s standards) well-insulated building heat losses can be reduced by about 1.6%.\n\nIt is widely believed that a literal radiator reflector of ordinary aluminium kitchen foil is useful. This highly reflective foil is used with the shiny side facing towards the back of the radiator. However, as little of the radiator's heat is released by radiation anyway, there is little advantage in reducing losses to it. There is also a risk that reflectors made from kitchen foil may soon become inefficient, as aluminium oxidizes very quickly and then loses its reflective quality.\n\nA more effective DIY radiator reflector is a thin insulating layer (against conduction) of a lightweight insulator such as expanded polystyrene foam veneer or 3mm polyethylene foam, as used for laminate flooring underlay.\n\nThere are only two radiator reflectors approved for use in the UK Government's Carbon Emission Reduction Target (CERT) Scheme administered by Ofgem (the UK Regulator of energy companies) – Radflek and Heatkeeper (also called Novitherm).\n\n"}
{"id": "41176434", "url": "https://en.wikipedia.org/wiki?curid=41176434", "title": "Radio spectrum scope", "text": "Radio spectrum scope\n\nThe radio spectrum scope (also radio panoramic receiver, panoramic adapter, pan receiver, pan adapter, panadapter, panoramic radio spectroscope, panoramoscope, panalyzor and band scope) was invented by Marcel Wallace and measures the magnitude of an input signal versus frequency within one or more radio bands - e.g. shortwave bands. A spectrum scope is normally a lot cheaper than a spectrum analyzer, because the aim is not high quality frequency resolution - nor high quality signal strength measurements.\n\nThe spectrum scope use can be to:\n\n"}
{"id": "10887247", "url": "https://en.wikipedia.org/wiki?curid=10887247", "title": "Rolling pin", "text": "Rolling pin\n\nA rolling pin is a cylindrical food preparation utensil used to shape and flatten dough. Two styles of rolling pin are found: rollers and rods. Roller types consists of a thick cylinder with small handles at each end; rod type rolling pins are usually thin tapered batons. Rolling pins of different styles and materials offer advantages over another, as they are used for different tasks in cooking and baking.\n\n\n\nRolling pins come in a variety of sizes, shapes and materials including glass, ceramic, acrylic, bakelite, copper, brass, aluminium, silicone, wood, stainless steel, marble, and plastic. Some are hollow and are able to be filled with cold or warm water to better roll a desired food. Marble rolling pins are often cooled in a refrigerator for maintaining a cold dough while making puff pastry.\n\nAn angry housewife wielding a rolling pin as a weapon is a common cliché in humour, as, for example, in the English comic strip \"Andy Capp\".\n\n"}
{"id": "36299990", "url": "https://en.wikipedia.org/wiki?curid=36299990", "title": "Scarabeo 9", "text": "Scarabeo 9\n\nScarabeo 9 is a Frigstad D90-type ultra deepwater 6th generation semi-submersible drilling rig. It is owned and operated by Saipem. It was named by Anna Tatka, the wife of Pietro Franco Tali, CEO of Saipem. The vessel is registered in Nassau, Bahamas.\n\n\"Scarabeo 9\" is one of the largest offshore drilling rigs in the world. It is the first (and as of 2012 the only) Frigstad Engineering developed Frigstad D90 design rig ever built. The rig is able to operate at the water depth up to , which is classified by the oil industry as \"ultra-deepwater\", and its drilling depth is . The water depth still suitable for its operations is twice as much as for \"Deepwater Horizon\". The drilling equipment was provided by the Norwegian engineering company Aker Solutions.\n\n\"Scarabeo 9\" has a length of and a breadth of . Its gross tonnage is 36,863, dead weight tonnage 23,965, and net tonnage 11,059 tonnes. The vessel is powered by eight Wärtsilä 12V32 diesel engines. It is equipped with two cranes.\n\nThe rig includes quarters for up to 200 workers. There is also a helicopter deck suitable for MI8, S61, and EH101 helicopters. The rig has been described by the industry sources as \"the latest technology for deepwater drilling operations.\"\n\n\"Scarabeo 9\" was ordered by Frigstad Offshore, a Singapore based offshore drilling rigs management service company. For this purpose, a special project company Frigstad Discoverer was established in 2006. The original name of the vessel was decided to be \"Frigstad Oslo\". In 2007, the project company was acquired by Italian engineering and offshore services provider company Saipem and it was decided to rename the rig \"Scarabeo 9\".\n\nThe rig cost US$750 million to build. It was constructed at the Yantai Raffles Shipyard in Yantai, China. The contract was signed on 5 April 2006, the keel was laid on 1 April 2008, and originally the construction was to be completed in September 2009. After several delays at the Yantai Raffles shipyard it was shipped to the Keppel FELS shipyard in Singapore for the final completion in 2010. The reason of delays was related to a number of orders which were carried out simultaneously at the Yantai Raffles shipyard. Changing shipyards caused additional cost of between US$70 and 100 million. Beside of the Keppel FELS shipyard also the SembCorp Marine's Jurong Shipyard in Singapore was considered for the completion works.\n\nOn its way to Singapore, a water leak occurred due to \"water raining into the tanks from the top that was not been drained out.\" The incident caused an extensive inspection to assure its seaworthiness. The main part of Keppel's work involved the completion and commissioning of marine and drilling systems on board.\n\nThe rig was delivered to Saipem on 25 August 2011. On her maiden voyage to Cuba, \"Scarabeo 9\" was escorted around the Cape of Good Hope by the Fairmount Marine owned tugboat \"Fairmount Glacier\".\n\nThe rig was specifically built for drilling in the waters of Cuba. It is compliant with the United States embargo against Cuba, which limits the amount of American technology that can be used in equipment used there, as less than 10% of its parts are American-made. The news agency Reuters has reported that only the blowout preventer of \"Scarabeo 9\" is manufactured in the United States. This is one of the most critical devices for well control during the offshore drilling as malfunctioning of the blowout preventer was one of the reasons of the Deepwater Horizon explosion. However, due to the United States embargo, the original equipment manufacturer is not allowed to provide spare parts or repair items for the blowout preventer.\n\nOn its way to Cuba, in Trinidad it was inspected by the United States Coast Guard and Interior Department (Bureau of Safety and Environmental Enforcement). According to the statement by the Bureau of Safety and Environmental Enforcement, the vessel is generally compliant with existing international and U.S. standards.\n\n\"Scarabeo 9\" drilled its first well on the Jagüey prospect in the North Cuba Basin for Repsol in the beginning of 2012. Provider of immediate well intervention and other well-related subsea services was Helix Energy Solutions Group. After drilling for Repsol, \"Scarabeo 9\" was contracted to drill for Petronas and Gazprom Neft on the Catoche field off the north coast of Pinar del Río Province, and for Petróleos de Venezuela on the Cabo de San Antonio prospect off the west coast of Cuba. Repsol has also contracted the rig for drilling in Brazil.\n"}
{"id": "47776440", "url": "https://en.wikipedia.org/wiki?curid=47776440", "title": "Scoubidou (tool)", "text": "Scoubidou (tool)\n\nA Scoubidou is a corkscrew-like tool that is used for the commercial harvesting of seaweed, whose invention is credited to Yves Colin in 1961. The device consists of an iron hook attached to a hydraulic arm. It superseded a common harvesting tool known as the guillotine shortly after its invention. The scoubidou is used primarily for harvesting \"Laminaria digitata\", a species used mainly for fertiliser.\n"}
{"id": "1513400", "url": "https://en.wikipedia.org/wiki?curid=1513400", "title": "Seine fishing", "text": "Seine fishing\n\nSeine ( ) fishing (or seine-haul fishing) is a method of fishing that employs a fishing net called a seine, that hangs vertically in the water with its bottom edge held down by weights and its top edge buoyed by floats. Seine nets can be deployed from the shore as a beach seine, or from a boat.\n\nBoats deploying seine nets are known as seiners. Two main types of seine net are deployed from seiners: \"purse seines\" and \"Danish seines\".\n\nThe word \"seine\" has origins in the Old French \"seigne\", the Latin \"sagena\", and the Greek σαγήνη \"sagēnē\" (a draw-net).\n\nSeines have been used widely in the past, including by stone age societies. For example, the Māori used large canoes to deploy seine nets which could be over a kilometer long. The nets were woven from green flax, with stone weights and light wood or gourd floats, and could require hundreds of men to haul.\nNative Americans on the Columbia River wove seine nets from spruce root fibers or wild grass, again using stones as weights. For floats they used sticks made of cedar which moved in a way which frightened the fish and helped keep them together.\n\nSeine nets are also well documented in ancient cultures in the Mediterranean region. They appear in Egyptian tomb paintings from 3000 BC. In ancient Roman literature, the poet Ovid makes many references to seine nets, including the use of cork floats and lead weights.\n\nA common type of seine is a purse seine, named such because along the bottom are a number of rings. A line (referred to as a purse-line) passes through all the rings, and when pulled, draws the rings close to one another, preventing the fish from \"sounding\", or swimming down to escape the net. This operation is similar to a traditional style purse, which has a drawstring.\nThe purse seine is a preferred technique for capturing fish species which school, or aggregate, close to the surface: sardines, mackerel, anchovies, herring, and certain species of tuna (schooling); and salmon soon before they swim up rivers and streams to spawn (aggregation). Boats equipped with purse seines are called purse seiners.\nPurse seine fishing can be a relatively sustainable [?] way of fishing, as it can result in smaller amounts of by-catch (unintentionally caught fish), especially when used to catch large species of fish (like herring or mackerel) that shoal tightly together. When used to catch fish that shoal together with other species, or when used in parallel with fish aggregating devices, the percentage of by-catch greatly increases.\n\nUse of purse seines is regulated by many countries. In Sri Lanka, using this type of net within 7 kilometers of the shore is illegal. However, they can be used in the deep sea, after obtaining permission from authorities. Purse seine fishing can have negative impacts on fish stocks because it can involve the bycatch of non-target species and it can put too much pressure on fish stocks.\n\nThe power block is a mechanized pulley used on some seiners to haul in the nets. According to the UN Food and Agriculture Organization, no single invention has contributed more to the success of purse seine net hauling than the power block.\n\nThe Puretic power block line was introduced in the 1950s and was the key factor in the mechanization of purse seining. The combination of these blocks with advances in fluid hydraulics and the new large synthetic nets changed the character of purse seine fishing. The original Puretic power block was driven by an endless rope from the warping head of a winch. Nowadays, power blocks are usually driven by hydraulic pumps powered by the main or auxiliary engine. Their rpm, pull and direction can be controlled remotely.\n\nA minimum of three people are required for power block seining; the skipper, skiff operator, and corkline stacker. In many operations a fourth person stacks the leadline, and often a fifth person stacks the web.\n\nIn certain parts of the western United States as well as Canada, specifically on the coast of British Columbia, drum seining is a method of seine fishing which was adopted in the late 1950s and is now used exclusively in that region.\n\nThe drum seine uses a horizontally mounted drum to haul and store the net instead of a power block. The net is pulled in over a roller, which spans the stern, and then passes through a spooling gear with upright rollers. The spooling gear is moved from side to side across the stern which allows the net to be guided and wound tightly on the drum.\n\nThere are several advantages to the drum seine over the power block. The net can be hauled very quickly - at more than twice the speed of using a power block, the net does not require overhead handling, and the process is therefore safer. The most important advantage is that the drum system can be operated with fewer deckhands. However, it is illegal to use a seine drum in the state of Alaska.\n\nA Danish seine, also occasionally called an anchor seine, consists of a conical net with two long wings with a bag where the fish collect. Drag lines extend from the wings, and are long so they can surround an area.\n\nA Danish seine is similar to a small trawl net, but the wire warps are much longer and there are no otter boards. The seine boat drags the warps and the net in a circle around the fish. The motion of the warps herds the fish into the central net.\n\nDanish seiner vessels are usually larger than purse seiners, though they are often accompanied by a smaller vessel. The drag lines are often stored on drums or coiled onto the deck by a coiling machine. A brightly coloured buoy, anchored as a \"marker\", serves as a fixed point when hauling the seine. A power block, usually mounted on a boom or a slewing deck crane, hauls the seine net.\n\nDanish seining works best on demersal fish which are either\nscattered on or close to the bottom of the sea, or are aggregated (schooling). They are used when there are flat but rough seabeds which are not trawlable. It is especially useful in northern regions, but not much in tropical to sub-tropical areas.\n\nThe net is deployed, with one end attached to an anchored dan (marker) buoy, by the main vessel, the seiner, or by a smaller auxiliary boat. A drag line is paid out, followed by a net wing. As the seiner sweeps in a big circle returning to the buoy, the deployment continues with the seine bag and the remaining wing, finishing with the remaining drag line. In this way a large area can be surrounded. Next the drag lines are hauled in using rope-coiling machines until the catch bag can be secured.\n\nThe seine netting method developed in Denmark. Scottish seining (\"fly dragging\") was a later modification. The original procedure is much the same as fly dragging except for the use of an anchored marker buoy when hauling, and closing the net and warps and net by winch.\n\n\n\n"}
{"id": "15349424", "url": "https://en.wikipedia.org/wiki?curid=15349424", "title": "Succade", "text": "Succade\n\nSuccade is the candied peel of any of the citrus species, especially from the citron or \"Citrus medica\" which is distinct with its extra-thick peel; in addition, the taste of the inner rind of the citron is less bitter than those of the other citrus. However, the term is also occasionally applied to the peel, root, or even entire fruit or vegetable like parsley, fennel and cucurbita which have a bitter taste and are boiled with sugar to get a special \"sweet and sour\" outcome.\n\nFruits which are commonly candied also include dates, cherries, pineapple, ginger, and the rind of watermelon.\n\nThe word succade is most probably derived from the Latin \"succidus\", but according to others the name may have originated from the Hebrew word sukkah, the temporary booth that Jews build on the holiday of Sukkot. The citron, known in Hebrew as an \"etrog\", is one of the symbolic Four Species used on that holiday. After Sukkot, some Jews candy the \"etrog\" or make marmalade from it.\n\nWhile the word \"Succade\" was widely used in German, today it is usually called \"Zitronat\". The French called it \" fruit glacé \" or \" fruit confit \", and is also known as \"candied fruit\" or \"crystallized fruit\". It has been around since the 14th century.\n\nThe \"citron\" fruits are halved, depulped, immersed in seawater or ordinary salt water to ferment for about 40 days, the brine being changed every two weeks; rinsed, and put in denser brine in wooden barrels for storage and for export. After partial de-salting and boiling to soften the peel, it is candied in a strong sugar solution. The candied peel is sun-dried or put up in jars for future use. Candying is traditionally done in Livorno, Italy, where they gathered the Corsican citrons from Corsica, the Diamante citrons from Liguria, Naples, Calabria and Sicily, and the Greek citron from Greece through Trieste.\n\nThe continual process of drenching the fruit in syrup causes the fruit to become saturated with sugar, thereby preventing the growth of spoilage microorganisms.\n\nIn the Eastern Bloc, ersatz succade and orangeat were prepared from unripe tomatoes and carrots respectively, as citrus fruits were scarce goods that could not be produced domestically.\n\nSuccade is sometimes used in cakes, as a filling for pound cake, oliebol, plum pudding, florentines, sfogliatelle, fruitcake or ontbijtkoek. It is also added to raisin bread. Succade is often combined with currants, raisins, cherries and hazelnuts. Candied citron peel is often coated in chocolate and eaten as confectionery. Chopped succade is also used in cannoli.\nRecipes vary from region to region, but the general principle is to boil the fruit, steep it in increasingly strong sugar solutions for a number of weeks, and then dry off any remaining water.\n\nThe high sugar content of finished glacé fruits inhibits the growth of microorganisms, and glacé fruits will keep for a number of years without any additional methods of preservation.\n\nFruits that hold up well to being preserved in this manner include cherries, plums, peaches, apricots, pears, starfruit, pineapple, apples, oranges, lemons, limes and clementines. Angelica is rarely seen in Western cooking except as a glacé fruit.\n\n\n"}
{"id": "7749319", "url": "https://en.wikipedia.org/wiki?curid=7749319", "title": "Tinapa", "text": "Tinapa\n\nTinapa, a Filipino term, is fish cooked or preserved through the process of smoking. It is a native delicacy in the Philippines and is often made from blackfin scad (\"Alepes melanoptera\", known locally as \"galunggong\"), or from milkfish, which is locally known as \"bangus\".\n\nThough \"tinapa\" is very much accessible in the country, it is also possible for one to cook it at home. \"Tinapa\" recipe mainly involves the process of washing the fishes and putting it in brine for an extended amount of time (usually 5 – 6 hours), air drying and finally smoking the fishes. The fish species which are commonly used for making \"tinapa\" could either be \"galunggong\" (scads) or \"bangus\" (milkfish). \n\nThe term \"tinapa\" can also refer to smoked meat which is also called \"tapa\".\n\n"}
