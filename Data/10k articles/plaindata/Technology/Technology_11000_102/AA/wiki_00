{"id": "40116346", "url": "https://en.wikipedia.org/wiki?curid=40116346", "title": "Alarm fatigue", "text": "Alarm fatigue\n\nAlarm fatigue or alert fatigue occurs when one is exposed to a large number of frequent alarms (alerts) and consequently becomes desensitized to them. Desensitization can lead to longer response times or missing important alarms. Alarm fatigue occurs in many industries, including construction and mining (where backup alarms sound so frequently that they often become senseless background noise), healthcare (where electronic monitors tracking clinical information such as vital signs and blood glucose sound alarms so frequently, and often for such minor reasons, that they lose the urgency and attention-grabbing power which they are intended to have), and the nuclear power industry. Like crying \"wolf\", such false alarms rob the valid alarms of the value they were intended to add (duly alerting people to danger).\n\nThe constant sounds of alarms and noises from blood pressure machines, ventilators and heart monitors causes a \"tuning out\" of the sounds due to the brain adjusting to stimulation. This issue is present in hospitals, in home care providers, nursing homes and other medical facilities alike. The Joint Commission's sentinel event reports 80 alarm-related deaths and 13 alarm-related serious injuries over the course of a few years. On April 18, 2013, the Joint Commission issued a sentinel event alert that highlighted the widespread problem of alarm fatigue in hospitals. Their recommendations included establishing guidelines to tailor alarm settings, training all members of the clinical team on safe use of alarms, and sharing information about alarm-related incidents. This alert resulted in designation in 2014 of clinical alarm system safety as a National Patient Safety Goal and it remains a goal in 2017. This Goal will force hospitals to establish alarm safety as a priority, identify the most important alarms, and establish policies to manage alarms by January 2016. ECRI Institute has listed alarms on its \"Top Ten Hazards List\" since 2007; in 2014 alarms was listed as the number one hazard.\n\nThe large number of alarms, especially of false alarms, has led to several unintended outcomes. Some consequences are disruption in patient care, desensitization to alarms, anxiety in hospital staff and patients, sleep deprivation and depressed immune systems, misuse of monitor equipment, and missed critical events. Some additional outcomes include workload increase, interference with communication, wasted time, patient dissatisfaction, and unnecessary investigations, referrals, or treatments.\n\nThere are many solutions proposed to reduce alarm fatigue in healthcare settings. One recommendation is to change alarm sounds to be softer and friendlier in order to improve identification of alarms by sound alone. Another recommendation is for clinicians to adjust the parameters and delays to alarms to match the patient’s traits and status. However, this directly trades sensitivity for specificity.\n\nAnother solution that has been proposed is to use centralized alarms. In this approach, alarms don't fire at the bedside, but fire at a central monitoring station where a trained healthcare provider evaluates each alarm and alerts the bedside clinician if they should intervene or evaluate the patient.\n\nBiomedical engineers may improve monitors by adjusting alarm algorithms. Currently, the alarm systems are very sensitive but not specific. This leads to a large amount of false alarms. The algorithms used can be adjusted to balance between sensitivity and specificity to limit the number of false alarms and still detect true deterioration.\n"}
{"id": "4256071", "url": "https://en.wikipedia.org/wiki?curid=4256071", "title": "Ammunition technician", "text": "Ammunition technician\n\nAn ammunition technician (AT) is a British Army soldier, formerly of the Royal Army Ordnance Corps but since 1993 of the Royal Logistic Corps, trained to inspect, repair, test, store, and modify all ammunition, guided missiles, and explosives used by the British Army. These technicians are also trained to use demolition to safely dispose of individual items of ammunition and explosives (EODs) or to conduct logistics disposal of bulk stocks of multi items. After gaining sufficient experience, those who show the appropriate qualities are given extra training to render safe improvised explosive devices (IEDs) by a process called improvised explosive device disposal. Experienced ATs may be called to give evidence as expert witnesses in criminal or coroner's courts in relation to ammunition or explosives or to EOD and IEDD duties.\n\nWithin the Royal Army Ordnance Corps, the receipt into service, storage, examination and issue of ammunition was possibly the oldest and most important function of the Corps. War could not be waged without ammunition, and to be waged successfully the ammunition had to be in every respect serviceable and dependable. The trade were previously called Ammunition Examiners (AE) and it was in the safeguarding of ammunition stockpiles during the wars that the Ammunition Examiner proved his worth. Promotion however was limited up to Warrant Officer Class 2 and at this stage the AE had to re-muster in the trade of RAOC Clerk in order to obtain higher rank. In 1948, the increased responsibility of the ammunition organization in Ordnance Services and in order to use the experience of these highly skilled tradesmen both as Warrant Officers and as Officers, the RAOC decided that promotion to WO1 would be introduced. RAOC Instruction No 466 introduced a new type of Quartermaster commission into the Royal Army Ordnance Corps to permit the Warrant Officer Ammunition Examiner being commissioned within the sphere of his normal employment on ammunition duties. These commissioned WOs would be called Assistant Inspecting Ordnance Officers (AIOOs).\n\nTraining was initially undertaken at Bramley in Hampshire at the School of Ammunition. However the school moved to Kineton in 1974. To qualify to attend the Ammunition Technician Class 2 course, a soldier must first pass a pre-select course, during which time they will be assessed for suitability for role. The pre-selection includes psychometric testing, leadership skills, problem solving, resource planning and numeracy tests.\n\nThe basic AT course is 9 months in duration, the first part of which is spent at The Royal Military College of Science. The instruction within the Defence College of Management and Technology forms the first phase of the 9-month course. The aim of the first part is to provide the scientific and technical basis for further training in ammunition and explosives. The syllabus is an integrated study of mathematics, ballistics, explosives and general chemistry, physics, metallurgy, electronics and the design of armoured vehicles, artillery and infantry weapons. Time is also spent on nuclear, biological and chemical weapons design and the related protection systems. The remainder of the course covers conventional land munitions, explosive demolitions, conventional munitions disposal, guided weapons and explosive theory and safety. The majority of the course takes place at the Defence EOD Munitions Search Training Regiment (DEMS Trg Regt). Training previously took place at the Defence EOD Munitions Search School Kineton, DEMSS Kineton, and before that the Army School of Ammunition.\n\nAfter 3 years gaining experience in trade, these technicians will be selected to return to Kineton to attend their Class 2 to Class 1 Upgrading Course, a 3-month course to broaden their technical knowledge and ability in munitions incident investigations, large scale demolitions and the disposal of chemical and biological munitions.\n\nThe Royal Logistic Corps Ammunition Technicians trained at Kineton are regarded throughout the world as the subject matter experts in the management of munitions and in Improvised Explosive Device (IED) disposal as a result of their combined experience in Palestine, Cyprus, Hong Kong, Northern Ireland, Iraq, Afghanistan, Aden, Malaya and other conflicts.\n\nCommissioned officers are known as Ammunition Technical Officers and for the Sandhurst entrant, they complete a 17-month technical course in the rank of Captain. ATs that become commissioned later in their service are also referred to as ATOs and will be granted the ato qualification by a testing board based on their experience, knowledge and competence.\n\nATs are employed within the Royal Logistic Corps of the British Army and are the technical experts in storing and processing ammunition in base depots or field storage sites at home or on operations where safety in storage is paramount to overall force protection. Being an Ammunition Technician calls for intelligence, clear thinking and analytical skills, a calm outlook coupled with excellent attention to detail, discipline and courage. ATs develop specialist skills to look after the MoDs global stockpiles of ammunition by carrying out surveillance tasks, testing, inspecting, maintaining and disposing of all sorts of ammunition, from bullet clips, anti-aircraft guided weapon systems, mines, mortars, tank rounds and aircraft bombs. The Ammunition Technician profession is not exclusive to the UK MoD but similar technical personnel also exist in the Canadian, Australian RAAOC, and New Zealand RNZALR. Ammunition Technicians trained at the Defence EOD Munitions Search School, Kineton also work on loan service engagements in a number of African, Far Eastern and Middle Eastern armed forces.\n\nIn the United Kingdom, bomb disposal is carried out in all three services (Royal Navy, Royal Air Force, and the Royal Logistic Corps and Royal Engineers of the British Army). The majority of counter terrorist bomb disposal and conventional munitions disposal activity is carried out by the Ammunition Technicians of the Royal Logistic Corps, the Royal Navy Clearance Divers deal with items below the high water mark and underwater tasks, the Royal Air Force deal with conventional and IED tasks on all RAF stations in the UK, as well as aircraft crash sites, weapons and other explosives; including ejection seats and stores release systems. The Royal Engineers deal with minefields, conventional, biological and chemical munitions and German WWII aircraft bombs that occasionally turn up.\n\nThe trade of Ammunition Technician is one of the most highly decorated professions in the British Army. The trade has been awarded 231 British gallantry awards as follows:\n\n\nIn addition, Ammunition Technicians and Ammunition Technical Officers have also received almost 200 Mention in Dispatches, King's or Queen's Commendations for Bravery.\n\nA further 100 awards of the MBE and BEM have been made to Ammunition Technicians for distinguished service within their trade.\n\nThese decorations have been awarded since 1940 and in places such as Aden, Afghanistan, Albania, Burma, Cyprus, Egypt, France, Germany, Gibraltar, Great Britain, Greece, Hong Kong, Iraq, Italy, Kuwait, Malaya, Malta, Northern Ireland, Pacific, Sicily and Yugoslavia.\n\nGeorge Cross\n\nGeorge Medal\n\nConspicuous Gallantry Cross\n\n\nMilitary Cross\n\n\nQueen's Gallantry Medal\nMBE for Gallantry\n\n\nBEM for Gallantry\n\nAlthough a highly decorated trade, the price of recognition for Ammunition Technicians and Ammunition Technical Officers has been high. The Ammunition Technician trade has lost a number of their colleagues killed in action whilst undertaking operational Explosive Ordnance Disposal tasks worldwide. Ammunition Technicians proudly have their own memorial at Marlborough Barracks, Temple Herdewyke in Warwickshire, the home of the trade.\n\nThe idea of a memorial was initiated by the senior Warrant Officers of the trade and supported by the Director of Land Service Ammunition and his staff. A RAOC EOD Memorial Working Party was set up and reported progress to the Director General of Ordnance Services. The memorial was funded by RAOC central funds, donations from industry and from private donations from individual technicians within the trade. There were also some significant donations in kind, all the bricks for the enclosure and surrounding wall were gifted by a local brickworks and the shrubbery was donated and planted by a local nursery. The memorial was designed by the Fine Arts Department of Coventry Polytechnic and sculpted from local sandstone. The memorial represents a single bomb disposal operator, dressed in the bomb suit and holding his protective helmet. This scene is one that every EOD operator will recognise as being the last few moments before donning the helmet and becoming totally shut off from the team and ready to make the longest walk into danger towards an explosive device. The memorial is enclosed behind double wrought iron gates bearing the trade badges of the ATO and AT. The gates lead into a walled garden with 2 stone benches. The walls bear grey slate tablets, each engraved with the name of those killed, the date and location of the incident. A small brass plaque records the award of posthumous gallantry medals or decorations.\n\nThe memorial was formally opened during a dedication service on 23 June 1991. The service of dedication was led by the Chaplain General to the Forces, The Reverend James Harkness OBE QHC MA with readings by WO1 (Staff Sergeant Major) B Johnson GC and Major General PWE Istead CB OBE GM, Representative Colonel Commandant, RAOC. Amongst the guests at the service where the widows and families of many of those whose names appear on the memorial. A parade and the annual service of remembrance by members of the units based at Kineton is held at the EOD Memorial on Remembrance Sunday in November each year.\n\nThe EOD memorial is dedicated to the fallen ATO's and AT's of The Royal Army Ordnance Corps and The Royal Logistic Corps who through their selfless commitment, have singularly taken the \"Longest Walk\" in the service of their country but sadly, have not returned. Members of the ammunition trade have been killed in Cyprus, Hong Kong, Northern Ireland, England, Iraq and Afghanistan, \"Sua Tela Tonanti / We Sustain\"\n\nIn Memoriam \n\nSSgt JA Culkin\n\nSSgt R Kirby\n\nSgt CC Workman\n\nCapt DA Stewardson\n\nWO2 CJL Davies\n\nSSgt CR Cracknell\n\nSgt AS Butcher\n\nMaj BC Calladene\n\nCapt JH Young\n\nWO 2 WJ Clark\n\nSgt RE Hills\n\nCapt BS Gritten\n\nSSgt RF Beckett\n\nCapt R Wilkinson\n\nSSgt AN Brammah\n\nSSgt VI Rose\n\nWO2 JA Maddocks\n\nSSgt JC Crawshaw\n\nWO2 E Garside\n\nCpl CW Brown\n\nSgt ME Walsh\n\nWO2 M O'Neill\n\nWO2 JR Howard\n\nSSgt CD Muir\n\nWO2 GJ O'Donnell GM+\n\nCapt DM Shepherd GM\n\nSSgt OSG Schmid GC\n\nCapt D Read\n\nSSgt BG Linley GM\n\nCapt LJ Head\n\n"}
{"id": "315968", "url": "https://en.wikipedia.org/wiki?curid=315968", "title": "Attitude indicator", "text": "Attitude indicator\n\nThe attitude indicator (AI), formerly known as the gyro horizon or artificial horizon, is a flight instrument that informs the pilot of the aircraft orientation relative to Earth's horizon, and gives an immediate indication of the smallest orientation change. The miniature aircraft and horizon bar mimic the relationship of the aircraft relative to the actual horizon. It is a primary instrument for flight in instrument meteorological conditions. \n\nThe essential components of the AI include a symbolic miniature aircraft mounted so that it appears to be flying relative to the horizon. An adjustment knob, to account for the pilot's line of vision, moves the aircraft up and down to align it against the horizon bar. The top half of the instrument is blue to represent the sky, while the bottom half is brown to represent the ground. The bank index at the top shows the aircraft angle of bank. Reference lines in the middle indicate the degree of pitch, up or down, relative to the horizon.\n\nMost Russian-built aircraft have a somewhat different design. The background display is colored as in a Western instrument, but moves up and down only to indicate pitch. A symbol representing the aircraft (which is fixed in a Western instrument) rolls left or right to indicate bank angle. A proposed hybrid version of the Western and Russian systems that would be more intuitive, never caught on.\n\nThe heart of the AI is a gyroscope (gyro) that spins at high speed, from either an electric motor, or through the action of a stream of air pushing on rotor vanes placed along its periphery. The stream of air is provided by a vacuum system, driven by a vacuum pump, or a venturi. Air passing through the narrowest portion of a venturi has lower air pressure through Bernoulli's Principle. The gyro is mounted in a double gimbal, which allows the aircraft to pitch and roll as the gyro stays vertically upright. A self-erecting mechanism, actuated by gravity, counteracts any precession due to bearing friction. It may take a few minutes for the erecting mechanism to bring the gyros to a vertical upright position after the aircraft engine is first powered up.\n\nAttitude indicators have mechanisms that keep the instrument level with respect to the direction of gravity. The instrument may develop small errors, in pitch or bank during extended periods of acceleration, deceleration, turns, or due to the earth curving underneath the plane on long trips. To start with, they often have slightly more weight in the bottom, so that when the aircraft is resting on the ground they will hang level and therefore they will be level when started. But once they are started, that pendulous weight in the bottom will not pull them level if they are out of level, but instead its pull will cause the gyro to precess. In order to let the gyro very slowly orient itself to the direction of gravity while in operation, the typical vacuum powered gyro has small pendulums on the rotor casing that partially cover air holes. When the gyro is out of level with respect to the direction of gravity, the pendulums will swing in the direction of gravity and either uncover or cover the holes, such that air is allowed or prevented from jetting out of the holes, and thereby applying a small force to orient the gyro towards the direction of gravity. Electric powered gyros may have different mechanisms to achieve a similar effect. \n\nOlder AIs were limited in the amount of pitch or roll that they would tolerate. Exceeding these limits would cause the gyro to tumble as the gyro housing contacted the gimbals, causing a precession force. Preventing this required a caging mechanism to lock the gyro if the pitch exceed 60° and the roll exceeded 100°. Modern AIs don't have this limitation and don't require a caging mechanism.\n\nAttitude indicators are also used on manned spacecraft and are called Flight Director Attitude Indicators (FDAI), where they indicate the craft's yaw angle (nose left or right), pitch (nose up or down), roll, and orbit relative to a fixed-space inertial reference frame from an Inertial Measurement Unit (IMU). The FDAI can be configured to use known positions relative to Earth or the stars, so that the engineers, scientists and astronauts can communicate the relative position, attitude, and orbit of the craft. \n\nAttitude and Heading Reference Systems (AHRS) are able to provide three-axis information based on ring laser gyroscopes, that can be shared with multiple devices in the aircraft, such as \"glass cockpit\" primary flight displays (PFDs). Rather than using a spinning gyroscope, modern AHRS use solid-state electronics, low-cost inertial sensors, rate gyros, and magnetometers.\n\nWith most AHRS systems, if an aircraft's AIs have failed there will be a standby AI located in the center of the instrument panel, where other standby basic instruments such as the airspeed indicator and altimeter are also available. These mostly mechanical standby instruments may be available even if the electronic flight instruments fail, though the standby attitude indicator may be electrically driven and will, after a short time, fail if its electrical power fails.\n\nThe Attitude Direction Indicator (ADI), or Flight Director Indicator (FDI), is an AI integrated with a Flight Director System (FDS). The ADI incorporates a computer that receives information from the navigation system, such as the AHRS, and processes this information to provide the pilot with a 3-D flight trajectory cue to maintain a desired path. The cue takes the form of V steering bars. The aircraft is represented by a delta symbol and the pilot flies the aircraft so that the delta symbol is placed within the V steering bars.\n\n"}
{"id": "45599643", "url": "https://en.wikipedia.org/wiki?curid=45599643", "title": "Barbara Beskind", "text": "Barbara Beskind\n\nBarbara Beskind is an American inventor and designer.\n\nBarbara Knickerbocker Beskind is a designer and internationally recognized pioneer in the field of occupational therapy. Beskind graduated in 1945 from the College of Home Economics at Syracuse University with a BS in Applied Arts and Design. At the end of World War II, she trained as an occupational therapist through the U.S. Army’s War Emergency Course and served for 20 years, retiring as a major in 1966. She went on to found the Princeton Center for Learning Disorders, the first independent private practice in occupational therapy in the U.S. She authored a clinical text published in 1980 on the treatment of children with learning disorders and holds a patent for inflatable equipment that helps learning-disordered children improve their balance. The American Occupational Therapy Association honored Beskind as a Charter Fellow in recognition of her innovative therapeutic techniques. In 1989 she retired after a 44-year career in occupational therapy.\n\nFollowing her retirement, she studied creative and non-fiction writing at Bennington College (three summer courses) and six semesters at Lebanon College. As a result, she has published three more books authored also under her maiden name Barbara Knickerbocker: an historical family autobiography, \"Powderkeg\"; a book of her art and poetry, \"Touches of Life in Time and Space\"; and an historical fiction, \"Flax to Freedom\".\n\nBeskind studied abstract art at Sharon Arts Center in Sharon, New Hampshire, with a focus on its early roots in the Russian avant-garde. After three trips to Russia to study these artists, she taught a non-credit course at Colby-Sawyer College in New London, New Hampshire.\n\nIn January 2013, Beskind saw IDEO founder David Kelley speak on \"60 Minutes\" (t.v. series) about the importance of cultivating a diversity of experience among team members developing new products and services. Beskind wrote to the company, offering to help IDEO design for aging and low-vision populations. Then at age 89, she began working for IDEO in their Bay Area offices and has been directly involved with client projects related to contact lenses, health care delivery, and retirement home services.\n\nAs a conceptual designer, she has developed specific ideas for a pair of glasses for those with macular degeneration such as she has. She is developing an alternative walker called the “Trekker” with vertical grips to promote good posture and to maintain alternative arm-leg movements. She has adapted ski poles to preserve good balance and gait patterns for those with vision and mobility problems.\n"}
{"id": "5403858", "url": "https://en.wikipedia.org/wiki?curid=5403858", "title": "Barrier board", "text": "Barrier board\n\nBarrier boards are typically long plastic or wooden beams used during road works and similar activities to cordon off areas, close roads or direct traffic.\n\nThey are also known as council barriers, Show Stoppers, works barricades, or safety barricades.\n\n\n"}
{"id": "17099795", "url": "https://en.wikipedia.org/wiki?curid=17099795", "title": "Blindspots analysis", "text": "Blindspots analysis\n\nBlindspots analysis (also blind spots analysis) is a method aimed at uncovering obsolete, incomplete, or incorrect assumptions in a decision maker’s mental scheme of the environment. Michael Porter used the term \"blind spots\" to refer to conventional wisdom which no longer holds true, but which still guides business strategy. The concept was further popularized by Barbara Tuchman, in her book \"The March of Folly\" (1984), to describe political decisions and strategies which were clearly wrong in their assumptions, and by other authors since, such as social psychologists Mahzarin Banaji and Anthony Greenwald in their study of prejudice.\n\nBen Gilad fully developed, in his book, \"Business Blindspots\" (1994), the following three-step \"Gilad method\" for uncovering blind spots\n\nUnderlying Blindspots Analysis is an assumption about the inherent biases of decision making at the top of organizations (business, government or otherwise) exceeding those of their subordinates or outsiders. While many top executives in business and government organizations are smart, capable people, they are also vulnerable to several decision biases that come with their powerful positions, including cognitive dissonance, motivated cognitions, overconfidence, and ego-involvement. The impaired ability of leaders to see reality for what it is, and the more objective (less ego-involved) analysis of analysts and mid-level planners means that Step 3 of the Blindspots Analysis can be a powerful tool for pointing to potential blinders\n\n\n"}
{"id": "19807602", "url": "https://en.wikipedia.org/wiki?curid=19807602", "title": "Broadcast Exchange Format", "text": "Broadcast Exchange Format\n\nBroadcast Exchange Format (BXF) is an SMPTE standard for data exchange in the broadcasting industry.\n\nBXF was developed to replace various archaic types of exchange for playlists, record lists and other data in broadcasting. Version 1.0 (SMPTE standard 2021) was published in 2008. Over 150 SMPTE members have been involved in defining the standard. BXF is XML based.\n\nMyers Information Systems was the first to use BXF, in partnership with Crispin Corporation.\n\n\n"}
{"id": "153852", "url": "https://en.wikipedia.org/wiki?curid=153852", "title": "Chief information officer", "text": "Chief information officer\n\nChief information officer (CIO), chief digital information officer (CDIO) or information technology (IT) director, is a job title commonly given to the most senior executive in an enterprise who works for the traditional information technology and computer systems that support enterprise goals.\n\nTypically, the CIO reports directly to the chief executive officer but may also report to the chief operating officer or chief financial officer. In military organizations, they report to the commanding officer. The Chief Information Officer role was first defined in 1981 by William R. Synnott, former Senior Vice President of the Bank of Boston, and William H. Gruber, former professor at the MIT Sloan School of Management.\n\nCIOs or CDIOs form a key part of any business that utilizes technology and data. In recent times, it has been identified that an understanding of just business or just IT is not sufficient. CIOs manage IT resources and plan \"ICT including policy and practice development, planning, budgeting, resourcing and training”. In addition to this, CIOs are becoming increasingly important in calculating how to increase profits via the use of ICT frameworks, as well as the vital role of reducing expenditure and limiting damage by setting up controls and planning for possible disasters. \"Computer Weekly\" magazine highlights that “53% of IT leaders report a shortage of people with high-level personal skills” in the workplace. Most organisations can't expect to fill demand for skilled resources and 57% of CIOs don't have the right learning and support mechanisms in place to enable current staff to meet the skill shortage. CIOs are needed to decrease the gulf between roles carried out by both IT professionals and non-IT professionals in businesses in order to set up effective and working relationships.\n\nThe Chief Information Officer of an organization is responsible for a number of roles. First and most importantly, the CIO must fulfill the role of business leader. As a CIO must make executive decisions regarding things such as the purchase of IT equipment from suppliers or the creation of new systems, they are therefore responsible for leading and directing the workforce of their specific organization. In addition, the CIO is ‘required to have strong organizational skills’. This is particularly relevant for a Chief Information Officer of an organization who must balance roles in order to gain a competitive advantage and keep the best interests of the organization's employees. CIOs also have the responsibility of recruiting, so it is important that they take on the best employees to complete the jobs the company needs fulfilling.\n\nIn addition, CIOs are directly required to map out both the ICT strategy and ICT policy of an organization. The ICT strategy covers future proofing, procurement, and the external and internal standards laid out by an organization. Similarly, the CIO must write up the ICT policy, detailing how ICT is utilized and applied. Both are needed for the protection of the organization in the short and long term and the process of strategizing for the future. Paul Burfitt, former CIO of AstraZeneca, also outlines the CIO's role of IT governance, which he refers to as the “clarifying” of “accountability and the role of committees”.\n\nIn recent years the CIO and their close cousin has become more closely involved in customer facing products. With the rising awareness in organisations that their customers are expecting digital services as part of their relationship with an organisation, CIOs have been tasked with product oriented responsibilities. Clear examples of this are seen at facilities management company MITIE where former CIO of energy firm Centrica Davi Cooper is the CTIO and has an agenda to create online services for their customers.\n\nAs the CIO has a large number of responsibilities such as provision of finance, recruitment of professionals and development of policy and strategy, the risks are consequently vast. The CIO of U.S company Target was forced into resignation in 2014 after the theft of 40 million credit card details and 70 million customer details by hackers. CIOs carry out a large number of roles and therefore the chance of failure is very high. In this way, any CIO must be knowledgeable about the industry so they can adapt and reduce the chance of error.\n\nWith the introduction of legislation such as the General Data Protection Regulation (GDPR) CIOs have now become increasingly focused on how their role is regulated and can lead to financial and reputation damage to a business. However, regulations such as GDPR have also been advantageous to CIOs enabling them to have the budget and authority in the organisation to make significant changes to the way information is managed. Sabah Khan-Carter of Rupert Murdoch's News Corp described GDPR as \"a really big opportunity for most organisations\".\n\nInformation technology and its systems have become so important that the CIO has come to be viewed in many organizations as a key contributor in formulating strategic goals for an organization. The prominence of the CIO position has greatly risen as information, and the information technology that drives it, has become an increasingly important part of the modern organization. Many CIOs are adding additional c-level titles to reflect the growing importance of technology in successfully running companies; this trend is referred to as the CIO-plus. The CIO may be a member of the executive committee of an organization, and/or may often be required to engage at board level depending on the nature of the organization and its operating structure and governance environment. No specific qualifications are intrinsic to the CIO position, though the typical candidate may have expertise in a number of technological fields - computer science, software engineering, or information systems. However, in healthcare there is a rising demand for CIOs to be qualified. The benefit of this is that they will then be seen as peers to their clinical peers and able to command greater respect and opportunity in the organisation.\n\nDespite the strategic nature of the role, a 2017 survey, conducted by Logicalis, of 890 CIOs across 23 countries found that 62% of CIOs spend 60% or more of their time on day to day IT activities.\n\nIn 2012, Gartner Executive Programs conducted a global CIO survey and received responses from 2,053 CIOs from 41 countries and 36 industries. Gartner reported that survey results indicated that the top ten technology priorities for CIOs for 2013 were analytics and business intelligence, mobile technologies, cloud computing, collaboration technologies, legacy modernization, IT management, customer relationship management, virtualization, security, and enterprise resource planning.\n\n\"CIO\" magazine's \"State of the CIO 2008\" survey asked 558 IT leaders whom they report to. The results were: CEO (41%), CFO (23%), COO (16%), Corporate CIO (7%) and Other (13%).\n\nTypically, a CIO is involved with driving the analysis and re-engineering of existing business processes, identifying and developing the capability to use new tools, reshaping the enterprise's physical infrastructure and network access, and with identifying and exploiting the enterprise's knowledge resources. Many CIOs head the enterprise's efforts to integrate the Internet into both its long-term strategy and its immediate business plans. CIOs are often tasked with either driving or heading up crucial IT projects that are essential to the strategic and operational objectives of an organization. A good example of this would be the implementation of an Enterprise Resource Planning (ERP) system, which typically has wide-ranging implications for most organizations.\n\nAnother way that the CIO role is changing is an increased focus on service management. As SaaS, IaaS, BPO and other more flexible value delivery techniques are brought into organizations the CIO usually functions as a 3rd party manager for the organization. In essence, a CIO in the modern organization is required to possess business skills and the ability to relate to the organization as a whole, as opposed to being a technological expert with limited functional business expertise. The CIO position is as much about anticipating trends in the market place with regard to technology as it is about ensuring that the business navigates these trends through expert guidance and proper strategic IT planning that is aligned to the corporate strategy of the organization.\n\nThe roles of Chief Information Officer, Chief Digital Officer and Chief Technology Officer are commonly blurred. Tom Silver, the North American senior vice president for Dice, states that CTOs are concerned with technology itself, often customer-facing, whereas CIOs are much more concerned with its applications in the business and how this can be managed.\n\nMore specifically, CIOs manage a business's IT systems and functions, creates and delivers strategies and policies, and places great emphasis on internal customers. In contrast to this, CTOs place emphasis on the external customers to the organization and focus on how different technology can make the company more profitable.\n\nThe traditional definition of CTOs focused on using technology as an external competitive advantage now includes CDOs who use the power of modern technologies, online design and big data to digitalise a business.\n\nIt is not uncommon for CIOs to be recognised and awarded annually, particularly in the technology space. These awards are commonly dictated by the significance of their contribution to the industry and generally occur in local markets only. Awards are generally judged by industry peers, or senior qualified executives such as the chief executive officer, chief operating officer or chief financial officer. Generally awards recognise substantial impact to the local technology market.\n\nIn Australia, the top 50 CIOs are recognised annually under the CIO50 banner. In the United States of America, United Kingdom and New Zealand CIOs are recognised under the CIO100 banner.\n\n\n\nCIO Applications provides knowledge network for CIOs to discuss their innovative enterprise solution and allows IT Vendors to learn about trending technologies, news and solutions that can help to grow their business."}
{"id": "6945", "url": "https://en.wikipedia.org/wiki?curid=6945", "title": "Chrominance", "text": "Chrominance\n\nChrominance (\"chroma\" or C for short) is the signal used in video systems to convey the color information of the picture, separately from the accompanying luma signal (or Y for short). Chrominance is usually represented as two color-difference components: U = B′ − Y′ (blue − luma) and V = R′ − Y′ (red − luma). Each of these difference components may have scale factors and offsets applied to it, as specified by the applicable video standard. \n\nIn composite video signals, the U and V signals modulate a color subcarrier signal, and the result is referred to as the chrominance signal; the phase and amplitude of this modulated chrominance signal correspond approximately to the hue and saturation of the color. In digital-video and still-image color spaces such as Y′CbCr, the luma and chrominance components are digital sample values.\n\nSeparating RGB color signals into luma and chrominance allows the bandwidth of each to be determined separately. Typically, the chrominance bandwidth is reduced in analog composite video by reducing the bandwidth of a modulated color subcarrier, and in digital systems by chroma subsampling.\n\nThe idea of transmitting a color television signal with distinct luma and chrominance components originated with Georges Valensi, who patented the idea in 1938. Valensi's patent application described:\nThe use of two channels, one transmitting the predominating color (signal T), and the other the mean brilliance (signal t) output from a single television transmitter to be received not only by color television receivers provided with the necessary more expensive equipment, but also by the ordinary type of television receiver which is more numerous and less expensive and which reproduces the pictures in black and white only.\nPrevious schemes for color television systems, which were incompatible with existing monochrome receivers, transmitted RGB signals in various ways.\n\nIn analog television, chrominance is encoded into a video signal using a subcarrier frequency. Depending on the video standard, the chrominance subcarrier may be either quadrature-amplitude-modulated (NTSC and PAL) or frequency-modulated (SECAM).\n\nIn the PAL system, the color subcarrier is 4.43 MHz above the video carrier, while in the NTSC system it is 3.58 MHz above the video carrier. The NTSC and PAL standards are the most commonly used, although there are other video standards that employ different subcarrier frequencies. For example, PAL-M (Brazil) uses a 3.58 MHz subcarrier, and SECAM uses two different frequencies, 4.250 MHz and 4.40625 MHz above the video carrier.\n\nThe presence of chrominance in a video signal is indicated by a color burst signal transmitted on the back porch, just after horizontal synchronization and before each line of video starts. If the color burst signal were visible on a television screen, it would appear as a vertical strip of a very dark olive color. In NTSC and PAL, hue is represented by a phase shift of the chrominance signal relative to the color burst, while saturation is determined by the amplitude of the subcarrier. In SECAM (R′ − Y′) and (B′ − Y′) signals are transmitted alternately and phase does not matter.\n\nChrominance is represented by the U-V color plane in PAL and SECAM video signals, and by the I-Q color plane in NTSC.\n\nDigital video and digital still photography systems sometimes use a luma/chroma decomposition for improved compression. For example, when an ordinary RGB digital image is compressed via the JPEG standard, the RGB colorspace is first converted (by a rotation matrix) to a YCbCr colorspace, because the three components in that space have less correlation redundancy and because the chrominance components can then be subsampled by a factor of 2 or 4 to further compress the image. On decompression, the Y′CbCr space is rotated back to RGB.\n\n"}
{"id": "6076335", "url": "https://en.wikipedia.org/wiki?curid=6076335", "title": "Clock recovery", "text": "Clock recovery\n\nIn serial communication of digital data, clock recovery is the process of extracting timing information from a serial data stream to allow the receiving circuit to decode the transmitted symbols. Clock recovery from the data stream is expedited by modifying the transmitted data. Wherever a serial communication channel does not transmit the clock signal along with the data stream, the clock must be regenerated at the receiver, using the timing information from the data stream. Clock recovery is a common component of systems communicating over wires, optical fibers, or by radio. \n\nSome digital data streams, especially high-speed serial data streams (such as the raw stream of data from the magnetic head of a disk drive and serial communication networks such as Ethernet) are sent without an accompanying clock signal. The receiver generates a clock from an approximate frequency reference, and then phase-aligns the clock to the transitions in the data stream with a phase-locked loop (PLL). This is one method of performing a process commonly known as \"clock and data recovery\" (CDR). Other methods include the use of a delay-locked loop and oversampling of the data stream. \n\nOversampling can be done \"blind\" using multiple phases of a free-running clock to create multiple samples of the input and then selecting the best sample. Or, a counter can be used that is driven by a sampling clock running at some multiple of the data stream frequency, with the counter reset on every transition of the data stream and the data stream sampled at some predetermined count. These two types of oversampling are sometimes called \"spatial\" and \"time\" respectively.\nThe best bit error ratio (BER) is obtained when the samples are taken as far away as possible from any data stream transitions. While most oversampling designs using a counter employ a sampling clock frequency that is an even multiple of the data stream, an odd multiple is better able to create a sampling point further from any data stream transitions and can do so at nearly half the frequency of a design using an even multiple. In oversampling type CDRs, the signal used to sample the data can be used as the recovered clock.\n\nClock recovery is very closely related to the problem of carrier recovery, which is the process of re-creating a phase-locked version of the carrier when a suppressed carrier modulation scheme is used. These problems were first addressed in a 1956 paper, which introduced a clock-recovery method now known as the Costas loop. Since then many additional methods have been developed.\n\nIn order for this scheme to work, a data stream must transition frequently enough to correct for any drift in the PLL's oscillator. The limit for how long a clock-recovery unit can operate without a transition is known as its maximum consecutive identical digits (CID) specification. To ensure frequent transitions, some sort of self-clocking signal is used, often a run length limited encoding; 8b/10b encoding is very common, while Manchester encoding serves the same purpose in old revisions of 802.3 local area networks.\n\n"}
{"id": "14241199", "url": "https://en.wikipedia.org/wiki?curid=14241199", "title": "CodeSynthesis XSD/e", "text": "CodeSynthesis XSD/e\n\nCodeSynthesis XSD/e is a validating XML parser/serializer and C++ XML Data Binding generator for Mobile and Embedded systems. It is developed by Code Synthesis and dual-licensed under the GNU GPL and a proprietary license. \n\nGiven an XML instance specification (XML Schema), XSD/e can produce three kinds of C++ mappings: Embedded C++/Parser for event-driven XML parsing, Embedded C++/Serializer for event-driven XML serialization, and Embedded C++/Hybrid which provides a light-weight, in-memory object model on top of the other two mappings.\n\nThe C++/Hybrid mapping generates C++ classes for types defined in XML Schema as well as parsing and serialization code. The C++ classes represent the data stored in XML as a statically-typed, tree-like object model and support fully in-memory as well as partially in-memory/partially event-driven XML processing. The C++/Parser mapping generates validating C++ parser skeletons for data types defined in XML Schema. One can then implement these parser skeletons to build a custom in-memory representation or perform immediate processing as parts of the XML documents become available. Similarly, the Embedded C++/Serializer mapping generates validating C++ serializer skeletons for types defined in XML Schema which can be used to serialize application data to XML.\n\nCodeSynthesis XSD/e itself is written in C++ and supports a number of embedded targets include Embedded Linux, VxWorks, QNX, LynxOS, iPhone OS and Windows CE.\n\n"}
{"id": "7165514", "url": "https://en.wikipedia.org/wiki?curid=7165514", "title": "Communications server", "text": "Communications server\n\nCommunications servers are open, standards-based computing systems that operate as a carrier-grade common platform for a wide range of communications applications and allow equipment providers to add value at many levels of the system architecture.\n\nBased on industry-managed standards such as AdvancedTCA, MicroTCA, Carrier Grade Linux and Service Availability Forum specifications, communications servers are the foundational platform upon which equipment providers build network infrastructure elements for deployments such as IP Multimedia Subsystem (IMS), IPTV and wireless broadband (e.g. WiMAX).\n\nSupport for communications servers as a category of server is developing rapidly throughout the communications industry. Standards bodies, industry associations, vendor alliance programs, hardware and software manufacturers, communications server vendors and users are all part of an increasingly robust communications server ecosystem.\n\nRegardless of their specific, differentiated features, communications servers have the following attributes: open, flexible, carrier-grade, and communications-focused.\n\n\n\n\nSeveral industry-managed standards are critical to the success of communications servers, including:\n\nThe Advanced Telecommunications Computing Architecture (ATCA) is a series of PCI Industrial Computers Manufacturers Group (PICMG) specifications, targeted to meet the requirements for carrier grade communications equipment. This series of specifications incorporates the latest trends in high speed interconnect technologies, next generation processors and improved reliability, manageability and serviceability.\n\nThe PICMG Advanced Mezzanine Card specification defines the base-level requirements for a wide range of high-speed mezzanine cards optimized for, but not limited to, AdvancedTCA Carriers. AdvancedMC enhances AdvancedTCA’s flexibility by extending its high-bandwidth, multi-protocol interface to individual hot-swappable modules.\n\nThis PICMG specification provides a framework for combining AdvancedMC modules directly, without the need for an AdvancedTCA or custom carrier. MicroTCA is aimed at smaller equipment – such as wireless base stations, Wi-Fi and WiMAX radios, and VoIP access gateways where small physical size low entry cost, and scalability are key requirements.\n\nAn enhanced version of Linux for use in a highly available, secure, scalable, and maintainable carrier grade system. The specification is managed by the CGL Working Group of the Open Source Development Labs.\n\nThese Service Availability Forum (SA Forum) specifications define standard interfaces for telecom platform management and high-availability software.\n\nThe Hardware Platform Interface (HPI) specification defines the interface between high availability middleware and the underlying hardware and operating system.\n\nAt a higher layer than HPI, the Application Interface Specification (AIS) defines the application programming interface between the high availability middleware and the application. AIS allows an application to run on multiple computing modules, and applications that support AIS can migrate more easily between computing platforms from different manufacturers that support the standard.\n\nIn addition to the standards development organizations mentioned above, four industry associations / vendor alliance programs are playing key roles in the development of the communications server ecosystem.\n\nSCOPE Alliance is an industry alliance committed to accelerating the deployment of carrier grade base platforms for service provider applications. Its mission is to help, enable and promote the availability of open carrier grade base platforms based on Commercial-Off-The-Shelf hardware / software and Free Open Source Software building blocks, and to promote interoperability to better serve Service Providers and consumers.\n\nThe Communications Platforms Trade Association (CP-TA) is an association of communications platforms and building block providers dedicated to accelerating the adoption of SIG-governed, open specification-based communications platforms through interoperability certification. With industry collaboration, the CP-TA plans to drive a mainstream market for open industry standards-based communications platforms by certifying interoperable products.\n\nThe Intel Communications Alliance is a community of communications and embedded developers and solutions providers committed to the development of modular, standards-based solutions on Intel technologies.\n\nThe Motorola Communications Server Alliance is an ecosystem of technology, service and solution providers aligned to provide standards-based solution elements validated with Motorola’s communications servers. Alliance participants receive access to Motorola embedded communications computing product roadmaps, development systems, and participate in marketing activities with Motorola.\n\nThe Mobicents Open Source Communications Community is an ecosystem of technology, service and solution providers aligned to provide Open Source, Open Standards-based communication software. Community members contribute to the Mobicents product roadmaps, research, development, and marketing activities.\n"}
{"id": "4468348", "url": "https://en.wikipedia.org/wiki?curid=4468348", "title": "Current clamp", "text": "Current clamp\n\nIn electrical and electronic engineering, a current clamp or current probe is an electrical device with jaws which open to allow clamping around an electrical conductor. This allows measurement of the current in a conductor without the need to make physical contact with it, or to disconnect it for insertion through the probe. Current clamps are typically used to read the magnitude of alternating current (AC) and, with additional instrumentation, the phase and waveform can also be measured. Some clamps meters can measure currents of 1000 A and more. Hall effect and vane type clamps can also measure direct current (DC).\n\nA common form of current clamp comprises a split ring made of ferrite or soft iron. A wire coil is wound round one or both halves, forming one winding of a current transformer. The conductor it is clamped around forms the other winding. Like any transformer this type works only with AC or pulse waveforms, with some examples extending into the megahertz range.\n\nWhen measuring current, the subject conductor forms the primary winding and the coil forms the secondary.\n\nThis type may also be used in reverse, to inject current into the conductor, for example in electromagnetic compatibility susceptibility testing to induce an interference current. Usually, the injection probe is specifically designed for this purpose. In this mode, the coil forms the primary and the test conductor the secondary.\n\nIn the iron vane type, the magnetic flux in the core directly affects a moving iron vane, allowing both AC and DC to be measured, and gives a true root mean square (RMS) value for non-sinusoidal AC waveforms. Due to its physical size it is generally limited to power transmission frequencies up to around 100 Hz.\n\nThe vane is usually fixed directly to the display mechanism of an analogue (moving pointer) clamp meter.\nThe calibration of the instrument is clearly non-linear.\n\nThe Hall effect type is more sensitive and is able to measure both DC and AC, in some examples up to the kilohertz (thousands of hertz) range. This type was often used with oscilloscopes, and with high-end computerized digital multimeters, however, they are becoming common place for more general use.\n\nResembling a current clamp in appearance and function is the Rogowski coil current sensor. This coreless transformer is used in clamp meters and power monitoring loggers. It has the advantage of better linearity, having no core to saturate, it can be made flexible, and does not require any magnetic or electrical contact at the opening end. The Rogowski coil gives a voltage proportional to the rate of change of current in the primary cable, so more signal processing is needed before the sensed values can be displayed.\n\nAn electrical meter with integral AC current clamp is known as a clamp meter, clamp-on ammeter, tong tester, or colloquially as an amp clamp.\n\nA clamp meter measures the vector sum of the currents flowing in all the conductors passing through the probe, which depends on the phase relationship of the currents. Only one conductor is normally passed through the probe. In particular if the clamp is closed around a two-conductor cable carrying power to equipment, the same current flows down one conductor and up the other; the meter correctly reads a net current of zero. As electrical cables for equipment have both insulated conductors (and possibly an earth wire) bonded together, clamp meters are often used with what is essentially a short extension cord with the two conductors separated, so that the clamp can be placed around only one conductor of this extension.\n\nA relatively recent development was a multi-conductor clamp meter with several sensor coils around the jaws of the clamp. This could be clamped around standard two- or three-conductor single-phase cables to provide a readout of the current flowing through the load, with no need to separate the conductors.\n\nThe reading produced by a conductor carrying a very low current can be increased by winding the conductor around the clamp several times; the meter reading divided by the number of turns is the current, with some loss of accuracy due to inductive effects.\n\nClamp meters are used by electricians, sometimes with the clamp incorporated into a general purpose multimeter.\n\nIt is simple to measure very high currents (hundreds of amperes) with the appropriate current transformer. Accurate measurement of low currents (a few milliamperes) with a current transformer clamp is more difficult. The range of any given meter can be extended by passing the conductor through the jaw multiple times. For example a 0–200 A meter can be turned into a 0–20 A meter by winding the conductor 10 times around the jaw's core.\nLess-expensive clamp meters use a rectifier circuit which actually reads mean current, but is calibrated to display the RMS current corresponding to the measured mean, giving a correct RMS reading only if the current is a sine wave. For other waveforms readings will be incorrect; when these simpler meters are used with non-sinusoidal loads such as the ballasts used with fluorescent lamps or high-intensity discharge lamps or most modern computer and electronic equipment, readings can be quite inaccurate. Meters which respond to true RMS rather than mean current are described as \"true RMS\".\n\nTypical hand-held Hall effect units can read currents as low as 200 mA, and units that can read down to 1 mA are available.\n\nThe Columbia tong test ammeter (illustrated) is an example of the iron vane type, used for measuring large AC currents up to 1000 amperes. The iron jaws of the meter direct the magnetic field surrounding the conductor to an iron vane that is attached to the needle of the meter. The iron vane moves in proportion to the strength of the magnetic field, and thus produces a meter indication proportional to the current. This type of ammeter can measure both AC and DC currents and provides a true RMS current measurement of non-sinusoidal or distorted AC waveforms. Interchangeable meter movements can be installed in the clamping assembly to provide various full-scale current values up to 1000 amperes. The iron vane is in a small cylinder that is inserted in a space at the hinged end of the clamp-on jaws. Several jaw sizes are available for clamping around large conductors and bus bars up to wide. As the illustration shows, the scale is very non-linear and unsuitable for measuring low currents, with currents of less than half the full-scale deflection crammed into a short section of the dial.\nClamp probes are used with some meters to measure electrical power and energy. The clamp measures the current and other circuitry the voltage; the true power is the product of the instantaneous voltage and current integrated over a cycle. Comprehensive meters designed to measure many parameters of electrical energy (power factor, distortion, instantaneous power as a function of time, phase relationships, etc.), energy analyzers, use this principle. A single clamp is used for single-phase measurements; with an appropriate instrument with three clamps, measurements may be made on three-phase power systems.\n"}
{"id": "43127616", "url": "https://en.wikipedia.org/wiki?curid=43127616", "title": "Cyclic corrosion testing", "text": "Cyclic corrosion testing\n\nCyclic Corrosion Testing (CCT) has evolved in recent years, largely within the automotive industry, as a way of accelerating real-world corrosion failures, under laboratory controlled conditions. \nAs the name implies, the test comprises different climates which are cycled automatically so the samples under test undergo the same sort of changing environment that would be encountered in the natural world. The intention being to bring about the type of failure that might occur naturally, but more quickly i.e. accelerated. By doing this manufacturers and suppliers can predict, more accurately, the service life expectancy of their products.\n\nUntil the development of Cyclic Corrosion Testing, the traditional Salt spray test was virtually all that manufacturers could use for this purpose. However, this test was never intended for this purpose. Because the test conditions specified for salt spray testing are not typical of a naturally occurring environment, this type of test cannot be used as a reliable means of predicting the ‘real world’ service life expectancy for the samples under test. The sole purpose of the salt spray test is to compare and contrast results with previous experience to perform a quality audit. So, for example, a spray test can be used to ‘police’ a production process and forewarn of potential manufacturing problems or defects, which might affect corrosion resistance. .\n\nTo recreate these different environments within an environmental chamber requires much more flexible testing procedures than are available in a standard salt spray chamber.\n\nThe lack of correlation between results obtained from traditional salt spray testing and the ‘real world’ atmospheric corrosion of vehicles, left the automotive industry without a reliable test method for predicting the service life expectancy of their products. This was and remains of particular concern in an industry where anti-corrosion warranties have been gradually increasing and now run to several years for new vehicles.\n\nWith ever increasing consumer pressure for improved vehicle corrosion resistance and a few ‘high profile’ corrosion failures amongst some vehicle manufactures – with disastrous commercial consequences, the automotive industry recognized the need for a different type of corrosion test.\n\nSuch a test would need to simulate the types of conditions a vehicle might encounter naturally, but recreate and accelerate these conditions, with good repeatability, within the convenience of the laboratory.\nCCT is effective for evaluating a variety of corrosion types, including galvanic corrosion and crevice corrosion.\n\nTaking results gathered largely from ‘real world’ exposure sites, automotive companies, led originally by the Japanese automobile industry, developed their own Cyclic Corrosion Tests. These have evolved in different ways for different vehicle manufacturers, and such tests still remain largely industry specific, with no truly international CCT standard. However, they all generally require most of the following conditions to be created, in a repeating sequence or ‘cycle’, though not necessarily in the following order:\n\n• A salt spray ‘pollution’ phase. This may be similar to the traditional salt spray test although in some cases direct impingement by the salt solution on the test specimens, or even complete immersion in salt water, is required. However, this ‘pollution’ phase is generally shorter in duration than a traditional salt spray test.\n\n• An air drying phase. Depending on the test, this may be conducted at ambient temperature, or at an elevated temperature, with or without control over the relative humidity and usually by introducing a continuous supply of relatively fresh air around the test samples at the same time. It is generally required that the samples under test should be visibly ‘dry’ at the end of this test phase.\n• A condensation humidity ‘wetting’ phase. This is usually conducted at an elevated temperature and generally a high humidity of 95-100%RH. The purpose of this phase is to promote the formation of condensation on the surfaces of the samples under test.\n\n• A controlled humidity/humidity cycling phase. This requires the tests samples to be exposed to a controlled temperature and controlled humidity climate, which can either be constant or cycling between different levels. When cycling between different levels, the rate of change may also be specified.\n\nThe above list is not exhaustive, since some automotive companies may also require other climates to be created in sequence as well, for example; sub-zero refrigeration, but it does list the most common requirements.\n\nThe below list is not exhaustive, but here are some examples of popular cyclic corrosion test standards,\n\n\n"}
{"id": "28867648", "url": "https://en.wikipedia.org/wiki?curid=28867648", "title": "DO-178C", "text": "DO-178C\n\nDO-178C, Software Considerations in Airborne Systems and Equipment Certification is the primary document by which the certification authorities such as FAA, EASA and Transport Canada approve all commercial software-based aerospace systems. The document is published by RTCA, Incorporated, in a joint effort with EUROCAE, and replaces DO-178B. The new document is called DO-178C/ED-12C and was completed in November 2011 and approved by the RTCA in December 2011. It became available for sale and use in January 2012.\n\nThe FAA approved AC 20-115C on 19 Jul 2013, making DO-178C a recognized \"acceptable means, but not the only means, for showing compliance with the applicable airworthiness regulations for the software aspects of airborne systems and equipment certification.\"\n\nSince the release of DO-178B, there have been strong calls by DERs (FAA Designated Engineering Representatives) for clarification/refinement of the definitions and boundaries between the key DO-178B concepts of high-level requirements, low-level requirements, and derived requirements and a better definition of the exit/entry criteria between systems requirements and system design (see ARP4754) and that of software requirements and software design (which is the domain of DO-178B). Other concerns included the meaning of verification in a model-based development paradigm and considerations for replacing some or all software testing activities with model simulation or formal methods. The release of DO-178C and the companion documents DO-278A (Ground Systems), DO-248C (Additional information with rationale for each DO-178C objective), DO-330 (Tool Qualification), DO-331 (Modeling), DO-332 (Object Oriented), and DO-333 (Formal Methods) were created to address the issues noted. The SC-205 members worked with the SAE S-18 committee to ensure that ARP4754A and the above noted DO-xxx documents provide a unified and linked process with complementary criteria.\n\nOverall, DO-178C keeps most of the DO-178B text, which has raised concerns that issues with DO-178B, such as the ambiguity about the concept of low-level requirements, may not be fully resolved.\n\nThe RTCA/EUROCAE joint committee work was divided into seven Subgroups:\n\nThe Model Based Development and Verification subgroup (SG4), was the largest of the working groups. All work is collected and coordinated via a web-site that is a collaborative work management mechanism. Working artifacts and draft documents were held in a restricted area available to group members only.\n\nThe work was focused on bringing DO-178B/ED-12B up to date with respect to current software development practices, tools, and technologies.\n\nThe Software Level, also known as the Design Assurance Level (DAL) or Item Development Assurance Level (IDAL), is determined from the safety assessment process and hazard analysis by examining the effects of a failure condition in the system. The failure conditions are categorized by their effects on the aircraft, crew, and passengers.\n\nDO-178C alone is not intended to guarantee software safety aspects. Safety attributes in the design and as implemented as functionality must receive additional mandatory system safety tasks to drive and show objective evidence of meeting explicit safety requirements. The certification authorities require and DO-178C specifies the correct DAL be established using these comprehensive analyses methods to establish the software level A-E. \"The software level establishes the rigor necessary to demonstrate compliance\" with DO-178C. Any software that commands, controls, and monitors safety-critical functions should receive the highest DAL - Level A.\n\nThe number of objectives to be satisfied (some with independence) is determined by the software level A-E. The phrase \"with independence\" refers to a separation of responsibilities where the objectivity of the verification and validation processes is ensured by virtue of their \"independence\" from the software development team. For objectives that must be satisfied with independence, the person verifying the item (such as a requirement or source code) may not be the person who authored the item and this separation must be clearly documented.\n\nDO-178 requires a documented connection (called a trace) between the certification artifacts. For example, a Low Level Requirement (LLR) traces up to a High Level Requirement (HLR). A traceability analysis is then used to ensure that each requirement is fulfilled by the source code, that each requirement is tested, that each line of source code has a purpose (is connected to a requirement), and so forth. Traceability ensures the system is complete. The rigor and detail of the certification artifacts is related to the software level.\n\nSC-205 was responsible for revising DO-178B/ED-12B to bring it up to date with respect to current software development and verification technologies. The structure of the document remains largely the same from B to C. Example changes include:\n\nDO-178B was not completely consistent in the use of the terms Guidelines and Guidance within the text. \"Guidance\" conveys a slightly stronger sense of obligation than \"guidelines\". As such, with the DO-178C, the SCWG has settled on the use of “guidance” for all the statements that are considered as \"recommendations\", replacing the remaining instances of \"guidelines\" with “supporting information” and using that phrase wherever the text is more \"information\" oriented than \"recommendation\" oriented. \n\nThe entire DO-248C/ED-94C document, \"Supporting Information for DO-178C and DO-278A\", falls into the “supporting information” category, not guidance.\n\nChapter 6.1 defines the purpose for the software verification process. DO-178C adds the following statement about the Executable Object Code:\nAs a comparison, DO-178B states the following with regard to the Executable Object Code:\nThe additional clarification fills a gap that a software developer may encounter when interpreting the document.\n\n\n"}
{"id": "28405015", "url": "https://en.wikipedia.org/wiki?curid=28405015", "title": "Data Processing Iran Co.", "text": "Data Processing Iran Co.\n\nData Processing Iran Company (DPI) () is a computer, technology and IT Consulting corporation headquartered in Tehran, Iran. DPI is currently the largest technology provider in Iran\n\nDPI manufactures and sells computer hardware and software (with a focus on the latter), and offers infrastructure services, hosting services, and consulting services in areas ranging from mainframe computers to nanotechnology.\n\nThe company also offers a series of Internet-related services, namely dedicated servers; colocation services; Web hosting services, such as shared hosting, shared mail, DNS recording, and domain registration services; and managed services, including network services, security solutions, managed application services, storage and backup solutions, monitoring and reporting, and professional services.\n\nDPI was established in 1959 as a regional branch for the IBM corporation. The company operated as a subsidiary until 1981, when IBM's operations in Iran were ceded to the Iranian government. In 2001, DPI became a private company, listed under the Tehran Stock Exchange. Over the company's history, DPI has signed numerous technology-sharing agreements with other software companies, including Mindscape, Dataproducts and Hypercom.\n\nCurrent members of the board of directors of DPI are:\n\n\n"}
{"id": "980282", "url": "https://en.wikipedia.org/wiki?curid=980282", "title": "Gas lift", "text": "Gas lift\n\nGas lift or bubble pumps use the artificial lift technique of raising a fluid such as water or oil by introducing bubbles of compressed air, water vapor or other vaporous bubbles into the outlet tube. This has the effect of reducing the hydrostatic pressure in the outlet tube vs. the hydrostatic pressure at the inlet side of the tube.\n\nDevices using this type of lift mechanism: \n\nIn the United States, gas lift is used in 10% of the oil wells that have insufficient reservoir pressure to produce the well. In the petroleum industry, the process involves injecting gas through the tubing-casing annulus. Injected gas aerates the fluid to reduce its density; the formation pressure is then able to lift the oil column and forces the fluid out of the wellbore. Gas may be injected continuously or intermittently, depending on the producing characteristics of the well and the arrangement of the gas-lift equipment.\n\nThe amount of gas to be injected to maximize oil production varies based on well conditions and geometries. Too much or too little injected gas will result in less than maximum production. Generally, the optimal amount of injected gas is determined by well tests, where the rate of injection is varied and liquid production (oil and perhaps water) is measured.\n\nAlthough the gas is recovered from the oil at a later separation stage, the process requires energy to drive a compressor to raise the pressure of the gas to a level where it can be re-injected.\n\nThe gas-lift mandrel is a device installed in the tubing string of a gas-lift well onto which or into which a gas-lift valve is fitted. There are two common types of mandrels. In a conventional gas-lift mandrel, a gas-lift valve is installed as the tubing is placed in the well. Thus, to replace or repair the valve, the tubing string must be pulled. In the side-pocket mandrel, however, the valve is installed and removed by wireline while the mandrel is still in the well, eliminating the need to pull the tubing to repair or replace the valve.\n\nA gas-lift valve is a device installed on (or in) a gas-lift mandrel, which in turn is put on the production tubing of a gas-lift well. Tubing and casing pressures cause the valve to open and close, thus allowing gas to be injected into the fluid in the tubing to cause the fluid to rise to the surface. In the lexicon of the industry, gas-lift mandrels are said to be \"tubing retrievable\" wherein they are deployed and retrieved attached to the production tubing. See gas-lift mandrel.\n\nGas lift operation can be optimized in different ways.The newest way is using risk-optimization which considers all aspects for gas lift allocation.\n\nAir lift uses compressed air to lift water in operations such as dredging and underwater archeology. It is also found in aquariums to keep water circulating. These forms of lift were used as far back as 1797 in mines to lift water from mine shafts. These systems used single point injection of air into the liquid stream, normally through a foot valve at the bottom of the string. Gas lift was used as early as 1864 in Pennsylvania to lift oil wells, also using compressed air, via an air pipe bringing the air to the bottom of the well. Air was used in Texas for large-scale artificial lift. In 1920 natural gas replaced air, lowering the risk of explosion. From 1929 until 1945 about 25000 patents were issued on different types of gas lift valves that could be used for unloading in stages. Some of these systems involved moving the tubing, or using wireline sinker bars to change the lift point. Others were spring operated valves. Ultimately, in 1944 W.R. King patented the pressurized bellows valve that is used today. In 1951 the sidepocket mandrel was developed for selectively positioning and retrieving gas lift valves with wireline.\n\n\n"}
{"id": "735638", "url": "https://en.wikipedia.org/wiki?curid=735638", "title": "Gillette", "text": "Gillette\n\nGillette is a brand of men's and women's safety razors and other personal care products including shaving supplies, owned by the multi-national corporation Procter & Gamble (P&G).\n\nBased in Boston, Massachusetts, United States, it was owned by The Gillette Company, a supplier of products under various brands until that company merged into P&G in 2005. The Gillette Company was founded by King C. Gillette in 1901 as a safety razor manufacturer.\n\nUnder the leadership of Colman M. Mockler Jr. as CEO from 1975–91, the company was the target of three takeover attempts, from Ronald Perelman and Coniston Partners. On October 1, 2005, Procter & Gamble finalized its merger with the Gillette Company.\n\nThe Gillette Company's assets were incorporated into a P&G unit known internally as \"Global Gillette\". In July 2007, Global Gillette was dissolved and incorporated into Procter & Gamble's other two main divisions, Procter & Gamble Beauty and Procter & Gamble Household Care. Gillette's brands and products were divided between the two accordingly. The Gillette R&D center in Boston, Massachusetts, and the Gillette South Boston Manufacturing Center (known as \"Gillette World Shaving Headquarters\"), still exist as functional working locations under the Procter & Gamble-owned Gillette brand name. Gillette's subsidiaries Braun and Oral-B, among others, have also been retained by P&G.\n\nIn some languages, the brand has become the genericized trademark for all razor blades, for example in Czech (\"žiletka\"), Estonian (\"žilett\"), Latvian (\"žilete\"), Macedonian (\"žilét\"), Polish (\"żyletka\"), and Serbo-Croatian (\"žilet\").\n\nThe first safety razor using the new disposable blade went on sale in 1903. Gillette maintained a limited range of models of this new type razor until 1921 when the original Gillette patent expired. In anticipation of the event, Gillette introduced a redesigned razor and offered it at a variety of prices in different cases and finishes, including the long running “aristocrat\". Gillette continued to sell the original razor but instead of pricing it at $5, it was priced at $1, making a Gillette razor truly affordable to every man regardless of economic class. In 1932 the Gillette Blue Blade, so-named because it was dipped in blue lacquer, was introduced. It became one of the most recognizable blades in the world. In 1934 the \"twist to open\" (TTO) design, was intuitiuted, which featured butterfly-like doors that made blade changing much easier than it had been, wherein the razor head had to be detached from the handle.\n\nRazor handles continued to advance to allow consumer to achieve a closer shave. In 1947, the new (TTO) model, the \"Super Speed\", was introduced. This was updated in 1955, with different versions being produced to shave more closely — the degree of closeness being marked by the color of the handle tip.\n\nIn 1955, the first \"adjustable\" razor was produced. This allowed for an adjustment of the blade to increase the closeness of the shave. The model, in various versions, remained in production until 1988.\n\nThe Super Speed razor was again redesigned in 1966 and given a black resin coated metal handle. It remained in production until 1988. A companion model the,\"Knack\", with a longer plastic handle, was produced from 1966 to 1975. In Europe, the Knack was sold as \"Slim Twist\" and \"G2000\" from 1978 to 1988, a later version known as \"G1000\" was made in England and available until 1998. A modern version of the Tech, with a plastic thin handle, is still produced and sold in several countries under the names 7 O'clock, Gillette, Nacet, Minora, Rubie and Economica.\n\n\n\nThe desire to release ever more expensive products, each claiming to be the \"best ever\", has led Gillette to make disputed claims for its products. In 2005, an injunction was brought by rival Wilkinson Sword which was granted by the Connecticut District Court which determined that Gillette's claims were both \"unsubstantiated and inaccurate\" and that the product demonstrations in Gillette's advertising were \"greatly exaggerated\" and \"literally false\". While advertising in the United States had to be rewritten, the court's ruling does not apply in other countries.\n\nProcter & Gamble (P&G) shaving products have been under investigation by the UK Office of Fair Trading as part of an inquiry into alleged collusion between manufacturers and retailers in setting prices. According to the \"Daily Mail\" newspaper, an industry insider claimed that the Fusion range of blades cost only £0.05 each to manufacture, yet sold for up to £2.43: a mark-up price of more than 4,750%.\n\nGillette was fined by Autorité de la concurrence in France in 2016 for price fixing on personal hygiene products.\n\nIn 1999, Gillette, as a company, was worth US$43 billion, and it was estimated that the brand value of Gillette was worth US$16 billion. This equated to 37% of the company's value, which was the same as DaimlerChrysler, one of the world's largest car manufacturers at the time.\nGillette has a long history of promotions for its products, especially towards young men. Current promotions include sponsorship of sports events such as Major League Baseball (since the 1940s, when it was the only sponsor for World Series television broadcasts) and the England national rugby league team, along with the Rugby League Four Nations. Gillette ships a razor to males in the United States around the time of their 18th birthday; as of 2010 Gillette has been sending the Fusion ProGlide. Athletes such as Roger Federer, Tiger Woods, Shoaib Malik, Derek Jeter, Thierry Henry, Kenan Sofuoglu, Park Ji-Sung, Dr DisRespect, Rahul Dravid, and Michael Clarke are sponsored by the company.\n\nThere were calls to boycott Gillette products given their association with Thierry Henry, after a handball by Henry went undetected by referees and allowed France to knock Ireland out of qualifying for the 2010 FIFA World Cup. Marketing experts have highlighted \"the curse of Gillette\", given the mishaps that happen to sports stars associated with the brand.\n\n\nUntil the late 1980s, Gillette Canada's headquarters were in the Montreal suburb of Mont-Royal, Quebec until they moved west to another Montreal suburb in Kirkland. The Kirkland offices were closed in 1999 and Gillette Canada moved to Mississauga, Ontario, a Toronto suburb following the Gillette acquisition of Duracell. The Mississauga offices were closed in 2005-06 after Procter & Gamble acquired Gillette, and Gillette's Canadian headquarters are located in downtown Toronto with parent Procter & Gamble on Yonge St.\n\n\n"}
{"id": "51157050", "url": "https://en.wikipedia.org/wiki?curid=51157050", "title": "Goodbaby International", "text": "Goodbaby International\n\nGoodbaby International Holdings Limited is headquartered in Hong Kong and is a durable juvenile products company that is listed on the Main Board of the Hong Kong Stock Exchange (1086:HK). Principally it engages in the manufacturing, designing, research and development, marketing and sale of many child stroller, child seat, and other child products worldwide. It operates brands such as Goodbaby, GB, Cybex, Evenflo, CBX, Rollplay, Happy Dino, Urbini, and other branded kids-children's products.\n\nCompany for the year ended 31 December 2015, the Group recorded total revenue of HK$6,951.1 million, representing a year-on-year increase of approximately 13%.\n\nGoodbaby International made 10,000 strollers a day under 15 different labels for mostly overseas brands.\n\nIn 1989 Zhenghuan Song designed a children’s rocking chair and founded Goodbaby International. In 1990 chairman Zhenghuan Song establishes R&D center in Kunshan, and in 1999, the companies becomes the largest supplier of strollers in North America.\n\nGoing Public\n\nIn 2011 the company goes public on the Hong Kong stock exchange. Ticker (HKEx: 01086)\n\nAcquisitions & Growth\n\nIn January 2014, Goodbaby International acquired Cybex. Goodbaby anticipated not only to extend the product portfolio into premium car safety seats and reinforce our footing in Europe, but also use Cybex's unique and extensive experience in branding and marketing.\n\nIn July 2014 Goodbaby International acquired Evenflo, one of the largest U.S. makers of strollers.\n\nGoodbaby International's core products include car seats, strollers, and other baby products.\n\n"}
{"id": "34188802", "url": "https://en.wikipedia.org/wiki?curid=34188802", "title": "IEA Solar Heating and Cooling Programme", "text": "IEA Solar Heating and Cooling Programme\n\nThe International Energy Agency Solar Heating and Cooling Programme (IEA SHC) is one of over 40 multilateral Technology Collaboration Programmes (also known as TCPs) of the International Energy Agency. It was one of the first of such programmes, founded in 1977. Its current mission is to \"advance international collaborative efforts for solar energy to reach the goal set in the vision of contributing 50% of the low temperature heating and cooling demand by 2030.\". Its international solar collector statistics \"Solar Heat Worldwide\" serve as a reference document for governments, financial institutions, consulting firms and non-profit organizations.\n\nThe IEA SHC's members are national governments, the European Commission and international organizations. Each of the members is represented by one representative in the management body called the Executive Committee. The IEA SHC Executive Committee meets twice per year and is headed by an elected chairman. The IEA SHC currently has 26 members (Australia, Austria, Belgium, Canada, China, Denmark, France, Germany, Italy, Mexico, Netherlands, Norway, Portugal, Singapore, South Africa, Spain, Sweden, Switzerland, Turkey, United Kingdom, European Commission, ECREEE, European Copper Institute, Gulf Organization for Research and Development, ISES, RCREEE ).\n\nThe IEA SHC aims at facilitating international collaboration in the research, development and demonstration of solar thermal energy and solar buildings. Their multi-year projects (also known as \"Tasks\") are conducted by researchers from different countries. Funding is provided by IEA SHC members, who usually pay one or more national research institutions to participate in the work.\n\nResearch topics include:\n\nAs well as work on:\n\nThe idea behind Task 13 was to push construction technology towards its limits to achieve the lowest possible total purchased energy consumption. Task 13 was part of the IEA \"Solar Heating and Cooling Programme\", to test the designs and techniques, and to monitor their performance.\n\nOn average, the houses were designed to require 44 kWh/m², 75% lower than the average 172 kWh/m² that would have been required had the houses been built to normal standards. (Analysis of 11 of the houses in use indicated that total savings made in practice was actually 60% ).The 44 kWh/m² resulted from:\n\nIn addition there was an average solar contribution designed to average 37 kWh/m², from a combination of passive solar gains, active solar, and photovoltaics.\n\nThe buildings were constructed to be airtight, superinsulated to roughly double normal standards, and to minimise thermal bridges. Masonry and several timber framed methods were represented, as well as newly designed steel strengthened polystyrene block walls. The Berlin \"Zero Heating Energy House\" included a seasonal thermal store.\n\nThe homes in the programme were:\n\nAmong the lessons learned were that:\n\nIn 2011, the IEA SHC Executive Committee announced an annual international conference on solar heating and cooling for buildings and industry. The first conference, SHC 2012 took place 9–11 July 2012 in San Francisco. SHC 2013 on 23–25 September 2013 in Freiburg, Germany., SHC 2014 on 13–15 October in Beijing, China, SHC 2015 on 2–4 December in Istanbul, Turkey. SHC 2013 and SHC 2015 were jointly with the European Solar Thermal Industry Federation (ESTIF), which had previously organized their own conference, ESTEC.\n\nApart from the reports and other publications of the research projects (Tasks), the Solar Heating and Cooling Programme publishes several cross-cutting documents, the most important one being the annual collector statistics \"Solar Heat Worldwide\". The SHC newsletter \"Solar Update\" is published twice per year.\n\n\n"}
{"id": "30292113", "url": "https://en.wikipedia.org/wiki?curid=30292113", "title": "ILabs", "text": "ILabs\n\niLabs is a non-profit Milan-based organization pursuing multidisciplinary research on radical extension of human life-span. It was founded in 1977 by Gabriele Rossi and Antonella Canonico, who advocate the scenario known as “Semi-Immortality”, an elaborate vision of an era of quasi-immortal individuals (“intelligent systems”), that is philosophically linked to other instances of Transhumanism and futurists’ theories (such as Raymond Kurzweil’s). \n\n\"iLabs\" led the development of Artificial Intelligence tools in Italy: the first relational database (1984), a self-monitoring and self-modifying application for production planning (1985), programs for data compression (1980), visual recognition (1983), cryptography (1987).\n\nStarting with the publishing of \"Semi-Immortality\" in November 2007, \"iLabs\" has also been devoting considerable resources to further improve the impact of the research program and raise the general awareness towards semi-immortality and related topics, resulting in a series of books, articles, interviews and events popular in the Italian transhumanist community and beyond.\n\nOn March 5, 2011 \"iLabs\" hosted the iLabs Singularity Summit, featuring speeches from Raymond Kurzweil and Aubrey De Grey.\n\n\"iLabs\" was founded in 1977 by two young Italian researchers, Antonella Canonico (psychologist) and Gabriele Rossi (computer scientist). The lab pursues a long-term research goal – the radical extension of human life-span – through the completion of mid-term projects in several fields.\n\nThroughout their history, \"iLabs\" has been pioneering the use of Artificial Intelligence and psychoneurophysiology (also known as Psychoneuroimmunology) in a variety of experimental and real-world settings. In 1984 the \"iLabs\" department of Artificial Intelligence created the first Italian data-base, \"Sistema I\", the first of a series of projects in computer science:\n\n\nThe department of Psychoneurophysiology was one of the first institution in the country promoting the field through innovative proposals in psychology, stress management and bio-statistics:\n\n\nThe department started focusing on psycho-oncology in 2000, with hundreds of subjects successfully treated since then.\n\n\"iLabs\" is privately owned and run since their foundation. \"iLabs\" started commercial spin-offs to apply their discoveries in business-oriented contexts; in turn, profits from commercial activities are used to fund the labs projects.\n\nThe main agenda is carried on, for the most part, by the departments of Artificial Intelligence and Psychoneurophysiology. However, \"iLabs\" relies on renowned experts to further widen the scope of the research program: professionals from mathematics, physics, logic, law, economics, medicine, biology have worked for the lab to address specific scientific challenges. The results of the projects are popularized through books, events, digital material, and spread in the academic community with peer-reviewed publications.\n\n\"iLabs\" partners include Humanity+, SENS Foundation, KurzweilAI, Science for Life Extension, Estropico. Four \"iLabs\" members are also in the Advisory Board of Lifeboat Foundation.\n\n“Semi-immortality” is the title of the \"iLabs\" volume detailing the first thirty years of research: since it is doubtful that full immortality can be truly achieved by physical entities, the term “semi-immortality” was preferred as the main tag over \"iLabs\" vision.\n\nThere are two explicit assumptions underlying the labs work: first, only a truly global perspective may lead mankind to a radical extension of the life-span. According to the founders, technology and science will play a major role, but without addressing serious challenges in ethics and social theory all the attempts would be doomed to failure. Second, achieving the “\"semi-immortality\"” is a hard, articulated task requiring huge efforts and resources, as well as the finest minds.\n\nIn 2008, \"iLabs\" published online the first version of their “Call for Players”, detailing the fields of study in their global agenda and explaining the labs’ structure and activities. The document’s explicit aim is to attract interested researchers to their projects.\n\nAccording to the document, the \"semi-immortality\" picture is declined in three core areas of study and intervention:\n\n\n\"iLabs\" projects in this area are based on the so-called \"Mathematics of the Models of Reference\". The theory –created by Gabriele Rossi and developed with Francesco Berto and Jacopo Tagliabue – use cellular automata theory as the building block of computation and a perfect isomorphism between matter and information as the main “philosophical” assumption.\n\n\"iLabs\" digital universe satisfies universality (it is equivalent to a Turing Machine) and perfect reversibility (a desideratum if one wants to easily preserve various quantities and never lose information), and it comes embedded in a first-order theory allowing computable, qualitative statements on the universe evolution.\n\nAfter two decades of studies on mind-body relations and psycho-oncology, \"iLabs\" started the development of a proprietary protocol for health monitoring, inspired by the success of evidence-based medicine.\n\nCollecting more than 80 million single blood tests, the Department of Psychoneurophysiology created \"iMed\", a statistical estimator of “biological age”. The algorithm takes as input a subject’s blood test and gives back a probability distribution; the statistics can then be used to compare the subject’s real age with her biological age, thereby yielding a quantitative assessment of the aging rate.\n\n\"iMed\" is now being expanded to include other simple and inexpensive test to improve the estimator accuracy: the hope is to develop an easy-to-use and reliable method to objectively assess the health status of any subject\n\nGiven the attention \"iLabs\" has always been putting on ethical aspects of the technological developments, it is not surprising that many activities are focused on forecasting social changes and planning new scenarios for justice, economy, education.\n\nGabriele Rossi – a popular entrepreneur and innovator in the Italian insurance market – wrote dozens of articles in popular and specialized magazines on the impact of the technological revolution (and the life-span extension) on the economy and the society as a whole.\n\nMore recently, \"iLabs\" published \"The Law in the Society of Semi-Immortality\": the essay argues for two radical changes in how justice and morality are currently conceived. Approaching the Singularity, artificial minds could be used to make the whole process of justice administration perfectly objective, progressively eliminating the human bias in legal judgments. Second, new values are likely to be promoted by the exponential technological trend: the book holds that, in the near future, the value of Truth is likely to have increasing importance in advanced societies.\n\nThe two claims are not uncontroversial, as several speakers and critics pointed out during the first presentations of the volume in the academic (SIFA 2010) and transhumanist community (Transvision 2010). To promote an informed debate on ethical and social issues of the technological evolution, \"iLabs\" writes monthly contributions to Italian national magazines and blogs.\n\n\n"}
{"id": "1521870", "url": "https://en.wikipedia.org/wiki?curid=1521870", "title": "Ivan Puluj", "text": "Ivan Puluj\n\nIvan Pului (son of Iwan Pului ; ; 2 February 1845 – 31 January 1918) was a Ukrainian physicist and inventor, who has been championed as an early developer of the use of X-rays for medical imaging. His contributions were largely neglected until the end of the 20th century.\n\nIvan Pului graduated with honors from Theological Faculty of the University of Vienna (1869), later also from the Department of Philosophy (1872). In 1876 Pulyui finished his doctorate on internal friction in gases at the University of Strasbourg under supervision of August Kundt. Pului taught at the Navy academy in Fiume (Rijeka, Croatia) (1874–1876), University of Vienna (1874–1884) and the German part of the Higher Technical School in Prague (1884–1916). He served as the rector of the Higher Technical School in Prague (German part) in 1888–1889. Puluj also worked as a state adviser on electrical engineering for Bohemian and Moravian local governments.\n\nIn addition he completed a translation of the Bible into the Ukrainian language.\n\nPuluj did heavy research into cathode rays, publishing several papers about those rays between 1880 and 1882. In 1881 as a result of experiments into what he called \"cold light\" Prof. Puluj developed the Puluj lamp; it was awarded the Silver Medal at the International Electrotechnical Exhibition in Paris, 1881. Throughout the world, it has become known as the \"lamp of Puluj\" and even it was mass-produced for some time.\n\nPuluj expertimented with his new device and published his results in a scientific paper, \"Luminous Electrical Matter and the Fourth State of Matter\" in the \"Notes of the Austrian Imperial Academy of Sciences\" (1880–1883), but expressed his ideas in an obscure manner using obsolete terminology. Puluj did gain some recognition when the work was translated and published as a book by the Royal Society in the UK.\n\nWhile Professor Puluj's finding were essentially X-rays, he did not recognise them as such at first, although he demonstrated X-ray pictures of a hand and fingers obtained by using his tube / lamp to his students. This credit later went on to Wilhelm Conrad Röntgen. Once Roentgen visited Professor Puluj's laboratory and the latter presented one of his tubes to Roentgen. Roentgen went home and in his laboratory started to conduct experiments with Puluj's tube. Puluj also continued to do research with his X-ray tubes. On 8 February 1896, just 6 weeks after Röntgen presented his finding about X-Rays, Puluj published his own findings in the French journal La Nature in Paris. He presented photographs that exhibited the skeleton of a stillborn child. His work was republished in various European scientific journals. Puluj would release further images of human body parts, including an image of a fractured human hand, and would suggest possible medical usages of this new technology. The quality of Puluj's pictures was much better than that of Roentgen's. \n\nPuluj made many other discoveries as well. He is particularly noted for inventing a device for determining the mechanical equivalent of heat that was exhibited at the \"Exposition Universelle\", Paris, 1878. Puluj also participated in opening of several power plants in Austria-Hungary.\n\n\n\n\nhttp://www.zobodat.at/pdf/SBAWW_81_2_0864-0923.pdf\n\nPuluj is also known for his contribution in promoting Ukrainian culture. He actively supported opening of a Ukrainian university in Lviv and published articles to support Ukrainian language. Together with P. Kulish and I. Nechuy-Levytsky he translated Gospels and Psalter into Ukrainian. Being a professor Puluj organized scholarships for Ukrainian students in Austria-Hungary.\n\n\n"}
{"id": "54438257", "url": "https://en.wikipedia.org/wiki?curid=54438257", "title": "Jenni Sidey", "text": "Jenni Sidey\n\nJennifer Anne MacKinnon Sidey-Gibbons (born August 3, 1988) is a Canadian astronaut, engineer, and lecturer. She was selected by the Canadian Space Agency (CSA) as one of the two members of the 2017 CSA group.\n\nDr. Jennifer (Jenni) Anne MacKinnon Sidey-Gibbons was born on August 3 1988 in Calgary, Alberta. \n\nJenni's interest in science was supported by her mother, who often took her to museums and found role models from the fields of Science and Engineering. Her uncle, a civil engineer, would also involve her in design tasks. One of the aforementioned tasks included designing a baseball pitch that would drain effectively during rain storms or a passive water treatment plant.\n\nJenni holds an honours bachelor's degree in Mechanical Engineering from McGill University. Whilst she was at McGill, she carried out research in collaboration with the Canadian Space Agency (CSA) and the National Research Council Flight Research Laboratory on flame propagation in microgravity. She then completed a Ph.D in 2015 in engineering at Jesus College, University of Cambridge, where she focused on combustion under the supervision of Professor Nondas Mastorakos.\n\nPrior to joining the Canadian Space Agency, Sidey-Gibbons worked as an assistant professor in internal combustion engines at the Department of Engineering of the University of Cambridge. The focus of her research was flames, how they are used, and how to stop them from emitting harmful pollutants. Precisely, she worked on the development of low-emission combustors for gas turbine engines.\n\nShe also taught undergraduate and graduate students in the Energy, Fluid Mechanics and Turbomachinery Division on topics ranging from conventional and alternative energy production to introductory thermodynamics and flame physics.\n\nAside from these formal responsibilities, she also actively acted as a role model for young women considering technical careers in science-related fields. Most notably, she is the co-founder of the Cambridge chapter of Robogals, a student-run international organization that aims to inspire and empower young women to study science, technology, engineering, and mathematics (STEM) through fun and educational initiatives. Through this work, she has taught programming to over 3,000 young girls across the United Kingdom.\n\nIn 2016, she was awarded the Institution of Engineering and Technology's Young Woman Engineer of the Year Award, as well as a Royal Academy of Engineering Young Engineer of the Year Award.\n\n"}
{"id": "10767533", "url": "https://en.wikipedia.org/wiki?curid=10767533", "title": "Journal of Environmental Engineering", "text": "Journal of Environmental Engineering\n\nThe Journal of Environmental Engineering is a monthly engineering journal published by the American Society of Civil Engineers. The journal covers interdisciplinary aspects of the research and practice in environmental engineering, systems engineering, and sanitation. Papers focus on design, development of engineering methods, management, governmental policies, and societal impacts of wastewater collection and treatment; the fate and transport of contaminants on watersheds, in surface waters, in groundwater, in soil, and in the atmosphere; environmental biology, microbiology, chemistry, fluid mechanics, and physical processes that control natural concentrations and dispersion of wastes in air, water, and soil; nonpoint-source pollution on watersheds, in streams, in groundwater, in lakes, and in estuaries and coastal areas; treatment, management, and control of hazardous wastes; control and monitoring of air pollution and acid deposition; airshed management; and design and management of solid waste facilities. A balanced contribution from consultants, practicing engineers, and researchers is sought on engineering solutions, and professional obligations and responsibilities.\n\nThe main editor is Dionysios D. Dionysiou of University of Cincinnati.\n\n"}
{"id": "27616229", "url": "https://en.wikipedia.org/wiki?curid=27616229", "title": "Kemao Market", "text": "Kemao Market\n\nKemao Market is one of five major electronics markets in Zhongguancun, Beijing.\n\nKemao Market opened its doors on 2004-02-21. The building is rated 5A-class and features a four-star hotel.\n\n\n"}
{"id": "16952826", "url": "https://en.wikipedia.org/wiki?curid=16952826", "title": "LIRIC Associates", "text": "LIRIC Associates\n\nLIRIC Associates was a computer security consultancy in the United Kingdom that offered risk and security assessment for global networks. It also provided advice on the design, architecture and policies required to secure complex global networks. The company was a provider of Information Technology, IT Security and Telecommunications. Its clients included UK government departments and private sector organizations.\n\nIt was acquired by Symantec on September 17, 2004.\n"}
{"id": "29259940", "url": "https://en.wikipedia.org/wiki?curid=29259940", "title": "LatencyTOP", "text": "LatencyTOP\n\nLatencyTOP is a Linux application for identifying operating system latency within the kernel and find out the operations/actions which cause the latency. LatencyTOP is a tool for software developers to visualize system latencies. Based on these observations, the source code of the application or kernel can be modified to reduce latency. It was released by Intel in 2008 under the GPLv2 license. It works for Intel, AMD and ARM processors.\n\nThe project appears to be dead as the website is gone and there have not been commits since 2009.\n\n\n"}
{"id": "12305782", "url": "https://en.wikipedia.org/wiki?curid=12305782", "title": "Lonza Group", "text": "Lonza Group\n\nLonza Group is a Swiss multinational, chemicals and biotechnology company, headquartered in Basel, with major facilities in Europe, North America and South Asia. Lonza was established under that name in the late 19th-century in Switzerland. The company provides product development services to the pharmaceutical and biologic industries, including organic, fine and performance chemicals, custom manufacturing of biopharmaceuticals, chemical synthesis capabilities, detection systems and services for the bioscience sector. \n\nLonza was founded in 1897 in the small Swiss town of Gampel, situated in the canton of Valais, taking its name from the nearby river. Initially the company produced electricity used to manufacture chemicals such as calcium carbide. Lonza moved to neighbouring Visp (where it retains a production site today) in 1909 and began manufacturing synthetic fertilisers, and moves into vitamins, acids, intermediates and additives followed. In 1974, the group merged with aluminium firm Alusuisse, after which the group moved into the biotechnology sector. Lonza de-merged from the Alusuisse-Lonza Group in 1999 and listed on the SWX Swiss Exchange.\n\nThe company expanded in the United States in 1969 and acquired smaller biopharmaceutical units in recent years. In 1996, Lonza acquired Celltech Biologics and began producing mammalian cell cultures and monoclonal antibodies. In October 2011, Lonza acquired American firm Arch Chemicals for $1.4 billion, as a result becoming the world's largest manufacturer of biocides. On 15 August 2016, the group announced its intention to acquire InterHealth Nutraceuticals, a leader in research, development, manufacture and marketing of value-added nutritional ingredients for use in dietary supplements. The acquisition of the US based company will be done at a value of up to . In December 2016, the company announced it would acquire Capsugel for , from private equity firm KKR.\n\nLonza is involved in the manufacturing of biologics with several pharmaceutical companies. Lonza entered into a partnership with Teva in 2009 to develop and manufacture biosimilars. In 2010, they made a deal with GlaxoSmithKline to manufacture therapeutic monoclonal antibodies. In 2014, Lonza entered into an agreement with Bristol-Myers Squibb to manufacture two biologic drugs. Lonza also manufactures Imbruvica for Pharmacyclics and Mydicar for Celladon. In 2015, Lonza contracted with Alexion to construct a new facility dedicated to Alexion manufacturing. In 2017, Lonza and Sanofi partnered to construct a new facility for production of biologics.\n\n, Lonza's chief executive officer (CEO) was Richard Ridinger, while the CEO during 2010–2013 was Stefan Borgas.\n\n, Lonza had a twenty-four self-described major sites located in the United States, India, the Czech Republic, Belgium, Spain, China, France, South Africa, Switzerland, Singapore, Mexico, Japan, Brazil and the United Kingdom. This site accounting shows significant changes from a 2016 self-description, which listed sixteen major sites. The presence in India was established in Genome Valley, where ground was broken in 2011.\n\nIn 2018, Lonza opened a facility in the United States in Pearland, Texas (a suburb of Houston). This is one of four sites which are focused on cell and gene therapy, the other three being in Portsmouth, New Hampshire; Geleen, Netherlands; and Singapore.\n\n, the company employed around 10,000 people, across about 18 countries. employment was noted as 14,500 across 100 sites.\n"}
{"id": "11270303", "url": "https://en.wikipedia.org/wiki?curid=11270303", "title": "Metadyne", "text": "Metadyne\n\nA metadyne is a direct current electrical machine with two pairs of brushes. It can be used as an amplifier or rotary transformer. It is similar to a third brush dynamo but has additional regulator or \"variator\" windings. It is also similar to an amplidyne except that the latter has a compensating winding which fully counteracts the effect of the flux produced by the load current. The technical description is \"a cross-field direct current machine designed to utilize armature reaction\". A metadyne can convert a constant-voltage input into a constant-current, variable-voltage output.\n\nThe word \"metadyne\" is derived from the Greek words for conversion of power. While the name is believed to have been coined by Joseph Maximus Pestarini (Italian language Giuseppe Massimo Pestarini) in a paper which he submitted to the Montefiore International Contest at Liège, Belgium in 1928, the type of machine which it described had been known since the 1880s. The first known British patent for a direct-current, cross-field generator was obtained by A. I. Gravier of Paris in 1882, and two further patents were obtained by E. Rosenberg in 1904 and 1907. Rosenberg later became the chief electrical engineer for Metropolitan-Vickers, and his machine produced a cross field by applying a short-circuit to an additional set of brushes. M. Osnos looked at the practical arrangements for several such machines in 1907, and in the same year, Felton and Guilleaume obtained a British patent, number 26,607, which described auxiliary windings, armature windings and multiple commutators, although all in fairly general terms. He also indicated that they could be used to transform a constant voltage into a constant current. Other patents were obtained prior to 1910 by Mather & Platt, Brown Boverei and Bruce Peebles.\n\nPestarini worked on developing the theory of such machines between 1922 and 1930, although he concentrated on their static characteristics, rather than their dynamic characteristics. He contributed three papers on the subject to \"Revue Générale de l'Electricité\" in 1930, which included some practical applications. The main one was the use of the constant-current output for the control of traction motors on electric vehicles and the operation of cranes, areas in which he had some practical experience, following trials in conjunction with the Alsthom Company in France. In 1930, he made a trip to Britain, and the Metropolitan-Vickers company took his ideas and developed a working system. Unlike Rosenberg's solution, Pestarini, who later became a Professor at the Institute Electrotechnico Nazionale Galileo Ferraris in Turin, connected the additional brushes to an external supply to produce a transformer metadyne. The machine worked as a voltage-to-current amplifier because the flux generated by the current to the load opposed the flux in the control circuit. Development work at Metropolitan-Vickers in the 1930s was led by Arnold Tustin, and the company held the British patents for the Metadyne.\n\nPestarini also visited the United States in 1930, although there is no record of the system being used there. The General Electric engineers, led by Ernst Alexanderson, were interested, but modified the design by the addition of a compensating winding, which counteracted the effect of the flux produced by the load current. This turned the machine from a voltage-to-current amplifier into a voltage-to-voltage amplifier, and they called the new variant an Amplidyne. The development costs were largely funded by US naval contracts for the development of vertical stabilisers, which were used to improve the aiming and firing of guns on ships. During the same period, the Macfarlane Engineering Company, who were based in Glasgow, developed a variant of the cross field machine quite independently, which they named the \"Magnicon\".\n\nPestarini filed a patent on the metadyne machine in France on 14 January 1932, and submitted it to the United States Patent Office at the end of the year, on 23 December. The US patent was granted on 30 January 1934. He submitted a second US patent for an improved machine in November 1946, which was granted on 10 June 1952.\n\nThe diagram shows three arrangements of a metadyne machine. In all cases, compensation windings have been omitted for clarity. The first arrangement represents a one-cycle cross-field machine. In a normal DC machine, the effect of the excitation current generates a flux (A1), which in turn generates a quadrature flux which is at right angles to the exciting flux. By wiring the quadrature brushes together, current is produced in the armature, and the flux that this produces (A2) is again at right angles to the quadrature axis, resulting in an armature reaction which is directly opposed to the original excitation. This feature is fundamental to the machine, and does not depend on its direction of rotation. When the armature reaction is partially compensated by a compensation winding, the uncompensated portion of the armature reaction acts in this way. As the output current rises, it suppresses the effect of the excitation, until it reaches a state where there is just enough excitation to maintain the current. Any further increase would eliminate the flux which sustains its operation, and the current is maintained irrespective of the resistance of the load or the back emf produced by it. The machine thus acts as a constant-current generator, where the current is proportional to the excitation.\n\nThe second diagram shows a machine with no excitation winding, but instead, a constant voltage is connected to the quadrature brushes. This produces a flux similar to the one produced by the rotation of the armature in the excitation flux in the first example. The operation of the machine is therefore very similar, with the output current rising until the flux it produces almost counteracts the flux generated by the applied voltage. Tustin has shown that the input and output power are the same, and so the machine transforms the constant-voltage input into a constant-current output. As with the metadyne generator, the Metadyne transformer can be partially compensated, and will continue to operate as a constant-current device until the compensation exceeds 97 per cent.\n\nThe third diagram shows a metadyne connected to two separate motors, and this arrangement was often used for the control of traction motors on electric trains. Connecting them in this way reduces the effective loading on the Metadyne, and enables a smaller machine to be installed. The Metadyne acts as a \"positive or negative booster\". If Vcc is the supply voltage, and V is the output voltage of the Metadyne, then the total voltage across the load can vary from 0 to 2·Vcc, as V varies between −Vcc and +Vcc. Although the system is prone to the currents in the two halves of the load becoming unbalanced, this can be corrected by the provision of extra series windings, which act like an additional circuit resistance.\n\nThe Rosenberg generator is very similar to the Metadyne generator, both in its construction and its electrical connection. It generally does not have a compensation winding, so that the whole of the armature reaction opposes the initial excitation. Parts of the magnetic circuit are normally not laminated, which creates delays between excitations and fluxes, but the machines are used in applications where a quick response is not essential. Their predominant use has been in trains, where they are axle-driven, and used to provide lighting and the charging of batteries. An axle-driven generator is subject to variable speeds and changes in the direction of rotation, but the characteristics of the machine allow it to produce useful energy down to very low speeds. At slow speeds, the output voltage increases with the square of the speed, but the magnetic circuit soon becomes saturated, resulting in much smaller increases as the speed increases. When used in circuits which include batteries charged from the output, a rectifier or reverse-current cut-out is normally required to prevent discharge of the batteries through the generator at very low speeds, or when the train stops.\n\nThe Magnicon, developed by Macfarlane's in Scotland, is similar to the Metadyne, but whereas the latter has a two-pole armature winding, the Magnicon has a four-pole lap winding, and is sometimes referred to as a Metadyne with a short-pitched armature winding. They have been supplied to operate hoists and winches on ships. The stator of a Magnicon has four polar projections, spaced at 90 degrees, and one pair of them is excited. The pair of brushes which are on the same axis as the excited poles are short-circuited, resulting in a large current. The magnetomotive force (MMF) of this current acts on the non-excited poles, creating a working flux (Φ) and the output voltage. As with a full-pitch Metadyne, the armature reaction of the output current is 90 degrees out of phase, and therefore opposes the original excitation. Advantages over the normal Metadyne are that the number of exciting and compensating coils is halved to two each per cycle, and the shorter pitch of the coils results in less overhang at the ends of the windings. However, the design creates idle currents in the armature, which result in losses, and on larger machines, where interpoles are required, each interpole must be fitted with two coils, one for each of the brush circuits. Tustin argues that there is little advantage of the Magnicon over the Metadyne for smaller machines, and for larger machines, which require interpoles to be fitted, insufficient analysis has been carried out to make a judgement.\n\nMetadynes have been used to control the aiming of large guns and for speed control in electric trains, in particular the London Underground O and P Stock. They have been superseded by solid state devices.\n\nIn the early 1930s, the London Underground were aware of the development of the metadyne equipment taking place at Metropolitan-Vickers, and the potential for regenerative braking which it provided. Before committing to an untried system, they therefore built a test train, by converting six cars originally built between 1904 and 1907 for the Metropolitan Railway. The work was carried out at Acton Works in 1934. Since a single metadyne unit could be used to control four motors, and each motor car had two motors, they were formed into two-car units, with a driving cab at the outer ends. By coupling the units together, trials of a two-car, four-car and six-car train could be carried out. The metadyne unit weighed about 3 tons, and consisted of three rotating machines, an exciter, a regulator and the actual metadyne machine, which were linked together mechanically. Electrically, the traction supply was fed into the machine, and the output fed the motors, without the need for starting resistances.\n\nThe test train ran through much of 1935 and 1936, and was tried on nearly all of the electrified tracks on the Metropolitan line and the District line. Once the concept had been proved to be reliable, the train was also used in passenger service. Besides the regenerative braking, the acceleration was found to be particularly smooth. When the decision was taken to proceed with the new system on the O and P stock, the test train was dismantled, and the equipment was fitted to three battery locomotives built by the Gloucester Railway Carriage and Wagon Company, which were part of a batch of nine vehicles supplied between 1936 and 1938. The equipment was particularly suitable for battery locomotives, as the lack of starting resistances reduced the amount of power wasted when starting and stopping frequently. At slow speeds, conventional control systems would often overheat, but the metadyne-equipped locomotives could pull trains weighing 100 tons for long distances at speeds as low as without problems. However, the complexity of the equipment, and the difficulty of maintaining the metadyne machine, resulted in the locomotives not being used sufficiently, and they were withdrawn for scrapping in 1977.\n\nThe main production run of O Stock consisted of 116 motor cars, which were formed into 58 two-car units. Trials began with a four-car formation on the District line between High Street Kensington and Putney Bridge in September 1937, and a six-car formation began working on the Hammersmith line in January 1938. There were some technical problems, caused by the demands put on the power supply system when a train of six motor cars started, and the amount of power that such a train attempted to return to the system when the regenerative brakes were used. This was partially mitigated by ordering a further 58 trailer cars, and converting each two-car unit into a three-car unit, by inserting a trailer car into the formation. A batch of P Stock was then ordered, to replace the trains on the Metropolitan line. Although O and P Stock units could be coupled together, the metadyne units in particular were not the same, and could not be interchanged between the builds. By the early 1950s, this was a serious disadvantage, when a series of failures occurred, which required extensive repairs. A decision was taken to remove the equipment and replace it with a Pneumatic Cam Motor (PCM) system, using spare controllers from the 1938 tube stock. The first converted train entered service on 31 March 1955, and the stock was redesignated at CO/CP Stock, since it contained cars from both batches. All of the metadyne equipment was subsequently replaced.\n\nDespite the shortcomings which led to its demise, the metadyne system as introduced in 1936 on the O Stock trains was the first in the world to provide regenerative braking on an electric multiple unit. Acceleration was smoother than on a train which switched starting resistances, and when braking the metadyne unit returned power to the tracks, which could be used by other trains if needed. However, conditions were not always ideal, and the substations were not really designed to cope with regeneration, which meant that often the train switched to rheostatic braking, where the power was dissipated in a resistance bank. The weight of the equipment was also a serious drawback.\n\nIn the period immediately before the Second World War, there was increasing interest in power-operated gun controls, although military authorities were nervous of introducing a complex system which would have to be maintained in the field. However, with the increasing speed of aircraft, the need to enable searchlights, anti-aircraft guns and naval guns to move ever faster to track their movement meant the some form of powered control was essential. Engineers were faced with the problem of making a heavy piece of equipment, such as a gun on its mounting carriage, track an input signal in a smooth and accurate fashion, with very little lag between changes in the input and the actual position of the gun mount. The gun needed to be aimed at the target at all times, and moving at the correct velocity to remain so.\n\nA human operator anticipates errors, and can also compensate for known lags in the operation of the system. Mimicking of this behaviour had been achieved for electronic signals and low-power electromechanical systems, but gun control was on a completely different scale, with machinery weighing tons and having significant inertia needing to move at velocities of up to 30 degrees per second, and accelerations of 10 degrees per second. In 1937, the Admiralty had placed an order with Metropolitan Vickers for a control system for an eight-barrel Pom-Pom gun. Pestarini had developed a similar system for the Italian navy. The original design used a single Metadyne to supply a constant current to the armatures of motors mounted on several guns. Each was then controlled by manually adjusting the field current. Tustin, who did most of the design work, found that the system had a large time constant, due to the inductance of the field windings. In order to improve its response, he supplied the field windings with a constant current, and used a partially compensated Metadyne to control the armature current of each motor. Tustin compared Ward Leonard control systems, Metadynes and Amplidynes, and accepted that each had its merits, but favoured the Metadyne, of which he had several years experience from their use in traction control.\n\n"}
{"id": "6738827", "url": "https://en.wikipedia.org/wiki?curid=6738827", "title": "Mobile phone throwing", "text": "Mobile phone throwing\n\nMobile phone throwing is an international sport that started in Finland in the year 2000. It's a sport in which participants throw mobile phones and are judged on distance or technique. World record holder is Tom Philipp Reinhardt from Germany with a throw of 136,75m.\n\nThere are usually four categories in the sport:\n\nThe phones used vary not just between events but between competitors, with any phone that weighs over 220 grams being acceptable. At some events, the choice is down to personal preference from those provided by the event organisers, while others provide only one model of phone.\n\nThe sponsor provides the phones used in the Mobile Phone Throwing World Championships. There are many different kinds of phones to choose from, weight between 220g to 400g. Every Competitor can choose between any of the phones that are provided. During the throw the competitor must stay within the throwing area. If the thrower steps over the area, the throw will be disqualified. The phone must land within the marked throwing sector. The official jury of the competition will accept or disqualify the throw. Also there is no dope testing but if the Jury believes the competitor is not mentally or physically able to throw they will not let them. The jury’s decisions are also final and cannot be protested.\n\nMobile Phone Throwing became a hit when first arranged in year 2000 in Savonlinna, Finland. Organizer was a translation and interpretation company Fennolingua. Its multi-national personnel and many athletics threw away their frustrations along with the mobile phones. Local recycling centers were a partner and they collected all the toxic waste.\nThe Mobile Phone Throwing World Championships has been arranged every year since the year 2000 in Savonlinna in late August. All the international interest on the Championships has now led to official national championships all over Europe. The First national championships were organized in Trondheim, Norway in June 2004. The first prize was a trip to the World Championships to Savonlinna, Finland. In February 2005 there was the first winter championships in Stoos, Switzerland. The winners from both Original and Freestyle categories won trips to the World Championships to Savonlinna. The first German championships were arranged in June 2005 in Northern Germany. People could also buy a new mobile phone at the Championships. The winner gets a new mobile phone. Throwers and international media were immediately very interested and every year the Championships gathered a wide range of nationalities to Savonlinna to throw the cell phones.\n\nLawrence University has hosted a Rotary Phone Throw in 2005, 2006, and 2007. This competition has similar rules to the mobile phone throw, yet uses rotary phones.\n\nNokia, which is one of the leading cell phone giants in the world, has its headquarters in Finland. Lead organizer Christine Land was inspired to create the event back in 2000. Back in the first event, a leading insurance company in Finland sponsored the event for recycling purposes. According to insurance companies, there are a lot of cell phones lying in thousands of lakes all around Finland. Many phones are returned to their mobile carrier stores in return for a new one; however, others are discarded instead of being recycled and they become a toxic waste. The battery in the cell phone can ultimately become a toxic waste and it must be taken care of properly.\n\nMillions of phones are replaced all over the world in favor for new ones. About 70 percent of overall toxic in landfills is made up by electronic scraps. Recycling these electronics hasn’t been made a huge priority in a lot of places all over the world. Many of the cell phones that we currently use contain products like plastic and metal that can be recycled on a daily basis. These being recycled can easily save energy. When these metals and plastics are place into landfills, they can pollute the air and contaminate some of our drinking water.\n\nLead, which comes in the coating of the cell phone, is a toxic element that can result in serious health effects when in high levels. When electronic waste is burned, it can produce toxic fumes that are very dangerous. Mobile phone recycling is a very important issue that insurance companies that sponsor the Mobile Phone Throwing Championships are very aware of.\n\nThe Mobile Phone Throwing World Championships have been held annually since 2000 in Savonlinna, Finland. The first national competition was held in Trondheim, Norway, in June 2004, with several other countries across Europe also staging their own events. The first winter championships were held in Stoos, Switzerland, in February 2005. Commonly, the prize for first place in a national event is entry to the world championships, and the grand prize for winning the world championship is a new mobile phone. Many events are supported by mobile phone recycling organizations and promote the recycling of the phones.\n\nIn the UK, the championships are held every August and organized by 8th Day UK Ltd. The first event, in 2005, was held at Richmond Golf Driving Range, and the 2006 event at Tooting Bec Athletics Track in London. ActionAid Recycling partnered the event to raise awareness for mobile phone recycling and raised money from donated phones for the first two years, but due to the involvement of a casino (Golden Palace) in the 2007 event and the associations this would have in relation to charity, are no longer involved.\n\nIn 2007, the UK event was held at Old Hamptonians Rugby Club on 12 August, with throws recorded from 3.70 m to 95.83 m, a new unofficial world record. The men's winner was Chris Hughff, and the ladies' winner was Jan Singleton, both successfully defending their 2006 titles. There were also throws recorded by a penguin and a gorilla, prompting a new category — fancy dress – to be incorporated for 2008. The 2008 competition was held at the same location, with Jan Singleton defending her title and Jeremy Gallop claiming the men's title. Madeleine James set a new benchmark for the Under 3 Category of 1.57 m.\n\nThe 2007 world championships were in Savonlinna as usual. The winner of the men's freestyle event, Taco Cohen of the Netherlands, won for a novel performance that incorporated juggling and acrobatics.\n\nTwo thousand nine saw the UK event move to Battersea Park Athletics stadium, with Jeremy Gallop (89.10 m) defending his title and Julia Geene (33.40 m) winning the ladies' event. Peter Yates set a new Vets record of 75.20 m, and Oliver James set an Under 2 record of 2.05 m. The fancy dress event was won by Morph, who threw 54.73 m. The event was run alongside the Sumo Suit Athletics World Championships.\n\nIn 2010, the event was again held together with the Sumo Suit Athletics World Championships in Battersea Park Athletics track. Both Jeremy Gallop and Julia Geens successfully defended their titles, with throws of 88.51 m and 32.00 m respectively. In the men's event, there was a 1st prize of an Xbox 360 sponsored by BuyMobilePhones.net in an effort to raise awareness about mobile recycling.\n\nThe World event for 2010 was to be held on 21 August 2010 but for unknown reasons (as yet), was canceled.\n\nOn August 24, 2013, the World Mobile Phone Throwing Championships took place in Savonlinna, Finland. Over eighty people from six different countries participated in the event that has grown immensely since its conception in 2000. The 2013 Champion for the Men’s Traditional Division, Riku Haverinen, cast his 220-gram mobile device a staggering 97.7 meters (the length of a full soccer field.) This is five meters short of the World record of 102.68 m set in 2012 by Chris Hughff from Great Britain. Coming in second in the Men’s Traditional Division was Ikka Aaltonen with 87.88 m, followed by Otto Sammalisto with a valiant effort of 86.09 m. These three top performers, all from Finland, represented the host country in the 2013 Championships. In the Women’s Traditional Division, the champion Asa Lundgren, threw her mobile device 40.41 m, impressive as it was only after a few times practicing. The Women’s Traditional Division was much closer than the Men’s, with second place with a score of 39.88 m from Louise Van De Ginste, Belgian winner from earlier in the summer of 2013. Van De Ginste was followed by third-place finisher Tanja Pakarinen, who launched a mobile device 36.69 m. Again the Finns dominated the competition with two of the top three finishers being from Finland. The Freestyle division was won by Erika Vilpponen, with a stellar performance with a circus bike paired with a throw of the mobile device backwards over the shoulder. It was a clear victory for Erika according to the panel of judges. After this year’s championships, new divisions are being looked at for future championships.\n\nOther tournaments\n\nThe Spanish championships are held in Tarragona, Catalonia, during the celebrations of Carnival in February or March. The 2009 tournament was held on Sunday, 22 February in the Francoli Park, Tarragona.\n\nOn August 1, 2008, the United States held its first national championship in South Hadley, Massachusetts, at Buttery Brook Park, sponsored by Family Wireless. The United States event has also dedicated itself to recycling cellular phones.\n\nThere was a second cell phone throwing contest held in the United States on July 5, 2009, at Szot Park in Chicopee, Massachusetts. It was sponsored by Family Wireless. The finalist for the American event was Daniel Taylor from San Diego, California. He threw an iPhone for the distance of 78.9 yards.\n\nThe first Czech mobile phone throwing tournament was held in June 2012 by non-profit organization ASEKOL which collects waste electrical and electronic equipment. The championship was held to inform people about separation of electric waste. It took place in Prague and world records were set in both men's and women's categories. Men’s record was beaten later in Savonlinna but the women’s record from Czech Republic is still the best one in its category. It was set by Tereza Kopicová who threw a cell phone 60.24 meters.\n\nThe first National Championships for Liechtenstein were held on 29 May 2010 in the town Ruggell. They were completed in accordance with the Finnish rules with the same four categories.\n\nJohannes Heeb won in the men's category with a throw of 74.2 m, and the women's category was won by Stephanie Parusel, throwing 48.5 m.\nManuel Hug won the juniors with a throw of 55.9 m, while the team category was won by Team \"Schulzentrum Unterland\" with 196 m.\nThe current Swiss national record holder, Dino Roguljic, made a \"celebrity\" appearance and managed to throw 66.2 m.\n\nFreestyle phone throwing is an event held in which contestants enter a mat that is around 1\" thick to soften the impact in case a phone is dropped. Players begin performing a series of tricks that include various flips, spins, high throws, etc. If one drops one's mobile phone, he or she is automatically out, and judges take his score from that point. One minute is given for a total \"run time\", and after the minute is up, if one has not dropped one's phone, he or she gets a score from 1 to 100. Phone classes include standard phones, which are free, to $50 phones with either no moving device such as slide or flip open, or with the standard flip open style. Second class is the \"Advanced\" class, which includes phones with full keyboards, slide movement devices, side-kick flip devices, or touch screens. This class is based on the fact that if the phone is dropped, there is more of a consequence since the phones are so elaborate and/or expensive.\n\nThe first National Championships for Belgium were held in July 2006 in Ghent. Since 2010 JIM Mobile is organizing the event. Last year's champion had a throw of 62.70 m. The Belgian Champion of 2011 had a throw of 63.94 m. More info can be found at www.bkgsmwerpen.be or www.jimmobile.be.\nOn 27 August 2014 javelin thrower Dries Feremans threw a world record of 110m42 in Kessel-Lo.\n\nIn August 2016 in Korneuburg the \"Internationalen Österreichischen Mobiltelefonweitwurfmeisterschaften\" were hosted by Tw!ne. Winner of the event was Jürgen Eberhart with a distance of 62,80m. \nOn the 23.September 2017 the Austrian Athletics Federation organized a competition during a public sports day in Vienna. 8 young javelin thrower competed in this event and were throwing iPhones. Adam Wiener won the event with 92,34m and his Sister Ivonne had a distance of 67,58m. oelv.at \n\nThe current records are:\n\n\n\nRecord Sources:\n\n"}
{"id": "350708", "url": "https://en.wikipedia.org/wiki?curid=350708", "title": "MultiMediaCard", "text": "MultiMediaCard\n\nIn consumer electronics, the MultiMediaCard (MMC) is a memory-card standard used for solid-state storage. Unveiled in 1997 by SanDisk and Siemens AG, MMC is based on a surface-contact low pin-count serial interface using a single memory stack substrate assembly, and is therefore much smaller than earlier systems based on high pin-count parallel interfaces using traditional surface-mount assembly such as CompactFlash. Both products were initially introduced using SanDisk NOR-based flash technology. MMC is about the size of a postage stamp: 24 mm × 32 mm × 1.4 mm. MMC originally used a 1-bit serial interface, but newer versions of the specification allow transfers of 4 or 8 bits at a time. MMC can be used in many devices that can use Secure Digital (SD) cards.\n\nTypically, an MMC operates as a storage medium for a portable device, in a form that can easily be removed for access by a PC. For example, a digital camera would use an MMC for storing image files. Via an MMC reader (typically a small box that connects via USB or some other serial connection, although some can be found integrated into the computer itself), a user could transfer pictures taken with the digital camera to his or her computer. Modern computers, both laptops and desktops, often have SD slots, which can additionally read MMCs if the operating system drivers can.\n\nMMCs are available in sizes up to and including 512 GB. They are used in almost every context in which memory cards are used, like cellular phones, digital audio players, digital cameras and PDAs. Since the introduction of SD cards, few companies build MMC slots into their devices (an exception is some mobile devices like the Nokia 9300 communicator in 2004, where the smaller size of the MMC is a benefit), but the slightly thinner, pin-compatible MMCs can be used in almost any device that can use SD cards if the software/firmware on the device is capable.\n\nWhile few companies build MMC slots into devices (SD cards are more common), the embedded MMC (eMMC) is still widely used in consumer electronics as a primary means of integrated storage in portable devices. It provides a low-cost flash-memory system with a built-in controller that can reside inside an Android or Windows phone or in a low-cost PC and can appear to its host as a bootable device, in lieu of a more expensive form of solid-state storage, such as a traditional solid-state drive.\n\nThis technology is a standard available to any company wanting to develop products based on it. There is no royalty charged for devices which host an MMC. A membership with the MMC Association must be purchased in order to manufacture the cards themselves.\n\nAs of July 2009, the specifications version 4.4 (dated March 2009) can be requested from the MMCA, and after registering with MMCA, can be downloaded free of charge. Older versions of the standard, as well as some optional enhancements to the standard such as MiCard and SecureMMC, must be purchased separately.\n\nA highly detailed version is available on-line that contains essential information for writing an MMC driver.\n\nAs of 23 September 2008, the MMCA group has turned over all specifications to the JEDEC organization including embedded MMC (e-MMC) and miCARD assets. JEDEC is an organization devoted to standards for the solid-state industry.\n\nAs of February 2015, the latest specifications version 5.1 can be requested from JEDEC, and after registering with JEDEC, can be downloaded free-of-charge. Older versions of the standard, as well as some optional enhancements to the standard such as MiCard and SecureMMC, must be purchased separately.\n\nIn 2004, the Reduced-Size MultiMediaCard (RS-MMC) was introduced as a smaller form factor of the MMC, about half the size: 24 mm × 18 mm × 1.4 mm. The RS-MMC uses a simple mechanical adapter to elongate the card so it can be used in any MMC (or SD) slot. RS-MMCs are currently available in sizes up to and including 2 GB.\n\nThe modern continuation of an RS-MMC is commonly known as MiniDrive (MD-MMC). A MiniDrive is generally a microSD card adapter in the RS-MMC form factor. This allows a user to take advantage of the wider range of modern MMCs available to exceed the historic 2 GB limitations of older chip technology.\n\nImplementations of RS-MMCs include Nokia and Siemens, who used RS-MMC in their Series 60 Symbian smartphones, the Nokia 770 Internet Tablet, and generations 65 and 75 (Siemens). However, since 2006 all of Nokia's new devices with card slots have used miniSD or microSD cards, with the company dropping support for the MMC standard in its products. Siemens exited the mobile phone business completely in 2006. Siemens continue to use MMC for some PLC storage leveraging MD-MMC advances.\n\nThe Dual-Voltage MultimediaCard (DV-MMC) is one of the first acceptable changes in MMC was the introduction of dual-voltage cards that can operate at 1.8 V in addition to 3.3 V. Running at lower voltages reduces the card's energy consumption, which is important in mobile devices. However, simple dual-voltage parts quickly went out of production in favour of MMCplus and MMCmobile which offer capabilities in addition to dual-voltage capability.\n\nThe version 4.x of the MMC standard, introduced in 2005, brought in two very significant changes to compete against SD cards: ability to run at higher speeds (26 MHz and 52 MHz) than the original MMC (20 MHz) or SD (25 MHz, 50 MHz) and a four- or eight-bit-wide data bus.\n\nVersion 4.x full-size cards and reduced-size cards can be marketed as MMCplus and MMCmobile respectively.\n\nVersion 4.x cards are fully backward compatible with existing readers but require updated hardware/software to use their new capabilities; even though the four-bit-wide bus and high-speed modes of operation are deliberately electrically compatible with SD, the initialization protocol is different, so firmware/software updates are required to use these features in an SD reader.\n\nMMCmicro is a micro-size version of MMC. With dimensions of 14 mm × 12 mm × 1.1 mm, it is even smaller and thinner than RS-MMC. Like MMCmobile, MMCmicro allows dual voltage, is backward compatible with MMC, and can be used in full-size MMC and SD slots with a mechanical adapter. MMCmicro cards have the high-speed and four-bit-bus features of the 4.x spec but not the eight-bit bus, due to the absence of the extra pins.\n\nIt was formerly known as \"S-card\" when introduced by Samsung on 13 December 2004. It was later adapted and introduced in 2005 by the MultiMediaCard Association (MMCA) as the third form factor memory card in the \"MultiMediaCard\" family.\n\nMMCmicro appears very similar to microSD but the two formats are not physically compatible and have incompatible pinouts.\n\nThe MiCard is a backward-compatible extension of the MMC standard with a theoretical maximum size of 2048 GB (2 TB) announced on 2 June 2007. The card is composed of two detachable parts, much like a microSD card with an SD adapter. The small memory card fits directly in a USB port while it also has MMC-compatible electrical contacts, which with an included electromechanical adapter fits in traditional MMC and SD card readers. To date, only one manufacturer (Pretec) has produced cards in this format.\n\nDeveloped by Industrial Technology Research Institute of Taiwan, at the time of the announcement twelve Taiwanese companies (including ADATA Technology, Asustek, BenQ, Carry Computer Eng. Co., C-One Technology, DBTel, Power Digital Card Co., and RiCHIP) had signed on to manufacture the new memory card. However, as of June 2011 none of the listed companies had released any such cards, and nor had any further announcements been made about plans for the format.\n\nThe card was announced to be available starting in the third quarter of 2007. It was expected to save the 12 Taiwanese companies who planned to manufacture the product and related hardware up to US$40 million in licensing fees, that presumably would otherwise be paid to owners of competing flash memory formats. The initial card was to have a capacity of 8 GB, while the standard would allow sizes up to 2048 GB. It was stated to have data transfer speeds of 480 Mbit/s (60 Mbyte/s), with plans to increase data throughput over time.\n\nAn additional, optional, part of the MMC 4.x specification is a DRM mechanism intended to enable MMC to compete with SD or Memory Stick in this area. Very little information is known about how SecureMMC works or how its DRM characteristics compare with its competitors.\n\nThe eMMC (embedded MMC) architecture puts the MMC components (flash memory plus controller) into a small ball grid array (BGA) IC package for use in circuit boards as an embedded non-volatile memory system. eMMC exists in 100, 153, 169 ball packages and is based on an 8-bit parallel interface. This is noticeably different from other versions of MMC as this is not a user-removable card, but rather a permanent attachment to the circuit board. In the event of an issue stemming from either the memory or its controller, the entire PCB (Printed Circuit Board) would need to be replaced.\n\neMMC does not support the SPI-bus protocol.\n\nAlmost all mobile phones and tablets used this form of flash for main storage up to 2016, in 2016 UFS started to take control of the market. The latest version of the eMMC standard (JESD84-B51) by JEDEC is version 5.1 released February 2015, with speeds rivaling discrete SATA-based SSDs (400 MB/s).\n\nSeagate, Hitachi and others are in the process of releasing SFF hard disk drives with an interface called CE-ATA. This interface is electrically and physically compatible with MMC specification. However, the command structure has been expanded to allow the host controller to issue ATA commands to control the hard disk drive.\n\n\n\n"}
{"id": "9691901", "url": "https://en.wikipedia.org/wiki?curid=9691901", "title": "NxTier", "text": "NxTier\n\nNxTier, is an ASP company that provided software solutions to the logistics and supply chain industry. Using their VSC platform, the industry has visibility and control of their data virtually anywhere in the world at any time. The VSC platform is an ASP application deployed over the Internet.\nThe platform allows logistic and supply chain industry providers to schedule and dispatch transportation management system, accept and transmit EDI, provide Web tracking functionality, manage orders with the OMS , perform business analytics and consolidate and optimize orders and loads.\n\nThe system was originally created specifically for the third-party logistics (3PL) market. Later it was quickly adopted by the specialized delivery, home delivery and the less than truckload (LTL) markets. Today it has furthered it reach to full truckload carrier market, the service industry; dispatch and scheduling, retail build out, appliance and furniture as well as retail replenishment. As the internet became a more reliable platform NxTier positioned its VSC platform to compete in the EDI space to reduce or eliminate value added network (VAN) based fees.\nNxtier VSC (Visibility, Synchronization, Control) platform provides multiple components of an ERP system. It also provides interfaces to other platforms and systems.\n\nThe platform is Internet-based, ERP and can be accessed with any internet connection.\n\nFounded in 1994, in Worcester, Massachusetts, to help the transportation industry increase visibility into the supply chain by giving them greater visibility and Control over their IT functions and data.\n\n"}
{"id": "21775926", "url": "https://en.wikipedia.org/wiki?curid=21775926", "title": "Open Science Grid Consortium", "text": "Open Science Grid Consortium\n\nThe Open Science Grid Consortium is an organization that administers a worldwide grid of technological resources called the Open Science Grid, which facilitates distributed computing for scientific research. Founded in 2004, the consortium is composed of service and resource providers, researchers from universities and national laboratories, as well as computing centers across the United States. Members independently own and manage the resources which make up the distributed facility, and consortium agreements provide the framework for technological and organizational integration.\n\nThe OSG is used by scientists and researchers for data analysis tasks which are too computationally intensive for a single data center or supercomputer. While most of the grid's resources are used for particle physics, research teams from disciplines like biology, chemistry, astronomy, and geographic information systems are currently using the grid to analyze data. Research using the grid's resources has been published in the Journal of Physical Chemistry.\n\nThe Open Science Grid was created in order to facilitate data analysis from the Large Hadron Collider, and about 70% of its 300,000 computing-hours per day are dedicated to the analysis of data from particle colliders. Once data has been collected and distributed by the LHC Computing Grid, the Open Science Grid assists physicists from institutions around the world in analysis. The grid has been designed so that resources and data are shared automatically: \n\nIn total, the OSG comprises over 25,000 computers with over 43,000 processors, most of which are running a distribution of Linux.\n72 institutions, including 42 universities, are consortium members who contribute resources to the grid. There are 90 distinct computational and storage nodes in the grid, which are distributed across the United States and Brazil.\n\nThe grid is peered with other grids, including TeraGrid, LHC Computing Grid, the European Grid Infrastructure, and XSEDE, allowing data and resources from those grids to be shared.\n\nThe grid's architecture has been studied by many researchers in the fields of computer science and information systems. Research about the OSG has been published in Science and Lecture Notes in Computer Science.\n\nThe consortium is funded by the Department of Energy and National Science Foundation, and has received a $30 million joint grant.\n"}
{"id": "262906", "url": "https://en.wikipedia.org/wiki?curid=262906", "title": "Open format", "text": "Open format\n\nAn open format is a file format for storing digital data, defined by a published specification usually maintained by a standards organization, and which can be used and implemented by anyone. For example, an open format can be implemented by both proprietary and free and open-source software, using the typical software licenses used by each. In contrast to open formats, closed formats are considered trade secrets. Open formats are also called free file formats if they are not encumbered by any copyrights, patents, trademarks or other restrictions (for example, if they are in the public domain) so that anyone may use them at no monetary cost for any desired purpose.\n\nSun Microsystems defines the criteria for open formats as follows:\n\nWithin the framework of Open Government Initiative, the federal government of the United States adopted the Open Government Directive, according to which: \"An open format is one that is platform independent, machine readable, and made available to the public without restrictions that would impede the re-use of that information\".\n\nThe State of Minnesota defines the criteria for open, XML-based file formats as follows:\n\n\nThe Commonwealth of Massachusetts \"defines open formats as specifications for data file formats that are based on an underlying open standard, developed by an open community, affirmed and maintained by a standards body and are fully documented and publicly available.\"\n\nThe Enterprise Technical Reference Model (ETRM) classifies four formats as \"Open Formats\":\n\nAccording to The Linux Information Project, the term \"open format\" should refer to \"any format that is published for anyone to read and study but which may or may not be encumbered by patents, copyrights or other restrictions on use\" – as opposed to a \"free format\" which is \"not\" encumbered by any copyrights, patents, trademarks or other restrictions.\n\nOpen formats include:\n\n\n\n"}
{"id": "20793415", "url": "https://en.wikipedia.org/wiki?curid=20793415", "title": "Oslo Manual", "text": "Oslo Manual\n\nThe Organisation for Economic Co-operation and Development's document \"The Measurement of Scientific and Technological Activities, Proposed Guidelines for Collecting and Interpreting Technological Innovation Data\", also known as the Oslo Manual, contains guidelines for collecting and using data on industrial innovation.\n\n"}
{"id": "33113928", "url": "https://en.wikipedia.org/wiki?curid=33113928", "title": "Precision mechanics", "text": "Precision mechanics\n\nPrecision mechanics (also \"fine mechanics\") is an engineering discipline that deals with the design and construction of smaller precision machines, often including measuring and control mechanisms of different kinds.\n\nThe study may be further defined as the practices of rigid body kinematics to the positioning and holding of objects on the micrometre scale and smaller.\n"}
{"id": "57893298", "url": "https://en.wikipedia.org/wiki?curid=57893298", "title": "Remote Activation Munition System", "text": "Remote Activation Munition System\n\nThe Remote Activation Munition System (RAMS) is a radio frequency controlled system that is used to remotely detonate demolition charges or remotely operate electronic equipment such as beacons, laser markers, and radios. It was developed by a team of researchers led by James Chopak at the Army Research Laboratory over the span of four years starting in 1996 and ending in 2000. The system consists of a transmitter and two different types of receivers, one to initiate blasting caps and one to initiate C4 directly.\n\nThe RAMS was designed to serve as a more portable and convenient alternative to conventional remote activation systems like the model XM-122, which was considered too big, heavy, and fragile for efficient use. In addition, the XM-122 was limited in its range (about 1km) and relied on very large high capacity batteries. In contrast, the RAMS weighed only a couple pounds and its microprocessor-based transmitter was powered by at most seven standard 9-volt batteries. The device was capable of reaching a range up to 2 kilometers, and the combination of the crystal filter in the receivers and the FM detector circuit made it possible to maintain high signal sensitivity at a low power consumption rate. In addition, the RAMS was operational in harsh environments with temperatures as low as -25 degrees F and as high as 140 degrees F. It was also capable of functioning when submerged in saltwater, up to depths of 66 feet. However, testing performed by the Army Research Laboratory have found that due to the low power levels of the RAMS receiver’s electrical signals output, the system has demonstrated a noticeable level of unreliability in performance past a certain distance.\n\nMore modern versions of the RAMS can weigh as little as three powers and can reach a range of more than 5 kilometers, allowing operators to stand farther away from the blast at a safer distance.\n"}
{"id": "44914640", "url": "https://en.wikipedia.org/wiki?curid=44914640", "title": "Ricoh GR digital cameras", "text": "Ricoh GR digital cameras\n\nThe Ricoh GR is a series of point-and-shoot, or compact, digital cameras made by Ricoh. The GR name was previously used for Ricoh's GR series of film cameras.\n\n\n"}
{"id": "51213003", "url": "https://en.wikipedia.org/wiki?curid=51213003", "title": "Rudyerd Boulton", "text": "Rudyerd Boulton\n\nWolfrid Rudyerd Boulton (April 5, 1901 – January 24, 1983) was an American ornithologist who worked extensively in Africa. Boulton held positions at the American Museum of Natural History and the Carnegie Museum of Natural History and traveled widely on expeditions to Africa. With his first wife, ethnomusicologist Laura Boulton, he made the first recordings of African tropical bird calls. Boulton was recruited into the Office of Strategic Services (OSS) during the Second World War because of his knowledge of Africa and his experience in foreign travel. He was responsible for monitoring the supply of uranium ore from the Belgian Congo for the Manhattan Project. Boulton transferred to the Central Intelligence Agency (CIA) in 1947 and resigned in 1958.\n\nBoulton moved to Southern Rhodesia in 1959 with his third wife and established the Atlantica Foundation, a charitable organization to encourage the study of African birds. The foundation received funding from the CIA and Boulton was later interviewed by Rhodesia's Central Intelligence Organisation over his links to the organization. Atlantica lost much of its funding following Rhodesia's Unilateral Declaration of Independence in 1965 and Boulton closed the foundation in 1978. He died in Zimbabwe (Rhodesia).\n\nW. Rudyerd Boulton was born in Beaver, Pennsylvania, on April 5, 1901. His first name is given variously as Wolfrid and Wilfred but was generally not used, being known as \"W. Rudyerd Boulton\", \"Rudyerd Boulton\" or \"Rud\". He joined the American Ornithologists' Union at around the age of 15 – he would remain a member for the whole of his life. He attended Amherst College and then the University of Pittsburgh, from which he graduated with Bachelor of Science degree in 1924. Immediately after graduation Boulton was made research assistant at the American Museum of Natural History's ornithology department. He was a participant of Arthur Stannard Vernay's 1925 expedition to Angola and in the same year married the ethnomusicologist Laura Boulton (\"née\" Crayton).\n\nBoulton transferred to the Carnegie Museum of Natural History in 1926 as assistant curator of birds and in 1929 joined the Sarah Lavanburg Straus expedition to Nyasaland (Malawi), Uganda and Kenya. In the course of this, with his wife, he made the first ever recordings of the calls of African tropical birds. Boulton was also a member of the 1929–30 Carnegie Museum African expedition. In 1931 Boulton was appointed assistant curator at the Field Museum of Natural History and, the same year, accompanied Ralph Pulitzer on his expedition to Angola. During the expedition he discovered a previously unknown species of warbler that he named in honour of his wife. In 1933 he published a children's book entitled \"Travelling with the Birds\". Boulton was a member of the 1934 Field Museum-Straus expedition that traveled 12,900 km from Dakar to Cameroon via Nigeria and later took part in expeditions to Panama, the West Indies and the Galapagos Islands. Boulton was promoted to curator of birds in 1937, a position he retained until 1946. He separated from his wife in 1938 and remarried four years later to Inez Cunningham Stark, an heiress, poet and patron of the arts.\n\nBoulton was recruited into the Office of Strategic Services, which was then the US intelligence agency, in 1942 due to his knowledge of Africa and overseas experience. He was appointed divisional deputy for Africa in the Secret Intelligence Branch. Boulton was responsible, from 1943, for co-coordinating a joint program with the X-2 Counter Espionage Branch to monitor the supply of uranium ore, vital to the success of the Manhattan Project, which was primarily obtained from the Belgian Congo. Despite his experience, Boulton was based primarily in Washington, DC, only once leaving the country – to visit North Africa in 1944. During the war Adolph W. Schmidt, later to be the American ambassador to Canada, served as one of his intelligence officers.\n\nBy February 1946 Boulton had become head of the Secret Intelligence Branch's Africa division and had to resign from his position at the Carnegie so that he could focus on his work for the government, although he remained a research associate of the museum. He transferred to the Central Intelligence Agency when it was established as the successor to the OSS in 1947. An ornithological expedition Boulton undertook in North Africa in 1952 may have been funded by the CIA as cover for an operation, or if not, by his wife. A further expedition to Southern Rhodesia and Angola in 1957 was funded by the Carnegie Museum. Boulton resigned from the CIA in April 1958, the same year his wife died.\n\nBoulton married for the third and final time in April 1959 to Louise Rehm, the widow of a former OSS colleague. Within months the couple had moved to Southern Rhodesia where they founded the Atlantica Foundation, a charitable organization to encourage ornithologists, particularly students, to study the birds of Africa. Boulton had a farm near Lake McIlwaine which housed an extensive art collection that was exhibited at the Rhodes National Gallery in December 1960. He sold some of his art, including works by Pablo Picasso, Marc Chagall, Wassily Kandinsky and Paul Klee to fund the work of Atlantica. Boulton and his wife made frequent donations to the organization, contributing fully one third of their wealth to it, and it also received financial support from the CIA. The foundation funded scholarships at Nyatsime College and donated books to more than 100 educational establishments.\n\nIn addition to his work at the foundation Boulton was managing editor of \"Rhodesia Science News\" and president of the Rhodesia Scientific Association. He also assisted with early conservation projects in the region to the south-west of Lusaka, which was established as Lochinvar National Park in 1972, and experimented with farming termites as a nutritional foodstuff. Boulton carried out expeditions with a mobile laboratory to the Kalahari Desert in Tanzania. The laboratory included advanced listening equipment and map-making tools, which he may well have used to carry out surveys for the CIA. Rhodesia's Central Intelligence Organisation was suspicious of Boulton's CIA connections and advanced equipment and he was interviewed by them.\n\nBoulton lost his international funding following Rhodesia's Unilateral Declaration of Independence in 1965 and the introduction of US sanctions in 1967. As a result, Atlantica was forced to heavily reduce its programs. It may well be that the Boultons would have left the increasingly isolated Rhodesia if Louise was able to, but she was struck by blindness and senility and required 24-hour care. Louise died in 1974 and two years later Boulton returned to the United States to manage the process of closing Atlantica. He transferred the remaining funds to the Conservation Trust of Rhodesia in 1978. The Atlantica properties were used to educate students from within the country and from Mozambique by the new black-majority government of Zimbabwe (which Rhodesia had been renamed).\n\nBoulton suffered two strokes in later life and was a wheelchair user. He died on January 24, 1983, in Zimbabwe.\n\nA species of African gecko, \"Rhoptropus boultoni\", is named in honour of Rudyerd Boulton.\n"}
{"id": "54422114", "url": "https://en.wikipedia.org/wiki?curid=54422114", "title": "Soft washing", "text": "Soft washing\n\nSoft washing is a cleaning method using low pressure and specialized solutions (typically bleach, water, and sometimes a surfactant) to safely remove mildew, bacteria, algae and other organic stains from roofs and other building exteriors. It is so named to differentiate the method from power washing. The Asphalt Roofing Manufacturers Association recommends low-pressure bleach or detergent assisted washing as the preferred method for cleaning asphalt roofs in order to prevent damage to the shingles. \n\nSoft washing is using an electric agricultural sprayer to apply a water-based, biodegradable disinfecting cleaning solution to kill the mold, mildew, bacteria, algae, fungus, moss and more on exterior home and building surfaces.\n\nSoft washing Equipment is distinctly different than power and pressure washing equipment. The electric diaphragm pump applies the cleaning solution at 40-80 PSI. The equipment used for soft washing may also have telescoping handles so that the cleaning solution can easily reach roof eaves, upper story windows, and other such areas, without this added pressure. \n"}
{"id": "58740", "url": "https://en.wikipedia.org/wiki?curid=58740", "title": "Timeline of low-temperature technology", "text": "Timeline of low-temperature technology\n\nThe following is a timeline of low-temperature technology and cryogenic technology (refrigeration down to –273.15 °C, –459.67 °F or 0 K).\n\n\n\n\n\n\n"}
{"id": "1657639", "url": "https://en.wikipedia.org/wiki?curid=1657639", "title": "Timeout (computing)", "text": "Timeout (computing)\n\nIn telecommunications and related engineering (including computer networking and programming), the term timeout or time-out has several meanings, including:\n\n\nTimeouts allow for more efficient usage of limited resources without requiring additional interaction from the agent interested in the goods that cause the consumption of these resources. The basic idea is that in situations where a system must wait for something to happen, rather than waiting indefinitely, the waiting will be aborted after the timeout period has elapsed. This is based on the assumption that further waiting is useless, and some other action is necessary.\n\nOther examples are\n\n\n"}
{"id": "3399064", "url": "https://en.wikipedia.org/wiki?curid=3399064", "title": "Total air temperature", "text": "Total air temperature\n\nIn aviation, stagnation temperature is known as total air temperature and is measured by a temperature probe mounted on the surface of the aircraft. The probe is designed to bring the air to rest relative to the aircraft. As the air is brought to rest, kinetic energy is converted to internal energy. The air is compressed and experiences an adiabatic increase in temperature. Therefore, total air temperature is higher than the static (or ambient) air temperature.\n\nTotal air temperature is an essential input to an air data computer in order to enable computation of static air temperature and hence true airspeed.\n\nThe relationship between static and total air temperatures is given by:\n\nwhere:\n\nIn practice, the total air temperature probe will not perfectly recover the energy of the airflow, and the temperature rise may not be entirely due to adiabatic process. In this case, an empirical recovery factor (less than 1) may be introduced to compensate:\n\n(1) :formula_6\n\nWhere:\n\nTypical recovery factors\n\nPlatinum wire ratiometer thermometer (\"flush bulb type\"): \"e\" ≈ 0.75 - 0.9\n\nDouble platinum tube ratiometer thermometer (\"TAT probe\"): \"e\" ≈ 1\n\nOther notations\n\nTotal air temperature (TAT) is also called: indicated air temperature (IAT) or ram air temperature (RAT)\nStatic air temperature (SAT) is also called: outside air temperature (OAT) or true air temperature\n\nThe difference between TAT and SAT is called ram rise (RR) and is caused by compressibility and friction of the air at high velocities.\n\n(2) :formula_7\n\nIn practice the ram rise is negligible for aircraft flying at (true) airspeeds under Mach 0.2\n\nFor airspeeds (TAS) over Mach 0.2, as airspeed increases the temperature exceeds that of still air. This is caused by a combination of kinetic (friction) heating and adiabatic compression\n\n\nThe total of kinetic heating and adiabatic temperature change (caused by adiabatic compression) is the Total Ram Rise.\n\nCombining equations (1) & (2), we get:\n\nIf we use the Mach number equation for dry air:\n\nwhere formula_11\n\nwe get\n\n(3) :formula_12\n\nWhich can be simplified to:\n\nby using formula_14\nand\n\nBy solving (3) for the above values with TAS in knots, a simple accurate formula for ram rise is then:\n\n"}
{"id": "482137", "url": "https://en.wikipedia.org/wiki?curid=482137", "title": "Tuned radio frequency receiver", "text": "Tuned radio frequency receiver\n\nA tuned radio frequency receiver (or TRF receiver) is a type of radio receiver that is composed of one or more tuned radio frequency (RF) amplifier stages followed by a detector (demodulator) circuit to extract the audio signal and usually an audio frequency amplifier. This type of receiver was popular in the 1920s. Early examples could be tedious to operate because when tuning in a station each stage had to be individually adjusted to the station's frequency, but later models had ganged tuning, the tuning mechanisms of all stages being linked together, and operated by just one control knob. By the mid 1930s, it was replaced by the superheterodyne receiver patented by Edwin Armstrong.\n\nThe TRF receiver was patented in 1916 by Ernst Alexanderson. His concept was that each stage would amplify the desired signal while reducing the interfering ones. Multiple stages of RF amplification would make the radio more sensitive to weak stations, and the multiple tuned circuits would give it a narrower bandwidth and more selectivity than the single stage receivers common at that time. All tuned stages of the radio must track and tune to the desired reception frequency. This is in contrast to the modern superheterodyne receiver that must only tune the receiver's RF front end and local oscillator to the desired frequencies; all the following stages work at a fixed frequency and do not depend on the desired reception frequency.\n\nAntique TRF receivers can often be identified by their cabinets. They typically have a long, low appearance, with a flip-up lid for access to the vacuum tubes and tuned circuits. On their front panels there are typically two or three large dials, each controlling the tuning for one stage. Inside, along with several vacuum tubes, there will be a series of large coils. These will usually be with their axes at right angles to each other to reduce magnetic coupling between them.\n\nA problem with the TRF receiver built with triode vacuum tubes is the triode's interelectrode capacitance. The interelectrode capacitance allows energy in the output circuit to feedback into the input. That feedback can cause instability and oscillation that frustrate reception and produce squealing or howling noises in the speaker. In 1922, Louis Alan Hazeltine invented the technique of \"neutralization\" that uses additional circuitry to partially cancel the effect of the interelectrode capacitance. Neutralization was used in the popular Neutrodyne series of TRF receivers. Under certain conditions, \"the neutralization is substantially independent of frequency over a wide frequency band.\" \"Perfect neutralization cannot be maintained in practice over a wide band of frequencies because leakage inductances and stray capacities\" are not completely canceled. The later development of the tetrode and pentode vacuum tubes minimized the effect of interelectrode capacitances and could make neutralization unnecessary; the additional electrodes in those tubes shield the plate and grid and minimize feedback.\n\nThe classic TRF receivers of the 1920s and 30s usually consisted of three sections: \n\nEach tuned RF stage consists of an amplifying device, a triode (or in later sets a tetrode) vacuum tube, and a tuned circuit which performs the filtering function. The tuned circuit consisted of an air-core RF coupling transformer which also served to couple the signal from the plate circuit of one tube to the input grid circuit of the next tube. One of the windings of the transformer had a variable capacitor connected across it to make a tuned circuit. A variable capacitor (or sometimes a variable coupling coil called a \"variometer\") was used, with a knob on the front panel to tune the receiver. The RF stages usually had identical circuits to simplify design.\n\nEach RF stage had to be tuned to the same frequency, so the capacitors had to be tuned in tandem when bringing in a new station. In some later sets the capacitors were \"ganged\", mounted on the same shaft or otherwise linked mechanically so that the radio could be tuned with a single knob, but in most sets the resonant frequencies of the tuned circuits could not be made to \"track\" well enough to allow this, and each stage had its own tuning knob.\n\nThe detector was usually a grid-leak detector. Some sets used a crystal detector (semiconductor diode) instead. Occasionally, a regenerative detector was used, to increase selectivity.\n\nSome TRF sets that were listened to with earphones didn't need an audio amplifier, but most sets had one to three transformer-coupled or RC-coupled audio amplifier stages to provide enough power to drive a loudspeaker.\n\nThe schematic diagram shows a typical TRF receiver. This particular example uses six triodes. It has two radio frequency amplifier stages, one grid-leak detector/amplifier and three class ‘A’ audio amplifier stages. There are 3 tuned circuits \"T1-C1, T2-C2, and T3-C3\". The second and third tuning capacitors, \"C2\" and \"C3\", are ganged together \"(indicated by line linking them)\" and controlled by a single knob, to simplify tuning. Generally, two or three RF amplifiers were required to filter and amplify the received signal enough for good reception.\n\nTerman characterizes the TRF's disadvantages as \"poor selectivity and low sensitivity in proportion to the number of tubes employed. They are accordingly practically obsolete.\" Selectivity requires narrow bandwidth, but the bandwidth of a filter with a given Q factor increases with frequency. So to achieve a narrow bandwidth at a high radio frequency required high-Q filters or many filter sections. Achieving constant sensitivity and bandwidth across an entire broadcast band was rarely achieved. In contrast, a superheterodyne receiver translates the incoming high radio frequency to a lower intermediate frequency which does not change. The problem of achieving constant sensitivity and bandwidth over a range of frequencies arises only in one circuit (the first stage) and is therefore considerably simplified.\n\nThe major problem with the TRF receiver, particularly as a consumer product, was its complicated tuning. All the tuned circuits need to track to keep the narrow bandwidth tuning. Keeping multiple tuned circuits aligned while tuning over a wide frequency range is difficult. In the early TRF sets the operator had to perform that task, as described above. A superheterodyne receiver only needs to track the RF and LO stages; the onerous selectivity requirements are confined to the IF amplifier which is fixed-tuned.\n\nDuring the 1920s, an advantage of the TRF receiver over the regenerative receiver was that, when properly adjusted, it did not radiate interference. The popular regenerative receiver, in particular, used a tube with positive feedback operated very close to its oscillation point, so it often acted as a transmitter, emitting a signal at a frequency near the frequency of the station it was tuned to. This produced audible heterodynes, shrieks and howls, in other nearby receivers tuned to the same frequency, bringing criticism from neighbors. In an urban setting, when several regenerative sets in the same block or apartment house were tuned to a popular station, it could be virtually impossible to hear. Britain, and eventually the US, passed regulations that prohibited receivers from radiating spurious signals, which favored the TRF.\n\nAlthough the TRF design has been largely superseded by the superheterodyne receiver, with the advent of semiconductor electronics in the 1960s the design was \"resurrected\" and used in some simple integrated radio receivers for hobbyist radio projects, kits, and low-end consumer products. One example is the ZN414 TRF radio integrated circuit from Ferranti in 1972 shown below\n\n"}
{"id": "272050", "url": "https://en.wikipedia.org/wiki?curid=272050", "title": "Unintentional radiator", "text": "Unintentional radiator\n\nAn unintentional radiator or incidental radiator is any device which creates radio frequency energy within itself, which is then unintentionally radiated from the device. This can interfere with other electronic devices.\n\nA computer is a typical example, where spurious emissions may not be contained within the case.\n\nA radio receiver will often use an intermediate frequency which is detectable outside the radio—the concept behind at least one audience measurement concept for roadside detection of radio stations which passing motorists are listening to.\n\nOther examples include the motor, transformer, dimmer, and corona from electrical powerlines. Unintentional radiation from these devices can create interference on AM radio, and on the video of television stations.\n\nIn North America, active devices that are characterized as \"unintentional radiators\" are governed by Part 15 of the FCC regulations and its CRTC equivalents in Canada. Globally, most domestic regulation of unintentional radiators are based on ITU recommendations.\n\nGenerally, this means the device leaks a signal at some level. Microprocessor-controlled appliances, anything with a clock signal, and switching voltage regulators all make some kind of noise, at the repetition frequency and at harmonics. In most countries, government agencies regulate how much leakage is tolerated. This prevents leakage from cable television systems, for example, from interfering with radio communications between aircraft and control towers.\n\nBecause it costs money to filter out noise, there is always a balance struck between regulatory compliance and perfect filtering in these devices. Microwave ovens or devices with microprocessors may leak within allowable limits but may generate an undesired signal that interferes with a licensed communications device. It also generally means that users who intentionally radiate signals (TV stations and cell phone companies) can order the device turned off if it interferes with their licensed operations.\nThere is an entire industry based on regulatory compliance: manufacturers shipping a product to a foreign country must comply with each country's limitations on leakage of interfering signals. For example, in Germany the TÜV issues regulatory rules for unintentional radiators. The big cylindrical bumps on the cable to monitors and laptop chargers are ferrite cores which reduce undesired signals.\n\n"}
{"id": "26217800", "url": "https://en.wikipedia.org/wiki?curid=26217800", "title": "University18", "text": "University18\n\nU18 is a large scale e-education venture based out of India. University18 works with Indian Universities in a public–private partnership, developing and delivering Accredited Degree and Diploma Programs to the Indian Learner around the Nation. U18 has a global footprint, with students spread across the world, and exam centers in Africa, Singapore, Malaysia, Dubai and the United Kingdom.\n\nUniversity18's Programs are all structured around a web based e-education platform, one that has both asynchronous and synchronous components - the Learning Management System, and a Virtual Classroom System. Regular online sessions, with Professors from various Institutions around the Nation, act to bridge the gap between full time classroom learning and Distance Learning.\n\nThe fact that U18 uses professors drawn from different Institutions give their programs some flexibility and flavor. The U18 premise is that classroom interaction, as well as the mentoring a classroom lecture (online) can provide, aid both the effort an adult learner makes and the challenges he/she faces by re-entering the formal education system after a break.\n\nThe Re.Vica Project of the European Commission, describes U18 thus -\n“University18, a nonprofit private-sector initiative (currently in partnership with the Karnataka State Open University), represents a significant development in the region which may well have great impact in the near future“.\n\nU18 is a nominee for the eIndia 2010 Awards, to be given out in August, at the eIndia 2010 convention at Hyderabad.\n\nUniversity18 currently has tie ups with Assam Don Bosco University and the University of Mysore\n\n\n\n"}
{"id": "841199", "url": "https://en.wikipedia.org/wiki?curid=841199", "title": "Vectorscope", "text": "Vectorscope\n\nA vectorscope is a special type of oscilloscope used in both audio and video applications. Whereas an oscilloscope or waveform monitor normally displays a plot of signal vs. time, a vectorscope displays an X-Y plot of two signals, which can reveal details about the relationship between these two signals. Vectorscopes are highly similar in operation to oscilloscopes operated in X-Y mode; however those used in video applications have specialized graticules, and accept standard television or video signals as input (demodulating and demultiplexing the two components to be analyzed internally).\n\nIn video applications, a vectorscope supplements a waveform monitor for the purpose of measuring and testing television signals, regardless of format (NTSC, PAL, SECAM or any number of digital television standards). While a waveform monitor allows a broadcast technician to measure the overall characteristics of a video signal, a vectorscope is used to visualize chrominance, which is encoded into the video signal as a subcarrier of specific frequency. The vectorscope locks exclusively to the chrominance subcarrier in the video signal (at 3.58 MHz for NTSC, or at 4.43 MHz for PAL) to drive its display. In digital applications, a vectorscope instead plots the Cb and Cr channels against each other (these are the two channels in digital formats which contain chroma information).\n\nA vectorscope uses an overlaid circular reference display, or \"graticule\", for visualizing chrominance signals, which is the best method of referring to the QAM scheme used to encode color into a video signal. The actual visual pattern that the incoming chrominance signal draws on the vectorscope is called the \"trace\". Chrominance is measured using two methods—color saturation, encoded as the amplitude, or gain, of the subcarrier signal, and hue, encoded as the subcarrier's phase. The vectorscope's graticule roughly represents saturation as distance from the center of the circle, and hue as the angle, in standard position, around it. The graticule is also embellished with several elements corresponding to the various components of the standard color bars video test signal, including boxes around the circles for the colors in the main bars, and perpendicular lines corresponding to the U and V components of the chrominance signal (and additionally on an NTSC vectorscope, the I and Q components). NTSC vectorscopes have one set of boxes for the color bars, while their PAL counterparts have two sets of boxes, because the R-Y chrominance component in PAL reverses in phase on alternating lines. Another element in the graticule is a fine grid at the nine-o'clock, or -U position, used for measuring differential gain and phase.\n\nOften two sets of bar targets are provided: one for color bars at 75% amplitude and one for color bars at 100% amplitude. The 100% bars represent the maximum amplitude (of the composite signal) that composite encoding allows for. 100% bars are not suitable for broadcast and are not broadcast-safe. 75% bars have reduced amplitude and are broadcast-safe.\n\nSome vectorscope models have only one set of bar targets. The vectorscope can be set up for 75% or 100% bars by adjusting the gain so that the color burst vector extends to the \"75%\" or \"100%\" marking on the graticule.\n\nThe reference signal used for the vectorscope's display is the color burst that is transmitted before each line of video, which for NTSC is defined to have a phase of 180°, corresponding to the nine-o'clock position on the graticule. The actual color burst signal shows up on the vectorscope as a straight line pointing to the left from the center of the graticule. In the case of PAL, the color burst phase alternates between 135° and 225°, resulting in two vectors pointing in the half-past-ten and half-past-seven positions on the graticule, respectively. In digital (and component analog) vectorscopes, colorburst doesn't exist; hence the phase relationship between the colorburst signal and the chroma subcarrier is simply not an issue. A vectorscope for SECAM uses a demodulator similar to the one found in a SECAM receiver to retrieve the U and V colour signals since they are transmitted one at a time (Thomson 8300 Vecamscope).\n\nOn older vectorscopes that use cathode ray tubes (CRTs), the graticule was often a silk-screened overlay superimposed over the front surface of the screen. One notable exception was the Tektronix WFM601 series of instruments, which are combined waveform monitors and vectorscopes used to measure CCIR 601 television signals. The waveform-mode graticule of these instruments is implemented with a silkscreen, whereas the vectorscope graticule (consisting only of bar targets, as this family did not support composite video) was drawn on the CRT by the electron beam. Modern instruments have graticules drawn using computer graphics, and both graticule and trace are rendered on an external VGA monitor or an internal VGA-compatible LCD display.\n\nMost modern waveform monitors include vectorscope functionality built in; and many allow the two modes to be displayed side-by-side. The combined device is typically referred to as a waveform monitor, and standalone vectorscopes are rapidly becoming obsolete.\n\nIn audio applications, a vectorscope is used to measure the difference between channels of stereo audio signals. One stereo channel drives the horizontal deflection of the display, and the other drives the vertical deflection. A monoaural signal, consisting of identical left and right signals, results in a straight line with a gradient of +1. Any stereo separation is visible as a deviation from this line, creating a Lissajous figure. If a straight line appears with a gradient of −1, this indicates that the left and right channels are 180° out of phase.\n\n"}
{"id": "28008974", "url": "https://en.wikipedia.org/wiki?curid=28008974", "title": "Vertical roller mill", "text": "Vertical roller mill\n\nVertical roller mill is a type of grinder used to grind materials into extremely fine powder for use in mineral dressing processes, paints, pyrotechnics, cements and ceramics. It is an energy efficient alternative for a ball mill.\n\nVertical roller mill has many different forms, but it works basically the same. All of these forms of machine come with a roller (or the equivalent of roller grinding parts), and roller along the track of the disc at the level of circular movement imposed by external grinding roller in the vertical pressure on the disc on the material being the joint action of compression and shear, and to crush.\nGebr. Pfeiffer,Loesche, Polysius, Atox, F L Smidth, OK mill,Zhongya，HRM， ICIL, Poittemill, Ecutec are the type of mills which are used worldwide.\nFamous Manufacturer, Hefei Zhongya Building Material Equipment Co., Ltd.\n\nMaterial grinding process motor through reducer rotating drive disc, the material falls from the mill under the central entrance and exit, under the action of centrifugal force to the disc edge by the roller to move and the crushing, grinding out lap after the material was speed up the flow to and vertical mill with one of the separator, after the meal by the separator back to the mill, the re-grinding; powder while grinding out with air, dust collection equipment in the system to collect down, that is, products. Established through the mill in the pneumatic conveying of materials, a larger air flow rate, which can use waste heat of gas, at the same time dry grinding operations.\n\nUsed in cement grinding production grinding parts of its various forms, there is a cylinder, cone type, ball type, etc., and roller surface is also flat, curved, convex round noodles. Applied to the grinding roller to roller grinding along the track bed in close contact with the material strength of a spring pressure, hydraulic, etc.\n\nIt has two pairs of grinding rollers, each pair of roller composed of two narrow rollers, mounted on the same axis and can rotate at different speeds. There are two circular disc slot, and roller to the tire shape, work pressure in the tank. Disc roller and the relative sliding velocity between small and roller can swing, even if the roller sleeve and the disc after the liner wear can guarantee a good abrasive, will not affect the grinding effect. Roller symmetrical structure, one side can be upside down after use and wear.\nMaterial grinding process: hydraulic pneumatic roller device through the pressure applied to the material on the crushing of materials, the materials have been moved to crush the edge of disc, from the disc around the nozzle from the exhaust air to improve these materials to Separator.Meal to separate returns after a concentrated mill, fine powder was collected in the precipitator unloading. In the gas flow is small, the meal can not be increased to enhance air flow, they would fall through the nozzle discharged outside the mill, bucket elevator to be transported to the feeder of vertical roller mill, and was re-grinding mill feed people. This cycle features of the way there:\n\n\n"}
{"id": "20214363", "url": "https://en.wikipedia.org/wiki?curid=20214363", "title": "Western Institute of Nanoelectronics", "text": "Western Institute of Nanoelectronics\n\nThe Western Institute of Nanoelectronics is a research institute founded in 2006 and headquartered at the UCLA Henry Samueli School of Engineering and Applied Science in Los Angeles, California, USA. The WIN Center networks multiple universities with Industry and government based sponsors (members of the Semiconductor Industry Association consortium NRI) and the National Institute of Standards and Technology (NIST) in pursuit of replacing Complementary Metal-Oxide Semiconductor Field-Effect Transistors (CMOS FET). WIN's research is focused on spintronics extending from materials, devices, and device-device interactions, metrology and circuits/architectures. Sponsors include:\n\nWIN is one of four research centers within the Nanoelectronics Research Initiative. Dr. Kang L. Wang serves as Director of the Center. Current WIN university participants include four University of California campuses (Los Angeles, Berkeley, Santa Barbara and Irvine) and Stanford University, Denver University, Portland State University, and University of Iowa. \n\nNRI's goal is to develop a radical, yet practical, new device that continues scaling of semiconductors beyond the predominantly silicon content found in chips that power today's computers and electronics. The aim is to demonstrate feasibility of such devices in simple circuits during the next 5–10 years. NRI is a research initiative of the Nanoelectronics Research Corporation (NERC). And NERC in turn is a subsidiary of the Semiconductor Research Corporation.\n\n"}
{"id": "4629159", "url": "https://en.wikipedia.org/wiki?curid=4629159", "title": "White sale", "text": "White sale\n\nA white sale is a marketing strategy in which a store steeply discounts its merchandise to increase sales during a short period of time.\n\nIn 1878, John Wanamaker of Philadelphia department store fame decreed January to be the time for a \"White sale.\" Bed linens, which were available in white only, were sold at a discount. This was done to increase sales for these items at a time of the year when sales were normally slow.\n\nToday, white sales usually revolve around household items. However, they no longer only involve items that are white in color, and they are not restricted to take place in the month of January. \"White sales\" should not be confused with sales on \"white goods,\" which is to say durable goods such as refrigerators, freezers, stoves, washing machines, and similar large appliances.\n"}
