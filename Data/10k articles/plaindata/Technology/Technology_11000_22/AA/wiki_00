{"id": "23241698", "url": "https://en.wikipedia.org/wiki?curid=23241698", "title": "AGRIS", "text": "AGRIS\n\nAGRIS (International System for Agricultural Science and Technology) is a global public domain database with more than 8 million structured bibliographical records on agricultural science and technology. It became operational in 1975 and The database is maintained by CIARD, and its content is provided by more than 150 participating institutions from 65 countries. The AGRIS Search system, allows scientists, researchers and students to perform sophisticated searches using keywords from the AGROVOC thesaurus, specific journal titles or names of countries, institutions, and authors.\n\nAs information management flourished in the 1970s, the AGRIS metadata corpus was developed to allow its users to have free access to knowledge available in agricultural science and technology. AGRIS was developed to be an international cooperative system to serve both developed and developing countries.\n\nWith the advent of the Internet, along with the promises offered by open access publishing, there was growing awareness that the management of agricultural science and technology information, would have various facets: standards and methodologies for interoperability and facilitation of knowledge exchange; tools to enable information management specialists to process data; information and knowledge exchange across countries. Common interoperability criteria were thus adopted in its implementation, and the AGRIS AP metadata was accordingly created in order to allow exchange and retrieval of Agricultural information Resources.\n\nAGRIS covers the wide range of subjects related to agriculture science and technology, including forestry, animal husbandry, aquatic sciences and fisheries, human nutrition, and extension. Its content includes unique grey literature such as unpublished scientific and technical reports, theses, conference papers, government publications, and more. A growing number (around 20%) of bibliographical records have a corresponding full text document on the web which can easily be retrieved by Google.\n\nOn 5 December 2013 AGRIS 2.0 was released. AGRIS 2.0 is at the same time:\n\n\nAccess to the AGRIS Repository is provided through the AGRIS Search Engine. As such, it:\n\nAGRIS data was converted to RDF and the resulting linked dataset created some 200 million triples.\nAGRIS is also registered in the Data Hub at http://thedatahub.org/dataset/agris\n\nThe AGRIS partners contributing to the AGRIS Database use several formats for exchanging data, including simple DC, from OAI-PMH systems.\nThe AGRIS AP format is anyway adopted directly by:\n\nFalling under the umbrella of CIARD, a joint initiative co-led by the CGIAR, GFAR and FAO, the new AGRIS aims to promote the sharing and management of agricultural science and technology information through the use of common standards and methodologies. These will incorporate Web 2.0 features, in order to make the search experience as comprehensive, intuitive and far-reaching as possible for users of the new AGRIS.\n\nFurthermore, the new AGRIS will also leverage the data and infrastructure of one of CIARD's projects: the CIARD RING. An acronym standing for Routemap to Information Nodes and Gateways (RING), the CIARD RING project is led by GFAR and it aims to:\n\n\nA directory of ARD (Agricultural Research for Development) information services will allow the monitoring, describing and classifying of existing services, whilst benchmarking them against interoperability criteria, to ensure for maximum outreach and global availability.\n\n\n\n"}
{"id": "146084", "url": "https://en.wikipedia.org/wiki?curid=146084", "title": "ANFO", "text": "ANFO\n\nANFO (or AN/FO, for ammonium nitrate/fuel oil) is a widely used bulk industrial explosive. Its name is commonly pronounced as \"an-fo\".\n\nIt consists of 94% porous prilled ammonium nitrate (NHNO) (AN), which acts as the oxidizing agent and absorbent for the fuel, and 6% number 2 fuel oil (FO).\n\nANFO has found wide use in coal mining, quarrying, metal mining, and civil construction in applications where its low cost and ease of use may outweigh the benefits of other explosives, such as water resistance, oxygen balance, higher detonation velocity, or performance in small-diameter columns. ANFO is also widely used in avalanche hazard mitigation.\n\nIt accounts for an estimated 80% of the of explosives used annually in North America.\n\nThe press and other media have used the term ANFO loosely and imprecisely in describing improvised explosive devices (IEDs), in cases of fertilizer bombs (see Malicious use below).\n\nThe use of ANFO originated in the 1950s.\n\nThe chemistry of ANFO detonation is the reaction of ammonium nitrate with a long-chain alkane (CH) to form nitrogen, carbon dioxide, and water. In an ideal stoichiometrically balanced reaction, ANFO is composed of about 94.3% AN and 5.7% FO by weight. In practice, a slight excess of fuel oil is added, as underdosing results in reduced performance while overdosing merely results in more post-blast fumes. When detonation conditions are optimal, the aforementioned gases are the only products. In practical use, such conditions are impossible to attain, and blasts produce moderate amounts of toxic gases such as carbon monoxide and nitrogen oxides (NO).\n\nThe fuel component of ANFO is typically diesel, but kerosene, coal dust, racing fuel, or even molasses have been used instead. Finely powdered aluminium in the mixture will sensitise it to detonate more readily.\n\nANFO is classified as a blasting agent, meaning that it decomposes through detonation rather than deflagration at a velocity higher than the speed of sound in the material but cannot be detonated with a No. 8 blasting cap without a sensitizer. ANFO has a moderate velocity compared to other industrial explosives, measuring 3,200 m/s in diameter, unconfined, at ambient temperature.\n\nANFO is a tertiary explosive, meaning that it cannot be set off by the small quantity of primary explosive in a typical blasting cap. A larger quantity of secondary explosive, known as a primer or a booster, must be used. One or two sticks of dynamite were historically used; current practice is to use Tovex or cast boosters of pentolite (TNT/PETN or similar compositions).\n\nIn the mining industry, the term ANFO specifically describes a mixture of solid ammonium nitrate prills and diesel fuel. Other explosives based on the ANFO chemistry exist; the most commonly used are emulsions. They differ from ANFO in the physical form the reactants take. The most notable properties of emulsions are water resistance and higher bulk density.\n\nWhile the density of pure crystalline ammonium nitrate is 1700 kg/m, individual prills of explosive-grade AN measure approximately 1300 kg/m. Their lower density is due to the presence of a small spherical air pocket within each prill: this is the primary difference between AN sold for blasting and that sold for agricultural use. These voids are necessary to sensitize ANFO: they create so-called \"hot spots\". Finely powdered aluminium can be added to ANFO to increase both sensitivity and energy; however, this has fallen out of favor due to cost.\n\nANFO has a bulk density of about 840 kg/m. In surface mining applications, it is typically augured into boreholes by dedicated trucks that mix the AN and FO components immediately before the product is dispensed. In underground mining applications, ANFO is typically blow-loaded.\n\nAN is highly hygroscopic, readily absorbing water from air. In humid environments, absorbed water interferes with its explosive function. AN is fully water-soluble; as such, it cannot be loaded into boreholes that contain standing water. When used in wet mining conditions, considerable effort must be taken to remove standing water and install a liner in the borehole; it is generally more productive to instead use a water-resistant explosive such as emulsion.\n\nIn most jurisdictions, ammonium nitrate need not be classified as an explosive for transport purposes; it is merely an oxidizer. Mines typically prepare ANFO on-site using the same diesel fuel that powers their vehicles. While many fuels can theoretically be used, the low volatility and cost of diesel make it ideal.\n\nANFO under most conditions is blasting cap–insensitive, so it is classified as a blasting agent and not a high explosive.\n\nAmmonium nitrate is widely used as a fertilizer in the agricultural industry. It is also found in instant cold packs. In many countries, its purchase and use are restricted to buyers who have obtained the proper license.\n\nThe Discovery Channel show \"MythBusters\" commonly used ANFO (with the help of detonation professionals), especially in episode 26: \"Salsa Escape, Cement Removal\" and episode 125: \"Knock Your Socks Off\" as well as the series finale when they not only blew up a recreational vehicle, but recreated the most iconic explosion in the show's history when they used 5,001 pounds of ANFO to blow up a cement truck.\n\nIt was used by Jim Caviezel's character in the 2006 film \"Déjà Vu\" in a domestic terrorism attack.\n\nIn the book \"The Third Day, The Frost\" by John Marsden, ANFO is used to blow up Cobblers Bay.\n\nIn a sub-plot of the book \"Executive Orders\" by Tom Clancy, anarchists make a truck bomb filled with ANFO but are caught before reaching their target.\n\nIn the book \"Battle Royale\" by Koshun Takami, students who attempt to bomb a school use ANFO.\n\nIn the movie \"The Dark Knight\" by Christopher Nolan, The Joker uses the compound to hold two ferries hostage, and tries to make the people on each ferry detonate the other ferry.\n\nIn Stephen King's novel \"Desperation\", novelist Johnny Marinville prevents the rest of the group from going forward, and proceeds to blow up the Pit and the \"ini\" inside with the ANFO, sacrificing himself.\n\nIt is often mentioned and used by the protagonist Vic in \"NOS4A2\" by Joe Hill.\n\nUnmixed ammonium nitrate can decompose explosively and has been responsible for several industrial disasters, including the 1922 Oppau explosion in Germany, the 1947 Texas City disaster in Texas City, Texas, the 2004 Ryongchon disaster in North Korea, and the 2013 West Fertilizer Company explosion in West, Texas. Environmental hazards include eutrophication in confined waters and nitrate/gas oil contamination of ground or surface water.\n\nANFO was first used maliciously in 1970 when protests by students became violent at the University of Wisconsin–Madison, who learned how to make and use ANFO from a Wisconsin Conservation Department booklet entitled \"Pothole Blasting for Wildlife\", resulting in the Sterling Hall bombing.\n\nANFO used to be widely used by the FLNC (National Liberation Front of Corsica) in Corsica, along with f15 explosive, in order to fight against French colonists and symbols. Five containers of 500 kilos each were used to blow up the Tax Office building in Bastia on 28 February 1987.\n\nThe ANFO car bomb was adopted by the Provisional IRA in 1972 and, by 1973, the Troubles were consuming 47,000 lb of ammonium nitrate for the majority of bombs. The Ulster Volunteer Force (UVF) also made use of ANFO bombs, often mixing in gelignite as a booster. The IRA detonated an ANFO truck bomb on Bishopsgate in London in 1993, killing one and causing £350 million in damage. It has also seen use by groups such as the Revolutionary Armed Forces of Colombia and ETA. In 1992, Shining Path perpetrated the Tarata bombing in Lima, Peru, using two ANFO truck bombs.\n\nA more sophisticated variant of ANFO (ammonium nitrate with nitromethane as the fuel, called ANNM) was used in the 1995 Oklahoma City bombing.\n\nThe Shijiazhuang bombings (Chinese: 靳如超爆炸案 or 石家庄\"3·16\"特大爆炸案) rocked the city of Shijiazhuang, China, on 16 March 2001. A total of 108 people were killed, and 38 others injured when, within a short time, several ANFO bombs exploded near four apartment buildings.\n\nImprovised bombs made with agricultural-grade AN are less sensitive and less efficient than the explosive-grade variety. In November 2009, a ban on ammonium sulfate, ammonium nitrate, and calcium ammonium nitrate fertilizers was imposed in the former Malakand Division – comprising the Upper Dir, Lower Dir, Swat, Chitral and Malakand districts of the North West Frontier Province (NWFP) of Pakistan – by the NWFP government, following reports that those chemicals were used by militants to make explosives.\n\nIn April 2010, police in Greece confiscated 180 kg of ANFO and other related material stashed in a hideaway in the Athens suburb of Kareas. The material was believed to be linked to attacks previously carried out by the \"Revolutionary Struggle\" terrorist group.\n\nIn January 2010, President Hamid Karzai of Afghanistan also issued a decree banning the use, production, storage, purchase, or sale of ammonium nitrate, after an investigation showed militants in the Taliban insurgency had used the substance in bomb attacks.\n\nOn 22 July 2011, an aluminium powder-enriched ANNM explosive, with total size of 950 kg (150 kg of aluminium powder), increasing demolition power by 10–30% over plain ANFO, was used in the Oslo bombing.\n\nOn 13 April 2016, two suspected IRA members were stopped in Dublin with 67 kg of ANFO.\n\nOn 6 March 2018, 8 members of the extreme right neo-Nazi group «Combat 18 Hellas» arrested in Athens, Greece accused for multiple attacks on immigrants and activists with 50 kg of ANFO in their possession.\n\nAmmonium nitrate and nitromethane (ANNM) is one of the most powerful improvised types of AN-based explosives. The relative effectiveness factor of ANNM varies depending on the mix, but does not exceed 1.0 (ANNM+AL = R.E.F. of 1.0–1.1). ANNM usually contains a 60:40 (kinepak) mix of AN and NM (60% ammonium nitrate, 40% nitromethane by mass), though this results in a wet slurry. Sometimes, more AN is added to reduce liquidity and make it easier to store and handle, as well as providing an oxygen-balanced mix. ANNM is also more sensitive to shock than standard ANFO and is therefore easier to detonate. When ANNM detonates, the primary products are HO, CO and N, but NO and other toxic gases are inevitably formed because of a negative oxygen balance. The balanced equation is as follows:\n\n3NHNO + 2CHNO -> 4N + 2CO + 9HO\n\nDepending on the detonation impetus (for example a #6 versus a #10 detonator), the products of the detonation can be decidedly unstoichiometric.\n\n"}
{"id": "22351676", "url": "https://en.wikipedia.org/wiki?curid=22351676", "title": "Abbud Pasha", "text": "Abbud Pasha\n\nMuhammad Ahmad Abbud Pasha () (1899–1963) was an Egyptian entrepreneur and business magnate.\n\nAbbud established a construction firm in the early 1920s, which went on to become the most successful business group in Egypt of that era. By the 1940s, he owned the Sugar Company, the Khedival Mail Line as well as the Egyptian General Omnibus Company. In addition, he was the largest shareholder of Banque Misr, and obtained a seat on its board of directors in 1950. In the same year, he became the first Egyptian director of the Suez Canal Company, which was then owned by foreigners.\n\nAbbud received the noble title of \"Pasha\" on 14 February 1931. He was also active in politics. He became a parliamentary deputy in 1926, representing the Wafd Party which he supported financially until Mustafa el-Nahhas became its leader in 1927. Abbud later reconciled with the party and started supporting it once again during its final years in power, and was close to party strongman Fuad Siraj al-Din.\n\nAbbud Pasha died in London in 1963, and was reported to be among the ten richest men in the world at the time of his death.\n\n\n"}
{"id": "3279094", "url": "https://en.wikipedia.org/wiki?curid=3279094", "title": "Acetogenesis", "text": "Acetogenesis\n\nAcetogenesis is a process through which acetate is produced from CO and an electron source (e.g., H, CO, formate, etc.) by anaerobic bacteria via the reductive acetyl-CoA or Wood-Ljungdahl pathway. The different bacterial species that are capable of acetogenesis are collectively termed acetogens. Some acetogens can synthesize acetate autotrophically from carbon dioxide and hydrogen gas.\n\nIn 1932, organisms were discovered that could convert hydrogen gas and carbon dioxide into acetic acid. The first acetogenic bacterium species, \" Clostridium aceticum\", was discovered in 1936 by Klaas Tammo Wieringa. A second species, \"Moorella thermoacetica\", attracted wide interest when it was first isolated because of its ability to convert glucose into three moles of acetic acid.\n\nThe precursor to acetic acid is the thioester acetyl CoA. The key aspects of the acetogenic pathway are several reactions that include the reduction of carbon dioxide to carbon monoxide and the attachment of the carbon monoxide to a methyl group. The first process is catalyzed by enzymes called carbon monoxide dehydrogenase. The coupling of the methyl group (provided by methylcobalamin) and the CO is catalyzed by acetyl CoA synthetase. \n\nThe unique metabolism of acetogens has significance in biotechnological uses. In carbohydrate fermentations, the decarboxylation reactions involved result in the loss of carbon into carbon dioxide. This loss is an issue with an increased requirement of minimization of CO emissions, as well as successful competition for fossil fuels with biofuel production being limited by monetary value. Acetogens can ferment glucose without any CO emissions and convert it to 3 acetates, which can theoretically increase product yield by 50%. Acetogeneis does not replace glycolysis with a different pathway, but is rather used by capturing CO from glycolysis and placing it through acetogenesis.\n"}
{"id": "44977165", "url": "https://en.wikipedia.org/wiki?curid=44977165", "title": "All-Star Orchestra", "text": "All-Star Orchestra\n\nAll-Star Orchestra is an orchestral music project created by Gerard Schwarz, former music director and conductor laureate of Seattle Symphony. It is a television and DVD project, filmed by 18 high definition video cameras without an audience for PBS, the Khan Academy, educators, students, \"and enthusiasts.\" Mr Schwarz assembled 95 leading orchestral musicians, of major symphony orchestras, from across the United States. The assembled players performed over a four-day period. In 2014, the program consisted of eight episodes. The second season began broadcast in the fall of 2015. The organization's web site provides detail of the second season's programs.\n\nThe first part of the program consists of Stravinsky's \"The Firebird\", Beethoven's Fifth Symphony, Ravel's \"Daphnis et Chloé\" Suite No. 2, Shostakovich's Symphony No. 5 and Brahms's Intermezzo in A.\n\nApproximately fifty percent of the musicians were chosen from the Philadelphia Orchestra, Boston Symphony Orchestra, the Chicago Symphony Orchestra, the National Symphony Orchestra, the Seattle Symphony, the Houston Symphony, the Tulsa Symphony, the Minnesota Orchestra, the Pittsburgh Symphony Orchestra, the Richmond Symphony Orchestra, the San Francisco Symphony, the Cincinnati Symphony Orchestra, the New Jersey Symphony Orchestra and others. The other fifty percent were chosen from the New York area; the Metropolitan Opera, the New York Philharmonic, the Orpheus, the Orchestra of St. Luke's and New York City Opera.\n\nSelected prominent musicians represented orchestras based in Boston, Chicago, Cincinnati, Cleveland, Dallas, Detroit, Hartford, Houston, Jacksonville, Los Angeles, Minneapolis, Nashville, New York, Newark, Philadelphia, Pittsburgh, Portland, Salt Lake City, San Francisco, Seattle, the Tampa Bay Area, and Washington, D.C.\n\n"}
{"id": "12712567", "url": "https://en.wikipedia.org/wiki?curid=12712567", "title": "Analytic applications", "text": "Analytic applications\n\nAnalytic applications are a type of business application software, used to measure and improve the performance of business operations. More specifically, analytic applications are a type of business intelligence. As such they use collections of historical data about business operations to provide business users with information and tools that allow them to make improvements in business functions.\n\nThe maturity levels for business intelligence are:\n\nIt may extend further to predictive analytics, or predictive analysis may form part of the analytic application - depending on both the subject matter under analysis, and the nature of the analysis required. \n\nAnalytic applications are typically described as a subset of performance management. They specifically relate to the analysis of a business process (such as sales pipeline analysis, accounts payable analytics, or risk adjusted profitability analysis) in support of decision making.\n\nTo qualify as an application (rather than simply as a data warehousing tool), these tools should promote some form of automation. The maturity level of this automation is as follows:\nIn most cases, these three levels are discrete functions, loosely banded together as a single product, and there is little automation of the process from end to end.\n"}
{"id": "40563257", "url": "https://en.wikipedia.org/wiki?curid=40563257", "title": "Atmospheric mining", "text": "Atmospheric mining\n\nAtmospheric mining is the process of extracting valuable materials or other non-renewable resources from the atmosphere. Due to the abundance of hydrogen and helium in the outer planets of the Solar System, atmospheric mining may be easier than mining terrestrial surfaces.\n\nAtmospheric mining of outer planets has not yet begun. There is some consensus that gas should be mined from gas planets, but this would be difficult with current technology; however Uranus & Neptune would be the easiest planets to mine for gas. Jupiter & Saturn are closer, but Jupiter has a lot of gravity to contend with, and it could be difficult navigating through the rings of Saturn. Also, Earth's atmosphere can be mined for carbon dioxide to produce fuel and to reduce the levels of greenhouse gas in the atmosphere.\n\nHydrogen may fuel chemical and nuclear propulsion.\n\nHelium-3 may fuel nuclear propulsion.\n\nMethane may fuel chemical propulsion.\n\nCarbon dioxide mining on earth will reduce the level of greenhouse gases & can also produce fuel.\n\nHydrogen and helium are abundant in outer planets.\n\nAn aerostat would be a buoyant station in the atmosphere that gathers and stores gases. A vehicle would transfer the gases from the aerostat to an orbital station above the planet.\n\nA scooper would be a vehicle that gathers and transfers gases from the atmosphere to an orbital station.\n\nA cruiser would be a vehicle in the atmosphere that gathers and stores gases. A smaller vehicle would transfer the gases from the cruiser to an orbital station.\n\n\n"}
{"id": "10131104", "url": "https://en.wikipedia.org/wiki?curid=10131104", "title": "Australian International Design Awards", "text": "Australian International Design Awards\n\nThe Australian International Design Awards is an industry body established by the Industrial Design Council of Australia (IDCA), founded in 1958. The awards are Australia's only national design awards for industrial design. Since 1991, the Australian International Design Awards has been a division of Standards Australia.\n\nIn 2007 the Australian Design Awards expanded its entry criteria to include all professionally designed products on the Australian market, including products designed in Australia. From 2007, the Australian Design Awards became known as the Australian International Design Awards to reflect the global nature of the awards.\n\nThe Australian International Design Awards has been recognised by the Commonwealth Government and the International Council of Societies of Industrial Design as a promotional body for the Australian design industry.\n\nThere are three types of accolades issued by the Australian International Design Awards, they are:\n\nSpecial Awards include:\n\nIn 1958 the Industrial Design Council of Australia (IDCA) was established funded by the Commonwealth Government. The goal was to educate manufacturers and consumers on the value of design, encourage and promote high standards of design in manufactured goods.\n\nFrom 1964 Good Design Labels began to appear on products and the Australian Design Index became a register of the best designed products in Australia. A panel of experts reviewed items for inclusion in the Index. Products meeting the criteria received the Good Design Label and other manufacturers were given constructive criticism on how to improve their products.\n\nIn 1964, the IDCA opened the first Australian Design Centre in Melbourne with a special exhibition of selected products from the Australian Design Index. Federal and state government funding helped establish a new Design Centre in Sydney, with more centres to follow in other cities. \n\nIn 1967 the Prince Philip Prize for Australian Design was set up, supported by Prince Philip, Duke of Edinburgh, with the aim of promoting greater awareness of good design in Australian engineering. The inaugural Prince Philip Prize was awarded in 1968. Over 90 entries were received and the winning entry was a self-propelled grain header, designed by Kenneth Gibson. The Prince Philip Prize continued for twenty years.\n\nThe IDCA faced funding difficulties in the mid-1970s and was forced to close temporarily in 1976. A new funding injection from the Commonwealth Government helped the Council reopen and a new 'innovation' recognition program was introduced.\n\nRecognising not only high quality but innovative Australian designed products, the Australian Design Award (ADA) program became a valuable promotional tool for manufacturers and designers and provided a source of revenue for the IDCA to continue its operations. The Prince Philip Prize continued to be awarded, but only to products which had received the ADA.\n\nDuring this time, publicity was at an all-time high. Televised coverage of the Awards presentation on ABC TV reached audiences of over four million and in 1979, the first annual yearbook of ADA winners was published. \n\nFor the next two decades, however, continuing funding issues, dwindling industry support and a lack of clear direction plagued the IDCA. In 1987 in an effort to reinvigorate the movement, the government re-launched the IDCA as the Australian Design Council and the Prince Philip Prize was folded, leaving the ADA as Australia's top design accolade.\n\nIn 1991, control of the Australian Design Council and the ADA program moved to Standards Australia. Under Standards Australia, the ADA program continued to run, but the Australian Design Council was disbanded in 1993. New formats and incentives for the ADA program such as the Australian Design Mark certification scheme were trialled during the second half of the '90s without success. In 1997 a revamped format was introduced to the Awards, using an online application form and first round internet shortlisting. It attracted more than one hundred applications. The first Presentation Night was held at the Metro Theatre in Sydney. In 1998, profession-based categories were introduced.\n\nHowever, the program was threatened by significant operating costs. The 1999 program was put on hold while Standards Australia explored other options to secure the future of the awards. The majority of staff was made redundant and for the first time in many years, no Design Awards were presented in Australia. The Design Institute of Australia was approached to take over the program but the financial commitment to posed too great a risk. \n\nArmed with a new business plan and financial model, the Board of Standards Australia approved another year for the awards. For the next few years, the ADA continued to grow in standing and support, buoyed by financial stability. A student design category was launched in 2002 supported by Dyson Appliances Australia and in 2004, product-focused categories were introduced.\n\nIn 2008, on the 50th anniversary of rewarding design and innovation excellence in Australia, internationally designed products available for sale in Australia were allowed to enter the awards for the first time. This bold move was aimed at raising the stakes for good design once again, allowing Australian design to be benchmarked against the best in the world. With the ongoing support of the design industry and Standards Australia, this opens the door to the next chapter in the history of the Design Awards.\n\nPrevious winners include:\n\n\n"}
{"id": "25527301", "url": "https://en.wikipedia.org/wiki?curid=25527301", "title": "Capacitive displacement sensor", "text": "Capacitive displacement sensor\n\nCapacitive displacement sensors “are non-contact devices capable of high-resolution measurement of the position and/or change of position of any conductive target”. They are also able to measure the thickness or density of non-conductive materials. Capacitive displacement sensors are used in a wide variety of applications including semiconductor processing, assembly of precision equipment such as disk drives, precision thickness measurements, machine tool metrology and assembly line testing. These types of sensors can be found in machining and manufacturing facilities around the world.\n\nCapacitance is an electrical property which is created by applying an electrical charge to two conductive objects with a gap between them. A simple demonstration is two parallel conductive plates of the same profile with a gap between them and a charge applied to them. In this situation, the Capacitance can be expressed by the equation:\nWhere \"C\" is the capacitance, ε is the permittivity of free space constant, \"K\" is the dielectric constant of the material in the gap, \"A\" is the area of the plates, and \"d\" is the distance between the plates.\n\nThere are two general types of capacitive displacement sensing systems. One type is used to measure thicknesses of conductive materials. The other type measures thicknesses of non conductive materials or the level of a fluid.\n\nA capacitive sensing system for conductive materials uses a model similar to the one described above, but in place of one of the conductive plates, is the sensor, and in place of the other, is the conductive target to be measured. Since the area of the probe and target remain constant, and the dielectric of the material in the gap (usually air) also remains constant, \"any change in capacitance is a result of a change in the distance between the probe and the target.\" Therefore, the equation above can be simplified to:\nWhere α indicates a proportional relationship.\nDue to this proportional relationship, a capacitive sensing system is able to measure changes in capacitance and translate these changes into distance measurements.\n\nThe operation of the sensor for measuring thickness of non-conductive materials can be thought of as two capacitors in series, with each having a different dielectric (and dielectric constant). The sum of the thicknesses of the two dielectric materials remains constant but the thickness of each can vary. The thickness of the material to be measured displaces the other dielectric. The gap is often an air gap, (dielectric constant = 1) and the material has a higher dielectric. As the material gets thicker, the capacitance increases and is sensed by the system.\n\nA sensor for measuring fluid levels works as two capacitors in parallel with constant total area. Again the difference in the dielectric constant of the fluid and the dielectric constant of air results in detectable changes in the capacitance between the conductive probes or plates.\n\nOne of the more common applications of capacitive sensors is for precision positioning. Capacitive displacement sensors can be used to measure the position of objects down to the nanometer level. This type of precise positioning is used in the semiconductor industry where silicon wafers need to be positioned for exposure. Capacitive sensors are also used to pre-focus the electron microscopes used in testing and examining the wafers.\n\nIn the disc drive industry, capacitive displacement sensors are used to measure the runout (a measure of how much the axis of rotation deviates from an ideal fixed line) of disc drive spindles. By knowing the exact runout of these spindles, disc drive manufacturers are able to determine the maximum amount of data that can be placed onto the drives. Capacitive sensors are also used to ensure that disc drive platters are orthogonal to the spindle before data is written to them.\n\nCapacitive displacement sensors can be used to make very precise thickness measurements. Capacitive displacement sensors operate by measuring changes in position. If the position of a reference part of known thickness is measured, other parts can be subsequently measured and the differences in position can be used to determine the thickness of these parts. In order for this to be effective using a single probe, the parts must be completely flat and measured on a perfectly flat surface. If the part to be measured has any curvature or deformity, or simply does not rest firmly against the flat surface, the distance between the part to be measured and the surface it is placed upon will be erroneously included in the thickness measurement. This error can be eliminated by using two capacitive sensors to measure a single part. Capacitive sensors are placed on either side of the part to be measured. By measuring the parts from both sides, curvature and deformities are taken into account in the measurement and their effects are not included in the thickness readings.\n\nThe thickness of plastic materials can be measured with the material placed between two electrodes a set distance apart. These form a type of capacitor. The plastic when placed between the electrodes acts as a dielectric and displaces air (which has dielectric constant of 1, different from the plastic). Consequently the capacitance between the electrodes changes. The capacitance changes can then be measured and correlated with the material's thickness.\n\nCapacitive sensors circuits can be constructed that are able to detect changes in capacitance on the order of a 10 picofarads (10 attofarads).\n\nWhile capacitive displacement sensors are most often used to sense changes in position of conductive targets, they can also be used to sense the thickness and/or density of non-conductive targets as well. A non-conductive object placed in between the probe and conductive target will have a different dielectric constant than the air in the gap and will therefore change the Capacitance between probe and target. (See the first equation above) By analyzing this change in capacitance, the thickness and density of the non-conductor can be determined.\n\nCapacitive displacement sensors are often used in metrology applications. In many cases, sensors are used “to measure shape errors in the part being produced. But they also can measure the errors arising in the equipment used to manufacture the part, a practice known as machine tool metrology”. In many cases, the sensors are used to analyze and optimize the rotation of spindles in various machine tools, examples include surface grinders, lathes, milling machines, and air bearing spindles. By measuring errors in the machines themselves, rather than simply measuring errors in the final products, problems can be dealt with and fixed earlier in the manufacturing process.\n\nCapacitive displacement sensors are often used in assembly line testing. Sometimes they are used to test assembled parts for uniformity, thickness or other design features. At other times, they are used to simply look for the presence or absence of a certain component, such as glue. Using capacitive sensors to test assembly line parts can help to prevent quality concerns further along in the production process.\n\nCapacitive displacement sensors share many similarities to eddy current (or inductive) displacement sensors; however capacitive sensors use an electric field as opposed to the magnetic field used by eddy current sensors This leads to a variety of differences between the two sensing technologies, with the most notable differences being that capacitive sensors are generally capable of higher resolution measurements, and eddy current sensors work in dirty environments while capacitive sensors do not.\n\n\n\n"}
{"id": "38365475", "url": "https://en.wikipedia.org/wiki?curid=38365475", "title": "Centre for Interactive Research on Sustainability", "text": "Centre for Interactive Research on Sustainability\n\nThe Centre for Interactive Research on Sustainability (CIRS) is located at the University of British Columbia's (UBC) Point Grey Campus in Vancouver. The building is dedicated to research collaboration and outreach on urban sustainability. It was officially opened in November 2011.\n\nThe CIRS vision is to be the most innovative high performance building in North America and an internationally recognized leader in accelerating the adopt of sustainable building and urban practices. \n\nSome key features:\nIt has been certified LEED Platinum by the Canada Green Building Council.\n\n\n\nThe CIRC was conceived in 2000 by Professor John Robinson and designed by Peter Busby who excelled in green building designs. Together, they organized and talked to potential partners which led to Alberto Cayuela, formally an employee of Stantec and now associate director responsible for CIRS, joining the team as program manager.\n\nOnce funding and concept was decided, the location was chosen. Originally, it was to be built on the UBC Point Grey campus, but, because CIRS was an inter-instituational project (UBC leads the project in conjunction with Simon Fraser University, BC Institute of Technology, and Emily Carr University of Art + Design) it was felt that the Great Northern Way campus would be a better position. In line with the idea of an inter-institutional project, the new position would expand its scope and breadth.\n\nIn 2008, the decision to move CIRS back to UBC Point Grey campus was made due to delays to the project timeline brought on by the complexity of building on the Great Northern Campus. Although, now only one institution (UBC) owned CIRS, other institutions continued to participate through other means.\n\nConstruction began in 2009 with excavation and foundation work for the first six months with the building being complete in August 2011.\n\nThe CIRS is constructed of wood and allows for various benefits such as reducing ecological impact, sequestering carbon, helping the mitigation of an ecological problem, bringing warmth to the building interiors, and also maximizing daylight and views.\n\nThe primary exterior material is white brick, clear glazing, wood, and neutral colored concrete. The benefits are the reduction of 'red list' materials, improvement of indoor air quality, reduction of material waste, and reduction of carbon emissions.\n\nThe CIRS achieves a net-positive energy performance and reduces UBC's overall energy consumption by over 1 million kilowatt hours per year. The energy system benefits the project by providing a comparison between energy modelling processes, allows for more opportunities for research and system testing, inspire a new approach to energy supply/consumption, reduce heat demand and carbon emissions of neighboring buildings, and provide opportunities for continued learning.\n\nCIRS is entirely water self-sufficient by harvesting rainwater and stored in a 100-cubic-meter cistern underneath the building. Around 1,226,000 litres of rainwater is harvested throughout the year, and average demand is 2000 litres per day. The benefits of a rainwater potable water system is that it creates awareness, educates the public on water supply/consumption, reduces potable water demand, promotes stormwater management, and provides an onsite water source for fire suppression.\n\nAll the reclaimed water originates either from the CIRS building or sewer system which is treated onsite using a Solar Aquatics wastewater treatment system and reused in the building for toilet flushing and to irrigate the living roof, landscaped areas and the living wall. The main benefit of this treatment system is awareness and engaging the public in viewing sustainability as a possibility. \n\nNarrow floor plates allowed for daylight penetration and light to enter the building from both sides. The benefits is that it improves the inhabitant health and comfort, creates a friendly space, reduces energy consumption and cost, and provides a surface for photovoltaic cells.\n\nThe CIRS uses a mixed mode system utilizing mostly passive natural ventilation. There are two mechanical air handling units that supply fresh filtered air: one to the large auditorium and the other to the rest of the building. In addition, there are manual operable windows that allows for airflow and temperature control. This system reduces energy consumption, connects the inhabitants with nature, and improve their health.\n\nThe CIRS has been publicly recognized by many as \"North America's Greenest Building\" and one of the greenest buildings in the world. Most recently in 2013, the CIRS won the Green Building Award awarded by the Wood WORKS! BC Wood Design Awards.\n\nRainwater harvesting in Canada\n"}
{"id": "32569356", "url": "https://en.wikipedia.org/wiki?curid=32569356", "title": "Contesting the Future of Nuclear Power", "text": "Contesting the Future of Nuclear Power\n\nContesting the Future of Nuclear Power: A Critical Global Assessment of Atomic Energy is a 2011 book by Benjamin K. Sovacool, published by World Scientific. Sovacool’s book addresses the current status of the global nuclear power industry, its fuel cycle, nuclear accidents, environmental impacts, social risks, energy payback, nuclear power economics, and industry subsidies. There is a postscript on the Japanese 2011 Fukushima nuclear disaster. Based on detailed analysis, Sovacool concludes \"that a global nuclear renaissance would bring immense technical, economic, environmental, political, and social costs\". He says that it is renewable energy technologies which will enhance energy security, and which have many other advantages.\nThe book says the marginal levelized cost for \"a 1,000-MWe facility built in 2009 would be 41.2 to 80.3 cents/kWh, presuming one actually takes into account construction, operation and fuel, reprocessing, waste storage, and decommissioning.\"\n\nIn a review by author Mark Diesendorf the book \"reviews the little-known research which shows that the life-cycle CO2 emissions of nuclear power may become comparable with those of fossil power as high-grade uranium ore is used up over the next several decades and low-grade uranium is mined and milled using fossil fuels\". Diesendorf says that one weakness of the book is the limited coverage of nuclear weapons proliferation. He says that governments of several countries (e.g., France, India, North Korea, Pakistan) have used nuclear power and/or research reactors to assist nuclear weapons development or to contribute to their supplies of nuclear explosives from military reactors.\n\n\n"}
{"id": "1543032", "url": "https://en.wikipedia.org/wiki?curid=1543032", "title": "DIY audio", "text": "DIY audio\n\nDIY Audio means \"do it yourself\" audio. Rather than buying a piece of possibly expensive audio equipment, such as a high-end audio amplifier or speaker, the person practicing DIY Audio will make it him/herself. Alternatively, a DIYer may take an existing manufactured item of vintage era and update or modify it. The benefits of doing so include the satisfaction of creating something enjoyable, the possibility that the equipment made or updated is of higher quality than commercially available products and the pleasure of creating a custom-made device for which no exact equivalent is marketed. Other motivations for DIY audio can include getting audio components at a lower cost, the entertainment of using the item, and being able to ensure quality of workmanship.\n\nAudio DIY came to prominence in the 1950s to 1960s, as audio reproduction was relatively new and the technology \"complex,\" audio reproduction equipment, and in particular high performance equipment, was not offered at the retail level. Kits and designs were available for consumers to build their own equipment. Famous vacuum tube kits from Dynaco, Heathkit, and McIntosh, as well as solid state (transistor) kits from Hafler allowed for consumers to build their own hi fidelity systems. Books and magazines were published which explained new concepts regarding the design and operation of vacuum tube and (later) transistor circuits.\n\nWhile audio equipment has become easily accessible in the current day and age, there still exists an interest in building and repairing one's own equipment including, but not limited to; pre-amplifiers, amplifiers, speakers, cables, CD players and turntables. Today, a network of companies, parts vendors, and on-line communities exist to foster this interest. DIY is especially active in loudspeaker and in tube amplification. Both are relatively simple to design and fabricate without access to sophisticated industrial equipment. Both enable the builder to pick and choose between various available parts, on matters of price as well as quality, allow for extensive experimentation, and offer the chance to use exotic or highly labor-intensive solutions, which would be expensive for a manufacturer to implement, but only require personal labor by the DIYer, which is a source of satisfaction to them.\n\nSince the 1960s, integrated circuits make construction of DIY audio systems easier, but the proliferation of surface mount components (which are very small and difficult to solder with a soldering iron) and fine pitch printed circuit boards (PCBs) arguably make the physical act of construction more difficult. Nevertheless, surface mounting is often used, as are conventional PCBs and electronic components, while some enthusiasts insist on using old-style perforated cardboard onto which individual components are \"hardwired\" and soldered. Test equipment is readily available for purchase and enables convenient testing of parts and systems. Specifications of parts and components are readily accessible through the Internet including data sheets and equipment designs.\n\nIt has become easier to make audio components from \"scratch\" rather than from \"kits\" due to the availability of CAD software for printed circuit board (PCB) layouts and electronic circuit simulation. Such software can be free, and a trial version may also be used. PCB vendors are more accessible than ever, and can manufacture PCBs in small quantities for the do-it-yourselfer. In fact, kits and chemicals for self-manufacturing one's own PCB can be obtained. Electronic parts and components are accessible online or in specialty shops, and various \"high-end\" parts vendors exist. On the other hand, a wide variety of kits, designs and premanufactured PCBs are available for almost any type of audio component.\n\nTo construct a device takes more than knowledge of circuits, many would urge that the mechanical aspects of cabinets, cases and chassis' are the most time consuming aspects of audio DIY. Drilling, metalworking and physical measurements are critical to constructing almost any DIY audio project, especially speakers. Measuring equipment such as a Vernier caliper is often essential. Woodworking skills are required to construct wooden enclosures (e.g. for speakers), with some enthusiasts going beyond traditional woodworking to CNC turning, and luxurious veneers and lacquers. Room acoustics solutions are also popular among DIYers, as they can be made with inexpensive and readily available insulating materials, and can be dimensioned to fit each particular room in a precise and aesthetically pleasing way.\n\nDIY audio involves \"projects\" directed to audio. Many DIY audio people fancy themselves to be audiophiles. These people use rare and expensive parts and components in their projects. Examples are the use of silver wire, expensive capacitors, non-standard solders of various alloys, and use of parts that have been cryogenically cooled.\n\nVacuum tube or \"valve\" projects are common in audio DIY. While, for mass market audio components, the vacuum tube has been replaced in modern times with the transistor and IC, the vacuum tube remains prominent in specialty high end audio equipment. Thus, interest exists in building components using vacuum tubes, and the vacuum tube is still widely available. There is a wide variety of tubes manufactured nowadays, and many tubes on the market are advertised as \"NOS\" (\"new, old stock\", meaning unused stock of old manufacture); not all of the latter being genuinely NOS. Circuits utilizing tubes often are far less complicated than those utilizing transistors or op-amps. Tube enthusiasts often use transformers, sometimes custom-made ones, or even hand-wind their own transformers using cores and wire of their own choice. Note that vacuum tube projects almost always use dangerously high voltages and should be undertaken with due care.\n\nIn case lead-containing solder is used instead of RoHS-compliant solder, appropriate environmental precautions with regard to lead and lead products should be taken.\n\nDIY audio can also involve \"tweaking\" of mass market components. It is thought that mass market audio components are compromised by the use of cheap or inferior internal parts that can be easily replaced with high quality substitutes. As a result, an audio component of improved characteristics may be obtained for relatively low cost. Some common changes include replacing opamps, replacing capacitors (recap), or even replacing resistors in order to increase signal-to-noise ratio. Changing an audio component in this way is similar to what a tweaker or modder does with a personal computer.\n\nCircuit bending is the creative customization of the circuits within electronic devices such as low voltage, battery-powered guitar effects, and small digital synthesizers to create new musical or visual instruments and sound generators. Emphasizing spontaneity and randomness, the techniques of circuit bending have been commonly associated with noise music, though many more conventional contemporary musicians and musical groups have been known to experiment with \"bent\" instruments. Circuit bending usually involves dismantling the machine and adding components such as switches and potentiometers that alter the circuit.\n\nAnother common practice in the DIY audio community is to attempt to \"clone\" or copy a preexisting design or component from a commercial manufacturer. This involves obtaining a lawful public version of, or lawfully reverse engineering, the circuit schematics for the design, and/or even the publicly available PCB layouts. Such a \"clone\" will not be a perfect copy since different brands and types of parts (often newer parts) will be used, and mechanical aspects of construction will likely differ. However, the circuit or other distinguishing features should be close to the original.\n\nThere are many reasons for wanting to recreate an existing design. The design might be historically important and/or out of production, so the only way to obtain the component is to build it. The design might be very simple so copying it is easily done. The commercial product might be very expensive but its design known, so it may be built for far less than it cost to be purchased. The original design may have some sentimental value to the person building the recreation, and the design built for the memories in one's past. The copy may be made to test or evaluate design concepts or principles in the original.\n\nAs an example, a well known \"clone\" includes amplifiers using high power integrated circuits, such as the National Semiconductor LM3875 and LM3886. The use of a high power IC as part of a quality audio amplifier was popularized by the 47 Labs Gaincard amplifier, and thus the DIY amplifiers using power ICs are often called \"chipamps\" or \"Gainclones.\"\n\nUsually cloning additionally involves improving or tweaking (see above) the original design, potentially by using more modern components (in the case of discontinued designs,) higher quality parts, or more efficient board layout.\n\nOperational amplifier (op-amps) swapping is the process of replacing an operational amplifier in audio equipment with a different one, in an attempt to improve performance or change the perceived sound quality. Op-amps are used in most audio devices, and most op-amps have the same pinouts, making replacement fairly simple. If the new device's parameters sometimes do not match it can lead to problems like high-frequency oscillation.\n\n"}
{"id": "2580396", "url": "https://en.wikipedia.org/wiki?curid=2580396", "title": "Deaerator", "text": "Deaerator\n\nA deaerator is a device that removes oxygen and other dissolved gases from water, such as feedwater for steam-generating boilers. Dissolved oxygen in feedwater will cause serious corrosion damage in a boiler by attaching to the walls of metal piping and other equipment and forming oxides (rust). Dissolved carbon dioxide combines with water to form carbonic acid that causes further corrosion. Most deaerators are designed to remove oxygen down to levels of 7 ppb by weight (0.005 cm³/L) or less, as well as essentially eliminating carbon dioxide.\n\nDeaerator Working Principle\n\nDeaeration relies on the principle that the solubility of a gas in water decreases as the water temperature increases and approaches saturation temperature. In the deaerator, water is heated up to close to saturation temperature with a minimum pressure drop and minimum vent. Deaeration is done by spraying the feedwater to provide a large surface area, and may involve flow over multiple layers of trays. This scrubbing (or stripping) steam is fed to the bottom of the deaeration section of the deaerator. When steam contacts the feedwater, it heats it up to saturation temperature and dissolved gases are released from the feedwater and vented from the deaerator through the vent. The treated water falls to the storage tank below the deaerator.\n\nThere are many different horizontal and vertical deaerators available from a number of manufacturers, and the actual construction details will vary from one manufacturer to another. Figures 1 and 2 are representative schematic diagrams that depict each of the two major types of deaerators.\n\nThere are two basic types of deaerators, the Termochimica spray&tray-type and the Stork spray-type:\n\nThe typical spray&tray-type deaerator in Figure 1 has a vertical domed deaeration section mounted above a horizontal boiler feedwater storage vessel. Boiler feedwater enters the vertical deaeration section through spray valves above the perforated trays and then flows downward through the perforations. Low-pressure deaeration steam enters below the perforated trays and flows upward through the perforations. Combined action of spray valves & trays guarantees very high performance (as confirmed by HEI std ) because of longer contact time between steam and water. Some designs use various types of packed bed, rather than perforated trays, to provide good contact and mixing between the steam and the boiler feed water.\n\nThe steam strips the dissolved gas from the boiler feedwater and exits via the vent valve at the top of the domed section. Should this vent valve not be opened sufficiently the deaerator will not work properly, causing high oxygen content in the feed water going to the boilers. Should the boiler not have an oxygen-content analyser, a high level in the boiler chlorides may indicate the vent valve not being far enough open. Some designs may include a vent condenser to trap and recover any water entrained in the vented gas. The vent line usually includes a valve and just enough steam is allowed to escape with the vented gases to provide a small visible telltale plume of steam.\n\nThe deaerated water flows down into the horizontal storage vessel from where it is pumped to the steam generating boiler system. Low-pressure heating steam, which enters the horizontal vessel through a \"sparger pipe\" in the bottom of the vessel, is provided to keep the stored boiler feedwater warm. External insulation of the vessel is typically provided to minimize heat loss.\n\nAs shown in Figure 2, the typical spray-type deaerator is a horizontal vessel which has a preheating section (E) and a deaeration section (F). The two sections are separated by a baffle (C). Low-pressure steam enters the vessel through a sparger in the bottom of the vessel.\n\nThe boiler feedwater is sprayed into section (E) where it is preheated by the rising steam from the sparger. The purpose of the feedwater spray nozzle (A) and the preheat section is to heat the boiler feedwater to its saturation temperature to facilitate stripping out the dissolved gases in the following deaeration section.\n\nThe preheated feedwater then flows into the deaeration section (F), where it is deaerated by the steam rising from the sparger system. The gases stripped out of the water exit via the vent at the top of the vessel. Again, some designs may include a vent condenser to trap and recover any water entrained in the vented gas. Also again, the vent line usually includes a valve and just enough steam is allowed to escape with the vented gases to provide a small and visible telltale plume of steam.\n\nThe deaerated boiler feedwater is pumped from the bottom of the vessel to the steam generating boiler system.\n\nThe deaerators in the steam generating systems of most thermal power plants use low pressure steam obtained from an extraction point in their steam turbine system. However, the steam generators in many large industrial facilities such as petroleum refineries may use whatever low-pressure steam is available.\n\nOxygen scavenging chemicals are very often added to the deaerated boiler feedwater to remove any last traces of oxygen that were not removed by the deaerator.\nThe type of chemical added depends on whether the location uses a volatile or non-volatile water treatment program. Most lower pressure systems (<650psi) use a non-volatile program. Most higher pressure systems (>650psi) and all systems where certain highly alloyed materials are present, are now using volatile programs as the old phosphate-based programs are phased out.\nVolatile programs are further broken down into oxidizing or reducing programs [(AVT(O) or AVT(R)] depending whether the waterside environment requires an oxidizing or reducing environment to reduce the incidence of flow-accelerated corrosion (FAC) which is a highly debated topic within the industry today. FAC-related failures have caused numerous accidents in which significant loss of property and life has occurred.\n\nThe most commonly used oxygen scavenger for lower pressure systems is sodium sulfite (NaSO). It is very effective and rapidly reacts with traces of oxygen to form sodium sulfate (NaSO) which is non-scaling. Another widely used oxygen scavenger is hydrazine (NH), used for locations using volatile programs.\n\nOther scavengers include 1,3-diaminourea (also known as carbohydrazide), diethylhydroxylamine (DEHA), nitrilotriacetic acid (NTA), ethylenediaminetetraacetic acid (EDTA), and hydroquinone.\n\n\n"}
{"id": "51377115", "url": "https://en.wikipedia.org/wiki?curid=51377115", "title": "Dell Technologies", "text": "Dell Technologies\n\nDell Technologies Inc. is an American multinational information technology corporation based in Round Rock, Texas. It was formed as a result of the acquisition of Dell and EMC Corporation (which later became Dell EMC).\n\nDell's products include personal computers, servers, smartphones, televisions, computer software, computer and network security, as well as information security services. Dell ranked No. 35 on the 2018 Fortune 500 rankings of the largest United States corporations by total revenue.\n\nAs of 2016, approximately 50% of the company's revenue is derived in the United States.\n\nDell operates under 3 divisions as follows:\n\nDell also owns 4 separate businesses: RSA, Pivotal Software, SecureWorks, and Boomi, Inc.\n\nOn October 12, 2015, Dell announced its intent to acquire the enterprise software and storage company EMC Corporation in a $67 billion transaction. It was labeled the \"highest-valued tech acquisition in history\". In addition to Michael Dell, Singapore's Temasek Holdings and Silver Lake Partners were major Dell shareholders that supported the transaction.\n\nOn September 7, 2016, Dell Inc. completed the merger with EMC Corp., which involved the issuance of $45.9 billion in debt and $4.4 billion common stock.\n\nThe Dell Services, Dell Software Group, and the Dell EMC Enterprise Content Divisions were sold shortly thereafter for proceeds of $7.0 billion, which was used to repay debt.In October 2017, It was reported that Dell would be investing $1 billion in IoT research and development.\n\nThe announcement came two years after Dell Inc. returned to private ownership, claiming that it faced bleak prospects and would need several years out of the public eye to rebuild its business. It's thought that the company's value has roughly doubled since then. EMC was being pressured by Elliott Management, a hedge fund holding 2.2% of EMC's stock, to reorganize their unusual \"Federation\" structure, in which EMC's divisions were effectively being run as independent companies. Elliott argued this structure deeply undervalued EMC's core \"EMC II\" data storage business, and that increasing competition between EMC II and VMware products was confusing the market and hindering both companies. \"The Wall Street Journal\" estimated that in 2014 Dell had revenue of $27.3 billion from personal computers and $8.9 billion from servers, while EMC had $16.5 billion from EMC II, $1bn from RSA Security, $6bn from VMware, and $230 million from Pivotal Software. EMC owns around 80% of the stock of VMware. The proposed acquisition will maintain VMware as a separate company, held via a new tracking stock, while the other parts of EMC will be rolled into Dell. Once the acquisition closes Dell will again publish quarterly financial results, having ceased these on going private in 2013.\n\nThe combined business is expected to address the markets for scale-out architecture, converged infrastructure and private cloud computing, playing to the strengths of both EMC and Dell. Commentators have questioned the deal, with FBR Capital Markets saying that though it makes a \"ton of sense\" for Dell, it's a \"nightmare scenario that would lack strategic synergies\" for EMC. \"Fortune\" said there was a lot for Dell to like in EMC's portfolio, but \"does it all add up enough to justify tens of billions of dollars for the entire package? Probably not.\" \"The Register\" reported the view of William Blair & Company that the merger would \"blow up the current IT chess board\", forcing other IT infrastructure vendors to restructure to achieve scale and vertical integration. The value of VMware stock fell 10% after the announcement, valuing the deal at around $63–64bn rather than the $67bn originally reported.\n\nOn January 29, 2018, it was reported that Dell Technologies was considering a reverse merger with its VMWare subsidiary in order to take the company public.\n"}
{"id": "2635037", "url": "https://en.wikipedia.org/wiki?curid=2635037", "title": "Demister (vapor)", "text": "Demister (vapor)\n\nA demister is a device often fitted to vapor–liquid separator vessels to enhance the removal of liquid droplets entrained in a vapor stream. Demisters may be a mesh-type coalescer, vane pack or other structure intended to aggregate the mist into droplets that are heavy enough to separate from the vapor stream. \n\nDemisters can reduce the residence time required to separate a given liquid droplet size by reducing the volume and associated cost of separator equipment. Demisters are often used where vapor quality is important in regard to entrained liquids, particularly where separator equipment costs are high (e.g., high-pressure systems) or where space or weight savings are advantageous.\n\nFor example, in the process of brine desalination on marine vessels, brine is flash-heated into vapor. In flashing, vapor carries over droplets of brine, which have to be separated before condensing, otherwise the distillate vapor would be contaminated with salt. This is the role of the demister. Demisted vapor condenses on tubes in the desalination plant, and product water is collected in the distillate tray.\n\n"}
{"id": "25814646", "url": "https://en.wikipedia.org/wiki?curid=25814646", "title": "Drilling riser", "text": "Drilling riser\n\nA drilling riser is a conduit that provides a temporary extension of a subsea oil well to a surface drilling facility. Drilling risers are categorised into two types: marine drilling risers used with subsea blowout preventer (BOP) and generally used by floating drilling vessels; and tie-back drilling risers used with a surface BOP and generally deployed from fixed platforms or very stable floating platforms like a spar or tension leg platform (TLP).\n\nA marine drilling riser has a large diameter, low pressure main tube with external auxiliary lines that include high pressure choke and kill lines for circulating fluids to the subsea blowout preventer (BOP), and usually power and control lines for the BOP.\nThe design and operation of marine drilling risers is complex, and the requirement for high reliability means an extensive amount of engineering analysis is required.\n\nWhen used in water depths greater than about 20 meters, the marine drilling riser has to be tensioned to maintain stability. A marine riser tensioner located on the drilling platform provides a near constant tension force adequate to maintain the stability of the riser in the offshore environment. The level of tension required is related to the weight of the riser equipment, the buoyancy of the riser, the forces from waves and currents, the weight of the internal fluids, and an adequate allowance for equipment failures.\n\nTo reduce the amount of tension required to maintain stability of the riser, buoyancy modules, known in the industry as 'buoyancy cakes', are added to the riser joints to make them close to neutrally buoyant when submerged.\nThe international standard ISO 13624-1:2009 covers the design, selection, operation and maintenance of marine riser systems for floating drilling operations. Its purpose is to serve as a reference for designers, for those who select system components, and for those who use and maintain this equipment. It relies on basic engineering principles and the accumulated experience of offshore operators, contractors, and manufacturers.\n\nA tie-back riser can be either a single large-diameter high pressure pipe, or a set of concentric pipes extending the casing strings in the well up to a surface BOP.\n\n"}
{"id": "247578", "url": "https://en.wikipedia.org/wiki?curid=247578", "title": "Dzong architecture", "text": "Dzong architecture\n\nDzong architecture is a distinctive type of fortress (, ) architecture found mainly in Bhutan and Tibet. The architecture is massive in style with towering exterior walls surrounding a complex of courtyards, temples, administrative offices, and monks' accommodation.\n\nDistinctive features include:\n\nDzongs serve as the religious, military, administrative, and social centers of their district. They are often the site of an annual \"tsechu\" or religious festival.\n\nThe rooms inside the dzong are typically allocated half to administrative function (such as the office of the \"penlop\" or governor), and half to religious function, primarily the temple and housing for monks. This division between administrative and religious functions reflects the idealized duality of power between the religious and administrative branches of government.\n\nTibet used to be divided into 53 prefecture districts also called \"dzongs\". There were two dzongpöns for each dzong, a lama and a layman. They were entrusted with both civil and military powers and are equal in all respects, though subordinate to the generals and the Chinese amban in military matters, until the expulsion of the ambans following the Xinhai Revolution in 1912. Today, 71 counties in the Tibet Autonomous Region are called \"dzong\"s in the Tibetic languages.\n\nBhutanese dzong architecture reached its zenith in the 17th century under the leadership of Ngawang Namgyal, the 1st Zhabdrung Rinpoche. The Zhabdrung relied on visions and omens to site each of the dzongs. Modern military strategists would observe that the dzongs are well-sited with regard to their function as defensive fortresses. Wangdue Phodrang dzong, for instance, is set upon a spur overlooking the confluence of the Sankosh (Puna Tsang) and Tang Rivers, thus blocking any attacks by southern invaders who attempted to use a river route to bypass the trackless slopes of the middle Himalayas in attacking central Bhutan. Drukgyel Dzong at the head of the Paro valley guards the traditional Tibetan invasion path over the passes of the high Himalayas.\n\nDzongs were frequently built on a hilltop or mountain spur. If the dzong is built on the side of a valley wall, a smaller dzong or watchtower is typically built directly uphill from the main dzong with the purpose of keeping the slope clear of attackers who might otherwise shoot downward into the courtyard of the main dzong below (see image at head of article).\n\nPunakha Dzong is distinctive in that it is sited on a relatively flat spit of land at the confluence of the Mo and Pho Rivers. The rivers surround the dzong on three sides, providing protection from attack. This siting proved inauspicious, however, when in 1994 a glacial lake 90 kilometers upstream burst through its ice dam to cause a massive flood on the Pho Chhu, damaging the dzong and taking 23 lives.\n\nBy tradition, dzongs are constructed without the use of architectural plans. Instead construction proceeds under the direction of a high lama who establishes each dimension by means of spiritual inspiration.\n\nIn previous times the dzongs were built using corvée labor which was applied as a tax against each household in the district. Under this obligation each family was to provide or hire a decreed number of workers to work for several months at a time (during quiet periods in the agricultural year) in the construction of the dzong.\n\nDzongs comprise heavy masonry curtain walls surrounding one or more courtyards. The main functional spaces are usually arranged in two separate areas: the administrative offices; and the religious functions - including temples and monks' accommodation. This accommodation is arranged along the inside of the outer walls and often as a separate stone tower located centrally within the courtyard, housing the main temple, that can be used as an inner defensible citadel. The main internal structures are again built with stone (or as in domestic architecture by rammed clay blocks), and whitewashed inside and out, with a broad red ochre band at the top on the outside. The larger spaces such as the temple have massive internal timber columns and beams to create galleries around an open central full height area. Smaller structures are of elaborately carved and painted timber construction.\n\nThe roofs are massively constructed in hardwood and bamboo, highly decorated at the eaves, and are constructed traditionally without the use of nails. They are open at the eaves to provide a ventilated storage area. They were traditionally finished with timber shingles weighted down with stones; but in almost all cases this has now been replaced with corrugated galvanised iron roofing. The roof of Tongsa Dzong, illustrated, is one of the few shingle roofs to survive and was being restored in 2006/7.\n\nThe courtyards, usually stone-flagged, are generally at a higher level than the outside and approached by massive staircases and narrow defensible entrances with large wooden doors. All doors have thresholds to discourage the entrance of spirits. Temples are usually set at a level above the courtyard with further staircases up to them.\n\nLarger modern buildings in Bhutan often use the form and many of the external characteristics of dzong architecture in their construction, although incorporating modern techniques such as a concrete frame.\n\nThe campus architecture of the University of Texas at El Paso (UTEP) is a rare example of dzong style seen outside the Himalayas. Initial phases were designed by El Paso architect Henry Trost, and later phases have continued in the same style.\n\nIn 2012, the Butanese government listed five dzongs to its tentative list for UNESCO World Heritage Site inscription in the future. The five dzongs are Punakha Dzong, Wangdue Phodrang Dzong, Paro Dzong, Trongsa Dzong and Dagana Dzong.\n\n\n\n"}
{"id": "15718137", "url": "https://en.wikipedia.org/wiki?curid=15718137", "title": "Energen Wave Power", "text": "Energen Wave Power\n\nThe Energen Wave Power device is a generator that uses the motion of near shore ocean surface waves to create electricity. It is an attenuating wave device designed for extracting energy available from a single wave over a large surface area. It has a simple robust design that will enable it to withstand harsh sea conditions. The design of the generator allows for single or modular installations. Modular installations will reduce costs making power generating sites more financially viable. The size of each generator can be changed to accommodate regional wave conditions to maximise power output and reduce energy unit costs. All power generating equipment is housed in the parallel floating pontoons, providing protection from the harsh corrosive sea conditions. \n\nThe Energen device consists of a series of semi-submerged cylindrical pivoting torque tubes connected to two large cylindrical pontoons. The wave-induced movement of these torque tubes is resisted by a hydraulic system which pumps high pressure oil through hydraulic motors. The hydraulic motors drive electrical generators to produce electricity. \n\nA 50th scale model has been tested at the Council for Scientific and Industrial Research in Stellenbosch and using actual wave data off the South African coast it is estimated that a single device will produce 1.4 MW of power, or 979 GW hours of electricity per annum.\n\n\n"}
{"id": "4343460", "url": "https://en.wikipedia.org/wiki?curid=4343460", "title": "Fenton, Murray and Jackson", "text": "Fenton, Murray and Jackson\n\nFenton, Murray and Jackson was an engineering company at the Round Foundry off Water Lane in Holbeck, Leeds, West Yorkshire, England.\n\nFenton Murray and Wood was founded in the 1790s by ironfounder Matthew Murray and textile machine engineer David Wood to build machine tools (mainly for the textile industry) and stationary steam engines. The company was capitalised by colliery owner James Fenton (1754–1834) as the main financier, and millwright William Lister, a sleeping partner.\n\nThe partnership was approached to design a locomotive that would exploit the rack-and-pinion patent granted to Blenkinsop in 1811. Their 1812 product, \"Salamanca\" was successful, and a total of six engines were built before Murray's death in 1826.\n\nAfter Wood's death in 1820, the company became Fenton, Murray and Jackson.\n\nIn 1824 the company supplied a 60hp beam engine to the commissioners of Deeping Fen as one of two erected at Pode Hole. The other engine was supplied by the Butterley Company as were the scoop wheels for both. Although the Butterley engine was purchased outright (for £3,300), it appears that the Fenton and Murray engine was not. The accounts for 1825 showed a payment of only £127/6/- for the Fenton engine. It was not uncommon for beam engines to be leased, purchased 'on terms', or paid for in other novel ways such as a share of the earnings. The engine was named \"Kesteven\" and worked until 1925.\n\nFrom 1831, work began building engines to Robert Stephenson's designs, both 2-2-0 \"Planets\" and 2-2-2 \"Patentees\", many of them under subcontract. Many were exported, and twenty of Daniel Gooch's \"Firefly\" class for the Great Western Railway. By 1840, they were turning out up to twenty engines a year.\n\nThe company's name appears on cast iron bollards still in situ at Victoria Lock (built 1843) on the River Shannon in Ireland as Fenton, Murray and Jackson Engineers of Leeds.\n\nHowever, by 1843, the boom was over and the company closed down.\n\n"}
{"id": "26627449", "url": "https://en.wikipedia.org/wiki?curid=26627449", "title": "Hopetech", "text": "Hopetech\n\nHopetech Sdn Bhd is a defunct systems integrator and solutions provider for automated revenue collection, road telematics and electronic payment systems in the transportation and electronic purse sectors. Hopetech core business was providing solutions for toll and fare ticketing system adopting the revenue collection solutions - either for cash, stored value tickets or electronic method of payments (using smartcards or transponders). They have jointly won the KTMB-Automatic Fare Collection contract from Ministry of Transport Malaysia in year 2012 but failed to handover and commission the project.\n\n\n\n"}
{"id": "50058104", "url": "https://en.wikipedia.org/wiki?curid=50058104", "title": "ICE (FPGA)", "text": "ICE (FPGA)\n\niCE is the brand name used for a family of low-power FPGAs produced by Lattice Semiconductor. Parts in the family are marketed with the \"world's smallest FPGA\" tagline, and are intended for use in portable and battery-powered devices (such as mobile phones), where they would be used to offload tasks from the device's main processor or SoC. By doing so, the main processor and its peripherals can enter a low-power state or be powered off entirely, potentially increasing battery life.\n\nLattice received the iCE brand as part of its 2011 acquisition of SiliconBlue Technologies.\n\nThe iCE brand was originally used by SiliconBlue Technologies Corporation, a former Santa Clara, California-based fabless designer of integrated circuits. SiliconBlue was a start-up founded in 2005 by former employees of Actel, AMD, Lattice, Monolithic Memories, and Xilinx. Most notable among the founders was John Birkner, one of the inventors of programmable array logic.\n\nIn 2006, SiliconBlue was funded with $16 million in \"Series A\" capital, and in June 2008 announced the iCE65 L series of devices. The devices were to be fabricated on TSMC's 65nm CMOS process node, which SiliconBlue claimed would provide reduced power consumption compared to contemporary FPGAs from other manufacturers. In October 2008, SiliconBlue raised a further $24 million in Series B capital.\n\nIn 2009, the first iCE65 L devices were shipped to customers. SiliconBlue also registered \"SiliconBlue Technologies (Hong Kong) Limited\", which remains as a subsidiary of Lattice Semiconductor.\n\nIn 2010, the lowest-end of the iCE65 P devices was announced by SiliconBlue. The devices were claimed to be as much as 30% faster than iCE65 L devices while maintaining similar power consumption. In the June of the same year, SiliconBlue closed a $15 million Series C funding round.\n\nIn April 2011, SiliconBlue announced that it was to release new product families, code-named \"Los Angeles\" and \"San Francisco,\" using a TSMC 40nm process node. The production of devices on a 40nm process node was further confirmed in June 2011, when SiliconBlue received $18 million in Series D funding to bring 40nm devices to market. The iCE40 product family was officially released in July 2011.\n\nOn 9 December 2011, SiliconBlue Technologies was acquired by Lattice Semiconductor in a $63.2 million cash buyout. As part of this buyout, Lattice received the iCE brand, manufacturing capabilities with TSMC, and a licence for various patents from Kilopass Technologies, including for its XPM one-time programmable (OTP) memory technology.\n\nIn April 2012, Lattice announced that the iCE65 families would be discontinued. The iCE40 LP and HX device families entered volume production the following month. The iCE40 LP family won the Elektra \"Digital Semiconductor Product of the Year\" award for 2012.\n\nIn July 2014, the iCE40 Ultra family was announced.\n\nIn February 2015, Lattice launched the iCE40 UltraLite device family. The devices in this family are claimed to operate at 30% less power than those of unspecified competitors, and are claimed to be the world's smallest FPGAs, being available in 1.4×1.4mm packages. The family won the 2015 Elektra \"Digital Semiconductor Product of the Year\" award.\n\nIn December 2016, Lattice launched the iCE40 UltraPlus device family. UltraPlus devices provide additional memory, additional processing elements, and support for newer interfaces and protocols compared to previous iCE40 Ultra/UltraLite devices.\n\niCE65 and iCE40 devices are constructed as an array of \"programmable logic blocks\" (PLBs), where a PLB is a block of eight logic cells. Each logic cell consists of a four-input lookup table (sometimes called a 4-LUT or LUT4) with the output connected to a D flip-flop (a 1-bit storage element). Within a PLB, each logic cell is connected to the following and preceding cell by carry logic, intended to improve the performance of constructs such as adders and subtractors. Interspersed with PLBs are blocks of RAM, each four kilobits in size. The number of RAM blocks varies depending on the device.\n\nCompared to LUT6-based architectures (such as Xilinx 7-series devices and Altera Stratix devices), a LUT4-based device is unable to implement as-complex logic functions with the same number of logic cells. For example, a logic function with seven inputs could be implemented in eight LUT4s or two LUT6s.\n\niCE devices use volatile SRAM to store configuration data. As a result, the data must be loaded onto the device each time power is lost. All iCE devices support loading configuration data from a programmer, from an external flash memory chip, or, with the exception of iCE40 \"LM\" devices, from a so-called NVCM, or non-volatile configuration memory. The NVCM is a one-time-programmable (OTP) memory integrated into the FPGA to negate the need for an external memory chip. Lattice claims that using the NVCM can improve design security by making reverse engineering more difficult.\n\nThe I/O pins on iCE devices are separated into up to four banks. On some devices each bank has its own power-supply pin (labelled V), allowing the logic-high voltage level of the I/O bank to be adjusted. Configurable I/O voltage levels are used by iCE devices to allow support for multiple interface standards with voltage levels between 1.8V and 3.3V, such as LVDS. iCE65 devices also listed being able to support SSTL through this method.\n\niCE FPGAs, as with most FPGAs and CPLDs, are typically designed for using a hardware description language (or HDL), which describes an electronic circuit. Lattice Diamond, the IDE provided by Lattice for developing on their FPGAs, supports the VHDL and Verilog languages, as well as the EDIF format.\n\nThe details of a specific FPGA's bitstream format (which defines how the internal elements of the FPGA are connected and interact with each other) are not usually published by FPGA vendors. This means that, generally, an engineer creating a design for an FPGA must use the tools provided by the FPGA's manufacturer. \n\nIn December 2015, at 32C3, a toolchain consisting of \"Yosys\" (Verilog synthesis frontend), \"Arachne-pnr\" (place and route and bitstream generation), and \"icepack\" (plain text-to-binary bitstream conversion) tools was presented by Clifford Wolf, one of the two developers (along with Mathias Lasser) of the toolchain. The toolchain is notable for being one of, if not the only, fully open-source toolchains for FPGA development. At the same December 2015 presentation, Wolf also demonstrated a RISC-V SoC design built using the open-source toolchain and running on an iCE40 \"HX8K\" device. As of April 2016, the toolchain supports iCE40 \"LP1K\", \"LP4K\", \"LP8K\", and \"HX\" devices.\n\nThe \"iCE65\" name was used by SiliconBlue Technologies for the devices it designed for a 65nm process node. Following the acquisition of SiliconBlue in 2011, the name was used by Lattice Semiconductor until the family was discontinued in April 2012.\n\nThe iCE65 L series of devices was intended for low-power applications and handheld devices. The series was first announced in mid 2008, and first shipped to volume customers in early 2009.\n\nInformation about a larger device in the series, the \"iCE65L16\", was listed on the SiliconBlue website in 2010, but no mention is made in a 2012 revision of the L-series datasheet. It is unclear whether the device was ever produced commercially.\n\nThe iCE65 P-series devices were marketed as a higher-performance version of the \"L-series\" devices, intended for use in display, memory, and SERDES applications, and were announced in early 2010. Three devices were listed as being part of the series but only one device, the lowest-end \"iCE65P04\", was fully specified. The latest datasheet for the family, published in 2011, lists the other two parts but does not give specifications. Whether these other two devices were ever commercially produced is unclear.\n\nLattice uses the \"iCE40\" name for its iCE-branded devices produced on a 40nm process node. The company has also used the codename \"Los Angeles\" in press releases. The iCE40 family was launched in July 2011 with iCE40 LP and HX parts, and was updated in July 2014 with the iCE40 Ultra parts, in February 2015 with the iCE40 UltraLite parts, and in December 2016 with the iCE40 UltraPlus parts.\n\nThe iCE40 Ultra, UltraLite, and UltraPlus devices are intended for applications with especially low limits on available space and power, such as in wearable technology and smart watches. They are offered in chip-scale, BGA, and QFN packages, with dimensions from 1.4×1.4mm to 7×7mm. All devices in family integrate one or two I²C hard cores, with \"Ultra\" and \"UltraPlus\" devices also including hard SPI bus cores and DSP blocks. \"UltraLite\" devices are claimed to operate at half the static current draw of \"Ultra\" devices (35μA compared to 71μA). Most devices in the family also include a PWM controller, intended to be used to drive IR or RGB LEDs.\n\nLattice launched the \"Ultra\" family in mid 2014, and the \"UltraLite\" family in early 2015. In 2015, the \"UltraLite\" family won the Elektra \"Digital Semiconductor Product of the Year\" award.\n\nIn September 2016, the Apple iPhone 7 was released and made use of an iCE5LP4K device.\n\nIn December 2016, Lattice launched the \"UltraPlus\" family intended for distributed processing and so-called \"mobile heterogeneous computing.\" The devices include a 1Mbit (4×256kbit) single-port RAM (compare with dual-ported RAM), additional DSP processing elements, and support for additional interfaces, such as MIPI I3C, D-PHY, and Virtual GPIO.\n\nThe iCE40 LP (low power) and LM (low power with hard IP) parts are intended for use in battery-powered devices as hardware accelerators and I/O port expanders, and for use in the same applications as \"iCE40 Ultra\" and \"UltraLite\" parts. Compared to the \"Ultra\" parts, \"LP\" and \"LM\" parts are available in a wider range of footprints, offer a greater number of resources (I/O pins, embedded RAM, and logic elements), but consume more power.\n\n\"LP\" devices differ from the \"Ultra\" devices in that they do not include hard IP cores. Instead, any interface logic must be implemented in the FPGA fabric. This is generally less preferable, as so-called \"soft cores\" are less power-efficient than hard cores, and often are unable to operate at the same frequencies. A soft core also reduces the number of logic cells available to the application. \"LM\" devices integrate two I²C and two SPI hard cores, as well as two strobe generators. Most \"LP\" and \"LM\" devices integrate one or two phase-locked loops.\n\nThe families were launched in mid 2011 and entered volume production in mid 2012. They won the Elektra \"Digital Semiconductor Product of the Year\" award for 2012. In 2015, it was announced that ZTE would use \"LM\" devices to provide sensor hub and infrared remote control functionality in its Star 2 smartphone.\n\nThe iCE40 HX devices are intended for high-performance applications. Compared to \"iCE40 LP\" and \"Ultra\" devices, they offer lower maximum propagation delay (7.30ns versus 9.00–9.36ns), and more I/O pins. \"HX\" series devices consume significantly more static power and are available only in significantly larger footprints compared to \"Ultra\" and \"LP\" parts (7×7mm to 2×2cm). Similarly to the \"LP\" devices, \"HX\" parts do not provide hard IP cores, but do provide one or two phase-locked loops. Unlike other \"iCE40\" devices, the \"HX\" parts are also available in QFP footprints.\n\nThe \"HX\" parts were launched in mid 2011 alongside the \"LP\" parts, and entered volume production in mid 2012.\n\n"}
{"id": "41401019", "url": "https://en.wikipedia.org/wiki?curid=41401019", "title": "IEC 62700", "text": "IEC 62700\n\nIEC Technical Specification 62700: DC Power supply for notebook computer is an IEC specification of a common standard for external laptop computer AC adapters. Laptops and AC adapters following this standard will have interchangeable power supplies, which will enable easy reuse of used power supplies (thereby reducing electronic waste) and make buying a new compatible power supply for a laptop simpler.\n\nThe specification was published on .\n\nDespite being an industry open organization with open participation, the standard has been criticized by some for not being openly available for review. Some alternatives include:\n\n\n"}
{"id": "27205653", "url": "https://en.wikipedia.org/wiki?curid=27205653", "title": "ISO-TimeML", "text": "ISO-TimeML\n\nISO 24617-1:2009, ISO-TimeML is the International Organization for Standardization ISO/TC37 standard for time and event markup and annotation. The scope is standardization of principles and methods relating to the annotation of temporal events in the contexts of electronic documentation and language.\n\nThe goals of ISO-TimeML are to provide a common model for the creation and use of temporal and event annotation, as a means of managing time-related data within documents, and to enable later categorization and data extraction with use of this meta-data.\n\nISO-TimeML was presented to the ISO for consideration as a standard in August 2007. In this presentation, the preliminaries of ISO-TimeML were outlined, and potential applications were examined. In the following year, revisions were made to ISO-TimeML as the standard transitioned from a New Project (NP) to a Working Project (WP). The ISO-TimeML voting period began in October 2008 and was approved as an international standard by March 2009.\n\nThe ISO/TC37 standards are currently elaborated as high level specifications and deal with word segmentation (ISO 24614), annotations (ISO 24611 a.k.a. MAF, ISO 24612 a.k.a. LAF, ISO 24615 a.k.a. SynAF, and ISO 24617-1 a.k.a. SemAF/Time), feature structures (ISO 24610), multimedia containers (ISO 24616 a.k.a. MLIF), and lexicons (ISO 24613 a.k.a. LMF). These standards are based on low level specifications dedicated to constants, namely data categories (revision of ISO 12620), language codes (ISO 639), scripts codes (ISO 15924), country codes (ISO 3166) and Unicode (ISO 10646).\n\nThe two level organization forms a coherent family of standards with the following common and simple rules:\n\nJoint work between ISO/TC 37/SC 4/WG 2 (TDG 3) and the TimeML\nWorking Group that was agreed on at the TDG 3 and LIRICS Working Group\nMeeting, USC/ISI, Marina del Rey, CA, U.S.A., 2006-04-20/21/22.\n\nProposed Project Leaders and Editors:\n\n\n"}
{"id": "632583", "url": "https://en.wikipedia.org/wiki?curid=632583", "title": "Igor Aleksander", "text": "Igor Aleksander\n\nIgor Aleksander FREng (born 26 January 1937) is an emeritus professor of Neural Systems Engineering in the Department of Electrical and Electronic Engineering at Imperial College London. He worked in artificial intelligence and neural networks and designed the world's first neural pattern recognition system in the 1980s.\n\nAleksander was educated in Italy and graduated from the University of the Witwatersrand in South Africa, arriving in the UK in the late 1950s, intending to become a research student under Colin Cherry. Instead he found work with Standard Telephones and Cables, later joining Queen Mary College where he gained a PhD, subsequently becoming a lecturer there in 1961. He moved to the University of Kent in 1968 as a reader in Electronics and then to Brunel University as professor in 1974. In 1984 he became professor at Imperial College London as professor of the Management of Information Technology. He was Head of Electrical Engineering and Gabor Professor of Neural Systems Engineering at Imperial College from 1988 to his retirement in 2002. He was elected Fellow of the Royal Academy of Engineering (1988), and he served as Pro-rector of External Relations at Imperial College (1997). In 2005 he presented the Bernard Price Memorial Lecture.\n\nHis work centred on the modelling capability of artificial neural networks. He devised neuromodels of the visual system in primates, visuo-verbal system in humans, the effect of anaesthetics on awareness, and artificial consciousness. He inspired the engineering design of one of the first stand alone neural pattern recognition systems, the WISARD (anonym for Wilkie Stonham Aleksander’s Recognition Device) which was named after the co-inventors Bruce Wilkie, John Stonham and Igor Aleksander. This Brunel University prototype WISARD was commercially developed and marketed by Computer Recognition Systems, Wokingham, under the trade name of ’CRS WISARD’ in 1984. After this, the further developments of this system do not appear to have been documented. A popular link for WISARD is that of “the wisard pattern recognition machine” at the Winton Gallery, British Science Museum.\n\nAleksander received an honorary degree in Computer Engineering from University of Palermo in Jul 2011.\n\n\n\n\n"}
{"id": "54396890", "url": "https://en.wikipedia.org/wiki?curid=54396890", "title": "Intermodalics", "text": "Intermodalics\n\nIntermodalics is a software engineering and robotics company headquartered in Leuven, Belgium.\n\nIt has been recognized as one of the most promising innovative companies of Belgium.\n\nIntermodalics was co-founded in Leuven, Belgium in 2011 by Dr. Peter Soetens and Dr. Ruben Smits as CEO and CTO respectively.\n\nThe company find its roots in the Robotics Research Group of KU Leuven, focusing on highly innovative projects.\n\nIt is very active in the Open Source community, most notably as developer and maintainer of the Open Robot Control Software project Orocos, a Free Software toolkit for realtime robot arm and machine tool control.\n\nDuring the summer of 2014 Intermodalics co-developed the first Neopter - a commercial drone swarm for light shows - for the attraction park Le Puy du Fou in France.\n\nIn January, 2015 the company announced a partnership with Google related to the Google Project Tango project.\n\nThat same year, the company launched Pick-it., a 3D robot vision system, mostly used for material handling solutions such as robot bin-picking.\n\nIn May, 2015 Peter Soetens was named one of the 50 Belgian inspiring innovators by the Belgian financial newspaper De Tijd.\n"}
{"id": "22847485", "url": "https://en.wikipedia.org/wiki?curid=22847485", "title": "International Nuclear Library Network", "text": "International Nuclear Library Network\n\nThe International Nuclear Library Network (INLN), coordinated by the IAEA Library, i.e. the Library of the International Atomic Energy Agency, is a consortium of nuclear libraries and knowledge centres. The Network seeks to promote the exchange of nuclear information and to strengthen strategic partnerships amongst members. The underlying strategy is twofold: first, whenever a new partner joins the network, the shared information base is enlarged; and second, the larger the information base becomes, the more attractive the network is. The 37 nuclear libraries coming from 27 countries that participate in the coalition have managed to enhance their information pool and extend their services to cover nuclear information and knowledge needs on a global scale.\n\nThe International Nuclear Library Network was founded in 2005 by the IAEA Library and the Atomic Energy of Canada Limited (AECL) Library. In its initial years, it counted a total of five members (in addition to the initiators, the National Atomic Energy Commission of Argentina, the Turkish Atomic Energy Agency and the Institute of Nuclear Physics of Uzbekistan Academy of Science. In 2006, the Australian Nuclear Science and Technology Organisation (ANSTO) also joined the network. In 2007, the INLN welcomed four new members: the China Nuclear Information Centre, the Nigerian Nuclear Regulatory Authority, the Obninsk State Technical University for Nuclear Power Engineering in the Russian Federation, and the Russian Association of Nuclear Science and Education (RANSE), thus increasing the membership to ten participants. In 2008, at the meeting of INLN Members and prospective members, during the 34th International Nuclear Information System (INIS) Liaison Officers Meeting in Vienna, a large number of nuclear libraries from around the world expressed their interest and subsequently joined the INLN: the Belarus INIS Center, Chair of Ecological Information Systems, the Nuclear Research Institute Rez plc of Czech Republic, the Egyptian Atomic Energy Authority (EAEA), the Ghana Atomic Energy Commission Library, the Bhabha Atomic Research Centre (BARC) of India, the Radiological Protection Institute of Ireland, the Japan Atomic Energy Agency (JAEA), the Instituto Nacional de Investigaciones Nucleares (ININ), Centro de Información y Documentación Nuclear (CIDN) of Mexico, the Centre National de l' Energie des Sciences et des Techniques Nucléaires (CNESTEN) of Morocco, the Korea Atomic Energy Research Institute (KAERI) at the University of Science and Technology (South Korea), the Vinca Nuclear Institute of Serbia, the Centre National des Sciences et Technologies Nucléaires (CNSTN) of Tunisia. In November of the same year, the Library Network of the Brazilian Nuclear Energy Commission (Comissão Nacional de Energia Nuclear – CNEN), consisting of seven nuclear libraries, became an INLN official member. In January 2009, the Commissariat à l'Énergie Atomique (CEA) - Centre de Saclay - Centre de Ressources Documentaires joined the Network, while in 2010, the National Atomic Energy Commission, the Norwegian Radiation Protection Authority, and the Pakistan Institute of Nuclear Science & Technology decided to become INLN members. The Nuclear Energy Regulatory Agency of Indonesia - Badan Pengawas Tenaga Nuklir and the New Zealand Institute of Environmental Science and Research – National Radiation Laboratory are the newest members, raising the total number of Nuclear Libraries/Information Centres participating in the INLN to 37.\n\nThe International Nuclear Library Network (INLN) represents a coalition of nuclear libraries. It is designed to be an Internet-based library and information system. Its information portal is hosted by the IAEA Library and it is supported by nuclear libraries and knowledge centres worldwide; it is developed and maintained in a decentralized manner, in accordance with agreed standards. It links not only bibliographic information to full-text and audio-visual information, but also librarians, library clients and researchers to the appropriate information. Moreover, the network links nuclear information workers to each other, forming a strong community of practice, sharing knowledge, best practices and lessons learned. Among the services that INLN members provide to each other are: research services, interlibrary loans, document delivery, current awareness services and any other services participating libraries wish to provide other libraries. The website offers a landing page hosting access to members' online catalogues. \nThe success of INLN is based on its democratic structure: all participating libraries are equal partners, valued according to what they bring as practitioners in terms of information and their willingness to share, rather than on predetermined hierarchical or status levels. The actual work of the network focuses on concrete actions, bringing the right information in the right format at the right time to the right place.\n\nMembership in the INLN is open to States who are Members of the IAEA and to UN organizations. Other interested international, intergovernmental and non-governmental organizations may also join, subject to agreement of the INLN Members. Each INLN Member has the same rights and responsibilities.\nTo apply for INLN membership, an official request is to be made by the appropriate library to the IAEA Library, which is the coordinating library of the Network. If the requesting library is located in an IAEA Member State or belongs to an UN organization, the IAEA Library as the coordinating library will make modifications to the INLN website and inform the applicant and other participants about a new member. \nFor other prospective participants, the coordinating library will seek approval from all existing member libraries/knowledge centres. The Coordinator, after receiving the approval from current members, will inform the applicant and the members of the outcome, and proceed with updating the INLN website.\n\n\n"}
{"id": "43127447", "url": "https://en.wikipedia.org/wiki?curid=43127447", "title": "International Safety Equipment Association", "text": "International Safety Equipment Association\n\nThe International Safety Equipment Association (ISEA) is a trade association of manufacturers of personal protective equipment and other safety equipment, with its offices in Arlington, Virginia. More than 100 companies are members. The ISEA is the secretariat organization for several American National Standards Institute technical standards for products such as high visibility clothing, eye protection, hard hats, chemical and dust protection, hand protection, and others; formerly this role was filled by the American Society of Safety Engineers.\n\nThe organization was founded in 1933 as the Industrial Safety Equipment Association. It adopted its current name in 2000. Since 2002 it has had a Web site accessible to the public and member organizations. In addition to technical standards development and market research, the organization also provides representation of the safety products industry to the American government.\n\n"}
{"id": "32311843", "url": "https://en.wikipedia.org/wiki?curid=32311843", "title": "Irish Seed Savers Association", "text": "Irish Seed Savers Association\n\nIrish Seed Savers Association is an Irish non-governmental organisation founded in 1991. The Irish Seed Savers Association maintains a seed bank with over 600 non-commercially available varieties of seed.\n\nThe Irish Seed Savers Association aims to:\n\n\nWith its projects the Irish Seed Savers Association found and saved over 140 different types of Irish apple trees and 25 native Irish grains. The Irish Seed Savers Association has a ten-acre site with purpose built facilities, native woodland and an apple orchard/nursery. Irish Seed Savers Association is a network partner of the European SAVE Foundation. Irish Seed Savers are also a member of the Irish Environmental Network.\n\n\n"}
{"id": "30901177", "url": "https://en.wikipedia.org/wiki?curid=30901177", "title": "Jockey box", "text": "Jockey box\n\nA jockey box is an insulated container containing ice and water, as well as a long coil of hollow tubing. The device is used to cool beverages being served on tap in temporary locations.\n\nOne end of the coiled tube in the box is fitted to the external supply of the beverage to be served (often moved from the supply container by gas pressure, as in the case of beer in kegs, and the other end is attached to taps for serving the beverage, which are often integrated into the box itself. Filling the jockey box with ice and water cools the coiled tubing, and when the beverage flows through the tube, the beverage is cooled to a temperature just above freezing, even if it was at room temperature before it entered the box. This allows cold drinks to be served on tap in temporary and outdoor venues.\n\nApart from the coil and taps, the jockey box resembles a cool box. Some jockey boxes force the liquid through a solid cold plate rather than a coil of tubing (see Alternative Portable Cooler Dispensers below).\n\nA glove compartment is also sometimes referred to as a \"jockey box,\" especially in the U.S. Upper Rocky Mountain states.\n\nIn the coil cooler, a stainless steel coil is covered with water and ice, in which the liquid is made run through. At the contact with the cold coil, the beverage becomes cold. \nOn the flip side, the cold plate works making the beverage run across an ice covered aluminum cold plate. When the plate makes contact with the ice, the plate become cold provoking the liquid that runs over it becomes cold too.\n\nA portable ice maker and water dispenser is a device that produces ice, and release chill water by a small in-house generator. Depending on the size, some of them require a water line connection, but most of them only needs to have water deposited in its container.\n"}
{"id": "32790043", "url": "https://en.wikipedia.org/wiki?curid=32790043", "title": "Kichō", "text": "Kichō\n\nA is a portable multi-paneled silk partition supported by a two-rod T-pole. It came into use in aristocratic households during and following the Heian period in Japan when it became a standard piece of furniture. They are similar in appearance to \"noren\".\n\nThe \"kichō\" is often placed just on the inside of bamboo blinds, forming a portable double privacy barrier to the outside of the house. They are also used as portable room dividers inside the house. Today, they are most often used as decorations or to hide boxes or other unsightly messes in a home.\n\nIn former times, they would often be used to hide noble women from public eyes when they visited shrines or temples, and to provide additional privacy for the women at home. Smaller versions called were carried by the female attendants of a noble woman in order to hide her from public view while she traveled.\n\n\n"}
{"id": "51214355", "url": "https://en.wikipedia.org/wiki?curid=51214355", "title": "Lauren States", "text": "Lauren States\n\nLauren States (born 1956) is a former Chief of Technology and then Vice President of Strategy and Transformation for IBM's Software Group Sales Division.\n\nStates was born in 1956 and she originally intended to be an actuary. However she learnt to program in COBOL and discovered a gift for technology. She has a degree in Economics from the Wharton School of the University of Pennsylvania.\n\nShe joined IBM as a trainee and worked her way up as an engineer resisting the invitations to become a manager.\n\nStates was inducted into the Women In Technology International Hall of Fame in 2014.\n"}
{"id": "42416555", "url": "https://en.wikipedia.org/wiki?curid=42416555", "title": "List of articles associated with nuclear issues in California", "text": "List of articles associated with nuclear issues in California\n\nThis is a list of Wikipedia articles that are relevant to the topic of nuclear power and nuclear weapons history in the US state of California:\n\n"}
{"id": "35848235", "url": "https://en.wikipedia.org/wiki?curid=35848235", "title": "List of industrial centres in India", "text": "List of industrial centres in India\n\nThe major industrial centres in India are listed below:\n"}
{"id": "26248738", "url": "https://en.wikipedia.org/wiki?curid=26248738", "title": "List of megaprojects", "text": "List of megaprojects\n\nThis is a list of megaprojects. \"Megaprojects are temporary endeavours (i.e. projects) characterized by: large investment commitment, vast complexity (especially in organizational terms), and long-lasting impact on the economy, the environment, and society\". The number of such projects is so large that the list may never be fully completed.\n\nMegaprojects may be defined as:\n\nMegaprojects require care in the project development process to reduce any possible optimism bias and strategic misrepresentation. Examples of megaprojects include bridges, tunnels, highways, railways, airports, seaports, power plants, dams, wastewater projects, Special Economic Zones (SEZ), oil and natural gas extraction projects, public buildings, information technology systems, aerospace projects, and weapons systems.\n\nThis list identifies a wide variety of examples of major historic and contemporary projects that meet one or both megaproject criteria identified above.\n\n\nWhile most megaprojects are planned and undertaken with careful forethought, some are undertaken out of necessity after a natural disaster occurs. There have also been a few man-made disasters.\n\n\n\n\n\nEvery Olympic Games and FIFA World Cup in the latter part of the 20th century and entering into the 21st century has cost more than $1 billion in arenas, hotels etc., usually several billions.\n\n\nGround transportation systems like roads, tunnels, bridges, terminals, railways, and mass transit systems are often megaprojects. Numerous large airports and terminals used for airborne passenger and cargo transportation are built as megaprojects.\n\nInfrastructure systems that include electricity, sewer, telecommunications, and transportation. Building cities that include Skyscrapers, housing, etc. requires concrete, steel, glass, and most especially that workers in all the trades to undertaker the massive scale of these megaprojects.\n\nPorts, waterways, canals, and locks for Passenger Ships and Cargo Ships carrying passengers and cargo from country to country and islands nations are built as megaprojects\n\n\n"}
{"id": "44199579", "url": "https://en.wikipedia.org/wiki?curid=44199579", "title": "Ministry of Popular Power for Communication and Information", "text": "Ministry of Popular Power for Communication and Information\n\nThe Ministry of Popular Power for Communication and Information (Minci) is a public ministry of the Government of Venezuela dedicated to communication, informing the Venezuelan public and promoting the Venezuelan government.\n\n\n\nAffiliated media organizations include:\nThe Institute Press and Society (Ipys) has criticized freedom of information and expression in Venezuela, though the Ministry of Popular Power for Communication and Information responded to these allegations by criticizing the Ipys report due to its alleged funding from United States organizations, such as the National Endowment for Democracy.\n\n"}
{"id": "31638625", "url": "https://en.wikipedia.org/wiki?curid=31638625", "title": "Ministry of Science and Technology (Thailand)", "text": "Ministry of Science and Technology (Thailand)\n\nThe Ministry of Science and Technology (; ; Abrv: MOST), is a Thai government body responsible for the oversight of science and technology in Thailand.\n\nThe Ministry of Science, Technology and Environment (MoSTE) was a Thai government ministry from 1992 until 2002. With the coming into effect of the Restructuring of Government Agencies Act of 2002, the ministry was reorganized into the following separate ministries:\n\n\nOn 24 October 2018, the Thai cabinet approved the creation of a Ministry of Higher Education and Research & Development. The new ministry will merge the Science and Technology Ministry, the Office of the Higher Education Commission (OHEC), the National Research Council of Thailand, and the Thailand Research Fund. The current Minister of Science and Technology, Suvit Maesincee, will head the new ministry. Its mission will be \"...to develop high technology, enhancing the efficiency of the R&D and support Thailand 4.0 policy, as well as human resource development.\" The government has allotted a 97 billion baht budget to the new ministry in its first year, FY2019.\n\nThe ministry's budget for FY2019 is 14,885.4 million baht.\n\n\n\n"}
{"id": "26004545", "url": "https://en.wikipedia.org/wiki?curid=26004545", "title": "Motorola A910", "text": "Motorola A910\n\nThe Motorola A910 is a clamshell mobile phone from Motorola, which uses MontaVista Linux as the operating system.\n\nMotorola started selling this phone in the first quarter of the year 2006. Utilizing a balanced Linux-Java operating system and Wi-Fi connectivity, the Motorola A910 surpasses its predecessors with user-friendly features, everything from text messaging to email management. It is also the only clamshell phone from Motorola with Wi-Fi, as well as the only non-touchscreen Motorola with Wi-Fi in Europe.\n\nThe phone is supplied with a number of applications including a POP and IMAP email client, Opera web browser, calendar and a viewer for PDF and Microsoft Office files. Calendar and address book can be synchronized with a Microsoft Exchange or SyncML server. The phone has a 1.3 megapixel camera with Self Portrait Viewfinder External Display and photo lighting, recording still and video images. RealPlayer is included to play sound audio files and streamed audio and video. The phone has 48 megabytes of internal flash memory for storing user data and a slot for a microSD card, which supports additional 2 GB of storage. Both Bluetooth and USB are provided for communication with another computer. Character entry is made by the keypad interface.\n\nThis phone is popular with Linux enthusiasts. It is able to establish an Ethernet connection between the phone and another computer over USB, Bluetooth or Wi-Fi. One can then telnet to the phone and be presented with a bash prompt. From the prompt one can, for example, mount a NFS drive(s) on the phone. The underlying operating system, Motorola EZX is Linux based, its kernel is open source. With the source code hosted on \"opensource.motorola.com\", it is possible to recompile and replace the kernel for this operating system. However Motorola did not publish a software development kit for native applications. Instead, they are expecting third party programs to be written in Java ME. The OpenEZX website is dedicated to providing free opensource software for this phone and others using the same OS.\n\n\n"}
{"id": "15995191", "url": "https://en.wikipedia.org/wiki?curid=15995191", "title": "Nanoradio", "text": "Nanoradio\n\nA nanoradio (also called carbon nanotube radio) is a nanotechnology acting as a radio transmitter and receiver by using carbon nanotubes. One of the first nanoradios was constructed in 2007 by researchers under Alex Zettl at the University of California, Berkeley where they successfully transmitted an audio signal. Due to the small size, nanoradios can have several possible applications such as radio function in the bloodstream.\n\nThe first observation of a nanoradio can be accredited to a Japanese physicist Sumio Iijima in 1991 who saw a \"a luminous discharge of electricity\" coming from a carbon nanotube on a graphite electrode. On October 31, 2007, a team of researchers under Alex Zettl at the University of California, Berkeley created one of the first nanoradios. Their experiment consisted of placing a multilayered nanotube placed on a silicon electrode and connecting it to a counter electrode through a wire and a DC battery. Both the electrode and nanotube were also put in a vacuum of about 10 Torr. They then placed the apparatus into a high-resolution transmission electron microscope to document the movement of the nanotube. They observed the nanoradio vibrating and transmitted a song called \"Layla\" by Eric Clapton. After some minor adjustments, the team was able to transmit and receive signals from a couple meters across the laboratory; however, the initial audio receptions from the radio were scratchy which Zettl believed was due to the lack of a better vacuum.\n\nThe small size, roughly 10 nanometers wide and hundreds of nanometers long, and composition of nanoradios provide several distinct properties. The small size of nanoradios enables electrons to pass through without much friction, making nanoradios efficient conductors. Nanoradios can also come in different sizes; they can be double-walled, tripled-walled and multi-walled. Aside from the different sizes, nanoradios can also take different shapes such as bent, straight or toroidal. Common among all nanoradios is how relatively strong they are. The resistance can be attributed to the strength of the bonds between carbon atoms.\n\nThe fundamental parts of a radio are the antenna, tuner, demodulator and amplifier. Carbon nanotubes are special in that they can function as these parts without the need of extra circuitry.\n\nThe nanoradio is small enough for electromagnetic signals to mechanically vibrate the nanoradio. The nanoradio essentially acts as an antenna by vibrating with the same frequency as the signal from incoming electromagnetic waves; this is in contrast with traditional radio antennas, which are generally stationary. The nanotube can vibrate in high frequencies, from \"thousands to millions of times per second.\" \n\nThe nanoradio can also function as a tuner by extending or reducing the length of the nanotube; doing so changes the resonance frequency at which it vibrates, enabling the radio to tune into specific frequencies. The length of the nanotube can be extended by pulling the tip with a positive electrode and can be shortened by removing atoms off the tip. Consequently, changing the length is permanent and can't be reversed; however, the method of varying the electric field can also affect the frequency that the nanoradio responds without being permanent.\n\nAs a benefit of the microscopic size and needle-like shape, the nanoradio functions naturally as an amplifier. The nanoradio exhibits field emission, in which a small voltage emits a flow of electrons; due to this, a small electromagnetic wave would produce a large flow of electrons, amplifying the signal.\n\nDemodulation is essentially the separation of the information signal from the carrier wave. When the nanoradio vibrates in sync with the carrier wave, the nanoradio responds only to the information signal and ignores the carrier wave; and so, the nanoradio can act as a demodulator without the need of circuitry.\n\nCurrently, chemotherapy uses chemicals that harm not only cancerous cells, but also healthy ones since they are put into the blood stream. Nanoradios can be used to prevent damage to healthy cells by remotely communicating with the radio to release drugs and specifically target cancerous cells. Nanoradios can also be injected into individual cells to release certain chemicals, enabling repair of specific cells. Nanoradios can also be used to monitor insulin levels of diabetes patients and use that information to release a drug or chemical.\n\nThe implanting of nanoradios in the body are currently not feasible due to power dissipation. The nanoradio radiates about 4.5 x 10 W of electromagnetic power; however, much of this power would be loss when passing through the body. The amount of energy input can be increased, but would generate too much heat in the body, posing a safety risk. Other issues include the difficulty of constructing a nanoradio due to its nanoscale size, requiring quantum models and precision to manufacture.\n"}
{"id": "1521987", "url": "https://en.wikipedia.org/wiki?curid=1521987", "title": "Online hotel reservations", "text": "Online hotel reservations\n\nOnline hotel reservations are a popular method for booking hotel rooms. Travelers can book rooms on a computer by using online security to protect their privacy and financial information and by using several online travel agents to compare prices and facilities at different hotels.\n\nPrior to the Internet, travelers could write, telephone the hotel directly, or use a travel agent to make a reservation. Nowadays, online travel agents have pictures of hotels and rooms, information on prices and deals, and even information on local resorts. Many also allow reviews of the traveler to be recorded with the online travel agent.\n\nOnline hotel reservations are also helpful for making last minute travel arrangements. Hotels may drop the price of a room if some rooms are still available. There are several websites that specialize in searches for deals on rooms.\n\nLarge hotel chains typically have direct connections to the airline national distribution systems (GDS) (Sabre, Galileo, Amadeus, and Worldspan). These in turn provide hotel information directly to the hundreds of thousands of travel agents that align themselves with one of these systems. Individual hotels and small hotel chains often cannot afford the expense of these direct connections and turn to other companies to provide the connections.\n\nSeveral large online travel sites are, in effect, travel agencies. These sites send the hotels' information and rates downstream to literally thousands of online travel sites, most of which act as travel agents. They can then receive commission payments from the hotels for any business booked on their websites.\n\nAn increasing number of hotels are building their own websites to allow them to market their hotels directly to consumers. Non-franchise chain hotels require a \"booking engine\" application to be attached to their website to permit people to book rooms in real time. One advantage of booking with the hotel directly is the use of the hotel's full cancellation policy as well as not needing a deposit in most situations. \n\nThe online booking engine applications are supported by Content management system(CMS).\n\nTo improve the likelihood of filling rooms, hotels tend to use several of the above systems. The content on many hotel reservation systems is becoming increasingly similar as more hotels sign up to all the sites. Companies thus have to either rely on specially negotiated rates with the hotels and hotel chains or trust in the influence of search engine rankings to draw in customers.\n\n"}
{"id": "4696317", "url": "https://en.wikipedia.org/wiki?curid=4696317", "title": "Plastic hinge", "text": "Plastic hinge\n\nIn the structural engineering beam theory term, plastic hinge, is used to describe the deformation of a section of a beam where plastic bending occurs. In earthquake engineering plastic hinge is also a type of energy damping device allowing plastic rotation [deformation] of an otherwise rigid column connection.\n\nIn plastic limit analysis of structural members subjected to bending, it is assumed that an abrupt transition from elastic to ideally plastic behaviour occurs at a certain value of moment, known as plastic moment (M). Member behaviour between M and M is considered to be elastic. When M is reached, a plastic hinge is formed in the member. In contrast to a frictionless hinge permitting free rotation, it is postulated that the plastic hinge allows large rotations to occur at constant plastic moment M.\n\nPlastic hinges extend along short lengths of beams. Actual values of these lengths depend on cross-sections and load distributions. But detailed analyses have shown that it is sufficiently accurate to consider beams rigid-plastic, with plasticity confined to plastic hinges at points. While this assumption is sufficient for limit state analysis, finite element formulations are available to account for the spread of plasticity along plastic hinge lengths.\n\nBy inserting a plastic hinge at a plastic limit load into a statically determinate beam, a kinematic mechanism permitting an unbounded displacement of the system can be formed. It is known as the collapse mechanism. For each degree of static indeterminacy of the beam, an additional plastic hinge must be added to form a collapse mechanism\n\nSufficient number of plastic hinges(N) required to make a collapse mechanism (unstable structure):\n\nN=Degree of static indeterminacy + 1\n"}
{"id": "5872573", "url": "https://en.wikipedia.org/wiki?curid=5872573", "title": "Scanning multichannel microwave radiometer", "text": "Scanning multichannel microwave radiometer\n\nThe Scanning Multichannel Microwave Radiometer (SMMR) [pronounced \"simmer\"] was a five-frequency microwave radiometer flown on the Seasat and Nimbus 7 satellites. Both were launched in 1978, with the Seasat mission lasting less than six months until failure of the primary bus. The Nimbus 7 SMMR lasted from 25 October 1978 until 20 August 1987. It measured dual-polarized microwave radiances, at 6.63, 10.69, 18.0, 21.0, and 37.0 GHz, from the Earth's atmosphere and surface. Its primary legacy has been the creation of areal sea-ice climatologies for the Arctic and Antarctic.\n\nThe final few months of operation was considerably fortuitous as it allowed the calibration of the radiometers and their products with the first results from the SSMI.\n\nJezek, K.C., C. Merry, D. Cavalieri, S.Grace, J. Bedner, D. Wilson and D. Lampkin 1991: Comparison between SMMR and SSM/I passive microwave data collected over the Antarctic Ice Sheet. Byrd Polar Research Center, The Ohio State university, Columbus, OH., BPRC Technical Report Number 91-03, .\n\n"}
{"id": "2462983", "url": "https://en.wikipedia.org/wiki?curid=2462983", "title": "Sling (climbing equipment)", "text": "Sling (climbing equipment)\n\nA sling or runner is an item of climbing equipment consisting of a tied or sewn loop of webbing. These can be wrapped around sections of rock, hitched to other pieces of equipment, or tied directly to a tensioned line using a Prusik style knot. They may be used as anchors, to extend an anchor to reduce rope drag, in anchor equalization, or to climb a rope.\n\nSlings come both sewn to length and assembled from loose webbing knotted as desired. Common sewn lengths include , , , and . They are available in widths of . Webbing for slings, also known as \"tape\", is sold off the reel, cut to length with a hot knife to prevent fraying, and tied as desired with a water knot.\n\nSewn slings have a rated breaking strength of at least . Short sewn slings are a component of quickdraws, sometimes known as \"dogbones\".\n\nTraditionally, slings have been made of nylon. Increasingly, ultra high molecular weight polyethylene sold under the brand names Dyneema, Dynex and Spectra is used. These have much lower melting points than nylon, making them a potentially poor choice where high rope friction may occur. However this specialty polyethylene is lighter, smaller, and absorbs less water than nylon, and therefore has become popular.\n\nA gear sling is a loop of webbing used to organize or carry equipment. These can be custom items meant only to carry light gear, fully load-bearing manufactured gear racks capable of doubling for a sling, or simply a regular sling used to rack gear.\n\n\n"}
{"id": "40831958", "url": "https://en.wikipedia.org/wiki?curid=40831958", "title": "Spiling", "text": "Spiling\n\nSpiling is a traditional technique used in temperate regions of the world for the prevention of erosion to river and stream banks.\n\nWillow spiling is currently used in the United Kingdom; live willow rods are woven between live willow uprights and the area behind is filled with soil for the willow to root into.\n\nKipling's poem \"The Land\" mentions it: \"They spiled along the water-course with trunks of willow-trees, And planks of elms behind 'em and immortal oaken knees.\"\n\nThe species of willow used are riparian (associated with rivers); the posts, in diameter, are usually \"Salix alba\" or \"S. fragilis\", and \"S. viminalis\" varieties are used for the interwoven rods. The living willow posts are driven into the bank, to a depth of or more, at intervals and the thinner rods are woven in between, the rods are best woven at an angle slightly above horizontal to ensure good survival rates. A row of stones, gabions or wooden planks held by posts can be added to the bottom of each \"spile\" to prevent undercutting when the willow is establishing itself. All works should be done during the dormant period, winter in temperate zones. A layer of seeded coir matting can be pegged onto the soil on top of the spiles to prevent the soil being washed out during flood events. This method is an example of soft engineering, techniques which tend to be less expensive and more sustainable than others. \n\n"}
{"id": "23434818", "url": "https://en.wikipedia.org/wiki?curid=23434818", "title": "Suitability analysis", "text": "Suitability analysis\n\nSuitability Analysis is the process and procedures used to establish the suitability of a system - that is, the ability of a system to meet the needs of a stakeholder or other user.\n\nBefore GIS (a computerized method that helps to determine suitability analysis) was widely used in the mid to late 20th century, city planners communicated their suitability analysis ideas by laying transparencies in increasing darkness over maps of the present conditions. This technique's descendant is used in a GIS application called multicriteria decision analysis. In the 1960s, a mechanism called the ecological inventory process was developed to document existing surrounding land conditions to help inform the analysis for the land in question. These mechanisms were computerized upon the advent of computers due to inefficiencies in the methods, such as the inability to overlay a large number of transparencies.\n\nIn order to feed a growing population that is pushing on the ability to extensively farm, suitability analysis is becoming more necessary to utilize the most productive land to its fullest potential, matching the needs of the plants more carefully to the existing assets in the environment. This technique is known as precision farming.\n\nSuitability analysis can also be used to track and label potential hazards, like earthquakes, contamination, or even crime. It can also be used to locate advantageous locations for commercial centers.\n\nSuitability analysis in a GIS context is a geographic, or GIS-based process used to determine the appropriateness of a given area for a particular use. The basic premise of GIS suitability analysis is that each aspect of the landscape has intrinsic characteristics that are to some degree either suitable or unsuitable for the activities being planned. Suitability is determined through systematic, multi-factor analysis of the different aspects of the terrain. Model inputs include a variety of physical, cultural, and economic factors. The results are often displayed on a map that is used to highlight areas from high to low suitability.\n\nA GIS suitability model typically answers the question, \"Where is the best location?\" — whether it involves finding the best location for a new road or pipeline, a new housing development, or a retail store. For instance, a commercial developer building a new retail store may\ntake into consideration distance to major highways and any competitors' stores, then combine the results with land use, population density, and consumer spending data to decide on the best location for that store.\n\n\nThe Possibility Space is a framework that allows for the analysis of all possible consequences and benefits of a suitability analysis. This is created through geometrical data analysis conducted in real time with technological land mapping, allowing for the development of multiple combinations of suitability. Physically it is a visual interactive database that allows for a holistic composition of suitability.\n\n\n\nWhen suitability analyses are done, several different usability options may be found for the same section of land. This can be advantageous or limiting. If the land is found suitable for two or more uses that can be combined, the land uses are found compatible. An example of this may be a building with businesses on the bottom floor with residences on upper floors. Compatible land uses result in a win-win development; a need for more commerce is met while meeting a need for more housing, while also keeping people on the street all day, thereby reducing the probability of crime. Conflicting land use occurs when a piece of land can be used only for one use or the other. This is exemplified by a piece of land that can either be used as agricultural land or developed into a housing tract--should the land be developed, it can no longer be used for agriculture. The suitability analysis comes back into play here by helping planners prioritize which need is greater (in the case of the example, is housing or agricultural land more necessary in light of economic or demand pressure).\n\n"}
{"id": "604602", "url": "https://en.wikipedia.org/wiki?curid=604602", "title": "TICOM", "text": "TICOM\n\nTICOM (Target Intelligence Committee) was a secret Allied project formed in World War II to find and seize German intelligence assets, particularly in the field of cryptology and signals intelligence.\n\nIt operated alongside other Western Allied efforts to extract German scientific and technological information and personnel during and after the war, including Operation Paperclip (for rocketry), Operation Alsos (for nuclear information) and Operation Surgeon (for avionics). Competition with the Soviet Union for these same spoils of war was intense, with direct payoffs including missile technology that led both to a heightened Cold War stalemate and landing a man on the Moon.\n\nThe project was initiated by the British, but when the US Army Chief of Staff General George Marshall learnt of it, it soon became Anglo-American. The aim was to seek out and capture the cryptologic secrets of Germany. The concept was for teams of cryptologic experts, mainly drawn from the code-breaking center at Bletchley Park, to enter Germany with the front-line troops and capture the documents, technology and personnel of the various German signal intelligence organizations before these precious secrets could be destroyed, looted, or captured by the Soviets. There were six such teams.\nThe Allied supposition that the Supreme Command of the German Armed Forces, the \"Oberkommando der Wehrmacht Chiffrierabteilung\" (abbreviated OKW/Chi) was the German equivalent of Bletchley Park, was found to be incorrect. Despite it being the top SIGINT agency in the German military, it did not set policy and did not co-ordinate or direct the signal intelligence work of the different services. It concentrated instead on employing the best cryptanalysts to design Germany's own secure communications systems, and to assist the individual services organisations. These were:\n\nDrs Huttenhain and Fricke of OKW/\"Chi\" were requested to write about the methods of solution of the German machines. This covered the un-steckered Enigma, the steckered Enigmas; Hagelin B-36 and BC-38; the cipher teleprinters Siemens and Halske T52 a/b, T52/c; the Siemens SFM T43; and the Lorenz SZ 40, SZ42 a/b. They assumed Kerckhoffs's principle that how the machines worked would be known, and addressed only the solving of keys, not the breaking of the machines in the first place. This showed that, at least amongst the cryptographers, the un-steckered Enigma was clearly recognized as solvable. The Enigmas with the plugboard (\"Steckerbrett\") were considered secure if used according to the instructions, but were less secure if stereotyped beginnings or routine phrases were used, or during the period of what they described as the \"faulty indicator technique\" - used up until May 1940. It was their opinion, however, that the steckered Enigma had never been solved.\n\nThe discovery in May 1945 of the Nazi Party's top secret FA signals intelligence and cryptanalytic agency at the Kaufbeuren Air Base in southern Bavaria came as a total surprise. The province of Luftwaffe chief Hermann Göring, it has been described as \"the richest, most secret, the most Nazi, and the most influential\" of all the German cryptanalytic intelligence agencies.\n\nThe greatest success for TICOM was the capture of the \"Russian Fish\", a set of German wide-band receivers used to intercept Soviet high-level radio teletype signals. In May 21, 1945, a party of TICOM Team 1 received tip that a German POW had knowledge of certain signals intelligence equipment and documentation relating Russian traffic. After identifying the remaining members of the unit, they were all taken back to their previous base at Rosenheim. The prisoners recovered about 7 ½ tons of equipment. One of the machines was re-assembled and demonstrated. TICOM officer 1st Lt. Paul Whitaker later reported. \"They were intercepting Russian traffic right while we were there…pretty soon they had shown us all we needed to see.\"\n\nIn Operation Stella Polaris the Finnish signals intelligence unit was evacuated to Sweden following the end of the Finland/Soviet cease-fire in September 1944. The records, including cryptographic material, ended up in the hands of Americans.\n\n\n\n"}
{"id": "12291959", "url": "https://en.wikipedia.org/wiki?curid=12291959", "title": "TN 60", "text": "TN 60\n\nThe TN 60 was a French nuclear missile warhead.\n\nThe 1 megaton TN 60 missile warhead entered service in early 1977 as an interim warhead for the MSBS M20 SLBM. The TN 60 was the first French warhead \"hardened\" to penetrate the Russian ABM defenses around Moscow. The TN 60 was replaced with the TN 61 starting in late 1977.\n\n"}
{"id": "1499203", "url": "https://en.wikipedia.org/wiki?curid=1499203", "title": "Transocean", "text": "Transocean\n\nTransocean Ltd. is the world's largest offshore drilling contractor and is based in Vernier, Switzerland. The company has offices in 20 countries, including Switzerland, Canada, United States, Norway, Scotland, India, Brazil, Singapore, Indonesia and Malaysia.\n\nIn 2010, Transocean was implicated in the Deepwater Horizon oil spill resulting from the explosion of one of its oil rigs in the Gulf of Mexico.\n\nThe company provides quarterly updates on the status of its fleet on its website.\n\nIn 2015, Chevron Corporation accounted for 14% of the company's revenues and Royal Dutch Shell accounted for 10% of the company's revenues.\n\nTransocean was formed as a result of the merger of Southern Natural Gas Company, later Sonat, with many smaller drilling companies.\n\nIn 1953, the Birmingham, Alabama-based Southern Natural Gas Company created The Offshore Company after acquiring the joint drilling operation DeLong-McDermott from DeLong Engineering and J. Ray McDermott. In 1954, the company launched Rig 51, the first mobile jackup rig, in the Gulf of Mexico. In 1967, the Offshore Company went public. In 1978, SNG turned it into a wholly owned subsidiary. In 1982, it was changed to Sonat Offshore Drilling Inc., reflecting a change in its parent's name. In 1993, Sonat spun it off.\n\nIn 1996, the company acquired Norwegian group Transocean ASA for US$1.5 billion. Transocean started in the 1970s as a whaling company and expanded through a series of mergers. The new company was called Transocean Offshore. The new company began building massive drilling operations with drills capable of going to 10,000 feet (as opposed to 3,000 feet at the time) and operating two drill operations on the same ship. Its first ship, \"Discoverer Enterprise\", cost nearly US$430 million and was . The Enterprise class drillship is the largest of the drilling ships.\n\nIn 1999, Transocean merged with Sedco Forex, the offshore drilling subsidiary of Schlumberger in a $3.2 billion stock transaction in which Schlumberger shareholders received shares of Transocean.\n\nSedco Forex had been formed from a merger of two drilling companies, the Southeastern Drilling Company (Sedco), founded in 1947 by Bill Clements and acquired by Schlumberger in 1985 for $1 billion and French drilling company Forages et Exploitations Pétrolières (Forex) founded in 1942 in German-occupied France for drilling in North Africa. Schlumberger first got a foothold in the company in 1959 and then assumed total control in 1964, and renamed it Forex Neptune Drilling Company.\n\nIn 2000, Transocean acquired R&B Falcon Corporation, owner of 115 drilling rigs, in a deal valued at $17.7 billion. With the acquisition, Transocean gained control of what at the time was the world's largest offshore operation. Among R&B Falcon's assets was the \"Deepwater Horizon\". R&B Falcon had acquired Cliffs Drilling Company in 1998.\n\nIn 2005, the company's \"Discoverer Spirit\" rig set a world record for the deepest offshore oil and gas well of .\n\nIn 2007, the US Department of Justice and the Securities and Exchange Commission filed a case against Transocean, alleging violations of the Foreign Corrupt Practices Act. The case alleged that Transocean paid bribes through its freight forwarding agents to Nigerian customs officials. Transocean later admitted to approving the bribes and agreed to pay USD $13,440,000 to settle the matter.\n\nIn 2007, the company merged with GlobalSantaFe Corporation in a transaction that created a company with an enterprise value of $53 billion. Shareholders of GlobalSantaFe Corporation received $15 billion of cash as well as stock in the new company for their shares. Robert E. Rose, who was non-executive chairman of GlobalSantaFe, was made Transocean's chairman. Rose had been chairman of Global Marine prior to its 2001 merger with Santa Fe International Corporation.\n\nIn 2008, the company moved its headquarters to Switzerland, resulting in a significantly lower tax rate.\n\nIn September 2009, its \"Deepwater Horizon\" rig established a well, the deepest well in history – more than 5,000 feet deeper than its stated design specification.\n\nIn 2010, Transocean was implicated in the Deepwater Horizon oil spill resulting from the explosion of one of its oil rigs in the Gulf of Mexico that was leased to BP.\n\nIn 2011, the company acquired Aker Drilling, which owned 4 harsh environment rigs used for drilling near Norway.\n\nIn 2012, the company sold 38 shallow water rigs and narrowed its focus on high-specification deepwater rigs.\n\nIn 2013, the company was added to the S&P 500 index.\n\nIn February 2015, CEO Steven Newman quit following a $2.2 billion quarterly loss.\n\nEffective on 30 March 2016, the company delisted its shares from the SIX Swiss Exchange, at which time its shares were removed from the Swiss Market Index.\n\nEffective on January 30, 2018, the company completed its acquisition of Songa Offshore.\n\nTransocean was rated as a leader in its industry for many years. However, since the company's 2007 merger with GlobalSantaFe, Transocean's reputation has suffered considerably, according to EnergyPoint Research, an independent oil service industry rating firm. From 2004 to 2007, Transocean was the leader or near the top among deep-water drillers in \"job quality\" and \"overall satisfaction.\" In 2008 and 2009, surveys ranked Transocean as last among deep-water drillers for \"job quality\" and next to last in \"overall satisfaction.\" In 2008 and 2009, Transocean ranked first for in-house safety and environmental policies, and in the middle of the pack for perceived environmental and safety record. The \"Deepwater Horizon\" explosion and massive oil spill, starting in April 2010, further hurt its reputation. \"Transocean is dominant, but the accident has definitely tarnished its reputation for worker safety and for being able to manage and deliver on extraordinarily complex deepwater projects,\" said Christopher Ruppel, an energy expert and managing director of capital markets at Execution Noble, an investment bank.\n\nOn 2 March 2002, a Scottish man was killed in an accident aboard the \"Transocean Leader\" drilling rig operated for BP, located about 138 kilometers (86 miles) west of Shetland, Scotland.\n\nOn 17 June 2003, one worker was killed, four others were hospitalised and 21 were evacuated after an explosion on a Transocean gas drilling rig in Galveston Bay, Texas.\n\nOn 24 August 2005, the UK Health and Safety Executive issued a notice to Transocean saying that, it had failed to maintain its \"remote blowout preventor control panel … in an efficient state, efficient working order and in good repair.\" On 21 November 2005, Transocean was found to be in compliance for this matter.\n\nOn 12 April 2007, the \"Bourbon Dolphin\" supply boat sank off the coast of Scotland while servicing the \"Transocean Rather\" drilling rig, killing eight people. The Norwegian Ministry of Justice established a Commission of Inquiry to investigate the incident, and the commission's report found a series of \"unfortunate circumstances\" led to the accident \"with many of them linked to Bourbon Offshore and Transocean.\"\n\nIn 2008, two Transocean workers were reportedly killed on the company's vessels.\n\nOn 20 April 2010, a fire was reported on a Transocean-owned semisubmersible drilling rig, \"Deepwater Horizon\". \"Deepwater Horizon\" was a RBS8D design of Reading & Bates Falcon, a firm that was acquired by Transocean in 2001. The fire broke out at 10:00 p.m. CDT UTC−5 in US waters of Mississippi Canyon 252 in the Gulf of Mexico. The rig was off the Louisiana coast. The US Coast Guard launched a rescue operation after the explosion which killed 11 workers and critically injured seven of the 126-member crew.\n\n\"Deepwater Horizon\" was completely destroyed and subsequently sank.\n\nAs the \"Deepwater Horizon\" sank, the riser pipe that connected the well-head to the rig was severed. As a result, oil began to spill into the Gulf of Mexico. Estimates of the leak were about 80,000 barrels per day – for 87 days.\n\nLouisiana Governor Bobby Jindal declared a state of emergency on 29 April, as the oil slick grew and headed toward the most important and most sensitive wetlands in North America, threatening to destroy wildlife and the livelihood of thousands of fishermen. The head of BP Group told CNN's Brian Todd on 28 April that the accident could have been prevented and focused blame on Transocean, which owned and partly manned the rig.\n\nTransocean came under fire from lawyers, representing the fishing and tourism businesses that were hit by the oil spill, and the United States Department of Justice for seeking to use a Limitation of Liability Act of 1851 to restrict its liability for economic damages to $26.7 million.\n\nDuring Congressional testimony, Transocean and BP blamed each other for the disaster. It emerged that a \"heated argument\" broke out on the platform 11 hours before the accident, in which Transocean and BP personnel disagreed on an engineering decision related to the closing of the well. On 14 May 2010, US President Barack Obama commented, \"I did not appreciate what I considered to be a ridiculous spectacle… executives of BP and Transocean and Halliburton [the firm responsible for cementing the well] falling over each other to point the finger of blame at somebody else. The American people could not have been impressed with that display, and I certainly wasn't.\"\n\nTransocean later claimed that 2010, the year in which the disaster occurred, was \"the best year in safety performance in our company’s history\". In a regulatory filing, Transocean said, \"Notwithstanding the tragic loss of life in the Gulf of Mexico, we achieved an exemplary statistical safety record as measured by our total recordable incident rate and total potential severity rate.\" They used this justification to award employees about two-thirds of the maximum possible safety bonuses. In response to broad criticism, including from Interior Secretary Ken Salazar, the company announced that its executives would donate the safety portion of the bonuses to a fund supporting the victims' families.\n\nThe offshore drilling facility \"Sedco 706\", operated by Transocean under contract from Chevron, began to leak in November 2011 while working on the \"Frade\" oil field. Oil began leaking from the seabed at a depth of approximately 1100 to 1200m. Damage included an oil slick (oil floating on the ocean surface) covering an area of approximately 80 km2 and growing. This put the oil at a distance of about 370 km from Rio de Janeiro, but other beautiful beaches are much closer (estimated 140 km). The Brazilian government sued Transocean and attempted to force the company to cease operations in Brazil, but a settlement was reached without a finding of fault or liability.\n\nIn the early hours of Monday 8 August 2016, the semi-submersible drilling rig \"Transocean Winner\" ran aground near Dalmore in the Carloway district of the Isle of Lewis in the Outer Hebrides, Scotland. The rig had been under tow by the tug Alp Forward in winds of galeforce, when the tow line broke. The rig subsequently drifted ashore at Dalmore and became stuck fast on rocks at 07.30 BST. Continuing poor weather meant that a damage inspection by salvors has been practically impossible, as personnel require to be airlifted on to the rig, in spite of it being close to the shore. The rig was carrying approximately 280 tons of diesel, to power its generators, of which 53 tons is thought to have leaked into the sea, and dispersed or evaporated in rough conditions. Environmental monitoring of plant and animal life is on-going, particularly in view of the economically important fish farming operations in nearby Loch Ròg.\n\nIn early April 2015, six activists with the environmental group Greenpeace boarded the ship Blue Marlin while at sea, which was carrying one of Transocean's rigs, the 120 meter (400 ft.) tall Polar Pioneer, and camped out in superstructure of the rig. The activists promised to continuing their protests as the rig undergoes final outfitting while it is anchored in the Port Angeles harbor in Washington State and then later in Puget Sound.\n\n"}
{"id": "49956129", "url": "https://en.wikipedia.org/wiki?curid=49956129", "title": "Trinitroethylorthocarbonate", "text": "Trinitroethylorthocarbonate\n\nTrinitroethylorthocarbonate also known as TNEOC is an oxidizer with excellent chemical stability. Its explosion point is 238 °C, and it begins to be decomposed at 200 °C. Its explosion heat is 5.797 J/g and specific volume is 694 L/kg. Its structure is closely related to that of trinitroethylorthoformate (TNEOF). Both are highly explosive and very shock-sensitive, and may be dissolved in nitroalkanes to reduce their shock-sensitivity.\n\nTrinitroethanol reacts with carbon tetrachloride under a catalyst of FeCl.\n"}
{"id": "1877473", "url": "https://en.wikipedia.org/wiki?curid=1877473", "title": "UK Payments Administration", "text": "UK Payments Administration\n\nThe UK Payments Administration Ltd (UKPA) is a United Kingdom service company that provides people, facilities and expertise to the UK payments industry.\n\nUKPA was created on 6 July 2009, as a successor of the Association for Payment Clearing Services (APACS) to support the systems behind UK payments, such as Bacs, CHAPS and the Cheque and Credit Clearing Company. APACS had been created in 1985 to oversee the majority of UK payment clearing systems and keep their operational efficiency and integrity in order.\n"}
