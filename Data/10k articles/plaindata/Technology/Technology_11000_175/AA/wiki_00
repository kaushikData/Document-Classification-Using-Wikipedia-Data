{"id": "22183423", "url": "https://en.wikipedia.org/wiki?curid=22183423", "title": "Bioceramic", "text": "Bioceramic\n\nBioceramics and bioglasses are ceramic materials that are biocompatible. Bioceramics are an important subset of biomaterials. Bioceramics range in biocompatibility from the ceramic oxides, which are inert in the body, to the other extreme of resorbable materials, which are eventually replaced by the body after they have assisted repair. Bioceramics are used in many types of medical procedures. Bioceramics are typically used as rigid materials in surgical implants, though some bioceramics are flexible. The ceramic materials used are not the same as porcelain type ceramic materials. Rather, bioceramics are closely related to either the body's own materials or are extremely durable metal oxides.\n\nPrior to 1925, the materials used in implant surgery were primarily relatively pure metals. The success of these materials was surprising considering the relatively primitive surgical techniques. The 1930s marked the beginning of the era of better surgical techniques as well as the first use of alloys such as vitallium.\n\nIn 1969, L. L. Hench and others discovered that various kinds of glasses and ceramics could bond to living bone Hench was inspired by the idea on his way to a conference on materials. He was seated next to a colonel who had just returned from the Vietnam War. The colonel shared that after an injury the bodies of soldiers would often reject the implant. Hench was intrigued and began to investigate materials that would be biocompatible. The final product was a new material which he called bioglass. This work inspired a new field called bioceramics. With the discovery of bioglass, interest in bioceramics grew rapidly.\n\nOn April 26, 1988, the first international symposium on bioceramics was held in Kyoto, Japan.\n\nCeramics are now commonly used in the medical fields as dental and bone implants. Surgical cermets are used regularly. Joint replacements are commonly coated with bioceramic materials to reduce wear and inflammatory response. Other examples of medical uses for bioceramics are in pacemakers, kidney dialysis machines, and respirators. The global demand on medical ceramics and ceramic components was about U.S. $9.8 billion in 2010. It was forecast to have an annual growth of 6 to 7 percent in the following years, with world market value predicted to increase to U.S. $15.3 billion by 2015 and reach U.S. $18.5 billion by 2018.\n\nBioceramics are meant to be used in extracorporeal circulation systems (dialysis for example) or engineered bioreactors; however, they're most common as implants. Ceramics show numerous applications as biomaterials due to their physico-chemical properties. They have the advantage of being inert in the human body, and their hardness and resistance to abrasion makes them useful for bones and teeth replacement. Some ceramics also have excellent resistance to friction, making them useful as replacement materials for malfunctioning joints. Properties such as appearance and electrical insulation are also a concern for specific biomedical applications.\n\nSome bioceramics incorporate alumina (AlO) as their lifespan is longer than that of the patient's. The material can be used in inner ear ossicles, ocular prostheses, electrical insulation for pacemakers, catheter orifices and in numerous prototypes of implantable systems such as cardiac pumps.\n\nAluminosilicates are commonly used in dental prostheses, pure or in ceramic-polymer composites. The ceramic-polymer composites are a potential way to filling of cavities replacing amalgams suspected to have toxic effects. The aluminosilicates also have a glassy structure. Contrary to artificial teeth in resin, the colour of tooth ceramic remains stable Zirconia doped with yttrium oxide has been proposed as a substitute for alumina for osteoarticular prostheses. The main advantages are a greater failure strength, and a good resistance to fatigue.\n\nVitreous carbon is also used as it is light, resistant to wear, and compatible with blood. It is mostly used in cardiac valve replacement. Diamond can be used for the same application, but in coating form.\n\nCalcium phosphate-based ceramics constitute, at present, the preferred bone substitute in orthopaedic and maxillofacial surgery. They are similar to the mineral phase of the bone in structure and/or chemical composition. The material is typically porous, which provide a good bone-implant interface due to the increase of surface area that encourages cell colonisation and revascularisation. Additionally, it has lower mechanical strength compared to bone, making highly porous implants very delicate. Since Young's modulus of ceramics is generally much higher than that of the bone tissue, the implant can cause mechanical stresses at the bone interface. Calcium phosphates usually found in bioceramics include hydroxyapatite (HAP) Ca(PO)(OH); tricalcium phosphate β (β TCP): Ca (PO); and mixtures of HAP and β TCP.\n\nTable 1: Bioceramics Applications\n\nTable 2: Mechanical Properties of Ceramic Biomaterials\nA number of implanted ceramics have not actually been designed for specific biomedical applications. However, they manage to find their way into different implantable systems because of their properties and their good biocompatibility. Among these ceramics, we can cite silicon carbide, titanium nitrides and carbides, and boron nitride. TiN has been suggested as the friction surface in hip prostheses. While cell culture tests show a good biocompatibility, the analysis of implants shows significant wear, related to a delaminating of the TiN layer. Silicon carbide is another modern-day ceramic which seems to provide good biocompatibility and can be used in bone implants.\n\nIn addition to being used for their traditional properties, bioactive ceramics have seen specific use for due to their biological activity. Calcium phosphates, oxides, and hydroxides are common examples. Other natural materials — generally of animal origin — such as bioglass and other composites feature a combination of mineral-organic composite materials such as HAP, alumina, or titanium dioxide with the biocompatible polymers (polymethylmethacrylate): PMMA, poly(L-lactic) acid: PLLA, poly(ethylene). Composites can be differentiated as bioresorbable or non-bioresorbable, with the latter being the result of the combination of a non-bioresorbable calcium phosphate (HAP) with a non-bioresorbable polymer (PMMA, PE). These materials may become more widespread in the future, on account of the many combination possibilities and their aptitude at combining a biological activity with mechanical properties similar to those of the bone.\n\nBioceramics' properties of being anticorrosive, biocompatible, and aesthetic make them quite suitable for medical usage. Zirconia ceramic has bioinertness and noncytotoxicity. Carbon is another alternative with similar mechanical properties to bone, and it also features blood compatibility, no tissue reaction, and non-toxicity to cells. None of the three bioinert ceramics exhibit bonding with the bone. However, bioactivity of bioinert ceramics can be achieved by forming composites with bioactive ceramics. Bioglass and glass ceramics are nontoxic and chemically bond to bone. Glass ceramics elicit osteoinductive properties, while calcium phosphate ceramics also exhibit non-toxicity to tissues and bioresorption. The ceramic particulate reinforcement has led to the choice of more materials for implant applications that include ceramic/ceramic, ceramic/polymer, and ceramic/metal composites. Among these composites ceramic/polymer composites have been found to release toxic elements into the surrounding tissues. Metals face corrosion related problems, and ceramic coatings on metallic implants degrade over time during lengthy applications. Ceramic/ceramic composites enjoy superiority due to similarity to bone minerals, exhibiting biocompatibility and a readiness to be shaped. The biological activity of bioceramics has to be considered under various \"in vitro\" and \"in vivo\" studies. Performance needs must be considered in accordance with the particular site of implantation.\n\nTechnically, ceramics are composed of raw materials such as powders and natural or synthetic chemical additives, favoring either compaction (hot, cold or isostatic), setting (hydraulic or chemical), or accelerating sintering processes. According to the formulation and shaping process used, bioceramics can vary in density and porosity as cements, ceramic depositions, or ceramic composites.\n\nA developing material processing technique based on the biomimetic processes aims to imitate natural and biological processes and offer the possibility of making bioceramics at ambient temperature rather than through conventional or hydrothermal processes [GRO 96]. The prospect of using these relatively low processing temperatures opens up possibilities for mineral organic combinations with improved biological properties through the addition of proteins and biologically active molecules (growth factors, antibiotics, anti-tumor agents, etc.). However, these materials have poor mechanical properties which can be improved, partially, by combining them with bonding proteins.\n\nCommon bioactive materials available commercially for clinical use include 45S5 bioactive glass, A/W bioactive glass ceramic, dense synthetic HA, and bioactive composites such as a polyethylene–HA mixture. All these materials form an interfacial bond with adjacent tissue.\n\nHigh-purity alumina bioceramics are currently commercially available from various producers. U.K. manufacturer Morgan Advanced Ceramics (MAC) began manufacturing orthopaedic devices in 1985 and quickly became a recognised supplier of ceramic femoral heads for hip replacements. MAC Bioceramics has the longest clinical history for alumina ceramic materials, manufacturing HIP Vitox® alumina since 1985. Some calcium-deficient phosphates with an apatite structure were thus commercialised as \"tricalcium phosphate\" even though they did not exhibit the expected crystalline structure of tricalcium phosphate.\n\nCurrently, numerous commercial products described as HA are available in various physical forms (e.g. granules, specially designed blocks for specific applications). HA/polymer composite (HA/polyethyelene, HAPEXTM) is also commercially available for ear implants, abrasives, and plasma-sprayed coating for orthopedic and dental implants.\n\nBioceramics have been proposed as a possible treatment for cancer. Two methods of treatment have been proposed: hyperthermia and radiotherapy. Hyperthermia treatment involves implanting a bioceramic material that contains a ferrite or other magnetic material. The area is then exposed to an alternating magnetic field, which causes the implant and surrounding area to heat up. Alternatively, the bioceramic materials can be doped with β-emitting materials and implanted into the cancerous area.\n\nOther trends include engineering bioceramics for specific tasks. Ongoing research involves the chemistry, composition, and micro- and nanostructures of the materials to improve their biocompatibility.\n\n"}
{"id": "1794477", "url": "https://en.wikipedia.org/wiki?curid=1794477", "title": "Boilie", "text": "Boilie\n\nBoilies are boiled paste fishing baits, usually combinations of fishmeals, milk proteins, bird foods, semolina and soya flour, which are mixed with eggs as a binding agent and then boiled to form hardish round baits which will last in the water. Additional flavourings and attractors are also usually included in the mix. The round shape allows the baits to be catapulted accurately when fishing at range.\n\nThe ability to provide a bait of a fairly large size with a hard outer skin, meant that the other species such as tench and bream were less able to consume the bait. Boiled baits also meant that they could be left longer in the water without fear that they had fallen off the hook, in the same way as bread or other traditional baits might.\n\nThough boilies are typically made and sold by large suppliers, many people choose to make their own unique homemade boilies.\n\nBoilies are now one of the most established carp baits, available in a huge range of colours and flavours. Boilies come in all different shapes and sizes, from tiny micro boilies (some even as small as eight millimetres) up to as large as 40 mm which are more suited to waters where 'nuisance fish' are present.\n\nThere are buoyant boilies, commonly known as pop-ups, that are used to make the bait sit just off the bed of the lake making them easier for the fish to find and take. Pop-ups can be used in various situations, where there is weed or silt present on a lake bed, or with a normal boilie to create a 'snowman' rig, the pop-up is generally smaller than the normal boilie, this creates what is known as a critically balanced bait, or neutral buoyancy, and makes it easier for the fish to take in the bait.\n\nThe carp angler has many types of boiled bait to choose from, some of which have added preservatives in them so that they can be kept at room temperature on shop shelves for a long time (shelf-life bait). Boilies that lack these added preservatives need to be refrigerated or frozen to stop them from going off; these are known as freezer baits. There have been many arguments discussing the pros and cons of both freezer and shelf-life boilies but the common opinion of many carp anglers is that due to the artificial preservatives in shelf-life baits they are not as nutritionally beneficial to the carp and therefore lack some attraction. Also, since in order to keep freezer baits fresh they need to be frozen soon after being rolled, not only will the ingredients used be of a much higher quality than in shelf lives but the ingredients used to make them will not lose much of their nutrients and attraction before being used in a fishing situation (much like frozen vegetables). Due to these facts freezer baits are often much more expensive than their shelf life counterparts.\n\nThe most commonly used set-up anglers use to present a boilie is a hair rig (the bait is not attached directly to the hook) which allows the boilie to sit off the back of the hook. This not only means that the bait will behave more naturally in the water (for example when disturbed by feeding fish) it also will often make the difference between a good hook hold and a bad one. Due to the nature in which a carp feeds the bait is blown out of the mouth soon after it has been picked up and the fact that the bait can move independently from the hook it allows the hook to stay back inside the mouth and find its way preferably into the bottom lip.\n\n\n"}
{"id": "332952", "url": "https://en.wikipedia.org/wiki?curid=332952", "title": "Cannula", "text": "Cannula\n\nA cannula (; from Latin \"little reed\"; plural \"cannulae\" or \"cannulas\") is a tube that can be inserted into the body, often for the delivery or removal of fluid or for the gathering of data. In simple terms, a cannula can surround the inner or outer surfaces of a trocar needle thus extending the effective needle length by at least half the length of the original needle. It is also called an intravenous (IV) cannula. Its size mainly ranges from 14 to 24 gauge. Different-sized cannula have different colours as coded.\n\nDecannulation is the permanent removal of a cannula (extubation), especially of a tracheostomy cannula, once a physician determines it is no longer needed for breathing.\n\nCannulae normally come with a trocar inside. The trocar is a needle, which punctures the body in order to get into the intended space. Many types of cannulae exist:\n\nIntravenous cannulae are the most common in hospital use. A variety of cannulae are used to establish cardiopulmonary bypass in cardiac surgery. A nasal cannula is a piece of plastic tubing that runs under the nose and is used to administer oxygen.\n\nA venous cannula is inserted into a vein, primarily for the administration of intravenous fluids, for obtaining blood samples and for administering medicines. An arterial cannula is inserted into an artery, commonly the radial artery, and is used during major operations and in critical care areas to measure beat-to-beat blood pressure and to draw repeated blood samples. Insertion of the venous cannula is a painful procedure that can lead to anxiety and stress. Use of a vapocoolant (cold spray) immediately before cannulation reduces pain during the procedure, without increasing the difficulty of cannulation.\n\nComplications may arise in the vein as a result of the cannulation procedure, the four main groups of complication are:\n\n\nA nasal cannula or an oral–nasal cannula consists of a flexible tube, usually with multiple short, open-ended branches for comfortable insertion into the nostrils and/or mouth, and may be used for the delivery of a gas (such as pure oxygen), a gas mixture (as, for example, during anesthesia), or to measure airflow into and out of the nose and/or mouth.\n\nThe removal of a tracheotomy tube is referred to as decannulation.\n\nA cannula is used in an emergency procedure to relieve pressure and bloating in cattle and sheep with ruminal tympany, due most commonly to their accidentally grazing wilted legume or legume-dominant pastures, particularly alfalfa, ladino, and red and white clover.\n\nCannulas are a component used in the insertion of the Verichip.\n\nMuch larger cannulas are used to research about the digestive system of cows.\n\nIn aesthetic medicine, a blunt-tip cannula or microcannula (also called smooth tip microcannula, blunt tipped cannula, or simply microcannula) is a small tube with an edge that is not sharp and an extrusion port or pore near the tip which is designed for atraumatic subdermal injections of fluids or gels.\n\nDepending on the size of the internal diameter, it can be used either for the injection of cosmetic wrinkle fillers like hyaluronic acid, collagen, poly-L-lactic acid, CaHA, etc., or for fat transfer (Liposuction). The advantage of using these is that they are less painful, have less risk of bruising, have less swelling, and a better safety profile. Accidental intravascular injections are more difficult with blunt-tip microcannulas, reducing the risk of skin necrosis, ulcers, and embolization to the retinal artery which can result in blindness. Indeed, in May 2015, the USA issued a warning of these risks as an FDA Safety Communication on the \"Unintentional Injection of Soft Tissue Filler Into Blood Vessels In the Face\".\n\nIn January 2012, the \"Dermasculpt\" microcannula was approved by the FDA for use in the United States for use with soft tissue fillers followed by the \"Magic Needle\", \"Softfil\", \"TSK by Air-Tite\", and \"Sculpt-face\". The primary structural differences between microcannulas is the distance of the extrusion port or pore from the tip (closer is more precise), the bluntness of the tip (tapered blunt tip is easier for entry), and the flexibility of the shaft (enough flexibility to move around sensitive structures but enough rigidity for precise placement.\n\nSince microcannula tips are blunt, a Pilot or Introducer needle is required for entry through the skin and the technique is to thread the microcannula through this tiny opening. Microcannula cosmetic injection techniques have been developed on how to best place cosmetic wrinkle fillers such as the Long MicroCannula Double Cross-Hatched Fan and the Wiggle Progression techniques.\n\nIn April 2016, the concept of the use of microcannula to inject more than cosmetic fillers was first published. The technique of Microcannula Injected Local Anesthesia (MILA) was described on the use of microcannula to inject local anesthesia with less pain, bruising, and swelling. Also introduced were Accelerated Healing After Platelet-Rich Plasma (AHA-PRP), Accelerated Healing After Platelet-Rich Fibrin Matrix (AHA-PRFM), and the use of microcannula to dissolve Sculptra nodules.\n\nCannulae are used in body piercing when using a standard IV needle (usually between 18GA and 12GA, although may be as large as 0GA, in which case the procedure is known as dermal punching and uses a biopsy punch without a cannula), and for inserting hooks for suspensions.\n\nDuring piercing, the fistula is created by inserting the needle. The needle is then removed, leaving the cannula in place, which is sometimes trimmed down. The cannula is then removed and sterile jewelry is inserted into the fistula simultaneously, in order to minimise trauma to the fresh fistula caused by insertion of blunt-ended jewelry.\n\nIn biological research, a push-pull cannula, which both withdraws and injects fluid, can be used to determine the effect of a certain chemical on a specific cell. The push part of the cannula is filled with a physiological solution plus the chemical of interest and is then injected slowly into the local cellular environment of a cell. The pull cannula then draws liquid from the extracellular medium, thus measuring the cellular response to the chemical of interest. This technique is especially used for neuroscience.\n\nIn general aviation, a cannula refers to a piece of plastic tubing that runs under the nose and is used to administer oxygen in non-pressurized aircraft flying above 10,000 feet sea level.\n\nIn synthetic chemistry, a cannula refers to a piece of stainless steel or plastic tubing used to transfer liquids or gases from one vessel to another without exposure to air. See more at Cannula transfer.\n\n\n"}
{"id": "9489565", "url": "https://en.wikipedia.org/wiki?curid=9489565", "title": "Cardington test", "text": "Cardington test\n\nThe Cardington Fire Tests were a series of large-scale fire tests conducted in real structures (wood, steel-concrete composite and concrete) at the BRE Cardington facility near Cardington, Bedfordshire, England. during the mid 1990s. After the tests, extensive computational and analytical studies of the behaviour of steel-framed composite structures in fire conditions were carried out by, among others, The University of Edinburgh, Sheffield University and Imperial College London. \n\nThe results were presented in the form of a main report, which identified the main findings, together with numerous supplementary reports exploring various phenomena in detail.\n"}
{"id": "3011836", "url": "https://en.wikipedia.org/wiki?curid=3011836", "title": "Church key", "text": "Church key\n\nA churchkey or church key is any of various kinds of bottle openers and can openers.\n\nA churchkey initially referred to a simple hand-operated device for prying the cap (called a \"crown cork\") off a glass bottle; this kind of closure was invented in 1892, although there is no evidence that the opener was called a \"church key\" at that time. The shape and design of some of these openers did resemble a large simple key. \nIn 1935, beer cans with flat tops were marketed, and a device to puncture the lids was needed. The same term, \"church key\", came to be used for this new invention: made from a single piece of pressed metal, with a pointed end used for piercing cans — devised by D.F. Sampson for the American Can Company, who depicted operating instructions on the cans, and typically gave away free \"quick and easy\" openers with their beer cans.\n\nPaint Can Openers include bottle openers.\n\nThe term in the beverage-opening sense is apparently not an old one; Merriam-Webster finds written attestation only since the 1950s. Several etymological themes exist. The main one is that the ends of some bottle openers resemble the heads of large keys such as have traditionally been used to lock and unlock church doors.\n\n\n"}
{"id": "27861414", "url": "https://en.wikipedia.org/wiki?curid=27861414", "title": "Coiled tubing truck", "text": "Coiled tubing truck\n\nA coiled tubing truck is used to deploy continuous tubing into an oil or gas well. They are normally used for well cleanouts (both over balanced or jetvac underbalanced, plug pulling or drill outs, fishing, spotting of acid or cement, installation of umbilicals such as flatak, or instrument and camera runs in vertical or horizontal oil and gas wells.\n\nThere are 4 types of coiled tubing trucks:\n"}
{"id": "38008960", "url": "https://en.wikipedia.org/wiki?curid=38008960", "title": "Compliant bonding", "text": "Compliant bonding\n\nCompliant bonding is used to connect gold wires to electrical components such as integrated circuit \"chips\". It was invented by Alexander Coucoulas in the 1960s. The bond is formed well below the melting point of the mating gold surfaces and is therefore referred to as a solid-state type bond. The compliant bond is formed by transmitting heat and pressure to the bond region through a relatively thick indentable or \"compliant medium\", generally an aluminum tape (Figure 1).\n\nSolid-state or pressure bonds form permanent bonds between a gold wire and a gold metal surface by bringing their mating surfaces in intimate contact at about 300 °C which is well below their respective melting points of 1064 °C, hence the term solid-state bonds.\n\nTwo commonly used methods of forming this type of bond are thermocompression bonding and thermosonic bonding. Both of these processes form the bonds with a hard faced bonding tool that makes direct contact to deform the gold wires against the gold mating surfaces (Figure 2).\n\nSince gold is the only metal that does not form an oxide coating which can interfere with making a reliable metal to metal contact, gold wires are widely used to make these important wire connections in the field of microelectronic packaging. During the compliant bonding cycle the bond pressure is uniquely\ncontrolled by the inherent flow properties of the aluminum compliant tape (Figure 3). Therefore, if higher bond pressures are needed to increase the final deformation (flatness) of a compliant bonded gold wire, a higher yielding alloy of aluminum could be employed. The use of a compliant medium also overcomes the thickness variations when attempting to bond a multiple number of conductor wires simultaneously to a gold metalized substrate (Figure 4). It also prevents the leads from being excessively deformed since the compliant member deforms around the leads during the bonding cycle thus eliminating mechanical failure of a bonded wire due to excessive deformation from a hard faced tool (Figure 3) which is employed by thermocompression, and thermosonic bonding.\n\nAn important application for compliant bonding arose in the early 1960s, when techniques were developed for fabricating a beam leaded silicon integrated circuit “chip” consisting of pre-attached electroformed 0.005-inch thick gold leads or “beams” extending from the silicon chip (Figure 5). Thus the beam leaded “chip” eliminated the need to thermosonically bond wires directly onto metallized pads of the fragile silicon chip (as shown in Figure 6) The extended ends of the electroformed beams could then be permanently solid-state bonded to a matching metallized sunburst circuit which has been pre-deposted on a ceramic substrate appropriately packaged in a computer in the making. Figure 7. shows a preshaped hard faced tool thermocompression bonding all of the beam leads of a chip in one bonding cycle. In order to avoid excessively deforming the fine beam leads with the hard bonding tool and putting them at risk of mechanical failure, the applied bonding forces have to be carefully monitored.\nThe invention of compliant bonding eliminated the problems associated with a hard faced bonding tool and therefore was ideally suited to simultaneously bond all of the extended electroplated gold beam leads to a matching gold metallized sunburst patterned ceramic substrate packaged in a computer (Figure 8). For example, compliant bonding eliminated the problems of using a hard faced bonding tool such as: attempting to uniformly deform the nominally 0.005-inch thick beams leads having slight variations in their thickness; excessive lead deformation that could cause mechanical damage and an ultimate \"costly\" failure of these fine beam leaded silicon chips which are the \"brains\" of our computers. The compliant bonding tape media offered the additional advantage of carrying the \"beam leaded silicon chip\" to the bonding site thus facilitating production. \nFigures 9. and 10 show that the compliant tape offers the advantage of carrying the beam leaded chip to the bonding site as discussed above. Figure 11 shows a beam leaded silicon integrated circuit compliantly bonded to a gold metallized sunburst pattern deposited on an alumina ceramic substrate which will be encapsulated and packaged in a computer-type device. Figure 12 shows the spent compliant member used to bond the chip in Figure 11 which clearly shows a mirror image of the uniformly bonded beam leads.\n\nThe two forms of integrated circuits discussed above were the beam leaded integrated circuit composed of attached electroformed gold leads or beams (Figure 5) and the silicon integrated circuit chip (Figure 6). With respect to the beam leaded silicon chip, both compliant and thermocompression bonding can be employed since each have their advantages. At this time, the most widely used form is the silicon integrated circuit chip, without the beam leads, which therefore requires electrical connections directly to the metallized silicon Chip (Figure 6). If wire connections is the method of choice to form these connections, thermosonic bonding gold wires directly to the silicon chip has been the process most widely used because of its proven reliability as a result of the low bonding parameters of force, temperature, and time needed to form the bond.\n"}
{"id": "7091340", "url": "https://en.wikipedia.org/wiki?curid=7091340", "title": "Composting Association", "text": "Composting Association\n\nThe Organics Recycling Group (ORG), formerly the Association for Organics Recycling (AfOR) and before that the Composting Association, is the leading trade organisation for the biodegradable waste management industry in the UK. It helped to develop the BSI PAS 100 industry standard for composts.\n\nORG was formed by the merger of AfOR and the Renewable Energy Association (REA) on 1 January 2013 which created a membership of around 1,100 companies, organisations and individuals.\n\nTo main objective of the group is to promote the sustainable management of biodegradable resources, covering both aerobic and anaerobic technologies such as windrow and in-vessel composting, anaerobic digestion and mechanical biological treatment. ORG specialises in issues covering the collection, treatment and use of these resources, to complement the work undertaken by the REA on matters such as distributed generation, financial incentives, the Renewables Obligation and planning.\n\n\n"}
{"id": "7500075", "url": "https://en.wikipedia.org/wiki?curid=7500075", "title": "Customer intelligence", "text": "Customer intelligence\n\nCustomer intelligence (CI) is the process of gathering and analyzing information regarding customers, and their details and activities, to build deeper and more effective customer relationships and improve decision-making by vendors.\n\nCustomer intelligence is a key component of effective customer relationship management (CRM), and when effectively implemented it is a rich source of insight into the behaviour and experience of a company's customer base. \n\nAs an example, some customers walk into a store and walk out without buying anything. Information about these customers/prospects (or their visits) may not exist in a traditional CRM system, as no sales are entered on the store cash register. Although no commercial transaction took place, knowing \"why\" customers leave the store (perhaps by asking them, or a store employee, to complete a survey) and using this data to make inferences about customer behaviour, is an example of CI.\n\nCustomer Intelligence begins with reference data – basic key facts about the customer, such as their geographic location.\n\nThis data is then supplemented with transaction data – reports of customer activity. This can be commercial information (for example purchase history from sales and order processing), interactions from service contacts over the phone and via e-mail.\n\nA further subjective dimension can be added, in the form of customer satisfaction surveys or agent data. \n\nFinally, a company can use competitor insight and mystery shopping to get a better view of how their service benchmarks in the market.\n\nBy mining this data, and placing it in context with wider information about competitors, conditions in the industry, and general trends, information can be obtained about customers' existing and future needs, how they reach decisions, and predictions made about their future behavior.\n\nSpeech analytics – used to monitor telephone conversations taking place between companies and customers, using phonetic analysis or speech to text to find keywords and phrases, classify call types and identify trends. \n\nClick tracking – used to monitor the popularity and usage of corporate web sites, this data can provide clues to product interest and buying intention. For example, a company may infer a customer is interested in purchasing a particular service if they are spending time browsing specific product pages.\n\nCustomer relationship management – software solutions used for Salesforce.com, adeptcrmsales and to manage customer relationships which can store data on the quantity, type and category of customer and prospect contacts. \n\nFrontline data capture which may (or may not) form part of a CRM software solution, but which is used by front line agents to record more subjective data regarding customer contacts, such as the root cause of the customer picking up the phone (e.g. they received their bill) or their emotional state.\n\nCustomer satisfaction and market research surveys, often mined via text analytics, which can additionally be applied, for customer intelligence purposes, to contact center notes, e-mail, and other textual sources.\n\nCustomer intelligence provides a detailed understanding of the experience customers have in interacting with a company, and allows predictions to be made regarding reasons behind customer behaviors.\n\nThis knowledge can then be applied to support more effective and strategic decision making – for example, understanding why customers call makes it easier to predict (and plan to reduce) call volumes in a contact centre.\n\n"}
{"id": "50952451", "url": "https://en.wikipedia.org/wiki?curid=50952451", "title": "Czech Institute of Informatics, Robotics and Cybernetics", "text": "Czech Institute of Informatics, Robotics and Cybernetics\n\nThe Czech Institute of Informatics, Robotics and Cybernetics (Český institut informatiky, robotiky a kybernetiky, CIIRC)] was established as a part of CTU on July 1, 2013. Its mission is to become an internationally respected research institute, which participates in education of students and is a place responsible for a technology transfer into the field of industry. In the premises of the CTU in Dejvice there has been two new buildings for CIIRC built since November 2013. The construction works should be finished and the buildings prepared for its inhabitants - employees of CIIRC at the end of summer 2016. CIIRC will reside in a new building, which used to serve as a Technical University canteen and one of the biggest Billa hypermarkets.\n\nCIIRC consists of eight research departments, CYPHY: Cyber-physical systems, INTSYS: Intelligent systems, IIG: Industrial informatics, RMP: Robotics and machine perception, IPA: Industrial production and automation, COGSYS: Cognitive systems and neurosciences, BEAT: Biomedical engineering and Assistive technologies, PLAT: Research Management of Platforms. Research departments are further divided into groups. The head of CIIRC is Vladimír Mařík.\n\n"}
{"id": "10224272", "url": "https://en.wikipedia.org/wiki?curid=10224272", "title": "Dimitrie Leonida Technical Museum", "text": "Dimitrie Leonida Technical Museum\n\nThe Romanian Technical Museum was founded in 1909 by Dimitrie Leonida, inspired by the München Technical Museum, he had visited during his studies in Charlottenburg Politechnic institute.\nIn 1908, with the help of the first promotions of mechanics and electricians from his school, the first in Romania, Leonida collected the first pieces for the museum.\nWhat is different in Leonida museum was the educational orientation of the museum and also the interactivity.\n\n"}
{"id": "1039146", "url": "https://en.wikipedia.org/wiki?curid=1039146", "title": "Directional drilling", "text": "Directional drilling\n\nDirectional drilling (or slant drilling) is the practice of drilling non-vertical wells. It can be broken down into four main groups: oilfield directional drilling, utility installation directional drilling (horizontal directional drilling), directional boring, and surface in seam (SIS), which horizontally intersects a vertical well target to extract coal bed methane.\n\nMany prerequisites enabled this suite of technologies to become productive. Probably, the first requirement was the realization that oil wells, or water wells, are not necessarily vertical. This realization was quite slow, and did not really grasp the attention of the oil industry until the late 1920s when there were several lawsuits alleging that wells drilled from a rig on one property had crossed the boundary and were penetrating a reservoir on an adjacent property. Initially, proxy evidence such as production changes in other wells was accepted, but such cases fueled the development of small diameter tools capable of surveying wells during drilling. Horizontal directional drill rigs are developing towards large-scale, micro-miniaturization, mechanical automation, hard stratum working, exceeding length and depth oriented monitored drilling.\n\nMeasuring the inclination of a wellbore (its deviation from the vertical) is comparatively simple, requiring only a pendulum. Measuring the azimuth (direction with respect to the geographic grid in which the wellbore was running from the vertical), however, was more difficult. In certain circumstances, magnetic fields could be used, but would be influenced by metalwork used inside wellbores, as well as the metalwork used in drilling equipment. The next advance was in the modification of small gyroscopic compasses by the Sperry Corporation, which was making similar compasses for aeronautical navigation. Sperry did this under contract to Sun Oil (which was involved in a lawsuit as described above), and a spin-off company \"Sperry Sun\" was formed, which brand continues to this day, absorbed into Halliburton. Three components are measured at any given point in a wellbore in order to determine its position: the depth of the point along the course of the borehole (measured depth), the inclination at the point, and the magnetic azimuth at the point. These three components combined are referred to as a \"survey\". A series of consecutive surveys are needed to track the progress and location of a wellbore.\n\nPrior experience with rotary drilling had established several principles for the configuration of drilling equipment down hole (\"bottom hole assembly\" or \"BHA\") that would be prone to \"drilling crooked hole\" (i.e., initial accidental deviations from the vertical would be increased). Counter-experience had also given early directional drillers (\"DD's\") principles of BHA design and drilling practice that would help bring a crooked hole nearer the vertical.\n\nIn 1934, H. John Eastman & Roman W. Hines of Long Beach, California, became pioneers in directional drilling when they and George Failing of Enid, Oklahoma, saved the Conroe, Texas, oil field. Failing had recently patented a portable drilling truck. He had started his company in 1931 when he mated a drilling rig to a truck and a power take-off assembly. The innovation allowed rapid drilling of a series of slanted wells. This capacity to quickly drill multiple relief wells and relieve the enormous gas pressure was critical to extinguishing the Conroe fire. In a May, 1934, \"Popular Science Monthly\" article, it was stated that \"Only a handful of men in the world have the strange power to make a bit, rotating a mile below ground at the end of a steel drill pipe, snake its way in a curve or around a dog-leg angle, to reach a desired objective.\" Eastman Whipstock, Inc., would become the world's largest directional company in 1973.\n\nCombined, these survey tools and BHA designs made directional drilling possible, but it was perceived as arcane. The next major advance was in the 1970s, when downhole drilling motors (aka mud motors, driven by the hydraulic power of drilling mud circulated down the drill string) became common. These allowed the drill bit to continue rotating at the cutting face at the bottom of the hole, while most of the drill pipe was held stationary. A piece of bent pipe (a \"bent sub\") between the stationary drill pipe and the top of the motor allowed the direction of the wellbore to be changed without needing to pull all the drill pipe out and place another whipstock. Coupled with the development of measurement while drilling tools (using mud pulse telemetry, networked or wired pipe or electromagnetism (EM) telemetry, which allows tools down hole to send directional data back to the surface without disturbing drilling operations), directional drilling became easier.\n\nCertain profiles cannot be drilled while the drill pipe is rotating. Drilling directionally with a downhole motor requires occasionally stopping rotation of the drill pipe and \"sliding\" the pipe through the channel as the motor cuts a curved path. \"Sliding\" can be difficult in some formations, and it is almost always slower and therefore more expensive than drilling while the pipe is rotating, so the ability to steer the bit while the drill pipe is rotating is desirable. Several companies have developed tools which allow directional control while rotating. These tools are referred to as rotary steerable systems (RSS). RSS technology has made access and directional control possible in previously inaccessible or uncontrollable formations. \n\nWells are drilled directionally for several purposes:\n\n\nMost directional drillers are given a blue well path to follow that is predetermined by engineers and geologists before the drilling commences. When the directional driller starts the drilling process, periodic surveys are taken with a downhole instrument to provide survey data (inclination and azimuth) of the well bore. These pictures are typically taken at intervals between 10–150 meters (30–500 feet), with 30 meters (90 feet) common during active changes of angle or direction, and distances of 60–100 meters (200–300 feet) being typical while \"drilling ahead\" (not making active changes to angle and direction). During critical angle and direction changes, especially while using a downhole motor, a measurement while drilling) (MWD) tool will be added to the drill string to provide continuously updated measurements that may be used for (near) real-time adjustments.\n\nThis data indicates if the well is following the planned path and whether the orientation of the drilling assembly is causing the well to deviate as planned. Corrections are regularly made by techniques as simple as adjusting rotation speed or the drill string weight (weight on bottom) and stiffness, as well as more complicated and time-consuming methods, such as introducing a downhole motor. Such pictures, or surveys, are plotted and maintained as an engineering and legal record describing the path of the well bore. The survey pictures taken while drilling are typically confirmed by a later survey in full of the borehole, typically using a \"multi-shot camera\" device.\n\nThe multi-shot camera advances the film at time intervals so that by dropping the camera instrument in a sealed tubular housing inside the drilling string (down to just above the drilling bit) and then withdrawing the drill string at time intervals, the well may be fully surveyed at regular depth intervals (approximately every 30 meters (90 feet) being common, the typical length of 2 or 3 joints of drill pipe, known as a stand, since most drilling rigs \"stand back\" the pipe withdrawn from the hole at such increments, known as \"stands\").\n\nDrilling to targets far laterally from the surface location requires careful planning and design. The current record holders manage wells over away from the surface location at a true vertical depth (TVD) of only 1,600–2,600 m (5,200–8,500 ft).\n\nThis form of drilling can also reduce the environmental cost and scarring of the landscape. Previously, long lengths of landscape had to be removed from the surface. This is no longer required with directional drilling.\n\nUntil the arrival of modern downhole motors and better tools to measure inclination and azimuth of the hole, directional drilling and horizontal drilling was much slower than vertical drilling due to the need to stop regularly and take time-consuming surveys, and due to slower progress in drilling itself (lower rate of penetration). These disadvantages have shrunk over time as downhole motors became more efficient and semi-continuous surveying became possible.\n\nWhat remains is a difference in operating costs: for wells with an inclination of less than 40 degrees, tools to carry out adjustments or repair work can be lowered by gravity on cable into the hole. For higher inclinations, more expensive equipment has to be mobilized to push tools down the hole.\n\nAnother disadvantage of wells with a high inclination was that prevention of sand influx into the well was less reliable and needed higher effort. Again, this disadvantage has diminished such that, provided sand control is adequately planned, it is possible to carry it out reliably.\n\nIn 1990, Iraq accused Kuwait of stealing Iraq's oil through slant drilling.\nThe United Nations redrew the border after the 1991 Gulf war, which ended the seven-month Iraqi occupation of Kuwait. As part of the reconstruction, 11 new oil wells were placed among the existing 600. Some farms and an old naval base that used to be in the Iraqi side became part of Kuwait.\n\nIn the mid-twentieth century, a slant-drilling scandal occurred in the huge East Texas Oil Field.\n\nBetween 1985 and 1993, the Naval Civil Engineering Laboratory (NCEL) (now the Naval Facilities Engineering Service Center (NFESC)) of Point Hueneme, California developed controllable horizontal drilling technologies. \nThese technologies are capable of reaching (3000–4500 m) and may reach (7500 m) when used under favorable conditions.\n\n\n"}
{"id": "31790833", "url": "https://en.wikipedia.org/wiki?curid=31790833", "title": "Drobe", "text": "Drobe\n\nDrobe (also referred to as Drobe Launchpad) was a computing news web site with a focus on the operating system. Its archived material is retained online, curated by editor Chris Williams.\n\n\"Drobe\" was founded in 1999 by Peter Price. In 2001, Peter handed the site over to Chris Williams as editor. After , it closed as a news site in 2009. It is retained as an historical archive. A few weeks after the site's closure Williams posted articles on \"Micro Men\", the television drama about the rivalry between Acorn and Sinclair in the 1980s. He subsequently stated that such articles may continue to appear periodically.\n\nAt launch, the site featured a news feed, POP email checker and a search facility \"incorporating AcornSearch.com\". , the site features archived articles, news and other media. It also hosts an online emulator for the BBC Micro, using the Java Runtime Environment.\n\nRegistered users were able to apply for user webspace in order to host their own projects. These subsites continue to be hosted by \"Drobe\".\n\n\n\n\n"}
{"id": "24630641", "url": "https://en.wikipedia.org/wiki?curid=24630641", "title": "Drum heater", "text": "Drum heater\n\nA drum heater – also called band heater, barrel heater, container heater or canister heater- is used to reduce viscosity of liquids and gels by heating in order to fill, pump or bottle the respective liquid or to prevent liquids from freezing inside the drum. Typical liquids for the bottling application are tar, varnish, resin, grease, chocolate or galantine. \nDrum heaters are available for drums between 20 and 220 litres.\n\nDrum ovens are designed to store and heat typically from four to 24 drums, usually stored in groups of four on pallets, for ease of insertion and removal by fork-lift. Heated by steam or radiant electric elements, with a single zone thermostat, they are commonly used where long-term storage at a fixed temperature is required to reduce liquid viscosity or keep solids above their melt temperature. Mobile single-drum electric ovens are also available.\n\nindustrially heaters of this type have an isolated heating element embedded in a glass fibre mat. Silicone coated drum heaters are suitable for steel drums, whereas polyester coatings at lower temperatures can be used for plastic containers. As these heaters are flexible and wrap around the container they also called band heaters. Up to three band heaters can be applied to a standard 200 litre barrel. \nThere is no additional insulation so heat is lost by radiation into the surrounding air and the surface temperatures can be hot enough to burn unprotected hands.\n\nThese electrical drum heaters have an outer layer of insulation to protect the user and to increase energy efficiency as significantly less heat is lost to the atmosphere, and thus their power consumption is considerably less than band heaters. Typically they are designed to cover the entire surface of the drum or container.\n\nMany viscous liquids and solids stored in steel drums require considerable energy input prior to removing from the container for processing. It is sometimes beneficial to add heat directly to the underside of the drum, using a bast heater. These are available in various constructions, using induction heating, steam or silicone heater mats inside a body with sufficient mechanical strength to support the weight of a full drum.\n\nIndividual steel drums can be heated by mains frequency induction heating. The magnetic field generated by the induction heater produces eddy currents in the drum wall, causing the steel to directly heat the contents.\n\n"}
{"id": "231500", "url": "https://en.wikipedia.org/wiki?curid=231500", "title": "Dual-modulus prescaler", "text": "Dual-modulus prescaler\n\nA dual modulus prescaler is an electronic circuit used in high-frequency synthesizer designs to overcome the problem of generating narrowly spaced frequencies that are nevertheless too high to be passed directly through the feedback loop of the system. The modulus of a prescaler is its frequency divisor. A dual-modulus prescaler has two separate frequency divisors, usually M and M+1.\n\nA frequency synthesizer produces an output frequency, f, which divided by the modulus is the reference frequency, f:\n\nformula_1\n\nThe modulus, N, is generally restricted to integer values, as the comparator will match when the waveform is in phase. Typically, the possible frequency multiples will be the channels for which the radio equipment is designed for, so f will usually be equal to the channel spacing. For example, on narrow-band radiotelephones, a channel spacing of 12.5 kHz is typical.\n\nSuppose that the programmable divider, using N, is only able to operate at a maximum clock frequency of 10 MHz, but the output f is in the hundreds of MHz range; . Interposing a fixed prescaler, which can operate at this frequency range, with a value M of say, 40, drops the output frequency into the operating range of the programmable divider. However, a factor of 40 has been introduced into the equation, so the output frequency is now:\n\nformula_2\n\nIf f remains at 12.5 kHz, only every 40th channel can be obtained. Alternatively, if f is reduced by a factor of 40 to compensate, it becomes 312.5 Hz, which is much too low to give good filtering and lock performance characteristics. It also means that programming the divider becomes more complex, as the modulus needs to be verified so that only those that give true channels are used, not every 1/40th of a channel that is available.\n\nThe solution is the dual modulus prescaler. The main divider is split into two parts, the main part N and an additional divider A which is strictly lesser than N. Both dividers are clocked from the output of the dual-modulus prescaler, but only the output of the N divider is fed back to the comparator. Initially, the prescaler is set to divide by M + 1. Both N and A count down until A reaches zero, at which point the prescaler is switched to a division ratio of M. At this point, the divider N has completed A counts. Counting continues until N reaches zero, which is an additional N - A counts. At this point the cycle repeats.\n\nformula_3\n\nSo while we still have a factor of M being multiplied by N, we can add an additional count, A, which effectively gives us a divider with a fractional part. Only the prescaler needs to be constructed from high-speed parts, and the reference frequency can remain equal to the desired output frequency spacing.\n\nThe diagram below shows the elements and arrangement of a frequency synthesizer with dual-modulus prescaler. (Compare with diagram on main synthesizer page).\n\nOne can compute A and N from the formulae:\n\nformula_4\n\nwhere V is the combined division ratio V = MN+A. For this to work properly, A must be strictly less than M, as well as less than or equal to N. These restrictions on values of A imply that you can't get every division ratio V. If V falls below M(M - 1), some channels will be missing.\n\nToday, most dual-modulus prescalers exist inside of PLL chips, making it impossible to probe actual signals during operation. The first dual-modulus prescalers were discrete ECL devices, separate from the PLL chips. Here is an example of a dual-modulus prescaler in use. This circuit happens to use a Motorola MC145158 with a Fujitsu MB-501 dual-modulus prescaler operating in the 128/129 mode. The PLL is locked at 917.94 MHz (f) with a channel spacing frequency of 30 kHz (f). The total integer count therefore is 30,598. Dividing this by 128 (M) yields a quotient of 239 with a remainder of 6, N and A respectively. The result of this frequency choice is that the prescaler spends most of its time counting at 128, and just a brief period at 129.\n\nThis is shown by the upper purple trace, the modulus control, A, counter output. These two screen captures differ only in the horizontal scale. The lower, yellow trace is the N counter output whose frequency corresponds to the channel spacing frequency of 30 kHz. The green trace is the output from the dual-modulus prescaler, which happens to correspond to 7.1714 MHz in the case that the prescaler is at 128 and 7.1158 when it is at 129. It is plainly obvious that the modulus control is low for precisely 6 cycles of the prescaler output. What is not obvious is the fact that the frequency changes by less than one percent between the two states of the modulus control. There will be cases where A = 0, resulting in the dual-modulus prescaler counting only by 128. This would happen at 906.24, 910.08, 913.92, 917.76, 921.60 MHz and so on.\n\n"}
{"id": "5124001", "url": "https://en.wikipedia.org/wiki?curid=5124001", "title": "Dual headphone adapter", "text": "Dual headphone adapter\n\nA dual headphone adapter, also known as a \"headphone splitter\" or \"audio jack splitter\", is a device that allows two headphones to be connected through to one audio jack. They can be used to listen to audio through multiple audio input devices, such as headphones on devices such as an MP3 player, CD player, modern Computer with audio-out compatibility (such as a headphone socket) or boombox. Although earbuds, a type of headphone design, can be shared with a friend (with one \"bud\" in another's ear), a dual headphone adapter can be more practical. Dual headphone adapters can be purchased at various audio and electronic stores. Headphone adapters can be used by inserting a 3.5mm Audio jack plug into the headphone jack.\n\nMany designs of headphone adapters have been created by various individuals and/or manufacturers. The most common types are the \"Y\" design, adapters with up to six headphone sockets, and wired headphone adapters.\n"}
{"id": "26672892", "url": "https://en.wikipedia.org/wiki?curid=26672892", "title": "ET3 Global Alliance", "text": "ET3 Global Alliance\n\nET3 Global Alliance is an American open consortium of licensees dedicated to global implementation of Evacuated Tube Transport Technologies (ET3). It was founded by Daryl Oster in 1997 with the goal of establishing a global transportation system utilizing car-sized cargo and passenger capsules traveling in 1.5m diameter tubes via frictionless superconductive maglev.\n\nOster claims that the ET3 system will be able to provide 50 times the amount of transportation per kilowatt-hour compared with electric cars and electric trains, costing only 20 cents' worth of electrical energy to get up to . ET3 claims that initial systems would travel at the speed of for in state trips, and later will be developed to 6,500 km/h (4,000 mph, hypersonic speed) for international travel that will allow passenger or cargo travel from New York to Beijing in 2 hours. The initial proof of concept system could be built in as little 3 years for operational transport.\n\nThe first patent issued in the field of evacuated tube transport was credited to Daryl Oster as inventor while the original assigner was issued to Et3.com Inc on Sept 14, 1999. ET3 has been active since 1999. But it was two years prior, on Oct 10 1997, when Daryl Oster filed the 1999 patent relating to evacuated tube transport. In 2001, Southwest Jiaotong University (SWJTU) invited Daryl to China to advance the field of Evacuated Tube Transport (ETT) and to discuss adopting High Temperature Superconducting Maglev (HTSM) as the levitation system for ETT.\n\nIn 2004, Daryl Oster had published \"A New Industrial Era Coming Initial Dialogue on Evacuated Tube Transport\".\n\nYaoping Zhang of SWJTU and Daryl Oster shared year long communications during the early 2000s via email which can be read in his 2004 published book. SWJTU became the first university institution to become licensees of the ET3 GA consortium. The most ET3 licensees held outside of the USA are held in China. SWJTU and individuals from China have contributed significant IP to the ET3 consortia. By 2007, Yaoping Zhang, a former professor of SWJTU, began promoting ETT as \"evolutionary transportation\". Yaoping Zhang currently operates ET3 GA's subsidiary ET3 China Inc.\n\nET3 has filed a series of new patents in 2014 relating to the field of high-temperature superconductivity (HTS). As of 2016, more than 380 licenses have been sold in 22 different countries, including China, where ET3 claims that more than a dozen licenses have been sold. Daryl Oster and his team met with Tesla Motors/SpaceX CEO Elon Musk in late July, 2013, to discuss the technology, resulting in Musk promising an investment in a prototype of ET3's design.\n\nIn March 2018, a ET3 paper entitled \"closing the infrastructure gap through innovative and sustainable solutions\" was published.\n\n\nPatent application (priority date 2013-03-14): Evacuated tube transport system with interchange capability - A High Temperature Superconductor Maglev (HTSM) for Evacuated Tube Transport (ETT) with a magnetic levitation structure for ETT capsule vehicles traveling in an evacuated tube.\nPatent application (priority date 2013-03-14): Evacuated tube and capsule having interchange capability - The method selectively energizes the force elements to enable the capsule to diverge or converge in an interchange. \n"}
{"id": "27046862", "url": "https://en.wikipedia.org/wiki?curid=27046862", "title": "EcoDisc", "text": "EcoDisc\n\nAn EcoDisc is a patented type of DVD which is thinner than a conventional DVD because it is made from a single layer of polycarbonate instead of two layers glued together. Because it contains less material, its manufacture produces only around half of the carbon dioxide of a conventional DVD, and the absence of a\nnon-biodegradable toxic glue layer makes it easier to recycle. Ecodiscs are prone to breakage (accidental or otherwise) since they are, as of May 2011, much less stiff than regular discs.\n\nEcoDisc Technology AG licences manufacturers of the discs, and also holds the rights to the EcoDisc logo.\n\nThe discs are used for covermounts by magazine publishers because their flexibility makes them less prone to damage during handling and transportation than rigid conventional DVDs and shipping costs are reduced because of their lower weight.\n\nThe maximum capacity of the discs is 4.7 GB, as of February 2011 double layer 8.5 GB capacity is not available.\n\nAlthough earlier versions of the EcoDisc occasionally showed problems with Slot-In drives (especially with those of Apple's MacBook), these problems have since been remedied.\n\nIn 2009 and 2010, Testronic Laboratories, a quality assurance and testing company, performed tests to assess compatibility on around 600 different models of player and drive, representing 80-90% of the installed product base, and found the disc compatible with all but one of the drives (the failing drive was a 1998 Sony DVP-S315).\n"}
{"id": "57111064", "url": "https://en.wikipedia.org/wiki?curid=57111064", "title": "Electronic fingerprint recognition", "text": "Electronic fingerprint recognition\n\nFingerprint scanners are security systems of biometrics. They are now used in police stations, security industries and most recently, on computers.\n\nEveryone has marks on their fingers. They can not be removed or changed. These marks have a pattern and this pattern is called the fingerprint. Every fingerprint is special, and different from any other in the world. Because there are countless combinations, fingerprints have become an ideal means of identification.\n\nThere are four types of fingerprint scanner: the optical scanner, the capacitance scanner, the ultrasonic scanner, and the thermal scanner. The basic function of these three types of scanners is to get an image of a person’s fingerprint and find a match for this print in the database.\nThe capacitance scanner is better, because the images are more exact and precise. Scanners are used for scanning.\n\nThere are two construction forms: the stagnant and the moving fingerprint scanner.\n\n"}
{"id": "16667158", "url": "https://en.wikipedia.org/wiki?curid=16667158", "title": "Electrostatic voltmeter", "text": "Electrostatic voltmeter\n\nElectrostatic voltmeter can refer to an electrostatic charge meter, known also as surface DC voltmeter, or to a voltmeter to measure large electrical potentials, traditionally called electrostatic voltmeter.\n\nA surface DC voltmeter is an instrument that measures voltage with no electric charge transfer. \n\nIt can accurately measure surface potential (voltage) on materials without making physical contact and so there is no electrostatic charge transfer or loading of the voltage source.\n\nMany voltage measurements cannot be made using conventional contacting voltmeters because they require charge transfer to the voltmeter, thus causing loading and modification of the source voltage. For example, when measuring voltage distribution on a dielectric surface, any measurement technique that requires charge transfer, no matter how small, will modify or destroy the actual data.\n\nIn practice, an electrostatic charge monitoring probe is placed close (1 mm to 5 mm) to the surface to be measured and the probe body is driven to the same potential as the measured unknown by an electronic circuit. This achieves a high accuracy measurement that is virtually insensitive to variations in probe-to-surface distances. The technique also prevents arc-over between the probe and measured surface when measuring high voltages.\n\nThe operating principle of an electrostatic voltmeter is similar to that of an electrometer, it is, however, designed to measure high potential differences; typically from a few hundred to many thousands volts.\n\nElectrostatic voltmeter utilizes the attraction force between two charged surfaces to create a deflection of a pointer directly calibrated in volts. Since the attraction force is the same regardless of the polarity of the charged surfaces (as long as the charge is opposite), the electrostatic voltmeter can measure both direct current]] and alternating current. \n\nTypical construction is shown in the engraving. The pivoted sector NN is attracted to the fixed sector QQ. The moving sector indicating the voltage by the pointer P and is counterbalanced by the small weight w. In newer instruments the weight is replaced by a spring, thus allowing the meter to be used both in horizontal and vertical positions. This form of design is shown in the photograph of the mechanism. The fixed sector is insulated from the rest of the meter. The butterfly shaped moving sector, made out of a thin aluminum foil, is pivoted below. Both the fixed and the moving sectors are highly polished and without any sharp corners to minimize high electrical stress areas. The movement of the sector is damped by the air vane attached by a curved piece of wire.\n"}
{"id": "28579811", "url": "https://en.wikipedia.org/wiki?curid=28579811", "title": "Elnec", "text": "Elnec\n\nElnec is a Slovak manufacturer of device programming systems for programmable integrated circuits.\n\nSince its founding in 1991, the company has been oriented towards developing and manufacturing developer tools like device programmers, emulators, simulators and logic analyzers.\n\nCore business of the company today is only the development and manufacture of equipment that transfers data into various non-volatile semiconductor devices. These devices can be sorted into three categories: Microcontroller, Flash Memory, Programmable Logic Devices.\n\nMost of Elnec device programmers can be referred as universal due to support of many programmable devices from different semiconductor companies as Microchip, STMicroelectronics, EM microelectronics, etc.\nElnec’s products are sold also under ODM names as B&K Precision, Dataman or Minato.\n\nOne of company's competitors is Data I/O.\n\nElnec production can be divided into groups:\n\n\n"}
{"id": "45662553", "url": "https://en.wikipedia.org/wiki?curid=45662553", "title": "Flubit", "text": "Flubit\n\nFlubit is a UK-owned online marketplace. Hosting a catalogue of over 70 million products across categories including home, garden, toys, books and electronics, it positions as a direct competitor to Amazon. \n\nOn 11 July 2018 Flubit.com was acquired by the blockchain technology group MonetaryUnit. \n\nAs of May 2015, Flubit had a catalog with over 500 onboard merchants and over 20 million products. By November 2016 the product catalogue had grown to 56 million products, from 1,500 merchants\n\nIn 2014 Flubit secured a partnership with Barclaycard to launch a new service on their bespoke offers platform named \"beat my price\".\n\nIn April 2015 Flubit released its Gift Finder tool. The feature uses several simple metrics based on a short customer questionnaire to create relevant suggestions for gifts priced with a Flubit discount.\n\nIn January 2016 Bertie Stephens and Adel Louertatani were listed in Debrett's \"Britain's 500 most influential\" list.\n\nIn September 2016 Flubit were awarded the Tech Track 100, listed in The Sunday Times\n\nFlubit.com was acquired by MonetaryUnit (MUE) on 11 July 2018 \n\nIn November 2016 Flubit.com launched SKU Cloud. The new brand offers two solutions. The \"sellers\" programme, formally known as WeFlubit, lets retailers integrate a shopping feed exposing their products to over 10 million shoppers per day. For partners, SKU Cloud offers a white-label online shopping solution by providing a live feed of over 54 million products supplied directly from the sellers programme.\n\n"}
{"id": "37250109", "url": "https://en.wikipedia.org/wiki?curid=37250109", "title": "Fruit waxing", "text": "Fruit waxing\n\nFruit waxing is the process of covering fruits (and, in some cases, vegetables) with artificial waxing material. Natural wax is removed first, usually by washing. Waxing materials may be either natural or petroleum-based.\n\nThe primary reasons for waxing are to prevent water loss (making up for the removal in washing of the natural waxes in fruits that have them, particularly citrus but also, for example, apples) and thus retard shrinkage and spoilage, and to improve appearance. Dyes may be added to further enhance appearance, and sometimes fungicides. Fruits were waxed to cause fermentation as early as the 12th or the 13th century; commercial producers began waxing citrus to extend shelf life in the 1920s and 1930s. Aesthetics—consumer preference for shiny fruit—has since become the main reason. In addition to fruit, some vegetables can usefully be waxed, such as cassava; vegetables commonly waxed include cucumbers, swedes or rutabagas, and green tomatoes. A distinction may be made between storage wax, pack-out wax (for immediate sale), and high-shine wax (for optimum attractiveness).\n\nThe waxing materials used depend to some extent on regulations in the country of production and/or export; both natural waxes (sugar-cane, carnauba, shellac, or resin) or petroleum-based waxes (usually proprietary formulae) are used. Wax may be applied in a volatile petroleum-based solvent but is now more commonly applied via a water-based emulsion. Blended paraffin waxes applied as an oil or paste are often used on vegetables. Brand names for waxes include Tal-Prolong, Semper-fresh, Frutox, Waxol, Fruit, vegetable kleen, Nipro Fresh and Decco Luster.\n\n\n"}
{"id": "32303617", "url": "https://en.wikipedia.org/wiki?curid=32303617", "title": "IControlPad", "text": "IControlPad\n\nThe iControlPad is a wireless game controller compatible with a variety of smartphones, tablets, and personal computers. It is designed for use as either a standalone gamepad or attached to appropriately sized devices, such as the iPhone, using a clamp system. Due to this, the iControlPad is able to add traditional physical gaming controls to devices which otherwise rely on inputs such as touchscreens and accelerometers.\n\nThe iControlPad's input controls include an eight-directional D-pad, dual analog nubs, six digital face buttons, and two digital trigger buttons on the gamepad's reverse. The sides of the iControlPad are detachable, with two different attachment types: rubber grips, for using the controller as a standard wireless gamepad; or plastic clamps, for connecting with a suitable handheld, such as a smartphone or iPod Touch. A mini USB port on the bottom of the iControlPad can be used to charge the internal 1500mAh battery, update the device's firmware, and charge attached devices using a USB On-The-Go connection and an appropriate adapter.\n\nThe iControlPad, a Bluetooth device, can be run in a wide variety of modes, including as a HID keyboard, mouse, joystick, and gamepad, among others, allowing compatibility with equipment which is limited to only certain types of input. One of the iControlPad's modes mimics the protocol used by the iCade, an arcade cabinet released for the Apple iPad, facilitating compatibility between apps designed for the iCade and the iControlPad hardware.\n\nDue to the iControlPad's ability to operate as a Bluetooth keyboard—by mapping the D-pad and buttons to standard keyboard keys—it is able to communicate with devices such as those running Apple's iOS, including the iPhone and iPad, which do not support Bluetooth gamepads. Since iOS natively supports keyboards, apps can be developed with iControlPad compatibility using either its own protocol or that of the iCade. Thus, the iControlPad is able to control video games and video game console emulators across multiple platforms.\n\nDevelopment of the iControlPad began in 2007, with testing using a hacked SNES gamepad to connect to an iPhone over the dock connection. Once the serial connection was working, the first prototype iControlPad was produced, using a design styled after the Sony PSP. This earliest concept was a one-piece case enveloping the iPhone, with a D-pad on the left side, and four face buttons on the right in a landscape orientation, and was first revealed in 2008.\n\nBy November 2009, a completely redesigned iControlPad prototype was under development. This much larger version moved the controls below the screen and added two analog nubs and two trigger buttons to the controller. This design, which featured clamps to attach it to the iPhone, was much closer to the version that was ultimately released, and would soon go into production.\n\nHowever, one large change was made very late in development. The team had secretly added Bluetooth support to the iControlPad, in order to increase compatibility beyond the iPhone and its proprietary connection. This proved fortunate when Apple began exercising its rights over the dock connector, suing an unlicensed accessory maker. Thus, the iControlPad team were forced to adapt to use the Bluetooth connection for the iPhone, and it was this version which finally became available for order in February 2011.\n\n\n\n\nReception for the iControlPad has been mostly positive. Register Hardware noted that while \"patience and geekery\" were required to get the controller working, the iControlPad \"almost perfectly solves the touchscreen game control conundrum\". Gadgetoid homed in on the device's usefulness for classic gaming, remarking that it was \"awesome [...] for emulation on the go\". TouchArcade's reviewer said while playing games with the iControlPad that \"the experience feels great\", but that \"[he couldn't] recommend that the typical gamer run out right now and grab one,\" due to its limited support on the iTunes App Store.\n\nEarly reviews were mixed on the quality of the controls, with DroidGamers describing them as \"very loose\", while, conversely, Register Hardware said \"the analogue nubs and face buttons work extremely well\". The controller's responsiveness was later improved by replacing the original rubber keymat with a larger one. In their review, Gadgetoid lauded the inputs as having \"a great tactile feel and a liberal amount of travel with a good response.\"\n\nA successor, the iControlPad 2, was successfully funded via Kickstarter in October 2012. As of November 2013, it has been cancelled, with little to no refunds.\n\n"}
{"id": "53055477", "url": "https://en.wikipedia.org/wiki?curid=53055477", "title": "Innovation in Malaysia", "text": "Innovation in Malaysia\n\nInnovation in Malaysia describes trends and developments in innovation in Malaysia.\n\nThe Najib Razak coalition government estimates that 6% annual growth is necessary to reach high-income status by 2020. This is a somewhat higher rate than both the average for the previous decade and the World Bank’s projection for 2016 and 2017 of about 4.2% growth.\n\nInnovation for inclusive and sustainable development has recently become a widely discussed area of public policy in Malaysia. Discussion in policy circles has centred on the need to address factors such as low farm productivity, increasing health-related problems, natural disasters, environmental problems and monetary inflation. In 2014, the government launched transdisciplinary research grants with the objective of including societal benefits among the performance criteria at Malaysia’s research universities and providing incentives to promote science in support of poverty alleviation and sustainable development. \n\nOn 16 November 2016, Malaysia ratified the Paris Agreement. According to the World Resources Institute, Malaysia contributed about 0.9% of global greenhouse gas emissions in 2012, taking into account land-use changes and forestry. 'Although Malaysia remains committed to reducing its carbon emissions by 40% by 2020 over 2012 levels, as pledged by the Malaysian prime minister at the climate summit in Warsaw in 2013, it faces growing sustainability challenges’.\n\nIn January 2014, Selangor, the most developed of Malaysia’s federated states, experienced water shortages. These were caused by high pollution levels and the drying of reservoirs as a consequence of overuse. Land clearing and deforestation are still major concerns, due to landslides and population displacements. Malaysia is the world’s second-biggest producer of palm oil. \n\nPalm oil exports represent the third-largest category of Malaysian exports after fossil fuels (petroleum and gas) and electronics. Approximately 58% of Malaysia was forested in 2010. With the government having committed to preserving at least half of all land as primary forest, Malaysia has little latitude to expand the extent of land already under cultivation. Rather, it will need to focus on improving productivity. \n\nIn 2014, oil and gas contributed nearly 32% of government revenue. Although natural gas represented about 40% of Malaysia’s energy consumption in 2008, there have been gas shortages since 2009, owing to the combination of a declining domestic gas supply and rising demand. To compound matters, the sharp drop in global oil prices between July and December 2014 forced the government to cut expenditure in January 2015 to maintain its budget deficit at 3%. A recent UNESCO budget review indicates that Malaysia will not be able to rely on its natural resources to propel itself towards high-income status by 2020.\n\nThere is rising inequality in Malaysia, with the disparity between the top 20% income-earners and the bottom 40% widening. The government’s Subsidy Rationalization Programme, which had first been rolled out in 2010, moved into high gear in 2014 with three consecutive increases in natural gas prices in a single year. The removal of energy subsidies, coupled with the introduction of a general sales tax on consumer goods in April 2015, is expected to increase the cost of living.\n\nThe four out of ten Malaysians in the lowest income bracket are also increasingly exposed to social and environmental risks. The incidence of dengue increased by 90% in 2013 over the previous year, for instance, with 39,222 recorded cases, in a trend which may be linked to deforestation and/or climate change. The rising crime rate is another concern.\n\nBetween 2008 and 2012, research spending rose from 0.79% to 1.13% of GDP. GDP grew steadily over the same period. Malaysia plans to raise this ratio to 2% of GDP by 2020. Whether or not it reaches this target will depend largely upon the dynamism of the business enterprise sector.\n\nResearch and development (R&D) are conducted predominantly in large-scale enterprises in the electronics, automotive and chemical industries. Small and medium-sized enterprises, which make up 97% of all private firms, contribute little. This is because most of the small and medium-sized enterprises that work as subcontractors for multinational firms have remained confined to the role of original equipment manufacturers. In order to help these small and medium-sized enterprises (SMEs) access the requisite knowledge, skills and finance that will enable them to participate in original design and original brand manufacturing, the government has adopted a strategy of connecting SMEs to the incubation facilities in the country’s numerous science and technology parks.\n\nForeign multinational firms are generally engaged in more sophisticated R&D than national firms. However, even the R&D conducted by foreign firms tends to be confined to process and product improvements, rather than pushing back the international technology frontier. Moreover, foreign multinationals are heavily dependent on their parent and subsidiary firms based outside Malaysia for personnel, owing to the lack of qualified human capital and research universities within Malaysia to call upon. \n\nA group of ten multinationals have decided to address these shortcomings. In order to satisfy the research needs of the electrical and electronics industries, which employ nearly 5 000 research scientists and engineers in Malaysia, Agilent Technologies, Intel, Motorola Solutions, Silterra and six other multinationals established a platform in 2012 to promote Collaborative Research in Engineering, Science and Technology (CREST) among industry, academia and the government. These multinational firms generate close to MYR 25 billion (circa US$ 6.9 billion) in annual revenue and spend nearly MYR 1.4 billion on research and development. They have utilized government research grants extensively since the government decided in 2005 to extend these grants beyond domestic firms to multinational beneficiaries. Besides research, the focus is on talent development, the ultimate aim being to help the industry add greater value to its products.\n\nSince the launch of export-oriented industrialization in 1971, multinational corporations have relocated to Malaysia, fuelling a rapid expansion in manufactured exports that has helped turn the country into one of the world’s leading exporters of electrical and electronic goods. Today, Malaysia is highly integrated in global trade, with manufacturing contributing over 60% of its exports. Half of these exports (49%) were destined for the East Asian market in 2010, compared to just 29% in 1980. The main destinations in East Asia are China, Indonesia, the Republic of Korea, Philippines, Singapore and Thailand. In 2013, Malaysia accounted for 6.6% of world exports of integrated circuits and other electronic components, according to the World Trade Organization.\n\nOver the past 15 years or so, the share of manufacturing in GDP has gradually declined as a natural consequence of the concomitant growth in services as a corollary of greater development. Modern manufacturing and services are deeply intertwined, as high-tech industries often have a massive services component. The development of the services sector is thus not, in itself, a cause for concern. \n\nHigh-tech manufacturing has stagnated in absolute terms in recent years and its share of global added value has slipped from 0.8% in 2007 to 0.6% in 2013. Over the same period, Malaysia’s global share of high-tech exports (goods and services) contracted from 4.6% to 3.5%, according to the World Trade Organization. The contribution of high-tech industries to national GDP has likewise dropped.This suggests that the shift towards services has neglected the development of high-tech services. Moreover, although the volume of manufacturing has not declined, less value is being added to manufactured goods than before. As a consequence, Malaysia’s trade surplus declined from 144 529 ringgits (MYR) in 2009 to MYR 91 539 in 2013 and Malaysia has been losing ground in high-tech exports. This means that Malaysian high-tech industries are contributing much less to manufactured exports than they did a decade ago. Even though patent applications with the Malaysian patent office have increased steadily over the years, there still seems to be little return on investment in R&D. Domestic applications also seem to be of lower quality than those of foreign applicants, with a cumulative grants-to-application ratio of 18% between 1989 and 2014, against 53% for foreign applicants over the same period.\n\nIn addition, academic or public research organizations in Malaysia appear to have a limited ability to translate research into intellectual property rights. The Malaysian Institute of Micro-electronic Systems, Malaysia’s forefront public R&D institute, which was corporatized in 1992, contributed 45–50% of Malaysia’s patents filed in 2010 but the low citations that have emerged from those patents suggest that the commercialization rate is low.\n\nWhile discovery and patenting are crucial for Malaysia's export-oriented competitiveness and growth strategy, there still seems to be little return on investment in research and development. The low commercialization rate can largely be attributed to a lack of university–industry collaboration, rigidities in research organizations and problems with co-ordinating policies. Universities seem to confine the commercialization of their research results to specific areas, such as health and information and communication technologies. In 2010, the government established the Malaysian Innovation Agency to spur the commercialization of research. \n\nFive years after its inception, the Malaysian Innovation Agency had made a limited impact on commercialization thus far, owing to the unclear delineation of its role in relation to the Ministry of Science, Technology and Innovation and the agency’s limited resources. Nevertheless, there is some evidence to suggest that the agency is beginning to play a catalytic role in driving commercialization and an innovative culture, especially as regards innovation beyond the hardware industry, which is where firms offering services, such as airline services, are active. \n\nOne public–private funding model involves the Malaysian Palm Oil Board, a public body born of the merger of the Palm Oil Research Institute of Malaysia and the Palm Oil Registration and Licensing Authority in 2000, by act of parliament. Through a tax levied on every tonne of palm oil and palm kernel oil produced in the country, the oil palm industry funds many of the research grants provided by the Malaysian Palm Oil Board. These grants amounted to MYR 2.04 billion (circa US$ 565 million) between 2000 and 2010. The Malaysian Palm Oil Board supports innovation in areas such as biodiesel and alternate uses for palm biomass and organic waste. Its research into biomass has led to the development of wood and paper products, fertilizers, bio-energy sources, polyethylene sheeting for use in vehicles and other products made of palm biomass.\nThe government is keen to develop endogenous research, in order to reduce the country’s reliance on industrial research undertaken by foreign multinational companies. By financing graduate study, the government helped to double enrolment in PhD programmes between 2007 and 2010 to 22,000. It has also introduced incentives to encourage expatriates to return to Malaysia through the Returning Expert Programme and plans to become the sixth-largest destination for international university students by 2020. It is hoped that the creation of the ASEAN Economic Community in 2015 will encourage scientific co-operation among member countries.\n\nThe creation of these research universities resulted from the government’s higher education strategy of 2006. A parallel goal of the strategy was to raise government spending on higher education. By financing graduate students, for instance, the government doubled enrolment in doctoral programmes between 2007 and 2010. According to the UNESCO Institute for Statistics, the number of full-time equivalent researchers in Malaysia tripled between 2008 and 2012 (from 16,345 to 52,052), carrying the researcher density to 1 780 per million inhabitants in 2012, which is well above the global average (1,083).\n"}
{"id": "41228673", "url": "https://en.wikipedia.org/wiki?curid=41228673", "title": "Internal combustion engine", "text": "Internal combustion engine\n\nAn internal combustion engine (ICE) is a heat engine where the combustion of a fuel occurs with an oxidizer (usually air) in a combustion chamber that is an integral part of the working fluid flow circuit. In an internal combustion engine, the expansion of the high-temperature and high-pressure gases produced by combustion applies direct force to some component of the engine. The force is applied typically to pistons, turbine blades, rotor or a nozzle. This force moves the component over a distance, transforming chemical energy into useful mechanical energy.\n\nThe first commercially successful internal combustion engine was created by Étienne Lenoir around 1859 and the first modern internal combustion engine was created in 1876 by Nikolaus Otto (see \"Otto engine\").\n\nThe term \"internal combustion engine\" usually refers to an engine in which combustion is intermittent, such as the more familiar four-stroke and two-stroke piston engines, along with variants, such as the six-stroke piston engine and the Wankel rotary engine. A second class of internal combustion engines use continuous combustion: gas turbines, jet engines and most rocket engines, each of which are internal combustion engines on the same principle as previously described. Firearms are also a form of internal combustion engine.\n\nIn contrast, in external combustion engines, such as steam or Stirling engines, energy is delivered to a working fluid not consisting of, mixed with, or contaminated by combustion products. Working fluids can be air, hot water, pressurized water or even liquid sodium, heated in a boiler. ICEs are usually powered by energy-dense fuels such as gasoline or diesel fuel, liquids derived from fossil fuels. While there are many stationary applications, most ICEs are used in mobile applications and are the dominant power supply for vehicles such as cars, aircraft, and boats.\n\nTypically an ICE is fed with fossil fuels like natural gas or petroleum products such as gasoline, diesel fuel or fuel oil. There is a growing usage of renewable fuels like biodiesel for CI (compression ignition) engines and bioethanol or methanol for SI (spark ignition) engines. Hydrogen is sometimes used, and can be obtained from either fossil fuels or renewable energy.\n\nVarious scientists and engineers contributed to the development of internal combustion engines. In 1791, John Barber developed the gas turbine. In 1794 Thomas Mead patented a gas engine. Also in 1794, Robert Street patented an internal combustion engine, which was also the first to use liquid fuel, and built an engine around that time. In 1798, John Stevens built the first American internal combustion engine. In 1807, French engineers Nicéphore (who went on to invent photography) and Claude Niépce ran a prototype internal combustion engine, using controlled dust explosions, the Pyréolophore. This engine powered a boat on the Saône river, France. The same year, the Swiss engineer François Isaac de Rivaz built an internal combustion engine ignited by an electric spark. In 1823, Samuel Brown patented the first internal combustion engine to be applied industrially.\n\nIn 1854 in the UK, the Italian inventors Eugenio Barsanti and Felice Matteucci tried to patent \"Obtaining motive power by the explosion of gases\", although the application did not progress to the granted stage. In 1860, Belgian Jean Joseph Etienne Lenoir produced a gas-fired internal combustion engine. In 1864, Nikolaus Otto patented the first atmospheric gas engine. In 1872, American George Brayton invented the first commercial liquid-fuelled internal combustion engine. In 1876, Nikolaus Otto, working with Gottlieb Daimler and Wilhelm Maybach, patented the compressed charge, four-cycle engine. In 1879, Karl Benz patented a reliable two-stroke gasoline engine. Later, in 1886, Karl Benz began the first commercial production of motor vehicles with the internal combustion engine. In 1892, Rudolf Diesel developed the first compressed charge, compression ignition engine. In 1926, Robert Goddard launched the first liquid-fueled rocket. In 1939, the Heinkel He 178 became the world's first jet aircraft.\n\nAt one time, the word \"engine\" (via Old French, from Latin \"ingenium\", \"ability\") meant any piece of machinery — a sense that persists in expressions such as \"siege engine\". A \"motor\" (from Latin \"motor\", \"mover\") is any machine that produces mechanical power. Traditionally, electric motors are not referred to as \"engines\"; however, combustion engines are often referred to as \"motors\". (An \"electric engine\" refers to a locomotive operated by electricity.)\n\nIn boating an internal combustion engine that is installed in the hull is referred to as an engine, but the engines that sit on the transom are referred to as motors.\n\nReciprocating piston engines are by far the most common power source for land and water vehicles, including automobiles, motorcycles, ships and to a lesser extent, locomotives (some are electrical but most use Diesel engines). Rotary engines of the Wankel design are used in some automobiles, aircraft and motorcycles.\n\nWhere high power-to-weight ratios are required, internal combustion engines appear in the form of combustion turbines or Wankel engines. Powered aircraft typically uses an ICE which may be a reciprocating engine. Airplanes can instead use jet engines and helicopters can instead employ turboshafts; both of which are types of turbines. In addition to providing propulsion, airliners may employ a separate ICE as an auxiliary power unit. Wankel engines are fitted to many unmanned aerial vehicles.\n\nICEs drive some of the large electric generators that power electrical grids. They are found in the form of combustion turbines in combined cycle power plants with a typical electrical output in the range of 100 MW to 1 GW. The high temperature exhaust is used to boil and superheat water to run a steam turbine. Thus, the efficiency is higher because more energy is extracted from the fuel than what could be extracted by the combustion turbine alone. In combined cycle power plants efficiencies in the range of 50% to 60% are typical. In a smaller scale Diesel generators are used for backup power and for providing electrical power to areas not connected to an electric grid.\n\nSmall engines (usually 2‐stroke gasoline engines) are a common power source for lawnmowers, string trimmers, chain saws, leafblowers, pressure washers, snowmobiles, jet skis, outboard motors, mopeds, and motorcycles.\n\nThere are several possible ways to classify internal combustion engines.\n\nThe base of a reciprocating internal combustion engine is the engine block, which is typically made of cast iron or aluminium. The engine block contains the cylinders. In engines with more than one cylinder they are usually arranged either in 1 row (straight engine) or 2 rows (boxer engine or V engine); 3 rows are occasionally used (W engine) in contemporary engines, and other engine configurations are possible and have been used. Single cylinder engines are common for motorcycles and in small engines of machinery. Water-cooled engines contain passages in the engine block where cooling fluid circulates (the water jacket). Some small engines are air-cooled, and instead of having a water jacket the cylinder block has fins protruding away from it to cool by directly transferring heat to the air. The cylinder walls are usually finished by honing to obtain a cross hatch, which is better able to retain the oil. A too rough surface would quickly harm the engine by excessive wear on the piston.\n\nThe pistons are short cylindrical parts which seal one end of the cylinder from the high pressure of the compressed air and combustion products and slide continuously within it while the engine is in operation. The top wall of the piston is termed its \"crown\" and is typically flat or concave. Some two-stroke engines use pistons with a deflector head. Pistons are open at the bottom and hollow except for an integral reinforcement structure (the piston web). When an engine is working the gas pressure in the combustion chamber exerts a force on the piston crown which is transferred through its web to a gudgeon pin. Each piston has rings fitted around its circumference that mostly prevent the gases from leaking into the crankcase or the oil into the combustion chamber. A ventilation system drives the small amount of gas that escape past the pistons during normal operation (the blow-by gases) out of the crankcase so that it does not accumulate contaminating the oil and creating corrosion. In two-stroke gasoline engines the crankcase is part of the air–fuel path and due to the continuous flow of it they do not need a separate crankcase ventilation system.\n\nThe cylinder head is attached to the engine block by numerous bolts or studs. It has several functions. The cylinder head seals the cylinders on the side opposite to the pistons; it contains short ducts (the \"ports\") for intake and exhaust and the associated intake valves that open to let the cylinder be filled with fresh air and exhaust valves that open to allow the combustion gases to escape. However, 2-stroke crankcase scavenged engines connect the gas ports directly to the cylinder wall without poppet valves; the piston controls their opening and occlusion instead. The cylinder head also holds the spark plug in the case of spark ignition engines and the injector for engines that use direct injection. All CI engines use fuel injection, usually direct injection but some engines instead use indirect injection. SI engines can use a carburetor or fuel injection as port injection or direct injection. Most SI engines have a single spark plug per cylinder but some have 2. A head gasket prevents the gas from leaking between the cylinder head and the engine block. The opening and closing of the valves is controlled by one or several camshafts and springs—or in some engines—a desmodromic mechanism that uses no springs. The camshaft may press directly the stem of the valve or may act upon a rocker arm, again, either directly or through a pushrod.\n\nThe crankcase is sealed at the bottom with a sump that collects the falling oil during normal operation to be cycled again. The cavity created between the cylinder block and the sump houses a crankshaft that converts the reciprocating motion of the pistons to rotational motion. The crankshaft is held in place relative to the engine block by main bearings, which allow it to rotate. Bulkheads in the crankcase form a half of every main bearing; the other half is a detachable cap. In some cases a single \"main bearing deck\" is used rather than several smaller caps. A connecting rod is connected to offset sections of the crankshaft (the crankpins) in one end and to the piston in the other end through the gudgeon pin and thus transfers the force and translates the reciprocating motion of the pistons to the circular motion of the crankshaft. The end of the connecting rod attached to the gudgeon pin is called its small end, and the other end, where it is connected to the crankshaft, the big end. The big end has a detachable half to allow assembly around the crankshaft. It is kept together to the connecting rod by removable bolts.\n\nThe cylinder head has an intake manifold and an exhaust manifold attached to the corresponding ports. The intake manifold connects to the air filter directly, or to a carburetor when one is present, which is then connected to the air filter. It distributes the air incoming from these devices to the individual cylinders. The exhaust manifold is the first component in the exhaust system. It collects the exhaust gases from the cylinders and drives it to the following component in the path. The exhaust system of an ICE may also include a catalytic converter and muffler. The final section in the path of the exhaust gases is the tailpipe.\n\nThe \"top dead center\" (TDC) of a piston is the position where it is nearest to the valves; \"bottom dead center\" (BDC) is the opposite position where it is furthest from them. A \"stroke\" is the movement of a piston from TDC to BDC or vice versa, together with the associated process. While an engine is in operation, the crankshaft rotates continuously at a nearly constant speed. In a 4-stroke ICE, each piston experiences 2 strokes per crankshaft revolution in the following order. Starting the description at TDC, these are:\n\n\nThe defining characteristic of this kind of engine is that each piston completes a cycle every crankshaft revolution. The 4 processes of intake, compression, power and exhaust take place in only 2 strokes so that it is not possible to dedicate a stroke exclusively for each of them. Starting at TDC the cycle consist of:\n\nWhile a 4-stroke engine uses the piston as a positive displacement pump to accomplish scavenging taking 2 of the 4 strokes, a 2-stroke engine uses the last part of the power stroke and the first part of the compression stroke for combined intake and exhaust. The work required to displace the charge and exhaust gases comes from either the crankcase or a separate blower. For scavenging, expulsion of burned gas and entry of fresh mix, two main approaches are described: Loop scavenging, and Uniflow scavenging, SAE news published in the 2010s that 'Loop Scavenging' is better under any circumstance than Uniflow Scavenging.\n\nSome SI engines are crankcase scavenged and do not use poppet valves. Instead the crankcase and the part of the cylinder below the piston is used as a pump. The intake port is connected to the crankcase through a reed valve or a rotary disk valve driven by the engine. For each cylinder a transfer port connects in one end to the crankcase and in the other end to the cylinder wall. The exhaust port is connected directly to the cylinder wall. The transfer and exhaust port are opened and closed by the piston. The reed valve opens when the crankcase pressure is slightly below intake pressure, to let it be filled with a new charge; this happens when the piston is moving upwards. When the piston is moving downwards the pressure in the crankcase increases and the reed valve closes promptly, then the charge in the crankcase is compressed. When the piston is moving upwards, it uncovers the exhaust port and the transfer port and the higher pressure of the charge in the crankcase makes it enter the cylinder through the transfer port, blowing the exhaust gases. Lubrication is accomplished by adding \"2-stroke oil\" to the fuel in small ratios. \"Petroil\" refers to the mix of gasoline with the aforesaid oil. This kind of 2-stroke engines has a lower efficiency than comparable 4-strokes engines and release a more polluting exhaust gases for the following conditions:\n\nThe main advantage of 2-stroke engines of this type is mechanical simplicity and a higher power-to-weight ratio than their 4-stroke counterparts. Despite having twice as many power strokes per cycle, less than twice the power of a comparable 4-stroke engine is attainable in practice.\n\nIn the USA, 2-stroke engines were banned for road vehicles due to the pollution. Off-road only motorcycles are still often 2-stroke but are rarely road legal. However, many thousands of 2-stroke lawn maintenance engines are in use.\n\nUsing a separate blower avoids many of the shortcomings of crankcase scavenging, at the expense of increased complexity which means a higher cost and an increase in maintenance requirement. An engine of this type uses ports or valves for intake and valves for exhaust, except opposed piston engines, which may also use ports for exhaust. The blower is usually of the Roots-type but other types have been used too. This design is commonplace in CI engines, and has been occasionally used in SI engines.\n\nCI engines that use a blower typically use \"uniflow scavenging\". In this design the cylinder wall contains several intake ports placed uniformly spaced along the circumference just above the position that the piston crown reaches when at BDC. An exhaust valve or several like that of 4-stroke engines is used. The final part of the intake manifold is an air sleeve which feeds the intake ports. The intake ports are placed at an horizontal angle to the cylinder wall (I.e: they are in plane of the piston crown) to give a swirl to the incoming charge to improve combustion. The largest reciprocating IC are low speed CI engines of this type; they are used for marine propulsion (see marine diesel engine) or electric power generation and achieve the highest thermal efficiencies among internal combustion engines of any kind. Some Diesel-electric locomotive engines operate on the 2-stroke cycle. The most powerful of them have a brake power of around 4.5 MW or 6,000 HP. The EMD SD90MAC class of locomotives use a 2-stroke engine. The comparable class GE AC6000CW whose prime mover has almost the same brake power uses a 4-stroke engine.\n\nAn example of this type of engine is the Wärtsilä-Sulzer RTA96-C turbocharged 2-stroke Diesel, used in large container ships. It is the most efficient and powerful reciprocating internal combustion engine in the world with a thermal efficiency over 50%. For comparison, the most efficient small four-stroke engines are around 43% thermally-efficient (SAE 900648); size is an advantage for efficiency due to the increase in the ratio of volume to surface area.\n\nSee the external links for a in-cylinder combustion video in a 2-stroke, optically accessible motorcycle engine.\n\nDugald Clerk developed the first two cycle engine in 1879. It used a separate cylinder which functioned as a pump in order to transfer the fuel mixture to the cylinder.\n\nIn 1899 John Day simplified Clerk's design into the type of 2 cycle engine that is very widely used today.\nDay cycle engines are crankcase scavenged and port timed. The crankcase and the part of the cylinder below the exhaust port is used as a pump. The operation of the Day cycle engine begins when the crankshaft is turned so that the piston moves from BDC upward (toward the head) creating a vacuum in the crankcase/cylinder area. The carburetor then feeds the fuel mixture into the crankcase through a reed valve or a rotary disk valve (driven by the engine). There are cast in ducts from the crankcase to the port in the cylinder to provide for intake and another from the exhaust port to the exhaust pipe. The height of the port in relationship to the length of the cylinder is called the \"port timing\".\n\nOn the first upstroke of the engine there would be no fuel inducted into the cylinder as the crankcase was empty. On the downstroke, the piston now compresses the fuel mix, which has lubricated the piston in the cylinder and the bearings due to the fuel mix having oil added to it. As the piston moves downward is first uncovers the exhaust, but on the first stroke there is no burnt fuel to exhaust. As the piston moves downward further, it uncovers the intake port which has a duct that runs to the crankcase. Since the fuel mix in the crankcase is under pressure, the mix moves through the duct and into the cylinder.\n\nBecause there is no obstruction in the cylinder of the fuel to move directly out of the exhaust port prior to the piston rising far enough to close the port, early engines used a high domed piston to slow down the flow of fuel. Later the fuel was \"resonated\" back into the cylinder using an expansion chamber design. When the piston rose close to TDC, a spark ignites the fuel. As the piston is driven downward with power, it first uncovers the exhaust port where the burned fuel is expelled under high pressure and then the intake port where the process has been completed and will keep repeating.\n\nLater engines used a type of porting devised by the Deutz company to improve performance. It was called the Schnurle Reverse Flow system. DKW licensed this design for all their motorcycles. Their DKW RT 125 was one of the first motor vehicles to achieve over 100 mpg as a result.\n\nInternal combustion engines require ignition of the mixture, either by spark ignition (SI) or compression ignition (CI). Before the invention of reliable electrical methods, hot tube and flame methods were used. Experimental engines with laser ignition have been built.\n\nThe spark ignition engine was a refinement of the early engines which used Hot Tube ignition. When Bosch developed the magneto it became the primary system for producing electricity to energize a spark plug. Many small engines still use magneto ignition. Small engines are started by hand cranking using a recoil starter or hand crank. Prior to Charles F. Kettering of Delco's development of the automotive starter all gasoline engined automobiles used a hand crank.\n\nLarger engines typically power their starting motors and ignition systems using the electrical energy stored in a lead–acid battery. The battery's charged state is maintained by an automotive alternator or (previously) a generator which uses engine power to create electrical energy storage.\n\nThe battery supplies electrical power for starting when the engine has a starting motor system, and supplies electrical power when the engine is off. The battery also supplies electrical power during rare run conditions where the alternator cannot maintain more than 13.8 volts (for a common 12V automotive electrical system). As alternator voltage falls below 13.8 volts, the lead-acid storage battery increasingly picks up electrical load. During virtually all running conditions, including normal idle conditions, the alternator supplies primary electrical power.\n\nSome systems disable alternator field (rotor) power during wide open throttle conditions. Disabling the field reduces alternator pulley mechanical loading to nearly zero, maximizing crankshaft power. In this case, the battery supplies all primary electrical power.\n\nGasoline engines take in a mixture of air and gasoline and compress it by the movement of the piston from bottom dead center to top dead center when the fuel is at maximum compression. The reduction in the size of the swept area of the cylinder and taking into account the volume of the combustion chamber is described by a ratio. Early engines had compression ratios of 6 to 1. As compression ratios were increased, the efficiency of the engine increased as well.\n\nWith early induction and ignition systems the compression ratios had to be kept low. With advances in fuel technology and combustion management, high performance engines can run reliably at 12:1 ratio. With low octane fuel, a problem would occur as the compression ratio increased as the fuel was igniting due to the rise in temperature that resulted. Charles Kettering developed a lead additive which allowed higher compression ratios, which was progressively abandoned for automotive use from the 1970s onward, partly due to lead poisoning concerns.\n\nThe fuel mixture is ignited at difference progressions of the piston in the cylinder. At low rpm, the spark is timed to occur close to the piston achieving top dead center. In order to produce more power, as rpm rises the spark is advanced sooner during piston movement. The spark occurs while the fuel is still being compressed progressively more as rpm rises.\n\nThe necessary high voltage, typically 10,000 volts, is supplied by an induction coil or transformer. The induction coil is a fly-back system, using interruption of electrical primary system current through some type of synchronized interrupter. The interrupter can be either contact points or a power transistor. The problem with this type of ignition is that as RPM increases the availability of electrical energy decreases. This is especially a problem, since the amount of energy needed to ignite a more dense fuel mixture is higher. The result was often a high RPM misfire.\n\nCapacitor discharge ignition was developed. It produces a rising voltage that is sent to the spark plug. CD system voltages can reach 60,000 volts. CD ignitions use step-up transformers. The step-up transformer uses energy stored in a capacitance to generate electric spark. With either system, a mechanical or electrical control system provides a carefully timed high-voltage to the proper cylinder. This spark, via the spark plug, ignites the air-fuel mixture in the engine's cylinders.\n\nWhile gasoline internal combustion engines are much easier to start in cold weather than diesel engines, they can still have cold weather starting problems under extreme conditions. For years, the solution was to park the car in heated areas. In some parts of the world, the oil was actually drained and heated over night and returned to the engine for cold starts. In the early 1950s, the gasoline Gasifier unit was developed, where, on cold weather starts, raw gasoline was diverted to the unit where part of the fuel was burned causing the other part to become a hot vapor sent directly to the intake valve manifold. This unit was quite popular until electric engine block heaters became standard on gasoline engines sold in cold climates.\n\nDiesel, PPC and HCCI engines, rely solely on heat and pressure created by the engine in its compression process for ignition. The compression level that occurs is usually twice or more than a gasoline engine. Diesel engines take in air only, and shortly before peak compression, spray a small quantity of diesel fuel into the cylinder via a fuel injector that allows the fuel to instantly ignite. HCCI type engines take in both air and fuel, but continue to rely on an unaided auto-combustion process, due to higher pressures and heat. This is also why diesel and HCCI engines are more susceptible to cold-starting issues, although they run just as well in cold weather once started. Light duty diesel engines with indirect injection in automobiles and light trucks employ glowplugs (or other pre-heating: see Cummins ISB#6BT) that pre-heat the combustion chamber just before starting to reduce no-start conditions in cold weather. Most diesels also have a battery and charging system; nevertheless, this system is secondary and is added by manufacturers as a luxury for the ease of starting, turning fuel on and off (which can also be done via a switch or mechanical apparatus), and for running auxiliary electrical components and accessories. Most new engines rely on electrical and electronic engine control units (ECU) that also adjust the combustion process to increase efficiency and reduce emissions.\n\nSurfaces in contact and relative motion to other surfaces require lubrication to reduce wear, noise and increase efficiency by reducing the power wasting in overcoming friction, or to make the mechanism work at all. Also, the lubricant used can reduce excess heat and provide additional cooling to components. At the very least, an engine requires lubrication in the following parts:\n\nIn 2-stroke crankcase scavenged engines, the interior of the crankcase, and therefore the crankshaft, connecting rod and bottom of the pistons are sprayed by the 2-stroke oil in the air-fuel-oil mixture which is then burned along with the fuel. The valve train may be contained in a compartment flooded with lubricant so that no oil pump is required.\n\nIn a \"splash lubrication system\" no oil pump is used. Instead the crankshaft dips into the oil in the sump and due to its high speed, it splashes the crankshaft, connecting rods and bottom of the pistons. The connecting rod big end caps may have an attached scoop to enhance this effect. The valve train may also be sealed in a flooded compartment, or open to the crankshaft in a way that it receives splashed oil and allows it to drain back to the sump. Splash lubrication is common for small 4-stroke engines.\n\nIn a \"forced\" (also called \"pressurized\") \"lubrication system\", lubrication is accomplished in a closed loop which carries motor oil to the surfaces serviced by the system and then returns the oil to a reservoir. The auxiliary equipment of an engine is typically not serviced by this loop; for instance, an alternator may use ball bearings sealed with their own lubricant. The reservoir for the oil is usually the sump, and when this is the case, it is called a \"wet sump\" system. When there is a different oil reservoir the crankcase still catches it, but it is continuously drained by a dedicated pump; this is called a \"dry sump\" system.\n\nOn its bottom, the sump contains an oil intake covered by a mesh filter which is connected to an oil pump then to an oil filter outside the crankcase, from there it is diverted to the crankshaft main bearings and valve train. The crankcase contains at least one \"oil gallery\" (a conduit inside a crankcase wall) to which oil is introduced from the oil filter. The main bearings contain a groove through all or half its circumference; the oil enters to these grooves from channels connected to the oil gallery. The crankshaft has drillings which take oil from these grooves and deliver it to the big end bearings. All big end bearings are lubricated this way. A single main bearing may provide oil for 0, 1 or 2 big end bearings. A similar system may be used to lubricate the piston, its gudgeon pin and the small end of its connecting rod; in this system, the connecting rod big end has a groove around the crankshaft and a drilling connected to the groove which distributes oil from there to the bottom of the piston and from then to the cylinder.\n\nOther systems are also used to lubricate the cylinder and piston. The connecting rod may have a nozzle to throw an oil jet to the cylinder and bottom of the piston. That nozzle is in movement relative to the cylinder it lubricates, but always pointed towards it or the corresponding piston.\n\nTypically a forced lubrication systems have a lubricant flow higher than what is required to lubricate satisfactorily, in order to assist with cooling. Specifically, the lubricant system helps to move heat from the hot engine parts to the cooling liquid (in water-cooled engines) or fins (in air-cooled engines) which then transfer it to the environment. The lubricant must be designed to be chemically stable and maintain suitable viscosities within the temperature range it encounters in the engine.\n\nCommon cylinder configurations include the straight or inline configuration, the more compact V configuration, and the wider but smoother flat or boxer configuration. Aircraft engines can also adopt a radial configuration, which allows more effective cooling. More unusual configurations such as the H, U, X, and W have also been used.\n\nMultiple cylinder engines have their valve train and crankshaft configured so that pistons are at different parts of their cycle. It is desirable to have the piston's cycles uniformly spaced (this is called \"even firing\") especially in forced induction engines; this reduces torque pulsations and makes inline engines with more than 3 cylinders statically balanced in its primary forces. However, some engine configurations require odd firing to achieve better balance than what is possible with even firing. For instance, a 4-stroke I2 engine has better balance when the angle between the crankpins is 180° because the pistons move in opposite directions and inertial forces partially cancel, but this gives an odd firing pattern where one cylinder fires 180° of crankshaft rotation after the other, then no cylinder fires for 540°. With an even firing pattern, the pistons would move in unison and the associated forces would add.\n\nMultiple crankshaft configurations do not necessarily need a cylinder head at all because they can instead have a piston at each end of the cylinder called an opposed piston design. Because fuel inlets and outlets are positioned at opposed ends of the cylinder, one can achieve uniflow scavenging, which, as in the four-stroke engine is efficient over a wide range of engine speeds. Thermal efficiency is improved because of a lack of cylinder heads. This design was used in the Junkers Jumo 205 diesel aircraft engine, using two crankshafts at either end of a single bank of cylinders, and most remarkably in the Napier Deltic diesel engines. These used three crankshafts to serve three banks of double-ended cylinders arranged in an equilateral triangle with the crankshafts at the corners. It was also used in single-bank locomotive engines, and is still used in marine propulsion engines and marine auxiliary generators.\n\nMost truck and automotive diesel engines use a cycle reminiscent of a four-stroke cycle, but with compression heating causing ignition, rather than needing a separate ignition system. This variation is called the diesel cycle. In the diesel cycle, diesel fuel is injected directly into the cylinder so that combustion occurs at constant pressure, as the piston moves.\n\nOtto cycle is the typical cycle for most of the cars internal combustion engines, that work using gasoline as a fuel. Otto cycle is exactly the same one that was described for the four-stroke engine. It consists of the same major steps: Intake, compression, ignition, expansion and exhaust.\n\nIn 1879, Nikolaus Otto manufactured and sold a double expansion engine (the double and triple expansion principles had ample usage in steam engines), with two small cylinders at both sides of a low-pressure larger cylinder, where a second expansion of exhaust stroke gas took place; the owner returned it, alleging poor performance. In 1906, the concept was incorporated in a car built by EHV (Eisenhuth Horseless Vehicle Company) CT, USA; and in the 21st century Ilmor designed and successfully tested a 5-stroke double expansion internal combustion engine, with high power output and low SFC (Specific Fuel Consumption).\n\nThe six-stroke engine was invented in 1883. Four kinds of six-stroke use a regular piston in a regular cylinder (Griffin six-stroke, Bajulaz six-stroke, Velozeta six-stroke and Crower six-stroke), firing every three crankshaft revolutions. These systems capture the wasted heat of the four-stroke Otto cycle with an injection of air or water.\n\nThe Beare Head and \"piston charger\" engines operate as opposed-piston engines, two pistons in a single cylinder, firing every two revolutions rather more like a regular four-stroke.\n\nThe very first internal combustion engines did not compress the mixture. The first part of the piston downstroke drew in a fuel-air mixture, then the inlet valve closed and, in the remainder of the down-stroke, the fuel-air mixture fired. The exhaust valve opened for the piston upstroke. These attempts at imitating the principle of a steam engine were very inefficient.\nThere are a number of variations of these cycles, most notably the Atkinson and Miller cycles. The diesel cycle is somewhat different.\n\nSplit-cycle engines separate the four strokes of intake, compression, combustion and exhaust into two separate but paired cylinders. The first cylinder is used for intake and compression. The compressed air is then transferred through a crossover passage from the compression cylinder into the second cylinder, where combustion and exhaust occur. A split-cycle engine is really an air compressor on one side with a combustion chamber on the other.\n\nPrevious split-cycle engines have had two major problems—poor breathing (volumetric efficiency) and low thermal efficiency. However, new designs are being introduced that seek to address these problems.\n\nThe Scuderi Engine addresses the breathing problem by reducing the clearance between the piston and the cylinder head through various turbo charging techniques. The Scuderi design requires the use of outwardly opening valves that enable the piston to move very close to the cylinder head without the interference of the valves. Scuderi addresses the low thermal efficiency via firing after top dead centre (ATDC).\n\nFiring ATDC can be accomplished by using high-pressure air in the transfer passage to create sonic flow and high turbulence in the power cylinder.\n\nJet engines use a number of rows of fan blades to compress air which then enters a combustor where it is mixed with fuel (typically JP fuel) and then ignited. The burning of the fuel raises the temperature of the air which is then exhausted out of the engine creating thrust. A modern turbofan engine can operate at as high as 48% efficiency.\n\nThere are six sections to a turbofan engine:\n\nA gas turbine compresses air and uses it to turn a turbine. It is essentially a jet engine which directs its output to a shaft. There are three stages to a turbine: 1) air is drawn through a compressor where the temperature rises due to compression, 2) fuel is added in the combuster, and 3) hot air is exhausted through turbine blades which rotate a shaft connected to the compressor.\n\nA gas turbine is a rotary machine similar in principle to a steam turbine and it consists of three main components: a compressor, a combustion chamber, and a turbine. The air, after being compressed in the compressor, is heated by burning fuel in it. The heated air and the products of combustion expand in a turbine, producing work output. About of the work drives the compressor: the rest (about ) is available as useful work output.\n\nGas Turbines are among the most efficient internal combustion engines. The General Electric 7HA and 9HA turbine combined cycle electrical plants are rated at over 61% efficiency.\n\nA gas turbine is a rotary machine somewhat similar in principle to a steam turbine. It consists of three main components: compressor, combustion chamber, and turbine. The air is compressed by the compressor where a temperature rise occurs. The compressed air is further heated by combustion of injected fuel in the combustion chamber which expands the air. This energy rotates the turbine which powers the compressor via a mechanical coupling. The hot gases are then exhausted to provide thrust.\n\nGas turbine cycle engines employ a continuous combustion system where compression, combustion, and expansion occur simultaneously at different places in the engine—giving continuous power. Notably, the combustion takes place at constant pressure, rather than with the Otto cycle, constant volume.\n\nThe Wankel engine (rotary engine) does not have piston strokes. It operates with the same separation of phases as the four-stroke engine with the phases taking place in separate locations in the engine. In thermodynamic terms it follows the Otto engine cycle, so may be thought of as a \"four-phase\" engine. While it is true that three power strokes typically occur per rotor revolution, due to the 3:1 revolution ratio of the rotor to the eccentric shaft, only one power stroke per shaft revolution actually occurs. The drive (eccentric) shaft rotates once during every power stroke instead of twice (crankshaft), as in the Otto cycle, giving it a greater power-to-weight ratio than piston engines. This type of engine was most notably used in the Mazda RX-8, the earlier RX-7, and other vehicle models. The engine is also used in unmanned aerial vehicles, where the small size and weight and the high power-to-weight ratio are advantageous.\n\nForced induction is the process of delivering compressed air to the intake of an internal combustion engine. A forced induction engine uses a gas compressor to increase the pressure, temperature and density of the air. An engine without forced induction is considered a naturally aspirated engine.\n\nForced induction is used in the automotive and aviation industry to increase engine power and efficiency. It particularly helps aviation engines, as they need to operate at high altitude.\n\nForced induction is achieved by a supercharger, where the compressor is directly powered from the engine shaft or, in the turbocharger, from a turbine powered by the engine exhaust.\n\nAll internal combustion engines depend on combustion of a chemical fuel, typically with oxygen from the air (though it is possible to inject nitrous oxide to do more of the same thing and gain a power boost). The combustion process typically results in the production of a great quantity of heat, as well as the production of steam and carbon dioxide and other chemicals at very high temperature; the temperature reached is determined by the chemical make up of the fuel and oxidisers (see stoichiometry), as well as by the compression and other factors.\n\nThe most common modern fuels are made up of hydrocarbons and are derived mostly from fossil fuels (petroleum). Fossil fuels include diesel fuel, gasoline and petroleum gas, and the rarer use of propane. Except for the fuel delivery components, most internal combustion engines that are designed for gasoline use can run on natural gas or liquefied petroleum gases without major modifications. Large diesels can run with air mixed with gases and a pilot diesel fuel ignition injection. Liquid and gaseous biofuels, such as ethanol and biodiesel (a form of diesel fuel that is produced from crops that yield triglycerides such as soybean oil), can also be used. Engines with appropriate modifications can also run on hydrogen gas, wood gas, or charcoal gas, as well as from so-called producer gas made from other convenient biomass. Experiments have also been conducted using powdered solid fuels, such as the magnesium injection cycle.\n\nPresently, fuels used include:\n\nEven fluidized metal powders and explosives have seen some use. Engines that use gases for fuel are called gas engines and those that use liquid hydrocarbons are called oil engines; however, gasoline engines are also often colloquially referred to as, \"gas engines\" (\"petrol engines\" outside North America).\n\nThe main limitations on fuels are that it must be easily transportable through the fuel system to the combustion chamber, and that the fuel releases sufficient energy in the form of heat upon combustion to make practical use of the engine.\n\nDiesel engines are generally heavier, noisier, and more powerful at lower speeds than gasoline engines. They are also more fuel-efficient in most circumstances and are used in heavy road vehicles, some automobiles (increasingly so for their increased fuel efficiency over gasoline engines), ships, railway locomotives, and light aircraft. Gasoline engines are used in most other road vehicles including most cars, motorcycles, and mopeds. Note that in Europe, sophisticated diesel-engined cars have taken over about 45% of the market since the 1990s. There are also engines that run on hydrogen, methanol, ethanol, liquefied petroleum gas (LPG), biodiesel, paraffin and tractor vaporizing oil (TVO).\n\nHydrogen could eventually replace conventional fossil fuels in traditional internal combustion engines. Alternatively fuel cell technology may come to deliver its promise and the use of the internal combustion engines could even be phased out.\n\nAlthough there are multiple ways of producing free hydrogen, those methods require converting combustible molecules into hydrogen or consuming electric energy. Unless that electricity is produced from a renewable source—and is not required for other purposes— hydrogen does not solve any energy crisis. In many situations the disadvantage of hydrogen, relative to carbon fuels, is its storage. Liquid hydrogen has extremely low density (14 times lower than water) and requires extensive insulation—whilst gaseous hydrogen requires heavy tankage. Even when liquefied, hydrogen has a higher specific energy but the volumetric energetic storage is still roughly five times lower than gasoline. However, the energy density of hydrogen is considerably higher than that of electric batteries, making it a serious contender as an energy carrier to replace fossil fuels. The 'Hydrogen on Demand' process (see direct borohydride fuel cell) creates hydrogen as needed, but has other issues, such as the high price of the sodium borohydride that is the raw material.\n\nSince air is plentiful at the surface of the earth, the oxidizer is typically atmospheric oxygen, which has the advantage of not being stored within the vehicle. This increases the power-to-weight and power-to-volume ratios. Other materials are used for special purposes, often to increase power output or to allow operation under water or in space.\n\nCooling is required to remove excessive heat — over heating can cause engine failure, usually from wear (due to heat-induced failure of lubrication), cracking or warping. Two most common forms of engine cooling are air-cooled and water-cooled. Most modern automotive engines are both water and air-cooled, as the water/liquid-coolant is carried to air-cooled fins and/or fans, whereas larger engines may be singularly water-cooled as they are stationary and have a constant supply of water through water-mains or fresh-water, while most power tool engines and other small engines are air-cooled. Some engines (air or water-cooled) also have an oil cooler. In some engines, especially for turbine engine blade cooling and liquid rocket engine cooling, fuel is used as a coolant, as it is simultaneously preheated before injecting it into a combustion chamber.\n\nInternal combustion engines must have their cycles started. In reciprocating engines this is accomplished by turning the crankshaft (Wankel Rotor Shaft) which induces the cycles of intake, compression, combustion, and exhaust. The first engines were started with a turn of their flywheels, while the first vehicle (the Daimler Reitwagen) was started with a hand crank. All ICE engined automobiles were started with hand cranks until Charles Kettering developed the electric starter for automobiles. This method is now the most widely used, even among non-automobiles.\n\nAs diesel engines have become larger and their mechanisms heavier, air starters have come into use. This is due to the lack of torque in electric starters. Air starters work by pumping compressed air into the cylinders of an engine to start it turning.\n\nTwo-wheeled vehicles may have their engines started in one of four ways:\n\n\n\nThere are also starters where a spring is compressed by a crank motion and then used to start an engine.\n\nSome small engines use a pull-rope mechanism called \"recoil starting,\" as the rope rewinds itself after it has been pulled out to start the engine. This method is commonly used in pushed lawn mowers and other settings where only a small amount of torque is needed to turn an engine over.\n\nTurbine engines are frequently started by an electric motor or by compressed air.\n\nEngine types vary greatly in a number of different ways:\n\nOnce ignited and burnt, the combustion products—hot gases—have more available thermal energy than the original compressed fuel-air mixture (which had higher chemical energy). The available energy is manifested as high temperature and pressure that can be translated into work by the engine. In a reciprocating engine, the high-pressure gases inside the cylinders drive the engine's pistons.\n\nOnce the available energy has been removed, the remaining hot gases are vented (often by opening a valve or exposing the exhaust outlet) and this allows the piston to return to its previous position (top dead center, or TDC). The piston can then proceed to the next phase of its cycle, which varies between engines. Any heat that is not translated into work is normally considered a waste product and is removed from the engine either by an air or liquid cooling system.\n\nInternal combustion engines are heat engines, and as such their theoretical efficiency can be approximated by idealized thermodynamic cycles. The thermal efficiency of a theoretical cycle cannot exceed that of the Carnot cycle, whose efficiency is determined by the difference between the lower and upper operating temperatures of the engine. The upper operating temperature of an engine is limited by two main factors; the thermal operating limits of the materials, and the auto-ignition resistance of the fuel. All metals and alloys have a thermal operating limit, and there is significant research into ceramic materials that can be made with greater thermal stability and desirable structural properties. Higher thermal stability allows for a greater temperature difference between the lower (ambient) and upper operating temperatures, hence greater thermodynamic efficiency. Also, as the cylinder temperature rises, the engine becomes more prone to auto-ignition. This is caused when the cylinder temperature nears the flash point of the charge. At this point, ignition can spontaneously occur before the spark plug fires, causing excessive cylinder pressures. Auto-ignition can be mitigated by using fuels with high auto-ignition resistance (octane rating), however it still puts an upper bound on the allowable peak cylinder temperature.\n\nThe thermodynamic limits assume that the engine is operating under ideal conditions: a frictionless world, ideal gases, perfect insulators, and operation for infinite time. Real world applications introduce complexities that reduce efficiency. For example, a real engine runs best at a specific load, termed its power band. The engine in a car cruising on a highway is usually operating significantly below its ideal load, because it is designed for the higher loads required for rapid acceleration. In addition, factors such as wind resistance reduce overall system efficiency. Engine fuel economy is measured in miles per gallon or in liters per 100 kilometres. The volume of hydrocarbon assumes a standard energy content.\n\nMost iron engines have a thermodynamic limit of 37%. Even when aided with turbochargers and stock efficiency aids, most engines retain an \"average\" efficiency of about 18%-20%. The latest technologies in Formula One engines have seen a boost in thermal efficiency to almost 47%. Rocket engine efficiencies are much better, up to 70%, because they operate at very high temperatures and pressures and can have very high expansion ratios.\n\nThere are many inventions aimed at increasing the efficiency of IC engines. In general, practical engines are always compromised by trade-offs between different properties such as efficiency, weight, power, heat, response, exhaust emissions, or noise. Sometimes economy also plays a role in not only the cost of manufacturing the engine itself, but also manufacturing and distributing the fuel. Increasing the engine's efficiency brings better fuel economy but only if the fuel cost per energy content is the same.\n\nFor stationary and shaft engines including propeller engines, fuel consumption is measured by calculating the brake specific fuel consumption, which measures the mass flow rate of fuel consumption divided by the power produced.\n\nFor internal combustion engines in the form of jet engines, the power output varies drastically with airspeed and a less variable measure is used: thrust specific fuel consumption (TSFC), which is the mass of propellant needed to generate impulses that is measured in either pound force-hour or the grams of propellant needed to generate an impulse that measures one kilonewton-second.\n\nFor rockets, TSFC can be used, but typically other equivalent measures are traditionally used, such as specific impulse and effective exhaust velocity.\n\nInternal combustion engines such as reciprocating internal combustion engines produce air pollution emissions, due to incomplete combustion of carbonaceous fuel. The main derivatives of the process are carbon dioxide , water and some soot — also called particulate matter (PM). The effects of inhaling particulate matter have been studied in humans and animals and include asthma, lung cancer, cardiovascular issues, and premature death. There are, however, some additional products of the combustion process that include nitrogen oxides and sulfur and some uncombusted hydrocarbons, depending on the operating conditions and the fuel-air ratio.\n\nNot all of the fuel is completely consumed by the combustion process. A small amount of fuel is present after combustion, and some of it reacts to form oxygenates, such as formaldehyde or acetaldehyde, or hydrocarbons not originally present in the input fuel mixture. Incomplete combustion usually results from insufficient oxygen to achieve the perfect stoichiometric ratio. The flame is \"quenched\" by the relatively cool cylinder walls, leaving behind unreacted fuel that is expelled with the exhaust. When running at lower speeds, quenching is commonly observed in diesel (compression ignition) engines that run on natural gas. Quenching reduces efficiency and increases knocking, sometimes causing the engine to stall. Incomplete combustion also leads to the production of carbon monoxide (CO). Further chemicals released are benzene and 1,3-butadiene that are also hazardous air pollutants.\n\nIncreasing the amount of air in the engine reduces emissions of incomplete combustion products, but also promotes reaction between oxygen and nitrogen in the air to produce nitrogen oxides (). is hazardous to both plant and animal health, and leads to the production of ozone (O). Ozone is not emitted directly; rather, it is a secondary air pollutant, produced in the atmosphere by the reaction of and volatile organic compounds in the presence of sunlight. Ground-level ozone is harmful to human health and the environment. Though the same chemical substance, ground-level ozone should not be confused with stratospheric ozone, or the ozone layer, which protects the earth from harmful ultraviolet rays.\n\nCarbon fuels contain sulfur and impurities that eventually produce sulfur monoxides (SO) and sulfur dioxide (SO) in the exhaust, which promotes acid rain.\n\nIn the United States, nitrogen oxides, PM, carbon monoxide, sulphur dioxide, and ozone, are regulated as criteria air pollutants under the Clean Air Act to levels where human health and welfare are protected. Other pollutants, such as benzene and 1,3-butadiene, are regulated as hazardous air pollutants whose emissions must be lowered as much as possible depending on technological and practical considerations.\n\n, carbon monoxide and other pollutants are frequently controlled via exhaust gas recirculation which returns some of the exhaust back into the engine intake, and catalytic converters, which convert exhaust chemicals to harmless chemicals.\n\nThe emission standards used by many countries have special requirements for non-road engines which are used by equipment and vehicles that are not operated on the public roadways. The standards are separated from the road vehicles.\n\nSignificant contributions to noise pollution are made by internal combustion engines. Automobile and truck traffic operating on highways and street systems produce noise, as do aircraft flights due to jet noise, particularly supersonic-capable aircraft. Rocket engines create the most intense noise.\n\nInternal combustion engines continue to consume fuel and emit pollutants when idling so it is desirable to keep periods of idling to a minimum. Many bus companies now instruct drivers to switch off the engine when the bus is waiting at a terminal.\n\nIn England, the Road Traffic Vehicle Emissions Fixed Penalty Regulations 2002 (Statutory Instrument 2002 No. 1808) introduced the concept of a \"\"stationary idling offence\". This means that a driver can be ordered \"by an authorised person ... upon production of evidence of his authorisation, require him to stop the running of the engine of that vehicle\" and a \"person who fails to comply ... shall be guilty of an offence and be liable on summary conviction to a fine not exceeding level 3 on the standard scale\"\". Only a few local authorities have implemented the regulations, one of them being Oxford City Council.\n\n\n\n"}
{"id": "2419876", "url": "https://en.wikipedia.org/wiki?curid=2419876", "title": "International Association for Cereal Science and Technology", "text": "International Association for Cereal Science and Technology\n\nThe International Association for Cereal Science and Technology (ICC) was founded in 1955 and was originally called the International Association for Cereal Chemistry. It was set up to develop international standard testing procedures for cereals and flour. It has currently more than fifty member countries and headquarters in Vienna, Austria.\n\n"}
{"id": "4755307", "url": "https://en.wikipedia.org/wiki?curid=4755307", "title": "International Association for Food Protection", "text": "International Association for Food Protection\n\nThe International Association for Food Protection (IAFP), founded in 1911, is a non-profit association of food safety professionals based in Des Moines, Iowa. The organization claims a membership of over 3,000 members from 50 nations. The mission of the IAFP is to provide food safety professionals worldwide with a forum to exchange information on protecting the food supply.\n\nThe Association provides its members with an information network on scientific, technical, and practical developments in food safety and sanitation through its two scientific journals, \"Food Protection Trends\" and \"Journal of Food Protection\", its educational Annual Meeting, and interaction with other food safety professionals.\n\nBefore 2000, it was known as the International Association of Dairy and Milk Inspectors (1911-1936), International Association of Milk Sanitarians (1936-1947), and International Association of Milk and Food Sanitarians, Inc. The name was changed in 1966 to the International Association of Milk, Food and Environmental Sanitarians. In 1999, it received the current name.\n\nIn the early 20th century, an increasing number of cities and states in the US passed policies to ensure safety of milk. The laws were a response to the food industry's deception at that time. There were types of alteration in milk products on the market, such as water-dilution or adding butter or cheaper substitutes. In addition, spoiled milk is danger to health: infant mortality rate was lower in cities that had monitored milk production and sale. The Association was established in 1911 based in Milwaukee, Wisconsin, and there were 35 members. One of the members was from Canada, and one from Australia; the rest were from the US. According to the Association's constitution, an object is to develop “uniform and efficient inspection of dairy farms, milk establishments, milk and milk products” by “men who have a thorough knowledge of dairy work.” The object reflects the historical context when the Association was born.\n\nIn 1923, the Association deemed pasteurization as necessary for processing milk products, and endorsed pasteurization as the only adequate safeguard for milk supplies. US Public Health Service's annual report on milk-borne outbreaks has been decreased from 40-60 per year in the 1920s to about 20 per year after World War II.\n\nIn the 1960s, the Association had expanded its vision on food protection from milk safety. As the President of the Association pointed out in 1960: “Today, we sanitarians must be equipped to deal with problems extending throughout the entire range of environmental health. We must solve problems of waste disposal, insect and rodent control, air pollution, housing, radiological poisoning and many others. Additionally, with more Americans eating out more often than ever before, the food service industry has become an area of responsibility such as would have been impossible for our founding Members to imagine. Recently the packaging of prepared foods of the ‘heat and eat’ variety has developed as a rapidly expanding industry that poses new sanitation problems for you to solve.” The name of the association was changed after a mail ballot in members at large in response to the expanded scope of the Association, the International Association of Milk, Food and Environmental Sanitarians (IAMFES).\n\nThe IAFP annual meeting is held in late July or early August each year, and boasts an attendance of over 2,200 people from U.S. and foreign local and federal governments, academia, food safety consultants and the food industry. Recent and future meeting locations are as follows:\n\n\n"}
{"id": "34664524", "url": "https://en.wikipedia.org/wiki?curid=34664524", "title": "Lesbian rule", "text": "Lesbian rule\n\nA lesbian rule was historically a flexible mason's rule made of lead that could be bent to the curves of a molding, and used to measure or reproduce irregular curves. Lesbian rules were originally constructed of a pliable kind of lead found on the island of Lesbos.\n\nThe rule is alluded to by Aristotle in his \"Nicomachean Ethics\" (bk V, ch. 10) as a metaphor for the importance of flexibility in equitable justice: \"For what is itself indefinite can only be measured by an indefinite standard, like the leaden rule used by Lesbian builders; just as that rule is not rigid but can be bent to the shape of the stone, so a special ordinance is made to fit the circumstances of the case.\"\n\nIn the early modern period the term was often used figuratively (as Aristotle had used it) to mean a pliant, flexible and accommodating principle of judgment. In his famous letter to the Louvain theologian , Thomas More referenced it when reproving Dorp for his attack on Erasmus' \"In Praise of Folly\": \"You praise Adriaan for being unbiased, yet you seem to suggest he is no more unbiased than a Lesbian rule, a rule made out of lead which, as Aristotle reminds us, is not always unbiased, since it bends to fit uneven shapes.\" Samuel Daniel in 1603 described equity as \"that Lesbian square, that building fit, Plies to the worke, not forc'th the worke to it\". In the later 17th century, the antiquary John Aubrey used the metaphor in a more pejorative sense, implying the distortion of evidence to fit a preconceived theory. He accused Inigo Jones, who had interpreted Stonehenge as a Roman monument, of having \"made a Lesbians rule, which is conformed to the stone: that is, he framed the Monument to his own Hypothesis, which is much differing from the Thing it self\".\n\n"}
{"id": "39506373", "url": "https://en.wikipedia.org/wiki?curid=39506373", "title": "Liberty Reserve", "text": "Liberty Reserve\n\nLiberty Reserve was a Costa Rica-based centralized digital currency service that billed itself as the \"oldest, safest and most popular payment processor, serving millions all around a world\". The site had over one million users when it was shut down by the United States government. Prosecutors argued that due to lax security, alleged criminal activity largely went undetected, which ultimately led to them seizing the service.\n\nIn May 2013, Liberty Reserve was shut down by United States federal prosecutors under the Patriot Act after an investigation by authorities across 17 countries. The United States charged founder Arthur Budovsky and six others with money laundering and operating an unlicensed financial transaction company. Liberty Reserve is alleged to have been used to launder more than $6 billion in criminal proceeds during its history.\n\nBased in San José Costa Rica, Liberty Reserve was a centralized digital currency service that allowed users to register and transfer money to other users with only a name, e-mail address, and birth date. No efforts were made by the site to verify identities of its users, making it an attractive payment processor to scam artists. Deposits could be made through third parties using a credit card or bankwire, among other deposit options. Liberty Reserve did not directly process deposits or withdrawals. Deposited funds were then \"converted\" into Liberty Reserve Dollars or Liberty Reserve Euros, which were tied to the value of the U.S. dollar and the euro, respectively, or to ounces of gold. No limits were placed on transaction sizes. The service made money by charging a small fee, about 1%, on each transfer. Transactions were \"100% irrevocable\". Liberty Reserve also offered shopping cart functionality and other merchant services.\n\nService was popular among currency brokers and multilevel marketing companies. According to Forex Magnates, a specialized forex news service, Liberty Reserve was \"the leading payment channel for traders in emerging and frontier markets.\" Richard Weber, head of the U.S. Internal Revenue Service criminal investigation unit, declared, \"If Al Capone were alive today, this is how he would be hiding his money\". At the time of its closure, Liberty Reserve had more than 1 million registered users, 200,000 of which were from the United States. It was a member of the Global Digital Currency Association.\n\nLiberty employees were required to sign a confidentiality agreement to \"maintain in strict confidentiality all information\" about the company, including \"administrative affairs, operations and financial details\" for 15 years after leaving the company. Additionally they were required to notify management if issued a warrant to reveal such information.\n\nFrom 2002 to 2006, United States businessmen Arthur Budovsky and Vladimir Kats ran a digital currency exchange service known as Gold Age. In July 2006, the duo were indicted on charges of operating an illegal financial business, a felony. They were sentenced to five years in prison in 2007, but the sentence was reduced to five years of probation. Budovsky fled the country, settling in Costa Rica. He subsequently became a naturalized citizen of Costa Rica when he married a Costa Rican woman in 2010, and renounced his American citizenship in 2012.\n\nLiberty Reserve was incorporated by Budovsky in Costa Rica in 2006. A 2007 interview with Joul Lee, the company's marketing manager, claimed it was founded as \"a private currency exchange system for import/export businesses\" and opened to the public in 2007.\n\nCosta Rican authorities became aware of Liberty Reserve in 2009 and informed the business it needed a license to operate as a money transmitting business. In 2011, Liberty Reserve was denied a business license in Costa Rica, according to state prosecutor José Pablo González, due to lack of transparency about how the business was funded. The business formally disbanded at that time, but company founder Arthur Budovsky continued to operate the business by funneling it through five other Costa Rican businesses, according to authorities. A criminal investigation was launched March 7, 2011 following \"suspicious\" bank activity. Later in 2011, the United States authorities asked Costa Rica to begin investigating Budovsky's business dealings. According to Bernardita Marín, associate director of the Costa Rican Drug Institute, Costa Rica seized funds from Liberty Reserve on three occasions from 2011 to 2013.\n\nIn 2011, Liberty Reserve was linked to (unrelated) attempts to sell thousands of stolen Australian bank account numbers and British bank cards. In 2012, a group of hackers attempted to blackmail anti-virus software company Symantec into transferring $50,000 into a Liberty Reserve account.\n\nAfter a multi-year investigation by officials in 17 countries, a sealed indictment was issued by the U.S. government in May 2013. U.S. prosecutors filed a case against Liberty Reserve, alleging it had handled $6 billion of criminal proceeds. Arthur Budovsky was arrested by Spanish police at Madrid's Barajas International Airport as he attempted to return to Costa Rica where he had citizenship. Budovsky and a second man were ordered jailed by Spanish authorities pending an extradition hearing. Earlier three homes and the five apparent shell businesses owned by Budovsky were raided. Four others, including Kats, were arrested across three countries: Costa Rica, Spain, and the United States. Two others are at large in Costa Rica.\n\nThe Liberty Reserve website was taken offline on May 24 and replaced with a notice saying the domain had been \"seized by the United States Global Illicit Financial Team.\" In Costa Rica, a court order was issued to seize the \"financial products and services\" of Budovsky, Maxim Chukharev, and the six apparent shell companies. More than a million dollars of luxury automobiles alone were seized.\n\nThe indictment, unsealed on May 28, charges the seven principal employees, as well as Liberty Reserve itself, with money laundering and operating an unlicensed money transmitting business, and seeks $25 million in damages. The charges were leveled using a provision of the Patriot Act, since Liberty Reserve was not an American company. The accused could face up to 30 years in prison.\n\nPreet Bharara, a United States prosecutor working on the case called Liberty Reserve a \"black market bank\", created and structured to \"facilitate criminal activity\". In total, Liberty Reserve \"processed an estimated 55 million separate financial transactions and is believed to have laundered more than $6 billion in criminal proceeds\". It has been linked to crimes including credit card fraud, identity theft, investment fraud, computer hacking, child pornography and narcotics trafficking.\n\nLiberty Reserve itself was accused by U.S. prosecutors of moving tens of millions of dollars through shell accounts. Forty-five bank accounts were seized or restricted by United States federal prosecutors under the Patriot Act as a result of the investigation. The U.S. attorney in Manhattan, Preet Bharara, stated the case \"may be the largest international money laundering case ever brought by the United States.\" \"The global enforcement action we announce today is an important step towards reining in the 'Wild West' of illicit Internet banking,\" Bharara said; \"As crime goes increasingly global, the long arm of the law has to get even longer, and in this case, it encircled the earth.\"\n\nOne specific allegation of the prosecutors is that the site played a role in laundering the $45 million stolen from the Bank of Muscat and the National Bank of Ras Al Khaimah in May 2013.\n\nAccording to Internet security analyst Brian Krebs, the closure of Liberty Reserve had the potential to \"cause a major upheaval in the cybercrime economy\". The closure of the site led to many individuals using the service for legitimate reasons losing access to their money. The head of EPay Tarjeta, a service which used Liberty Reserve, remarked \"We seem to be acceptable collateral damage ... we have committed no crime.\"\n\nUnited States attorney Preet Bharara stated users of Liberty Reserve could contact his office to inquire about getting their funds returned.\n\nOn May 29, Budovsky's wife came forward with an accusation that she had been paid $800 to marry him in 2010 so that he could become a Costa Rican citizen. According to her, the plan was to divorce two years later, although the couple were still married at the time of his arrest.\n\nIn December 2014, chief technology officer Mark Marmilev was given the maximum sentence of five years after pleading guilty to operating an \"unlicensed money transmitting business.\"\n\nIn January 2016, Arthur Budovsky pleaded guilty to one count of conspiring to commit money laundering.\nOn May 6, 2016, Budovsky was sentenced to 20 years in prison.\n\n\n"}
{"id": "36039340", "url": "https://en.wikipedia.org/wiki?curid=36039340", "title": "List of Xbox 360 retail configurations", "text": "List of Xbox 360 retail configurations\n\nThe Xbox 360 video game console has appeared in various retail configurations during its life-cycle. At its launch, the Xbox 360 was available in two retail configurations: the morning \"Xbox 360\" package (unofficially known as the 20 GB Pro or Premium), priced at US$399.99 or £279.99, and the \"Xbox 360 Core,\" priced at US$299.99 and £209.99. The original shipment of Xbox 360s included a cut-down version of the Media Remote as a promotion. The Elite package was launched later at a retail price of US$479.99. The \"Xbox 360 Core\" was replaced by the \"Xbox 360 Arcade\" in October 2007 and a 60 GB version of the Xbox 360 Pro was released on August 1, 2008. The Pro package was discontinued and marked down to US$249.99 on August 28, 2009 to be sold until stock ran out, while the Elite was also marked down in price to US$299.99.\n\nIn June 2010, Microsoft announced a new, redesigned, model and the discontinuation of the Elite and Arcade models.\n\nTechnically designated the Xbox 360 S, commonly known as the Xbox 360 Slim, and marketed simply as the Xbox 360; current Xbox 360 consoles are based on a redesign of the Xbox 360 hardware which was officially announced on June 14, 2010 during a press briefing prior to that year's E3. It was speculated that a complete redesign of the Xbox 360 hardware was being produced after pictures of a possible new motherboard design surfaced on March 17, 2010. Ads later surfaced on June 13, 2010 showing a slimmer Xbox 360 design, which was expected to include a 250 GB hard drive and integrated Wi-Fi functionality.\n\nThe console's casing is revised in comparison to the previous models, with a glossy black finish and capacitive power and eject buttons. The internal hardware was redesigned using a new motherboard codenamed \"Valhalla\", which integrates the CPU, GPU, and eDRAM into a single package using a 45 nm fabrication process. As the CPU and GPU are integrated into the same die, they may also share the same heatsink and fan, which reduces the console's noise output, and its power consumption by roughly half in comparison to the original Xenon motherboard. The S has two additional rear USB ports, as well as a proprietary port used to connect the Kinect sensor. \n\nThe motherboard has an integrated 2.4 GHz 802.11 b/g/n Wi-Fi adapter, and a TOSLINK S/PDIF optical audio connector. The S no longer includes Memory Unit slots; USB drives can alternatively be used to expand storage. The external hard disk drive connector has also been swapped for an internal bay for use with a proprietary hard drive. The hard drive bay is designed such that a specially formatted 2.5' hard drive may be loaded in. Data can be transferred from a previous console using a USB transfer cable sold separately. If removed from its casing, a hard drive from a previous generation 360 can be implanted into the drive bay instead of purchasing a hard drive branded for use with the new model.\n\nUnlike previous generations of the console which had names to distinguish different SKUs, the new models' retail units were branded by their internal storage capacity in a similar fashion to current models of its main competitor the PlayStation 3. When the first new models began to ship, remaining stock of the Elite package dropped in price to US$249.99 or A$349 and the Arcade dropped to US$149.99.\n\nThe first Xbox 360 S SKU revealed included a 250 GB hard drive and its casing featured a glossy black finish. It was shipped to US retailers the same day it was announced () and went on sale later that week. It was released in Australia on , in New Zealand on and in Europe on . It retails at US$299.99, £199.99, A$449.00, NZ$499.00 or €249.00, replacing the Xbox 360 Elite at that price point.\n\nIn August 2011, Microsoft announced they would be streamlining their models by discontinuing the glossy finish and that future 250 GB consoles would use the matte finish found on 4 GB models.\n\nA second SKU which included 4 GB of internal flash storage and had a matte black casing (much like the Xbox 360 Elite) was released on in the US and in Europe. It replaced the Xbox 360 Arcade and is priced at US$199.99, £149.99 or €199.99. Although this model has on-board storage, \"Xbox Product Director\" Aaron Greenberg confirmed that it does have a drive bay which Microsoft has \"the opportunity to use in the future\". On , Microsoft announced a 250 GB stand-alone hard drive for use with Xbox 360 S models priced at US$129.99.\n\nIn June 2011, Microsoft announced a \"Jon Jones Edition\" Xbox 360 S console to coincide with the launch of Gears of War 3, which featuring custom finish, a 320 GB hard drive and sounds from the Gears of War 3 game which are played when the console is switched on or the disc tray is opened.\n\nOther 320 GB Xbox 360 S limited editions soon followed. Like the 250 GB \"Super Elite\" consoles, 320 GB Xbox 360 S consoles were only available as part of limited/special edition console bundles (), with stand-alone 320 GB hard drives also being available for purchase.\n\nAt Microsoft's E3 press conference on June 10, 2013, another hardware revision of the Xbox 360 known as E was unveiled for immediate availability. It is a revision of the S model with a new Xbox One-inspired casing, carrying a more rectangular appearance and a two-tone color scheme. It has one fewer USB port and no S/PDIF, YPbPr component or S-video connections.. Its internal hardware is otherwise similar to the S model. SKUs and pricing for the new model are identical to those of the previous model.\n\nThe Xbox 360 Core was an entry level Xbox 360 which was later replaced with the \"Arcade\". Although available at launch in other regions, it was not available in Japan until November 2, 2006. The Core system came bundled with a composite video cable, capable of only SDTV resolutions. The console was however capable of the same HDTV resolutions (up to 1080i) as the other models when connected to a separately sold component/d-terminal cable. In October 2006, 1080p support was added for all models in a system update, including the \"Core\" using either the component/d-terminal cable, or the new VGA cable (although 1080p via component was not widely supported by televisions). It may also utilize a separately sold Xbox 360 hard drive. Unlike all other SKUs, it shipped with a wired version of the Xbox 360 controller, instead of the wireless version found in other SKUs.\n\nThe Xbox 360 (sometimes referred to as Pro or Premium and packaged as simply Xbox 360 with the subheading \"Go Pro\") included all the features of the Xbox 360 Core and included a hybrid composite/component cable with optional optical out instead of the composite AV cable included with the Core. This model also included a detachable hard disk drive (initially 20 GB, while later models had 60 GB) to store downloaded content, provide compatibility with original Xbox games, and store game data. The included hard drive came with game demos, video clips and a free Live Arcade game, \"Hexic HD\". In July 2007, this version of the Xbox 360 began appearing with the Zephyr motherboard (the motherboard used in the Elite) which features HDMI 1.2 output and an improved GPU heatsink. Although this model did include an HDMI output, it did not come with an HDMI cable. Starting at the end of September 2007, the newest systems were shipped with the new \"Falcon\" motherboard. This motherboard includes the new 65 nm CPUs, making them quieter and cooler than the older systems. On August 1, 2008, the 20 GB version was discontinued and was replaced by a 60 GB HDD model at the same price. Holiday 2008 consoles were bundled with \"\" and \"Kung Fu Panda\". Price cuts that took effect on September 4, 2008 reduced the price from $349 to $299. The Xbox 360 configuration, following its discontinuation, retailed for $250 until stocks were exhausted.\n\nThe Xbox 360 Elite included a 120 GB hard drive and a matte black finish. The Elite retail package also included a controller and headset that match the system's black finish. The initial release price was $479.99 USD, C$549.99, £299.99, and A$729.95. The Elite was released in North America on April 29, 2007, Europe on August 24, 2007, and Australia on August 30, 2007. These Elites (and other Xbox 360 models using the Falcon) can be identified from earlier versions by a re-designed power connector and a power supply rated to 175 W. In 2009, Elite models using the Jasper chipset became available. These can also be identified by their power supply, which is rated at 150W and has a 12.1A 12v rail. Christmas 2008 consoles were bundled with \"\" and \"Kung Fu Panda\". The Elite's price tag was cut from $449 to $399 on September 4, 2008. With the announcement of the new Xbox 360 250 GB model, the Elite dropped in price to US$249.99 for remaining units until stocks were exhausted.\n\nThe Xbox 360 Arcade replaced the Xbox 360 Core as the entry-level Xbox 360 on , while retaining the Core's price of US$279.99. It was publicly revealed by Microsoft's president of Entertainment Devices division Robbie Bach to the \"Financial Times\" on October 18, 2007, and officially announced on October 22, 2007, although it was available in stores far earlier. It included a wireless controller, a composite AV cable, HDMI 1.2 output, a 256 MB memory unit and five Xbox Live Arcade titles: \"Boom Boom Rocket\", \"Feeding Frenzy\", \"Luxor 2\", \"Pac-Man Championship Edition\", and \"Uno\" on a single disk, which also included a \"Welcome Video\" and several game trailers and demos. Like its predecessor the \"Core\", it did not include a hard disk drive, which is required for Xbox software backwards compatibility. In Autumn (Fall) 2008, with the introduction of the Jasper motherboard revision, the memory unit was removed from the package and replaced with a 256 MB internal memory chip. This was later upgraded to a 512 MB chip in Summer 2009. Holiday 2008 consoles were bundled with \"Sega Superstars Tennis\". With the price cuts on , the Arcade fell from US$279 to US$199 in the US. In the UK, with the 2009 Elite price drop and discontinuation of the \"Premium\" Pro SKU, the Arcade price rose from £129.99 to £159.99. With the unveiling of the Xbox 360 S redesign, the Arcade dropped in price to US$149.99 for remaining units until stocks are exhausted. The Arcade was replaced at the US$200 price tier by the 4 GB Xbox 360 S.\n\nThe Xbox 360 Elite has also been configured with a 250 GB hard disk drive and two wireless controllers on special limited editions of the console. Also referred to as the Xbox 360 Super Elite, the console retailed at US$399.99, £249.99 and A$599.00 as of November 10, 2009. A \"\" bundle included a special limited-edition black console featuring Modern Warfare 2 branding. Other bundles included a standard Elite finish and either the game \"Forza Motorsport 3\" or both \"Halo 3\" and \"\". On March 9, 2010, alongside the release of \"Final Fantasy XIII\", Microsoft released another 250 GB bundle with the same extras as the Modern Warfare bundle except bundled with the \"Final Fantasy XIII\" game. Unlike other Super Elites, this console and its accessories featured the same white color scheme as \"Pro\" models. This bundle also included exclusive downloadable items for use with Xbox 360 avatars and a specially branded 250 GB hard drive. In April 2010, a \"\" Super Elite bundle featuring the same extras as the other bundles was released alongside the \"Splinter Cell: Conviction\" game.\n\nInformation is based on current specifications for standard packages. Older or holiday packages may differ from current configurations.\n\nOn a few occasions, Microsoft has produced special editions of the console, usually to coincide with the release of a major product. These special editions are typically custom-colored Xbox 360 models, and are produced in limited numbers.\n\n\n\n\n\nA white console with green accents was released in 2005 only to the Launch Team as a gift from Microsoft. The consoles came complete with a 20 GB HDD also in green to match the top and bottom sections that are typically grey in color. The HDD plate was also personalized and engraved with the team member's gamertag. Few examples have been seen with simply the console release date November 22, 2005 engraved on the HDD plate. The consoles came with a limited issue controller to accompany the console. The difference with the controller again is the grey part on the controller is replaced with a green molded plastic, not painted contrary to popular belief, to match the console. Each console came with a plain white faceplate. However, as an additional gift, each team member was given an additional packaged faceplate with one of a kind art with the caption \"I Made This\" on the USB door of the faceplate. Very few examples have been sold off from original team member's collection. Larry Hryb, better known as \"Major Nelson\", is known to own one which he displays pictures of on his website having been a member of the Launch Team. The special edition Launch Team console, hard drive, controller and the special faceplate were never sold in stores or meant for the general public. It is unknown yet how many of these very rare consoles exist today.\n\nAs with the original Xbox, Microsoft has continued bundling two video game titles in console retail packaging during the holiday season. During the holidays of 2007, the Xbox 360 Pro and Elite packages were bundled with \"Forza Motorsport 2\" and \"\". In the UK, Ireland and the Netherlands, \"Forza Motorsport 2\" was bundled with \"Viva Piñata\".\n\nHoliday 2008 Xbox 360 and Xbox 360 Elite packages were bundled with \"\" and \"Kung Fu Panda\", while Arcade consoles were bundled with \"Sega Superstars Tennis\". Holiday 2009 packages included an Elite console, LEGO Batman and Pure. Holiday 2010 bundles included a 250 GB Xbox 360 S console, Forza Motorsport 3 and a voucher to download Alan Wake from the Xbox Live Marketplace.\n\nTwo bundles were available in the 2011 holiday season. The first, which retailed for US$399.99, contained a 250 GB Xbox 360 S, Kinect sensor, a copy of Kinect Adventures, a download voucher for Carnival Games: Monkey See Monkey Do and 3 months of Xbox Live Gold. The second, which retailed for US$299.99 contained a 250 GB Xbox 360 S, Fable III, a download voucher for , and 3 months of Xbox LIVE Gold.\n\nDuring the 2012 holiday season, Microsoft released several Xbox 360 S bundles at temporarily discounted prices, including a 4 GB Xbox 360 console with a Kinect sensor, \"Kinect Adventures\", and \"Kinect Disneyland Adventures\" for US$249.99; and a 250 GB Xbox 360 console with a physical copy of \"Forza Motorsport 4 (Essentials Edition)\" and a digital download voucher for \"\" for US$249.99.\n\nFor the 2013 holiday season, there were three Xbox 360 S bundles available, with each including one month of Xbox Live Gold. The 250GB Kinect Holiday Value Bundle included \"\", \"Kinect Adventures!\", and a download voucher for \"Forza Horizon\". The 4GB Kinect Holiday Value Bundle included \"Kinect Sports: Season Two\" and \"Kinect Adventures!\". The 250GB Holiday Value Bundle included \"Halo 4\" and a download voucher for \"Tomb Raider\".\n\nIn 2014, three Xbox 360 E bundles were sold, all with one month of Xbox Live Gold included. The 500GB Holiday Value Bundle included \"\" and a download voucher for \"\". A similar bundle, with a blue console and controller, was sold at Walmart. A 4GB Kinect Bundle was sold at Target, including \"Kinect Adventures!\", \"Kinect Sports\", and \"Forza Horizon\".\n"}
{"id": "53494176", "url": "https://en.wikipedia.org/wiki?curid=53494176", "title": "List of largest pharmaceutical companies by revenue", "text": "List of largest pharmaceutical companies by revenue\n\nThe following is a list of the top independent pharmaceutical & biotechnology companies ranked by their revenue generated ($10 billion) in the respective financial year, it does not include biotechnology companies that are now owned by, or part of, larger pharmaceutical groups.\n\nThe change is the companies relative position in this list, compared to their relative position in the prior year; i.e., an increase would be moving closer to rank 1 and vice versa. Green cells indicate years where revenue increased compared to the previous year, red cells indicate those years where there has been a decrease.\n\n"}
{"id": "2999708", "url": "https://en.wikipedia.org/wiki?curid=2999708", "title": "Ludicorp", "text": "Ludicorp\n\nLudicorp was the company, based in Vancouver, British Columbia, Canada, that created Flickr and Game Neverending. It was founded in 2002 by Stewart Butterfield, Caterina Fake and Jason Classon and was bought by Yahoo! in 2005.\n\nTheir team consisted of:\n\n\n"}
{"id": "3644442", "url": "https://en.wikipedia.org/wiki?curid=3644442", "title": "Moon pool", "text": "Moon pool\n\nA moon pool is a feature of marine drilling platforms, drillships and diving support vessels, some marine research and underwater exploration or research vessels, and underwater habitats, in which it is also known as a wet porch. It is an opening in the floor or base of the hull, platform, or chamber giving access to the water below, allowing technicians or researchers to lower tools and instruments into the sea. It provides shelter and protection so that even if the ship is in high seas or surrounded by ice, researchers can work in comfort rather than on a deck exposed to the elements. A moon pool also allows divers or small submersible craft to enter or leave the water easily and in a more protected environment.\n\nMoon pools can be used in chambers below sea level, especially for the use of scuba divers, and their design requires more complex consideration of air and water pressure acting on the moon pool surface.\n\nMoon pools originated in the oil drilling industry, which uses them in drilling at sea or in lakes, to pass drilling equipment into the water from a platform or drillship. Drill pipes need to run vertically through the structure or hull and the moon pool provides the means to do this.\n\nVery deep moon pools are used in underwater habitats—submerged chambers used by divers engaged in underwater research, exploration, marine salvage, and recreation. In this case, shown in part D of the diagram, there is no dry access between the chamber and the sea surface, and the moon pool is the only entry or exit to the chamber. Submerged chambers provide dry areas for work and rest without the need to ascend to the surface. This kind of submerged chamber uses the same principles as the diving bell, except they are fixed to the seafloor, and may be called a wet porch, wet room, or wet bell. Sometimes the term moon pool is used to mean the complete chamber, not just the opening in the bottom and the air–water interface.\n\nThe alternative to a moon pool in an underwater habitat is the lock-out chamber, which is essentially like a fixed submarine, maintaining internal air pressures lower than ambient sea pressure down to 1 atmosphere, with an airlock to enable entry and exit underwater. Underwater habitats may have connected chambers with moon pools and lock-out chambers.\n\n\n"}
{"id": "24833024", "url": "https://en.wikipedia.org/wiki?curid=24833024", "title": "Nanofluidic circuitry", "text": "Nanofluidic circuitry\n\nNanofluidic circuitry is a nanotechnology aiming for control of fluids in nanometer scale. Due to the effect of an electrical double layer within the fluid channel, the behavior of nanofluid is observed to be significantly different compared with its microfluidic counterparts. Its typical characteristic dimensions fall within the range of 1–100 nm. At least one dimension of the structure is in nanoscopic scale. Phenomena of fluids in nano-scale structure are discovered to be of different properties in electrochemistry and fluid dynamics.\n\nWith the development of microfabrication and nanotechnology, the study of microfluidics and nanofluidics is drawing more attention. Research on microfluidic found its advantages in DNA analysis, lab-on-a-chip, and micro-TAS. Devices in a microfluidic system include channels, valves, mixers, and pumps. Integration of these microfluidic devices enables sorting, transporting, and mixing of substances within fluids. However, the failure of moving parts in these systems is usually the critical issue and the main drawback. Mechanisms to control flow without using mechanical parts are always desired for reliability and lifetime.\n\nIn 1997, Wei and Bard discovered that ion rectification occurs at the tip of a nano-sized pipe. They observed that the surface charge at the wall of a nano-pipet induced a non-neutral electrical potential within the orifice. The electrical potential then modifies the concentration of ion species, resulting in an asymmetric current-voltage characteristic for the current through the pipet.\n\nTransport of ions in the electrolyte can be adjusted by tuning the pH value in a dilute ionic solution, or by introducing an external electrical potential to change the surface charge density of the wall. As an analogy to semiconductor devices, the mechanism to control charge carrier transport in electronic devices was established in the area of nanofluidics. In nanofluidics, the active control of ion transport is realized using nano-scale channels or pores.\n\nResearch efforts on micro-scaled fluidic systems started to focus on the rectifying phenomena, which can be seen only in nano-scaled systems. In 2006, Professor Majumdar and Professor Yang in University of California, Berkeley built the first \"nanofluidic\" transistor. The transistor can be turn on or off by an external electrical signal, allowing the control of ionic fluids in a nano-scaled channel. Their work implies a possibility to create a nanofluidic circuitry with logic functions.\n\nThe main researchers in the area of nanofluidic devices include Arun Majumdar and Peidong Yang in University of California - Berkeley, Harold Craighead and Brian Kirbyat Cornell University, Juan Santiago at Stanford University, Albert van den Berg in University of Twente, Zuzanna Siwy in University of California - Irvine, and Mark Shannon in University of Illinois - Urbana-Champaign.\n\nFor electrolyte solution in a channel with a macro- or micro-scaled radius, surface charges at the wall attract counterions and repel co-ions due to electrostatic force. Therefore, an electrical double layer exists between the wall of channel and the solution. The dimension of the electrical double layer is determined by the Debye length in this system, which is typically much smaller than the channel radius. Most of the solution in the channel is electrically neutral due to the shielding effect of the electrical double layer.\n\nIn a nanochannel, however, the solution is charged when the dimension of channel radius is smaller than the Debye length. Therefore, it is possible to manipulate the flow of ions inside the nanochannel by introducing surface charges on the wall or by applying an external electrical potential.\n\nIonic concentration of solution has an important effect on the ion transport. Because a higher concentration leads to a shorter Debye length for the electrical double layer at the channel wall. Its rectifying effect decreases with increasing ionic concentration. On the other hand, ion rectification can be improved by having a dilute solution.\n\nTo analyze the transport of ions in the channel, behaviors of system in electrochemistry as well as fluid mechanics need to be considered. The Poisson–Nernst–Planck (PNP) equations are utilized to describe ionic current flowing through a channel, and the Navier–Stokes (NS) equations are used to represent the fluid dynamics in the channel.\n\nThe PNP equations consist of the Poisson equation:\n\nformula_1\n\nand the Nernst–Planck equations, which gives the particle flux of ion species formula_2 due to a concentration gradient and electric potential gradient:\n\nformula_3\n\nwhere formula_4 is the electrostatic potential, formula_5 is the unit charge of electron, formula_6 is the permittivity in vacuum, and formula_7 is the dielectric constant of solution; formula_8, formula_9 and formula_10 are the diffusivity, the number density of ions, and the valence of ion species formula_2.\n\nThe solution in steady-state satisfies the continuity equation. To describe fluid velocity field in the channel, using Navier–Stokes equations:\n\nformula_12\n\nformula_13\nformula_14\n\nwhere formula_15, formula_16, formula_17, and formula_18 are pressure, velocity vector, viscosity, and density of fluid, respectively. The equations above are usually solved with numerical algorithm to determine the velocity, pressure, electric potential, and ionic concentration in the fluid, as well as the electric current flow through the channel.\n\nIonic selectivity is defined to evaluate the performance of a nano-channel for ionic flow control. Ionic selectivity is the ratio of the difference in currents of majority and minority carriers to the total current carried by both positive and negative ions, formula_19. For a nanochannel with perfect control over cation and anion, the selectivity is unity. For a nanochannel without ionic flow control, the selectivity is zero.\n\nformula_20\n\n\nNanofluidic diodes are utilized for rectification of ionic transport. A diode in electronic circuits limits the flow of electric current to one direction. A nanofluidic diode has the same function to restrict the ionic flow in one direction. A nanofluidic diode is a channel with its radius dimension of several nanometers. The inner surface of the channel is coated with surface charges. Current rectification can occur when the surface charges at the wall are of the same sign. It is also observed that, when a half of the channel is coated with opposite sign or electrically neutral, the rectification will be enhanced.\n\nWhen the wall of the channel is coated with positive charges, the negative charged ions in the electrolyte will be attracted and accumulated within the channel. In this case, the flow of positive charges passing through the channel is not favorable, resulting in a decrease in ionic current. Therefore, the ionic current becomes asymmetric if the biasing voltage is reversed.\n\nBy applying an additional electrode on a nanochannel as the gate electrode, it is possible to adjust the electrical potential inside the channel. A nanofluidic field-effect transistor can be made of silica nanotubes with an oxide as the dielectric material between the metal gate electrode and the channel. The tuning of the ionic current, therefore, can be achieved by changing the voltage applied on the gate. The gate bias and the source-drain bias are applied to adjust the cation and anion concentration within the nanochannel, therefore tuning the ionic current flowing through it.\n\nThis concept is an analogy to the structure of a metal-oxide semiconductor field-effect transistor (MOSFET) in electronic circuits. Similar to a MOSFET, a nanofluidic transistor is the fundamental element for building a nanofluidic circuitry. There is possibility to achieve a nanofluidic circuitry, which is capable of logic operation and manipulation for ionic particles.\n\nSince the conductance of ionic current flow is controlled by the gate voltage, using a material with high dielectric constant as the wall of the channel is desired. In this case, there is a stronger field seen within the channel due to a higher gate capacitance. A channel surface with a low surface charge is also desired in order to strengthen the effect of potential tuning by gate electrode. This increases the ability to spatially and temporally tune the ionic and electrostatic environment in the channel.\n\nBy introducing an asymmetric field effect along the nanochannel, a field-effect reconfigurable nanofluidic diode is feasible, which features post-fabrication reconfiguration of the diode functions, such as the forward/reverse directions and the rectification degrees. Unlike the nanofluidic field-effect transistor, where only the amount of ions/molecules is regulated by an electrostatic potential, the field-effect reconfigurable diode can be used to control both directions and magnitudes of ion/molecule transport. This device could be deemed as the building blocks for ionic counterpart of the electronic field-programmable gate array.\n\nIonic bipolar transistors can be made from two conical channels with the smallest opening in nano-scaled dimension. By introducing opposite surface charges at each side, it is able to rectify ionic current as an ionic diode. An ionic bipolar transistor is built by combining two ionic diodes and forming a PNP junction along the inner surface of the channel. While the ionic current is from emitter end to collector end, the strength of the current can be modulated by the base electrode. The surface charge at the channel wall can be modified using chemical methods, by changing the electrolyte concentration or pH value.\n\nNanofuidic triode is a three-terminal double junction nanofluidic device composed of positive-charged alumina and negative-charged silica nanochannels. The device is essentially a three-terminal bipolar junction transistor. By controlling the voltage across emitter and collector terminals, one can regulate the ion current from base terminal to one of the other two terminals, functioning as an ionic single-pole, double-throw switch.\n\nWhen surface charges present at the wall of a channel of micro-scaled width, counterions are attracted and co-ions are repelled by electrostatic force. The counterions form a shielding area near the wall. This region penetrate into solution to a certain distance called Debye length until the electric potential decays to the bulk value of neutrality. The Debye length is ranging typically from 1 nm to 100 nm for aqueous solutions.\n\nIn nano-channels, the Debye length is usually comparable with the channel width, therefore solution within the channel is charged. Ions inside the fluid is no longer shielded from surface charge. Instead, surface charge affect the dynamics of ions within a nano-channel.\n\nIt requires a channel to be narrow and long for it to have a good selectivity. In other words, a channel with a high aspect ratio has a better selectivity. To further increase its selectivity, it is required to have a highly charged wall.\n\nThe performance of ionic selectivity also largely related to the applied bias. With a low bias, a high selectivity is observed. With the increase of the bias voltage, there is an apparent decrease in the selectivity. For a nanochannel with a low aspect ratio, high selectivity is possible when the bias voltage is low.\n\nThe advantage of nanofluidic devices is from its feasibility to be integrated with electronic circuitry. Because they are built using the same manufacturing technology, it is possible to make a nanofluidic system with digital integrated circuit on a single chip. Therefore, the control and manipulation of particles in the electrolyte can be achieved in a real-time.\n\nFabrication of nano-channels is categorized into top-down and bottom-up methods. Top-down methods are the conventional processes utilized in the IC industry and Microelectromechanical systems research. It begins with photolithography on a bulk silicon wafer. Bottom-up methods, in contrast, starts with atoms or molecules with intrinsic nano-scaled dimension. By organize and combine these building blocks together, it is able to form a nanostructures as small as only a few nanometers.\n\nA typical method of top-down fabrication includes photolithography to define the geometry of channels on a substrate wafer. The geometry is created by several thin-film deposition and etching steps to form trenches. The substrate wafer is then bonded to another wafer to seal the trenches and form channels. Other technologies to fabricate nano-channels include surface micromachining with sacrificial layers, nano-imprinting lithography, and soft-lithography.\n\nThe most common method utilized for bottom-up fabrication is self-assembled monolayers (SAM). This method usually use biological materials to form a molecular monolayer on the substrate. Nano-channels can also be fabricated from the growth of carbon nanotubes (CNT) and quantum wires. The bottom-up methods usually give well-defined shapes with characteristic length about few nanometers. For these structures to be utilized as nanofluidic devices, the interconnection between nano-channels and microfluidic systems becomes an important issue.\n\nThere exist several ways to coat the inner surface with specific charges. Diffusion-limited patterning can be utilized because a bulk solution only penetrate the entrance of a nanochannel within a certain distance. Because the diffusion speed is different for each reactant. By introducing several steps of reactants flowing into the nanochannel, it is possible to pattern the surface with different surface charges inside the channel.\n\nNanofluidic devices have been built for application in chemistry, molecular biology and medicine. The main purposes to use nanofluidic devices are separation and measurement of solutions containing nanoparticles for drug delivery, gene therapy and nanoparticle toxicology on a micro-total-analysis system. An important advantage of micro- and nano-scaled systems is the small amount of sample or reagent used in analysis. This reduces the time required for sample processing. It is also possible to achieve analysis in an array, which further speeds up processes and increases throughput of analysis.\n\nNanochannels are utilized to achieve single-molecule sensing and diagnosis, as well as DNA separation. In many cases, nanofluidic devices are integrated within a microfluidic system to facilitate logic operation of fluids. The future of nanofluidic systems will be focused on several areas such as analytical chemistry and biochemistry, liquid transport and metering, and energy conversion.\n\nIn nanofluidics, the valence numbers of the ions determines their net electrophoretic velocities. In other words, the velocity of an ion in the nano-channel is related not only to its ion mobility but also its ion valence. This enables the sorting function of nanofluidics, which cannot be done in a micro-channel. Therefore, it is possible to do sorting and separation for short strand DNA by using a nanochannel. For the single-molecule DNA application, the final goal is to sequence a strand of genomic DNA in a reproducible and precise result. Similar application can also be found in chromatography, or separation of various ingredients in the solution.\n\nApplication also can be found in synthesis of fibers. Polymer fibers can be created by electrospinning the monomers at an interface between liquid and vacuum. An organized polymer structure is formed from a flow of monomers aligning on a substrate.\n\nThere is also an attempt to bring nanofluidic technology into energy conversion. In this case, the electrical charged wall behaves as the stator, while the flowing solution as the rotor. It is observed that when the pressure-driven solvent flowing through a charged nanochannel, it can generate a streaming current and a streaming potential. This phenomenon can be used in electrical energy harvesting.\n\nAdvances in nanofabrication techniques and concerns about energy shortage make people interested in this idea. The main challenge is to increase efficiency, which is now only a few percent, compared with efficiencies of up to about 95 per cent for standard rotational electromagnetic generators.\n\nRecent studies focus on the integration of nanofluidic devices into microsystems. An interface should be created for the connection between two length-scales. A system with solely nanofluidic devices standalone is impractical because it would requires a large driving pressure to make fluids flow into the nano-channel.\n\nNanofluidic devices are powerful in their high sensitivity and accurate manipulation of sample materials even down to a single molecule. Nevertheless, the drawback of nanofuidic separation systems is the relatively low sample throughput and its result in detection. One possible approach to deal with the problem is to use parallel separation channels with parallel detection in each channel. In addition, a better approach for detection needs to be created in view of the very small quantities of molecules present.\n\nOne of the biggest challenges in this research area are due to the peculiar size-effect. Researchers try to solve the problems caused by the extremely high surface-to-volume ratios. Under this condition, adsorption of molecules can lead to large losses and can also change the surface properties.\n\nAnother issue arises when the sample for detection is a relatively large molecule, such as DNA or protein. In the application for large molecule, clogging is a concern because the small size of the nanochannel makes it easy to happen. A low friction coating at inner surface of the channel is desired to avoid blocking of fluid channels in this application.\n\n\n"}
{"id": "11764275", "url": "https://en.wikipedia.org/wiki?curid=11764275", "title": "Orgalime", "text": "Orgalime\n\nOrgalime (derived originally from the French Organisme de Liaison des Industries Métalliques Européennes) is the European Engineering Industries Association, speaking for 40 trade federations representing companies in the mechanical, electrical, electronics and metalworking and metal articles industries of 23 European countries. The industry employs about 11 million people in Europe and represents some 28% of the output of manufactured products and a third of the manufactured exports of the European Union. Orgalime was elected Number 1 Manufacturing sector association in Brussels in 2015 and nominated in the top 5 Trade Associations in Brussels the same year. The Orgalime Director General was furthermore awarded the Association Leadership Award in 2017. Since November 2015, Tomas Hedenborg, CEO of Fastems, is the President of Orgalime. \n\nOrgalime was formally created in late 1954, therefore pre-dating the official European Union project. Founding associations came from Austria, Belgium, France, West Germany, Italy, the Netherlands, Switzerland, the UK, Sweden, Finland, Denmark and Norway. Meetings and informal collaboration between industries had begun in 1948, and although initially created as an informal club without any financial demands, the organisation became increasingly structured and eventually developed a secretariat in the early 1950s. Various other engineering groups had been created at the same time as the European Coal and Steel Community developed, such as MEFTA and COLIME. Orgalime members decided in 1960 to incorporate the groups into Orgalime as working groups.\nOrgalime has since been registered on the Transparency Register operated jointly by the European Commission and the European Parliament. \n\nOrgalime represents a large distribution of industry in Europe. The engineering industry is the largest industrial branch in the EU, with a turnover of some €2,000 billion in 2016. The industry furthermore accounts for over a quarter of manufacturing output and a third of the manufactured exports of the European Union. \n\nOrgalime works in a multi-layered manner. \n\nThe majority (90%) of the companies represented are small and medium-sized enterprises covering a broad industry cross-section in terms of product, market segment and geographical spread. Whilst a significant part of the output of the engineering industries are either capital goods or destined for Business-to-business transactions, there is also a large consumer goods output, particularly in the electrical and electronic branch.\nOrgalime is part of an extensive network at European level which includes the Confederation of European Business (BUSINESSEUROPE), other branch federations, a sister organisation CEEMET representing the employers in the engineering industry, as well as a number of individual European sector committees/associations representing the interests of individual engineering products or product lines.\n\nAs part of its service, Orgalime publishes a range of standard conditions of contract for:\n\nDue to the nature of the companies and sectors that Orgalime represents, its secretariat works on a large number of issues.\nThere are several departments all working on respective topics, including:\n\n\nWith the advent of Industry 4.0 (also known as the Fourth Industrial Revolution), Orgalime has been vocal about the digitisation of industry in Europe. The association has provided input on the European Union’s current strategy of a Digital Single Market. The association has furthermore called on the European Institutions to develop an industrial strategy - a call that was coordinated with some 130 European industry associations in 2016 and 2017. President of the Commission, Jean-Claude Juncker announced in his State of the Union (European Union) Speech on Wednesday 13 September 2017 that an industrial strategy would be implemented. \n\nOrgalime has worked closely on the trade issues, notably the recent CETA agreement as well as TTIP. Orgalime has strongly supported EU trade strategies with a focus on shaping globalisation and improving competitiveness of the EU’s industrial base. \n\nThe association has notably worked on proposals for a more resource efficient Europe, encouraging the use of low carbon, energy efficient, environmentally friendly, and overall sustainable energy systems. \nOrgalime closely monitors the development of the European Horizon 2020 programme and has already organised debates concerning the Future Framework Programme 9. Innovation and research have been prioritised by Orgalime in its messages to the European Institutions. \n\nIn 2014, Orgalime launched a Vision Paper ‘Technology for the World – Manufactured in Europe’ to present the European Commission and the European Parliament with Orgalime's vision for the future of the engineering industry. The Vision Paper was updated in 2016 to coincide with the Commission's mid-term review of its framework programme. \n"}
{"id": "24663426", "url": "https://en.wikipedia.org/wiki?curid=24663426", "title": "PICMG 2.4", "text": "PICMG 2.4\n\nPICMG 2.4 is a specification by PICMG that standardizes user IO pin mappings from ANSI/VITA standard IP sites to J3/P3, J4/P4, and J5/P5 on a CompactPCI backplane.\n\nAdopted : 9/9/1998\n\nCurrent Revision : 1.0\n"}
{"id": "46450132", "url": "https://en.wikipedia.org/wiki?curid=46450132", "title": "Procedural design", "text": "Procedural design\n\nSoftware Procedural Design (SPD) converts and translates structural elements into procedural explanations. SPD starts straight after data design and architectural design. This has now been mostly abandoned mostly due to the rise in preference of Object Oriented Programming and design patterns\n"}
{"id": "1832854", "url": "https://en.wikipedia.org/wiki?curid=1832854", "title": "Quarter stick", "text": "Quarter stick\n\nA quarter stick is a large firecracker that falls within a certain range of dimensions. Typically, a quarter stick consists of a thick walled cardboard tube containing approximately 1 oz. (28.4 grams) of pyrotechnic flash powder, with a short length of Visco fuse protruding from the side or end of the device. No true standard for dimensions and construction exists, as these devices are products of bootleg manufacturers.\n\nThe term \"quarter stick\" is based on a quarter-stick of dynamite, which it somewhat resembles. However, quarter stick firecrackers do not contain nitroglycerin as dynamite does, and have far less explosive power.\n\nIn the United States, quarter sticks and similar large firecrackers are illegal to manufacture or possess without a BATFE High Explosives Manufacturing License.\n\nThey are sometimes colloquially known as M-1000s or \"block busters\".\n\n\nhttps://web.archive.org/web/20120107185413/http://atf.gov/publications/factsheets/factsheet-illegal-explosives-devices.html\n"}
{"id": "50711232", "url": "https://en.wikipedia.org/wiki?curid=50711232", "title": "SIA S.p.A.", "text": "SIA S.p.A.\n\nSIA S.p.A. is an Italian company operating in the area of ICT, providing solutions and technologies to the banking and finance sector in addition to platforms for financial markets and e-payment services.\n\nThe company was founded in 1977 as Società Interbancaria per l'Automazione by Banca d'Italia, ABI and a pool of Italian banks. During the 1980s, SIA created the Rete Nazionale Interbancaria (RNI – national interbank network) and contributed to developing the interbank payments system in compliance with the White Paper on payment systems in Italy, published by Banca d’Italia in 1987.\n\nIn 1983 SIA launched Bancomat and in 1987 introduced POS payments.\n\nIn 1992, from a branch of SIA’s business, SSB - Società per i Servizi Bancari was born, a firm specializing in services in the field of electronic money. The new company worked on further developments in payment cards: Bancomat/Pagobancomat, FASTpay, borsellino elettronico (e-purse with the MINIpay product) and the Microcircuito project for the migration from magnetic stripe cards to microchip cards.\n\nIn 1999, SIA merged with Cedborsa, changing name to \"Società Interbancaria per l'Automazione - Cedborsa S.p.A.\", working on the automation of the Borsa Italiana markets and the launch of the Italian gross payments markets e-MID and MTS.\n\nIn 2003, SIA developed the interbank payments system in Romania, a condition for the country’s entry into the European Union.\n\nIn the early 2000s, SIA created and managed the technology platform for the STEP2 project, the first continental ACH (Automated Clearing House) for retail payments in Euro.\n\nIn the same period, SIA created the RTGS (Real Time Gross Settlement System) platform for the central banks of Sweden, Norway, Egypt and Palestine.\n\nIn May 2007, the merger between SIA and SSB gave birth to SIA-SSB S.p.A., a name simplified in 2011 to SIA S.p.A.\n\nIn 2012, in partnership with Colt, SIA was awarded the tender announced by the European Central Bank and becomes a licensed Network Service Provider appointed to create the network infrastructure connecting central securities depositaries (CSD), central banks in the Eurosystem and major bank groups at European level to TARGET2-Securities (T2S).\n\nA year later, together with the same partner, SIA won the tender announced by Deutsche Bundesbank (which was also operating on behalf of Banca d’Italia, Banque de France and Banco de España) to create the network infrastructure linking the four central banks charged with managing the single platform of the Eurosystem to settle large payments and securities transactions.\n\nOver the course of 2013, SIA incorporated its Belgian subsidiary SiNSYS, a company operating in the processing of payment cards, and acquired Emmecom, an Italian firm in the sector of fixed, mobile and satellite telecommunications networks.\n\nIn the area of mobile payments, during 2014 SIA launched a service called Jiffy for sending and receiving cash in real time to and from a user’s contacts by smartphone.\n\nAt the end of 2014, SIA incorporated its subsidiary RA Computer and the payments Gateway business of its TSP subsidiary. This led to the direct control of payment institution PI4PAY (effective from July 2011).\n\nIn January 2016, SIA acquired 69% of UBIQ, a startup born in 2012 from a spin-off of Parma University, specialized in designing and developing innovative technological solutions, particularly in the field of promotions, where it operates with the brand Ti Frutta.\n\nIn April 2016, Reserve Bank of New Zealand (RBNZ), New Zealand’s central bank, chose SIA to develop the new Real-Time Gross Settlement (RTGS) system, to create the new domestic interbank payments system, replacing the Exchange Settlement Account System (ESAS).\n\nOn June 2, 2016, SIA and Raphaels Bank, an issuing bank known for enabling innovation in payments, have agreed to a partnership agree for the development and launch of payment solutions in the UK and throughout Europe.\n\nIn August 2016, ČSOB, one of the largest commercial banks in the Czech Republic and part of the Belgian KBC Group, and SIA launched the first mobile wallet for NFC payments in the Czech Republic that supports both the MasterCard and VISA circuits.\n\nDuring the same month, Unicredit Business Integrated Solutions (UBIS), a company in the Unicredit Group, and SIA signed an agreement for the sale to the latter – for the sum of €500 million – of the processing activities of around 13.5 million payment cards and the management of 206,000 POS terminals and 12,000 ATMs in Italy, Germany and Austria. UBIS also signed with SIA a ten-year outsourcing contract for the supply of processing services for transactions made using debit, credit and prepaid cards, and for the management of POS and ATM terminals.\n\nIn the final part of 2016, SIA launched a series of partnerships in the e-payments market: American Express Italia has chosen SIA to launch a completely digital and paperless service to support the request for new credit cards. This project uses authentication and digital signature systems to request a card in paperless mode. Moreover, Friday 23 December 2016 saw the completion of the acquisition by SIA of the processing activities of around 13.5 million payment cards and the management of 206,000 POS terminals and 12,000 ATM terminals in Italy, Germany and Austria from Unicredit Business Integrated Solutions (UBIS), a company in the Unicredit Group, for the sum of €500 million. Following this agreement, SIA S.p.A. on January 1, 2017 established two new companies: P4cards S.r.l., based in Verona, and Pforcards GmbH, based in Vienna, to manage the payment cards processing activities.\n\nIn the spring of 2017 the Central Bank of Iceland (CBI) has chosen SIA to implement and support the new real-time gross settlement system (RTGS) and the new instant payment platform. These technology infrastructures developed by SIA, planned to go live in 2018, will replace CBI’s current mainframe-based real-time solutions for high and low-value payment systems, which have been operating since 2001. Central Bank of Iceland manages all interbank payments in the country. Despite the small population, it processes a quite significant daily volume of transactions: up to 1 million payments with a peak of 160,000 per hour.\n\nIn May, Thomson Reuters has launched the “SIABookbuilding” application on its flagship desktop platform Eikon, offering sell-side professionals a new fully integrated application for the IPO syndication and distribution process. SIABookbuilding is available through App Studio, Eikon’s third-party development platform.\n\nA couple of weeks later, Poste Italiane and SIA have signed a deal that allows holders of debit cards and Postepay cards to use the Extra Sconti App, which is based on a cash-back mechanism to credit consumers' postal current accounts for supermarket purchases of brands recommended by the Extra Sconti App.\n\nOn July 27, SIA celebrated its first 40 years of existence..\n\nOn May 25th 2018, SIA and First Data Corporation have signed an agreement for SIA to acquire First Data’s card processing businesses in parts of Central and Southeastern Europe for €375 million. In 2017, these businesses generated a combined revenue of approximately €100 million for First Data. This acquisition by SIA provides card processing, card production, call center and back-office services, including 13.3 million payment cards, 1.4 billion transactions, in addition to the management of POS terminals and ATMs. These businesses are primarily located in 7 countries: Greece, Croatia, Czech Republic, Hungary, Romania, Serbia and Slovakia.\n\nOn November 29th 2018 the Board of Directors of SIA, meeting under the chairmanship of Giuliano Asperti, appointed Nicola Cordone to the position of Chief Executive Officer of the Company, after having co-opted him as Director.\n\n\nIn 2017 the SIA Group processed overall the clearing of 13.1 billion transactions (+7% compared to 2016), 6.1 billion card transactions (+41.1%) and 3.3 billion payment transactions (+7.1%) relating to credit transfers and collections.\nOn the financial markets, the number of trading and post-trading transactions rose to 56.2 billion from 47.4 billion in 2016, an increase of 18.8%.\nSIA handled a volume of traffic of over 784 terabytes of data, up 19.8% compared to 2016, on the 174,000 km of the SIAnet network, with total infrastructure availability and 100% service levels.\n\n2017 saw a rise in SIA’s revenues of €403.4 million, with a growth of €12.6 million (+3.2%).\nEbitda is down at €114.6 million from €118.6 million in 2016 (-3.4%) and operating result reached €88.5 million (-12.1%). Net profit is €63.4 million, down by €6.4 million (-9.1%) compared to the previous financial year. These results, as well as SIA’s net financial position, were affected by the acquisition of the cards business unit from UBIS for a sum of €500 million.\n\n"}
{"id": "3922489", "url": "https://en.wikipedia.org/wiki?curid=3922489", "title": "Science and Development Network", "text": "Science and Development Network\n\nSciDev.Net is a not-for-profit organisation that produces news, views and analysis about science and technology in the context of global development. It primarily engages with development professionals, policymakers, researchers, the media and the informed public.\n\nThe organisation was founded in 2001 in response to the significant gap in scientific knowledge between rich and poor countries and with the understanding that “those who stand to benefit the most from modern science and technology are also those with the least access to information about it\". SciDev.Net seeks to redress this imbalance via its free-to-access website, regional networks and specialist workshops.\n\nSciDev.Net aims to help individuals and organisations apply evidence and insights from science and technology to decision-making in order to have a positive impact on equitable and sustainable development and poverty reduction.\n\nThe global edition is based in London and there are six regional desks based in Latin America & Caribbean (Spanish), Middle East and North and West Africa (Arabic), South Asia, South-East Asia & Pacific, Sub-Saharan Africa (English and French).\n\nSciDev.Net is a company limited by guarantee and a registered charity in England and Wales (registered charity number 1089590).\n\nThe SciDev.Net website is made up of a global and six regional editions. SciDev.Net publishes in four languages: English, Spanish, French and Arabic. Content includes: News, Analysis, Multimedia, Practical Guides, Learning Series reports, Opinions editorials, Spotlights and Data Visualisations.\n\nNews: SciDev.Net's news coverage is at the heart of its website and articles are added daily. Freelance journalists throughout the developing world write much of this material and work closely with a team of editors to ensure timely and accurate coverage of breaking news.\n\nAnalysis blogs: SciDev.Net analysis blogs focus on vulnerable or marginalised groups who tend to be neglected in mainstream development journalism. They aim to bridge the gap between science and development and provide an analysis of how each can inform the other. SciDev.Net analysis blogs include: \nOpinions: SciDev.Net opinion pieces are exclusive contributions from the world’s leading experts in science and international development. SciDev.Net has published exclusive contributions from figures such as Calestous Juma, Paul Boateng, Mark Lynas, Gordon Conway and Mariéme Jamme.\n\nMultimedia: SciDev.Net produces original multimedia content such as videos, podcasts, photo essays, image galleries and audio-video slideshows.\n\nPractical guides: Written by experts in their field, practical guides help readers strengthen and learn new skills. These guides:\nSpotlights: These special collections of articles focus on a ‘hot topic’ and provide an in-depth look at the key issues facing developing countries. Spotlights published to date include:\nData visualisations: Since 2014 SciDev.Net has been producing data visualisations. These interactive features transforms the latest issues in international development into accessible information that informs data-led decision making. SciDev.Net have worked with a number of partners, including SightSavers, to produce data visualisations on a wide variety of topics including:\n\nThe SciDev.Net website was restructured and relaunched in March 2008 to provide access to material via ‘topic gateways’, which bring together news updates and analysis on key issues. The topics covered are:\n\nScience and technology news is also available via ‘regional editions’:\n\n\nCoverage is informed by regional advisory groups consisting of an extensive number of journalists, consultants, advisors and registered users based in developing countries. They work to ensure that a developing country perspective is represented.\n\nSciDev.Net has over 15 years’ experience of specifically supporting southern journalists and researchers to communicate scientific evidence through workshops and on-the-job mentoring. Since its inception, SciDev.Net has delivered workshops for approximately 1,500 journalists.\n\nIn 2013 SciDev.Net piloted a new approach to capacity building centered upon training for trainers. The new approach provides a blend of face-to-face workshops, networking programmes, awards, mentoring and online learning for journalists, researchers and policymakers.\n\nVisitors who sign-up with SciDev.Net receive a free weekly and/or daily email with all the latest stories from the website. These are available for each edition English, Spanish, Arabic and French. Those who sign-up can comment on articles and submit announcements, events, jobs and grants to the noticeboard for free and these are featured on the website and in the weekly emails.\n\nThe latest news can appear instantly on other websites through a free SciDev.Net global, regional or topic specific newsfeed. Each newsfeed carries the latest news stories, including a headline, introductory sentence and link to the full article.\n\nFor busy researchers or editors who need to sift through information from many sources, RSS (Really Simple Syndication) enables instantaneous delivery of SciDev.Net news stories to a 'news reader' soon as they are published.\n\nAll SciDev.Net website material is free to reproduce under a Creative Commons Attribution 2.0 licence. Under the terms of this licence users are permitted to copy, distribute, display and perform the content, and make derivative works so long as the original author and website are quoted as the source.\n\nHundreds of media outlets have syndicated SciDev.Net’s work including global media houses such as The Guardian, The BBC and The Thomson Reuters Foundation as well as regional news networks like AllAfrica, The Asian Scientist and Dawn.\n\nFunders of SciDev.Net include:\n\n\nPartners \nSciDev.Net works with a range of organisations at global, regional and national levels to achieve shared objectives. These include:\n\n\nSciDev.Net is also affiliated with TWAS, the Academy of Sciences for the Developing World, based in Trieste, Italy.\n\n"}
{"id": "47383676", "url": "https://en.wikipedia.org/wiki?curid=47383676", "title": "Scottish photography", "text": "Scottish photography\n\nScottish photography is the creation of durable images by recording light, or other electromagnetic radiation, in Scotland, or by Scottish people.\n\nScotland played a major role in the technical development of photography in the nineteenth century through the efforts of figures including James Clerk Maxwell and David Brewster. Its artistic development was pioneered by Robert Adamson and artist David Octavius Hill, whose work is considered to be some of the first and finest artistic uses of photography. Thomas Roger was one of the first commercial photographers. Thomas Keith was one of the first architectural photographers. George Washington Wilson pioneered instant photography and landscape photography. Clementina Hawarden and Mary Jane Matherson were amongst the first female photographers. War photography was pioneered by John McCosh, James Robertson, Alexander Graham and Mairi Chisholm.\n\nIn the early twentieth century notable photographic work in Scotland included that of visiting artists, such as Alvin Langdon Coburn and Paul Strand. There was also the record of the Gorbals in Glasgow made by Bert Hardy, Joseph MacKenzie and Oscar Marzaroli. Having pioneered photography in the late nineteenth century, the artistic attainment of native photographers was not high in the early twentieth century. In the late twentieth century, photography in Scotland enjoyed a renaissance, encouraged by figures including Richard Hough and Murray Johnston. More recent exponents who have received acclaim include Pradip Malde, Maud Sulter and Owen Logan.\n\nScotland lacks a national gallery of photography, but there are the dedicated Stills and Portfolio galleries in Edinburgh and space dedicated to photography at the Scottish National Portrait Gallery and the Street Level Gallery in Glasgow. Photography is taught at degree and further education level in Scotland and the history of photography is usually taught within the context of art history.\n\nIn the early nineteenth century Scottish scientists David Brewster (1781–1868) and James Clerk Maxwell (1831–79) played a major part in the development of the techniques of photography. Brewster was an expert on optics and had refined the microscope and invented the kaleidoscope. He collaborated with a number of figures, including Henry Fox Talbot (1800–77), who had developed the calotype photographic process. Maxwell was among the first scientists to investigate colour and produced the first colour photograph in 1861, displayed by using three magic lantern projectors with different colour filters.\n\nAccording to photo historian David Burn \"there is a good case to be made for the proposition that the greatest contribution that Scotland has made to the visual arts, possibly to the arts as a whole, is in the art of photography\". A circle of scientists and artists gathered around Brewster in Edinburgh with the objective of refining the science and art of photography. In 1839 of the five examples of the daguerrotype and twenty \"Photogenic Drawings\" by Talbot were shown at the Exhibition of Arts, Manufacturers and Practical Sciences at the Assembly Rooms, Edinburgh. In 1841 four more daguerreotypes were displayed at the annual exhibition of the Royal Scottish Academy. By 1843 the Edinburgh Calotype Club had been formed, probably the world's first photographic club. The club dissolved in the mid-1850s as new processes appeared, such as the albumen print and wet collodion process. The Glasgow Photographic Society was founded in 1855, The Photographic Society of Scotland formed in 1856 and the Edinburgh Photographic Society in 1861.\n\nMembers of the Edinburgh Calotype Club included chemist Robert Adamson (1821–48) and artist David Octavius Hill (1821–70), who as Hill & Adamson formed the first photographic studio in Scotland at Rock House in Edinburgh in 1843. They originally worked together with the aim of recording the members of the Disruption Assembly, which saw the division of the Church of Scotland in 1843, for a large composite painting, but soon the photographs themselves became a major medium. Their output of around 3,000 calotype images in four years are considered some of the first and finest artistic uses of photography. They produced images influenced by genre art included their \"Edinburgh Ale\" (1843–46), where the poses of three drinking companions are based on scenes from Dutch art. Hill and Adamson also pioneered the recording of rural life as a suitable subject for photography, recording the fisher folk of Newhaven, near Edinburgh. Adamson trained Thomas Roger (1833–88) of St. Andrews, who was one of the first commercial photographers and beside commercial portraits, produced many genre style compositions.\n\nOther pioneers included Thomas Annan (1829–87), who took portraits and landscapes, and whose photographs of the Glasgow slums were among the first to use the medium as a social record. Annan also pioneered the recording of industrial change, photographing the pumping stations and canals of the new waterways to Glasgow in the 1850s. The theme of industrial development was taken up by Evelyn Carey (1858–1932), who recorded step-by-step the building of the Forth Railway Bridge. Also interested in urban landscapes was Archibald Burns, who occupied part of the Rock House studio and whose \"The Horse Wynd\" and \"The Cowgate\" (both 1871) emphasised the picturesque aspects of Edinburgh's urban landscape. John Thomson (1852–90) undertook a similar documentary study of London street life between 1876 and 1877.\n\nOther important figures included Thomas Keith (1827–95), one of the first architectural photographers. George Washington Wilson (1823–93) pioneered instant photography and landscape photography, becoming \"photographer to the Queen\" and his Aberdeen company was the largest producer of topographical prints by 1880. Clementina Hawarden (1822–65) produced posed portraits that were among the first in a tradition of female photography. In the 1850s amateur photographer Mary Jane Matherson took her camera outside to create compositions that can be described as genre art, including \"A Picnic in the Glen\" and \"An Angler at Rest\".\nDocumentary war photography was pioneered by Scottish surgeon John McCosh (1805–85), who produced photographs of the Second Sikh War (1848–49) and the Second Burmese War (1852–53). Scottish photographer James Robertson (1813–88) worked in the Crimean War (1853–56), producing a record of the Siege of Sevastopol in 1853 and then being employed by the British army in India. The émigré Scot Alexander Graham played a major role in photographing the American Civil War, taking iconic images of \"President Lincoln on the Battlefield of Antietam\" (1862) and \"Home of a Rebel Sharpshooter\" (1863). Other Scots that made there reputation as photographers abroad, often being the first to exploit the medium there include William Carrick (1827–78) in Russia, William Notman (1826–91) and Alexander Henderson (1831–1913) in Canada, James MacDonald In Israel, John Thompson in Asia, Robert MacPherson (1811–72) in Italy and George Valentine (1852–90) in New Zealand.\n\nAnnan's son James Craig Annan (1864–1946) popularised the work of Hill & Adamson in the US and worked with American photographic advocate Alfred Stieglitz (1864–1946). Annan and Stieglitz pioneered the more stable photogravure process. The younger Annan also joined the Linked Ring, which broke away from the Royal Photographic Society in 1892 with a manifesto that saw photography as much as an art as a science and that it should be treated as the equal of conventional visual art. He championed the \"artistic\" framing and hanging of photographic exhibitions.\n\nA number of Scots acted as photographers in the First World War (1914–18), including dispatch rider and ambulance driver Mairi Chisholm (1896–1981) who used a lightweight Vest Pocket Kodak camera, that allowed her to capture an image during a bombardment, in her \"Chishom and Knocker Under Fire at Pervyse\" (1917), which can be seen as the first \"action shots\" of war.\n\nHaving pioneered photography in the late nineteenth century, the artistic attainment of native photographers was not high in the early twentieth century. Notable photographic work in Scotland included that of visiting artists, such as Alvin Langdon Coburn (1882–1966), who produced illustrations for the work of Robert Louis Stevenson and Paul Strand (1890–1976), who produced atmospheric depictions of Hebridean landscapes. There was also the record of the Gorbals in Glasgow made by figures such as Bert Hardy (1913–95), Joseph MacKenzie (1929-2015) and Oscar Marzaroli (1933–88), and by Wolfgang Suschitzky (b. 1912) in Glasgow and Dundee.\nIn the late twentieth century, photography in Scotland enjoyed a renaissance, encouraged by figures including Richard Hough (1945–85), who founded the Stills Gallery for photography in Edinburgh in 1977, and Murray Johnston (1949–90), who was its director (1982–86). The 1986 \"Constructed Narratives\" exhibition at the Stills Gallery signalled the renaissance, showcasing the work of Calum Colvin (b. 1961) and Ron O'Donnell (b. 1952), both of whom produced intensely coloured and constructed photographic works. Important practitioners in Scotland included the American Thomas Joshua Cooper (b. 1946). More recent exponents who have received acclaim include Pradip Malde (b. 1957), Maud Sulter (1960–2008) and Owen Logan (b. 1963). The range of contemporary talent was showcased by the 1995 \"Light in the Dark Room\" exhibition.\n\nScotland lacks a national gallery of photography, but there are the dedicated Stills and Portfolio galleries in Edinburgh. There is also space dedicated to photography at the Scottish National Portrait Gallery and the Street Level Gallery in Glasgow. Traditional art galleries also increasingly accept exhibitions of photographic art.\n\nPhotography is taught at degree level in Scotland at Edinburgh College of Art, Napier University, Glasgow School of Art and Duncan of Jordanstone College of Art and Design within the University of Dundee, where there is a particular emphasis on cutting edge technology. The history of photography is usually taught within the context of art history at institutions including St Andrews University. It is also taught at further education colleges throughout the country.\n"}
{"id": "290235", "url": "https://en.wikipedia.org/wiki?curid=290235", "title": "Seven Sisters (oil companies)", "text": "Seven Sisters (oil companies)\n\n\"Seven Sisters\" was a common term for the seven transnational oil companies of the \"Consortium for Iran\" oligopoly or cartel, which dominated the global petroleum industry from the mid-1940s to the mid-1970s. Alluding to the seven mythological Pleiades sisters fathered by the titan Atlas, the business usage was popularized in the 1950s by businessman Enrico Mattei, then-head of the Italian state oil company Eni. The industry group consisted of:\n\nPreceding the 1973 oil crisis, the Seven Sisters controlled around 85 percent of the world's petroleum reserves. Since then, industry dominance has shifted to the OPEC cartel and state-owned oil and gas companies in emerging-market economies, such as Saudi Aramco, Gazprom (Russia), China National Petroleum Corporation, National Iranian Oil Company, PDVSA (Venezuela), Petrobras (Brazil), and Petronas (Malaysia). In 2007, the \"Financial Times\" called these \"the new Seven Sisters\".\n\nAccording to consulting firm PFC Energy, by 2012 only 7% of the world's known oil reserves were in countries that allowed private international companies free rein. Fully 65% were in the hands of state-owned companies.\n\nIn 1951, Iran nationalized its oil industry previously controlled by the Anglo-Iranian Oil Company (now BP), and Iranian oil was subjected to an international embargo. In an effort to bring Iranian oil production back to international markets, the U.S. State Department suggested the creation of a consortium of major oil companies, several of which were daughter corporations of John D. Rockefeller's original Standard Oil monopoly. The \"Consortium for Iran\" was subsequently formed by the following companies:\n\nThe head of the Italian state oil company (Eni), Enrico Mattei, sought membership for his company, but was rejected by what he dubbed the \"Seven Sisters\", the American and British companies that largely controlled the Middle East's oil production after World War II. British writer Anthony Sampson took over the term when he wrote the book \"The Seven Sisters\" in 1975, to describe the oil cartel that tried its best to eliminate competitors and keep control of the world's oil resource. The term for the oil cartel was further popularized, along with a fictional logo, in \"\", a 1981 film about apocalyptic fuel shortages.\n\nBeing politically influential, vertically integrated, well organized, and able to negotiate cohesively as a cartel, the Seven Sisters were initially able to exert considerable power over Third World oil producers. However, in recent decades, the dominance of the Seven Sisters and their successor companies has been challenged by the following trends:\n\nAs of 2017, the surviving companies from the Seven Sisters are BP, Chevron, ExxonMobil and Royal Dutch Shell, which form four members of the \"supermajors\" group.\n\n\n\n"}
{"id": "2526409", "url": "https://en.wikipedia.org/wiki?curid=2526409", "title": "Sex swing", "text": "Sex swing\n\nA sex swing (also known as a sling) is a type of harness designed to allow sexual intercourse between one partner suspended by the swing and another who moves freely. Though there is considerable variety in the design, the most common sex swings have a support for the back, another for the buttocks, and stirrups for each leg, which can be adjusted whilst the user is suspended. \n\nThe following types of sex swing exist: traditional swing, door swing, body swing, and fantasy swing. The body swing is suspended from one partner's body. The door swing is mounted on a door. The fantasy swing is suspended from an eye bolt in the ceiling. In popular culture, sex swing is mostly associated with BDSM practices. \n\nA sex swing or sling is designed to assist in sexual activity. Materials for constructing these devices include nylon webbing, heavy canvas, leather, neoprene, heavy rubber, wood,and steel. The designs provide access to the passive (receptive) partner's genitalia, perineum, buttocks, and anal areas while supporting the individual in a comfortable position, with the hips flexed, to allowing the passive partner to fully relax.\n\nThe most common sex swing is similar to a hammock and holds the receptive partner in a supine position (back down). A variant of the hammock style is a hanging platform of wood or metal, padded for comfort, on which the receptive partner reclines. The hanging platform differs from the hammock style in that, being rigid, the receptive partner's back is straight rather than bowed. Both the hammock and hanging platform designs are often equipped with loops or stirrups to support the ankles or calves up without the requiring exertion by the receptive partner. This allows the receptive partner's hips to be flexed without exertion, improving access to the areas of interest.\n\nHammock-style swings vary in the number of suspension points from three (two at the hips and one at the head), to four (two at the hips and two at the shoulders), to five (two each at hips and shoulders, plus one at the head). The functional difference between these variants lies in the stability of the sling when in use: the more suspension points, the less movement. However, many users prefer to have a greater range of movement.\n\nAnother variation is the sit swing, which holds the receptive partner upright in sitting position, rather than supine. Sit swings take most of the body's weight with large padded loops around the thighs, or some similar arrangement. Sit swings are commonly suspended from a single point attached to the ceiling, to a free-standing sex swing stand, or a door. Door swings offer quicker installation and lower cost, but sacrifice some flexibility.\n\nThe main purpose of sex swings is to make sexual intercourse more exciting and effortless. A sex swing may enable greater freedom of movement during intercourse or assist with challenging sex positions. Swings may be used for vaginal and anal sex, fellatio, and cunnilingus.\n\nSex swings can enable individuals with a physical impairment or disability to enjoy a wide range of sexual activities. Individuals with muscular weakness or arthritis may use a sex swing to reduce the strain on affected muscles or joints. Individuals who use a wheelchair can engage in sexual activity using a sex swing suspended from a hoist.\n\nIt is important to follow the provided instructions when using a sex swing. Secure mounting is vital to avoid injury to the suspended partner from falling. The use of toggle bolts, such as would be used to hang a dreamcatcher, is not appropriate. Installing springs, if supplied, is important to reduce physical shock to the suspended partner caused by rapid deceleration when bouncing in a sex swing.\n"}
{"id": "56163", "url": "https://en.wikipedia.org/wiki?curid=56163", "title": "Starship", "text": "Starship\n\nA starship, starcraft or interstellar spacecraft is a theoretical spacecraft designed for traveling between planetary systems, as opposed to an aerospace-vehicle designed for orbital spaceflight or interplanetary travel.\n\nThe term is mostly found in science fiction, because such craft is not known to have ever been constructed. Reference to a \"star-ship\" appears as early as 1882 in \"\" (1882).\n\nWhilst the Voyager and Pioneer probes have travelled into local interstellar space, the purpose of these unmanned craft was specifically interplanetary and they are not predicted to reach another star system (although \"Voyager 1\" will travel to within 1.7 light years of Gliese 445 in approximately 40,000 years). Several preliminary designs for starships have been undertaken through exploratory engineering, using feasibility studies with modern technology or technology thought likely to be available in the near future.\n\nIn April 2016, scientists announced Breakthrough Starshot, a Breakthrough Initiatives program, to develop a proof-of-concept fleet of small centimeter-sized light sail spacecraft, named \"StarChip\", capable of making the journey to Alpha Centauri, the nearest extrasolar star system, at speeds of 20% and 15% of the speed of light, taking between 20 and 30 years to reach the star system, respectively, and about 4 years to notify Earth of a successful arrival.\n\nTo travel between stars in a reasonable time using rocket-like technology requires very high effective exhaust velocity jet, and enormous energy to power this, such as might be provided by fusion power or antimatter.\n\nThere are very few scientific studies that investigate the issues in building a starship. Some examples of this include:\n\n\nThe Bussard ramjet is an idea to use nuclear fusion of interstellar gas to provide propulsion.\n\nExamined in an October 1973 issue of \"Analog\", the Enzmann Starship proposed using a 12,000 ton ball of frozen deuterium to power thermonuclear powered pulse propulsion units. Twice as long as the Empire State Building and assembled in-orbit, the proposed spacecraft would be part of a larger project preceded by interstellar probes and telescopic observation of target star systems.\n\nThe NASA Breakthrough Propulsion Physics Program (1996–2002), was a professional scientific study examining advanced spacecraft propulsion systems.\n\nA common literary device is to posit a faster-than-light propulsion system (such as warp drive) or travel through hyperspace, although some starships may be outfitted for centuries-long journeys of slower-than-light travel. Other designs posit a way to boost the ship to near-lightspeed, allowing relatively \"quick\" travel (i.e. decades, not centuries) to nearer stars. This results in a general categorization of the kinds of starships:\n\n\nCertain common elements are found in most fiction that discusses starships.\n\nFiction that discusses slower-than-light starships is relatively rare, since the time scales are so long. Instead of describing the interaction with the outside world, those fictions tend to focus on setting the whole story within the world of the (often very large) starship during its long travels. Sometimes the starship \"is\" a world, in perception or reality.\n\nTravel at velocities greater than the speed of light is impossible according to the known laws of physics, although apparent FTL is not excluded by general relativity. The Alcubierre drive provides a theoretical way of achieving FTL, although it requires negative mass, which has not yet been discovered. Nevertheless, Harold G. White at NASA has designed the White–Juday warp-field interferometer to detect a microscopic instance of a warping of space-time according to the Alcubierre drive.\n\nThe following is a listing of some of the most widely known vessels in various science fiction franchises. The most prominent cultural use and one of the earliest common uses of the term \"starship\" was in \"\".\n\nThis list is not exhaustive.\n\n\n\n"}
{"id": "1068539", "url": "https://en.wikipedia.org/wiki?curid=1068539", "title": "Tajine", "text": "Tajine\n\nA tajine or tagine (Arabic: الطاجين, Tifinagh: ⵜⴰⵊⵉⵏ) is a Maghrebi dish which is named after the earthenware pot in which it is cooked. It is also called a Maraq/marqa in North Africa.\n\nFrom Moroccan Arabic طجين \"ṭajīn\", from Arabic طاجن \"ṭājin\" \"shallow earthen pot\", from \"tagēnon\" \"frying-pan, saucepan\".\n\nThe tagine dates back to Harun al-Rashid who was a ruler of the Early Muslim conquests. The earliest writings about the concept of cooking in a tajine appear in the famous \"One Thousand and One Nights\", an Arabic-language story collection from the ninth century. It is also mentioned during the Abbasid Caliphate (which stretched from the Middle East to North Africa and al-Andalus during the ninth century. The dish would have been already famous amongst the nomadic Bedouin people of the Arabian Peninsula, who added dried fruits like dates, apricots and plums to give it its unique taste. Tagine is now often eaten with french fries, either on the top or on the side.\n\nToday, the cooking-pot and its traditional broth is primarily prepared in the Middle East and North Africa. In North Africa it is called a Tajine, while in the Middle East it is called a \"maraq\" (broth) or a \"qidra\" (cooking pot). There are different ways to prepare the tajine. In the original qidra style \"saman\" (clarified butter) is used to lubricate the surface and a puree of chopped onion is added for flavour and aroma. For \"muqawlli\"-style cooking, the ingredients are placed in olive oil to enrich the flavours.\n\nThere are many descriptions of how to prepare a tajine from Arab scholars. A famous description is the one from ibn al-Adim:\nThe traditional tajine pottery, sometimes painted or glazed, consists of two parts: a circular base unit that is flat with low sides and a large cone- or dome-shaped cover that sits on the base during cooking. The cover is designed to return all condensation to the bottom. That process can be improved by adding cold water into the specially designed well at the top of the lid.\n\nTajine is traditionally cooked over hot charcoal leaving an adequate space between the coals and the tajine pot to avoid having the temperature rise too quickly. Large bricks of charcoal are used, specifically for their ability to stay hot for hours. Other methods are to use a tajine in a slow oven or on a gas or electric stove top, on the lowest heat necessary to keep the stew simmering gently. A diffuser, a circular utensil placed between the tajine and the flame, is used to evenly distribute the stove's heat. European manufacturers have created tajines with heavy cast-iron bottoms that can be heated on a cooking stove to a high temperature, which permits the browning of meat and vegetables before cooking.\n\nTajine cooking may be replicated by using a slow cooker or similar item, but the result will be slightly different. Many ceramic tajines are decorative items as well as functional cooking vessels. Some tajines, however, are intended only to be used as decorative serving dishes.\n\nMoroccan and Algerian tajine dishes are slow-cooked savory stews, typically made with sliced meat, poultry or fish together with vegetables or fruit. Spices, nuts, and dried fruits are also used. Common spices include ginger, cumin, turmeric, cinnamon, and saffron. Paprika and chili are used in vegetable tajines. The sweet and sour combination is common in tajine dishes like lamb with dates and spices. Tajines are generally served with bread. Because the domed or cone-shaped lid of the tajine pot traps steam and returns the condensed liquid to the pot, a minimal amount of water is needed to cook meats and vegetables. This method of cooking is practical in areas where water supplies are limited or where public water is not yet available.\n\nWhat Tunisians refer to as a \"tajine\" is very different from the Moroccan dish. Tunisian tajine is more like an Italian frittata or an eggah. First, a simple ragout is prepared, of meat cut into very small pieces, cooked with onions and spices, such as a blend of dried rosebuds and ground cinnamon known as \"baharat\" or a robust combination of ground coriander and caraway seeds; this is called tabil. Then something starchy is added to thicken the juices. Common thickeners include cannellini beans, chickpeas, breadcrumbs or cubed potatoes. When the meat is tender, it is combined with the ingredients which have been chosen to be the dominant flavouring. Examples include fresh parsley, dried mint, saffron, sun-dried tomatoes, cooked vegetables and stewed calves' brains. Next, the stew is enriched with cheese and eggs. Finally, this egg and stew is baked in a deep pie dish, either on the stove or in the oven until top and bottom are crisply cooked and the eggs are just set. When the tajine is ready, it is turned out onto a plate and sliced into squares, accompanied by wedges of lemon. Tunisian tajines can also be made with seafood or as a completely vegetarian dish.\n\nIn rural parts of Tunisia, home cooks place a shallow earthenware dish over olive-wood coals, fill it, cover it with a flat earthenware pan, and then pile hot coals on top. The resulting tajine is crusty on top and bottom, moist within and is infused with a subtle smoky fragrance.\nA similar dish known as \"tavvas\" is found in Cypriot cuisine.\n\n\n"}
{"id": "4038553", "url": "https://en.wikipedia.org/wiki?curid=4038553", "title": "Tom Merritt", "text": "Tom Merritt\n\nThomas Andrew Merritt (born June 28, 1970) is a technology journalist, writer, and broadcaster best known as the host of several podcasts. He is the former co-host of Tech News Today on the TWiT.tv Network, and was previously an Executive Editor for CNET and developer and co-host of the daily podcast \"Buzz Out Loud\".\n\nHe currently hosts \"Daily Tech News Show\", \"Cordkillers\", and \"Sword and Laser\", among other shows.\n\nMerritt was born in Greenville, Illinois, to a food scientist father who worked on the Coffee-Mate project. Merritt received a BS in journalism from the University of Illinois at Urbana-Champaign and pursued graduate work in communications at the University of Texas at Austin.\n\nMerritt's career in radio began in 1986 as a DJ for WGEL, a country music station located in Greenville, Illinois. In 1993, he worked as an intern for National Public Radio's \"Morning Edition\". From 1999 to 2004 Merritt worked for TechTV in San Francisco as an Executive Web Producer and served as a radio host with TechTV until 2003.\n\nMerritt started with CNET in 2004. In addition to his duties as co-host of Buzz Out Loud with Molly Wood, Merritt also had a regular column & podcast (co-hosted with Rafe Needleman) dealing with consumer technology. He also co-hosted the tech support call-in program CNET Live with fellow editor Brian Cooley, and was the host of CNET Top 5.\n\nOn April 16, 2010, Merritt announced he would be stepping down as co-host of Buzz Out Loud, and that he would be joining the TWiT.tv Network as a full-time daily host beginning June 1, 2010. During his last Buzz Out Loud episode on May 14, 2010 he announced that his main focus at TWiT.tv would be a new daily show, Tech News Today.\n\nPrior to joining TWiT as an employee, Merritt had a long-standing working relationship with former TechTV colleague Leo Laporte's network having regularly appeared on This Week in Tech as either a guest or as a relief host. His general discussion podcast with Roger Chang, East Meets West, also featured on TWiT Live.\n\n\"Tech News Today\" launched on June 1, 2010. Merritt was a regular host along with Sarah Lane, Iyaz Akhtar and Jason Howell.\n\nUpon joining TWiT.tv, Merritt brought with him two shows previously produced by cartoonist Scott Johnson's Frogpants Studios. Originally started on July 7, 2009, \"Fourcast\" featured Merritt and Johnson inviting various guests to discuss the future and what it might contain in a so-called virtual fireside setting. Meanwhile, \"Current Geek Weekly\" is a weekly discussion of geek culture stories and the companion podcast to the Current Geek podcasts still produced by Frogpants Studios. Merritt still appeared on the Frogpants Network for a segment called \"Tom's Tech Time\" on Wednesdays on the Scott Johnson/Brian Ibbott-hosted podcast \"The Morning Stream\".\n\nOn November 10, 2010, Merritt officially launched his second new show on TWiT, FrameRate. Focusing on video in its many and varied forms (television, film, internet), Merritt co-hosted the show with magician and NSFW podcast host Brian Brushwood.\n\nOn January 20, 2011, TWiT officially launched \"Triangulation\", a new show Merritt co-hosting with Leo Laporte and interviewing a notable figure in technology. In July 2012 he stopped hosting the show because he \"wanted to work on other projects\".\n\nIn addition to these regular shows, Merritt hosted live breaking news coverage of major technology events on TWiT Live such as WWDC, Google I/O, and the resignation and passing of Steve Jobs. These are later released as \"TWiT Live Specials\" podcasts. He has acted as a relief host for Laporte on TWiT, Windows Weekly, Security Now and other shows when Laporte has been unavailable.\n\nOn October 22, 2012, Merritt announced that he would be moving to Los Angeles to accommodate his wife's new employment at YouTube, but would still continue to present on the TWiT network over Skype.\n\nOn December 5, 2013, Leo Laporte announced that Merritt's contract would not be renewed. Leo stated that the decision was based on the need for an in-studio anchor for \"Tech News Today\". Merritt hosted his last edition of \"Tech News Today\" on December 30, 2013.\n\nSince February 4, 2008, Merritt has hosted \"Sword & Laser\", a sci-fi based book club podcast, co-hosted with his former CNET colleague, Veronica Belmont.\n\nOn June 22, 2010, he launched a new show for Revision3, entitled \"Tom's Top 5\". The show counts down a new Top 5 list every week, and is released every Tuesday. Its final episode was published on November 1, 2011. He did a similar show on CNET.\n\nMerritt has written a sci-fi novel, Boiling Point, which describes a near future United States civil war. He later narrated it as an audiobook.\n\nIn 2006 he wrote another novel, United Moon Colonies, of which he posted chapters on his blog. Both novels were published on Lulu.com with a Creative Commons license.\n\nMerritt appeared in two early episodes of his \"Frame Rate\" co-host Brian Brushwood's Revision3 show \"Scam School\".\n\nOn March 3, 2013, Merritt and Molly Wood began the It's a Thing podcast. According to the site's about page, \"It’s a Thing is a show grown from the brain of Molly Wood, derived from a regular segment on the CNET podcast Gadgettes. Tom and Molly started the hit podcast “Buzz Out Loud” which they co-hosted for years after the turn of the century. They missed doing shows together, so they decided that in itself, should become a thing. Again.\"\n\nIn early January 2014 Merritt began co-hosting a podcast with Brian Brushwood called Cordkillers, while also starting a beta for a new Daily Tech News Show.\n\nIn 2012, newspapers reported calls by consumer groups to boycott Apple products in response to accounts of worker suicides and dangerous working conditions at the Foxconn plant in China. Merritt, who has followed tech news since 2005, responded by saying \"Boycotts of Apple might be good to nudge Apple into doing something to improve conditions, but, you're going to have to boycott buying electronics if you really wanted to punish China. I'm not sure that that's called for, necessarily.\"\n\nCiting a Forbes infographic showing Foxconn with, reportedly, fewer suicides per million workers (18) than the number of suicides per million Chinese citizens (220), Merritt suggested it might be worth investigating whether the lower rate at Foxconn may be due to \"the fact that people who are gainfully employed are in some way less likely to commit suicide.\" There may be other reasons besides the working conditions at the plant for worker suicides.\n\nInstead of boycotts, he advocated addressing the dangerous working conditions in a broader context. Comparing the conditions at Foxconn to coal mines of the 1800s and early 1900s, he added, \"There may be similar types of abuses going on at Foxconn, but we have those kinds of conditions arise because the conditions that the workers are in before they take the job are worse. That doesn't excuse the conditions, but you don't just want to get rid of the factory. You don't want to just get rid of the coal mine and send people back into abject poverty. What you want to do is put pressure on the coal mine or the factory or whatever to begin to change their ways and improve those conditions so that everybody wins.\"\n\nMerritt is married to Eileen Rivera and they live in Los Angeles, California, with their dogs Sawyer and Rey (another dog, Jango, died of natural causes on January 25, 2017), and formerly lived in Marin County and Oakland. Merritt is a fan of Major League Baseball's St. Louis Cardinals.\n\n\n"}
{"id": "406804", "url": "https://en.wikipedia.org/wiki?curid=406804", "title": "Unexploded ordnance", "text": "Unexploded ordnance\n\nUnexploded ordnance (UXO, sometimes abbreviated as UO), unexploded bombs (UXBs), or explosive remnants of war (ERW) are explosive weapons (bombs, shells, grenades, land mines, naval mines, cluster munition, etc.) that did not explode when they were employed and still pose a risk of detonation, sometimes many decades after they were used or discarded. UXO does not always originate from wars; areas such as military training grounds can also hold significant numbers, even after the area has been abandoned. UXO from World War I continue to be a hazard, with poisonous gas filled munitions still a problem. When unwanted munitions are found, they are sometimes destroyed in controlled explosions, but accidental detonation of even very old explosives also occurs, sometimes with fatal results.\n\nSeventy-eight countries are contaminated by land mines, which kill 15–20,000 people every year while severely maiming countless more. Approximately 80% of casualties are civilian, with children as the most affected age group. An estimated average of 50% of deaths occurs within hours of the blast. In recent years, mines have been used increasingly as weapons of terror against local civilian populations specifically.\n\nIn addition to the obvious danger of explosion, buried UXO can cause environmental contamination. In some heavily used military training areas, munitions-related chemicals such as explosives and perchlorate (a component of pyrotechnics and rocket fuel) can enter soil and groundwater.\n\nUnexploded ordnance, however old, may explode. Even if it does not explode, environmental pollutants are released as it degrades. Recovery, particularly of deeply-buried projectiles, is difficult and hazardous—jarring may detonate the charge. Once recovered, explosives must either be detonated in place—sometimes requiring hundreds of homes to be evacuated—or transported safely to a site where they can be destroyed.\n\nUnexploded ordnance from at least as far back as the American Civil War still poses a hazard worldwide, both in current and former combat areas and on military firing ranges. A major problem with unexploded ordnance is that over the years the detonator and main charge deteriorate, frequently making them more sensitive to disturbance, and therefore more dangerous to handle. Construction work may disturb unsuspected unexploded bombs, which may then explode. There are countless examples of people tampering with unexploded ordnance that is many years old, often with fatal results. For this reason it is universally recommended that unexploded ordnance should not be touched or handled by unqualified persons. Instead, the location should be reported to the local police so that bomb disposal or Explosive Ordnance Disposal (EOD) professionals can render it safe.\n\nAlthough professional EOD personnel have expert knowledge, skills and equipment, they are not immune to misfortune because of the inherent dangers: in June 2010, construction workers in Göttingen, Germany discovered an Allied bomb dating from World War II buried approximately below the ground. German EOD experts were notified and attended the scene. Whilst residents living nearby were being evacuated and the EOD personnel were preparing to disarm the bomb, it detonated, killing three of them and severely injuring 6 others. The dead and injured each had over 20 years of hands-on experience, and had previously rendered safe between 600 and 700 unexploded bombs. The bomb which killed and injured the EOD personnel was of a particularly dangerous type because it was fitted with a delayed-action chemical fuze (with an integral anti-handling device) which had not operated as designed, but had become highly unstable after over 65 years underground. The type of delayed-action fuze in the Göttingen bomb was commonly used: a glass vial containing acetone was smashed after the bomb was released; the acetone was intended, as it dripped downwards, to disintegrate celluloid discs holding back a spring-loaded trigger that would strike a detonator when the discs degraded sufficiently after some minutes or hours. These bombs, when striking soft earth at an angle, often ended their trajectory not pointing downwards, so that the acetone did not drip onto and weaken the celluloid; but over many years the discs degraded until the trigger was released and the bomb detonated spontaneously, or when weakened by being jarred.\n\nIn November 2013 four US Marines were killed by an explosion whilst clearing unexploded ordnance from a firing range at Camp Pendleton. The exact cause is not known, but the Marines had been handing grenades they were collecting to each other, which is permitted but discouraged, and it is thought that a grenade may have exploded after being kicked or bumped, setting off hundreds of other grenades and shells.\n\nA dramatic example of munitions and explosives of concern (MEC) threat is the wreck of the SS \"Richard Montgomery\", sunk in shallow water about from the town of Sheerness and from Southend, which still contains 1,400 tons of explosives. When the deeper World War II wreck of the \"Kielce\", carrying a much smaller load of explosives, exploded in 1967, it produced an earth tremor measuring 4.5 on the Richter scale.\n\nNorth Africa, and in particular the desert areas of The Sahara, is heavily mined and with serious consequences for the local population. Egypt is the most heavily mined country in the world (by number) with as much as 19.7 million mines as of 2000.\n\nLand mines and other explosive remnants of war are not limited to North Africa, however; they pose a persistent threat to local people all over the continent, including the countries of Ethiopia, Somalia, Nigeria, Senegal, Angola, Kenya, Uganda and South Africa to mention just a few. In the Tropics, typhoons and floods often displace and spread landmines, further aggravating the problem. In Mozambique, as much as 70% of the country is now contaminated with mines because of this.\n\nDuring the long Colombian conflict that began around 1964, a very large number of landmines were deployed in rural areas across Colombia. The landmines are homemade and were placed primarily during the last 25 years of the conflict, hindering rural development significantly. The rebel groups of FARC and the smaller ELN are usually blamed for having placed the mines. All departments of Colombia are affected, but Antioquia, where the city of Medellin is located, holds the largest amounts. After Afghanistan, Colombia has the second-highest number of landmine casualties, with more than 11,500 people killed or injured by landmines since 1990, according to Colombian government figures.\n\nIn September 2012, the Colombian peace process began officially in Havana and in August 2016, the US and Norway initiated an international five-year demining program, now supported by another 24 countries. Both the Colombian military and FARC are taking part in the demining efforts. The program intends to rid Colombia of landmines and other UXO by 2021 and it has been funded with nearly US$112 million, including US$30 million from the US (as part of the larger US foreign policy Plan Colombia) and US$9.4 million from the EU. Experts however, have estimated that it will take at least a decade due to the difficult terrain.\n\nWhile, unlike many countries in Europe and Asia, the United States has not been subjected to aerial bombardment, according to the Department of Defense, \"millions of acres\" may contain UXO, Discarded Military Munitions (DMM) and Munitions Constituents (e.g., explosive compounds).\n\nAccording to US Environmental Protection Agency documents released in late 2002, UXO at 16,000 domestic inactive military ranges within the United States pose an \"imminent and substantial\" public health risk and could require the largest environmental cleanup ever, at a cost of at least US$14 billion. Some individual ranges cover , and, taken together, the ranges comprise an area the size of Florida.\n\nOn Joint Base Cape Cod (JBCC) on Cape Cod, Massachusetts, decades of artillery training have contaminated the only drinking water for thousands of surrounding residents. A costly UXO recovery effort is under way.\n\nUXO on US military bases has caused problems for transferring and restoring Base Realignment and Closure (BRAC) land. The Environmental Protection Agency's efforts to commercialize former munitions testing grounds are complicated by UXO, making investments and development risky.\n\nUXO cleanup in the US involves over of land and 1,400 different sites. Estimated cleanup costs are tens of billions of dollars. It costs roughly $1,000 to demolish a UXO on site. Other costs include surveying and mapping, removing vegetation from the site, transportation, and personnel to manually detect UXOs with metal detectors. Searching for UXOs is tedious work and often 100 holes are dug to every 1 UXO found. Other methods of finding UXOs include digital geophysics detection with land and airborne systems.\n\nIn December 2007, UXO was discovered in new development areas outside Orlando, Florida, and construction had to be halted. Other areas nearby are also affected; for example boaters avoid the Indian River Lagoon, which contains UXO thought to be left from live bombing runs performed during World War II by pilots from nearby DeLand Naval Air Station.\n\nPlum Tree Island National Wildlife Refuge in Poquoson, Virginia was heavily used as a bombing range by pilots from nearby Langley Air Force Base from 1917 through the 1950s. The former bombing range was transferred to the US Fish and Wildlife Service in 1972. Air Force records show that of various-sized bombs were dropped in just one exercise in December 1938. Because the area is alternately marshy or sandy, many of the bombs didn't explode and instead were partly or completely buried in the mud and sand or lie in the surf just offshore. In 1958 three teenage boys who landed their boat on the island were seriously injured when a practice bomb exploded. As of 2007 the US military had not removed a single bomb from the Island. The island is adjacent to the Poquoson Flats, a popular destination for fishermen and recreational boaters. Some signs that have been placed offshore to warn boaters of the hidden danger posed by the UXO in the surf or buried beneath the idyllic-looking sand beach and salt marsh have been blown down by storms and have not been replaced. According to the US Army Corps of Engineers, the cleanup of the UXO on Plumtree Island could take years and cost tens of millions of dollars.\n\nDuring World War I, the US Chemical Corps was established at American University, based in the University's McKinley Building. After the war, many toxic chemicals and weaponry were buried in or around the Northwest DC community where the university is located. Excavations in the area were carried out after significant discoveries were made in 2010.\n\nAlthough comparatively rare, unexploded ordnance from the American Civil War is still occasionally found and is still deadly 150 years later. Union and Confederate troops fired an estimated 1.5 million artillery shells and explosive cannonballs at each other from 1861 to 1865. As many as one in five did not explode. In 1973, during the restoration of Weston Manor, an 18th-century plantation house in Hopewell, Virginia that was shelled by Union gunboats during the Civil War, a live cannonball was found embedded in the dining room ceiling. The ball was disarmed and is shown to visitors to the plantation. In 1999, a Civil War cannonball fell from a large tree in the yard of country music singer Jimmy Dean's home overlooking the James River, where it had been lodged since the battle of Chaffin's Bluff.\n\nIn late March 2008, a , mortar shell was uncovered at the Petersburg National Battlefield, the site of a 292-day siege. The shell was taken to the city landfill where it was safely detonated by ordnance disposal experts. Also in 2008, Civil War enthusiast Sam White was killed when a , naval cannonball he was attempting to disarm in the driveway of his home in a Richmond, Virginia suburb exploded. The explosion sent a chunk of shrapnel crashing into a house away.\n\nThousands of tons of UXOs remain buried across Japan, particularly in Okinawa, where over 200,000 tons of ordnance were dropped during the final year of the Second World War. From 1945 until the end of the U.S. occupation of the island in 1972, the JSDF and US military disposed of 5,500 tons of UXO. Over 30,000 UXO disposal operations have been conducted on Okinawa by the JSDF since 1972, and it is estimated it could take close to a century to dispose of the remaining UXOs on the islands. No injuries or deaths have been reported as a result of UXO disposal, however. Tokyo and other major cities, including Kobe, Yokohama and Fukuoka, were targeted by several massive air raids during the Second World War, which left behind numerous UXOs. Shells from Imperial Army and Navy guns also continue to be discovered.\n\nOn 29 October 2012, an unexploded US bomb with a functioning detonator was discovered near a runway at Sendai Airport during reconstruction following the 2011 Tōhoku earthquake and tsunami, resulting in the airport being closed and all flights cancelled. The airport reopened the next day after the bomb was safely contained, but closed again on 14 November while the bomb was defused and safely removed.\n\nIn March 2013, an unexploded Imperial Army anti-aircraft shell measuring long was discovered at a construction site in Tokyo's North Ward, close to the Kaminakazato Station on the JR Keihin Tohoku Line. The shell was detonated in place by a JSDGF UXO disposal squad in June, causing 150 scheduled rail and Shinkansen services to be halted for three hours and affecting 90,000 commuters. In July, an unexploded US bomb from an air raid was discovered near the Akabane Station in the North Ward and defused on site by the JSDGF in November, resulting in the evacuation of 3,000 households nearby and causing several trains to be halted for an hour while the UXO was being defused.\n\nOn 13 April 2014, the JSGDF defused an unexploded US oil incendiary bomb discovered at a construction site in Kurume, Fukuoka Prefecture, which required the evacuation of 740 people living nearby.\n\nOn 16 March 2015, a bomb was found in central Osaka.\n\nMost countries of Southeast Asia – and all countries of Indochina specifically – are contaminated with unexploded ordnance. Most of the UXOs of today are remnants from the Vietnam War which, apart from Vietnam, also included neighbouring Cambodia and Laos, but other conflicts and civil wars has also contributed.\n\nLaos is considered the world's most heavily bombed nation. During the period of the Vietnam War, over half a million American bombing missions dropped more than 2 million tons of ordnance on Laos, most of it anti-personnel cluster bombs. Each cluster bomb shell contained hundreds of individual bomblets, \"bombies\", about the size of a tennis ball. An estimated 30% of these munitions did not detonate. Ten of the 18 Laotian provinces have been described as \"severely contaminated\" with artillery and mortar shells, mines, rockets, grenades, and other devices from various countries of origin. These munitions pose a continuing obstacle to agriculture and a special threat to children, who are attracted by the toylike devices. \n\nSome 288 million cluster munitions and about 75 million unexploded bombs were left across Laos after the war ended. From 1996–2009, more than 1 million items of UXO were destroyed, freeing up 23,000 hectares of land. Between 1999 and 2008, there were 2,184 casualties (including 834 deaths) from UXO incidents.\n\nIn Vietnam, 800,000 tons of landmines and unexploded ordnance is buried in the land and mountains. From 1975 to 2015, up to 100,000 people have been injured or killed by bombs left over from the war.\n\nAt present, all 63 provinces and cities are contaminated with UXO and landmines. However, it is possible to prioritize demining for the Northern border provinces of Lang Son, Ha Giang and the six Central provinces of Nghe An, Ha Tinh, Quang Binh, Quang Tri, Thua Thien and Quang Ngai. Particularly in these 6 central provinces, up to 2010, there were 22,760 victims of landmines and UXO, of which 10,529 died and 12,231 were injured.\n\n\"The National Action Plan for the Prevention and Fighting of Unexploded Ordnance and Mines from 2010 to 2025\" has been prepared and promulgated by the Vietnamese Government in April 2010.\n\nWestern Asia, including the Middle East and border states towards Russia, is severely affected by UXO, in particular land mines. Not only are civilians killed and maimed regularly, it also impedes economic growth and development by restricting the use of natural resources and farmland.\n\nIraq is widely contaminated with unexploded remnants of war from the Iran–Iraq War (1980–88), the Gulf War (1990–91), the Iraq War (2003–11) and lately the ongoing Iraq Civil War. The UXO in Iraq poses a particularly serious threat to civilians as millions of cluster bomb munitions were dropped in towns and densely populated areas by the US and British air forces, mostly in the first few weeks of the invasion in 2003. An estimated 30% of the munitions failed to detonate on impact and small unexploded bombs are regularly found in and around homes in Iraq, frequently maiming or killing civilians and restricting land use. From 1991 to 2009, an estimated 8,000 people were killed by cluster bomblets alone, 2,000 of which were children. Land mines are another part of the UXO problem in Iraq as they litter large areas of farmland and many oil fields, severely affecting economic recovery and development.\n\nReporting and monitoring is lacking in Iraq and no completely reliable survey and overview of the local threat levels exists. Useful statistics on injuries and deaths caused by UXO is also missing, only singular local reports exist. UNDP and UNICEF however, issued a partial survey report in 2009, concluding that the entire country is contaminated and more than 1.6 million Iraqis are affected by UXO. More than 1,730 km2 (670 square miles) in total are saturated with unexploded ordnance (including land mines). The south-east region and Baghdad are the most heavily contaminated areas and UNDP has designated around 4,000 communities as \"hazard areas\".\n\nIn the aftermath of the 2006 war between Israel and Lebanon, it is estimated that southern Lebanon is littered with one million undetonated cluster bombs – approximately 1.5 bombs per Lebanese inhabitant of the region, dropped by Israeli Defense Forces in the last days of the war.\n\nDespite massive demining efforts, Europe is still affected to some extent by UXO from mainly World War I and World War II, some countries more than others. However, newer and present military conflicts are also affecting some areas severely, in particular the countries of former Yugoslavia in western Balkans and Ukraine.\n\nAs a legacy of the Civil war in Yugoslavia (1991–2001), Albania, Bosnia-Herzegovina, Croatia and Kosovo are all affected badly by UXO, mostly land mines.\n\nIn the Ardennes region of France, large-scale citizen evacuations were necessary during MEC removal operations in 2001. In the forests of Verdun French government \"démineurs\" working for the \"Département du Déminage\" still hunt for poisonous, volatile, and/or explosive munitions and recover about 900 tons every year. The most feared are corroded artillery shells containing chemical warfare agents such as mustard gas. French and Flemish farmers still find many UXOs when ploughing their fields, the so-called \"iron harvest\".\n\nIn Belgium, Dovo, the country's bomb disposal unit, recovers between 150 and 200 tons of unexploded bombs each year. Over 20 members of the unit have been killed since it was formed in 1919.\n\nGermany has a specialized unit for defusing bombs called ( (\"KMBD\"), \"Explosive Ordnance Disposal Service\"). It is considered one of the busiest worldwide as it deactivates a bomb every two weeks.\n\nThousands of UXOs from the Second World War are still uncovered each year in Germany. The daily average is 15, most of them aerial bombs. Concentration is especially high in Berlin, where many artillery shells and smaller munitions from the Battle of Berlin are uncovered each year. While most cases only make local news, one of the more spectacular finds in recent history was an American 500-pound aerial bomb discovered in Munich on 28 August 2012. As it was deemed too unsafe for transport, it had to be exploded in situ, shattering windows over a wide area of Schwabing and causing structural damage to several homes despite precautions to minimize damage.\n\nOne of the largest individual pieces ever found was an unexploded 'Tallboy' bomb uncovered in the Sorpe Dam in 1958.\nIn 2011, a 1.8-tonne RAF bomb from the Second World War was uncovered in Koblenz on the bottom of the Rhine River after a prolonged drought. It caused the evacuation of 45,000 people from the city and was then called \"the biggest bomb-related evacuation in Germany's post-war history\". In May 2015, Some 20,000 people had to leave their homes in Cologne in order to defuse a one-tonne bomb.\n\nOn December 20, 2016 another 1.8-tonne RAF bomb was found in the city centre of Augsburg and prompted the evacuation of 54,000 people on December 25. In May 2017, 50,000 people in Hanover had to be evacuated in order to defuse three British unexploded bombs.\n\nOn 29 August 2017, a British HC 4000 bomb was discovered during construction work near the Goethe University in Frankfurt, requiring the evacuation of approximately 65,000 people within a radius of 1.5 km. This was the largest evacuation in Germany since the Second World War. Later, it was successfully defused on 3 September.\n\nOn 8 April 2018, a 1.8-tonne bomb was defused in Paderborn, which caused the evacuation of more than 26,000 people. On 24 May 2018, a bomb was defused in Dresden after the initial attempts of deactivation failed, and caused a small explosion. On 3 July 2018, a bomb was disabled in Potsdam which caused 10,000 people to be evacuated from the region.\n\nSince the 1980s, more than 750,000 pieces of UXO from to the Spanish Civil War (1936-1939) has been recovered and destroyed by the Guardia Civil in Spain. In the 2010s, around 1,000 bombs, artillery shells and grenades have been defused every year.\n\nUkraine is contaminated with UXO from WW I, WW II, former Soviet military training and the current Ukraine Crisis, including the War in Donbass. Most of the UXO from the world wars has presumably been removed by demining efforts in the mid 1970s, but sporadic remnants may remain in unknown locations. The UXO from the recent military conflicts includes both landmines and cluster bomblets dropped and set by both Ukrainian, anti-government and Russian forces. Reports of booby traps harming civilians also exist. Ukraine reports that Donetsk and Luhansk Oblast are the regions mostly affected by unexploded submunitions. Proper, reliable statistics are currently unavailable, and information from the involved combatants are possibly politically biased and partly speculative. However, 600 deaths and 2,000 injured due to UXO in 2014 and 2015 alone have been accounted for.\n\nUXO is standard terminology in the United Kingdom, although in artillery, especially on practice ranges, an unexploded shell is referred to as a \"blind\", and during the Blitz in World War II an unexploded bomb was referred to as a \"UXB\".\n\nMost current UXO risk is limited to areas in cities, mainly London, Sheffield and Portsmouth, that were heavily bombed during the Blitz, and to land used by the military to store ammunition and for training.\nAccording to the Construction Industry Research and Information Association (CIRIA), from 2006 to 2009 over 15,000 items of ordnance were found in construction sites in the UK. It is not uncommon for many homes to be evacuated temporarily when a bomb is found. 1,000 residents were evacuated in Plymouth in April 2007 when a Second World War bomb was discovered, and in June 2008 a 1,000 kg bomb was found in Bow in East London. In 2009 CIRIA published \"Unexploded Ordnance (UXO) – a guide for the construction industry\" to provide advice on assessing the risk posed by UXO.\n\nThe burden of Explosive Ordnance Disposal in the UK is split between Royal Engineers Bomb Disposal Officers, Royal Logistic Corps Ammunition Technicians in the Army, Clearance Divers of the Royal Navy and the Armourers of the Royal Air Force. The Metropolitan Police of London is the only force not to rely on the Ministry of Defence, although they generally focus on contemporary terrorist devices rather than unexploded ordnance and will often call military teams in to deal with larger and historical bombs.\n\nBuried and abandoned aerial and mortar bombs, artillery shells, and other unexploded ordnance from World War II have threatened communities across the islands of the South Pacific. the Office of Weapons Removal and Abatement in the U.S. Department of State's Bureau of Political-Military Affairs invested more than $5.6 million in support of conventional weapons destruction programs in the Pacific Islands.\n\nOn the battlefield of Peleliu Island in the Republic of Palau UXO removal made the island safe for tourism.\nAt Hell's Point Guadalcanal Province in the Solomon Islands an explosive ordnance disposal training program was established which safely disposed of hundreds of items of UXO. It trained police personnel to respond to EOD call-outs in the island's highly populated areas.\nOn Mili Atoll and Maloelap Atoll in the Marshall Islands removal of UXO has allowed for population expansion into formerly inaccessible areas.\n\nIn the Marianas, World War II-era unexploded ordnance is still often found and detonated under controlled conditions.\n\nProtocol V of the Convention on Certain Conventional Weapons requires that when active hostilities have ended the parties must clear the areas under their control from \"explosive remnants of war\". Land mines are covered similarly by Protocol II.\n\nMany weapons, including aerial bombs in particular, are discovered during construction work, after lying undetected for decades. Having failed to explode while resting undiscovered is no guarantee that a bomb will not explode when disturbed. Such discoveries are common in heavily bombed cities, without a serious enough threat to warrant systematic searching.\n\nWhere there is known to be much unexploded ordnance, in cases of unexploded subsoil ordnance a remote investigation is done by visual interpretation of available historical aerial photographs. Modern techniques can combine geophysical and survey methods with modern electromagnetic and magnetic detectors. This provides digital mapping of UXO contamination with the aim to better target subsequent excavations, reducing the cost of digging on every metallic contact and speeding the clearance process. Magnetometer probes can detect UXO and provide geotechnical data before drilling or piling is carried out.\n\nIn the U.S., the Strategic Environmental Research and Development Program (SERDP) and Environmental Security Technology Certification Program (ESTCP) Department of Defense programs fund research into the detection and discrimination of UXO from scrap metal. Much of the cost of UXO removal comes from removing non-explosive items that the metal detectors have identified, so improved discrimination is critical. New techniques such as shape reconstruction from magnetic data and better de-noising techniques will reduce cleanup costs and enhance recovery.\nThe Interstate Technology & Regulatory Council published a Geophysical Classification for Munitions Response guidance document in August 2015.\nUXO or UXBs (as they are called in some countries – unexploded bombs) are broadly classified into buried and unburied. The disposal team carries out reconnaissance of the area and determines the location of the ordnance. If is not buried it may be dug up carefully and disposed of. But if the bomb is buried it becomes a huge task. A team is formed to find the location of the bomb using metal detectors and then the earth is dug carefully.\n\n\n\n"}
{"id": "6026902", "url": "https://en.wikipedia.org/wiki?curid=6026902", "title": "Waste treatment technologies", "text": "Waste treatment technologies\n\nThere are a number of different waste treatment technologies for the disposal, recycling, storage, or energy recovery from different waste types. Each type has its own associated methods of waste Management\n\nMunicipal solid waste consists mainly of household and commercial waste which is disposed of by or on behalf of a local authority. Landfills waste are categorized by either being hazardous, non-hazardous or inert waste. In order for a landfill design to be considered it must abide by the following requirements: final landforms profile, site capacity, settlement, waste density, materials requirements and drainage.\n\nThe advantages of the incineration are reduction of volume and mass by burning, reduction to a percentage of sterile ash, source of energy, increase of income by selling bottom ash, and is also environmentally acceptable.\nThe disadvantages of incineration are the following:\nEmissions from incinerators consist of particulates, heavy metals, pollutant gases, odor dust and litter. Due to incomplete combustion, products such as dioxins and furans are formed.\n\nThe human sewage and the process waste from the manufacturing industries are the two major sources of the waste water. In Thailand, the total volume of the wastewater from industries is much greater than that of the domestic sewage. As a result, an effective method is needed. Microbial remediation of xenobiotics has shown to be effective and the low cost technology, but it still has several limitations. Consequently, the genetic engineering approaches are used to create the new strain of microbes (Genetically engineered microorganisms, GEMS) which have better catabolic potential than the wild type species for bioremediation. There are four major approaches to GEM development for the bioremediation application which include the modification of enzyme specificity and affinity, pathway construction and regulation, bioprocess development, monitoring and control and lastly, bio-affinity bio-receptor sensor application for chemical sensing, toxicity reduction and end point analysis. These allow the extensive use of genetically engineered microorganism. In the far future, the genetically engineered microorganisms could possibly be used to control the green house gases, convert the waste to the value-added product as well as to reduce and capture the carbon dioxide gases from the atmosphere (carbon sequestration), but lots of research still need to be done in order to make these potential become true. There is a concern regarding the use of genetically engineered microbes for the remediation of pollutants. Once the genetically microorganisms has been added, it may disperse uncontrollably and hard to be removed. However, the serious major\nconcern that leads to the limitation of GEMs development seem to be the lack of information on gene as well as the regulatory constraints.\n\n"}
{"id": "9007508", "url": "https://en.wikipedia.org/wiki?curid=9007508", "title": "Wet stacking", "text": "Wet stacking\n\nWet stacking is a condition in diesel engines in which unburned fuel passes on into the exhaust system. The word \"stacking\" comes from the term \"stack\" for exhaust pipe or chimney stack. The oily exhaust pipe is therefore a \"wet stack\".\n\nThis condition can have several causes. The most common cause is idling the engine for long intervals, which does not generate enough heat in the cylinder for a complete burn. \"Idling\" may be running at full rated operating speed, but with very little load applied. Another is excessive fueling. That may be caused by weak or leaky injectors, fuel settings turned up too high or overfueling for the given rpms. Cold weather running or other causes that prevent the engine from reaching proper operating temperature can cause a buildup of fuel due to incomplete burn that can result in 'wetstacking'. \nIn diesel generators, it is usually because the diesel engine is running at only a small percentage of its rated output. For efficient combustion, a diesel should be run under at least 60 per cent of its rated power output.\n\nIt is detectable by the presence of a black ooze around the exhaust manifold, piping and turbocharger, if fitted. It can be mistaken for lubricating oil in some cases, but it consists of the \"heavy ends\" of the diesel fuel which do not burn when combustion temperature is too low. The heavier, more oily components of diesel fuel contain more stored energy than a comparable quantity of, say, gasoline, but diesel requires an adequate loading of the engine in order to keep combustion temperature high enough to make use of it. Often, one can hear a slight miss in the engine due to fuel buildup. When the engine is first placed under a load after long periods of idling and wetstacking it may blow some black exhaust out as it burns that excess fuel off. Continuous black exhaust from the stack when under a constant load is also an indication that all the fuel is not being burned.\n"}
