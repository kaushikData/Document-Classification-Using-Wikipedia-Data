{"id": "10600906", "url": "https://en.wikipedia.org/wiki?curid=10600906", "title": "3D optical data storage", "text": "3D optical data storage\n\n3D optical data storage is any form of optical data storage in which information can be recorded or read with three-dimensional resolution (as opposed to the two-dimensional resolution afforded, for example, by CD).\n\nThis innovation has the potential to provide petabyte-level mass storage on DVD-sized discs (120mm). Data recording and readback are achieved by focusing lasers within the medium. However, because of the volumetric nature of the data structure, the laser light must travel through other data points before it reaches the point where reading or recording is desired. Therefore, some kind of nonlinearity is required to ensure that these other data points do not interfere with the addressing of the desired point.\n\nNo commercial product based on 3Doptical data storage has yet arrived on the mass market, although several companies are actively developing the technology and claim that it may become available 'soon'.\n\nCurrent optical data storage media, such as the CD and DVD store data as a series of reflective marks on an internal surface of a disc. In order to increase storage capacity, it is possible for discs to hold two or even more of these data layers, but their number is severely limited since the addressing laser interacts with every layer that it passes through on the way to and from the addressed layer. These interactions cause noise that limits the technology to approximately 10layers. 3D optical data storage methods circumvent this issue by using addressing methods where only the specifically addressed voxel (volumetric pixel) interacts substantially with the addressing light. This necessarily involves nonlinear data reading and writing methods, in particular nonlinear optics.\n\n3D optical data storage is related to (and competes with) holographic data storage. Traditional examples of holographic storage do not address in the third dimension, and are therefore not strictly \"3D\", but more recently 3D holographic storage has been realized by the use of microholograms. Layer-selection multilayer technology (where a multilayer disc has layers that can be individually activated e.g. electrically) is also closely related.\n\nAs an example, a prototypical 3D optical data storage system may use a disc that looks much like a transparent DVD. The disc contains many layers of information, each at a different depth in the media and each consisting of a DVD-like spiral track. In order to record information on the disc a laser is brought to a focus at a particular depth in the media that corresponds to a particular information layer. When the laser is turned on it causes a photochemical change in the media. As the disc spins and the read/write head moves along a radius, the layer is written just as a DVD-R is written. The depth of the focus may then be changed and another entirely different layer of information written. The distance between layers may be 5 to 100 micrometers, allowing >100 layers of information to be stored on a single disc.\n\nIn order to read the data back (in this example), a similar procedure is used except this time instead of causing a photochemical change in the media the laser causes fluorescence. This is achieved e.g. by using a lower laser power or a different laser wavelength. The intensity or wavelength of the fluorescence is different depending on whether the media has been written at that point, and so by measuring the emitted light the data is read.\n\nThe size of individual chromophore molecules or photoactive color centers is much smaller than the size of the laser focus (which is determined by the diffraction limit). The light therefore addresses a large number (possibly even 10) of molecules at any one time, so the medium acts as a homogeneous mass rather than a matrix structured by the positions of chromophores.\n\nThe origins of the field date back to the 1950s, when Yehuda Hirshberg developed the photochromic spiropyrans and suggested their use in data storage. In the 1970s, Valeri Barachevskii demonstrated that this photochromism could be produced by two–photon excitation, and finally at the end of the 1980s Peter M. Rentzepis showed that this could lead to three-dimensional data storage. Most of the developed systems are based to some extent on the original ideas of Rentzepis. A wide range of physical phenomena for data reading and recording have been investigated, large numbers of chemical systems for the medium have been developed and evaluated, and extensive work has been carried out in solving the problems associated with the optical systems required for the reading and recording of data. Currently, several groups remain working on solutions with various levels of development and interest in commercialization.\n\nData recording in a 3D optical storage medium requires that a change take place in the medium upon excitation. This change is generally a photochemical reaction of some sort, although other possibilities exist. Chemical reactions that have been investigated include photoisomerizations, photodecompositions and photobleaching, and polymerization initiation. Most investigated have been photochromic compounds, which include azobenzenes, spiropyrans, stilbenes, fulgides, and diarylethenes. If the photochemical change is reversible, then rewritable data storage may be achieved, at least in principle. Also, MultiLevel Recording, where data is written in \"grayscale\" rather than as \"on\" and \"off\" signals, is technically feasible.\n\nAlthough there are many nonlinear optical phenomena, only multiphoton absorption is capable of injecting into the media the significant energy required to electronically excite molecular species and cause chemical reactions. Two-photon absorption is the strongest multiphoton absorbance by far, but still it is a very weak phenomenon, leading to low media sensitivity. Therefore, much research has been directed at providing chromophores with high two-photon absorption cross-sections.\n\nWriting by two-photon absorption can be achieved by focusing the writing laser on the point where the photochemical writing process is required. The wavelength of the writing laser is chosen such that it is not linearly absorbed by the medium, and therefore it does not interact with the medium except at the focal point. At the focal point two-photon absorption becomes significant, because it is a nonlinear process dependent on the square of the laser fluence.\n\nWriting by two-photon absorption can also be achieved by the action of two lasers in coincidence. This method is typically used to achieve the parallel writing of information at once. One laser passes through the media, defining a line or plane. The second laser is then directed at the points on that line or plane that writing is desired. The coincidence of the lasers at these points excited two-photon absorption, leading to writing photochemistry.\n\nAnother approach to improving media sensitivity has been to employ resonant two-photon absorption (also known as \"1+1\" or \"sequential\" two–photon absorbance). Nonresonant two-photon absorption (as is generally used) is weak since in order for excitation to take place, the two exciting photons must arrive at the chromophore at almost exactly the same time. This is because the chromophore is unable to interact with a single photon alone. However, if the chromophore has an energy level corresponding to the (weak) absorption of one photon then this may be used as a stepping stone, allowing more freedom in the arrival time of photons and therefore a much higher sensitivity. However, this approach results in a loss of nonlinearity compared to nonresonant two–photon absorbance (since each two-photon absorption step is essentially linear), and therefore risks compromising the 3D resolution of the system.\n\nIn microholography, focused beams of light are used to record submicrometre-sized holograms in a photorefractive material, usually by the use of collinear beams. The writing process may use the same kinds of media that are used in other types of holographic data storage, and may use two–photon processes to form the holograms.\n\nData may also be created in the manufacturing of the media, as is the case with most optical disc formats for commercial data distribution. In this case, the user can not write to the disc it is a ROM format. Data may be written by a nonlinear optical method, but in this case the use of very high power lasers is acceptable so media sensitivity becomes less of an issue.\n\nThe fabrication of discs containing data molded or printed into their 3D structure has also been demonstrated. For example, a disc containing data in 3D may be constructed by sandwiching together a large number of wafer-thin discs, each of which is molded or printed with a single layer of information. The resulting ROM disc can then be read using a 3D reading method.\n\nOther techniques for writing data in three-dimensions have also been examined, including:\n\nPersistent spectral hole burning (PSHB), which also allows the possibility of spectral multiplexing to increase data density. However, PSHB media currently requires extremely low temperatures to be maintained in order to avoid data loss.\n\nVoid formation, where microscopic bubbles are introduced into a media by high intensity laser irradiation.\n\nChromophore poling, where the laser-induced reorientation of chromophores in the media structure leads to readable changes.\n\nThe reading of data from 3D optical memories has been carried out in many different ways. While some of these rely on the nonlinearity of the light-matter interaction to obtain 3D resolution, others use methods that spatially filter the media's linear response. Reading methods include:\n\nTwo photon absorption (resulting in either absorption or fluorescence). This method is essentially two-photon microscopy.\n\nLinear excitation of fluorescence with confocal detection. This method is essentially confocal laser scanning microscopy. It offers excitation with much lower laser powers than does two-photon absorbance, but has some potential problems because the addressing light interacts with many other data points in addition to the one being addressed.\n\nMeasurement of small differences in the refractive index between the two data states. This method usually employs a phase contrast microscope or confocal reflection microscope. No absorption of light is necessary, so there is no risk of damaging data while reading, but the required refractive index mismatch in the disc may limit the thickness (i.e. number of data layers) that the media can reach due to the accumulated random wavefront errors that destroy the focused spot quality.\n\nSecond harmonic generation has been demonstrated as a method to read data written into a poled polymer matrix.\n\nOptical coherence tomography has also been demonstrated as a parallel reading method.\n\nThe active part of 3D optical storage media is usually an organic polymer either doped or grafted with the photochemically active species. Alternatively, crystalline and sol-gel materials have been used.\n\nMedia for 3D optical data storage have been suggested in several form factors: disk, card and crystal.\n\nA disc media offers a progression from CD/DVD, and allows reading and writing to be carried out by the familiar spinning disc method.\n\nA credit card form factor media is attractive from the point of view of portability and convenience, but would be of a lower capacity than a disc.\n\nSeveral science fiction writers have suggested small solids that store massive amounts of information, and at least in principle this could be achieved with 5D optical data storage.\n\nThe simplest method of manufacturing the molding of a disk in one piece is a possibility for some systems. A more complex method of media manufacturing is for the media to be constructed layer by layer. This is required if the data is to be physically created during manufacture. However, layer-by-layer construction need not mean the sandwiching of many layers together. Another alternative is to create the medium in a form analogous to a roll of adhesive tape.\n\nA drive designed to read and write to 3D optical data storage media may have a lot in common with CD/DVD drives, particularly if the form factor and data structure of the media is similar to that of CD or DVD. However, there are a number of notable differences that must be taken into account when designing such a drive.\n\nParticularly when two-photon absorption is utilized, high-powered lasers may be required that can be bulky, difficult to cool, and pose safety concerns. Existing optical drives utilize continuous wave diode lasers operating at 780 nm, 658 nm, or 405 nm. 3D optical storage drives may require solid-state lasers or pulsed lasers, and several examples use wavelengths easily available by these technologies, such as 532 nm (green). These larger lasers can be difficult to integrate into the read/write head of the optical drive.\n\nBecause the system must address different depths in the medium, and at different depths the spherical aberration induced in the wavefront is different, a method is required to dynamically account for these differences. Many possible methods exist that include optical elements that swap in and out of the optical path, moving elements, adaptive optics, and immersion lenses.\n\nIn many examples of 3D optical data storage systems, several wavelengths (colors) of light are used (e.g. reading laser, writing laser, signal; sometimes even two lasers are required just for writing). Therefore, as well as coping with the high laser power and variable spherical aberration, the optical system must combine and separate these different colors of light as required.\n\nIn DVD drives, the signal produced from the disc is a reflection of the addressing laser beam, and is therefore very intense. For 3D optical storage however, the signal must be generated within the tiny volume that is addressed, and therefore it is much weaker than the laser light. In addition, fluorescence is radiated in all directions from the addressed point, so special light collection optics must be used to maximize the signal.\n\nOnce they are identified along the z-axis, individual layers of DVD-like data may be accessed and tracked in similar ways to DVDs. The possibility of using parallel or page-based addressing has also been demonstrated. This allows much faster data transfer rates, but requires the additional complexity of spatial light modulators, signal imaging, more powerful lasers, and more complex data handling.\n\nDespite the highly attractive nature of 3D optical data storage, the development of commercial products has taken a significant length of time. This results from limited financial backing in the field, as well as technical issues, including:\n\nDestructive reading. Since both the reading and the writing of data are carried out with laser beams, there is a potential for the reading process to cause a small amount of writing. In this case, the repeated reading of data may eventually serve to erase it (this also happens in phase change materials used in some DVDs). This issue has been addressed by many approaches, such as the use of different absorption bands for each process (reading and writing), or the use of a reading method that does not involve the absorption of energy.\n\nThermodynamic stability. Many chemical reactions that appear not to take place in fact happen very slowly. In addition, many reactions that appear to have happened can slowly reverse themselves. Since most 3D media are based on chemical reactions, there is therefore a risk that either the unwritten points will slowly become written or that the written points will slowly revert to being unwritten. This issue is particularly serious for the spiropyrans, but extensive research was conducted to find more stable chromophores for 3D memories.\n\nMedia sensitivity. two-photon absorption is a weak phenomenon, and therefore high power lasers are usually required to produce it. Researchers typically use Ti-sapphire lasers or s to achieve excitation, but these instruments are not suitable for use in consumer products.\n\nMuch of the development of 3D optical data storage has been carried out in universities. The groups that have provided valuable input include:\n\nIn addition to the academic research, several companies have been set up to commercialize 3D optical data storage and some large corporations have also shown an interest in the technology. However, it is not yet clear whether the technology will succeed in the market in the presence of competition from other quarters such as hard drives, flash storage, and holographic storage.\n\n\n"}
{"id": "101596", "url": "https://en.wikipedia.org/wiki?curid=101596", "title": "8-track tape", "text": "8-track tape\n\nThe 8-track tape (formally Stereo 8; commonly known as the eight-track cartridge, eight-track tape, or simply eight-track) is a magnetic tape sound-recording technology that was popular in the United States from 1964 to 1988, when the Compact Cassette format took over. The format is regarded as an obsolete technology, and was relatively unknown outside the United States, the United Kingdom, Canada, New Zealand, Australia, Germany and Japan.\n\nStereo 8 was created in 1964 by a consortium led by Bill Lear of Lear Jet Corporation, along with Ampex, Ford Motor Company, General Motors, Motorola, and RCA Victor Records (RCA - Radio Corporation of America). It was a further development of the similar Stereo-Pak four-track cartridge introduced by Earl \"Madman\" Muntz (marketing and television set dealer), which was adapted by Muntz from the Fidelipac cartridge developed by George Eash. A later quadraphonic (four-channel sound as opposed to earlier more widely used stereo/two channel sound) version of the format was announced by RCA in April 1970 and first known as Quad-8, then later changed to just Q8.\n\nThe original format for magnetic tape sound reproduction was the reel-to-reel tape recorder, first available in the United States in the late 1940s, but too expensive and bulky to be practical for amateur home use until well into the 1950s. Loading a reel of tape onto the machine and threading it through the various guides and rollers proved daunting to some casual users—certainly, it was more difficult than putting a vinyl record on a record player and flicking a switch. Because in early years each tape had to be dubbed from the master tape in real-time to maintain good sound quality, prerecorded tapes were more expensive to manufacture, and costlier to buy, than vinyl records which could be stamped much more quickly than their own playing time.\n\nTo eliminate the nuisance of tape-threading, various manufacturers introduced cartridges that held the tape inside a metal or plastic housing to eliminate handling. Most were intended only for low-fidelity voice recording in dictation machines. The first tape cartridge designed for general consumer use, including music reproduction, was the Sound Tape or Magazine Loading Cartridge (RCA tape cartridge), introduced in 1958 by RCA. Prerecorded stereophonic music cartridges were available, and blank cartridges could be used to make recordings at home, but the format failed to gain popularity.\n\nThe endless loop tape cartridge was first designed in 1952 by Bernard Cousino around a single reel carrying a continuous loop of standard 1/4-inch, plastic, oxide-coated recording tape running at per second. Program starts and stops were signaled by a one-inch-long metal foil that activates the track-change sensor. (Bill Lear had tried to create an endless-loop wire recorder in the 1940s, but gave up in 1946. He would be inspired by Earl Muntz's four-track design in 1963.)\n\nInventor George Eash invented a cartridge design in 1953, called the Fidelipac cartridge. The Eash cartridge was later licensed by manufacturers, notably the Collins Radio Company, which first introduced a cartridge system for broadcasting at the National Association of Broadcasters 1959 annual show. Fidelipac cartridges (nicknamed \"carts\" by DJs and radio engineers) were used by many radio stations for commercials, jingles, and other short items. Eash later formed Fidelipac Corporation to manufacture and market tapes and recorders, as did several others, including Audio-Pak (Audio Devices Corp.).\n\nThere were several attempts to sell music systems for cars, beginning with the Chrysler Highway Hi-Fi of the late 1950s (which used discs). Entrepreneur, marketer and television set dealer Earl \"Madman\" Muntz of Los Angeles, California, however, saw a potential in these \"broadcast carts\" for an automobile music system. In 1962, he introduced his Stereo-Pak four-track cartridge stereo system and tapes, mostly in California and Florida. The four tracks were divided into two \"programs\", typically corresponding to the two sides of an LP record, with each program comprising two tracks read simultaneously for stereo (two channel) sound playback. He licensed popular music albums from the major record companies and duplicated them on these four-track cartridges, or \"CARtridges\", as they were first advertised.\n\nThe Lear Jet Stereo 8 track cartridge was designed by Richard Kraus while working under Bill Lear and for his Lear Jet Corporation in 1963. The major change was to incorporate a neoprene rubber and nylon pinch roller into the cartridge itself, rather than to make the pinch roller a part of the tape player, reducing mechanical complexity. Lear also eliminated some of the internal parts of the Eash cartridge, such as the tape-tensioning mechanism and an interlock that prevented tape slippage. By doubling the number of tracks from 4 to 8, the recording length doubled to 80 minutes.\n\nIn 1964, Lear's aircraft company constructed 100 demonstration Stereo 8 players for distribution to executives at RCA and the auto companies.\n\nThe popularity of both four-track and eight-track cartridges grew from the booming automobile industry. In September 1965, the Ford Motor Company introduced factory-installed and dealer-installed eight-track tape players as an option on three of its 1966 models (the sporty Mustang, Thunderbird, and the luxurious high-end Lincoln), and RCA Victor introduced 175 Stereo-8 Cartridges from its RCA Victor and RCA Camden labels of recording artists catalogs. By the 1967 model year, all of Ford's vehicles offered this tape player upgrade option. Most of the initial factory installations were separate players from the radio (such as shown in the image), but dashboard mounted 8-track units were offered in combination with an AM radio, as well as with AM/FM receivers. Muntz, and a few other manufacturers, also offered 4/8 or \"12-track\" players that were capable of playing cartridges of either format, 4-track or 8-track. With the backing of the U.S. automakers, the eight-track format quickly won out over the four-track format, with Muntz abandoning it completely by late 1970.\n\nDespite its problems, the format gained steady popularity because of its convenience and portability. Home players were introduced in 1966 that allowed consumers to share tapes between their homes and portable systems. By the late 1960s, the 8-track segment was the largest in the consumer electronics market and the popularity of 8-track systems for cars helped generate demand for home units. \"Boombox\" type portable players were also popular but eight-track player/recorders failed to gain wide popularity and few manufacturers offered them except for manufacturer Tandy Corporation (for its Radio Shack electronics stores). With the availability of cartridge systems for the home, consumers started thinking of eight-tracks as a viable alternative to 33 rpm album style vinyl records, not only as a convenience for the car. Within a year, prerecorded releases on the eight-track tape format began to arrive within a month of the vinyl release. The eight-track format became by far the most popular and offered the largest music library of all the tape systems.\n\nEight-track players were fitted as standard equipment in most Rolls-Royce and Bentley cars of the period for sale in Great Britain and worldwide. Optional 8-track players were available in many cars and trucks through the early 1980s.\n\nAmpex, based in Elk Grove Village, Illinois, set up a European operation (Ampex Stereo Tapes) in London, England, in 1970 under general manager Gerry Hall, with manufacturing in Nivelles, Belgium, to promote 8-track product (as well as musicassettes) in Britain and in Europe, but it struggled and folded in 1974.\n\nQuadraphonic sound on eight-track cartridges (announced by RCA in April 1970) were also produced, Ford being particularly eager to promote in-car quadraphonic players as a pricey option. Neither Chrysler, General Motors, nor American Motors of the other three of the \"Big Four\" American automotive companies however offered a quadraphonic tape player. The format enjoyed a moderate amount of success for a time in the early 1970s but faded by mid-decade. These cartridges are sought by collectors since they provide four channels of discrete sound, unlike matrixed formats such as SQ, which Columbia/CBS Records used for their quadraphonic sound vinyl records.\n\nDaisuke Inoue invented the first karaoke machine in 1971 called the Juke-8.\n\nMilton Bradley's (MB) OMNI Entertainment System was an electronic quiz machine game first released in 1980, similar to \"Jeopardy!\" or later \"You Don't Know Jack\" video game series, using 8-track tapes for playback analog audio for questions, instructions and answers as well as digital signals in magnetic tape data storage on remaining tracks to load the right answer for counting the score. In 1978, the Mego Corporation launched the 2-XL toy robot, which was similar.\n\nEight-track players became less common in homes and vehicles in the late 1970s. The compact cassette arrived in 1962, and by the 1970s the eight-track cartridges had greatly diminished in popularity. In some Latin American countries as well as European, the format was abandoned in the mid-1970s in favor of the smaller tape cassette which was one-third the size.\n\nIn the U.S.A., eight-track cartridges were phased out of retail stores by late 1982. Some titles were still available as eight-track tapes through Columbia House and RCA (BMG) Music Service \"Record Clubs\" until late 1988. Many of these late-period releases are highly collectible because of the low numbers that were produced and the few customers who ever purchased them. Among the most rare is Stevie Ray Vaughan's \"Texas Flood\". Another is Bruce Springsteen and the E Street Band's \"Live/1975-85\", which was one of the very few boxed sets to be released on vinyl, cassette, compact disc, and eight-track tape.\n\nThere is a debate among collectors about the last commercial eight-track released by a major label, but the fan/enthusiast internet website \"8 Track Heaven\" cannot find a major label release past Fleetwood Mac's \"Greatest Hits\" in November 1988. Radio Shack (Tandy Corporation) continued to sell blank eight-track cartridges for home recording use under its Realistic brand until 1990.\n\nThe professional broadcast cart format survived for more than another decade at most local radio stations. It was used to play and switch short brief jingles, advertisements, station identifications, and music content until it was replaced by various computer-based methods in the 1990s. This format survived longer because it was used for relatively short sound loops, where starting from the beginning was more important than other criteria. The endless loop tape concept continues to be used in modern movie projectors, although in that application the spool is actively rotated and not drawn by tension on the film. That too, however, is now being supplanted by digital cinema technologies.\n\nSome independent artists still release eight-track tapes, such as the U.\nS. band RTB2, who released \"We Are a Strange Man\" in 2011. Also, bands sometimes release eight-tracks as special releases; for example, The Melvins released a limited-time, live eight-track album, Cheap Trick issued a limited edition version of their album \"The Latest\" on the format on June 23, 2009, and power electronics solo artist Waves Crashing Piano Chords has released multiple eight-track tapes since 2012, as well as starting an eight-track tape label, H8-Track Stereo. In the book Journals, Kurt Cobain wrote about wanting to release Nirvana's album \"In Utero\" as an 8-track tape, but this never happened. Apart from a select group of highly collectible artists, the record club issues, and the quadraphonic releases, many eight-track tapes seem to have limited value to most collectors, especially if the tapes have been misused or appear to be worn. They are however still to be found at some thrift second-hand stores, flea markets and internet auctions.\n\nIn the Cousino, Eash, Muntz, and Lear cartridges, tape was pulled from the center of the reel, passed across the opening at one end of the cartridge and wound back onto the outside of the same reel. The spool itself was freewheeling and the tape was driven only by tension from the capstan and pinch roller.\n\nWith a reel turning at a constant rate, the tape around the hub has a lower linear velocity than the tape at the outside of the reel, so the tape layers must slip past each other as they approach the center. The tape was coated with a slippery backing material, usually graphite and patented by Bernard Cousino, to ease the continuous slip between the tape layers. While the design allowed simple, cheap, and mobile players, unlike a two-reel system, it did not permit rewinding of the tape. Some players offered fast-forward by speeding up the motor while cutting off the audio.\n\nMuntz's cartridge had used two pairs of stereo tracks in the same configuration as then-current \"quarter track\" reel-to-reel tapes. This format was intended to parallel his source material, which was usually a single LP (long playing) record with two sides. Program switching was achieved by physically moving the head up and down mechanically by a lever. The Stereo 8 version doubled the amount of programming on the tape by providing eight total tracks, usually comprising four programs of two tracks each. Lear touted this as a great improvement, because much more music could be held inside a standard cartridge housing, but in practice this resulted in a slight loss of sound quality and an increase in background noise from the narrower tape tracks. Unlike the Stereo-Pak, the Stereo 8 could switch between tracks automatically, with the use of a small length of conductive foil at the splice joint on the tape, which would cause the player to change tracks as it passed the head assembly.\n\nThe cartridges have an audible pause due to the presence of a length of metallic foil, which a sensor detects and signals the end of the tape and acts as a splice for the loop. The foil passes across a pair of electrical contacts which are in the tape path. Contact of the foil closes an electrical circuit that engages a solenoid which mechanically shifts the tape head to the level of the next track.\n\nMost players produced a mechanical click when switching programs, although early Lear players switched silently. Because of the expense of producing tape heads capable of reading eight tracks, most eight-track players have heads that read just two tracks. Switching from program to program is accomplished by moving the head itself. Since the alignment of the head to the tape is crucial to any tape system, and because eight-track systems were generally designed to be cheap, this configuration further degraded the sound of the eight-track tape.\n\nThe Stereo 8 system was fairly simple, mechanically, but presented difficulties in various primary areas:\n\n\nWhen the sliding tape pack would pull itself tight, for whatever reason, a jammed 8-track cartridge was the result. A quick solution was to hold the cartridge in one hand, facing down, while pulling out a section of, about 4-6' in length from the outer winding side. A quick tug on the tape would cause it to immediately wind in and the result was a loosened up tape pack that would play correctly.\n\nFailing that, another solution was to open the cartridge, cut the tape at the splice, and relieve the excess tension by manually unwinding one or two sections from the outer edge of tape (\"loop length\") while keeping the reel stationary, then re-splicing the tape, with a fresh piece of foil. Another, simpler fix was to shake the cassette in the plane of the tape reel with a rotary motion, sometimes this would cause the windings inside to rotate and loosen. If the cartridge has shed its graphite backing, it would have to be discarded. Small businesses that specialize in transferring audio tapes to digital format can remove the tape from the surrounding plastic cartridge box and play it on a small reel-to-reel player to extract maximum sound fidelity.\n\nA decrease in the quality of the parts used in the eight-track cartridge, that is, \"plastic\" pinch rollers, lubricant quality and quantity, etc., was another blow to the faltering format. As these problems further reduced the reliability, sound quality, and consistent tape speed, the eight-track eventually developed a reputation for being unreliable.\n\nThe Stereo 8 introduced the problem of dividing up the programming intended for a two-sided LP record into four programs. Often this resulted in songs being split into two parts (the split was often made during an instrumental break or a repeated chorus), song orders being reshuffled, shorter songs being repeated, and songs separated by long passages of silence. Some eight-tracks included extra musical content to fill in time such as a piano solo on Lou Reed's \"Berlin\", extra verses on The Rolling Stones' \"Some Girls\" and a guitar solo in Pink Floyd's \"Animals\".\n\nIn rare instances, an eight-track was able to be arranged exactly like the record album version, without any song breaks. Examples of this are \"Quadrophenia\" by The Who, and some versions of \"Days of Future Passed\" by The Moody Blues. Other examples of this rarity are \"Freeways\" by Bachman-Turner Overdrive, \"Live - Bursting Out\" by Jethro Tull, \"Live Bullet\" and \"Nine Tonight\" by Bob Seger and the Silver Bullet Band, \"Caught Live + 5\" by The Moody Blues, \"The Concert in Central Park\" by Simon & Garfunkel, \"Layla and Other Assorted Love Songs\" by Derek and The Dominoes, \"Octave\" by The Moody Blues, the US version of \"Three Sides Live\" by Genesis, \"Pictures at Eleven\" by Robert Plant and \"Coda\" by Led Zeppelin (a record club only release).\n\n\n"}
{"id": "32682509", "url": "https://en.wikipedia.org/wiki?curid=32682509", "title": "ATLAS-I", "text": "ATLAS-I\n\nATLAS-I was the largest NNEMP (Non-Nuclear Electromagnetic Pulse) generator in the world, designed to test the radiation hardening of strategic aircraft systems against EMP pulses from nuclear warfare. Built at a cost of $60 million, it was composed of two parts: a pair of powerful Marx generators capable of simulating the electromagnetic pulse effects of a high-altitude nuclear explosion (HANE) of the type expected during a nuclear war, and a giant wooden trestle built in a bowl-shaped arroyo, designed to elevate the test aircraft above ground interference and orient it below the pulse in a similar manner to what would be seen in mid-air.\n\nTrestle is the world's largest structure composed entirely of wood and glue laminate.\n\nThe electromagnetic pulse was produced by a pair of Marx generators built by Maxwell Laboratories of San Diego, California. The generators were mounted on pedestals constructed of wood in the same manner as the main test platform, one on each side of a large wedge shaped steel structure which acted as a ground plane for the horizontally polarized pulse. Each Marx generator consisted of a stack of 50 trays, each containing two large capacitors and a plasma switch. A large peaking capacitor, used to adjust the shape of the resulting pulse, was also part of the design. Each generator was enclosed in a large fiberglass structure which was filled with sulfur hexafluoride (SF) acting as an insulating gas. The tray capacitors were slowly charged such that each tray had up to 100kV of potential. When discharged through the plasma switches, the 50 trays in series could (ideally) produce up to 5 megavolts of electrical potential in a pulse with a rise-time in the 100 nanosecond range. The generators on either side of the wedge were charged to opposite polarities and fired into twin transmission lines (antennas) mounted on either side of the test platform. When triggered simultaneously the resulting EM waves from each generator combined at the sharp point of the wedge building, adding to a total electrical potential of 10 megavolts. The transmission lines were terminated into a 50 ohm low inductance resistive load mounted on a tall wooden tower at the far end of the platform. The result was a fast 200 gigawatt pulse of electromagnetic flux powerful enough to reliably reproduce (at short range) the deleterious effects of a thermonuclear detonation on electronic circuitry as created by such examples as the HARDTACK I, ARGUS and DOMINIC I (Operation Fishbowl) high altitude nuclear tests.\n\nDue to their higher flight altitude and nuclear payload, Strategic Air Command bombers were the primary object of the tests, but fighters, transport aircraft and even missiles were also tested for EMP hardness on Trestle. In addition to electronics survivability tests, numerous sensors located inside, beneath and to the sides of the aircraft would gather additional data on the airframe's EMP permeability to be used in design considerations for future Cold War aircraft and to identify areas which needed additional EM hardening.\n\nThe advances made in EMP generation technology by Sandia during the operation of Trestle greatly assisted in the construction of the much more powerful 40 megavolt, 50 terawatt (50,000 gigawatt) Z Machine at Sandia during the 1990s. Technological advances during the 2000s have since boosted this output to 290 terawatts (290,000 gigawatts), high enough to actually study nuclear fusion at the point of detonation.\nThe primary wooden structure of Trestle was 1,000 feet long, 125 feet (about 12 stories) tall, and constructed of 6.5 million board-feet of lumber, sufficient to support a fully loaded B-52 (then the largest and heaviest strategic bomber in the US inventory) while also minimizing any chance of interference from the ground or the structure itself, creating a reasonable simulation of airborne conditions. A mix of Douglas Fir and Southern Yellow Pine were used for the timbers, as both showed excellent EMP transparency with the former having the best tensile strength and the latter the best weather resistance. By using an all glued laminated timber structure and woodworking joints to mate the giant timbers, with the joints being held together with wooden bolts and nuts, measurements from the EMP tests would not be skewed by large amounts of ferrous material in the structure. Some metal was used in the construction as critically loaded joints incorporated a circular steel sheer ring that surrounded the wooden bolt clamping the joint. Even the fire escape along one side of the trestle and the whole of the extensive fire suppression piping were constructed of fiberglass.\n\nThe wedge shaped structure was of steel I-beam construction. The entire structure was covered with a wire mesh similar to livestock fencing in order to create an enormous Faraday cage. A multistory building was constructed inside the wedge which served as offices, laboratories and testing facilities. The second floor of the building housed a large electromagnetically shielded room, supplied by Electromagnetic Filter Company of Palo Alto, California, which contained the data acquisition electronics, the Marx generator charging and firing control and field strength monitoring instrumentation. The data acquisition system consisted of a large number of state-of-the-art Tektronix 7912AD digitizers along with a large array of DEC PDP-11 computers. The pulse monitoring instrumentation consisted of a number of B dot and H field sensors mounted on the exterior of the wedge connected to oscilloscopes fitted with Polaroid o'scope cameras needed to capture the transient pulse data. The open-air third floor held large inflatable gas bags which could store the sulfur hexafluoride (SF) gas from the Marx generator enclosures when they needed to be opened for maintenance.\n\nThe ATLAS-I program was shut down after the end of the Cold War in 1991, which brought an end to destructive EMP testing of aircraft, being replaced by far cheaper computer simulations as technology improved. Despite going without maintenance for over 20 years, the wooden trestle structures are all still standing and it remains the biggest metal-free wood laminate structure in the world. The trestle has, however, become a significant fire hazard since the [pentachlorophenol-isobutane-ether treated] wood has dried considerably in the desert conditions and the automatic fire sprinkler system was deactivated in 1991. Efforts are underway to secure the funding necessary to have the structure protected as a national historic landmark, although efforts are complicated by the top secret nature of the Sandia/Kirtland facility it is situated on.\n\nThe trestle structure is still easily visible from commercial aircraft landing and taking off from Albuquerque International Sunport, lying about one mile to the southeast of the threshold of Runway 26.\n\n"}
{"id": "34008776", "url": "https://en.wikipedia.org/wiki?curid=34008776", "title": "Ammunition categories for carriage on scheduled flights", "text": "Ammunition categories for carriage on scheduled flights\n\nAmmunition categories are used to define ammunition for weapons in terms of safety, for storage and transport. The Division system categorises any explosive material by the speed of burning and amount of explosive.The United Nations has also instituted a categorisation system specifically for handgun ammunition. Both these systems can be used together to define safe amounts of ammunition that can be carried on various modes of transport, including civilian scheduled flights.\n\n\n\nThe United Nations has categorised all forms of dangerous goods and the categories regarding explosive materials are listed at List of UN numbers#UN 0001 to 1000\n\nThe phrase Div. 1.4S, UN 0012 or UN 0014 denotes the categories of ammunition that the IATA permits to be carried on passenger flights. In simple terms, each passenger may carry up to 5 kg of weapons cartridges of less than 19.1 mm caliber being either blanks or with solid projectile(s), in their checked baggage. \nThe IATA published the minimum requirement for an airline for the carriage of dangerous goods in a table, where ammunition of the following nature can only be carried subject to the following permissions and approvals:\n\n\n\n\n\n"}
{"id": "4862767", "url": "https://en.wikipedia.org/wiki?curid=4862767", "title": "Balance board", "text": "Balance board\n\nA balance board is a device used as a circus skill, for recreation, balance training, athletic training, brain development, therapy, musical training and other kinds of personal development.\n\nIt is a lever similar to a see-saw that the user usually stands on, usually with the left and right foot at opposite ends of the board. The user's body must stay balanced enough to keep the board's edges from touching the ground and to keep from falling off the board.\n\nA different challenge is presented by each of the five basic types of balance boards and their subtypes. Some of them can be attempted successfully by three-year-olds and elderly people, and some, because of their steepness and speed, are difficult and dangerous for professional athletes.\n\nIn their design, what differentiates the five types (and their subtypes) is how unstable each of them is, i.e., in how many and in which of the three dimensions of space each board turns and/or sways and how freely its fulcrum contacts the board and the ground.\n\nIn 1953, Stanley Washburn Jr. filed a patent for a balance board with the intention of its use for recreation. These boards quickly become popular for skiers and surfers to practice their balancing skills in the off season or when natural conditions were poor. The balance board is a device that has come to be used for training in sports and martial arts, for physical fitness and for non-athletic purposes that are listed here.\nIt is used to develop balance, motor coordination skills, weight distribution and core strength; to prepare people, before and after they reach old age, to avoid injurious falls; to prevent sports injuries, especially to the ankle and knee; and for rehabilitation after injuries to several parts of the body.\n\nUses of a balance board beyond its athletic origin have become more common: to expand neural networks that enable the left and right hemispheres of the brain to communicate with each other, thereby increasing its efficiency; to develop sensory integration and cognitive skills in children with developmental disorders; to make dancers lighter on their feet; to teach singers optimal posture for the control of air-flow; to teach musicians how to hold their instrument; to shake off writer's block and other inhibitors of creativity; as an accessory to yoga and as a form of yoga, cultivating holistic health, self-awareness and calm. \n\nSome people use a balance board for recreational purposes, enjoying the challenge that this equipment presents.\n\nThe balance board is used as a circus skill for recreation and performance. Many circus performers refer to the balance board as the rola bola. Skillful and dramatic balancing acts using the rola bola are performed by circus performers in traditional circus as well as by freelance circus skills artists. The performance can involve a single rola bola or a stack of multiple rola bolas on top of one another to increase the challenge and visual spectacle. Some circus performers also combine the use of the rola bola with other circus skills such as Juggling or Equilibristics to present a different visual spectacle.\n\nThe user stands on a board or other platform which is on top of an unstable ground-contacting member, the fulcrum. The height of the fulcrum of most models is between 3 and 6 inches (i.e., the top of the fulcrum is that distance above the ground). Due to the fulcrum's instability, the user must remain balanced and coordinated in order to prevent the board from touching the ground.\n\nThus, the rider stimulates, exercises and teaches the parts of the body that implement the act of balancing (toes, soles, ankles, knees, hips, shoulders, arms and neck) and the parts of the body and brain that create the sense of balance and that engineer the implementation of the act of balancing (inner ears, cerebellum, proprioceptors and eyes).\n\nThe degrees of movement through which the board can move – sliding, pivoting, rotating, tilting, rolling or some combination of those – and the speed of the board differ in different types and subtypes of models, depending on the shape and size of the fulcrum, whether it is attached to the board and, if it isn't attached, the method(s) by which it is constrained by the board, if any. With an increase of speed and with each additional degree of movement through which one model or another can move, the need to avoid losing control of the board forces the rider to exercise considerably more skill in order to avoid falling.\n\nIn rocker boards and wobble boards, the fulcrum is attached to the board. In rocker-roller boards and sphere-and-ring boards, the fulcrum is a separate piece from the board. In sphere-and-ring boards, the fulcrum (an inflatable or solid ball) is constrained by a ring on the board's underside. In some rocker-roller boards, the fulcrum (a cylinder or mainly a cylinder) isn't constrained by the board (except by their friction), and in most rocker-rollers the cylinder is constrained by the board in any of five ways (a different number and combination of those ways in each type of rocker-roller) that are described below, in this article's \"Rocker-Roller Boards\" section.\n\nPositions other than standing are also used, in order to work on particular muscles and skills. See below, under the heading \"Fitness Exercises.\"\n\nFor better foot traction, the stood-on surface of most boards is manufactured with an unsmooth texture: for plastic models, in the molding; for wooden models, with grip tape or rubber. A smooth surface under the feet or shoes can cause a user to slip off a balance board and fall.\n\nWobble boards are the only type of balance board that is commonly made of plastic. Being no longer than their width, they don't need to be as strong and warp-resistant as other balance boards.\n\nThere are more than a hundred models of balance boards on the market in the United States. Each of them is a version of one of about fifteen types of balance board. Each of these models and types can be classified as one of five basic types of balance board according to two binary parameters: whether its fulcrum is attached to the board and whether the board tilts in only two opposite directions (left and right or forward and back) or in every direction (360 degrees).\n\nMore specifically:\n\nIn other words:\n\nThose five analogies are not precise definitions. They ignore some details of models' structure.\n\nA rocker board is the most basic and least challenging type of balance board. It is a flat board with a fulcrum attached to the board's underside. In some models the fulcrum is perpendicular to the board's length and in the other models the fulcrum is two rockers that are parallel to each other and parallel to the board's length, one in front of the person who is standing on the board and one behind. The ground-contacting edge of the fulcrum is curved in most models and is flat in some models.\n\nWith one foot placed at each end of the board, the user can tilt it from side to side until the balance point is found and can then either try to keep the board stationary or continue rocking.\n\nRocker boards offer only one degree of movement: part rotation about the longitudinal axis, i.e., banking (left-right tilting).\n\nMost rocker boards are made by manufacturers of toys or of gym equipment.\n\nRocker-roller boards add a degree of instability to the rocker board that makes them much more challenging for the rider than a rocker board is. Rather than on a fixed pivot, a rocker-roller's board is placed on a cylindrical roller; this fulcrum is a wheel that moves in relation to the ground and in relation to the board. The board's pivot point shifts back and forth as the cylinder rolls beneath it. In almost all models the two flat ends of the wheel/roller are about as far from each other as the width of the board. In most models the axis of the roller is perpendicular to the board's length. Thus, as the rider's weight moves over the roller, the board both tilts from side to side and also slides sideways. In models whose roller can be placed with its axis parallel to the board's length, the board slides and tilts toward the front and back (if the rider's feet are oriented accordingly).\n\nThe roller has a different form in different models. Some are a cylinder and some are a cylinder in their midsection that tapers toward the two ends. That tapering enables tricky moves by the rider. Four of the models produced by Vew-Do Balance Boards each have a roller whose shape has a different taper, designed for doing different tricks and simulating different boardsports. (One of those four is the roller at the top of the photograph to the side of this paragraph.) Vew-Do's patent gives information about the effect that its roller's shape has on the board's movement.\n\nHow the roller and rocker interface can vary. Rollers may have grooves to fit a guide on the board to keep the roller aligned with the rocker and prevent the rocker from sliding along the roller. Rockers may have guard rails at the ends to prevent the rocker from rolling off the roller.\n\nThe diameter of the roller of almost all rocker-rollers is between 3.5 and 6 inches at its widest section. The rollers of the rola bolas that circus performers use are usually between 7 and 9 inches in diameter.\n\nThe fulcrum of almost all wobble boards is a semi-sphere or smaller spherical cap (or a shape that is approximately such) whose flat side is attached to the center of the board's underside. This allows the board to pivot in all directions during the same ride: forward-backward, left-right and anywhere in between, i.e., toward 360 degrees. Standing on a wobble board exercises muscles that are not exercised by standing on boards that tilt in only two (opposite) directions. In almost all models, the board's length and width are about the same size; a circle is the usual shape.\n\nWobble boards are widely used in child development, gymnasiums, sport training, prevention of injuries to the ankle and knee, rehabilitation after ankle, knee and hip injuries and physiotherapy.\n\nThe basic exercise is standing on the wobble board with both feet and tilting in any direction without letting the board tilt so far that its edge touches the ground. Some of the many other common exercises are squats; standing on the board with one foot while keeping the other foot off the ground; push-ups (pressing down on the board with the hands while lying face-down with only the knees or toes contacting the ground); and sit-ups (with the board under one's rear end). Any exercise is much more work when a person's weight is on a wobble board than when s/he is supported by a stable and level base such as a floor.\n\nFor additional muscle exercise while wobbling, some models can be attached to an elastic stretch band. Each hand pulls up one of the band's two ends. Each end of the band fits through one of two opposed holes near the rim of the board, for quick attachment and detachment.\n\nA wobble board offers full rotation about the vertical axis (i.e., yawing, i.e., twisting), part rotation about the transverse/lateral axis (i.e., pitching, i.e., backward-forward tilting) and part rotation about the longitudinal axis (i.e., banking, i.e., left-right tilting). A fourth and fifth degree of movement, translation (i.e., sliding) in the dimensions of the two horizontal axes, are possible for almost all wobble boards, i.e., all models except ones that have a stationary base. This sliding occurs much less often and usually across a shorter distance than in the case of a rocker-roller board and sphere-and-ring board. It is usually unintended and unwanted by a wobble board user and, presumably, also by the manufacturer and might more accurately be called skidding than sliding.\n\nPlastic is the material that most wobble boards are made of. Wooden models are better able than plastic ones to withstand long use, such as in a gym. Among plastic models, how heavy-duty the plastic is varies.\n\nWobble boards are made by manufacturers of gym, sports and physical therapy equipment.\n\nA sphere, either an inflatable rubber ball (such as a basketball) or a solid polyurethane ball, is the fulcrum on which the board is balanced, and the fulcrum is kept contained under the board by a guard rail or ring on the underside. By redistributing his/her weight across the board, the rider can move the board in any direction– side to side, forward and backward, twisting, diagonal, and full rotations or any combination of these movements. A rider can move the board vertically by doing an advanced maneuver called an ollie. It can also be tilted in any direction and fully rotated. Sphere-and-ring boards provide the greatest freedom of movement of any type of balance board, allowing rotation about all axes (yawing, pitching and banking) and translation (i.e., sliding) in both transverse (i.e., lateral) and longitudinal directions. They, like wobble boards, simultaneously exercise muscles that are not exercised by use of boards that tilt about only one axis (in two opposite directions).\n\nWhen balancing or riding on a sphere-and-ring board, the difficulty and ride speed, which is how fast the rider can move the board on the ball, are determined by the following:\n\n\nSpring boards were added to the market in 2013 with the release of StrongBoard balance, which holds a patent on the multi-spring design. The board’s fulcrum consists of four individual compression springs situated between the base and the platform. Once a user stands on the board, the springs compress. The user’s body then searches for stabilization, but because of the force applied to the springs from the user’s body weight, the board will not stabilize. By redistributing weight across the board, the rider can move the board in any direction – engaging core stabilizing muscles. The four compression springs provide additional support for a variety of exercises, target the core, and improve balance, core strength, agility, and posture.\n\nThese are underwater balance boards. They were developed for physical therapy and are used also for recreation. Besides the general advantages of aquatic therapy over non-aquatic therapy (the use of the smooth resistance of water instead of the jerky resistance of weights and the avoidance of burdening an injured joint with excessive weight– in this case, the weight of the patient's own body), aquatic balance boards have the specific advantage over non-aquatic balance boards of saving a patient who slips off of a board from the impact of falling and crashing into a floor. Slipping off of an aquatic balance board is safe as long as the user knows to avoid inhaling while underwater and knows how to tread water.\n\nThree models are produced by Theraquatics Australia: The Theraquatics Balance Board is a V-shaped rocker board that a user stands, kneels or sits on. The Wonder Board is a V-shaped rocker board that a user kneels or sits on. The Aquatic Balance Board (a.k.a. the Aquafit Balance Board) is a wobble board. The holes in it allow water to fill it, making it neutrally buoyant (i.e., neither sinking on its own nor floating up to the water's surface) so that it is easier to control and safer than it would be if this wobble board were more buoyant.\nTwo products that Theraquatics Australia calls balance boards, its Star Balance Board and its Theraquatics Balance Board with Straps, are not balance boards in the familiar sense of the term, though they can be used for practicing balance skills.\n\nProper use of a balance board is a test of both physical skill and the user's sense of balance.\n\nAnother sensation often experienced by a user of a balance board is the sensation of falling. Apart from actually falling, this often occurs during sharp accelerations caused by leaning too far or too quickly. Feeling like falling can raise fears and provoke reflexes that while useful on a stable surface can be counter-productive on a balance board, such as throwing the arms forward to catch the fall or over-correcting and sharply shifting weight onto the opposite leg, causing a fall in the opposite direction.\n\nFalls from balance boards can break bones, sprain joints, and tear tendons, ligaments and cartilage. These risks can be diminished by preparing the space, wearing protective gear and following manufacturers' other safety recommendations.\n\nRisk can be lowered by anticipating falls and clearing the surrounding area of objects that the rider might fall onto, and making sure that the surface is soft.\n\nImportant protective gear is gear that protects the joints, the head and face, and otherwise protects from bumps and scrapes during falls. Care should be taken in selecting a helmet, as the weight could make falls worse or the shape might be unsuited for protecting from falls and might be pressed into the neck during impact.\n\nStanding on a balance board is extremely dangerous for a person who is prone to dizziness or whose balance is impaired, such as by being tired or under the influence of alcohol or other drugs.\n\n"}
{"id": "39975065", "url": "https://en.wikipedia.org/wiki?curid=39975065", "title": "Cayman Chemical Company", "text": "Cayman Chemical Company\n\nCayman Chemical Company is an American biotechnology company founded in 1980, headquartered in Ann Arbor, MI. The company provides chemicals that are used primarily by universities and pharmaceutical companies for research and the development of medicines. The company is also known as a provider of reference standards to state and federal crime labs for use in the detection of rapidly evolving designer drugs. Small quantities of these known reference standards are analyzed using mass spectrometry and gas chromatography techniques to match against forensic evidence, usually confiscated by law enforcement, or forensic toxicological evidence collected in the form of biological samples such as urine, blood, or tissue.\n\nCayman Chemical Company was incorporated on June 6, 1980 in Denver, Colorado. It was initially a marine natural products company. Building on earlier environmental studies in the North Sound of Grand Cayman Island, Cayman initially sold prostaglandin standards as research chemicals. The company operated in Denver for several years before relocating to Ann Arbor, Michigan.\n\nCommercializing the patented work of Philippe Pradelles and others, Cayman exploited the acetylcholinesterase enzyme of electric eels to develop a range of sensitive enzyme immunoassays for prostaglandins in the late 1980s.\nThe availability of these assays enabled the development of Celebrex by Searle/Monsanto, relying on measurements of Prostaglandin E2 and Thromboxane B2, and of Singulaire by Merck & Co, relying on measurements of unstable Leukotrienes.\n\nCayman Europe was established in January, 2005 in Tallinn, Estonia. Their operations focus on the distribution of Cayman Chemical products in Europe.\n\nFacilities are located in Neratovice, a town 18 miles north of Prague in the Czech Republic. Production is focused on pharmaceutical ingredients for generic drug formulators, specifically bulk prostaglandins.\n\nFounded in 1968, Biomol GmbH in Hamburg, Germany, distributes more than 300,000 research antibodies, assay kits, specialty reagents and related life science products to research, diagnostic and biopharmaceutical customers in Germany and Europe.\n\n"}
{"id": "4241352", "url": "https://en.wikipedia.org/wiki?curid=4241352", "title": "Center of Molecular Immunology", "text": "Center of Molecular Immunology\n\nThe Center of Molecular Immunology (Centro de Inmunología Molecular) or CIM, is a cancer research institution located on the west side of Havana, Cuba. Opened on December 5, 1994, it focuses on the research and production of new biopharmaceutical products for the treatment of cancer and other nontransmissible diseases.\n\nThe design and construction of the Center of Molecular Immunology was modeled on current Good Manufacturing Practice guidelines recommended by the World Health Organization and adopted by Cuba, particularly those also adopted by the member countries of the European Union, then known as the European Community.\n\nResearch labs, production areas and administrative offices share the 15,000 square meters of the two floor facility.\n\nApproximately 800 employees work at the CIM, most of them scientists and engineers. These personnel are administratively organized in three main areas: Research and Development, Production, and Quality Assurance.\n\nBasic research projects are focused on cancer immunotherapy, especially the development of molecular vaccines. These include antibody engineering, cellular engineering, bioinformatics and regulation of the immune response.\n\nCIM conducts clinical trials in diagnostic imaging and cancer therapy of varying origin, as well as other immune system diseases, in highly specialized hospitals.\n\nIn August 2011 it was announced that the Center of Molecular Immunology released CimaVax-EGF, the first therapeutic cancer vaccine for lung cancer.\n\nIn December 2012, a new therapeutic cancer vaccine was approved by Cuban regulatory agency. CIMAbid (racotumomab or 1E10)obtained conditional approval for the treatment of lung cancer. This vaccine is an antiidiotypic vaccine targeting N-glicolil GM3, a tumor specific antigen. The project leader was Dr Ana María Vazquez. These vaccine has been associated with statistically significant extension of survival of lung cancer patients with excellent safety profile and continues to be investigated in confirmatory clinical trials. \n\nToday CIM produces biopharmaceutical products, such as anti-CD3 monoclonal antibody for the treatment of patients with organ transplant rejection, human recombinant erythropoietin for the treatment of anemia, granulocyte colony-stimulating factor for the treatment of neutropenia, and a humanized monoclonal antibody that recognizes the epidermal growth factor receptor for cancer treatment, as well as other monoclonal antibodies for tumor imaging.\n\nSince 1992 CIMAB S.A. has been devoted to the commercialization of biopharmaceutical products for the Cuban market and abroad, especially monoclonal antibodies and other recombinant proteins for the diagnosis and treatment of cancer and other diseases related to the immune system.\n\nCIMAB S.A. is the exclusive representative for the commercialization of the products and services of the Center of Molecular Immunology.\n\nIt was announced in July 2008 that CIM had also received approval from the Cuban regulatory authorities for a lung cancer vaccine, targeting EGFR. The vaccine has been in development since 1992 and the project led by Gisela Gonzalez.\n\n\n\n"}
{"id": "56097330", "url": "https://en.wikipedia.org/wiki?curid=56097330", "title": "Coda.io", "text": "Coda.io\n\nCoda is a cloud-based document editor founded by Shishir Mehrotra and Alex DeNeui. The software provides word-processing, spreadsheet and database functionality. In 2017, it was reported that Coda has raised $60 million in capital. \n\n"}
{"id": "5548325", "url": "https://en.wikipedia.org/wiki?curid=5548325", "title": "Compensation (engineering)", "text": "Compensation (engineering)\n\nIn engineering, compensation is planning for side effects or other unintended issues in a design. In a more simpler term, it's a \"counter-procedure\" plan on expected side effect performed to produce more efficient and useful results. The design of an invention can itself also be to compensate for some other existing issue or exception.\n\nOne example is in a voltage-controlled crystal oscillator (VCXO), which is normally affected not only by voltage, but to a lesser extent by temperature. A temperature-compensated version (a TCVCXO) is designed so that heat buildup within the enclosure of a transmitter or other such device will not alter the piezoelectric effect, thereby causing frequency drift.\n\nAnother example is motion compensation on digital cameras and video cameras, which keep a picture steady and not blurry.\n\nOther examples in electrical engineering include:\n\nThere are also examples in civil engineering:\n\n"}
{"id": "17182301", "url": "https://en.wikipedia.org/wiki?curid=17182301", "title": "Credit card", "text": "Credit card\n\nA credit card is a payment card issued to users (cardholders) to enable the cardholder to pay a merchant for goods and services based on the cardholder's promise to the card issuer to pay them for the amounts so paid plus the other agreed charges. The card issuer (usually a bank) creates a revolving account and grants a line of credit to the cardholder, from which the cardholder can borrow money for payment to a merchant or as a cash advance. In other words, credit cards combine payment services with extensions of credit. Complex fee structures in the credit card industry may limit customers' ability to comparison shop, helping to ensure that the industry is not price-competitive and helping to maximize industry profits. Due to concerns about this, many legislatures have regulated credit card fees.\n\nA credit card is different from a charge card, which requires the balance to be repaid in full each month. In contrast, credit cards allow the consumers a continuing balance of debt, subject to interest being charged. A credit card also differs from a cash card, which can be used like currency by the owner of the card. A credit card differs from a charge card also in that a credit card typically involves a third-party entity that pays the seller and is reimbursed by the buyer, whereas a charge card simply defers payment by the buyer until a later date.\nThe size of most credit cards is 85.60 mm × 53.98 mm ( ×   inches) and rounded corners with a radius of 2.88–3.48 mm, conforming to the ISO/IEC 7810 ID-1 standard, the same size as ATM cards and other payment cards, such as debit cards.\n\nCredit cards have a printed or embossed bank card number complying with the ISO/IEC 7812 numbering standard. The card number's \"prefix\", called the Bank Identification Number, is the sequence of digits at the beginning of the number that determine the bank to which a credit card number belongs. This is the first six digits for MasterCard and Visa cards. The next nine digits are the individual account number, and the final digit is a validity check code.\n\nBoth of these standards are maintained and further developed by ISO/IEC JTC 1/SC 17/WG 1. Credit cards have a magnetic stripe conforming to the ISO/IEC 7813. Many modern credit cards have a computer chip embedded in them as a security feature.\n\nIn addition to the main credit card number, credit cards also carry issue and expiration dates (given to the nearest month), as well as extra codes such as issue numbers and security codes. Not all credit cards have the same sets of extra codes nor do they use the same number of digits.\n\nCredit card numbers were originally embossed to allow easy transfer of the number to charge slips. With the decline of paper slips, some credit cards are no longer embossed and in fact the card number is no longer in the front.\n\nThe concept of using a card for purchases was described in 1887 by Edward Bellamy in his utopian novel \"Looking Backward\". Bellamy used the term \"credit card\" eleven times in this novel, although this referred to a card for spending a citizen's dividend from the government, rather than borrowing, making it more similar to a Debit card.\n\nCharge coins and other similar items were used from the late 19th century to the 1930s. They came in various shapes and sizes; with materials made out of celluloid (an early type of plastic), copper, aluminum, steel, and other types of whitish metals. Each charge coin usually had a little hole, enabling it to be put in a key ring, like a key. These charge coins were usually given to customers who had charge accounts in department stores, hotels, and so on. A charge coin usually had the charge account number along with the merchant's name and logo.\n\nThe charge coin offered a simple and fast way to copy a charge account number to the sales slip, by imprinting the coin onto the sales slip. This sped the process of copying, previously done by handwriting. It also reduced the number of errors, by having a standardised form of numbers on the sales slip, instead of various kind of handwriting style.\n\nBecause the customer's name was not on the charge coin, almost anyone could use it. This sometimes led to a case of mistaken identity, either accidentally or intentionally, by acting on behalf of the charge account owner or out of malice to defraud both the charge account owner and the merchant. Beginning in the 1930s, merchants started to move from charge coins to the newer Charga-Plate.\n\nThe Charga-Plate, developed in 1928, was an early predecessor of the credit card and was used in the U.S. from the 1930s to the late 1950s. It was a 2½\" × 1¼\" rectangle of sheet metal related to Addressograph and military dog tag systems. It was embossed with the customer's name, city, and state. It held a small paper card on its back for a signature. In recording a purchase, the plate was laid into a recess in the imprinter, with a paper \"charge slip\" positioned on top of it. The record of the transaction included an impression of the embossed information, made by the imprinter pressing an inked ribbon against the charge slip. Charga-Plate was a trademark of Farrington Manufacturing Co. Charga-Plates were issued by large-scale merchants to their regular customers, much like department store credit cards of today. In some cases, the plates were kept in the issuing store rather than held by customers. When an authorized user made a purchase, a clerk retrieved the plate from the store's files and then processed the purchase. Charga-Plates speeded back-office bookkeeping and reduced copying errors that were done manually in paper ledgers in each store.\n\nIn 1934, American Airlines and the Air Transport Association simplified the process even more with the advent of the Air Travel Card. They created a numbering scheme that identified the issuer of the card as well as the customer account. This is the reason the modern UATP cards still start with the number 1. With an Air Travel Card, passengers could \"buy now, and pay later\" for a ticket against their credit and receive a fifteen percent discount at any of the accepting airlines. By the 1940s, all of the major US airlines offered Air Travel Cards that could be used on 17 different airlines. By 1941, about half of the airlines' revenues came through the Air Travel Card agreement. The airlines had also started offering installment plans to lure new travelers into the air. In October 1948, the Air Travel Card became the first internationally valid charge card within all members of the International Air Transport Association.\n\nThe concept of customers paying different merchants using the same card was expanded in 1950 by Ralph Schneider and Frank McNamara, founders of Diners Club, to consolidate multiple cards. The Diners Club, which was created partially through a merger with Dine and Sign, produced the first \"general purpose\" charge card and required the entire bill to be paid with each statement. That was followed by Carte Blanche and in 1958 by American Express which created a worldwide credit card network (although these were initially charge cards that later acquired credit card features).\n\nUntil 1958, no one had been able to successfully establish a \"revolving credit\" financial system in which a card issued by a third-party bank was being generally accepted by a large number of merchants, as opposed to merchant-issued revolving cards accepted by only a few merchants. There had been a dozen attempts by small American banks, but none of them were able to last very long. In September 1958, Bank of America launched the \"BankAmericard\" in Fresno, California, which would become the first successful recognizably modern credit card. This card succeeded where others failed by breaking the chicken-and-egg cycle in which consumers did not want to use a card that few merchants would accept and merchants did not want to accept a card that few consumers used. Bank of America chose Fresno because 45% of its residents used the bank, and by sending a card to 60,000 Fresno residents at once, the bank was able to convince merchants to accept the card. It was eventually licensed to other banks around the United States and then around the world, and in 1976, all BankAmericard licensees united themselves under the common brand Visa. In 1966, the ancestor of MasterCard was born when a group of banks established Master Charge to compete with BankAmericard; it received a significant boost when Citibank merged its own Everything Card, launched in 1967, into Master Charge in 1969.\n\nEarly credit cards in the U.S., of which BankAmericard was the most prominent example, were mass-produced and mass mailed unsolicited to bank customers who were thought to be good credit risks. They have been mailed off to unemployables, drunks, narcotics addicts and to compulsive debtors, a process President Johnson's Special Assistant Betty Furness found very like \"giving sugar to diabetics\". These mass mailings were known as \"drops\" in banking terminology, and were outlawed in 1970 due to the financial chaos they caused. However, by the time the law came into effect, approximately 100 million credit cards had been dropped into the U.S. population. After 1970, only credit card applications could be sent unsolicited in mass mailings.\n\nBefore the computerization of credit card systems in America, using a credit card to pay at a merchant was significantly more complicated than it is today. Each time a consumer wanted to use a credit card, the merchant would have to call their bank, who in turn had to call the credit card company, which then had to have an employee manually look up the customer's name and credit balance. This system was computerized in 1973 under the leadership of Dee Hock, the first CEO of Visa, allowing transaction time to decrease substantially to less than one minute. However, until always-connected payment terminals became ubiquitous at the beginning of the 21st century, it was common for a merchant to accept a charge, especially below a threshold value or from a known and trusted customer, without verifying it by phone. Books with lists of stolen card numbers were distributed to merchants who were supposed in any case to check cards against the list before accepting them, as well as verifying the signature on the charge slip against that on the card. Merchants who failed to take the time to follow the proper verification procedures were liable for fraudulent charges, but because of the cumbersome nature of the procedures, merchants would often simply skip some or all of them and assume the risk for smaller transactions.\n\nThe fractured nature of the U.S. banking system under the Glass–Steagall Act meant that credit cards became an effective way for those who were traveling around the country to move their credit to places where they could not directly use their banking facilities. There are now countless variations on the basic concept of revolving credit for individuals (as issued by banks and honored by a network of financial institutions), including organization-branded credit cards, corporate-user credit cards, store cards and so on.\n\nIn 1966, Barclaycard in the United Kingdom launched the first credit card outside the United States.\n\nAlthough credit cards reached very high adoption levels in the US, Canada and the UK during the latter 20th century, many cultures were more cash-oriented or developed alternative forms of cashless payments, such as Carte bleue or the Eurocard (Germany, France, Switzerland, and others). In these places, adoption of credit cards was initially much slower. Due to strict regulations regarding bank overdrafts, some countries, France in particular, were much quicker to develop and adopt chip-based credit cards which are seen as major anti-fraud credit devices. Debit cards and online banking (using either ATMs or PCs) are used more widely than credit cards in some countries. It took until the 1990s to reach anything like the percentage market penetration levels achieved in the US, Canada, and UK. In some countries, acceptance still remains low as the use of a credit card system depends on the banking system of each country; while in others, a country sometimes had to develop its own credit card network, e.g. UK's Barclaycard and Australia's Bankcard. Japan remains a very cash-oriented society, with credit card adoption being limited mainly to the largest of merchants; although stored value cards (such as telephone cards) are used as alternative currencies, the trend is toward RFID-based systems inside cards, cellphones, and other objects.\n\nThe design of the credit card itself has become a major selling point in recent years. The value of the card to the issuer is often related to the customer's usage of the card, or to the customer's financial worth. This has led to the rise of Co-Brand and Affinity cards, where the card design is related to the \"affinity\" (a university or professional society, for example) leading to higher card usage. In most cases a percentage of the value of the card is returned to the affinity group.\n\nA growing field of numismatics (study of money), or more specifically exonumia (study of money-like objects), credit card collectors seek to collect various embodiments of credit from the now familiar plastic cards to older paper merchant cards, and even metal tokens that were accepted as merchant credit cards. Early credit cards were made of celluloid plastic, then metal and fiber, then paper, and are now mostly polyvinyl chloride (PVC) plastic.\nHowever the chip part of credit cards is not made from plastic but from metals.\n\nA credit card issuing company, such as a bank or credit union, enters into agreements with merchants for them to accept their credit cards. Merchants often advertise which cards they accept by displaying acceptance marks – generally derived from logos – or this may be communicated in signage in the establishment or in company material (e.g., a restaurant's menu may indicate which credit cards are accepted). Merchants may also communicate this orally, as in \"We take (brands X, Y, and Z)\" or \"We don't take credit cards\".\n\nThe credit card issuer issues a credit card to a customer at the time or after an account has been approved by the credit provider, which need not be the same entity as the card issuer. The cardholders can then use it to make purchases at merchants accepting that card. When a purchase is made, the cardholder agrees to pay the card issuer. The cardholder indicates consent to pay by signing a receipt with a record of the card details and indicating the amount to be paid or by entering a personal identification number (PIN). Also, many merchants now accept verbal authorizations via telephone and electronic authorization using the Internet, known as a card not present transaction (CNP).\n\nElectronic verification systems allow merchants to verify in a few seconds that the card is valid and the cardholder has sufficient credit to cover the purchase, allowing the verification to happen at time of purchase. The verification is performed using a credit card payment terminal or point-of-sale (POS) system with a communications link to the merchant's acquiring bank. Data from the card is obtained from a magnetic stripe or chip on the card; the latter system is called Chip and PIN in the United Kingdom and Ireland, and is implemented as an EMV card.\n\nFor card not present transactions where the card is not shown (e.g., e-commerce, mail order, and telephone sales), merchants additionally verify that the customer is in physical possession of the card and is the authorized user by asking for additional information such as the security code printed on the back of the card, date of expiry, and billing address.\n\nEach month, the cardholder is sent a statement indicating the purchases made with the card, any outstanding fees, and the total amount owed. In the US, after receiving the statement, the cardholder may dispute any charges that he or she thinks are incorrect (see , which limits cardholder liability for unauthorized use of a credit card to $50). The Fair Credit Billing Act gives details of the US regulations. The cardholder must pay a defined minimum portion of the amount owed by a due date, or may choose to pay a higher amount. The credit issuer charges interest on the unpaid balance if the billed amount is not paid in full (typically at a much higher rate than most other forms of debt). In addition, if the cardholder fails to make at least the minimum payment by the due date, the issuer may impose a late fee or other penalties. To help mitigate this, some financial institutions can arrange for automatic payments to be deducted from the cardholder's bank account, thus avoiding such penalties altogether, as long as the cardholder has sufficient funds.\n\nMany banks now also offer the option of electronic statements, either in lieu of or in addition to physical statements, which can be viewed at any time by the cardholder via the issuer's online banking website. Notification of the availability of a new statement is generally sent to the cardholder's email address. If the card issuer has chosen to allow it, the cardholder may have other options for payment besides a physical check, such as an electronic transfer of funds from a checking account. Depending on the issuer, the cardholder may also be able to make multiple payments during a single statement period, possibly enabling him or her to utilize the credit limit on the card several times.\n\nCredit card advertising regulations in the US include the Schumer box disclosure requirements. A large fraction of junk mail consists of credit card offers created from lists provided by the major credit reporting agencies. In the United States, the three major US credit bureaus (Equifax, TransUnion and Experian) allow consumers to opt out from related credit card solicitation offers via its Opt Out Pre Screen program.\n\nCredit card issuers usually waive interest charges if the balance is paid in full each month, but typically will charge full interest on the entire outstanding balance from the date of each purchase if the total balance is not paid.\n\nFor example, if a user had a $1,000 transaction and repaid it in full within this grace period, there would be no interest charged. If, however, even $1.00 of the total amount remained unpaid, interest would be charged on the $1,000 from the date of purchase until the payment is received. The precise manner in which interest is charged is usually detailed in a cardholder agreement which may be summarized on the back of the monthly statement. The general calculation formula most financial institutions use to determine the amount of interest to be charged is (APR/100 x ADB)/365 x number of days revolved. Take the annual percentage rate (APR) and divide by 100 then multiply to the amount of the average daily balance (ADB). Divide the result by 365 and then take this total and multiply by the total number of days the amount revolved before payment was made on the account. Financial institutions refer to interest charged back to the original time of the transaction and up to the time a payment was made, if not in full, as a residual retail finance charge (RRFC). Thus after an amount has revolved and a payment has been made, the user of the card will still receive interest charges on their statement after paying the next statement in full (in fact the statement may only have a charge for interest that collected up until the date the full balance was paid, i.e. when the balance stopped revolving).\n\nThe credit card may simply serve as a form of revolving credit, or it may become a complicated financial instrument with multiple balance segments each at a different interest rate, possibly with a single umbrella credit limit, or with separate credit limits applicable to the various balance segments. Usually this compartmentalization is the result of special incentive offers from the issuing bank, to encourage balance transfers from cards of other issuers. In the event that several interest rates apply to various balance segments, payment allocation is generally at the discretion of the issuing bank, and payments will therefore usually be allocated towards the lowest rate balances until paid in full before any money is paid towards higher rate balances. Interest rates can vary considerably from card to card, and the interest rate on a particular card may jump dramatically if the card user is late with a payment on that card \"or any other credit instrument\", or even if the issuing bank decides to raise its revenue.\n\nA credit card's grace period is the time the cardholder has to pay the balance before interest is assessed on the outstanding balance. Grace periods may vary, but usually range from 20 to 55 days depending on the type of credit card and the issuing bank. Some policies allow for reinstatement after certain conditions are met.\n\nUsually, if a cardholder is late paying the balance, finance charges will be calculated and the grace period does not apply. Finance charges incurred depend on the grace period and balance; with most credit cards there is no grace period if there is any outstanding balance from the previous billing cycle or statement (i.e. interest is applied on both the previous balance and new transactions). However, there are some credit cards that will only apply finance charge on the previous or old balance, excluding new transactions.\n\n\nThe flow of information and money between these parties — always through the card associations — is known as the interchange, and it consists of a few steps.\n\n\nA credit card register is a transaction register used to ensure the increasing balance owed from using a credit card is enough below the credit limit to deal with authorization holds and payments not yet received by the bank and to easily look up past transactions for reconciliation and budgeting.\n\nThe register is a personal record of banking transactions used for credit card purchases as they affect funds in the bank account or the available credit. In addition to check number and so forth the code column indicates the credit card. The balance column shows available funds after purchases. When the credit card payment is made the balance already reflects the funds were spent. In a credit card's entry, the deposit column shows the available credit and the payment column shows total owed, their sum being equal to the credit limit.\n\nEach check written, debit card transaction, cash withdrawal, and credit card charge is entered manually into the paper register daily or several times per week. Credit card register also refers to one transaction record for each credit card. In this case the booklets readily enable the location of a card’s current available credit when ten or more cards are in use.\n\nAs well as convenient credit, credit cards offer consumers an easy way to track expenses, which is necessary for both monitoring personal expenditures and the tracking of work-related expenses for taxation and reimbursement purposes. Credit cards are accepted in larger establishments in almost all countries, and are available with a variety of credit limits, repayment arrangements. Some have added perks (such as insurance protection, rewards schemes in which points earned by purchasing goods with the card can be redeemed for further goods and services or cashback).\n\nSome countries, such as the United States, the United Kingdom, and France, limit the amount for which a consumer can be held liable in the event of fraudulent transactions with a lost or stolen credit card.\n\nBusiness credit cards are specialized credit cards issued in the name of a registered business, and typically they can only be used for business purposes. Their use has grown in recent decades. In 1998, for instance, 37% of small businesses reported using a business credit card; by 2009, this number had grown to 64%.\n\nBusiness credit cards offer a number of features specific to businesses. They frequently offer special rewards in areas such as shipping, office supplies, travel, and business technology. Most issuers use the applicant's personal credit score when evaluating these applications. In addition, income from a variety of sources may be used to qualify, which means these cards may be available to businesses that are newly established. In addition, most major issuers of these cards do not report account activity to the owner's personal credit unless there is a default. This may have the effect of protecting the owner's personal credit from the activity of the business.\n\nBusiness credit cards are offered by almost all major card issuers—like American Express, Visa, and MasterCard in addition to local banks and credit unions. Charge cards for businesses, however, are currently only offered by American Express.\n\nA secured credit card is a type of credit card secured by a deposit account owned by the cardholder. Typically, the cardholder must deposit between 100% and 200% of the total amount of credit desired. Thus if the cardholder puts down $1,000, they will be given credit in the range of $500–1,000. In some cases, credit card issuers will offer incentives even on their secured card portfolios. In these cases, the deposit required may be significantly less than the required credit limit, and can be as low as 10% of the desired credit limit. This deposit is held in a special savings account. Credit card issuers offer this because they have noticed that delinquencies were notably reduced when the customer perceives something to lose if the balance is not repaid.\n\nThe cardholder of a secured credit card is still expected to make regular payments, as with a regular credit card, but should they default on a payment, the card issuer has the option of recovering the cost of the purchases paid to the merchants out of the deposit. The advantage of the secured card for an individual with negative or no credit history is that most companies report regularly to the major credit bureaus. This allows building a positive credit history.\n\nAlthough the deposit is in the hands of the credit card issuer as security in the event of default by the consumer, the deposit will not be debited simply for missing one or two payments. Usually the deposit is only used as an offset when the account is closed, either at the request of the customer or due to severe delinquency (150 to 180 days). This means that an account which is less than 150 days delinquent will continue to accrue interest and fees, and could result in a balance which is much higher than the actual credit limit on the card. In these cases the total debt may far exceed the original deposit and the cardholder not only forfeits their deposit but is left with an additional debt.\n\nMost of these conditions are usually described in a cardholder agreement which the cardholder signs when their account is opened.\n\nSecured credit cards are an option to allow a person with a poor credit history or no credit history to have a credit card which might not otherwise be available. They are often offered as a means of rebuilding one's credit. Fees and service charges for secured credit cards often exceed those charged for ordinary non-secured credit cards. For people in certain situations, (for example, after charging off on other credit cards, or people with a long history of delinquency on various forms of debt), secured cards are almost always more expensive than unsecured credit cards.\n\nSometimes a credit card will be secured by the equity in the borrower's home.\n\nA \"prepaid credit card\" is not a true credit card, since no credit is offered by the card issuer: the cardholder spends money which has been \"stored\" via a prior deposit by the cardholder or someone else, such as a parent or employer. However, it carries a credit-card brand (such as Discover, Visa, MasterCard, American Express, or JCB) and can be used in similar ways just as though it were a credit card. Unlike debit cards, prepaid credit cards generally do not require a PIN. An exception are prepaid credit cards with an EMV chip. These cards do require a PIN if the payment is processed via Chip and PIN technology.\n\nAfter purchasing the card, the cardholder loads the account with any amount of money, up to the predetermined card limit and then uses the card to make purchases the same way as a typical credit card. Prepaid cards can be issued to minors (above 13) since there is no credit line involved. The main advantage over secured credit cards (see above section) is that the cardholder is not required to come up with $500 or more to open an account. With prepaid credit cards purchasers are not charged any interest but are often charged a purchasing fee plus monthly fees after an arbitrary time period. Many other fees also usually apply to a prepaid card.\n\nPrepaid credit cards are sometimes marketed to teenagers for shopping online without having their parents complete the transaction. Teenagers can only use funds that are available on the card which helps promote financial management to reduce the risk of debt problems later in life.\n\nPrepaid cards can be used globally. The prepaid card is convenient for payees in developing countries like Brazil, Russia, India, and China, where international wire transfers and bank checks are time consuming, complicated and costly.\n\nBecause of the many fees that apply to obtaining and using credit-card-branded prepaid cards, the Financial Consumer Agency of Canada describes them as \"an expensive way to spend your own money\". The agency publishes a booklet entitled \"Pre-paid Cards\" which explains the advantages and disadvantages of this type of prepaid card.\n\nA digital card is a digital cloud-hosted virtual representation of any kind of identification card or payment card, such as a credit card.\n\nThe main benefit to the cardholder is convenience. Compared to debit cards and checks, a credit card allows small short-term loans to be quickly made to a cardholder who need not calculate a balance remaining before every transaction, provided the total charges do not exceed the maximum credit line for the card.\n\nDifferent countries offer different levels of protection. In the UK, for example, the bank is jointly liable with the merchant for purchases of defective products over £100.\n\nMany credit cards offer rewards and benefits packages, such as enhanced product warranties at no cost, free loss/damage coverage on new purchases, various insurance protections, for example, rental car insurance, common carrier accident protection, and travel medical insurance.\n\nCredit cards can also offer a loyalty program, where each purchase is rewarded with points, which may be redeemed for cash or products. Research has examined whether competition among card networks may potentially make payment rewards too generous, causing higher prices among merchants, thus actually impacting social welfare and its distribution, a situation potentially warranting public policy interventions.\n\nThe table below contains a list of benefits offered in the United States for consumer credit cards. Benefits may vary in other countries or business credit cards.\n\nLow introductory credit card rates are limited to a fixed term, usually between 6 and 12 months, after which a higher rate is charged. As all credit cards charge fees and interest, some customers become so indebted to their credit card provider that they are driven to bankruptcy. Some credit cards often levy a rate of 20 to 30 percent after a payment is missed. In other cases, a fixed charge is levied without change to the interest rate. In some cases universal default may apply: the high default rate is applied to a card in good standing by missing a payment on an unrelated account from the same provider. This can lead to a snowball effect in which the consumer is drowned by unexpectedly high interest rates. Further, most card holder agreements enable the issuer to arbitrarily raise the interest rate for any reason they see fit. First Premier Bank at one point offered a credit card with a 79.9% interest rate; however, they discontinued this card in February 2011 because of persistent defaults.\n\nComplex fee structures in the credit card industry limit customers' ability to comparison shop, help ensure that the industry is not price-competitive and help maximize industry profits.\n\nResearch shows that a substantial fraction of consumers (about 40 percent) choose a sub-optimal credit card agreement, with some incurring hundreds of dollars of avoidable interest costs.\n\nSeveral studies have shown that consumers are likely to spend more money when they pay by credit card. Researchers suggest that when people pay using credit cards, they do not experience the abstract pain of payment. Furthermore, researchers have found that using credit cards can increase consumption of unhealthy food.\n\nMerchants that accept credit cards must pay interchange fees and discount fees on all credit-card transactions. In some cases merchants are barred by their credit agreements from passing these fees directly to credit card customers, or from setting a minimum transaction amount (no longer prohibited in the United States, United Kingdom or Australia). The result is that merchants are induced to charge all customers (including those who do not use credit cards) higher prices to cover the fees on credit card transactions. The inducement can be strong because the merchant's fee is a percentage of the sale price, which has a disproportionate effect on the profitability of businesses that have predominantly credit card transactions, unless compensated for by raising prices generally. In the United States in 2008 credit card companies collected a total of $48 billion in interchange fees, or an average of $427 per family, with an average fee rate of about 2% per transaction.\n\nCredit card rewards result in a total transfer of $1,282 from the average cash payer to the average card payer.\n\nFor merchants, a credit card transaction is often more secure than other forms of payment, such as cheques, because the issuing bank commits to pay the merchant the moment the transaction is authorized, regardless of whether the consumer defaults on the credit card payment (except for legitimate disputes, which are discussed below, and can result in charges back to the merchant). In most cases, cards are even more secure than cash, because they discourage theft by the merchant's employees and reduce the amount of cash on the premises. Finally, credit cards reduce the back office expense of processing checks/cash and transporting them to the bank.\n\nPrior to credit cards, each merchant had to evaluate each customer's credit history before extending credit. That task is now performed by the banks which assume the credit risk. Credit cards can also aid in securing a sale especially if the customer does not have enough cash on hand or in a checking account. Extra turnover is generated by the fact that the customer can purchase goods and services immediately and is less inhibited by the amount of cash in pocket and the immediate state of the customer's bank balance. Much of merchants' marketing is based on this immediacy.\n\nFor each purchase, the bank charges the merchant a commission (discount fee) for this service and there may be a certain delay before the agreed payment is received by the merchant. The commission is often a percentage of the transaction amount, plus a fixed fee (interchange rate).\n\nMerchants are charged several fees for accepting credit cards. The merchant is usually charged a commission of around 1 to 4 percent of the value of each transaction paid for by credit card. The merchant may also pay a variable charge, called a merchant discount rate, for each transaction. In some instances of very low-value transactions, use of credit cards will significantly reduce the profit margin or cause the merchant to lose money on the transaction. Merchants with very low average transaction prices or very high average transaction prices are more averse to accepting credit cards. In some cases merchants may charge users a \"credit card supplement\" (or surcharge), either a fixed amount or a percentage, for payment by credit card. This practice was prohibited by most credit card contracts in the United States until 2013, when a major settlement between merchants and credit card companies allowed merchants to levy surcharges. Most retailers have not started using credit card surcharges, however, for fear of losing customers.\n\nMerchants in the United States have been fighting what they consider to be unfairly high fees charged by credit card companies in a series of lawsuits that started in 2005. Merchants charged that the two main credit card processing companies, MasterCard and Visa, used their monopoly power to levy excessive fees in a class-action lawsuit involving the National Retail Federation and major retailers such as Wal-Mart. In December 2013, a federal judge approved a $5.7 billion settlement in the case that offered payouts to merchants who had paid credit card fees, the largest antitrust settlement in U.S. history. Some large retailers, such as Wal-Mart and Amazon, chose to not participate in this settlement, however, and have continued their legal fight against the credit card companies.\n\nMerchants are also required to lease or purchase processing equipment, in some cases this equipment is provided free of charge by the processor. Merchants must also satisfy data security compliance standards which are highly technical and complicated. In many cases, there is a delay of several days before funds are deposited into a merchant's bank account. Because credit card fee structures are very complicated, smaller merchants are at a disadvantage to analyze and predict fees.\n\nFinally, merchants assume the risk of chargebacks by consumers.\n\nCredit card security relies on the physical security of the plastic card as well as the privacy of the credit card number. Therefore, whenever a person other than the card owner has access to the card or its number, security is potentially compromised. Once, merchants would often accept credit card numbers without additional verification for mail order purchases. It's now common practice to only ship to confirmed addresses as a security measure to minimise fraudulent purchases. Some merchants will accept a credit card number for in-store purchases, whereupon access to the number allows easy fraud, but many require the card itself to be present, and require a signature (for magnetic stripe cards). A lost or stolen card can be cancelled, and if this is done quickly, will greatly limit the fraud that can take place in this way. European banks can require a cardholder's security PIN be entered for in-person purchases with the card.\n\nThe Payment Card Industry Data Security Standard (PCI DSS) is the security standard issued by the Payment Card Industry Security Standards Council (PCI SSC). This data security standard is used by acquiring banks to impose cardholder data security measures upon their merchants.\n\nThe goal of the credit card companies is not to eliminate fraud, but to \"reduce it to manageable levels\". This implies that fraud prevention measures will be used only if their cost are lower than the potential gains from fraud reduction, whereas high-cost low-return measures will not be used – as would be expected from organizations whose goal is profit maximisation.\n\nInternet fraud may be by claiming a chargeback which is not justified (\"friendly fraud\"), or carried out by the use of credit card information which can be stolen in many ways, the simplest being copying information from retailers, either online or offline. Despite efforts to improve security for remote purchases using credit cards, security breaches are usually the result of poor practice by merchants. For example, a website that safely uses TLS to encrypt card data from a client may then email the data, unencrypted, from the webserver to the merchant; or the merchant may store unencrypted details in a way that allows them to be accessed over the Internet or by a rogue employee; unencrypted card details are always a security risk. Even encrypted data may be cracked.\n\nControlled payment numbers (also known as virtual credit cards or disposable credit cards) are another option for protecting against credit card fraud where presentation of a physical card is not required, as in telephone and online purchasing. These are one-time use numbers that function as a payment card and are linked to the user's real account, but do not reveal details, and cannot be used for subsequent unauthorised transactions. They can be valid for a relatively short time, and limited to the actual amount of the purchase or a limit set by the user. Their use can be limited to one merchant. If the number given to the merchant is compromised, it will be rejected if an attempt is made to use it a second time.\n\nA similar system of controls can be used on physical cards. Technology provides the option for banks to support many other controls too that can be turned on and off and varied by the credit card owner in real time as circumstances change (i.e., they can change temporal, numerical, geographical and many other parameters on their primary and subsidiary cards). Apart from the obvious benefits of such controls: from a security perspective this means that a customer can have a Chip and PIN card secured for the real world, and limited for use in the home country. In this eventuality a thief stealing the details will be prevented from using these overseas in non chip and pin EMV countries. Similarly the real card can be restricted from use on-line so that stolen details will be declined if this tried. Then when card users shop online they can use virtual account numbers. In both circumstances an alert system can be built in notifying a user that a fraudulent attempt has been made which breaches their parameters, and can provide data on this in real time.\n\nAdditionally, there are security features present on the physical card itself in order to prevent counterfeiting. For example, most modern credit cards have a watermark that will fluoresce under ultraviolet light. Most major credit cards have a hologram. A Visa card has a letter V superimposed over the regular Visa logo and a MasterCard has the letters MC across the front of the card. Older Visa cards have a bald eagle or dove across the front. In the aforementioned cases, the security features are only visible under ultraviolet light and are invisible in normal light.\n\nThe United States Secret Service, Federal Bureau of Investigation, U.S. Immigration and Customs Enforcement, and U.S. Postal Inspection Service are responsible for prosecuting criminals who engage in credit card fraud in the United States. However, they do not have the resources to pursue all criminals, and in general they only prosecute cases exceeding $5,000.\n\nThree improvements to card security have been introduced to the more common credit card networks, but none has proven to help reduce credit card fraud so far. First, the cards themselves are being replaced with similar-looking tamper-resistant smart cards which are intended to make forgery more difficult. The majority of smart card (IC card) based credit cards comply with the EMV (Europay MasterCard Visa) standard. Second, an additional 3 or 4 digit card security code (CSC) is now present on the back of most cards, for use in card not present transactions. Stakeholders at all levels in electronic payment have recognized the need to develop consistent global standards for security that account for and integrate both current and emerging security technologies. They have begun to address these needs through organisations such as PCI DSS and the Secure POS Vendor Alliance.\n\nCode 10 calls are made when merchants are suspicious about accepting a credit card.\n\nThe operator then asks the merchant a series of YES or NO questions to find out whether the merchant is suspicious of the card or the cardholder. The merchant may be asked to retain the card if it is safe to do so. The merchant may receive a reward for returning a confiscated card to the issuing bank, especially if an arrest is made.\n\nCredit card issuers (banks) have several types of costs:\n\nBanks generally borrow the money they then lend to their customers. As they receive very low-interest loans from other firms, they may borrow as much as their customers require, while lending their capital to other borrowers at higher rates. If the card issuer charges 15% on money lent to users, and it costs 5% to borrow the money to lend, and the balance sits with the cardholder for a year, the issuer earns 10% on the loan. This 10% difference is the \"net interest spread\" and the 5% is the \"interest expense\".\n\nThis is the cost of running the credit card portfolio, including everything from paying the executives who run the company to printing the plastics, to mailing the statements, to running the computers that keep track of every cardholder's balance, to taking the many phone calls which cardholders place to their issuer, to protecting the customers from fraud rings. Depending on the issuer, marketing programs are also a significant portion of expenses.\n\nWhen a cardholder becomes severely delinquent on a debt (often at the point of six months without payment), the creditor may declare the debt to be a charge-off. It will then be listed as such on the debtor's credit bureau reports. (Equifax, for instance, lists \"R9\" in the \"status\" column to denote a charge-off.)\n\nA charge-off is considered to be \"written off as uncollectable\". To banks, bad debts and fraud are part of the cost of doing business.\n\nHowever, the debt is still legally valid, and the creditor can attempt to collect the full amount for the time periods permitted under state law, which is usually three to seven years. This includes contacts from internal collections staff, or more likely, an outside collection agency. If the amount is large (generally over $1,500–2,000), there is the possibility of a lawsuit or arbitration.\n\nMany credit card customers receive rewards, such as frequent flyer points, gift certificates, or cash back as an incentive to use the card. Rewards are generally tied to purchasing an item or service on the card, which may or may not include balance transfers, cash advances, or other special uses. Depending on the type of card, rewards will generally cost the issuer between 0.25% and 2.0% of the spread. Networks such as Visa or MasterCard have increased their fees to allow issuers to fund their rewards system. Some issuers discourage redemption by forcing the cardholder to call customer service for rewards. On their servicing website, redeeming awards is usually a feature that is very well hidden by the issuers. With a fractured and competitive environment, rewards points cut dramatically into an issuer's bottom line, and rewards points and related incentives must be carefully managed to ensure a profitable portfolio. Unlike unused gift cards, in whose case the breakage in certain US states goes to the state's treasury, unredeemed credit card points are retained by the issuer.\n\nIn relative numbers the values lost in bank card fraud are minor, calculated in 2006 at 7 cents per 100 dollars worth of transactions (7 basis points). In 2004, in the UK, the cost of fraud was over £500 million. When a card is stolen, or an unauthorized duplicate made, most card issuers will refund some or all of the charges that the customer has received for things they did not buy. These refunds will, in some cases, be at the expense of the merchant, especially in mail order cases where the merchant cannot claim sight of the card. In several countries, merchants will lose the money if no ID card was asked for, therefore merchants usually require ID card in these countries. Credit card companies generally guarantee the merchant will be paid on legitimate transactions regardless of whether the consumer pays their credit card bill.\nMost banking services have their own credit card services that handle fraud cases and monitor for any possible attempt at fraud. Employees that are specialized in doing fraud monitoring and investigation are often placed in Risk Management, Fraud and Authorization, or Cards and Unsecured Business. Fraud monitoring emphasizes minimizing fraud losses while making an attempt to track down those responsible and contain the situation. Credit card fraud is a major white collar crime that has been around for many decades, even with the advent of the chip based card (EMV) that was put into practice in some countries to prevent cases such as these. Even with the implementation of such measures, credit card fraud continues to be a problem.\n\nOffsetting the costs are the following revenues:\n\nIn addition to fees paid by the card holder, merchants must also pay interchange fees to the card-issuing bank and the card association. For a typical credit card issuer, interchange fee revenues may represent about a quarter of total revenues.\n\nThese fees are typically from 1 to 6 percent of each sale, but will vary not only from merchant to merchant (large merchants can negotiate lower rates), but also from card to card, with business cards and rewards cards generally costing the merchants more to process. The interchange fee that applies to a particular transaction is also affected by many other variables including: the type of merchant, the merchant's total card sales volume, the merchant's average transaction amount, whether the cards were physically present, how the information required for the transaction was received, the specific type of card, when the transaction was settled, and the authorized and settled transaction amounts. In some cases, merchants add a surcharge to the credit cards to cover the interchange fee, encouraging their customers to instead use cash, debit cards, or even cheques.\n\nInterest charges vary widely from card issuer to card issuer. Often, there are \"teaser\" rates in effect for initial periods of time (as low as zero percent for, say, six months), whereas regular rates can be as high as 40 percent. In the U.S. there is no federal limit on the interest or late fees credit card issuers can charge; the interest rates are set by the states, with some states such as South Dakota, having no ceiling on interest rates and fees, inviting some banks to establish their credit card operations there. Other states, for example Delaware, have very weak usury laws. The teaser rate no longer applies if the customer does not pay their bills on time, and is replaced by a penalty interest rate (for example, 23.99%) that applies retroactively.\n\nThe major fees are for:\n\nIn the U.S., the Credit CARD Act of 2009 specifies that credit card companies must send cardholders a notice 45 days before they can increase or change certain fees. This includes annual fees, cash advance fees, and late fees.\n\nConsumers who keep their account in good order by always staying within their credit limit, and always making at least the minimum monthly payment will see interest as the biggest expense from their card provider. Those who are not so careful and regularly surpass their credit limit or are late in making payments are exposed to multiple charges that were typically as high as £25–35 until a ruling from the Office of Fair Trading that they would presume charges over £12 to be unfair which led the majority of card providers to reduce their fees to £12.\n\nThe Credit CARD Act of 2009 requires that consumers opt into over-limit charges. Some card issuers have therefore commenced solicitations requesting customers to opt into overlimit fees, presenting this as a benefit as it may avoid the possibility of a future transaction being declined. Other issuers have simply discontinued the practice of charging overlimit fees. Whether a customer opts into the overlimit fee or not, banks will in practice have discretion as to whether they choose to authorize transactions above the credit limit or not. Of course, any approved over limit transactions will only result in an overlimit fee for those customers who have opted into the fee. This legislation took effect on 22 February 2010. Following this Act, the companies are now required by law to show on a customer's bills how long it would take them to pay off the balance.\n\nThe higher fees originally charged were claimed to be designed to recoup the card operator's overall business costs and to try to ensure that the credit card business as a whole generated a profit, rather than simply recovering the cost to the provider of the limit breach, which has been estimated as typically between £3–£4. Profiting from a customer's mistakes is arguably not permitted under UK common law, if the charges constitute penalties for breach of contract, or under the Unfair Terms in Consumer Contracts Regulations 1999.\n\nSubsequent rulings in respect of personal current accounts suggest that the argument that these charges are penalties for breach of contract is weak, and given the Office of Fair Trading's ruling it seems unlikely that any further test case will take place.\n\nWhilst the law remains in the balance, many consumers have made claims against their credit card providers for the charges that they have incurred, plus interest that they would have earned had the money not been deducted from their account. It is likely that claims for amounts charged in excess of £12 will succeed, but claims for charges at the OFT's £12 threshold level are more contentious.\n\nThe Government of Canada maintains a database of the fees, features, interest rates and reward programs of nearly 200 credit cards available in Canada. This database is updated on a quarterly basis with information supplied by the credit card issuing companies. Information in the database is published every quarter on the website of the Financial Consumer Agency of Canada (FCAC).\n\nInformation in the database is published in two formats. It is available in PDF comparison tables that break down the information according to type of credit card, allowing the reader to compare the features of, for example, all the student credit cards in the database.\n\nThe database also feeds into an interactive tool on the FCAC website. The interactive tool uses several interview-type questions to build a profile of the user's credit card usage habits and needs, eliminating unsuitable choices based on the profile, so that the user is presented with a small number of credit cards and the ability to carry out detailed comparisons of features, reward programs, interest rates, etc.\n\nCredit card debt has increased steadily. Since the late 1990s, lawmakers, consumer advocacy groups, college officials and other higher education affiliates have become increasingly concerned about the rising use of credit cards among college students. The major credit card companies have been accused of targeting a younger audience, especially college students, many of whom are already in debt with college tuition fees and college loans and who typically are less experienced at managing their own finances. Credit card debt may also negatively affect their grades as they are likely to work more both part and full-time positions.\n\nAnother controversial area is the universal default feature of many North American credit card contracts. When a cardholder is late paying a particular credit card issuer, that card's interest rate can be raised, often considerably. With universal default, a customer's other credit cards, for which the customer may be current on payments, may also have their rates and credit limit changed. The universal default feature allows creditors to periodically check cardholders' credit portfolios to view trade, allowing these other institutions to decrease the credit limit or increase rates on cardholders who may be late with another credit card issuer. Being late on one credit card will potentially affect all the cardholder's credit cards. Citibank voluntarily stopped this practice in March 2007 and Chase stopped the practice in November 2007.\n\nThe fact that credit card companies can change the interest rate on debts that were incurred when a different rate of interest was in place is similar to adjustable rate mortgages where interest rates on current debt may rise. However, in both cases, this is agreed to in advance, and is a trade off that allows a lower initial rate as well as the possibility of an even lower rate (mortgages, if interest rates fall) or perpetually keeping a below-market rate (credit cards, if the user makes their debt payments on time). The universal default practice was encouraged by federal regulators, particularly those at the Office of the Comptroller of the Currency (OCC), as a means of managing the changing risk profiles of cardholders.\n\nAnother controversial area is the trailing interest issue. Trailing interest is the practice of charging interest on the entire bill no matter what percentage of it is paid. US Senator Carl Levin raised the issue of millions of Americans affected by hidden fees, compounding interest and cryptic terms. Their woes were heard in a Senate Permanent Subcommittee on Investigations hearing which was chaired by Senator Levin, who said that he intends to keep the spotlight on credit card companies and that legislative action may be necessary to purge the industry. In 2009, the C.A.R.D. Act was signed into law, enacting protections for many of the issues Levin had raised.\n\nIn the United States, some have called for Congress to enact additional regulations on the industry to expand the disclosure box clearly disclosing rate hikes, use plain language, incorporate balance payoff disclosures, and also to outlaw universal default. At a congress hearing around 1 March 2007, Citibank announced it would no longer practice this, effective immediately. Opponents of such regulation argue that customers must become more proactive and self-responsible in evaluating and negotiating terms with credit providers. Some of the nation's influential top credit card issuers, which are among the top fifty corporate contributors to political campaigns, successfully opposed it.\n\nIn the United Kingdom, merchants won the right through The Credit Cards (Price Discrimination) Order 1990 to charge customers different prices according to the payment method. As of 2007, the United Kingdom was one of the world's most credit card-intensive countries, with 2.4 credit cards per consumer, according to the UK Payments Administration Ltd.\n\nIn the United States until 1984, federal law prohibited surcharges on card transactions. Although the federal Truth in Lending Act provisions that prohibited surcharges expired that year, a number of states have since enacted laws that continue to outlaw the practice; California, Colorado, Connecticut, Florida, Kansas, Massachusetts, Maine, New York, Oklahoma, and Texas have laws against surcharges. As of 2006, the United States probably had one of the world's highest if not the top ratio of credit cards per capita, with 984 million bank-issued Visa and MasterCard credit card and debit card accounts alone for an adult population of roughly 220 million people. The credit card per US capita ratio was nearly 4:1 as of 2003 and as high as 5:1 as of 2006.\n\nMany credit cards can also be used in an ATM to withdraw money against the credit limit extended to the card, but many card issuers charge interest on cash advances before they do so on purchases. The interest on cash advances is commonly charged from the date the withdrawal is made, rather than the monthly billing date. Many card issuers levy a commission for cash withdrawals, even if the ATM belongs to the same bank as the card issuer. Merchants do not offer cashback on credit card transactions because they would pay a percentage commission of the additional cash amount to their bank or merchant services provider, thereby making it uneconomical. Discover is a notable exception to the above. A customer with a Discover card may get up to $120 cash back if the merchant allows it. This amount is simply added to the card holder's cost of the transaction and no extra fees are charged as the transaction is not considered a cash advance.\n\nMany credit card companies will also, when applying payments to a card, do so, for the matter at hand, at the end of a billing cycle, and apply those payments to everything before cash advances. For this reason, many consumers have large cash balances, which have no grace period and incur interest at a rate that is (usually) higher than the purchase rate, and will carry those balances for years, even if they pay off their statement balance each month.\n\nCredit cards are a risky way for entrepreneurs to acquire capital for their start ups when more conventional financing is unavailable. Len Bosack and Sandy Lerner used personal credit cards to start Cisco Systems. Larry Page and Sergey Brin's start up of Google was financed by credit cards to buy the necessary computers and office equipment, more specifically \"a terabyte of hard disks\". Similarly, filmmaker Robert Townsend financed part of \"Hollywood Shuffle\" using credit cards. Director Kevin Smith funded \"Clerks\" in part by maxing out several credit cards. Actor Richard Hatch also financed his production of \"\" partly through his credit cards. Famed hedge fund manager Bruce Kovner began his career (and, later on, his firm Caxton Associates) in financial markets by borrowing from his credit card. UK entrepreneur James Caan (as seen on \"Dragons' Den\") financed his first business using several credit cards.\n\nTravellers from the U.S. had encountered problems abroad because many countries have introduced smart cards, but the U.S. had not. , the U.S. banking system had not updated the cards and associated readers in the U.S., stating that the costs were prohibitive. As of 2015, the smart cards had been introduced and put into use in the United States. Other problems with credit cards have involved mis-sold policies on top of the products, such as the additional mis-sold policies which is still causing problems for clients in the UK.\n\n\n"}
{"id": "68056", "url": "https://en.wikipedia.org/wiki?curid=68056", "title": "DIVX", "text": "DIVX\n\nDIVX (Digital Video Express) was an unsuccessful attempt by Circuit City and the entertainment law firm Ziffren, Brittenham, Branca and Fischer to create an alternative to video rental in the United States.\n\nDIVX was a rental format variation on the DVD player in which a customer would buy a DIVX disc (similar to a DVD) for approximately US$4.50, which was watchable for up to 48 hours from its initial viewing. After this period, the disc could be viewed by paying a continuation fee to play it for two more days. Viewers who wanted to watch a disc an unlimited number of times could convert the disc to a \"DIVX silver\" disc for an additional fee. \"DIVX gold\" discs that could be played an unlimited number of times on any DIVX player were announced at the time of DIVX's introduction, but no DIVX gold titles were ever released.\n\nEach DIVX disc was marked with a unique barcode in the burst cutting area that could be read by the player, and used to track the discs. The status of the discs was monitored through an account over a phone line. DIVX player owners had to set up an account with DIVX to which additional viewing fees could be charged. The player would call an account server over the phone line to charge for viewing fees similar to the way DirecTV and Dish Network satellite systems handle pay-per-view.\n\nIn addition to the normal Content Scramble System (CSS) encryption, DIVX discs used Triple DES encryption and an alternative channel modulation coding scheme, which prevented them from being read in standard DVD players.\n\nDIVX players manufactured by Zenith Electronics, Thomson Consumer Electronics (RCA and ProScan), and Matsushita Electric (Panasonic) started to become available in mid-1998. Because of widespread studio support, manufacturers anticipated that demand for the units would be high. Initially, the players were approximately twice as expensive as standard DVD players, but price reductions occurred within months of release, due to economies of scale.\n\nThe initial trial of the DIVX format was run in the San Francisco and Richmond, California, areas starting on June 8, 1998. Initially only a single Zenith player was available, along with 19 titles. A nationwide rollout began three months later, on September 25, with players and 150 titles available in 190 stores. In total 87,000 players were sold during 1998, with 535,000 discs across 300 titles being sold.\n\nDIVX was sold primarily through the Circuit City, Good Guys, Ultimate Electronics, and Future Shop retailers. The format was promoted to consumers as an alternative to traditional video rental schemes with the promise of \"No returns, no late fees.\" Though consumers could just discard a DIVX disc after the initial viewing period, several DIVX retailers maintained DIVX recycling bins on their premises.\n\nBy March 1999, around 419 titles were available in the DIVX format.\n\nA movement on the Internet was initiated against DIVX, particularly in home theater forums. The DIVX catalog of titles was released primarily in pan and scan format with limited special features, usually only a trailer. This caused many home theater enthusiasts to become concerned that the success of DIVX would significantly diminish the release of films on the DVD format in the films' original aspect ratios and with supplementary material. Many people in various technology and entertainment communities were afraid that there would be DIVX exclusive releases, and that the then-fledgling DVD format would suffer as a result. DreamWorks, 20th Century Fox, and Paramount Pictures, for instance, initially released their films exclusively on the DIVX format. DIVX featured stronger encryption technology than DVD (Triple DES), which many studios stated was a contributing factor in the decision to support DIVX first.\n\nIn addition to the hostile Internet response, competitors such as Hollywood Video ran advertisements touting the benefits of \"Open DVD\" over DIVX, with one ad in the Los Angeles Times depicting a hand holding a telephone line with the caption, \"Don't let anyone feed you the line.\" The terminology \"Open DVD\" had been used by DVD supporters in response to DIVX's labeling of DVD as \"Basic DVD\" and DIVX/DVD players as \"DIVX-enhanced.\"\n\nInformational freedom advocates were concerned that the players' \"dial-home\" ability could be used to spy on people's watching habits.\n\nAllegations of anti-competitive vaporware, as well as concerns within the software industry prompted David Dranove of Northwestern University and Neil Gandal of Tel Aviv University and University of California, Berkeley, to conduct an empirical study designed to measure the effect of the DIVX announcement on the DVD market. This study suggests that the DIVX announcement slowed the adoption of DVD technology. According to Dranove and Gandal, the study suggests that the \"general antitrust concern about vaporware seems justified.\"\n\nThe format was discontinued on June 16, 1999, because of the costs of introducing the format, as well as its very limited acceptance by the general public. It was shot down by Blockbuster Video stores not wanting to carry it. Also Circuit City announced a $114 million after-tax loss, and Variety estimated the total loss on the scheme was around $337 million. Over the next two years the DIVX system was phased out. Customers could still view all their DIVX discs and were given a $100 refund for every player that was purchased before June 16, 1999. All discs that were unsold at the end of the summer of 1999 were destroyed. The program officially cut off access to accounts on July 7, 2001. The player's Security Module, which had an internal Real-Time Clock, ceased to allow DIVX functions after 30 days without a connection to the central system. Unsold players were liquidated in online auctions, but not before being modified to remove the DIVX Security Module. As a result, certain player models demonstrated lockups when DIVX menus were accessed.\n\nOn the company website to announce discontinuation of the product on June 16, 1999, it stated: \"All DIVX-featured DVD players are fully functional DVD players and will continue to operate as such. All DIVX discs, including those previously purchased by consumers and those remaining in retailer inventories, can be viewed on registered players anytime between now and June 30, 2001. Subsequent viewings also will be available during that period. Discs can no longer be upgraded to unlimited viewing, known as DIVX Silver. Customers who have converted discs to DIVX Silver can continue viewing the discs until June 30, 2001, or can receive a full refund of the conversion price at their request\". This meant no DIVX discs could play any content after June 30, 2001, rendering the medium worthless.\n\nDIVX appeared as a \"dishonorable mention\" alongside \"PC World\"<nowiki>'</nowiki>s list of \"25 Worst Tech Products of All Time\" in 2006.\n\n\n"}
{"id": "50214801", "url": "https://en.wikipedia.org/wiki?curid=50214801", "title": "Daktylios", "text": "Daktylios\n\nDaktylios (, meaning ring) is the name given to the road space rationing or alternate-day travel developed for Athens during the 1990s. Unlike congestion pricing systems, it does not operate on the basis of drivers paying fees for entering the city centre. Instead, it depends on the parity of the date and of the vehicle's registration plate, the vehicle type as well as the time of the week/month.\nThere are three \"Daktylios\" areas.\n\nThe Inner \"Daktylios\", which covers the area of the city center, allows vehicles into it depending primarily on the parity of the date and of the vehicle's registration plate, the vehicle type as well as the time of the week/month. On odd dates, only vehicles with an odd plate number are allowed to proceed into the \"Daktylios\". On even dates, only cars with even plate numbers are allowed. Lorries and other heavy vehicles are always banned and residents within the Inner \"Daktylios\" area are required to have a special pass that grants them access on all times of the year. The Inner \"Daktylios\" is not valid on weekends, bank holidays and during certain months (usually between July and October), with the dates changing every year depending on the expected levels of traffic.\n\nThe Outer \"Daktylios\" covers a much wider area and is implemented only during times of extreme congestion or high pollution. It affects only heavy vehicles and certain roads within the area covered are not subjected to the Outer \"Daktylios\" restrictions.\n\nThe Green \"Daktylios\" is a scheme that allows electric cars, hybrid cars and all vehicles produced after 2011 that produce less than 140g/km of CO2 access to the Inner \"Daktylios\" anytime.\n\nThe \"Daktylios\" scheme has been criticised for being ineffective, with the primary reason being that many people own two cars, one with odd and one with even plate numbers, granting them daily access and the other one being that it is occasionally violated. However, there are no plans to phase it out or replace it with a congestion pricing system.\n\n"}
{"id": "57859530", "url": "https://en.wikipedia.org/wiki?curid=57859530", "title": "Data plan", "text": "Data plan\n\nData plan refers to data quotas from a telecommunications or data hosting contract. Data plans are offered by internet service providers. These include mobile data plans, offered on cellular networks, from cellular telephony companies, and those from conventional fixed land line links, amongst other forms of offered data communications. Network data hosting servers also offer plans based on data served, such as for websites.\n"}
{"id": "4620242", "url": "https://en.wikipedia.org/wiki?curid=4620242", "title": "Detonator (railway)", "text": "Detonator (railway)\n\nA railway detonator (torpedo in North America) is a coin-sized device that is used as a loud warning signal to train drivers. It is placed on the top of the rail, usually secured with two lead straps, one on each side. When the wheel of the train passes over, it explodes emitting a loud bang. It was invented in 1841 by English inventor Edward Alfred Cowper.\n\nTypical uses of detonators include:\n\nOn a high-speed line, detonators may need to be placed on both rails.\n\nAs with all explosives, detonators can become unstable over time and must therefore be replaced regularly.\n\nThey are triggered by pressure rather than impact. This makes them safe during transport, as they normally cannot detonate in a bag or storage container. Detonating them by striking with a hammer does work.\n\nUpon hearing the noise of a torpedo exploding, the engineer reduces speed to 20 mph or less, not resuming its original speed until at least two miles beyond where it encountered the device. They were traditionally used in pairs to ensure that the sound registered with train crews. Torpedoes are essentially obsolete in the U.S. as soundproof construction of modern locomotive cabs renders them useless.\n\nQuoting from the terminology book of the Brotherhood of Railroad Signalmen:\n\nA torpedo is a device which is strapped to the top of a rail. When a train drives over the torpedo, it emits a very loud \"bang\" which can be heard over the noise of the engine, and signals the engineer to stop immediately. Torpedoes are generally placed by the flagman when protecting a train ahead.\n\nTorpedoes are about 2\" x 2\", red in color, about 3/4\" high, and have two lead straps attached, which hold it to the rail. The torpedo has discs inside and are filled with detonating powder. The torpedo was invented about 1874.\n\nDefined by the Rule Book, when protecting a line for whatever reason (failed train, obstruction fouling the running lines), three detonators must be placed 20 yards apart. These signal an emergency stop to the driver. Detonators are also used to protect engineering works where the line remains open. The controller of the possession will communicate with the operative with the detonators as to when to place them on the track, or lift them to allow a train through.\n\nDetonators were used where hazards had to be secured and there was no time for other signaling or if there was a danger that another signal might not be recognizable in time, for example due to fog or snow. To give the emergency signal, three detonators were placed in short succession, with the explosion of a single detonator being a stop signal. Since 1986 detonators have no longer been used on German railways. Only the ICE 3 trains that travel to France still have detonators on board because of French regulations.\n\nThe use of detonators has been superseded by radio communications since the early 1950s. In November 2010, the Taiwan Railway Administration deployed 800 detonators for destruction on maintenance tracks. It received media attention, emitting sounds similar to the culturally significant firecrackers.\n\nToday known as audible track warning signals, or audible track warning devices, detonators are used to attract the attention of train crews when track repairs or an obstruction are ahead, or when a hand signaller is acting for a signal.\n\nThe Detonators used to attract the attention of train crews in case of;\n\nMany mechanical signal boxes in the UK were equipped with detonator placers that placed detonators on a running line when a lever was operated. The levers were painted a striking white and black chevron pattern, pointing upwards for the \"Up\" line, downwards for the \"Down\" line. In some cases, the placers were fed from a cartridge holding a number of detonators.\n\nAccording to \"Military and Civilian Pyrotechnics\" by Ellern, page 376, FORMULA 155 – Railroad Torpedo, is by mass:\n\n\nThe length of Garratt locomotives made the sound of a detonator hard to hear, so New South Wales 60 class locomotive had \"sound pipes\" to bring the noise of the explosion to the crew.\n"}
{"id": "21280519", "url": "https://en.wikipedia.org/wiki?curid=21280519", "title": "Dragon beam", "text": "Dragon beam\n\nDragon beam is a horizontal, diagonal beam in the corner(s) of some traditional timber framed buildings. The term is commonly used in both hip roof framing and jettying. Older publications may use the synonyms dragging beam, dragging piece, dragging tie, dragon piece or dragon tie. Inconsistencies in modern usage are discussed below. In French it is called a coyer or enrayure.\n\nThe etymology of dragon is unclear. The term may be descended from German (a carrier), Danish (bearing beam, joist, girder) or Dutch (beam). The origin has also been proposed as a corruption of diagonal or diagon.\n\nThe dragon beam lies parallel to and below a hip rafter and carries the rafter. The dragon beam is carried by the wall on the outer end and by a horizontal piece between the two walls on the inside end. There are conflicting usages for this term in the U.K. and U.S.A. (see below). The most common usage seems to be combination dragon beam/cross tie.\n\nIn buildings with jetties on adjacent walls the dragon beam is a horizontal, diagonal beam projecting from a corner which supports the jetties. Sometimes the post below the dragon beam is called a dragon post.\n"}
{"id": "52322527", "url": "https://en.wikipedia.org/wiki?curid=52322527", "title": "FoldiMate", "text": "FoldiMate\n\nFoldiMate is a California-based company developing a robotic laundry folding machine.\n\nFoldimate was founded by Gal Rozov, an Israeli software engineer who decided that folding laundry was a tedious chore that could be done effectively by a robot. In 2010, Rozov quit his job as a software developer and product manager and spent two years developing his laundry-folding device. In 2012, he moved to the United States to work with a robotic team in Silicon Valley. By 2013, he had a patented technology. In 2016, after an initial round of investment, he produced the first prototype. The prototype presented at CES 2017 generated much interest.\n\nThe company exhibited an updated prototype of Foldimate at CES 2018.\n\nIn January 2018, BSH Hausgeräte expressed an interest in partnering with Foldimate.\n\nThe FoldiMate is slightly larger than a standard washing machine. According to the developers, it can fold a full wash in less than 4 minutes.\n\nThe user clips the piece of clothing on two hooks and the item is pulled into the machine. Then a series of rollers and arms moves in all directions to straighten and fold it. The machine can fold shirts, tops, trousers and dresses, but not small pieces of clothing like underwear or large items like sheets. The folded items are returned in a stack through a window at the bottom of the machine.\n\nPrevious versions included anti-wrinkling technology and fragrance features, but the product was redesigned and simplified with the aim of readying it for the market by the end of 2019.\n\n"}
{"id": "14041062", "url": "https://en.wikipedia.org/wiki?curid=14041062", "title": "Function–means tree", "text": "Function–means tree\n\nIn engineering design, a function–means tree (a.k.a. function/means tree or F/M tree) is a method for functional decomposition and concept generation. At the top level, main functions are identified. Under each function, a means (or solution element) is attached. Alternative solution elements can also be attached. Each means is in turn decomposed into functions with means attached to each of them. A well-elaborated function means tree span, a design space where all concepts under consideration are represented. Requirements can be attached to functions.\n\nIn addition to product level requirements, there might be requirements on sub functions that may be a consequence of means at a higher level. The function means tree is a tool that can aid in the creative part of the design process. It can also be a tool for mapping requirements to parts in a design. This requires that there be also a mapping between means and parts in the product architecture.\n\n"}
{"id": "11023014", "url": "https://en.wikipedia.org/wiki?curid=11023014", "title": "GTX Corp", "text": "GTX Corp\n\nGTX Corp (OTCMKTS:GTXO) is a technology licensee that develops miniaturized Global Positioning System (GPS) tracking technology for a wide variety of consumer branded products.\n\nGTX Corp is a holding company that owns and operates two subsidiaries engaged in the IoT and wearable technology business. Global Trek Xploration (GTX) was founded in 2002 and became publicly traded in 2008. The Company is headquartered in Los Angeles, California, with a European distribution and fulfillment center in Ireland / U.K. GTX Corp also utilizes international re-sellers/distributors in Canada, Mexico, Australia, Nepal, Italy, Switzerland, Austria, Sweden, Norway, The Netherlands, Finland, Baltics and Russia, that service customers in over 20 countries.\n\nGTX Corp is committed to pioneering, creating and delivering Smart, Mobile and Wearable IoT Technology. Empowering the global community and keeping you connected to who and what matters most with GPS tracking and recovery location-based services.\n\nGTX Corp (GTXO) is a pioneer in wearable technology and enterprise GPS real-time personal location-based services. GTX is known for its lifestyle enhancing and award-winning patented GPS Smart Shoe, blockbuster Smartphone GPS Tracking App, and innovative GPS SmartSole. GTX Corp also owns and operates LOCiMOBILE, Inc which develops applications for smart phones and tablets and Code Amber Alertag. The Company has a comprehensive intellectual property strategy and owns an extensive portfolio of patents, patents pending, registered trademarks, copy rights and URL’s, and was recently featured in a 38-page research piece which outlines the value proposition of the Company’s IP portfolio.\n\nGTX Corp is a wearable technology companies with 3 lines of GPS equipped tracking technology - Stand Alone devices, Embedded tracking devices, and a Tracking App. Each of these technologies are connected to and report to the GTX tracking platform and information is accessible through an Online Monitoring Portal and Smart Locator app. GTX Corp is known to make the GPS location-reporting platform which consists of a module that measures 2.20 × 1.36 inches. It is made to combine GPS satellite tracking and cellular transmission in a chipset with secure access to a back-end portal.\n\nThe Take-Along VL2000 model is equipped features such as GSM/GPRS for data and voice SOS alert notifications, locate-on-demand, and a sleep mode with motion sensor for up to 14 days battery life. The Prime Lite model is a robust device, small enough to embed in a host of consumer or enterprise products for quick and easy tracking.\n\nGPS tracking insoles utilize an invisible wandering assisted technology. The patented GPS SmartSole™ feature a miniaturized GPS tracking device implanted in the insoles and powered by a rechargeable battery with a 2-3 day single charge. The GPS SmartSole™ sends signals to a central monitoring website showing the exact location of the wearer/device using a combination of satellite and cellular technology. After activating and setting up a tracking account with GTX Corp, you will be able to monitor the person wearing the GPS SmartSole™, using a computer, tablet or smartphone.\n\nTrack My Workforce application for computer, tablet, and/or smartphone, allows employers to easily track and monitor their employees, drivers, and sales reps by having them download a simple Android or Apple app on their smartphones or tablets.\n\nCurrently, GTX Corp GTX owns a patent portfolio consisting of over 80 patents worldwide.\n\nGTX makes and sells products and services that help find people and things through a comprehensive tracking platform which is protected by an extensive IP portfolio including the ‘286’ patent issued last year, which is not just restricted to footwear but has far broader implications. The ‘286’ patent effects ANY tracking device, from embedded devices as in the case of the GPS SmartSole®, to other products like a GPS watch, and further to the ubiquitous stand alone, hand held wireless GPS trackers.\n\n\n\n\n"}
{"id": "45226822", "url": "https://en.wikipedia.org/wiki?curid=45226822", "title": "Green Information Technology", "text": "Green Information Technology\n\nGreen Information Technology (Green IT) refer to a systematic application of ecological sustainability criteria (such as pollution prevention, product stewardship, use of clean technologies) to the creation, sourcing, use and disposal of the IT technical infrastructure as well as within the IT human and managerial practices that directly or indirectly address environmental sustainability in organizations.\n\n\n "}
{"id": "32047031", "url": "https://en.wikipedia.org/wiki?curid=32047031", "title": "Hanwha Group", "text": "Hanwha Group\n\nHanwha Group () is one of the largest business conglomerates (\"chaebol\") in South Korea. Founded in 1952 as Korea Explosives Co. (), the group has grown into a large multi-profile business conglomerate, with diversified holdings stretching from explosives, their original business, to retail to financial services. In 1992 the company adopted its abbreviation as the new name: \"Hanwha\".\n\nThe group owns Hanwha Eagles, which is a professional baseball club in South Korea. The current Chairman of Hanwha is Kim Seung-yeon.\n\nHanwha has 56 affiliates in South Korea alone and 226 networks (affiliates, branches and representative offices) around the world (as of June 2016).\nHowever, on the other branches, other ceos have taken root. Some include Kim Ji-Dilleon and others.\n\nThe Chinese branding for Hanwha is 韓華 (Simplified Chinese is 韩华). Chinese is the only foreign language in which Hanwha has a translated name. Officially, hanja(韓華) has not been used in South Korea.\n\n\n\n\n\n\n\n"}
{"id": "213607", "url": "https://en.wikipedia.org/wiki?curid=213607", "title": "History of the telescope", "text": "History of the telescope\n\nThe earliest known telescope appeared in 1608 in the Netherlands when an eyeglass maker named Hans Lippershey tried to obtain a patent on one. Although Lippershey did not receive his patent, news of the new invention soon spread across Europe. The design of these early refracting telescopes consisted of a convex objective lens and a concave eyepiece. Galileo improved on this design the following year and applied it to astronomy. In 1611, Johannes Kepler described how a far more useful telescope could be made with a convex objective lens and a convex eyepiece lens and by 1655 astronomers such as Christiaan Huygens were building powerful but unwieldy Keplerian telescopes with compound eyepieces.\n\nIsaac Newton is credited with building the first reflector in 1668 with a design that incorporated a small flat diagonal mirror to reflect the light to an eyepiece mounted on the side of the telescope. Laurent Cassegrain in 1672 described the design of a reflector with a small convex secondary mirror to reflect light through a central hole in the main mirror.\n\nThe achromatic lens, which greatly reduced color aberrations in objective lenses and allowed for shorter and more functional telescopes, first appeared in a 1733 telescope made by Chester Moore Hall, who did not publicize it. John Dollond learned of Hall's invention and began producing telescopes using it in commercial quantities, starting in 1758.\n\nImportant developments in reflecting telescopes were John Hadley's production of larger paraboloidal mirrors in 1721; the process of silvering glass mirrors introduced by Léon Foucault in 1857; and the adoption of long-lasting aluminized coatings on reflector mirrors in 1932. The Ritchey-Chretien variant of Cassegrain reflector was invented around 1910, but not widely adopted until after 1950; many modern telescopes including the Hubble Space Telescope use this design, which gives a wider field of view than a classic Cassegrain.\n\nDuring the period 1850–1900, reflectors suffered from problems with speculum metal mirrors, and a considerable number of \"Great Refractors\" were built from 60 cm to 1 metre aperture, culminating in the Yerkes Observatory refractor in 1897; however, starting from the early 1900s a series of ever-larger reflectors with glass mirrors were built, including the Mount Wilson 60-inch (1.5 metre), the 100-inch (2.5 metre) Hooker Telescope (1917) and the 200-inch (5 metre) Hale telescope (1948); essentially all major research telescopes since 1900 have been reflectors. A number of 4-metre class (160 inch) telescopes were built on superior higher altitude sites including Hawaii and the Chilean desert in the 1975–1985 era. The development of the computer-controlled alt-azimuth mount in the 1970s and active optics in the 1980s enabled a new generation of even larger telescopes, starting with the 10-metre (400 inch) Keck telescopes in 1993/1996, and a number of 8-metre telescopes including the ESO Very Large Telescope, Gemini Observatory and Subaru Telescope.\n\nThe era of radio telescopes (along with radio astronomy) was born with Karl Guthe Jansky's serendipitous discovery of an astronomical radio source in 1931. Many types of telescopes were developed in the 20th century for a wide range of wavelengths from radio to gamma-rays. The development of space observatories after 1960 allowed access\nto several bands impossible to observe from the ground, including X-rays and longer wavelength infrared bands.\n\nObjects resembling lenses date back 4000 years although it is unknown if they were used for their optical properties or just as decoration.\nGreek accounts of the optical properties of water filled spheres (5th century BC) followed by many centuries of writings on optics, including Ptolemy (2nd century) in his \"Optics\", who wrote about the properties of light including reflection, refraction, and color, followed by Ibn Sahl (10th century) and Ibn Al-Haytham (11th century).\n\nActual use of lenses dates back to the widespread manufacture and use of eyeglasses in Northern Italy beginning in the late 13th century. The invention of the use of concave lenses to correct near-sightedness is ascribed to Nicholas of Cusa in 1451.\n\nThe first record of a telescope comes from the Netherlands in 1608. It is in a patent filed by Middelburg spectacle-maker Hans Lippershey with the States General of the Netherlands on 2 October 1608 for his instrument \"for seeing things far away as if they were nearby\". A few weeks later another Dutch instrument-maker, Jacob Metius also applied for a patent. The States General did not award a patent since the knowledge of the device already seemed to be ubiquitous but the Dutch government awarded Lippershey with a contract for copies of his design.\n\nThe original Dutch telescopes were composed of a convex and a concave lens—telescopes that are constructed this way do not invert the image. Lippershey's original design had only 3x magnification. Telescopes seem to have been made in the Netherlands in considerable numbers soon after this date of \"invention\", and rapidly found their way all over Europe.\n\nIn 1655 Dutch diplomat William de Boreel tried to solve the mystery of who invented the telescope. He had a local magistrate in Middelburg follow up on Boreel's childhood and early adult recollections of a spectacle maker named \"Hans\" who he remembered as the inventor of the telescope. The magistrate was contacted by a then unknown claimant, Middelburg spectacle maker Johannes Zachariassen, who testified that his father, Zacharias Janssen invented the telescope and the microscope as early as 1590. This testimony seemed convincing to Boreel, who now recollected that Zacharias and his father, Hans Martens, must have been who he remembered. Boreel's conclusion that Zacharias Janssen invented the telescope a little ahead of another spectacle maker, Hans Lippershey, was adopted by Pierre Borel in his 1656 book \"De vero telescopii inventore\". Discrepancies in Boreel's investigation and Zachariassen's testimony (including Zachariassen misrepresenting his date of birth and role in the invention) has led some historians to consider this claim dubious. The \"Janssen\" claim would continue over the years and be added on to with Zacharias Snijder in 1841 presenting 4 iron tubes with lenses in them claimed to be 1590 examples of Janssen's telescope and historian Cornelis de Waard's 1906 claim that the man who tried to sell a broken telescope to astronomer Simon Marius at the 1608 Frankfurt Book Fair must have been Janssen.\n\nIn 1682, the minutes of the Royal Society in London Robert Hooke noted Thomas Digges' 1571 \"Pantometria\", (a book on measurement, partially based on his father Leonard Digges' notes and observations) seemed to support an English claim to the invention of the telescope, describing Leonard as having a \"fare seeing glass\" in the mid 1500s based on an idea by Roger Bacon. Thomas described it as \"\"by proportional Glasses duly situate in convenient angles, not only discovered things far off, read letters, numbered pieces of money with the very coin and superscription thereof, cast by some of his friends of purpose upon downs in open fields, but also seven miles off declared what hath been done at that instant in private places\".\" Comments on the use of proportional or \"perspective glass\" are also made in the writings of John Dee (1575) and William Bourne (1585). Bourne was asked in 1580 to investigate the Diggs device by Queen Elizabeth I's chief advisor Lord Burghley. Bourne's is the best description of it, and from his writing it seemed to consist of peering into a large curved mirror that reflected the image produced by a large lens. The idea of an \"Elizabethan Telescope\" has been expanded over the years, including astronomer and historian Colin Ronan concluding in the 1990s that this reflecting/refracting telescope was built by Leonard Digges between 1540 and 1559. This \"backwards\" reflecting telescope would have been unwieldy, it needed very large mirrors and lens to work, the observer had to stand backwards to look at an upside down view, and Bourne noted it had a very narrow field of view making it unsuitable for military purposes. The optical performance required to see the details of coins lying about in fields, or private activities seven miles away, seems to be far beyond the technology of the time and it could be the \"perspective glass\" being described was a far simpler idea, originating with Bacon, of using a single lens held in front of the eye to magnify a distant view.\n\nA 1959 research paper by Simon de Guilleuma claimed that evidence he had uncovered pointed to the French born spectacle maker Juan Roget (died before 1624) as another possible builder of an early telescope that predated Hans Lipperhey's patent application. inventing early telescope-like devices.\n\nLippershey's application for a patent was mentioned at the end of a diplomatic report on an embassy to Holland from the Kingdom of Siam sent by the Siamese king Ekathotsarot: \"Ambassades du Roy de Siam envoyé à l'Excellence du Prince Maurice, arrivé à La Haye le 10 Septemb. 1608\" (\"Embassy of the King of Siam sent to his Excellency Prince Maurice, arrived at The Hague on 10 September 1608\"). This report was issued in October 1608 and distributed across Europe, leading to experiments by other scientists, such as the Italian Paolo Sarpi, who received the report in November, and the English mathematician and astronomer Thomas Harriot, who used a six-powered telescope by the summer of 1609 to observe features on the moon.\n\nThe Italian polymath Galileo Galilei was in Venice in June 1609 and there heard of the \"Dutch perspective glass\" by means of which distant objects appeared nearer and larger. Galileo states that he solved the problem of the construction of a telescope the first night after his return to Padua from Venice and made his first telescope the next day by fitting a convex lens in one extremity of a leaden tube and a concave lens in the other one. A few days afterwards, having succeeded in making a better telescope than the first, he took it to Venice where he communicated the details of his invention to the public and presented the instrument itself to the doge Leonardo Donato, who was sitting in full council. The senate in return settled him for life in his lectureship at Padua and doubled his salary.\n\nGalileo spent his time to improving the telescope, producing telescopes of increased power. His first telescope had a 3x magnification, but he soon made instruments which magnified 8x and finally, one nearly a meter long with a 37mm objective (which he would stop down to 16mm or 12mm) and a 23x magnification. With this last instrument he began a series of astronomical observations in October or November 1609, discovering the satellites of Jupiter, hills and valleys on the Moon, the phases of Venus and observed spots on the sun (using the projection method rather than direct observation). Galileo noted that the revolution of the satellites of Jupiter, the phases of Venus, rotation of the Sun and the tilted path its spots followed for part of the year pointed to the validity of the sun-centered Copernican system over other Earth-centered systems such as the one proposed by Ptolemy. Galileo's instrument was the first to be given the name \"telescope\". The name was invented by the Greek poet/theologian Giovanni Demisiani at a banquet held on April 14, 1611 by Prince Federico Cesi to make Galileo Galilei a member of the Accademia dei Lincei. The word was created from the Greek \"tele\" = 'far' and \"skopein\" = 'to look or see'; \"teleskopos\" = 'far-seeing'.\n\nThese observations, together with Galileo's improvement of the instrument, led to the adoption of the name of the Galilean telescope for these early forms of telescope that employed a negative lens.\n\nJohannes Kepler first explained the theory and some of the practical advantages of a telescope constructed of two convex lenses in his \"Catoptrics\" (1611). The first person who actually constructed a telescope of this form was the Jesuit Christoph Scheiner who gives a description of it in his \"Rosa Ursina\" (1630).\n\nWilliam Gascoigne was the first who commanded a chief advantage of the form of telescope suggested by Kepler: that a small material object could be placed at the common focal plane of the objective and the eyepiece. This led to his invention of the micrometer, and his application of telescopic sights to precision astronomical instruments. It was not till about the middle of the 17th century that Kepler's telescope came into general use: not so much because of the advantages pointed out by Gascoigne, but because its field of view was much larger than in the Galilean telescope.\n\nThe first powerful telescopes of Keplerian construction were made by Christiaan Huygens after much labor—in which his brother assisted him. With one of these: an objective diameter of and a focal length, he discovered the brightest of Saturn's satellites (Titan) in 1655; in 1659, he published his \"Systema Saturnium\" which, for the first time, gave a true explanation of Saturn's ring—founded on observations made with the same instrument.\n\nThe sharpness of the image in Kepler's telescope was limited by the chromatic aberration introduced by the non-uniform refractive properties of the objective lens. The only way to overcome this limitation at high magnifying powers was to create objectives with very long focal lengths. Giovanni Cassini discovered Saturn's fifth satellite (Rhea) in 1672 with a telescope long. Astronomers such as Johannes Hevelius were constructing telescopes with focal lengths as long as . Besides having really long tubes these telescopes needed scaffolding or long masts and cranes to hold them up. Their value as research tools was minimal since the telescope's frame \"tube\" flexed and vibrated in the slightest breeze and sometimes collapsed altogether.\n\nIn some of the very long refracting telescopes constructed after 1675, no tube was employed at all. The objective was mounted on a swiveling ball-joint on top of a pole, tree, or any available tall structure and aimed by means of string or connecting rod. The eyepiece was handheld or mounted on a stand at the focus, and the image was found by trial and error. These were consequently termed aerial telescopes. and have been attributed to Christiaan Huygens and his brother Constantijn Huygens, Jr. although it is not clear that they invented it. Christiaan Huygens and his brother made objectives up to diameter and focal length and others such as Adrien Auzout made telescopes with focal lengths up to . Telescopes of such great length were naturally difficult to use and must have taxed to the utmost the skill and patience of the observers. Aerial telescopes were employed by several other astronomers. Cassini discovered Saturn's third and fourth satellites in 1684 with aerial telescope objectives made by Giuseppe Campani that were in focal length.\n\nThe ability of a curved mirror to form an image may have been known since the time of Euclid and had been extensively studied by Alhazen in the 11th century. Galileo, Giovanni Francesco Sagredo, and others, spurred on by their knowledge that curved mirrors had similar properties to lenses, discussed the idea of building a telescope using a mirror as the image forming objective. Niccolò Zucchi, an Italian Jesuit astronomer and physicist, wrote in his book \"Optica philosophia\" of 1652 that he tried replacing the lens of a refracting telescope with a bronze concave mirror in 1616. Zucchi tried looking into the mirror with a hand held concave lens but did not get a satisfactory image, possibly due to the poor quality of the mirror, the angle it was tilted at, or the fact that his head partially obstructed the image.\n\nIn 1636 Marin Mersenne proposed a telescope consisting of a paraboloidal primary mirror and a paraboloidal secondary mirror bouncing the image through a hole in the primary, solving the problem of viewing the image. James Gregory went into further detail in his book \"Optica Promota\" (1663), pointing out that a reflecting telescope with a mirror that was shaped like the part of a conic section, would correct spherical aberration as well as the chromatic aberration seen in refractors. The design he came up with bears his name: the \"Gregorian telescope\"; but according to his own confession, Gregory had no practical skill and he could find no optician capable of realizing his ideas and after some fruitless attempts, was obliged to abandon all hope of bringing his telescope into practical use.\n\nIn 1666 Isaac Newton, based on his theories of refraction and color, perceived that the faults of the refracting telescope were due more to a lens's varying refraction of light of different colors than to a lens's imperfect shape. He concluded that light could not be refracted through a lens without causing chromatic aberrations, although he incorrectly concluded from some rough experiments that \"all\" refracting substances would diverge the prismatic colors in a constant proportion to their mean refraction. From these experiments Newton concluded that no improvement could be made in the refracting telescope. Newton's experiments with mirrors showed that they did not suffer from the chromatic errors of lenses, for all colors of light the angle of incidence reflected in a mirror was equal to the angle of reflection, so as a proof to his theories Newton set out to build a reflecting telescope. Newton completed his first telescope in 1668 and it is the earliest known functional reflecting telescope. After much experiment, he chose an alloy (speculum metal) of tin and copper as the most suitable material for his objective mirror. He later devised means for grinding and polishing them, but chose a spherical shape for his mirror instead of a parabola to simplify construction. He added to his reflector what is the hallmark of the design of a \"Newtonian telescope\", a secondary \"diagonal\" mirror near the primary mirror's focus to reflect the image at 90° angle to an eyepiece mounted on the side of the telescope. This unique addition allowed the image to be viewed with minimal obstruction of the objective mirror. He also made all the tube, mount, and fittings. Newton's first compact reflecting telescope had a mirror diameter of 1.3 inches and a focal ratio of f/5. With it he found that he could see the four Galilean moons of Jupiter and the crescent phase of the planet Venus. Encouraged by this success, he made a second telescope with a magnifying power of 38x which he presented to the Royal Society of London in December 1672. This type of telescope is still called a Newtonian telescope.\n\nA third form of reflecting telescope, the \"Cassegrain reflector\" was devised in 1672 by Laurent Cassegrain. The telescope had a small convex hyperboloidal secondary mirror placed near the prime focus to reflect light through a central hole in the main mirror.\n\nNo further practical advance appears to have been made in the design or construction of the reflecting telescopes for another 50 years until John Hadley (best known as the inventor of the octant) developed ways to make precision aspheric and parabolic speculum metal mirrors. In 1721 he showed the first parabolic Newtonian reflector to the Royal Society. It had a diameter, focal length speculum metal objective mirror. The instrument was examined by James Pound and James Bradley. After remarking that Newton's telescope had lain neglected for fifty years, they stated that Hadley had sufficiently shown that the invention did not consist in bare theory. They compared its performance with that of a diameter aerial telescope originally presented to the Royal Society by Constantijn Huygens, Jr. and found that Hadley's reflector, \"will bear such a charge as to make it magnify the object as many times as the latter with its due charge\", and that it represents objects as distinct, though not altogether so clear and bright.\n\nBradley and Samuel Molyneux, having been instructed by Hadley in his methods of polishing speculum metal, succeeded in producing large reflecting telescopes of their own, one of which had a focal length of . These methods of fabricating mirrors were passed on by Molyneux to two London opticians —Scarlet and Hearn— who started a business manufacturing telescopes.\n\nThe British mathematician, optician James Short began experimenting with building telescopes based on Gregory's designs in the 1730s. He first tried making his mirrors out of glass as suggested by Gregory, but he later switched to speculum metal mirrors creating Gregorian telescopes with original designers parabolic and elliptic figures. Short then adopted telescope-making as his profession which he practised first in Edinburgh, and afterward in London. All Short's telescopes were of the Gregorian form. Short died in London in 1768, having made a considerable fortune selling telescopes.\n\nSince speculum metal mirror secondaries or diagonal mirrors greatly reduced the light that reached the eyepiece, several reflecting telescope designers tried to do away with them. In 1762 Mikhail Lomonosov presented a reflecting telescope before the Russian Academy of Sciences forum. It had its primary mirror tilted at four degrees to telescope's axis so the image could be viewed via an eyepiece mounted at the front of the telescope tube without the observer's head blocking the incoming light. This innovation was not published until 1827, so this type came to be called the Herschelian telescope after a similar design by William Herschel.\nAbout the year 1774 William Herschel (then a teacher of music in Bath, England) began to occupy his leisure hours with the construction of reflector telescope mirrors, finally devoted himself entirely to their construction and use in astronomical research. In 1778, he selected a reflector mirror (the best of some 400 telescope mirrors which he had made) and with it, built a focal length telescope. Using this telescope, he made his early brilliant astronomical discoveries. In 1783, Herschel completed a reflector of approximately in diameter and focal length. He observed the heavens with this telescope for some twenty years, replacing the mirror several times. In 1789 Herschel finished building his largest reflecting telescope with a mirror of and a focal length of , (commonly known as his 40-foot telescope) at his new home, at Observatory House in Slough, England. To cut down on the light loss from the poor reflectivity of the speculum mirrors of that day, Herschel eliminated the small diagonal mirror from his design and tilted his primary mirror so he could view the formed image directly. This design has come to be called the Herschelian telescope. He discovered Saturn's sixth known moon, Enceladus, the first night he used it (August 28, 1789), and on September 17, its seventh known moon, Mimas. This telescope was world's largest telescope for over 50 years. However, this large scope was difficult to handle and thus less used than his favorite 18.7-inch reflector.\n\nIn 1845 William Parsons, 3rd Earl of Rosse built his Newtonian reflector called the \"Leviathan of Parsonstown\" with which he discovered the spiral form of galaxies.\n\nAll of these larger reflectors suffered from the poor reflectivity and fast tarnishing nature of their speculum metal mirrors. This meant they need more than one mirror per telescope since mirrors had to be frequently removed and re-polished. This was time consuming since the polishing process could change the curve of the mirror so it usually had to be \"re-figured\" to the correct shape.\n\nFrom the time of the invention of the first refracting telescopes it was generally supposed that chromatic errors seen in lenses simply arose from errors in the spherical figure of their surfaces. Opticians tried to construct lenses of varying forms of curvature to correct these errors. Isaac Newton discovered in 1666 that chromatic colors actually arose from the un-even refraction of light as it passed through the glass medium. This led opticians to experiment with lenses constructed of more than one type of glass in an attempt to canceling the errors produced by each type of glass. It was hoped that this would create an \"achromatic lens\"; a lens that would focus all colors to a single point, and produce instruments of much shorter focal length.\n\nThe first person who succeeded in making a practical achromatic refracting telescope was Chester Moore Hall from Essex, England. He argued that the different humours of the human eye refract rays of light to produce an image on the retina which is free from color, and he reasonably argued that it might be possible to produce a like result by combining lenses composed of different refracting media. After devoting some time to the inquiry he found that by combining two lenses formed of different kinds of glass, he could make an achromatic lens where the effects of the unequal refractions of two colors of light (red and blue) was corrected. In 1733, he succeeded in constructing telescope lenses which exhibited much reduced chromatic aberration. One of his instruments had an objective measuring with a relatively short focal length of .\n\nHall was a man of independent means and seems to have been careless of fame; at least he took no trouble to communicate his invention to the world. At a trial in Westminster Hall about the patent rights granted to John Dollond (Watkin v. Dollond), Hall was admitted to be the first inventor of the achromatic telescope. However, it was ruled by Lord Mansfield that it was not the original inventor who ought to profit from such invention, but the one who brought it forth for the benefit of mankind.\n\nIn 1747, Leonhard Euler sent to the Prussian Academy of Sciences a paper in which he tried to prove the possibility of correcting both the chromatic and the spherical aberration of a lens. Like Gregory and Hall, he argued that since the various humours of the human eye were so combined as to produce a perfect image, it should be possible by suitable combinations of lenses of different refracting media to construct a perfect telescope objective. Adopting a hypothetical law of the dispersion of differently colored rays of light, he proved analytically the possibility of constructing an achromatic objective composed of lenses of glass and water.\n\nAll of Euler's efforts to produce an actual objective of this construction were fruitless—a failure which he attributed solely to the difficulty of procuring lenses that worked precisely to the requisite curves. John Dollond agreed with the accuracy of Euler's analysis, but disputed his hypothesis on the grounds that it was purely a theoretical assumption: that the theory was opposed to the results of Newton's experiments on the refraction of light, and that it was impossible to determine a physical law from analytical reasoning alone.\n\nIn 1754, Euler sent to the Berlin Academy a further paper in which starting from the hypothesis that light consists of vibrations excited in an elastic fluid by luminous bodies—and that the difference of color of light is due to the greater or lesser frequency of these vibrations in a given time— he deduced his previous results. He did not doubt the accuracy of Newton's experiments quoted by Dollond.\n\nDollond did not reply to this, but soon afterwards he received an abstract of a paper by the Swedish mathematician and astronomer, Samuel Klingenstierna, which led him to doubt the accuracy of the results deduced by Newton on the dispersion of refracted light. Klingenstierna showed from purely geometrical considerations (fully appreciated by Dollond) that the results of Newton's experiments could not be brought into harmony with other universally accepted facts of refraction.\n\nAs a practical man, Dollond at once put his doubts to the test of experiment: he confirmed the conclusions of Klingenstierna, discovered a difference far beyond his hopes in the refractive qualities of different kinds of glass with respect to the divergence of colors, and was thus rapidly led to the construction of lenses in which first the chromatic aberration—and afterwards—the spherical aberration were corrected.\n\nDollond was aware of the conditions necessary for the attainment of achromatism in refracting telescopes, but relied on the accuracy of experiments made by Newton. His writings show that with the exception of his bravado, he would have arrived sooner at a discovery for which his mind was fully prepared. Dollond's paper recounts the successive steps by which he arrived at his discovery independently of Hall's earlier invention—and the logical processes by which these steps were suggested to his mind.\n\nIn 1765 Peter Dollond (son of John Dollond) introduced the triple objective, which consisted of a combination of two convex lenses of crown glass with a concave flint lens between them. He made many telescopes of this kind.\n\nThe difficulty of procuring disks of glass (especially of flint glass) of suitable purity and homogeneity limited the diameter and light gathering power of the lenses found in the achromatic telescope. It was in vain that the French Academy of Sciences offered prizes for large perfect disks of optical flint glass.\n\nThe difficulties with the impractical metal mirrors of reflecting telescopes led to the construction of large refracting telescopes. By 1866 refracting telescopes had reached in aperture with many larger \"Great refractors\" being built in the mid to late 19th century. In 1897, the refractor reached its maximum practical limit in a research telescope with the construction of the Yerkes Observatorys' refractor (although a larger refractor Great Paris Exhibition Telescope of 1900 with an objective of diameter was temporarily exhibited at the Paris 1900 Exposition). No larger refractors could be built because of gravity's effect on the lens. Since a lens can only be held in place by its edge, the center of a large lens will sag due to gravity, distorting the image it produces.\n\nIn 1856–57, Karl August von Steinheil and Léon Foucault introduced a process of depositing a layer of silver on glass telescope mirrors. The silver layer was not only much more reflective and longer lasting than the finish on speculum mirrors, it had the advantage of being able to be removed and re-deposited without changing the shape of the glass substrate. Towards the end of the 19th century very large silver on glass mirror reflecting telescopes were built.\n\nThe beginning of the 20th century saw construction of the first of the \"modern\" large research reflectors, designed for precision photographic imaging and located at remote high altitude clear sky locations such as the 60-inch Hale telescope of 1908, and the Hooker telescope in 1917, both located at Mount Wilson Observatory. These and other telescopes of this size had to have provisions to allow for the removal of their main mirrors for re-silvering every few months. John Donavan Strong, a young physicist at the California Institute of Technology, developed a technique for coating a mirror with a much longer lasting aluminum coating using thermal vacuum evaporation. In 1932, he became the first person to \"aluminize\" a mirror; three years later the and telescopes became the first large astronomical telescopes to have their mirrors aluminized. 1948 saw the completion of the Hale reflector at Mount Palomar which was the largest telescope in the world up until the completion of the massive BTA-6 in Russia twenty-seven years later. The Hale reflector introduced several technical innovations used in future telescopes, including hydrostatic bearings for very low friction, the Serrurier truss for equal deflections of the two mirrors as the tube sags under gravity, and the use of Pyrex low-expansion glass for the mirrors. The arrival of substantially larger telescopes had to await the introduction of methods other than the rigidity of glass to maintain the proper shape of the mirror.\n\nThe 1980s saw the introduction of two new technologies for building larger telescopes and improving image quality,\nknown as active optics and adaptive optics. In active optics, an image analyser senses the aberrations of a star image\na few times per minute, and a computer adjusts many support forces on the primary mirror and the location of the secondary mirror\nto maintain the optics in optimal shape and alignment. This is too slow to correct for atmospheric blurring effects, but enables the use of thin single mirrors up to 8 m diameter, or even larger segmented mirrors. This method was pioneered by the ESO New Technology Telescope in the late 1980s.\n\nThe 1990s saw a new generation of giant telescopes appear using active optics, beginning with the construction of the first of the two Keck telescopes in 1993. Other giant telescopes built since then include: the two Gemini telescopes, the four separate telescopes of the Very Large Telescope, and the Large Binocular Telescope.\n\nAdaptive optics uses a similar principle, but applying corrections several hundred times per second to\ncompensate the effects of rapidly changing optical distortion due to the motion of turbulence in the Earth's atmosphere. Adaptive optics works by measuring the distortions in a wavefront and then compensating for them by rapid changes of actuators applied to a small deformable mirror or with a liquid crystal array filter. AO was first envisioned by Horace W. Babcock in 1953, but did not come into common usage in astronomical telescopes until advances in computer and detector technology during the 1990s made it possible to calculate the compensation needed in real time. In adaptive optics, the high-speed corrections needed mean that a fairly bright star is needed very close to the target of interest (or an artificial star is created by a laser). Also, with a single star or laser the corrections are only effective over a very narrow field (tens of arcsec), and current systems operating on several 8-10m telescopes work mainly in near-infrared wavelengths for single-object observations.\n\nDevelopments of adaptive optics include systems with multiple lasers over a wider corrected field, and/or working above kiloHertz rates for good correction at visible wavelengths; these are currently in progress but not yet in routine operation as of 2015.\n\nThe twentieth century saw the construction of telescopes which could produce images using wavelengths other than visible light starting in 1931 when Karl Jansky discovered astronomical objects gave off radio emissions; this prompted a new era of observational astronomy after World War II, with telescopes being developed for other parts of the electromagnetic spectrum from radio to gamma-rays.\n\nRadio astronomy began in 1931 when Karl Jansky discovered that the Milky Way was a source of radio emission while doing research on terrestrial static with a direction antenna. Building on Jansky's work, Grote Reber built a more sophisticated purpose-built radio telescope in 1937, with a dish; using this, he discovered various unexplained radio sources in the sky. Interest in radio astronomy grew after the Second World War when much larger dishes were built including: the Jodrell bank telescope (1957), the Green Bank Telescope (1962), and the Effelsberg telescope (1971). The huge Arecibo telescope (1963) is so large that it is fixed into a natural depression in the ground; the central antenna can be steered to allow the telescope to study objects up to twenty degrees from the zenith. However, not every radio telescope is of the dish type. For example, the Mills Cross Telescope (1954) was an early example of an array which used two perpendicular lines of antennae in length to survey the sky.\n\nHigh-energy radio waves are known as microwaves and this has been an important area of astronomy ever since the discovery of the cosmic microwave background radiation in 1964. Many ground-based radio telescopes can study microwaves. Short wavelength microwaves are best studied from space because water vapor (even at high altitudes) strongly weakens the signal. The Cosmic Background Explorer (1989) revolutionized the study of the microwave background radiation.\n\nBecause radio telescopes have low resolution, they were the first instruments to use interferometry allowing two or more widely separated instruments to simultaneously observe the same source. Very long baseline interferometry extended the technique over thousands of kilometers and allowed resolutions down to a few milli-arcseconds.\n\nA telescope like the Large Millimeter Telescope (active since 2006) observes from , bridging between the far-infrared/submillimeter telescopes and longer wavelength radio telescopes including the microwave band from about to in wavelength.\n\nAlthough most infrared radiation is absorbed by the atmosphere, infrared astronomy at certain wavelengths can be conducted on high mountains where there is little absorption by atmospheric water vapor. Ever since suitable detectors became available, most optical telescopes at high-altitudes have been able to image at infrared wavelengths. Some telescopes such as the UKIRT, and the IRTF — both on Mauna Kea — are dedicated infrared telescopes. The launch of the IRAS satellite in 1983 revolutionized infrared astronomy from space. This reflecting telescope which had a mirror, operated for nine months until its supply of coolant (liquid helium) ran out. It surveyed the entire sky detecting 245,000 infrared sources—more than 100 times the number previously known.\n\nAlthough optical telescopes can image the near ultraviolet, the ozone layer in the stratosphere absorbs ultraviolet radiation shorter than 300 nm so most ultra-violet astronomy is conducted with satellites. Ultraviolet telescopes resemble optical telescopes, but conventional aluminium-coated mirrors cannot be used and alternative coatings such as magnesium fluoride or lithium fluoride are used instead. The Orbiting Solar Observatory satellite carried out observations in the ultra-violet as early as 1962. The International Ultraviolet Explorer (1978) systematically surveyed the sky for eighteen years, using a aperture telescope with two spectroscopes. Extreme-ultraviolet astronomy (10–100 nm) is a discipline in its own right and involves many of the techniques of X-ray astronomy; the Extreme Ultraviolet Explorer (1992) was a satellite operating at these wavelengths.\n\nX-rays from space do not reach the Earth's surface so X-ray astronomy has to be conducted above the Earth's atmosphere. The first X-ray experiments were conducted on sub-orbital rocket flights which enabled the first detection of X-rays from the Sun (1948) and the first galactic X-ray sources: Scorpius X-1 (June 1962) and the Crab Nebula (October 1962). Since then, X-ray telescopes (Wolter telescopes) have been built using nested grazing-incidence mirrors which deflect X-rays to a detector. Some of the OAO satellites conducted X-ray astronomy in the late 1960s, but the first dedicated X-ray satellite was the Uhuru (1970) which discovered 300 sources. More recent X-ray satellites include: the EXOSAT (1983), ROSAT (1990), Chandra (1999), and Newton (1999).\n\nGamma rays are absorbed high in the Earth's atmosphere so most gamma-ray astronomy is conducted with satellites. Gamma-ray telescopes use scintillation counters, spark chambers and more recently, solid-state detectors. The angular resolution of these devices is typically very poor. There were balloon-borne experiments in the early 1960s, but gamma-ray astronomy really began with the launch of the OSO 3 satellite in 1967; the first dedicated gamma-ray satellites were SAS B (1972) and Cos B (1975). The Compton Gamma Ray Observatory (1991) was a big improvement on previous surveys. Very high-energy gamma-rays (above 200 GeV) can be detected from the ground via the Cerenkov radiation produced by the passage of the gamma-rays in the Earth's atmosphere. Several Cerenkov imaging telescopes have been built around the world including: the HEGRA (1987), STACEE (2001), HESS (2003), and MAGIC (2004).\n\nIn 1868, Fizeau noted that the purpose of the arrangement of mirrors or glass lenses in a conventional telescope was simply to provide an approximation to a Fourier transform of the optical wave field entering the telescope. As this mathematical transformation was well understood and could be performed mathematically on paper, he noted that by using an array of small instruments it would be possible to measure the diameter of a star with the same precision as a single telescope which was as large as the whole array— a technique which later became known as astronomical interferometry. It was not until 1891 that Albert A. Michelson successfully used this technique for the measurement of astronomical angular diameters: the diameters of Jupiter's satellites (Michelson 1891). Thirty years later, a direct interferometric measurement of a stellar diameter was finally realized by Michelson & Francis G. Pease (1921) which was applied by their 20 ft (6.1 m) interferometer mounted on the 100 inch Hooker Telescope on Mount Wilson.\n\nThe next major development came in 1946 when Ryle and Vonberg (Ryle and Vonberg 1946) located a number of new cosmic radio sources by constructing a radio analogue of the Michelson interferometer. The signals from two radio antennas were added electronically to produce interference. Ryle and Vonberg's telescope used the rotation of the Earth to scan the sky in one dimension. With the development of larger arrays and of computers which could rapidly perform the necessary Fourier transforms, the first aperture synthesis imaging instruments were soon developed which could obtain high resolution images without the need of a giant parabolic reflector to perform the Fourier transform. This technique is now used in most radio astronomy observations. Radio astronomers soon developed the mathematical methods to perform aperture synthesis Fourier imaging using much larger arrays of telescopes —often spread across more than one continent. In the 1980s, the aperture synthesis technique was extended to visible light as well as infrared astronomy, providing the first very high resolution optical and infrared images of nearby stars.\n\nIn 1995 this imaging technique was demonstrated on an array of separate optical telescopes for the first time, allowing a further improvement in resolution, and also allowing even higher resolution imaging of stellar surfaces. The same techniques have now been applied at a number of other astronomical telescope arrays including: the Navy Prototype Optical Interferometer, the CHARA array, and the IOTA array. A detailed description of the development of astronomical optical interferometry can be found here .\n\nIn 2008, Max Tegmark and Matias Zaldarriaga proposed a \"Fast Fourier Transform Telescope\" design in which the lenses and mirrors could be dispensed with altogether when computers become fast enough to perform all the necessary transforms.\n\n\n"}
{"id": "37482023", "url": "https://en.wikipedia.org/wiki?curid=37482023", "title": "HusITa", "text": "HusITa\n\nhusITa (Human Services Information Technology Applications) is an international virtual associationand a registered US non-profit organizationestablished with the mission of promoting the ethical and effective use of information technology in the human services. The main focus of husITa, and the claim to expertise of its associates, is situated at the intersection of three core domains: information technology, human services, and social development. husITa pursues its mission through international conferences, publications and research dissemination directed at technology applications and innovations that promote social well-being.\n\nFor much of its early history husITa operated as an informal international network of human service academics and practitioners. One of the outcomes of its first international conferencehusITa1 held in 1987 in Birmingham, Englandwas the establishment of a working group to determine the feasibility of an international body 'to highlight the importance of human service computing, to guide developments, and to foster international co-operation'.\n\nThe working group was composed of Hein de Graaf (Netherlands), Walter LaMendola (USA), Dick Schoech (USA), and Stuart Toole (UK). Initial projects identified by the working group included the development of research agendas, position papers, repositories of information, and promoting a second husITa conference in 1989. Bryan Glastonbury was later added to the group as secretary. The working group met in Colorado, Denver for three days in May 1988 and published a report on the issues that a husITa international organization would need to address.\n\nAlthough the 1988 Denver meeting agreed its objectives, husITa wasn't formally established as an organization for another twelve years. The structure of the formal organization was later agreed to at Denver in 2000. The founding members at the Denver 2000 meeting were: Hein de Graaf, Walter LaMendola, Rob MacFadden, Jo Ann Regan, Jackie Rafferty, Jan Steyaert, Dick Schoech, Stuart Toole, and Victor Savtschenko.\n\nhusITa's objectives (agreed by the 1988 Denver working group) are to:\n\nThe \"Journal of Technology in Human Services\" is the official journal of husITa. Formerly known as \"Computers in Human Services\" it was launched in 1985 as a Haworth Press publication. Dick Schoech, a professor of social work at the University of Texas at Arlington, was its founding editor. The \"Journal of Technology in Human Services\" is a peer-reviewed, refereed journal now published by Taylor & Francis. Its scope includes the potential of information and communication technologies in mental health, developmental disability, welfare, addictions, education, and other human services. The current Editor-in-Chief is Dr. Lauri Goldkind (Associate Professor, Fordham University, USA), with Dr. Chitat Chan (Assistant Professor, Hong Kong Polytechnic University, Hong Kong) serves as the Associate Editor-in-Chief.\n\nhusITa1: husITa's first international conference was held between in September 1987 in Birmingham, England.\n\nhusITa2: \"Computer Technology and Human Services in the 90’s: Advancing Theory and Practice\", June 1991 in New Brunswick, New Jersey.\n\nhusITa3: \"Information Technology and the Quality of Life and Services\", June 1993 in Maastricht, the Netherlands. The same year saw the formation of a husITa Foundation in the Netherlands which continued until its disestablishment in 2003.\n\nhusITa4: \"Information Technology in the Human Services: Dreams and Realities\", June 1996 in Lapland, Finland.\n\nhusITa5: \"Social Services in the Information Society: Closing the GAP\", August–September 1999 in Budapest, Hungary.\n\nhusITa6: \"Technology and Human Services in a Multicultural Society\", September 2001, in Charleston, South Carolina. However, the conference was cut short as a result of the terrorist attacks in the USA on 11 September 2001. A brief husITa board meeting was held, the by-laws were approved, and officers were elected.\n\nhusITa7: \"Digital Inclusion-Building a Digital Inclusive Society\", August 2004 in Hong Kong, China. It had been delayed from its planned date of 2003 due to an outbreak of SARS.\n\nhusITa8: \"Information Technology and Diversity in Human Services: Promoting Strength Through Difference\", August 2007 in Toronto, Canada.\n\nhusITa9: \"ICT as a Context for Human Services\", June 2010 in Hong Kong, China. This event was held in conjunction with the 2010 Joint World Conference on Social Work and Social Development.\n\nhusITa14: \"Sustainable & Ethical use of Technology\". July 9–12, Melbourne, Australia. Held in conjunction with the 2014 Joint World Conference on Social Work, Education, and Social Development.\n\nhusITa was built on the activity of an international network of human service organizations, academics and practitioners in the USA, the UK and the Netherlands. The section below highlights some of the key organizations, people and events.\n\nIn 1978, Gunther R. Geiss, aprofessor of social work at Adelphi University, New York, conducted a survey of US schools of social work. The survey sought to identify faculty members who had used computers in their administrative, teaching or research activities, or who had consulted or participated in the design and development of computer-based information systems. There were over 80 positive responses indicating a wide range of activities and levels of involvement. The survey initiated the development of a system to track and communicate with individuals with expertise in computers and human services.\n\nWalter LaMendola, professor of social work at the University of Denver in Colorado, described an incident during a social work conference in 1979 suggesting early indications of the resistance of some social work professionals to computer use in the human services. This is a theme which has continued throughout the history of technology use in the human services and continues to the present day. Some aspects of this resistance can be considered as a well-founded concern about the ethical issues surrounding human service technology applications. However, other aspects of technology resistance seem to be a less rational form of Neo-Luddism.\n\nGrowing interest in the use of technology in the human services led a group of US human service technology specialists, meeting at a Council of Social Work Education conference in Louisville Kentucky in 1981, to form the Computer Use In Social Services Network (CUSSN). By the end of 1981 the network had over 350 members. The CUSSN newsletter continued in print until 1992 when it was merged with the first academic journal on human service technology \"Computers in Human Services\".\n\nIn 1984 Gunther Geiss was sponsored by the Silberman Fund to organize the Wye Plantation Conference on Human Services Technology. Conference members developed pre-conference position papers via EYES: a centralized email system.\n\nIn 1985, CUSSN developed CUSSNet CUSSNet, a PC and FidoNet based networking system that automatically exchanged emails between members each night during off-peak telephone hours. FidoNet was a PC distributed email, bulletin board, and file sharing system that preceded the Internet. CUSSNet quickly developed nodes in major cities in the US, the UK, and the Netherlands.\n\nThe name husITa (Human Service Information Technology Applications) was coined in 1983 by Walter LaMendola and Brian Klepinger at the University of Denver.\n\nThe Human Service Microcomputer Conference was held in Seattle.\n\nIn 1984 Stuart Toole formed Computer Applications in Social Work (CASW) in the UK to set up and run national conferences and to publish the CASW journal.\n\nBased on the success of the first UK technology conferences, Stuart Toole, Walter LaMendola, and Brian Klepinger agreed to pursue an international conference in 1987: this conference was to become HUSITA's first international conference HUSITA1.\n\nThe CASW journal was later renamed New Technology in the Human Services in [when] and continued in publication under this title until it was closed in 2003.\n\nIn 1986 the second UK conference held on social welfare computing was held.\n\nIn 1985 Bryan Glastonbury, from the University of Southampton published \"Computers in Social Work\", the first major academic text on technology and human services. In the same year the University of Southampton began publishing the journal New Technology in the Human Services under the editorship of Bryan Glastonbury. In the same year the University of Southampton established the Centre for Human Service Technology (CHST) with Jackie Rafferty as Director. In 2007, Jan Steyaert joined as adjunct research professor and together they edited a special issue of the British Journal of Social Work on \"social work in the digital age\".\nThe Centre for Human Service Technology is based at the University of Southampton in England. CHST is an international, multi-disciplinary research center focused on influencing the appropriate use of technology in social work practice and education and researching its implementation and impact.\n\nIn July 1997 the CTI Centre for Human Services held a conference on \"Social Services and Learning Technology\" hosted by the Institute for Health and Community Services at the University of Bournemouth.\n\nIn 2003 the journal New Technology in Human Services ceased publication.\n\nIn 1986 Hein de Graaf, Director of the CREON foundation in the Netherlands, organised the first of a series of three-day gatherings. The CREON foundation was a Dutch foundation for computer research, expertise and support in field of the human services. The gatherings, called WELCOM, were designed to increase the knowledge and understanding of information technology in the Dutch human services. WELCOM1 was held in 1986 in Bussum with Walter LaMendola as the main international speaker. WELCOM2, a smaller Dutch-only conference, took place in 1987, again in Bussum. The next WELCOM event, WELCOM3: a combined conference and fair, was held jointly with HUSITA3 in Maastricht in 1993.\n\nFollowing the success of HUSITA's first international conference held in Birmingham in 1987 the Dutch Ministry of Social Welfare, Health and Cultural Affairs organized an informal meeting of European experts in the field of Information Technology and Human Services. The meeting was one of the outcomes of a feasibility study carried out by the CREON foundation, concerning international cooperation in this field. A main conclusion of this feasibility study was that an international (European) network should be established for the exchange of products, ideas, expertise, experiences and skills with respect to the introduction and use of IT in the human services. As a first step in this approach the informal meeting of experts was organized. The European network of organizations was called ENITH (European Network Information Technology and Human services).\n\nIn 1992 an ENITH3 Expert Meeting on “IT Applications and the Quality of Life and Services” was held in The Netherlands.\n\nIn 1994, from September 21 to September 23 an ENITH4 conference was held in Berlin, Germany. Bernd kolleck chaired this conference.\n\nIn September 1995 a CAUSA5/ENITH5 conference on “The Impact of Information Technology on Social Policy,” was held in Eindhoven, The Netherlands. Jan Steyaert chaired this conference.\n"}
{"id": "58030745", "url": "https://en.wikipedia.org/wiki?curid=58030745", "title": "Ionic Coulomb blockade", "text": "Ionic Coulomb blockade\n\nIonic Coulomb blockade (ICB) is an electrostatic phenomenon that appears in ionic transport through mesoscopic electro-diffusive systems (artificial nanopores and biological ion channels) and manifests itself as oscillatory dependences of the conductance on the fixed charge formula_1 in the pore ( or on the external voltage formula_2, or on the bulk concentration formula_3). \n\nICB represents an ion-related counterpart of the better-known electronic Coulomb blockade (ECB) that is observed in quantum dots. Both ICB and ECB arise from quantisation of the electric charge and from an electrostatic exclusion principle and they share in common a number of effects and underlying physical mechanisms. ICB provides some specific effects related to the existence of ions of different valence formula_4 (different in both sign and value), in contrast to the single-valence electrons of ECB (formula_5). Coulomb blockade can also apear in superconductors; in such a case the free charge carriers are Cooper pairs (formula_6) \n\nICB effects appear in tiny pores whose self-capacitance formula_7 is so small that the charging energy formula_8 of a single ion becomes large compared to the thermal energy per particle formula_9. In such cases there is strong quantisation of the energy spectrum inside the pore, and the system may either be “blockaded” against the entry of any additional ions or, in the opposite extreme, it may show resonant barrier-less conduction, depending on the free energy bias coming from formula_1, formula_2, or formula_12. \n\nThe ICB model claims that formula_1 is a primary determinant of conduction and selectivity for particular ions, and the predicted oscillations in conductance and an associated Coulomb staircase of channel occupancy \"vs\" formula_1 are expected to be strong effects in the cases of divalent ions or trivalent ions. \n\nSome effects, now recognised as belonging to ICB, were discovered and considered earlier in precursor papers on electrostatics-governed conduction mechanisms in channels and nanopores. \n\nThe manifestations of ICB have been observed in water-filled sub-nanometre pores through a 2D <chem>MoS2</chem> monolayer, revealed by Brownian dynamics (BD) simulations of calcium conductance bands in narrow channels, and account for a diversity of effects seen in biological ion channels. ICB predictions have also been confirmed by a mutation study of divalent blockade in the NaChBac bacterial channel. \n\nICB effects may be derived on the basis of a simplified electrostatics/Brownian dynamics model of a nanopore or of the selectivity filter of an ion channel. The model represents the channel/pore as a charged hole through a water-filled protein hub embedded in the membrane. Its fixed charge formula_1 is considered as a uniform, centrally placed, rigid ring (Fig.1). The channel is assumed to have geometrical parameters length formula_16nm and radius formula_17nm, allowing for the single-file movement of partially hydrated ions. \n\nThe model represents the water and protein as continuous media with dielectric constants formula_18 and formula_19 respectively. The mobile ions are described as discrete entities with valence formula_4 and of radius formula_21, moving stochastically through the pore, governed by the self-consistently coupled Poisson's electrostatic equation and Langevin stochastic equation.\n\nThe model is applicable to both cationic and anionic biological ion channels and to artificial nanopores.\n\nThe  mobile ion is assumed to be partially hydrated  (typically retaining its first hydration shell ) and carrying charge formula_22 where formula_23 is the elementary charge (e.g. the formula_24 ion with formula_25). The model allows one to derive the pore and ion parameters satisfying the barrier-less permeation conditions, and to do so from basic electrostatics taking account of charge quantisation.\n\nThe potential energy formula_26 of a channel/pore containing formula_27 ions can be decomposed into electrostatic energyformula_28 , dehydration energy, formula_29 and ion-ion local interaction energy formula_30:formula_31\nThe basic ICB model makes the simplifying approximation that formula_32, whence:formula_33where formula_34 is the net charge of the pore when it contains formula_35 identical ions of valence formula_4, the sign of the moving ions being opposite to that of the formula_1, formula_38 represents the electrostatic self-capacitance of the pore, and formula_39 is the electric permittivity of the vacuum.\n\nThermodynamics and statistical mechanics describe systems that have variable numbers of particles via the chemical potential formula_40, defined as Gibbs free energy formula_41 per particle :formula_42, where formula_43 is the Gibbs free energy for the system of formula_35 particles. In thermal and particle equilibrium with bulk reservoirs, the entire system has a common value of chemical potential formula_45 (the Fermi level in other contexts). The free energy needed for the entry of a new ion to the channel is defined by the excess chemical potential formula_46 which (ignoring an entropy term ) can be written as formula_47 where formula_8 is the charging energy (self-energy barrier) of an incoming ion, formula_49 is the self-potential of ion and formula_50is its affinity (i.e. energy of attraction to the binding site formula_1). The difference in energy between formula_8 and formula_53 (Fig.2.) defines the ionic energy level separation (Coulomb gap) and give rise to most of the observed ICB effects. \n\nIn selective ion channels, the favoured ionic species passes through the channel almost at the rate of free diffusion, despite the strong affinity to the binding site. This conductivity-selectivty paradox has been explained as being a consequaence of selective barrier-less conduction. In the ICB model, this occurs when formula_54 is almost exactly balanced by formula_50 (formula_56), which happens for a particular value of formula_1 (Fig.2.) . This resonant value of formula_1 depends on the ionic properties formula_4 and formula_21 (implicitly, via the formula_21-dependent dehydration energy ), thereby providing a basis for selectivity. \n\nThe ICB model explicitly predicts an oscillatory dependence of conduction on formula_1, with two interlaced sets of singularities associated with a sequentially increasing number of ions formula_63 in the channel (Fig.3A). Electrostatic blockade points formula_64 correspond to minima in the ground state energy of the pore (Fig.3C).formula_65 The formula_66 points (formula_67) are equivalent to neutralisation points where formula_68.\n\nResonant conduction points formula_69 correspond to the barrier-less condition: formula_70, or formula_71.\n\nThe values of formula_72 are given by the simple formulaeformula_73i.e. the period of conductance oscillations in formula_1, formula_75.\n\nFor formula_25, in a typical ion channel geometry, formula_77, and ICB becomes strong. Consequently, plots of the BD-simulated <chem>Ca^2+</chem>current formula_78 \"vs\" formula_1 exhibit multi-ion conduction bands \"- strong Coulomb blockade oscillations\" between minima formula_80and maxima formula_81(Fig.3A)) . The point formula_82 corresponds to an uncharged pore with formula_83. Such pores are blockaded for ions of either sign.\n\nThe ICB oscillations in conductance correspond to a \"Coulomb staircase\" in the pore occupancy formula_84, with transition regions corresponding to formula_81 and saturation regions corresponding to formula_80 (Fig.3B) . The shape of the staircase is described by the Fermi-Dirac (FD) distribution , similarly to the Coulomb staircases of quantum dots . Thus, for the formula_87 transition, the FD function is: formula_88Here formula_89 is the excess chemical potential for the particular ion and formula_90 is an equivalent bulk occupancy related to pore volume. The saturated FD statistics of occupancy is equivalent to the Langmuir isotherm or to Michaelis-Menten kinetics.\n\nIt is the factor formula_91 that gives rise to the concentration-related shift in the staircase seen in Fig.3B.\n\nAddition of the partial excess chemical potentials formula_92 coming from different sources formula_93(including dehydration, local binding, volume exclusion etc. ) leads to the ICB barrier-less condition formula_94 leads to a proper shift in the ICB resonant points formula_81, described by a \"shift equation<nowiki>\"</nowiki> : formula_96 i.e. the additional energy contributions formula_97 lead to shifts in the resonant barrier-less point formula_98.\n\nThe more important of these shifts (excess potentials) are:\n\n\nFollowing its prediction based on analytic theory and molecular dynamics simulations, experimental evidence for ICB emerged from experiments on monolayer <chem>MoS2</chem> pierced by a single formula_102nm nanopore. Highly non-Ohmic conduction was observed between aqueous ionic solutions on either side of the membrane. In particular, for low voltages across the membrane, the current remained close to zero, but it rose abruptly when a threshold of about formula_103mV was exceeded. This was interpreted as complete ionic Coulomb blockade of current in the (uncharged) nanopore due to the large potential barrier at low voltages. But the application of larger voltages pulled the barrier down, producing accessible states into which transitions could occur, thus leading to conduction.\n\nThe realisation that ICB could occur in biological ion channels accounted for several experimentally observed features of selectivity, including: \n\nValence selectivity is the channel's ability to discriminate between ions of different valence formula_4, wherein e.g. a calcium channel favours formula_24 ions over formula_106 ions by a factor of up to 1000×. Valence selectivity has been attributed variously to pure electrostatics,\nor to a charge space competition mechanism,\nor to a snug fit of the ion to ligands,\nor to quantised dehydration.\nIn the ICB model, valence selectivity arises from electrostatics, namely from formula_4-dependence of the value of formula_108 needed to provide for barrier-less conduction. \n\nCorrespondingly, the ICB model provides explanations of why site-directed mutations that alter formula_1 can destroy the channel by blockading it, or can alter its selectivity from favouring formula_24 ions to favouring formula_106 ions, or \"vice versa\" \".\"\n\nDivalent (e.g. formula_24) blockade of monovalent (e.g. formula_106) currents is observed in some types of ion channel. Namely, formula_106 ions in a pure sodium solution pass unimpeded through a calcium channel, but are blocked by tiny (nM) extracellular concentrations of formula_24 ions. ICB provides a transparent explanation of both the phenomenon itself and of the Langmuir-isotherm-shape of the current \"vs.\" formula_116 attenuation curve, deriving them from the strong affinity and an FD distribution of <chem>Ca^2+</chem>ions. Similarly, ICB can account for the divalent (Iodide <chem>I^2-</chem>) blockade that has been observed in biological chloride (<chem>Cl-</chem>)-selective channels.\n\nICB and ECB should be considered as two versions of the same fundamental electrostatic phenomenon. Both ICB and ECB are based on charge quantisation and on the finite single-particle charging energy formula_117, resulting in close similarity of the governing equations and manifestations of these closely related phenomena. Nonetheless, there are important distinctions between ICB and ECB: their similarities and differences are summarised in Table 1. \n\nDespite appearing in completely classical systems, ICB exhibits some phenomena reminiscent of quantum-mechanics (QM). They arise because the charge/entity discreteness of the ions leads to quantisation of the energy formula_8 spectrum and hence to the QM-analogies:\n\n\n"}
{"id": "523288", "url": "https://en.wikipedia.org/wiki?curid=523288", "title": "Isuzu Motors", "text": "Isuzu Motors\n\n, trading as Isuzu (, ), is a Japanese commercial vehicle and diesel engine manufacturing company headquartered in Tokyo. Its principal activity is the production, marketing and sale of Isuzu commercial vehicles and diesel engines.\n\nIt also has a number of subsidiaries, included Anadolu Isuzu (a Turkish joint venture with Anadolu Group), Sollers-Isuzu (a Russian joint venture with Sollers JSC), SML Isuzu (formerly Swaraj Mazda), Jianxi Isuzu Motors (a Chinese joint venture with Jiangling Motors Company Group), Isuzu Astra Motor Indonesia, Isuzu Malaysia (Isuzu HICOM), Isuzu UK, Isuzu South Africa, Isuzu Philippines, Taiwan Isuzu Motors, Isuzu Vietnam, and Isuzu Motors India.\n\nIsuzu has assembly and manufacturing plants in Fujisawa, as well as in the Tochigi and Hokkaidō prefectures. Isuzu-branded vehicles are sold in most commercial markets worldwide. Isuzu's primary market focus is on commercial diesel-powered truck, buses and construction, while their Japanese competitor Yanmar focuses on commercial-level powerplants and generators.\n\nBy 2009, Isuzu had produced over 21 million diesel engines, which can be found in vehicles all over the world. Isuzu diesel engines are used by dozens of vehicle manufacturers, including Ford Motor Company and the Renault–Nissan–Mitsubishi Alliance.\n\nIsuzu Motors' history began in 1916, when Tokyo Ishikawajima Shipbuilding and Engineering Co., Ltd. planned a cooperation with the Tokyo Gas and Electric Industrial Co. to build automobiles. The next step was taken in 1918, when a technical cooperation with Wolseley Motors Limited was initiated, yielding exclusive rights to the production and sales of Wolseley vehicles in East Asia. In 1922 came the first ever Japan-produced passenger car, a Wolseley model, the A9. The CP truck followed two years later; 550 of these were built by 1927. In 1933, Ishikawajima Automotive Works merged with DAT Automobile Manufacturing Inc. (a predecessor of Nissan) and changed its name to Automobile Industries Co., Ltd. The products of this company, marketed as \"Sumiya\" and \"Chiyoda\", were renamed Isuzu (after the Isuzu River) in 1934, following a meeting with the Japanese Government's Ministry of Trade and Industry (MITI). The word Isuzu translated into English means \"fifty bells\"—hence the focus on \"bell\" in both the later Bellel and the Bellett.\n\nIn 1937 Automobile Industries was reorganized and formed into a new company, Tokyo Automobile Industries Co., Ltd. It was founded with a capital of ¥1,000,000. Only in 1949 was Isuzu finally adopted as the company name. Meanwhile, in 1942, Hino Heavy Industries was split off from Isuzu, becoming a separate corporation. Truck production () began anew in 1945, with the permission of the occupation authorities. Beginning in 1953 the Hillman Minx passenger car is produced under license of Rootes Group. The Minx remained in production until 1962, after the 1961 introduction of Isuzu's first own car, the Bellel. Being a small producer making cars which were somewhat too large and pricey for the Japanese market at the time, Isuzu spent some time looking for a commercial partner. Under pressure from MITI, who were attempting to limit the number of automobile manufacturers in Japan, a cooperation with Fuji Heavy Industries (Subaru) began in 1966. This joint sales-service collaboration was seen as the first step towards an eventual merger. The Subaru 1000 was even shown in Isuzu's 1967 annual vehicle brochure, as a suitable complement to the larger Isuzu lineup. This tie-up was over by 1968, when an agreement with Mitsubishi was formed. This ended even quicker, by 1969, and the next year an equally short-lived collaboration was entered with Nissan. A few months later, in September 1971, what was to prove a more durable capital agreement was signed with General Motors.\n\nThe first result of GM taking a 34% stake in Isuzu is seen in 1972, only months later, when the Chevrolet LUV becomes the first Isuzu-built vehicle to be sold in the United States. To symbolize the new beginning, Isuzu also developed a new logo for 1974, with two vertical pillars which are stylized representations of the first syllable in いすゞ (\"Isuzu\"). In 1974 Isuzu introduced the Gemini, which was co-produced with General Motors as the T-car. It was sold in the United States as Buick's Opel by Isuzu, and in Australia as the Holden Gemini. As a result of the collaboration, certain American GM products are sold to Japanese customers through Isuzu dealerships. Holden's Statesman was also briefly sold (246 examples) with Isuzu badging in Japan during the seventies. Isuzu exports also increased considerably as a result of being able to use GM networks, from 0.7% of production in 1973 to 35.2% by 1976; this while overall production increased more than fourfold in the same period. As a result of the GM joint venture, Isuzu engines were also used by existing GM divisions (some USA-market Chevrolet automobiles had Isuzu powertrains e.g. the Chevette and early S10/S15 trucks manufactured prior to 1985).\nIn 1981 Isuzu began selling consumer and commercial vehicles under their own brand in the United States. The Isuzu P'Up was the first model sold to consumers as an Isuzu, rather than as a Chevrolet or Buick. Isuzu's then president Toshio Okamoto then initiated a collaboration with small-car expert Suzuki to develop a global small car for GM, the S-car. A three-way agreement of co-ownership was signed in August 1981, with Isuzu and Suzuki exchanging shares and General Motors taking a 5% share of Suzuki. Following on from this, in 1985 Isuzu and GM established the IBC Vehicles venture in the United Kingdom, producing locally built versions of Isuzu and Suzuki light vans (the Isuzu Fargo and Suzuki Carry); to be sold in the European market under Vauxhall's Bedford brand. During this period Isuzu also developed a worldwide presence as an exporter of diesel engines, with their powerplants in use by Opel/Vauxhall, Land Rover, Hindustan, and many others. Two Isuzu model lines (Gemini, Impulse) were marketed as part of the Geo division (Spectrum, Storm) when it was initially launched as a Chevrolet subsidiary. In the domestic Japanese market, OEM deals with other manufacturers were entered to aid the poorly performing passenger car arm. It led to the badging of Suzukis, beginning in 1986, and Subaru small commercial vehicles as Isuzus (Geminett, Geminett II). This OEM tie-up occurred alongside the establishment of SIA (Subaru-Isuzu Automotive), an American joint venture with Fuji Heavy Industries (the parent company of Subaru). Shortly afterwards, the Lafayette, Indiana plant became operational.\n\nIsuzu ended US sales of the Impulse (Geo Storm) in 1992, and the following year it stopped exporting the Stylus (the basis for the Geo Spectrum), the last Isuzu-built car sold in the US.\n\nIn 1993 Isuzu began a new vehicle exchange program with Honda, whereby Honda sold the Isuzu Rodeo and Isuzu Trooper as the Honda Passport and Acura SLX, respectively. In return Isuzu began selling the Honda Odyssey as the Isuzu Oasis. Thus, Honda's lineup gained two SUVs, and Isuzu's lineup gained a minivan. In the Japanese market, the Gemini (Stylus) was now a rebadged Honda Domani and the Aska (originally based on the GM J-car) was a Honda Accord.\n\nIsuzu's United States sales reached a peak in 1996 after the introduction of the Isuzu Hombre pickup, a badge-engineered GM truck (using the sheetmetal of the Brazil-market Chevrolet S10). Isuzu resurrected the beloved Amigo in 1998, before changing the name of the 2-door convertible to Rodeo Sport in 2001 in an attempt to associate it with the better selling 4-door Rodeo. The Rodeo Sport was discontinued in 2003, while production of the Rodeo and Axiom ceased a year later. By this point sales in North America had slowed to just 27,188, with the discontinued Rodeo and Axiom making up 71% of that total. \n\nIn 1998 General Motors and Isuzu formed DMAX, a joint venture to produce diesel engines. GM raised its stake in Isuzu to 49% the following year, effectively gaining control of the company and quickly followed this up by appointing an American GM executive to head Isuzu's North American Operations. This marked the first time a non-Japanese executive had ever held such a high position at Isuzu. In 2001 G.M. and Isuzu announced plans to share distribution networks and for Chevrolet to market an Isuzu product.\n\nThe production version of the heralded VehiCROSS was introduced to the US in 1999, but met with mixed reviews, as its high pricetag, unique styling and two-door configuration did not seem to meet with market demands. Production of the VehiCROSS and other sport utility vehicles, including the Trooper, ended in 2001 as part of a major financial reorganization which eliminated almost 10,000 jobs. GM had been pushing the company to focus exclusively on producing commercial vehicles and engines. \n\n\nIn most of Asia and Africa, Isuzu is mostly known for trucks of all sizes, after Isuzu small automobile sales drastically plummeted and Isuzu had to drop all sales of sedans and compact cars in the late 1990s. In the days when Isuzu sold passenger cars, they were known for focusing on the diesel-engined niche. In 1983, for instance, long before the explosion in diesel sales, diesels represented 63.4% of their passenger car production. In 2009, Isuzu abandoned the United States consumer market due to lack of sales. Isuzu as a corporation has always been primarily a manufacturer of small to medium compact automobiles and commercial trucks of sizes medium duty and larger, but markets around the world show different needs.\n\nIsuzu Motors America discontinued the sale of passenger vehicles in the United States effective January 31, 2009. The company explained to its dealers that it had not been able to secure replacements for the Isuzu Ascender and Isuzu i-Series that would be commercially viable. Isuzu sold 7,098 cars in the year 2007. This action did not affect Isuzu's commercial vehicle or industrial diesel engine operations in the United States. Isuzu has a contract with Budget Truck Rental to manufacture their rental trucks, shared with Ford, GMC, and Navistar International.\n\nIn Australia, Isuzu was for many years a major supplier of light commercial and domestic vehicles to Holden (General Motors). However, by 2008, Holden was sourcing few Isuzus. At this time Isuzu began to sell the D-Max under the Isuzu name.\n\nIsuzu's entry in the Thai market proved to be one of its most successful. Its presence in the country began in 1966 when it established a manufacturing facility for pick-up trucks in the Samuthprakarn province with a capacity of 155,000 units per year. The automaker quickly became a market leader so that by 2002, the company transferred its production base from its original location in Fujisuwa, Japan to Thailand. Isuzu claimed the largest share of the Thai commercial vehicle market, outperforming its competitors for at least 23 years. By 2006, the company transferred to an industrial zone in Chacheongsao province to support further production expansion. By 2017, Isuzu has been exporting pick-up trucks, with shipments reaching North America, Latin America, Australia, and Japan. It the same year, it announced that its profit climbed 7 percent and has doubled its annual truck production to meet overseas demands.\n\nList of Isuzu Japanese facilities\n\nThe Fujisawa Plant was built and opened for production November 1961. It is located at Tsuchidana, Fujisawa, Kanagawa, and is still producing commercial vehicles for domestic Japanese use and international exports. The Toghichi Plant, located at Hakuchu, Ohira-Machi, Tochigi, Tochigi, is where the engines are currently built.\n\nMimamori-kun, which means to watch, monitor, or observe in Japanese, (literally \"Mr. Watcher\") is a commercial vehicle telematics service developed by Isuzu Motors for monitoring and tracking commercial vehicle operations and movements in Japan. The service uses GPS satellite tracking services, and began February 2004. It is connected to the internet and provides government mandated driver activity logs, and records how long the driver was on-duty and how much time was spent driving.\n\nThe service also records when the driver took lunch breaks, where the truck stopped and for how long, and when the driver logged off for his duty shift. The service has been modified for personal use in Japan to keep track of family members, to include elderly members of health status and location of children for safety purposes.\n\nSome of the main features include Internet Digital Tachograph, the first of its kind wirelessly in Japan, combined with hands-free communication, voice guidance, and text messages displayed from the dispatch office. The system also has a password enabled vehicle theft prevention feature that will not let the vehicle start without the driver having entered a password.\n\n\n\n\nDiesel engines are a major part of the Isuzu Motor's business with over 20 million engines worldwide. The diesel power division, known as the PowerTrain Division, of Isuzu Motors America, is located in Plymouth, Michigan.\n\nSouthwest Products - Covering California, Nevada and Arizona.\n\nUnited Engines\n\nMack Boring Parts\n\nM & L Engine\n\nAg Equipment\nGenerator Sets\n\nConstruction Equipment\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1569059", "url": "https://en.wikipedia.org/wiki?curid=1569059", "title": "Jane Morris", "text": "Jane Morris\n\nJane Morris (née Jane Burden; 19 October 1839 – 26 January 1914) was an embroiderer and English artists' model who embodied the Pre-Raphaelite ideal of beauty. She was a model and muse to William Morris (1834–1896), the English textile designer, poet, novelist, translator, and socialist activist, whom she later married, and to Dante Gabriel Rossetti.\n\nJane Burden was born in Oxford, the daughter of a stableman, Robert Burden, and his wife Ann Maizey, who was a laundress. At the time of her birth, her parents were living at St Helen's Passage, in the parish of St Peter-in-the-East, off Holywell Street in Oxford which has since been marked with a blue plaque. Her mother Ann was illiterate and probably came to Oxford as a domestic servant. Little is known of Jane Burden's childhood, but it was poor and deprived.\n\nIn October 1857, Burden and her sister Elizabeth, known as \"Bessie,\" attended a performance of the Drury Lane Theatre Company in Oxford. Jane Burden was noticed by Dante Gabriel Rossetti and Edward Burne-Jones who were members of a group of artists painting the Oxford Union murals, based on Arthurian tales. Struck by her beauty, they asked her to model for them. Burden sat mostly for Rossetti as a model for Queen Guinevere and afterwards for William Morris, who was working on an easel painting, \"La Belle Iseult\", now in the Tate Gallery. During this period, Morris fell in love with Burden and they became engaged, though by her own admission she was not in love with Morris.\n\nBurden's education was limited and she was probably destined to go into domestic service like her mother. After her engagement, she was privately educated to become a gentleman's wife. Her keen intelligence allowed her to recreate herself. She was a voracious reader who became proficient in French and Italian, and she became an accomplished pianist with a strong background in classical music. Her manners and speech became refined to an extent that contemporaries referred to her as \"Queenly.\" Later in life, she had no trouble moving in upper-class circles. She was possibly the model for the heroine of the 1884 novel \"Miss Brown\" by Vernon Lee upon which George Bernard Shaw based the character of Eliza Doolittle in his play \"Pygmalion\" (1914) and the later film \"My Fair Lady\" (1964). She also became a skilled needlewoman who would be later renowned for her embroideries.\n\nJane married William Morris at St Michael at the Northgate in Oxford on 26 April 1859. Her father was described as a groom, in stables at 65 Holywell Street. After the marriage, the Morrises lived at Red House in Bexleyheath, Kent. While living there, they had two daughters, Jane Alice \"Jenny,\" born in January 1861, and Mary \"May\" (March 1862–1938), who later edited her father's works. They moved to 26 Queen Square in London, which they shared with the design firm of Morris, Marshall, Faulkner & Co., and later bought Kelmscott House in Hammersmith as their main residence.\n\nIn 1871, Morris and Rossetti took out a joint tenancy on Kelmscott Manor on the Gloucestershire–Oxfordshire–Wiltshire borders. William Morris went to Iceland, leaving his wife and Rossetti to furnish the house and spend the summer there. Jane Morris had become closely attached to Rossetti and became a favourite muse of his. Their relationship is reputed to have started in 1865 and lasted, on differing levels, until his death in 1882. They shared a deep emotional relationship, and she inspired Rossetti to write poetry and create some of his best paintings. Her discovery of his dependence on the drug, chloral hydrate, which was taken for insomnia, eventually led her to distance herself from him, although they stayed in touch until he died in 1882.\n\nIn 1884, Jane Morris met the poet and political activist Wilfrid Scawen Blunt at a house party given by her close friend, Rosalind Howard (later Countess of Carlisle). There appears to have been an immediate attraction between them. By 1887 at the latest, they had become lovers. Their sexual relationship continued until 1894 and they remained close friends until his death.\n\nJane Morris was an ardent supporter of Irish Home Rule.\n\nA few months before her death, she bought Kelmscott Manor to secure it for her daughters' future. However, she did not return to the house after having purchased it. Jane Morris died on 26 January 1914, while staying at 5 Brock Street in Bath. She is buried in the churchyard of St. George's Church in Kelmscott.\n\nPaintings of Jane Morris by Dante Gabriel Rossetti:\n\nPhotographs of Jane Burden by Rossetti are available at .\n\nBy William Morris:\n\nBy Edward Burne-Jones:\n\nBy Evelyn De Morgan:\n\n"}
{"id": "42776985", "url": "https://en.wikipedia.org/wiki?curid=42776985", "title": "Josephine Webb", "text": "Josephine Webb\n\nJosephine Webb (born June 21, 1918) is an American electrical engineer who obtained two patents for oil circuit breaker contact design, known colloquially as \"switchgear\". She designed an eighteen-inch, full newspaper size fax machine with superior resolution. She co-founded Webb Consulting Company with her husband, also an electrical engineer. She is one of the first female electrical engineers, and considered a pioneer by the Society of Women Engineers. At Purdue University, she was one out of a total of five women engineers. She turned 100 in June 2018.\n\nWebb was born Josephine Rohas in Niagara Falls, New York. She grew up in a one-parent household in Buffalo; her father served in World War I and never returned home. Her brother, Roderick, was two-and-a-half years older. Webb considered him a great influence in her early life, and by causality, her later life. When Roderick became interested in radio, they both became ham radio operators. He helped her get her license. She joined the ham radio club at Kenmore High School. At thirteen years old, Josephine was the youngest YL operator.\n\nAs a child, Webb loved aviation and often visited the local airport. She was good at math and joined her high school's science club. She had always been fascinated by technical subjects. She graduated Kenmore High School in 1934. She worked for two years before attending Purdue University.\n\nWebb's brother urged her to apply to an out-of-state scholarship. She majored in electrical engineering and graduated from Purdue University in 1940. She became a Buhl Research Fellow in the Electrical Engineering Department of the Carnegie Institute of Technology for two years. She was a member of Sigma Xi.\n\nWebb has two daughters, one was born in 1948 and the other 1952. The family had a laboratory, which was also set up as a measurements laboratory, as an addition to their home.\n\nIn 1942, she joined Westinghouse Electric Corporation as a Design Engineer. where among other duties, she worked on the electrical grids for the Coulee, Hoover, and Boulder Dams. It was during her tenure with the company that she obtained two patents for oil circuit breaker contact design.\n\nIn 1946, Webb became Director of Development for the Facsimile Development Laboratory at the Alden Products Company where she designed an eighteen-inch, full newspaper size fax machine with exceptional resolution for that time. Later, Webb co-founded the Webb Consulting Company with her husband, Herbert Webb. They specialized in electrical-electronic measurement instrumentation, communications applications, and photographic test devices. They worked for clients as diverse as Boeing and the U.S. Bureau of Mines.\n\nIn addition to the consulting business, Webb also took a position in 1977 with North Idaho College where she began development of a Computer Center and worked on several government grants for enhancing the campus and its educational programs.\n\nWebb holds four patents for her innovative work and has been active in many professional organizations including IEEE, NSPE, and SWE where she holds Fellows status.\n\n"}
{"id": "10435998", "url": "https://en.wikipedia.org/wiki?curid=10435998", "title": "Katia Sycara", "text": "Katia Sycara\n\nKatia Sycara () is a professor in the Robotics Institute, School of Computer Science at Carnegie Mellon University internationally known for her research in artificial intelligence, particularly in the fields of negotiation, autonomous agents and multi-agent systems. She directs the Advanced Agent-Robotics Technology Lab at Robotics Institute, Carnegie Mellon University. She also serves as academic advisor for PhD students at both Robotics Institute and Tepper School of Business.\n\nBorn in Greece, she went to the United States to pursue advanced education through various scholarships, including a Fulbright (1965-1969). She received a B.S. in Applied Mathematics from Brown University, M.S. in Electrical Engineering from the University of Wisconsin–Milwaukee, and Ph.D. in Computer Science from Georgia Institute of Technology. She was awarded an Honorary Doctorate from the University of the Aegean in 2004.\n\nSycara is a pioneer in the field of semantic web, case-based reasoning, autonomous agents and multi-agent systems.\n\nShe has authored or co-authored more than 700 technical papers dealing with multi-agent systems, software agents, web services, semantic web, human–computer interaction, human-robot interaction, negotiation, case-based reasoning and the application of these techniques to crisis action planning, scheduling, manufacturing, healthcare management, financial planning and e-commerce. She has led multimillion-dollar research effort funded by DARPA, NASA, AFOSR, ONR, AFRL, NSF and industry.\n\nThrough an ONR MURI program and though the COABS DARPA program, Prof. Sycara's group has developed the RETSINA multiagent infrastructure, a toolkit that enables the development of heterogeneous software agents that can dynamically coordinate in open information environments (e.g. the Internet). RETSINA has been used in multiple applications including supporting human joint mission teams for crisis response; creating autonomous agents for situation awareness and information fusion; financial portfolio management, negotiations and coalition formation for e-commerce, and coordinating robots for Urban Search and Rescue.\n\nSycara is one of the contributors to the development of OWL-S, the Darpa-sponsored language for Semantic Web services, as well as matchmaking and brokering software for agent discovery, service integration and semantic interoperation.\n\nSycara is a Fellow of Institute of Electrical and Electronic Engineers (IEEE), and a Fellow of American Association for Artificial Intelligence (AAAI).\n\nSycara is the recipient of the 2002 ACM/SIGART Agents Research Award. She is also the recipient of the 2015 Group Decision and Negotiation (GDN) Award of the Institute for Operations Research and the Management Sciences (INFORMS) GDN Section for her outstanding contributions to the field of group decision and negotiation. According to the citation of the award:\nSycara's robot teams have won multiple international awards. In the 2005 Robocup Urban Search and Rescue (US Open) held in Atlanta, her team won the First-in-Class Award for Autonomy, and the First-in-Class Award for Mobility. Two years later, again in Atlanta, she led another team that became a world champions in the 2007 International Robocup Search and Rescue Simulation League Competition. In 2008, her robotic team placed third in the Worldwide Robocup Championship Competition in the Urban Search and Rescue Virtual robots League held in Beijing, China.\n\nIn 2005, she received the Outstanding Alumnus Award from the University of Wisconsin–Milwaukee.\n\nSycara is the founding Editor-in-Chief of the journal Autonomous Agents and Multi-Agent Systems; Editor-in-Chief, of the Springer Series on Agents; and Area Editor of AI and Management Science, the journal \"Group Decision and Negotiation.\" She is a member of the Editorial Board, the Kluwer book series on \"Multiagent Systems, Artificial Societies and Simulated Organizations\"; member of the editorial board, the journals \"Agent Oriented Software Engineering\", \"Web Intelligence and Agent Technologies\", \"Journal of Infonomics\", \"Fundamenda Informaticae\", and \"Concurrent Engineering: Research and Applications\"; and member of the editorial board of the \"ETAI journal on the Semantic Web\" (1998–2001). She was on the Editorial Board of \"IEEE Intelligent Systems and their Applications\" (1992–1996), and \"AI in Engineering\" (1990–1996).\n\nShe is a member of the Scientific Advisory Board of France Telecom, 2003-2009; member of the Scientific Advisory Board of the Institute of Informatics and Telecommunications of the Greek National Research Center Demokritos, 2004-2012; member of the AAAI Executive Council (1996–99); member of the OASIS Technical committee on the development of UDDI (Universal Description and Discovery for Interoperability) software which is an industry standard; and an invited expert for W3C (the World Wide Web Consortium) Working Group on Web Services Architecture. She was a founding member of the Board of Directors of the International Foundation of Multiagent Systems (IFMAS), and founding member of the Semantic Web Science Association.\n\nSycara served as the program chair of the Second International Semantic Web Conference (ISWC 2003); general chair, of the Second International Conference on Autonomous Agents (Agents 98); chair of the Steering Committee of the Agents Conference (1999–2001); scholarship chair of AAAI (1993–1999);\nand the US co-chair for the US-Europe Semantic Web Services Initiative.\n\n"}
{"id": "1887912", "url": "https://en.wikipedia.org/wiki?curid=1887912", "title": "Letter-quality printer", "text": "Letter-quality printer\n\nA letter-quality printer was a form of computer impact printer that was able to print with the quality typically expected from a business typewriter such as an IBM Selectric.\n\nA letter-quality printer operates in much the same fashion as a typewriter. A metal or plastic printwheel embossed with letters, numbers, or symbols strikes an inked ribbon, depositing the ink (or carbon, if an expensive single-strike ribbon was installed) on the page and thus printing a character. \n\nOver time, several different technologies were developed including automating ordinary typebar typewriter mechanisms (such as the Friden Flexowriter), daisy wheel printers (dating from a 1939 patent, but brought to life in the 1970s by Diablo engineer David S. Lee) where the type is moulded around the edge of a wheel, and \"golf ball\" (the popular informal name for \"typeball\", as used in the IBM Selectric typewriter) printers where the type is distributed over the face of a globe-shaped printhead (including automating IBM Selectric mechanisms such as the IBM 2741 terminal). The daisy wheel and Selectric-based printers offered the advantage that the typeface was readily changeable by the user to accommodate varying needs. \n\nThese printers were referred to as \"letter-quality printers\" during their heyday, and could produce text which was as clear and crisp as a typewriter (though they were nowhere near the quality of printing presses). Most were available either as complete computer terminals with keyboards (or with a keyboard add-on option) that could double as a typewriter in stand-alone (\"off-line\") mode, or as print-only devices. Because of its low cost at the time, the daisy wheel printer became the most successful, the method used by Diablo, Qume, Brother and Apple.\n\nLetter-quality impact printers, however, were slow, noisy, incapable of printing graphics or images (unless the programmable microspacing and over-use of the dot were employed), sometimes limited to monochrome, and limited to a fixed set (usually one) of typefaces without operator intervention, though certain font effects like underlining and boldface could be achieved by overstriking. Soon, dot-matrix printers (such as the Oki Microline 84) would offer \"Near Letter Quality\" (NLQ) modes which were much faster than daisy-wheel printers, could produce graphics well, but were still very noticeably lower than \"letter quality\". Nowadays, printers using non-impact printing (for example laser printers, inkjet printers, and other similar means) have replaced traditional letter-quality printers in most applications. The quality of inkjet printers can approach the old letter-quality impact printers (but can be limited by factors such as paper type).\n\nDedicated word processors and WP software for general-purpose computers that rose in popularity in the late 1970s and 1980s would use features such as microspacing (usually by 1/120 of an inch horizontally and, possibly, 1/48 of an inch vertically) to implement subscripts, proportional spacing, underlining, and so on. The more rudimentary software packages would implement bold text by overtyping the character in exactly the same spot (for example, using the backspace control code), but better software would print the letter in 3 slightly different positions. Software did exist to (slowly) produce pie charts on such printers (and on some daisywheels the dot was reinforced with metal to cope with extra wear).\n\n"}
{"id": "55175339", "url": "https://en.wikipedia.org/wiki?curid=55175339", "title": "Lifewire", "text": "Lifewire\n\nLifewire is a technology information and advice website. It was a top 10 technology-information site in 2017, reaching 6 million monthly US unique users each month. The website's owner is Dotdash, originally About.com, which launched Lifewire in 2016 as one of its spin-off vertical sites.\n\nLifewire was the third standalone brand of About.com, an IAC-owned media company, which broke up its collections of DIY and how-to information into branded vertical websites. Lifewire was preceded by Verywell, a health info website, and The Balance, a personal finance site. Since About.com was one of the top 50 biggest sites online, Lifewire became a top 15 technology website in the United States as soon as it was launched in October 2016.\n\nTraffic for Lifewire rose between 65% and 70% from its launch till February 2017.\n\nThe purpose of Lifewire is to offer advice and answers on common technology questions and problems in a clear and simplified format.\n\nWhen it was launched, Lifewire featured 16,000 articles written by 40 experts, teaching readers what is new in the world of technology and explaining how to better use devices they already own. Lifewire CEO described the website style \"as if your BFF happened to be an iPhone expert.\"\n"}
{"id": "53787782", "url": "https://en.wikipedia.org/wiki?curid=53787782", "title": "List of broadcast video formats", "text": "List of broadcast video formats\n\nThis list of broadcast formats is a review of the most popular formats used to broadcast video information over Cable Television, Satellite, the Internet, and other means. Video broadcasting was popularized by the advent of the television during the middle of the twentieth century.\n\nRecently, Internet streaming has almost surpassed television as the top video broadcast platform.\nBelow is a list of broadcast video formats.\n\n\n\n\n"}
{"id": "34330667", "url": "https://en.wikipedia.org/wiki?curid=34330667", "title": "Ministry of Popular Culture", "text": "Ministry of Popular Culture\n\nThe Ministry of Popular Culture (, commonly abbreviated to MinCulPop) was a ministry of the Italian government from 1937 to 1944. It was established by the Fascist government in 1922 as the \"Press Office of the Presidency of the Council\", before being renamed to \"Press Office of the Head of Government\" in 1925. In 1934 it became the \"Secretariat for Press and Propaganda\". It became a ministry in 1935 and was given its definitive designation in 1937. During its existence, it controlled most of the literary and radio channels in Italy. It was the Italian analogue of the German Reich Ministry of Public Enlightenment and Propaganda.\n\nThe Ministry famously outlawed the importation and translation of all American comic books, with the lone exception of Mickey Mouse, in 1938.\n\nThe Ministry was officially suppressed by the Kingdom of Italy on July 3, 1944, having remained vacant ever since the overthrow of Benito Mussolini in the 25 Luglio coup a year earlier. During the Italian Social Republic, Mussolini revived the Ministry of Popular Culture and appointed Ferdinando Mezzasoma as its head.\n\n"}
{"id": "629881", "url": "https://en.wikipedia.org/wiki?curid=629881", "title": "Mouse keys", "text": "Mouse keys\n\nMouse keys is a feature of some graphical user interfaces that uses the keyboard (especially numeric keypad)\nas a pointing device (usually replacing a mouse). Its roots lie in the earliest days of visual editors when line and column navigation was controlled with arrow keys\nToday, mouse keys usually refers to the numeric keypad layout standardized with the introduction of the X Window System in 1984.\n\nHistorically, MouseKeys supported GUI programs when many terminals had no dedicated pointing device. As pointing devices became ubiquitous, the use of mouse keys narrowed to situations where a pointing device was missing, unusable, or inconvenient. Such situations may arise from the following:\n\nThe X Window System MouseKeysAccel control applies action (usually cursor movement) repeatedly while a direction key\n\nThe first \"mk_time_to_max\" actions increase smoothly according to an exponential.\n\nformula_1\n\nThese five parameters are configurable.\n\nUnder the X Window Systems Xorg and XFree86 used on Unix-like systems such as Linux, BSD, and AIX, MouseKeys (and MouseKeysAccel) is nominally (de)activated by ++. MouseKeys without acceleration (also known as plot mode) is sometimes available with Shift+NumLock. This is independent of the Window Manager in use and may be overridden by a configuration file. The setxkbmap utility can be used to temporary enable mouse keys under Xorg:\n\ncodice_1\n\nThere are also various utilities to allow more precise control via user-configurable key bindings, such as xmousekeys and xdotool.\n\nMouseKeys for Apple Inc's Mac OS X is enabled and configured via the Accessibility ([apple] → System Preferences → Accessibility → Mouse & Trackpad).\n\nMicrosoft changed the method of enabling between Windows 2000, Windows XP (added diagonal cursor movement and MouseKeysAccel), and Windows Vista.\n\nReplacing the mouse keys by the numeric keypad is as follows:\nTyping (with the numeric keypad) is equivalent to clicking the selected button. By default, the selected button is the primary button (nominally under index finger, left button for most right-handed people and right button for most left-handed people). Typing (with the numeric keypad) selects the alternate button (nominally under ring finger, right button for most right-handed people and left button for most left-handed people). Typing (with the numeric keypad) selects the modifier button (nominally under the middle finger, middle button of a 3-button mouse). Typing (with the numeric keypad) selects the primary button. The selection remains in effect until a different button is selected.\n\nAssignment of left/middle/right button to primary/modifier/alternate, alternate/modifier/primary, or something else is settable by many means. Some mice have a switch, that swaps assignment of right and left keys. Many laptop bioses have a setting for mouse button assignment. Many window managers have a setting that permutes the assignment. Within the X Window System core protocol, permutation can be applied by xmodmap(1).\n\nOther than , all other numeric keys from the numeric keypad are used to move the pointer on the screen. For example, will move the pointer upwards, while will move it diagonally downwards to the left.\n\n"}
{"id": "2210469", "url": "https://en.wikipedia.org/wiki?curid=2210469", "title": "Net2Phone", "text": "Net2Phone\n\nNet2phone provides VoIP and cloud computing-based telephony products and services. The company is a subsidiary of IDT Corporation.\n\nThe company was founded in 1990.\n\nOn July 30, 1999, during the dot-com bubble, the company became a public company via an initial public offering, raising $81 million. Shares rose 77% on the first day of trading to $26 per share. After completion of the IPO, IDT owned 57% of the company. Within a few weeks, the shares increased another 100% in value, to $53 per share.\n\nIn March 2000, in a transaction facilitated by IDT CEO Howard Jonas, a consortium of telecommunications companies led by AT&T announced a $1.4 billion investment for a 32% stake in the company, buying shares for $75 each. The transaction was completed in August 2000. AOL had expressed an interest in buying all or part of the company but was not agreeable to the price.\n\nIn 2000, the company acquired Aplio.\n\nIn August 2000, Jonathan Fram, president of the company, left to join eVoice.\n\nIn September 2000, the company formed Adir Technologies, a joint venture with Cisco Systems. In March 2002, the company sued Cisco for breach of contract.\n\nIn 2001, the company acquired iPing.\n\nIn February 2002, the company announced 110 layoffs, or 28% of its workforce.\n\nIn October 2004, Liore Alroy became chief executive officer of the company.\n\nOn March 13, 2006, IDT acquired the shares of the company that it did not already own for $2.05 per share.\n\nIn January 2017, the company acquired LiveNinja.\n"}
{"id": "3275060", "url": "https://en.wikipedia.org/wiki?curid=3275060", "title": "PLECS", "text": "PLECS\n\nPLECS (Piecewise Linear Electrical Circuit Simulation) is a software tool for system-level simulations of electrical circuits developed by Plexim.\nIt is especially designed for power electronics but can be used for any electrical network.\n\nThe program Simulink is ideally suited for the simulation of controls. Therefore, Simulink is also a convenient tool for the design of closed loop controlled electrical systems. PLECS enhances Simulink with the capability to simulate electrical circuits directly. The user can simply enter a circuit as a schematic of electrical components. At Simulink block level the circuit is represented as a subsystem, so the user can build controls and other non-electrical elements around it and take full advantage of the Simulink environment and its toolboxes.\n\nThe concept of integration into Simulink has the advantage that only the part of the system in which electrical units are of interest needs to be modeled as an electrical circuit. The simulation of all non-electrical parts such as controls and mechanics should be done in Simulink.\n\nMATLAB can be employed to compute circuit parameters and to post process and visualize the simulation results.\n\nThere is also a standalone version of PLECS that allows simulation of electrical circuits and control systems directly within the PLECS package. The standalone version uses GNU Octave as its numerical engine in place of MATLAB.\n\nMost circuit simulation programs model switches as highly nonlinear elements. Due to steep voltage and current transients, the simulation becomes slow when switches are toggled. In the most simple case a switch is modeled as a variable resistance that changes between a very small and a very large value. In other cases, it is represented by a sophisticated semiconductor model.\n\nWhen simulating complex power electronic systems, however, the processes during switching are of little interest. Here, it is more appropriate to use ideal switches that toggle instantaneously between a closed and an open circuit. This approach, which is implemented in PLECS, has two major advantages: Firstly, it yields systems that are piecewise-linear across switching instants, (thus resolving the otherwise difficult problem of simulating the non-linear discontinuity that occurs in the equivalent-circuit at the switching instant). Secondly, to handle discontinuities at the switching instants, only two integration steps are required (one for before the instant, and one after). Both of these advantages speed up the simulation considerably.\n\n"}
{"id": "35151800", "url": "https://en.wikipedia.org/wiki?curid=35151800", "title": "PROJ.4", "text": "PROJ.4\n\nPROJ.4 (or proj) is a library for performing conversions between cartographic projections. The library is based on the work of Gerald Evenden at the USGS, but is now an OSGeo project maintained by Howard Butler. The library also ships with executables for performing these transformations from the command line.\n\nThe first release of proj was developed by Gerald Evenden in the early 1980s as a Ratfor program. It was based on the General Cartographic Transformation Package or GCTP, which consisted of Fortran subroutines that could be used to project geographic data. The second release of proj from 1985 was rewritten in C to run on UNIX systems. The third release of proj from 1990, named PROJ.3, was expanded to support approximately 70 cartographic projections. Evenden further developed a fourth release in 1994, named PROJ.4, which is the current name of the software. The last version maintained by Evenden was 4.3, released on September 24, 1995.\n\nAfter over four years of inactivity, Frank Warmerdam became the new maintainer and released version 4.4 on March 21, 2000. As of May 2008, PROJ.4 become part of the MetaCRS project, a confederation of coordinate systems related projects under incubation with OSGeo.\n\nIn February 2018, the PROJ.4 product became PROJ, as it reaches the major version 5. As Kristian Evers says on proj@lists.maptools.org :\n\nThis is a List of Versions from 4.3.3 onward extracted from pj_release.c.\n\n\n"}
{"id": "275174", "url": "https://en.wikipedia.org/wiki?curid=275174", "title": "Polyglot (computing)", "text": "Polyglot (computing)\n\nIn computing, a polyglot is a computer program or script written in a valid form of multiple programming languages, which performs the same operations or output independent of the programming language used to compile or interpret it.\n\nGenerally polyglots are written in a combination of C (which allows redefinition of tokens with a preprocessor) and a scripting language such as Lisp, Perl or sh.\n\nPolyglot markup is similar, but about markup language context.\n\nPolyglot persistence is similar, but about databases.\n\nThe two most commonly used techniques for constructing a polyglot program are to make liberal use of languages that use different characters for comments and to redefine various tokens as others in different languages. Often good use is made of syntax quirks. These are demonstrated in this public domain polyglot written in ANSI C, PHP and bash:\n\nNote the following:\n\nSome less-common languages also offer possibilities to create Polyglot code. Here is a small sample, written simultaneously in SNOBOL4, Win32Forth, PureBasicv4.x, and REBOL:\n\nThe term is sometimes applied to programs that are valid in more than one language, but do not strictly perform the same function in each. One use for this form is a file that runs as a DOS batch file, then re-runs itself in Perl:\n\nThis allows creating Perl scripts that can be run on DOS systems with minimal effort.\n\n"}
{"id": "40429206", "url": "https://en.wikipedia.org/wiki?curid=40429206", "title": "Pore (bread)", "text": "Pore (bread)\n\nPores are the air pockets found in leavened bread, where carbon dioxide from the fermentation process creates a network of primarily interconnected void structures. The degree to which pores form are a major determiner in the texture (\"crumb\") of the bread. Pore size varies between varieties of bread. Sourdough bread is a variety with larger pores. Rye bread has smaller pores and a denser crumb.\n\n"}
{"id": "13906614", "url": "https://en.wikipedia.org/wiki?curid=13906614", "title": "Real Time Information Group", "text": "Real Time Information Group\n\nThe Real Time Information Group (also RTI) is an organisation in the United Kingdom supporting the development of bus passenger information systems; its 85 members include local authorities, bus operators and system suppliers together with representatives from the UK government and other key industry groups such as ITS(UK), Travel Information Highway, and the UTMC Development Group (UDG).\n\nThe main output of the group is guidelines, standards, case studies and best practice documents. These documents are produced by RTIG on behalf of its members, usually with the assistance of specialist working groups.\n\nIn 2000, when real-time information (RTI) systems were beginning to be considered by UK local authorities to provide travellers with up-to-the-minute bus arrival and departure passenger information, it was realised that cross-boundary bus services made it imperative to coordinate projects around the UK. Technical and operational standards would therefore be required. A group of local authorities and bus operators began to meet regularly to discuss how to achieve this; and so RTIG was born.\n\nSubstantial government funding for projects around the UK, in particular from 2002 to 2004, provided an enormous boost to the development of RTI systems. The expanding and maturing market caused RTIG to reflect on its role, and in 2003 it determined to recreate itself as a subscription group - with the important step that the systems industry was to be a full and equal partner in its work. Equally importantly, it has maintained excellent links with central UK Government, from whom the Group continues to receive project funding for work of national scope and importance.\n\nThe National RTI Strategy, ratified in March 2007, establishes a framework for how industry stakeholders and government need to work together to deliver benefit to passengers. RTIG's role has, as a consequence, been expanded to cover all aspects of technology in public transport, from systems to support disabled travellers through to safety and security systems.\n\nIn 2002 the group produced the first UK Annual RTI Survey, which surveyed the use of RTI technology by local authorities and passenger transport executives across England, as well as plans for the following two years. In 2004 the survey was extended to include Wales and in 2005 Scotland.\n\nIn order to reflect the widening deployment of bus-related technologies, the 2006 annual survey was re-branded as the ‘RTIG Passenger Transport Technology Survey’ and included questions on services for disabled travellers – partly in response to new obligations on bus operators under the Disability Discrimination Act 2005. The 2007 survey continues to focus broadly on public transport and traffic management technology and has been expanded to include questions on bus CCTV and other security technologies. The 2007 report is expected to be publicly available in the first quarter of 2008.\n\nThe annual survey provides details on:\n\n\nThe majority of the documents, standards and guidelines produced by RTIG is done with the assistance of voluntary working groups. These working groups are made up of industry experts who lend their knowledge to particular projects. Examples of RTIG working groups include:\n\n\nWorking groups involve non-members where relevant; so, the Disability WG includes representatives from charities such a RNIB and Guide Dogs.\n\nThe RTIG library houses all of the documents produced by the group to date. These documents are held by RTIG electronically and distributed to members on request, or via the ‘members area’ of the RTIG website. The library catalogue is publicly available from the RTIG website .\n\nPublicly available documents in the Library include:\n\nMembers have access to a wider range of standards and guidelines, and to the outputs of RTIG workshops (see below).\n\nThe group also publishes a quarterly newsletter which provides both members and non-members with news on RTIG projects and events.\n\nRTIG runs quarterly workshops, which are held at a different UK venue each time. Each workshop has a central theme and attract presentations from a wide variety of stakeholders. Workshops will include an update on ITS news, an update on RTIG projects and working group activities, and a set of presentations based around the workshop theme.\n\nPrevious workshop venues (and their respective themes) include :\n\n\nRTIG have assisted in the development of a number of standards, including the Service Interface for Real Time Information (SIRI). RTIG have also developed a number of best practice guidelines.\n\n"}
{"id": "13798359", "url": "https://en.wikipedia.org/wiki?curid=13798359", "title": "Reformed methanol fuel cell", "text": "Reformed methanol fuel cell\n\nReformed Methanol Fuel Cell (RMFC) or Indirect Methanol Fuel Cell (IMFC) systems are a subcategory of proton-exchange fuel cells where, the fuel, methanol (CHOH), is reformed, before being fed into the fuel cell. RMFC systems offer advantages over direct methanol fuel cell (DMFC) systems including higher efficiency, smaller cell stacks, no water management, better operation at low temperatures, and storage at sub-zero temperatures because methanol is a liquid from -97.0 °C to 64.7 °C (-142.6 °F to 148.5 °F). The tradeoff is that RMFC systems operate at hotter temperatures and therefore need more advanced heat management and insulation. The waste products with these types of fuel cells are carbon dioxide and water.\n\nMethanol is used as a fuel because it is naturally hydrogen dense (a hydrogen carrier) and can be steam reformed into hydrogen at low temperatures compared to other hydrocarbon fuels. Additionally, methanol is naturally occurring, biodegradable, and energy dense.\n\nRMFC systems consist of a fuel processing system (FPS), a fuel cell, a fuel cartridge, and the BOP (the balance of plant).\n\nThe fuel cartridge stores the methanol fuel, which is often diluted with up to 40% (by volume) water.\n\nMethanol→Partial oxidation(POX)/Autothermal reforming (ATR)→Water gas shift reaction (WGS)→preferential oxidation (PROX)\nThe reformer converts methanol to H and CO, a reaction that occurs at temperatures of 250 °C to 300 °C.\n\n→The membrane electrode assembly (MEA) fuel cell stack produces electricity in a reaction that combines H (reformed from methanol in the fuel processor) and O and produces water (HO) as a byproduct.\n\n→Tail gas combustor (TGC) catalytic combustion afterburner or (catalytic combustion) with a platinum-alumina (Pt–Al2O3) catalyst→condenser\n\nThe balance of plant (BOP) consists of any fuel pumps, air compressors, and fans required to circulate the gas and liquid in the system. A control system is also often needed to operate and monitor the RMFC.\n\nRMFC systems have reached an advanced stage of development. For instance, a small system developed by Ultracell for the United States military, , has met environmental tolerance, safety, and performance goals set by the United States Army Communications-Electronics Research, Development and Engineering Center, and is commercially available.\n\nLarger systems 350W to 8 MW are also available for multiple applications, such as power plant generation, backup power generation and battery range extension. One application in the field is improving performance of a heavy duty smaller electric vehicle Video of solution\n\n\n"}
{"id": "371708", "url": "https://en.wikipedia.org/wiki?curid=371708", "title": "Research Triangle Park", "text": "Research Triangle Park\n\nThe Research Triangle Park (RTP) is the largest research park in the United States. It is named for the three hub cities of Raleigh, Durham, and Chapel Hill, or more properly for the three major research universities in those three cities (NC State University, Duke University, and the University of North Carolina at Chapel Hill respectively). The Research Triangle region of North Carolina received its name as an extension of the name of the park. Besides the three anchor cities, the park is also bounded by the communities of Morrisville and Cary. Approximately one fourth of the park's territory lies in Wake County, but the majority of its land is in Durham County.\n\nRTP is one of the most prominent high-tech research and development parks in the United States. It was created in 1959 by state and local governments, nearby universities, and local business interests. Karl Robbins bought the land where the park is now built.\nThe park covers situated in a pine forest with of built space. The park is traversed by Interstate 40; the Durham Freeway; and NC 540.\n\nThe park is home to over 200 companies employing 50,000 workers and 10,000 contractors, including the second largest IBM operation in the world, smaller only than the one in India; the company has around 14,000 employees in RTP.\n\nThe park hosts one of GlaxoSmithKline's largest R&D centers with approximately 5,000 employees. Cisco Systems' campus in RTP, with approximately 5,000 employees, is the second highest concentration of its employees outside of its Silicon Valley corporate headquarters. The National Institute of Health has its National Institute of Environmental Health Sciences located in RTP and Durham.\n\nResearch Triangle Park is owned and managed by the Research Triangle Foundation, a private non-profit organization. In August 2017, Scott Levitan was named the organization's new President and CEO, making him the 9th leader since the foundation was established.\n\nFollowing World War II, North Carolina's economy could no longer depend upon their traditional industries of agriculture, textiles, and furniture; their market share was in decline and jobs were leaving. Academics at N.C. State and Duke came up with the idea of creating the park so that the universities could do research together, leverage the area's strengths, and keep graduates in state.\n\nEstablished in 1951 and located in North Carolina, Research Triangle Park was created to increase innovation in the area. It is bordered by Duke University, North Carolina State University, and the University of North Carolina at Chapel Hill. At first, the park struggled to recruit innovators, but in 1965, Research Triangle Park saw its largest surge of growth, thanks to heavy recruiting by the state's government and Archibald (Archie) Davis. In their article \"The Growth of Research Triangle Park,\" Link and Scott posit that entrepreneurial culture and, especially, leadership contributed the most to its success as a cluster. Archie Davis promoted a culture of innovation and entrepreneurship by locating the park near universities, actively recruiting organizations (like the American Academy of Arts and Sciences), and used his vision to raise funding for the park.\n\nDavis strongly believed that profits could not be the only driver for creating the park - the betterment of the community should be the key goal. \"The love of this state … was the motivation for the Research Triangle idea,\" he said. \"Research Triangle is a manifestation of what North Carolina is all about.\" Research Triangle Park remains a nonprofit.\n\nThe park is an unincorporated area, and state law prohibits municipalities from annexing areas within the park. Some local government functions are served by the Durham-Wake Counties Research and Production Service District, a special tax district created in 1986 that is conterminous with the park, wherein the property tax rate is limited to 10 cents per $100 valuation. The park has special zoning as a Research Applications District in the Wake County portion, and a Scientific Research Park in the Durham County portion. As of October 2012, both zoning areas are in the process of being revised to allow higher density development. The zoning changes are coupled with legislative changes allowing for Urban Research Service Districts (URSD) within the Park, which can include a mix of retail and residential usages. These newly permitted URSDs could levy taxes at the same rate as a neighboring city.\n\nOn October 1, 2015, former President and CEO of the Research Triangle Foundation, Bob Geolas, announced RTP's plans for a $50 million redevelopment involving the formation of \"Park Center.\" $20 million will be allocated from Durham County, $10 million from the Durham-Wake Counties Research and Production Service District, and $20 million as a result of land purchases and site work provided by the Research Triangle Foundation of North Carolina.\n\nPark Center is to be over 300,000 square feet of public space at the heart of the Research Triangle Park. This public area will include retail outlets, food and beverage venues, and entertainment space. Geolas states that \"We want to make all of this as local as possible. I would like to have as few chains, no chains if we could. I'd love for it all to be local. Local coffee, local food, local produce, local products.\"\n\nThe redevelopment plans also include exploring partnerships with regional transit groups. The hope of the Research Triangle Foundation is to broaden public transportation to and from the area. According to Geolas, \"We are currently having discussions about bringing the Regional Transit Center over to Park Center so that we can connect with all of our transit links.\" A public commuter rail is also in talks.\n\nThe Research Triangle Park Foundation operates three buildings within RTP. These three buildings are The Frontier, The Lab, and The Archie K. Davis Conference Center. The Frontier is a collaboration space that was first opened in January 2015. Since its inception, over 20,000 visitors have come through The Frontier's doors. The Lab is a full-service lab and office space that houses multiple Research and Development companies. The Archie K. Davis Conference Center is a 6,800-square-foot event space. The AKD Conference Center is also home to the Research Triangle Park Foundation main offices.\n\n\n\n"}
{"id": "12811705", "url": "https://en.wikipedia.org/wiki?curid=12811705", "title": "Shukhov cracking process", "text": "Shukhov cracking process\n\nThe Shukhov cracking process is a thermal cracking process invented by Vladimir Shukhov and Sergei Gavrilov. Shukhov designed and built the first thermal cracking techniques important to the petrochemical industry. His patent (Shukhov cracking process – patent of Russian empire No. 12926 from November 27, 1891) on cracking was used to invalidate Standard Oil's patents (Burton process – Patent of United States No. 1,049,667 on January 7, 1913) on oil refineries. In 1937 the Shukhov cracking process was superseded by catalytic cracking. It is still in use today to produce diesel.\n\n\n"}
{"id": "342913", "url": "https://en.wikipedia.org/wiki?curid=342913", "title": "Soho Foundry", "text": "Soho Foundry\n\nSoho Foundry is a factory created in 1795 by Matthew Boulton and James Watt and their sons Matthew Robinson Boulton and James Watt Jr. at Smethwick, West Midlands, England (), for the manufacture of steam engines. Now owned by Avery Weigh-Tronix, it is used for the manufacture of weighing machines.\n\nThe early history of the Soho Foundry is of pivotal importance both to the history of the industrial revolution and to the study of the development of management theory. The Soho Foundry stood out from other factories of the day in the sophistication of its planning, its production processes and its management techniques; practising concepts that wouldn't become commonplace until a century later. Comparing its workings to the techniques of mass production and scientific management made famous by Henry Ford and Frederick Winslow Taylor in the United States in the early 20th century, the economist Eric Roll wrote \"Neither Taylor, Ford nor any other modern experts devised anything in the way of plan that cannot be discovered at Soho before 1805\".\n\nThe factory was built on the edge of the Birmingham Canal on land bought in 1795. The following year the foundry was open.\nThe Soho Foundry was planned with a degree of sophistication unprecedented for a factory of its time. Its products were produced out of standardised interchangeable parts, reducing the need to supervise work as it was executed, simplifying stock control and enabling more efficient repair of faults for customers. Production processes were broken down into small tasks, enabling an extremely high degree of specialisation among workers – one document from 1801, for example, describes how a team of four specific workers was \"to be constantly employed in fitting nozzles\". These tasks took place in a series of workshops spatially located along the flow of production, minimising the expense and time-wastage of the movement of materials through the works.\n\nThe accounting procedures of the foundry bore a striking similarity to modern processes, with each of the three main operating departments - the Foundry Department which made cast iron parts, the Smithy Department which made wrought iron parts, and the Fitting Department which machined the parts and assembled them together - being operated as separate profit centres.\n\nThe Soho Foundry was also innovative in the field of personnel management, setting up executive development programmes, sickness benefit schemes and welfare programmes.\nIn 1912, the manager of the company William Edward Hipkins, died at the sinking of Titanic while he was travelling as a first class passenger. He was 55 and his body was not recovered. \n\nBy 1840 James Watt Jr. owned the factory after the death of the founding Boulton and Watt. He died in 1848 and his place was taken by H. W. Blake and the name changed from \"Soho Foundry\" to \"James Watt & Co.\". \n\nIn 1857 the screw engines for the steamship \"SS Great Eastern\" were built at the foundry. In 1860 a new mint was started at the Foundry, the Manufactory having closed in April 1850 by Matthew's grandson, Matthew Piers Watt Boulton.\n\nIn 1861 tests were performed at the Soho Foundry for the London Pneumatic Despatch Company.\n\nIn 1895 W & T Avery Ltd. acquired the Foundry as a going concern.\n\nIt is now the home of Avery Weigh-Tronix and Avery Berkel, who make weighing scales. The site includes William Murdoch's cottage and overlooks Black Patch Park.\n\nThere was a small museum there, open only by appointment, but is now closed.\n\nThe grade II listed \"Pooley\" gates, of cast iron, are marked with \"a Liver bird above ropework draped with cloth, flanked by nautical symbols including oars, flags and bugles, ships' wheels and intersecting dolphins\". A plaque reads: \"These gates were cast by Henry Pooley and Son about 1840 for the Sailors' Home, Liverpool. The Avery and Pooley Foundries were amalgamated in 1931\". There was an active campaign to return these gates to Liverpool, resulting in the approval by Sandwell Council in March 2011 of an application to return them. After restoration the gates were returned to Liverpool on 8 August 2011 and were re-erected under the name \"The Sailors Home Gateway\" in the pedestrian section of Paradise Street in Liverpool One, close to the original site of the Sailors' Home.\n\nThe building is a Grade II* listed building. The gates and adjacent canal bridge are Grade II listed.\n\nThe oldest working steam engine, built here, is the Smethwick Engine built to recover water used in the nearby canal locks at Smethwick Summit, and now in Thinktank, Birmingham Science Museum.\n\n\n\n"}
{"id": "5019763", "url": "https://en.wikipedia.org/wiki?curid=5019763", "title": "TMS6100", "text": "TMS6100\n\nThe Texas Instruments TMS6100 is a 1 or 4-bit serial mask(factory)-programmed read-only memory IC. It is a companion chip to the TMS5100, CD2802, TMS5110, (rarely) TMS5200, and (rarely) TMS5220 speech synthesizer ICs, and was mask-programmed with LPC data required for a specific product. It holds 128Kib (16KiB) of data, and is mask-programmed with a start address for said data on a 16KiB boundary. It is also mask-programmable whether the /CE line needs to be high or low to activate, and also what the two (or four) 'internal' CE bits need to be set to activate, effectively making the total addressable area 18 bits. Finally, it is mask-programmable whether the bits are read out 1-bit serially or 4 at a time.\n\nThe \"TMS6125\" is a smaller, 32Kib (4KiB) version of effectively the same chip, with some minor changes to the 'address load' command format to reflect its smaller size.\n\nTexas Instruments calls both of these serial roms (TMS6100 and TMS6125) \"VSM\"s (Voice Synthesis Memory) on their datasheets and literature, and they will be referred to as such for the rest of this article.\n\nBoth VSMs use 'local addressing', meaning the chip keeps track of its own address pointer once loaded. Hence every bit in the chip can be sequentially read out, even though internally the chip stores data in 8-bit bytes.\n\nThe VSM has supports 4 basic commands, based on two input pins called 'M0' and 'M1':\n\nWhen used on Texas Instruments' Consumer Division products, the VSMs are always marked CDxxxxx where xxxxx is a 4 or 5 digit mask rom ID code, typically 23xx or 62xxx. The chips are SOMETIMES (typically after 1981) also marked TMC0350, as this seems to be the 'internal use class' of the chip. The very first VSMs (used in the Speak and Spell, 1978 version) were marked TMC0351 and TMC0352 and did not have a CD number. These two SEEM to have been 'grandfathered in' as CD2300 and CD2301, as all later VSMs start at CD2302 (used in the 'Vowel Power' Speak and Spell expansion cartridge). It should also be noted that CD22xx, CD25xx and CD28xx numbers are used for chips that are NOT VSMs.\n\nWhen used on Texas Instruments' non-consumer division products (such as generic voice chips for other computers/measurement devices which were still TI-branded) the chips are labeled VMxxxxx.\n\nWhen used on 3rd party products, the chips are marked CMxxxxx instead. The numbering scheme for the xxxxx part seems to be common between both CDxxxxx and CMxxxxx chips (but NOT VMxxxxx chips), so no chip of either series will have a common xxxxx number.\n"}
{"id": "11430131", "url": "https://en.wikipedia.org/wiki?curid=11430131", "title": "Taurus World Stunt Awards", "text": "Taurus World Stunt Awards\n\nThe Taurus World Stunt Awards is a yearly award ceremony held midyear that honors stunt performers in movies. It is held each year in Los Angeles. The first awards were given out in 2001. However the deciding committee has been around since the year 2000. The awards were created by Dietrich Mateschitz, the founder of Red Bull.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the videogame \"\", a player can be nominated for the Taurus World Stunt Awards.\n\nThe award ceremony is normally broadcast on American television channels such as Entertainment News (E!).\n"}
{"id": "39456566", "url": "https://en.wikipedia.org/wiki?curid=39456566", "title": "Terahertz spectroscopy and technology", "text": "Terahertz spectroscopy and technology\n\nTerahertz spectroscopy detects and controls properties of matter with electromagnetic fields that are in the frequency range between a few hundred gigahertz and several terahertz (abbreviated as THz). In many-body systems, several of the relevant states have an energy difference that matches with the energy of a THz photon. Therefore, THz spectroscopy provides a particularly powerful method in resolving and controlling individual transitions between different many-body states. By doing this, one gains new insights about many-body quantum kinetics and how that can be utilized in developing new technologies that are optimized up to the elementary quantum level.\n\nDifferent electronic excitations within semiconductors are already widely used in lasers, electronic components, computers, to mention a few. At the same time, they constitute an interesting many-body system whose quantum properties can be modified, e.g., via a nanostructure design. Consequently, THz spectroscopy on semiconductors is relevant in revealing both new technological potentials of nanostructures as well as in exploring the fundamental properties of many-body systems in a controlled fashion.\n\nThere are a great variety of techniques to generate THz radiation and to detect THz fields. One can, e.g., use an antenna, a quantum-cascade laser, a free-electron laser, or optical rectification to produce well-defined THz sources. The resulting THz field can be characterized via its electric field \"E\"(\"t\"). Present-day experiments can already output \"E\"(\"t\") that has a peak value in the range of MV/cm (megavolts per centimeter). To estimate how strong such fields are, one can compute the level of energy change such fields induce to an electron over microscopic distance of one nanometer (nm), i.e., \"L\" = 1 nm. One simply multiplies the peak \"E\"(\"t\") with elementary charge \"e\" and \"L\" to obtain \"e\" \"E\"(\"t\") \"L\" = 100 meV. In other words, such fields have a major effect on electronic systems because the mere field strength of \"E\"(\"t\") can induce electronic transitions over microscopic scales. One possibility is to use such THz fields to study Bloch oscillations where semiconductor electrons move through the Brillouin zone, just to return to where they started, giving rise to the Bloch oscillations.\n\nThe THz sources can be also extremely short, down to single cycle of THz field's oscillation. For one THz, that means duration in the range of one picosecond (ps). Consequently, one can use THz fields to monitor and control ultrafast processes in semiconductors or to produce ultrafast switching in semiconductor components. Obviously, the combination of ultrafast duration and strong peak \"E\"(\"t\") provides vast new possibilities to systematic studies in semiconductors.\n\nBesides the strength and duration of \"E\"(\"t\"), the THz field's photon energy plays a vital role in semiconductor investigations because it can be made resonant with several intriguing many-body transitions. For example, electrons in conduction band and holes, i.e., electronic vacancies, in valence band attract each other via the Coulomb interaction. Under suitable conditions, electrons and holes can be bound to excitons that are hydrogen-like states of matter. At the same time, the exciton binding energy is few to hundreds of meV that can be matched energetically with a THz photon. Therefore, the presence of excitons can be uniquely detected based on the absorption spectrum of a weak THz field. Also simple states, such as plasma and correlated electron–hole plasma can be monitored or modified by THz fields.\n\nIn optical spectroscopy, the detectors typically measure the intensity of the light field rather than the electric field because there are no detectors that can directly measure electromagnetic fields in the optical range. However, there are multiple techniques, such as antennas and electro-optical sampling, that can be applied to measure the time evolution of \"E\"(\"t\") directly. For example, one can propagate a THz pulse through a semiconductor sample and measure the transmitted and reflected fields as function of time. Therefore, one collects information of semiconductor excitation dynamics completely in time domain, which is the general principle of the terahertz time-domain spectroscopy.\n\nBy using short THz pulses, a great variety of physical phenomena have already been studied. For unexcited, intrinsic semiconductors one can determine the complex permittivity or THz-absorption coefficient and refractive index, respectively. The frequency of transversal-optical phonons, to which THz photons can couple, lies for most semiconductors at several THz. Free carriers in doped semiconductors or optically excited semiconductors lead to a considerable absorption of THz photons.\n\nThe THz fields can be applied to accelerate electrons out of their equilibrium. If this is done fast enough, one can measure the elementary processes, such as how fast the screening of the Coulomb interaction is built up. This was experimentally explored in Ref. where it was shown that screening is complete within tens of femtoseconds in semiconductors. These insights are very important to understand how electronic plasma behaves in solids.\n\nThe Coulomb interaction can also pair electrons and holes into excitons, as discussed above. Due to their analog to the hydrogen atom, excitons have bound states that can be uniquely identified by the usual quantum numbers 1\"s\", 2\"s\", 2\"p\", and so on. In particular, 1\"s\"-to-2\"p\" transition is dipole allowed and can be directly generated by \"E\"(\"t\") if the photon energy matches the transition energy. In gallium arsenide-type systems, this transition energy is roughly 4 meV that corresponds to 1 THz photons. At resonance, the dipole \"d\" defines the Rabi energy Ω = \"d\" \"E\"(\"t\") that determines the time scale at which the 1\"s\"-to-2\"p\" transition proceeds.\n\nFor example, one can excite the excitonic transition with an additional optical pulse which is synchronized with the THz pulse. This technique is called transient THz spectroscopy. Using this technique one can follow the formation dynamics of excitons or observe THz gain arising from intraexcitonic transitions.\n\nSince a THz pulse can be intense and short, e.g., single-cycle, it is experimentally possible to realize situations where duration of the pulse, time scale related to Rabi- as well as the THz photon energy ħω are degenerate. In this situation, one enters the realm of extreme nonlinear optics where the usual approximations, such as the rotating-wave approximation (abbreviated as RWA) or the conditions for complete state transfer, break down. As a result, the Rabi oscillations become strongly distorted by the non-RWA contributions, the multiphoton absorption or emission processes, and the dynamic Franz–Keldysh effect, as measured in Refs.\n\nBy using a free-electron laser, one can generate longer THz pulses that are more suitable for detecting the Rabi oscillations directly. This technique could indeed demonstrate the Rabi oscillations, or actually the related Autler–Townes splitting, in experiments. The Rabi splitting has also been measured with a short THz pulse and also the onset to multi-THz-photon ionization has been detected, as the THz fields are made stronger. Recently, it has also been shown that the Coulomb interaction causes nominally dipole-forbidden intra-excitonic transitions to become partially allowed.\n\nTerahertz transitions in solids can be systematically approached by generalizing the semiconductor Bloch equations and the related many-body correlation dynamics. At this level, one realizes the THz field are directly absorbed by two-particle correlations that modify the quantum kinetics of electron and hole distributions. Therefore, a systematic THz analysis must include the quantum kinetics of many-body correlations, that can be treated systematically, e.g., with the cluster-expansion approach. At this level, one can explain and predict a wide range of effects with the same theory, ranging from Drude-like response of plasma to extreme nonlinear effects of excitons.\n\n"}
{"id": "3183108", "url": "https://en.wikipedia.org/wiki?curid=3183108", "title": "Test light", "text": "Test light\n\nA test light, test lamp, voltage tester, or mains tester is a piece of electronic test equipment used to determine the presence of electricity in a piece of equipment under test. A test light is simpler and less costly than a measuring instrument such as a multimeter, and often suffices for checking for the presence of voltage on a conductor. Properly designed test lights include features to protect the user from accidental electric shock. Non-contact test lights can detect voltage on insulated conductors. \n\nThe test light is an electric lamp connected with one or two insulated wire leads. Often, it takes the form of a screwdriver with the lamp connected between the tip of the screwdriver and a single lead that projects out the back of the screwdriver. By connecting the flying lead to an earth (ground) reference and touching the screwdriver tip to various points in the circuit, the presence or absence of voltage at each point can be determined, allowing simple faults to be detected and traced to their root cause. For higher voltages, a statiscope consisting of a neon glow tube mounted on a long insulating handle can be used to detect AC voltages of 2000 volts or more.\n\nFor low voltage work (for example, in automobiles), the lamp used is usually a small, low-voltage incandescent light bulb. These lamps usually are designed to operate on approximately 12 V; application of an automotive test lamp on mains voltage will destroy the lamp and may cause a short-circuit fault in the tester.\n\nFor line voltage (mains) work, the lamp is usually a small neon lamp connected in series with an appropriate ballast resistor. These lamps often can operate across a wide range of voltages from 90V up to several hundred volts. In some cases, several separate lamps are used with resistive voltage dividers arranged to allow additional lamps to strike as the applied voltage rises higher. The lamps are mounted in order from lowest voltage to highest, this minimal bar graph providing a crude indication of voltage.\n\nIncandescent bulbs may also be used in some electronic equipment repair, and a trained technician can usually tell the approximate voltage by using the brightness as a crude indicator.\n\nA hand-held test lamp necessarily puts the user in proximity to live circuits. Accidental contact with live wiring can result in a short circuit or electric shock. Inexpensive or home-made test lamps may not include sufficient protection against high-energy faults. It is customary to connect a test lamp to a known live circuit both before and after testing an unknown circuit, to check for failure of the test lamp itself.\n\nIn the UK, guidelines established by the Health and Safety Executive (HSE) provide recommendations for the construction and use of test lamps. Probes must be well-insulated, with minimal exposure of live terminals, with finger guards to prevent accidental contact, and must not expose live wires if the test lamp glass bulb is broken. To limit the energy delivered in case of a short-circuit, test lights must have a current-limiting fuse or current-limiting resistor and fuse. The HSE guidelines also recommend procedures to validate operation of the test light. When a known live circuit is not available, a separate proving unit that provides a known test voltage and sufficient power to illuminate the lamp is used to confirm operation of the lamp before and after testing a circuit.\n\nSince energy to operate the test lamp is drawn from the circuit under test, some high-impedance leakage voltages may not be detectable using this type of non-amplified test equipment.\n\nA low-cost type of test lamp only contacts one side of the circuit under test, and relies on stray capacitance and current passing through the user's body to complete the circuit. The device may have the form of a screwdriver. The tip of the tester is touched to the conductor being tested (for instance, it can be used on a wire in a switch, or inserted into a hole of an electric socket). A neon lamp takes very little current to light, and thus can use the user's body capacitance to earth ground to complete the circuit.\n\nScrewdriver-type test lamps are very inexpensive, but cannot meet the construction requirements of UK GS 38. If the shaft is exposed, a shock hazard to the user exists, and the internal construction of the tester provides no protection against short-circuit faults. Failure of the resistor and lamp series network can put the user in direct metallic contact with the circuit under test. For example, water trapped inside the screwdriver may allow enough leakage current to shock the user. Even if an internal short circuit does not electrocute the user, the resulting electric shock may result in a fall or other injury. The lamp provides no indication below the strike voltage of the neon lamp, and so cannot detect certain hazardous leakage conditions. Since it relies on capacitance to complete the circuit, direct-current potential cannot be reliably indicated. If the user of the screwdriver is isolated from ground and capacitively coupled to other nearby live wires, a false negative may occur when testing a live circuit, and a false positive when testing a dead circuit. False negatives may also occur in brightly lit areas which make the neon glow hard to see.\n\nAmplified electronic testers (informally called electrical tester pens, test pens, or voltage detectors) rely on capacitive current only, and essentially detect the changing electric field around AC energized objects. This means that no direct metallic contact with the circuit is required. The user must touch the top of the handle to provide a ground reference (through stray capacitance to ground), at which point the indicator LED will light up or a speaker will buzz, if the conductor being tested is live. Additional energy to light the lamp and power the amplifier is supplied by a small internal battery, and does not flow through the user's body.\n\nWhen the device is placed near a live conductor, a capacitive voltage divider is established, comprising the parasitic capacitance between the conductor and the sensor, and between the sensor to ground (through the user's body). When the tester detects current flowing through this divider, it indicates the presence of voltage.\n\nSome amplified testers will give a stronger indication (brighter light or louder buzz) to gauge relative strength of the detected field, thus giving some clues about the location of an energized object. Other testers give only a simple on/off indication of a detected electric field. Professional-grade testers will also have a feature to reassure the user that the battery and lamp are working.\n\nVoltage detector pens are made for either line-voltage or lower-voltage (around 50 volt) ranges. A tester intended for mains-voltage detection may not provide any indication on lower-voltage control circuits such as those used for doorbells or HVAC control.\n\nUnlike tong ammeters which sense changing magnetic fields, these detectors can be used even if no current is flowing through the wire in question, because they sense the alternating electric field radiating from the AC voltage on the conductor.\n\nA non-contact tester which senses electric fields cannot detect voltage inside shielded or armored cables (a fundamental limitation due to the Faraday cage effect). Another limitation is that DC voltage cannot be detected by this method, since DC current does not pass through capacitors (in the steady state), so the tester is not activated.\n\nThese types of testers can be used on series-connected strings of mini Christmas lights to detect which bulb has failed and broken the circuit, causing the set (or a section of it) not to light. By pointing the end of the detector at the tip of each bulb, it can be determined whether it is still connected at least on one side. The first bulb which does not register is likely the one just past the problem bulb. (Burnt-out bulbs will still show as good, if there is a bypass shunt which completes the circuit.) Flipping the set's plug over and reinserting it in the outlet will cause the opposite end of the set or circuit to register instead.\n\nA receptacle tester (outlet tester or socket tester) plugs into an outlet, and can detect some types of wiring errors. The particular error in wiring is shown by various combinations of three lights. Detectable errors include reversed hot/neutral, missing electrical ground or neutral, and others. However, leakage current through surge protective metal oxide varistors connected between neutral and ground of a power strip can give a false indication that a ground connection exists.\n\nA lamp and battery can be used to test for contact closure or wire continuity. Care must be taken to ensure that all circuits are completely de-energized before use of a continuity tester lamp, or the lamp will be destroyed. Sometimes a flashlight (torch) is field-modified or factory-manufactured with test leads, to allow the flashlight to be used as a continuity tester.\n\n\n"}
{"id": "27328585", "url": "https://en.wikipedia.org/wiki?curid=27328585", "title": "Three-phase AC railway electrification", "text": "Three-phase AC railway electrification\n\nThree-phase AC railway electrification was used in Italy, Switzerland and the United States in the early twentieth century. Italy was the major user, from 1901 until 1976, although lines through two tunnels also used the system; the Simplon Tunnel between Switzerland and Italy from 1906 to 1930 (but not connected to the Italian system), and the Cascade Tunnel of the Great Northern Railway in the United States from 1909 to 1939. The first standard gauge line was in Switzerland, from Burgdorf to Thun (), since 1899. \n\nThe system provides regenerative braking with the power fed back to the system, so is particularly suitable for mountain railways (provided the grid or another locomotive on the line can accept the power). The locomotives use three-phase induction motors. Lacking brushes and commutators, they require less maintenance. The early Italian and Swiss systems used a low frequency (16⅔ Hz), and a relatively low voltage (3,000 or 3,600 volts) compared with later AC systems.\n\nThe overhead wiring, generally having two separate overhead lines and the rail for the third phase, was more complicated, and the low-frequency used required a separate generation or conversion and distribution system. Train speed was restricted to one to four speeds, with two or four speeds obtained by pole-changing or cascade operation or both. \n\nThe following is a list of the railways that have used this method of electrification in the past:\n\n\nThe system is only used today for four rack (mountain) railways, where the overhead wiring is less complicated and restrictions on the speeds available less important. New motive power avoids speed restrictions, as it is built with solid-state converters. The four systems are as follows:\n\n\nThey are nowadays industrial rather than low frequency (50 Hz, or 60 Hz (Brazil)), using between 725 and 3,000 volts.\n\nThis list shows the voltage and frequency used in various systems, historical and current. It is not complete.\n\nThis category does not cover railways with a single-phase (or DC) supply which is converted to three-phase on the locomotive or power car, \"e.g.\", most railway equipment from the 1990s and earlier using solid-state converters. The Kando system of the 1930s developed by Kálmán Kandó at the Ganz Works, and used in Hungary and Italy, used rotary phase converters on the locomotive to convert the single-phase supply to three phases, as did the Phase-splitting system on the Norfolk and Western Railroad in the United States.\n\nUsually, the locomotives had one, two, or four motors on the body chassis (not on the bogies), and did not require gearing. The induction motors are designed to run at a particular synchronous speed, and when they run above the synchronous speed downhill, power is fed back to the system. Pole changing and cascade (concatenation) working was used to allow two or four different speeds, and resistances (often liquid rheostats) were required for starting. In Italy freight locomotives used plain cascade with two speeds, ; while express locomotives used cascade combined with pole-changing, giving four speeds, 37, 50, 75 and 100 km/h (23, 31, 46 and 62 mph). With the use of 3,000 or 3,600 volts at 16⅔ (16.7) Hz, the supply could be fed directly to the motor without an onboard transformer.\n\nGenerally, the motor(s) fed a single axle, with other wheels linked by connecting rods, as the induction motor is sensitive to speed variations and with non-linked motors on several axles the motors on worn wheels would do little or even no work as they would rotate faster. This motor characteristic led to a mishap in the Cascade Tunnel to a GN east-bound freight train with four electric locomotives, two on the head and two pushing. The two pushers suddenly lost power and the train gradually slowed to a stop, but the lead unit engineer was unaware that his train had stopped, and held the controller on the power position until the usual time to transit the tunnel had elapsed. Not seeing daylight, he finally shut down the locomotive, and found that the wheels of his stationary locomotive had ground through two-thirds of the rail web.\n\nGenerally two separate overhead wires are used, with the rail for the third phase, though occasionally three overhead wires are used. At junctions, crossovers and crossings the two lines must be kept apart, with a continuous supply to the locomotive, which must have two live conductors wherever it stops. Hence two collectors per overhead phase are used, but the possibility of bridging a dead section and causing a short circuit from the front collector of one phase to the back collector of the other phase must be avoided. The resistance of the rails used for the third phase or return is higher for AC than for DC due to \"skin effect\", but lower for the low frequency used than for industrial frequency. Losses are also increased, though not in the same proportion, as the impedance is largely reactive.\nThe locomotive needs to pick up power from two (or three) overhead conductors. Early locomotives on the Italian State Railways used a wide bow collector which covered both wires but later locomotives used a wide pantograph with two collector bars, side by side. A three-phase system is also prone to larger lengthwise gaps between sections, owing to the complexity of two-wire overhead, and so a long pickup base is needed. In Italy this was achieved with the long bow collectors reaching right to the ends of the locomotive, or with a pair of pantographs, also mounted as far apart as possible.\n\nIn the United States, a pair of trolley poles were used. They worked well with a maximum speed of . The dual conductor pantograph system is used on four mountain railways that continue to use three-phase power (Corcovado Rack Railway in Rio de Janeiro, Brazil, Jungfraubahn and Gornergratbahn in Switzerland and the Petit train de la Rhune in France).\n\n\n"}
{"id": "9916529", "url": "https://en.wikipedia.org/wiki?curid=9916529", "title": "Topic-based authoring", "text": "Topic-based authoring\n\nIn technical communication, topic-based authoring is a modular approach to content creation where content is structured around topics that can be mixed and reused in different contexts. It is defined in contrast with \"book-oriented\" or \"narrative\" content, written in the linear structure of written books.\n\nThis authoring approach is popular in the technical publications and documentation arenas, as it is adequate for technical documentation. Tools supporting this approach typically store content in XML document format in a way that facilitates content reuse, content management, and makes the dynamic assembly of personalized information possible.\n\nA topic is a discrete piece of content that is about a specific subject, has an identifiable purpose, and can stand alone (does not need to be presented in context for the end-user to make sense of the content). Topics are also reusable. They can, when constructed properly (without reliance on other content for its meaning), be reused in any context anywhere needed.\n\nThe Darwin Information Typing Architecture (DITA) is a standard designed to help authors create topic-based content. The standard is managed by the Organization for the Advancement of Structured Information Standards (OASIS) DITA Technical Committee.\n\n\n"}
{"id": "26271156", "url": "https://en.wikipedia.org/wiki?curid=26271156", "title": "Uwars", "text": "Uwars\n\nUWARS - Universal Water Activated Release System- is a life-preserving system made by Conax Florida Corporation. The UWARS is attached to a buckle or harness that, when submerged in water, releases a harness and thus prevents drowning.\n"}
{"id": "47202079", "url": "https://en.wikipedia.org/wiki?curid=47202079", "title": "Windshield sun shades", "text": "Windshield sun shades\n\nWindshield sun shades (also known as sun screen shades, sunscreens, car shades, sun shields, heat shields, or UV shields) are protective shields attached to a car's windshield to keep the sun from reaching the interior and help reduce the temperature inside it.\n\nThe interior of a closed car parked in a non-shaded spot exposed to extensive sunlight can sustain sun and heat damage. Sunlight passing through the car's windshield produces the greenhouse effect, heating the interior to a temperature much higher than the outside air. Excessive heat and prolonged exposure to direct sunlight can make the dashboard fade and eventually crack, cause seat upholstery discoloration and aging, etc.\n\nThe windshield glass itself blocks most of the UV light and some of the infrared radiation. But it can't protect from the visible light that mostly penetrates through it and gets absorbed by the objects inside the car. The visible light that passes into the interior through the windshield is converted into the infrared light which, in its turn, is blocked by the glass and gets trapped inside, heating up the interior. Windshield sun shades have a reflective surface to bounce the light back, reducing the interior temperature\n\nTypically, sun shields look like aluminum foil that covers the inside of a windshield to reflect the sun's rays. But today the automotive aftermarket offers various options, from simple car shades manufactured from high-density foam with a reflective metalized polyester film to automatic sun shades operated using a remote control.\n\nRemovable windshield sun shades are popular thanks to ease of installation and storage as well as an affordable price.\n\nThey are available in many designs, including accordion fold, roll up, etc. There are also permanently installed sun shades built directly into the window frame. Some vehicles can be equipped with power sun shades that move up and down with a touch of a button. Some retractable sun shades are attached with an adhesive, suction cups, etc. and roll up on the top or fold at the sides of the windshield. Some car shades are designed for winter use and attach to the outside of the windshield (typically with straps) to keep it snow- and ice-free.\n\nA number of companies manufacture car shades, either universal or custom patterned for a precise fit. Covercraft offers easy to use sunscreens in a number of colors and patterns. WeatherTech provides vehicle-specific windshield sun shades that protect both from UV rays in the summer and snow and frost buildup in the winter thanks to reflective surface on the front side and the reverse black side that absorbs the heat in the winter. Coverking manufactures both sun shields and frost shields for summer and winter use respectively.\n"}
