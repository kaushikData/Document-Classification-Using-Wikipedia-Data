{"id": "8998943", "url": "https://en.wikipedia.org/wiki?curid=8998943", "title": "845 (vacuum tube)", "text": "845 (vacuum tube)\n\nThe 845 power triode is a radio transmitting vacuum tube which can also be used as an audio amplifier and modulation tube. Typically, the plate is machined from solid graphite in order to accommodate high current dissipation (up to 100 watts) and voltage. Some current production 845 tubes have metal plates.\n\nThe 845 tube has a bayonet mount and thoriated filaments which glow like lightbulbs when powered up. The glass envelope is about 2-5/16\" in diameter and 6 inches tall, with the a total tube height of about 7-7/8 inches. It was first released by RCA in 1931. It saw extensive use in RCA AM radio transmitters\n\n"}
{"id": "42441909", "url": "https://en.wikipedia.org/wiki?curid=42441909", "title": "AC 25.1309-1", "text": "AC 25.1309-1\n\nAC 25.1309–1 is an FAA Advisory Circular (AC) (Subject: \"System Design and Analysis\") that describes acceptable means for showing compliance with the airworthiness requirements of § 25.1309 of the Federal Aviation Regulations. The present unreleased but working draft of AC 25.1309–1 is the Aviation Rulemaking Advisory Committee recommended revision \"B-Arsenal Draft\" (2002); the present \"released\" version is A (1988). The FAA and EASA have accepted proposals by type certificate applicants to use the Arsenal Draft on recent development programs.\n\nAC 25.1309–1 establishes the principle that the more severe the hazard resulting from a system or equipment failure, the less likely that failure must be. Failures that are catastrophic must be extremely improbable.\n\nThe airworthiness requirements for transport category airplanes are contained in Title 14, Code of Federal Regulations (14 CFR) part 25 (commonly referred to as part 25 of the Federal Aviation Regulations (FAR)). Manufacturers of transport category airplanes must show that each airplane they produce of a given type design complies with the relevant standards of part 25.\n\nAC 25.1309–1 describes acceptable means for showing compliance with those airworthiness requirements. It recognizes Aerospace Recommended Practices ARP4754 and ARP4761 (or their successors) as such means:\n\nAC 25.1309–1 provides background for important concepts and issues within airplane system design and analysis.\nThe circular provides a rationale for the upper limit for the Average Probability per Flight Hour for Catastrophic Failure Conditions of 1 x 10 or \"Extremely Improbable\". Failure Conditions having less severe effects could be relatively more likely to occur; that is, an inverse relationship between severity and likelihood.\n\nThis AC presents the FAA \"Fail-Safe Design Concept\", which applies basic objectives pertaining to failures:\nThe AC lists design principles or techniques used to ensure a safe design. Usually, a combination of at least two safe design techniques are needed to provide a fail-safe design; i.e. to ensure that Major Failure Conditions are Remote, Hazardous Failure Conditions are Extremely Remote, and Catastrophic Failure Conditions are Extremely Improbable.\n\nWith emergence of \"highly integrated systems\" that perform complex and interrelated functions, particularly through the use of electronic technology and software-based techniques [e.g., Integrated Modular Avionics (IMA) ], concerns arose that traditionally \"quantitative\" functional-level design and analysis techniques previously applied to simpler systems were no longer adequate. As such the AC includes expanded, methodical approaches, both qualitative and quantitative, that consider the \"integration\" of the \"whole airplane and its systems\".\n\nA main task of AC 25.1309–1 is to provide standard definitions of terms (including hazard and probability classifications) for consistent use throughout the framework set up for the accomplishment of functional airplane safety. Where regulations (FAR) and standards (ARP) may use such terms as \"failure condition\", and \"extremely improbable\", AC 25.1309–1 defines their specific meanings. In this respect, AC 25.1309–1 is comparable to ISO 26262–1 Vocabulary, at least in regard to the relative dependent standards. Key definitions include:\n\n\n\nClassified failure conditions are assigned qualitative and quantitative safety objectives, giving guidance to development and operation.\nThe AC defines the acceptable safety level for equipment and systems as installed on the airplane and establishes an inverse relationship between Average Probability per Flight Hour and the severity of Failure Condition effects:\nThe safety objectives associated with Catastrophic Failure Conditions may be satisfied by demonstrating that:\nThe failure conditions \"Catastrophic\" through \"No Safety Effect\" are assigned Functional and Item Design Assurance Levels A, B, C, D, E, respectively.\n\nFirst released in 1982, AC 25.1309–1 has been revised to embody increasing experience in development of airplanes and to address the increasing integration and computerization of aircraft functions.\n\nAC 25.1309–1 recommended that top-down analysis should identify each system function and evaluate its criticality, i.e., either non-essential, essential, or critical. The terms Error, Failure, and Failure Condition were defined. Functions were classified Critical, Essential, and Non-Essential according to the severity of the failure conditions they could contribute to; but the conditions were not expressly classified. Failures of Critical, Essential, and Non-Essential functions were expected to be, respectively, Extremely Improbable (10 or less), Improbable (10 or less), or no worse than Probable (10).\n\nPreviously, system safety analysis was quantitative; that is, it was dependent on evaluating the probability of system failures from physical faults of components. But with the increasing use of digital avionics (i.e., software) it was recognized that development error was a significant contributor to system failure. During system certification in the late 1970s, it became clear that the classical statistical methods of safety assessment for flight critical software based systems were not possible. Existing quantitative methods could not predict system failure rates resultant from development errors. \"Qualitative\" methods were instead recommended for reducing specification, design, and implementation errors in the development of digital avionics. \n\nThe guidance of DO-178 (initial release) was recommended by AC 25.1309–1 for development of essential and critical functions implemented in software.\n\nAC 25.1309–1A introduced the FAA Fail-Safe Design Concept to this Advisory Circular. This revision also introduced recommended design principles or techniques in order to ensure a safe design.\n\nThe concept of function criticality was replaced with classification of failure conditions according to severity of effects (cf., Probabilistic risk assessment). Failure conditions having Catastrophic, Major, or Minor effects were to have restricted likelihoods, respectively, of Extremely Improbable (10 or less), Improbable (10 or less), or no worse than Probable (10).\n\nSoftware was still considered to be assessed and controlled by other means; that is, by RTCA/DO-178A or later revision, via Advisory Circular AC 20-115A.\n\nIn May 1996, the FAA Aviation Rulemaking Advisory Committee (ARAC) was tasked with a review of harmonized FAR/JAR 25.1309, AC 1309-1A, and related documents, and to consider revision to AC 1309-1A incorporating recent practice, increasing complex integration between aircraft functions and the systems that implement them, and the implications of new technology. This task was published in the Federal Register at 61 FR 26246-26247 (1996-05-24). The focus was to be on safety assessment and fault-tolerant critical systems.\n\nIn 2002, the FAA provided a Notice of Proposed Rulemaking (NPRM) relevant to 14 CFR Part 25. Accompanying this notice is the \"Arsenal draft\" of AC 1309–1. Existing definitions and rules in § 25.1309 and related standards have posed certain problems to the certification of transport category airplanes. Said problems are discussed at length within the NPRM. The FAA proposed revisions to several related standards in order to eliminate such problems and to clarify the intent of these standards. In some proposed changes, definitions or conventions developed in lower level regulations or standards were adopted or revised within the subsequent Advisory Circular.\n\nExperience in application of the prior circulars and ARPs witnessed the division of the \"Major\" failure condition into two conditions (for example, Hazardous-severe/Major and Major). Additionally, this experience recognised the existence of failure conditions that have \"no effect on safety\", which could be so classified and thereby assigned no safety objectives. Catastrophic Failure Condition was previously defined as \"any failure condition which would prevent continued safe flight and landing\"; but is now defined as \"Failure conditions which would result in multiple fatalities, usually with the loss of the airplane.\"\n\nThe FAA Fail-Safe Design Concept and design principles or techniques for safe design are maintained. However, owing to the increasing development of Highly Integrated Systems in aircraft, qualitative controls previously considered necessary for safe software development are extended to the aircraft function level. (Similar guidance (Functional Safety framework) has been provided for highly integrated automotive systems through the 2011, release of ISO 26262.)\n\n"}
{"id": "54270366", "url": "https://en.wikipedia.org/wiki?curid=54270366", "title": "ARINC 629", "text": "ARINC 629\n\nThe ARINC 629 computer bus was introduced in May 1995 and is used on the Boeing 777, and Airbus aircraft. The ARINC 629 bus operates as a multiple-source, multiple-sink system; each terminal can transmit data to, and receive data from, every other terminal on the data bus. This allows much more freedom in the exchange of data between units in the avionics system. ARINC 629 has the ability to accommodate up to a total of 128 terminals on a data bus and supports a data rate of 2 Mbps.\n\nThe ARINC 629 data bus was developed by the Airlines Electronic Engineering Committee (AEEC) to replace the ARINC 429 bus.\nThe ARINC 629 data bus was based on the Boeing DATAC bus.\n\nWhile some people expected that the Boeing 777 would be the first and last aircraft to use ARINC 629 databus, the ARINC 629 databus is also used on the Airbus A330 and A340.\n"}
{"id": "1846371", "url": "https://en.wikipedia.org/wiki?curid=1846371", "title": "Air–fuel ratio", "text": "Air–fuel ratio\n\nAir–fuel ratio (AFR) is the mass ratio of air to a solid, liquid, or gaseous fuel present in a combustion process. The combustion may take place in a controlled manner such as in an internal combustion engine or industrial furnace, or may result in an explosion (e.g., a dust explosion, gas or vapour explosion or in a thermobaric weapon). \n\nThe air-fuel ratio determines whether a mixture is combustible at all, how much energy is being released, and how much unwanted pollutants are produced in the reaction. Typically a range of fuel to air ratios exists, outside of which ignition will not occur. These are known as the lower and upper explosive limits.\n\nIn an internal combustion engine or industrial furnace, the air-fuel ratio is an important measure for anti-pollution and performance-tuning reasons. If exactly enough air is provided to completely burn all of the fuel, the ratio is known as the stoichiometric mixture, often abbreviated to stoich. Ratios lower than stoichiometric are considered \"rich\". Rich mixtures are less efficient, but may produce more power and burn cooler. Ratios higher than stoichiometric are considered \"lean.\" Lean mixtures are more efficient but may cause higher temperatures, which can lead to the formation of nitrogen oxides. Some engines are designed with features to allow lean-burn. For precise air-fuel ratio calculations, the oxygen content of combustion air should be specified because of different air density due to different altitude or intake air temperature, possible dilution by ambient water vapor, or enrichment by oxygen additions.\n\nIn theory a stoichiometric mixture has just enough air to completely burn the available fuel. In practice this is never quite achieved, due primarily to the very short time available in an internal combustion engine for each combustion cycle. Most of the combustion process is completed in approximately 2 milliseconds at an engine speed of . (100 revolutions per second; 10 milliseconds per revolution) This is the time that elapses from the spark plug firing until 90% of the fuel–air mix is combusted, typically some 80 degrees of crankshaft rotation later. Catalytic converters are designed to work best when the exhaust gases passing through them are the result of nearly perfect combustion.\n\nA stoichiometric mixture unfortunately burns very hot and can damage engine components if the engine is placed under high load at this fuel–air mixture. Due to the high temperatures at this mixture, detonation of the fuel–air mix while approaching or shortly after maximum cylinder pressure is possible under high load (referred to as knocking or pinging), specifically a \"pre-detonation\" event in the context of a spark-ignition engine model. Such detonation can cause serious engine damage as the uncontrolled burning of the fuel air mix can create very high pressures in the cylinder. As a consequence, stoichiometric mixtures are only used under light to low-moderate load conditions. For acceleration and high load conditions, a richer mixture (lower air–fuel ratio) is used to produce cooler combustion products and thereby prevent detonation and overheating of the cylinder head.\n\nThe stoichiometric mixture for a gasoline engine is the ideal ratio of air to fuel that burns all fuel with no excess air. For gasoline fuel, the stoichiometric air–fuel mixture is about 14.7:1 i.e. for every one gram of fuel, 14.7 grams of air are required. The fuel oxidation reaction is:\nAny mixture greater than 14.7:1 is considered a lean mixture; any less than 14.7:1 is a rich mixture – given perfect (ideal) \"test\" fuel (gasoline consisting of solely \"n\"-heptane and iso-octane). In reality, most fuels consist of a combination of heptane, octane, a handful of other alkanes, plus additives including detergents, and possibly oxygenators such as MTBE (methyl tert-butyl ether) or ethanol/methanol. These compounds all alter the stoichiometric ratio, with most of the additives pushing the ratio downward (oxygenators bring extra oxygen to the combustion event in liquid form that is released at time of combustions; for MTBE-laden fuel, a stoichiometric ratio can be as low as 14.1:1). Vehicles that use an oxygen sensor or other feedback loop to control fuel to air ratio (lambda control), compensate automatically for this change in the fuel's stoichiometric rate by measuring the exhaust gas composition and controlling fuel volume. Vehicles without such controls (such as most motorcycles until recently, and cars predating the mid-1980s) may have difficulties running certain fuel blends (especially winter fuels used in some areas) and may require different jets (or otherwise have the fueling ratios altered) to compensate. Vehicles that use oxygen sensors can monitor the air–fuel ratio with an air–fuel ratio meter.\n\nIn the typical air to natural gas combustion burner, a double cross limit strategy is employed to ensure ratio control. (This method was used in World War II). The strategy involves adding the opposite flow feedback into the limiting control of the respective gas (air or fuel). This assures ratio control within an acceptable margin.\n\nThere are other terms commonly used when discussing the mixture of air and fuel in internal combustion engines.\n\nMixture is the predominant word that appears in training texts, operation manuals and maintenance manuals in the aviation world.\n\nThe air–fuel ratio is the most common reference term used for mixtures in internal combustion engines. The term is also used to define mixtures used for industrial furnace heated by combustion. The AFR in mass units is employed in fuel oil fired furnaces, while volume (or mole) units are used for natural gas fired furnaces.\n\nAir–fuel ratio is the ratio between the \"mass\" of air and the mass of fuel in the fuel–air mix at any given moment. The mass is the mass of all constituents that compose the fuel and air, whether combustible or not. For example, a calculation of the mass of natural gas—which often contains carbon dioxide (), nitrogen (), and various alkanes—includes the mass of the carbon dioxide, nitrogen and all alkanes in determining the value of \"m\".\n\nFor pure octane the stoichiometric mixture is approximately 15.1:1, or \"λ\" of 1.00 exactly.\n\nIn naturally aspirated engines powered by octane, maximum power is frequently reached at AFRs ranging from 12.5 to 13.3:1 or \"λ\" of 0.850 to 0.901.\n\nAir-fuel ratio of 12:1 is considered as maximum output ratio, where as the air-fuel ratio of 16:1 is considered as maximum fuel economy ratio.\n\nFuel–air ratio is commonly used in the gas turbine industry as well as in government studies of internal combustion engine, and refers to the ratio of fuel to the air.\n\nAir–fuel equivalence ratio, \"λ\" (lambda), is the ratio of actual AFR to stoichiometry for a given mixture. \"λ\" = 1.0 is at stoichiometry, rich mixtures \"λ\" < 1.0, and lean mixtures \"λ\" > 1.0.\n\nThere is a direct relationship between \"λ\" and AFR. To calculate AFR from a given \"λ\", multiply the measured \"λ\" by the stoichiometric AFR for that fuel. Alternatively, to recover \"λ\" from an AFR, divide AFR by the stoichiometric AFR for that fuel. This last equation is often used as the definition of \"λ\":\n\nBecause the composition of common fuels varies seasonally, and because many modern vehicles can handle different fuels, when tuning, it makes more sense to talk about \"λ\" values rather than AFR.\n\nMost practical AFR devices actually measure the amount of residual oxygen (for lean mixes) or unburnt hydrocarbons (for rich mixtures) in the exhaust gas.\n\nThe fuel–air equivalence ratio, \"ϕ\" (phi), of a system is defined as the ratio of the fuel-to-oxidizer ratio to the stoichiometric fuel-to-oxidizer ratio. Mathematically,\n\nwhere, \"m\" represents the mass, \"n\" represents number of moles, suffix st stands for stoichiometric conditions.\n\nThe advantage of using equivalence ratio over fuel–oxidizer ratio is that it takes into account (and is therefore independent of) both mass and molar values for the fuel and the oxidizer. Consider, for example, a mixture of one mole of ethane () and one mole of oxygen (). The fuel–oxidizer ratio of this mixture based on the mass of fuel and air is\n\nand the fuel-oxidizer ratio of this mixture based on the number of moles of fuel and air is\n\nClearly the two values are not equal. To compare it with the equivalence ratio, we need to determine the fuel–oxidizer ratio of ethane and oxygen mixture. For this we need to consider the stoichiometric reaction of ethane and oxygen,\n\nThis gives\n\nThus we can determine the equivalence ratio of the given mixture as\n\nor, equivalently, as\n\nAnother advantage of using the equivalence ratio is that ratios greater than one always mean there is more fuel in the fuel–oxidizer mixture than required for complete combustion (stoichiometric reaction), irrespective of the fuel and oxidizer being used—while ratios less than one represent a deficiency of fuel or equivalently excess oxidizer in the mixture. This is not the case if one uses fuel–oxidizer ratio, which take different values for different mixtures.\n\nThe fuel–air equivalence ratio is related to the air–fuel equivalence ratio (defined previously) as follows:\n\nThe relative amounts of oxygen enrichment and fuel dilution can be quantified by the mixture fraction, Z, defined as\nwhere\n\"Y\" and \"Y\" represent the fuel and oxidizer mass fractions at the inlet, \"W\" and \"W\" are the species molecular weights, and \"v\" and \"v\" are the fuel and oxygen stoichiometric coefficients, respectively. The stoichiometric mixture fraction is\nThe stoichiometric mixture fraction is related to \"λ\" (lambda) and \"ϕ\" (phi) by the equations\nassuming\n\nIn industrial fired heaters, power plant steam generators, and large gas-fired turbines, the more common terms are percent excess combustion air and percent stoichiometric air. For example, excess combustion air of 15 percent means that 15 percent more than the required stoichiometric air (or 115 percent of stoichiometric air) is being used.\n\nA combustion control point can be defined by specifying the percent excess air (or oxygen) in the oxidant, or by specifying the percent oxygen in the combustion product. An air–fuel ratio meter may be used to measure the percent oxygen in the combustion gas, from which the percent excess oxygen can be calculated from stoichiometry and a mass balance for fuel combustion. For example, for propane () combustion between stoichiometric and 30 percent excess air (AFR between 15.58 and 20.3), the relationship between percent excess air and percent oxygen is: \n\n\n"}
{"id": "6255084", "url": "https://en.wikipedia.org/wiki?curid=6255084", "title": "Anthony French", "text": "Anthony French\n\nAnthony Philip French (November 19, 1920 – February 3, 2017) was a British professor of physics at the Massachusetts Institute of Technology. He was born in Brighton, England. French was a graduate of Cambridge University, receiving his B.A. in 1942 and Ph.D. in 1948, both in physics. In 1942, he began working on the British effort to build an atomic bomb (codenamed Tube Alloys) at the Cavendish Laboratory. By 1944, Tube Alloys had been merged with the American Manhattan Project and he was sent to Los Alamos.\n\nWhen the war ended, he returned to the United Kingdom, where he spent a couple of years at the newly formed Atomic Energy Research Establishment. He later joined the faculty at Cambridge, where he conducted his research at Cavendish and became a Fellow and Director of Studies in Natural Sciences at Pembroke College, Cambridge. In 1955, French arrived at the University of South Carolina, where he was made chairman of the physics department. He left South Carolina in 1962 to take a faculty position in the MIT Physics Department, where he remained until his death. French's main interest was undergraduate physics education. He was chairman of the Commission on Physics Education of the International Union of Pure and Applied Physics (1975-1981) and president of the American Association of Physics Teachers (1985-1986). He was also a Fellow of the American Physical Society.\n\n\n"}
{"id": "41788064", "url": "https://en.wikipedia.org/wiki?curid=41788064", "title": "Bin Qasim Industrial Zone", "text": "Bin Qasim Industrial Zone\n\nThe Bin Qasim Industrial Zone is one of the largest industrial areas in Karachi, Sindh, Pakistan. It consists of more than 25,000 acres of land in the Port Qasim (Bin Qasim) town area. Contained within this zone are many industrial units, ranging from medium to large in employment volume. Currently, there are 180 Large and Medium Size units operating in the area. \n\nThe Bin Qasim Industrial Zone has the potential to emerge as a Financial Hub of Karachi in the future: currently, there are a number of banks and insurance companies operating in the area. There are many large scale industrial companies operating in the area such as Pakistan Steel, Lotte Pakistan PTA Ltd., Toyota Indus Motors, Pak Suzuki, Engro Polymer, FFC Jordan, Procter & Gamble, Ghandhara Nissan, National Foods, Nestle Pakistan, Fa uji Oil Terminals, Ali Danyal Industries, Gamalux Oleochemicals, Shujabad Agro Industries, PAN Industries,IFFCO Pakistan Ltd., Mapak Oil Ltd., Universal Cables Ltd., Faisalabad Oil Refinery and Tripak Films Ltd. are the prominent industrial unit.\n\n\"BQATI\" or Bin Qasim Association of Trade and Industry, BQATI (formerly Port Qasim Association of Trade & Industry, PQATI) is a representative body of industries located in Bin Qasim industrial Zones. The Association was initially established with clear objectives to promote industrial activities in the area and to contribute positively to the economic well-being of the Country by way of enhancing industrial production.\n\n"}
{"id": "21575068", "url": "https://en.wikipedia.org/wiki?curid=21575068", "title": "Bluetooth Low Energy", "text": "Bluetooth Low Energy\n\nBluetooth Low Energy (Bluetooth LE, colloquially BLE, formerly marketed as Bluetooth Smart) is a wireless personal area network technology designed and marketed by the Bluetooth Special Interest Group (Bluetooth SIG) aimed at novel applications in the healthcare, fitness, beacons, security, and home entertainment industries. Compared to Classic Bluetooth, Bluetooth Low Energy is intended to provide considerably reduced power consumption and cost while maintaining a similar communication range.\n\nMobile operating systems including iOS, Android, Windows Phone and BlackBerry, as well as macOS, Linux, Windows 8 and Windows 10, natively support Bluetooth Low Energy. The Bluetooth SIG predicts that by 2018 more than 90% of Bluetooth-enabled smartphones will support Bluetooth Low Energy.\n\nBluetooth Low Energy is not backward-compatible with the previous (often called \"classic\") Bluetooth Basic Rate/Enhanced Data Rate (BR/EDR) protocol. The Bluetooth 4.0 specification permits devices to implement either or both of the LE and BR/EDR systems.\n\nBluetooth Low Energy uses the same 2.4 GHz radio frequencies as classic Bluetooth, which allows dual-mode devices to share a single radio antenna. LE does, however, use a simpler modulation system.\n\nIn 2011, the Bluetooth SIG announced the Bluetooth Smart logo so as to clarify compatibility between the new low energy devices and other Bluetooth devices.\n\n\nWith the May 2016 Bluetooth SIG branding information, the Bluetooth SIG began phasing out the Bluetooth Smart and Bluetooth Smart Ready logos and word marks and has reverted to using the Bluetooth logo and word mark. The logo uses a new blue color.\n\nThe Bluetooth SIG identifies a number of markets for low energy technology, particularly in the smart home, health, sport and fitness sectors. Cited advantages include:\n\nIn 2001, researchers at Nokia determined various scenarios that contemporary wireless technologies did not address. The company began developing a wireless technology adapted from the Bluetooth standard which would provide lower power usage and cost while minimizing its differences from Bluetooth technology. The results were published in 2004 using the name Bluetooth Low End Extension.\n\nAfter further development with partners in particular Logitech and within the European project MIMOSA, actively promoted and supported also by STMicroelectronics since its early stage, the technology was released to the public in October 2006 with the brand name Wibree. After negotiations with Bluetooth SIG members, an agreement was reached in June 2007 to include Wibree in a future Bluetooth specification as a Bluetooth ultra low power technology.\n\nThe technology was marketed as Bluetooth Smart and integration into version 4.0 of the Core Specification was completed in early 2010. The first smartphone to implement the 4.0 specification was the iPhone 4S, released in October 2011. A number of other manufacturers released Bluetooth Low Energy Ready devices in 2012.\n\nThe Bluetooth SIG officially unveiled Bluetooth 5 on 16 June 2016 during a media event in London. One change on the marketing side is that they dropped the point number, so it now just called Bluetooth 5 (and not Bluetooth 5.0 or 5.0 LE like for Bluetooth 4.0). This decision was made allegedly to \"simplifying marketing, and communicating user benefits more effectively\". On the technical side, Bluetooth 5 will quadruple the range by using increased transmit power or coded physical layer, double the speed by using optional half of the symbol time compared to Bluetooth 4.x, and provide an eight-fold increase in data broadcasting capacity by increasing the advertising data length of low energy Bluetooth transmissions compared to Bluetooth 4.x, which could be important for IoT applications where nodes are connected throughout a whole house.\n\nThe Bluetooth SIG released Mesh Profile and Mesh Model specifications officially on 18 July 2017. Mesh specification enables using Bluetooth Low Energy for many-to-many device communications for home automation, sensor networks and other applications.\n\nBorrowing from the original Bluetooth specification, the Bluetooth SIG defines several profiles — specifications for how a device works in a particular application — for low energy devices. Manufacturers are expected to implement the appropriate specifications for their device in order to ensure compatibility. A device may contain implementations of multiple profiles.\n\nMajority of current low energy application profiles is based on the generic attribute profile (GATT), a general specification for sending and receiving short pieces of data known as attributes over a low energy link. Bluetooth mesh profile is the exception to this rule as it is based on General Access Profile (GAP).\n\nBluetooth mesh profiles use Bluetooth Low Energy to communicate with other Bluetooth Low Energy devices in the network. Each device can pass the information forward to other Bluetooth Low Energy devices creating a \"mesh\" effect. For example, switching off an entire building of lights from a single smartphone.\n\n\nThere are many profiles for Bluetooth Low Energy devices in healthcare applications. The Continua Health Alliance consortium promotes these in cooperation with the Bluetooth SIG.\n\n\nProfiles for sporting and fitness accessories include:\n\n\n\n\n\"Electronic leash\" applications are well suited to the long battery life possible for 'always-on' devices. Manufacturers of iBeacon devices implement the appropriate specifications for their device to make use of proximity sensing capabilities supported by Apple's iOS devices.\n\nRelevant application profiles include:\n\n\n\nStarting in late 2009, Bluetooth Low Energy integrated circuit implementations were announced by a number of manufacturers. Implementations commonly use software radio so updates to the specification can be accommodated through a firmware upgrade.\n\nCurrent mobile devices are commonly released with hardware and software support for both classic Bluetooth and the Bluetooth Low Energy.\n\n\nBluetooth Low Energy technology operates in the same spectrum range (the 2.400–2.4835 GHz ISM band) as classic Bluetooth technology, but uses a different set of channels. Instead of the classic Bluetooth 79 1-MHz channels, Bluetooth Low Energy has 40 2-MHz channels. Within a channel, data is transmitted using Gaussian frequency shift modulation, similar to classic Bluetooth's Basic Rate scheme. The bit rate is 1 Mbit/s (with an option of 2 Mbit/s in Bluetooth 5), and the maximum transmit power is 10 mW (100 mW in Bluetooth 5). Further details are given in Volume 6 Part A (Physical Layer Specification) of the Bluetooth Core Specification V4.0.\n\nBluetooth Low Energy uses frequency hopping to counteract narrowband interference problems. Classic Bluetooth also uses frequency hopping but the details are different; as a result, while both FCC and ETSI classify Bluetooth technology as an FHSS scheme, Bluetooth Low Energy is classified as a system using digital modulation techniques or a direct-sequence spread spectrum.\n\nMore technical details may be obtained from official specification as published by the Bluetooth SIG. Note that power consumption is not part of the Bluetooth specification.\n\nBLE devices are detected through a procedure based on broadcasting advertising packets. This is done using 3 separate channels (frequencies), in order to reduce interference. The advertising device sends a packet on at least one of these three channels, with a repetition period called the advertising interval. For reducing the chance of multiple consecutive collisions, a random delay of up to 10 milliseconds is added to each advertising interval. The scanner listens to the channel for a duration called the scan window, which is periodically repeated every scan interval.\n\nThe discovery latency is therefore determined by a probabilistic process and depends on the three parameters (viz., the advertising interval, the scan interval and the scan window). The discovery scheme of BLE adopts a periodic-interval based technique, for which upper bounds on the discovery latency can be inferred for most parametrizations. While the discovery latencies of BLE can be approximated by models for purely periodic interval-based protocols, the random delay added to each advertising interval and the three-channel discovery can cause deviations from these predictions, or potentially lead to unbounded latencies for certain parametrizations.\n\nAll Bluetooth Low Energy devices use the Generic Attribute Profile (GATT). The application programming interface offered by a Bluetooth Low Energy aware operating system will typically be based around GATT concepts. GATT has the following terminology:\n\n\nSome service and characteristic values are used for administrative purposes – for instance, the model name and serial number can be read as standard characteristics within the \"Generic Access\" service. Services may also include other services as sub-functions; the main functions of the device are so-called \"primary\" services, and the auxiliary functions they refer to are \"secondary\" services.\n\nServices, characteristics, and descriptors are collectively referred to as \"attributes\", and identified by UUIDs. Any implementer may pick a random or pseudorandom UUID for proprietary uses, but the Bluetooth SIG have reserved a range of UUIDs (of the form \"xxxxxxxx-0000-1000-8000-00805F9B34FB\" ) for standard attributes. For efficiency, these identifiers are represented as 16-bit or 32-bit values in the protocol, rather than the 128 bits required for a full UUID. For example, the \"Device Information\" service has the short code 0x180A, rather than 0000180A-0000-1000-... . The full list is kept in the Bluetooth Assigned Numbers document online.\n\nThe GATT protocol provides a number of commands for the client to discover information about the server. These include:\n\nCommands are also provided to \"read\" (data transfer from server to client) and \"write\" (from client to server) the values of characteristics:\n\nFinally, GATT offers \"notifications\" and \"indications\". The client may request a notification for a particular characteristic from the server. The server can then send the value to the client whenever it becomes available. For instance, a temperature sensor server may notify its client every time it takes a measurement. This avoids the need for the client to poll the server, which would require the server's radio circuitry to be constantly operational.\n\nAn \"indication\" is similar to a notification, except that it requires a response from the client, as confirmation that it has received the message.\n\nBluetooth Low Energy is designed to enable devices with low power consumption. Several chipmakers including Cambridge Silicon Radio, Dialog Semiconductor, Nordic Semiconductor, STMicroelectronics, Cypress Semiconductor, Silicon Labs and Texas Instruments have introduced their Bluetooth Low Energy optimized chipsets over the last few years. Devices with peripheral and central roles have different power requirements. A study by beacon software company, Aislelabs, reported that peripherals, such as proximity beacons, usually function for 1–2 years with a 1,000mAh coin cell battery. This is possible because of power efficiency of Bluetooth Low Energy protocol which only transmits small packets as compared to Bluetooth Classic which is also suitable for audio and high bandwidth data.\n\nIn contrast, a continuous scan for the same beacons in central role can consume 1,000 mAh in a few hours. Android and iOS devices also have very different battery impact depending on type of scans and number of Bluetooth Low Energy devices in the vicinity. With the newer chipsets and advances in software, both Android and iOS phones now have negligible power consumption in real-life Bluetooth Low Energy use scenarios.\n\n\n\n"}
{"id": "2429977", "url": "https://en.wikipedia.org/wiki?curid=2429977", "title": "Box truss", "text": "Box truss\n\nA box truss is a structure composed of three or more \"chords\" connected by transverse and/or diagonal structural elements.\n\nBox trusses are commonly used in certain types of aircraft fuselages, electric power pylons, large radio antennas, and many bridge structures. (For various truss arrangements used see truss bridge.)\n\nBy using what are in effect stiff panels in a cylindrical arrangement the resulting structure can have a high resistance to axial torsion (twisting along its long axis) and a higher resistance to buckling in its highly loaded sides.\n\nWhen finished as an open structure the truss will be less subject to wind drag and to aeroelastic effects than would a completely enclosed structure.\n"}
{"id": "5898", "url": "https://en.wikipedia.org/wiki?curid=5898", "title": "Carabiner", "text": "Carabiner\n\nA carabiner () or karabiner is a specialized type of shackle, a metal loop with a spring-loaded gate used to quickly and reversibly connect components, most notably in safety-critical systems. The word is a shortened form of \"Karabinerhaken\" (or also short \"Karabiner\"), a German phrase for a \"spring hook\" used by a carbine rifleman, or carabinier, to attach items to a belt or bandolier.\n\nCarabiners, often called D-Rings by military professionals, are widely used in rope-intensive activities such as climbing, arboriculture, caving, sailing, hot air ballooning, rope rescue, construction, industrial rope work, window cleaning, whitewater rescue, and acrobatics. They are predominantly made from both steel and aluminium. Those used in sports tend to be of a lighter weight than those used in commercial applications and rope rescue. \nOften referred to as carabiner-style or as mini-biners, carabiner keyrings and other light-use clips of similar style and design have also become popular. Most are stamped with a “Not For Climbing” or similar warning due to a common lack of load-testing and safety standards in manufacturing. While from an etymological perspective any metal attaching link with a spring gate is technically a carabiner, the strict usage among the climbing community specifically refers only to those devices manufactured and tested for load-bearing in safety-critical systems like rock and mountain climbing.\n\nCarabiners on hot air balloons are used to connect the envelope to the basket and are rated at 2.5 tonne, 3 tonne or 4 tonne.\n\nLoad-bearing screw-gate carabiners are used to connect the diver's umbilical to the surface supplied diver's harness. They are usually rated for a safe working load of 5 kN or more (equivalent to a weight in excess of approximately 500kg).\n\nCarabiners come in four characteristic shapes:\n\nThere are three broad categories of carabiner: auto locking, manual locking, and non-locking.\n\nNon-locking carabiners (or \"snap-links\") have a sprung swinging gate that accepts a rope, webbing sling, or other hardware. Rock climbers frequently connect two non-locking carabiners with a short length of nylon web to create a quickdraw.\n\nTwo gate types are common:\n\n\nBoth solid and wire gate carabiners can be either 'straight gate' or 'bent gate'. Bent-gate carabiners are easier to clip a rope into using only one hand, and so are often used for the rope-end carabiner of quickdraws and alpine draws used for lead climbing.\n\nLife supporting carabiners such as those used in tree climbing need to be strong (thus strength ratings), but also secure against unintentional opening under use. All carabiners with a spring loaded gate \"are self closing\" (single action). Several are also \"self locking\" (double action), some even \"self double locking\" (triple action).\n\nLocking carabiners have the same general shape as non-locking carabiners but have an additional mechanism securing the gate. These mechanisms may be either threaded sleeves (\"screw-lock\"), spring-loaded sleeves (\"twist-lock\"), magnetic levers (\"Magnetron\"), other spring loaded unlocking levers or opposing double spring loaded gates (\"Twin-Gate\").\n\n\nCarabiners are marked on the side with single letters showing their intended area of use, for example, K (via ferrata), B (base), and H (for belaying with an Italian or Munter hitch).\n\n\n\nAmerican National Standards Institute/American Society of Safety Engineers standard ANSI Z359.1-2007 \"Safety Requirement for Personal Fall Arrest Systems, Subsystems and Components\", section 3.2.1.4 (for snap hooks and carabiners) is a voluntary consensus standard. This standard requires that all connectors/ carabiners support a minimum breaking strength (MBS) of and feature an auto-locking gate mechanism which supports a minimum breaking strength (MBS) of .\n\n\n"}
{"id": "199105", "url": "https://en.wikipedia.org/wiki?curid=199105", "title": "Cheddite", "text": "Cheddite\n\nCheddite is a class of explosive materials invented in 1897 by E. A. G. Street of the firm of Berges, Corbin et Cie and originally manufactured in the town of Chedde in Haute-Savoie, France in the early twentieth century.\n\nClosely related to Sprengel explosives, cheddites consisted of a high proportion of inorganic chlorates mixed with nitroaromatics (e.g. nitrobenzene or dinitrotoluene) plus a little paraffin or castor oil as a moderant for the chlorate. Several different types were made, and they were principally used in quarrying. Due to availability of ingredients and easy production process it was also the most common explosive material manufactured by the Polish Underground State in occupied Poland during World War II; it was used for production of the R wz. 42 and Filipinka hand grenades. \n\nSince the 1970s, Cheddite is the commercial name for an explosive compound used as an explosive primer for shotgun cartridges. It contains 90% potassium chlorate, 7% paraffin, 3% petroleum jelly, and traces of carbon black.\n"}
{"id": "664931", "url": "https://en.wikipedia.org/wiki?curid=664931", "title": "Christopher Latham Sholes", "text": "Christopher Latham Sholes\n\nChristopher Latham Sholes (February 14, 1819 – February 17, 1890) was an American inventor who invented the QWERTY keyboard, and along with Samuel W. Soule, Carlos Glidden and John Pratt, has been contended as one of the inventors of the first typewriter in the United States. He was also a newspaper publisher and Wisconsin politician.\n\nBorn in Mooresburg, in Montour County, Pennsylvania, Sholes moved to nearby Danville and worked there as an apprentice to a printer. After completing his apprenticeship, Sholes moved to Milwaukee, Wisconsin in 1837, and later to Southport, Wisconsin (present-day Kenosha). He became a newspaper publisher and politician, serving in the Wisconsin State Senate from 1848 to 1849 as a Democrat, in the Wisconsin State Assembly from 1852 to 1853 as a Free Soiler, and again in the Senate as a Republican from 1856 to 1857. He was instrumental in the successful movement to abolish capital punishment in Wisconsin; his newspaper, \"The Kenosha Telegraph\", reported on the trial of John McCaffary in 1851, and then in 1853 he led the campaign in the Wisconsin State Assembly. He was the younger brother of Charles Sholes (1816–1867), who was also a newspaper publisher and politician who served in both houses of the Wisconsin State Legislature and as mayor of Kenosha.\n\nIn 1845, Sholes was working as editor of the \"Southport Telegraph\", a small newspaper in Kenosha, Wisconsin. During this time he heard about the alleged discovery of the Voree Record, a set of three minuscule brass plates unearthed by James J. Strang, a would-be successor to Joseph Smith, founder of the Latter Day Saint movement. Strang asserted that this proved that he was a true prophet of God, and he invited the public to call upon him and see the plates for themselves. Sholes accordingly visited Strang, examined his \"Voree Record,\" and wrote an article about their meeting. He indicated that while he could not accept Strang's plates or his prophetic claims, Strang himself seemed to be \"honest and earnest\" and his disciples were \"among the most honest and intelligent men in the neighborhood.\" As for the \"record\" itself, Sholes indicated that he was \"content to have no opinion about it.\"\n\nTypewriters with various keyboards had been invented as early as 1714 by Henry Mill and have been reinvented in various forms throughout the 1800s. It is believed to be Sholes among others, who have invented the first one to be commercially successful, however many contest it and couple his inventions with that of Frank Haven Hall, Samuel W. Soule, Carlos Glidden, Giuseppe Ravizza and John Pratt.\n\nSholes had moved to Milwaukee and became the editor of a newspaper. Following a strike by compositors at his printing press, he tried building a machine for typesetting, but this was a failure and he quickly abandoned the idea. He arrived at the typewriter through a different route. His initial goal was to create a machine to number pages of a book, tickets, and so on. He began work on this at Kleinsteubers machine shop in Milwaukee, together with a fellow printer Samuel W. Soule, and they patented a numbering machine on November 13, 1866.\n\nSholes and Soule showed their machine to Carlos Glidden, a lawyer and amateur inventor at the machine shop working on a mechanical plow, who wondered if the machine could not be made to produce letters and words as well. Further inspiration came in July 1867, when Sholes came across a short note in \"Scientific American\" describing the \"Pterotype\", a prototype typewriter that had been invented by John Pratt. From the description, Sholes decided that the Pterotype was too complex and set out to make his own machine, whose name he got from the article: the \"typewriting machine\", or \"typewriter\".\n\nFor this project, Soule was again enlisted, and Glidden joined them as a third partner who provided the funds. The \"Scientific American\" article (unillustrated) had figuratively used the phrase \"literary piano\"; the first model that the trio built had a keyboard literally resembling a piano. It had black keys and white keys, laid out in two rows. It did not contain keys for the numerals 0 or 1 because the letters O and I were deemed sufficient:\n\nAt this stage, the Sholes-Glidden-Soule typewriter was only one among dozens of similar inventions. They wrote hundreds of letters on their machine to various people, one of whom was James Densmore of Meadville, Pennsylvania. Densmore foresaw that the typewriter would be highly profitable, and offered to buy a share of the patent, without even having laid eyes on the machine. The trio immediately sold him one-fourth of the patent in return for his paying all their expenses so far. When Densmore eventually examined the machine in March 1867, he declared that it was good for nothing in its current form, and urged them to start improving it. Discouraged, Soule and Glidden left the project, leaving Sholes and Densmore in sole possession of the patent.\n\nRealizing that stenographers would be among the first and most important users of the machine, and therefore best in a position to judge its suitability, they sent experimental versions to a few stenographers. The most important of them was James O. Clephane, of Washington D.C., who tried the instruments as no one else had tried them, subjecting them to such unsparing tests that he destroyed them, one after another, as fast as they could be made and sent to him. His judgments were similarly caustic, causing Sholes to lose his patience and temper. But Densmore insisted that this was exactly what they needed:\n\nSholes took this advice and set to improve the machine at every iteration, until they were satisfied that Clephane had taught them everything he could. By this time, they had manufactured 50 machines or so, at an average cost of $250. They decided to have the machine examined by an expert mechanic, who directed them to E. Remington and Sons (which later became the Remington Arms Company), manufacturers of firearms, sewing machines, and farm tools. In early 1873 they approached Remington, who decided to buy the patent from them. Sholes sold his half for $12,000, while Densmore, still a stronger believer in the machine, insisted on a royalty, which would eventually fetch him $1.5 million.\n\nSholes returned to Milwaukee and continued to work on new improvements for the typewriter throughout the 1870s, which included the QWERTY keyboard (1873). James Densmore had suggested splitting up commonly used letter combinations in order to solve a jamming problem caused by the slow method of recovering from a keystroke: weights, not springs, returned all parts to the \"rest\" position. This concept was later refined by Sholes and the resulting QWERTY layout is still used today on both typewriters and English language computer keyboards, although the jamming problem no longer exists.\n\nSholes died on February 17, 1890 after battling tuberculosis for nine years, and is buried at Forest Home Cemetery in Milwaukee.\n\n\n"}
{"id": "6551283", "url": "https://en.wikipedia.org/wiki?curid=6551283", "title": "Cyclic pump", "text": "Cyclic pump\n\nA Cyclic pump is an apparatus which moves a fluid in a periodic uni-directional direction from one containment system to another while overcoming static conditions that would, without intervention, not move. The intervention predicated by the pump alters pressures, volumes and sometimes temperatures of fluids (gasseous, liquid, colloidal, plasmic, etc.) in such a way that the fluids are transported to other chambers or enclosures (including pipes), thus \"flowing\" in a consistent direction, usually having characteristics of pulsation (as is the case with the Human heart) or of uniform motion (as is the case with an Automobile motor oil pump). Cyclic pumps are generally incorporated into machines to deal with all sorts of fluids associated with that machine's functionality.\n\n"}
{"id": "1547295", "url": "https://en.wikipedia.org/wiki?curid=1547295", "title": "Demeton-S-methyl", "text": "Demeton-S-methyl\n\nDemeton-S-methyl is an organic compound with the molecular formula CHOPS. It was used as an organothiophosphate acaricide and organothiophosphate insecticide. It is flammable. With prolonged storage, Demeton-S-methyl becomes more toxic due to formation of a sulfonium derivative which has greater affinity to the human form of the acetylcholinesterase enzyme, and this may present a hazard in agricultural use.\n\n"}
{"id": "56818738", "url": "https://en.wikipedia.org/wiki?curid=56818738", "title": "Ethylenedinitramine", "text": "Ethylenedinitramine\n\nEthylenedinitramine (EDNA)is an explosive chemical compound of the nitroamine class.\n\nEdnatol is a high explosive comprising about 58% ethylenedinitramine and 42% TNT.\n"}
{"id": "17856623", "url": "https://en.wikipedia.org/wiki?curid=17856623", "title": "Fabric structure", "text": "Fabric structure\n\nIn architecture, fabric structures are forms of constructed fibers that provide end users a variety of aesthetic free-form building designs. Custom-made fabric structures are engineered and fabricated to meet worldwide structural, flame retardant, weather-resistant, and natural force requirements.\nFabric structures are considered a sub-category of tensile structure.\n\nA fabric structure's material selection, proper design, engineering, fabrication, and installation are integral components to ensuring a sound structure.\n\nMost fabric structures are composed of actual fabric rather than meshes or films. Typically, the fabric is coated and laminated with synthetic materials for increased strength, durability, and environmental resistance. Among the most widely used materials are polyesters laminated or coated with polyvinyl chloride (PVC), and woven fiberglass coated with polytetrafluoroethylene (PTFE).\n\nThe traditional fabric for fabric structures is light cotton twill, light canvas, or heavy proofed canvas.\n\nStrength, durability, cost, and stretch make polyester material the most widely used in fabric structures. Polyesters that are laminated or coated with PVC films are usually the least expensive option for longer-term fabrications.\nLaminates generally consist of vinyl films over woven or knitted polyester meshes (called scrims or substrates), while vinyl-coated polyesters usually have a high-count, high-tensile base fabric coated with a bondable substance that provides extra strength. Precontraint fabric is made by placing the polyester fabric under tension both before and during the coating process. This results in a weave that has increased dimensional stability.\n\nA laminated fabric usually is composed of a reinforcing polyester scrim pressed between two layers of unsupported PVC film. For most fabric structure uses, however, it refers to two or more layers of fabric or film joined by heat, pressure, and an adhesive to form a single ply.\n\nWith an open-weave or mesh polyester scrim, the exterior vinyl films bond to themselves through the openings in the fabric. Heavier fabric scrims, however, are too tightly woven to allow the same bonding. In this case, an adhesive is used to bond the exterior films to the base fabric.\n\nA good chemical bond is critical to both prevention of delamination and development of seam strengths. The seam is created when vinyl-coated fabrics are welded together. The adhesive enables the seam to meet shear forces and load requirements for a structure at all temperatures. The adhesive prevents wicking of moisture into the scrim’s fibers, which also prevents fungal growth or freezing that could affect the exterior coating's adhesion to the scrim. Adhesives are water-based to comply with EPA regulations.\n\nOpen-weave scrims generally make the fabric more economical, although this can also depend on the number and type of features that you require in the vinyl. Almost any color, UV resistance vinyl coated polyester, and colorfastness may be incorporated into the vinyl. However, the more features added, the higher the cost of the fabric.\n\nVinyl coated polyester is the most frequently used material for flexible fabric structures. It is made up of a polyester scrim, a bonding or adhesive agent, and exterior PVC coatings. The scrim supports the coating (which is initially applied in liquid form) and provides the tensile strength, elongation, tear strength, and dimensional stability of the resulting fabric. Vinyl-coated polyester is manufactured in large panels by heat-sealing an over-lap seam with either a radio-frequency welder or a hot-air sealer. A proper seam will be able to carry the load requirements for the structure. The seam area should be stronger than the original coated fabric when testing for tensile strength.\n\nThe base fabric's tensile strength is determined by the size (denier) and strength (tenacity) of the yarns and the number of yarns per linear inch or meter. The larger the yarn and the more yarns per inch, the greater the finished product's tensile strength.\n\nThe adhesive agent acts as a chemical bond between the polyester fibers and the exterior coating and also prevents wicking, or fibers absorbing water, which could result in freeze-thaw damage in the fabric.\n\nThe PVC coating liquid (vinyl Organisol or Plastisol) contains chemicals to achieve the desired properties of color, water and mildew resistance, and flame retardancy. Fabric can also be manufactured that contains high levels of light transmission or can be made completely opaque. After the coating has been applied to the scrim, the fabric is put through a heating chamber that dries the liquid coating. PVC coatings are available in a range of colors, although non-standard colors can be pricey. Colors may be subject to minimum order runs that allow the coating machine to clear out traces of any previous color.\n\nWoven fiberglass coated with PTFE (Teflon or silicone) is also a widely used base material. Glass fibers are drawn into continuous filaments, which are then bundled into yarns. The yarns are woven to form a substrate. The fiberglass carries a high ultimate tensile strength, behaves elastically, and does not suffer from significant stress relaxation or creep. The PTFE coating is chemically inert, can withstand temperatures from 100 °F upwards to 450 °F+. It is also immune to radiation and can be cleaned with water. PTFE fiberglass is additionally Energy Star and Cool Roof Rating Council certified. During scientific tests of its solar properties, it was discovered that PTFE fiberglass membranes reflect as much as 73 percent of the sun’s energy while holding just seven percent on its exterior surface. Certain grades of PTFE fiberglass can absorb 14 percent of the sun’s energy while allowing 13 percent of natural daylight and seven percent of re-radiated energy (solar heat) to transmit through.\n\nBecause of its energy efficiency, high melting temperature and lack of creep, fiberglass-based fabrics have been the material of choice for stadium domes and other permanent structures, particularly in the United States. However, when properly constructed, polyester structures may be equally durable.\n\nA number of polymers consisting mainly of polyethylene, polypropylene or combinations of the two are available for fabric structures.\n\nPVDF woven fabric are available for fabric structures.\n\nePTFE woven fabric are available for fabric structures..\n\nBlackout material, also known as blockout material, is an opaque fabric. Blackout fabric consists of a laminate that sandwiches an opaque layer between two white exterior layers. Heating and lighting of a structure may be controlled because the fabric does not allow light to permeate the top or walls. The opaque quality also prevents stains, dirt, repairs, or slightly mismatched panels on the structure's exterior from being noticed from the inside.\n\nMost fabrics used for fabric structures have some form of topcoating applied to the exterior or coating to make cleaning easier. Topcoating provides a hard surface on the outside of the material, forming a barrier that aids in preventing dirt from sticking to the material, while allowing the fabric to be cleaned with water. As the material ages, the topcoating will eventually erode, exposing the fabric to dirt and making it more difficult to clean. The thicker the topcoating, the longer it will last. However, coatings that are too thick will embrittle and crack when folded.\n\nThere are several commonly used topcoatings:\n\nTITAN W exploits the most innovative nanotechnology methods for the use of particular particles in combination with fluoropolymers (PVDF) mixed with acrylic resin.\nThe effectiveness of TITAN W compared to coatings based on acrylic lacquering and PVDF has been shown by numerous tests on deterioration by atmospheric agents, both accelerated and outdoors (weathering test).\n\nThe result is a coating that gives the fabric the following advantages:\n\nWith these additional advantages, the fabrics coated with TITAN W lacquering will offer better durability compared to the existing coatings used. In addition to the above-mentioned characteristics, fabrics with TITAN W lacquer are high frequency and hot air weldable.\n\nWhen discussing fabric properties for use on a structure, there are several terms that are commonly used:\n\nWhen deciding on a fabric it is imperative to keep certain fabric properties in mind. These include stress versus strain (unit load versus unit elongation), expected service life, the mechanisms of joining the material together (welding, gluing, etc.), and the fabric’s behavior in or around fire.\n\nStress versus strain data should be obtained in both uniaxial and biaxial forms. This information characterizes the fabric in terms of stiffness, elasticity, and plasticity. This is essential information when determining the material's response under load in a load-carrying application. Shear strength, shear strain, and Poisson's ratios, though difficult to obtain, are fundamental when analyzing a fabric as a structural material.\n\nThere can be multiple advantages to fabric structures over traditional buildings in certain scenarios. In some cases, no lighting is required as the fabric used is generally translucent, which makes it an energy efficient solution. Mobility: You can move them, either on wheels or relocate them completely. Savings: They cost about half of what a traditional structure costs.\n\nFabric properties: When discussing fabric properties for use on a structure, there are several terms that are commonly used:\n\nTensile strength is a basic indicator of relative strength. It is fundamental for architectural fabrics that function primarily in tension.\nTear Strength is important in that if a fabric ruptures in place, it generally will do so by tearing. This can occur when a local stress concentration or local damage results in the failure of one yarn, which thereby increases the stress on remaining yarns.\n\nAdhesion strength is a measure of the strength of the bond between the base material and coating or film laminate that protects it. It is useful for evaluating the strength of welded joints for connecting strips of fabric into fabricated assembly.\nFlame retardancy does not have the same meaning as flameproofing. Fabric that contains a flame-retardant coating can withstand even a very hot point source. However, it can still burn if a large ignition source is present.\n\nOf course, other properties must be factored in when determining a material's suitability for a structure. To fully understand a fabric's value and usefulness, consider the following:\n\n"}
{"id": "31474404", "url": "https://en.wikipedia.org/wiki?curid=31474404", "title": "Field strength meter", "text": "Field strength meter\n\nIn telecommunications, a field strength meter is an instrument that measures the electric field strength emanating from a transmitter.\n\nIn ideal free space, the electric field strength produced by a transmitter with an isotropic radiator is readily calculated.\n\nwhere\n\nThe factor formula_2 is an approximation of formula_3\n\nwhere formula_4 formula_5 is the impedance of free space. formula_5 is the symbol for ohms.\n\nIt is clear that electric field strength is inversely proportional to the distance between the transmitter and the receiver. However, this relation is impractical for calculating the field strength produced by terrestrial transmitters, where reflections and attenuation caused by objects around the transmitter or receiver may affect the electrical field strength considerably.\n\nField strength meter is actually a simple receiver. After a tuner circuit, the signal is detected and fed to a microammeter, which is scaled in dBμ. The frequency range of the tuner is usually within the terrestrial broadcasting bands. Some FS meters can also receive satellite (TVRO and RRO) frequencies. Most modern FS meters have AF and VF circuits and can be used as standard receivers. Some FS meters are also equipped with printers to record received field strength.\n\nWhen measuring with a field strength meter it is important to use a calibrated antenna such as the standard antenna supplied with the meter. For precision measurements the antenna must be at a standard height. A value of standard height frequently employed for VHF and UHF measurements is . Gain correction tables may be provided with the meter, that take into account the change of antenna gain with frequency.\n\nThe CCIR defines the minimum field strength for satisfactory reception. These are shown in the table below. (Band II is reserved for FM radio broadcasting and the other bands are reserved for TV broadcasting.)\n"}
{"id": "50174828", "url": "https://en.wikipedia.org/wiki?curid=50174828", "title": "Frank Beauchamp", "text": "Frank Beauchamp\n\nColonel Sir Frank Beachim Beauchamp CBE (born Mells, Somerset 1866, died Worthing, West Sussex, 17 June 1950) was an industrialist who owned mines in the Somerset coalfield, notably in Midsomer Norton and Radstock. He was the first baronet of the Beauchamp Baronetcy of Woodborough, in the County of Somerset, created for him in 1918. He was also a Conservative county councillor for thirty-nine years.\n\nHe was born in 1866 in Mells, Somerset where he was educated privately. He grew up at Norton Hall (formerly Norton Down House), a now-demolished mansion in Midsomer Norton. His father, also involved in coalmines, was William Beauchamp, and his sister Rose was the sister-in-law of the composer and music administrator Sir Reginald Thatcher.\n\nIn 1897 he married Mabel Constance Bannon whose photographic portrait is part of the Lafayette Negative Archive at the Victoria & Albert Museum in London. They had four children including Sir Douglas Clifford Beauchamp, the second and last Beauchamp baronet, and Irene Mabel.\n\nHe was a magistrate, a Conservative member of Somerset County Council from 1907 to 1946 and a Parliamentary candidate in 1910. In May 1915 he joined the army and after eighteen months in Britain he went to the US in an advisory capacity to the War Department in Washington D.C.. He reached the rank of colonel and was created a baronet in 1918 and appointed a CBE in 1919. Sir Frank's business interests were centred on Radstock and Midsomer Norton where he owned a number of collieries, a coal distribution business, a wagon works and a gasworks. He was a partner with his brother Louis in the Norton Hill colliery in Midsomer Norton, chairman of the East Bristol colliery company and director of the Somerset colliery company.\n\nHe lived at Woodborough House, a now-demolished mansion between Peasedown St John and Radstock, Somerset.\n\nDue to ill health he spent his final years in Worthing, West Sussex, where he died on 17 June 1950. He is buried in the churchyard of St Peter's Church, Camerton, Somerset.\n"}
{"id": "56361", "url": "https://en.wikipedia.org/wiki?curid=56361", "title": "Freeflying", "text": "Freeflying\n\nFreeflying is a skydiving discipline which began in the late 1980s, involving freefalling in various vertical orientations, as opposed to the traditional \"belly-to-earth\" orientation. The discipline is known to have originated when Olav Zipser began experimenting with non-traditional forms of bodyflight. Zipser founded the FreeFly Clowns as a two-person competitive team with Mike Vail in 1992, and was joined by Omar Alhegelan (1st ever FAI Freestyle World Cup & World Champion), Charles Bryan, and Stefania Martinengo in 1994. The FreeFly Clowns are also credited with opening the first school to teach freeflying, The First School of Modern SkyFlying.\n\nFreeflying broke into the limelight in 1996 when the SSI Pro Tour added freeflying as a three-person competitive discipline at the second televised event (with Skysurfing), part of ESPN's Destination Extreme series. 150 countries watched the FreeFly Clowns (Olav Zipser, Charles Bryan and Omar Alhegelan) as they took 1st place in all four international competitions along with other teams like, the Flyboyz (Eli Thompson, Mike Ortiz, Knut Krecker, Fritz Pfnür), Team AirTime (Tony Urugallo, Jim O'Reilly, Peter Raymond, Brian Germain), and many other pioneers of freeflying showed off their best moves. In 1996 and 1997, the SSI Pro Tour staged eight televised events in both North America and Europe with $36,000 in cash prizes awarded to freefly teams. SSI invited the 1997 Pro World Champions, the Flyboyz, to participate in the 1998 ESPN X Games as an unofficial exhibition.\nThe resulting global television exposure made legends out of the FreeFly Clowns, the Flyboyz, and others. A once fledgling offshoot of the mainstream, freeflying now comprises fully one-half of the overall skydiving community.\n\nOlav Zipser's Space Games used the space ball as a research and measuring device to provide a constant speed and direction from which individual athletes could be trained, rated, raced against each other and judged. The Space Games took FreeFlying to the next level from 1998.\n\nThanks to the efforts by Arizona Freeflight llc (Omar Alhegelan & Kama Mountz) who ran test competitions & wrote and submitted the rules for futures competitions; In 2000 FreeFly was accepted as a skydiving discipline by the International Parachute Commission (IPC) and the first official FreeFly National & international Championships were held worldwide.\n\nFreeflying is an expansion of skydiving which includes the traditional belly-to-earth positions, but extends into vertical flight where the flyer is in an upright position (falling feet first) or in an inverted position (falling head first). These positions increase freefall speeds and make new types of formations and routines possible.\n\nA freeflyer, in order to fully understand the aerodynamic power of his/her body in freefall, needs to first learn to control all of the skydiving forms: box position (belly-to-earth, traditional skydiving position), back flying (back-to-earth), head-up flying, head-down flying, and side flying. These positions are not held for the duration of a skydive. Freeflying can, and usually does, involve constant transitions in position and speeds, with vertical and horizontal orientations. This can involve constantly flowing skydives, with all positions explored, or more static skydives where flyers are concentrating on building a large formation while flying in one of these freefly positions.\n\nDue to the increased freefall speed and potentially faster horizontal speeds, freeflying has dangers beyond that of a normal skydive. Extra care must be taken for freefall skydive groups to stay away from belly-to-earth skydivers to avoid collisions. Since most parachutes are not designed to be opened at speeds higher than that of normal belly flying, freeflyers must transition back to the \"belly to earth\" position and slow down their descent for several seconds before deploying their parachute.\n\nWhile freeflying is a younger and more extreme addition to skydiving, it is becoming a popular event in competitions and world records.\n\nBack flying is the ability to fly on the back in a stable and controlled fashion. This skill is critical so that when the flyer flips out of some of the more advanced positions they stay in control and do not endanger themselves or other skydivers.\n\nSit flying is called such because it looks similar to the position taken while sitting in a chair.\n\nFor flying a sit, the feet are oriented toward the relative wind and 90-degree bends maintained at the knees, hips, and shoulders. To move around, the flyer redirects the airflow in the opposite direction the jumper wants to go. Newtonian mechanics then push the flyer in the desired direction. Fall rate changes (descending faster or slower) can also be made.\n\nA person falling in the \"head down\" position has less cross-sectional area exposed to the air while falling, which results in much faster fall rates. Average speeds while flying head down are around . Due to the increased speed, every movement made can cause the skydiver to become unstable or disoriented; thus increasing the risk involved in skydiving.\n\nThe world's largest vertical (head down) formation took place on Friday, 31 July 2015, when a multinational team of 164 skydivers, some traveling at speeds of over 200 mph, linked over Skydive Chicago, in Ottawa, Illinois, United States. This broke the previous record of 138 linked skydivers, set on Saturday, 4 August 2012, also at Skydive Chicago.\n\nMarc Hauser set the world record for the fastest horizontal free fall at 304 km/h in Empuriabrava, Spain, without specialized equipment, in October 2012.\n\n"}
{"id": "42063265", "url": "https://en.wikipedia.org/wiki?curid=42063265", "title": "GFS Chemicals", "text": "GFS Chemicals\n\nGFS Chemicals Inc, formerly known as G. Frederick Smith Chemical Company, is a privately owned specialty chemical company with headquarters in Powell, Ohio and manufacturing facilities in Columbus, Ohio. It was founded by G. Frederick Smith in Urbana, Illinois in 1924, and moved to Columbus, Ohio in 1928.\n\nGFS Chemicals currently serves over seventy countries and a variety of industries, including: Alternative energy; energy storage; pharmaceuticals; biotechnology; electronics; etching; and environmental and research analytics. The company has approximately 100 employees. Its various divisions are managed by three separate units: Organic Specialty Materials, Inorganic Specialty Materials, and Analytical Reagents & Research Chemicals Catalog Division.\n\nThe Smith Chemical Company was started in G. Frederick Smith’s garage in Urbana, Illinois as a result of his use of magnesium perchlorate as a super drying agent. Smith enlisted the help of his brothers Allyne (who studied engineering at Ohio State) and Clarence (who worked for a local newspaper).\n\nOrders quickly outpaced their ability to fill them. In 1928 G. Frederick Smith Chemical Company moved to Columbus, Ohio on McKinley Avenue, and began to sell magnesium perchlorate under the trade name \"Dehydrite\" for A.H. Thomas Co., later Thomas Scientific. G. Frederick Smith Chemical Company would become one of the leading perchloric acid and perchlorate salt producers in the world.\n\nUniversity of Illinois graduate student Charles Getz, in an attempt to store milk anaerobically to prevent spoilage, accidentally invents the world's first aerosol dispensed product, instant whipped cream. He and G.F. Smith found nitrous oxide to be the most suitable gas and started the Aerated Products Company, later known as Instantwhip. The company was later turned over to G.F. Smith’s son Clifton, although Getz would retain the patent.\n\nSmith started producing commercial quantities of 1,10-phenanthroline and its derivatives, producing a range of indicators for use in analytical chemistry. He investigated the preparation of cerium compounds for use as titrants in oxidation and reduction reactions. Aided by phenanthroline indicators, he produced hexanitratocerate as a primary standard. Studies of periodic acid, iodic acid, and their salts prompted a new line of products.\n\nIn the 1940s, Rare earth products, such as ceric ammonium nitrate, and heteroaromatic ligands, are introduced to the product line.\n\nIn the 1960s, the company developed and produced new high purity redistilled inorganic acids in response. \n\nIn the 1960s Allyne Smith retired and turned control of G. Frederick Smith Chemical Company over to Darrell Hutchinson, his son-in-law, to run as President and CEO.\n\nIn the 70s, the company . collaborated with Motorola on the first ceric ammonium nitrate-based chrome etchants for use in early computer chips. The firm became the sole US manufacturer of perchloric acid after Hooker Occidental Petroleum) decides to abandon that field.\n\nG. Frederick Smith died in 1976.\n\nIn the 80s, Instantwhip and G. Frederick Chemical Co. parted ways following a stock swap agreement between those with ownership stakes in both of the firms.\nIn the 1990s G. Frederick Smith Chemical Co. shortened its name to GFS Chemicals, Inc. and acquired Ericsen Instruments' Karl Fischer reagent business (now the Watermark brand), and several other specialty organic product lines from Farchan Laboratories and Shawnee Chemical Company.\n\nIn the 90s GFS became ISO 9001:2008 Certified. A new plant was built specifically for the production of perchloric acid. In ___, J. Steel Hutchinson became President of GFS Chemicals.\n\nA new organic production facility was built, allowing for the expansion of previously purchased product linesn, including liquid ammonia chemistry, silanes, acetylenes, and other organics.\n\nThe company acquired APS Analytical Standards, a maker of turbidimeters, colorimeters, and spectrometers. This business was moved from California to Columbus.\n\nDistilled acids demand increased, and GFS expanded its distillation equipment. Inorganic and organic research labs were built for continued research and quality control testing.\n\nThe company attained Society of Chemical Manufacturers and Affiliates (SOCMA) Chem Steward Tier I Certification.\n\nThe firmGFS partnered with Ensign-Bickford Aerospace and Defense for the manufacture of Ammonia Borane.\n\nA Catalog Order Distribution Center was purchased on Kaderly Drive on the west side of Columbus; it now houses all GFS catalog fulfillment activities, private labeling and analytical laboratory reagents manufacturing.\n\nDue to import issues with China, GFS embarked on a research campaign and becomes the only domestically secured chain of supply for cerium.\n\nA new grade of ceric ammonium nitrate was introduced with tighter metals tolerances for use as an electronics-grade etchant component.\n\nThe American Association for Laboratory Accreditation (A2LA) expanded accreditation to GFS for ISO/IEC 17025:2005 for turbidity, pH buffers, and conductivity standards.\n\nLocated in Columbus, OH, the Inorganic manufacturing facility is the original cornerstone of GFS Chemicals, as first used by G. Frederick Smith.\n\nLocated on River Street Columbus, OH, the Organic manufacturing facility as a location is a relatively new facility. GFS has always performed organic chemistries, however after the acquisition period more space was needed to further develop the business and to explore the future scientific opportunities which could be advanced. These newer developments have at times crossed over with the Inorganic side with anhydrous Lithium Perchlorate in the form of Lithium Perchlorate-Diethyl Ether to promote synthetic organic chemical synthesis in bulk, as an example. They added a Kilo lab and a Distillation lab as well in the proceeding years.\n\nLocated on Kaderly Drive, Columbus, OH, the newest addition to the GFS landscape is also the oldest business. G. Frederick Smith was spurred to create many of the chemical compounds, especially perchlorates and trace metal perchloric acid, in response to the needs of his colleagues in analytical chemistry. This tradition is continued here and is the most commonly associated part of the business when a chemist or researcher hears of GFS Chemicals. The trace metal acids are especially popular among researchers who are looking to digest organics for study without the risk of metal contamination. Recently the American Association of Laboratory Accreditation expanded the accreditation to meet ISO/IEC 17025:2005 for the standards and buffers produced here as well as EPA approval for the AMCO Clear® line of turbidity standards.\n"}
{"id": "53769731", "url": "https://en.wikipedia.org/wiki?curid=53769731", "title": "Harman Becker Automotive Systems", "text": "Harman Becker Automotive Systems\n\nHarman Becker Automotive Systems GmbH, commonly known as Becker, is a part of the car division of the American manufacturing company, Harman International Industries, a subsidiary of South Korean company Samsung Electronics.\n\nThe present company goes back to the German car radio and navigation systems manufacturer, Becker. This firm was founded in 1949 from a repair workshop in the Baden town of Pforzheim. Its founder was Max Egon Becker. In 1995, the US concern, Harman International, took over the firm.\n\nThe company, with its head office in Karlsbad near Karlsruhe and other bases in the USA and Hungary developed and integrated complete infotainment systems worldwide. Its product range runs from navigation systemes, voice control and HMIs to audio and entertainment technologies.\nFrom its earliest days, \"Harman Becker Automotive Systems\" was a supplier to Mercedes-Benz, but also supplies marques such as Audi, Porsche, Peugeot, Hyundai, Ferrari, Rolls-Royce, BMW and Mini. Worldwide, \"Harman Becker\" has 28 bases in the following countries: Germany, USA, Great Britain, France, Sweden, Hungary, Canada, Mexico, South Africa (to 2008), Japan, South Korea and China. Since 11 January 2010 Harman Becker Automotive Systems has pulled out of the market for mobile navigation. The trademarks Becker Traffic Assist, Becker Traffic Assist Pro etc. were given to United Navigation. Under the latter's roof, the brands Falk and Becker continue to run.\n\nSince 2008, as part of the strategy of its parent concern, Harman International, the company's divisions have been increasingly based in low-wage economies, so that the number of employees in its German bases has dropped from 3,800 in 2008 to 2,250 in 2013. The bases of Hechingen, Villingen-Schwenningen, Schaidt and Hamburg (Innovative Systems, sold in 2008 to Neusoft China as \"Neusoft Technology Solutions\") were closed down or sold off.\n\nIn November 2016, it is understood that Samsung Electronics has been acquired Harman for eight billion US dollars. The acquisition was completed on March 10, 2017.\n\n"}
{"id": "17366380", "url": "https://en.wikipedia.org/wiki?curid=17366380", "title": "Hasee", "text": "Hasee\n\nHasee Computer Company, Ltd. () is a Chinese personal computer manufacturer headquartered in Shenzhen, Guangdong, China. It is the second largest Chinese computer maker.\n\nHasee claims it was founded in 1995 as a manufacturer of graphics cards, but a 2008 \"The New York Times\" article places its beginnings in the early 2000s. In addition to its domestic market, Hasee products are sold worldwide.\n\nProducts include no frills systems sold at low prices. In 2003, some of its desktop models were referred to as \"among the cheapest on the [Chinese] market,\" and in 2008 a Hasee laptop could be purchased for little more than US$370. C. 2010, Hasee calls some of its products \"competitively priced,\" and claims it desires to \"provide the world with more affordable... computing products...\"\n\nHasee's products include laptops, desktops, smartphones, tablets, and panel PCs. In the mid-2000s, Hasee manufactured its own motherboards, and c. 2010 the company states motherboard manufacture continues.\n\nHasee's subsidiaries include Shenzhen Hasee Computer Co Ltd, Shenzhen Paradise Science and Technology Co Ltd, Shenzhen Hass IC Co Ltd, Shenzhen Creative Science and Technology Co Ltd, Hasee Electronics Fty, and Shenzhen Paradise Advertisement Co Ltd.\n\nFacilities include 230,000 sq meters in Hasee Industrial Park located in Bantian, Shenzhen, and the total floor-space of all Hasee facilities was estimated to be 400,000 sq meters in 2004.\n\nProduction bases, as of 2004, include a site in Longgang, Shenzhen.\n\nWhite box (computer hardware)\n\n"}
{"id": "2377089", "url": "https://en.wikipedia.org/wiki?curid=2377089", "title": "Hexamethylene triperoxide diamine", "text": "Hexamethylene triperoxide diamine\n\nHexamethylene triperoxide diamine (HMTD) is a high explosive organic compound. HMTD is an organic peroxide, a heterocyclic compound with a cage-like structure. It is a primary explosive. It has been considered as an initiating explosive for blasting caps in the early part of 20th century, mostly because of its high initiating power (higher than that of mercury fulminate) and its inexpensive production. As such, it was quickly taken up as a primary explosive in mining applications. However, it has since been superseded by more (chemically) stable compounds such as dextrinated lead azide and DDNP (which contains no lead or mercury). HMTD is widely used in amateur made blasting caps.\n\nFirst synthesised in 1885 by Legler, HMTD may be prepared by the reaction of an aqueous solution of hydrogen peroxide and hexamine in the presence of citric acid, acetic acid or dilute sulfuric acid as a catalyst. The hydrogen peroxide needs to be at least 12% w/w, lower concetrations lead to poor yields. Citric acid is overall superior to other acids. Common synthetic procedure uses 5 ml of 30% hydrogen peroxide, 2 g of citric acid and 1 g of hexamine with about 50% yields.\nThe molecule adopts a cage-like structure.\n\nNo peroxide has found practical use as an explosive as a consequence of the weak oxygen–oxygen bond, which leads to poor thermal and chemical stability and a high sensitivity to shock (physical impact). Like other organic peroxides such as acetone peroxide (TATP), HMTD is unstable and detonates upon shock, friction, static electricity discharges, concentrated sulfuric acid, strong UV radiation and heat. Cases of detonation caused by the simple act of screwing a lid on a jar containing HMTD have been reported. Common static electricity discharges have been reported to cause detonation. It is, however, less unstable than many other peroxides under normal conditions; exposure to ultraviolet light increases its sensitivity. It also reacts with most common metals, which can lead to detonation. HMTD is chemically very stable when pure (free of acids, bases, and metal ions) and does not quickly sublime like its acetone counterparts. Like all primary explosives, HMTD should be handled without any direct contact with hands or any part of human body in a grounded, static electricity controlled area. Only minimal amounts of HMTD should be handled at any one time.\n\nHMTD is a more powerful initiating explosive than mercury fulminate, but its poor thermal and chemical stability prevents its use in detonators. Neverthelss, HMTD is one of the three most widely used primary explosives in improvised, amateur made blasting caps. The other being TATP and SA.DS.\n\nHMTD is a common source of injury among amateur chemists, particularly finger amputations. Most of these injuries are caused by small amounts of HMTD that inadvertently detonate in close proximity of fingers, since small amounts (grams) are generally not powerful enough to amputate fingers from distances larger than 5 – 10 cm. Experienced amateurs handle HMTD in such a manner as to avoid any close contact between fingers and the explosive itself, from synthesis to final detonation. Such measures, for example, include using multiple filter papers during the filtration step that are exchanged as not to have more than 0.2 g of HMTD on a single filter paper, pre-bent papers with cotton wrapped wooden rods for manipulation and blast mitigation devices for final filling.\n\nCalculated (Explo5) detonation pressure P at crystal density 1.597 g/cm is 218 kbar with velocity of detonation VoD = 7777 m/s. Explosion temperature is 3141 K, energy of explosion is 5612 kJ/kg (or 3400 - 4000 kJ/kg per various sources) and volume of explosion gases at STP is calculated to be 826 l/kg. Loose powder has density close to 0.4 g/cm, hence the common detonation velocities are closer to 3000 m/s and P is closer to 15 kbar.\n\nHMTD is overall slightly more sensitive than fresh TATP and can be considered to be slightly more dangerous than an average primary explosive. It is important to note that the variance of friction force between different surfaces (e.g. different kinds of paper) is often greater than the variance between the friction sensitivity of a given pair of primary explosives. This leads to different values for friction sensitivity measured at different laboratories.\n\nDespite no longer being used in any military application, and despite its shock sensitivity, HMTD remains a common home-made explosive and has been used in a large number of suicide bombings and other attacks throughout the world. For example, it was one of the components in the explosives intended to bomb Los Angeles International Airport in the 2000 millennium attack plots\nand the 2016 New York and New Jersey bombings.\n"}
{"id": "43475691", "url": "https://en.wikipedia.org/wiki?curid=43475691", "title": "Klebanoff–Saric Wind Tunnel", "text": "Klebanoff–Saric Wind Tunnel\n\nThe Klebanoff–Saric Wind Tunnel (KSWT) is a low-speed, low-disturbance wind tunnel located at Texas A&M University. This facility is mainly used to study laminar-turbulent boundary layer transition by means of flat-plate and swept-wing experiments. Measurement techniques used include hotwire anemometry, infrared thermography, and naphthalene flow visualization.\n\nOriginally built by Dr. Phillip Klebanoff in 1970 at the National Bureau of Standards in Maryland, this tunnel was later moved by Dr. William Saric to Arizona State University in 1984, and it was then moved by Saric to Texas A&M University in 2005.\n"}
{"id": "5338695", "url": "https://en.wikipedia.org/wiki?curid=5338695", "title": "Lead user", "text": "Lead user\n\nLead user is a term developed by Eric von Hippel in 1986(). His definition for lead user is:\n\nIn other words, lead users are users of a product or service that currently experience needs still unknown to the public and who also benefit greatly if they obtain a solution to these needs. Because lead users innovate, they are considered to be one example or type of the creative consumer phenomenon, that is, those \"customers who adapt, modify, or transform a proprietary offering\" ().\n\nThe Lead User Method is a market research tool that may be used by companies and / or individuals seeking to develop breakthrough products. Lead User methodology was originally developed by Dr. Eric von Hippel of the Massachusetts Institute of Technology (MIT) and first described in the July 1986 issue of Management Science. In contrast to the traditional market research techniques that collect information from the users at the center of the target market, the Lead User method takes a different approach, collecting information about both needs and solutions from the leading edges of the target market and from analogue markets, markets facing similar problems in a more extreme form.\n\nThe methodology involves four major steps: \n\nThe methodology is based upon the idea that breakthrough products may be developed by identifying leading trends in the to-be-developed product’s associated marketplace(s). Once the trend or broader problem to be solved has been identified, the developers seek out “Lead Users”- people or organizations that are attempting to solve a particularly extreme or demanding version of the stated problem.\n\nFor example, a company seeking to create a breakthrough in flashlight design may seek out policemen, home inspectors, or others who require bright, efficient lights as part of their day-to-day business. Once these “lead users” have been identified, networking is employed and the lead users are interviewed so as to gain their insight into how they solve the problem for themselves. The lead users are also queried to determine whether they have knowledge of individuals or organizations who are considered to be “outside the market” and have even more extreme portable lighting needs than the policemen or home inspectors; in our example, these users might be photographers, divers, or movie lighting designers. (See the “Examples of Lead User Method” section of this article for more examples of lead user identification.) By learning from both the lead users and the outside-the-market users, companies may identify new methods or approaches towards creating innovative products that are true breakthroughs via ideas that may not have surfaced by simply examining existing users with traditional market research techniques.\n\nResearch on lead users emerged from studies on sources of innovation. It was first found that users (as opposed to manufacturers) are often the first to develop new products that are commercially successful ( , ). Additionally, it was found that innovation by users tended to be concentrated among the “lead users” of those products and processes (, , , , ). These “lead users” were individuals or organizations who had experienced needs for a given innovation earlier than the majority of the target market (). Recent research highlights the fact that lead users exist for services also (Skiba and Herstatt 2009, Skiba 2010, Oliveira and Von Hippel 2011).\n\nVarious studies have explored the effectiveness of this theory in terms of identifying any user innovations. The effect found in these studies tends to be very large; for example, Urban and Von Hippel (1988) found that 82 percent of a given lead-user cluster had developed their own version of, or had modified a specific type of, the industrial product under study… whereas only 1 percent of the non-lead users had done this.\nEmpirical studies have also found that many of the innovations developed by users have commercial attractiveness. For example, Urban and Von Hippel (1988) found that lead user theory can be effectively utilized in industrial software product development; Morrison, Roberts, and Von Hippel (2000) found that many IT innovations developed by libraries had broader potential value; and Luthje (2003) found that 48 percent of surgical innovations developed by surgeons in university clinics in Germany could be produced as commercial products.\n\nBased on its widespread success, it has been suggested that the lead user methodology should be integrated into corporate new product development efforts (Urban and Von Hippel, 1988). Companies may benefit (to a large extent) as they try to learn from lead users about the needs and solutions encountered at the leading edge of the market. Increasingly, this type of customer integration is being discussed among innovation management scholars (Enkel, Javier, and Gassmann, 2005; Luthje and Herstatt, 2004). The idea is also spreading rapidly in the business world (Coyne, 2000; Dehne, 2003; Intrachooto, 2004); for example, lead-user concepts developed and used at 3M showed product sales potential that was an average of eight times higher than for sales of products using more traditional development concepts / processes (Lilien et al., 2002).\n\nWhile the lead user methodology has proven to be very successful, select literature highlights some product development scenarios in which the Lead User method may be less effective. For example, the following was pointed out on October 14, 2007 on “TechITEasy.org”:\n\n\nLiterature also suggests that an additional obstacle to the adoption of this kind of process is related to a general resistance to innovation and / or change that can be found in typically bureaucratic organizations; these organizations tend to resist disruptive changes in processes which many force the company to evolve, (although this is exactly the purpose of such an approach). While the lead user methodology can reliably lead to breakthroughs, adopting the approach can be difficult for some organizations and on the whole, the technique itself is useful to the extent that the product and / or service under study is lead user friendly (i.e. if it’s not a top-secret or quick time-to-market idea). [need reference to the mention literature]\n\nThe lead user method can be utilized in any industry and at any level of product complexity. The following are examples where the Lead User method was utilized to create a new product which satisfied a specific need:\n\n3M\n\nThe lead user method was utilized in 3M’s Medical-Surgical Division to develop a breakthrough surgical drape product. 3M assembled a team of lead users which included a veterinarian surgeon, a makeup artist, doctors from developing countries and military medics.\n\nHilti AG\n\nHilti utilized the lead user method to develop a simplified pipe hanger. Hilti put together a lead user group consisting of lead layout engineers, researchers from construction departments of institutes, an engineer from a professional organization in Bonn, and two engineers from municipal building departments.\n\nNortel\n\nNortel utilized the lead user method to develop a new class of web applications for voice, video and data. Nortel put together a group of lead users including law enforcement professionals, paramedics, military personnel, animal trackers and professional storm trackers.\n\nSense Worldwide\n\nSense Worldwide has been featured in Wired Magazine and Fast Company for using Lead Users such as dominatrices, Nigerian hackers, medical tourists and OCD sufferers in their innovation work.\n\nLocal Motors\n\nLocal Motors is the first car company to utilize the lead user method to co-create vehicles online with its virtual community of designers, fabricators, engineers and enthusiasts. The world’s first vehicle produced using co-creative method is the Local Motors Rally Fighter.\n\n\nWebsites\n\nPapers\n"}
{"id": "53135", "url": "https://en.wikipedia.org/wiki?curid=53135", "title": "Magnetic anomaly detector", "text": "Magnetic anomaly detector\n\nA magnetic anomaly detector (MAD) is an instrument used to detect minute variations in the Earth's magnetic field. The term refers specifically to magnetometers used by military forces to detect submarines (a mass of ferromagnetic material creates a detectable disturbance in the magnetic field); military MAD equipment is a descendent of geomagnetic survey or aeromagnetic survey instruments used to search for minerals by detecting their disturbance of the normal earth-field.\n\nGeoexploration by measuring and studying variations in the Earth's magnetic field has been conducted by scientists since 1843. The first uses of magnetometers were for the location of ore deposits. Thalen's \"The Examination of Iron Ore Deposits by Magnetic Measurements\", published in 1879, was the first scientific treatise describing this practical use.\n\nMagnetic anomaly detectors employed to detect submarines during World War II harnessed the fluxgate magnetometer, an inexpensive and easy to use technology developed in the 1930s by Victor Vacquier of Gulf Oil for finding ore deposits. MAD gear was used by both Japanese and U.S. anti-submarine forces, either towed by ship or mounted in aircraft to detect shallow submerged enemy submarines. The Japanese called the technology \"jikitanchiki\" (磁気探知機, \"Magnetic Detector\"). After the war, the U.S. Navy continued to develop MAD gear as a parallel development with sonar detection technologies.\n\nSatellite, near-surface and oceanic data from detectors was used to create the World Digital Magnetic Anomaly Map published by the Commission for the Geological Map of the World (CGMW) in July 2007.\n\nTo reduce interference from electrical equipment or metal in the fuselage of the aircraft, the MAD sensor is placed at the end of a boom or on a towed aerodynamic device. Even so, the submarine must be very near the aircraft's position and close to the sea surface for detection of the anomaly, because magnetic fields decrease as the inverse cube of distance. The size of the submarine, its hull composition and orientation determine the detection range. MAD devices are usually mounted on aircraft.\n\nFor aeromagnetic survey applications the magnetic sensor can be mounted on an aircraft (typically on a long probe in front of or behind the aircraft to reduce the magnetic effects of the aircraft itself) or in a towed device. A chart is produced that geologists and geophysicists can study to determine the distribution and concentration of magnetic minerals which are related to geology and mineral deposits..\n\n"}
{"id": "778440", "url": "https://en.wikipedia.org/wiki?curid=778440", "title": "Medical identification tag", "text": "Medical identification tag\n\nA medical identification tag is a small emblem or tag worn on a bracelet, neck chain, or on the clothing bearing a message that the wearer has an important medical condition that might require immediate attention. The tag is often made out of stainless steel or sterling silver. The intention is to alert a paramedic, physician, emergency department personnel or other first responders (emergency medical services, community first responder, Emergency medical responder) of the condition even if the wearer is not conscious enough, old enough, or too injured to explain. A wallet card with the same information may be used instead of or along with a tag, and a stick-on medical ID tag may be added or used alone.\n\nA type of medic identification alert is the USB medical alert tag, essentially a USB flash drive with capacity to store a great deal of emergency information, including contacts and medical conditions. This information is accessible by any computer with a USB port. However, the practical effectiveness of such a system is limited in many cases by medical computer systems that restrict the use of USB devices which may carry malware. It is also possible that a device carried by an unconscious person may not be their own, or not be up to date, with concomitant risks to health and legal liability of medical personnel.\n\nAnother new type of medic identification alert is QR code based medical alert stickers. The QR code on the sticker links to a web service that contains the individual's emergency information. The information is accessed by any first responder or emergency personnel by scanning the QR code by using a smartphone. Since a web service is used to store the information there is normally no limitation of how much information that can be stored.\n\nTypical conditions warranting wearing of such a tag are:\n\nIn addition to mention of the relevant medical condition(s), the tag may have a telephone number that medical personnel can call for more information, for example that of physician, care-giver or next of kin. Where applicable and provided, the wearer's national health service user number can enable access to a more detailed case history. Basically, the medical information tag, engraved with the wearer's personal medical problem or history, speak for the wearer when the wearer can't. Incidentally and where the symptoms can mislead, such a tag may also be useful as evidence of such a condition to law enforcement personnel.\n\nThere are various types of medical ID available. The most common form of medical ID is jewelry which provides a logo or inscription indicating a particular medical condition. These medical identification tags can be made out of stainless steel (usually classified as 316L and known as surgical stainless steel), sterling silver or gold. If found by emergency personnel the inscription provides an indication of your special medical needs. Tags are available with pre-engraved conditions or can be custom engraved with your specific medical histories and have the benefit of that all information is self-contained and does not require any form of technology to view in case of an emergency.\n\nAnother type of medical ID jewelry indicates membership in a medical information organization such as the MedicAlert Foundation, and American Medical ID. Such medical ID jewelry includes a member identification number and a toll-free number for medical emergency personnel to contact the organization and obtain full information about the wearer's medical conditions, treatment, and history. These organizations maintain a database of medical information on their members and can provide it to medical personnel when requested.\n\nThe newest technology allows the user to carry stickers with an NFC Tag. A similar technology allows the user to carry stickers with a QR code. By scanning the NFC Tag or the QR code with a smartphone, you will reach the stored medical alert information. Apple's IOS 8 operating system includes the facility for a mobile phone to contain the owner's medical emergency information.\n\nSilicone bracelets, preprinted with a general medical condition or allergy, are also popular. The lack of personalization may be a deterrent. Recently patients have begun to \"tattoo\" their medical condition on their wrist or arm. Although a permanent tattoo might be considered, a temporary tattoo works as well. Other items include stick on tags that stick onto a driver's license, wallet, or cell phone which are practical for the person who does not want to carry something extra advertising their medical condition.\n\nAnother type of medical jewelry is a pendant or wrist strap containing a wireless alert button, also known as a panic button, worn in the home as part of a wireless medical alert system. This type of medical jewelry sends a signal to a dialing console which contacts a medical alarm monitoring service or directly dials first responders when an emergency occurs.\n\nDevices marked \"ICE\" which can hold a significant amount of data and are readable by a computer are sold, typically USB flash drives with password-protected data entry providing read-only access to emergency medical data. However, it has been pointed out by a staff nurse with experience in trauma and critical care that such devices are worse than useless, at least in most situations in the UK, as medical computer systems are designed not to accept USB storage devices due to the risk of computer viruses. Additionally, there is no guarantee that ICE information even pertains to an unconscious person carrying it; using incorrect information can lead to patient harm and legal liability.\n\n"}
{"id": "729876", "url": "https://en.wikipedia.org/wiki?curid=729876", "title": "Microreactor", "text": "Microreactor\n\nA microreactor or microstructured reactor or microchannel reactor is a device in which chemical reactions take place in a confinement with typical lateral dimensions below 1 mm;\nthe most typical form of such confinement are microchannels. Microreactors are studied in the field of micro process engineering, together with other devices (such as micro heat exchangers) in which physical processes occur. The microreactor is usually a continuous flow reactor (contrast with/to a batch reactor). Microreactors offer many advantages over conventional scale reactors, including vast improvements in energy efficiency, reaction speed and yield, safety, reliability, scalability, on-site/on-demand production, and a much finer degree of process control.\n\nGas-phase microreactors have a long history but those involving liquids started to appear in the late 1990s. One of the first microreactors with embedded high performance heat exchangers were made in the early 1990s by the Central Experimentation Department (\"Hauptabteilung Versuchstechnik\", HVT) of Forschungszentrum Karlsruhe\nin Germany, using mechanical micromachining techniques that were a spinoff from the manufacture of separation nozzles for uranium enrichment. As research on nuclear technology was drastically reduced in Germany, microstructured heat exchangers were investigated for their application in handling highly exothermic and dangerous chemical reactions. This new concept, known by names as microreaction technology or micro process engineering, was further developed by various research institutions. An early example from 1997 involved that of azo couplings in a pyrex reactor with channel dimensions 90 micrometres deep and 190 micrometres wide.\n\nUsing microreactors is somewhat different from using a glass vessel. These reactors may be a valuable tool in the hands of an experienced chemist or reaction engineer:\n\n\n\nOne of the simplest forms of a microreactor is a 'T' reactor. A 'T' shape is etched into a plate with a depth that may be 40 micrometres and a width of 100 micrometres: the etched path is turned into a tube by sealing a flat plate over the top of the etched groove. The cover plate has three holes that align to the top-left, top-right, and bottom of the 'T' so that fluids can be added and removed. A solution of reagent 'A' is pumped into the top left of the 'T' and solution 'B' is pumped into the top right of the 'T'. If the pumping rate is the same, the components meet at the top of the vertical part of the 'T' and begin to mix and react as they go down the trunk of the 'T'. A solution of product is removed at the base of the 'T'.\n\nMicroreactors can be used to synthesise material more effectively than current batch techniques allow. The benefits here are primarily enabled by the mass transfer, thermodynamics, and high surface area to volume ratio environment as well as engineering advantages in handling unstable intermediates. Microreactors are applied in combination with photochemistry, electrosynthesis, multicomponent reactions and polymerization (for example that of butyl acrylate). It can involve liquid-liquid systems but also solid-liquid systems with for example the channel walls coated with a heterogeneous catalyst. Synthesis is also combined with online purification of the product. Following Green Chemistry principles, microreactors can be used to synthesize and purify extremely reactive Organometallic Compounds for ALD and CVD applications, with improved safety in operations and higher purity products.\n\nIn microreactor studies a Knoevenagel condensation was performed with the channel coated with a zeolite catalyst layer which also serves to remove water generated in the reaction. The same reaction was performed in a microreactor covered by polymer brushes.\nA Suzuki reaction was examined in another study with a palladium catalyst confined in a polymer network of polyacrylamide and a triarylphosphine formed by interfacial polymerization:\n\nThe combustion of propane was demonstrated to occur at temperatures as low as 300 °C in a microchannel setup filled up with an aluminum oxide lattice coated with a platinum / molybdenum catalyst:\n\nEnzymes immobilized on solid supports are increasingly used for greener, more sustainable chemical transformation processes. Microreactors are used to study enzyme-catalyzed ring-opening polymerization of ε-caprolactone to polycaprolactone. A novel microreactor design developed by Bhangale et al. enabled to perform heterogeneous reactions in continuous mode, in organic media, and at elevated temperatures. Using microreactors, enabled faster polymerization and higher molecular mass compared to using batch reactors. It is evident that similar microreactor based platforms can readily be extended to other enzyme-based systems, for example, high-throughput screening of new enzymes and to precision measurements of new processes where continuous flow mode is preferred. This is the first reported demonstration of a solid supported enzyme-catalyzed polymerization reaction in continuous mode.\n\nMicroreactors can also enable experiments to be performed at a far lower scale and far higher experimental rates than currently possible in batch production, while not collecting the physical experimental output. The benefits here are primarily derived from the low operating scale, and the integration of the required sensor technologies to allow high quality understanding of an experiment. The integration of the required synthesis, purification and analytical capabilities is impractical when operating outside of a microfluidic context.\n\nResearchers at the Radboud University Nijmegen and Twente University, the Netherlands, have developed a microfluidic high-resolution NMR flow probe. They have shown a model reaction being followed in real-time. The combination of the uncompromised (sub-Hz) resolution and a low sample volume can prove to be a valuable tool for flow chemistry.\n\nMettler Toledo and Bruker Optics offer dedicated equipment for monitoring, with attenuated total reflectance spectrometry (ATR spectrometry) in microreaction setups. The former has been demonstrated for reaction monitoring. The latter has been successfully used for reaction monitoring and determining dispersion characteristics of a microreactor.\n\nMicroreactors, and more generally, micro process engineering, are the subject of worldwide academic research. A prominent recurring conference is IMRET, the \"International Conference on Microreaction Technology\". Microreactors and micro process engineering have also been featured in dedicated sessions of other conferences, such as the Annual Meeting of the American Institute of Chemical Engineers (AIChE), or the International Symposia on Chemical Reaction Engineering (ISCRE). Research is now also conducted at various academic institutions around the world, e.g. at the Massachusetts Institute of Technology (MIT) in Cambridge/MA, University of Illinois Urbana-Champaign, Oregon State University in Corvallis/OR, at University of California, Berkeley in Berkeley/CA in the United States, at the EPFL in Lausanne, Switzerland, at Eindhoven University of Technology in Eindhoven, at Radboud University Nijmegen in Nijmegen, Netherlands and at the LIPHT of Université de Strasbourg in Strasbourg and of the University of Lyon, CPE Lyon, France.\n\nDepending on the application focus, there are various hardware suppliers and commercial development entities to service the evolving market. One view to technically segment market, offering and market clearing stems from the scientific and technological objective of market agents:\n"}
{"id": "1530070", "url": "https://en.wikipedia.org/wiki?curid=1530070", "title": "NT (cassette)", "text": "NT (cassette)\n\nNT is a digital memo recording system introduced by Sony in 1992, sometimes marketed under the name Scoopman. The system stored memos using helical scanning on special microcassettes, which were 30 × 21.5 × 5 mm with a tape width of 2.5 mm, with a recording capacity of up to 120 minutes. The Scoopmen cassettes are offered in three versions: The Sony NTC-60, -90, and -120, each describing the length of time (in minutes) the cassette can record.\n\nNT stands for \"Non-Tracking\", meaning the head does not precisely follow the tracks on the tape. Instead, the head moves over the tape at approximately the correct angle and speed, but performs more than one pass over each track. The data in each track is stored on the tape in blocks with addressing information that enables reconstruction in memory from several passes. This considerably reduced the required mechanical precision, reducing the complexity, size, and cost of the recorder.\n\nAnother feature of NT cassettes is \"Non-Loading\", which means instead of having a mechanism to pull the tape out of the cassette and wrap it around the drum, the drum is pushed inside the cassette to achieve the same effect. This also significantly reduces the complexity, size, and cost of the mechanism.\n\nAudio sampling is in stereo at 32 kHz with 12 bit nonlinear quantization, corresponding to 17 bit linear quantization. Data written to the tape is packed into data blocks and encoded with LDM-2 low deviation modulation.\n\nThe Sony NT-2 Digital Micro Recorder, introduced in 1996 and shown here, features a real-time clock that records a time signal on the digital track along with the sound data, making it useful for journalism, police and legal work. Due to the machine's buffer memory, it is capable of automatically reversing the tape direction at the end of the reel without an interruption in the sound. The recorder uses a single \"AA\"-size cell for primary power, plus a separate CR-1220 lithium cell to provide continuous power to the real-time clock. The Sony NT-2, an improved successor to the Sony NT-1 Digital Micro Recorder, introduced in 1992, was the final machine in the series. The NT cassette systems cost more than a DAT recorder in their day, are fragile and relatively unreliable compared to other emerging recording technologies, and being unable to compete soon disappeared from the market. The devices are considered curiosities for collectors.\n\n"}
{"id": "11994909", "url": "https://en.wikipedia.org/wiki?curid=11994909", "title": "Nasolabial fold", "text": "Nasolabial fold\n\nThe nasolabial folds, commonly known as \"smile lines\" or \"laugh lines\", are facial features. They are the two skin folds that run from each side of the nose to the corners of the mouth. They are defined by facial structures that support the buccal fat pad. They separate the cheeks from the upper lip. The term derives from Latin \"nasus\" for \"nose\" and \"labium\" for \"lip\".\n\nWith ageing the fold may grow in length and depth. Dermal fillings may be used to replace lost fats and collagen in this facial area.\n\n"}
{"id": "7583854", "url": "https://en.wikipedia.org/wiki?curid=7583854", "title": "Occupancy sensor", "text": "Occupancy sensor\n\nAn occupancy sensor is an indoor motion detecting devices used to detect the presence of a person to automatically control lights or temperature or ventilation systems. The sensors use infrared, ultrasonic, microwave, or other technology. The term encompasses devices as different as PIR sensors, hotel room keycard locks and smart meters. Occupancy sensors are typically used to save energy, provide automatic control, and comply with building codes.\n\nA vacancy sensor works like an occupancy sensor, however, lights must be manually turned ON, but will automatically turn OFF when motion is no longer detected.\n\nOccupancy sensor types include:\n\nMotion sensors are often used in indoor spaces to control electric lighting. If no motion is detected, it is assumed that the space is empty, and thus does not need to be lit. Turning off the lights in such circumstances can save substantial amounts of energy. In lighting practice occupancy sensors are sometime also called \"presence sensors\" or \"vacancy sensors\". Some occupancy sensors (e.g. LSG's Pixelview, Philips Lumimotion, Ecoamicatechs Sirius etc.) also classify the number of occupants, their direction of motion, etc., through image processing. Pixelview is a camera-based occupancy sensor, using a camera that is built into each light fixture.\n\nOccupancy sensors for lighting control typically use infrared (IR), ultrasonic, tomographic motion detection, microwave sensors, or camera-based sensors (image processing). The field of view of the sensor must be carefully selected/adjusted so that it responds only to motion in the space served by the controlled lighting. For example, an occupancy sensor controlling lights in an office should not detect motion in the corridor outside the office. Tomographic motion detection systems have the unique benefit of detecting motion through walls and obstructions, yet do not trigger as easily from motion on the outside of the detection area like traditional microwave sensors.\n\nSensors and their placement are never perfect, therefore most systems incorporate a delay time before switching. This delay time is often user-selectable, but a typical default value is 15 minutes. This means that the sensor must detect no motion for the entire delay time before the lights are switched. Most systems switch lights off at the end of the delay time, but more sophisticated systems with dimming technology reduce lighting slowly to a minimum level (or zero) over several minutes, to minimize the potential disruption in adjacent spaces. If lights are off and an occupant re-enters a space, most current systems switch lights back on when motion is detected. However, systems designed to switch lights off automatically with no occupancy, and that require the occupant to switch lights on when they re-enter are gaining in popularity due to their potential for increased energy savings. These savings accrue because in a spaces with access to daylight the occupant may decide on their return that they no longer require supplemental electric light.\n\n"}
{"id": "22663184", "url": "https://en.wikipedia.org/wiki?curid=22663184", "title": "Positive (photography)", "text": "Positive (photography)\n\nA positive is a film or paper record of a scene that represents the color and luminance of objects in that scene with the same colors and luminances (as near as the medium will allow). Color transparencies are an example of positive photography: the range of colors presented in the medium is limited by the tonal range of the original image (dark and light areas correspond). It is opposed to a negative where colors and luminances are reversed: this is due to the chemical or electrical processes involved in recording the scene. Positives can be turned into negatives by appropriate chemical or electronic processes. Often, with the use of digital imaging, computers can automatically complete this process.\n\nimage area : light transfer from image area & non image area are opaque .<positive>\n-ve making : image area are opaque and non image transparent .<negative>\n"}
{"id": "56461420", "url": "https://en.wikipedia.org/wiki?curid=56461420", "title": "Quarterhill", "text": "Quarterhill\n\nQuarterhill Inc. (formerly WiLan Inc.) is a Canadian public technology holding company, based in Ottawa, Ontario. It was founded in 1992 as a wireless technology company. In the mid-2000s, it gradually transitioned into a patent licensing company. In 2017, it renamed itself Quarterhill, and is attempting to become a holding company specializing in the Internet of Things. It is listed on the Toronto Stock Exchange and NASDAQ.\n\nWiLan was founded in 1992 by Hatim Zaghloul and Michel Fattouche to commercialize their Wideband Orthogonal Frequency Division Multiplexing (WOFDM) technology. It was originally based in Calgary, Alberta, and the name was an acronym for \"Wireless Local Area Network\". In March 1998, WiLAN had its initial public offering on the Alberta Stock Exchange at $2.50 per share (it later listed on the Toronto Stock Exchange). WiLAN enjoyed significant success in the context of the tech bubble at the time, but its fortunes changed when the bubble collapsed. From October 1999 to March 2000, WiLan's share price increased from $10 to $94, but it fell back down to $12 by November 2000. By February 2006, the price had fallen to 66 cents.\n\nIn 2005, WiLAN reached a significant patent liscensing deal with Cisco Systems. In 2006 WiLAN divested its various technology product lines to refocus its business on licensing intellectual property and patent rights. As part of this change of strategy, it replaced it CEO, Bill Dunbar, by its current CEO Jim Skippen. Founder Hatim Zaghloul, who disagreed with the company's change of strategy, resigned from the board. At the same time, the company moved its headquarters from Calgary to Ottawa.\n\nThe company's change in strategy was initially successful, and it managed to sign a $50 million license deal with Nokia, allowing Nokia to use WiLAN's wireless patents. From 2006 to 2011, WiLAN expanded its portfolio of patents from 20 to 3,000. WiLAN in turn licensed out this patented technology to companies such as Nokia, LG Electronics, Samsung, Fujitsu, RIM, Intel, Broadcom and Panasonic. From 2011 to 2015, WiLAN experienced significant tailwinds, as a less favorable legal climate led to licencees less willing to pay for patents. Of particular note was WiLAN's 2013 loss of a patent lawsuit against Apple Inc.\n\nin June 2015, WiLAN announced it had signed a multi-year licensing agreement with Samsung, allowing Samsung to use technology in WiLAN's new Qimonda portfolio. In late 2015, WiLAN announced that, beginning in October 2015, it would undergo a significant company restructuring. As part of this restructuring, WiLAN announced that it would spin off its research and development unit and cut its dividend to shareholders. The restructuring would affect 30 per cent of WiLAN's workforce. The company would also focus on licensing patent portfolios owned by other companies and helping companies monetize their patents.\n\nIn April 2017, WiLAN acquired International Road Dynamics, a Saskatoon-based road traffic management system engineering company, for $63.5 million. In May 2017, it acquired VIZIYA, a software services provider, for $40 million. These acquisitions were part of a broader restructuring at WiLAN from a patent licensing company to a more diversified technology holding company, specializing in the industrial Internet of Things. As part of the restructuring, WiLan was renamed Quarterhill, although the Will Inc., was created, which contains WiLan Inc., International Road Dynamics, and VIZIYA as subsidiaries.\n\nQuarterhill's principal business is patent licensing, through its WiLAN subsidiary. Its business model is to acquire technology patents, and then license them to companies who use them. The model relies on the threat of legal action against companies who use its patents without permission. It has thus sometimes been described as a patent troll, because it profits from patents, but does not commercialize or develop technologies itself. As of 2015, WiLAN had more than 250 licencees.\n\n"}
{"id": "2075016", "url": "https://en.wikipedia.org/wiki?curid=2075016", "title": "Recuyell of the Historyes of Troye", "text": "Recuyell of the Historyes of Troye\n\nRecuyell of the Historyes of Troye or Recueil des Histoires de Troye (1464) is a translation by William Caxton of a French courtly romance written by Raoul Lefèvre, chaplain to Philip III, Duke of Burgundy. It was the first book printed in the English language.\n\n\"Recuyell\" (\"recueil\" in Modern French) simply means \"collection\" in English. Hence, the work in Modern English would read \"A Collection of the Histories of Troy\". Caxton's translations and sometimes his titles incorporated words from other European languages.\n\nCaxton, probably with the assistance of Colard Mansion and Johann Veldener, printed his translation in 1473 or 1474 (traditionally \"ca. 1475\") in Bruges. Just 18 copies still exist, and when the Duke of Northumberland sold one in 2014, it fetched more than £1 million.\n\nA presentation copy of the first edition with a specially made engraving showing Caxton presenting the book to Margaret of York is now in the Huntington Library, California, having previously been in the collections of the Duke of Roxburghe and the Duke of Devonshire. This royal \"patronage\" may have been more a form of advertising than a representation of traditional medieval patronage relationships.\n\nThe English translation forms the source for the late Tudor morality play \"Horestes\" (1567).\n\n\n"}
{"id": "45251171", "url": "https://en.wikipedia.org/wiki?curid=45251171", "title": "Red Pitaya (hardware)", "text": "Red Pitaya (hardware)\n\nRed Pitaya is an open-source hardware project intended to be alternative for many expensive laboratory measurement and control instruments.\n\nThe core selling point is inclusion of 2x 125MS/s RF input and 2x 125MS/s RF outputs, with 50 MHz analogue bandwidth and 14 bit analog-to-digital and digital-to-analog converters. The software includes oscilloscope, spectrum analyzer, signal generator, LCR meter (the LCR add on costs an additional 400 euros), and 50 MHz 2x2 MIMO PID controller. It can be re-programmed to become other devices, as all the IO ports are connected to a common field-programmable gate array (FPGA). There are also auxiliary ADC (250kS/s) and digital IO.\n\nIt has three USB 2.0 ports, Wifi, Ethernet connector. Internally, it uses Linux as operating system. The mass storage device for the operating system is a micro-SD card.\n\nDue to the wide bandwidth of the ADC and DAC, the Red Pitaya can be used as a software defined radio receiver and transmitter and in other radio frequency applications. HAMLAB, a fully featured SDR HF tranceiver with an output power of 10 W based on the Red Pitaya board is expected to be released in the amateur radio market in October 2016.\n\nAlthough the software (including HDL source code) for this project is made freely available, the device is not a fully Open Source Hardware project, because the device's electrical schematics are not made openly available.\n\n\n"}
{"id": "32680683", "url": "https://en.wikipedia.org/wiki?curid=32680683", "title": "Sascha Meinrath", "text": "Sascha Meinrath\n\nSascha Meinrath, an Internet freedom activist holds the Palmer Chair in Telecommunications at Penn State University. He is the founder of X-Lab, a future-focused technology policy and innovation project, and promotes the \"Internet in a Suitcase\" effort to create \"ad hoc\" mesh wireless technologies. Meinrath founded the Open Technology Institute in 2008 and directed the Institute while also serving as Vice President of the New America Foundation. He is also the co-founder and executive director of the CUWiN Foundation, a non-profit launched in 2000 that aims to develop \"decentralized, community-owned networks that foster democratic cultures and local content,\" and in 2007 founded the Open Source Wireless Coalition, \"a global partnership of open source wireless integrators, researchers, implementors and companies dedicated to the development of open source, interoperable, low-cost wireless network technologies.\" In 2012 he was elected as an Global Fellow for leading support for Internet freedom in the United States and around the globe, as well as named to Newsweek's Digital Power Index Top 100 influencers among other “public servants defining digital regulatory boundaries” for his efforts to develop open-source, low-cost community wireless networks and his role in fighting Stop Online Piracy Act (SOPA) and the Protect IP Act (PIPA). In 2013 Time named Meinrath to the TIME Tech 40: The Most Influential Minds in Tech for his work to protect Internet freedom.\n\nMeinrath was born in New Haven, Connecticut. He received his Bachelor of Arts in Psychology from Yale University in 1997, and a Masters of Arts in Social-Ecological Psychology from the University of Illinois at Urbana-Champaign.\n\nIn 2004 Meinrath worked as a policy analyst for Free Press, a national media reform organization. In 2007 he moved to Washington, D.C., to become the Research Director of the Wireless Futures Program at the New America Foundation. \n\nMeinrath launched the Open Technology Institute at the New America Foundation in 2008 to \"serve as a hub of impartial research, open discourse, innovative fieldwork, and new tech development\". Although based in Washington, DC, staff extend to both coast of the United States as well as advisors and fellows in Europe. Major projects include Measurement Lab and Commotion Wireless. In naming Meinrath to the Digital Power Index Top 100 Influencers Newsweek highlighted the Open Technology Institute's efforts to develop open-source, low-cost community wireless networks, particularly in underserved areas.\n\nTogether with Google and a wide range of academics, researchers and institutions, Meinrath launched Measurement Lab (M-Lab), an open, distributed server platform for researchers to deploy Internet measurement tools founded in 2009. The project has grown to have 99 servers at two-dozen locations around the globe supporting a range of broadband and computer networking measurement tools. The largest measurement platform of its kind in the world, Measurement Lab currently collects 500 Gigabytes of data daily. All the data collected by M-Lab is made available to the research community.\n\nCommotion, is an open source “device-as-infrastructure” communication platform that integrates users’ existing cell phones, Wi-Fi enabled computers, and other wireless-capable devices to create community- and metro-scale, peer-to-peer communications networks. The project builds on existing mesh wireless technologies and gained widespread attention when, in 2011, the State Department announced funding for Commotion to lower barriers for building distributed communications networks. The project has been described as the \"Internet in a Suitcase\" by the New York Times.\"Internet in a Suitcase\". Community wireless networks have been deployed with local community organizations in communities such as Philadelphia, Detroit and Brooklyn in the United States as well as Dahanu and Dharamshala, India, and Somaliland, Ethiopia, Additionally, Commotion was deployed with Occupy DC as well in the aftermath of Hurricane Sandy.\n\nMeinrath was a leading voice against the Stop Online Piracy Act (SOPA) and the Protect IP Act (PIPA). He highlighted the human rights concerns raised by legislation including the likely collective punishment resulting from empowering law enforcement to take down an entire domain due to something posted on a single blog, as well as the implications for Internet freedom policies. In naming Meinrath to their Digital Power Index Top 100 Influencers, Newsweek noted his role as “one of the more prominent Internet culture leaders” to fight against the Stop Online Piracy Act (SOPA) and PROTECT IP Act. Following the defeat of SOPA and PIPA, Meinrath hosted the Washington, DC launch party for the Internet Defense League.\n\nMeinrath hosts the regular International Summit for Community Wireless Networks (IS4CWN), a convening of leaders in community networks, mesh networking, and next-generation wireless technologies. The first summit was held in Urbana-Champaign, Illinois in 2004 launching the community wireless movement. Past locations have also included St. Charles, Missouri, Washington, DC, and Vienna, Austria. The eighth and most recent IS4CWN was held in October, 2013 in Berlin, Germany.\n\n\n"}
{"id": "3629534", "url": "https://en.wikipedia.org/wiki?curid=3629534", "title": "Sheraton style", "text": "Sheraton style\n\nSheraton is a late 18th-century neoclassical English furniture style, in vogue \"ca\" 1785 - 1820, that was coined by 19th century collectors and dealers to credit furniture designer Thomas Sheraton, born in Stockton-on-Tees, England in 1751 and whose books, \"The Cabinet Dictionary\" (1803) of engraved designs and the \"Cabinet Maker's & Upholsterer's Drawing Book\" (1791) of furniture patterns exemplify this style. \n\nThe Sheraton style was inspired by the Louis XVI style and features round tapered legs, fluting and most notably contrasting veneer inlays. Sheraton style furniture takes lightweight rectilinear forms, using satinwood, mahogany and tulipwood, sycamore and rosewood for inlaid decorations, though painted finishes and brass fittings are also to be found. Swags, husks, flutings, festoons, and rams' heads are amongst the common motifs applied to pieces of this style.\n\nWithout pedantic archaeology, it brought the Neoclassical taste of architects like Robert Adam within reach of the middle class. In many respects Sheraton style corresponds with the contemporary Directoire style of France. The Sheraton style was the most reproduced style in the United States during the Federal period.\n\n"}
{"id": "2052772", "url": "https://en.wikipedia.org/wiki?curid=2052772", "title": "Solenoid valve", "text": "Solenoid valve\n\nA solenoid valve is an electromechanical device in which the solenoid uses an electric current to generate a magnetic field and thereby operate a mechanism which regulates the opening of fluid flow in a valve.\n\nSolenoid valves differ in the characteristics of the electric current they use, the strength of the magnetic field they generate, the mechanism they use to regulate the fluid, and the type and characteristics of fluid they control. The mechanism varies from linear action, plunger-type actuators to pivoted-armature actuators and rocker actuators. The valve can use a two-port design to regulate a flow or use a three or more port design to switch flows between ports. Multiple solenoid valves can be placed together on a manifold.\n\nSolenoid valves are the most frequently used control elements in fluidics. Their tasks are to shut off, release, dose, distribute or mix fluids. They are found in many application areas. Solenoids offer fast and safe switching, high reliability, long service life, good medium compatibility of the materials used, low control power and compact design.\n\nThere are many valve design variations. Ordinary valves can have many ports and fluid paths. A 2-way valve, for example, has 2 ports; if the valve is open, then the two ports are connected and fluid may flow between the ports; if the valve is closed, then ports are isolated. If the valve is open when the solenoid is not energized, then the valve is termed normally open (N.O.). Similarly, if the valve is closed when the solenoid is not energized, then the valve is termed normally closed. There are also 3-way and more complicated designs. A 3-way valve has 3 ports; it connects one port to either of the two other ports (typically a supply port and an exhaust port).\n\nSolenoid valves are also characterized by how they operate. A small solenoid can generate a limited force. If that force is sufficient to open and close the valve, then a direct acting solenoid valve is possible. An approximate relationship between the required solenoid force \"F\", the fluid pressure \"P\", and the orifice area \"A\" for a direct acting solenoid valve is:\n\nWhere \"d\" is the orifice diameter. A typical solenoid force might be . An application might be a low pressure (e.g., ) gas with a small orifice diameter (e.g., for an orifice area of and approximate force of ).\nWhen high pressures and large orifices are encountered, then high forces are required. To generate those forces, an internally piloted solenoid valve design may be possible. In such a design, the line pressure is used to generate the high valve forces; a small solenoid controls how the line pressure is used. Internally piloted valves are used in dishwashers and irrigation systems where the fluid is water, the pressure might be and the orifice diameter might be .\n\nIn some solenoid valves the solenoid acts directly on the main valve. Others use a small, complete solenoid valve, known as a pilot, to actuate a larger valve. While the second type is actually a solenoid valve combined with a pneumatically actuated valve, they are sold and packaged as a single unit referred to as a solenoid valve. Piloted valves require much less power to control, but they are noticeably slower. Piloted solenoids usually need full power at all times to open and stay open, where a direct acting solenoid may only need full power for a short period of time to open it, and only low power to hold it.\n\nA direct acting solenoid valve typically operates in 5 to 10 milliseconds. The operation time of a piloted valve depends on its size; typical values are 15 to 150 milliseconds.\n\nPower consumption and supply requirements of the solenoid vary with application, being primarily determined by fluid pressure and line diameter. For example, a popular 3/4\" 150 psi sprinkler valve, intended for 24 VAC (50 - 60 Hz) residential systems, has a momentary inrush of 7.2 VA, and a holding power requirement of 4.6 VA. Comparatively, an industrial 1/2\" 10000 psi valve, intended for 12, 24, or 120 VAC systems in high pressure fluid and cryogenic applications, has an inrush of 300 VA and a holding power of 22 VA. Neither valve lists a minimum pressure required to remain closed in the un-powered state.\n\nWhile there are multiple design variants, the following is a detailed breakdown of a typical solenoid valve design.\n\nA solenoid valve has two main parts: the solenoid and the valve. The solenoid converts electrical energy into mechanical energy which, in turn, opens or closes the valve mechanically. A direct acting valve has only a small flow circuit, shown within section E of this diagram (this section is mentioned below as a pilot valve). In this example, a diaphragm piloted valve multiplies this small pilot flow, by using it to control the flow through a much larger orifice. \n\nSolenoid valves may use metal seals or rubber seals, and may also have electrical interfaces to allow for easy control. A spring may be used to hold the valve opened (normally open) or closed (normally closed) while the valve is not activated.\nThe diagram to the right shows the design of a basic valve, controlling the flow of water in this example. At the top figure is the valve in its closed state. The water under pressure enters at A. B is an elastic diaphragm and above it is a weak spring pushing it down. The diaphragm has a pinhole through its center which allows a very small amount of water to flow through it. This water fills the cavity C on the other side of the diaphragm so that pressure is equal on both sides of the diaphragm, however the compressed spring supplies a net downward force. The spring is weak and is only able to close the inlet because water pressure is equalized on both sides of the diaphragm.\n\nOnce the diaphragm closes the valve, the pressure on the outlet side of its bottom is reduced, and the greater pressure above holds it even more firmly closed. Thus, the spring is irrelevant to holding the valve closed.\n\nThe above all works because the small drain passage D was blocked by a pin which is the armature of the solenoid E and which is pushed down by a spring. If current is passed through the solenoid, the pin is withdrawn via magnetic force, and the water in chamber \"C\" drains out the passage \"D\" faster than the pinhole can refill it. The pressure in chamber C drops and the incoming pressure lifts the diaphragm, thus opening the main valve. Water now flows directly from A to F. \n\nWhen the solenoid is again deactivated and the passage D is closed again, the spring needs very little force to push the diaphragm down again and the main valve closes. In practice there is often no separate spring; the elastomer diaphragm is molded so that it functions as its own spring, preferring to be in the closed shape.\n\nFrom this explanation it can be seen that this type of valve relies on a differential of pressure between input and output as the pressure at the input must always be greater than the pressure at the output for it to work. Should the pressure at the output, for any reason, rise above that of the input then the valve would open regardless of the state of the solenoid and pilot valve.\n\nSolenoid valve designs have many variations and challenges.\n\nCommon components of a solenoid valve:\n\nThe core or plunger is the magnetic component that moves when the solenoid is energized. The core is coaxial with the solenoid. The core's movement will make or break the seals that control the movement of the fluid. When the coil is not energized, springs will hold the core in its normal position.\n\nThe plugnut is also coaxial.\n\nThe core tube contains and guides the core. It also retains the plugnut and may seal the fluid. To optimize the movement of the core, the core tube needs to be nonmagnetic. If the core tube were magnetic, then it would offer a shunt path for the field lines. In some designs, the core tube is an enclosed metal shell produced by deep drawing. Such a design simplifies the sealing problems because the fluid cannot escape from the enclosure, but the design also increases the magnetic path resistance because the magnetic path must traverse the thickness of the core tube twice: once near the plugnut and once near the core. In some other designs, the core tube is not closed but rather an open tube that slips over one end of the plugnut. To retain the plugnut, the tube might be crimped to the plugnut. An O-ring seal between the tube and the plugnut will prevent the fluid from escaping.\n\nThe solenoid coil consists of many turns of copper wire that surround the core tube and induce the movement of the core. The coil is often encapsulated in epoxy. The coil also has an iron frame that provides a low magnetic path resistance.\nThe valve body must be compatible with the fluid; common materials are brass, stainless steel, aluminum, and plastic.\n\nThe seals must be compatible with the fluid.\n\nTo simplify the sealing issues, the plugnut, core, springs, shading ring, and other components are often exposed to the fluid, so they must be compatible as well. The requirements present some special problems. The core tube needs to be non-magnetic to pass the solenoid's field through to the plugnut and the core. The plugnut and core need a material with good magnetic properties such as iron, but iron is prone to corrosion. Stainless steels can be used because they come in both magnetic and non-magnetic varieties. For example, a solenoid valve might use 304 stainless steel for the body, 305 stainless steel for the core tube, 302 stainless steel for the springs, and 430 F stainless steel (a magnetic stainless steel) for the core and plugnut.\n\nMany variations are possible on the basic, one-way, one-solenoid valve described above:\n\nSolenoid valves are used in fluid power pneumatic and hydraulic systems, to control cylinders, fluid power motors or larger industrial valves. Automatic irrigation sprinkler systems also use solenoid valves with an automatic controller. Domestic washing machines and dishwashers use solenoid valves to control water entry into the machine. They are also often used in paintball gun triggers to actuate the CO2 hammer valve. Solenoid valves are usually referred to simply as \"solenoids.\" \n\nSolenoid valves can be used for a wide array of industrial applications, including general on-off control, calibration and test stands, pilot plant control loops, process control systems, and various original equipment manufacturer applications. \n\nIn 1910, ASCO Numatics became the first company to develop and manufacture the solenoid valve.\n\n\n\n"}
{"id": "17730660", "url": "https://en.wikipedia.org/wiki?curid=17730660", "title": "Solving the E-waste Problem", "text": "Solving the E-waste Problem\n\nSolving the E-waste Problem (StEP) is an international initiative, created to develop solutions to address issues associated with Waste Electrical and Electronic Equipment (WEEE). \nSome of the most eminent players in the fields of Production, Reuse and Recycling of Electrical and Electronic Equipment (EEE), government agencies and NGOs as well as UN Organisations count themselves among its members. StEP encourages the collaboration of all stakeholders connected with e-waste, emphasising a holistic, scientific yet applicable approach to the problem.\n\nWaste Electrical and Electronic Equipment (WEEE) is increasing every day. The volume of WEEE is becoming a serious environmental problem that has yet to become recognised by the greater public. To guarantee the neutrality required to give analysis and recommendations the necessary credibility, StEP has been started.\nAfter a starting period of three years, initiated by the United Nations University UNU, promotion team wetzlar and Hewlett-Packard, the StEP Initiative had its official launch in March 2007.\n\n“One of the most important aims of the StEP Initiative is to elaborate a set of global guidelines for the treatment of e-waste and the promotion of sustainable material recycling” Press communiqué of the initiative \n\nThe initiative comprises five cooperating task forces, each addressing specific aspects of e-waste, while covering the entire life-cycle of electric and electronic equipment. In all its activities, the initiative places emphasis on working with policy-making bodies to allow results from its research to impact current practices. StEP is being coordinated by the science and research body of the UN System, the United Nations University (UNU).\nThe long-term goal of StEP “is to develop – based on scientific analysis – a globally accepted standard for the refurbishment, recycling of e-waste. Herewith, StEP’s aim is to reduce dangers to humans and the environment, which result from inadequate and irresponsible treatment practices, and advance resource efficiency.” (Ruediger Kuehr, Executive Secretary of the StEP Initiative). To achieve this, StEP conceives and implements projects based on the results of multidisciplinary dialogues. The projects seek to develop sustainable solutions that reduce environmental risk and enhance development.\n\nThe supreme body of the StEP Initiative is its general assembly, which decides its general direction and development. This general assembly is based on a memorandum of understanding, which is signed by all members and states the guiding principles of StEP. A Secretariat, hosted by the UNU in Bonn, is mandated with the accomplishment of the day-to-day managerial work of the initiative. A steering committee, composed of representatives from key stakeholders, monitors the progress of the initiative.\nThe core work is accomplished by the five task forces (TF): “Policy”, “ReDesign”, “ReUse”, “ReCycle” and “Capacity Building”. These task forces conduct research and analysis in their respective domains and seek to implement innovative projects.\n\nTF 1 – Policy: The aim of this task force is to assess and analyse current governmental approaches and regulations related to WEEE. Starting from this analysis, recommendations for future regulating activities shall be formulated.\n\nTF2 – ReDesign: This task force works on the design of EEE, focusing on the reduction of negative consequences of electrical and electronic appliances throughout their entire life cycle. The task force especially takes heed of the situation in developing countries.\n\nTF3 – ReUse: The focus of this task force lies in the development of sustainable, transmissible principles and standards for the reuse of EEE.\n\nTF4 – ReCycle: The objective of this task force is to improve infrastructures, systems and technologies to realize a sustainable recycling on a global level.\n\nTF5 – Capacity Building: The aim of this task force is to draw attention to the problems connected to WEEE. This aim shall be achieved by making the results of the research of the task forces and other stakeholders publicly available. In doing so, the task force relies on personal networks, the internet, collaborative working tools etc. \n\nGuiding Principles\n\n\"\"1. StEP’s work is founded on scientific assessments and incorporates a comprehensive view of the social, environmental and economic aspects of e-waste.\n\n\"2. StEP conducts research on the entire life-cycle of electronic and electrical equipment and their corresponding global supply, process and material flows.\"\n\n\"3. StEP’s research and pilot projects are meant to contribute to the solution of e-waste problems.\"\n\n\"4. StEP condemns all illegal activities related to e-waste including illegal shipments and reuse/ recycling practices that are harmful to the environment and human health.\"\n\n\"5. StEP seeks to foster safe and eco/energy-efficient reuse and recycling practices around the globe in a socially responsible manner.\"\"\n\n- Quote from the website: -\n\n\n"}
{"id": "68040", "url": "https://en.wikipedia.org/wiki?curid=68040", "title": "Standard Industrial Classification", "text": "Standard Industrial Classification\n\nThe Standard Industrial Classification (SIC) is a system for classifying industries by a four-digit code. Established in the United States in 1937, it is used by government agencies to classify industry areas. The SIC system is also used by agencies in other countries, e.g., by the United Kingdom's Companies House.\n\nIn the United States the SIC code is being supplanted by the six-digit North American Industry Classification System (NAICS code), which was released in 1997; however certain government departments and agencies, such as the U.S. Securities and Exchange Commission (SEC), still use the SIC codes.\n\nThe SIC codes can be grouped into progressively broader industry classifications: industry group, major group, and division. The first 3 digits of the SIC code indicate the industry group, and the first two digits indicate the major group. Each division encompasses a range of SIC codes: \n\nTo look at a particular example of the hierarchy, SIC code 2024 (ice cream and frozen desserts) belongs to industry group 202 (dairy products), which is part of major group 20 (food and kindred products), which belongs to the division of manufacturing.\n\nIn the early 1900s, each branch of a United States government agency would conduct business analysis using its own methods and metrics, unknown and meaningless to other branches. In the 1930s, the government needed standardized and meaningful ways in which to measure, analyze and share data across its various agencies. Thus, the Standard Industrial Classification system was born. SIC codes are four-digit numerical representations of major businesses and industries. SIC codes are assigned based on common characteristics shared in the products, services, production and delivery system of a business.\n\nSIC codes have a hierarchical, top-down structure that begins with general characteristics and narrows down to the specifics. The first two digits of the code represent the major industry sector to which a business belongs. The third and fourth digits describe the sub-classification of the business group and specialization, respectively. For example, \"36\" refers to a business that deals in \"Electronic and Other Equipment.\" Adding \"7\" as a third digit to get \"367\" indicates that the business operates in \"Electronic, Component and Accessories.\" The fourth digit distinguishes the specific industry sector, so a code of \"3672\" indicates that the business is concerned with \"Printed Circuit Boards.\"\n\nThe U.S. Census Bureau, Bureau of Labor Statistics, Internal Revenue Service and Social Security Administration utilize SIC codes in their reporting, although SIC codes are also used in academic and business sectors. The Bureau of Labor Statistics updates the codes every three years and uses SIC to report on work force, wages and pricing issues. The Social Security Administration assigns SIC codes to businesses based on the descriptions provided by employers under the primary business activity entry on employer ID applications.\n\nOver the years, the U.S. Census has identified three major limitations to using the SIC system. The first limitation surrounds its definition and mistaken classification of employee groups. For example, administrative assistants in the automotive industry support all levels of the business, yet the SIC defines these employees as part of the \"Basic Sector\" of manufacturing jobs when they should be reported as \"Non-Basic.\" Secondly, SIC codes were developed for traditional industries prior to 1970. Business has changed considerably since then from manufacturing-based to mostly service-based. As a result, and thirdly the SIC has been slow to recognize new and emerging industries, such as those in the technology sector.\n\nThe Office of Management and Budget, or OMB, was tasked with revising the SIC system to reflect changing economic conditions. The OMB established the Economic Classification Policy Committee in 1992 to develop a new system representative of the current industrial climate. The result was the North American Industrial Classification System, or NAICS, a collaborative effort between Canada, the U.S. and Mexico. NAICS expanded the four-digit SIC code to a six-digit code, and it provided more flexibility in handling emerging industries. The new code was implemented in Canada and the United States in 1997 and in Mexico one year later.\n\nThe following table is from the SEC's website, which allows searching for companies by SIC code in its database of filings. The acronym NEC stands for \"not elsewhere classified\".\n\n\n"}
{"id": "4106274", "url": "https://en.wikipedia.org/wiki?curid=4106274", "title": "Standard components (food processing)", "text": "Standard components (food processing)\n\nStandard components is a food technology term, when manufacturers buy in a standard component they would use a pre-made product in the production of their food.\nThey help products to be the same in consistency, they are quick and easy to use in batch production of food products.\nSome examples are pre-made stock cubes, marzipan, icing, ready made pastry.\n\nManufacturers use standard components as they save time and sometimes cost a lot less and it also helps with consistency in products.\nIf a manufacturer is to use a standard component from another supplier it is essential that a precise and accurate specification is produced by the manufacturer so that the component meets the standards set by the manufacturer.\n\n\n"}
{"id": "2576676", "url": "https://en.wikipedia.org/wiki?curid=2576676", "title": "Steelcase", "text": "Steelcase\n\nSteelcase is a United States-based furniture company founded in 1912 in Grand Rapids, Michigan. The company produces office furniture, architectural and technology products for office environments and the education, health care and retail industries. It is the largest office furniture manufacturer in the world. It has facilities, offices, and factories in the Americas, Europe, Asia, the Middle East, Australia and Africa.\n\nSteelcase was founded as the Metal Office Furniture Company in 1912 by Peter M. Wege in Grand Rapids, Michigan. Wege was a veteran of the sheet metal and fireproofing industry and had filed approximately 25 patents prior to starting the company. The Metal Office Furniture Company's first products included fireproof metal safes and four-drawer filing cabinets, which Wege invented and manufactured for the Macey Company.\n\nIn 1914, the company received its first product patent for \"The Victor,\" a fireproof steel wastebasket. The Victor gained popularity due to its light weight—achieved through a patented process of bending flat steel at right angles to create boxes—and its ability to prevent fires at a time when smoking was common indoors, particularly in the workplace. In 1915, the company began manufacturing and distributing steel desks after designing and producing 200 for Boston's first skyscraper, the Custom House Tower. In 1937, the company collaborated with Frank Lloyd Wright on office furniture for the Johnson Wax Headquarters. The partnership lasted two years and resulted in some of the first modern workstations.\n\nThe name Steelcase was a result of an advertising campaign to promote metal office furniture over wood and was trademarked in 1921. The company officially changed its name to Steelcase, Inc. in 1954.\n\nThe company became known as the leader of the furniture industry in the late 1960s due to the volume of its sales. Steelcase expanded into new markets during the 1970s, including Asia, Europe, and North Africa. In 1973, the company debuted the Series 9000 furniture line, an office system that became a best seller and the company's flagship brand. That same year, the company delivered the largest single furniture shipment to the then-new Sears Tower. The delivery included 43,565 pieces of furniture and furnished 44 floors.\n\nDuring the 1980s and 1990s, Steelcase shifted its approach to designing and creating products in response to changes in the workplace and economy. The company also focused on creativity and internal innovation by working closely with architects and interior designers to develop products as well the company's own workspace in Grand Rapids. The company's current headquarters were built in 1983 on 901 44th St. SE in Grand Rapids, Michigan. In 1989, Steelcase opened the pyramid-shaped Steelcase Inc. Corporate Development Center. The center contained ten research laboratories and workspaces meant to encourage interdisciplinary collaboration on product development. Steelcase vacated the Pyramid in 2010, and the Pyramid was sold to Switch (company) in 2016. In 1996, Steelcase became the majority stakeholder in design firm IDEO and the firm's CEO, David M. Kelley, became Steelcase's vice president of technical discovery and innovation. Steelcase sold its shares back to IDEO's managers starting in 2007.\n\nIn 1996, Steelcase was found at fault in a patent infringement suit brought against them by Haworth, Inc., another furniture company. Steelcase was ordered to pay $211.5 million in damages and interest, thus ending a 17-year dispute with Haworth.\n\nSteelcase became a publicly traded company in 1998 under the symbol SCS. During the 2000s, Steelcase reorganized its workforce and began integrating modern technologies in its products. In 2000, the company opened Steelcase University, a center for ongoing employee development and learning. Steelcase's wood furniture plant in Caledonia, MI earned LEED certification in 2001, becoming the first plant to receive the certification. In 2002, Steelcase partnered with IBM to create BlueSpace, a \"smart office\" prototype designed using new office technologies. In 2010, Steelcase and IDEO launched new models for higher education classrooms called LearnLabs.\n\nSteelcase is a publicly traded furniture manufacturer headquartered in Grand Rapids, Michigan. It is the largest office furniture manufacturer in the world. The company has approximately 80 locations and 11,000 employees worldwide. Its locations include Global Business Centers in Mexico, Malaysia, and Romania that provide support to local Steelcase dealerships and offices; showrooms called WorkLife Centers across the United States, Europe, and Asia, and in Toronto, Mexico City, Sydney, Melbourne, and Dubai; and manufacturing facilities in North America, Europe, the Middle East, and Asia.\n\nThe company designs and produces furniture, furniture systems, architectural products, textiles, wall surfaces, and ergonomic and technology tools for workspaces, education, and health care. Its products are sold online and through approximately 800 dealer locations. The company also offers workplace consulting services.\n\nJames Keane has served as CEO and president of the company since March 2014. The company's revenue equaled US$3 billion in 2014, an increase from $2.9 billion in 2013. Steelcase ranked 753 on the Fortune 1000 list in 2014.\n\nThe three main brands of Steelcase Inc. are Steelcase, Coalesse, and turnstone. The company also has several other brands and subsidiaries, including Designtex, PolyVision, and Steelcase Health and Education.\n\nSteelcase acquired Designtex in 1988. Designtex offers interior textiles and upholstery. The company established its office accessories brand, Details, in 1990. In 1993, the company launched Turnstone, a line of furniture designed for small businesses and home offices. PolyVision was acquired in 2001 and makes light-weight ceramic steel surfaces used for writing, projection, architecture, and interiors. In 2006, Nurture was founded to create products for the health care industry, including furniture and interiors for waiting rooms, offices, and clinics. The brand became Steelcase Health in 2014. Steelcase merged three of its subsidiaries, Brayton International, Metro Furniture and Vecta to form Coalesse in 2008. Coalesse products are meant for what the company calls \"live/work” spaces, a result of the frequent overlap of home and office in modern working habits.\n\nIn 1985, Steelcase purchased the Meyer May House designed by Frank Lloyd Wright and restored it, opening it to the public in 1987. A corporate art program has resulted in a collection including pieces by Pablo Picasso, Andy Warhol and Dale Chihuly.\n\nThe company employs a research group called WorkSpace Futures to study workplace trends. In 2010, Steelcase underwent a three-year project to update its Grand Rapids headquarters to promote employee productivity and employee well-being, including redesigning a cafeteria into an all-purpose work environment that provides food service and space for meetings, socializing, and independent work.\n\nSteelcase's sustainability efforts have included reducing packaging, using regional facilities to reduce shipping distance, cutting greenhouse gas emissions and water consumption, and a goal to reduce its environmental footprint by 25 percent by 2020. As of 2012, Steelcase had reduced its waste by 80 percent, greenhouse gas emissions by 37 percent and water consumption by 54 percent since 2006. According to the company's WorkFutures group, the company also analyzes its supply chain and materials chemistry to determine product sustainability. As of 2014, the company leads its industry in Cradle to Cradle-certified products. In 2016, Steelcase employees volunteered 38,913 hours and the Steelcase Foundation donated more than US$5.7 million.\n\nSteelcase's most noteworthy products include \"The Victor\" wastebasket, a patented steel fireproof container released in 1914. The Victor became a top seller for the recently founded company and was eventually expanded into a line of products. The company released Multiple 15 desks in 1946, which introduced standardized desk sizing and became a universal industry standard. Series 9000 was released in 1973 and became Steelcase's most popular line of office systems. The Leap chair, introduced in 1999, sold 5,000 units a week during its first year and became the company's most popular release. The ergonomic office chair was designed with eight adjustable areas for users to control, including chair height, armrest positioning, lumbar support, seat depth, and back positioning. The chair was developed over four years, cost $35 million to design, and resulted in 11 academic studies and 23 patents. The company released the Gesture chair in 2013, which is designed to support the way workers naturally sit.\n\nSteelcase was named among Fortune's most admired Home Equipment and Furnishings companies in 2011, 2012, 2013, and 2014. The company won the Editors' Choice award at the 2014 NeoCon product competition for \"Quiet Spaces,\" a series of workspaces designed for introverts and a collaboration with Susan Cain, author of \"\". In addition, Steelcase's SOTO II Worktools won a Silver Award in the Office Accessories category.\n"}
{"id": "24529342", "url": "https://en.wikipedia.org/wiki?curid=24529342", "title": "Super Bit Mapping", "text": "Super Bit Mapping\n\nSuper Bit Mapping (SBM) is a noise shaping process, developed by Sony for CD mastering.\n\nSony claims that the Super Bit Mapping process converts a 20-bit signal from master recording into a 16-bit signal nearly without sound quality loss, using noise shaping to improve signal to noise ratio over the frequency bands most acutely perceived by human hearing.\n\nAudible quantization error is reduced by noise shaping the error according to an equal-loudness contour.\n\nThis processing takes place in dedicated hardware inside the recording device. A similar process is used in Sony's DSD to PCM conversion and is called SBM Direct.\n\n"}
{"id": "365264", "url": "https://en.wikipedia.org/wiki?curid=365264", "title": "System programming", "text": "System programming\n\nSystem programming (or systems programming) is the activity of programming computer system software. The primary distinguishing characteristic of systems programming when compared to application programming is that application programming aims to produce software which provides services to the user directly (e.g. word processor), whereas systems programming aims to produce software and software platforms which provide services to other software, are performance constrained, or both (e.g. operating systems, computational science applications, game engines and AAA video games, industrial automation, and software as a service applications).\n\nSystem programming requires a great degree of hardware awareness. Its goal is to achieve efficient use of available resources, either because the software itself is performance critical (AAA video games) or because even small efficiency improvements directly transform into significant monetary savings for the service provider (cloud based word processors).\n\nThe following attributes characterize systems programming:\n\nSystems programming is sufficiently different from application programming that programmers tend to specialize in one or the other.\n\nIn system programming, often limited programming facilities are available. The use of automatic garbage collection is not common and debugging is sometimes hard to do. The runtime library, if available at all, is usually far less powerful, and does less error checking. Because of those limitations, monitoring and logging are often used; operating systems may have extremely elaborate logging subsystems.\n\nImplementing certain parts in operating systems and networking requires systems programming, for example implementing paging (virtual memory) or a device driver for an operating system.\n\nOriginally systems programmers invariably wrote in assembly language. Experiments with hardware support in high level languages in the late 1960s led to such languages as PL/S, BLISS, BCPL, and extended ALGOL for Burroughs large systems. Forth also has applications as a systems language. \nIn the 1980s, C became ubiquitous, aided by the growth of Unix.\nMore recently a subset of C++ called Embedded C++ has seen some use, for instance it is used in the I/O Kit drivers of macOS.\n\nFor historical reasons, some organizations use the term \"systems programmer\" to describe a job function which would be more accurately termed systems administrator. This is particularly true in organizations whose computer resources have historically been dominated by mainframes, although the term is even used to describe job functions which do not involve mainframes. This usage arose because administration of IBM mainframes often involved the writing of custom assembler code (IBM's Basic Assembly Language (BAL)), which integrated with the operating system such as OS/MVS, DOS/VSE or VM/CMS. Indeed, some IBM software products had substantial code contributions from customer programming staff. This type of programming is progressively less common, but the term \"systems programmer\" is still the de facto job title for staff directly administering IBM mainframes.\n\n\n"}
{"id": "43447190", "url": "https://en.wikipedia.org/wiki?curid=43447190", "title": "Takaaki Kidani", "text": "Takaaki Kidani\n\nAfter Kidani graduated from Musashi University with a degree in economics, he worked at Yamaichi Securities. He handled American duties.\n\nKidani retired from Yamaichi Securities in 1994. In March 25 of the same year, he founded Broccoli as a startup company. At first, his business focused on event management, but from 1994 to 1998, he started Comic Castle for dōjinshi. In 1996, he established his first chain of retail stores . His company has its center focus on anime and video games. Their company performance increased after producing the anime series Di Gi Charat and his company was then listed on the JASDAQ Securities Exchange.\n\nHe founded Bushiroad in May 2007 as a company that produces card games and assumed the position of president. The origin of the name Bushiroad comes from the failure to make \"Neppu Kairiku Bushi Road\" into a film due to various reasons when he led the project in the days of Broccoli. The film was restarted in March 2013 with Kidani as the executive producer.\n\nOn October 2, 2017, Bushiroad announced that Kidani would be stepping down as the company's Representative Director on October 20 and joining its board of directors.\n\n"}
{"id": "12521936", "url": "https://en.wikipedia.org/wiki?curid=12521936", "title": "TechTarget", "text": "TechTarget\n\nTechTarget is an American company which offers data-driven marketing services to business-to-business technology vendors. It uses purchase intent data gleaned from the readership of its 140 + technology focused web sites to help tech vendors reach buyers actively researching relevant IT products and services.\n\nTechTarget, Inc. was founded in 1999 and is headquartered in Newton, Massachusetts with offices in Atlanta, London, Munich, Paris, San Francisco, Singapore and Sydney.\n\nTechTarget was founded in 1999 by Greg Strakosch and Don Hawk as a spin-off of United Communications Group (UCG). In 2001, the company was recognized by B2B Magazine on the Media Power 50 list. And in 2005, AdAge named CEO Greg Strakosch a Top 25 Newsmaker.\n\nIn 2016, TechTarget named Michael Cotoia as CEO and board member, and elected Greg Stakosch as executive chairman. The company had its initial public offering in May 2007, listing on the NASDAQ exchange with symbol TTGT.\n\nIts current board of directors includes EMC Corporation founder Roger Marino, Atlanta Hawks co-owner Bruce Levenson, former CFO of the New York Times Co. , , CEO Michael Cotoia and executive chairman Greg Strakosch.\n\nSince launch, the company has made several acquisitions aimed at building its technology content reach: In 2003, TechTarget acquired Information Security Magazine. In 2004, the company acquired publications including: Bitpipe.com, TheServerSide.com and TheServerSide.net. In 2007 (the year the company went public), TechTarget acquired KnowledgeStorm for $58 million. In 2008, TechTarget acquired BrianMadden.com and BriForum. The company acquired LeMagIT and opened local operations in France. In 2011, it made a significant acquisition of Uk-based Computer Weekly, which had been published as a weekly print magazine by Reed Business Information for more than 45 years. \n\nThe company launched its current purchase intent data services (IT Deal Alert) for technology vendors in 2014 and has since added to the portfolio with a product called Priority Engine.\n"}
{"id": "433041", "url": "https://en.wikipedia.org/wiki?curid=433041", "title": "Technocapitalism", "text": "Technocapitalism\n\nTechnocapitalism (a portmanteau word combining \"technology\" and \"capitalism\") refers to changes in capitalism associated with the emergence of new technology sectors, the power of corporations, and new forms of organization.\n\nLuis Suarez-Villa, in his 2009 book \"Technocapitalism: A Critical Perspective on Technological Innovation and Corporatism\" argues that it is a new version of capitalism that generates new forms of corporate organization designed to exploit \"intangibles\" such as creativity and new knowledge. The new organizations, which he refers to as \"experimentalist organizations\" are deeply grounded in technological research, as opposed to manufacturing and services production. They are also heavily dependent on the corporate appropriation of research outcomes as intellectual property.\n\nThis approach is further developed by Suarez-Villa in his 2012 book \"Globalization and Technocapitalism: The Political Economy of Corporate Power and Technological Domination\", in which he relates the emergence of technocapitalism to globalization and to the growing power of technocapitalist corporations. Taking into account the new relations of power introduced by the corporations that control technocapitalism, he considers new forms of accumulation involving intangibles—such as creativity and new knowledge—along with intellectual property and technological infrastructure. This perspective on globalization—and the effect of technocapitalism and its corporations—also takes into account the growing global importance of intangibles, the inequalities created between nations at the vanguard of technocapitalism and those that are not, the increasing importance of brain-drain flows between nations, and the rise of what he refers to as a \"techno-military-corporate\" complex that is rapidly replacing the old military-industrial complex of the second half of the 20th century. \n\nThe concept behind technocapitalism is part of a line of thought that relates science and technology to the evolution of capitalism. At the core of this idea of the evolution of capitalism is that science and technology are not divorced from society—or that they exist in a vacuum, or in a separate reality of their own—out of reach of social action and human decision. Science and technology are part of society, and they are subject to the priorities of capitalism as much as any other human endeavor, if not more so. Prominent scientists in the early 20th century, such as John Bernal, posited that science has a social function, and cannot be seen as something apart from society. Other scientists at that time, such as John Haldane, related science to social philosophy, and showed how critical approaches to social analysis are very relevant to science, and to our understanding of the need for science. In our time, this line of thought has encouraged philosophers such as Andrew Feenberg to adopt and apply a critical theory approach to technology and science, providing many important insights on how scientific and technological decisions—and their outcomes—are shaped by society, and by capitalism and its institutions.\n\nThe term \"technocapitalism\" has been used by one author to denote aspects and ideas that diverge sharply from those explained above. Dinesh D'Souza, writing about Silicon Valley in an article, used the term to describe the corporate environment and venture capital relationships in a high tech-oriented local economy. His approach to the topic was consonant with that of business journals and the corporate management literature. Some newspaper articles have also used the term occasionally and in a very general sense, to denote the importance of advanced technologies in the economy.\n"}
{"id": "12041842", "url": "https://en.wikipedia.org/wiki?curid=12041842", "title": "The Oil Gush in Balakhany", "text": "The Oil Gush in Balakhany\n\nThe Oil Gush in Balakhany () is a film directed by the pioneer of cinema in Azerbaijan, Alexandre Michon, it was filmed on August 4, 1898 in Balakhany, Baku and presented at the International Paris Exhibition. The film was shot using a 35mm film on a Lumière cinematograph.\n\n"}
{"id": "20854014", "url": "https://en.wikipedia.org/wiki?curid=20854014", "title": "Turn indicator stalk", "text": "Turn indicator stalk\n\nThe turn indicator stalk or turn signal lever is the control lever which operates the turn signal or indicator lights on the front, sides and rear of the vehicle. It is usually operated by lifting or lowering the lever, the direction being commensurate with the clockwise or anticlockwise direction in which the steering wheel is about to be turned.\n\nIn left hand drive (LHD) vehicles, the turn indicator stalks are usually located on the left of the steering column.\n\nIn right-hand-drive (RHD) motor vehicles, the indicator stalk is located on either the left or right of the steering column, depending on the manufacturer. European RHD cars generally have the stalk on the left (often using the same component as LHD cars), while Asia-Pacific RHD cars generally have the stalk on the right (mirroring the configuration of a LHD vehicle). Some manufacturers such as Subaru still have variations in the model lineup as to where the turn indicator stalk is located.\n\nMany other functions have been added to the turn signal stalk. Frequently headlamps and high beam controls are integrated into the turn signal control, the former requiring either a twisting motion or the use of a small switch, and the latter requiring movement of the control fore and aft.\n\nMany modern cars have a \"one-touch\" feature on their stalks. This is primarily based on (motorway) lane-switching, where a single flick of the indicator will cause it to flash between two and six times.\n\nSome cars have forgone the traditional stalk-mounted indicators for either a switch (as used on a motorcycle) or a button. Examples are the Caterham 7 and the Ferrari 458.\n\n"}
{"id": "35984394", "url": "https://en.wikipedia.org/wiki?curid=35984394", "title": "Virtus (chipset)", "text": "Virtus (chipset)\n\nVirtus is the name of a new chipset developed by scientists at the Nanyang Technological University and ASTAR Institute for Infocomm Research. It allows communication between devices at speeds as high as 1000 times faster than the current Bluetooth technology. \n"}
{"id": "5710697", "url": "https://en.wikipedia.org/wiki?curid=5710697", "title": "Wake Forest Innovation Quarter", "text": "Wake Forest Innovation Quarter\n\nWake Forest Innovation Quarter in Winston-Salem, North Carolina, is an innovation district focused on research, business and education in biomedical science, information technology, digital media, clinical services and advanced materials. The Innovation Quarter, operated by Wake Forest Baptist Medical Center, is home to academic groups, private companies and other organizations located on 330 acres in downtown Winston-Salem. Its tenants include departments from five academic institutions—Wake Forest School of Medicine, Wake Forest University, Forsyth Technical Community College, Winston-Salem State University, UNC School of the Arts— as well as private businesses and other organizations. One tenant is the Wake Forest Institute for Regenerative Medicine (WFIRM), which is working to engineer more than 30 different replacement tissues and organs and to develop healing cell therapies. The science and research conducted at WFIRM is behind two start-up companies at Innovation Quarter. The ability of researchers and scientists to work alongside entrepreneurs furthers a goal of Innovation Quarter to develop new treatments and cures for disease and advances in technology.\n\nThe idea of a research park in Winston-Salem was a community-wide effort that began in the early 1990s in the wake of R. J. Reynolds Tobacco Company closing many of its former downtown warehouse and manufacturing buildings. Wake Forest School of Medicine's Department of Physiology and Pharmacology moved into one former Reynolds warehouse in 1993, along with eight researchers from Winston-Salem State University. Civic committees and discussion led to a master plan being announced in 2002 for what was then called Piedmont Triad Research Park.\n\nThe first new building, One Technology Place, opened in 2000, occupied by Targacept Inc., a biopharmaceutical company that was spun out of R.J. Reynolds Tobacco. The company developed drugs to treat nervous system diseases and disorders.\n\nBiotech Place opened in February 2012. The 242,000-square-foot structure is composed of two former Reynolds warehouses that have been renovated into a modern biotech research facility, with custom-designed wet and labs as well as Class A office space. The $100 million project was Winston-Salem's most expensive ever downtown project; it houses Wake Forest School of Medicine's departments of Physiology and Pharmacology, Biomedical Engineering, and Immunology and Microbiology, as well as the Childress Institute for Pediatric Trauma. Private businesses—Carolina Liquid Chemistries, Allegacy Federal Credit Union, Brioche Doree cafe—also are tenants at Biotech Place.\nPiedmont Triad Research Park was renamed in March 2013 as Wake Forest Innovation Quarter in recognition of the shift from biotechnology to a mix of biomedical and material sciences, information technology, and other health and communications fields.\n\nEarly 2014 saw Inmar Inc., an information technology company, move into another renovated former R.J. Reynolds building in the Innovation Quarter. Inmar relocated 900 employees from other sites in Winston-Salem to its new, state-of-the-art headquarters. The company also announced a partnership with the Division of Public Health Sciences of Wake Forest School of Medicine in which Inmar's digital analytics will be used to help locate and enroll patients for clinical trials conducted by the school. The Division of Public Health Sciences in early 2015 completed a move into the Innovation Quarter, in a building called 525@vine. The 525@vine building is adjacent to Inmar's headquarters. The 525@vine building, a five-story R.J. Reynolds factory built in 1926 and renovated in 2012-13, also houses the School of Medicine's Physician Assistant program, as well as Forsyth Technical Community College’s Emerging Technologies Center, which trains more than 1,200 students annually.\n\nWith state and federal funding, and the cooperation of neighboring communities, Innovation Quarter's expansion plan includes private businesses, retail and residential units. Among the work is relocating Norfolk Southern Railroad lines, construction of a new rail bridge and burying Duke Energy transmission lines. More than $17 million from the City of Winston-Salem and Forsyth County, have helped leverage $350 million in state, federal and private investment at Innovation Quarter.\n\nThe renaming of Piedmont Triad Research Park to Wake Forest Innovation Quarter came shortly after Wake Forest Baptist Medical Center created a new operating division, Wake Forest Innovations, to establish and manage new business, partnerships, licenses and start-up companies based on the discoveries, intellectual property and research assets of the medical center and Wake Forest University.\n\nWake Forest Innovations has separate units that market its scientific business assets-core laboratories, preclinical translational services, for example-to outside partners, while also promoting discovery and innovation and the licensing of technologies. Wake Forest Innovation Quarter is another unit within Wake Forest Innovations.\n\nwww.wakeforestinnovationquarter.com\n"}
