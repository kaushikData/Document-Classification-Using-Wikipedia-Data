{"id": "32716937", "url": "https://en.wikipedia.org/wiki?curid=32716937", "title": "Activprimary", "text": "Activprimary\n\nActivprimary is a software application designed specifically for teachers and children in the primary education sector who use an Activboard Interactive Whiteboard, Promethean's Interactive Whiteboard. Activprimary was designed and implemented by Nigel Pearce together with a software development team at Promethean (Blackburn, England). Activprimary is similar in functionality to the software application Activstudio but is designed with a simpler user interface aimed towards the younger learner.\n\n\nActivprimary provides a suite of ‘interactive whiteboard centric’ tools. The main feature is to allow the user to prepare and present files known as flipcharts, an electronic document that can contain a combination of vector and raster object data including lines, shapes, rich text, images, video, FLASH media and other third party document types.\n\nThe use of the term 'flipchart' in describing these electronic documents derives from the similarity in the way a physical flipchart is commonly used, effectively starting with a set of 'blank canvas' pages upon which to present.\n\nThe flipchart also captures and stores any notes (termed ‘annotations’) which may be written on the surface of the interactive whiteboard using an electronic pen. Additionally the program allows the user to write (or ‘annotate’) directly over other applications, WEB Browser content and live video clips.\n\nActivprimary includes integrated tools to centrally manage additional interactive inputs including remote annotation and telepointing using wireless handheld Activslates and Activote Student Response Devices.\n\nActivprimary incorporates many of today’s standard ‘whiteboard centric’ concepts, including the spotlight, revealer and zoom functions, a sound recorder, resource library and the interactive protractor, ruler and compass tools. A large selection of editable Restrictors, Properties and Actions can be used to enable the creation of interactive multimedia.\n\nActivprimary has been translated into 32 languages and operates on most versions of the Windows and MAC operating systems.\n\nWhilst Activprimary was originally designed for use on the Activboard Interactive Whiteboard, the program can also be used with the standard computer mouse for preparation purposes and is also suitable for running with other types of input device such as a graphics tablet or touchscreen.\n\nActivPrimary uses a proprietary 'flipchart' format ending in the file extension '.flp'. This utilizes PKZip compression technology to reduce flipchart file sizes.\n\n\n"}
{"id": "54468621", "url": "https://en.wikipedia.org/wiki?curid=54468621", "title": "Alaxala Networks", "text": "Alaxala Networks\n\n, commonly known as its brand Alaxala, is a Japanese company headquartered in Kawasaki, Kanagawa, Japan, that offers networking hardware products.\n\nOriginally, in 2004, Alaxala Networks Corp. was established, as the merger between Hitachi and NEC networking hardware divisions. The name of Alaxala was derived from \"ALL Access for eXpert and Latent Association\", and more that \"Ala\" in Latin means \"Wing\", and \"X\" means \"networking eXchange\", so the company intended the two \"Ala\" (Hitachi and NEC) connected and collaborated by \"X\" tightly.\nThe company is basically fabless company, designing the products, mostly ordering to manufacture them to Hitachi enterprise servers division factory in Hadano, Kanagawa, Japan. The company offers networking hardware products, network router and network switch etc. Alaxala products are sold and installed mostly in Japanese domestic market and for enterprises, but we can find several products at some online shopping, Amazon.com etc.. The business type and scope is same as Allied Telesis, that is also the company in Japan. And they are collaborated for producing networking hardware sold in both brands.\n\nAlaxala official agencies are Hitachi, NEC, Itochu Techno-Solutions, Net One Systems, and Alaxala has contributed to offer the various networking hardware products to major communication companies in Japan, such as NTT, KDDI and Softbank etc..\n\nOn August 24, 2012, Alaxala was known that Tokyo Stock Exchange (TSE) announced that TSE core system outage happened due to the software bugs on Alaxala networking hardware.\n\nAlaxala also sponsors IT professional certifications for Alaxala products, like Cisco and Oracle etc..\n\n\n"}
{"id": "644341", "url": "https://en.wikipedia.org/wiki?curid=644341", "title": "Analog Devices", "text": "Analog Devices\n\nAnalog Devices, Inc., also known as ADI or Analog, is an American multinational semiconductor company specializing in data conversion and signal processing technology, headquartered in Norwood, Massachusetts. In 2012, Analog Devices led the worldwide data converter market with a 48.5% share, according to analyst firm Databeans.\n\nThe company manufactures analog, mixed-signal and digital signal processing (DSP) integrated circuits (ICs) used in electronic equipment. These technologies are used to convert, condition and process real-world phenomena, such as light, sound, temperature, motion, and pressure into electrical signals.\n\nAnalog Devices has approximately 100,000 customers in the following industries: communications, computer, industrial, instrumentation, military/aerospace, automotive, and consumer electronics applications.\n\nThe company was founded by two MIT graduates, Ray Stata and Matthew Lorber in 1965. The same year, the company released its first product, the model 101 op amp, which was a hockey-puck sized module used in test and measurement equipment. In 1967, the company published the first issue of its technical magazine, Analog Dialogue.\n\nIn 1969, Analog Devices filed an initial public offering and became a publicly traded company. Ten years later, the company was listed on the New York Stock Exchange.\n\nIn 1973, the company was the first to launch laser trim wafers and the first CMOS digital-to-analog converter. By 1996, the company reported over $1 billion in company revenue. That same year, Jerald Fishman was named President and CEO, a position he held until his death in 2013 (see below).\n\nIn 2000, ADI's sales grew by over 75% to $2.578 Billion and the company acquired five companies including BCO Technologies PLC, a manufacturer of thick film semiconductors, for $150 million.\n\nIn January 2008, ON Semiconductor completed the acquisition of the CPU Voltage and PC Thermal Monitoring Business from ADI., for $184 million.\n\nBy 2004, ADI had a customer base of 60,000 and its portfolio included over 10,000 products.\n\nIn July 2016, Analog and Linear Technology agreed that Analog would acquire Linear in an approximately $14.8 billion cash and stock deal.\n\nAnalog Devices is headquartered in Norwood, Massachusetts, with regional headquarters\nlocated in Shanghai, China; Munich, Germany; Limerick, Ireland; and Tokyo, Japan.\nAnalog Devices has fabrication plants located in the United States and in Ireland. The company's testing facility is located in the Philippines.\nDesign centers are located in Australia, Canada, China, Egypt, England, Germany, India, Israel,\nJapan, Scotland, Spain, Taiwan and Turkey.\n\nRaymond Stata is a founder of Analog Devices and was responsible for the business strategy and product roadmap. After founding the company in 1965, Stata served as the company's chairman of the board of directors since 1973, executive officer since 1996, CEO from 1973 to 1996 and president from 1971 to 1991. In addition, Stata is also a trustee of the Massachusetts Institute of Technology, his alma mater and was awarded the IEEE Founders medal in 2003. Stata received the EE Times \"Lifetime Achievement\" award in 2008. Stata served as the chairman of the Semiconductor Industry Association for the year 2011.\n\nVincent Roche became President and CEO of Analog Devices in May 2013. He first joined the company in 1988 as a marketing director in Limerick, Ireland.\n\nBarrie Gilbert was named the first Technology Fellow of Analog Devices in 1979. In addition, Gilbert is an IEEE Life Fellow and holds over 65 patents. Gilbert is best known for the \"Gilbert cell\" – an electronic multiplying mixer. At Analog Devices, Gilbert started the company's Northwest Labs design center in Oregon and continued to work on RF products crafted with high-speed nonlinear circuit techniques.\n\nPaul Brokaw is an expert on integrated circuit design who has spent most of his career at Analog Devices, where he holds the position of Analog Fellow. Brokaw is the inventor of many analog IC circuits, including the Brokaw bandgap reference and holds over 100 patents. He is also an IEEE Life Fellow.\n\nRobert Adams is Technical Fellow and manager of audio development at Analog Devices Inc. Adams holds many patents related to the audio and electronic field. He is a member of the IEEE and a Fellow in the Audio Engineering Society. Adams received a finalist ranking for the EDN Innovation and Innovator of the Year award in 1995.\n\nJerald G. Fishman was the CEO and president of Analog Devices from 1996 until his death in March 2013. In 2004, Fishman was named CEO of the Year by Electronic Business. He was a 35-year veteran of Analog Devices and also served on the board of directors of Analog Devices, Cognex Corporation and Xilinx.\n\nAnalog Devices products include analog signal processing and digital signal processing technologies. These technologies include data converters, amplifiers, radio frequency (RF) technologies, embedded processors or digital signal processing (DSP) ICs, power management, and interface products.\n\nData converters include analog-to-digital converters (ADCs) and digital-to-analog converters (DACs) that convert electrical signal representations of real-world analog phenomena, such as light, sound, waveforms, temperature, motion, and pressure into digital signals or data, and back again. Analog Devices ADC and DAC ICs are used in medical systems, scientific instrumentation, wireless and wired communications, radar, industrial process control, audio and video equipment, and other digital-processing-based systems, where an accurate signal conversion is critical. Data converters account for more than 50% of ADI's revenue. ADI's companion amplifier ICs provide accurate, high-speed and precise signals for driving data converters and are key for applications such as digital audio, current sensing, and precision instrumentation.\n\nThe company's data converter chips are used by National Instruments in high-precision measurement instrumentation systems. Its data converters and amplifiers are also used by scientists and researchers in project \"IceCube\" – an underground telescope that uses digital optical modules (DOMS) to detect subatomic particles in the South Pole.\n\nPower management products for customers in the industrial, wireless infrastructure and digital camera markets support signal chain design requirements, such as dynamic range, transient performance, and reliability.\n\nInterface products include a broad range of interface IC products offered by the company in product categories such as CAN (controller area network), digital isolators, level translators, LVDS, mobile I/O expander and keyboard Controller, USB, and RS-232.\n\nAmplifiers includes precision and operational amplifiers, instrumentation, current sense, differential amplifiers, audio amplifiers, video amplifiers/buffers/filters, variable gain amplifiers, comparators, voltage, other specialty amplifiers and products for special linear functions.\n\nRadio frequency integrated circuits (RFICs) address the RF signal chain and simplify RF system development. The company's RF portfolio includes TruPwr RMS power detectors and logarithmic amplifiers; PLL and DDS synthesizers; RF prescalers; variable gain amplifiers; ADC drivers, gain blocks, LNAs and other RF amplifiers.\n\nProcessors and DSP are programmable signal processing integrated circuits that execute specialized software programs, or algorithms, associated with processing digitized real-time data. Analog Devices Processors and DSPs are the Blackfin, SHARC, SigmaDSP, TigerSHARC, ADSP-21xx and Precision Analog Microcontrollers. These make up the company's embedded processing and DSP portfolio, that are multi-DSP signal processing,\n\nAnalog Devices had a line of micro-electromechanical systems (MEMS) microphones until it sold that business to InvenSense in 2013. Analog Devices MEMS microphones were found in smart phones, tablet PCs, security systems, and medical applications. ADI's MEMS accelerometers were designed into game pad controllers by Microsoft, Logitech and Pellican.\n\nAnalog Devices sells linear, mixed-signal, MEMS and digital signal processing technologies for medical imaging, patient monitoring, medical instrumentation and home healthcare. The company's precision signal-processing components and Blackfin digital signal processors are included in Karmelsonix's Wholter, an overnight pulmonary monitor, and the Wheezometer, a personal asthmatic assessment device. Accelerometers produced by Analog Devices are included in ZOLL Medical's PocketCPR, which measures the depth of chest compressions and provides audible and visual feedback to a rescuer to allow adjustment to proper depth and to the correct rate of compression.\n\nAnalog Devices develops components for safety systems, such as stability control systems and driver assistance systems, infotainment and interior applications. Powertrain systems in hybrid and electric vehicles use high-precision data conversion products in battery monitoring and control systems.\n\nAnalog Devices industrial market includes process control systems that help drive productivity, energy efficiency and reliability.\n\nAnalog Devices has technology for consumer electronics, which includes signal processing circuits for image processing, auto focus, and image stabilization for digital still cameras and camcorders, audio and video processors for home theater systems, DVD recorders, and high-definition televisions and advanced touch screen controllers for portable media devices.\n\nIn 2009, Databeans published its report on the top semiconductor analog suppliers. Analog Devices was named number two with other suppliers including: Texas Instruments, National Semiconductor (acquired by Texas Instruments in 2011), Maxim Integrated Products, and Linear Technology (collaborating with Analog Devices since March 2017). Other competitors include: Infineon Technologies, STMicroelectronics and Intersil Corporation.\nIn 2011, Analog Devices was the third ranked analog semiconductor supplier.\n\nIn 1967, Analog Devices first published \"Analog Dialogue\". Dan Sheingold took the position of editor two years later, which he held for over four decades. The current editor is Scott Wayne. It is currently the longest-running in-house publication in the electronics industry.\n\n\"Analog Dialogue\" is a forum for the exchange of circuits, systems, and software for real-world signal processing and is the technical magazine published by Analog Devices. It discusses products, applications, technology, and techniques for analog, digital, and mixed-signal processing. \"Analog Dialogue\" is published monthly on the Web. The featured technical articles are also compiled in quarterly print editions.\n\nIn 2009, Analog Devices announced EngineerZone, an online technical support community. EngineerZone was launched so the design engineering community (customers, prospects, partners, employees and students) can ask questions, share knowledge and search for answers to their questions in an open forum.\n\nAnalog Devices circuits from the lab reference circuits are engineered and tested for quick system integration to help solve design challenges ranging from common to complex. Reference circuits are smaller, modular designs that are more broadly applicable than application-specific reference designs.\n\nEach reference circuit is documented with test data, theory of operation, and component selection decision criteria. In addition, reference circuits are tailored to meet real-world system integration needs and may also include board layout schematics, CAD tools models, device drivers, and evaluation hardware.\n\n"}
{"id": "40956660", "url": "https://en.wikipedia.org/wiki?curid=40956660", "title": "Beauty micrometer", "text": "Beauty micrometer\n\nThe beauty micrometer, also known as the beauty calibrator, was a device designed in the early 1930s to help in the identification of the areas of a person's face which need to have their appearance reduced or enhanced by make-up. The inventors include famed beautician Max Factor Sr. A 2013 \"Wired\" article described the device as \"a \"Clockwork Orange\" style device\" that combines \"phrenology, cosmetics and a withering pseudo-scientific analysis\". A photograph of Factor, using the device on actress Marjorie Reynolds featured in a 1935 article in science magazine \"Modern Mechanix\" and, when republished by \"The Guardian\" in 2013, the caption described it as being \"a contraption that looks like an instrument of torture\".\n\nPlaced on and around the head and face, the beauty micrometer uses flexible metal strips which align with a person's facial features. The screws holding the strips in place allow for 325 adjustments, enabling the operator to make fine measurements with a precision of one thousandth of an inch. The inventors stated that there are two key measurements that they looked for: the heights of the nose and forehead should be the same, and the eyes should be separated by the width of one eye. When an imperfection is identified, corrective make-up can be applied to enhance or subdue the feature. The company Max Factor claims that the device helped Max Factor, Sr. to better understand the female face.\n\nThe beauty micrometer was completed in 1932 and was primarily intended for use in the movie industry. When an actor's face is shown in a very large scale their \"flaws\" are magnified and can become \"glaring distortions\", according to the \"Modern Mechanix\" article. This device was intended to remedy the perceived problem, and the inventors also envisioned it being used in beauty shops. However, it did not become popular and did not gain widespread usage. Only one beauty micrometer is believed to exist. It is featured in a display at the Hollywood Entertainment Museum and came up for auction in 2009, falling significantly short of the $10,000–$20,000 estimate.\n"}
{"id": "18208370", "url": "https://en.wikipedia.org/wiki?curid=18208370", "title": "CAADRIA", "text": "CAADRIA\n\nThe Association for Computer-Aided Architectural Design Research in Asia (CAADRIA) (founded in 1996) provides a platform for CAAD-related academics and professionals to share experiences, best practices, and results in education and research in Asia and beyond.\n\nThe association has been created with several objectives. It aims to facilitate dissemination of information about CAAD amongst Asian Schools of Architecture, Planning, Engineering, & Building Sciences in Asia, and encourage the exchange of software, courseware, experiences and staff/students amongst schools. It conducts several activities to identify research and development needs specific to CAAD education and initiate collaboration to satisfy them, and promote research and teaching in CAAD which enhances creativity rather than simply production technologies.\n\nMembership to the association is open to individuals (teachers, researchers, students, and architects) and institutions (universities, libraries), either as active member (someone who attends the conference) or as sponsor member. Typically, one becomes an individual member through attending and registering at a CAADRIA conference, or by applying for membership to the secretariat. Institutions in Asia, Australia and New Zealand can apply for institutional membership and gain access to CumInCAD database.\n\nMembership lasts for one year, and gives access to digital proceedings of CAADRIA, email announcements of forthcoming events of interest and one vote at the Annual General Meeting of CAADRIA.\n\nCAADRIA is governed by an administrative council elected by the membership. Responsibility for organisation, administration and guidance of the association is delegated to the executive committee by virtue of the election of the officers. Membership of the council is restricted to members working in countries within the Asia Pacific region; term of office is two years. There is an annual general meeting (normally convened during the annual conference) at which the business of the association is agreed by the membership. This forum is considered the highest level of authority and shall be the only one with the power to amend the charter of CAADRIA.\n\nThe Secretariat is composed of volunteer staff, with a limited budget being made available through membership dues and supervised by the executive committee.\n\nThe annual conference is the main event organised under the auspices of the association. It is organised by a member in good standing, who volunteers for the organisation. The organiser is supported by members of the administrative council, in particular the conference liaison. Over the years, the association has developed the policy to circulate the conference location in such a way that southern, eastern, western, and northern parts of Europe are reached regularly – although this principle can only be maintained on the basis of available organisers. In the past years, the following CAADRIA conferences have been organised:\n\nEach year the conference papers are gathered into a proceedings publication which is distributed to members, and available to the public via the organization's web site.\n\nCumInCAD is a cumulative index of publications about computer-aided architectural design. Full texts, in PDF, of CAADRIA proceedings are available through this archive. Public CumInCAD records are available via an Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH) feed and records are available via multiple bibliographic archives and citation indexes online.\n\nThe International Journal of Architectural Computing (IJAC) is an international, peer-reviewed academic journal in the area of CAAD. This journal was established as an international collaboration between the sister organisations and continues to be supported by those sister organisations, representatives of which are members of the editorial board. It is produced quarterly, both in print and online, and a special rate of subscription is available to all members of CAADRIA.\n\n\nThe Sasada Award is instituted in the memory of Professor Tsuyoshi Sasada (1941-2005), Emeritus Professor at Osaka University, as well as co-founder and Fellow of CAADRIA. This award is given to an individual whose sustained record of contributions demonstrates or promises significant impact on the field of computer-aided design. In keeping with Tee’s spirit, the award recipient should have contributed to the next generation of researchers and academics, to the wider profession and practice in computer-aided design and research, and earned recognition in the peer community. The Sasada Award is announced and presented at the annual CAADRIA conference. The Award supports the winner to undertake an academic visit at the Osaka Lab and covers return airfare, accommodation and meals for one week. The Award recipient is expected to contribute to the academic and research activities at the Osaka Lab in the form of seminars and interaction with research students and staff.\n\nThe Best Paper Award Committee is organized at the conference site by the Exco member in charge of awards. The Site Coordinator identifies two local members who are appropriately qualified to join this committee. All papers printed in proceedings and presented at the conference are eligible for this award. The paper is awarded on the basis of the clarity of writing, good structure and argument, contribution to the field, a good conclusion, contribution and relevance to CAADRIA, use of appropriate illustrations and tables, and appropriate references.\n\nThe Best Presentation Award Committee is organized at the conference site by the Exco member in charge of awards. The Site Coordinator identifies two local members who are appropriately qualified to join this committee. Only presentations made by one of the authors are eligible for this award. The presentations are awarded on the basis of: good use of time, how well-paced they are, good use of appropriate media, delivery, contribution and relevance to CAADRIA, and how well questions have been handled.\n\nYoung CAADRIA Award is funded by CAADRIA and consists of a waiver of the conference delegation fees if the abstract is selected for a paper presentation. It is awarded on the basis of the CV provided.\n\nCAADRIA is one of five sister organisations in CAAD worldwide that\nshare the same mission: ACADIA (North America, founded 1981), SIGraDi (Ibero-America, founded 1997), eCAADe (Europe, founded 1983), and ASCAAD (Arabic countries of west Asia, and North Africa, founded 2001).\n\nA strongly related organisation in the area of CAAD is the CAAD Futures Foundation (global scope, founded 1985).\n\n"}
{"id": "7691807", "url": "https://en.wikipedia.org/wiki?curid=7691807", "title": "Context-aware pervasive systems", "text": "Context-aware pervasive systems\n\nContext-aware computing refers to a general class of mobile systems that can sense their physical environment, and adapt their behavior accordingly.\n\nContext-aware systems are a component of a ubiquitous computing or pervasive computing environment. Three important aspects of context are: where you are; who you are with; and what resources are nearby. Although location is a primary capability, location-aware does not necessarily capture things of interest that are mobile or changing. Context-aware in contrast is used more generally to include nearby people, devices, lighting, noise level, network availability, and even the social situation, e.g., whether you are with your family or a friend from school.\n\nThe concept emerged from ubiquitous computing research at Xerox PARC and elsewhere in the early 1990s. The term 'context-aware' was first used by Schilit and Theimer in their 1994 paper \"Disseminating Active Map Information to Mobile Hosts\" where they describe a model of computing \"in which users interact with many different mobile and stationary computers\" and classify a context-aware systems as one that can \"adapt according to its location of use, the collection of nearby people and objects, as well as the changes to those objects over time over the course of the day.\"\n\n\n"}
{"id": "407979", "url": "https://en.wikipedia.org/wiki?curid=407979", "title": "Curta", "text": "Curta\n\nThe Curta is a small mechanical calculator developed by Curt Herzstark. The Curta's design is a descendant of Gottfried Leibniz's Stepped Reckoner and Charles Thomas's Arithmometer, accumulating values on cogs, which are added or complemented by a stepped drum mechanism. It has an extremely compact design: a small cylinder that fits in the palm of the hand.\n\nCurtas were considered the best portable calculators available until they were displaced by electronic calculators in the 1970s.\n\nThe Curta was conceived by Curt Herzstark in the 1930s in Vienna, Austria. By 1938, he had filed a key patent, covering his complemented stepped drum, Deutsches Reichspatent (German National Patent) No. 747073. This single drum replaced the multiple drums, typically around 10 or so, of contemporary calculators, and it enabled not only addition, but subtraction through nines complement math, essentially subtracting by adding. The nines' complement math breakthrough eliminated the significant mechanical complexity created when \"borrowing\" during subtraction. This drum would prove to be the key to the small, hand-held mechanical calculator the Curta would become.\n\nHis work on the pocket calculator stopped in 1938 when the Nazis forced him and his company to concentrate on manufacturing precision instruments for the German army.\n\nHerzstark, the son of a Catholic mother and Jewish father, was taken into custody in 1943 and eventually sent to Buchenwald concentration camp, where he was encouraged to continue his earlier research:\nIn the camp, Herzstark was able to develop working drawings for a manufacturable device.\nBuchenwald was liberated by U.S. troops on April 11, 1945, and by November Herzstark had located a factory in Sommertal, near Weimar, whose machinists were skilled enough to produce three working prototypes.\n\nSoviet forces had arrived in July, and Herzstark feared being sent to Russia, so, later that same month, he fled to Austria. He began to look for financial backers, at the same time filing continuing patents as well as several additional patents to protect his work. The Prince of Liechtenstein eventually showed interest in the manufacture of the device, and soon a newly formed company, Contina AG Mauren, began production in Liechtenstein.\n\nIt was not long before Herzstark's financial backers, thinking they had got from him all they needed, contrived to force him out by reducing to zero the value of all of the company's existing stock, including his one-third interest. These were the same people who had earlier elected not to have Herzstark transfer ownership of his patents to the company, so that, should anyone sue, they would be suing Herzstark, not the company, thereby protecting themselves at Herzstark's expense. This ploy now backfired: without the patent rights, they could manufacture nothing. Herzstark was able to negotiate a new agreement, and money continued to flow to him.\n\nCurtas were considered the best portable calculators available until they were displaced by electronic calculators in the 1970s. Herzstark continued to make money from his invention until that time, although, like many inventors before him, he was not among those who profited the most from his invention. The Curta, however, lives on, being a highly popular collectible, with thousands of machines working just as smoothly as they did at the time of their manufacture 40, 50 or 60 years previous.\n\nThe Curta Type I was sold for $125 in the later years of production, and the Type II was sold for $175. While only 3% of Curtas were returned to the factory for warranty repair, a small, but significant number of buyers returned their Curtas in pieces, having attempted to disassemble them. Reassembling the machine was more difficult, requiring intimate knowledge of the orientation of, and installation order for, each part and sub-assembly, plus special guides designed to hold the pieces in place during assembly. Also, many identical looking parts, each with slightly different dimensions, required test fitting and selection as well as special tools to adjust tolerances.\n\nThe machines currently have a high curiosity value and sell for around US$1,000, but going as high as US$1,900 for models in a pristine condition with notable numbers.\n\nNumbers are entered using slides (one slide per digit) on the side of the device. The \"revolution counter\" and \"result counter\" reside around the shiftable carriage, at the top of the machine. A single turn of the crank adds the input number to the result counter, at any carriage position, and increments the corresponding digit of the revolution counter. Pulling the crank upwards slightly before turning performs a subtraction instead of an addition. Multiplication, division, and other functions require a series of crank and carriage-shifting operations.\n\nThe Curta was affectionately known as the \"pepper grinder\" or \"peppermill\" due to its shape and means of operation. It was also termed the \"math grenade\" due to its superficial resemblance to a certain type of hand grenade.\n\nThe Type I Curta has eight digits for data entry (known as \"setting sliders\"), a six-digit revolution counter, and an eleven-digit result counter. According to the advertising literature, it weighs only . Serial number 70154, produced in 1969, weighs .\n\nThe larger Type II Curta, introduced in 1954, has eleven digits for data entry, an eight-digit revolution counter, and a fifteen-digit result counter. It weighs , based on weighing serial number 550973, produced in early 1966.\n\nAn estimated 140,000 Curta calculators were made (80,000 Type I and 60,000 Type II). According to Curt Herzstark, the last Curta was produced in 1972.\n\nThe Curta was popular among contestants in sports car rallies during the 1960s, 1970s and into the 1980s. Even after the introduction of the electronic calculator for other purposes, they were used in time-speed-distance (TSD) rallies to aid in computation of times to checkpoints, distances off-course and so on, since the early electronic calculators did not fare well with the bounces and jolts of rally racing.\n\nContestants who used such calculators were often called \"Curta-crankers\" by those who were limited to paper and pencil, or who used computers linked to the car's wheels.\n\nThe Curta was also favored by commercial and general-aviation pilots before the advent of electronic calculators because of its precision and the user's ability to confirm the accuracy of his or her manipulations via the revolution counter. Because calculations such as weight and balance are critical for safe flight, precise results free of pilot error are essential.\n\nThe Curta plays a role in William Gibson's \"Pattern Recognition\" (2003) as an interesting piece of historic computing machinery as well as a crucial \"trade\" item.\n\nIn 2016 a Curta was designed by Marcus Wu that could be produced on a 3D printer. The fine tolerances needed are beyond present printer technology and this means the Curta has to be much bigger, about the size of a coffee can. Mythbuster Adam Savage received one fully printed and assembled at 3x scale from Wu. Wu's creation weighs about three pounds.\n\n\"Kein Geschenk für den Führer\" (No gift for the leader) by Curt Herzstark \n\n"}
{"id": "48390991", "url": "https://en.wikipedia.org/wiki?curid=48390991", "title": "Deep Space Atomic Clock", "text": "Deep Space Atomic Clock\n\nThe Deep Space Atomic Clock (DSAC) is a miniaturized, ultra-precise mercury-ion atomic clock for precise radio navigation in deep space. It is orders of magnitude more stable than existing navigation clocks, and has been refined to limit drift of no more than 1 nanosecond in 10 days. It is expected that a DSAC would incur no more than 1 microsecond of error in 10 years of operations. It is expected to improve the precision of deep space navigation, and enable more efficient use of tracking networks. The project is managed by NASA's Jet Propulsion Laboratory and it will be deployed as part of the U.S. Air Force's Space Test Program 2 (STP-2) mission aboard a SpaceX Falcon Heavy rocket in March 2019.\n\nCurrent ground-based atomic clocks are fundamental to deep space navigation, however, they are too large to be flown in space. This results in tracking data being collected and processed here on Earth (a two-way link) for most deep space navigation applications. The Deep Space Atomic Clock (DSAC) is a miniaturized and stable mercury ion atomic clock that is as stable as a ground clock. The technology could enable autonomous radio navigation for spacecraft's time-critical events such as orbit insertion or landing, promising new savings on mission operations costs. It is expected to improve the precision of deep space navigation, enable more efficient use of tracking networks, and yield a significant reduction in ground support operations.\n\nIts applications in deep space include:\n\nOver 20 years, engineers at NASA's Jet Propulsion Laboratory have been steadily improving and miniaturizing the mercury-ion trap atomic clock. The DSAC technology uses the property of mercury ions' hyperfine transition frequency at 40.50 GHz to effectively \"steer\" the frequency output of a quartz oscillator to a near-constant value. DSAC does this by confining the mercury ions with electric fields in a trap and protecting them by applying magnetic fields and shielding. \n\n\nIts development will include a test flight in low-Earth orbit, while using GPS signals to demonstrate precision orbit determination and confirm its performance in radio navigation. It will be deployed as part of the U.S. Air Force's Space Test Program 2 (STP-2) mission aboard a SpaceX Falcon Heavy rocket probably in March 2019 during the third test flight of the Falcon Heavy.\n\n"}
{"id": "30441255", "url": "https://en.wikipedia.org/wiki?curid=30441255", "title": "Dialog Semiconductor", "text": "Dialog Semiconductor\n\nDialog Semiconductor PLC is a UK-based manufacturer of semiconductor based system solutions. The company is headquartered in the United Kingdom in Reading, with a global sales, R&D and marketing organization. Dialog creates highly integrated standard (ASSP) and custom (ASIC) mixed-signal integrated circuits (ICs), optimised for smartphone, computing, IoT, LED Solid State Lighting (SSL) and smart home applications. Dialog operates a fabless business model, but maintains its own test and physical laboratories at its office in Kirchheim.\n\nIn 2015, it had approximately $1.35 billion in revenue and was one of the fastest growing European public semiconductor companies. It currently has approximately 1,650 employees worldwide. The company is listed on the Frankfurt (FWB: DLG) stock exchange (Regulated Market, Prime Standard, ISIN GB0059822006) and is a member of the German TecDax index.\n\nOn 10 February 2011, Dialog announced that it had completed a transaction to acquire SiTel Semiconductor B.V. (\"SiTel\"), a short-range wireless, digital cordless and VoIP technology company. In July 2013, Dialog announced that it has completed a transaction to acquire iWatt, an AC/DC power conversion and solid state lighting technologies company.\n\nDialog Semiconductor is the exclusive designer of power management integrated circuits (PMICs) for the Apple iPhone, iPad, and Watch, which made up more than 70% of Dialog’s 2016 sales. Its share value halved from April to December 2017 after private bank Bankhaus Lampe changed its outlook for the company from “hold” to “sell” in the belief Apple would replace Dialog’s PMIC designs with its own, predicting in-house design would be part of a general strategic shift for Dialog’s main client: Apple had established its own PMIC design centres in Munich and California, and in April 2017, Apple graphics chip maker Imagination Technologies declared that Apple would be designing its graphics chips in-house by 2020.\nDialog Semiconductor originated from the European operations of International Microelectronic Products, Inc. – a Silicon Valley company founded in 1981\n\n\n"}
{"id": "20123839", "url": "https://en.wikipedia.org/wiki?curid=20123839", "title": "Direct Corporate Access", "text": "Direct Corporate Access\n\nDirect Corporate Access (DCA) is part of the Faster Payments Service which provides a same day clearing payment service to UK member banks. Direct Corporate Access (DCA) will provide Banks' business customers with direct access to the Faster Payments Service (FPS) clearing service in a very similar way that Bacstel-IP provides access to BACS.\n\nDirect Corporate Access only enables submission of files of payments, however as the central FPS processes payments individually, VocaLink the operators of DCA will split the files into individual instructions for processing through FPS.\n\nIt was developed by APACS on behalf of the FPS member banks and the infrastructure went live in March 2009. Barclays was the first bank live for customer sponsorship in August 2009.\n\nAlbany Software was the first solution supplier to successfully process a payment through the Faster Payments Service via DCA, using Albany ePAY on Wednesday 22 July 2009.\n\n\nSecure-IP is a clone of the existing Bacstel-IP channel used for BACS. Files of payments are secured using a smart card or hardware security module (HSM).\n\nFiles of payments submitted by Secure-IP will be disaggregated by VocaLink, the operators of DCA, and submitted into the Faster Payments Service. Disaggregation and acceptance may take up to thirty minutes and beneficiaries will receive access to the funds within two hours of acceptance.\n\nThe software used to access Secure-IP must be approved. This is an extension to the existing Bacs Approved Software Service (BASS). In March 2009 Albany Software and Bottomline Technologies received approved status for their first DCA capable products. They were joined in May 2009 by Barron McCann. In 2010 they were joined by Direct Debit Ltd. (now part of Bottomline) and Experian. In 2011 they were joined by Fundtech.\n\nDCA payments are processed by the sponsoring bank according to their Service Level Agreements - for Barclays this is currently a maximum of 30 minutes before the payment is sent to the beneficiary bank. Beneficiary banks which are members of FPS must apply the funds to the beneficiary account within 2 hours, although in most cases it is done immediately. \n\nBureau organisations can submit files of payments on behalf of other registered service users of Direct Corporate Access.\n\nThe bureau needs to be sponsored by a DCA-enabled bank (initially only Barclays) and to use a bureau/DCA version of the approved Bacs Approved Software Service (BASS) software.\n\nThe bureau's sponsor bank issues PKI certificates for authorising files in the form of a smart card or hardware security module (HSM). The same PKI certificates can be used to authorise Bacs file submissions via Bacstel-IP.\n\nFile submissions can only be made on behalf service of users of a DCA-enabled bank (initially only Barclays).\n\n\n\n"}
{"id": "57021483", "url": "https://en.wikipedia.org/wiki?curid=57021483", "title": "Distributed element circuit", "text": "Distributed element circuit\n\nDistributed element circuits are electrical circuits composed of lengths of transmission lines or other distributed components. These circuits perform the same functions as conventional circuits composed of passive components such as capacitors, inductors, and transformers. They are used mostly at microwave frequencies where conventional components are difficult or impossible to implement.\n\nA major advantage of distributed element circuits is that they can be produced cheaply as a printed circuit board for consumer products such as satellite television. They are also made in coaxial and waveguide formats for applications such as radar, satellite communication, and microwave links.\n\nA phenomenon that is much used in distributed element circuits is that a length of transmission line can be made to behave as a resonator. Distributed element components that do this include stubs, coupled lines, and cascaded lines. Circuits built from these components include filters, power dividers, directional couplers, and circulators.\n\nDistributed element circuits were investigated in the 1920s and 1930s but did not become important until World War II when they were used in radar. After the war their use was at first limited to military, space, and broadcasting infrastructure use but improvements in materials science in the field soon led to wider applications.\n\nDistributed element circuits are those circuits designed using the distributed element model. This model is an alternative to the lumped element model in which the passive electrical elements of electrical resistance, capacitance and inductance are assumed to be \"lumped\" at one point in space in, respectively, a resistor, capacitor or inductor lumped component. The distributed element model is used when this assumption no longer holds and those quantities are considered to be distributed in space. The assumption breaks down when there is a significant time taken for electromagnetic waves to travel from one terminal of the component to the other. \"Significant\" in this context is a time in which a noticeable phase change has occurred. The amount of phase change is very much dependent on the frequency or wavelength of the wave. A common rule of thumb amongst engineers is to change from the lumped to the distributed model when distances involved are more than one-tenth of a wavelength (36° phase change). In any case, the lumped model completely fails at one-quarter wavelength (90° phase change) with entirely incorrect results. Due to this dependence on wavelength, the distributed element model is used mostly at the higher frequencies, at low frequencies distributed element components are too bulky. Distributed designs become feasible above and are the technology of choice at microwave frequencies above .\n\nThere is no clear-cut demarcation in the frequency at which these models should be used. The changeover is usually somewhere in the range 100 to but the scale of the technology being used is significant. Miniaturised circuits can continue to use the lumped model to a higher frequency. Printed circuit boards (pcbs) using through-hole technology are larger than an equivalent design using surface-mount technology. Hybrid integrated circuits are smaller than pcb technologies and monolithic integrated circuits are smaller than both. Hence, integrated circuits can use lumped designs at higher frequencies than printed circuits and this is done in some radio frequency integrated circuits. The choice is particular significant for hand-held devices because lumped element designs generally result in a smaller product.\n\nThe overwhelming majority of distributed element circuits are composed of lengths of transmission line. This is a particularly simple form to model. The cross-sectional dimensions of the line are unvarying along its length and are chosen to be small compared to the signal wavelength, thus, distribution along the length of the line only need be considered. Such an element of a distributed circuit is entirely characterised by its length and characteristic impedance. A further simplification occurs in commensurate line circuits where all the elements are made the same length. With commensurate circuits, a lumped circuit design prototype consisting of capacitors and inductors can be directly converted into a distributed circuit with one-to-one correspondence between the elements of the lumped circuit and the distributed circuit.\n\nCommensurate line circuits are important because they have an established design theory. Circuits consisting of arbitrary lengths of transmission line, or even arbitrary shapes, do not. An arbitrary shape can certainly be \"analysed\" with Maxwell's equations to determine its behaviour but finding useful structures is a matter of trial and error, or guesswork.\n\nAn important difference between distributed element circuits and lumped element circuits is that the frequency response of a distributed circuit periodically repeats as shown in the Chebyshev filter example, whereas the equivalent lumped circuit does not. This is a result of the transfer function of lumped forms being a rational function of complex frequency whereas distributed forms are an irrational function. Another difference is that cascade connected lengths of line introduce a fixed delay at all frequencies (assuming an ideal line). There is no equivalent in lumped circuits for a fixed delay, although an approximation could be constructed over a limited frequency range.\n\nDistributed element circuits are cheap and easy to manufacture in some formats, but take up more space than lumped element circuits. This is problematic in mobile devices, especially hand-held ones, where space is at a premium. If the operating frequencies are not too high, the designer may choose to go down the route of miniaturising components rather than switching to distributed elements. On the other hand, parasitic elements and resistive losses in lumped components are greater with increasing frequency as a proportion of the nominal value of the lumped element impedance. In some cases designers may choose to go to a distributed element design, even if lumped components are available at that frequency, in order to benefit from the improved quality. Distributed element designs tend to have greater power handling capability. With a lumped component, all the power being passed by the circuit is concentrated in a small volume.\n\nThere are many forms of transmission line and any of them can be used to construct distributed element circuits. The historically earliest, and still most widely used, form is a pair of conductors. The most common form of paired conductors is twisted pair as used for telephone lines and internet connections. This is not commonly used for distributed element circuits because the signal frequencies used on these cables are lower than the point where distributed element designs become advantageous. However, designers frequently start with a lumped element design and then convert it into an open-wire distributed element design. \"Open wires\" are a pair of parallel uninsulated conductors, for instance as used for telephone lines on telegraph poles. The designer does not usually intend to implement the circuit in this form, it is merely an intermediate step in the design process. Distributed element designs with conductor pairs are limited to a few specialist uses such as Lecher lines and the twin-lead used for antenna connections.\n\nCoaxial line, a centre conductor surrounded by a shielding conductor, is widely used for interconnecting units of microwave equipment. It is also used for longer distance transmissions. Manufacture of coaxial distributed element devices was common in the second half of the 20th century. However, these types in many applications have been replaced by planar forms due to cost and size considerations. Air dielectric coaxial is used for low-loss and high-power applications. It is still common for distributed element circuits in other media to transition to a coaxial connector at the circuit ports.\n\nThe majority of modern distributed element circuits use planar transmission lines, especially those in mass-produced consumer items. There are several forms of planar line, but the kind known as microstrip is the most common. It can be manufactured by the same process as printed circuit boards and hence is cheap to make. It also lends itself to integration with lumped circuits on the same board. Other forms of printed planar lines include stripline and finline and many variations. Planar lines can also be used in monolithic microwave integrated circuits where they are integral to the device chip.\n\nMany of the distributed element designs can be directly implemented in waveguide. However, there is an additional complication with waveguides in that multiple modes are possible. These sometimes exist simultaneously and this situation has no analogy in conducting lines. Waveguides have the advantages of lower loss and higher quality resonators over conducting lines, but their relative expense and bulk means that microstrip is often preferred. Waveguide mostly finds uses in high-end products such as high-power military radars and the upper microwave bands where planar formats are too lossy. Waveguide becomes bulkier with lower frequency which militates against its use in the lower bands.\n\nIn a few specialist applications such as the mechanical filters in high-end radio transmitters (marine, military, amateur radio) electronic circuits can be implemented as mechanical components. This is done largely because of the high quality of the mechanical resonators. They are used in the radio frequency band below microwave frequencies where waveguides might otherwise be used. Mechanical circuits too can be implemented, in whole or in part, as distributed element circuits. The frequency at which the transitions to distributed element design becomes feasible or necessary are much lower with mechanical circuits. This is because the speed signals travel through mechanical media is much lower than the speed of electrical signals.\n\nThere are several structures that are repeatedly used in distributed element circuits. Some of the common ones are described below.\n\nA stub is a short length of line that branches to the side of a main line. The end of the stub is often left open- or short-circuited but may also be terminated with a lumped component. A stub can be used on its own, for instance for impedance matching, or several of them can be used together in a more complex circuit such as a filter. A stub can be designed as the equivalent of a lumped capacitor, inductor, or resonator.\n\nDepartures from constructing with uniform transmission lines in distributed element circuits are rare. One such departure that is widely used is the radial stub which is shaped like a sector of a circle. They are often used in pairs, one either side of the main transmission line. Such pairs are called butterfly or bowtie stubs.\n\nCoupled lines are two transmission lines between which there is some electromagnetic coupling. The coupling can be direct or indirect. In indirect coupling the two lines are run closely together for a distance with no screening between them. The strength of the coupling depends on the distance between the lines and the cross section presented to the other line. In direct coupling, branch lines directly connect the two main lines together at intervals.\n\nCoupled lines are a common method of constructing power dividers and directional couplers. Another property of coupled lines is that they act as a pair of coupled resonators. This property is used in many distributed element filters.\n\nCascaded lines are lengths of transmission line where the output of one line is connected to the input of the next. Multiple cascaded lines of different characteristic impedances can be used to construct a filter or a wide-band impedance matching network. This is called a stepped impedance structure. A single cascaded line one quarter of a wavelength long forms a quarter-wave impedance transformer. This has the useful property of transforming any impedance network into its dual. In this role it is called an impedance inverter. This structure can be used in filters to implement a lumped element prototype in ladder topology as a distributed element circuit. The quarter-wave transformers are alternated with some kind of distributed element resonator to achieve this. However, this is now a dated design and more compact inverters such as the impedance step are used instead. An impedance step is the discontinuity formed at the junction of two cascaded transmission lines of different characteristic impedance.\n\nHelical resonators consist of a helix of wire in a cavity. One end is unconnected and the other is bonded to the cavity wall. Although superficially similar to a lumped inductor, helical resonators are, in fact, a distributed element component and are used in the VHF and lower UHF bands.\n\nFilters form a major part of the circuits constructed with distributed elements. There is a very wide range of structures used for constructing filters. These include stubs, coupled lines and cascaded lines. Variations of these themes include interdigital filters, combline filters and hairpin filters. Many filters are constructed in conjunction with dielectric resonators. More recent developments include fractal filters.\n\nAs with lumped element filters, the more elements that are used, the closer the filter comes to an ideal response and the structure can become quite complex. On the other hand, for simple narrowband requirements just a single resonator may suffice such as a single stub or a spurline filter.\n\nImpedance matching for narrow-band applications is frequently achieved with a single matching stub. However, for wide-band applications the impedance matching network takes on a definite filter-like design. The designer starts by prescribing the required frequency response and designs a filter with that response. The only difference from a standard filter design is that the source and load impedances of the filter differ.\n\nA directional coupler is a four-port device that couples power flowing in one direction only from one path to another. Two of the ports are the input port and output port of the main line. A proportion of the power entering the input port is coupled to a third port, called the \"coupled port\". None of the power entering the input port is coupled to the fourth port, usually called the \"isolated port\". For power flowing in the reverse direction and entering the output port, a reciprocal situation occurs. Some of the power is coupled to the isolated port, but none is coupled to the coupled port.\n\nPower dividers are often constructed as a directional coupler with the isolated port permanently terminated in a matched load making them effectively a three-port device. There is no essential difference between the two devices. The term \"directional coupler\" tends to be used when the coupling factor (the proportion of power reaching the coupled port) is low and \"power divider\" when the coupling factor is high. A power combiner is simply a power splitter used in reverse. In distributed element implementations using coupled lines, indirectly coupled lines are more suitable for low-coupling directional couplers and directly coupled branch line couplers are more suitable for high-coupling power dividers.\n\nDistributed element designs rely on the length of the elements being one-quarter wavelength (or some other length). This will only be true at one particular frequency. Simple designs thus have a limited bandwidth over which they will work successfully. As with impedance matching networks, a wide-band design requires multiple sections and the design starts to take on a filter-like form.\n\nA directional coupler that splits power equally between the output and coupled ports (a coupler) is called a \"hybrid\". The term \"hybrid\" originally referred to a hybrid transformer, a lumped device used in telephones, but now has a wider meaning. A widely used distributed element hybrid that does not fall into the category of coupled lines is the \"hybrid ring\" or rat-race coupler. Each of the four ports is connected to a ring of transmission line at different points. It works by waves travelling in opposite directions around the ring setting up standing waves. At some points on the ring, destructive interference results in a null and no power will leave a port set at that point. At other points, constructive interference maximises the power transferred.\n\nAnother way of using a hybrid coupler is to produce the sum and difference of two signals. The diagram shows a hybrid ring being used in this way. The two input signals are fed into the ports marked as 1 and 2. The sum of the two signals appears at the port marked as Σ and the difference at the port marked as Δ.\n\nBesides their use as couplers and power dividers, directional couplers can be used in balanced mixers, frequency discriminators, attenuators, phase shifters, and antenna array feed networks.\n\nA circulator is usually a 3-port or 4-port device in which power entering one port is transferred to the next port in rotation as if round a circle. Power can only flow in one direction around the circle (clockwise or anticlockwise) and no power is transferred to any of the other ports. Most distributed element circulators are based on ferrite materials.\n\nUses of circulators include using as an isolator to protect a transmitter or other equipment from damage due to reflections from the antenna, and a duplexer connecting antenna, transmitter and receiver of a radio system.\n\nAn unusual application of a circulator is in a reflection amplifier where the negative resistance of a Gunn diode is used to reflect back more power than it received. The circulator is used to direct the input and output power flows to separate ports.\n\nPassive circuits, both lumped and distributed, are nearly always reciprocal. Circulators are an exception to this. There are several ways to define or represent reciprocity. A convenient one for circuits at microwave frequencies (where distributed element circuits are used) is in terms of their S-parameters. A reciprocal circuit will have an S-parameter matrix, [\"S\"], that is symmetric. From the definition of a circulator it is clear that this will not be the case;\n\nfor an ideal 3-port circulator. Thus circulators must be non-reciprocal by definition. This also means that it is impossible to build a circulator from standard passive components, either lumped or distributed. The presence of the ferrite, or some other non-reciprocal material or system, is essential for the device to work.\n\nDistributed elements are passive elements, but most applications will require active components in some role. A microwave hybrid integrated circuit uses distributed elements for many passive components, but active components such as diodes and transistors, and some passive components, are discrete. The active components may be packaged, or they may be placed on the substrate in chip form without any individual packaging to reduce their size and eliminate the packaging parasitics.\n\nDistributed amplifiers consist of a number of amplifying devices, usually FETs, with all their inputs connected via one transmission line, and all their outputs via another transmission line. The lengths of the two lines must be equal between each transistor for the circuit to work correctly. Each transistor adds to the output of the amplifier. This is different from a conventional multistage amplifier where the gain is \"multiplied\" by the gain of each stage. A distributed amplifier thus has lower gain than a conventional amplifier with the same number of transistors. However, it has significantly greater bandwidth. In a conventional amplifier, the bandwidth is reduced by each additional stage. In a distributed amplifier the overall bandwidth is the same as the bandwidth of a single stage. Distributed amplifiers are used when a single large transistor, or a complex multi-transistor amplifier would be too large to treat as a lumped component. The linking transmission lines serve to separate the individual transistors.\n\nDistributed element modelling was first used in electrical network analysis by Oliver Heaviside in 1881. Heaviside used this to find a correct description of the behaviour of signals on the transatlantic telegraph cable. Transmission of early transatlantic telegraph had been difficult and slow due to an effect now called dispersion but this was not well understood at the time. Heaviside's analysis, now referred to as the telegrapher's equations, identified the problem and suggested methods for overcoming it. This is still the standard analysis of transmission lines.\n\nWarren P. Mason was the first to investigate the possibility of distributed element circuits. He filed a patent in 1927 for a coaxial filter designed by this method. Mason and Sykes published the definitive paper on the method in 1937. Mason was also the first to suggest a distributed element acoustic filter in his 1927 doctoral thesis, and a distributed element mechanical filter in a patent filed in 1941. In fact, the acoustic work had come first and as a result of it Mason's colleagues at Bell Labs in the Radio Department requested him to assist with coaxial and waveguide filters.\n\nMason's work was concerned with the coaxial form and other conducting wires, although much of it could also be adapted for waveguide. Prior to World War II there was not much call for distributed element circuits. The frequencies chosen for radio transmissions at the time were always lower than the point at which distributed elements became advantageous. This was due to lower frequencies having a greater range, a primary consideration for broadcast purposes. This all changed with the wartime requirements for radar. There was a surge in distributed element filter development (an essential component of radars) and the technology was extended from the coaxial domain into the waveguide domain.\n\nThe wartime work was mostly not published until after the war for secrecy reasons. This made it difficult to disentangle who exactly was responsible for each development. An important centre for this research was the MIT Radiation Laboratory (Rad Lab), but work was also undertaken elsewhere in the US and Britain. The Rad Lab work was published by Fano and Lawson. Another wartime development was the hybrid ring. This work was carried out at Bell Labs and was published after the war by W. A. Tyrrell. Tyrrell describes hybrid rings implemented in waveguide and analyses them in terms of the well known waveguide magic tee. However, other researchers soon published coaxial versions of this device.\n\nGeorge Matthaei led a research group at Stanford Research Institute that included Leo Young. The group was responsible for many filter designs. Matthaei himself first described the interdigital filter and the combline filter. The group's work was published in a landmark book in 1964 covering the state of distributed element circuit design at that time. It remained a major reference work for many years.\n\nPlanar formats started to take off with the invention of stripline by Robert M. Barrett. Stripline was another wartime invention, but the details were not published until 1951. Microstrip, invented in 1952, became a commercial rival to stripline. However, planar formats did not start to become widely used in microwave applications until better dielectric materials became available for the substrates in the 1960s. Another structure that had to wait for better materials was the dielectric resonator. Its advantages of compact size and high quality were first pointed out by R. D. Richtmeyer in 1939. However, materials with good temperature stability were not found until the 1970s. Dielectric resonator filters are now common in both waveguide and transmission line filters.\n\nOn the theoretical front, an important development was the commensurate line theory of Paul I. Richards published in 1948. Kuroda's identities, an important set of transforms that allowed many designs to be practically implemented, were provided by Kuroda in 1955.\n\n"}
{"id": "1855872", "url": "https://en.wikipedia.org/wiki?curid=1855872", "title": "EMortgages", "text": "EMortgages\n\nAn eMortgage is an electronic mortgage where the loan documentation is created, executed, transferred and stored electronically. \n\nIn the United States eMortgages are made legally enforceable by the Electronic Signatures in Global and National Commerce Act and the Uniform Electronic Transactions Act. Standardization of eMortgages is being facilitated in the United States by the Mortgage Industry Standards Maintenance Organization (MISMO) eMortgage workgroup, which builds on the existing MISMO data standards, adding data elements and electronic signature capabilities to create an infrastructure for fully electronic mortgages. The eMortgage infrastructure is built around the concept of a SMART Document and the SMART DOC implementation guide.\n\nOn June 30, 2000, the U.S. Congress passed the Electronic Signatures in Global and National Commerce Act, which together with the state laws like the Uniform Electronic Transactions Act enable the origination of enforceable electronic mortgages. \n\nOn June 28, 2002, Fannie Mae announced its readiness to purchase eMortgages in the role of investor in the secondary mortgage market. \n\n"}
{"id": "2608153", "url": "https://en.wikipedia.org/wiki?curid=2608153", "title": "Elegant degradation", "text": "Elegant degradation\n\nElegant degradation is a term used in engineering to describe what occurs to machines which are subject to constant, repetitive stress.\n\nExternally, such a machine maintains the same appearance to the user, appearing to function properly. Internally, the machine slowly weakens over time. Eventually, unable to withstand the stress, it breaks down. Compared to graceful degradation, the operational quality does not decrease at all, but the breakdown may be just as sudden.\n\nThis term's meaning varies depending on context and field, and may not be strictly considered exclusive to engineering.\n"}
{"id": "7522652", "url": "https://en.wikipedia.org/wiki?curid=7522652", "title": "Elmore delay", "text": "Elmore delay\n\nElmore delay is a simple approximation to the delay through an RC network in an electronic system. It is often used in applications such as logic synthesis, delay calculation, static timing analysis, placement and routing, since it is simple to compute (especially in tree structured networks, which are the vast majority of signal nets within ICs) and is reasonably accurate. Even where it is not accurate, it is usually faithful, in the sense that reducing the Elmore delay will almost always reduce the true delay, so it is still useful in optimization.\n\nElmore delay can be thought of in several ways, all mathematically identical.\n\nThere are many extensions to Elmore delay. It can be extended to upper and lower bounds, to include inductance as well as R and C, to be more accurate (higher order approximations) and so on. See delay calculation for more details and references.\n\n"}
{"id": "44035472", "url": "https://en.wikipedia.org/wiki?curid=44035472", "title": "Exit sign", "text": "Exit sign\n\nAn exit sign is a device in a public facility (such as a building, aircraft, or boat) denoting the location of the closest emergency exit in case of fire or other emergency. Most relevant codes (fire, building, health, or safety) require exit signs to be permanently lit.\n\nExit signs are designed to be absolutely unmistakable and understandable to anyone. In the past, this generally meant exit signs that show the word \"EXIT\" or the equivalent in the local language, but increasingly exit signs around the world are in pictogram form, with or without supplementary text.\n\nEarly exit signs were generally either made of metal and lit by a nearby incandescent light bulb or were a white glass cover with \"EXIT\" written in red, placed directly in front of a single-bulb light fixture. An inherent flaw with these designs was that in a fire, the power to the light often failed. In addition, the fixtures could be difficult to see in a fire where smoke often reduced visibility, despite being relatively bright. The biggest problem was that the exit sign was hardly distinguishable from an ordinary safety lighting fixture commonly installed above doors in the past. The problem was partially solved by using red-tinted globes instead.\n\nBetter signs were soon developed that more resembled today's modern exit sign, with an incandescent bulb inside a rectangular-shaped box that backlit the word \"EXIT\" on one or both sides. Being larger than its predecessors, this version of the exit sign solved some of the visibility problems. The sign was only useful as long as mains power remained on.\n\nAs battery-backup systems became smaller and more efficient, some exit signs began to use a dual-power system. Under normal conditions, the exit sign was lit by mains power and the battery was in a charged state. In the event of a power outage, the battery would supply power to light the sign. It continued to discharge until mains power returned to the unit, or the battery was no longer able to provide sufficient power to light the sign. Early battery-backup systems were big, heavy, and costly. Modern systems are lightweight, can be installed virtually anywhere, and are integrated into the fixture, rather than requiring a separate box. As batteries improved, so did the amount of time that a fixture could remain lit on batteries.\n\nWhile exit signs were more visible due to large letters, even a 60-watt incandescent bulb shining through a plastic or glass cover could appear somewhat dim under certain conditions. Incandescent bulbs are still in use because they are cheap and common, even though they use more electricity and require more or less frequent replacement. Incandescent bulbs lit 24/7 have a greatly extended lifespan compared to ones that cycle on and off. When used in exit signs, they are often operated at a lower voltage than rated, which further extends their lifetime, at the tradeoff of reduced light output and greatly reduced energy efficiency.\n\nWith the development of fluorescent lamp and light-emitting diode technology, exit signs could be made even brighter to compensate for the limited visibility in a fire situation, while using less electricity. Fluorescent lamps work in the same way as incandescent bulbs, back-lighting both sides of an exit fixture from within. LED signs combine a large number of bright light-emitting diodes to illuminate the sign from inside. An exit sign is constantly lit; fluorescent bulbs need to be changed more often than LEDs, although the absence of frequent on/off cycles extends the life of fluorescent lamps significantly. Generally, LEDs have a very long life, and may last for 10 years or more of continuous use, although their brightness may gradually diminish. \n\nRadioluminescent and phosphorescent signs that require no electricity have also been developed, and have been used since the 1970s. Radioluminescence uses the radioactive decay of tritium gas to illuminate the sign, while phosphorescence uses light-emitting phosphors to glow in the dark. While both of these signs meet California State Fire Marshal standards, electricity is used in the vast majority of signs.\n\nMost exit signs in the world, except in countries such as the US, Australia, Hong Kong, Philippines, and Singapore, are of pictogram type. Australia, Singapore, and Hong Kong have made changes to their respective life safety codes to encourage pictogram use. \n\nIn the US, regulations require text based exit signs for all standard mounted applications. However, globalization has led to limited adoption of the ISO pictogram in the US. For example, the NFPA has approved the ISO pictogram as an option for low level glow-in-the-dark signs. New York City local law 26 requires these low level symbolic signs in all high rise buildings. In tunnels, the Transportation Research Board recommends using the ISO symbol. Newer Airbus, Bombardier CS100 and Boeing 787 Dreamliner planes started using the new pictogram exit signs, which were approved by the FAA in 2012 and 2014 (depending on aircraft type).\n\nModern exit signs often can be seen indicating the path to an exit in commercial and large residential buildings that comply with fire code. Certain circumstances, such as the year a building was built, may leave it exempt from some of these codes. In most situations, the owner of the building is responsible for complying with exit sign requirements. This is especially true in older buildings that serve as multiple residences, such as apartment buildings, hotels, and campus dormitories. Modern signs can also adopt active and/or dissuasive signage solutions . \n\nModern fixtures are usually in a rugged plastic or metal housing securely bolted to the wall or ceiling. The signs have the word \"EXIT\", or a picture representing exit, on both sides. Single-sided signs are also available for wall-mount installations. The signs often have metal or plastic knock-outs which can be removed so that an arrow is also lit pointing left or right. Modern exit signs are often combined with other safety devices, such as emergency floodlighting for supplementary area illumination . Exit signs draw a relatively small amount of power, and can generally be added onto any existing electrical circuit without adverse effects. Modern exit signs are also, to some degree, flame retardant.\n\nIn addition, specialized LED lamps with \"candelabra\" sized screw-in bases are available to replace the always-on incandescent lamps in older exit signs. This allows the existing fixtures to be easily upgraded to save energy, without the expense of complete replacement.\n\nMost recently, LEC (light emitting capacitor) exit signs have come to market. Also called electroluminescent (EL), these signs only consume 1/4W of power, and have an operational life of 30+ years which far exceeds the typical 10-year life of an LED sign.\n\nIt is a serious offense as a building owner or landlord to not comply with the fire/building code in terms of Exit signage. In July 2016, for example, a fire in a Toronto apartment caused the death of one person and injured many others.An investigation found that Emergency and Exit lights were not properly illuminated, and the landlord was fined about $20,000 for \"not properly illuminating exit signs, and having no record of emergency lighting tests\", and another $50,000 for other infractions to the code.\n\nSince visibility may be reduced in a fire, due to smoke or failure of electric lighting, the sign is often permanently illuminated, usually by one of:\n\n\nIn most regions, including the European Union, Australia, New Zealand, Japan, South Korea, and China, exit signs have green lettering. (In this color scheme, red is reserved to indicate prohibited activities.) In most European countries, pictograms are used in place of the word \"exit\". European sign directive 92/58/EEC of 24 June 1992 indicates that the signs should be green in color to indicate a safe place of exit. BS EN 1838:1999, BS 5266-7:1999 also governs the emergency lighting applications.\n\nIn the US, exit signs can have either red or green lettering, but traditionally have been red. Many states or cities have enacted building codes which specify the sign color. For example, in Baltimore, Salt Lake City, and Portland, Oregon, green is required. New York City, Rhode Island, and Chicago (along with the rest of Illinois) require that exit signs have red text.\n\nNew and renovated buildings in Canada are required to use the international standard green \"running man\" pictogram. The 1995 Canadian national building code required \"red letters on a contrasting background or white letters on a red background ... spelling EXIT or SORTIE\", however the 2010 Code calls for a switch from the red EXIT signs to the green \"running-man\" signs. The national building code informs provincial and municipal building codes but does not have legal status itself. Most Canadian jurisdictions require the international green \"running-man\" pictogram, however some have allowed red \"EXIT\" signs to be maintained in older properties so long as one style is used consistently throughout the building.\n\nGreen fluorescent signs can be seen better in dark conditions than other colors, as the human rod cell is more sensitive to these wavelengths.\n\nNewly installed exit signs in Australia are green with white \"running man\" figure (AS2293).\n\nThere is a trend towards providing a more accessible, socially inclusive exit sign design based on universal design principles, including consideration for people with disability in an overall exit sign strategy for a building or facility.\n\nThe Accessible Exit Sign Project started in Australia in 2014, has spread to New Zealand and the United States, and is an international awareness campaign that promotes the need for an accessible means of egress. Advocates proposes that appropriate exit signage to identify the accessible means of egress is a critical component to the successful emergency planning for any building.\n\nThe proposed new exit sign design features an \"Accessible Means of Egress Icon\", which includes an adaptation of the \"running man\" symbol with a new wheelchair symbol. The design is considered an enhanced version of the ISO 7010 and ISO 21542 accessible exit sign that shows the \"running man\" and International Symbol of Access at the end of the sign. The universally inclusive design with the \"running man\" and 'Accessible Means of Egress Icon' wheelchair symbol essentially share the same upper torso, and the design shows the two moving through the door together. The Global Alliance on Accessible Technologies and Environments (GAATES) has stated that the introduction of the \"Accessible Means of Egress Icon\" onto exit signage changes the current discriminatory approach to emergency exit signs and presents a fully inclusive design. \"The combined ‘Running Man’ and ‘Accessible Means of Egress Icon’ shown above are working together to escape the building. They move in unison, display the same urgency and motion and appear to be travelling at the same speed. Their heads are forward, showing their haste. Arms are extended and motioning back and forth as they move through the doorway.\"\n\nThe accessible exit signs are now being produced in Australia, New Zealand, United States and United Kingdom, also featuring braille and tactile lettering suitable for people that have low vision or are blind. The design is intended to show where wheelchair accessible exit routes, evacuation lifts, evacuation devices and areas of refuge are located. The concept also provides more intuitive building design to assist people that are blind or have low vision to locate an exit. The design also meets the intent of the UN Convention on the Rights of Persons with Disabilities which requires signatory countries to consider the need for universal design in buildings.\n"}
{"id": "286681", "url": "https://en.wikipedia.org/wiki?curid=286681", "title": "Flash point", "text": "Flash point\n\nThe flash point of a volatile material is the lowest temperature at which vapours of the material will ignite, when given an ignition source. \n\nThe flash point is sometimes confused with the autoignition temperature, the temperature that results in spontaneous autoignition. The fire point is the lowest temperature at which vapors of the material will keep burning after the ignition source is removed. The fire point is higher than the flash point, because at the flash point more vapor may not be produced rapidly enough to sustain combustion. Neither flash point nor fire point depends directly on the ignition source temperature, but ignition source temperature is far higher than either the flash or fire point.\n\nThe flash point is a descriptive characteristic that is used to distinguish between flammable fuels, such as petrol (gasoline in the US), and combustible fuels, such as diesel.\n\nIt is also used to characterize the fire hazards of fuels. Fuels which have a flash point less than are called flammable, whereas fuels having a flash point above that temperature are called combustible.\n\nAll liquids have a specific vapor pressure, which is a function of that liquid's temperature and is subject to Boyle's Law. As temperature increases, vapor pressure increases. As vapor pressure increases, the concentration of vapor of a flammable or combustible liquid in the air increases. Hence, temperature determines the concentration of vapor of the flammable liquid in the air. A certain concentration of a flammable or combustible vapor is necessary to sustain combustion in air, the lower flammable limit, and that concentration is different and is specific to each flammable or combustible liquid. The flash point is the lowest temperature at which there will be enough flammable vapor to induce ignition when an ignition source is applied\n\nThere are two basic types of flash point measurement: open cup and closed cup. In open cup devices, the sample is contained in an open cup which is heated and, at intervals, a flame brought over the surface. The measured flash point will actually vary with the height of the flame above the liquid surface and, at sufficient height, the measured flash point temperature will coincide with the fire point. The best-known example is the Cleveland open cup (COC).\n\nThere are two types of closed cup testers: non-equilibrial, such as Pensky-Martens, where the vapours above the liquid are not in temperature equilibrium with the liquid, and equilibrial, such as Small Scale (commonly known as Setaflash), where the vapours are deemed to be in temperature equilibrium with the liquid. In both these types, the cups are sealed with a lid through which the ignition source can be introduced. Closed cup testers normally give lower values for the flash point than open cup (typically lower) and are a better approximation to the temperature at which the vapour pressure reaches the lower flammable limit.\n\nThe flash point is an empirical measurement rather than a fundamental physical parameter. The measured value will vary with equipment and test protocol variations, including temperature ramp rate (in automated testers), time allowed for the sample to equilibrate, sample volume and whether the sample is stirred.\n\nMethods for determining the flash point of a liquid are specified in many standards. For example, testing by the Pensky-Martens closed cup method is detailed in ASTM D93, IP34, ISO 2719, DIN 51758, JIS K2265 and AFNOR M07-019. Determination of flash point by the Small Scale closed cup method is detailed in ASTM D3828 and D3278, EN ISO 3679 and 3680, and IP 523 and 524.\n\nCEN/TR 15138 Guide to Flash Point Testing and ISO TR 29662 Guidance for Flash Point Testing cover the key aspects of flash point testing.\n\nGasoline (petrol) is a fuel used in a spark-ignition engine. The fuel is mixed with air within its flammable limits and heated by compression and subject to Boyle's Law above its flash point, then ignited by the spark plug. To ignite, the fuel must have a low flash point, but in order to avoid preignition caused by residual heat in a hot combustion chamber, the fuel must have a high autoignition temperature.\n\nDiesel fuel flash points vary between . Diesel is suitable for use in a compression-ignition engine. Air is compressed until it has been heated above the autoignition temperature of the fuel, which is then injected as a high-pressure spray, keeping the fuel–air mix within flammable limits. In a diesel-fueled engine, there is no ignition source (such as the spark plugs in a gasoline engine). Consequently, diesel fuel must have a high flash point and a low autoignition temperature.\n\nJet fuel flash points also vary with the composition of the fuel. Both Jet A and Jet A-1 have flash points between , close to that of off-the-shelf kerosene. Yet both Jet B and JP-4 have flash points between .\n\nFlash points of substances are measured according to standard test methods described and defined in a 1938 publication by T.L. Ainsley of South Shields entitled \"Sea Transport of Petroleum\" (Capt. P. Jansen). The test methodology defines the apparatus required to carry out the measurement, key test parameters, the procedure for the operator or automated apparatus to follow, and the precision of the test method. Standard test methods are written and controlled by a number of national and international committees and organizations. The three main bodies are the CEN / ISO Joint Working Group on Flash Point (JWG-FP), ASTM D02.8B Flammability Section and the Energy Institute's TMS SC-B-4 Flammability Panel.\n\n"}
{"id": "27682494", "url": "https://en.wikipedia.org/wiki?curid=27682494", "title": "GSF Development Driller II", "text": "GSF Development Driller II\n\nGSF Development Driller II is a fifth generation, Vanuatu-flagged dynamic positioning semi-submersible ultra-deepwater drilling rig owned and operated by Transocean. The vessel is capable of drilling in water depths up to with drilling depth of , upgradeable to .\n\n\"Development Driller II\" was deployed in drilling a relief well on the Macondo Prospect to stop the massive oil spill caused by the explosion and subsequent loss of \"Deepwater Horizon\".\n\n"}
{"id": "30875067", "url": "https://en.wikipedia.org/wiki?curid=30875067", "title": "Gristmill", "text": "Gristmill\n\nA gristmill (also: grist mill, corn mill or flour mill) grinds cereal grain into flour and middlings. The term can refer to both the grinding mechanism and the building that holds it.\n\nThe Greek geographer Strabo reports in his \"Geography\" a water-powered grain-mill to have existed near the palace of king Mithradates VI Eupator at Cabira, Asia Minor, before 71 BC.\nThe early mills had horizontal paddle wheels, an arrangement which later became known as the \"Norse wheel\", as many were found in Scandinavia. The paddle wheel was attached to a shaft which was, in turn, attached to the centre of the millstone called the \"runner stone\". The turning force produced by the water on the paddles was transferred directly to the runner stone, causing it to grind against a stationary \"bed\", a stone of a similar size and shape. This simple arrangement required no gears, but had the disadvantage that the speed of rotation of the stone was dependent on the volume and flow of water available and was, therefore, only suitable for use in mountainous regions with fast-flowing streams. This dependence on the volume and speed of flow of the water also meant that the speed of rotation of the stone was highly variable and the optimum grinding speed could not always be maintained.\n\nVertical wheels were in use in the Roman Empire by the end of the first century BC, and these were described by Vitruvius. The peak of Roman technology is probably the Barbegal aqueduct and mill where water with a 19-metre fall drove sixteen water wheels, giving a grinding capacity estimated at 2.4 to 3.2 tonnes per hour. Water mills seem to have remained in use during the post-Roman period, and by 1000 AD, mills in Europe were rarely more than a few miles apart.\nIn England, the Domesday survey of 1086 gives a precise count of England's water-powered flour mills: there were 5,624, or about one for every 300 inhabitants, and this was probably typical throughout western and southern Europe. From this time onward, water wheels began to be used for purposes other than grist milling. In England, the number of mills in operation followed population growth, and peaked at around 17,000 by 1300.\n\nLimited extant examples of gristmills can be found in Europe from the High Middle Ages. An extant well-preserved waterwheel and gristmill on the Ebro River in Spain is associated with the Real Monasterio de Nuestra Senora de Rueda, built by the Cistercian monks in 1202. The Cistercians were known for their use of this technology in Western Europe in the period 1100 to 1350.\n\nGeared gristmills were also built in the medieval Near East and North Africa, which were used for grinding grain and other seeds to produce meals. Gristmills in the Islamic world were powered by both water and wind. The first wind-powered gristmills were built in the 9th and 10th centuries in what are now Afghanistan, Pakistan and Iran.\n\nAlthough the terms \"gristmill\" or \"corn mill\" can refer to any mill that grinds grain, the terms were used historically for a local mill where farmers brought their own grain and received back ground meal or flour, minus a percentage called the \"miller's toll.\" Early mills were almost always built and supported by farming communities and the miller received the \"miller's toll\" in lieu of wages. Most towns and villages had their own mill so that local farmers could easily transport their grain there to be milled. These communities were dependent on their local mill as bread was a staple part of the diet.\n\nClassical mill designs are usually water-powered, though some are powered by the wind or by livestock. In a watermill a sluice gate is opened to allow water to flow onto, or under, a water wheel to make it turn. In most watermills the water wheel was mounted vertically, i.e., edge-on, in the water, but in some cases horizontally (the tub wheel and so-called Norse wheel). Later designs incorporated horizontal steel or cast iron turbines and these were sometimes refitted into the old wheel mills.\n\nIn most wheel-driven mills, a large gear-wheel called the \"pit wheel\" is mounted on the same axle as the water wheel and this drives a smaller gear-wheel, the \"wallower\", on a main driveshaft running vertically from the bottom to the top of the building. This system of gearing ensures that the main shaft turns faster than the water wheel, which typically rotates at around 10 rpm.\n\nThe millstones themselves turn at around 120 rpm. They are laid one on top of the other. The bottom stone, called the \"bed\", is fixed to the floor, while the top stone, the \"runner\", is mounted on a separate spindle, driven by the main shaft. A wheel called the \"stone nut\" connects the runner's spindle to the main shaft, and this can be moved out of the way to disconnect the stone and stop it turning, leaving the main shaft turning to drive other machinery. This might include driving a mechanical sieve to refine the flour, or turning a wooden drum to wind up a chain used to hoist sacks of grain to the top of the mill house. The distance between the stones can be varied to produce the grade of flour required; moving the stones closer together produces finer flour.\n\nThe grain is lifted in sacks onto the \"sack floor\" at the top of the mill on the hoist. The sacks are then emptied into bins, where the grain falls down through a hopper to the millstones on the \"stone floor\" below. The flow of grain is regulated by shaking it in a gently sloping trough (the \"slipper\") from which it falls into a hole in the center of the runner stone. The milled grain (flour) is collected as it emerges through the grooves in the runner stone from the outer rim of the stones and is fed down a chute to be collected in sacks on the ground or \"meal\" floor. A similar process is used for grains such as wheat to make flour, and for maize to make corn meal.\n\nIn order to prevent the vibrations of the mill machinery from shaking the building apart, a gristmill will often have at least two separate foundations.\n\nAmerican inventor Oliver Evans revolutionized this labor-intensive process at the end of the eighteenth century when he patented and promoted a fully automated mill design.\n\nModern mills typically use electricity or fossil fuels to spin heavy steel, or cast iron, serrated and flat rollers to separate the bran and germ from the endosperm. The endosperm is ground to create white flour, which may be recombined with the bran and germ to create whole grain or graham flour. The different milling techniques produce visibly different results, but can be made to produce nutritionally and functionally equivalent output. Stone-ground flour is, however, preferred by many bakers and natural food advocates because of its texture, nutty flavour, and the belief that it is nutritionally superior and has a better baking quality than steel-roller-milled flour. It is claimed that, as the stones grind relatively slowly, the wheat germ is not exposed to the sort of excessive temperatures that could cause the fat from the germ portion to oxidize and become rancid, which would destroy some of the vitamin content. Stone-milled flour has been found to be relatively high in thiamin, compared to roller-milled flour, especially when milled from hard wheat.\n\nGristmills only grind \"clean\" grains from which stalks and chaff have previously been removed, but historically some mills also housed equipment for threshing, sorting, and cleaning prior to grinding.\n\nModern mills are usually \"merchant mills\" that are either privately owned and accept money or trade for milling grains or are owned by corporations that buy unmilled grain and then own the flour produced.\n\nOne common pest found in flour mills is the Mediterranean flour moth. Moth larvae produce a web-like material that clogs machinery, sometimes causing grain mills to shut down.\n\n\nPeople\n\n\n\n\n"}
{"id": "46186742", "url": "https://en.wikipedia.org/wiki?curid=46186742", "title": "History of infrastructure", "text": "History of infrastructure\n\nInfrastructure before 1700 consisted mainly of roads and canals. Canals were used for transportation or for irrigation. Sea navigation was aided by ports and lighthouses. A few advanced cities had aqueducts that serviced public fountains and baths, while fewer had sewers.\n\nThe earliest railways were used in mines or to bypass waterfalls, and were pulled by horses or by people. In 1811 John Blenkinsop designed the first successful and practical railway locomotive, and a line was built connecting the Middleton Colliery to Leeds.\n\nThe electrical telegraph was first successfully demonstrated on 25 July 1837 between Euston and Camden Town in London. It entered commercial use on the Great Western Railway over the from Paddington station to West Drayton on 9 April 1839. In 1876, Alexander Graham Bell achieved the first successful telephone transmission of clear speech. Soon, a bell was added for signalling, and then a switch-hook, and telephones took advantage of the exchange principle already employed in telegraph networks.\n\nIn 1863 the London Underground was created. In 1890, it first started using electric traction and deep-level tunnels. At the Paris Exposition of 1878, electric arc lighting had been installed along the Avenue de l'Opera and the Place de l'Opera. In 1925, Italy was the first country to build a freeway-like road, which linked Milan to Como.\n\nIn 1982, the Internet Protocol Suite (TCP/IP) was standardised and the concept of a world-wide network of fully interconnected TCP/IP networks called the Internet was introduced.\n\nInfrastructure before 1700 consisted mainly of roads and canals. Canals were used for transportation or for irrigation. Sea navigation was aided by ports and lighthouses. A few advanced cities had aqueducts that serviced public fountains and baths, while fewer had sewers.\n\nThe first roads were tracks that often followed game trails, such as the Natchez Trace.\n\nThe first paved streets appear to have been built in Ur in 4000 BCE. Corduroy roads were built in Glastonbury, England in 3300 BCE and brick-paved roads were built in the Indus Valley Civilisation on the Indian subcontinent from around the same time. In 500 BCE, Darius I the Great started an extensive road system in Persia (Iran), including the Royal Road.\n\nWith the rise of the Roman Empire, the Romans built roads using deep roadbeds of crushed stone as an underlying layer to ensure that they kept dry. On the more heavily travelled routes, there were additional layers that included six sided capstones, or pavers, that reduced the dust and reduced the drag from wheels.\n\nIn the medieval Islamic world, many roads were built throughout the Arab Empire. The most sophisticated roads were those of the Baghdad, Iraq, which were paved with tar in the 8th century.\n\nThe oldest known canals were built in Mesopotamia c. 4000 BCE, in what is now Iraq and Syria. The Indus Valley Civilisation in India and Pakistan from c3300 BCE had a sophisticated canal irrigation system. In Egypt, canals date back to at least 2300 BCE, when a canal was built to bypass the cataract on the Nile near Aswan.\n\nIn ancient China, large canals for river transport were established as far back as the Warring States (481-221 BCE). By far the longest canal was the Grand Canal of China completed in 609 CE, still the longest canal in the world today at .\n\nIn Europe, canal building began in the Middle Ages because of commercial expansion from the 12th century. Notable canals were the Stecknitz Canal in Germany in 1398, the Briare Canal connecting the Loire and Seine in France in 1642, followed by the Canal du Midi in 1683 connecting the Atlantic to the Mediterranean. Canal building progressed steadily in Germany in the 17th and 18th centuries with three great rivers, the Elbe, Oder, and Weser being linked by canals.\n\nAs traffic levels increased in England and roads deteriorated, toll roads were built by \"Turnpike Trusts\", especially between 1730 and 1770. Turnpikes were also later built in the United States. They were usually built by private companies under a government franchise.\n\nWater transport on rivers and canals carried many farm goods from the US frontier between the Appalachian Mountains and Mississippi River in the early 19th century, but the shorter road route over the mountains had advantages.\n\nIn France, Pierre-Marie-Jérôme Trésaguet is widely credited with establishing the first scientific approach to road building about the year 1764. It involved a layer of large rocks, covered by a layer of smaller gravel. John Loudon McAdam (1756–1836) designed the first modern highways, and developed an inexpensive paving material of soil and stone aggregate known as macadam.\n\nIn Europe, particularly Britain and Ireland, and then in the early US and the Canadian colonies, inland canals preceded the development of railroads during the earliest phase of the Industrial Revolution. In Britain between 1760 and 1820 over one hundred canals were built.\n\nIn the United States, navigable canals reached into isolated areas and brought them in touch with the world beyond. By 1825 the Erie Canal, long with 82 locks, opened up a connection from the populated northeast to the fertile Great Plains. During the 19th century, the length of canals grew from to over , with a complex network in conjunction with Canada making the Great Lakes navigable, although some canals were later drained and used as railroad rights-of-way.\n\nThe earliest railways were used in mines or to bypass waterfalls, and were pulled by horses or by people. In 1811 John Blenkinsop designed the first successful and practical railway locomotive, and a line was built connecting the Middleton Colliery to Leeds. The Liverpool and Manchester Railway, considered to be the world's first intercity line, opened in 1826. In the following years, railways spread throughout the United Kingdom and the world, and became the dominant means of land transport for nearly a century.\n\nIn the US, the 1826 Granite Railway in Massachusetts was the first commercial railroad to evolve through continuous operations into a common carrier. The Baltimore and Ohio, opened in 1830, was the first to evolve into a major system. In 1869, the symbolically important transcontinental railroad was completed in the US with the driving of a golden spike at Promontory, Utah.\n\nThe electrical telegraph was first successfully demonstrated on 25 July 1837 between Euston and Camden Town in London. It entered commercial use on the Great Western Railway over the from Paddington station to West Drayton on 9 April 1839.\n\nIn the United States, the telegraph was developed by Samuel Morse and Alfred Vail. On 24 May 1844, Morse made the first public demonstration of his telegraph by sending a message from the Supreme Court Chamber in the US Capitol in Washington, DC to the B&O Railroad outer depot (now the B&O Railroad Museum) in Baltimore. The Morse/Vail telegraph was quickly deployed in the following two decades. On 24 October 1861, the first transcontinental telegraph system was established.\n\nThe first successful transatlantic telegraph cable was completed on 27 July 1866, allowing transatlantic telegraph communications for the first time. Within 29 years of its first installation at Euston Station, the telegraph network crossed the oceans to every continent but Antarctica, making instant global communication possible for the first time.\n\nTar-bound macadam, or tarmac, was applied to macadam roads towards the end of the 19th century in cities such as Paris. In the early 20th century tarmac and concrete paving were extended into the countryside.\n\nMany notable sea canals were completed in this period, such as the Suez Canal in 1869, the Kiel Canal in 1897, and the Panama Canal in 1914.\n\nIn 1876, Alexander Graham Bell achieved the first successful telephone transmission of clear speech. The first telephones had no network, but were in private use, wired together in pairs. Users who wanted to talk to different people had as many telephones as necessary for the purpose. A user who wished to speak, whistled into the transmitter until the other party heard. Soon, however, a bell was added for signalling, and then a switch-hook, and telephones took advantage of the exchange principle already employed in telegraph networks. Each telephone was wired to a local telephone exchange, and the exchanges were wired together with trunks. Networks were connected together in a hierarchical manner until they spanned cities, countries, continents, and oceans.\n\nAt the Paris Exposition of 1878, electric arc lighting had been installed along the Avenue de l'Opera and the Place de l'Opera, using electric Yablochkov arc lamps, powered by Zénobe Gramme alternating current dynamos.\n\nYablochkov candles required high voltages, and it was not long before experimenters reported that the arc lights could be powered on a seven-mile (11 km) circuit. Within a decade scores of cities would have lighting systems using a central power plant that provided electricity to multiple customers via electrical transmission lines. These systems were in direct competition with the dominant gaslight utilities of the period.\n\nThe first electricity system supplying incandescent lights was built by the Edison Illuminating Company in lower Manhattan, eventually serving one square mile with six \"jumbo dynamos\" housed at Pearl Street Station.\n\nThe first transmission of three-phase alternating current using high voltage took place in 1891 during the International Electro-Technical Exhibition in Frankfurt. A 25 kilovolt transmission line, approximately long, connected Lauffen on the Neckar with Frankfurt. Voltages used for electric power transmission increased throughout the 20th century. By 1914 fifty-five transmission systems operating at more than 70,000 V were in service, the highest voltage then being used was 150,000  V.\n\nIn the 19th century major treatment works were built in London in response to cholera threats. The Metropolis Water Act 1852 was enacted. \"Under the Act, it became unlawful for any water company to extract water for domestic use from the tidal reaches of the Thames after 31 August 1855, and from 31 December 1855 all such water was required to be effectively filtered. The \"Metropolitan Commission of Sewers\" was formed, water filtration was made compulsory, and new water intakes on the Thames were established above Teddington Lock.\n\nThe technique of purification of drinking water by use of compressed liquefied chlorine gas was developed in 1910 by US Army Major Carl Rogers Darnall, Professor of Chemistry at the Army Medical School. Darnall's work became the basis for present day systems of municipal water purification.\n\nIn 1863 the London Underground was created. In 1890, it first started using electric traction and deep-level tunnels. Soon afterwards, Budapest and many other cities started using subway systems. By 1940, nineteen subway systems were in use.\n\nIn 1925, Italy was the first country to build a freeway-like road, which linked Milan to Como, known as the Autostrada dei Laghi. In Germany, the autobahns formed the first limited-access, high-speed road network in the world, with the first section from Frankfurt am Main to Darmstadt opening in 1935. The first long-distance rural freeway in the United States is generally considered to be the Pennsylvania Turnpike, which opened on October 1, 1940. In the United States, the Interstate Highway System was authorised by the Federal-Aid Highway Act of 1956. Most of the system was completed between 1960 and 1990.\n\nResearch into packet switching started in the early 1960s. The ARPANET in particular led to the development of protocols for internetworking, where multiple separate networks could be joined together into a network of networks\nThe first two nodes of what would become the ARPANET were interconnected on 29 October 1969. Access to the ARPANET was expanded in 1981 when the National Science Foundation (NSF) developed the Computer Science Network (CSNET). In 1982, the Internet Protocol Suite (TCP/IP) was standardised and the concept of a world-wide network of fully interconnected TCP/IP networks called the Internet was introduced. TCP/IP network access expanded again in 1986 when the National Science Foundation Network (NSFNET) provided access to supercomputer sites in the United States from research and education organisations. Commercial internet service providers (ISPs) began to emerge in the late 1980s and early 1990s. The ARPANET was decommissioned in 1990. The Internet was commercialised in 1995 when NSFNET was decommissioned, removing the last restrictions on the use of the Internet to carry commercial traffic. The Internet started a rapid expansion to Europe and Australia in the mid to late 1980s and to Asia in the late 1980s and early 1990s.\nDuring the late 1990s, it was estimated that traffic on the public Internet grew by 100 percent per year, while the mean annual growth in the number of Internet users was thought to be between 20% and 50%. As of 31 March 2011, the estimated total number of Internet users was 2.095 billion (30.2% of world population).\n\n\n"}
{"id": "12780833", "url": "https://en.wikipedia.org/wiki?curid=12780833", "title": "Hoosier cabinet", "text": "Hoosier cabinet\n\nA Hoosier cabinet (also known as a \"Hoosier\") is a type of cupboard or free–standing kitchen cabinet that also serves as a workstation. It was popular in the first few decades of the 20th century, since most houses did not have built–in kitchen cabinetry. The Hoosier Manufacturing Co. of New Castle, Indiana, was one of the earliest and largest manufacturers of this product, causing the term \"Hoosier cabinet\" to become a generic term for that type of furniture. By 1920, the Hoosier Manufacturing Company had sold two million cabinets.\n\nHoosier style cabinets were also made by dozens of other companies, and most were in the Hoosier State or located nearby. Some of the larger manufacturers were Campbell-Smith-Ritchie (Boone); Coppes Brothers and Zook (the Napanee); McDougall Company; and G. I. Sellers and Sons. Hoosier cabinets evolved over the years to include more accessories and innovations that made life easier for cooks in the kitchen. They peaked in popularity in the 1920s, and declined as homes began to be constructed with built-in kitchen cabinets and counter tops. The Hoosier Manufacturing Company was sold in 1942 and liquidated. Today, Hoosier cabinets are valued by antique collectors.\n\nFrom 1890 to 1930, more houses were built in the United States than all of the country's prior years combined. Very few homes had built-in kitchen cabinets during the 19th century, and it was not until the late 1920s that built-in cabinets became a standard kitchen furnishing.\n\nAround the 1890s, several furniture manufacturers in Indiana discovered that a stand-alone kitchen cabinet with storage and a workspace (essentially a baker's cabinet with extra storage) was easy to sell. It was a kitchen workstation with ingredient and equipment storage where the cook could complete all food preparation and not move until it was time to cook the food. The Hoosier Manufacturing Company was one of those early manufacturers. In 1900, the company moved a short distance from Albany, Indiana to New Castle, Indiana. It began improving its manufacturing and distribution process, and was a strong believer in advertising. By 1906, it had 146 employees. The company's product was nationally promoted as a step saver, and its popularity led to the term Hoosier cabinet becoming a generic term for that style of kitchen cabinet—similar to the term kleenex (the facial tissue called Kleenex manufactured by Kimberly-Clark) becoming a generic term for facial tissues.\n\nThus, Hoosier cabinets were very popular from 1900 to 1930. Hoosier Manufacturing sold two million cabinets from its inception to 1920, and additional cabinets were sold by the company's competitors. Given that there were approximately 20 million households in the United States at that time, as much as 10 percent of homes had Hoosier cabinets made by Hoosier Manufacturing, and an additional unknown quantity had Hoosier cabinets made by competing companies.\n\nHoosier cabinets remained popular into the 1920s, but by that time houses began to be built with more modern kitchens that included built-in cabinets, counter tops, and other fixtures. Thus supplanted, the Hoosier cabinet largely disappeared. Some of the manufacturers diversified into built-in cabinets and kitchen furniture. However, the Great Depression made sales even more difficult. By 1935, Hoosier cabinets were considered \"old fashioned\". The two largest manufacturers, Hoosier Manufacturing and G. I. Sellers and Sons, were closed in 1942 and 1950, respectively.\n\nA Hoosier cabinet is a stand-alone kitchen cabinet on small casters. It is considered an improved version of a baker's cabinet. A baker's cabinet is a table with one or more bins underneath. It has a small work surface and a shallower upper section on top of the table that was used for storing bowls, pans, and kitchen utensils. The Hoosier cabinet expands on the baker's cabinet by offering a pull-out workspace/shelf and storage for everything a cook would need. The base section usually has one large compartment with the slide-out shelf covered in metal that offers more workspace, and several drawers to one side. The top portion is shallower and has several smaller compartments with doors.\n\nThe majority of the Hoosier cabinets are about wide by deep by high. In addition to their storage capacity, they offer about of counter space that was not available in the standard kitchen of the early 20th century. A distinctive feature of the Hoosier cabinet is its many moving parts and accessories. As originally supplied, they were equipped with various racks and other hardware to hold and organize spices and various staples. One particularly distinctive item is the combination flour-bin/sifter, a hopper that could be used without having to remove it from the cabinet. A similar sugar bin was also common. Additional accessories and innovations were added over the years. Special glass jars were manufactured to fit the cabinet and its racks. Original sets of Hoosier glassware consisted of coffee and tea canisters, a salt box, and four to eight spice jars. Some manufacturers also included a cracker jar. Colored glassware, ant-proof casters, and even ironing boards were innovations added later. Later models even included cards with reminders for grocery shopping and tips for meal planning.\n\nHoosier cabinets were made mostly from the late 1890s through the 1930s, reaching their peak in popularity during the 1920s. The major manufacturers of Hoosier cabinets at that time were located in Indiana. Hoosier Manufacturing was the largest.\n\nThe company began as Campbell & Smith in 1892, and it was primarily a lumber yard and planing mill. Lebanon is the county seat of Boone County, Indiana, and Campbell-Smith-Ritchie named its Hoosier-style kitchen cabinets Boone Kitchen Cabinets in honor of the county. In 1905, the lumberyard was destroyed by fire. The company built a new facility on the edge of town. By 1910, its kitchen cabinet making business was doing so well that the lumberyard portion of the business was discontinued. An Indiana inspection report for 1913 described the company as engaged in manufacturing furniture and having 90 employees. It was also the largest employer inspected in Lebanon.\n\nThe company advertised nationally, and claimed its product was designed by women from all over the nation. Models of the Boone cabinets were differentiated with names such as the Mary Boone, Bertha Boone (a larger model with storage closets at each end), and Betty Boone (a small model for apartments or smaller homes). Some included a hidden ironing board. The company, like many companies, prospered until the Great Depression. Demand for Hoosier cabinets declined at the time, first because of the difficult economic times, but also because homes began to be built with built–in cabinetry in the kitchen. The company responded by cutting back on employee hours and diversifying into built-in kitchen cabinets and breakfast dining sets. This strategy enabled it to survive into 1940 when it was sold. The company continued for another 18 months, but was then liquidated.\n\nThe Coppes Brothers and Zook decided to concentrate on kitchen cabinets in 1914. Their manufacturing facility was located in Nappanee, Indiana, and their Hoosier cabinet brand name was the Napanee Dutch Kitchenet (spelled with only one \"p\"). Using data from a study by a famous efficiency engineer (Harrington Emerson), the company claimed that their product could save 1,592 steps per day.\n\nEarlier, the Coppes brothers had a sawmill business, a box manufacturing factory, and a furniture manufacturing company. An Indiana inspection report for 1913 described their company (named Coppes, Zook, and Mutschler Co. at that time) as a \"saw mill, etc.\" and having 178 employees in a town with a population of 2,260. Right before the Great Depression started, the company began manufacturing built-in kitchen cabinets. This product was very successful, and continued for many years after the demise of the Hoosier cabinets.\n\nThe Hoosier Manufacturing Co. began in Albany, Indiana in the year 1898. The founders were glassmaker James McQuinn, his son Emmett McQuinn, and two business partners from Muncie, Indiana—John M. Maring, and Thomas Hart. Maring and Hart served as the president and vice president, respectively. The two McQuinns ran the business, with the elder McQuinn as the plant's general manager. The younger McQuinn was the advertising manager. Originally, the company used a former furniture manufacturing plant to make a seed separator used on farms. A secondary product, a stand-alone kitchen cabinet, sold better than the seed separator—and quickly became the company's main product. Each of the early Hoosier Cabinets was hand–made. The cabinet was similar to a baker's cabinet, with storage bins below a work space and a two-door upper section. However, the Hoosier Cabinet had \"meticulously organized interior storage\", which enabled it to serve as a kitchen workstation with all the necessary equipment and material within arm's reach.\n\nThe company's Albany facility was destroyed by a fire in 1900. At that time, the owners decided to restart in New Castle, Indiana, which is located about south of Albany. A profile of the McQuinns from a 1902 publication discussed the New Castle facility, and said that the \"output of the factory now amounts to nearly 200 complete kitchen cabinets per week, and sales are made of the article in every state in the Union, and many foreign countries.\"\n\nIn addition to its product, Hoosier Manufacturing's success can be attributed to its strengths in advertising, distribution, and manufacturing. Hoosier Manufacturing created its own dealer network, since some furniture dealers were not fond of a product that competed with their wares. In cases where they had no dealer, products were sold directly from the factory. The company was also a strong believer in advertising. Advertising was conducted in newspapers and national magazines such as Ladies' Home Journal and The Saturday Evening Post, where the reader was likely to be a woman. In 1903, the company began streamlining its manufacturing process by using interchangeable hardware, standardizing its products, and using an assembly line. The employee responsible for these innovations, Harry Hall, was also granted patents related to innovations for the cabinet and for a safety apparatus.\n\nBy 1913, Hoosier Manufacturing, listed as a furniture maker by Indiana factory inspectors, had 470 employees. In 1916, the company sold its one millionth Hoosier Cabinet and was clearly the leader in free-standing kitchen cabinets. By 1920, two million had been sold. During its peak years, the company produced nearly 700 cabinets per day, and was the largest manufacturer of kitchen cabinets in the United States.\n\nFree–standing kitchen cabinets began declining in popularity by 1930, and Hoosier Manufacturing responded by making kitchen cabinets that were meant to be built-in to the home. \nThe company also began manufacturing kitchen tables and breakfast sets. During World War II, supplies and man-power became scarce. The company was sold and liquidated in 1942.\n\nMcDougall was one of the early manufacturers of a Hoosier cabinet. An advertisement from 1919 identified the McDougall as \"the first kitchen cabinet\". George McDougall began the McDougall Company in Indianapolis, Indiana, sometime after the Civil War. The company's products were pie safes and kitchen tables. In 1898, the company name was changed to G. P. McDougall and Son. George's son Charles traveled to learn more about the furniture business, and persuaded his father to equip their kitchen tables with flour bins—a product that eventually became known as baker's cabinets. Charles also traveled to Europe and the European influence can be seen on McDougall cabinets. In 1909, a disgruntled employee set the factory on fire, destroying the entire facility.\n\nCharles McDougall, with several business partners, restarted the business in Frankfort, Indiana, in 1910. The company was simply named McDougall Company, and Hoosier cabinets were its product. The plant utilized the latest technology available for furniture manufacturing. The McDougall Hoosier cabinet had a patented auto-front roll door that dropped down to open instead of rolling up. Its flour bin had a glass front, which enabled one to tell how much flour was in the bin. In 1913, the McDougall plant had 148 employees, making it the largest (based on employees) factory in Frankfort.\n\nNear the end of the 1920s, the McDougall front door was changed to be similar to those used by other Hoosier cabinets. Its flour bins were also made smaller. The Great Depression was not good to the company, and it was reorganized in 1931. The company lasted only a few years longer before closing.\n\nThe G. I. Sellers Company was founded in Kokomo, Indiana, in 1888. The company made chifforobes, cabinets, and tables—and oak was their choice material. They grew to become the second largest manufacturer of Hoosier cabinets. By 1905, their manufacturing complex covered five city blocks. During that year, their plant was destroyed by fire. In order to restart their business as soon as possible, the company purchased a furniture factory in Elwood, Indiana. At that time, the company name was changed to the G. I. Sellers and Sons Company, and manufacturing was focused on Hoosier cabinets and tables.\n\nBy 1913, the Sellers plant employed 99 people, making it the second largest factory (based on employees) in Elwood. Among features Sellers promoted were an automatic lowering flour bin, glass drawer pulls, hand-rubbed finish on oak, and ant-proof casters. The company initiated a \"Votes for Women\" contest for little girls in 1914. Prizes were Junior Special Kitcheneed Cabinets, which were two–thirds the size of the Sellers Kitcheneed Special.\n\nIn 1922, Wilfred Sellers (company president) noted that the company typically produced 75,000 to 85,000 cabinets per year. Sellers introduced its Kitchenaire models in 1927, which had smaller flour bins but more drawers. In the early 1930s, coloring was featured, and new products were sold such as built-in kitchen cabinets and breakfast sets. During World War II, the company had difficulty acquiring raw materials and employing skilled workers. It ceased operations in 1950.\n\nBetween 1899 and 1949, over 40 manufacturers of stand-alone kitchen cabinets are known to have existed, and 18 of them were located in Indiana. Many of these manufacturers had much smaller advertising budgets compared to the major manufacturers, and it can be difficult to find information about them. Factory inspectors for the state of Indiana list at least eight companies, in addition to the major manufacturers, as kitchen cabinet makers in Indiana in 1913. Those companies are Cardinal Cabinet Company (Wabash), Greencastle Cabinet Company (Greencastle), Paul Manufacturing Company (Fort Wayne), Roach-Brown Manufacturing (Indianapolis), C. F. Schmoe Furniture Company (Shelbyville), Showers Brothers (Bloomington), Spiegle Cabinet Company (Shelbyville), and Wasmuch, Endicott Company (Andrews). Hoosier cabinets could also be mail-ordered from Montgomery Ward and Sears, Roebuck and Company.\n\n\nHoosier cabinets can be found on display in museums such as the Henry Ford Museum, the Children's Museum of Indianapolis, and the Kentucky Museum. However, Hoosier cabinets are found in other places too. Numerous antique dealers and restoration companies are involved with Hoosiers because nostalgic homeowners want this piece of furniture in their home. People are also interested in reproductions of the Hoosier cabinet.\n\nOne author credits the work of Philip and Phyllis Kennedy for boosting the popularity of Hoosier cabinets. The Kennedys researched the cabinet's history and procedures for restoration during the 1980s. In a 1999 book, another author discussing the Hoosier cabinet mentioned that some cooks are \"scouring antique stores, farm auctions, and flea markets for this unique, and still useful, piece of Americana.\"\n\nRestored Hoosier cabinets are often sought after for use in early-twentieth-century-style kitchens, in homes from that era or in new homes built in the styles of that era.\" In 2004, Hoosier cabinets sold at auction were often priced at over $1,000 (equivalent to $ in ). Oak versions with all of the accessories (flour bin, sugar bin, glass spice jars, etc.) have sold for over $2,000 (equivalent to $ in ). Cabinets without the original accessories are typically lower priced. Some of the accessories, such as the flour sifters and the spice jars made by Sneath Glass also sell quickly. As always, beware of counterfeits and Hoosier Manufacturing nameplates added to old cabinets.\n\n\"Hoosier cabinets live on as more than inspiration for new varieties of kitchen furniture. A hundred years after their rise to prominence, restored originals and recent reproductions have become cherished objects in many a home.\" \"The cabinets' sheer eccentricity, combined with their attractiveness as historic artifacts, has earned them a following equal to that of any group of collectors.\"\n\n\n"}
{"id": "13418907", "url": "https://en.wikipedia.org/wiki?curid=13418907", "title": "IEC 62379", "text": "IEC 62379\n\nIEC 62379 is a control engineering standard for the common control interface for networked digital audio and video products. IEC 62379 uses Simple Network Management Protocol to communicate control and monitoring information.\n\nIt is a family of standards that specifies a control framework for networked audio and video equipment and is published by the International Electrotechnical Commission. It has been designed to provide a means for entering a common set of management commands to control the transmission across the network as well as other functions within the interfaced equipment.\n\nThe parts within this standard include:\n\nPart one is common to all equipment that conforms to IEC 62379 and a preview of the published document can be downloaded from the IEC web store here, a section of the International Electrotechnical Commission web site. More information is available at the project group web site.\n\nPart 2, Audio has now been published and a preview can be downloaded from the IEC web store, a section the International Electrotechnical Commission web site.\n\nA first edition of Part 3, Video has been submitted to the IEC International Electrotechnical Commission technical committee for the commencement of the standardization process for this part.\nIt contains the video MIB required by Part 7.\n\nPart 7, Measurement, has been submitted to the IEC International Electrotechnical Commission technical committee for the commencement of the standardization process for this part.\nThis part specifies those aspects that are specific to the measurement requirements of the EBU ECN-IPM Group, a member of the Expert Communities Networks. An associated document EBU TECH 3345 has recently been published by the EBU European Broadcasting Union.\n\nPart 3 (Document 100/1896/NP) and Part 7 (Document 100/1897/NP) have been approved by IEC TC 100.\n\nPart 5.2, Transmission over Networks - Signalling, has now been published and can be downloaded from the IEC web store, \n\nIEC 62379-3:2015 Common control interface for networked digital audio and video products - Part 3: Video has now been published and can be downloaded from the IEC web store.\n\nIEC 62379-7:2015 Common control interface for networked digital audio and video products - Part 7: Measurements has now been published and can be downloaded from the IEC web store.\nIEC 62379-7:2015 is the standadised (and extended) version of EBU TECH 3345 - End-to-End IP Network Measurement - MIB & Parameters, which can be obtained from here: published by the EBU European Broadcasting Union.\n\n"}
{"id": "19989979", "url": "https://en.wikipedia.org/wiki?curid=19989979", "title": "Indexed file", "text": "Indexed file\n\nAn indexed file is a computer file with an index that allows easy random access to any record given its file key.\n\nThe key must be such that it uniquely identifies a record. If more than one index is present the other ones are called \"alternate indexes\". The indexes are created with the file and maintained by the system.\n\nSupport for indexed files is built into COBOL and PL/I. Other languages with more limited I/O facilities such as C support indexed files through add-on packages in a runtime library such as C-ISAM.\n\nThe COBOL language supports indexed files with the following command in the codice_1 section\n\nIn recent systems relational databases are often used in place of indexed files.\n\n\nهثم\n"}
{"id": "34276994", "url": "https://en.wikipedia.org/wiki?curid=34276994", "title": "James L. Hoard", "text": "James L. Hoard\n\nJames Lynn Hoard (December 28, 1905 – April 10, 1993) was an American chemist, a member of the Manhattan Project.\nHoard was also notable for his work on the Manhattan project where he overcame some important challenges with the uranium compounds. \nHoard was a member of the National Academy of Sciences, a recipient of the award for Distinguished Service in the Advancement of Inorganic Chemistry from the American Chemical Society, a Guggenheim fellow.\nThe New York Times called him \"an expert in crystallography whose work helped to explain crystalline and molecular structures\".\n\nHoard received a doctorate in chemistry from the California Institute of Technology in 1932. He accepted a faculty position at the Cornell University, was promoted to full professor in 1942, and advanced to emeritus status in 1971.\n"}
{"id": "17156", "url": "https://en.wikipedia.org/wiki?curid=17156", "title": "Kipper", "text": "Kipper\n\nA kipper is a whole herring, a small, oily fish, that has been split in a butterfly fashion from tail to head along the dorsal ridge, gutted, salted or pickled, and cold-smoked over smouldering woodchips (typically oak).\n\nIn the British Isles and a few North American regions, they are often eaten for breakfast. In Great Britain, kippers, along with other preserved smoked or salted fish such as the bloater and buckling, were also once commonly enjoyed as a high tea or supper treat, most popularly with inland and urban working-class populations before World War II.\n\nThe English philologist and ethnographer Walter William Skeat derives the word from the Old English \"kippian\", to spawn. The word has various possible parallels, such as Icelandic \"kippa\" which means \"to pull, snatch\" and the Germanic word \"kippen\" which means \"to tilt, to incline\". Similarly, the Middle English \"kipe\" denotes a basket used to catch fish. Another theory traces the word kipper to the \"kip\", or small beak, that male salmon develop during the breeding season.\n\nAs a verb, \"kippering\" (\"to kipper\") means to preserve by rubbing with salt or other spices before drying in the open air or in smoke. \nOriginally applied to the preservation of surplus fish (particularly those known as \"kips,\" harvested during spawning runs), \"kippering\" has come to mean the preservation of any fish, poultry, beef or other meat in like manner. The process is usually enhanced by cleaning, filleting, butterflying or slicing the food to expose maximum surface area to the drying and preservative agents.\n\nAlthough the exact origin of the kipper is unknown, this processes of slitting, gutting, and smoke-curing fish is well documented. According to Mark Kurlansky, \"Smoked foods almost always carry with them legends about their having been created by accident—usually the peasant hung the food too close to the fire, and then, imagine his surprise the next morning when …\". For instance Thomas Nashe wrote in 1599 about a fisherman from Lothingland in the Great Yarmouth area who discovered smoking herring by accident. Another story of the accidental invention of kipper is set in 1843, with John Woodger of Seahouses in Northumberland, when fish for processing was left overnight in a room with a smoking stove. These stories and others are known to be untrue because the word \"kipper\" long predates this. Smoking and salting of fish—in particular of spawning salmon and herring which are caught in large numbers in a short time and can be made suitable for edible storage by this practice predates 19th century Britain and indeed written history, probably going back as long as humans have been using salt to preserve food.\n\nA kipper is also sometimes referred to as a \"red herring\", although particularly strong curing is required to produce a truly red kipper.\nThe term appears in a mid-13th century poem by the Anglo-Norman poet Walter of Bibbesworth, \"He eteþ no ffyssh But heryng red.\" Samuel Pepys used it in his diary entry of 28 February 1660 \"Up in the morning, and had some red herrings to our breakfast, while my boot-heel was a-mending, by the same token the boy left the hole as big as it was before.\"\n\nThe dyeing of kippers was introduced as an economy measure in the First World War by avoiding the need for the long smoking processes. This allowed the kippers to be sold quickly, easily and for a substantially greater profit. Kippers were originally dyed using a coal tar dye called brown FK (the FK is an abbreviation of \"for kippers\"), kipper brown or kipper dye. Today, kippers are usually brine dyed using a natural annato dye, giving the fish a deeper orange/yellow colour. European Community legislation limits the acceptable daily intake (ADI) of Brown FK to 0.15 mg/kg. Not all fish caught are suitable for the dyeing process, with mature fish more readily sought, because the density of their flesh improves the absorption of the dye. An \"orange kipper\" is a kipper that has been dyed orange.\n\nKippers from the Isle of Man and some Scottish producers are not dyed: The smoking time is extended in the traditional manner.\n\"Cold-smoked\" fish that have not been salted for preservation must be cooked before being eaten safely (they can be boiled, fried, grilled, jugged or roasted, for instance). \"Kipper snacks\" (see below) are precooked and may be eaten without further preparation. In general, oily fish are preferred for smoking as the heat is evenly dispersed by the oil, and the flesh resists flaking apart like drier species.\n\nIn the United Kingdom, kippers are often served for breakfast, lunch or supper. In the United States, where kippers are much less commonly eaten than in the UK, they are almost always sold as either canned \"kipper snacks\" or in jars found in the refrigerated foods section.\n\nKippers produced in the Isle of Man are exported around the world. Thousands are produced annually in the town of Peel, where two kipper houses, Moore's Kipper Yard (founded 1882) and Devereau and Son (founded 1884), smoke and export herring.\n\nMallaig, once the busiest herring port in Europe, is famous for its traditionally smoked kippers, as well as Stornoway kippers and Loch Fyne kippers. The harbour village of Craster in Northumberland is famed for Craster kippers, which are prepared in a local smokehouse, sold in the village shop and exported around the world.\n\nConnors Brothers Limited, of Black's Harbour, New Brunswick, Canada, is one of the world's largest producers of sardines and herring. Their \"kippered snacks\", are smoked and salted, undyed.\n\nThe Manx word for kipper is , literally \"red herring\"; the Irish term is \"scadán dearg\" with the same meaning.\n\n\"Kipper time\" is the season in which fishing for salmon is forbidden in Great Britain, originally the period 3 May to 6 January, in the River Thames by an Act of Parliament. \"Kipper season\" refers (particularly among fairground workers, market workers, taxi drivers and the like) to any lean period in trade, particularly the first three or four months of the year.\n\nThe sailors of the Royal Canadian Navy use the term \"kippers\" as a slang for members of the Royal Navy.\n\nThe term \"kippering\" is used in slang to mean being immersed in a room filled with cigarette or other tobacco smoke.\n\nIn recent years \"Kipper\" has become a nickname for a member of the British political party UKIP.\n\nIn the popular children's books The Railway Series, and in the television show Thomas the Tank Engine and friends, The Flying Kipper is a nickname for a fast fish train usually pulled by Henry the Green Engine. This train is most notable for being involved in the story, The Flying Kipper, when Henry is involved in a serious accident that forces him into being rebuilt.\n\n\n"}
{"id": "1914096", "url": "https://en.wikipedia.org/wiki?curid=1914096", "title": "Lynn Townsend White Jr.", "text": "Lynn Townsend White Jr.\n\nLynn Townsend White Jr. (April 29, 1907 – March 30, 1987) was an American historian. He was a professor of medieval history at Princeton from 1933 to 1937, and at Stanford from 1937 to 1943. He was president of Mills College, Oakland, from 1943 to 1958 and a professor at University of California, Los Angeles from 1958 until 1987. Lynn White helped to found The Society of History and Technology (SHOT) and was president from 1960 to 1962. He won the Pfizer Award for \"Medieval Technology and Social Change\" from the History of Science Society (HSS) and the Leonardo da Vinci medal and Dexter prize from SHOT in 1964 and 1970. He was president of the History of Science Society from 1971 to 1972. He was president of The Medieval Academy of America from 1972-1973, and the American Historical Association in 1973.\n\nWhite began his career as medieval historian focusing on the history of Latin monasticism in Sicily during the Norman Period but realized the coming conflict in Europe would interfere with his access to source materials. While at Princeton he read the works of Lefebvre des Noëttes, and Marc Bloch. This led to his first work in the history of technology, \"Technology and Invention in the Middle Ages\" in 1940.\n\nNoettes was a retired French cavalry officer who made his hobby the history of horses. He wrote that the utilization of animals in antiquity was inefficient because the ancients were limited by the technologies of their period, specifically the lack of horseshoes and a bad harness design. White expanded Noettes’ conclusions into a thesis of his own that encompassed the relationship of the newly realized efficient horse and the agricultural revolution of the time.\n\nWhite pointed to new methods of crop rotation and plowing and tied them to the rise of manor-based collective farming and the shift in European prosperity and power from the Mediterranean to the North. White also touched on the stirrup, the lateen sail, the wheel barrow, the spinning wheel, the hand crank, water-driven mills and wind mills. He concluded: \"The chief glory of the later Middle Ages was not its cathedrals or its epics or its scholasticism: it was the building for the first time in history of a complex civilization which rested not on the backs of sweating slaves or coolies but primarily on non-human power\" and he credited this as well as Western primacy in technology to Western theology’s \"activist\" tradition and \"implicit assumption of the infinite worth of even the most degraded human personality\" and its \"repugnance towards subjecting any man to monotonous drudgery.\"\n\nIn 1942, White published a paper titled \"Christian Myth and Christian History\" in which he wrote about the relationship between historians and Christianity. He wrote: \"Having lost faith that God revealed himself uniquely at one single point in history, we are relapsing into the essentially static or repetitive view of the time-process typical of antiquity and of the East\" and \"the Virgin Mother, undefiled yet productive, bearing Christ into the world by the action of the Spirit of God, is so perfect an analogue of the most intimate experience of the soul, that powerful myth has sustained dubious history; for, to the believer, myth and history have been one\" and \"Christianity above all other religions has rashly insisted that its myth really happened in time\" and \"we stand amid the debris of our inherited religious system.\" White held out hope for a Christianity that celebrated its myths and made no pretensions to history, and saw Catholicism as the most progressive in this respect.\n\nAt Mills College, White published on education and women, including \"Women's Colleges and the Male Dominance\" (1947), \"Unfitting Women for Life\" (1949), \"Educating Women in a Man's World\" (1950), and \"The Future of Women's Education\" (1953).\n\nAt UCLA, he used a set of lectures from 1957 to form his best-known work, \"Medieval Technology and Social Change\" in 1962. This book revisited almost all the themes from \"Technology and Invention in the Middle Ages\" 22 years earlier, but included a controversial theory about the stirrup. White contended in the first section of the book that the stirrup made shock combat possible, and therefore had a crucial role in shaping the feudal system. He believed this was the motivation for Charles Martel to accelerate confiscation of church-held lands and distribute it to his knights, who would bear the cost of themselves with expensive horses in to support him in battle. In the second section of the book, White explained the shift in power from the Mediterranean to Northern Europe as a result of increased productivity due to technological changes that produced a \"heavy plow,\" better harnesses for horses to pull the plow, and a three-field crop rotation scheme. In the third part of the book, he examined medieval machines that converted motion and energy. The most notable was the compound crank. The work elicited over 30 reviews, many of which were hostile. P. H. Sawyer and R. H. Hilton wrote the most scathing of the early reviews, beginning with:\n\n\"Technical determinism in historical studies has often been combined with adventurous speculations particularly attractive to those who like to have complex developments explained by simple causes. The technical determinism of Professor Lynn White Jr., however, is peculiar in that, instead of building new and provocative theories about general historical development on the basis of technical studies, he gives a misleadingly adventurist cast to old-fashioned platitudes by supporting them with a chain of obscure and dubious deductions from scanty evidence about the progress of technology.\"\n\nNevertheless, the book has been in print for 51 years and still stands as a seminal work in the field.\n\nIn 1967, White conjectured that the Christian influences in the Middle Ages were at the root of ecological crisis in the 20th century. He gave a lecture on December 26, 1966, titled, \"The Historical Roots of Our Ecologic Crisis\" at the Washington meeting of the AAAS, that was later published in the journal \"Science\". White's article was based on the premise that \"all forms of life modify their context\", i.e. every living organism in some way alters its environment or habitat. He believed man's relationship with the natural environment was always a dynamic and interactive one, even in the Middle Ages, but marked the Industrial Revolution as a fundamental turning point in our ecological history. He suggests that at this point the hypotheses of science were married to the possibilities of technology and our ability to destroy and exploit the environment was vastly increased. Nevertheless, he also suggests that the mentality of the Industrial Revolution, that the earth was a resource for human consumption, was much older than the actuality of machinery, and has its roots in medieval Christianity and attitudes towards nature. He suggests that \"what people do about their ecology depends on what they think about themselves in relation to things around them.\" siting the Genesis creation story he argued that Judeo-Christian theology had swept away pagan animism and normalized exploitation of the natural world because:\n\n\nHe posited that these beliefs have led to an indifference towards nature which continues to impact in an industrial, \"post-Christian\" world. He concludes that applying more science and technology to the problem will not help, that it is humanity's fundamental ideas about nature that must change; we must abandon \"superior, contemptuous\" attitudes that makes us \"willing to use it [the earth] for our slightest whim.\" White suggests adopting St. Francis of Assisi as a model in imagining a \"democracy\" of creation in which all creatures are respected and man's rule over creation is delimited.\n\nWhite's ideas set off an extended debate about the role of religion in creating and sustaining the West's destructive attitude towards the exploitation of the natural world. It also galvanized interest in the relationship between history, nature and the evolution of ideas, thus stimulating new fields of study like environmental history and ecotheology. Equally, however, many saw his argument as a direct attack on Christianity and other commentators think his analysis of the impact of the Bible, and especially Genesis is misguided. They argue that Genesis provides man with a model of \"stewardship\" rather than dominion, and asks man to take care of the world's environment. Others, such as Lewis W. Montcrief, argue that our relation to the environment has been influenced by many more varied and complex cultural/historical phenomena,and that the result we see today cannot simply be reduced to the influence of the Judeo-Christian tradition. Later responses to his article include criticism not just of the central argument but also of the validity of his suggestion \"I propose Francis as a patron saint for ecologists.\" Jan J Boersema's article \"Why is St Francis of Assisi the patron saint of ecologists?\" in \"Science and Christian Belief\" 2002 (vol 14 pp. 51–77). Boersema argues that the historical evidence for Francis's status as such a patron saint is weak both in Francis' own writings and in the reliable sources about his life.\n\nWhite was an historian, but had also earned a master's degree at Union Theological Seminary and was the son of a Calvinist professor of Christian Ethics, and considered religion integral to the development of Western technology. From his \"Technology and Invention in the Middle Ages\" of 1940, through his \"Dynamo and Virgin Reconsidered\" of 1958, to his \"Medieval Technology and Social Change\" (Oxford University Press, 1962), his work refuted the assumption that the Middle Ages were too preoccupied with theology and/or chivalry to concern themselves with technology, the assumption behind Henry Adams' antitheses of Virgin vs. dynamo, but widespread elsewhere as well.\n\nHis work tied together that of many predecessors, above all that of Marc Bloch, to whose memory \"Medieval Technology and Social Change\" is dedicated. White argued, \"Since, until recent centuries, technology was chiefly the concern of groups which wrote little, the role which technological development plays in human affairs has been neglected,\" and declared, \"If historians are to attempt to write the history of mankind, and not simply the history of mankind as it was viewed by the small and specialized segments of our race which have had the habit of scribbling, they must take a fresh view of the records, ask new questions of them, and use all the resources of archaeology, iconography, and etymology to find answers when no answers can be discovered in contemporary writings.\"\n\n\n\n"}
{"id": "672179", "url": "https://en.wikipedia.org/wiki?curid=672179", "title": "Marble (toy)", "text": "Marble (toy)\n\nA marble is a small spherical toy often made from glass, clay, steel, plastic or agate. These balls vary in size. Most commonly, they are about in diameter, but they may range from less than to over , while some art glass marbles for display purposes are over wide. Marbles can be used for a variety of games called marbles. They are often collected, both for nostalgia and for their aesthetic colors. In the North of England the objects and the game are called \"taws\", with larger taws being called \"bottle washers\" after the use of a marble in Codd-neck bottles, which were often collected for play.\n\nIn the early twentieth century, small balls of stone, identified by archaeologists as marbles, were found on excavation near Mohenjo-daro. Marbles are often mentioned in Roman literature, as in Ovid's poem \"Nux\" (which mentions playing the game with walnuts), and there are many examples of marbles from excavations of sites associated with Chaldeans of Mesopotamia and ancient Egypt. They were commonly made of clay, stone or glass. Marbles arrived in Britain, imported from the Low Countries, during the medieval era.\n\nIn 1503 the town council of Nuremberg, Germany, limited the playing of marble games to a meadow outside the town.\n\nIt is unknown where marbles were first manufactured, but the \"original\" marbles were designated \"made in Germany\". The game has become popular throughout the US and other countries. Ceramic marbles entered inexpensive mass production in the 1870s.\n\nA German glassblower invented marble scissors, a device for making marbles, in 1846. The first mass-produced toy marbles (clay) made in the US were made in Akron, Ohio, by S. C. Dyke, in the early 1890s. Some of the first US-produced glass marbles were also made in Akron, by James Harvey Leighton. In 1903, Martin Frederick Christensen—also of Akron, Ohio—made the first machine-made glass marbles on his patented machine. His company, The M. F. Christensen & Son Co., manufactured millions of toy and industrial glass marbles until they ceased operations in 1917. The next US company to enter the glass marble market was Akro Agate. This company was started by Akronites in 1911, but located in Clarksburg, West Virginia. Today, there are only two American-based toy marble manufacturers: Jabo Vitro in Reno, Ohio, and Marble King, in Paden City, West Virginia.\n\nIn Australia, games were played with marbles of different sizes. The smallest and most common was about in diameter. The two larger, more valuable sizes were referred to as semi-bowlers and tom-bowlers, being about and in diameter respectively. They were used in much the same way as ordinary marbles, although sometimes they would be declared invalid because of the advantage of their larger mass and inertia. Owners of large marbles were also afraid to use them lest they be lost to another player as \"keepsies\". They were usually of the clear \"cat's eye\" or milk glass type, just bigger.\n\n\"Firing\" a marble meant that a player had to flick his/her marble from a stationary position of his hand. No part of the hand firing the marble was permitted to be in front of the position where the marble had been resting on the ground. Using that hand, (s)he would flick or fire the marble from his/her hand, usually with the knuckle on the back of his/her hand resting on the ground, and usually using the thumb of that hand to do so. All shots of the game were conducted in this manner throughout except the very initial pitch towards the bunny hole that started the game.\n\nOnce a player was able to land his/her marble within the hole, (s)he would immediately then fire his marble at his opponents' marbles. However, if any player hit another player's marble before his/her own marble had been to 'visit' the bunny hole, the act would be referred to as \"a kiss\"; the game would be over, and all or both players (in the case of two players only) would have to retreat back to the starting line to restart the game, without result. This, of course, could be quite annoying or frustrating if a player had already built up quite a few hits on another player's marble. So, most skilled players did not resort to this kind of tactic.\n\nThe overall aim was to hit a particular marble 3 times after getting into the hole, then you had to \"run away\", before the final contact shot was allowed to be played - which was called \"the kill\". Once a player made a kill on another marble, if the game was 'for keeps', (s)he would then get to keep the marble [bunny] (s)he had 'killed'. The format of playing this game was that each time you successfully hit another player's marble, you were to have another shot - even if it was not the marble you had originally intended to hit.\n\nOf course, the ploy was to hit the particular opponent marble 3 times, and then 'run away' to the bunny hole, because once you rested the marble into the hole, you immediately had your shot again, thus leaving no opportunity at all for your opponent to retreat his/her marble before \"the Kill\" was made on it.\n\nIn India, there are many games with marbles. One simple game with marbles is called \"Cara\" in which every player puts one or more marbles in a long line of marbles with each marble being one centimeter or slightly more, apart from each other. After this, each player throws another marble as far as possible from the line, perpendicularly. In this game, the player whose marble is farthest from the line of marbles gets the first chance to hit the marble's line and subsequent players who get to hit the line have their distance from the line in decreasing order. Any player who hits and displaces a marble in the line of marbles gets to take that and all marbles to the right of it. Usually, marbles in the line are smaller marbles and the players have bigger marbles for hitting the line of smaller marbles.This game needs the playground to be flat and hard and with no loose soil for effecting games of \"Cara\". The number of players can be anywhere between 2 to 30. The distances of the marbles thrown determine order of players who get to hit the line are anywhere from 10 to 30 meters and may depend on player's desire to hit the marble line first and how much risk they will take so that they would be at some distance and yet be able to hit the line of marbles and get more than 2 marbles. Players have to roll their marbles from a distance to hit the line of marbles. Each player gets to hit the line of marbles only once assuming there are marbles left in the line and every player gets a turn in order. In a line of twenty marbles, its reasonable to get at least 5 to 20 marbles depending on how skillful somebody is at hitting the line of marbles. When all marbles are taken by players as above the game is restarted with players putting their marbles in the line and trying to win as many marbles as possible.If some marbles are left in the line after each player takes a chance, the players again throw their marbles perpendicularly to this line and start to roll their marble to hit the line according to the above rules. This process is repeated until all marbles are taken in the game.\n\nIn Uganda, a popular marbles game is called \"dool\". It requires a small pit dug in the ground for two or more players, each with his own marble. To improvise, Ugandans also use the seeds of a candlenut tree, locally referred to as \"Kabakanjagala\" (The King loves me). To start a game, a throwing line is drawn on the ground using chalk or a stick about a meter (or some feet) from the pit. Then the players roll their marbles close to the pit. The one whose marble falls in gets points equivalent to one game. If a second marble falls in and hits the first, a player gets more points than the previous player, but all have to return to the throwing line. When no marble falls in, the player whose marble rolls closest to the pit starts the firing session. When he misses, the next opponent fires. You can only fire 24 consecutive times per turn earning one point for each hit. But all that time, a player must make sure the gap between the marbles is bigger than two stretches of the hand. If an opponent realises that it isn't, then he can make a call, pick his marble and place it anywhere. When a player is targeting a marble placed near the hole, he must avoid knocking it into the hole or else give away an advantage. There are various rules for \"dool\" but the player with the most points wins. Favoured fingers include the middle finger for blasting strength and small finger for accurate long distance target.\n\nThe British and World Marbles Championship has been held at Tinsley Green, West Sussex, England, every year since 1932. (Marbles has been played in Tinsley Green and the surrounding area for many centuries: \"TIME\" magazine traces its origins to 1588.) Traditionally, the marbles-playing season started on Ash Wednesday and lasted until midday on Good Friday: playing after that brought bad luck. More than 20 teams from around the world take part in the championship, each Good Friday; German teams have been successful several times since 2000, although local teams from Crawley, Copthorne and other Sussex and Surrey villages often take part as well; the first championship in 1932 was won by Ellen Geary, a young girl from London.\n\n\nThere are various types of marbles, and names vary from locality to locality.\nMarble players often grow to collect marbles after having outgrown the game. Marbles are categorized by many factors including condition, size, type, manufacturer/artisan, age, style, materials, scarcity, and the existence of original packaging (which is further rated in terms of condition). A marble's worth is primarily determined by type, size, condition and eye-appeal, coupled with the law of supply and demand. Ugly, but rare marbles may be valued as much as those of very fine quality. However, this is the exception, rather than the rule - \"Condition is King\" when it comes to marbles. Any surface damage (characterized by missing glass, such as chips or pits) typically cuts book value by 50% or more.\n\nDue to the large market, there are many related side businesses that have sprung up such as numerous books and guides, web sites dedicated to live auctions of marbles only, and collector conventions. Additionally, many glass artisans produce art marbles for the collectors' market only, with some selling for thousands of dollars.\n\nMarbles are made using many techniques. They can be categorized into two general types: hand-made and machine-made.\n\nMarbles were originally made by hand. Stone or ivory marbles can be fashioned by grinding. Clay, pottery, ceramic, or porcelain marbles can be made by rolling the material into a ball, and then letting dry, or firing, and then can be left natural, painted, or glazed. Clay marbles, also known as \"crock marbles\" or \"commies\" (\"common\"), are made of slightly porous clay, traditionally from local clay or leftover earthenware (\"crockery\"), rolled into balls, then glazed and fired at low heat, creating an opaque imperfect sphere that is frequently sold as the poor boy's \"old timey\" marble. Glass marbles can be fashioned through the production of glass rods which are stacked together to form the desired pattern, cutting the rod into marble-sized pieces using marble scissors, and rounding the still-malleable glass.\n\nOne mechanical technique is dropping globules of molten glass into a groove made by two interlocking parallel screws. As the screws rotate, the marble travels along them, gradually being shaped into a sphere as it cools. Colour is added to the main batch glass and/or to additional glass streams that are combined with the main stream in a variety of ways. For example, in the \"cat's-eye\" style, coloured glass veins are injected into a transparent main stream. Applying more expensive coloured glass to the surface of cheaper transparent or white glass is also a common technique.\n\nThere were a lot of businesses that made marbles in Ohio. One major marble manufacturing company is Marble King, located in Paden City, West Virginia, which was featured in the television shows \"Made in America\", \"Some Assembly Required\" and \"The Colbert Report\". Currently, the world's largest manufacturer of playing marbles is Vacor de Mexico. The company makes 90 percent of the world’s marbles. Over 12 million are produced daily.\n\nMarbles are also made in China and may contain lead, arsenic, and/or cadmium due to the manufacturing process of old glass.\n\nArt marbles are high-quality collectible marbles arising out of the art glass movement. They are sometimes referred to as contemporary glass marbles to differentiate them from collectible antique marbles, and are spherical works of art glass.\n\nCollectible contemporary marbles are made mostly in the United States by individual artists such as Josh Simpson.\n\nArt marbles are usually around 50mm marble (a size also known as a \"toe breaker\"), but can vary, depending on the artist and the print.\n\n\n\n\n"}
{"id": "20636", "url": "https://en.wikipedia.org/wiki?curid=20636", "title": "Memex", "text": "Memex\n\nThe memex (originally coined \"at random\", though sometimes said to be a portmanteau of \"memory\" and \"index\") is the name of the hypothetical proto-hypertext system that Vannevar Bush described in his 1945 \"The Atlantic Monthly\" article \"As We May Think\". Bush envisioned the memex as a device in which individuals would compress and store all of their books, records, and communications, \"mechanized so that it may be consulted with exceeding speed and flexibility\". The memex would provide an \"enlarged intimate supplement to one's memory\". The concept of the memex influenced the development of early hypertext systems (eventually leading to the creation of the World Wide Web) and personal knowledge base software. The hypothetical implementation depicted by Bush for the purpose of concrete illustration was based upon a document bookmark list of static microfilm pages and lacked a true hypertext system, where parts of pages would have internal structure beyond the common textual format. Early electronic hypertext systems were thus inspired by memex rather than modeled directly upon it.\n\nIn \"As We May Think\", Bush describes a memex as an electromechanical device enabling individuals to develop and read a large self-contained research library, create and follow associative trails of links and personal annotations, and recall these trails at any time to share them with other researchers. This device would closely mimic the associative processes of the human mind, but it would be gifted with permanent recollection. As Bush writes, \"Thus science may implement the ways in which man produces, stores, and consults the record of the race\".\n\nThe technology used would have been a combination of electromechanical controls, microfilm cameras and readers, all integrated into a large desk. Most of the microfilm library would have been contained within the desk, but the user could add or remove microfilm reels at will. A memex would hypothetically read and write content on these microfilm reels, using electric photocells to read coded symbols recorded next to individual microfilm frames while the reels spun at high speed, stopping on command. The coded symbols would enable the memex to index, search, and link content to create and follow associative trails.\n\nThe top of the desk would have slanting translucent screens on which material could be projected for convenient reading. The top of the memex would have a transparent platen. When a longhand note, photograph, memoranda, or other things were placed on the platen, the depression of a lever would cause the item to be photographed onto the next blank space in a section of the memex film.\n\nThe memex would become \"'a sort of mechanized private file and library'. It would use microfilm storage, dry photography, and analog computing to give postwar scholars access to a huge, indexed repository of knowledge any section of which could be called up with a few keystrokes.\"\n\nThe vision of the memex predates, and is credited as the inspiration for, the first practical hypertext systems of the 1960s. Bush describes the memex and other visions of As We May Think as projections of technology known in the 1930s and 1940s in the spirit of Jules Verne's adventures, or Arthur C. Clarke's 1945 proposal to orbit geosynchronous satellites for global telecommunication. The memex proposed by Bush would create \"trails\" of links connecting sequences of microfilm frames, rather than links in the modern sense where a hyperlink connects a single word, phrase or picture within a document and a local or remote destination.\n\nAn associative trail as conceived by Bush would be a way to create a new \"linear\" sequence of microfilm frames across any arbitrary sequence of microfilm frames by creating a chained sequence of links in the way just described, along with personal comments and \"side trails\". At the time Bush saw the current ways of indexing information as limiting and instead proposed a way to store information that was analogous to the mental association of the human brain: storing information with the capability of easy access at a later time using certain cues (in this case, a series of numbers as a code to retrieve data). The closest analogy with the modern Web browser would be to create a list of bookmarks to articles relevant to a topic, and then to have some mechanism for automatically scrolling through the articles (for example, use Google to search for a keyword, obtain a list of matches, repeatedly use the \"open in new tab\" feature of the Web browser, and then visit each tab sequentially). Modern hypertext systems with word and phrase-level linking offer more sophistication in connecting relevant information, but until the rise of wiki and other social software models, modern hypertext systems have rarely imitated Bush in providing individuals with the ability to create personal trails and share them with colleagues – or publish them widely.\n\nThe memex would have features other than linking. The user could record new information on microfilm, by taking photos from paper or from a touch-sensitive translucent screen. A user could \"...insert a comment of his own, either linking it into the main trail or joining it by a side trail to a particular item. ...Thus he builds a trail of his interest through the maze of materials available to him.\" A user could also create a copy of an interesting trail (containing references and personal annotations) and \"...pass it to his friend for insertion in his own memex, there to be linked into the more general trail.\" As observers like Tim Oren have pointed out, the memex could be considered to be a microfilm-based precursor to the personal computer. The September 10, 1945, Life magazine article showed the first illustrations of what the memex desk could look like, as well as illustrations of a head-mounted camera, which a scientist could wear while doing experiments, and a typewriter capable of voice recognition and of reading text by speech synthesis. Considered together, these memex machines were probably the earliest practical description of what we would call today the Office of the future.\n\n\"Given a memex, a scholar could create her own knowledge tools as connections within reams of information, share these tools, and use complexes of tools to create yet more sophisticated knowledge that could in turn be deployed toward this work. The memex has been envisioned as a means of turning an information explosion into a knowledge explosion. This remains one of the defining dreams of new media.\"\n\nBush's idea for the memex extended far beyond a mechanism which might augment the research of one individual working in isolation. In Bush's idea, the ability to connect, annotate, and share both published works and personal trails would profoundly change the process by which the \"world's record\" is created and used:\n\nBush states that \"technical difficulties of all sorts have been ignored,\" but that, \"also ignored are means as yet unknown which may come any day to accelerate technical progress as violently as did the advent of the thermionic tube.\" Indeed, anyone who stops to consider the performance consequences of trail following – let alone link-directed pointer chasing – over a microfilm library of near universal scope should quickly come to the conclusion that microfilm is no more appropriate a technology for implementing AWMT's vision than Jules Verne's cannon is an appropriate technology for sending astronauts to the Moon. In both cases the vision may be more significant than the specific technology used to describe it. See Michael Buckland's conclusion: \"Bush's contributions in this area were twofold: (i) A significant engineering achievement by the team under his leadership in building a truly rapid prototype microfilm selector, and (ii) a speculative article, 'As We May Think,' which, through its skillful writing and the social prestige of its author, has had an immediate and lasting effect in stimulating others.\"\n\nIn \"Memex: Getting Back on the Trail\", Tim Oren argues that Bush's original vision expressed in AWMT describes a \"...private device into which public encyclopedias and colleague's trails might be inserted to be joined with the owner's own work.\"\n\nHowever, in Bush's manuscript draft of \"Memex II\" of 1959, Bush says, \"Professional societies will no longer print papers...\" and states that individuals will either order sets of papers to come on tape – complete with photographs and diagrams – or download 'facsimiles' by telephone. Each society would maintain a 'master memex' containing all papers, references, tables \"intimately interconnected by trails, so that one may follow a detailed matter from paper to paper, going back through the classics, recording criticism in the margins.\"\n\nThe AWMT paper did not describe any automatic search, nor any universal metadata scheme such as a standard library classification or a hypertext element set like the Dublin core. Instead, when the user made an entry, such as a new or annotated manuscript, typescript or image, he was expected to index and describe it in his personal code book. By consulting his code book, the user could retrace annotated and generated entries.\n\nBetween 1990 and 1994, Paul Flaherty, a Stanford student who was looking for a project, was introduced by his wife to her supervisor. The supervisor had just seen a demonstration of the World Wide Web and suggested it could be improved and better conformed to the memex described by Vannevar Bush if links did not have to be manually inserted and instead one could follow a link simply by using the words themselves. Flaherty went on to create AltaVista, the first searchable, full-text database of a large part of the Web.\n\nBy 1999, many companies had created web annotation systems, where web site publishers or users could annotate web pages. Sentius Corporation, for one, developed technology that would automatically insert hyperlinks into web or text documents, from a dictionary of terms. This was used in particular to display Japanese translations of English medical terms when the mouse hovered over a term, while maintaining the look of a standard text document normally, and then extended for other purposes.\nThis idea directly influenced computer pioneers J.C.R. Licklider (see his 1960 paper \"Man-Computer Symbiosis\"), Douglas Engelbart (see his 1962 report \"Augmenting Human Intellect\"), and also led to Ted Nelson's groundbreaking work in concepts of hypermedia and hypertext.\n\n\"As We May Think\" also predicted many kinds of technology invented after its publication in addition to hypertext such as personal computers, the Internet, the World Wide Web, speech recognition, and CD-ROM encyclopedias such as Encarta and online encyclopedias such as Wikipedia: \"Wholly new forms of encyclopedias will appear, ready-made with a mesh of associative trails running through them, ready to be dropped into the memex and there amplified.\"\n\nBush's influence is still evident in research laboratories of today in Gordon Bell's MyLifeBits (from Microsoft Research), which implements path-based systems reminiscent of the Memex, is especially impactful in the areas of information retrieval and information science.\n\nA fictional implementation of the memex appears in The Laundry Files series by Charles Stross.\n\nA high-performance computing cluster (HPC) at the Carnegie Institution for Science is named \"Memex\".\n\nIn early 2014, the Defense Advanced Research Projects Agency (DARPA) released a statement on their website outlining the preliminary details of the \"Memex program\", which aims at developing new search technologies overcoming some limitations of text-based search. DARPA wants the Memex technology developed in this research to be usable for search engines that can search for information on the Deep Web – the part of the Internet that is largely unreachable by commercial search engines like Google or Yahoo. DARPA's website describes that \"The goal is to invent better methods for interacting with and sharing information, so users can quickly and thoroughly organize and search subsets of information relevant to their individual interests\". As reported in a 2015 \"Wired\" article, the search technology being developed in the Memex program \"aims to shine a light on the dark web and uncover patterns and relationships in online data to help law enforcement and others track illegal activity\". DARPA intends for the program to replace the centralized procedures used by commercial search engines, stating that the \"creation of a new domain-specific indexing and search paradigm will provide mechanisms for improved content discovery, information extraction, information retrieval, user collaboration, and extension of current search capabilities to the deep web, the dark web, and nontraditional (e.g. multimedia) content\". In their description of the program, DARPA explains the program's name as a tribute to Bush's original Memex invention, which served as an inspiration.\n\nIn April 2015, it was announced parts of Memex would be open sourced. Modules were available for download.\n\nIn 1967, Vannevar Bush published a retrospective article entitled \"Memex Revisited\" in his book \"Science Is Not Enough\". Published 22 years after his initial conception of the Memex, Bush details the various technological advancements that have made his vision a possibility. Specifically, Bush cites photocells, transistors, cathode ray tubes, magnetic and video tape, \"high-speed electric circuits\", and \"miniaturization of solid-state devices\" such as the TV and radio. The article claims that magnetic tape would be central to the creation of a modern Memex device. The erasable quality of the tape is of special significance, as this would allow for modification of information stored in the proposed Memex.\n\nIn the article, Bush stresses the continued importance of supplementing \"how creative men think\" and relates that the systems for indexing data are still insufficient and rely too much on linear pathways rather than the association-based system of the human brain. Bush writes that a machine with the \"speed and flexibility\" of the brain is not attainable, but improvements could be made in regard to the capacity to obtain informational \"permanence and clarity\".\n\nBush also relates that, unlike digital technology, Memex would be of no significant aid to business or profitable ventures, and as a consequence its development would occur only long after the mechanization of libraries and the introduction of what he describes as the specialized \"group machine\", which would be useful for the sharing of ideas in fields such as medicine. Furthermore, although Bush discusses the compressional ability and rapidity so key to modern machines, he relates that speed will not be an integral part of Memex, stating that a tenth of a second would be an acceptable interval for its data retrieval, rather than the billionths of a second that modern computers are capable of. \"For Memex,\" he writes, \"the problem is not swift access, but selective access\". Bush states that although the code-reading and potential linking capabilities of the rapid selector would be key to the creation of Memex, there is still an issue of enabling \"moderately rapid access to really large memory storage\". There is an issue concerning selection, Bush conveys, and despite the fact that improvements have been made in the speed of digital selection, according to Bush, \"selection, in the broad sense, is still a stone adze in the hands of the cabinetmaker\". Bush goes on to discuss the record-making process and how Memex could incorporate systems of voice-control and user-propagated learning. He proposes a machine that could respond to \"simple remarks\" as well as build trails based on its user's \"habits of association,\" as Belinda Barnet described them in \"The Technical Evolution of Vannevar Bush's Memex.\" Barnet also makes the distinction between the idea of a constructive Memex and the \"permanent trails\" described in \"As We May Think,\" and attributes Bush's machine learning concepts to Claude Shannon's mechanical mouse and work with \"feedback and machine learning\".\n\n\n\n\n"}
{"id": "32453380", "url": "https://en.wikipedia.org/wiki?curid=32453380", "title": "Memo motion", "text": "Memo motion\n\nMemo motion or spaced-shot photography is a tool of time and motion study that analyzes long operations by using a camera. It was developed 1946 by Marvin E. Mundel at Purdue University, who was first to save film material while planning studies on kitchen work.\n\nMundel published the method in 1947 with several studies in his textbook \"Systematic Motion and time study\". A study showed the following advantages of Memo-Motion in regard to other forms of time and motion study:\n\nAs a versatile tool of work study it was used in the US to some extent, but rarely in Europe and other industrial countries mainly because of difficulties procuring the required cameras. Today Memo-Motion could have a comeback because more and more workplaces have conditions which it can explore.\n"}
{"id": "2019941", "url": "https://en.wikipedia.org/wiki?curid=2019941", "title": "Micro combined heat and power", "text": "Micro combined heat and power\n\nMicro combined heat and power or micro-CHP or mCHP is an extension of the idea of cogeneration to the single/multi family home or small office building in the range of up to 50 kW. Local generation has the potential for a higher efficiency than traditional grid-level generators since it lacks the 8-10% energy losses from transporting electricity over long distances. It also lacks the 10–15% energy losses from heat transfer in district heating networks due to the difference between the thermal energy carrier (hot water) and the colder external environment. The most common systems use natural gas as their primary energy source and emit carbon dioxide.\n\nCombined heat and power (CHP) systems for homes or small commercial buildings are often fueled by natural gas to produce electricity and heat. A micro-CHP system usually contains a small fuel cell or a heat engine as a prime mover used to rotate a generator which provides electric power, while simultaneously utilizing the waste heat from the prime mover for an individual building's heating, ventilation, and air conditioning. A micro-CHP generator may primarily follow heat demand, delivering electricity as the by-product, or may follow electrical demand to generate electricity and use heat as the by-product. When used primarily for heating, micro-CHP systems may generate more electricity than is instantaneously being demanded in circumstances of fluctuating electrical demand.\n\nThe heat engine version is a small scale example of cogeneration schemes which have been used with large electric power plants. The purpose of cogeneration is to utilize more of the energy in the fuel. The reason for using such systems is that heat engines, such as steam power plants which generate the electric power needed for modern life by burning fuel, are not very efficient. Due to Carnot's theorem, a heat engine cannot be 100% efficient; it cannot convert anywhere near all the heat produced from the fuel it burns into organized forms of energy such as electricity. Therefore, heat engines always produce a surplus of low-temperature waste heat, called \"secondary heat\" or \"low-grade heat\". Modern plants are limited to efficiencies of about 33–63% at most, so 37–67% of the energy is exhausted as waste heat. In the past this energy was usually wasted to the environment. Cogeneration systems, built in recent years in cold-climate countries, utilize the waste heat produced by large power plants for heating by piping hot water from the plant into buildings in the surrounding community.\n\nHowever, it is not practical to transport heat long distances due to heat loss from the pipes. Since electricity can be transported practically, it is more efficient to generate the electricity near where the waste heat can be used. So in a \"micro-combined heat and power system\" (micro-CHP), small power plants are instead located where the secondary heat can be used, in individual buildings. Micro-CHP is defined by the EC as being of less than 50 kW electrical power output, however, others have more restrictive definitions, all the way down to <5 kWe.\n\nIn centralized power plants, the supply of \"waste heat\" may exceed the local heat demand. In such cases, if it is not desirable to reduce the power production, the excess waste heat must be disposed of (e.g. cooling towers or sea cooling) without being used. A way to avoid excess waste heat is to reduce the fuel input to the CHP plant, reducing both the heat and power output to balance the heat demand. In doing this, the power production is limited by the heat demand.\n\nIn a traditional power plant delivering electricity to consumers, about 34.4% of the heat content of the primary heat energy source, such as biomass, coal, solar thermal, natural gas, petroleum or uranium, reaches the consumer, although the efficiency can be 20% for very old plants and 45% for newer gas plants. In contrast, a CHP system converts 15%–42% of the primary heat to electricity, and most of the remaining heat is captured for hot water or space heating. In total, over 90% of the heat from the primary energy source (LHV based) can be used when heat production does not exceed the thermal demand.\n\nCHP systems are able to increase the total energy utilization of primary energy sources, such as fuel and concentrated solar thermal energy. Thus CHP has been steadily gaining popularity in all sectors of the energy economy, due to the increased costs of electricity and fuel, particularly fossil fuels, and due to environmental concerns, particularly climate change.\n\nCHP systems have benefited the industrial sector since the beginning of the industrial revolution. For three decades, these larger CHP systems were more economically justifiable than micro-CHP, due to the economy of scale. After the year 2000, micro-CHP has become cost effective in many markets around the world, due to rising energy costs. The development of micro-CHP systems has also been facilitated by recent technological developments of small heat engines. This includes improved performance and cost-effectiveness of fuel cells, Stirling engines, steam engines, gas turbines, diesel engines and Otto engines.\n\nPEMFC fuel cell mCHP operates at low temperature (50 to 100 °C) and needs high purity hydrogen, its prone to contamination, changes are made to operate at higher temperatures and improvements on the fuel reformer. SOFC fuel cell mCHP operates at a high temperature (500 to 1,000 °C) and can handle different energy sources well but the high temperature requires expensive materials to handle the temperature, changes are made to operate at a lower temperature. Because of the higher temperature SOFC in general has a longer start-up time and need continuous heat output even in times when there is no thermal demand.\n\nCHP systems linked to absorption chillers can use waste heat for refrigeration.\n\nA 2013 UK report from Ecuity Consulting stated that MCHP is the most cost-effective method of utilizing gas to generate energy at the domestic level.\n\nDelta-ee consultants stated in 2013 that with 64% of global sales the fuel cell micro-combined heat and power passed the conventional engine-based micro-CHP systems in sales in 2012.\n\nMicro-CHP engine systems are currently based on several different technologies:\n\nThere are many types of fuels and sources of heat that may be considered for micro-CHP. The properties of these sources vary in terms of system cost, heat cost, environmental effects, convenience, ease of transportation and storage, system maintenance, and system life. Some of the heat sources and fuels that are being considered for use with micro-CHP include: natural gas, LPG, biomass, vegetable oil (such as rapeseed oil), woodgas, solar thermal, and lately also hydrogen, as well as multi-fuel systems. The energy sources with the lowest emissions of particulates and net-carbon dioxide include solar power, hydrogen, biomass (with two-stage gasification into biogas), and natural gas. Due to the high efficiency of the CHP process, cogeneration has still lower carbon emissions compared to energy transformation in fossil driven boilers or thermal power plants.\n\nThe majority of cogeneration systems use natural gas for fuel, because natural gas burns easily and cleanly, it can be inexpensive, it is available in most areas and is easily transported through pipelines which already exist for over 60 million homes.\n\nReciprocating internal combustion engines are the most popular type of engine used in micro-CHP systems. Reciprocating internal combustion engine based systems can be sized such that the engine operates at a single fixed speed, usually resulting in a higher electrical or total efficiency. However, since reciprocating internal combustion engines have the ability to modulate their power output by changing their operating speed and fuel input, micro-CHP systems based on these engines can have varying electrical and thermal output designed to meet changing demand.\n\nNatural gas is suitable for internal combustion engines, such as Otto engine and gas turbine systems. Gas turbines are used in many small systems due to their high efficiency, small size, clean combustion, durability and low maintenance requirements. Gas turbines designed with foil bearings and air-cooling operate without lubricating oil or coolants. The waste heat of gas turbines is mostly in the exhaust, whereas the waste heat of reciprocating internal combustion engines is split between the exhaust and cooling system.\n\nExternal combustion engines can run on any high-temperature heat source. These engines include the Stirling engine, hot \"gas\" turbocharger, and the steam engine. Both range from 10%-20% efficiency, and as of 2014, small quantities are in production for micro-CHP products.\n\nOther possibilities include the Organic Rankine cycle, which operates at lower temperatures and pressures using low-grade heat sources. The primary advantage to this is that the equipment is essentially an air-conditioning or refrigeration unit operating as an engine, whereby the piping and other components need not be designed for extreme temperatures and pressures, reducing cost and complexity. Electrical efficiency suffers, but it is presumed that such a system would be utilizing waste heat or a heat source such as a wood stove or gas boiler that would exist anyway for purposes of space heating.\n\nThe future of combined heat and power, particularly for homes and small businesses, will continue to be affected by the price of fuel, including natural gas. As fuel prices continue to climb, this will make the economics more favorable for energy conservation measures, and more efficient energy use, including CHP and micro-CHP.\n\nFuel cells generate electricity and heat as a by product. The advantages for a stationary fuel cell application over stirling CHP are no moving parts, less maintenance, and quieter operation. The surplus electricity can be delivered back to the grid.\n\nPEMFC fuel cells fueled by natural gas or propane use a steam reformer to convert methane in the gas supply into carbon dioxide and hydrogen; the hydrogen then reacts with oxygen in the fuel cell to produce electricity. A PEMFC fuel cell based micro-CHP has an electrical efficiency of 37% LHV and 33% HHV and a heat recovery efficiency of 52% LHV and 47% HHV with a service life of 40,000 hours or 4000 start/stop cycles which is equal to 10 year use. An estimated 138,000 Fuel cell CHP systems below 1 kW had been installed in Japan by the end of 2014. Most of these CHP systems are PEMFC based (85%) and the remaining are SOFC systems.\n\nIn 2013 Lifetime is around 60,000 hours. For PEM fuel cell units, which shut down at night, this equates to an estimated lifetime of between ten and fifteen years.\n\nUnited States Department of Energy (DOE) Technical Targets: 1–10 kW residential combined heat and power fuel cells operating on natural gas.\n\nStandard utility natural gas delivered at typical residential distribution line pressures.\nRegulated AC net/lower heating value of fuel.\nOnly heat available at 80 °C or higher is included in CHP energy efficiency calculation.\nCost includes materials and labor costs to produce stack, plus any balance of plant necessary for stack operation. Cost defined at 50,000 unit/year production (250 MW in 5 kW modules).\nBased on operating cycle to be released in 2010.\nTime until >20% net power degradation.\n\nThermoelectric generators operating on the Seebeck Effect show promise due to their total absence of moving parts. Efficiency, however, is the major concern as most thermoelectric devices fail to achieve 5% efficiency even with high temperature differences.\n\nThis can be achieved by Photovoltaic thermal hybrid solar collector, another option is Concentrated photovoltaics and thermal (CPVT), also sometimes called combined heat and power solar (CHAPS), is a cogeneration technology used in concentrated photovoltaics that produce both electricity and heat in the same module. The heat may be employed in district heating, water heating and air conditioning, desalination or process heat.\n\nCPVT systems are currently in production in Europe, with Zenith Solar developing CPVT systems with a claimed efficiency of 72%.\n\nSopogy produces a micro concentrated solar power (microCSP) system based on parabolic trough which can be installed above building or homes, the heat can be used for water heating or solar air conditioning, a steam turbine can also be installed to produce electricity.\n\nThe recent development of small scale CHP systems has provided the opportunity for in-house power backup of residential-scale photovoltaic (PV) arrays. The results of a recent study show that a PV+CHP hybrid system not only has the potential to radically reduce energy waste in the status quo electrical and heating systems, but it also enables the share of solar PV to be expanded by about a factor of five. In some regions, in order to reduce waste from excess heat, an absorption chiller has been proposed to utilize the CHP-produced thermal energy for cooling of PV-CHP system. These trigen+PV systems have the potential to save even more energy.\n\nTo date, micro-CHP systems achieve much of their savings, and thus attractiveness to consumers, by the value of electrical energy which is replaced by the autoproduced electricity. A \"generate-and-resell\" or net metering model supports this as home-generated power exceeding the instantaneous in-home needs is sold back to the electrical utility. This system is efficient because the energy used is distributed and used instantaneously over the electrical grid. The main losses are in the transmission from the source to the consumer which will typically be less than losses incurred by storing energy locally or generating power at less than the peak efficiency of the micro-CHP system. So, from a purely technical standpoint dynamic demand management and net-metering are very efficient.\n\nAnother positive to net-metering is the fact that it is fairly easy to configure. The user's electrical meter is simply able to record electrical power exiting as well as entering the home or business. As such, it records the net amount of power entering the home. For a grid with relatively few micro-CHP users, no design changes to the electrical grid need be made. Additionally, in the United States, federal and now many state regulations require utility operators to compensate anyone adding power to the grid. From the standpoint of grid operator, these points present operational and technical as well as administrative burdens. As a consequence, most grid operators compensate non-utility power-contributors at less than or equal to the rate they charge their customers. While this compensation scheme may seem almost fair at first glance, it only represents the consumer’s cost-savings of not purchasing utility power versus the true cost of generation and operation to the micro-CHP operator. Thus from the standpoint of micro-CHP operators, net-metering is not ideal.\n\nWhile net-metering is a very efficient mechanism for using excess energy generated by a micro-CHP system, it does have detractors. Of the detractors' main points, the first to consider is that while the main generating source on the electrical grid is a large commercial generator, net-metering generators \"spill\" power to the smart grid in a haphazard and unpredictable fashion. However, the effect is negligible if there are only a small percentage of customers generating electricity and each of them generates a relatively small amount of electricity. When turning on an oven or space heater, about the same amount of electricity is drawn from the grid as a home generator puts out. If the percentage of homes with generating systems becomes large, then the effect on the grid may become significant. Coordination among the generating systems in homes and the rest of the grid may be necessary for reliable operation and to prevent damage to the grid.\n\nThe largest deployment of micro-CHP is in Japan in 2009 where over 90,000 units in place, with the vast majority being of Honda's \"ECO-WILL\" type. Six Japanese energy companies launched the 300 W–1 kW PEMFC/SOFC ENE FARM product in 2009, with 3,000 installed units in 2008, a production target of 150,000 units for 2009–2010 and a target of 2,500,000 units in 2030. 20,000 units where sold in 2012 overall within the Ene Farm project making an estimated total of 50,000 PEMFC and up to 5,000 SOFC installations. For 2013 a state subsidy for 50,000 units is in place. The ENE FARM project will pass 100.000 systems in 2014, 34.213 PEMFC and 2.224 SOFC were installed in the period 2012-2014, 30,000 units on LNG and 6,000 on LPG.\n\nSold by various gas companies and as of 2013, installed in a total of 131,000 homes. Manufactured by Honda using their single cylinder EXlink engine capable of burning natural gas or propane. Each unit produces 1 kW of electricity and 2.8 kW of hot water.\n\n\n\nIn South Korea, subsidies will start at 80 percent of the cost of a domestic fuel cell. The Renewable Portfolio Standard program with renewable energy certificates runs from 2012 to 2022. \nQuota systems favor large, vertically integrated generators and multinational electric utilities, if only because certificates are generally denominated in units of one megawatt-hour. They are also more difficult to design and implement than a Feed-in tariff. Around 350 residential mCHP units where installed in 2012.\n\n\nThe European public–private partnership Fuel Cells and Hydrogen Joint Undertaking Seventh Framework Programme project ene.field aims to deploy by 2017 up 1,000 residential fuel cell Combined Heat and Power (micro-CHP) installations in 12 EU member states. \n\nPowercell Sweden is a fuel cell company that develop environmentally friendly electric generators with the unique fuel cell and reformer technology that is suitable for both existing and future fuel.\n\nIn Germany, ca 50 MW of mCHP up to 50 kW units have been installed in 2015. The German government is offering large CHP incentives, including a market premium on electricity generated by CHP and an investment bonus for micro-CHP units. The German testing project Callux has 500 mCHP installations per nov 2014. North Rhine-Westphalia launched a 250 million subsidy program for up to 50 kW lasting until 2017.\n\n\n\nIt is estimated that about 1,000 micro-CHP systems were in operation in the UK as of 2002. These are primarily Whispergen using Stirling engines, and Senertec Dachs reciprocating engines. The market is supported by the government through regulatory work, and some government research money expended through the Energy Saving Trust and Carbon Trust, which are public bodies supporting energy efficiency in the UK. Effective as of 7 April 2005, the UK government has cut the VAT from 20% to 5% for micro-CHP systems, in order to support demand for this emerging technology at the expense of existing, less environmentally friendly technology. The reduction in VAT is effectively a 10.63% subsidy for micro-CHP units over conventional systems, which will help micro-CHP units become more cost competitive, and ultimately drive micro-CHP sales in the UK. Of the 24 million households in the UK, as many as 14 to 18 million are thought to be suitable for micro-CHP units.\nTwo fuel cell varieties of mCHP co-generation units are almost ready for mainstream production and are planned for release to commercial markets in early in 2014. With the UK Government's Feed-In-Tariff available for a 10-year period, a wide uptake of the technology is anticipated.\n\n\n\nThe Danish mCHP project 2007 to 2014 with 30 units is on the island of Lolland and in the western town Varde. Denmark is currently part of the Ene.field project.\n\nThe micro-CHP subsidy was ended in 2012. To test the effects of mCHP on a smart grid, 45 natural gas SOFC units (each 1,5 kWh) from Republiq Power (Ceramic Fuel Cells) will be placed on Ameland in 2013 to function as a virtual power plant.\n\nThe federal government is offering a 10% tax credit for smaller CHP and micro-CHP commercial applications.\n\nIn 2007, the United States company \"Climate Energy\" of Massachusetts introduced the \"Freewatt, a micro-CHP system based on a Honda MCHP engine bundled with a gas furnace (for warm air systems) or boiler (for hydronic or forced hot water heating systems). Through a pilot program scheduled for mid-2009 in the Canadian province of Ontario, the Freewatt system is being offered by home builder Eden Oak with support from ECR International, Enbridge Gas Distribution and National Grid.\n\nThe Freewatt is no longer commercially available (since at least 2014). Through testing it was found to operate at 23.4 and 51% electrical and waste heat recovery efficiency, respectively.\n\nMarathon Engine Systems, a Wisconsin company, produces a variable electrical and thermal output micro-CHP system called the ecopower with an electrical output of 2.2-4.7 kWe. The ecopwer was independently measured to operate at 24.4 and 70.1% electrical and waste heat recovery efficiency, respectively.\n\n\nTesting is underway in Ameland, the Netherlands for a three-year field testing until 2010 of HCNG were 20% hydrogen is added to the local CNG distribution net, the appliances involved are kitchen stoves, condensing boilers, and micro-CHP boilers.\n\nMicro-CHP Accelerator, a field trial performed between 2005 and 2008, studied the performance of 87 Stirling engine and internal combustion engine devices in residential houses in the UK. This study found that the devices resulted in average carbon savings of 9% for houses with heat demand over 54 GJ/year.\n\nAn ASME (American Society of Mechanical Engineers) paper fully describes the performance and operating\nexperience with two residential sized Combined Heat and Power units which were in operation from 1979\nthrough 1995.\n\nOregon State University, funded by the U.S. Department of Energy's Advanced Research Project Agency - Energy (ARPA-e), tested the state of the art micro-CHP systems in the United States. The results showed that the nominally 1 kWe state-of-the-art micro-CHP system operated at an electrical and total efficiency (LHV based) of 23.4 and 74.4%, respectively. The nominally 5 kWe state-of-the-art system operated at an electrical and total efficiency (LHV based) of 24.4 and 94.5%, respectively. The most popular 7 kWe home backup generator (not CHP) operated at an electrical efficiency (LHV based) of 21.5%. The price of the emergency backup generator was an order of magnitude lower than the 5 kWe generator, but the projected life span of the system was over 2 orders of magnitude lower. These results show the trade-off between efficiency, cost, and durability.\n\nThe U.S. Department of Energy's Advanced Research Project Agency - Energy (ARPA-e) has funded $25 million towards mCHP research in the GENerators for Small Electrical and Thermal Systems (GENSETS) program. 12 project teams have been selected to develop a 1 kWe mCHP technology that can achieve 40% electrical efficiency, have a 10-year system life, and cost under $3000.\n\n\n"}
{"id": "4286719", "url": "https://en.wikipedia.org/wiki?curid=4286719", "title": "Millrind", "text": "Millrind\n\nA millrind or simply rind is an iron support, usually four-armed or cross-shaped, for the upper (\"runner\") stone in a pair of millstones.\n\nThe rind is affixed to the top of the square-section main shaft or spindle and supports the entire weight of the runner stone, which can be as much as several tons. The face of a runner stone usually has a carved depression, called the \"Spanish cross\", to accommodate the millrind. The rind is necessary because the grain is fed through the runner stone's central hole, so the spindle cannot be inserted through it like a cartwheel on an axle. \nA later refinement, replacing the cross, was to mount a \"mace\" onto the spindle, which fitted into a gimbal let into the runner stone. The device allowed the runner stone to move in two planes and thus follow the nether (stationary) stone more closely, but great care had to be taken to ensure that its weight was properly balanced. The separation of the nether stone from the runner, controlling the fineness of the grind, was adjusted by the \"tenter\" mechanism: a screw jack to raise or lower the bearing carrying the base of spindle.\n\nThe millrind occasionally appears as a charge in heraldry, in which it is often known by the French name \"fer-de-moline\" (\"iron of a mill\"). Like real millrinds, the fer-de-moline is highly variable in form. The 16th century writer Bossewell characterized it as a symbol fit for judges and magistrates, who keep men on a straight course just as a millrind does with a runner stone. However it is more often found as a rebus in the arms of families with names like Miller or Milne. \n\nAnother charge based on the millrind is the cross moline, which takes the form of a cross with bifurcated ends (sometimes with a pierced centre and sometimes without). In early blazons the term \"fer-de-moline\" often refers to the cross moline.\n"}
{"id": "20766780", "url": "https://en.wikipedia.org/wiki?curid=20766780", "title": "Nuclear fusion–fission hybrid", "text": "Nuclear fusion–fission hybrid\n\nHybrid nuclear fusion–fission (hybrid nuclear power) is a proposed means of generating power by use of a combination of nuclear fusion and fission processes. The basic idea is to use high-energy fast neutrons from a fusion reactor to trigger fission in otherwise nonfissile fuels like U-238 or Th-232. Each neutron can trigger several fission events, multiplying the energy released by each fusion reaction hundreds of times. This would not only make fusion designs more economical in power terms, but also be able to burn fuels that were not suitable for use in conventional fission plants, even their nuclear waste. \n\nThe concept dates to the 1950s, and was strongly advocated by Hans Bethe during the 1970s. At that time the first powerful fusion experiments were being built, but it would still be many years before they could be economically competitive. Hybrids were proposed as a way of greatly accelerating their market introduction, producing energy even before the fusion systems reached break-even. However, detailed studies of the economics of the systems suggested they could not compete with existing fission reactors.\n\nThe idea was abandoned and lay dormant until the 2000s, when the continued delays in reaching break-even led to a brief revival around 2009, notably as the basis of the LIFE program. This program was cancelled when the underlying technology, from the National Ignition Facility, failed to reach its design performance goals. Apollo Fusion, a company founded by Google executive Mike Cassidy in 2017, was also reported to be focused on using the subcritical nuclear fusion-fission hybrid method. \n\nIn general terms, the hybrid is similar in concept to the fast breeder reactor, which uses a compact high-energy fission core in place of the hybrid's fusion core. Another similar concept is the accelerator-driven subcritical reactor, which uses a particle accelerator to provide the neutrons instead of nuclear reactions.\nConventional fission power plants rely on the chain reaction caused when nuclear fission events release neutrons that cause further fission events. Each fission event in uranium releases two or three neutrons, so by careful arrangement and the use of various absorber materials, you can balance the system so one of those neutrons causes another fission event while the other one or two are lost. This careful balance is known as criticality.\n\nNatural uranium is a mix of several isotopes, mainly a trace amount of U-235 and over 99% U-238. When they undergo fission, both of these elements release fast neutrons with an energy distribution peaking around 1 to 2 MeV. This energy is too low to cause fission in U-238, which means it cannot sustain a chain reaction. U-235 will undergo fission when struck by neutrons of this energy, so it is possible for U-235 to sustain a chain reaction, as is the case in a nuclear bomb. However, the probability of one neutron causing fission in another U-235 atom before it escapes the fuel is too low to maintain criticality in a mass of natural uranium, so the chain reaction can only occur in fuels with increased amounts of U-235. This is accomplished by concentrating, or \"enriching\", the fuel, increasing the amount of U-235 to produce enriched uranium, while the leftover, now mostly U-238, is a waste product known as depleted uranium.\n\nU-235 will undergo fission more easily if the neutrons are of lower energy, the so-called \"thermal neutrons\". Neutrons can be slowed to thermal energies through collisions with a neutron moderator material, the easiest to use being the hydrogen atoms found in water. By placing the fission fuel in water, the probability that the neutrons will cause fission in another U-235 is greatly increased, which means the level of enrichment needed to reach criticality is greatly reduced. This leads to the concept of \"reactor-grade\" enriched uranium, with the amount of U-235 increased from just less than 1% to between 3 and 5% depending on the reactor design. This is in contrast to\" weapons-grade\" enrichment, which increases to the U-235 to at least 20%, and more commonly, over 90%.\n\nIn order to maintain criticality, the fuel has to retain that extra concentration of U-235. However, a typical fission reactor burns off enough of the U-235 to cause the reaction to stop over a period on the order of a few months. A combination of burnup of the U-235 along with the creation of neutron absorbers, or \"poisons\", as part of the fission process eventually results in the fuel mass not being able to maintain criticality. This burned up fuel has to be removed and replaced with fresh fuel. The result is nuclear waste that is highly radioactive and filled with long lived radionuclides that present a safety concern.\n\nThe waste contains most of the U-235 it started with, only 1% or so of the energy in the fuel is extracted by the time it reaches the point where it is no longer fissile. One solution to this problem is to reprocess the fuel, which uses chemical processes to separate the U-235 (and other non-poison elements) from the waste, and then uses that U-235 in fresh fuel loads. This reduces the amount of new fuel that needs to be mined, and also concentrates the unwanted portions of the waste into a smaller load. Reprocessing is expensive, however, and has generally been more expensive than simply buying fresh fuel from the mine.\n\nAnother possibility is to \"breed\" Pu-239 from the U-238 through neutron capture, or various other means. In order to do this, higher energy neutrons are required, which means they cannot be moderated as in a conventional reactor. The simplest way to achieve this is to further enrich the original fuel well beyond what is needed for use in a moderated reactor, to the point where the U-235 maintains criticality even with the fast neutrons. The extra fast neutrons escaping the fuel load can then be used to breed fuel in a U-238 assembly surrounding the reactor core, most commonly taken from the stocks of depleted uranium. The Pu-239 is then chemically separated and mixed into fresh fuel for conventional reactors, in the same fashion as normal reprocessing, but the total volume of fuel created in this process is much greater. In spite of this, like reprocessing, the economics of breeder reactors has proven unattractive, and commercial breeder plants have ceased operation.\n\nFusion reactors typically burn a mixture of deuterium (D) and tritium (T). When heated to millions of degrees, the kinetic energy in the fuel begins to overcome the natural electrostatic repulsion between nuclei, the so-called coulomb barrier, and the fuel begins to undergo fusion. This reaction gives off an alpha particle and a high energy neutron of 14 MeV. A key requirement to the economic operation of a fusion reactor is that the alphas deposit their energy back into the fuel mix, heating it so that additional fusion reactions take place. This leads to a condition not unlike the chain reaction in the fission case, known as \"ignition\".\n\nDeuterium can be obtained by the separation of hydrogen isotopes in sea water (see heavy water production). Tritium has a short half life of just over a decade, so only trace amounts are found in nature. To fuel the reactor, the neutrons from the reaction are used to breed more tritium through a reaction in a \"blanket\" of lithium surrounding the reaction chamber. Tritium breeding is key to the success of a D-T fusion cycle, and to date this technique has not been demonstrated. Predictions based on computer modeling suggests that the breeding ratios are quite small and a fusion plant would barely be able to cover its own use. Many years would be needed to breed enough surplus to start another reactor.\n\nFusion–fission designs essentially replace the lithium blanket with a blanket of fission fuel, either natural uranium ore or even nuclear waste. The fusion neutrons have more than enough energy to cause fission in the U-238, as well as many of the other elements in the fuel, including some of the transuranic waste elements. The reaction can continue even when all of the U-235 is burned off; the rate is controlled not by the neutrons from the fission events, but the neutrons being supplied by the fusion reactor.\n\nFission occurs naturally because each event gives off more than one neutron capable of producing additional fission events. Fusion, at least in D-T fuel, gives off only a single neutron, and that neutron is not capable of producing more fusion events. When that neutron strikes fissile material in the blanket, one of two reactions may occur. In many cases, the kinetic energy of the neutron will cause one or two neutrons to be struck out of the nucleus without causing fission. These neutrons still have enough energy to cause other fission events. In other cases the neutron will be captured and cause fission, which will release two or three neutrons. This means that every fusion neutron in the fusion–fission design can result in anywhere between two and four neutrons in the fission fuel.\n\nThis is a key concept in the hybrid concept, known as \"fission multiplication\". For every fusion event, several fission events may occur, each of which gives off much more energy than the original fusion, about 11 times. This greatly increases the total power output of the reactor. This has been suggested as a way to produce practical fusion reactors in spite of the fact that no fusion reactor has yet reached break-even, by multiplying the power output using cheap fuel or waste. However, a number of studies have repeatedly demonstrated that this only becomes practical when the overall reactor is very large, 2 to 3 GWt, which makes it expensive to build.\n\nThese processes also have the side-effect of breeding Pu-239 or U-233, which can be removed and used as fuel in conventional fission reactors. This leads to an alternate design where the primary purpose of the fusion–fission reactor is to reprocess waste into new fuel. Although far less economical than chemical reprocessing, this process also burns off some of the nastier elements instead of simply physically separating them out. This also has advantages for non-proliferation, as enrichment and reprocessing technologies are also associated with nuclear weapons production. However, the cost of the nuclear fuel produced is very high, and is unlikely to be able to compete with conventional sources.\n\nA key issue for the fusion–fission concept is the number and lifetime of the neutrons in the various processes, the so-called \"neutron economy\".\n\nIn a pure fusion design, the neutrons are used for breeding tritium in a lithium blanket. Natural lithium consists of about 92% Li-7 and the rest is mostly Li-6. Li-7 requires neutron energies even higher than those released by fission, around 5 MeV, well within the range of energies provided by fusion. This reaction produces Tritium and Helium-4, and another slow neutron. Li-6 can react with high or low energy neutrons, including those released by the Li-7 reaction. This means that a single fusion reaction can produce several tritiums, which is a requirement if the reactor is going to make up for natural decay and losses in the fusion processes.\n\nWhen the lithium blanket is replaced, or supplanted, by fission fuel in the hybrid design, neutrons that do react with the fissile material are no longer available for tritium breeding. The new neutrons released from the fission reactions can be used for this purpose, but only in Li-6. One could process the lithium to increase the amount of Li-6 in the blanket, making up for these losses, but the downside to this process is that the Li-6 reaction only produces one tritium atom. Only the high-energy reaction between the fusion neutron and Li-7 can create more than one tritium, and this is essential for keeping the reactor running.\n\nTo address this issue, at least some of the fission neutrons must also be used for tritium breeding in Li-6. Every one that does is no longer available for fission, reducing the reactor output. This requires a very careful balance if one wants the reactor to be able to produce enough tritium to keep itself running, while also producing enough fission events to keep the fission side energy positive. If these cannot be accomplished simultaneously, there is no reason to build a hybrid. Even if this balance can be maintained, it might only occur at a level that is economically infeasible.\n\nThrough the early development of the hybrid concept the question of overall economics appeared difficult to handle. A series of studies starting in the late 1970s provided a much clearer picture of the hybrid in a complete fuel cycle, and allowed the economics to be better understood. These studies appeared to indicate there was no reason to build a hybrid.\n\nOne of the most detailed of these studies was published in 1980 by Los Alamos National Laboratory (LANL). Their study noted that the hybrid would produce most of its energy indirectly, both through the fission events in its own reactor, and much more by providing Pu-239 to fuel conventional fission reactors. In this overall picture, the hybrid is essentially identical to the breeder reactor, which uses fast neutrons from plutonium fission to breed more fuel in a fission blanket in largely the same fashion as the hybrid. Both require chemical processing to remove the bred Pu-239, both presented the same proliferation and safety risks as a result, and both produced about the same amount of fuel. Since that fuel is the primary source of energy in the overall cycle, the two systems were almost identical in the end.\n\nWhat was not identical, however, was the technical maturity of the two designs. The hybrid would require considerable additional research and development before it would be known if it could even work, and even if that were demonstrated, the end result would be a system essentially identical to breeders which were already being built at that time. The report concluded:\n\nThe investment of time and money required to commercialize the hybrid cycle could only be justified by a real or perceived advantage of the hybrid over the classical FBR. Our analysis leads us to conclude that no such advantage exists. Therefore, there is not sufficient incentive to demonstrate and commercialize the fusion–fission hybrid.\n\nThe fusion process alone currently does not achieve sufficient gain (power output over power input) to be viable as a power source. By using the excess neutrons from the fusion reaction to in turn cause a high-yield fission reaction (close to 100%) in the surrounding subcritical fissionable blanket, the net yield from the hybrid fusion–fission process can provide a targeted gain of 100 to 300 times the input energy (an increase by a factor of three or four over fusion alone). Even allowing for high inefficiencies on the input side (i.e. low laser efficiency in ICF and Bremsstrahlung losses in Tokamak designs), this can still yield sufficient heat output for economical electric power generation. This can be seen as a shortcut to viable fusion power until more efficient pure fusion technologies can be developed, or as an end in itself to generate power, and also consume existing stockpiles of nuclear fissionables and waste products.\n\nIn the LIFE project at the Lawrence Livermore National Laboratory LLNL, using technology developed at the National Ignition Facility, the goal is to use fuel pellets of deuterium and tritium surrounded by a fissionable blanket to produce energy sufficiently greater than the input (laser) energy for electrical power generation. The principle involved is to induce inertial confinement fusion (ICF) in the fuel pellet which acts as a highly concentrated point source of neutrons which in turn converts and fissions the outer fissionable blanket. In parallel with the ICF approach, the University of Texas at Austin is developing a system based on the tokamak fusion reactor, optimising for nuclear waste disposal versus power generation. The principles behind using either ICF or tokamak reactors as a neutron source are essentially the same (the primary difference being that ICF is essentially a point-source of neutrons while Tokamaks are more diffuse toroidal sources).\n\nThe surrounding blanket can be a fissile material (enriched uranium or plutonium) or a fertile material (capable of conversion to a fissionable material by neutron bombardment) such as thorium, depleted uranium or spent nuclear fuel. Such subcritical reactors (which also include particle accelerator-driven neutron spallation systems) offer the only currently-known means of active disposal (versus storage) of spent nuclear fuel without reprocessing. Fission by-products produced by the operation of commercial light water nuclear reactors (LWRs) are long-lived and highly radioactive, but they can be consumed using the excess neutrons in the fusion reaction along with the fissionable components in the blanket, essentially destroying them by nuclear transmutation and producing a waste product which is far safer and less of a risk for nuclear proliferation. The waste would contain significantly reduced concentrations of long-lived, weapons-usable actinides per gigawatt-year of electric energy produced compared to the waste from a LWR. In addition, there would be about 20 times less waste per unit of electricity produced. This offers the potential to efficiently use the very large stockpiles of enriched fissile materials, depleted uranium, and spent nuclear fuel.\n\nIn contrast to current commercial fission reactors, hybrid reactors potentially demonstrate what is considered inherently safe behavior because they remain deeply subcritical under all conditions and decay heat removal is possible via passive mechanisms. The fission is driven by neutrons provided by fusion ignition events, and is consequently not self-sustaining. If the fusion process is deliberately shut off or the process is disrupted by a mechanical failure, the fission damps out and stops nearly instantly. This is in contrast to the forced damping in a conventional reactor by means of control rods which absorb neutrons to reduce the neutron flux below the critical, self-sustaining, level. The inherent danger of a conventional fission reactor is any situation leading to a positive feedback, runaway, chain reaction such as occurred during the Chernobyl disaster. In a hybrid configuration the fission and fusion reactions are decoupled, i.e. while the fusion neutron output drives the fission, the fission output has no effect whatsoever on the fusion reaction, completely eliminating any chance of a positive feedback loop.\n\nThere are three main components to the hybrid fusion fuel cycle: deuterium, tritium, and fissionable elements. Deuterium can be derived by separation of hydrogen isotopes in sea water (see heavy water production). Tritium may be generated in the hybrid process itself by absorption of neutrons in lithium bearing compounds. This would entail an additional lithium bearing blanket and a means of collection. The third component is externally derived fissionable materials from demilitarized supplies of fissionables, or commercial nuclear fuel and waste streams. Fusion driven fission also offers the possibility of using Thorium as a fuel, which would greatly increase the potential amount of fissionables available. The extremely energetic nature of the fast neutrons emitted during the fusion events (up to 0.17 the speed of light) can allow normally non-fissioning U-238 to undergo fission directly (without conversion first to Pu-239), enabling refined natural Uranium to be used with very low enrichment, while still maintaining a deeply subcritical regime.\n\nPractical engineering designs must first take into account safety as the primary goal. All designs should incorporate passive cooling in combination with refractory materials to prevent melting and reconfiguration of fissionables into geometries capable of un-intentional criticality. Blanket layers of Lithium bearing compounds will generally be included as part of the design to generate Tritium to allow the system to be self-supporting for one of the key fuel element components. Tritium, because of its relatively short half-life and extremely high radioactivity, is best generated on site to obviate the necessity of transportation from a remote location. D-T fuel can be manufactured on site using Deuterium derived from heavy water production and Tritium generated in the hybrid reactor itself. Nuclear spallation to generate additional neutrons can be used to enhance the fission output, with the caveat that this is a tradeoff between the number of neutrons (typically 20-30 neutrons per spallation event) against a reduction of the individual energy of each neutron. This is a consideration if the reactor is to use natural Thorium as a fuel. While high energy (0.17c) neutrons produced from fusion events are capable of directly causing fission in both Thorium and U-238, the lower energy neutrons produced by spallation generally cannot. This is a tradeoff which affects the mixture of fuels against the degree of spallation used in the design.\n\n\n\n\n"}
{"id": "11423298", "url": "https://en.wikipedia.org/wiki?curid=11423298", "title": "Online refuelling", "text": "Online refuelling\n\nIn nuclear power technology, online refuelling is a technique for changing the fuel of a nuclear reactor while the reactor is critical. This allows the reactor to continue to generate electricity during routine refuelling, and therefore improve the availability and profitability of the plant.\n\nOnline refuelling allows a nuclear reactor to continue to generate electricity during periods of routine refuelling, and therefore improves the availability and therefore the economy of the plant. Additionally, this allows for more flexibility in reactor refuelling schedules, exchanging a small number of fuel elements at a time rather than high-intensity offline refuelling programmes.\n\nThe ability to refuel a reactor while generating power has the greatest benefits where refuelling is required at high frequency, for example during the production of plutonium suitable for nuclear weapons during which low-burnup fuel is required from short irradiation periods in a reactor. Conversely, frequent rearrangement of fuel within the core can balance the thermal load and allow higher fuel burnup, therefore reducing both the fuel requirements, and subsequently the amount of high-level nuclear waste for disposal.\n\nAlthough online refuelling is generally desirable, it requires design compromises which means that it is often uneconomical. This includes added complexity to refuelling equipment, and the requirement for these to pressurise during refuelling gas and water-cooled reactors. Online refuelling equipment for Magnox reactors proved to be less reliable than the reactor systems, and retrospectively its use was regarded as a mistake. [Molten salt reactors] and [pebble bed reactors] also require online handling and processing equipment to replace the fuel during operation.\n\nReactors with online refuelling capability to date have typically been either liquid sodium cooled, gas cooled, or cooled by water in pressurised channels. Water-cooled reactors utilising pressurised vessels, for example PWR and BWR reactors and their Generation III descendants, are unsuitable for online refuelling as the coolant is depressurised to allow for disassembly of the pressure vessel and therefore requires a major reactor shutdown. This is typically carried out every 18–24 months.\n\nNotable past and present nuclear power plant designs that have incorporated the ability to refuel online include:\nThere are a number of planned reactor designs which include provision for online refuelling, including pebble-bed and molten salt Generation IV reactors.\n"}
{"id": "26974467", "url": "https://en.wikipedia.org/wiki?curid=26974467", "title": "Optogan", "text": "Optogan\n\nThe Optogan group of companies is a vertically integrated producer of High Brightness LEDs based in St. Petersburg, Russian Federation. The group is also active in Finland and Germany. Founded in 2004 by 3 graduates of Ioffe Phisico-Technical university it is currently owned by various private and government investment funds.\n\nOptogan was founded in 2004 in Espoo, Finland by Dr. Maxim Odnoblyudov, Dr. Vladislav Bougrov and Dr. Alexey Kovsh, all graduates of the chair of Nobel Prize laureate Zhores Alferov. It has received several rounds of financing from various European VC investment funds. After the initial technology development stage the company R&D and pilot production line were expanded to MST Factory site in Dortmund, Germany. After 5 years of technology development stage in 2008 the company was acquired by a strategic investor – Russian private equity group Onexim. In 2009 Russian government investment funds Rusnano and RIK have joined Onexim group as investors to develop a full-scale production facility for HB LEDs in Strelna Free-Economic zone near St. Petersburg, Russian Federation.\n\nOnexim Group – 50% +1 share\nOAO RIK – 33% -1 share\nRusnano - 17%\n\nOptogan is investing in further R&D of GaN technology and building an LED manufacturing facility capable of producing 1,5 billion HB LEDs/month near St.Petersburg in Russia.\n\nOptogan is marketing a range of products, which fall into four main categories: LED chips, packaged LEDs, LED Matrixes (standard light engines consisting of surface mounted LEDs) and Lumminaires.\n\nOptogan company possesses the whole value chain of technologies in solid state lighting (SSL) necessary for the production of GaN based light emitting diodes (LEDs) and consumer-oriented SSL luminaries. The chain of technologies include \n Massive breakthrough in GaN based LED technology started form pioneering works by S. Nakamura in early nineties of last century. OptoGaN technologies are characterized by improved quality of GaN wafers and patented LED epilayer structure with enhanced light generation capability, original f-PowerTM chip design enabling uniform electric current distribution in excess of 300 A/cm2, effectiveness of LEDs as high as 110 lm/W which is at the front level of World leading LED manufactures.\n\n"}
{"id": "32487957", "url": "https://en.wikipedia.org/wiki?curid=32487957", "title": "Outerra", "text": "Outerra\n\nOuterra is a Slovak computer software company best known for its middleware 3D planetary graphics engine, called Outerra engine, in development since 2008. The engine renders high-quality terrain, terrain texturing, flora and water flow normal maps using relatively sparse and highly compressed data through fractal processing and other types of procedural generation. The game \"Anteworld\" uses real world data to create a virtual replica of planet Earth.\n\nFeatures of the engine and its tech-demo \"Anteworld\" include:\n\n\nThe developers of Outerra in 2012 released an alpha tech-demo for the engine called \"Anteworld\" (the name comes from the Latin prefix \"Ante-\", meaning \"prior-to in time\" - \"A world that was\") that consists of a digital replica of the whole planet Earth at a 1:1 scale. The virtual world can be explored in a free-camera mode as well as in vehicles such as planes, boats and cars. It also features a first person walking mode. For the mirror world real world data was used - ingame the user can blend in an embedded Google Maps of real Earth that is synchronized with the current camera position. Since June 2013 Anteworld provides support for Oculus Rift. Furthermore, user-made objects such as houses and vehicles can be spawned and used in the sandbox game. While the tech-demo is free some features require an upgrade to the $15 full version. An accompanying novella that is loosely tied to the game written by C. Shawn Smith is planned as well.\n\nIn 2013 a separate non-profit motivated group of hobbyists released a first version of terrain data of Middle-Earth compiled for Outerra. The goal of this Digital Elevation Model project, which was launched by Oshyan Greene and Carl Lingard in 2006, is what they summarize as a \"living, breathing Middle Earth\" - a highly detailed model which includes rivers, vegetation, buildings, roads and subterranean features. The sources for the maps include Tolkien's maps (such as the ones in \"The Lord of the Rings\" and \"The Silmarillion\"), Strachey's \"Journeys of Frodo\", Fonstad's \"The Atlas of Middle-earth\", the locations used in Peter Jackson's movies as well as fan made maps.\n\nIn February 2014 the developers announced ongoing development, which was planned since October 2009, to add another planet to the two already existing ones (Earth and the fictional Middle Earth) - Mars. Additionally in 2014 they stated that \"ultimately the whole solar system should be accessible\" in a single game on Twitter.\n\nTitanIM (Titan Integrated Military) is an Outerra-based military simulation platform that was revealed in December 2014 at the world's largest modeling, simulation and training conference oriented at military use, I/ITSEC in Orlando. TitanIM was granted exclusive license to the Outerra engine for military use. The initial public release version of the software is known as Titan Vanguard.\n\n\n"}
{"id": "20028344", "url": "https://en.wikipedia.org/wiki?curid=20028344", "title": "PUCRS Museum of Science and Technology", "text": "PUCRS Museum of Science and Technology\n\nThe PUCRS Museum of Science and Technology is a Brazilian museum run by the Pontifical Catholic University of Rio Grande do Sul (PUCRS), located in the city of Porto Alegre in Rio Grande do Sul, on Avenida Ipiranga 6681, Building 40, Parthenon in the neighborhood, near the PUCRS. The visiting hours are from Tuesdays to Sundays, from 9am to 5pm.\n\nThe museum was inaugurated on December 9, 1998. It is the only interactive museum of natural sciences in Latin America and one of the best in the world, offering activities for all ages and showing areas of trial-attractions on the universe, Earth, environment and man, among others. \n\nThe permanent exhibition area of the public has about 700 interactive experiments, covering many areas of knowledge. Even the visitor can participate in experiments that resulted in the current scientific knowledge. \n\nThe permanent collection includes several million pieces and presents one of the best displays of natural sciences from around the country. These include parts of paleontological geopark of paleorrota, a major exhibition of birds and stuffed animals and mineral resources Brazilians, some with thousands of samples.\n\n\n"}
{"id": "14357056", "url": "https://en.wikipedia.org/wiki?curid=14357056", "title": "Pan-European automated clearing house", "text": "Pan-European automated clearing house\n\nA pan-European automated clearing house (PE-ACH) is a clearing house that is able to settle SEPA compliant credit transfers and direct debits across the Eurozone.\n\nAt present there is only one PE-ACH in operation – STEP2 – which was established by the Euro Banking Association in April 2003. According to a survey done by Equens, in the future one PE-ACH might be less relevant as banks will settle their transactions via multiple clearing houses rather than using one central clearing house.\n"}
{"id": "966255", "url": "https://en.wikipedia.org/wiki?curid=966255", "title": "Participatory design", "text": "Participatory design\n\nParticipatory design (originally co-operative design, now often co-design) is an approach to design attempting to actively involve all stakeholders (e.g. employees, partners, customers, citizens, end users) in the design process to help ensure the result meets their needs and is usable. Participatory design is an approach which is focused on processes and procedures of design and is not a design style. The term is used in a variety of fields e.g. software design, urban design, architecture, landscape architecture, product design, sustainability, graphic design, planning, and even medicine as a way of creating environments that are more responsive and appropriate to their inhabitants' and users' cultural, emotional, spiritual and practical needs. It is one approach to placemaking.\n\nRecent research suggests that designers create more innovative concepts and ideas when working within a co-design environment with others than they do when creating ideas on their own.\n\nParticipatory design has been used in many settings and at various scales. For some, this approach has a political dimension of user empowerment and democratization. For others, it is seen as a way of abrogating design responsibility and innovation by designers.\n\nIn several Scandinavian countries, during the 1960s and 1970s, participatory design was rooted in work with trade unions; its ancestry also includes action research and sociotechnical design.\n\nIn participatory design, participants (putative, potential or future) are invited to cooperate with designers, researchers and developers during an innovation process. Potentially, they participate during several stages of an innovation process: they participate during the initial exploration and problem definition both to help define the problem and to focus ideas for solution, and during development, they help evaluate proposed solutions. Maarten Pieters and Stefanie Jansen describe co-design as part of a complete co-creation process, which refers to the \"transparent process of value creation in ongoing, productive collaboration with, and supported by all relevant parties, with end-users playing a central role\" and covers all stages of a development process. \n\nIn \"Co-designing for Society\", Deborah Szebeko and Lauren Tan list various precursors of co-design, starting with the Scandinavian participatory design movement and then state \"Co-design differs from some of these areas as it includes all stakeholders of an issue not just the users, throughout the entire process from research to implementation.\"\n\nIn contrast, Elizabeth Sanders and Pieter Stappers state that \"the terminology used until the recent obsession with what is now called co-creation/co-design\" was \"participatory design\".\n\nFrom the 1960s onwards there was a growing demand for greater consideration of community opinions in major decision-making. In Australia many people believed that they were not being planned 'for' but planned 'at'. (Nichols 2009). A lack of consultation made the planning system seem paternalistic and without proper consideration of how changes to the built environment affected its primary users. In Britain 'the idea that the public should participate was first raised in 1965' (Taylor, 1998, p. 86). However the level of participation is an important issue. At a minimum public workshops and hearings have now been included in almost every planning endeavour. Yet this level of consultation can simply mean information about change without detailed participation. Involvement that 'recognises an active part in plan making' (Taylor, 1998, p. 86) has not always been straightforward to achieve. Participatory design has attempted to create a platform for active participation in the design process, for end users.\n\nParticipatory design was actually born in Scandinavia and called \"cooperative design\". However, when the methods were presented to the US community 'cooperation' was a word that didn't resonate with the strong separation between workers and managers - they weren't supposed to discuss ways of working face-to-face. Hence, 'participatory' was instead used as the initial Participatory Design sessions weren't a direct cooperation between workers and managers, sitting in the same room discussing how to improve their work environment and tools, but there were separate sessions for workers and managers. Each group was participating in the process, not directly cooperating. (in historical review of Cooperative Design, at a Scandinavian conference).\n\nIn Scandinavia, research projects on user participation in systems development date back to the 1970s. The so-called \"collective resource approach\" developed strategies and techniques for workers to influence the design and use of computer applications at the workplace: The Norwegian Iron and Metal Workers Union (NJMF) project took a first move from traditional research to working with people, directly changing the role of the union clubs in the project.\n\nThe Scandinavian projects developed an action research approach, emphasizing active co-operation between researchers and workers of the organization to help improve the latter's work situation. While researchers got their results, the people whom they worked with were equally entitled to get something out of the project. The approach built on people's own experiences, providing for them resources to be able to act in their current situation. The view of organizations as fundamentally harmonious—according to which conflicts in an organization are regarded as pseudo-conflicts or \"problems\" dissolved by good analysis and increased communication—was rejected in favor of a view of organizations recognizing fundamental \"un-dissolvable\" conflicts in organizations (Ehn & Sandberg, 1979).\n\nIn the Utopia project (Bødker et al., 1987, Ehn, 1988), the major achievements were the experience-based design methods, developed through the focus on hands-on experiences, emphasizing the need for technical and organizational alternatives (Bødker et al., 1987).\n\nThe parallel Florence project (Gro Bjerkness & Tone Bratteteig) started a long line of Scandinavian research projects in the health sector. In particular, it worked with nurses and developed approaches for nurses to get a voice in the development of work and IT in hospitals. The Florence project put gender on the agenda with its starting point in a highly gendered work environment.\n\nThe 1990s led to a number of projects including the AT project (Bødker et al., 1993) and the EureCoop/EuroCode projects (Grønbæk, Kyng & Mogensen, 1995).\n\nIn recent years, it has been a major challenge to participatory design to embrace the fact that much technology development no longer happens as design of isolated systems in well-defined communities of work (Beck, 2002). At the dawn of the 21st century, we use technology at work, at home, in school, and while on the move.\n\nCo-design is often used by trained designers who recognize the difficulty in properly understanding the cultural, societal, or usage scenarios encountered by their user. C. K. Prahalad and Venkat Ramaswamy are usually given credit for bringing co-creation/co-design to the minds of those in the business community with the 2004 publication of their book, The Future of Competition: Co-Creating Unique Value with Customers. They propose:\n\nThe phrase co-design is also used in reference to the simultaneous development of interrelated software and hardware systems. The term co-design has become popular in mobile phone development, where the two perspectives of hardware and software design are brought into a co-design process.\n\nResults directly related to integrating co-design into existing frameworks is \"researchers and practitioners have seen that co-creation practiced at the early front end of the design development process can have an impact with positive, long-range consequences.\"\n\nDiscourses in the PD literature have been sculpted by three main concerns: (1) the politics of design, (2) the nature of participation, and (3) methods, tools and techniques for carrying out design projects. (Finn Kensing1 & Jeanette Blomberg, 1998, p. 168)\n\nThe politics of design have been the concern for many design researchers and practitioners. Kensing and Blomberg illustrate the main concerns which related to the introduction of new frameworks such as system design which related to the introduction of computer-based systems and power dynamics that emerge within the workspace. The automation introduced by system design has created concerns within unions and workers as it threatened their involvement in production and their ownership over their work situation.\n\nMajor international organizations such as Project for Public Spaces create opportunities for rigorous participation in the design and creation of place, believing that it is the essential ingredient for successful environments. Rather than simply consulting the public, PPS creates a platform for the community to participate and co-design new areas, which reflect their intimate knowledge. Providing insights, which independent design professionals such as architects or even local government planners may not have.\n\nUsing a method called Place Performance Evaluation or (Place Game), groups from the community are taken on the site of proposed development, where they use their knowledge to develop design strategies, which would benefit the community.\n\"Whether the participants are schoolchildren or professionals, the exercise produces dramatic results because it relies on the expertise of people who use the place every day, or who are the potential users of the place.\" This successfully engages with the ultimate idea of participatory design, where various stakeholders who will be the users of the end product, are involved in the design process as a collective.\n\nSimilar projects have had success in Melbourne, Australia particularly in relation to contested sites, where design solutions are often harder to establish. The Talbot Reserve in the suburb of St. Kilda faced numerous problems of use, such as becoming a regular spot for sex workers and drug users to congregate. A Design In, which incorporated a variety of key users in the community about what they wanted for the future of the reserve allowed traditionally marginalised voices to participate in the design process. Participants described it as 'a transforming experience as they saw the world through different eyes.' (Press, 2003, p. 62). This is perhaps the key attribute of participatory design, a process which, allows multiple voices to be heard and involved in the design, resulting in outcomes which suite a wider range of users. It builds empathy within the system and users where it is implemented, which makes solving larger problems more holistically. As planning affects everyone it is believed that 'those whose livelihoods, environments and lives are at stake should be involved in the decisions which affect them' (Sarkissian and Perglut, 1986, p. 3)\n\nParticipatory design has many applications in development and changes to the built environment. It has particular currency to planners and architects, in relation to placemaking and community regeneration projects. It potentially offers a far more democratic approach to the design process as it involves more than one stakeholder. By incorporating a variety of views there is greater opportunity for successful outcomes. Many universities and major institutions are beginning to recognise its importance. The UN, Global studio involved students from Columbia University, University of Sydney and Sapienza University of Rome to provide design solutions for Vancouver's downtown eastside, which suffered from drug- and alcohol-related problems. The process allowed cross-discipline participation from planners, architects and industrial designers, which focused on collaboration and the sharing of ideas and stories, as opposed to rigid and singular design outcomes. (Kuiper, 2007, p. 52)\n\nMany local governments require community consultation in any major changes to the built environment. Community involvement in the planning process is almost a standard requirement in most strategic changes. Community involvement in local decision making creates a sense of empowerment. The City of Melbourne Swanston Street redevelopment project received over 5000 responses from the public allowing them to participate in the design process by commenting on seven different design options. While the City of Yarra recently held a 'Stories in the Street' consultation, to record peoples ideas about the future of Smith Street. It offered participants a variety of mediums to explore their opinions such as mapping, photo surveys and storytelling. Although local councils are taking positive steps towards participatory design as opposed to traditional top down approaches to planning, many communities are moving to take design into their own hands.\n\nPortland, Oregon City Repair Project is a form of participatory design, which involves the community co-designing problem areas together to make positive changes to their environment. It involves collaborative decision-making and design without traditional involvement from local government or professionals but instead runs on volunteers from the community. The process has created successful projects such as intersection repair, which saw a misused intersection develop into a successful community square.\n\nPeer-to-peer urbanism is a form of decentralized, participatory design for urban environments and individual buildings. It borrows organizational ideas from the open-source software movement, so that knowledge about construction methods and urban design schemes is freely exchanged.\n\nIn the English-speaking world, the term has a particular currency in the world of software development, especially in circles connected to Computer Professionals for Social Responsibility (CPSR), who have put on a series of Participatory Design Conferences. It overlaps with the approach Extreme Programming takes to user involvement in design, but (possibly because of its European trade union origins) the Participatory Design tradition puts more emphasis on the involvement of a broad population of users rather than a small number of user representatives.\n\nParticipatory design can be seen as a move of end-users into the world of researchers and developers, whereas empathic design can be seen as a move of researchers and developers into the world of end-users. There is a very significant differentiation between user-design and user-centered design in that there is an emancipatory theoretical foundation, and a systems theory bedrock (Ivanov, 1972, 1995), on which user-design is founded. Indeed, user-centered design is a useful and important construct, but one that suggests that users are taken as centers in the design process, consulting with users heavily, but not allowing users to make the decisions, nor empowering users with the tools that the experts use. For example, Wikipedia content is user-designed. Users are given the necessary tools to make their own entries. Wikipedia's underlying wiki software is based on user-centered design: while users are allowed to propose changes or have input on the design, a smaller and more specialized group decide about features and system design.\n\nParticipatory work in software development has historically tended toward two distinct trajectories, one in Scandinavia and northern Europe, and the other in North America. The Scandinavian and northern European tradition has remained closer to its roots in the labor movement (e.g., Beck, 2002; Bjerknes, Ehn, and Kyng, 1987). The North American and Pacific rim tradition has tended to be both broader (e.g., including managers and executives as \"stakeholders\" in design) and more circumscribed (e.g., design of individual \"features\" as contrasted with the Scandinavian approach to the design of \"entire systems\" and design of the \"work that the system is supposed to support\") (e.g., Beyer and Holtzblatt, 1998; Noro and Imada, 1991). However, some more recent work has tended to combine the two approaches (Bødker et al., 2004; Muller, 2007).\n\nDistributed participatory design (DPD) is a design approach and philosophy that supports the direct participation of users and other stakeholders in system analysis and design work. Nowadays design teams most often are distributed, which stress a need for support and knowledge gathered from design of distributed systems. Distributed participatory design aims to facilitate understanding between different stakeholders in distributed design teams by giving each the opportunity to engage in hands-on activities.\n\nForms of distributed participatory design, or mass-participatory design, are now most frequently seen online. Web-based, mass-participatory design platforms are increasingly seen in both the consumer product and architectural design industries, facilitating direct public engagement at multiple stages of the design process. Website designs and planning now incorporate social networking into their interfaces to increase distributed participation. This integration helps \"funnel\" or redirect traffic towards a website or group of websites, increasing website exposure, traffic, and the number of access points to a website. By providing users with multiple venues for interacting with and giving feedback to companies, content creators, and other users online have been able to strengthen networks and online communities.\n\nThe interconnectivity factor of online participatory design encourages users to engage in participation on websites outside those they regularly visit enabling the connection, strengthening, and creation of online communities and fandoms. On social networking websites, companies and creators create specialized pages or accounts for interacting with any users and consumers more directly and in more readily usable forms, as opposed to older methods of retrieving feedback. Through using various websites, users can participate in the design process with the medium they are most comfortable using, accommodating differences in technical background and navigating unfamiliar websites.\n\nFeedback can typically be seen in the form of comment sections, rating systems, or reviews from which companies and content creators may determine possible future changes in design and organization. Not only does this make evaluation and feedback by users easier, but also serves as a way to more efficiently categorize and process the information in a more effective and organized manner. The use of distributed participatory design on the internet has eliminated many of the intermediate steps between a consumer's response and the company's reception thus decreasing transaction costs.\n\nIn terms of distributed participatory design, YouTube and their content creators, or YouTubers, incorporate many of these elements into their website designs and planning. Video pages contain a 'share' function that allows for individuals to circulate a link to a video through various social media sites to increase exposure and possibly redirect people to other sites content creators use for circulating media and for receiving reactions. Additionally, feedback can appear in the form of comments and ratings. Each video has separate comment sections for users to leave input and ideas in. YouTube also uses a rating system of thumbs up/thumbs down to provide the content creators with a statistic on how well a video was received. Many popular YouTubers use social media networks such as Facebook, Twitter, Instagram, and Google+ to announce video updates and any information on external projects. Through managing social networks, website, and YouTube channel, the content creators can manage the distributed participation effectively and maintain their fanbases as well as update them on any changes in the design or content creation process.\n\n\n"}
{"id": "2347310", "url": "https://en.wikipedia.org/wiki?curid=2347310", "title": "Pennisetum purpureum", "text": "Pennisetum purpureum\n\nPennisetum purpureum, also known as Napier grass, elephant grass or Uganda grass, is a species of perennial tropical grass native to the African grasslands. It has low water and nutrient requirements, and therefore can make use of otherwise uncultivated lands. Historically, this wild species has been used primarily for grazing; recently, however, it has been incorporated into a pest management strategy. This technique involves the desired crop being planted alongside a 'push' plant, which repels pests, in combination with a 'pull' crop around the perimeter of the plot, which draw insects out of the plot. Napier grass has shown potential at attracting stemborer moths (a main cause of yield loss in Africa) away from maize and hence is the \"pull\" crop. This strategy is much more sustainable, serves more purposes and is more affordable for farmers than insecticide use. In addition to this, Napier grasses improve soil fertility, and protect arid land from soil erosion. It is also utilized for firebreaks, windbreaks, in paper pulp production and most recently to produce bio-oil, biogas and charcoal.\n\n\"Pennisetum purpureum\" is a monocot C4 perennial grass in the Poaceae family. It is tall and forms in robust bamboo-like clumps. It is a heterozygous plant, but seeds rarely fully form; more often it reproduces vegetatively through stolons which are horizontal shoots above the soil that extend from the parent plant to offspring. This species has high biomass production, at about 40 tons/ha/year and can be harvested 4-6 times per year. Additionally it requires low water and nutrient inputs.\n\nNapier can be propagated through seeds, however as seed production is inconsistent, collection is difficult. Alternatively, it can be planted through stem cuttings of the stolons. The cuttings can be planted by inserting them along furrows 75 cm apart, both along and between rows.\n\nStemborers (\"Busseola fusca\" and \"Chilo partellus\") are the cause of 10% of total yield loss in Southern and Eastern Africa and on average 14-15% in sub-Saharan Africa. The larvae cause immense damage to maize and sorghum by burrowing into their stems and eating from within. This not only makes them difficult to detect and remove but also damages the vascular tissue necessary for plant growth. Insecticide effectiveness is low against stemborers, as larvae are protected by protective cell wall layers around the stem. Insecticides are also expensive for poor farmers and can build chemical resistance by the pests. In addition, chemicals are carried into final food products. Instead of trying to prevent the occurrence of pests, the push-pull strategy (also known as stimuli-deterrent) aims to guide their inevitable biological evolution to prevent damage to valued crops. The method proposes that sorghum or corn be intercropped with Desmodium (the \"push\" plant), which repels the moths as they look to lay their eggs. Desmodium also provides a ground cover and is nitrogen fixing, which improves soil fertility while decreasing labour involved with weeding. This deterrent is used in combination with Napier grass planted around the perimeter of the plot. A study of Kenyan farmers using the push-pull strategy reported an 89% reduction in Striga (a parasitic weed), an 83% increase in soil fertility, and 52% effectiveness in stemborer control. Considering that striga, stemborers, and low soil fertility together cause yield losses of an estimated 7 billion US dollars or enough to feed 27 million people, the implementation of this technique could significantly reduce food insecurity.\n\nAlthough promising as a sustainable and affordable option, the success of push-pull pest management highly depends on proper implementation in combination with other good ecological practices. Firstly, not all varieties of Napier grass function as a trap. In a study of eight varieties, only two bana and Ugandan hairless Napier varieties significantly attracted female moths for egg placement over maize. Of these two, only bana significantly decreased survival rates. In a farmer's field, it is recommended that three rows of bana Napier grass be planted as a border crop around the entire field. Potential exists to improve the push-pull strategy through further trials with different intercrops, by manipulating allelochemicals in each intercrop, as well as by investigating insect sensitivity to natural chemicals. Once prominent in a field, it is difficult to rid the area of the stemborer pests as larvae can remain dormant, and therefore push-pull management will not have the intended effect. It is recommended that if an infestation is particularly severe, neither corn, nor sorghum should be planted in the same field the following year but instead rotated with other crops. It is also important to burn infested stalks or, if they have an intended use, to leave them out in the sun for three days. The use of push-pull pest management must be used in combination with good ecological practices to yield the desired results.\nFinally, the establishment of a push-pull system requires increased labour in the primary stages and a large enough land plot to allow space for a non-food crop to be planted; these factors often deter its adoption. A program could increase adoption rates through promoting its use in combination with livestock, giving economic value to the planting of Napier.\nMore information can be found at http://www.push-pull.net/ as well as specifics for implementation at http://www.push-pull.net/farmers_guide_2012.pdf .\n\nNapier grass is the most important fodder crop for the dairy farmers in East Africa. Its high productivity makes it particularly suited to feed cattle and buffaloes. It is also an important forage for elephants in Africa, hence its name \"elephant grass\". Hairless varieties, such as Ugandan hairless, have much higher value as fodder. As it is able to grow with little water and nutrients, grazing has made productive use of arid lands for food production. Furthermore, livestock can be incorporated into the pull-push management system providing another economically viable purpose for the ‘trap’ plant. Napier grass is valuable to African landscapes as it prevents soil erosion. It can also serve as a fire break, a wind break, and to improve soil fertility.\nMore recently, Napier has been used to alleviate pressure on food production as there is 2Gha of non-arable land suitable for energy crop production. Thermal pyrolytic conversion is used to produce charcoal, biogas and bio-oil. Although this technology is not currently in use, it could be implemented as a means of providing energy to African communities, while enriching the soils of the local landscape. It is also used as source of fuel. The young leaves and shoots are edible and are cooked to make soups and stews.\n\nA Dutch company also successfully turned the grass into a plastic that can be used for packaging.\n\n"}
{"id": "31586643", "url": "https://en.wikipedia.org/wiki?curid=31586643", "title": "Policies promoting wireless broadband in the United States", "text": "Policies promoting wireless broadband in the United States\n\nPolicies promoting wireless broadband are policies, rules, and regulations supporting the \"National Wireless Initiative\", a plan to bring wireless broadband Internet access to 98% of Americans.\n\nSpectrum is limited and much of it already in use. This raises the issue of space and strength of supporting the network. The infrastructure has to reach across the entire United States in areas that normally do not have Internet access. The main concept is to bring wireless service to residents in areas that may otherwise not have access to it. The public's interest in this plan is important as the people are the ones who will utilize this service. Network neutrality raises issues on freedom of information and who will have control over how the information is released, or even lack of control.\n\nThe Memorandum on Unleashing the Wireless Broadband Revolution claimed that wireless Internet access had the potential to enhance economic competition and improve the quality of life. The Internet is considered an important part of the economy and advanced business opportunities as it is a vital infrastructure. The Code of Federal Regulations says that this is the beginning of the next transformation in information technology, as we encounter the wireless broadband revolution.\n\nThe initial plan by President George W. Bush was to have broadband availability for all Americans by 2007. In February, 2011, President Obama announced details of the \"National Wireless Initiative\" or \"Wireless Innovation and Infrastructure Initiative\".\n\nFederal Communications Commission (FCC) Chairperson Michael K. Powell created the Wireless Broadband Access Task Force to help bring the plan together. The members study existing wireless broadband policies, making recommendations in the FCC's policies for acceleration in the deployment of the wireless technologies and services. This is completed by seeking out the expertise, experience, and advice of consumers, state and local governments, the industry, and other stakeholders. These recommendations are intended to assist with how to make policies and further progress the process for the national wireless plan. They are based on the inquiry of the state of wireless broadband as well as the FCC's policies that impact these services.\n\nPowell commented in a statement that this broadband plan is a catalyst for positive change, bringing resources and jobs to communities across the country. The CTIA The Wireless Association encouraged legislative action that recognizes the unique and invaluable role of wireless in providing Americans Internet access.\n\nThis plan included issues such as:\n\nThe plan is to free up enough of the radio spectrum from licensed and unlicensed space. To free up space, the FCC intends to hold incentive auctions, spurring innovation, by current licensees voluntarily giving up their spectrum space. The CTIA has requested the FCC to obtain more capacity on the spectrum by placing priority on additional spectrum through the national wireless plan. This hopes to ensure space on the spectrum for the wireless broadband to work. The auction would also require legislation to conduct for the spectrum to be reassigned and reallocated. In this plan, legislation is needed for the FCC to hold these auctions to enable the current spectrum holders to realize a portion of the revenues if they participate. These voluntary incentive auctions for licensees are a critical part of freeing the spectrum, as well as encouraging the government to more efficiently use the spectrum. The auction is intended to bring profit back to the United States and new licensees. However, it was brought up in February 2011 that it is not clear on whether these incentive auctions will take place. One goal of the plan is to reduce the national deficit by approximately $10 billion through these license auctions and other business opportunities. The auctions and increased spectrum efficiency from the government would raise $27.8 billion over the next ten years. The government will be expected to use the spectrum more efficiently using new financial-compensation tools with commitments to using advanced technologies. There is also the idea to spur innovation by using $3 billion of the spectrum proceeds for research and development of newer wireless technologies and applications.\n\nWireless requires bandwidth, and because of this, there would need to be enough of the spectrum obtained to sustain the bandwidth of the network. It also needs to be considered that upload links and downstream communication can require a difference amount of space. In the FCC News in 2005, it is mentioned that there needs to be enough spectrum to account for the unbalancing of broadband services. The service typically needs a larger amount of bandwidth for downstream than for upload links. Wireless will only work with adequate spectrum to support the initiative and the many devices, networks, and applications that it will run. President Obama has set a goal of freeing 500 MHz of the spectrum for any wireless device within a decade. The CTIA also supports this within the next ten years. The space would also need to be within the stronger part of the spectrum. According to the 112th United States Congress, the Public Safety Spectrum and Wireless Innovation Act calls for this to be within the 700 MHz D block spectrum for rural and urban areas and was originally requested to be done before the Digital TV transition. The 700 MHz D block refers to the portion of the spectrum between the following frequencies\n\n\nIn its pursuit to support the FCC, the CTIA also recommended the spectrum in the following ranges\n\n\nWith 4G deployment rising, it is critical to have the airwave space to support future innovation and to avoid the spectrum crunch. This provides clearance in the spectrum that is already allocated to wireless carriers. The CTIA also have requested access to existing utility poles where new construction is not possible. Although the spectrum is wide, the science and physics of the spectrum still create limited amounts of space.\n\nPresident Obama also plans to increase public safety by reallocating the D block of the spectrum and $500 million within the Wireless Innovation (WIN) Fund. The 700 MHz D block spectrum would be reallocated and integrated for public safety entities. The Communications Act of 1934 would be amended to increase the electromagnetic spectrum by 10 MHz for public safety. An important part of this spectrum plan is that there is already space for public safety. One piece that needs to be determined is how to integrate the 700 MHz D block that will be reallocated with the existing public safety spectrum. The current 20 MHz of public safety spectrum needs to be determined how to be licensed as well. The considerations are nationwide, regional, statewide or some combination in accordance with the public interest.\n\nIn Section 205 of the Public Safety Spectrum and Wireless Innovation Act of the 112th United States Congress, it is required to have a review of the use of the spectrum after a certain period of time of the deployment. After no more than 5 years of the implementation of the wireless network plan, the Commission must conduct a survey and submit a report regarding the public safety spectrum. This includes how the spectrum is being used, recommendations on whether more spectrum is needed and determine if there is opportunity for some of the spectrum to be returned for other commercial purposes. The report intends to make sure that there is the right amount of spectrum and ensure it is being used for the correct purposes, as there is only a limited amount of spectrum overall. From the Administration of Barack Obama, they also would like to test the value of underutilized spectrum to be able to open new avenues of use. Since spectrum space is limited, looking at other spectrum could allow other licensees to move elsewhere, perhaps freeing up more for wireless. Utilizing other spectrum would allow development of advanced technologies. The Secretary of Commerce, National Science Foundation (NSF), Department of Defense, Department of Justice, NASA and other organizations have been designated to create a plan to explore these innovative spectrum-sharing technologies.\n\nIn the 1950s, the town of Ten Sleep, Wyoming, had just set up their phone service using federal subsidies and stringing copper wire to every home. In 2005, they upgraded to fiber optic cable, giving the residents high-speed Internet access. This scenario caught President Obama's eye in terms of success, which he hopes to duplicate with the national wireless broadband. On February 10, 2011, he pointed to this example of what he wants to replicate and hopes it will help progress more economic development by providing Internet to almost all Americans. Brendan Greenley from \"Business Week\" magazine does not believe Obama's plan will create another success story. While examples are helpful to reference when creating a plan, not all plans react the same way. There are many different factors involved, such as geography and the type of users involved.\n\nHow a technology is designed and built is just as important as the technology itself. Without a proper infrastructure, a national wireless broadband network would not benefit the country. George Ford from the Phoenix Center commented that a reasonable target for broadband would be 95% Internet availability to Americans in five years and questioned the need for coverage across the entire country. There has been a large permeation of Internet users over the last five years. However, there are still reasons to have wired networking. As stated by Brendan Greeley, call centers and data storage facilities placed in smaller towns need the speed and capacity that a wired fiber optic network can provide. Wireless networks pose challenges that wired networks do not. One challenge is the infrastructure of a wireless network, like the spectrum, versus a wired network. For Ten Sleep, they installed fiber optic cable to increase their network speed. Wireless does not have this capability. Fiber optic cable has more capacity than the electromagnetic spectrum, meaning even if the entire spectrum was allocated to the national wireless network, there still would not be the same capacity in fiber optic.\n\nTechnology is growing at an incredible speed, with one important technology being the speed of information. The newest generation of speed at \"4G\" is being deployed at rapid speeds throughout the United States by the leading carriers, and promises to be greatly beneficial to the economy and society. Next-generation technology is ten times faster than current speeds and is capable of benefiting all Americans, helping public safety increase, and further progressing the innovation for wireless applications, equipment and services. The advancement of technology is intended to move us forward and catch up with other nations that have already implemented these technologies. The technological advances in wireless broadband, like mobile broadband, provide a solid foundation for improved delivery of services.\n\nA supported infrastructure is important for any network, whether technologically based or socially based. The national wireless network is no different. Under Section 105 Interoperability of the bill S.28, the Commission must establish the technical and operational rules to ensure the national wireless networks are interoperable. It has so far yet to be established as to who will actually support the network, being the government or a private Internet service provider. Rules are to be established to permit a public safety licensee to authorize a service provider to construct and operate the wireless network. It is also in the plan to have the service provider use their licensee spectrum if the authorization would expedite broadband communications. The supporting parties will also have to ensure the safety of the network by protecting and monitoring against cyber attacks and any other form of security threat. It is imperative to have a secure network that is accessible for the nation to use. A safe environment is needed for new capabilities to be secure, trustworthy and provide necessary safeguards for privacy of users and public safety.\n\nPresident Obama estimated a one-time investment of $5 billion and a reformation of the Universal Service Fund to help millions of Americans get access to these technologies. Another estimated cost of the wireless broadband plan is $7.2 billion from stimulus funds. Another plan is calling for $10.7 billion to commit to the support, development and deployment of the wireless broadband. Despite the cost, wireless would help with public safety affordance of greater levels of effectiveness and interoperability. With broadband technologies developing, equipment and services are getting faster and cheaper. Again, Obama proposes to pay for the wireless network by having broadcasters give back their privilege to the spectrum for government auction. The auction would then be mostly profit so costs would come from the infrastructure and maintenance of the network, not the spectrum space. It is still questioned whether the costs are too high and if the end benefits outweigh those costs. In a report by George Ford at Fox News Channel, he stated that spending money on the last frontier of broadband has small incremental value. Obama not only estimated a one-time investment, but also stated a public safety cost. He called for a $10.7 billion investment to ensure public safety benefits from the technology and to have $3.2 billion to reallocate the D block of the spectrum as mentioned before. This band of the spectrum would be reserved solely for public safety as stated under current law. Another $7 billion would be needed to support the deployment of the network, and then $500 million from the Wireless Innovation (WIN) Fund for research and development, and to tailor the network to public safety needs. Although many billions of dollars will go towards building this plan, reducing the national deficit by billions of dollars can be considered worthwhile. Again, President Obama does hope that this plan will cut the nation deficit by $9.6 billion over the next ten years.\n\nThe thought of whether the cost is too high also raises other points in the media. John Horrigan of the Pew Research Center commented that the high cost of broadband now is why more Americans are not already using it. There is also the consideration that not every American has access to a computer. Although smartphones with Internet capabilities have been on the rise for many years now, there is still a reason the entire nation is not \"online\". Whether it is the cost of an Internet-ready device or computer, or obtaining Internet service, cost is still a big factor for this technology. In terms of cost, one-third of Americans who do not have broadband access say cost prohibits them from purchasing it.\n\nPublic interest is important for policies promoting wireless broadband for Americans. The interest of the public is important because if the public does not condone the cost and the utility, the nation will have to cover the failure of the deployment. However, the public may also consider this to be an excellent development for our country. Part of the national wireless broadband goal is to enable businesses to grow faster, help students learn more, and assist public safety officers with having the best, state-of-the-art technology and communications available. During his State of the Union address, President Obama announced a National Wireless Initiative to make available high speed wireless Internet services for 98% of Americans. This plan was primarily designed to get this technology to reach more rural areas that otherwise did not have the opportunity of obtaining the service. On February 10, 2011 President Obama was commended for his proposal on pursuing the plan with the idea that this would greatly increase jobs and innovation. The concept of the \"last mile\" is often brought up for Internet Service Providers (ISP) as they try to expand their network, often time having to stop before the last house on the block because of cost. However, even though this issue happens throughout rural areas, 57% of Americans use broadband services with 91% already having access according to the Pew Internet & American Life Project. Those that have Internet can access an incredible amount of information at any time as long as they have an Internet-ready device. There has also been an increase in the applications that utilize Internet services. The proliferation of wireless applications is on the rise and continues to empower users and communities.\n\nA national wireless broadband network is not only about providing Internet access for personal computers in the home, but for anyone with a wireless Internet-ready device. In 2006, the number of households passed over for high-speed Internet was 119 million, and over the past two years, the cable industry has invested $23 billion into their networks. As the number of homes serviced declines, broadband technology is able to develop. Commissioner Robert M. McDowell commented that the broadband technology has been the fastest in penetration of any technology in modern history. With the broadband technology, the number of devices that are Internet-ready has been increasing year after year. Both cell phone and laptops with wireless capabilities have increased Internet usage dramatically and have each grown more prevalent since 2009. It is no longer working professionals with high Internet use, but young adults as well. About 47% of adults go online with a laptop, up from 39% as of April 2009. Also, 40% of adults use a mobile phone, up from 32% in 2009. The world has seen great technologies, and the Internet is one of the fastest growing because of the number of devices designed to utilize it. According to Pew Research, 59% of adults access the Internet wirelessly through some type of wireless device. Again, this is an increase from the 51% in April 2009. Internet access has become a critical part of our lives. The deployment and development of wireless broadband as well as other technologies is critical to ensuring this reliable and ubiquitous service is available to Americans.\n\nSome have not only considered personal or business related uses of national wireless, but the health related uses for hospitals and their patients. As Blair Levin from the Technology, Innovation and Government Reform Team of President Obama states, this will create a world-class broadband platform allowing modernizing of health care records and reforming education. Being able to have the instant availability to health records for doctors and patients, and being able to teach and be taught from anywhere in the United States is a concept some may never have considered, or thought possible. As John Horrigan from the Pew Research Center stated, people not understanding the technology being of relevancy to them is a bigger barrier than the cost of the technology itself. Michael Powell stated Americans benefit most when policies enable consumers and businesses to fully utilize the benefits of emerging technology.\n\nPublic safety has been mentioned in terms of the spectrum use and cost. Public safety is also important in regards to public interest. With the implementation of national wireless broadband, this initiative helps improve public safety communications. The Commission for 9/11 has noted that our homeland security is vulnerable due to lack of interoperable wireless communication among first responders. The plan would allow all public safety officials to be on the same network, and get the correct information quicker and safer. With 4G networking, they can be provided with a unique opportunity to deploy a system in conjunction with the commercial infrastructure already available.\n\nNetwork neutrality is becoming a big issue with the policies involved in the national wireless plan. If the plan comes together, the question of restricting access to the Internet is reason for concern. Network neutrality is the concept of having no rules and regulations for what consumers are able to access through the Internet by their ISPs. As stated by the CTIA, our economic conditions make it hard to understand why people want to impose network neutrality rules, and inject uncertainty in an industry that seems to be working well for the U.S. This is for both wired and wireless broadband networks. These types of infrastructures cannot be managed for customers and expectations with a one-size-fits-all approach. The debate is due to the types of restrictions ISPs should be allowed to have if consumers are paying for the service they want. As with wired Internet access, the CTIA has stated that they strongly believe that regulation is not necessary and may do more harm than good.\n\nMore users are obtaining access to the internet and have the wireless devices to access it. It is no surprise that wireless is the fastest growing broadband service. There is also an increase in the number of users that rely solely on wireless instead of wired connections. Wireless service providers are constantly competing to create the best network with the best service and quality. With more towers and increased advanced technologies, wireless has become a convenient and widely accessible mode of communication. The negative, however, is the technology which wireless Internet uses. As previously stated, the spectrum itself is limited and wireless data networks rely on the finite source.\n\nSince the FCC has developed the plan to open the spectrum for the wireless network, the issue of network neutrality is cause for concern for some. The CTIA stated that the imposition of network neutrality will inject uncertainty in the market. Since this concept supports users having access to the information they want through the Internet, it raises the problem of consumers having limited options. This could ultimately harm consumers and hamper innovation.\n\nPros:\n\nCons:\n\n"}
{"id": "1095160", "url": "https://en.wikipedia.org/wiki?curid=1095160", "title": "Rope splicing", "text": "Rope splicing\n\nRope splicing in ropework is the forming of a semi-permanent joint between two ropes or two parts of the same rope by partly untwisting and then interweaving their strands. Splices can be used to form a stopper at the end of a line, to form a loop or an eye in a rope, or for joining two ropes together. Splices are preferred to knotted rope, since while a knot typically reduces the strength by 20–40%, a splice is capable of attaining a rope's full strength. However, splicing usually results in a thickening of the line and, if subsequently removed, leaves a distortion of the rope. Most types of splices are used on 3-strand rope, but some can be done on 12-strand or greater single-braided rope, as well as most double braids.\nWhile a spliced 3-strand rope's strands are interwoven to create the splice, a braided rope's splice is constructed by simply pulling the rope into its jacket. \n\n\n\n\n\nSplices are often tapered to make the thicker splice blend into the rest of the line. There are two main types of tapering, the standard and the \"West Coast Taper\". \n\nA fid is a hand tool made from wood, plastic, or bone and is used in the process of working with rope. A variety of fid diameters are available depending on the size of rope being used. Styles of fid designs include:\n\nA Marlinspike is a tool, usually made of steel and often part of a sailor's pocketknife, which is used to separate strands of rope from one another. They can range in size anywhere from 3 inches to 5 feet long, with a round or flattened point.\n\nA pulling fid is often used for smaller diameters of braided ropes. Also a Softfid is a great tool when dealing with tightly braided ropes.\n\n\n"}
{"id": "37713528", "url": "https://en.wikipedia.org/wiki?curid=37713528", "title": "SIE Neftehim", "text": "SIE Neftehim\n\nSIE Neftehim () is a Russian joint stock company based in Krasnodar, undertaking research and development in the petrochemical industry. Its origins lie in the research laboratory attached to Krasnodar Refinery, which first developed the use of aluminium-platinum catalysts in the Soviet Union. Established in its present form in 1996, the company continues to develop catalytic techniques, isomerization processes, and other petroleum refining technologies.\n\nCreation and establishing of the scientific and research enterprise on development and introduction of petrochemical processes in Krasnodar is closely related to solving the issue of introduction of catalytic reforming process for gasoline fractions in Russian oil refining industry. The main step was construction of catalytic reforming semi-production unit, which allowed testing this process and submitting the necessary data for designing and construction of large scale commercial units. Besides, all new reforming catalysts were tested at the unit beginning with AP-56 (АП-56) and out to KR (КР) catalysts. Since the 1950s, the enterprise has been repeatedly reorganized and renamed:\n\n\nThe modern image, as well as research and activity trends of the enterprise started to form after its corporatization in 1995-1996. The enterprise underwent some structural changes, inefficient researches were reduced and the main efforts were focused on the major trends, i.e. reforming and isomerization of gasoline fractions. The company searches for and develops technologies and catalysts for Russian and foreign refineries. The enterprise is operated in accordance with the ISO 9001:2008 quality management system in the sphere of development and introduction of petrochemical processes.\n\nJSC SIE Neftehim is directed by its major shareholders – Alexander N. Shakun (General Director) and Marina L. Fedorova (Technical Director).\n\n\n\nAt the present time activity of the enterprise is focused on development of Design Basis or Basic Engineering Design for detailed engineering of isomerization (technology Isomalk-2, isomerization n-butane Isomalk-3, isomerization n-heptane Isomalk-4) and reforming units, as well as on isomerization and reforming catalysts supply, monitoring of isomerization and reforming units operation, analytical research of hydrocarbon fractions and catalysts, creation of new catalysts.\n\n\n"}
{"id": "8950098", "url": "https://en.wikipedia.org/wiki?curid=8950098", "title": "Soluforce", "text": "Soluforce\n\nSoluforce is a brandname of Pipelife for a Reinforced Thermoplastic Pipe (RTP).\n\nSoluforce is an RTP, which is a flexible pipe, supplied in long length coils up to 400m length and pressure ratings from 30 to 90 bar. In the last few years, this type of pipe has been acknowledged as the standardised solution for oilfield flowlines and also recently used offshore for water injection risers and oil flowlines.\n\nA very great advantage of this pipe is also its very fast installation time compared to steel pipe as speeds up to 1000 m/h have been reached installing RTP in ground surface. \nThe pipe mainly benefits applications where steel fails due to corrosion and installation time is an issue. Beside the application for oil transportation, this pipe is also applied for gas transport lines.\n\nThe Soluforce RTP is a three layer pipe construction:\n\nSoluforce is used for the following applications:\n\nSolufoce RTP is tested and acknowledged by the following instances:\n\n\n\n\n"}
{"id": "1629562", "url": "https://en.wikipedia.org/wiki?curid=1629562", "title": "Suicide booth", "text": "Suicide booth\n\nA suicide booth is a fictional machine for committing suicide. Suicide booths appear in numerous fictional settings, including the American animated series \"Futurama\" and the manga \"Gunnm/Battle Angel Alita\". Compulsory self-execution booths were also featured in an episode of the original \"Star Trek\" TV series entitled \"A Taste of Armageddon\".\n\nThe concept can be found as early as 1893. When a series of suicides were vigorously discussed in United Kingdom newspapers, critic William Archer suggested that in the golden age there would be penny-in-the-slot machines by which a man could kill himself.\n\nModern writer Martin Amis provoked a small controversy in January 2010 when he facetiously advocated \"suicide booths\" for the elderly, of whom he wrote:\nFollowing Archer's statement in 1893, the 1895 story \"The Repairer of Reputations\" by Robert W. Chambers featured the Governor of New York presiding over the opening of the first \"Government Lethal Chamber\" in the then-future year of 1920, after the repeal of laws against suicide:\nHowever, as Chambers's protagonist who relates the story is suffering from brain damage, it remains ambiguous whether or not he is an unreliable narrator.\n\nIn Robert Sheckley's \"Immortality, Inc.\" (1959), the protagonist wakes up in an unfamiliar future and, while wandering dazed in a starkly changed New York, finds himself in what he thinks might be a bread line, but turns out to be a line for the suicide booths. In the movie \"Freejack\" (loosely based on \"Immortality, Inc.\"), suicide booths are not shown, but advertisements for suicide-assistance services are visible against the city skyline.\n\nIn Ivan Efremov's 1968 novel \"The Bull's Hour\", suicide booths are referred to as the \"palaces of tender death\" (). They're commonly used on the planet Tormance to control population growth.\n\nKurt Vonnegut's \"purple-roofed Ethical Suicidal Parlors\" appear in two stories: \"Welcome to the Monkey House\" and \"God Bless You, Mr. Rosewater\". In these Ethical Suicide Parlors, a patron receives a free meal in the adjoining Howard Johnson's diner before committing suicide. It is considered a citizen's patriotic duty to commit suicide, again as a means of population control.\n\nIn John Christopher's novel \"The City of Gold and Lead\", human slaves in the aliens' domed cities voluntarily use the \"Place of Happy Release\" when they are no longer able to serve. The slave is killed instantly and then cremated.\n\nWhile not a booth, suicide chambers are used to allow people to choose a pleasant form of euthanasia in the movie \"Soylent Green\". The character Sol Roth (Edward G. Robinson) leaves a note saying that he is \"going home,\" a euphemism for committing state-approved suicide via a large, well-appointed, attended suicide chamber. Music and a video chosen by the client are played while he or she waits for the drugs to take their fatal effect. Roth chooses Ludwig van Beethoven's Pastoral Symphony and a video of Earth's natural wonders and scenes of pastoral beauty.\n\nIn Arthur C. Clarke's \"The City and the Stars\", set well over a billion years in the future, in the city Diaspar, human beings resort to suicide when they are tired of life, but with the provision of being re-created at some future date. The computers that store memories of suicided humans decide when and whom to resurrect. Sometimes they create a person who has never existed before.\n\nIn the world of \"Futurama\", Stop-and-Drop suicide booths resemble phone booths and cost one quarter per use. The booths have at least three modes of death: \"quick and painless\", \"slow and horrible\", and \"clumsy bludgeoning\" though, it is also implied that \"electrocution, with a side order of poison\" exists, and that the eyes can be scooped out for an extra charge. After a mode of death is selected and executed, the machine cheerfully says, \"You are now dead. Thank you for using Stop-and-Drop, America's favorite suicide booth since 2008\", or in \"\", \"You are now dead, please take your receipt\", and at this time many untaken receipts are shown.\n\nThe first appearance of a suicide booth in \"Futurama\" is in \"Space Pilot 3000\", in which the character Bender wants to use it. Fry at first mistakes the suicide booth for a phone booth, and Bender offers to share it with him. Fry requests a collect call, which the machine interprets as a \"slow and horrible\" death. It then turns out that \"slow and horrible\" can be survived by pressing oneself against the side of the booth, leading Bender to accuse the machine of being a rip-off. In \"\", after failing to initially chase down Fry in the year 2000, Bender wants to kill himself, but mistakes a regular phone booth for a suicide booth. A suicide booth reappeared in \"\" where Bender once again attempts to end his life, but is saved when dropped into the League of Robots' lair. During the season 6 episode \"Ghost in the Machines\", Bender commits suicide in a booth named Lynn that is still angry at him over the end of their relationship six months earlier; his ghost eventually makes its way back to his body so he can continue living.\n\nAccording to series co-creator Matt Groening, the suicide booth concept was inspired by a 1937 Donald Duck cartoon, \"Modern Inventions\", in which Donald Duck visits a Museum of the Future and is nearly killed by various push button gadgets. The suicide booth was closely enough associated with Bender's character that in 2001 it was featured as the display stand for the Bender action figure. It was also one of the many features of the series which troubled the executives at Fox when Groening and David X. Cohen first pitched the series.\n\nIn the \"\" episode \"A Taste of Armageddon\", people who were deemed war casualties by the government of Eminiar VII were required to enter suicide booths. Treaty arrangements require that everyone who is calculated as \"dead\" in the hypothetical thermonuclear war simulated using computers actually die, without actually damaging any infrastructure. In the end, the computers are destroyed, the war can no longer be calculated in this way, the treaty breaks down, and faced with a real threat, (presumably) peace begins.\n\nAfter the Heaven's Gate mass suicide event was linked by tabloids to an extreme fascination with science fiction and \"Star Trek\" in particular it was noted that multiple episodes, including \"A Taste of Armageddon\", actually advocated an anti-suicide standpoint as opposed to the viewpoint expressed by the Heaven's Gate group.\n\nIn the seventeenth season \"Simpsons\" episode \"Million Dollar Abie\", a suicide machine called a \"diePod\" (a pun on the iPod) is featured. The diePod allows the patient to choose visual and auditory themes that present themselves as the patient is killed. It also shows three different modes, namely, \"Quick Painless Death\", \"Slow and Painful Death\", and \"Megadeath\" (a pun on a band of a similarly spelled name). It was a reference to the suicide building in \"Soylent Green\". Being a direct parody of the aforementioned scene, Abraham Simpson receives the opportunity to select his final vision and musical accompaniment: 1960s-era footage of \"cops beatin' up hippies\" to the tune of \"Pennsylvania 6-5000\" by the Glenn Miller Orchestra.\n\nIn the \"Battle Angel Alita\" series, the suicide booth is located in Tiphares and is called 'EndJoy'. The Endjoy was a public suicide booth located in the Dome Park of Tiphares. As the Endjoy was entered it played soothing music and a message stating \"Welcome to Endjoy, now just relax and step into the inner hatch\". After Alita destroyed it, she pulled out a giant grinder from down below the structure. According to Dr. Russell it is every Tipharian's right to end their own life if they wish. Using the Endjoy is considered a privilege and the invention of a superior race. Presumably constructed by the Medical Inspection Bureau (M.I.B.), Alita noticed people going into it, but not coming out after she was resurrected on Tiphares by Desty Nova. She entered the Endjoy to investigate and tore it to shreds, ripping out the grinder and exposing a current of water that she used to wash herself of the dead Tiphareans' blood. Russell was shocked at Alita's actions, but was forced to reveal what had happened to Lou Collins. Despite Alita's actions, she was not targeted by the M.I.B.\n\nIn the movie \"Logan's Run\", set in 2274 CE, the remnants of human civilization live in a sealed domed city, a pseudo-utopia run by a computer that manages all aspects of their lives, including reproduction. The citizens live a mostly hedonistic lifestyle, but have been told that, in order to maintain the city, every resident must undergo the ritual of \"Carrousel\" at the age of 30, where they are vaporized with the promise of being \"Renewed\".\n\nThe closest thing to a suicide booth to have been actually constructed is the \"Euthanasia Machine\" invented by Philip Nitschke, consisting of software titled \"Deliverance\", which asks the patient a series of questions, and automatically administers a lethal injection if the correct answers are made. The system and questions are so constructed that the supplier of the machine cannot be held responsible for ending the life of the patient, who takes responsibility by operating it.\n\nThe machine was legalized for a short time as it was solely constructed for those suffering from various diseases to end their life.\n\n\n<br>\n"}
{"id": "19387740", "url": "https://en.wikipedia.org/wiki?curid=19387740", "title": "Sulfate crust", "text": "Sulfate crust\n\nSulfate crust is a zone observed in the axial (central) parts of burning coal dumps and related sites. It is a zone built mainly by anhydrous sulfate minerals, such as godovikovite and millosevichite. The outer zone can easily be hydrated giving rise to minerals like tschermigite and alunogen. The zone forms due to interaction with hot (even around 600 °C) coal-derived gases (mainly NH and SO) with the \"sterile\" material (i.e. shales and other rocks serving as the source of Al, Fe, Ca and other cations) in case of the lack of vents for the gases to escape into the atmosphere. \n\n"}
{"id": "32564255", "url": "https://en.wikipedia.org/wiki?curid=32564255", "title": "Sustainability and systemic change resistance", "text": "Sustainability and systemic change resistance\n\nThe environmental sustainability problem has proven difficult to solve. The modern environmental movement has attempted to solve the problem in a large variety of ways. But little progress has been made, as shown by severe ecological footprint overshoot and lack of sufficient progress on the climate change problem. Something within the human system is preventing change to a sustainable mode of behavior. That system trait is systemic change resistance. Change resistance is also known as organizational resistance, barriers to change, or policy resistance.\n\nWhile environmentalism had long been a minor force in political change, the movement strengthened significantly in the 1970s with the first Earth Day in 1970, in which over 20 million people participated, with publication of \"The Limits to Growth\" in 1972, and with the first United Nations Conference on the Human Environment in Stockholm in 1972. Early expectations the problem could be solved ran high. 114 out of 132 members of the United Nations attended the Stockholm conference. The conference was widely seen at the time as a harbinger of success:\n\nHowever, despite the work of a worldwide environmental movement, many national environmental protection agencies, creation of the United Nations Environment Programme, and many international environmental treaties, the sustainability problem continues to grow worse. The latest ecological footprint data shows the world’s footprint increased from about 50% \"undershoot\" in 1961 to 50% \"overshoot\" in 2007, the last year data is available.\n\nIn 1972 the first edition of \"The Limits to Growth\" analyzed the environmental sustainability problem using a system dynamics model. The widely influential book predicted that:\n\nYet thirty-two years later in 2004 the third edition reported that:\n\nChange resistance runs so high that the world’s top two greenhouse gas emitters, China and the United States, have never adopted the Kyoto Protocol treaty. In the US resistance was so strong that in 1999 the US Senate voted 95 to zero against the treaty by passing the Byrd–Hagel Resolution, despite the fact Al Gore was vice-president at the time. Not a single senator could be persuaded to support the treaty, which has not been brought back to the floor since.\n\nDue to prolonged change resistance, the climate change problem has escalated to the climate change crisis. Greenhouse gas emissions are rising much faster than IPCC models expected: “The growth rate of [fossil fuel] emissions was 3.5% per year for 2000-2007, an almost four fold increase from 0.9% per year in 1990-1999. … This makes current trends in emissions higher than the worst case IPCC-SRES scenario.” \n\nThe Copenhagen Climate Summit of December 2009 ended in failure. No agreement on binding targets was reached. The Cancun Climate Summit in December 2010 did not break the deadlock. The best it could do was another non-binding agreement:\n\nThis indicates no progress at all since 1992, when the United Nations Framework Convention on Climate Change was created at the Earth Summit in Rio de Janeiro. The 2010 Cancun agreement was the functional equivalent of what the 1992 agreement said:\n\nNegotiations have bogged down so pervasively that: “Climate policy is gridlocked, and there’s virtually no chance of a breakthrough.” “Climate policy, as it has been understood and practised by many governments of the world under the Kyoto Protocol approach, has failed to produce any discernible real world reductions in emissions of greenhouse gases in fifteen years.” \n\nThese events suggest that change resistance to solving the sustainability problem is so high the problem is currently unsolvable.\n\nUnderstanding change resistance requires seeing it as a distinct and separate part of the sustainability problem. Tanya Markvart’s 2009 thesis on \"Understanding Institutional Change and Resistance to Change Towards Sustainability\" stated that:\n\nThe thesis focuses specifically on developing “an interdisciplinary theoretical framework for understanding institutional change and resistance to change towards sustainability.”\n\nJack Harich's 2010 paper on \"Change Resistance as the Crux of the Environmental Sustainability Problem\" argues there are two separate problems to solve. A root cause analysis and a system dynamics model were used to explain how:\n\nThe paper discussed the two subproblems:\n\nThe proper coupling subproblem is what most people consider as “the” problem to solve. It is called decoupling in economic and environmental fields, where the term refers to economic growth without additional environmental degradation. Solving the proper coupling problem is the goal of environmentalism and in particular ecological economics: “Ecological economics is the study of the interactions and co-evolution in time and space of human economies and the ecosystems in which human economies are embedded.” \n\nChange resistance is also called barriers to change. Hoffman and Bazerman, in a chapter on “Understanding and overcoming the organizational and psychological barriers to action,” concluded that:\n\nJohn Sterman, current leader of the system dynamics school of thought, came to the same conclusion:\n\nThese findings indicate there are at least two subproblems to be solved: change resistance and proper coupling. Given the human system’s long history of unsuccessful attempts to self-correct to a sustainable mode, it appears that high change resistance is preventing proper coupling. This may be expressed as an emerging principle: systemic change resistance is the crux of the sustainability problem and must be solved first, before the human system can be properly coupled to the greater system it lives within, the environment.\n\nSystemic change resistance differs significantly from individual change resistance. “Systemic means originating from the system in such a manner as to affect the behavior of most or all social agents of certain types, as opposed to originating from individual agents.” Individual change resistance originates from individual people and organizations. How the two differ may be seen in this passage:\n\nIf sources of systemic change resistance are present, they are the principal cause of individual change resistance. According to the fundamental attribution error it is crucial to address systemic change resistance when present and avoid assuming that change resistance can be overcome by bargaining, reasoning, inspirational appeals, and so on. This is because:\n\nPeter Senge, a thought leader of systems thinking for the business world, describes the structural source of systemic change resistance as being due to an “implicit system goal:” \n\nSenge’s insight applies to the sustainability problem. Until the “implicit system goal” causing systemic change resistance is found and resolved, change efforts to solve the proper coupling part of the sustainability problem may be, as Senge argues, “doomed to failure.”\n\nPresently environmentalism is focused on solving the proper coupling subproblem. For example, the following are all proper coupling solutions. They attempt to solve the direct cause of the sustainability problem’s symptoms:\n\nThe direct cause of environmental impact is the three factors on the right side of the I=PAT equation where Impact equals Population times Affluence (consumption per person) times Technology (environmental impact per unit of consumption). It is these three factors that solutions like those listed above seek to reduce.\n\nThe top environmental organization in the world, the United Nations Environmental Programme (UNEP), focuses exclusively on proper coupling solutions:\n\nThe six areas are all direct practices to reduce the three factors of the I=PAT equation.\nAl Gores' 2006 documentary film \"An Inconvenient Truth\" described the climate change problem and the urgency of solving it. The film concluded with Gore saying:\n\nThe four solutions Gore mentions are proper coupling practices. There is, however, a hint of acknowledgement that overcoming systemic change resistance is the real challenge, when Gore says “...we just have to have the determination to make it happen. We have everything that we need to reduce carbon emissions, everything but political will.”\n\nThe twenty-seven solutions that appear during the film’s closing credits are mostly proper coupling solutions. The first nine are:\n\n\nSome solutions are attempts to overcome individual change resistance, such as:\n\n\nHowever none of the twenty-seven solutions deal with overcoming systemic change resistance.\n\nEfforts here are sparse because environmentalism is currently not oriented toward treating systemic change resistance as a distinct and separate problem to solve.\n\nOn how to specifically overcome the change resistance subproblem, Markvart examined two leading theories that seemed to offer insight into change resistance, Panarchy theory and New Institutionalism, and concluded that:\n\nTaking a root cause analysis and system dynamics modeling approach, Harich carefully defined the three characteristics of a root cause and then found a main systemic root cause for both the change resistance and proper coupling subproblems. Several sample solution elements for resolving the root causes were suggested. The point was made that the exact solution policies chosen do not matter nearly as much as finding the correct systemic root causes. Once these are found, how to resolve them is relatively obvious because once a root cause is found by structural modeling, the high leverage point for resolving it follows easily. Solutions may then push on specific structural points in the social system, which due to careful modeling will have fairly predictable effects.\n\nThis reaffirms the work of Donella Meadows, as expressed in her classic essay on Leverage Points: Places to Intervene in a System. The final page stated that:\n\nHere Meadows refers to the leverage point for resolving the proper coupling subproblem rather than the leverage point for overcoming change resistance. This is because the current focus of environmentalism is on proper coupling.\n\nHowever, if the leverage points associated with the root causes of change resistance exist and can be found, the system will not resist changing them. This is an important principle of social system behavior.\n\nFor example, Harich found the main root cause of successful systemic change resistance to be high \"deception effectiveness.\" The source was special interests, particularly large for-profit corporations. The high leverage point was raising \"general ability to detect manipulative deception.\" This can be done with a variety of solution elements, such as \"The Truth Test.\" This effectively increases truth literacy, just as conventional education raises reading and writing literacy. Few citizens resist literacy education because its benefits have become so obvious.\n\nPromotion of corporate social responsibility (CSR) has been used to try to overcome change resistance to solving social problems, including environmental sustainability. This solution strategy has not worked well because it is voluntary and does not resolve root causes. Milton Friedman explained why CSR fails: \"The social responsibility of business is to increase profits.\" Business cannot be responsible to society. It can only be responsible to its shareholders.\n\n"}
{"id": "49716057", "url": "https://en.wikipedia.org/wiki?curid=49716057", "title": "Système d'aide à la conduite, à l'exploitation et à la maintenance", "text": "Système d'aide à la conduite, à l'exploitation et à la maintenance\n\nThe Système d'aide à la conduite, à l'exploitation et à la maintenance (SACEM) is an embedded, automatic speed train protection system for rapid transit railways. The name means \"Driver Assistance, Operation, and Maintenance System\".\n\nIt was developed in France by GEC-Alsthom and Siemens in the 1980s. It was first deployed on the RER A suburban railway in Paris in 1989.\n\nAfterwards it was installed:\nIn 2017 the SACEM system in Paris was enhanced with Automatic Train Operation (ATO) and will be put in full operation until end of 2018.\n\nThe SACEM system in Paris is to be enhanced to a fully fledged CBTC system named NExTEO. First to be deployed on the new prolongated line RER E in 2024, it is proposed to replace signalling and control on all RER lines.\n\nThe SACEM system enable a train to receive signals from devices under the tracks. A receiver in the train cabin interprets the signal, and sends data to the console so the driver can see it. A light on the console indicates the speed control setting: an orange light means \"Jog Mode\", or ; a red light means full stop. If the driver alters the speed, a warning buzzer may sound. If the system determines that the speed might be unsafe, and the driver does not change it within a few seconds, SACEM engages the emergency brake. SACEM also allows for a reduction in potential train bunching and easier recovery from delays, therefore safely increasing operating frequencies as much as possible especially during rush hour.\n\n"}
{"id": "23173874", "url": "https://en.wikipedia.org/wiki?curid=23173874", "title": "Vienna rectifier", "text": "Vienna rectifier\n\nThe Vienna Rectifier is a pulse-width modulation rectifier, invented in 1993 by Johann W. Kolar.\n\nVienna Rectifier provides following features:\n\nThe Vienna Rectifier is a unidirectional three-phase three-switch three-level Pulse-width modulation (PWM) rectifier. It can be seen as a three-phase diode bridge with an integrated boost converter.\n\nThe Vienna Rectifier is useful wherever six-switch converters are used for achieving sinusoidal mains current and controlled output voltage, when no energy feedback from the load into the mains is available. In practice, use of the Vienna Rectifier is advantageous when space is at a sufficient premium to justify the additional hardware cost. These include:\nFigure 2 shows the top and bottom views of an air-cooled 10 kW-Vienna Rectifier (400 kHz PWM), with sinusoidal input current s and controlled output voltage. Dimensions are 250mm x 120mm x 40mm, resulting in a power density of 8.5 kW/dm. The total weight of the converter is 2.1 kg \n\nFigure 3 shows the system behavior, calculated using the power-electronics circuit simulator. Between the output voltage midpoint (0) and the mains midpoint (M) the common mode voltage u0M appears, as is characteristic in three-phase converter systems.\n\nIt is possible to separately control the input current shape in each branch of the diode bridge by inserting a bidirectional switch into the node, as shown in Figure 3. The switch Ta controls the current by controlling the magnetization of the inductor. Switched on charges the inductor which drives the current through the bidirectional switch. Deactivating the switch increases causes the current to bypass the switch and flow through the freewheeling diodes Da+ and Da-. This results in a negative voltage across the inductor and drains it. This demonstrates the ability of the topology to control the current in phase with the mains voltage (Power factor correction capability).\n\nTo generate a sinusoidal power input which is in phase with the voltage\nformula_1\nthe average voltage space vector over a pulse-period must satisfy:\nformula_2\nFor high switching frequencies or low inductivities we require (formula_3) formula_4.\nThe available voltage space vectors required for the input voltage are defined by the switching states(sa,sb,sc) and the direction of the phase currents. For example, for formula_5, i.e. for the phase-rangeformula_6 of the period(formula_7) the phase of the input current space vector is formula_8). Fig. 4 shows the conduction states of the system, and from this we get the input space vectors shows in Fig. 5 \n"}
{"id": "2368120", "url": "https://en.wikipedia.org/wiki?curid=2368120", "title": "Virtual university", "text": "Virtual university\n\nA virtual university provides higher education programs through electronic media, typically the Internet. Some are bricks-and-mortar institutions that provide online learning as part of their extended university courses while others solely offer online courses. They are regarded as a form of distance education. The goal of virtual universities is to provide access to the part of the population who would not be able to attend a physical campus, for reasons such as distance—in which students live too far from a physical campus to attend regular classes; and the need for flexibility—some students need the flexibility to study at home whenever it is convenient for them to do so.\n\nSome of these organizations exist only as loosely tied combines of universities, institutes or departments that together provide a number of courses over the Internet, television or other media, that are separate and distinct from programs offered by the single institution outside of the combine. Others are individual organizations with a legal framework, yet are called \"virtual\" because they appear only on the Internet, without a physical location aside from their administration units. Still other virtual universities can be organized through specific or multiple physical locations, with or without actual campuses to receive program delivery through technological media that is broadcast from another location where professors give televised lectures.\n\nProgram delivery in a virtual university is administered through Information and communications technology such as web pages, e-mail and other networked sources.\n\nAs virtual universities are relatively new and vary widely, questions remain about accreditation and the quality of assessment.\n\nThe defining characteristic of all forms and generations of distance education is the separation of student and teacher in time and space. Distance education can be seen as the precursor to online learning. Before the advent of virtual universities, many higher education institutions offered some distance education through print-based correspondence courses. These courses were often referred to as a \"course in a box\". These have been developed so that students can obtain almost immediate feedback from professors and online tutors through e-mails or online discussions.\n\nWhen the term \"virtual\" was first coined in the computational sense, it applied to things that were simulated by the computer, like virtual memory. Over time, the adjective has been applied to things that physically exist and are created or carried on by means of computers.\n\nThe Open University in the United Kingdom was the world’s first successful distance teaching university. It was founded in the 1960s on the belief that communications technology could bring high quality degree-level learning to people who had not had the opportunity to attend campus universities. The idea for a \"wireless university\" was first discussed at the BBC (British Broadcasting Corporation) by the educationalist and historian J.C. Stobbart. From these early beginnings more ideas came forth until finally the Labour Party under the leadership of Harold Wilson formed an advisory committee to establish an Open University.\n\nWith the goal of bringing higher education to all those who wanted to access it, the committee came up with various scenarios before settling on the name \"Open University\". The first idea floated in the UK was to have a \"teleuniversity\" which would combine broadcast lectures with correspondence texts and visits to conventional universities. In the \"teleuniversity\" scenario courses are taught on the radio and television and in fact many universities adopted the use of this technology for their distance education courses. The name \"teleuniversity\" morphed into the \"University of Air\" which still had the same goal of reaching the lower income groups who did not have access to higher education. The name \"University of Air\" did not stick and by the time the first students were admitted in January 1971 the name had become what it is today \"Open University\". OU proved that it was possible to teach university-level courses to students at a distance.\n\nBy 1980, total student numbers at OU had reached 70,000 and some 6,000 people were graduating each year. The 1980s saw increased expansion continue as more courses and subject areas were introduced; as the importance of career development grew, so the university began to offer professional training courses alongside its academic programmes. By the mid-nineties the OU was using the internet. As of 2008, more than 180,000 students were interacting with OU online from home.\n\nThe idea of a virtual university as an institution that used computers and telecommunications instead of buildings and transport to bring students and teachers together for university courses was first published in works like \"De-Schooling Society\" by Ivan Illich that introduced the concept of the use of computer networks as switchboards for learning, in 1970. In 1971 George Kasey, a media(activist)ethicist, delivered a series of lectures on \"the Philosophy of Communications De-Design\" under the sponsorship of Phil Jacklin PhD, professor at University of California San Jose, a member of \"The (San Francisco)Bay Area Committee for Open Media and Public Access.\" The lectures contained the theoretical outlines for use of telecommunications and media for de-schooling and de-design of mainstream education and an alternative Virtual Free University system. By 1972 George Kasey established \"Media Free Times - periodical Multimedia Random Sampling of Anarchic Communications Art\" a prototype for remote learning with the use of \"multi-media periodicals,\" that are now commonly referred to as \"web pages\". In 1995 by John Tiffin and Lalita Rajasingham in their book \"In Search Of the Virtual Class: Education in an Information Society\" (London and New York, Routledge). It was based on a joint research project at Victoria University of Wellington that ran from 1986-1996. Called the virtual class laboratory it used dedicated telecommunication systems to make it possible for students to attend class virtually or physically and was at first supported by a number of telecommunication organisations. Its purpose was to seek the critical factors in using ICT for university level education. In 1992 the virtual class lab moved onto the Internet.\n\nA number of other universities were involved in the late eighties in pioneering initiatives and experiments were conducted between Victoria University in New Zealand, the University of Hawaii, Ohio State University and Waseda University to try and conduct classes and courses at an international level via telecommunications. This led to the concept of a Global Virtual University.\n\nProviding access to higher education for all students, especially adult learners, is made easier by the fact that most virtual universities have no entry requirements for their undergraduate courses. Entry requirements are needed for the courses that are aimed at postgraduates or those who work in specific jobs.\n\nStudying in a virtual university has essential differences from studying in a brick and mortar university. There are no buildings and no campus to go to because students receive learning materials over the Internet. In most cases, only a personal computer and an Internet connection is needed—even for learning laboratory experiments and technical materials, such as robotics, that traditionally required physical presence of students in the classroom. Course materials can include printed material, books, audio and video cassettes, TV programmes, CD-ROM/software, and web sites. Support is offered to learners from the professor or a tutor online through e-mails if they are having problems with the course.\n\nTaking courses on-line means that students will be learning in their own time by reading course material, working on course activities, writing assignments and perhaps working with other students through interactive teleconferences. Online learning can be an isolating experience since the student spends the majority of their time working by themselves. Some learners do not mind this kind of solo learning, but others find it a major stumbling block to successful completion of courses. Because of the potential difficulty of maintaining the schedule needed to be successful when learning online, some virtual universities apply the same type of time management as traditional schools. Many courses operate to a timetable, which the student receives with the course materials. These may include the planned activities for each week of the course and due dates for the assignments. If the course has an exam, the students will be informed where they have to go to write it.\n\nAn example of a university that maintains a tight schedule is the Virtual Global University (VGU) in Germany. VGU offers a graduate program \"International Master of Business Informatics\" (MBI)—a master program in information technology and management that takes an average of four semesters to complete (for full-time students). Each course has a lecture or a virtual class meeting every week. Afterwards, students get a homework assignment; for example, they have to solve an exercise, elaborate on some problem, discuss a case study, or take a test. Lecturers give them immediate feedback, and one week later, the same happens again.\n\nCoursework can be same for a Virtual University as the On-campus University in certain cases. NYU Tandon Online, for example, provides the same course work to its online students as the on-campus students at the NYU Tandon School of Engineering. This is done using advanced technologies.\n\nWhen online courses first began, the primary mode of delivery was through a two way audio-visual network. Then as well as now, many of the virtual study programs were mainly based on text documents, but multimedia technologies have become increasingly popular as well. These web-based delivery modes are used in order to expand access to programs and services that can be offered anytime and anywhere. The spectrum of teaching modes in virtual education includes courses based on hypertext, videos, audios, e-mails, and video conferencing. Teaching on the web through courseware such as WebCT and Blackboard are also used. See Virtual education.\n\nStudents taking \"virtual\" courses are doing real work to get their degrees, and educators preparing and teaching those courses spend real time in doing so. That is, students meet a comparable level of academic learning outcomes and are evaluated through programs constructed according to standard university-level criteria. Though it should not be assumed, virtual universities may be accredited in the same way as traditional universities and operate according to a similar set of academic standards.\n\nHowever, questions remain about accreditation and the quality of assessment. Accreditation is required to assure students that the online institute has certified online instructors who have the expertise and educational qualifications to design and carry out the curriculum. Assessment standards need to be particularly closely monitored in virtual universities. For example, respondents in studies of opinions about online degrees will rate an online degree from Stanford the same as an on-campus degree, because the name of the granting institution is recognized.\n\nSome \"open universities\" also teach online:\n"}
{"id": "6159966", "url": "https://en.wikipedia.org/wiki?curid=6159966", "title": "Wincor Nixdorf", "text": "Wincor Nixdorf\n\nWincor Nixdorf was a German corporation that provided retail and retail banking hardware, software, and services. Wincor Nixdorf was engaged primarily in the sale, manufacture, installation and service of self-service transaction systems (such as ATMs), retail banking equipment, lottery terminals, postal terminals, software and services for global financial and commercial markets. On August 15, 2016, through the merger of Wincor Nixdorf and Diebold Inc., Diebold Nixdorf was formed as an international technology and services company.\n\nFounded by Heinz Nixdorf, Nixdorf Computer was formed in 1952. In 1990 the company was purchased by Siemens AG and renamed Siemens Nixdorf Informationssysteme. The company was re-focused exclusively on its current product set in 1998 and renamed Siemens Nixdorf Retail and Banking Systems GmbH. Following a buyout by Kohlberg Kravis Roberts and Goldman Sachs Capital Partners on October 1, 1999, the company was renamed Wincor Nixdorf. The company was taken public May 19, 2004 with a successful IPO. On November 8, 2006, Chief Executive Officer Karl-Heinz Stiller announced his resignation from the Board of Directors. Eckard Heidloff was elected as his replacement.\n\nFollowing the end of the InterBold partnership in 1997, IBM eventually entered into a re-distribution arrangement with Wincor Nixdorf. IBM sells and services Wincor Nixdorf machinery in several countries in the Americas.\n\nUnisys also has a relationship with Wincor Nixdorf in some Latin American countries such as Brazil.\n\nOn July 12, 2016, the media reported that at least 34 ATMs belonging to the Taiwanese had been hacked via a Malware heist. Over 70 million Taiwan dollar (equivalent to 2 million USD) had been withdrawn illegally. The ATMs had been manufactured by Wincor Nixdorf. According to the investigative authority, the international criminal hacker group used mobile devices instead of bank cards to activate the cash-withdrawal process. Security experts from the company were sent to investigate the security breach. At the time of the incident, there were approximately one thousand cash machines of this same type in use in Taiwan. A detailed investigation into the vulnerability began immediately.\n\nOn July 17, 2016, Taiwanese authorities declared the case to be solved. The hacker group was found to consist of 16 people from Russia, Romania, and Latvia.\n"}
