{"id": "10508666", "url": "https://en.wikipedia.org/wiki?curid=10508666", "title": "AN/ALQ-128", "text": "AN/ALQ-128\n\nThe AN/ALQ-128 Electronic Warfare Warning Set is a Electronic countermeasure receiver manufactured by Magnavox and used on the F-15C/D/E. The system, along with the Loral AN/ALR-56C radar warning receiver, is used to give the ALQ-135(V), the F-15s automatic countermeasure system, information through radar warning suites that allows it to provide active jamming against enemy radar threats.\n\n\n\n"}
{"id": "55817338", "url": "https://en.wikipedia.org/wiki?curid=55817338", "title": "Algorithmic bias", "text": "Algorithmic bias\n\nAlgorithmic bias occurs when a computer system reflects the implicit values of the humans who are involved in coding, collecting, selecting, or using data to train the algorithm. Algorithmic bias is found across platforms, including but not limited to search engine results and social media platforms, and can have impacts ranging from inadvertent privacy violations to reinforcing social biases of race, gender, sexuality, and ethnicity. The study of algorithmic bias is most concerned with algorithms that reflect \"systematic and unfair\" discrimination. This bias has only recently been addressed in legal frameworks, such as the 2018 European Union's General Data Protection Regulation.\n\nAs algorithms expand their ability to organize society, politics, institutions, and behavior, sociologists have become concerned with the ways in which unanticipated output and manipulation of data can impact the physical world. Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise, and in some cases, reliance on algorithms can displace human responsibility for their outcomes. Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design.\n\nAlgorithmic bias has been cited in cases ranging from election outcomes to the spread of online hate speech. Problems in understanding, researching, and discovering algorithmic bias stem from the proprietary nature of algorithms, which are typically treated as trade secrets. Even when full transparency is provided, the complexity of algorithms poses a barrier to understanding their functioning. Furthermore, algorithms may change, or respond to input or output in ways that cannot be anticipated or easily reproduced for analysis. In many cases, even within a single website or application, there is no single \"algorithm\" to examine, but a network of many interrelated programs and data inputs, even between users of the same service.\n\nAlgorithms are difficult to define, but may be generally understood as sets of instructions within computer programs that determine how these programs read, collect, process, and analyze data to generate some readable form of analysis or output. Newer computers can process millions of these algorithmic instructions per second, which has boosted the design and adoption of technologies such as machine learning and artificial intelligence. By analyzing and processing data, algorithms are the backbone of search engines, social media websites, recommendation engines, online retail, online advertising, and more.\n\nContemporary social scientists are concerned with algorithmic processes embedded into hardware and software applications because of their political and social impact, and question the underlying assumptions of an algorithm's neutrality. The term \"algorithmic bias\" describes systematic and repeatable errors that create unfair outcomes, such as privileging one arbitrary group of users over others. For example, a credit score algorithm may deny a loan without being unfair, if it is consistently weighing relevant financial criteria. If the algorithm recommends loans to one group of users, but denies loans to another set of nearly identical users based on unrelated criteria, and if this behavior can be repeated across multiple occurrences, an algorithm can be described as \"biased\". This bias may be intentional or unintentional.\n\nBias can be introduced to an algorithm in several ways. During the assemblage of a database, data must be collected, digitized, adapted, and entered according to human-designed cataloging criteria. Next, programmers assign priorities, or hierarchies, for how a program assesses and sorts that data. This requires human decisions about how data is categorized, and which data is included or discarded. Some algorithms collect their own data based on human-selected criteria, which can also reflect the bias of human designers. Other algorithms may reinforce stereotypes and preferences as they process and display \"relevant\" data for human users, for example, by selecting information based on previous choices of a similar user or group of users.\n\nBeyond assembling and processing data, bias can emerge as a result of design. For example, algorithms that work by sorting, that determine the allocation of resources or scrutiny (such as determining school placements), or those that classify and identify users, may inadvertently discriminate against a category when determining risk based on similar users (as in credit scores). Meanwhile, recommendation engines that work by associating users with similar users, or that make use of inferred marketing traits, might rely on inaccurate associations that reflect broad ethnic, gender, socio-economic, or racial stereotypes. Another example comes from determining criteria for what is included and excluded from results. This criteria could present unanticipated outcomes for search results, such as in flight-recommendation software that omits flights that do not follow the sponsoring airline's flight paths. Algorithms may also display an \"uncertainty bias\", offering more confident assessments when larger data sets are available. This can skew algorithmic processes toward results that more closely correspond with larger samples, which may disregard data from underrepresented populations.\n\nThe earliest computer programs were designed to mimic human reasoning and deductions, and were deemed to be functioning when they successfully and consistently reproduced that human logic. In his 1976 book Computer Power and Human Reason, Artificial Intelligence pioneer Joseph Weizenbaum suggested that bias could arise both from the data used in a program, but also from the way a program is coded.\n\nWeizenbaum wrote that programs are a sequence of rules created by humans for a computer to follow. By following those rules consistently, such programs \"embody law,\" that is, enforce a specific way to solve problems. The rules a computer follows are based on the assumptions of a computer programmer for how these problems might be solved. That means the code could incorporate the programmer's imagination of how the world works, including his or her biases and expectations. While a computer program can incorporate bias in this way, Weizenbaum also noted that any data fed to a machine additionally reflects \"human decisionmaking processes\" as data is being selected.\n\nFinally, he noted that machines might also transfer good information with unintended consequences if users are unclear about how to interpret the results. Weizenbaum warned against trusting decisions made by computer programs that a user doesn't understand, comparing such faith to a tourist who can find his way to a hotel room exclusively by turning left or right on a coin toss. Crucially, the tourist has no basis of understanding how or why he arrived at his destination, and a successful arrival does not mean the process is accurate or reliable.\n\nAn early example of algorithmic bias resulted in as many as 60 women and ethnic minorities denied entry to St. George's Hospital Medical School per year from 1982 to 1986, based on implementation of a new computer-guidance assessment system that denied entry to women and men with \"foreign-sounding names\" based on historical trends in admissions.\n\nThough well-designed algorithms frequently determine outcomes that are equally (or more) equitable than the decisions of human beings, cases of bias still occur, and are difficult to predict and analyze. The complexity of analyzing algorithmic bias has grown alongside the complexity of programs and their design. Decisions made by one designer, or team of designers, may be obscured among the many pieces of code created for a single program; over time these decisions and their collective impact on the program's output may be forgotten. In theory, these biases may create new patterns of behavior, or \"scripts,\" in relationship to specific technologies as the code interacts with other elements of society. Biases may also impact how society shapes itself around the data points that algorithms require. For example, if data shows a high number of arrests in a particular area, an algorithm may assign more police patrols to that area, which could lead to more arrests.\n\nThe decisions of algorithmic programs can be seen as more authoritative than the decisions of the human beings they are meant to assist, a process described by author Clay Shirky as \"algorithmic authority\". Shirky uses the term to describe \"the decision to regard as authoritative an unmanaged process of extracting value from diverse, untrustworthy sources,\" such as search results. This neutrality can also be misrepresented by the language used by experts and the media when results are presented to the public. For example, a list of news items selected and presented as \"trending\" or \"popular\" may be created based on significantly wider criteria than just their popularity.\n\nBecause of their convenience and authority, algorithms are theorized as a means of delegating responsibility away from humans. This can have the effect of reducing alternative options, compromises, or flexibility. Sociologist Scott Lash has critiqued algorithms as a new form of \"generative power\", in that they are a virtual means of generating actual ends. Where previously human behavior generated data to be collected and studied, powerful algorithms increasingly could shape and define human behaviors.\n\nConcerns over the impact of algorithms on society have lead to the creation of working groups in organizations such as Google and Microsoft, which have co-created a working group named Fairness, Accountability,\nand Transparency in Machine Learning. Ideas from Google have included community groups that patrol the outcomes of algorithms and vote to control or restrict outputs they deem to have negative consequences.\n\nPre-existing bias in an algorithm is a consequence of underlying social and institutional ideologies. Such ideas may influence or create personal biases within individual designers or programmers. Such prejudices can be explicit and conscious, or implicit and unconscious. Poorly selected input data will influence the outcomes created by machines. Encoding pre-existing bias into software can preserve social and institutional bias, and without correction, could be replicated in all future uses of that algorithm.\n\nAn example of this form of bias is the British Nationality Act Program, designed to automate the evaluation of new UK citizens after the 1981 British Nationality Act. The program accurately reflected the tenets of the law, which stated that \"a man is the father of only his legitimate children, whereas a woman is the mother of all her children, legitimate or not.\" In its attempt to transfer a particular logic into an algorithmic process, the BNAP inscribed the logic of the British Nationality Act into its algorithm, which would perpetuate it even if the act was eventually repealed.\n\nTechnical bias emerges through limitations of a program, computational power, its design, or other constraint on the system. Such bias can also be a restraint of design, for example, a search engine that shows three results per screen can be understood to privilege the top three results slightly more than the next three, as in an airline price display. Another case is software that relies on randomness for fair distributions of results. If the random number generation mechanism is not truly random, it can introduce bias, for example, by skewing selections toward items at the end or beginning of a list.\n\nA \"decontextualized algorithm\" uses unrelated information to sort results, for example, a flight-pricing algorithm that sorts results by alphabetical order would be biased in favor of American Airlines over United Airlines. The opposite may also apply, in which results are evaluated in contexts different from which they are collected. Data may be collected without crucial external context: for example, when facial recognition software is used by surveillance cameras, but evaluated by remote staff in another country or region, or evaluated by non-human algorithms with no awareness of what takes place beyond the camera's field of vision. This could create an incomplete understanding of a crime scene, for example, potentially mistaking bystanders for those who commit the crime.\n\nLastly, technical bias can be created by attempting to formalize decisions into concrete steps on the assumption that human behavior works in the same way. For example, software weighs data points to determine whether a defendant should accept a plea bargain, while ignoring the impact of emotion on a jury. Another unintended result of this form of bias was found in the plagiarism-detection software Turnitin, which compares student-written texts to information found online and returns a probability score that the student's work is copied. Because the software compares long strings of text, it is more likely to identify non-native speakers of English than native speakers, as the latter group might be better able to change individual words, break up strings of plagiarized text, or obscure copied passages through synonyms. Because it is easier for native speakers to evade detection as a result of the technical constraints of the software, this creates a scenario where Turnitin identifies foreign-speakers of English for plagiarism while allowing more native-speakers to evade detection.\n\nEmergent bias is the result of the use and reliance on algorithms across new or unanticipated contexts. Algorithms may not have been adjusted to consider new forms of knowledge, such as new drugs or medical breakthroughs, new laws, business models, or shifting cultural norms. This may exclude groups through technology, without providing clear outlines to understand who is responsible for their exclusion. Similarly, problems may emerge when training data (the samples \"fed\" to a machine, by which it models certain conclusions) do not align with contexts that an algorithm encounters in the real world.\n\nIn 1990, an example of emergent bias was identified in the software used to place US medical students into residencies, the National Residency Match Program (NRMP). The algorithm was designed at a time when few married couples would seek residencies together. As more women entered medical schools, more students were likely to request a residency alongside their partners. The process called for each applicant to provide a list of preferences for placement across the US, which was then sorted and assigned when a hospital and an applicant both agreed to a match. In the case of married couples where both sought residencies, the algorithm weighed the location choices of the higher-rated partner first. The result was a frequent assignment of highly preferred schools to the first partner and lower-preferred schools to the second partner, rather than sorting for compromises in placement preference.\n\nAdditional emergent biases include:\n\nUnpredictable correlations can emerge when large data sets are compared to each other. For example, data collected about web-browsing patterns may align with signals marking sensitive data (such as race or sexual orientation). By selecting according to certain behavior or browsing patterns, the end effect would be almost identical to discrimination through the use of direct race or sexual orientation data. In other cases, the algorithm draws conclusions from correlations, without being able to understand those correlations. For example, one triage program gave lower priority to asthmatics who had pneumonia than asthmatics who did not have pneumonia. The program algorithm did this because it simply compared survival rates: asthmatics with pneumonia are at the highest risk. Historically, for this same reason, hospitals typically give such asthmatics the best and most immediate care.\n\nEmergent bias can occur when an algorithm is used by unanticipated audiences. For example, machines may require that users can read, write, or understand numbers, or relate to an interface using metaphors that they do not understand. These exclusions can become compounded, as biased or exclusionary technology is more deeply integrated into society.\n\nApart from exclusion, unanticipated uses may emerge from the end user relying on the software rather than their own knowledge. In one example, an unanticipated user group lead to algorithmic bias in the UK, when the British National Act Program was created as a proof-of-concept by computer scientists and immigration lawyers to evaluate suitability for British citizenship. The designers had access to legal expertise beyond the end users in immigration offices, whose understanding of both software and immigration law would likely have been unsophisticated. The agents administering the questions relied entirely on the software, which excluded alternative pathways to citizenship, and used the software even after new case laws and legal interpretations lead the algorithm to become outdated. As a result of designing an algorithm for users assumed to be legally savvy on immigration law, the software's algorithm indirectly lead to bias in favor of applicants who fit a very narrow set of legal criteria set by the algorithm, rather than by the more broader criteria of UK immigration law.\n\nEmergent bias may also create a feedback loop, or recursion, if data collected for an algorithm results in real-world responses which are fed back into the algorithm. For example, simulations of the predictive policing software (PredPol), deployed in Oakland, California, suggested an increased police presence in black neighborhoods based on crime data reported by the public. The simulation showed that the public reported crime based on the sight of police cars, regardless of what police were doing. The simulation interpreted police car sightings in modeling its predictions of crime, and would in turn assign an even larger increase of police presence within those neighborhoods. The Human Rights Data Analysis Group, which conducted the simulation, warned that in places where racial discrimination is a factor in arrests, such feedback loops could reinforce and perpetuate racial discrimination in policing.\n\nCorporate algorithms could be skewed to invisibly favor financial arrangements or agreements between companies, without the knowledge of a user who may mistake the algorithm as being impartial. For example, American Airlines created a flight-finding algorithm in the 1980s. The software presented a range of flights from various airlines to customers, but weighed factors that boosted its own flights, regardless of price or convenience. In testimony to the United States Congress, the president of the airline stated outright that the system was created with the intention of gaining competitive advantage through preferential treatment.\n\nIn a 1998 paper describing Google, it was shown that the founders of the company adopted a policy of transparency in search results regarding paid placement, arguing that \"advertising-funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers.\" This bias would be an \"invisible\" manipulation of the user.\n\nA series of studies about undecided voters in the US and in India found that search engine results were able to shift voting outcomes by about 20%. The researchers concluded that candidates have \"no means of competing\" if an algorithm, with or without intent, boosted page listings for a rival candidate. Facebook users who saw messages related to voting were more likely to vote. A 2010 randomized trial of Facebook users showed a 20% increase (340,000 votes) among users who saw messages encouraging voting, as well as images of their friends who had voted. Legal scholar Jonathan Zittrain has warned that this could create a \"digital gerrymandering\" effect in elections, \"the selective presentation of information by an intermediary to meet its agenda, rather than to serve its users\", if intentionally manipulated.\n\nIn 2016, the professional networking site LinkedIn was discovered to recommend male variations of women's names in response to search queries. The site did not make similar recommendations in searches for male names. For example, \"Andrea\" would bring up a prompt asking if users meant \"Andrew,\" but queries for \"Andrew\" did not ask if users meant to find \"Andrea\". The company said this was the result of an analysis of users' interactions with the site.\n\nIn 2012, the department store franchise Target was cited for gathering data points to infer when women customers were pregnant, even if they had not announced it, and then sharing that information with marketing partners. Because the data had been predicted, rather than directly observed or reported, the company had no legal obligation to protect the privacy of those customers.\n\nWeb search algorithms have also been accused of bias. Google's results may prioritize pornographic content in search terms related to sexuality, for example, \"lesbian\". This bias extends to the search engine showing popular but sexualized content in neutral searches. For example, \"Top 25 Sexiest Women Athletes\" articles displayed as first-page results in searches for \"women athletes\". In 2017, Google adjusted these results along with others that surfaced hate groups, racist views, child abuse and pornography, and other upsetting and offensive content. Other examples include the display of higher-paying jobs to male applicants on job search websites.\n\nIn 2018, Amazon.com turned off a system it developed to screen job applications when they realized it was biased against women.\n\nAlgorithms have been criticized as a method for obscuring racial prejudices in decision-making. Lisa Nakamura has noted that census machines were among the first to adopt the punch-card processes that lead to contemporary computing, and that their use as categorization and sorting machines for race has been long established and socially tolerated.\n\nOne example is the use of risk assessments in criminal sentencing in the United States and parole hearings, judges were presented with an algorithmically generated score intended to reflect the risk that a prisoner will repeat a crime. For the time period starting in 1920 and ending in 1970, the nationality of a criminals's father was a consideration in those risk assessment scores. Today, these scores are shared with judges in Arizona, Colorado, Delaware, Kentucky, Louisiana, Oklahoma, Virginia, Washington, and Wisconsin. An independent investigation by ProPublica found that the scores were inaccurate 80% of the time, and disproportionately skewed to suggest blacks to be at risk of relapse, 77% more often than whites.\n\nIn 2015, Google apologized when black users complained that an image-identification algorithm in its Photos application identified them as gorillas. In 2010, Nikon cameras were criticized when image-recognition algorithms consistently asked Asian users if they were blinking. Such examples are the product of bias in biometric data sets. Biometric data is drawn from aspects of the body, including racial features either observed or inferred, which can then be transferred into data points.\n\nBiometric data about race may also be inferred, rather than observed. For example, a 2012 study showed that names commonly associated with blacks were more likely to yield search results implying arrest records, regardless of whether there is any police record of that individual's name.\n\nIn 2017 a Facebook algorithm designed to remove online hate speech was found to advantage white men over black children when assessing objectionable content, according to internal Facebook documents. The algorithm, which is a combination of computer programs and human content reviewers, was created to protect broad categories rather than specific subsets of categories. For example, posts denouncing \"Muslims\" would be blocked, while posts denouncing \"Radical Muslims\" would be allowed. An unanticipated outcome of the algorithm is to allow hate speech against black children, because they denounce the \"children\" subset of blacks, rather than \"all blacks,\" whereas \"all white men\" would trigger a block, because whites and males are not considered subsets. Facebook was also found to allow ad purchasers to target \"Jew haters\" as a category of users, which the company said was an inadvertent outcome of algorithms used in assessing and categorizing data. The company's design also allowed ad buyers to block African-Americans from seeing housing ads.\n\nSurveillance camera software may be considered inherently political because it requires algorithms to distinguish normal from abnormal behaviors, and to determine who belongs in certain locations at certain times. The ability of such algorithms to recognize faces across a racial spectrum has been shown to be limited by the racial diversity of images in its training database; if the majority of photos belong to one race or gender, the software is better at recognizing other members of that race or gender. A 2002 analysis of software used to identify individuals in CCTV images found several examples of bias when run against criminal databases. The software was assessed as identifying men more frequently than women, older people more frequently than the young, and identified Asians, African-Americans and other races more often than whites. Additional studies of facial recognition software have found the opposite to be true when trained on non-criminal databases, with the software being the least accurate in identifying darker-skinned females.\n\nIn 2011, users of the gay hookup application Grindr reported that the Android store's recommendation algorithm was linking Grindr to applications designed to find sex offenders, which critics said inaccurately related homosexuality with pedophilia. Writer Mike Ananny criticized this association in \"The Atlantic\", arguing that such associations further stigmatized gay men. In 2009, online retailer Amazon de-listed 57,000 books after an algorithmic change expanded its \"adult content\" blacklist to include any book addressing sexuality or gay themes, such as the critically acclaimed novel \"Brokeback Mountain\".\n\nSeveral problems impede the study of large-scale algorithmic bias, hindering the application of academically rigorous studies and public understanding.\n\nCommercial algorithms are proprietary, and may be treated as trade secrets. Treating algorithms as trade secrets protects companies, such as search engines, where a transparent algorithm might reveal tactics to manipulate search rankings. This makes it difficult for researchers to conduct interviews or analysis to discover how algorithms function. Critics suggest that such secrecy can also obscure possible unethical methods used in producing or processing algorithmic output.\n\nAlgorithmic processes are complex, often exceeding the understanding of the people who use them. Large-scale operations may not be understood even by those involved in creating them. The methods and processes of contemporary programs are often obscured by the inability to know every permutation of a code's input or output.\n\nSocial scientist Bruno Latour has identified this process as blackboxing, a process in which \"scientific and technical work is made invisible by its own success. When a machine runs efficiently, when a matter of fact is settled, one need focus only on its inputs and outputs and not on its internal complexity. Thus, paradoxically, the more science and technology succeed, the more opaque and obscure they become.\" Others have critiqued the black box metaphor, suggesting that current algorithms are not one black box, but a network of interconnected ones.\n\nAn example of this complexity can be found in the range of inputs into customizing feedback. The social media site Facebook factored in at least 100,000 data points to determine the layout of a user's social media feed in 2013. Furthermore, large teams of programmers may operate in relative isolation from one another, and be unaware of the cumulative effects of small decisions within connected, elaborate algorithms. Not all code is original, and may be borrowed from other libraries, creating a complicated set of relationships between data processing and data input systems.\n\nAdditional complexity occurs through machine learning and the personalization of algorithms based on user interactions such as clicks, time spent on site, and other metrics. These personal adjustments can confuse general attempts to understand algorithms. One unidentified streaming radio service reported that it used five unique music-selection algorithms it selected for its users, based on their behavior. This creates different experiences of the same streaming services between different users, making it harder to understand what these algorithms do.\nCompanies also run frequent A/B tests to fine-tune algorithms based on user response. For example, the search engine Bing can run up to ten million subtle variations of its service per day, creating different experiences of the service between each use and/or user.\n\nA significant barrier to understanding the tackling of bias in practice is that categories, such as demographics of individuals protected by anti-discrimination law, are often not explicitly considered when collecting and processing data. In some cases, there is little opportunity to collect this data explicitly, such as in device fingerprinting, ubiquitous computing and the Internet of Things. In other cases, the data controller may not wish to collect such data for reputational reasons, or because it represents a heightened liability and security risk. It may also be the case that, at least in relation to the European Union's General Data Protection Regulation, such data falls under the 'special category' provisions (Article 9), and therefore comes with more restrictions on potential collection and processing.\n\nAlgorithmic bias does not only include protected categories, but can also concerns characteristics less easily observable or codifiable, such as political viewpoints. In these cases, there is rarely an easily accessible or non-controversial ground truth, and removing the bias from such a system is more difficult.\n\nFurthermore, false and accidental correlations can emerge from a lack of understanding of protected categories, for example, insurance rates based on historical data of car accidents which may overlap, strictly by coincidence, with residential clusters of ethnic minorities.\n\nThere have been several attempts to create methods and tools that can detect and observe biases within an algorithm. These emergent field focuses on tools which are typically applied to the data used by the program rather than the algorithm's internal processes. These methods may also analyze a program's output and its usefulness.\n\nCurrently, a new IEEE standard is being drafted that aims to specify methodologies which help creators of algorithms eliminate issues of bias and articulate transparency (i.e. to authorities or end users) about the function and possible effects of their algorithms. The project was approved February 2017 and is sponsored by the Software & Systems Engineering Standards Committee, a committee chartered by the IEEE Computer Society. A draft of the standard is expected to be submitted for balloting in June 2019.\n\nThe General Data Protection Regulation (GDPR), the European Union's revised data protection regime that was implemented in 2018, addresses \"Automated individual decision-making, including profiling\" in Article 22. These rules prohibit \"solely\" automated decisions which have a \"significant\" or \"legal\" effect on an individual, unless they are explicitly authorised by consent, contract, or member state law. Where they are permitted, there must be safeguards in place, such as a right to a human-in-the-loop, and a non-binding right to an explanation of decisions reached. While these regulations are commonly considered to be new, nearly identical provisions have existed across Europe since 1995, in Article 15 of the Data Protection Directive. The original automated decision rules and safeguards found in French law since the late 1970s.\n\nThe GDPR addresses algorithmic bias in profiling systems, as well as the statistical approaches possible to clean it, directly in recital 71, noting that ... the controller should use appropriate mathematical or statistical procedures for the profiling, implement technical and organisational measures appropriate ... that prevents, inter alia, discriminatory effects on natural persons on the basis of racial or ethnic origin, political opinion, religion or beliefs, trade union membership, genetic or health status or sexual orientation, or that result in measures having such an effect.Like the non-binding right to an explanation in recital 71, the problem is the non-binding nature of recitals. While it has been treated as a requirement by the Article 29 Working Party that advised on the implementation of data protection law, its practical dimensions are unclear. It has been argued that the Data Protection Impact Assessments for high risk data profiling (alongside other pre-emptive measures within data protection) may be a better way to tackle issues of algorithmic discrimination, as it restricts the actions of those deploying algorithms, rather than requiring consumers to file complaints or request changes.\n\nThe United States has no general legislation controlling algorithmic bias, approaching the problem through various state and federal laws that might vary by industry, sector, and by how an algorithm is used. Many policies are self-enforced or controlled by the Federal Trade Commission. In 2016, the Obama administration released the National Artificial Intelligence Research and Development Strategic Plan, which was intended to guide policymakers toward a critical assessment of algorithms. It recommended researchers to \"design these systems so that their actions and decision-making are transparent and easily interpretable by humans, and thus can be examined for any bias they may contain, rather than just learning and repeating these biases\". Intended only as guidance, the report did not create any legal precedent.\n\nIn 2017, New York City passed the first algorithmic accountability bill in the United States. The bill, which went into effect on January 1, 2018, required \"the creation of a task force that provides recommendations on how information on agency automated decision systems may be shared with the public, and how agencies may address instances where people are harmed by agency automated decision systems.\" The task force is required to present findings and recommendations for further regulatory action in 2019.\n"}
{"id": "46628", "url": "https://en.wikipedia.org/wiki?curid=46628", "title": "Automated teller machine", "text": "Automated teller machine\n\nAn automated teller machine (ATM) is an electronic telecommunications device that enables customers of financial institutions to perform financial transactions, such as cash withdrawals, deposits, transfer funds, or obtaining account information, at any time and without the need for direct interaction with bank staff.\n\nATMs are known by a variety of names, including automatic teller machine in the United States (ATM, American, British, Australian, Malaysian, South African, Singaporean, Indian, Maldivian, Hiberno, Philippines and Sri Lankan English), often redundantly ATM machine, automated banking machine (ABM, Canadian English). Although ABM is used in Canada, ATM is still very commonly used in Canada and many Canadian organization used ATM over ABM. In British English, the terms cash point, cash machine, minibank (the official name of the Yorkshire bank ATMs), and \"hole in the wall\" are most widely used. Other terms include cashline, nibank, tyme machine, cash dispenser, bankomat or bancomat. Many ATMs have a sign above them, indicating the name of the bank or organisation that owns the ATM, and possibly including the networks to which it can connect. In Canada, ABMs that are not operated by a financial institution are known as \"white-label ABMs\".\n\nAccording to the ATM Industry Association (ATMIA), there are now close to 3.5 million ATMs installed worldwide. However, the use of ATMs in Australia is gradually declining – most notably in retail precincts.\n\nOn most modern ATMs, customers are identified by inserting a plastic ATM card (or some other acceptable payment card) into the ATM, with authentication being by the customer entering a personal identification number (PIN), which must match the PIN stored in the chip on the card (if the card is so equipped), or in the issuing financial institution's database.\n\nUsing an ATM, customers can access their bank deposit or credit accounts in order to make a variety of financial transactions such as cash withdrawals, check balances, or credit mobile phones. ATMs can be used to withdraw cash in a foreign country. If the currency being withdrawn from the ATM is different from that in which the bank account is denominated, the money will be converted at the financial institution's exchange rate.\n\nThe idea of out-of-hours cash distribution developed from bankers' needs in Asia (Japan), Europe (Sweden and the United Kingdom) and North America (the United States). Little is known of the Japanese device other than that it was called \"Computer Loan Machine\" and supplied cash as a three-month loan at 5% p.a. after inserting a credit card. The device was operational in 1966.\n\nAdrian Ashfield invented the basic idea of a card combining the key and user's identity in February 1962. This was granted UK Patent 959,713 for \"Access Controller\" in June 1964 and assigned to W. S. Atkins & Partners who employed Ashfield. He was paid ten shillings for this, the standard sum for all patents. It was originally intended to dispense petrol but the patent covered all uses.\n\nIn the US patent record, Luther George Simjian has been credited with developing a \"prior art device\". Specifically his 132nd patent (US3079603), which was first filed on 30 June 1960 (and granted 26 February 1963). The roll-out of this machine, called Bankograph, was delayed by a couple of years, due in part to Simjian's Reflectone Electronics Inc. being acquired by Universal Match Corporation. An experimental Bankograph was installed in New York City in 1961 by the City Bank of New York, but removed after six months due to the lack of customer acceptance. The Bankograph was an automated envelope deposit machine (accepting coins, cash and cheques) and did not have cash dispensing features.\n\nIt is widely accepted that the first cash machine was put into use by Barclays Bank in its Enfield Town branch in North London, United Kingdom, on 27 June 1967. This machine was inaugurated by English comedy actor Reg Varney. This instance of the invention is credited to the engineering team led by John Shepherd-Barron of printing firm De La Rue, who was awarded an OBE in the 2005 New Year Honours. Transactions were initiated by inserting paper cheques issued by a teller or cashier, marked with carbon-14 for machine readability and security, which in a later model were matched with a six-digit personal identification number (PIN). Shepherd-Barron stated \"It struck me there must be a way I could get my own money, anywhere in the world or the UK. I hit upon the idea of a chocolate bar dispenser, but replacing chocolate with cash.\"\n\nThe Barclays–De La Rue machine (called De La Rue Automatic Cash System or DACS) beat the Swedish saving banks' and a company called Metior's machine (a device called Bankomat) by a mere nine days and Westminster Bank's–Smith Industries–Chubb system (called Chubb MD2) by a month. The online version of the Swedish machine is listed to have been operational on 6 May 1968, while claiming to be the first online ATM in the world (ahead of a similar claim by IBM and Lloyds Bank in 1971). The collaboration of a small start-up called Speytec and Midland Bank developed a fourth machine which was marketed after 1969 in Europe and the US by the Burroughs Corporation. The patent for this device (GB1329964) was filed in September 1969 (and granted in 1973) by John David Edwards, Leonard Perkins, John Henry Donald, Peter Lee Chappell, Sean Benjamin Newcombe, and Malcom David Roe.\n\nBoth the DACS and MD2 accepted only a single-use token or voucher which was retained by the machine, while the Speytec worked with a card with a magnetic stripe at the back. They used principles including Carbon-14 and low-coercivity magnetism in order to make fraud more difficult.\n\nThe idea of a PIN stored on the card was developed by a group of engineers working at Smiths Group on the Chubb MD2 in 1965 and which has been credited to James Goodfellow (patent GB1197183 filed on 2 May 1966 with Anthony Davies). The essence of this system was that it enabled the verification of the customer with the debited account without human intervention. This patent is also the earliest instance of a complete \"currency dispenser system\" in the patent record. This patent was filed on 5 March 1968 in the US (US 3543904) and granted on 1 December 1970. It had a profound influence on the industry as a whole. Not only did future entrants into the cash dispenser market such as NCR Corporation and IBM licence Goodfellow's PIN system, but a number of later patents reference this patent as \"Prior Art Device\".\n\nDevices designed by British (i.e. Chubb, De La Rue) and Swedish (i.e. Asea Meteor) quickly spread out. For example, given its link with Barclays, Bank of Scotland deployed a DACS in 1968 under the 'Scotcash' brand. Customers were given personal code numbers to activate the machines, similar to the modern PIN. They were also supplied with £10 vouchers. These were fed into the machine, and the corresponding amount debited from the customer's account.\n\nA Chubb-made ATM appeared in Sydney in 1969. This was the first ATM installed in Australia. The machine only dispensed $25 at a time and the bank card itself would be mailed to the user after the bank had processed the withdrawal.\n\nAsea Metior's Bankomat was the first ATM installed in Spain on January 9, 1969, in downtown Madrid by Banesto. This device dispensed 1,000 peseta bills (1 to 5 max). Each user had to introduce a security personal key using a combination of the ten numeric buttons. In March of the same year an ad with the instructions to use the Bancomat was published in the same newspaper.\n\nAfter looking firsthand at the experiences in Europe, in 1968 the ATM was pioneered in the U.S. by Donald Wetzel, who was a department head at a company called Docutel. Docutel was a subsidiary of Recognition Equipment Inc of Dallas, Texas, which was producing optical scanning equipment and had instructed Docutel to explore automated baggage handling and automated gasoline pumps.\n\nOn September 2, 1969, Chemical Bank installed the first ATM in the U.S. at its branch in Rockville Centre, New York. The first ATMs were designed to dispense a fixed amount of cash when a user inserted a specially coded card. A Chemical Bank advertisement boasted \"On Sept. 2 our bank will open at 9:00 and never close again.\" Chemical's ATM, initially known as a Docuteller was designed by Donald Wetzel and his company Docutel. Chemical executives were initially hesitant about the electronic banking transition given the high cost of the early machines. Additionally, executives were concerned that customers would resist having machines handling their money. In 1995, the Smithsonian National Museum of American History recognised Docutel and Wetzel as the inventors of the networked ATM.\n\nBy 1974, Docutel had acquired 70 percent of the U.S. market; but as a result of the early 1970s worldwide recession and its reliance on a single product line, Docutel lost its independence and was forced to merge with the U.S. subsidiary of Olivetti.\n\nWetzel was recognised by the United States Patent Office as having invented the ATM in the form of U.S. Patent # 3,761,682; the application had been filed in October 1971 and the patent was granted in 1973. However, the U.S. patent record cites at least three previous applications from Docutel, all relevant to the development of the ATM and where Wetzel does not figure, namely US Patent # 3,662,343, U.S. Patent # 3651976 and U.S. Patent # 3,68,569. These patents are all credited to Kenneth S. Goldstein, MR Karecki, TR Barnes, GR Chastian and John D. White.\n\nThe first modern ATM was an IBM 2984 and came into use at Lloyds Bank, High Street, Brentwood, Essex, the UK in December 1972. The IBM 2984 was designed at the request of Lloyds Bank. The 2984 Cash Issuing Terminal was the first true ATM, similar in function to today's machines and named by Lloyds Bank: Cashpoint. Cashpoint is still a registered trademark of Lloyds Banking Group in the UK but is often used as a generic trademark to refer to ATMs of all UK banks. All were online and issued a variable amount which was immediately deducted from the account. A small number of 2984s were supplied to a U.S. bank. A couple of well known historical models of ATMs include the IBM 3614, IBM 3624 and 473x series, Diebold 10xx and TABS 9000 series, NCR 1780 and earlier NCR 770 series.\n\nThe first switching system to enable shared automated teller machines between banks went into production operation on February 3, 1979, in Denver, Colorado, in an effort by Colorado National Bank of Denver and Kranzley and Company of Cherry Hill, New Jersey.\n\nThe newest ATM at Royal Bank of Scotland allows customers to withdraw cash up to £130 without a card by inputting a six-digit code requested through their smartphones.\n\nATMs can be placed at any location but are most often placed near or inside banks, shopping centers/malls, airports, railway stations, metro stations, grocery stores, petrol/gas stations, restaurants, and other locations. ATMs are also found on cruise ships and on some US Navy ships, where sailors can draw out their pay.\n\nATMs may be on- and off-premises. On-premises ATMs are typically more advanced, multi-function machines that complement a bank branch's capabilities, and are thus more expensive. Off-premises machines are deployed by financial institutions and Independent Sales Organisations (ISOs) where there is a simple need for cash, so they are generally cheaper single function devices.\n\nIn the US, Canada and some Gulf countries, banks may have drive-thru lanes providing access to ATMs using an automobile.\n\nIn recent times, countries like India and some countries in Africa are installing ATMs in rural areas, which are solar powered.\n\nMost ATMs are connected to interbank networks, enabling people to withdraw and deposit money from machines not belonging to the bank where they have their accounts or in the countries where their accounts are held (enabling cash withdrawals in local currency). Some examples of interbank networks include NYCE, PULSE, PLUS, Cirrus, AFFN, Interac, Interswitch, STAR, LINK, MegaLink, and BancNet.\n\nATMs rely on authorization of a financial transaction by the card issuer or other authorizing institution on a communications network. This is often performed through an ISO 8583 messaging system.\n\nMany banks charge ATM usage fees. In some cases, these fees are charged solely to users who are not customers of the bank that operates the ATM; in other cases, they apply to all users.\n\nIn order to allow a more diverse range of devices to attach to their networks, some interbank networks have passed rules expanding the definition of an ATM to be a terminal that either has the vault within its footprint or utilises the vault or cash drawer within the merchant establishment, which allows for the use of a scrip cash dispenser.\n\nATMs typically connect directly to their host or ATM Controller on either ADSL or dial-up modem over a telephone line or directly on a leased line. Leased lines are preferable to plain old telephone service (POTS) lines because they require less time to establish a connection. Less-trafficked machines will usually rely on a dial-up modem on a POTS line rather than using a leased line, since a leased line may be comparatively more expensive to operate compared to a POTS line. That dilemma may be solved as high-speed Internet VPN connections become more ubiquitous. Common lower-level layer communication protocols used by ATMs to communicate back to the bank include SNA over SDLC, TC500 over Async, X.25, and TCP/IP over Ethernet.\n\nIn addition to methods employed for transaction security and secrecy, all communications traffic between the ATM and the Transaction Processor may also be encrypted using methods such as SSL.\n\nThere are no hard international or government-compiled numbers totaling the complete number of ATMs in use worldwide. Estimates developed by ATMIA place the number of ATMs currently in use at 3 million units, or approximately 1 ATM per 3000 people in the world.\n\nTo simplify the analysis of ATM usage around the world, financial institutions generally divide the world into seven regions, due to the penetration rates, usage statistics, and features deployed. Four regions (USA, Canada, Europe, and Japan) have high numbers of ATMs per million people. Despite the large number of ATMs, there is additional demand for machines in the Asia/Pacific area as well as in Latin America. Macau may have the highest density of ATMs at 254 ATMs per 100,000 adults. ATMs have yet to reach high numbers in the Near East and Africa.\n\nThe world's highest ATM is located at the Khunjerab Pass in Pakistan. Installed at an elevation of 15,397 feet by the National Bank of Pakistan, it is designed to work in temperatures of up to -40 degree Celsius.\n\nAn ATM is typically made up of the following devices:\n\nDue to heavier computing demands and the falling price of personal computer–like architectures, ATMs have moved away from custom hardware architectures using microcontrollers or application-specific integrated circuits and have adopted the hardware architecture of a personal computer, such as USB connections for peripherals, Ethernet and IP communications, and use personal computer operating systems.\n\nBusiness owners often lease ATMs from service providers. However, based on the economies of scale, the price of equipment has dropped to the point where many business owners are simply paying for ATMs using a credit card.\n\nNew ADA voice and text-to-speech guidelines imposed in 2010, but required by March 2012 have forced many ATM owners to either upgrade non-compliant machines or dispose them if they are not upgradable, and purchase new compliant equipment. This has created an avenue for hackers and thieves to obtain ATM hardware at junkyards from improperly disposed decommissioned machines.\n\nThe vault of an ATM is within the footprint of the device itself and is where items of value are kept. Scrip cash dispensers do not incorporate a vault.\n\nMechanisms found inside the vault may include:\n\nATM vaults are supplied by manufacturers in several grades. Factors influencing vault grade selection include cost, weight, regulatory requirements, ATM type, operator risk avoidance practices and internal volume requirements. Industry standard vault configurations include Underwriters Laboratories UL-291 \"Business Hours\" and Level 1 Safes, RAL TL-30 derivatives, and CEN EN 1143-1 - CEN III and CEN IV.\n\nATM manufacturers recommend that a vault be attached to the floor to prevent theft, though there is a record of a theft conducted by tunnelling into an ATM floor.\n\nWith the migration to commodity Personal Computer hardware, standard commercial \"off-the-shelf\" operating systems and programming environments can be used inside of ATMs. Typical platforms previously used in ATM development include RMX or OS/2.\n\nToday, the vast majority of ATMs worldwide use a Microsoft Windows operating system, primarily Windows XP Professional or Windows XP Embedded. In early 2014, 95% of ATMs were running Windows XP. A small number of deployments may still be running older versions of the Windows OS, such as Windows NT, Windows CE, or Windows 2000.\n\nThere is a computer industry security view that general public desktop operating systems(os) have greater risks as operating systems for cash dispensing machines than other types of operating systems like (secure) real-time operating systems (RTOS). RISKS Digest has many articles about ATM operating system vulnerabilities.\n\nLinux is also finding some reception in the ATM marketplace. An example of this is Banrisul, the largest bank in the south of Brazil, which has replaced the MS-DOS operating systems in its ATMs with Linux. Banco do Brasil is also migrating ATMs to Linux. Indian-based Vortex Engineering is manufacturing ATMs which operate only with Linux.\nCommon application layer transaction protocols, such as Diebold 91x (911 or 912) and NCR NDC or NDC+ provide emulation of older generations of hardware on newer platforms with incremental extensions made over time to address new capabilities, although companies like NCR continuously improve these protocols issuing newer versions (e.g. NCR's AANDC v3.x.y, where x.y are subversions). Most major ATM manufacturers provide software packages that implement these protocols. Newer protocols such as IFX have yet to find wide acceptance by transaction processors.\n\nWith the move to a more standardised software base, financial institutions have been increasingly interested in the ability to pick and choose the application programs that drive their equipment. WOSA/XFS, now known as CEN XFS (or simply XFS), provides a common API for accessing and manipulating the various devices of an ATM. J/XFS is a Java implementation of the CEN XFS API.\n\nWhile the perceived benefit of XFS is similar to the Java's \"Write once, run anywhere\" mantra, often different ATM hardware vendors have different interpretations of the XFS standard. The result of these differences in interpretation means that ATM applications typically use a middleware to even out the differences among various platforms.\n\nWith the onset of Windows operating systems and XFS on ATMs, the software applications have the ability to become more intelligent. This has created a new breed of ATM applications commonly referred to as programmable applications. These types of applications allows for an entirely new host of applications in which the ATM terminal can do more than only communicate with the ATM switch. It is now empowered to connected to other content servers and video banking systems.\n\nNotable ATM software that operates on XFS platforms include Triton PRISM, Diebold Agilis EmPower, NCR APTRA Edge, Absolute Systems AbsoluteINTERACT, KAL Kalignite Software Platform, Phoenix Interactive VISTAatm, Wincor Nixdorf ProTopas, Euronet EFTS and Intertech inter-ATM.\n\nWith the move of ATMs to industry-standard computing environments, concern has risen about the integrity of the ATM's software stack.\n\nSecurity, as it relates to ATMs, has several dimensions. ATMs also provide a practical demonstration of a number of security systems and concepts operating together and how various security concerns are addressed.\n\nEarly ATM security focused on making the terminals invulnerable to physical attack; they were effectively safes with dispenser mechanisms. A number of attacks resulted, with thieves attempting to steal entire machines by ram-raiding. Since the late 1990s, criminal groups operating in Japan improved ram-raiding by stealing and using a truck loaded with heavy construction machinery to effectively demolish or uproot an entire ATM and any housing to steal its cash.\n\nAnother attack method, \"plofkraak\", is to seal all openings of the ATM with silicone and fill the vault with a combustible gas or to place an explosive inside, attached, or near the machine. This gas or explosive is ignited and the vault is opened or distorted by the force of the resulting explosion and the criminals can break in. This type of theft has occurred in the Netherlands, Belgium, France, Denmark, Germany and Australia. These types of attacks can be prevented by a number of gas explosion prevention devices also known as gas suppression system. These systems use explosive gas detection sensor to detect explosive gas and to neutralise it by releasing a special explosion suppression chemical which changes the composition of the explosive gas and renders it ineffective.\n\nSeveral attacks in the UK (at least one of which was successful) have involved digging a concealed tunnel under the ATM and cutting through the reinforced base to remove the money.\n\nModern ATM physical security, per other modern money-handling security, concentrates on denying the use of the money inside the machine to a thief, by using different types of Intelligent Banknote Neutralisation Systems.\n\nA common method is to simply rob the staff filling the machine with money. To avoid this, the schedule for filling them is kept secret, varying and random. The money is often kept in cassettes, which will dye the money if incorrectly opened.\n\nThe security of ATM transactions relies mostly on the integrity of the secure cryptoprocessor: the ATM often uses general commodity components that sometimes are not considered to be \"trusted systems\".\n\nEncryption of personal information, required by law in many jurisdictions, is used to prevent fraud. Sensitive data in ATM transactions are usually encrypted with DES, but transaction processors now usually require the use of Triple DES. Remote Key Loading techniques may be used to ensure the secrecy of the initialisation of the encryption keys in the ATM. Message Authentication Code (MAC) or Partial MAC may also be used to ensure messages have not been tampered with while in transit between the ATM and the financial network.\n\nThere have also been a number of incidents of fraud by Man-in-the-middle attacks, where criminals have attached fake keypads or card readers to existing machines. These have then been used to record customers' PINs and bank card information in order to gain unauthorised access to their accounts. Various ATM manufacturers have put in place countermeasures to protect the equipment they manufacture from these threats.\n\nAlternative methods to verify cardholder identities have been tested and deployed in some countries, such as finger and palm vein patterns, iris, and facial recognition technologies. Cheaper mass-produced equipment has been developed and is being installed in machines globally that detect the presence of foreign objects on the front of ATMs, current tests have shown 99% detection success for all types of skimming devices.\n\nOpenings on the customer side of ATMs are often covered by mechanical shutters to prevent tampering with the mechanisms when they are not in use. Alarm sensors are placed inside ATMs and their servicing areas to alert their operators when doors have been opened by unauthorised personnel.\n\nTo protect against hackers, ATMs have a built-in firewall. Once the firewall has detected malicious attempts to break into the machine remotely, the firewall locks down the machine.\n\nRules are usually set by the government or ATM operating body that dictate what happens when integrity systems fail. Depending on the jurisdiction, a bank may or may not be liable when an attempt is made to dispense a customer's money from an ATM and the money either gets outside of the ATM's vault, or was exposed in a non-secure fashion, or they are unable to determine the state of the money after a failed transaction. Customers often commented that it is difficult to recover money lost in this way, but this is often complicated by the policies regarding suspicious activities typical of the criminal element.\n\nIn some countries, multiple security cameras and security guards are a common feature. In the United States, The New York State Comptroller's Office has advised the New York State Department of Banking to have more thorough safety inspections of ATMs in high crime areas.\n\nConsultants of ATM operators assert that the issue of customer security should have more focus by the banking industry; it has been suggested that efforts are now more concentrated on the preventive measure of deterrent legislation than on the problem of ongoing forced withdrawals.\n\nAt least as far back as July 30, 1986, consultants of the industry have advised for the adoption of an emergency PIN system for ATMs, where the user is able to send a silent alarm in response to a threat. Legislative efforts to require an emergency PIN system have appeared in Illinois, Kansas and Georgia, but none have succeeded yet. In January 2009, Senate Bill 1355 was proposed in the Illinois Senate that revisits the issue of the reverse emergency PIN system. The bill is again supported by the police and denied by the banking lobby.\n\nIn 1998, three towns outside Cleveland, Ohio, in response to an ATM crime wave, adopted legislation requiring that an emergency telephone number switch be installed at all outdoor ATMs within their jurisdiction. In the wake of a homicide in Sharon Hill, Pennsylvania, the city council passed an ATM security bill as well.\n\nIn China and elsewhere, many efforts to promote security have been made. On-premises ATMs are often located inside the bank's lobby, which may be accessible 24 hours a day. These lobbies have extensive security camera coverage, a courtesy telephone for consulting with the bank staff, and a security guard on the premises. Bank lobbies that are not guarded 24 hours a day may also have secure doors that can only be opened from outside by swiping the bank card against a wall-mounted scanner, allowing the bank to identify which card enters the building. Most ATMs will also display on-screen safety warnings and may also be fitted with convex mirrors above the display allowing the user to see what is happening behind them.\n\nAs of 2013, the only claim available about the extent of ATM-connected homicides is that they range from 500 to 1,000 per year in the US, covering only cases where the victim had an ATM card and the card was used by the killer after the known time of death.\n\nThe term is used to describe one method criminals utilize to steal money from an ATM. The thieves gain physical access through a small hole drilled in the machine. They disconnect the existing hard drive and connect an external drive using an industrial endoscope. They then depress an internal button that reboots the device so that it is now under the control of the external drive. They can then have the ATM dispense all of its cash.\n\nIn recent years, many ATMs also encrypt the hard disk. This means that actually creating the software for jackpotting is a lot more difficult to do.\n\nATMs were originally developed as cash dispensers, and have evolved to provide many other bank-related functions: \nIn some countries, especially those which benefit from a fully integrated cross-bank network (e.g.: Multibanco in Portugal), ATMs include many functions that are not directly related to the management of one's own bank account, such as:\n\nIncreasingly, banks are seeking to use the ATM as a sales device to deliver pre approved loans and targeted advertising using products such as ITM (the Intelligent Teller Machine) from Aptra Relate from NCR. ATMs can also act as an advertising channel for other companies.*\nHowever, several different ATM technologies have not yet reached worldwide acceptance, such as:\nVideoconferencing teller machines are currently referred to as Interactive Teller Machines. Benton Smith, in the Idaho Business Review writes \"The software that allows interactive teller machines to function was created by a Salt Lake City-based company called uGenius, a producer of video banking software. NCR, a leading manufacturer of ATMs, acquired uGenius in 2013 and married its own ATM hardware with uGenius' video software.\" \nBefore an ATM is placed in a public place, it typically has undergone extensive testing with both test money and the backend computer systems that allow it to perform transactions. Banking customers also have come to expect high reliability in their ATMs, which provides incentives to ATM providers to minimise machine and network failures. Financial consequences of incorrect machine operation also provide high degrees of incentive to minimise malfunctions.\n\nATMs and the supporting electronic financial networks are generally very reliable, with industry benchmarks typically producing 98.25% customer availability for ATMs and up to 99.999% availability for host systems that manage the networks of ATMs. If ATM networks do go out of service, customers could be left without the ability to make transactions until the beginning of their bank's next time of opening hours.\n\nThis said, not all errors are to the detriment of customers; there have been cases of machines giving out money without debiting the account, or giving out higher value notes as a result of incorrect denomination of banknote being loaded in the money cassettes. The result of receiving too much money may be influenced by the card holder agreement in place between the customer and the bank.\n\nErrors that can occur may be mechanical (such as card transport mechanisms; keypads; hard disk failures; envelope deposit mechanisms); software (such as operating system; device driver; application); communications; or purely down to operator error.\n\nTo aid in reliability, some ATMs print each transaction to a roll-paper journal that is stored inside the ATM, which allows its users and the related financial institutions to settle things based on the records in the journal in case there is a dispute. In some cases, transactions are posted to an electronic journal to remove the cost of supplying journal paper to the ATM and for more convenient searching of data.\n\nImproper money checking can cause the possibility of a customer receiving counterfeit banknotes from an ATM. While bank personnel are generally trained better at spotting and removing counterfeit cash, the resulting ATM money supplies used by banks provide no guarantee for proper banknotes, as the Federal Criminal Police Office of Germany has confirmed that there are regularly incidents of false banknotes having been dispensed through ATMs. Some ATMs may be stocked and wholly owned by outside companies, which can further complicate this problem.\nBill validation technology can be used by ATM providers to help ensure the authenticity of the cash before it is stocked in the machine; those with cash recycling capabilities include this capability.\n\nIn India, whenever a transaction fails with an ATM due to network or technical issue and if the amount does not get dispensed in spite of account being debited then the banks are supposed to return the debited amount to the customer within 7 working days from the day of receipt of complaint. Banks are also liable to pay the late fees in case of delay in repayment of funds post 7 days.\n\nAs with any device containing objects of value, ATMs and the systems they depend on to function are the targets of fraud. Fraud against ATMs and people's attempts to use them takes several forms.\n\nThe first known instance of a fake ATM was installed at a shopping mall in Manchester, Connecticut in 1993. By modifying the inner workings of a Fujitsu model 7020 ATM, a criminal gang known as the Bucklands Boys stole information from cards inserted into the machine by customers.\n\nWAVY-TV reported an incident in Virginia Beach in September 2006 where a hacker, who had probably obtained a factory-default administrator password for a filling station's white-label ATM, caused the unit to assume it was loaded with US$5 bills instead of $20s, enabling himself—and many subsequent customers—to walk away with four times the money withdrawn from their accounts. This type of scam was featured on the TV series \"The Real Hustle\".\n\nATM behaviour can change during what is called \"stand-in\" time, where the bank's cash dispensing network is unable to access databases that contain account information (possibly for database maintenance). In order to give customers access to cash, customers may be allowed to withdraw cash up to a certain amount that may be less than their usual daily withdrawal limit, but may still exceed the amount of available money in their accounts, which could result in fraud if the customers intentionally withdraw more money than what they had in their accounts.\n\nIn an attempt to prevent criminals from shoulder surfing the customer's personal identification number (PIN), some banks draw privacy areas on the floor.\n\nFor a low-tech form of fraud, the easiest is to simply steal a customer's card along with its PIN. A later variant of this approach is to trap the card inside of the ATM's card reader with a device often referred to as a Lebanese loop. When the customer gets frustrated by not getting the card back and walks away from the machine, the criminal is able to remove the card and withdraw cash from the customer's account, using the card and its PIN.\n\nThis type of fraud has spread globally. Although somewhat replaced in terms of volume by skimming incidents, a re-emergence of card trapping has been noticed in regions such as Europe, where EMV chip and PIN cards have increased in circulation.\n\nAnother simple form of fraud involves attempting to get the customer's bank to issue a new card and its PIN and stealing them from their mail.\n\nBy contrast, a newer high-tech method of operating, sometimes called card skimming or card cloning, involves the installation of a magnetic card reader over the real ATM's card slot and the use of a wireless surveillance camera or a modified digital camera or a false PIN keypad to observe the user's PIN. Card data is then cloned into a duplicate card and the criminal attempts a standard cash withdrawal. The availability of low-cost commodity wireless cameras, keypads, card readers, and card writers has made it a relatively simple form of fraud, with comparatively low risk to the fraudsters.\n\nIn an attempt to stop these practices, countermeasures against card cloning have been developed by the banking industry, in particular by the use of smart cards which cannot easily be copied or spoofed by unauthenticated devices, and by attempting to make the outside of their ATMs tamper evident. Older chip-card security systems include the French Carte Bleue, Visa Cash, Mondex, Blue from American Express and EMV '96 or EMV 3.11. The most actively developed form of smart card security in the industry today is known as EMV 2000 or EMV 4.x.\n\nEMV is widely used in the UK (Chip and PIN) and other parts of Europe, but when it is not available in a specific area, ATMs must fall back to using the easy–to–copy magnetic stripe to perform transactions. This fallback behaviour can be exploited. However, the fallback option has been removed on the ATMs of some UK banks, meaning if the chip is not read, the transaction will be declined.\n\nCard cloning and skimming can be detected by the implementation of magnetic card reader heads and firmware that can read a signature embedded in all magnetic stripes during the card production process. This signature, known as a \"MagnePrint\" or \"BluPrint\", can be used in conjunction with common two-factor authentication schemes used in ATM, debit/retail point-of-sale and prepaid card applications.\n\nThe concept and various methods of copying the contents of an ATM card's magnetic stripe onto a duplicate card to access other people's financial information was well known in the hacking communities by late 1990.\n\nIn 1996, Andrew Stone, a computer security consultant from Hampshire in the UK, was convicted of stealing more than £1 million by pointing high-definition video cameras at ATMs from a considerable distance and recording the card numbers, expiry dates, etc. from the embossed detail on the ATM cards along with video footage of the PINs being entered. After getting all the information from the videotapes, he was able to produce clone cards which not only allowed him to withdraw the full daily limit for each account, but also allowed him to sidestep withdrawal limits by using multiple copied cards. In court, it was shown that he could withdraw as much as £10,000 per hour by using this method. Stone was sentenced to five years and six months in prison.\n\nA talking ATM is a type of ATM that provides audible instructions so that people who cannot read a screen can independently use the machine, therefore effectively eliminating the need for assistance from an external, potentially malevolent source. All audible information is delivered privately through a standard headphone jack on the face of the machine. Alternatively, some banks such as the Nordea and Swedbank use a built-in external speaker which may be invoked by pressing the talk button on the keypad. Information is delivered to the customer either through pre-recorded sound files or via text-to-speech speech synthesis.\n\nA postal interactive kiosk may share many components of an ATM (including a vault), but it only dispenses items related to postage.\n\nA scrip cash dispenser may have many components in common with an ATM, but it lacks the ability to dispense physical cash and consequently requires no vault. Instead, the customer requests a withdrawal transaction from the machine, which prints a receipt. The customer then takes this receipt to a nearby sales clerk, who then exchanges it for cash from the till.\n\nA teller assist unit (TAU) is distinct in that it is designed to be operated solely by trained personnel and not by the general public, does integrate directly into interbank networks, and usually is controlled by a computer that is not directly integrated into the overall construction of the unit.\n\nA Web ATM is an online interface for ATM card banking that uses a smart card reader. All the usual ATM functions are available, except for withdrawing cash. Most banks in Taiwan provide these online services.\n\n\n\n"}
{"id": "33624283", "url": "https://en.wikipedia.org/wiki?curid=33624283", "title": "B.O.S. Better Online Solutions", "text": "B.O.S. Better Online Solutions\n\nB.O.S. Better Online Solutions is a publicly traded company, headquartered in Israel, that provides RFID and supply chain solutions. Its shares are traded on the NASDAQ Capital Market.\n\nB.O.S. Better Online Solutions specializes in RFID solutions as a more efficient alternative to the traditional barcode object identification method. In an interview with \"The Wall Street Transcript\" in 2008, president Shmuel Koren commented that the use of RFID is expected to expand rapidly as the cost of RFID tagging declines, regulatory institutions increasingly approve RFID technology, and companies recognize the potential of RFID to increase efficiency.\n\nB.O.S. clients have included Unilever, Otto, IKEA, and Israel's Ministry of Agriculture. In 2011 B.O.S. was awarded Frost & Sullivan's Entrepreneurial Company of the Year award.\n\n\n\nB.O.S. Better Online Solutions was founded in 1990 by Israel (Izzy) Gal after he left IIS (Intelligent Information Systems, LTD., an Israeli company where Gal worked as a product manager for AS/400 related products. In its early years B.O.S. specialized in twinax terminal emulation software. In 2009 B.O.S. sold the rights to its e-Twinax Controller, a controller providing TCP/IP compatibility with older twinax devices without the use of SNA, to Phoenix-based 10ZiG Technology. In 2010 B.O.S. closed its two U.S. supply chain subsidiaries (formerly Summit Radio Corp.) due to turbulence in the executive jet market.\n\nIn 1996 B.O.S. began trading on what was then the Nasdaq SmallCap Market (today the Nasdaq Capital Market), and in 2000 the company's shares were upgraded to the Nasdaq National Market. Between 2002 and 2009 B.O.S. shares were listed on the Tel Aviv Stock Exchange. In 2009 B.O.S.'s shares were transferred to the Nasdaq Capital Market.\n\nIn 2004 Adiv Baruch replaced founder Israel Gal as CEO of the company. In 2006, Baruch was replaced by Shmuel Koren. Koren resigned in 2008 and was succeeded by Shalom Daskal. Daskal left the company in 2009, his place taken by Yuval Viner. Viner has been CEO of the company since 2009.\n\n\n"}
{"id": "18656877", "url": "https://en.wikipedia.org/wiki?curid=18656877", "title": "Bearing reducer", "text": "Bearing reducer\n\nA Bearing reducer is a bearing that designates the full integration of high-precision reduction gear and high-precision radial-axial bearing in a compact unit. This transmission system allows the utilization of the bearing reducer in several technics, such as robotics and automation, machine tools, measuring equipment, navigation systems, the aircraft industry, the military and medicine field, the woodworking field, the printers branch, the machines for the textile industry and glass treatment, and the filling machines.\n"}
{"id": "23558433", "url": "https://en.wikipedia.org/wiki?curid=23558433", "title": "Bender Machine Works", "text": "Bender Machine Works\n\nThe \"Bender Machine Works\" in Hayward, Wisconsin, is a dairy equipment manufacturer that played a major role in the history of the dairy farming business in the United States from the 1950s to the 1980s, producing milk pipeline and milk transfer cart components, and washing/vacuum-releasing equipment.\n\nThe company continues to be in business but now focuses its manufacturing on private label electromechanical milk pipeline washing systems.\n\nThe Bender Washer/Releaser was a vacuum-operated device first invented and patented by Lloyd Bender of Hayward, WI, USA in the 1950s, used to both wash dairy milking equipment and to transfer milk from piping containing a vacuum, into a storage tank at normal atmospheric pressure.\n\nAs a non-electric pneumatically operated device, the washer/releaser could be used on small dairy farms without electricity that used an engine to supply milking vacuum, and used well water to cool milk.\n\nThe Washer/Releaser could be used in association with both dairy pipelines and the \"Step-Saver\", a milk transfer cart used with bucket milkers. The Step-Saver and the Washer/Releaser were once extremely common in the 1960s to 1980s on small family dairy farms in the United States, to reduce the labor of bucket milking before the milk pipeline became more popular.\n\"(Expanded topic section from Dairy farming article.)\"\n\nAs barns began to increase in size from perhaps 6 to 12 cows to 30 or 40 cows, the bucket milker became a very laborious milking system. As the barn length increased, the farmer had to walk an increasing distance from the cow to the milk bulk tank to dump the collected milk. An early vacuum milk-transport system known as the Step-Saver was developed to save the farmer the trouble of carrying the heavy steel buckets of milk all the way back to the storage tank in the milkhouse. The system used a very long vacuum hose coiled around a receiver cart, and connected to a vacuum-breaker device in the milkhouse.\n\nFollowing milking each cow, the bucket milker would be dumped into the receiver cart. A foot pedal on the base of the cart lifted the cover, which kept contaminating dust and debris out of the cart, and allowed the farmer to hold the heavy bucket milker with both hands while pouring. A diffuser plate in the top of the cart prevented milk from splashing out while rapidly pouring the milk, and a large filter disk under the diffuser removed any debris from the milk.\n\nMilk collected in a chamber below the filter, and was slowly sucked through the long hose to the milkhouse. When empty, a large float ball in the bottom of the cart would settle down over the drain hole to seal the line and retain system vacuum. When milk was poured into the cart, the ball would float up, unsealing the drain.\n\nAn automatic vacuum breaker in the milkhouse cyclically pulled milk from the cart into a glass jar using system vacuum, followed by a release of vacuum to atmospheric pressure, allowing the milk to flow into the bulk tank by gravity flow. When the float level in the jar dropped to setpoint, system vacuum was reapplied to restart the process. Check valves on the vacuum breaker milk hose prevented milk from flowing backwards to the cart when the jar vacuum was released.\n\nAs the farmer milked the cows in series, the cart would be rolled further down the center aisle, the long milk hose unwrapped from the cart, and hung on open hooks along the ceiling of the aisle.\n\nUse of the step saver and similar non-pipeline milk transfer systems is described in the legislative code of the Wisconsin Department of Agriculture, Trade, and Consumer Protection (DATCP).\n\nThe Bender Machine Works also developed some of the first automated \"wash-in-place\" milk pipeline systems, which is generally constructed similar to an automatic dishwasher with automated fill, drain, soap dispensing, and various wash cycles. They continue to manufacture these washing systems today.\n\n\n"}
{"id": "27739462", "url": "https://en.wikipedia.org/wiki?curid=27739462", "title": "Centre for Cellular and Molecular Platforms", "text": "Centre for Cellular and Molecular Platforms\n\nThe Centre for Cellular and Molecular Platforms (C-CAMP) is an initiative of the Department of Biotechnology, Ministry of Science, Technology and Earth Sciences, Govt. of India. Established in 2009 with a mandate to enable cutting edge Life Sciences Research and Innovation, C-CAMP is the country’s most exciting life sciences innovation hub bringing together academia, industry and the startup ecosystem - all on one platform. It is also a part of the one of the country’s earliest Bio clusters – the Bangalore Life Sciences Cluster (BliSc) - comprising National Centre for Biological Sciences (NCBS), Institute for Stem Cell Biology and Regenerative Medicine (inStem) and C-CAMP. \n\nC-CAMP is envisioned to be an enabler for cutting-edge Bioscience research and innovation through its two major arms: services in state-of-the-art platform technologies and facilitation of Life Sciences innovation and entrepreneurship. It acts as the provider and developer of high-end technologies in life sciences to enable scientific activity and entrepreneurship. C-CAMP allows investigators to use techniques as tools and not be limited by technological barriers while pursuing challenging scientific questions. 250+ institutes and organisations from all across India have accessed its technology platforms. Altogether C-CAMP supported publications now number 100+. \nFrom lab-space and high-end technical resources, to active involvement in Seed Funding schemes, scientific and business Mentorship and Accelerator programs, it provides a comprehensive, all-round support system to bio entrepreneurs with exciting deep-science ideas.\nC-CAMP has directly funded, incubated and mentored over 100 start-ups over the last few years and is connected to about 500 startups and entrepreneurs across the country. \n\nC-CAMP is mandated to provide high-end platform technologies and expertise to all the academic and industrial scientific researchers in and around India. Some functions provided by its facilities include:\n\nAs a part of C-CAMP’s core mandate to nurture entrepreneurs in Biosciences across the country, it supports and handholds innovators on their journey to translate a bench-level idea into a full-fledged profit-making business venture. The support facilities C-CAMP provides are:\n\n"}
{"id": "6717182", "url": "https://en.wikipedia.org/wiki?curid=6717182", "title": "Ceramic glaze", "text": "Ceramic glaze\n\nCeramic glaze is an impervious layer or coating of a vitreous substance which has been fused to a ceramic body through firing. Glaze can serve to color, decorate or waterproof an item. Glazing renders earthenware vessels suitable for holding liquids, sealing the inherent porosity of unglazed biscuit earthenware. It also gives a tougher surface. Glaze is also used on stoneware and porcelain. In addition to their functionality, glazes can form a variety of surface finishes, including degrees of glossy or matte finish and color. Glazes may also enhance the underlying design or texture either unmodified or inscribed, carved or painted.\n\nMost pottery produced in recent centuries has been glazed, other than pieces in unglazed terracotta, biscuit porcelain or some other types. Tiles are almost always glazed on the surface face, and modern architectural terracotta is very often glazed. Glazed brick is also common. Domestic sanitary ware is invariably glazed, as are many ceramics used in industry, for example ceramic insulators for overhead power lines.\n\nThe most important groups of traditional glazes, named after their main ceramic fluxing agent, are:\n\n\nModern materials technology has invented new vitreous glazes that do not fall into these traditional categories.\n\nGlazes need to include a ceramic flux which functions by promoting partial liquefaction in the clay bodies and the other glaze materials. Fluxes lower the high melting point of the glass formers silica, and sometimes boron trioxide. These glass formers may be included in the glaze materials, or may be drawn from the clay beneath. \nRaw materials of ceramic glazes generally include silica, which will be the main glass former. Various metal oxides, such as sodium, potassium, and calcium, act as flux and therefore lower the melting temperature. Alumina, often derived from clay, stiffens the molten glaze to prevent it from running off the piece. Colorants, such as iron oxide, copper carbonate, or cobalt carbonate, and sometimes opacifiers like tin oxide or zirconium oxide, are used to modify the visual appearance of the fired glaze.\n\nGlaze may be applied by dry-dusting a dry mixture over the surface of the clay body or by inserting salt or soda into the kiln at high temperatures to create an atmosphere rich in sodium vapor that interacts with the aluminium and silica oxides in the body to form and deposit glass, producing what is known as salt glaze pottery. Most commonly, glazes in aqueous suspension of various powdered minerals and metal oxides are applied by dipping pieces directly into the glaze. Other techniques include pouring the glaze over the piece, spraying it onto the piece with an airbrush or similar tool, or applying it directly with a brush or other tool.\n\nTo prevent the glazed article from sticking to the kiln during firing, either a small part of the item is left unglazed, or it's supported on small refractory supports such as kiln spurs and Stilts that are removed and discarded after the firing. Small marks left by these spurs are sometimes visible on finished ware.\n\nDecoration applied under the glaze on pottery is generally referred to as underglaze. Underglazes are applied to the surface of the pottery, which can be either raw, \"greenware\", or \"biscuit\"-fired (an initial firing of some articles before the glazing and re-firing). A wet glaze—usually transparent—is applied over the decoration. The pigment fuses with the glaze, and appears to be underneath a layer of clear glaze. An example of underglaze decoration is the well-known \"blue and white\" porcelain famously produced in England, the Netherlands, China, and Japan. The striking blue color uses cobalt as cobalt oxide or cobalt carbonate.\nDecoration applied on top of a layer of glaze is referred to as overglaze. Overglaze methods include applying one or more layers or coats of glaze on a piece of pottery or by applying a non-glaze substance such as enamel or metals (e.g., gold leaf) over the glaze.\n\nOverglaze colors are low-temperature glazes that give ceramics a more decorative, glassy look. A piece is fired first, this initial firing being called the glost firing, then the overglaze decoration is applied, and it is fired again. Once the piece is fired and comes out of the kiln, its texture is smoother due to the glaze.\n\nHistorically, glazing of ceramics developed rather slowly, as appropriate materials needed to be discovered, and also firing technology able to reliably reach the necessary temperatures was needed.\n\nGlazed brick goes back to the Elamite Temple at Chogha Zanbil, dated to the 13th century BC. The Iron Pagoda, built in 1049 in Kaifeng, China, of glazed bricks is a well-known later example.\n\nLead glazed earthenware was probably made in China during the Warring States Period (475 – 221 BCE), and its production increased during the Han Dynasty. High temperature proto-celadon glazed stoneware was made earlier than glazed earthenware, since the Shang Dynasty (1600 – 1046 BCE). \n\nDuring the Kofun period of Japan, Sue ware was decorated with greenish natural ash glazes. From 552 to 794 AD, differently colored glazes were introduced. The three colored glazes of the Tang Dynasty were frequently used for a period, but were gradually phased out; the precise colors and compositions of the glazes have not been recovered. Natural ash glaze, however, was commonly used throughout the country.\n\nIn the 13th century, flower designs were painted with red, blue, green, yellow and black overglazes. Overglazes became very popular because of the particular look they gave ceramics.\n\nFrom the eighth century, the use of glazed ceramics was prevalent in Islamic art and Islamic pottery, usually in the form of elaborate pottery. Tin-opacified glazing was one of the earliest new technologies developed by the Islamic potters. The first Islamic opaque glazes can be found as blue-painted ware in Basra, dating to around the 8th century. Another significant contribution was the development of stoneware, originating from 9th century Iraq. Other centers for innovative ceramic pottery in the Islamic world included Fustat (from 975 to 1075), Damascus (from 1100 to around 1600) and Tabriz (from 1470 to 1550).\n\nAs of 2012, over 650 ceramic manufacturing establishments were reported in the United States. Floor tile, wall tile, sanitary-ware, bathroom accessories, kitchenware, and tableware are all potential ceramic-containing products that are available for consumers. Heavy metals are dense metals used in glazes to produce a particular color or texture. Glaze components are more likely to be leached into the environment when non-recycled ceramic products are exposed to warm or acidic water. Leaching of heavy metals occurs when ceramic products are glazed incorrectly or damaged. Lead and chromium are two heavy metals commonly used in ceramic glazes that are heavily monitored by government agencies due to their toxicity and ability to bioaccumulate.\n\nMetals used in ceramic glazes are typically in the form of metal oxides.\n\nCeramic manufacturers primarily use lead(II) oxide (PbO) as a flux for its low melting range, wide firing range, low surface tension, high index of refraction, and resistance to devitrification. \n\nIn polluted environments, nitrogen dioxide reacts with water () to produce nitrous acid () and nitric acid ().\n\nSoluble Lead(II) nitrate () forms when lead(II) oxide (PbO) of leaded glazes is exposed to nitric acid () \n\nPbO + 2 → + \n\nBecause lead exposure is strongly linked to a variety of health problems, collectively referred to as lead poisoning, the disposal of leaded glass (chiefly in the form of discarded CRT displays) and lead-glazed ceramics is subject to toxic waste regulations.\n\nChromium(III) oxide () is used as a colorant in ceramic glazes. Chromium(III) oxide can undergo a reaction with calcium oxide (CaO) and atmospheric oxygen in temperatures reached by a kiln to produce calcium chromate (). The oxidation reaction changes chromium from its +3 oxidation state to its +6 oxidation state. Chromium(VI) is very soluble and the most mobile out of all the other stable forms of chromium.\n\nChromium may enter water systems via industrial discharge. Chromium(VI) can enter the environment directly or oxidants present in soils can react with chromium(III) to produce chromium(VI). Plants have reduced amounts of chlorophyll when grown in the presence of chromium(VI).\n\nChromium oxidation during manufacturing processes can be reduced with the introduction of compounds that bind to calcium. Ceramic industries are reluctant to use lead alternatives since leaded glazes provide products with a brilliant shine and smooth surface. The United States Environmental Protection Agency has experimented with a dual glaze, barium alternative to lead, but they were unsuccessful in achieving the same optical effect as leaded glazes.\n\n"}
{"id": "55588470", "url": "https://en.wikipedia.org/wiki?curid=55588470", "title": "Chondrometer", "text": "Chondrometer\n\nA chondrometer is a measuring instrument designed to determine the bulk density of grain. Grain density is measured in kilograms per hectolitre (Imp. pounds per bushel). It is thus also referred to as the hectolitre mass.\n\nDensity is a guide to wheat quality and determines the price and the space required to store and transport the crop.\n\nA chondrometer consists of a filling hopper, a measuring container, a straightedge, a weighing instrument. The filling hopper allows the grain to fall into the measuring container in a reproducible pattern as several measurements are taken, and all readings need to be within a strict degree of accuracy. The measuring cylinder has flat top edge to it can levelled using the straightedge (a strickle) to give a set volume. Today, the measuring instrument can be a set of digital scales with an accuracy greater than 0.1 g, though in the past it was a steelyard balance with the measuring cylinder hooking directly onto the scales.\n\n\n"}
{"id": "9891243", "url": "https://en.wikipedia.org/wiki?curid=9891243", "title": "Climbing formwork", "text": "Climbing formwork\n\n<section begin=cf defn />Climbing formwork is a special type formwork for vertical concrete structures that rises with the building process. While relatively complicated and costly, it can be an effective solution for buildings that are either very repetitive in form (such as towers or skyscrapers) or that require a seamless wall structure (using gliding formwork, a special type of climbing formwork).\n\nVarious types of climbing formwork exist, which are either relocated from time to time, or can even move on their own (usually on hydraulic jacks, required for self-climbing and gliding formworks).<section end=cf defn />\n\nBest known in the construction of towers, skyscrapers and other tall vertical structures, it allows the reuse of the same formwork over and over and over for identical (or very similar) sections / stories further up the structure. It can also enable very large concrete structures to be constructed in one single pour (which may take days or weeks as the formwork rises with the process), thus creating seamless structures with enhanced strength and visual appearance, as well as reducing construction times and material costs (at the joints which would otherwise require extra reinforcement / connectors).\n\nThe climbing formwork structure normally does not only contain the formwork itself, but also usually provides working space / scaffolds for construction crews. It may also provide areas for machinery and screens for weather protection, up to being fully enclosed while yet staying modular around a changing building structure.\n\n\n\n"}
{"id": "11437272", "url": "https://en.wikipedia.org/wiki?curid=11437272", "title": "Compact Video Cassette", "text": "Compact Video Cassette\n\nCompact Video Cassette (CVC) was one of the first analog recording videocassette formats to use a tape smaller than its earlier predecessors of VHS and Betamax, and was developed by Funai Electronics of Japan for portable use. The first model of VCR for the format was the Model 212, introduced in 1980 by both Funai and Technicolor as they had created a joint venture to manufacture and introduce the format to the home movie market. The system, which included the VCR and a hand held video camera, was very small and lightweight for its time.\n\nThe CVC format used a cassette slightly larger than an audio cassette (approximately 105x66x13mm) and was loaded with 6.5mm magnetic tape. Unlike most other video cassette formats that have two spools of fixed diameter, being so small in size CVC's were designed to operate in a manner similar to a standard audio cassette: as the magnetic tape vacated one spool it would pass over the head of the player and be fed to the second spool of the cassette filling it out. Initially only V30 tapes were available which ran for 30 minutes, then later V45 (45 minute) and V60 (60 minute) models were introduced. The format was released for NTSC, PAL and SECAM television systems (with cassettes labelled \"VExx\") and, like most analogue systems, tapes had to be played on machines using the same TV system as the recording.\n\nFunai 212 came with a JVC model GX-44E hand held Vidicon tube camera with a zoom lens. Model 212D was the NTSC version and 212E was PAL for Europe. The deck and electronics from the 212 were also used to build the model 335 Technicolor Video Showcase, which included a colour video monitor, speaker and contained an internal 12V battery. A lightweight television tuner pack was available to enable the 212 to record off-air television programming, but since it contained no timer it was not possible to set it for unattended recordings. Grundig also produced a CVC-format VCR for the PAL market, the VP100, based on the 212E but smaller. The VP100 weighed only 2.3 kg with battery, and had a separate power pack. Model 212 was also available in France as a SECAM recorder, the variant letter for this model is unknown. SECAM tapes play in monochrome on PAL players.\n\n\nTechnicolor hoped that CVC would compete with 8mm film, but the Vidicon tube used for the bundled camera had poor low-light sensitivity, limiting its usefulness for home indoor use. An even worse attribute of the cassettes was the low quality of the tape stock which was prone to dropouts (appearing as lines of white snow) during video playback. These dropouts would show much more prominently than on wider tape formats. A drawback of the CVC player resulted in the mechanism's loading ring frequently failing to complete its intended travel as the decks aged. The load ring failure would render the unit unusable.\n\n"}
{"id": "1152274", "url": "https://en.wikipedia.org/wiki?curid=1152274", "title": "Cultural industry", "text": "Cultural industry\n\nAccording to international organizations such as UNESCO and the General Agreement on Tariffs and Trade (GATT), cultural industries (sometimes also known as \"creative industries\") combine the creation, production, and distribution of goods and services that are cultural in nature and usually protected by intellectual property rights.\n\nThe notion of cultural industries generally includes textual, music, television, and film production and publishing, as well as crafts and design. For some countries, architecture, the visual and performing arts, sport, advertising, and cultural tourism may be included as adding value to the content and generating values for individuals and societies. They are knowledge-based and labour-intensive, creating employment and wealth. By nurturing creativity and fostering innovation societies will maintain cultural diversity and enhance economic performance.\n\nCultural industries worldwide have adapted To The New digital technologies and to the arrival of national, regional and international (de)regulatory policies. These factors have radically altered the context in which cultural goods, services, and investments flow between countries and, consequently, these industries have undergone a process of internationalization and progressive concentration, resulting in the formation of a few big conglomerates: a new global oligopoly.\n\n\n1. \"Exploring The Cultural and Creative Industries Debate\". Culture Action Europe. Retrieved 2013-07-07.\n[nonexistent/incorrect reference]\n"}
{"id": "35221697", "url": "https://en.wikipedia.org/wiki?curid=35221697", "title": "Dalian Hi-Tech Zone", "text": "Dalian Hi-Tech Zone\n\nDalian Hi-tech Zone ( or ) or DHZ is an industrial district in the western suburb of Dalian City, Liaoning Province, China. It spreads about 30 kilometers along Lushun South Road and Guoshui Highway in Shahekou District and Lushunkou District, where the world's high technology companies and organizations are situated.\n\nThe construction of DHZ started in 1991. Dalian Software Park was added in 1998. The second phase of Dalian Software Park was kicked off in 2003 at the site of Dalian Ascendas IT Park, which officially opened in 2007. DHZ is often called \"Lushun South Road Software Industry Belt\".\n\nThe whole area of Dalian Hi-tech Zone is under the control of Dalian Hi-tech Industrial Zone Administrative Committee, but all parks therein, except Dalian Hi-tech Zone and Animation Industry Base, written below, are managed by private enterprises. They are from east to west:\nShahekou District\nAlong Lushun South Road:\n\n\nNow being reclaimed from the Yellow Sea\n\n\nFor example, Crystal Digital Technology (), etc.\n\n\n\nLushunkou District\n\n\nUnder construction\nAlong GuoShui Highway (Guojia - Shuishiying Villages):\n\nUnder construction\n\n\nThere are universities and research centers in this area, where about half of all the universities of Dalian are located. From east to west are:\n\n\n\n\n\n\nIn Dalian City, there are:\n\n\nThere are other parks, often called the \"hi-tech zones\" of Dalian.\n\nDalian BEST City () is located along Lushun North Road.\n\nNew Jinzhou District including Dalian Development Area has DD Port and other hi-tech areas.\n\n\n"}
{"id": "3615684", "url": "https://en.wikipedia.org/wiki?curid=3615684", "title": "Diagrid", "text": "Diagrid\n\nThe diagrid (a portmanteau of diagonal grid) is a framework of diagonally intersecting metal, concrete or wooden beams that is used in the construction of buildings and roofs. It requires less structural steel than a conventional steel frame. Hearst Tower in New York City, designed by Norman, Lord Foster, uses 21 percent less steel than a standard design. The diagrid obviates the need for columns and can be used to make large column-free expanses of roofing. Another iconic building designed by Lord Foster, 30 St Mary Axe, known as \"The Gherkin\", also uses the diagrid system.\n\nBritish architect Ian Ritchie wrote in 2012: \n\n\n"}
{"id": "25854285", "url": "https://en.wikipedia.org/wiki?curid=25854285", "title": "Digifold", "text": "Digifold\n\nDigifold is a new generation of four and six corner folding device for box gluers and associated machinery. Development began in the late 1990s, and was first exhibited at IPEX 2002 trade show. A system using Siemens S7-200 PLC and Siemens Servo controllers was developed throughout the late 1990s, into the year 2000. It was the first system, in the box folding industry, to use carbon fibre as a construction material (used in the drive shafts), and advanced aluminium (aluminum) alloys in the clamping devices. Although it was not the first servo driven backfolding system, it used advanced materials to improve speed and reduce cost over comparable systems (notably from Jagenberg, Bobst and others)\n\n\n"}
{"id": "35737744", "url": "https://en.wikipedia.org/wiki?curid=35737744", "title": "ELM327", "text": "ELM327\n\nThe ELM327 is a programmed microcontroller produced by ELM Electronics for translating the on-board diagnostics (OBD) interface found in most modern cars. The ELM327 command protocol is one of the most popular PC-to-OBD interface standards and is also implemented by other vendors.\n\nThe original ELM327 is implemented on the PIC18F2480 microcontroller from Microchip Technology.\n\nELM327 is one of a family of OBD translators from ELM Electronics. Other variants implement only a subset of the OBD protocols.\n\nThe ELM327 abstracts the low-level protocol and presents a simple interface that can be called via a UART, typically by a hand-held diagnostic tool or a computer program connected by USB, RS-232, Bluetooth or Wi-Fi. New applications include smartphones.\n\nThere are a large number of programs available that connect to the ELM327.\n\nThe function of such software may include\n\nELM327 Functions\n\n\nThe protocols supported by ELM327 are:\n\n\nThe ELM327 command set is similar to the Hayes AT commands.\n\nThe ELM327 is a PIC microcontroller that has been customized with ELM Electronics' proprietary code that implements the testing protocols. When ELM Electronics sold version 1.0 of its ELM327, it did not enable the copy protection feature of the PIC microcontroller. Consequently, anyone could buy a genuine ELM327, and read ELM's proprietary binary microcontroller software using a device programmer. With this software, pirates could trivially produce ELM327 clones by purchasing the same microcontroller chips and programming them with the copied code. ELM327 clones were widely sold in devices claiming to contain an ELM327 device, and problems have been reported with the clones. The problems reflect bugs that were present in ELM's version 1.0 microcode; those making the clones may continue to sell the old version.\n\nAlthough these pirate clones may contain the ELM327 v1.0 code, they may falsely report the version number as the current version provided by the genuine ELM327, and in some cases report an as-yet non-existent version. Released software versions for the ELM327 are 1.0, 1.3a, 1.4b, 2.1 and 2.2 only. The actual function of these pirate clones is nonetheless limited to the function of the original ELM327 v1.0, with inherent deficiencies.\n\nMost of the clones are reporting [ELM327 v1.5], despite the fact that Elm Electronics does not have a version 1.5.\n\nWe note that a free Android app \"ELM327 Identifier\" tests 103 different AT codes and uses this information to determine which version of the ELM327 chip is actually present (or is being emulated by a pirate chip). Most of the pirated adapters that claim to be \"v.1.5\" or \"v.2.1\" actually fail most of the AT codes beyond v.1.4 and will not execute codes characteristic of ELM327 v.1.4b, 2.0, 2.1 or 2.2. Further, many of the pirated clone adapters do not even have OBDII port pins 2 and 10 connected, rendering them incapable of connecting to the ECU of many Ford and GM vehicles using the SAE J1850-PWM or -VPW protocols.\n\n\n"}
{"id": "29324444", "url": "https://en.wikipedia.org/wiki?curid=29324444", "title": "Engineering Code Snippets Project", "text": "Engineering Code Snippets Project\n\nECS (Engineering Code Snippets) is an open-source project for engineering software codes and programs, developed at the Katholieke Universiteit Leuven. ECS was started in order to provide code snippets, and complete programs which can prove useful to engineers, scientists and technologists worldwide. Code snippets in different platforms such as C, C++, Java, MATLAB, PHP, C#, and HTML are being made available through this project. All submitted codes are reviewed to ensure that they run without errors, and only then, they are published. The project uses a unique tagging feature for each code snippet which include a title, brief description about the code, specific instructions for running the code, and the creation date.\n\nThe web is cluttered with snippets of code, which often serve no meaningful purpose. This makes the life of a programmer very difficult, with unreliable codes that do not compile being inserted into large software projects resulting in bugs that go undetected until reported by users. As such, this project aims to provide a comprehensive framework where code snippets are well categorized, and serve the specific function of engineering projects. The attempt made in this project is to be able to organize code snippets by category, and provide an open source framework, which makes it easy for programmers to access a growing, uncluttered database online and use it for the purposes of their own software development efforts. This open-source resource also aims to facilitate the ease of use of snippets for the purposes of education of engineering principles, by providing examples that fit the curricula of engineering schools.\n\nThe foundations of this project were laid in November 2009, and an intranet effort was launched as a start up point. Later this project was moved to the world wide web to facilitate global access. The project pages are licensed with a Creative Commons Attribution license, and the project aims to fulfill the GNU Project aims of freedom to run the program, freedom to access the code, freedom to redistribute the program to anyone, and freedom to improve the software.\n\n"}
{"id": "24355528", "url": "https://en.wikipedia.org/wiki?curid=24355528", "title": "Fracturing Responsibility and Awareness of Chemicals Act", "text": "Fracturing Responsibility and Awareness of Chemicals Act\n\nThe Fracturing Responsibility and Awareness of Chemicals Act (, , dubbed as the FRAC Act) is a legislative proposal in the United States Congress to define hydraulic fracturing as a federally regulated activity under the Safe Drinking Water Act. The proposed act would require the energy industry to disclose the chemical additives used in the hydraulic fracturing fluid. The gas industry opposes the legislation.\n\nThe bill was introduced to both houses of the 111th United States Congress on June 9, 2009. The House bill was introduced by representatives Diana DeGette, D-Colo., Maurice Hinchey D-N.Y., and Jared Polis, D-Colo. The Senate version was introduced by senators Bob Casey, D-Pa., and Chuck Schumer, D-N.Y. The bill was re-introduced to both houses of the 112th United States Congress on March 15, 2011, by representative Diana DeGette and senator Bob Casey.\n\nThe Environmental Protection Agency (EPA) blames the lack of information about the contents of hydraulic fracturing fluid on the 2005 Energy Policy Act because it exempts hydraulic fracturing from federal water laws. The Act calls for the \"chemical constituents (but not the proprietary chemical formulas) used in the fracturing process.\" Once these constituents are determined the information must be revealed to the public through the Internet. The FRAC Act states that in any case where a physician or the State finds that a medical emergency exists, and that the chemical formulas are needed to treat the ailing individual, the firm must disclose the chemical identity to the State or physician—even if that proprietary formula is a trade-secret chemical. Material Safety Data Sheets, required by OSHA under 29 CFR 1910.1200 are developed and made available to first responders and other emergency planning and response officials.\n\nThe drilling industry does not agree with this pending policy. They see it as \"an additional layer of regulation that is unneeded and cumbersome.\" The Independent Petroleum Association of America believes that states already sufficiently regulate hydraulic fracturing. Their research suggests that federal regulation could result in the addition of about $100,000 to each new natural gas well. Energy in Depth, a lobbying group, says the new regulation would be an \"unnecessary financial burden on a single small-business industry, American oil, and natural gas producers.\" This group also claims that the FRAC Act could result in half of the United States oil wells and one third of the gas wells being closed. Also, the bill could cause domestic gas production to drop by 245 billion cubic feet per year along with four billion dollars in lost revenue to the federal government. The Environmental Protection Agency claims that the Safe Drinking Water Act is flexible in that it defers regulation of fracturing and drilling to the state. According to an industry-funded study, since most states currently have regulations on fracturing, they would most likely agree with the state's policy and there would not be much change.\n\nThe 111th United States Congress adjourned on January 3, 2011, without taking any significant action on the FRAC Act. The FRAC Act was re-introduced in both houses of the 112th United States Congress. In the Senate, Sen. Bob Casey (D-PA) introduced S. 587 on March 15, 2011. In the House, Rep. Diana DeGette (D-CO) introduced H.R. 1084 on March 24, 2011. The FRAC Act was reintroduced by Senator Casey to the 113th United States Congress as S. 1135 on Jun 11, 2013 and again as S. 785 on March 18, 2015 to the 114th United States Congress\n\nThe current version was introduced to the 115th United States Congress by Senator Casey on April 6, 2017 as S. 865. It was read twice and referred to the Committee on Environment and Public Works where it awaits discussion.\n\n\n"}
{"id": "68869", "url": "https://en.wikipedia.org/wiki?curid=68869", "title": "Fruit press", "text": "Fruit press\n\nA fruit press is a device used to separate fruit solids - stems, skins, seeds, pulp, leaves, and detritus - from fruit juice.\n\nIn the United States, Madeline Turner invented the Turner's Fruit-Press, in 1916.\n\nA cider press is used to crush apples or pears. In North America, the unfiltered juice is referred to as cider, becoming known as apple juice once filtered; in Britain it is referred to as juice regardless of whether it is filtered or not (the term cider is reserved for the fermented (alcoholic) juice). Other products include cider vinegar, (hard) cider, apple wine, apple brandy, and apple jack.\n\nThe traditional cider press is a ram press. Apples are ground up and placed in a cylinder, and a piston exerts pressure. The cylinder and/or piston is \"leaky\" and the juice is forced from the solids. The traditional cider press has not changed much since the early modern period. The only difference being that in earlier versions of the press horses were used to power the machine. Diderot's Encyclopedie offers a portrayal of the traditional cider press,\n\n\"This is how the cider mill is made. Imagine a circular trough made of wood connected to two wooden millstones like those used in a windmill, but fixed differently. In a windmill, they are horizontal, but in the cider mill they are placed in the trough vertically. They are fixed to a vertical piece of wood that turns on itself and which is placed in the centre of the circular part of the trough; a long axle passes through them; the axle is joined to the vertical axis; its other end juts out from the trough; a horse → is harnessed to it; the ← horse → pulls the axle by walking round the trough, which also moves the pressing stones in the trough where the apples are pounded. When they are judged to be sufficiently crushed, that is to say, enough for all the juice to be extracted from them, the apples are removed with a wooden spade and put into a large vat nearby. Enough apples are pounded to make a pulp or pomace.\" \n\nCider presses often have attachments to grind the apples prior to pressing. Such combination devices are commonly referred to as cider mills.\n\nIn communities with many small orchards, it is common for one or more persons to have a large cider mill for community use. These community mills allow orchard owners to avoid the capital, space, and maintenance requirements for having their own mill. These larger mills are typically powered by electrical or gasoline engines. Mill operators also deal with the solids, which attract wasps or hornets. Cider mills typically give patrons a choice between paying by the gallon/litre or splitting the cider with the mill operator.\n\nLarger orchardists may prefer to have their own presses because it saves on fees, or because it reduces cartage. Orchardists of any size may believe their own sanitation practices to be superior to that of community mills, as some patrons of community mills may make cider from low quality fruit (windfall apples, or apples with worms). Those making speciality ciders, such as pear cider, may want to have their own press.\n\nThe world's largest cider press is located in Berne, Indiana USA.\n\nA wine press is a device used to press grapes during wine making.\n\nAn oil press is a device used to extract oil from plants and fruits.\n\nGiven the simplicity of the design, and high usability with some people (e.g. those owning an orchard), some people have started building their own do-it-yourself (DIY) fruit press and have uploaded detailed instructions on how to do so.\n\n\n"}
{"id": "24674696", "url": "https://en.wikipedia.org/wiki?curid=24674696", "title": "G.9972", "text": "G.9972\n\nG.9972 (also known as G.cx) is a Recommendation developed by ITU-T that specifies a coexistence mechanism for networking transceivers capable of operating over electrical power line wiring. It allows G.hn devices to coexist with other devices implementing G.9972 and operating on the same power line wiring.\n\nG.9972 received consent during the meeting of ITU-T Study Group 15, on October 9, 2009, and final approval on June 11, 2010.\n\nG.9972 specifies two mechanisms for coexistence between G.hn home networks and broadband over power lines (BPL) Internet access networks:\n"}
{"id": "382293", "url": "https://en.wikipedia.org/wiki?curid=382293", "title": "George Foreman Grill", "text": "George Foreman Grill\n\nThe George Foreman Lean Mean Fat-Reducing Grilling Machine, commonly known as the George Foreman Grill, is a portable electrically heated grill manufactured by Spectrum Brands. It is promoted by former boxing champion George Foreman. Since its introduction in 1994, over 100 million George Foreman grills have been sold worldwide.\n\nThe concept for the grill was created by Michael Boehm of Batavia, Illinois. The original intention was to create an indoor grill that would provide a unique benefit of cooking on both sides at once. A second key benefit was to reduce the fat content of hamburgers and other meats by draining away the fat into a separate reservoir. Michael Boehm designed the product with a floating hinge and slanted grilling surface to accommodate foods of different thicknesses and drain fat away from the food. Engineering work was done by Bob Johnson. Bob and Mike brought a JVC camcorder and a sample of the product in bright yellow to the office of Barbara Westfield at Salton. The video was played showing fat dripping down from the grill into the collection tray. They presented the product as: THE FAJITA EXPRESS. The fajita grill had been promoted at industry trade shows in the early 1990s, but received little interest.\n\nThe slanted grill concept was pitched by Tsann Kuen to Salton Inc. After 1 year, and several trade shows, Salton sent samples of the grill to George Foreman's colleagues who then sent the grill to Foreman to test out. Boehm was not involved in teaming up the grill and Foreman. Salton made several changes to the technical function of the product including removing the 4 'risers' which Mike and Bob had so that the user could lift up the grill, slide in the tray (two were included) and fill taco shells. One tray was for grease of fajita meat, and the second tray was to hold a taco shell. This is the exact demo shown to Barbara Westfield at the original presentation at the Salton office in Mt. Prospect Illinois.\n\nThe Lean Mean Fat-Reducing Grilling Machine, as it became known, was introduced in 1994 and promoted with distinctive infomercials which featured Foreman. A combination of his affable personality and the unique features of the product made it a huge success. Such was the popularity of these infomercials that Foreman's famous tagline, \"It's so good I put my name on it!\", is now part of popular culture. In Asia, the grill is endorsed and promoted by both George Foreman and Jackie Chan.\n\nThe product has a clamshell design which simultaneously heats the top and bottom surfaces of the food, eliminating the need to flip it. Each heating surface is grooved to reduce contact area, and covered in a non-stick coating. The lower heating surface is angled to allow melted fat and other fluids to drain through the grooves into a removable drip tray, which clearly shows the amount removed from the food. This arrangement has been marketed as a way to \"knock out the fat\", suggesting a healthier way to cook. \n\nThe grill is offered in various sizes for cooking individual or multiple servings. In 2006, the George Foreman \"Next Grilleration\" Health Grill was launched, which features detachable grilling plates for easier cleaning. It has gone through several other versions, differing in their control interfaces and design.\n\nIn 2014, an updated Foreman Grill was released called the \"Evolve Grill\". The Evolve grill features interchangeable ceramic (Teflon-free) plates, in addition to the well known clam-shell design that simultaneously heats the top and bottom surfaces. The plates available are the traditional grill plates, waffle plates, a bake dish, mini burgers, a flat griddle, and a muffin pan.\n\nThe worldwide popularity of the George Foreman grill has resulted in sales of over 100 million units since it was first launched, a feat that was achieved in a little over 15 years. Although Foreman has never confirmed exactly how much he has earned from the endorsement, Salton Inc paid him $138 million in 1999 in order to buy out the right to use his name. Previous to that he was being paid about 40% of the profits on each grill sold (earning him $4.5 million a month in payouts at its peak), so it is estimated he has made a total of over $200 million from the endorsement, a sum that is substantially more than he earned as a boxer.\n\nThe success of the George Foreman Grill spawned a variety of similar celebrity-endorsed products such as the Evander Holyfield Real Deal Grill, for which Holyfield starred in a 2007 infomercial, and the Carl Lewis Health Grill. (The Jackie Chan Grill is the same grill, but targets the Asian market, and is endorsed by both Jackie Chan and George Foreman.) None of these imitators, however, achieved the level of success of the Foreman Grill.\n\nA Play-Doh set was created to look like a George Foreman grill.\n\n\n"}
{"id": "4286922", "url": "https://en.wikipedia.org/wiki?curid=4286922", "title": "Glass-ceramic-to-metal seals", "text": "Glass-ceramic-to-metal seals\n\nGlass-to-metal seals have been around for many years, with one of the most common uses being lamp bulb seals. A more recent invention is glass-ceramic-to-metal seals.\n\n\"Glass-ceramics\" are polycrystalline ceramic materials prepared by the controlled crystallization of suitable glasses, normally silicates. Depending on the starting glass composition and the heat-treatment schedule adopted, glass-ceramics can be prepared with tailored thermal expansion characteristics. This makes them ideal for sealing to a variety of different metals, ranging from low expansion tungsten (W) or molybdenum (Mo) to high expansion stainless steels and nickel-based superalloys.\n\nGlass-ceramic-to-metal seals offer superior properties over their glass equivalents including more refractory behaviour, in addition to their ability to seal successfully to many different metals and alloys. They have been used in electrical feed-through seals for such applications as vacuum interrupter envelopes and pyrotechnic actuators, in addition to many applications where a higher temperature capability than is possible with glass-to-metal seals is required, including solid oxide fuel cells.\n\nIn the formation of a glass-ceramic-to-metal seal, the parts to be joined are first heated, normally under inert atmosphere, in order to melt the glass and allow it to wet and flow into the metal parts, in much the same way as when preparing a more conventional glass-to-metal seal. The temperature is then normally reduced into a temperature regime where many microscopic nuclei are formed in the glass. The temperature is then raised again into a regime where the major crystalline phases can form and grow to create the polycrystalline ceramic material with thermal expansion characteristics matched to that of the particular metal parts.\n\nThe white opaque \"glue\" between the panel and the funnel of a colour TV cathode ray tube is a\ndevitrified solder glass based on the system --. While\nthis is a glass-ceramic-to-glass seal, the basic patent of S.A. Claypoole considers\nglass-ceramic-to-metal seals as well.\n\n"}
{"id": "24246419", "url": "https://en.wikipedia.org/wiki?curid=24246419", "title": "Global Mapper", "text": "Global Mapper\n\nGlobal Mapper is a geographic information system (GIS) software package currently developed by Blue Marble Geographics that runs on Microsoft Windows. The GIS software competes with ESRI, GeoMedia, Manifold System, and MapInfo GIS products. Global Mapper handles both vector, raster, and elevation data, and provides viewing, conversion, and other general GIS features. Global Mapper has an active user community with a mailing list and online forums.\n\nIn 1995 the USGS was in need of a Windows viewer for their data products, so they developed the dlgv32 application for viewing their DLG (Digital Line Graph) vector data products. Between 1995 and 1998 the dlgv32 application was expanded to include support for viewing other USGS data products, including DRG (topographic maps) and DEM (digital elevation model) and SDTS-DLG and SDTS-DEM data products. The development process is described in detail in the USGS paper titled 'A Programming Exercise'.\n\nIn 1998 the USGS released the source code for dlgv32 v3.7 to the public domain. In 2001, the source code for dlgv32 was further developed by a private individual into the commercial product dlgv32 Pro v4.0 and offered for sale via the internet. Later that same year the product was renamed to Global Mapper and become a commercial product of the company Global Mapper Software LLC. The USGS was distributing a version of the software under the name dlgv32 Pro (Global Mapper).\n\nOn November 2, 2011 Blue Marble Geographics, at the annual user conference, announced they had purchased Global Mapper LLC.\n\nSince the initial commercialization of the Global Mapper product in 2001, there have been yearly major product releases and numerous intermediate point releases adding additional functionality to the software, the most recent being 19.1 \n\nA mobile version of Global Mapper, Global Mapper Mobile, was released in 2016.\n\n"}
{"id": "44153629", "url": "https://en.wikipedia.org/wiki?curid=44153629", "title": "Gun tunnel", "text": "Gun tunnel\n\nA gun tunnel is an intermediate altitude hypersonic wind tunnel that can be configured to produce hypersonic flows at roughly 30 to 40 km altitude. This uses a piston for isentropic compression. The hypersonic facility at IISC Bangalore, India has a high enthalpy gun tunnel, which can produce Schlieren imaging and produce up to 8 megajoules of energy. Using a piston can be very tricky due to reflecting of shocks. At the facility, they use aluminium diaphragm to produce shocks and paper diaphragm to avoid shocks and pass through the hypersonic chamber. The pressure used is significantly higher like 30 times the atmosphere.\n"}
{"id": "17599399", "url": "https://en.wikipedia.org/wiki?curid=17599399", "title": "Headspace technology", "text": "Headspace technology\n\nHeadspace technology is a technique developed in the 1980s to elucidate the odor compounds present in the air surrounding various objects. Usually the objects of interest are odoriferous objects such as plants, flowers and foods. Similar techniques are also used to analyze the interesting scents of locations and environments such as tea shops and saw mills. After the data is analyzed, the scents can then be recreated by a perfumer.\n\nOne of the early pioneers of this technology includes Roman Kaiser who used it to measure and characterize the scents of tropical rainforest. Headspace techniques have since been used extensively to sample in vivo floral headspace of a large variety of numerous taxa and their aromatic compounds such as fatty acid derivatives (aldehydes, alcohols and ketones), benzenoids and isoprenoids.\n\nThe headspace equipment involves a hollow dome or sphere-like objects which forms an airtight seal and surrounds the objects of interest. Inert gases are passed into the space containing the object or a vacuum is established such that the odor compounds are removed from the headspace. These compounds are in turn captured using a variety of techniques, among them cold surfaces, solvent traps, and adsorbent materials, with the latter techniques capable of longer periods of collection. The samples can then be analyzed using techniques such as gas chromatography, mass spectrometry, or Carbon-13 NMR.\n\nSeveral companies have patented similar headspace technologies:\n"}
{"id": "3021657", "url": "https://en.wikipedia.org/wiki?curid=3021657", "title": "Icemaker", "text": "Icemaker\n\nAn icemaker, ice generator, or ice machine may refer to either a consumer device for making ice, found inside a home freezer; a stand-alone appliance for making ice, or an industrial machine for making ice on a large scale. The term \"ice machine\" usually refers to the stand-alone appliance.\n\nThe \"ice generator\" is the part of the ice machine that actually produces the ice. This would include the evaporator and any associated drives/controls/subframe that are directly involved with making and ejecting the ice into storage. When most people refer to an ice generator, they mean this ice-making subsystem alone, minus refrigeration.\n\nAn \"ice machine\", however, particularly if described as 'packaged', would typically be a complete machine including refrigeration, controls, and dispenser, requiring only connection to power and water supplies.\n\nThe term \"icemaker\" is more ambiguous, with some manufacturers describing their packaged ice machine as an icemaker, while others describe their generators in this way.\n\nIn 1748, the first known artificial refrigeration was demonstrated by William Cullen at the University of Glasgow. Mr. Cullen never used his discovery for any practical purposes. This may be the reason why the history of the icemakers begins with Oliver Evans, an American inventor who designed the first refrigeration machine in 1805. In 1834, Jacob Perkins built the first practical refrigerating machine using ether in a vapor compression cycle. The American inventor, mechanical engineer and physicist received 21 American and 19 English patents (for innovations in steam engines, the printing industry and gun manufacturing among others) and is considered today the father of the refrigerator.\n\nIn 1844, an American physician, John Gorrie, built a refrigerator based on Oliver Evans' design to make ice to cool the air for his yellow fever patients. His plans date back to 1842, making him one of the founding fathers of the refrigerator. Unfortunately for John Gorrie, his plans of manufacturing and selling his invention were met with fierce opposition by Frederic Tudor, the Boston “Ice King”. By then, Tudor was shipping ice from the United States to Cuba and was planning to expand his business to India. Fearing that Gorrie’s invention would ruin his business, he began a smearing campaign against the inventor. In 1851, John Gorrie was awarded \"U.S. Patent 8080\" for an ice machine. After struggling with Tudor's campaign and the death of his partner, John Gorrie also passed away, bankrupt and humiliated. His original icemaker plans and the prototype machine are held today at the National Museum of American History, Smithsonian Institution in Washington, D.C.\n\nIn 1853, Alexander Twining was awarded \"U.S. Patent 10221\" for an icemaker. Twining’s experiments led to the development of the first commercial refrigeration system, built in 1856. He also established the first artificial method of producing ice. Just like Perkins before him, James Harrison started experimenting with ether vapor compression. In 1854, James Harrison successfully built a refrigeration machine capable of producing 3,000 kilograms of ice per day and in 1855 he received an icemaker patent in Australia, similar to that of Alexander Twining. Harrison continued his experiments with refrigeration. Today he is credited for his major contributions to the development of modern cooling system designs and functionality strategies. These systems were later used to ship refrigerated meat across the globe. \n\nIn 1867, Andrew Muhl built an ice-making machine in San Antonio, Texas, to help service the expanding beef industry before moving it to Waco in 1871. In 1873, the patent for this machine was contracted by the Columbus Iron Works, which produced the world's first commercial icemakers. William Riley Brown served as its president and George Jasper Golden served as its superintendent.\n\nIn 1876, German engineer Carl von Linde patented the process of liquefying gas that would later become an important part of basic refrigeration technology (U.S. Patent 1027862). In 1879 and 1891, two African American inventors patented improved refrigerator designs in the United States (Thomas Elkins – U.S. patent #221222 and respectively John Standard – U.S. patent #455891).\n\nIn 1902, the Teague family of Montgomery purchased control of the firm. Their last advertisement in \"Ice and Refrigeration\" appeared in March 1904. In 1925, controlling interest in the Columbus Iron Works passed from the Teague family to W.C. Bradely of W.C. Bradley, Co.\n\nProfessor Jurgen Hans is credited with the invention of the first ice machine to produce edible ice in 1929. In 1932 he founded a company called Kulinda and started manufacturing edible ice, but by 1949 the business switched its central product from ice to central air conditioning.\n\nThe ice machines from the late 1800s to the 1930s used toxic gases such as ammonia (NH3), methyl chloride (CH3Cl), and sulfur dioxide (SO2) as refrigerants. During the 1920s, several fatal accidents were registered. They were caused by the refrigerators leaking methyl chloride. In the quest of replacing dangerous refrigerants – especially methyl chloride – collaborative research ensued in American corporations. The result of this research was the discovery of Freon. In 1930, General Motors and DuPont formed Kinetic Chemicals to produce Freon, which would later become the standard for almost all consumer and industrial refrigerators. The Freon produced back then was chlorofluorocarbon, a moderately toxic gas causing ozone depletion.\n\nAll refrigeration equipments are made of four key components; the evaporator, the condenser, the compressor and the throttle valve. Ice machines all work the same way. The function of the compressor is to compress low-pressure refrigerant vapor to high-pressure vapor, and deliver it to the condenser. Here, the high-pressure vapor is condensed into high-pressure liquid, and drained out through the throttle valve to become low-pressure liquid. At this point, the liquid is conducted to the evaporator, where heat exchanging occurs, and ice is created. This is one complete refrigeration cycle.\n\nAutomatic icemakers for the home were first offered by the Servel company around 1953. They are usually found inside the freezer compartment of a refrigerator. They produce crescent-shaped ice cubes from a metal mold. An electromechanical or electronic timer first opens a solenoid valve for a few seconds, allowing the mold to fill with water from the domestic cold water supply. The timer then closes the valve and lets the ice freeze for about 30 minutes. Then, the timer turns on a low-power electric heating element inside the mold for several seconds, to melt the ice cubes slightly so they will not stick to the mold. Finally, the timer runs a rotating arm that scoops the ice cubes out of the mold and into a bin, and the cycle repeats. If the bin fills with ice, the ice pushes up a wire arm, which shuts off the icemaker until the ice level in the bin goes down again. The user can also lift up the wire arm at any time to stop the production of ice.\n\nLater automatic icemakers in Samsung refrigerators use a flexible plastic mold. When the ice cubes are frozen, which is sensed by a Thermistor, the timer causes a motor to invert the mold and twist it so that the cubes detach and fall into a bin.\n\nEarly icemakers dropped the ice into a bin in the freezer compartment; the user had to open the freezer door to obtain ice. In 1965, Frigidaire introduced icemakers that dispensed from the front of the freezer door. In these models, pressing a glass against a cradle on the outside of the door runs a motor, which turns an auger in the bin and delivers ice cubes to the glass. Most dispensers can optionally route the ice through a crushing mechanism to deliver crushed ice. Some dispensers can also dispense chilled water.\n\nPortable icemakers are units that can fit on a countertop. They are the fastest and smallest icemakers on the market. The ice produced by a portable icemaker is bullet shaped and has a cloudy, opaque appearance. The first batch of ice can be made within 10 minutes of turning the appliance on and adding water. The water is pumped into a small tube with metal pegs immersed in the water. Because the unit is portable, water must be filled manually. The water is pumped from the bottom of the reservoir to the freeze tray. The pegs use a heating and cooling system inside to freeze the water around them and then heat up so the ice slips off the peg and into the storage bin. Ice begins to form in a matter of minutes, however, the size of ice cubes depends on the freezing cycle - a longer cycle results in thicker cubes. Portable icemakers will not keep the ice from melting, but the appliance will recycle the water to make more ice. Once the storage tray is full, the system will turn off automatically.\n\nBuilt-in icemakers are engineered to fit under a kitchen or bar counter, but they can be used as freestanding units. Some produce crescent-shaped ice like the ice from a freezer icemaker; the ice is cloudy and opaque instead of clear, because the water is frozen faster than in others which are clear cube icemakers. In the process, tiny air bubbles get trapped, causing the cloudy appearance of the ice. However, most under-counter ice makers are clear ice makers which in which the ice is missing the air bubbles, and therefore the ice is clear and melts much slower.\n\nCommercial ice makers improve the quality of ice by using moving water. The water is run down a high nickel content stainless steel evaporator. The surface must be below freezing. Salt water requires lower temperatures to freeze and will last longer. Generally used to package seafood products. Air and undissolved solids will be washed away to such an extent that in horizontal evaporator machines the water has 98% of the solids removed, resulting in very hard, virtually pure, clear ice. In vertical evaporators the ice is softer, more so if there are actual individual cube cells. Commercial ice machines can make different sizes of ice like flakers, crushed, cube, octagon, and tube.\n\nWhen the sheet of ice on the cold surface reaches the desired thickness, the sheet is slid down onto a grid of wires, where the sheet's weight causes it to be broken into the desired shapes, after which it falls into a storage bin.\n\nFlake ice is made of the mixture of brine and water (max salt per ton of water), in some cases can be directly made from brine water. Thickness between , irregular shape with diameters from .\n\nThe evaporator of the flake ice machine is a vertically placed drum-shape stainless steel container, equipped with the rotating blade which spins and scratches the ice off the inner wall of the drum. When operating, the principal shaft and blade spins anti-clockwise pushed by the reducer. Water is sprayed down from the sprinkler; ice is formed from the water brine on the inner wall. The water tray at the bottom catches the cold water while deflecting Ice and re-circulates it back into the sump. The sump will typically use a float valve to fill as needed during production. Flake machines have a tendency to form an ice ring inside the bottom of the drum. Electric heaters are in wells at the very bottom to prevent this accumulation of ice where the crusher does not reach. Some machines use scrapers to assist this. This system utilizes a low temperature condensing unit; like all ice machines. Most manufactures also utilize an E.P.R.V. (Evaporator pressure regulating valve.)\n\nSea water flake ice machine can make ice directly from the sea water. This ice can be used in fast cooling of fish and other sea products. The fishing industry is the largest user of flake ice machines. Flake ice can lower the temperature of cleaning water and sea products, therefore it resists the growth of bacteria and keeps the seafood fresh.\n\nBecause of its large contact and less damage with refrigerated materials, it is also applied in vegetable, fruit and meat storing and transporting.\n\nIn baking, during the mixing of flour and milk, flake ice can be added to prevent the flour from self-raising.\n\nIn most cases of biosynthesis and chemosynthesis, flake ice is used to control the reaction rate and maintain the liveness. Flake ice is sanitary, clean with rapid temperature reduction effect.\n\nFlake ice is used as the direct source of water in concrete cooling process, more than 80% in weight. Concrete will not crack if has been mixed and poured in constant and low temperature.\n\nFlake ice is also used for artificial snowing, so it is widely applied in ski resorts and entertainment park.\n\nCube ice machines are classified as small ice machines, in contrast to tube ice machines, flake ice machines, or other ice machines. Common capacities range from to . Since the emergence of cube ice machines in the 1970s, they have evolved into a diverse family of ice machines.\n\nCube ice machines are commonly seen as vertical modular devices. The upper part is an evaporator, and the lower part is an ice bin. The refrigerant circulates inside pipes of self-contained evaporator, where it conducts the heat exchange with water, and freezes the water into ice cubes. When the water is thoroughly frozen into ice, it is automatically released, and falls into the ice bin.\n\nIce machines can have either a self-contained refrigeration system where the compressor is built into the unit, or a remote refrigeration system where the refrigeration components are located elsewhere, often the roof of the business. \n\nMost compressors are either positive displacement compressors or radial compressors. Positive displacement compressors are currently the most efficient type of compressor, and have the largest refrigerating effect per single unit (). They have a large range of possible power supplies, and can be , , or even higher. The principle behind positive displacement compressors utilizes a turbine to compress refrigerant into high-pressure vapor. Positive displacement compressors are of four main types: screw compressor, rolling piston compressor, reciprocating compressor, and rotary compressor.\n\nScrew compressors can yield the largest refrigerating effect among positive displacement compressors, with their refrigerating capacity normally ranging from to . Screw compressors also can be divided to single-screw type and dual-screw type. Dual-screw type is more often seen in use because it is very efficient.\n\nRolling piston compressors and reciprocating compressors have similar refrigerating effects, and maximum refrigerating effect can reach .\n\nReciprocating compressors are the most common type of compressor because the technology is mature and reliable. Their refrigerating effect ranges from to . They compress gas by utilizing a piston pushed by a crank shaft.\n\nRotary compressors, mainly used in air conditioning equipment, have a very low refrigerating effect, normally not exceeding . They work by compressing gas using a piston pushed by a rotor, which spins in an isolated compartment.\n\nAll condensers can be classified as one of three types: air cooling, water cooling, or evaporative cooling.\n\n\nA tube ice generator is an ice generator in which the water is frozen in tubes that are extended vertically within a surrounding casing—the freezing chamber. At the bottom of the freezing chamber there is a distributor plate having apertures surrounding the tubes and attached to the separate chamber into which a warm gas is passed to heat the tubes and cause the ice rods to slide down.\n\nTube ice can be used in cooling processes, such as temperature controlling, fresh fish freezing, and beverage bottle freezing. It can be consumed alone or with food or beverages.\n\nToday, there are around 2 billion household refrigerators and over 40 million square meters of cold-storage facilities operating worldwide. In the U.S. alone, over 8 million \"refrigerators\" are sold each year. Beyond our homes, restaurants and supermarkets, refrigeration has global applications with positive impact upon the economy, technology, social dynamics, health, and the environment.\n\nThe refrigeration industry employs more than 2 million people worldwide, especially in the service sectors. Refrigeration is necessary for the implementation of many current or future energy sources (hydrogen liquefying for alternative fuels in the automotive industry and thermonuclear fusion production for the alternative energy industries). \n\nIn the food industry, refrigeration contributes to reducing post-harvest losses while supplying safe, foods to consumers by enabling perishable foods to be preserved at all stages from production to consumption by the end-user.\n\nIn the medical sector, refrigeration is used for vaccine, organs, stem cells’ and others’ storage, while cryotechnology is used in surgery and other medical research courses of action.\n\nRefrigeration is used in biodiversity maintenance based on the cryopreservation of genetic resources (cells, tissues, and organs of plants, animals and micro-organisms);\n\nRefrigeration enables the liquefaction of CO2 for underground storage, allowing the potential separation of CO2 from fossil fuels in power stations via cryogenic technology.\n\nAt an environmental level, the impact of refrigeration is due to: atmospheric emissions of refrigerant gases used in refrigerating installations and the energy consumption of these refrigerating installations which contribute to CO2 emissions – and consequently to global warming – thus reducing global energy resources. The atmospheric emissions of refrigerant gases are based on the leaks occurring in insufficiently leak-tight refrigerating installations or during maintenance-related refrigerant-handling processes.\n\nDepending on the refrigerants used, these installations and their subsequent leaks can lead to: ozone depletion (chlorinated refrigerants like CFCs and HCFCs) and/or global warming, by exerting an additional greenhouse effect (fluorinated refrigerants: CFCs, HCFCs and HFCs). From a consumption point of view, it is necessary to remember that households are responsible for about 26.2% of the global energy consumption. While the Montreal Protocol banned the use of CFCs followed by HCFCs, the global efforts aimed at reducing the environmental impact of refrigeration have taken three courses of action: \n\nIn their continuous research of methods to replace ozone-depleting refrigerants and greenhouse refrigerants (CFCs, HCFCs and HFCs, respectively) the scientific community together with the refrigerant industry came up with alternative all-natural refrigerants which are eco-friendly. According to a report issued by the UN Environment Programme, “\"the increase in HFC emissions is projected to offset much of the climate benefit achieved by the earlier reduction in the emissions of Ozone depleting substances\"”. Among non-HFC refrigerants found to successfully replace the traditional ones are ammonia, hydrocarbons and carbon dioxide.\n\nThe history of refrigeration began with the use of ammonia. After more than 120 years, this substance is still the preeminent refrigerant used by household, commercial and industrial refrigeration systems. The major problem with ammonia is its toxicity at relatively low concentrations. On the other hand, ammonia has zero impact on the ozone layer and very low global warming effects. While deaths caused by ammonia exposure are extremely rare, the scientific community has come up with safer and technologically solid mechanisms of preventing ammonia leakage in modern refrigerating equipment. This problem out of the way, ammonia is considered an eco-friendly refrigerant with numerous applications.\n\nCarbon dioxide has been used as a refrigerant for many years. Just like ammonia, it has fallen in almost complete disuse due to its low critical point and its high operating pressure. Carbon dioxide has zero impact on the ozone layer and the global warming effects of the quantities required for use as a refrigerant are also negligible. Modern technology is solving such issues and CO2 is widely used today as an alternative to traditional refrigeration in several fields: industrial refrigeration (CO2 is usually combined with ammonia, either in cascade systems or as a volatile brine), the food industry (food and retail refrigeration), heating (heat pumps) and the transportation industry (transport refrigeration).\n\nHydrocarbons are natural products with high thermodynamic properties, zero ozone-layer impact and negligible global warming effects. One issue with hydrocarbons is that they are highly flammable, restricting their use to specific applications in the refrigeration industry.\n\nIn 2011, the EPA has approved three alternative refrigerants to replace hydrofluorocarbons (HFCs) in commercial and household freezers via the Significant New Alternatives Policy (SNAP) program. The three alternative refrigerants legalized by the EPA were hydrocarbons propane, isobutane and a substance called HCR188C – a hydrocarbon blend (ethane, propane, isobutane and n-butane). HCR188C is used today in commercial refrigeration applications (supermarket refrigerators, stand-alone refrigerators and refrigerating display cases), in refrigerated transportation, automotive air-conditioning systems and retrofit safety valve (for automotive applications) and residential window air-conditioners.\n\nIn October 2016, negotiators from 197 countries have reached an agreement to reduce emissions of chemical refrigerants that contribute to global warming, re-emphasizing the historical importance of the Montreal Protocol and aiming to increase its impact upon the use greenhouse gases besides the efforts made to reduce ozone depletion caused by the chlorofluorocarbons. The agreement, closed at a United Nations meeting in Kigali, Rwanda set the terms for a rapid phasedown of hydrofluorocarbons (HFCs) which would be stopped from manufacturing altogether and have their uses reduced over time.\n\nThe UN agenda and the Rwanda deal aims to find a new generation of refrigerants to be safe from both an ozone layer and greenhouse effect point of view. The legally binding agreement could reduce projected emissions by as much as 88% and lower global warming with almost 0.5 degrees Celsius (nearly 1 degree Fahrenheit) by 2100.\n\n\n"}
{"id": "40272", "url": "https://en.wikipedia.org/wiki?curid=40272", "title": "Industrial sociology", "text": "Industrial sociology\n\nIndustrial sociology, until recently a crucial research area within the field of sociology of work, examines\nOne branch of industrial sociology is labour process theory (LPT). In 1974, Harry Braverman wrote \"Labor and Monopoly Capital\", which provided a critical analysis of scientific management. This book analysed capitalist productive relations from a Marxist perspective. Following Marx, Braverman argued that work within capitalist organizations was exploitative and alienating, and therefore workers had to be coerced into servitude. For Braverman the pursuit of capitalist interests over time ultimately leads to deskilling and routinization of the worker. The Taylorist work design is the ultimate embodiment of this tendency.\n\nBraverman demonstrated several mechanisms of control in both the factory blue-collar and clerical white-collar labour force.\nHis key contribution is his \"deskilling\" thesis. Braverman argued that capitalist owners and managers were incessantly driven to deskill the labour force to lower production costs and ensure higher productivity. Deskilled labour is cheap and above all easy to control due to the workers' lack of direct engagement in the production process. In turn work becomes intellectually or emotionally unfulfilling; the lack of capitalist reliance on human skill reduces the need of employers to reward workers in anything but a minimal economic way.\n\nBraverman's contribution to the sociology of work and industry (i.e., industrial sociology) has been important and his theories of the labour process continue to inform teaching and research. Braverman's thesis has, however, been contested, by Andrew Freidman in his work \"Industry and Labour\" (1977). In it, Freidman suggests that whilst the direct control of labour is beneficial for the capitalist under certain circumstances, a degree of \"responsible autonomy\" can be granted to unionized or \"core\" workers, in order to harness their skill under controlled conditions. Also, Richard Edwards showed in 1979 that although hierarchy in organizations has remained constant, additional forms of control (such as technical control via email monitoring, call monitoring; bureaucratic control via procedures for leave, sickness etc.) has been added to gain the interests of the capitalist class versus the workers. Duncan Gallie has shown how important it is to approach the question of skill from a social class perspective. In his study, the majority of non-manual, intermediate and skilled manual workers believed that their work had come to demand a higher level of skill, but the majority of manual worker felt that the responsibility and skill needed in their work had either remained constant or declined. This means that Braverman's claims can't be applied to all social classes.\n\nThe notion the particular type of technology workers were exposed to shapes their experience was most forcefully argued in a classic study by Robert Blauner. He argued that some work is alienating more than other types because of the different technologies workers use. Alienation, to Blauner, has four dimensions: powerlessness, meaninglessness, isolation, and self-estrangement. Individuals are powerless when they can't control their own actions or conditions of work; work is meaningless when it gives employees little or no sense of value, interest or worth; work is isolating when workers cannot identify with their workplace; and work is self-estranging when, at the subjective level, the worker has no sense of involvement in the job.\n\nBlauner's claims however fail to recognize that the same technology can be experienced in a variety of ways. Studies have shown that cultural differences with regard to management–union relations, levels of hierarchical control, and reward and performance appraisal policies mean that the experience of the same kind of work can vary considerably between countries and firms. The individualization of work and the need for workers to have more flexible skills in order to respond to technological changes means that Blauner's characterization of work experience is no longer valid. Additionally, workers today may work in teams to alleviate workers' sense of alienation, since they are involved in the entire process, rather than just a small part of it. In conclusion, automative technologies and computerized work systems have typically enhanced workers' job satisfaction and skill deployment in the better-paid, secure public and private sector jobs. But, in more non-skilled manual work, they have just perpetuated job dissatisfaction, especially for the many women involved in this type of work.\n\n"}
{"id": "23938855", "url": "https://en.wikipedia.org/wiki?curid=23938855", "title": "Information technology specialist (military)", "text": "Information technology specialist (military)\n\nInformation Technology Specialist or lnformation Systems Operator-Analyst is a Military Occupational Specialty (MOS) in the United States Army. Information Technology Specialists have the responsibility of maintaining, processing and troubleshooting military computer systems and operations.\n\nSoldiers training to perform the duties of an Information Technology Specialists will attend the IT Specialists course at Fort Gordon, Georgia. Fort Gordon provides training for Initial Entry Training (IET) Soldiers, as well as Reclassifying Soldiers, from other Occupational Specialties, or Prior Service members. For training purposes, though all Soldiers in the course will receive the same training, they are separated as MOS-I for Initial Training, and MOS-T for re-class.\n\nThe Information Technology Specialists course is approximately 20 weeks long, and soldiers in training will be classed into one of the several phases during training. This is a continuation of the phases started in Army basic training, and all service members in training regardless of rank or time in service will participate.\n\nPhase IV (weeks 1 - 3 of AIT) begins with the first week of AIT and ends when the Soldier demonstrates proficiency to move up in phase. The first opportunity to phase-up will be provided at 15 days with subsequent opportunities provided weekly thereafter. Use, possession or purchase of alcohol is prohibited during all Phases except as religious sacrament.\n\nIn order for an MOS-I Soldier to move from Phase IV to Phase V privileges, the Soldier must pass Knowledge Exam, recite the Soldier’s Creed, sing their branch song and army song, and pass Class A, Wall Locker, and Room Inspections. Additionally, Soldiers must demonstrate proper discipline, pass all academic requirements, and pass their APFT with 60 points in each event.\n\nPhase V (weeks 3-10 of AIT). Soldiers are authorized to wear civilian clothes, on-post pass privileges, Company Commanders can authorize extended weekend off-post pass privileges to Phase V Soldiers, however, members are subject to unit recall. Phase V Soldiers are allowed to use personal linen on their beds in accordance with policies established throughout each battalion.\n\nSoldiers are not authorized to ride in a POV or rental car without Company Commander approval, and alcohol consumption by MOS-I Soldiers is prohibited\n\nMarried Soldiers in Phase V with PCS orders to Fort Gordon or Fort Meade may be authorized to live off-post or in GMH managed post housing, if the spouse and/or children are in the immediate area, and upon approval from their Battalion Commander.\n\nIn order for an MOS-I Soldier to move from Phase V to Phase V+ privileges the Soldier must recite the Soldier’s Creed, sing their branch song and the army song, and pass Class A, Wall Locker, and Room Inspections, complete the first level of the Structured Self Development course (SSD1)\n\nPhase V+ (weeks 11 through the end of AIT). Company Commanders may authorize the use of rental cars. Company Commanders may authorize Soldiers to purchase, register, and/or possess a privately owned vehicle (POV) once Soldiers complete 21 weeks of training. Since the training is no longer 22 weeks we are not authorized to have privately owned vehicles unless on a holiday block leaves such as Christmas. They also have other privileges\n\nThe IT Specialist course at Fort Gordon is Advanced Individual Training (AIT) and is administered by the 15th Regimental Signal Brigade, 551st Signal Battalion. The course is designed to prepare soldiers by teaching them skills necessary to become proficient in identifying computer-user problems and resolution coordination, installing, configuring and monitoring local and wide-area networks, hardware and software, compiling, entering and processing information. Soldiers will also will be taught to provide customer and network administration services such as passwords, electronic mail accounts, security and troubleshooting, conducting data system studies, preparing documentation and specifications for proposals and assisting in the design, preparation, editing and testing of computer programs. The course syllabus is as follows:\n\nProvides the student with the necessary skills and knowledge to identify, install, configure, administer, and perform Unit Level Maintenance (ULM) on a microcomputer. This includes the installation and configuration of operating systems (OS) and virus software.\n\nProvides the student with necessary skills and knowledge to define data communications terms and network terminology, perform basic Network Administrator (NA) functions.\n\nProvides the student with the skills and knowledge to correctly install and configure Microsoft XP Professional, and establish user accounts. The skills to install, establish, and troubleshoot Outlook E-mail accounts.\n\nThe Information Assurance (IA) brief presents information on IA as it applies to an Automated Information System (AIS) within the Department of Defense (DoD). Initial training is provided on recognizing vulnerabilities and potential threats within computer systems and networks.\n\nProvides the student with skills and knowledge necessary to understand the capabilities of the Transmission Control Protocol/Internet Protocol (TCP/IP) suite, correctly assign IP address according to host and network requirements, and create additional network addresses by subnetting IP addresses.\n\nProvides the student with skills and knowledge necessary to install, configure, and perform administrative functions on CISCO Catalyst Switch, and extend switched networks by utilizing Virtual Local Area Networks (VLANs).\n\nProvides the student with skills and knowledge necessary to install, configure, and perform administrative functions on CISCO Routers, and establish networks by utilizing IP addressing schemes, routing protocols, Inter-VLANs routing, and access-list.\n\nProvides the student with skills and knowledge necessary to install and configure Microsoft Server 2003/Exchange Server, create a domain, join an existing domain, manage user and groups, establish user accounts, perform backup and restore, set permissions on files and folders, control assets, and configure Active Directory.\n\nProvides the student with functional knowledge of the UNIX Operating System (OS) structure, and teaches UNIX commands to include syntax and optional parameters, and the use of the Screen Editor (VI). The students will acquire the basic skills and knowledge necessary to function as a UNIX System Administrator (SA).\n\nProvides the student with skills and knowledge necessary to implement standard Solaris features to set file and directory permissions, add/delete user’s, groups, host, and printers, configure a Solaris Network, and manage multiple tasks on a Solaris-based system.\n\nProvides the student with the skills and knowledge to perform basic systems scans with Simple Network Management Protocol (SNMP), introduced in 1988, is the most widely used network management tool that provides management capability for TCP/IP-based networks. Then we include Voice over IP (VOIP) with Call Manager Express (CME) to give them an updated understanding of communications over the network they just set up and verified connectivity.\n\nProvides an introduction to RETINA and HERCULES, scanning network vulnerabilities for the student to better understand his role in network management. These scanners can look for security vulnerabilities within the system configuration policy on our CPUs. When all that is completed they now can monitor the network for any new devices added, or to maintain maximum capacity.\n\nIs an application of practical exercises in which the student will assemble, troubleshoot, and repair an automated information system (AIS), and demonstrate his/her knowledge on the basic procedures required to troubleshoot and repair basic networks and computers.\n\nProvides an introduction to the Force XXI Battle Command Brigade and Below (FBCB2) battle command control system. The student is provided instruction on the functional operation of the FBCB2 system within the tactical network\n\nProvides the student the ability to configure and monitor the network device and systems that comprise the Tactical Internet (TI) Network. Systems include the Internet Controller (INC), various Cisco Networking Devices, Enhanced Position Locator Reporting System (EPLRS), and Precision Lightweight Global Positioning System Receiver (PLGR).\n\nProvides an introduction to the operation of the Maneuver Control System (MCS) Automated Information System (AIS), and its role and functions in the support command and control systems of a maneuver unit tactical operations center (TOC).\n\nThe DTOC annex provides the student with an operational overview of the Digital Tactical Operation Center (DTOC) and the associated network connectivity within the Army Battle Command System (ABCS).\n\nThe M3 annex is a culmination of practical applications, which emphasizes the critical task taught during the training of all annexes. The students are required to design a network based on the scenarios presented and equipment provided. The students must create and connect network devices, install and configure Microsoft Windows Operating Systems (Server, Exchange, and Client), install and configure Cisco Routers and Switches, and install, configure, and administer a Solaris and UNIX Operating System.\n\nIn addition, Soldiers will receive training to enable them to meet the DOD's 8570 certification IAT level 2 requirement by earning the Security+ Certification offered by the Computing Technology Industry Association (CompTIA), as well as the CIO G-6/NETCOM Information Assurance Technical Level 1 Certification. The course includes the following modules:\n\n\n\n"}
{"id": "7302129", "url": "https://en.wikipedia.org/wiki?curid=7302129", "title": "Institute of Food Technologists", "text": "Institute of Food Technologists\n\nThe Institute of Food Technologists (IFT) is an international, non-profit scientific society of professionals engaged in food science, food technology, and related areas in academia, government and industry. It has more than 17,000 members from more than 95 countries.\n\nAs food technology grew from the individual family farm to the factory level, including the slaughterhouse for meat and poultry processing, the cannery for canned foods, and bakeries for bread, the need to have personnel trained for the food industry did also. Literature such as Upton Sinclair's \"The Jungle\" in 1906 about slaughterhouse operations would be a factor in the establishment of the U.S. Food and Drug Administration (FDA) later that year. The United States Department of Agriculture was also interested in food technology, and research was already being done at agricultural colleges in the United States, including the Massachusetts Institute of Technology (MIT), the University of Illinois at Urbana-Champaign, the University of Wisconsin–Madison, and the University of California, Berkeley. By 1935, two MIT professors, Samuel C. Prescott and Bernard E. Proctor decided that it was time to hold an international conference regarding this. A detailed proposal was presented to MIT President Karl Taylor Compton in 1936 was presented with $1500 of financial aid from MIT for a meeting to be held from June 30 to July 2, 1937 with Compton asking how many people would be in attendance at this meeting. Prescott replied with \"fifty or sixty people.\" 500 people actually attended the event.\n\nThis meeting proved so successful that in early 1938 that a second conference would be held in 1939. Initially led by George J. Hucker of the New York State Agricultural Experiment Station (part of Cornell University) in Geneva, New York, a small group meeting was held on August 5, 1938 on forming an organization with an expanded group meeting in New York City on January 16, 1939 to further discuss this. The second conference was held at MIT June 29 to July 1, 1939 with Proctor as conference chair. 600 people attended this event. At the final session, the chairman of the session Fred C. Blanck of the United States Department of Agriculture, proposed that an organization be established as the Institute of Food Technologists. This was approved unanimously. Its first officers were Prescott as President, Roy C. Newton of Swift & Company in Chicago, Illinois as Vice President, and Hucker as Secretary-Treasurer. By 1949, IFT had 3,000 members.\n\nRegional sections were established in IFT as early as 1940 in northern California (San Francisco, Bakersfield, Sacramento). The first IFT Award, the Nicholas Appert Award was established in 1942 by IFT's Chicago section with additional awards being established since then. For the first ten years, IFT officers were President, Vice-President, Secretary, and Treasurer. In 1949, IFT moved into offices in Chicago and created a permanent position of Executive Secretary to run daily organizational operations. Retired U.S. Army Colonel Charles S. Lawrence was named the first Executive Secretary, a position he would hold until 1961 when he was replaced by Calvert L. Willey. During Willey's term as Executive Director (Executive Secretary 1961–1966), IFT would grow from 6,000 members in 1961 to 23,000 members in 1987. Additionally, IFT Divisions were established in 1971 with the Refrigerated and Frozen Foods Division being the first. The IFT Student Division was established in 1975, and was reorganized in 1984 to be the IFT Student Association with the chairperson serving as a member of the IFT Board of Directors.\n\nThe governing body of IFT consists of a Board of Directors made up of the President, President-Elect, Immediate Past-President, Treasurer, Student Association President, Student Association Immediate Past President, IFT Foundation Chair, and Executive Vice President, along with twelve board members. IFT also communicates with the news media, using sixty scientists to discuss the scientific perspective on food issues. IFT is also active in the international level by its membership in the International Union of Food Science and Technology, headquartered in Oakville, Ontario, Canada. Education has always been a focus of IFT, going as far back as 1941, with the desire to have uniform education standards in food technology. Education standards for undergraduate students were approved by IFT in 1966 for food science and technology. These standards were revised and updated in 1977, 1992, 2001, and 2011. Today, IFT sits on the advisory council for the International Food Protection Training Institute.\n\nIFT largest in-person gathering is IFT’s Annual Meeting & Food Expo. This event, which offers more than 100 scientific and applied education sessions and an expo featuring 1,000+ exhibiting companies, regularly attracts 23,000 food professionals from more than 90 countries.\n\nIFT offers the Certified Food Scientist (CFS) designation. The CFS is the only globally-recognized certification for food scientists, and has more than 1600 certificants in 55 countries. The CFS program is officially endorsed by the Australian Institute of Food Science Technology (AIFST) as well as the Canadian Institute of Food Science and Technology (CIFST). Earning the CFS consists of meeting certain academic and work experience requirements and passing an exam in the following content areas:\n\nIt was designed to meet the International Standards Organization (ISO) 17024 standards for certification programs.\n\nIFT’s Global Food Traceability Center (GFTC) is the global resource and authoritative voice on food traceability. Supported by its Founding Sponsors and guided by its Advisory Council, GFTC is designed to accelerate adoption and implementation of practical traceability solutions across the global food system.\n\nThe Center works to deliver events, research, and support services that will help to increase understanding of food traceability across four business platforms:\n\nAll awards except the Loncin prize have this reference listed below.\n\n\nThese are divisions of interest by the IFT Members\nThese are usually cities, states, and regions. If a region is mentioned, a city in that region is mentioned which include areas surrounded by the city.\nBetween IFT's founding in 1939 and 1949, the institute had elected a secretary and treasurer that kept up with the daily operations of the institute. By 1949, the membership had reached 3,000 and it was decided to create an Executive Secretary position and establish national office in Chicago. Since then, the position's name has changed twice to its current name. There have been six Executive Vice Presidents shown below:\n\n\nThere are four member grades within IFT:\n\n\n2011-12: Roger Clemens\n\n2012-13: John Ruff\n\n2013-14: Janet E. Collins\n\n2014-15: Mary Ellen Camire\n\n2015-16: Colin Dennis\n\n2016-17: John Coupland\n\nThe Institute also has many publications that are both in print and online that are shown below:\n\n\nBoth \"Food Technology\" and the \"Journal of Food Science\" can be accessed in print or online. Other publications are shown below:\n\nIn 2016 they initiated and funded the documentary Food Evolution by Scott Hamilton Kennedy\n\n\nOne constant that IFT has concern with is food safety, mainly with pathogens in food (see food microbiology) and how to counter these harmful bacteria. The global trade of food is also concerned with food safety and food security, specifically with ingredient availability and consumer tastes worldwide. Food traceability, food fraud, obesity, and how science and technology will help feed the world’s projected 9 billion-plus people in 2050 have all become main issues for IFT.\n"}
{"id": "20519283", "url": "https://en.wikipedia.org/wiki?curid=20519283", "title": "International Small Group and Tree Planting Program", "text": "International Small Group and Tree Planting Program\n\nThe International Small Group and Tree Planting Program, or TIST, is a comprehensive sustainable development program for developing-world locations. TIST was started in 2000 and exists in Africa (Kenya, Tanzania, Uganda) and India. The TIST program has three separate but related aims: development, commercial opportunity, and replication.\nThe development goal of the TIST program is to empower and equip subsistence farmers to restore their natural environment, increase soil fertility, create jobs, strengthen the local community, and move from famine to surplus.\n\nTIST trains and encourages small groups to develop and share \"best practices.\" TIST introduces improved farming and land use techniques to isolated subsistence farmers who are now planting millions of new trees. Using a combination of small group development and training programs and providing small stipends to groups, TIST helps local farmers meet their economic needs, even during severe dry seasons.\n\nSmall groups agree to meet the program requirements and assure tree survival and use of improved, sustainable land use techniques for years to come. The improved farming practices and tree planting will improve local welfare by stabilizing the local food supply and by providing families with additional income from TIST tree benefits and payments.\n\nTIST small groups are also educated about HIV/AIDS and equipped to formulate a response to this pandemic at the group and village level. Adopting conservation farming techniques increases food and decreases annual physical effort after the first seedbeds are created. Family members can continue to plant in these seedbeds year after year and have food.\n\nMany of the 86,000 TIST farmer participants were currently using the traditional 3-stone cook stoves or handmade mud stoves. Through work with Envirofit International, TIST has been able to bring healthy cook stoves and the training necessary for an effective replacement for their traditional stoves to many of TIST members.\n\nProject areas\nTIST currently operates in four countries:\n\n\nThe initial pilot implementation site of TIST was in Mpwapwa, Tanzania. Mpwapwa is located southeast of Tanzania’s capital, Dodoma.\n\n"}
{"id": "5933562", "url": "https://en.wikipedia.org/wiki?curid=5933562", "title": "International Software Testing Qualifications Board", "text": "International Software Testing Qualifications Board\n\nThe International Software Testing Qualifications Board (ISTQB) is a software testing qualification certification organisation that operates internationally. Founded in Edinburgh in November 2002, ISTQB is a non-profit association legally registered in Belgium.\n\nISTQB Certified Tester is a standardized qualification for software testers and the certification is offered by the ISTQB. The qualifications are based on a syllabus, and there is a hierarchy of qualifications and guidelines for accreditation and examination. The ISTQB is a software testing qualification certification organization having over 500,000 certifications issued; the ISTQB consists of 58 member boards worldwide representing 81 countries (date: May 2017).\n\nCurrent ISTQB product portfolio follows a Matrix approach characterized by\n\nISTQB streams focus on:\n\nThe exam for the Foundation Level has a theoretical nature and requires knowledge of software development - especially in the field of software testing.\n\nThe different Advanced Level exams are more practical and require deeper knowledge in special areas. Test Manager deals with planning and control of the test process. Test Analyst concerns, amongst other things, reviews and black box testing methods. Technical Test Analyst includes component tests (also called unit test), requiring knowledge of white box testing and non-functional testing methods – this section also includes test tools.\n\nPre-conditions relate to certification exams and provide a natural progression through the ISTQB Scheme which helps people pick the right certificate and informs them about what they need to know.\n\nThe ISTQB Core Foundation is a pre-condition for any other certification.\n\nAdditional rules for ISTQB pre-conditions are summarized in the following:\n\nSuch rules are depicted from a graphical point of view in the ISTQB Product Portfolio map.\n\nThe Foundation and Advanced exams consist of a multiple choice tests.\n\nCertification is valid for life (Foundation Level and Advanced Level), and there is no requirement for recertification.\n\nTesting boards are responsible for the quality and the auditing of the examination. Worldwide there are testing boards in 57 countries (date: May 2017). In the United States, the corresponding organisation is the American Software Testing Qualifications Board (ASQTB); in Britain, the UK Testing Board (UKTB); and in India, the Indian Testing Board (ITB).\n\n\n\n"}
{"id": "21291407", "url": "https://en.wikipedia.org/wiki?curid=21291407", "title": "Japanese typewriter", "text": "Japanese typewriter\n\nThe first practical was invented by Kyota Sugimoto in 1915. Out of the thousands of kanji characters, Kyota's original typewriter used 2,400 of them. He obtained the patent rights to the typewriter that he invented in 1929. Until the popularization of word processor technology, the Japanese typewriter contributed greatly to increased efficiency of document preparation at Japanese companies and government offices.\n\n\n"}
{"id": "35984041", "url": "https://en.wikipedia.org/wiki?curid=35984041", "title": "List of LoveFilm Instant compatible devices", "text": "List of LoveFilm Instant compatible devices\n\nThis is a list of devices with LoveFilm Instant capability.\n"}
{"id": "53901368", "url": "https://en.wikipedia.org/wiki?curid=53901368", "title": "List of sensors used in digital cameras", "text": "List of sensors used in digital cameras\n\nThe following is a list of sensors used in digital cameras.\n\n"}
{"id": "647768", "url": "https://en.wikipedia.org/wiki?curid=647768", "title": "Mallet", "text": "Mallet\n\nA mallet is a kind of hammer, often made of rubber or sometimes wood, that is smaller than a maul or beetle, and usually has a relatively large head. The term is descriptive of the overall size and proportions of the tool, and not the materials it may be made of, though most mallets have striking faces that are softer than steel.\n\nMallets are used in various industries, such as upholstery work, and a variety of other general purposes. It is a tool of preference for wood workers using chisels with plastic, metal, or wooden handles, as they give a softened strike with a positive drive. It is the most commonly used mallet.\n\nLess common mallets include:\n\nMallets of various types are some of the oldest forms of tools, and have been found in stone age gravesites.\n\nMallets used as drumsticks are often used to strike a marimba, xylophone, glockenspiel, metallophone, or vibraphone, collectively referred to as mallet percussion. The sticks usually have shafts made of rattan, birch, or fiberglass. Rattan shafts are more flexible than the other materials. Heads vary in size, shape, and material; they may be made of metal, plastic, rubber, or wood, and some are wrapped with felt, cord, or yarn. Heavier heads produce louder sounds, while harder heads produce sharper and louder sounds, with more overtones.\n\nMallets are commonly used as children's toys. Lightweight wooden mallets are used for peg toys. Toy mallets are also used in games such as Whac-A-Mole. Another type of toy mallet is a plastic mallet made of soft, hollow vinyl, with bellows and a built-in whistle, so that when the mallet is struck, it produces a sharp, chirping sound.\n\nThe misuse of wooden mallets in the workplace became a classic gag in the Looney Tunes, Hanna-Barbera, Nickelodeon, Disney, and MGM cartoons, including some more recent 3D animations. Characters like Roger Rabbit, Bugs Bunny, Donald Duck, Daffy Duck, Tom and Jerry and Screwy Squirrel made use of mallets as part of their arsenal in the Golden Age of animation.\n\nOn the British daytime shows TV-AM and GMTV, and in the spinoff children's shows hosted by Timmy Mallet, a game was played called \"Mallet's Mallet\" where a participant had to associate words in a certain time period. Failing contestants were hit on the head by Mallet, using a soft pink and yellow mallet with black \"Mallet's Mallet\" writing on both sides. The person who got hit then had a sticking plaster affixed to their head by Mallet.\n\nIn slapstick anime and manga, it is a common gag for an angry character to pull a large mallet out of thin air, and strike the person or thing that is angering him/her.\n"}
{"id": "5088477", "url": "https://en.wikipedia.org/wiki?curid=5088477", "title": "Mobile offshore base", "text": "Mobile offshore base\n\nMobile offshore base (MOB), sometimes called a joint mobile offshore base (JMOB), is a concept for supporting military operations beyond the home shores, where conventional land bases are not available, by deploying on the high seas or in coastal waters, in-theater multipurpose floating base assembled from individual platforms. In essence, a MOB is a multipurpose modular self-propelled floating platform, or several interconnected platforms, that can perform multiple functions of a sea base including strike, deployment and logistics. An ocean-wise semi-submersible wave and wind resistant platform capable to move at one-half the speed of conventional prepositioning monohull cargo ship has been researched and proposed, but never built.\n\nThe Mobile offshore base concept emerged during a search for a more cost effective option of sustaining in-theater strike, flight, maintenance, supply and other forward logistics support needs compared with utilizing traditional joint logistics approaches including nuclear-powered aircraft carriers and \"large medium speed roll-on/roll-off\" (LMSR) sealift ships. MOB modules were projected as semi-submersible units having significantly smaller wave-induced motions compared to conventional hulls.\n\nIn theory, the modularity of a MOB allows the full spectrum of air support, ranging from vertical/short takeoff and landing (VSTOL) aircraft using a single platform to conventional takeoff and landing (CTOL) aircraft utilizing several serially aligned modules approaching 2 km (6,000 feet) in length. The cluster could have an air strip that could hold a large aircraft such as C-130 or C-17. In addition, a MOB accepts ship-borne cargo, provides nominally 280,000 m² (3 million square feet) for equipment storage and maintenance, stores 40 million litres (10 million gallons) of fuel, houses up to 3,000 troops (an Army heavy brigade), and discharges resources to the shore via a variety of landing craft. It was argued, that once positioned, the MOB would operate as a sea base for an extended period, so it would need to have port-like facilities for unloading and loading conventional container and roll-on/roll-off ships.\n\nThe idea of the MOB was first seriously considered when the United States entered Operation Desert Shield (1990–91). The U.S. was forced to request the use of allied bases, which, besides strictly military considerations, proved to be politically sensitive in the case of Saudi Arabia. With the MOB concept the U.S. could have a base anywhere in the world in as little as a month. The base as conceived would have had virtually unlimited capabilities, and most of its creators did not envision just a floating air strip, but a town-sized base.\n\nThe \"joint mobile offshore base\" (JMOB) was a MOB concept for expeditionary warfare and humanitarian and commercial operations developed in the 1990s by McDermott International, Inc. of Arlington, Virginia. The JMOB was to be composed of five self-propelled units creating a one-mile long runway that could accommodate a fully loaded C-17. NATO was thought to be interested in the concept at the time.\n\nA technical report presented to the U.S. Congress in April 2000 identified that such a base was technologically feasible and could be built by the defense industry of the United States.\n\nIn December 1999, the Office of Naval Research (ONR) in response to a congressional mandate issued a report which delineated the impracticality of MOBs, \"the largest floating offshore structure ever conceived by maritime engineers\", on the grounds of high cost and vulnerability to threats such as missile attack. In January 2001, the Institute for Defense Analyses (IDA) stated that MOB \"would not be capable of effectively replacing conventional sealift\" because it provides an inferior delivery capability to the existing \"joint logistics over the shore\" (JLOTS) system. The report concluded that the estimated US$5 billion to US$8 billion MOB project was less cost effective than other existing at the time solutions.\n\n\n"}
{"id": "23675841", "url": "https://en.wikipedia.org/wiki?curid=23675841", "title": "Oyster wave energy converter", "text": "Oyster wave energy converter\n\nThe Oyster is a hydro-electric wave energy device that uses the motion of ocean waves to generate electricity. It is made up of a Power Connector Frame (PCF), which is bolted to the seabed, and a Power Capture Unit (PCU). The PCU is a hinged buoyant flap that moves back and forth with movement of the waves. The movement of the flap drives two hydraulic pistons that feed high-pressured water to an onshore hydro-electric turbine, which drives a generator to make electricity. Oyster is stationed at the European Marine Energy Centre (EMEC) at its Billia Croo site in Orkney, Scotland.\n\nAquamarine Power installed Oyster at the EMEC in August 2009. On 20 November 2009, Oyster was officially launched and connected to the National Grid (UK) by the First Minister of Scotland, Alex Salmond.\n\nCurrent developments are underway to build a more efficient and powerful second-generation device, Oyster 2.\n\nOyster was developed by Edinburgh-based Aquamarine Power, a company that focuses on wave energy. The concept originated from research at Queen’s University, Belfast, led by professor Trevor Whittaker, Head of the Wave Power Research Centre at Queen's. Aquamarine Power also teamed up with Renewable Technology Ventures Ltd (STVL), a subsidiary of Scottish and Southern Energy (SEE), to fund the Oyster project. Aquamarine Power was able to secure a £6.3m investment from Scottish Enterprise. In addition, Scottish Enterprises awarded Aquamarine Power a £3.15 million grant from the Wave and Tidal Energy: Research, Development and Demonstration Support fund (WATERS). Aquamarine Power also received £1.5m from Sigma Capital Group plc. Altogether, Aquamarine Power was able to raise £11 million to stage this project.\n\nIn June 2009, Aquamarine Power signed a £2.5 million contract with Fugro Seacore to install the Oyster device at the European Marine Energy Centre test site at Billia Croo. Oyster was installed 400 metres offshore, west of the Orkney mainland, in 12 metre-deep water. Oyster was installed in August 2009; however it was officially launched on 20 November 2009 by the First Minister of Scotland, Alex Salmond. That same day, Oyster was connected the National Grid (UK) and began generating electricity.\n\nAquamarine Power hopes to commercialize Oyster and has already signed an agreement with Scottish and Southern Energy to develop up to 1000MW of wave farms by 2020.\n\nOyster harnesses the energy of near-shore ocean waves; it was designed to operate in water 10 to 12 metres deep. The Oyster is made up of a Power Connector Frame (PCF) and a Power Capture Unit (PCU). The 36-ton PCF is bolted to the seabed by 1-by-4 meter concrete piles that are drilled 14 metres deep into the seabed. The PCF requires careful and accurate positioning and leveling to compensate for the uneven, rocky seabed. The PCU is a 200-ton, 18-by-12-by-4 metre buoyant flap that is hinged to the PCF. In order to lower the PCU into the water to hinge it to the PCF, 120 tons of seawater must be pumped into ballast tanks within the PCU to provide sufficient negative buoyancy to aid its descent into the water. The PCU is almost entirely submerged underwater; only 2 metres of the device poke above the water. The PCU sways back and forth with the movement of the waves, and this movement of the flap drives two hydraulic pistons that pump high-pressured water through three sub-sea pipeline to an onshore hydro-electric water turbine. The turbine then drives a 315 kW electrical generator, which converts the wave energy into electricity.\n\nThe European Marine Energy Centre classifies Oyster as an Oscillating Wave Surge Converter:\n\nThere are several advantages to using a device like the Oyster:\n\n\nThere are also many disadvantages to using a device like the Oyster:\n\n\nCurrent developments are underway to construct an improved, second-generation Oyster device, Oyster 2. In December 2010, Aquamarine Power signed a £4 million contract with Scotland’s leading fabrication contractor Burntisland Fabrications Ltd (BiFab). BiFab began manufacturing Oyster 2 at its manufacturing plant in Methil, Fife, Scotland, and the device is expected to be in full operation by the summer rine Energy Centre]].\n\nAquamarine Power has secured funding from many sources for the development of Oyster 2:\n\n\nOyster 2 will employ the same basic technology of the original Oyster; however, it will feature a different shape that will maximize the amount of energy that the device is able to capture from the waves. Oyster 2 will consist of three 800 kW flaps that will all be linked to one pipeline leading to an onshore 2.4 MW hydro-electric generator. Each flap will measure 26 metres, making it 50% larger than the original Oyster. Oyster 2 will also have a 250% greater power output. Aquamarine Power estimates that a small farm of 20 Oyster 2 devices will be capable of supplying enough electricity for over 12,000 homes, compared to the 9,000 homes the original Oyster is capable of powering.\n\nAt the European Marine Energy Centre’s Billia Croo site, a single Oyster 800 rated at 800 kW was grid-connected in June 2012 and will undergo testing until 2015. By mid 2014 the Oyster 800 had completed 20,000 hours of operation.\n\n\n"}
{"id": "35332340", "url": "https://en.wikipedia.org/wiki?curid=35332340", "title": "Payments as a service", "text": "Payments as a service\n\nPayments as a service (PaaS) is a marketing phrase used to describe a software as a service to connect a group of international payment systems. The architecture is represented by a layer – or overlay – that resides on top of these disparate systems and provides for two-way communications between the payment system and the PaaS. Communication is governed by standard APIs created by the PaaS provider.\n\nSince the 1980s, credit cards and international wire transfer systems like the Society for Worldwide Interbank Financial Telecommunication (SWIFT) were primary methods for making and receiving electronic cross-border payments. Within individual countries, payers and payees have used various electronic systems to make such payments. In the United States, for instance, the Federal Reserve Bank operates the automated clearing house (ACH) system. In most EU countries direct debit is the preferred method of facilitating electronic payments. \n\nWith the advent of the World Wide Web, it became necessary to provide alternative payment systems. At first, consumers were hesitant to use their credit cards on the Web due to security concerns. Entrepreneurs tried to market an \"electronic wallet\". As early as 1994, CyberCash allowed consumers to make secure purchases over the Internet.\n\nCyberCash eventually failed. In March of 2000, PayPal was formed and became a predominant electronic wallet in the U.S. Similar regional services include WebMoney and Yandex.Money in Russia and Alipay in China. While popular in their own countries, these do not have significant global reach. PayPal and Moneybookers (Skrill) became regional electronic wallets, providing greater liquidity, but still do not provide for the free flow of funds between all popular electronic wallet solutions.\n\nPaaS is designed to allow merchants and other market participants to utilize local, regional and global payments options through a single interface. The complexity of moving funds between providers is handled by the PaaS layer and is hidden from the user. Generally speaking, there is only one interface between a merchant and PaaS. Because only one interface is required, merchants or users are only required to maintain one financial repository.\n"}
{"id": "6798309", "url": "https://en.wikipedia.org/wiki?curid=6798309", "title": "Pin Index Safety System", "text": "Pin Index Safety System\n\nThe Pin Index Safety System, or PISS, is a safety system that uses geometric features on the yoke to ensure that pneumatic connections between a gas cylinder and a machine that uses pressurized gases are not connected to the wrong gas yoke. This system can be seen on an anesthesia machines and portable oxygen administration sets. It has no purpose other than a physical barrier to connecting the wrong cylinder.\n\nThe pin index safety system uses a face seal between the cylinder valve and the associated yoke clamp. There are two holes in specific positions on the cylinder valve body below the outlet port , in positions associated with the gas mixture, which prevent connection of the cylinder to a yoke or pressure regulator with a mis-matched set of pins. The holes accept pins 4 mm diameter by 6 mm long which are correctly aligned with the holes.\n\nEach gas cylinder has a pin configuration to fit its respective gas yoke. Refer to the diagram for pin numbers; dimensions are in millimeters.\n\n\n\nThe pin index system is a safety system (PISS) designed to ensure the correct gas is filled into the correct cylinder, and that the cylinder will only connect to the correct equipment. The positions of the holes on the cylinder valve correspond with the pins fitted to the yoke attached to the equipment. The pin positions for each medical gas are unique. If an attempt is made to fit the wrong gas cylinder to the yoke a tight seal will not be made, as the pins cannot locate.\n\nThe system requires a seal between the yoke and valve to prevent leakage. This is called a Bodok seal, and is a moulded rubber washer (or Neoprene) supported by a metal rim.\n\nIt is possible to bypass the pin-index system if the pins are removed, damaged or corroded, if extra washers are used, or on some valves with a short face above the orifice, by inverting the gas cylinder. There is one report of the cylinder being painted the wrong colour leading to error.\n\nLarge cylinders are more likely to be fitted with threaded connectors. One such system are the American CGA fittings. In Europe there are British Standard (BS), German Standard (DIN) and French (AFNOR) connections, and Japan has the Japanese Standard (JIS).\n\nBlanking plugs (dummy cylinder heads) can be inserted into empty yokes to ensure that there is no leak out of the yoke when not in use.\n"}
{"id": "9481141", "url": "https://en.wikipedia.org/wiki?curid=9481141", "title": "Polyvinyl nitrate", "text": "Polyvinyl nitrate\n\nPolyvinyl nitrate (PVN) is a polymeric explosive material, an ester of nitric acid and polyvinyl alcohol. It is prepared by nitration of polyvinyl alcohol or transesterification of polyvinyl acetate. It is a thermoplastic substance with softening zone between 30-45 °C, depending on the molecular weight of the starting polyvinyl alcohol.\n\nPolyvinyl nitrate can be used in some moldable propellants and explosives.\n"}
{"id": "41234142", "url": "https://en.wikipedia.org/wiki?curid=41234142", "title": "Readout integrated circuit", "text": "Readout integrated circuit\n\nReadout Integrated Circuit (ROIC) is an integrated circuit specifically used for reading detectors of a particular type. They are compatible with different types of detectors such as infrared and ultraviolet. The primary purpose for ROICs is to accumulate the photocurrent from each pixel and then transfer the resultant signal onto output taps for readout. Conventional ROIC technology stores the signal charge at each pixel and then routes the signal onto output taps for readout. This requires storing large signal charge at each pixel site and maintaining signal-to-noise ratio (or dynamic range) as the signal is read out and digitized.\n"}
{"id": "3775394", "url": "https://en.wikipedia.org/wiki?curid=3775394", "title": "Relative bearing indicator", "text": "Relative bearing indicator\n\nA relative bearing indicator (RBI) shows the bearing of some source relative to a vehicle carrying a detector. It is most commonly used in conjunction with an ADF (automatic direction finder) in an aircraft navigating with the aid of an NDB (non-directional beacon).\n"}
{"id": "43038157", "url": "https://en.wikipedia.org/wiki?curid=43038157", "title": "Robo-advisor", "text": "Robo-advisor\n\nRobo-advisors or Robo-advisers are a class of financial adviser that provide financial advice or Investment management online with moderate to minimal human intervention. They provide digital financial advice based on mathematical rules or algorithms. These algorithms are executed by software and thus financial advice do not require a human advisor. The software utilizes its algorithms to automatically allocate, manage and optimize clients' assets.\n\nThere are over 100 robo-advisory services. Investment management robo-advice is considered a breakthrough in formerly exclusive wealth management services, bringing services to a broader audience with lower cost compared to traditional human advice. Robo-advisors typically allocate a client's assets on the basis of risk preferences and desired target return. While robo-advisors have the capability of allocating client assets in many investment products such as stocks, bonds, futures, commodities, real estate, the funds are often directed towards ETF portfolios. Clients can choose between offerings with passive asset allocation techniques or active asset management styles.\n\nThe first robo-advisors were launched in 2008 during the financial crisis. In 2010, Jon Stein, a 30-year old entrepreneur, launched Betterment, and robo-advisors increased in popularity. The first robo-advisers were used as online interface to manage and balance client's assets by financial managers. Robo-adviser technology was not new to this field, as this kind of software has been in use by financial advisers and managers since early 2000's. But they were made publicly available in 2008 for the first to general public who were in dire need to manage their assets personally. By the end of 2015, robo-advisers from almost 100 companies around the globe were managing $60 billion assets of clients and it is estimated that it will hit $2 trillion by the end of 2020. In June 2016, robo-advisor Wealthfront announced a partnership with the Nevada State Treasurer to offer a 529 plan for college savings. In 2017, Betterment raised $70 million of financing and Personal Capital raised $40 million of financing. \n\nIn 2016, Hong Kong based 8 Securities launched one of Asia's first robo-advisors. In 2017, Singapore based StashAway received a capital markets services license from the Monetary Authority of Singapore.\n\nA robo-advisor can be defined as \"a self-guided online wealth management service that provides automated investment advice at low costs and low account minimums, employing portfolio management algorithms.\"\n\nLegally, the term \"financial advisor\" applies to any entity giving advice about securities. Most robo-advisor services are instead limited to providing portfolio management (i.e. allocating investments among asset classes) without addressing issues such as estate and retirement planning and cash-flow management, which are also the domain of financial planning.\n\nOther designations for these financial technology companies include \"automated investment advisor\", \"automated investment management\", \"online investment advisor\" and \"digital investment advisor.\"\n\nWhile robo-advisors are most common in the United States, they are also present in Europe, Australia, India, Canada, and Asia In Canada, Bmo smartfolio and Wealthsimple are examples of robo-advisors.\n\nThe tools they employ to manage client portfolios differ little from the portfolio management software already widely used in the profession. The main difference is in distribution channel. Until recently, portfolio management was almost exclusively conducted through human advisors and sold in a bundle with other services. Now, consumers have direct access to portfolio management tools, in the same way that they obtained access to brokerage houses like Charles Schwab and stock trading services with the advent of the Internet. Robo-advisors are extending into newer business avenues.\n\nThe customer acquisition costs and time constraints faced by traditional human advisors have left many middle-class investors underadvised or unable to obtain portfolio management services because of the minimums imposed on investable assets. The average financial planner has a minimum investment amount of $50,000, while minimum investment amounts for robo-advisors start as low as $500 in the United States and as low as £1 in the United Kingdom. In addition to having lower minimums on investable assets compared to traditional human advisors, robo-advisors charge fees ranging from 0.2% to 1.0% of Assets Under Management while traditional financial planners charged average fees of 1.35% of Assets Under Management according to a survey conducted by AdvisoryHQ News.\n\nIn the United States, robo-advisors must be registered investment advisors, which are regulated by the Securities and Exchange Commission. In the United Kingdom they are regulated by the Financial Conduct Authority.\n\nAs of October 2017, robo-advisors had $224 billion in assets under management.\n\nThe following are the largest robo-advisors by assets under management:\nApart from B2C, there exists large B2B players that provide Robo-advisory platforms to Banks, Wealth Managers, Insurance firms and other professional players in the industry. They use sophisticated algorithms for goal based investing and efficient financial planning. Some firms capitalising on Machine Learning technology, provide more innovative building block complimenting the platform. \n"}
{"id": "2724842", "url": "https://en.wikipedia.org/wiki?curid=2724842", "title": "Software craftsmanship", "text": "Software craftsmanship\n\nSoftware craftsmanship is an approach to software development that emphasizes the coding skills of the software developers themselves. It is a response by software developers to the perceived ills of the mainstream software industry, including the prioritization of financial concerns over developer accountability.\n\nHistorically, programmers have been encouraged to see themselves as practitioners of the well-defined statistical analysis and mathematical rigor of a scientific approach with computational theory. This has changed to an engineering approach with connotations of precision, predictability, measurement, risk mitigation, and professionalism. Practice of engineering led to calls for licensing, certification and codified bodies of knowledge as mechanisms for spreading engineering knowledge and maturing the field.\n\nThe Agile Manifesto, with its emphasis on \"individuals and interactions over processes and tools\" questioned some of these assumptions. The Software Craftsmanship Manifesto extends and challenges further the assumptions of the Agile Manifesto, drawing a metaphor between modern software development and the apprenticeship model of medieval Europe.\n\nThe movement traces its roots to the ideas expressed in written works. The Pragmatic Programmer by Andy Hunt and Dave Thomas and Software Craftsmanship by Pete McBreen explicitly position software development as heir to the guild traditions of medieval Europe. The philosopher Richard Sennet wrote about software as a modern craft in his book The Craftsman. Freeman Dyson, in his essay \"Science as a Craft Industry\", expands software crafts to include mastery of using software as a driver for economic benefit:\n\nFollowing initial discussion, conferences were held in both London and Chicago, after which, a manifesto was drafted and put online to gather signatories. This was followed by the development of practices to further develop the movement including the exchange of talent in \"Craftsman Swaps\" and the assessment of skills in \"Craftsmanship Spikes\"\n\nIn 1992, Jack W. Reeves' essay \"What Is Software Design?\" suggested that software development is more a craft than an engineering discipline. Seven years later, in 1999, \"The Pragmatic Programmer\" was published. Its sub-title, \"From Journeyman to Master\", suggested that programmers go through stages in their professional development akin to the medieval guild traditions of Europe.\n\nIn 2001, Pete McBreen's book \"Software Craftsmanship\" was published. It suggested that software developers need not see themselves as part of the engineering tradition and that a different metaphor would be more suitable.\n\nIn his August keynote at Agile 2008, Uncle Bob proposed a fifth value for the Agile Manifesto, namely \"Craftsmanship over Crap\". He later changed his proposal to \"Craftsmanship over Execution\".\n\nIn December 2008, a number of aspiring software craftsmen met in Libertyville, Illinois with the intent of establishing a set of principles for Software Craftsmanship. Three months later, a summary of the general conclusions was decided on. It was presented publicly, for both viewing and signing, in the form of a Manifesto for Software Craftsmanship.\n\nIn April 2009, two of the companies in the software craftsmanship movement, 8th Light and Obtiva, experimented with a \"Craftsman Swap.\" The Chicago Tribune covered this event on 15 June 2009. In January 2010, a second Craftsman Swap was held between Obtiva and Relevance.\n\nIn March, 2013, Software Craftsmanship : Professionalism Pragmatism Pride was published by Sandro Mancuso\nIn Sandro's own words this -\n\"Proposing a very different mindset for developers and companies, a strong set of technical disciplines and practices, mostly based on Extreme Programming, and with a great alignment with Agile methodologies, Software Craftsmanship promises to take our industry to the next level, promoting professionalism, technical excellence, the death of the production line and factory workers attitude.\"\n\n\n"}
{"id": "25861897", "url": "https://en.wikipedia.org/wiki?curid=25861897", "title": "Sonic interaction design", "text": "Sonic interaction design\n\nSonic interaction design is the study and exploitation of sound as one of the principal channels conveying information, meaning, and aesthetic/emotional qualities in interactive contexts. Sonic interaction design is at the intersection of interaction design and sound and music computing. If interaction design is about designing objects people interact with, and such interactions are facilitated by computational means, in sonic interaction design, sound is mediating interaction either as a display of processes or as an input medium.\n\nResearch in this area focuses on experimental scientific findings about human sound reception in interactive contexts. \n\nDuring closed-loop interactions, the users manipulate an interface that produces sound, and the sonic feedback affects in turn the users’ manipulation. In other words, there is a tight coupling between auditory perception and action. Listening to sounds might not only activate a representation of how the sound was made: it might also prepare the listener to react to the sound. Cognitive representations of sounds might be associated with action-planning schemas, and sounds can also unconsciously cue a further reaction on the part of the listener. \n\nSonic interactions have the potential to influence the users’ emotions: the quality of the sounds affects the pleasantness of the interaction, and the difficulty of the manipulation influences whether the user feels in control or not.\n\nProduct design in the context of sonic interaction design is dealing with methods and experiences for designing interactive products having a salient sonic behaviour. Products, in this context, are either tangible and functional objects that are designed to be manipulated, or usable simulations of such objects as in virtual prototyping. Research and development in this area relies on studies from other disciplines, such as:\n\nIn design research for sonic products a set of practices have been inherited from a variety of fields. Such practices have been tested in contexts where research and pedagogy naturally intermix. Among these practices it suffices to mention:\n\nIn the context of sonic interaction design, interactive art and music projects are designing and researching aesthetic experiences where sonic interaction is in the focus. The creative and expressive aspects – the aesthetics – are more important than conveying information through sound. Practices include installations, performances, public art and interactions between humans through digitally-augmented objects/environments. These often integrate elements such as embedded technology, gesture-sensitive devices, speakers or context-aware systems. \n\nThe experience is in the focus, addressing how humans are affected by the sound, and vice versa. Interactive art and music allows us to question existing paradigms and models of how we interact with technology and sound, going beyond paradigms of control (human controlling a machine). Users are part of a loop which includes action and perception. \n\nInteractive art and music projects invite explorative actions and playful engagement. There is also a multi-sensory aspect, especially haptic-audio and audio-visual projects are popular. Amongst many other influences, this field is informed by the development of the roles of instrument-maker, composer and performer merging. \n\nArtistic research in sonic interaction design is about productions in the interactive arts and performing arts, exploiting the role of enactive engagement with sound–augmented interactive objects.\n\nSonification is the data-dependent generation of sound, if the transformation is systematic, objective and reproducible, so that it can be used as scientific method.\n\nFor sonic interaction design, sonification provides a set of methods to create interaction sounds that encode relevant data, so that the user can perceive or interpret the conveyed information. Sonification does not necessarily need to represent huge amounts of data in sound, but may only convey one or few data values in a sound. To give an example, imagine a light switch that, on activation would create a short sound that depends on the electric power consumed through the cable: more energy-wasting lamps would perhaps systematically result in more annoying switch sounds. This example shows that sonification aims to provide some information by using its systematic transformation into sound. \n\nThe integration of data-driven elements in interaction sound may serve different purposes: \n\nWithin the field of sonification, sonic interaction design acknowledges the importance of human interaction for understanding and using auditory feedback. Within sonic interaction design, sonification can help and offer solutions, methods, and techniques to inspire and guide the design of products or interactive systems.\n\n"}
{"id": "3417685", "url": "https://en.wikipedia.org/wiki?curid=3417685", "title": "Spoonplug", "text": "Spoonplug\n\nA spoonplug is a form of fishing lure used for deep water casting and trolling. It was invented in 1946 by Elwood L. \"Buck\" Perry. Perry combined science with a logical approach to fishing to create a \"total fishing system.\" He is credited as being the father of structure fishing and was later inducted into the National Freshwater Fishing Hall of Fame.\n"}
{"id": "38724644", "url": "https://en.wikipedia.org/wiki?curid=38724644", "title": "Squash ball machine", "text": "Squash ball machine\n\nA squash ball machine is a mechanical device that automatically throws out squash balls at different speeds and angles. Its main purpose is to help players to develop their ball hitting technique.\n\nThe features of squash ball machines are modeled after the rules of squash and the constituency of squash balls.\n\nSquash is a ballgame that is played by hitting a ball with a racket against a wall, similar to a backboard, so that the ball bounces off the wall. The opponent player has to hit the ball back against the wall without letting the ball bounce more than once. The game can be played either by two people (the \"singles\" version), or by four people (the doubles version), in either a smaller court (singles and softball doubles) or a larger one (hardball doubles).\n\nThe balls used in squash are small rubber balls that can differ in terms of diameter, weight and softness. These features determine the speed and bouncing amplitude of the ball, with softer balls being generally slower than the harder balls. The temperature of the ball also affects the amount of bounce: the warmer the ball, the more it bounces. Because cold squash balls have little bounce, the squash balls are hit multiple times at the beginning of a game to warm them up. It also means that a game becomes gradually faster as it advances and that softer balls may be too slow to be used in colder climates.\n\nBecause squash balls bounce with different amplitude and frequency depending on their temperature, squash ball machines need to encompass this feature and allow for heating the balls first to give the right feel. The heating system makes squash ball machines different from tennis ball machines which only pump out balls at different speeds and angles.\n\nThe heating function of the machine can be turned down or off, e.g. for slower balls in beginner practices. In higher-end machines, the heat is thermostatically controlled to automatically keep the balls at game temperature.\n\nThe currently available models have ball capacities of 50-60 balls. Both ball speed range and ball frequencies are adjustable and vary from 30 to 150 km/h and 1.5-12 balls per second, respectively, with the .75 second interval suitable for quick volleys and 10 second interval for other strokes. Changing the speed and frequency of the ball coming out of the machine is intended to help the player to visualize real-game situations where the ball does not always come back into play at the same speed. The adjustable speed is also useful for increasing the speed as the player becomes more skillful.\n\nThe machines have different modes and functions for practicing forehands and backhands, fixed, random, decay, interval training, vertical swing, lob and drop. They are powered by electrical AC and DC power supplies and can be controlled remotely. Protective eyewear is recommended while using the machine.\n\nSquash ball machines cannot be used for squash tennis which is played with tennis balls that are considerably bigger than squash balls.\n\nSquash ball machines are used as training aids both by solo players and by coaches in squash practices.\n\nIndividual players use the machine as a feeding (serving) device in the absence of other players to practice their swinging technique and to develop footwork and timing.\n\nIn connection with coaching, the following benefits have been mentioned:\n\nThe main limitation of the ball machine is the same as its main benefit, that of excluding a human partner. Because competitive games are played against another person, the singular use of the machine would mean that a player would not learn to anticipate and react to the movement of the ball from the opponent player based on \"pre-impact cues\" that precede the time the ball is struck, missing valuable time for planning his or her own response, and would not develop tactical skills. Because balls that are thrown by the machine are independent of player's own strokes, the player would also not develop self-reflection and awareness about his or her role in directing the game.\n\n"}
{"id": "29327110", "url": "https://en.wikipedia.org/wiki?curid=29327110", "title": "Switch adapted toys", "text": "Switch adapted toys\n\nSwitch adapted toys are toys which have been adapted so that their original switches are redirected to a larger switch that is easier for the child to interact with. Many children with limited fine and gross motor skills cannot play with regular battery-operated toys.\n\nFor children or young people who have profound and extremely limiting physical and intellectual disabilities, operating a switch adapted toy may be the first independent thing they can do, which builds confidence and enjoyment as well as intellectual stimulation and potential learning.\n\nDepending on the user's abilities, different switches are available for different purposes, including: finger switches, foot switches and button switches of many different sizes and varieties. Some switches have special textural coverings - soft fabric, \"squishy\" latex, pom pom, or \"bump\" patterned for the visually impaired. Use of switches depends on the user's muscle tone, spasticity, visual ability, cognitive function and interests. Switch adapted toys can generate blinking or changing lights and music, speaking, vibration, noises, fans, aromatherapy, massage, songs with number, alphabet or nursery rhyme content and even voice recording and playback.\n\nA number of suppliers exist in Australia but they use imported switch adapted toys which means the toys are expensive to buy. Toy libraries like Noah's Ark in Perth, Western Australia stock a good range of switch adapted toys so that families can borrow to explore the best type of switch and the most interesting toys for their child.\n\nToy libraries import switch adapted toys or have volunteer helpers who are able to take an existing battery operated toy and convert it to switch adapted. Toys which have only one function are perfect for adaptation but more complex, multi-function toys may not be so easy. Adapting a battery toy at home can be achieved with the use of a small number of tools. \n\nA crowd funding page Toys for Tots through The Cerebral Palsy Alliance exists to generate funds needed to create a supply of switch adapted toys for use in a number of designated local libraries in Perth, starting a programme offering the toys to eligible children in locations closer to their homes.\n\nPediastaff Newsletter on how to use switch adapted toys as therapy.\n\nTechnical Solutions Australia Innovative Assistive Technology \"Special Needs Toys\" \n\n<nowiki>http://tecsol.com.au/cms123/index.php?option=com_content&view=article&id=49&category_id=23&main_cat=23&Itemid=</nowiki>88\n"}
{"id": "22979159", "url": "https://en.wikipedia.org/wiki?curid=22979159", "title": "TechAmerica", "text": "TechAmerica\n\nTechAmerica is a US-based technology trade association. It was formed from the merger of AeA (formerly known as the America Electronics Association), the Cyber Security Industry Alliance (CSIA), the Government Electronics & Information Technology Association (GEIA), and the Information Technology Association of America (ITAA) in 2009. The organization claims to be the \"high-tech industry's leading trade association\". TechAmerica represents 1,200 companies within the public and commercial sectors of the economy. TechAmerica's stated goal is to provide \"grassroots to global\" representation for its members. To this end, the organization maintains an advocacy program in all 50 US state capitals, in Washington, DC, and in several international locations. In May 2014, CompTIA, a nonprofit trade association that serves IT professionals, announced it had acquired TechAmerica in a move to expand its public-sector presence.\n\nIn 2009, AeA and ITAA merged to form TechAmerica.\n\nAeA started as the West Coast Electronics Manufacturing Association (WCEMA), formed by David Packard and 25 of Hewlett-Packard's suppliers in 1943. Within 20 years, the association had gathered over 200 members. In 1969, WCEMA was rebranded the Western Electronic Manufacturers Association (WEMA). Less than two years following that rebranding, membership reached to over 600. Once again, the association was renamed in 1977 to the American Electronics Association. In 2001, the branding was shortened to AeA.\n\nThe Association of Data Processing Services Organization (ADAPSO) was formed in 1961. This association was renamed in 1991 to the Information Technology Association of America (ITAA). In 2008, ITAA merged with the Cyber Security Industry Alliance (CSIA) and the Government Electronics Industry Association (GEIA).\n\nThe organization's website was attacked in April 2012 for their support of the controversial CISPA bill.\n\nIn July 2013, TechAmerica sold its standards program to SAE International.\n\nOn November 4, 2013 it was announced that four TechAmerica lobbyists: Trey Hodgkins, Pam Walker, Erica McCann and Carol Henton had resigned, lured to the Information Technology Industry Council (ITI) which was able to raise $50,000 each from more than a dozen of its members to fund the acquisition of the four TechAmerica lobbyists.\n\nTechAmerica filed a lawsuit against ITI and three of the departing lobbyists in D.C. Superior Court. TechAmerica's complaints include that the defecting lobbyists conspired in their new positions to use old contacts and other information acquired while at TechAmerica to help ITI find new clients for its neophyte effort focused on government procurement \n\n\n\nTodd Thibodeaux\nPresident & CEO\n\nElizabeth Hyman\nExecutive Vice President, Public Advocacy\n\nNancy Hammervik\nSenior Vice President, Industry Relations\n\nDavid Sommer\nChief Financial Officer\n\nL. Daniel Liutikas\nChief Legal Officer\n"}
{"id": "36926801", "url": "https://en.wikipedia.org/wiki?curid=36926801", "title": "Wells light", "text": "Wells light\n\nA Wells light was a large paraffin-fuelled (kerosene) blowlamp used for engineering work, particularly for illumination, in Victorian times. At a time before widespread electrical lighting, they were the most common form of high-powered portable illumination used for construction work, particularly railways, civil engineering, shipyards and ironworks.\n\nThe Wells light was a typical blowlamp in principle, consisting of a floor-standing fuel tank with the burner on a tall post above it. It was distinguished by its large size, the more common plumber's blow lamp being a hand-held tool of about a pint in capacity. Wells lights were made in a number of sizes, the smallest Nº1 being of 800 candlepower with a 15 inch flame. Weighing when filled, it was advertised as \"can be carried by a boy\". The largest Nº3 produced 2,000 candlepower, weighed 240 lbs, and was available with barrow wheels for portability.\n\nThe burner of the Wells light used a vaporiser that heated the oil before it escaped the nozzle, so vaporising immediately. As the lamp was intended for use with heavy, sooty oils as well, this vaporiser was constructed of a square frame of straight tubes. Each tube was closed with a screwed plug that could be removed for cleaning. For initial lighting, the vaporiser would be preheating by burning a little oil in a tray beneath the burner. The burners were mounted horizontally, although some models were produced with a flexible hose to the burner that allowed it to be rotated vertically.\n\nPressure was provided by a hand-worked stirrup pump on the tank. Once pressurised, a large lamp could burn for some hours before the pressure fell enough to require more pumping. A particular feature of the Wells design was that the tank could be refilled whilst the lamp was still burning (i.e. without releasing the pressure in the tank) by using this same pressurisation pump as a fuel pump to suck in oil from another container.\n\nThe major use of the Wells light was for the illumination of outdoor construction work. They were portable and simple to operate. Their fuel was cheap and commonly available, especially as the Wells' pressure burner could burn a much lower and cheaper grade of oil than the lamp paraffin that was pure enough to not clog a wick lamp. Oil fuels were still more expensive than coal gas though, so fixed lighting, such as in factories, remained on town mains gas.\n\nThe Wells light pre-dated mains electricity but was contemporaneous with early use of the arc lamp.\n\nElectricity had two disadvantages: firstly it required an on-site generating plant. This was expensive and also represented a long-term capital investment that took time simply to build it beforehand. In some cases, the use of a semi-portable engine could provide a generating plant more quickly. Where the need for lighting was mobile, as for the construction of railways or canals, the Wells light had a clear advantage.\n\nSecondly, although the carbon arc lamp was bright, and relatively economical for the illumination produced, individual lamps were expensive and complicated, although powerful. This encouraged their use as the minimum number of large lamps to cover an entire work site. As the arc lamp also has a very small source of light, this gave a particularly harsh lighting. There was a sudden contrast between the illuminated and shadow areas, especially where a point was only in sight of a single lamp. This was recognised as a trip and obstacle hazard, as well as making even the light areas difficult to work under. The Wells light was specifically contrasted with the point-source of the arc lamp and the relatively shadow-free illumination was cited as an advantage in their adverts. Because the smaller Wells lights were so portable, they could be carried into the best position to illuminate a deep shaft or inside a ship's hull.\n\nAs well as illumination, the Wells light was also used where a large portable blowlamp was required for heating. They were sometimes used for heating iron rivets, inserted red hot, during the assembly of structural ironwork. Although such rivets were normally heated in small portable coke braziers (which also used a cheaper fuel), the Wells light was favoured for some final assembly work at height, as the smaller lights were considered lighter and easier to lift into place high atop a bridge.\n\nWells lights were also used in the erection and repair of large stationary steam engines. Typically this was for the shrink-fitting of components such as large crankpins in crankshafts or flywheels. The crankshaft web would be heated to expand it until the pin could be slid or gently hammered into place. On cooling, the pin would be securely held in place.\n\nA. C. Wells & Co. began in Cheetham, Manchester. Their first product was a range of engineer's lamps, simple cast-iron wick lamps that were widely used before electric battery torches. This type of lamp was not very bright and their limited light has been blamed for several engineering failures and losses of life, where an inspection was poorly carried out, owing to poor light.\n\nThe Wells light, with its pressure burner, was an attempt to produce the first really bright portable lighting. It was an immediate success, working well and having no significant competition from other makers. The name \"Wells light\" soon became a genericized name for this type of light, even though almost all were Wells' own, or licensed, products.\n\nWells lights were also produced under licence in Canada, by James Cooper of Montreal, and sold as 'Wallwork & Wells' patent lights.\n\nIn later years, Wells used their knowledge of small pressure vessels and pumps to produce a range of paint spraying equipment. Up to the 1960s, they also produced waste oil filtration equipment.\n"}
