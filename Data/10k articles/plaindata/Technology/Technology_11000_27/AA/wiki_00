{"id": "28368540", "url": "https://en.wikipedia.org/wiki?curid=28368540", "title": "8K resolution", "text": "8K resolution\n\n8K resolution refers to any screen or display with around 8000 pixels width. 8K UHD is the current highest ultra high definition television (UHDTV) resolution in digital television and digital cinematography. 8K in 8K UHD refers to the horizontal resolution of 7,680 pixels, forming the total image dimensions of (7680×4320), also known as 4320p, which refers to the vertical resolution.\n\n8K UHD has twice as many horizontal and twice as many vertical pixels as 4K UHD, as well as four times the linear resolution of 1080p (Full HD), and six times the linear resolution of 720p.\n\nHigh-resolution displays such as 8K allow for each pixel to be indistinguishable to the human eye when viewed at a typical distance from the screen. 8K resolution can also be used for the purpose of creating enhanced lower resolution videos through a combination of cropping techniques and/or with downsampling techniques used in video and film editing. Resolutions such as 8K allows filmmakers to shoot in a high resolution with a wide lens or at a further distance, in the case of potentially dangerous subjects (such as in wildlife documentaries), by being able to zoom and crop digitally in post-production. The technique involves taking a portion of the original 8K image and cropping it to match a smaller resolution such as the current industry standard for high-definition televisions (4K, 1080p, and 720p).\n\n8K display resolution is the successor to 4K resolution. TV manufacturers pushed to make 4K a new standard by 2017. The feasibility of a fast transition to this new standard is questionable in view of the absence of broadcasting resources.\n\n, few cameras had the capability to shoot video in 8K, with NHK being one of the only companies to have created a small broadcasting camera with an 8K image sensor. By 2018 Red Digital Cinema Camera Company had delivered three 8K cameras in both a Full Frame sensor and Super 35 sensor. Until major content sources are available, 8K is speculated to become a mainstream consumer display resolution around 2023 as mentioned in UHD forum Phase-B recommendations. Despite this, \nfilmmakers are pushing demand for 8K cameras due to their ability to capture better 4K footage.\n\nJapan's public broadcaster NHK was the first to start research and development of 4320p resolution in 1995. The format was standardized by SMPTE in October 2007, Interface standardized by SMPTE in August 2010 and Recommended as the international standard for television by lTU-R in 2012. Followed by public displays at electronics shows and screenings of 2014 Winter Olympics in Sochi and public viewings in February 2014 and the FIFA World Cup in Brazil in June 2014 using HEVC with partners AstroDesign and Ikegami Electronics.\n\nOn January 6, 2015, the MHL Consortium announced the release of the superMHL specification which will support 8K resolution at 120 fps, 48-bit video, the Rec. 2020 color space, high dynamic range support, a 32-pin reversible superMHL connector, and power charging of up to 40 watts.\n\nOn March 1, 2016, The Video Electronics Standards Association (VESA) unveiled DisplayPort 1.4, a new format that lets the use of 8K resolution (7680×4320) at 60 Hz with HDRR and 32 audio channels through USB-C.\n\nOn January 4, 2017, the HDMI Forum announced HDMI 2.1 featuring support for 8k video with HDR, will be \"released early in Q2 2017\".\n\nOn April 6, 2013, Astrodesign Inc. announced the AH-4800, capable of recording 8K resolution.\nIn April 2015 it was announced by Red that their newly unveiled Red Weapon VV is also capable of recording 8K footage. \nIn October 2016 they announced two additional 8K cameras, Red Weapon 8K S35 and Red Epic-W 8K S35. The Red Weapon Dragon VV has been discontinued , when Red unveiled the Red Weapon Monstro VV, their fourth camera capable of shooting 8K, with additional improvements in dynamic range and noise reduction, among other features.\n\nIn 2007, the original 65 mm negative of the 1992 film \"Baraka\" was re-scanned at 8K with a film scanner built specifically for the job at FotoKem Laboratories, and used to remaster the 2008 Blu-ray release. \"Chicago Sun-Times\" critic Roger Ebert described the Blu-ray release as \"the finest video disc I have ever viewed or ever imagined.\" A similar 8K scan/4K intermediate digital restoration of \"Lawrence of Arabia\" was made for Blu-ray and theatrical re-release during 2012 by Sony Pictures to celebrate the film's 50th anniversary. According to Grover Crisp, executive VP of restoration at Sony Pictures, the new 8K scan has such high resolution that when examined, showed a series of fine concentric lines in a pattern \"reminiscent of a fingerprint\" near the top of the frame. This was caused by the film emulsion melting and cracking in the desert heat during production. Sony had to hire a third party to minimise or eliminate the rippling artifacts in the new restored version.\n\nOn May 17, 2013, the Franklin Institute premiered \"To Space and Back\", an 8K×8K, 60 fps, 3D video running approximately 25 minutes. During its first run at the Fels Planetarium it was played at 4K, 60 fps.\n\nIn November 2013, NHK screened the experimental-drama short film \"The Chorus\" at Tokyo Film Festival which was filmed in 8k and 22.2 sound format.\n\nOn May 1, 2015, an 8K abstract computer animation was screened at the Filmatic Festival at the University of California, San Diego. The work was created as an assignment in the VIS 40/ICAM 40 Introduction to Computing in the Arts class taught at UCSD by Associate Teaching Professor Brett Stalbaum during the winter quarter of 2015, with each student producing 300 8192×4800 pixel frames. The work's music soundtrack was composed by Mark Matamoros.\n\nOn January 6, 2016, director James Gunn stated that the 2017 film \"Guardians of the Galaxy Vol. 2\" would be the first (feature) film to be shot in 8K, using the Red Weapon 8K VV. \"Villain\" (2017) is the first Indian film to be shot completely in 8K resolution.\n\nJapanese public broadcaster NHK began research and development on 8K in 1995, having spent over $1 billion on R & D since then. Codenamed Super Hi-Vision (named after its old Hi-Vision analog HDTV system), NHK also was simultaneously working on the development of 22.2 channel surround sound audio. The world's first 8K television was unveiled by Sharp at the Consumer Electronics Show (CES) in 2012. Experimental transmissions of the resolution were tested with the 2012 Summer Olympics, and at the Cannes Film Festival showcasing \"Beauties À La Carte\", a 27-minute short showcased publicly on a 220” screen, with a three-year roadmap that entails the launch of 8K test broadcasting in 2016, with plans to roll out full 8K services by 2018, and in time for the 2020 Summer Olympics. On December 1, 2018, NHK launched BS8K, a broadcast channel with 8K capability.\n\n8K UHD is a resolution of 7680 × 4320 (33.2 megapixels) and is one of the two resolutions of ultra high definition television, the other being 4K UHD. In 2013, a transmission network's capability to carry HDTV resolution was limited by internet speeds and relied on satellite broadcast to transmit the high data rates. The demand is expected to drive the adoption of video compression standards and to place significant pressure on physical communication networks in the near future.\n\n8K UHD has four times the horizontal and vertical resolution of the 1080p HDTV format, with sixteen times as many pixels overall.\n\n8K fulldome is a resolution of 8192×8192 (67.1 megapixels) and is the resolution of top-end modern projection for hemispherical fulldome theatres often seen in planetaria.\n\n\n\n\n"}
{"id": "3405961", "url": "https://en.wikipedia.org/wiki?curid=3405961", "title": "ANS synthesizer", "text": "ANS synthesizer\n\nThe ANS synthesizer is a photoelectronic musical instrument created by Russian engineer Evgeny Murzin from 1937 to 1957. The technological basis of his invention was the method of graphical sound recording used in cinematography (developed in Russia concurrently with USA), which made it possible to obtain a visible image of a sound wave, as well as to realize the opposite goal—synthesizing a sound from an artificially drawn sound spectrogram.\n\nIn this case the sine waves generated by the ANS are printed onto five glass discs using a process that Murzin (an optical engineer) had to develop himself. Each disc has 144 individual tracks printed onto it, for a total of 720 microtones (discrete pitches), spanning 10 octaves. This yields a resolution of 1/72 octave (16.67 cents). The modulated light from these wheels is then projected onto the back of the synthesizer's interface. These are arranged in a continuous swath vertically, with low frequencies at the bottom and high frequencies at the top.\n\nThe user interface consists of a glass plate covered in non-drying opaque black mastic, which constitutes a drawing surface upon which the user makes marks by scratching through the mastic, and therefore allowing light to pass through at that point. In front of the glass plate sits a vertical bank of twenty photocells that send signals to twenty amplifiers and bandpass filters, each with its own gain adjust control. It is akin to a ten-octave equalizer with two knobs per octave. The ANS is fully polyphonic and will generate all 720 pitches simultaneously if required (a vertical scratch would accomplish this).\n\nThe glass plate can then be scanned left or right in front of the photocell bank in order to transcribe the drawing directly into pitches. In other words, it plays what one has drawn, similar to how a score is written. This process can be aided with a gear-motor drive (similar to an engineering lathe) or it can be moved manually. The scan speed is adjustable down to zero. The speed at which the score scans has no relation to pitch but serves only as a means of controlling duration.\n\nMurzin named his invention in honour of the composer Alexander Nikolayevich Scriabin (ANS): Scriabin (1872–1915) was an occultist, theosophist, and early exponent of color-sound theories in composition. The synthesizer was housed in the electronic-music studio situated above the Scriabin Museum (just off of the Arbat in central Moscow) before moving to the basement of the central university on the corner of Bolshaya Nikitskaya. It was saved from the scrapheap thanks to Stanislav Kreichi, who persuaded the university to look after it.\n\nThe ANS was used by Stanislav Kreichi, Alfred Schnittke, Edison Denisov, Sofia Gubaidulina, and other Soviet composers. Edward Artemiev wrote many of his scores of the movies of Andrei Tarkovsky with the help of the ANS. Of particular note is Artemiev's score of Tarkovsky's \"Solaris\" in which the ANS was used to abstract, sci-fi effect akin to ambient music.\nAfter several years at the Theremin Center, the ANS (there is only one—the original was destroyed and this is the improved version) is now located in the Glinka State Central Museum of Musical Culture in Moscow.\n\nAn album of works by the composers mentioned above, called \"Musical Offering\" was released on Melodiya (C60 30721 000) in 1990—although the recordings date from the 1960s and 1970s. Recordings by Stanislav Kreichi—\"Ansiana\" and \"Voices and Movement\"—as well as earlier works (\"Electroshock Presents: Electroacoustic Music\") that used the synthesizer are available on Electroshock Records. A soundtrack of the film \"Into Space\" (1961) in collaboration with Edward Artimiev remains unreleased. In 2002, BBC Radio 4 broadcast a program about the ANS by Isobel Clouter as part of her \"Soundhunter\" series. In 2004, the British experimental group Coil released \"CoilANS\", a boxed set of experimental drone music performed on the ANS. The Norwegian artist Zweizz released a cassette in 2007 on which side B is made entirely out of ANS recordings. The British experimental group T.A.G.C. utilized sounds generated on the ANS on two compositions that were released in 1996 on the \"Deepnet\" compilation album.\n\n\n"}
{"id": "57088590", "url": "https://en.wikipedia.org/wiki?curid=57088590", "title": "American Union of Decorative Artists and Craftsmen", "text": "American Union of Decorative Artists and Craftsmen\n\nThe American Union of Decorative Artists and Craftsmen (AUDAC) was an American society of designers and decorative artists that was active from 1928 until the early 1930s. The group aimed to bring modern principles of design, such as those promoted in Europe by the Wiener Werkstätte and the Bauhaus, to decorative arts in the United States.\n\nThe modernist furniture designer Paul T. Frankl immigrated from Vienna to New York in 1914. He worked to establish Europe's flourishing principles of contemporary design in the United States by writing books such as \"New Dimensions: The Decorative Arts of Today\" and by establishing a collective of fellow designers called the American Union of Decorative Artists and Craftsmen (AUDAC) in 1928. Early members included designers who exhibited at the American Designers' Gallery, which showed interiors and furnishings designed by contemporary New York designers, such as Donald Deskey and Ilonka Karasz.\n\nAUDAC was modeled on European decorative arts societies such as the Société des Artistes Décorateurs in France. They published books on the new American design such as \"Modern American Design\" and \"Annual of American Design\" and put together influential exhibitions of their members' work, including one at the Brooklyn Museum in 1931 which showed work by Russel Wright, Ruth Reeves, and Rockwell Kent among many others.\n\nThe group was well known by the early 1930s when Radio City Music Hall commissioned Donald Deskey and other AUDAC members to design the fabrics, textiles, and furnishings for the interior. Many members (see list below) went on long careers and well-known places in the history of twentieth-century design. However, due to the 1930s economic depression, the formal association was defunct by 1934.\n\nAUDAC placed an ad in a supplement for the magazine \"Creative Art\" in March 1930 which listed contact information for many of its members and described the mission of the American Union of Decorative Artists and Craftsmen thus:\n\nFounded by Paul T. Frankl in 1928.\n\nMembers included: \n"}
{"id": "14547147", "url": "https://en.wikipedia.org/wiki?curid=14547147", "title": "Architectural engineer (PE)", "text": "Architectural engineer (PE)\n\nArchitectural engineer (PE) is a professional engineering designation in the United States. The architectural engineer applies the knowledge and skills of broader engineering disciplines to the design, construction, operation, maintenance, and renovation of buildings and their component systems while paying careful attention to their effects on the surrounding environment.\n\nWith the establishment of a specific \"Architectural Engineering\" NCEES professional engineering registration examination in the 1990s and first offering in April 2003, architectural engineering is now recognized as a distinct engineering discipline in the United States.\n\nNote that in the United States \"architectural engineering technology\" is different from architectural engineering; in the United States architectural engineering technologists tend to be drafters or assistants for the design and construction process, while in Europe, Canada, South Africa and other countries Architectural technologists have a role similar to Architects and Architectural Engineers.\n\n\nA common combined specialization is \"Mechanical, Electrical and Plumbing\", better known by its abbreviation MEP. An MEP design engineer has experience in HVAC, lighting/electrical, and plumbing systems' analysis and design.\n\n\nPrograms accredited by the Engineering Accreditation Commission (EAC) of ABET and that are members of Architectural Engineering Institute (AEI) are denoted below.\n\n"}
{"id": "1763527", "url": "https://en.wikipedia.org/wiki?curid=1763527", "title": "Big Brutus", "text": "Big Brutus\n\nBig Brutus is the nickname of the Bucyrus-Erie model 1850-B electric shovel, which was the second largest of its type in operation in the 1960s and 1970s. Big Brutus is the centerpiece of a mining museum in West Mineral, Kansas where it was used in coal strip mining operations. The shovel was designed to dig from down to unearth relatively shallow coal seams, which would themselves be mined with smaller equipment.\n\nFabrication of Big Brutus was completed in May 1963, after which it was shipped in 150 railroad cars to be assembled in Kansas. It operated until 1974, when it became uneconomical to mine coal at the site. At that time it was considered too big to move and was left in place.\n\nBig Brutus, while not the largest electric shovel ever built, is the largest electric shovel still in existence. The Captain, at 28 million pounds – triple that of Big Brutus – was the largest shovel and one of the largest land-based mobile machines ever built, only exceeded by some dragline and bucket-wheel excavators. It was scrapped in 1992, after receiving extreme damage from an hours-long internal fire.\n\nThe Pittsburg & Midway Coal Mining Company donated Big Brutus in 1984 as the core of a mining museum which opened in 1985. In 1987, the American Society of Mechanical Engineers designated Big Brutus a Regional Historic Mechanical Engineering Landmark. It was listed on the National Register of Historic Places in 2018.\n\nThe museum offers tours and camping.\n\n\n"}
{"id": "14719594", "url": "https://en.wikipedia.org/wiki?curid=14719594", "title": "Bleachfield", "text": "Bleachfield\n\nA bleachfield or bleaching green was an open area used for spreading cloth on the ground to be purified and whitened by the action of the sunlight. Bleaching fields were usually found in and around mill towns in Great Britain and were an integral part of textile manufacture during the Industrial Revolution.\n\nWhen cloth-making was still a home-based occupation, the bleachfields could be found on Scottish crofts and English farm fields. Just as wool needed fulling and flax needed retting, so did the semi-finished fabrics need space and time outdoors to bleach. In the 18th century there were many linen bleachfields in Scotland, particularly in Perthshire, Renfrewshire in the Scottish Lowlands, and the outskirts of Glasgow. By the 1760s, linen manufacture became a major industry in Scotland, second only to agriculture. For instance, in 1782 alone, Perthshire produced 1.7 million yards of linen, worth £81,000 (£ as of 2018).\n\nBleachfields were also common in northern England; for instance, the name of the town of Whitefield, on the outskirts of Manchester, is thought to derive from the medieval bleachfields used by Flemish settlers.\n\nBleachfields became redundant after Charles Tennant developed a bleaching powder based on chlorine, which permitted year-round processing of fabric indoors, but many of the factories continued to be called bleachfields.\n\nA bleachfield is similar to, but should not be confused with, a tenterground. Bleachfields were a popular subject for Dutch painters in the 17th century. One of the stained glass windows made by Stephen Adam for the Maryhill Burgh Halls in 1878, shows linen bleachers at work.\n\n\n"}
{"id": "10322996", "url": "https://en.wikipedia.org/wiki?curid=10322996", "title": "CALS Raster file format", "text": "CALS Raster file format\n\nThe CALS Raster file format is a standard for the interchange of graphics data. It was developed by the United States Department of Defense (DoD) as part of the Continuous Acquisition and Life-cycle Support (CALS) initiative. It defines a standard storing raster (bit-mapped) image data, either uncompressed or compressed using CCITT Group 4 compression.\n\n\n"}
{"id": "1634947", "url": "https://en.wikipedia.org/wiki?curid=1634947", "title": "Carfax (company)", "text": "Carfax (company)\n\nCarfax, Inc. is a commercial web-based service that supplies vehicle history reports to individuals and businesses on used cars and light trucks for the American and Canadian consumers.\n\nIn 1984 Carfax was founded in Columbia, Missouri, by a computer professional named Ewin Barnett III working together with Robert Daniel Clark, an accountant from Huntingdon, Pennsylvania. The company is now headquartered in Centreville, Virginia, with a data center operation in Columbia, Missouri. Barnett was initially trying to combat odometer fraud. By working closely with the Missouri Automobile Dealers Association, in 1986 he offered the early version Carfax vehicle history report to the dealer market. These reports were developed with a database of just 10,000 records and were distributed via fax machine. By the end of 1993, Carfax obtained title information from nearly all fifty states. In December 1996, the company's website was launched to offer consumers the same vehicle history reports already available to businesses. In the fall of 1999, Carfax became a wholly owned subsidiary of R.L. Polk & Company. In 2013, IHS acquired Polk and Carfax, which added to its Automotive offerings. In 2013 Carfax introduced a free service to help vehicle owners keep their cars well maintained – myCARFAX. Car owners can track their service history, receive automatic service alerts and get critical information about open safety recalls reported for their car. In March 2016 IHS had a merger of equals with Markit, and in July of that year became IHS Markit. \nA similar company with the same name operated in New Zealand in 2007 and 2008 when they changed their name to Carjam Online Limited.\n\nCarfax offers several free products and services and charges a fee for more comprehensive reports.\n\nIn 2013, Carfax launched a new free service called myCARFAX. The service provides free access to vehicle service information reported to the company and alerts users to recommended maintenance.\n\nThe company offers four vehicle research services—Lemon Check, Record Check, Recall Check, and Problem Car. While these services are helpful, they do not contain all of the information provided in a full Carfax vehicle history report.\n\nThe company also provides Car Safety and Reliability Ratings, which provides access to reviews and other data from sources such as the National Highway Traffic Safety Administration, the Insurance Institute for Highway Safety, J.D. Power and Associates, IntelliChoice and others.\n\nThe Carfax Vehicle History Report is the company's core product. Users purchase either a single report or create an account for building multiple reports for different vehicles, allowing consumers to utilize Carfax over a period of time as they search for a vehicle. Additionally, buyers can request Carfax reports for free from auto dealers who offer Carfax service, and some automakers routinely provide Carfax reports as part of their pre-owned vehicle programs.\n\nIn early 2012 the company launched a new service which uses service information reported to CARFAX to help vehicle owners with things like oil changes and routine vehicle maintenance. The service was later expanded to include alerts about open safety recalls issued by vehicle manufacturers. Vehicle owners who are interested in having service information included on their CARFAX Vehicle History Report can find local service shops who will report them to CARFAX.\n\nIn April 2014, Carfax introduced another free service called CARFAX Used Car Listings. The new service gives used car shoppers the opportunity to search for used cars using make, model, and a vehicle's history. For example, consumers may search for vehicles with \"no reported accident\" or \"service records\".\n\nCarfax claims to have access to twenty billion records from more than 100,000 sources, including motor vehicle departments for all 50 U.S. states and all 10 Canadian provinces. The company's information sources include U.S. state title and registration records, auto and salvage auctions, Canadian motor vehicle records, rental and fleet vehicle companies, consumer protection agencies, state inspection stations, extended warranty companies, insurance companies, fire and police departments, manufacturers, inspection companies, service and repair facilities, dealers and import/export companies.\n\nCARFAX lists only information that is reported to them and consumers should not take this report to be a complete accident history. Not all accidents are disclosed and CARFAX uses the language \"no accidents have been reported to CARFAX,\" the emphasis being on \"reported\". Consumers should not rely on CARFAX alone when checking out a used vehicle.\n\nAlthough Carfax continuously expands its database and resources, some information is not allowed to be provided. Under the 1994 U.S. \"Drivers Privacy Protection Act\", personal information such as names, telephone numbers and addresses of current or previous owners are neither collected nor reported. Carfax does not have access to every facility and mistakes are sometimes made by those who input data. In the event information is disputed but cannot be verified, Carfax allows consumers and dealerships to add information to its reports.\n\nIn a 2006 class action lawsuit, West v. Carfax, Inc., the plaintiff claimed that Carfax violated consumer protection laws by not disclosing the limitations of their service, specifically their inability to check accident records in 23 states in the U.S. while stating that their database contains information from all 50 states. The lawsuit was settled in May 2007 in the Trumbull County Common Pleas Court in Warren, Ohio. Carfax spokesman Larry Gamache said more than 10 million consumers were affected. The company asserts that it has major accident information from all 50 states and it backs up its claim with a buyback guarantee. The settlement in the West v. Carfax, Inc lawsuit was overturned, not on the merits of the issue, but on the terms of the settlement which did not offer enough to the impacted consumers and because \"not enough consumers were notified and the judge should not have agreed to the settlement without knowing more about what it would cost Carfax. \n\n\n"}
{"id": "3282728", "url": "https://en.wikipedia.org/wiki?curid=3282728", "title": "Carpet beater", "text": "Carpet beater\n\nA carpet beater or carpetbeater (also referred to as a rug beater or rugbeater, mattenklopper, carpet whip, rug whip, clothes-beater, dust beater or dustbeater, carpet duster, wicker slapper, rug duster, or pillow fluffer, and formerly also as a carpet cleaner or rug cleaner) is a housecleaning tool that was in common use until the vacuum cleaner became affordable during the early 20th century. Carpets, rugs, clothes, cushions, and bedding were hung over a clothesline or railing and the dust and dirt was beaten out of them. Typically made of wood, rattan, cane, wicker, spring steel or coiled wire, antique rug beaters have become very collectible. Modern mass-production versions can also be in plastic or wire.\n\nIts use in cleaning has been largely replaced since the 1950s by the carpet sweeper and then the vacuum cleaner, although they are still sold in many household stores throughout Europe. In Germany an outdoor carpet hanger for beating is called a \"Teppichstange\" (carpet bar) or \"Klopfstange\". In Poland it is called \"trzepak\" (a noun from the word \"trzepać\", \"to beat\"; the beater itself is called \"trzepaczka\"). In the Netherlands it is called \"mattenklopper\" (mat knocker)\n\nSince the 1990s, it is very rare to see anyone using a trzepak for its prime function. In the newest housing developments, trzepak are rarely installed. Some people preferred to beat carpets in winter on the snow - they laid the carpet face down and beat it. This method had some advantages - for instance, insects would freeze to death even if they were not expelled through beating - but it left a dirty and unpleasant-looking patch on the snow, and therefore some communities forbade beating on the snow for aesthetic reasons.\n"}
{"id": "32034310", "url": "https://en.wikipedia.org/wiki?curid=32034310", "title": "Caviar spoon", "text": "Caviar spoon\n\nCaviar spoons are traditionally made of inert materials, such as mother of pearl, gold, animal horn, and wood.\n\nThere is a custom that caviar should not be served with a metal spoon, because metal may impart an undesirable flavour. Some food experts point out that caviar is stored and sold in metal tins, and therefore any effect of metal on caviar flavour is a misconception; however, others point out that silver is reactive, and may affect caviar flavour.\nCaviar spoons range in length from 3 to 5 inches, and have a small shallow bowl that may be either oval or paddle shaped.\n\n"}
{"id": "2544439", "url": "https://en.wikipedia.org/wiki?curid=2544439", "title": "Coherent control", "text": "Coherent control\n\nCoherent control is a quantum mechanical based method for controlling dynamical processes by light.\nThe basic principle is to control quantum interference phenomena typically by shaping the phase of a laser pulses. The basic ideas have proliferated finding vast application in spectroscopy \nmass spectra, quantum information processing, laser cooling, ultracold physics and more.\n\nThe initial idea was to control the outcome of chemical reactions. Two approaches were pursued: In the time domain a pump dump scheme where the control is the time delay between pulses\nand in the frequency domain, interfering pathways controlled by one and three photons.\nThe two basic methods eventually merged with the introduction of optimal control theory\n\nExperimental realisations soon followed. In the time domain \nand in the frequency domain. \nTwo interlinked developments accelerated the field of coherent control: Experimentally it was the development \nof pulse shaping by a spatial light modulator \nand its employment in coherent control.\nThe second development was the idea of automatic feedback control\n\nCoherent control aims to steer a quantum system from an initial state to a target state via\nan external field. For given initial and final (target) states the coherent control is termed state-to-state\ncontrol. A generalisation is steering simultaneously an arbitrary set of initial pure states to an\narbitrary set of final states, i.e. controlling a unitary transformation. Such an application sets the\nfoundation for a quantum gate operation.\n\nControllability of a closed quantum system has\nbeen addressed by Tarn and Clark. \nTheir theorem based in control theory states that for a finite dimensional closed\nquantum system, the system is completely controllable, i.e. an arbitrary unitary transformation\nof the system can be realized by an appropriate application of the controls, \nif the control operators and the unperturbed Hamiltonian generate the Lie algebra of all Hermitian operators.\nComplete controllability implies state-to-state controllability.\n\nThe computational task of finding a control field for a particular state to state\ntransformation is difficult and becomes more difficult with the increase in the size of the system.\nThis task is in the class of hard inversion problems of high computational complexity. \nThe algorithmic task of finding the\nfield that generates a unitary transformation scales factorially more difficult with the size of the\nsystem. This is because a larger number of state-to-state control fields have to be found\nwithout interfering with the other control fields.\n\nOnce constraints are imposed controllability can be degraded. For example, what is the minimum time required to achieve a control objective\n. this is termed quantum speed limit.\n\nThe constructive approach uses a set of predetermined control fields for which the control outcome can be inferred.\nThe pump dump scheme in the time domain and the three vs one photon interference scheme in the frequency domain are prime examples.\nAnother constructive approach is based on adiabtic ideas. \nThe most well studied method is Stimulated raman adiabatic passage STIRAP \nwhich employs an auxiliary state to achieve complete state-to-state population transfer.\n\nOne of the most prolific generic pulse shapes is a chirped pulse a pulse with a varying frequency in time.\n\nOptimal control as applied in coherent control seeks the optimal control field for steering a quantum system to its objective. For state-to-state control the objective is defined as the maximum overlap\nat the final time T with the state formula_1: \nwhere the initial state is formula_3.\nThe time dependent control Hamiltonian has the typical form: \nwhere formula_5 is the control field. Optimal control solves for the optimal field formula_6\nusing the calculus of variations introducing Lagrange multipliers. A new objective functional is defined\nwhere formula_8 is a wavefunction like Lagrange multiplier and the formula_9 parameter regulates the integral intensity.\nVariation of formula_10 with respect to formula_11 and formula_12 leads to two coupled Schrödinger equations \nA forward equation for formula_13 with initial condition formula_14\nand a backward equation for the Lagrange multiplier formula_15 with final condition formula_16. Finding a solution requires an iterative approach.\nDifferent algorithms have been applied for obtaining the control field such as the Krotov method.\n\nA local in time alternative method has been developed, where at each time step\nthe field is calculated to direct the state to the target. A related method has been called tracking \n\nCoherent control has been applied to unimolecular chemical reactions controlling the final outcome\n\nCoherent control has been applied to bimolecular chemical reactions.\n\nCoherent control has been applied to biological photoisomerization of Retinal.\n\nCoherent control has been applied in the field of nuclear magnetic resonance NMR.\n\nCoherent control has been applied in the field of ultracold matter for photoassociation and for laser cooling of internal degrees of freedom \n\nApplications of coherent control to quantum information processing \n\nAn important issue is the spectral selectivity of two photon coherent control\n. These ideas can be applied to single pulse Raman spectroscopy\nand microscopy.\n\nCoherent control has been applied in attosecond physics.\n\nAs one of the cornerstones for enabling quantum technologies, optimal quantum control keeps evolving and expanding into areas as diverse as quantum-enhanced sensing, manipulation of single spins, photons, or atoms, optical spectroscopy, photochemistry, magnetic resonance (spectroscopy as well as medical imaging), quantum information processing and quantum simulation.\nShapiro, Moshe, and Paul Brumer. \"Principles of the quantum control of molecular processes.\" Principles of the Quantum Control of Molecular Processes, by Moshe Shapiro, Paul Brumer, pp. 250. . Wiley-VCH, February 2003. 1 (2003).\n\nRice, Stuart Alan, and Meishan Zhao. Optical control of molecular dynamics. New York: John Wiley, 2000.\n\nd'Alessandro, Domenico. Introduction to quantum control and dynamics. CRC press, 2007.\n\nDavid J. Tannor, \"Introduction to Quantum Mechanics: A Time-dependent Perspective\", (University Science Books, Sausalito, 2007).\n"}
{"id": "5769850", "url": "https://en.wikipedia.org/wiki?curid=5769850", "title": "Coopmans approximation", "text": "Coopmans approximation\n\nThe Coopmans approximation is a method for approximating a fractional-order integrator in a continuous process with constant space complexity. The most correct and accurate methods for calculating the fractional integral require a record of all previous history, and therefore would require a linear space complexity solution O(\"n\"), where \"n\" is the number of samples measured for the complete history.\n\nThe fractor ( fractional capacitor ) is an analog component useful in control systems. In order to model the components behavior in a digital simulation, or replace the fractor in a digital controller, a linear solution is untenable. In order to reduce the space complexity however, it is necessary to lose information in some way. \n\nThe Coopmans approximation is a robust, simple method that uses a simple convolution to compute the fractional integral, then recycles old data back through the convolution. The convolution sets up a weighting table as described by the fractional calculus, which varies based on the size of the table, the sampling rate of the system, and the order of the integral. Once computed the weighting table remains static.\n\nThe data table is initialized as all zeros, which represents a lack of activity for all previous time. New data is added to the data buffer in the fashion of a ring buffer, so that the newest point is written over the oldest data point.\nThe convolution is solved by multiplying corresponding elements from the weight and data tables, and summing the resulting products. As described, the loss of the old data by overwriting with new data will cause echoes in a continuous system as disturbances that were absorbed into the system are suddenly removed.\n\nThe solution to this is the crux of the Coopmans approximation, where the old data point, multiplied by its corresponding weight term, is added to the newest data point directly. This allows a smooth (though exponential, rather than power law) decay of the system history. This approximation has the desirable effect of removing the echo, while preserving the space complexity of the solution.\n\nThe negative effect of the approximation is that the phase character of the solution is lost as the system frequency approaches DC. However, all digital systems are guaranteed to suffer this flaw, as all digital systems have finite memory, and therefore will fail as the memory requirement approaches infinity.\n"}
{"id": "18101972", "url": "https://en.wikipedia.org/wiki?curid=18101972", "title": "Crown block", "text": "Crown block\n\nA crown block is the stationary section of a block and tackle that contains a set of pulleys or sheaves through which the drill line (wire rope) is threaded or reeved and is opposite and above the traveling block.\n\nThe combination of the traveling block, crown block and wire rope drill line gives the ability to lift weights in the hundreds of thousands of pounds. On larger drilling rigs, when raising and lowering the derrick, line tensions over a million pounds are not unusual.\n\n"}
{"id": "57477295", "url": "https://en.wikipedia.org/wiki?curid=57477295", "title": "Design Optimization", "text": "Design Optimization\n\nDesign optimization is an engineering design methodology using a mathematical formulation of a design problem to support selection of the optimal design among many alternatives. Design optimization involves the following stages:\n\n\nThe formal mathematical (standard form) statement of the design optimization problem is \n\nformula_1\n\nwhere\n\n\nThe problem formulation stated above is a convention called the \"negative null form\", since all constraint function are expressed as equalities and negative inequalities with zero on the right-hand side. This convention is used so that numerical algorithms developed to solve design optimization problems can assume a standard expression of the mathematical problem.\n\nWe can introduce the vector-valued functions\n\nformula_11 \" \"\n\nto rewrite the above statement in the compact expression\n\nformula_12\n\nWe call formula_13 the \"set\" or \"system of\" (\"functional\") \"constraints\" and formula_9 the \"set constraint\".\n\nDesign optimization applies the methods of mathematical optimization to design problem formulations and it is sometimes used interchangeably with the term engineering optimization. When the objective function \"f\" is a vector rather than a scalar, the problem becomes a multi-objective optimization one. If the design optimization problem has more than one mathematical solutions the methods of global optimization are used to identified the global optimum.\n\nOptimization Checklist \n\n\nA detailed and rigorous description of the stages and practical applications with examples can be found in the book Principles of Optimal Design.\n\nPractical design optimization problems are typically solved numerically and many exist in academic and commercial forms. There are several domain-specific applications of design optimization posing their own specific challenges in formulating and solving the resulting problems; these include, shape optimization, wing-shape optimization, topology optimization, architectural design optimization, power optimization. Several books, articles and journal publications are listed below for reference.\n\n\n\n\n"}
{"id": "5292533", "url": "https://en.wikipedia.org/wiki?curid=5292533", "title": "Diffuser (optics)", "text": "Diffuser (optics)\n\nIn optics, a diffuser (also called a light diffuser or optical diffuser) is any material that diffuses or scatters light in some manner to transmit soft light. Diffuse light can be easily obtained by reflecting light from a white surface, while more compact diffusers may use translucent material, including ground glass, teflon, holographs, opal glass, and greyed glass.\n\nA perfect (reflecting) diffuser (PRD) is a theoretical perfectly white surface with Lambertian reflectance (its brightness appears the same from any angle of view). It does not absorb light, giving back 100% of the light it receives. Reflective diffusers can be easily characterised by scatterometers.\n\nA flash diffuser spreads the light from the flash of a camera. In effect, the light will not come from one concentrated source (like a spotlight), but rather will spread out, bounce from reflective ceilings and walls, thus getting rid of harsh light, and hard shadows. This is particularly useful for portrait photographers, since harsh light and hard shadows are usually not considered flattering in a portrait.\n\nA diffusion filter (sometimes called a \"shoot-through\" diffuser) is used in front of a flash or studio light to soften the light on the scene being shot.\n\nRecently, photopolymers have been used for making holographic diffusers. Photopolymers offer better performance than other materials and have a large viewing angle. Also, the process of synthesizing photopolymers is much simpler.\n\nA diffractive diffuser is a kind of Diffractive Optical Element (DOE) that exploits the principles of diffraction and refraction. It uses a Diffraction orders to manipulate monochromatic light, giving it a specific spatial-configuration and intensity profile. Diffractive diffusers are commonly used in commercially available LED illumination systems. Usually, the diffuser material is GaN or fused silica with processed rough surfaces. LED diffusers can be characterized online using scatterometry-based metrology.\n\n"}
{"id": "45287126", "url": "https://en.wikipedia.org/wiki?curid=45287126", "title": "Eikon", "text": "Eikon\n\nEikon is a set of software products provided by Refinitiv for financial professionals to monitor and analyse financial information. It provides access to real time market data, news, fundamental data, analytics, trading and messaging tools.\n\nThe main product runs on Windows and lets users compose customized views on multiple screens. It also extends Microsoft Office with data retrieval addins. Eikon subscribers can access Eikon data from their smartphones, iPad, or from a web browser.\n\nThomson Reuters launched Eikon in 2010 as a replacement of Reuters 3000 Xtra, Reuters’ earlier platform. In October 2018 Eikon was transferred to Refinitiv as a result of a larger deal between Blackstone and the Financial & Risk business of Thomson Reuters.\n\nAt the end of 2013, an Eikon subscription was reported to cost from $300 to $1,800 per month, the average setup being around $800 per month.\n"}
{"id": "26916790", "url": "https://en.wikipedia.org/wiki?curid=26916790", "title": "Electronic Banking Internet Communication Standard", "text": "Electronic Banking Internet Communication Standard\n\nThe Electronic Banking Internet Communication Standard (EBICS) is a German transmission protocol developed by the German Banking Industry Committee for sending payment information between banks over the internet. It grew out of the earlier BCS-FTAM protocol that was developed in 1995, with the aim of being able to use internet connections and TCP/IP. It is mandated for use by German banks and has also been adopted by France and Switzerland.\n\nEBICS supports Single Euro Payments Area SEPA as the standard can be used as the secure communication channel to initiate SEPA Direct Debits and SEPA Credit Transfers using the internet. SEPA concentrates on standardisation of clearing protocols in the inter-bank networks. The SEPA-Clearing guidelines do not supplant any national clearing transmission protocols. Theoretically the national banking transmission standards could be prolonged for decades.\n\nIn 2005, the German Zentraler Kreditausschuss (ZKA / \"Central Credit Committee\") initiated a project to replace the national banking clearing system based on FTAM (short BCS-FTAM). The design goals were specifically set to create a transmission protocol that can be used by other countries as well.\n\nOn 1 January 2006, the new EBICS transmission protocol was included in the German DFÜ-Abkommen (EDI-Agreement - enacted first on 15 March 1995). Since 1 January 2008, all German banks must support the EBICS transmission protocol and the required support for BCS-FTAM ends on 31 December 2010. It is expected that German banks will cease support for BCS-FTAM during 2011.\n\nOn 14 November 2008, a cooperation with the French \"Comité Français d’Organisation et de Normalisation Bancaire\" (CFONB - standardisation office in the banking sector of France) was pronounced such that EBICS would be adopted for usage in France. On 5 May 2009, a joint committee was created to resolve a modified EBICS. On 12 February 2010, a common EBICS for Germany and France was published (Version 2.4.2).\n\nMost changes on the common EBICS involved to embed the French ETEBAC-3 message types and ETEBAC-5 signature elements into the EBICS transmission format where currently ETEBAC is transported via X.25 packet network lines (in Germany the BCS-FTAM protocol is using ISDN direct lines). French Telecom closed its X.25 network in November 2011.\n\nThe EBICS protocol is based on an IP network. It allows use of standard HTTP with TLS encryption (HTTPS) for transport of data elements. The routing data elements are encoded in XML and optionally signed and encrypted with X.509 PKI certificates (replacing older RSA keys). The EBICS transmission protocol can be used to wrap SEPA-XML statements as they come forward.\n\nThe standard does include two major areas - for usage in the bank-client transmission including statements of account (MT940/STA) and for interbanking clearing. The German Bundesbank has adopted the EBICS transmission protocol on 28 January 2009 to accept clearing information to be routed to the SWIFTnet interbanking network. The Bundesbank will only accept SEPA statements via SWIFTnet FileAct or EBICS submissions. \n\n\n"}
{"id": "9553082", "url": "https://en.wikipedia.org/wiki?curid=9553082", "title": "Fernseh", "text": "Fernseh\n\nThe Fernseh AG television company was registered in Berlin on July 3, 1929 by John Logie Baird, Robert Bosch and other partners with an initial capital of 100,000 Reichsmark. Fernseh AG did research and manufacturing of television equipment.\n\nThe company name \"Fernseh AG\" is a compound of \"Fernsehen\" ‘television’ and \"Aktiengesellschaft (AG)\" ‘joint-stock company’. The company was mainly known by its German abbreviation \"FESE\". See section see also on this page for other uses.\n\nIn 1929 Fernseh AG's original board of directors included: Emanuel Goldberg, Oliver George Hutchinson (for Baird), David Ludwig Loewe, and Erich Carl Rassbach (for Bosch) and Eberhard Falkenstein who did the legal work.\nCarl Zeiss's company worked alongside the early Bosch company. Much of the early work was in the area of research and development. Along with early TV sets (DE-6, E1, DE10) Fernseh AG made the first \"Remote Truck\"/\"OB van\", an \"intermediate-film\" mobile television camera in August 1932. This was a film camera that had its film developed in the truck and a \"telecine\" then transmitted the signal almost \"live\".\n\n\nIn 1972 Robert Bosch renamed its TV division: Fernsehanlagen GmbH (Fernseh facilities). The company supplied almost all the studio equipment for the 1972 Summer Olympics in Munich.\n\n\n\nPast and current offices in the cities of acquisitions (see History):\n\n"}
{"id": "30992263", "url": "https://en.wikipedia.org/wiki?curid=30992263", "title": "Flatbed trolley", "text": "Flatbed trolley\n\nFlatbed trolleys are a common form of transport in distribution environments, for moving bulk loads. A very simple design offers a basic flat platform with four casters and a fixed handle which is used to either push or pull the platform with the load on the platform. Without a flat surface it becomes an \"open frame\" trolley and without a handle it is a bogie or dolly. \n\nThe frame is usually fabricated steel. The primary flatbed surface can be constructed from wooden boards, plastic, steel or mesh. Flatbed casters can vary dramatically, made of solid rubber, air filled pneumatic or cast iron. The caster is generally the component on the flatbed trolley that limits the safe working capacity.\n\nSpecialised trolleys include the piano dolly, which consistently features small multi-swivel castors and a stronger than usual frame. The \"U-boat\" – used to move and stock goods by retailers such as grocery stores – has two high handles on opposite ends of a thin flatbed. Modern factory systems commonly track individual trolleys digitally to facilitate automated bills of lading; automated systems may have remotely operated or autonomous trolleys for transport during storage and access. \n\nThe trolley shown at right is termed \"a Turntable Trolley\" due to its steering mechanism.\n\n"}
{"id": "237199", "url": "https://en.wikipedia.org/wiki?curid=237199", "title": "Forklift", "text": "Forklift\n\nA forklift (also called lift truck, jitney, fork truck, fork hoist, and forklift truck) is a powered industrial truck used to lift and move materials over short distances. The forklift was developed in the early 20th century by various companies, including Clark, which made transmissions, and Yale & Towne Manufacturing, which made hoists. Since World War II, the use and development of the forklift truck have greatly expanded worldwide. Forklifts have become an indispensable piece of equipment in manufacturing and warehousing. In 2013, the top 20 manufacturers worldwide posted sales of $30.4 billion, with 944,405 machines sold.\n\nThe middle nineteenth century through the early 20th century saw the developments that led to today's modern forklifts. The forerunners of the modern forklift were manually powered hoists that were used to lift loads. In 1906, the Pennsylvania Railroad introduced battery powered platform trucks for moving luggage at their Altoona, Pennsylvania train station. World War I saw the development of different types of material handling equipment in the United Kingdom by Ransomes, Sims & Jefferies of Ipswich. This was in part due to the labor shortages caused by the war. In 1917, Clark in the United States began developing and using powered tractor and powered lift tractors in their factories. In 1919, the Towmotor Company, and Yale & Towne Manufacturing in 1920, entered the lift truck market in the United States. Continuing development and expanded use of the forklift continued through the 1920s and 1930s. The introduction of hydraulic power and the development of the first electric power forklifts, along with the use of standardized pallets in the late 1930s, helped to increase the popularity of forklift trucks.\n\nThe start of World War II, like World War I before, spurred the use of forklift trucks in the war effort. Following the war, more efficient methods for storing products in warehouses were being implemented. Warehouses needed more maneuverable forklift trucks that could reach greater heights and new forklift models were made that filled this need. For example, in 1954, a British company named Lansing Bagnall, now part of KION Group, developed what was claimed to be the first narrow aisle electric reach truck. The development changed the design of warehouses leading to narrower aisles and higher load stacking that increased storage capability. During the 1950s and 1960s, operator safety became a concern due to the increasing lifting heights and capacities. Safety features such as load backrests and operator cages, called overhead guards, began to be added to forklifts produced in this era. In the late 1980s, ergonomic design began to be incorporated in new forklift designs to improve operator comfort, reduce injuries and increase productivity. During the 1990s, exhaust emissions from forklift operations began to be addressed which led to emission standards being implemented for forklift manufacturers in various countries. The introduction of AC power forklifts, along with fuel cell technology, are also refinements in continuing forklift development.\n\nForklifts are rated for loads at a specified maximum weight and a specified forward center of gravity. This information is located on a nameplate provided by the manufacturer, and loads must not exceed these specifications. In many jurisdictions, it is illegal to alter or remove the nameplate without the permission of the forklift manufacturer.\n\nAn important aspect of forklift operation is that it must have rear-wheel steering. While this increases maneuverability in tight cornering situations, it differs from a driver’s traditional experience with other wheeled vehicles. While steering, as there is no caster action, it is unnecessary to apply steering force to maintain a constant rate of turn.\n\nAnother critical characteristic of the forklift is its instability. The forklift and load must be considered a unit with a continually varying center of gravity with every movement of the load. A forklift must never negotiate a turn at speed with a raised load, where centrifugal and gravitational forces may combine to cause a disastrous tip-over accident. The forklift is designed with a load limit for the forks which is decreased with fork elevation and undercutting of the load (i.e., when a load does not butt against the fork \"L\"). A loading plate for loading reference is usually located on the forklift. A forklift should not be used as a personnel lift without the fitting of specific safety equipment, such as a \"cherry picker\" or \"cage\".\n\nForklifts are a critical element of warehouses and distribution centers. It’s imperative that these structures be designed to accommodate their efficient and safe movement. In the case of Drive-In/Drive-Thru Racking, a forklift needs to travel inside a storage bay that is multiple pallet positions deep to place or retrieve a pallet. Often, forklift drivers are guided into the bay through guide rails on the floor and the pallet is placed on cantilevered arms or rails. These maneuvers require well-trained operators. Since every pallet requires the truck to enter the storage structure, damage is more common than with other types of storage. In designing a drive-in system, dimensions of the fork truck, including overall width and mast width, must be carefully considered.\n\nForklift hydraulics are controlled either with levers directly manipulating the hydraulic valves or by electrically controlled actuators, using smaller \"finger\" levers for control. The latter allows forklift designers more freedom in ergonomic design.\n\nForklift trucks are available in many variations and load capacities. In a typical warehouse setting most forklifts have load capacities between one and five tons. Larger machines, up to 50 tons lift capacity, are used for lifting heavier loads, including loaded shipping containers.\n\nIn addition to a control to raise and lower the forks (also known as blades or tines), the operator can tilt the mast to compensate for a load's tendency to angle the blades toward the ground and risk slipping off the forks. Tilt also provides a limited ability to operate on non-level ground. Skilled forklift operators annually compete in obstacle and timed challenges at regional forklift rodeos.\n\nThe following is a list, in no particular order, of the more common lift truck types:\n\n\n\n\n\nAt the other end of the spectrum from the counterbalanced forklift trucks are more 'high end' specialty trucks:\n\nThese are, unlike most lift trucks, front-wheel steer and are a hybrid VNA (very narrow aisle) truck designed to be both able to offload trailers and place the load in narrow aisle racking. Increasingly these trucks are able to compete in terms of pallet storage density, lift heights and pallet throughput with guided very narrow aisle trucks, while also being capable of loading trucks, which VNA units are incapable of doing.\nThese are rail- or wire-guided and available with lift heights up to 40 feet non-top-tied and 98 feet top-tied. Two forms are available: 'man-down' and 'man-riser', where the operator elevates with the load for increased visibility or for multilevel 'break bulk' order picking. This type of truck, unlike articulated narrow aisle trucks, requires a high standard of floor flatness.\nOmnidirectional technology (such as Mecanum wheels) can allow a forklift truck to move forward, diagonally and laterally, or in any direction on a surface. An omnidirectional wheel system is able to rotate the truck 360 degrees in its own footprint or strafe sideways without turning the truck cabin. One example is the Airtrax Sidewinder. This forklift truck has also made an appearance in the TV series called 'Mythbusters'.\n\nIn North America, some internal combustion powered industrial vehicles carry Underwriters Laboratories ratings that are part of UL 558. Industrial trucks that are considered \"safety\" carry the designations GS (Gasoline Safety) for gasoline powered, DS (Diesel Safety) for diesel powered, LPS (Liquid Propane Safety) for liquified propane or GS/LPS for a dual fuel gasoline/liquified propane powered truck.\n\nUL 558 is a two-stage Safety Standard. The basic standard, which is G, D, LP, and G/LP is what Underwriter's Laboratories considers the bare minimum required for a lift truck. This is a voluntary standard, and there is no requirement in North America at least by any Government Agency for manufacturers to meet this standard.\n\nThe slightly more stringent GS, DS, LPS, and GP/LPS, or Safety standard does provide some minimal protection, however it is extremely minimal. In the past Underwriter's Laboratory offered specialty EX and DX safety certifications. If you require higher levels of protection you must contact your local Underwriter's Laboratory Office and check ask them what the correct safety standard is for your workplace. \nUL 583 is the Electric equivalent of UL 558. As with UL 558 it is a two-stage standard. \nThese are for operation in potentially explosive atmospheres found in chemical, petrochemical, pharmaceutical, food and drink, logistics or other industries handling flammable material. Commonly referred to as Pyroban trucks in Europe, they must meet the requirements of the ATEX 94/9/EC Directive if used in Zone 1, 2, 21 or 22 areas and be maintained accordingly.\n\nIn order to decrease work wages, reduce operational cost and improve productivity, automated forklifts have also been developed.\n\nA typical counterbalanced forklift contains the following components:\n\n\nBelow is a list of common forklift attachments:\n\nAny attachment on a forklift will reduce its nominal load rating, which is computed with a stock fork carriage and forks. The actual load rating may be significantly lower.\n\nIt is possible to replace an existing attachment or add one to a lift that doesn't already have one. Considerations include forklift type, capacity, carriage type, and number of hydraulic functions (that power the attachment features). As mentioned in the preceding section, replacing or adding an attachment may reduce (down-rate) the safe lifting capacity of the forklift truck (See also General operations, below).\n\nForklift attachment manufacturers offer online calculators to estimate the safe lifting capacity when using a particular attachment. However, only the forklift truck manufacturer can give accurate lifting capacities. Before installing any attachment contact your local authorized dealer of your forklift brand and ask them to begin re-rating your lift according to the attachment you want to install. Once re-rated the forklift should have a new factory authorized specification plate, to replace the original plate, installed showing the new rating for the lift.\n\nIn the context of attachment, a hydraulic function consists of a valve on the forklift with a lever near the operator that provides two passages of pressurized hydraulic oil to power the attachment features. Sometimes an attachment has more features than your forklift has hydraulic functions and one or more need to be added. There are many ways of adding hydraulic functions (also known as adding a valve). The forklift manufacturer makes valves and hose routing accessories, but the parts and labor to install can be prohibitively expensive. Other ways include adding a solenoid valve in conjunction with a hose or cable reel that diverts oil flow from an existing function. However, hose and cable reels can block the operator's view and are problematic, easily damaged. The Ditto Valve kit uses a solenoid valve and special HydWire hoses, in which the wire reinforcing braid doubles as an electrical conduit. These hoses replace those already on the forklift, nesting in the original reeving, keeping it safe from damage and out of the operator's field of vision.\n\nThere are many national as well as continental associations related to the industrial truck industry. Some of the major organizations are listed as:\n\nThere are many significant contacts among these organizations and they have established joint statistical and engineering programs. One program is the \"World Industrial Trucks Statistics (WITS)\" which is published every month to the association memberships. The statistics are separated by area (continent), country and class of machine. While the statistics are generic and do not count production from most of the smaller manufacturers, the information is significant for its depth. These contacts have brought to a common definition of a Class System to which all the major manufacturers adhere.\n\nForklift safety is subject to a variety of standards worldwide. The most important standard is the ANSI B56—of which stewardship has now been passed from the American National Standards Institute (ANSI) to the Industrial Truck Standards Development Foundation (ITSDF) after multi-year negotiations. ITSDF is a non-profit organization whose only purpose is the promulgation and modernization of the B56 standard.\n\nOther forklift safety standards have been implemented in the United States by the Occupational Safety and Health Administration (OSHA) and in the United Kingdom by the Health and Safety Executive.\n\nIn many countries, forklift truck operators must be trained and certified to operate forklift trucks. Certification may be required for each individual class of lift that an operator would use.\n\nForklift training has many names, such as forklift licensing or forklift certification. Whichever term is used, training must adhere to federal or national standards.\n\nHealth care providers should not recommend that workers who drive or use heavy equipment such as forklifts treat chronic or acute pain with opioids. Workplaces which manage workers who perform safety-sensitive operations should assign workers to less sensitive duties for so long as those workers are treated by their physician with opioids.\n\nIn the United States, workplace forklift training is governed federally by OSHA the Occupational Safety and Health Administration. In 1999, OSHA updated its 29 CFR 1910.178 regulations governing \"Powered Industrial Trucks\" (the term OSHA uses to include forklifts among other types of industrial vehicles.) A major component of these regulations deals with forklift operator training. The standard requires employers to develop and implement a training program based on the general principles of safe truck operation, the types of vehicle(s) being used in the workplace, the hazards of the workplace created by the use of the vehicle(s), and the general safety requirements of the OSHA standard. OSHA believes that trained operators must know how to do the job properly and do it safely as demonstrated by workplace evaluation. Formal (lecture, video, etc.) and practical (demonstration and practical exercises) training must be provided. Employers must also certify that each operator has received the training and evaluate each operator at least once every three years. Prior to operating the truck in the workplace, the employer must evaluate the operator's performance and determine the operator to be competent to operate a powered industrial truck safely. Refresher training is needed whenever an operator demonstrates a deficiency in the safe operation of the truck.\n\nIn the UK, the Provision and Use of Work Equipment Regulations state that operators of forklift trucks must be adequately trained, the general standards of that training and good operating practice are found in the HSE Code of Practice 117 (Third edition) issued in 2013. Third party organisations have developed de facto 'best practice' standards for forklift training, commonly referred to in the UK as a 'forklift licence', these are no longer recognised as proof of training as defined in the COP 117 (third edition) and as such training is not a legal requirement as is commonly believed. Organised training however helps to demonstrate that an employer has taken steps to ensure its 'duty of care' in the unfortunate event of an accident.\n\nIn the UK, forklift training is carried out by a number of different voluntary standard training organisations, They can be directly recognised by the HSE who have formed a new organisation known as \"Accrediting Body Association Work place transport 2012\". In all cases qualified forklift instructors must be registered with at least one of the voluntary training organisations. Although RTITB operators are registered on a database which has to be a 3 yearly basis, the amount of time determined between refresher courses is subject to the H&S Executive, Insurance companies or company policies. The H&S Executive (HSG136 Workplace Transport Safety) does recommend re-training/testing every 3 to 5 years.\n\nForklift instructors throughout the UK tend to operate either as small independent training companies or as a part of a larger training provider. Training is delivered in one of two ways; on-site (sometimes referred to as in-house training) where training is delivered to a clients' premises making use of their own equipment, or off-site (public courses) at a training centre. Training centres offer the opportunity for the unemployed with little or no forklift operating experience to achieve a certificate of competence and increase their employment opportunities. Training certification standards at schools tends to follow closely the standard required by their individual Training Standards Accrediting Body to which they are affiliated. It is not unusual for a Training school to be registered with one or more body at any one time.\n\nThe British Industrial Truck Association (BITA) categorises the different forklift truck types into groups and assigned a unique identifier to each classification. Known as the ‘BITA List’ it has become accepted as a standard in the UK. Forklift training certificates display the appropriate BITA classification to clearly identify the confines of the certification.\n\nPrior to 2011 all States and Territories of Australia independently regulated occupational health and safety in that state, including forklift licensing.\n\nWhilst the Occupational Health and Safety laws of the different states were based on similar underlying principles there were differences between the various jurisdictions in the detail and application of those Occupational Health and Safety laws.\n\nIn 2008 the Inter-Governmental Agreement for Regulatory and Operational Reform in Occupational Health and Safety was formed between the Commonwealth of Australia and the six states and two territories of Australia to formalize cooperation between these jurisdictions on the harmonization of Occupational Health and Safety legislation.\n\nAs a result, the national Model Work Health and Safety Act (WHS) was enacted following a review of work health and safety laws across Australia, which review included significant public consultation. This act was finalized in June 2011.\n\nThis act formed a framework for the individual jurisdictions to enact supporting legislation, as the individual jurisdictions are tasked with managing State and Territory Occupational Health and Safety laws, including the issue of licences coming under the legislation.\n\nEach individual state and territory issue licences in their own jurisdiction, including what is known as \"high-risk work licences\" for high-risk work. Forklift licences are classed as \"high-risk work licences\".\n\nTo obtain a forklift licence in any State or Territory an applicant must undertake a training course with an approved training organisation and then, on completion of the course, apply to the appropriate State or Territory for a forklift licence. The unit of competence is known as the National High Risk Licence Unit of Competence TLILIC2001 – Licence to Operate a Forklift Truck, or in the case of an LO licence Unit of Competence TLILIC2002 – Licence to Operate an Order Picking Forklift Truck. There is a fee attached which varies from jurisdiction to jurisdiction.\n\nForklift licences issued in one jurisdiction are recognized in all. Licence cancellation in one jurisdiction is also recognized in all.\n\nForklift operator training is divided into two types:\n\nThe operator's certificate is based on the Approved Code of Practice for Training Operators and Instructors of Powered Industrial Lift Trucks ('ACOP') published in 1995 by the then Department of Labour. It gives permission for operators to operate a forklift in an enclosed space (i.e. a space not considered to be a 'road'). The F endorsement is an additional qualification which allows an operator to use a forklift on a public road. Operators with an operator's certificate are not required to have a car driver licence, but to hold an F endorsement an operator must hold a class 1 (car) driver licence for forklifts up to 18000 kg or a class 2 (heavy rigid vehicle) driver licence for forklifts over 18000 kg as the endorsements are printed on the licence.\n\nThe ACOP is a set of best practices, guidelines and recommendations for training a forklift operator. However, training should be tailored to the operator's specific needs and the attachments they use, as required under the Health and Safety at Work Act 2015.\n\nTraining consists of a theory session delivered in-person or online followed by a brief practical assessment of 15–20 minutes. If the ACOP guidelines are followed this consists of a stacking and destacking pallets at low, medium and high levels, as well as driving forwards and in reverse around a coned figure-of-eight while carrying a load.\n\nUnit standards are available for forklift training but are not required. The primary unit standard is US10851.\n\nThe ACOP deals specifically with a forklift operator using only the standard forks. Forklift attachments, such as barrel clamps, fork extensions, rotators and personnel cages are covered under a separate unit standard (US10852). It is not mandatory to achieve a unit standard; a company can simply induct the operator on the attachments used.\n\nA number of solutions can be found on the market today to reduce occupational hazards caused by forklifts.\n\nThese are proximity sensors that detect objects and pedestrians from a few centimeters to several meters. The sensor makes the difference between a person and an object and alerts the driver without useless alarms. \nBased on stereovision, an algorithm analyses in real time if a person is in a blind zone of the forklift.\n\nUltrasonic sensors are proximity sensors that detect objects at distances ranging from a few centimeters to several meters. The sensor beeps and measures the time it takes for the signal to return. It does not discriminate between people and objects. Any obstacle located behind the truck will be detected. Normally, this type of sensor is used only for detection in rear areas.\n\nThese are solutions that alert forklift drivers of the people found in its vicinity. Pedestrians must carry a radio frequency device (electronic tags) which, emit a signal when a truck detects them, alerting the driver of the potential risk of an accident. It detects both in the front and at the back and it differentiates between people and the usual obstacles found in warehouses. For this reason, the driver is only alerted when there is a pedestrian near the truck. \nThere are different solutions on the market:\n\nEvery year Modern Materials publishes a Top 20 Global Ranking of Forklift Manufacturers by sales in dollars. A modified copy of the report is below in a sortable table.\n\n\n"}
{"id": "6901481", "url": "https://en.wikipedia.org/wiki?curid=6901481", "title": "Global distribution system", "text": "Global distribution system\n\nA global distribution system (GDS) is a computerised network system owned or operated by a company that enables transactions between travel industry service providers, mainly airlines, hotels, car rental companies, and travel agencies. The GDS mainly uses \"real-time inventory\" (for e.g. number of hotel rooms available, number of flight seats available, or number of cars available) to service providers. Travel Agencies traditionally relied on GDS for services, products & rates in order to provision travel-related services to the end consumers. Thus, a GDS can link services, rates and bookings consolidating products and services across all three travel sectors: i.e., airline reservations, hotel reservations, car rentals.\n\nGDS is different from a computer reservations system, which is a reservation system used by the service providers (also known as vendors). Primary customers of GDS are travel agents (both online and office-based) to make reservation on various reservation systems run by the vendors. GDS holds no inventory; the inventory is held on the vendor's reservation system itself. A GDS system will have real-time link to the vendor's database. For example, when a travel agency requests a reservation on the service of a particular airline company, the GDS system routes the request to the appropriate airline's computer reservations system.\n\nA mirror image of the passenger name record (PNR) in the airline reservations system is maintained in the GDS system. If a passenger books an itinerary containing air segments of multiple airlines through a travel agency, the passenger name record in the GDS system would hold information on their entire itinerary, each airline they fly on would only have a portion of the itinerary that is relevant to them. This would contain flight segments on their own services and inbound and onward connecting flights (known as info segments) of other airlines in the itinerary. e.g. if a passenger books a journey from Amsterdam to London on KLM, London to New York on British Airways, New York to Frankfurt on Lufthansa through a travel agent and if the travel agent is connected to Amadeus GDS. The PNR in the Amadeus GDS would contain the full itinerary, the PNR in KLM would show the Amsterdam to London segment along with British Airways flight as an onward info segment. Likewise the PNR in the Lufthansa system would show the New York to Frankfurt segment with the British Airways flight as an arrival information segment. The PNR in British Airways system would show all three segments. One as a live segment and the other two as arrival and onward info segments.\n\nSome GDS systems (primarily Amadeus CRS and SABRE) also have a dual use capability for hosting multiple computer reservations system, in such situations functionally the computer reservations system and the GDS partition of the system behave as if they were separate systems.\n\nWhile the most famous GDS systems globally are Amadeus, Sabre and Travelport (Galileo, Worldspan and Apollo) there are other smaller regional GDS systems in specific markets. For. e.g. Travelsky is a state owned company in China.\n\nGDS in the travel industry originated from a traditional legacy business model that existed to inter-operate between airline vendors and travel agents. During the early days of computerized reservations systems flight ticket reservations were not possible without a GDS. As time progressed, many airline vendors (including budget and mainstream operators) have now adopted a strategy of 'direct selling' to their wholesale and retail customers (passengers). They invested heavily in their own reservations and direct-distribution channels and partner systems. This helps to minimize direct dependency on GDS systems to meet sales and revenue targets and allows for a more dynamic response to market needs. These technology advancements in this space facilitate an easier way to cross-sell to partner airlines and via travel agents, eliminating the dependency on a dedicated global GDS federating between systems. Also, multiple price comparison websites eliminate the need of dedicated GDS for point-in-time prices and inventory for both travel agents and end-customers. Hence some experts argue that these changes in business models may lead to complete phasing out of GDS in the Airline space by the year 2020.\n\nLufthansa Group announced in June 2015 that it was imposing an additional charge of €16 when booking through an external Global Distribution System rather than their own systems. They stated their choice was based upon that the costs of using external systems was several times higher than their own. Several other airlines including Air France–KLM and Emirates also stated that they are following the development.\n\nHowever, hotels and car rental industry continue to benefit from GDS, especially last-minute inventory disposal using GDS to bring additional operational revenue. GDS here is useful to facilitate global reach using existing network and low marginal costs when compared to online air travel bookings. \nSome GDS companies are also in the process of investing and establishing significant offshore capability in a move to reduce costs and improve their profit margins to serve their customer directly accommodating changing business models.\n"}
{"id": "15363687", "url": "https://en.wikipedia.org/wiki?curid=15363687", "title": "ISO 14698", "text": "ISO 14698\n\nThe ISO 14698 Standards features two International Standards on biocontamination control for cleanrooms. IEST, the Secretariat and Administrator of ISO Technical Committee 209, helped develop this series of ISO 14698 Standards.\n\n\nISO 14698-1 was first written in 2003. ISO 14698-1 describes the principles and basic methodology for a formal system to assess and control biocontamination, where cleanroom technology is applied, in order that biocontamination in zones at risk can be monitored in a reproducible way and appropriate control measures can be selected. In zones of low or negligible risk this standard may be used as a source of information.\n\nISO 14698-2 became available to the public in October 2003. ISO 14698-2 gives guidance on basic principles and methodological requirements for all microbiological data evaluation, and the estimation of biocontamination data obtained from sampling for viable particles in zones at risk, as specified by the system selected. This is not intended for testing the performance of microbiological counting techniques of determining viable units.\n\n\n\nDateTime Report Generated: 19 February 2009 3:59:00 PM (CET)\n"}
{"id": "28915147", "url": "https://en.wikipedia.org/wiki?curid=28915147", "title": "Instruction window", "text": "Instruction window\n\nAn instruction window in computer architecture refers to the set of instructions which can execute out-of-order in a speculative processor.\n\nIn particular, in a conventional design, the instruction window consists of all instructions which are in the re-order buffer (ROB). In such a processor, any instruction within the instruction window can be executed when its operands are ready. Out-of-order processors derive their name because this may occur out-of-order (if operands to a younger instruction are ready before those of an older instruction).\n\nThe instruction window has a finite size, and new instructions can enter the window (usually called \"dispatch\" or \"allocate\") only when other instructions leave the window (usually called \"retire\" or \"commit\"). Instructions enter and leave the instruction window in program order, and an instruction can only leave the window when it is the oldest instruction in the window and it has been completed. Hence, the instruction window can be seen as a sliding window in which the instructions can become out-of-order. All execution within the window is speculative (i.e., side-effects are not applied outside the CPU) until it is committed in order to support asynchronous exception handling like interrupts.\n\nThis paradigm is also known as restricted dataflow because instructions within the window execute in dataflow order (not necessarily in program order) but the window in which this occurs is restricted (of finite size).\n\nThe instruction window is distinct from pipelining: instructions in an in-order pipeline are not in an instruction window in the conventionally understood sense, because they cannot execute out of order with respect to one another. Out-of-order processors are usually built around pipelines, but many of the pipeline stages (e.g., front-end instruction fetch and decode stages) are not considered to be part of the instruction window.\n\n"}
{"id": "4460347", "url": "https://en.wikipedia.org/wiki?curid=4460347", "title": "List of largest biotechnology and pharmaceutical companies", "text": "List of largest biotechnology and pharmaceutical companies\n\nThe following is a list of the independent biotechnology companies listed on a stock exchange (as indicated) with current market capitalization of at least USD 10 billion, in decreasing order.\n\nThe list does not include biotechnology companies that are currently owned by, or a part of, larger pharmaceutical groups.\n\n\n"}
{"id": "830206", "url": "https://en.wikipedia.org/wiki?curid=830206", "title": "Magnetic amplifier", "text": "Magnetic amplifier\n\nThe magnetic amplifier (colloquially known as a \"mag amp\") is an electromagnetic device for amplifying electrical signals. The magnetic amplifier was invented early in the 20th century, and was used as an alternative to vacuum tube amplifiers where robustness and high current capacity were required. World War II Germany perfected this type of amplifier, and it was used in the V-2 rocket. The magnetic amplifier was most prominent in power control and low-frequency signal applications from 1947 to about 1957, when the transistor began to supplant it. The magnetic amplifier has now been largely superseded by the transistor-based amplifier, except in a few safety critical, high-reliability or extremely demanding applications. Combinations of transistor and mag-amp techniques are still used.\n\nVisually a mag amp device may resemble a transformer, but the operating principle is quite different from a transformer – essentially the mag amp is a saturable reactor. It makes use of magnetic saturation of the core, a non-linear property of a certain class of transformer cores. For controlled saturation characteristics, the magnetic amplifier employs core materials that have been designed to have a specific B-H curve shape that is highly rectangular, in contrast to the slowly tapering B-H curve of softly saturating core materials that are often used in normal transformers.\n\nThe typical magnetic amplifier consists of two physically separate but similar transformer magnetic cores, each of which has two windings: a control winding and an AC winding. A small DC current from a low-impedance source is fed into the series-connected control windings. The AC windings may be connected either in series or in parallel, the configurations resulting in different types of mag amps. The amount of control current fed into the control winding sets the point in the AC winding waveform at which either core will saturate. In saturation, the AC winding on the saturated core will go from a high-impedance state (\"off\") into a very low-impedance state (\"on\") – that is, the control current controls the point at which voltage the mag amp switches \"on\".\n\nA relatively small DC current on the control winding is able to control or switch large AC currents on the AC windings. This results in current amplification.\n\nTwo magnetic cores are used because the AC current will generate high voltage in the control windings. By connecting them in opposite phase, the two cancel each other, so that no current is induced in the control circuit.\n\nThe magnetic amplifier is a static device with no moving parts. It has no wear-out mechanism and has a good tolerance to mechanical shock and vibration. It requires no warm-up time.\nMultiple isolated signals may be summed by additional control windings on the magnetic cores. The windings of a magnetic amplifier have a higher tolerance to momentary overloads than comparable solid-state devices. The magnetic amplifier is also used as a transducer in applications such as current measurement and the flux gate compass. The reactor cores of magnetic amplifiers withstand neutron radiation extremely well. For this special reason magnetic amplifiers have been used in nuclear power applications.\n\nThe gain available from a single stage is limited and low compared to electronic amplifiers. Frequency response of a high-gain amplifier is limited to about one-tenth the excitation frequency, although this is often mitigated by exciting magnetic amplifiers with currents at higher than utility frequency. Solid-state electronic amplifiers can be more compact and efficient than magnetic amplifiers. The bias and feedback windings are not unilateral and may couple energy back from the controlled circuit into the control circuit. This complicates the design of multistage amplifiers when compared with electronic devices.\n\nMagnetic amplifiers were important as modulation and control amplifiers in the early development of voice transmission by radio. A magnetic amplifier was used as voice modulator for a 2 kilowatt Alexanderson alternator, and magnetic amplifiers were used in the keying circuits of large high-frequency alternators used for radio communications. Magnetic amplifiers were also used to regulate the speed of Alexanderson alternators to maintain the accuracy of the transmitted radio frequency. Magnetic amplifiers were used to control large high-power alternators by turning them on and off for telegraphy or to vary the signal for voice modulation. The alternator's frequency limits were rather low to where a frequency multiplier had to be utilized to generate higher radio frequencies than the alternator was capable of producing. Even so, early magnetic amplifiers incorporating powdered-iron cores were incapable of producing radio frequencies above approximately 200 kHz. Other core materials, such as ferrite cores and oil-filled transformers, would have to be developed to allow the amplifier to produce higher frequencies.\n\nThe ability to control large currents with small control power made magnetic amplifiers useful for control of lighting circuits, for stage lighting and for advertising signs. Saturable reactor amplifiers were used for control of power to industrial furnaces. Magnetic amplifiers are still used in some arc welders.\n\nSmall magnetic amplifiers were used for radio tuning indicators, control of small motor and cooling fan speed, control of battery chargers.\n\nMagnetic amplifiers were used extensively as the switching element in early switched-mode (SMPS) power supplies, as well as in lighting control. Semiconductor-based solid-state switches have largely superseded them, though recently there has been some regained interest in using mag amps in compact and reliable switching power supplies. PC ATX power supplies often use mag amps for secondary side voltage regulation. Cores designed specifically for switch mode power supplies are currently manufactured by several large electromagnetics companies, including Metglas and Mag-Inc.\n\nMagnetic amplifiers were used by locomotives to detect wheel slip, until replaced by Hall Effect current transducers. The cables from two traction motors passed through the core of the device. During normal operation the resultant flux was zero as both currents were the same and in opposite directions. The currents would differ during wheel slip, producing a resultant flux that acted as the Control winding, developing a voltage across a resistor in series with the AC winding which was sent to the wheel slip correction circuits.\n\nMagnetic amplifiers can be used for measuring high DC-voltages without direct connection to the high voltage and are therefore still used in the HVDC-technique. The current to be measured is passed through the two cores, possibly by a solid bus bar. There is almost no voltage drop in this bus bar. The output signal, proportional to the ampere turns in the control current bus bar, is derived from the alternating excitation voltage of the magnetic amplifier, there is no voltage created or induced on the bus bar. The output signal has only a magnetic connection with the bus bar so the bus may be, quite safely, at any (EHT) voltage with respect to the instrumentation.\n\nInstrumentation magnetic amplifiers are commonly found on space craft where a clean electromagnetic environment is highly desirable. \n\nThe German Kriegsmarine made extensive use of the magnetic amplifiers. They were used for the master stable element systems, for slow moving transmission for controlling guns, directors and rangefinders and train and elevation controls. Magnetic amplifiers were used in aircraft systems (avionics) before the advent of high reliability semiconductors. They were important in implementing early autoland systems and Concorde made use of the technology for the control of its engine air intakes before development of a system using digital electronics. Magnetic amplifiers were used in stabilizer controls of V2 rockets.\n\nMagnetic amplifiers were widely studied during the 1950s as a potential switching element for mainframe computers. Mag amps could be used to sum several inputs in a single core, which was useful in the arithmetic logic unit (ALU). Custom tubes could do the same, but transistors could not, so the mag amp was able to combine the advantages of tubes and transistors in an era when the latter were expensive and unreliable.\n\nThe principles of magnetic amplifiers were applied non linearly to create magnetic digital logic gates. That era was short, lasting from the mid-1950s to about 1960, when new fabrication techniques produced great improvements in transistors and dramatically lowered their cost. Only one large-scale mag amp machine was put into production, the UNIVAC Solid State, but a number of contemporary late-1950s/early-1960s computers used the technology, like the Ferranti Orion and the English Electric KDF9, or the one-off MAGSTEC. The best known type of this is magnetic core memory.\n\nA voltage source and a series connected variable resistor may be regarded as a direct current signal source for a low resistance load such as the control coil of a saturable reactor which amplifies the signal. Thus, in principle, a saturable reactor is already an amplifier, although before 20th century they were used for simple tasks, such as controlling lighting and electrical machinery as early as 1885.\n\nIn 1904 radio pioneer Reginald Fessenden placed an order for a high frequency rotary mechanical alternator from the General Electric Company capable of generate AC at a frequency of 100 kHz to be used for continuous wave radio transmission over great distances. The design job was given to General Electric engineer Ernst F. Alexanderson who developed the 2 kW Alexanderson alternator. By 1916 Alexanderson added a magnetic amplifier to control the transmission of these rotary alternators for transoceanic radio communication. \n\nThe experimental telegraphy and telephony demonstrations made during 1917 attracted the attention of the US Government, especially in light of partial failures in the transoceanic cable across the Atlantic Ocean. The 50 kW alternator was commandeered by the US Navy and put into service in January 1918 and was used until 1920, when a 200 kW generator-alternator set was built and installed.\n\nMagnetic amplifiers were extensively used in electricity power generation from the early 1960s onwards. They provided the small signal amplification for generator automatic voltage regulation (AVR) from a small error signal at milliwatt (mW) level to 100 kilowatt (kW) level. This was in turn converted by a rotating machine (exciter) to 5 megawatt (MW) level, the excitation power required by a typical 500 MW Power Plant Turbine Generator Unit. They proved durable and reliable. Many are recorded in service through the mid-1990s and some are still in use at older generating stations, notably in hydroelectric plants operating in northern California.\n\nIn the 1970s, Robert Carver designed and produced several high quality high-powered audio amplifiers, calling them magnetic amplifiers. In fact, they were in most respects conventional audio amplifier designs with unusual power supply circuits. They were not magnetic amplifiers as defined in this article. That should not be confused with real magnetic audio amplifiers, which also exist.\n\n\n\n"}
{"id": "40149193", "url": "https://en.wikipedia.org/wiki?curid=40149193", "title": "Maleyka Abbaszadeh", "text": "Maleyka Abbaszadeh\n\nMaleyka Mehdi qizi Abbaszadeh (, née Mustafayeva, born 1953, Baku) is the Chair of the State Students Admission Commission of Azerbaijan.\n\nMaleyka Abbaszadeh was born in Baku and attended Public School #189. In 1975, she graduated from the Moscow State University majoring in calculus and cybernetics. She did an internship at the Azerbaijan National Academy of Sciences Institute of Cybernetics and for the next two years, at the Institute of Calculus Mathematics of the USSR Academy of Sciences. In 1988, she defended her masters thesis in cybernetics at the Ukrainian Academy of Sciences Institute of Cybernetics. Until 1994, Abbaszadeh worked on a number of government and non-government projects in the field of education.\n\nIn 1994, she was appointed Deputy Chair of the State Students Admission Commission (SSAC), a newly established organisation in charge of the national entrance test for aspiring post-secondary students. In October 2000, she was appointed head of the said committee. As the Chair of the SSAC, Abbaszadeh was a vocal critic of the education policy under ex-Minister of Education Misir Mardanov, which, according to her, yielded lower examination score among prospective students year after year. In 2012, she openly questioned the transparency of the Best School and Best Teacher award contest held by the Ministry of Education. In 2013, she accused the Ministry of reporting false numbers of students currently enrolled in secondary schools. Abbaszadeh also pointed out significant flaws and errors in the Ministry-approved textbooks.\n\nAccording to opposition media, Abbaszadeh's towering criticism of Mardanov was in fact part of the behind-the-scene campaign orchestrated by the Presidential Administration and aimed at removing Misir Mardanov from his position. Mardanov, one of the most controversial ministers, was replaced by Mikayil Jabbarov as the Education Minister on 19 April 2013 by a presidential decree.\n\nAbbaszadeh was married to Ilgar Abbaszadeh, but is now divorced. They have two daughters. Abbaszadeh's daughter Nargiz Birk-Petersen was one of the three presenters of the Eurovision Song Contest 2012 held in Baku.\n"}
{"id": "23183493", "url": "https://en.wikipedia.org/wiki?curid=23183493", "title": "Mineral Products Association", "text": "Mineral Products Association\n\nThe Mineral Products Association (MPA) is the United Kingdom trade association for the aggregates, asphalt, cement, concrete, dimension stone, lime, mortar, and silica sand industries.\n\nThe Mineral Products Association (MPA) is the trade association for the aggregates, asphalt, cement, concrete, dimension stone, lime, mortar and silica sand industries. With the affiliation of British Precast, the British Association of Reinforcement (BAR), Eurobitume, QPA Northern Ireland, MPA Scotland and the British Calcium Carbonate Federation, it has a growing membership of 480 companies and is the sectoral voice for mineral products. MPA membership is made up of the vast majority of independent SME quarrying companies throughout the UK, as well as the 9 major international and global companies. It covers 100% of UK cement production, 90% of aggregates production, 95% of asphalt and over 70% of ready-mixed concrete and precast concrete production. Each year the industry supplies £20 billion worth of materials and services to the Economy and is the largest supplier to the construction industry, which has annual output valued at £144billion. Industry production represents the largest materials flow in the UK economy and is also one of the largest manufacturing sectors.\n\nThe Mineral Products Association (MPA) was formed in March 2009 from the merger of the Quarry Products Association, the British Cement Association and The Concrete Centre. MPA was officially launched in June 2009.\n\nThe Mineral Products Association has offices in London, Glasgow and Fron in Wales.\n\nQPA Northern Ireland is affiliated to the MPA and has offices in Crumlin, County Antrim. The British Precast Concrete Federation (BPCF), the trade Association of precast concrete manufacturers, is a member of the MPA and is based in Leicester.\n\nMPA has regional divisions, for London & South East, South West, East Anglia, Midlands, Wales, North, Scotland and Northern Ireland.\n\nThe Concrete Centre was formed in 2003 and since 2009 has been part of the Mineral Products Association. The Concrete Centre promotes the use of concrete in construction through the provision of resources to enable designers to follow best practice for the design of concrete and masonry. The Concrete Centre publishes a journal, \"Concrete Quarterly\" which was first published by the Cement and Concrete Association in 1947. The journal showcases the use of concrete in construction projects in the United Kingdom and worldwide. The CQ archive is available online \n\n"}
{"id": "11172708", "url": "https://en.wikipedia.org/wiki?curid=11172708", "title": "Nanometrology", "text": "Nanometrology\n\nNanometrology is a subfield of metrology, concerned with the science of measurement at the nanoscale level. Nanometrology has a crucial role in order to produce nanomaterials and devices with a high degree of accuracy and reliability in nanomanufacturing.\n\nA challenge in this field is to develop or create new measurement techniques and standards to meet the needs of next-generation advanced manufacturing, which will rely on nanometer scale materials and technologies. The needs for measurement and characterization of new sample structures and characteristics far exceed the capabilities of current measurement science. Anticipated advances in emerging U.S. nanotechnology industries will require revolutionary metrology with higher resolution and accuracy than has previously been envisioned.\n\nControl of the critical dimensions are the most important factors in nanotechnology. Nanometrology today, is to a large extent based on the development in semiconductor technology. Nanometrology is the science of measurement at the nanoscale level. Nanometer or nm is equivalent to 10^-9 m. In Nanotechnology accurate control of dimensions of objects is important. Typical dimensions of nanosystems vary from 10 nm to a few hundred nm and while fabricating such systems measurement up to 0.1 nm is required.\n\nAt nanoscale due to the small dimensions various new physical phenomena can be observed. For example, when the crystal size is smaller than the electron mean free path the conductivity of the crystal changes. Another example is the discretization of stresses in the system. It becomes important to measure the physical parameters so as to apply these phenomena into engineering of nanosystems and manufacturing them. The measurement of length or size, force, mass, electrical and other properties is included in Nanometrology.\nThe problem is how to measure these with reliability and accuracy. The measurement techniques used for macro systems cannot be directly used for measurement of parameters in nanosystems. Various techniques based on physical phenomena have been developed which can be used for measure or determine the parameters for nanostructures and nanomaterials. Some of the popular ones are X-Ray diffraction, transmission electron microscopy, High Resolution Transmission Electron Microscopy, atomic force microscopy, scanning electron microscopy, field emission scanning electron microscopy and Brunauer, Emmett, Teller method to determine specific surface.\n\nNanotechnology is an important field because of the large number of applications it has and it has become necessary to develop more precise techniques of measurement and globally accepted standards. Hence progress is required in the field of Nanometrology.\n\nNanotechnology can be divided into two branches. The first being molecular nanotechnology which involves bottom up manufacturing and the second is engineering nanotechnology which involve the development and processing of materials and systems at nanoscale. The measurement and manufacturing tools and techniques required for the two branches are slightly different.\n\nFurthermore, Nanometrology requirements are different for the industry and research institutions. Nanometrology of research has progressed faster than that for industry mainly because implementing nanometrology for industry is difficult. In research oriented nanometrology resolution is important whereas in industrial nanometrology accuracy is given precedence over resolution. Further due to economic reasons it is important to have low time costs in industrial nanometrology it is not important for research nanometrology. The various measurement techniques available today require a controlled environment like in vacuum, vibration and noise free environment. Also, in industrial nanometrology requires that the measurements be more quantitative with minimum number of parameters.\n\nMetrology standards are objects or ideas that are designated as being authoritative for some accepted reason. Whatever value they possess is useful for comparison to unknowns for the purpose of establishing or confirming an assigned value based on the standard. The execution of measurement comparisons for the purpose of establishing the relationship between a standard and some other measuring device is calibration. The ideal standard is independently reproducible without uncertainty. The worldwide market for products with nanotechnology applications is projected to be at least a couple of hundred billion dollars in the near future. Until recently, there almost no established internationally accepted standards for nanotechnology related field. The International Organisation for Standardization TC-229 Technical Committee on Nanotechnology recently published few standards for terminology, characterization of nanomaterials and nanoparticles using measurement tools like AFM, SEM, Interferometers, optoacoustic tools, gas adsorption methods etc. Certain standards for standardization of measurements for electrical properties have been published by the International Electrotechnical Commission.\nSome important standards which are yet to be established are standards for measuring thickness of thin films or layers, characterization of surface features, standards for force measurement at nanoscale, standards for characterization of critical dimensions of nanoparticles and nanostructures and also Standards for measurement for physical properties like conductivity, elasticity etc.\n\nBecause of the importance of nanotechnology in the future, countries around the world have programmes to establish national standards for nanometrology and nanotechnology. These programmes are run by the national standard agencies of the respective countries. In the United States, National Institute of Standards and Technology has been working on developing new techniques for measurement at nanoscale and has also established some national standards for nanotechnology. These standards are for nanoparticle characterization, Roughness Characterization, magnification standard, calibration standards etc.\n\nIt is difficult to provide samples using which precision instruments can be calibrated at nanoscale. Reference or calibration standards are important for repeatability to be ensured. But there are no international standards for calibration and the calibration artefacts provided by the company along with their equipment is only good for calibrating that particular equipment. Hence it is difficult to select a universal calibration artefact using which we can achieve repeatability at nanoscale. At nanoscale while calibrating care needs to be taken for influence of external factors like vibration, noise, motions caused by thermal drift and creep and internal factors like the interaction between the artefact and the equipment which can cause significant deviations.\n\nIn the last 70 years various techniques for measuring at nanoscale have been developed. Most of them based on some physical phenomena observed on particle interactions or forces at nanoscale. Some of the most commonly used techniques are Atomic Force Microscopy, X-Ray Diffraction, Scanning Electron Microscopy, Transmission Electron Microscopy, High Resolution Transmission Electron Microscopy, and Field Emission Scanning Electron Microscopy.\n\nAtomic force microscopy (AFM) is one of the most common measurement techniques. It can be used to measure Topology, grain size, frictional characteristics and different forces. It consists of a silicon cantilever with a sharp tip with a radius of curvature of a few nanometers. The tip is used as a probe on the specimen to be measured. The forces acting at the atomic level between the tip and the surface of the specimen cause the tip to deflect and this deflection is detected using a laser spot which is reflected to an array of photodiodes.\n\nScanning tunneling microscopy (STM) is another instrument commonly used. It is used to measure 3-D topology of the specimen. The STM is based on the concept of quantum tunneling. When a conducting tip is brought very near to the surface to be examined, a bias (voltage difference) applied between the two can allow electrons to tunnel through the vacuum between them. Measurements are made by monitoring the current as the tip's position scans across the surface, which can then be used to display an image.\n\nAnother commonly used instrument is the scanning electron microscopy (SEM) which apart from measuring the shape and size of the particles and topography of the surface can be used to determine the composition of elements and compounds the sample is composed of. In SEM the specimen surface is scanned with a high energy electron beam. The electrons in the beam interact with atoms in the specimen and interactions are detected using detectors. The interactions produced are back scattering of electrons, transmission of electrons, secondary electrons etc. To remove high angle electrons magnetics lenses are used.\n\nThe instruments mentioned above produce realistic pictures of the surface are excellent measuring tools for research. Industrial applications of nanotechnology require the measurements to be produced need to be more quantitative. The requirement in industrial nanometrology is for higher accuracy than resolution as compared to research nanometrology.\n\nA coordinate measuring machine (CMM) that works at the nanoscale would have a smaller frame than the CMM used for macroscale objects. This is so because it may provide the necessary stiffness and stability to achieve nanoscale uncertainties in x,y and z directions. The probes for such a machine need to be small to enable a 3-D measurement of nanometre features from the sides and from inside like nanoholes. Also for accuracy laser interferometers need to be used. NIST has developed a surface measuring instrument, called the Molecular Measuring Machine. This instrument is basically an STM. The x- and y-axes are read out by laser interferometers. The molecules on the surface area can be identified individually and at the same time the distance between any two molecules can be determined. For measuring with molecular resolution, the measuring times become very large for even a very small surface area. Ilmenau Machine is another nanomeasuring machine developed by researchers at the Ilmenau University of Technology.\nThe components of a nano CMM include nanoprobes, control hardware, 3D-nanopositioning platform, and instruments with high resolution and accuracy for linear and angular measurement.\n\nIn metrology at macro scale achieving traceability is quite easy and artefacts like scales, laser interferometers, step gauges, and straight edges are used. At nanoscale a crystalline highly oriented pyrolytic graphite (HOPG), mica or silicon surface is considered suitable used as calibration artefact for achieving traceability. But it is not always possible to ensure traceability. Like what is a straight edge at nanoscale and even if take the same standard as that for macroscale there is no way to calibrate it accurately at nanoscale. This so because the requisite internationally or nationally accepted reference standards are not always there. Also the measurement equipment required to ensure traceability has not been developed. The generally used for traceability are miniaturisation of traditional metrology standards hence there is a need for establishing nanoscale standards. Also there is a need to establish some kind of uncertainty estimation model. Traceability is one of the fundamental requirements for manufacturing and assembly of products when multiple producers are there.\n\nTolerance is the permissible limit or limits of variation in dimensions, properties, or conditions without significantly affecting functioning of equipment or a process. Tolerances are specified to allow reasonable leeway for imperfections and inherent variability without compromising performance. In nanotechnology the systems have dimensions in the range of nanometers. Defining tolerances at nanoscale with suitable calibration standards for traceability is difficult for different nanomanufacturing methods. There are various integration techniques developed in the semiconductor industry that are used in nanomanufacturing.\n\n\nThere are a variety of nanostructures like nanocomposites, nanowires, nanopowders, nanotubes, fullerenes nanofibers, nanocages, nanocrystallites, nanoneedles, nanofoams, nanomeshes, nanoparticles, nanopillars, thin films, nanorods, nanofabrics, quantumdots etc. The most common way to classify nano structures is by their dimensions.\nNanostructures can be classified on the basis of the grain structure and size there are made up of. This is applicable in the cas of 2-dimensional and 3-Dimensional Nanostructurs.\n\nFor nanopowder to determine the specific surface area the B.E.T. method is commonly used. The drop of pressure of nitrogen in a closed container due to adsorption of the nitrogen molecules to the surface of the material inserted in the container is measured. Also, the shape of the nanopowder particles is assumed to be spherical.\n\nWhere \"D\" is the effective diameter, \"ρ\" is the density and \"A\" is the surface area found from the B.E.T. method.\n\n\n"}
{"id": "8051192", "url": "https://en.wikipedia.org/wiki?curid=8051192", "title": "Note (perfumery)", "text": "Note (perfumery)\n\nNotes in perfumery are descriptors of scents that can be sensed upon the application of a perfume. Notes are separated into three classes; top/head notes, middle/heart notes, and base notes; which denote groups of scents which can be sensed with respect to the time after the application of a perfume. These notes are created carefully with knowledge of the evaporation process and intended use of the perfume. The presence of one note may alter the perception of another—for instance, the presence of certain base or heart notes will alter the scent perceived when the top notes are strongest, and likewise the scent of base notes in the dry-down will often be altered depending on the smells of the heart notes.\n\nThe idea of \"notes\" is used primarily for the marketing of fine fragrances. The term is sometimes used by perfumers to describe approximately scents or the perfumery process to laypeople.\n\nFragrant materials are listed by Poucher in order of volatility and are grouped under respective evaporation coefficients (perfume notes) that range from 1 to 100. \nTop notes are otherwise called the head notes.\n\nPerceived immediately upon application of a perfume, top notes consist of small, light molecules that evaporate quickly. They form a person's initial impression of a perfume and thus are very important in the selling of the product. The scents of this note class are usually described as \"fresh\", \"assertive\" or \"sharp\". The compounds that contribute to top notes are strong in scent, very volatile, and evaporate quickly. \n\nAlthough not as saliently perceived, the heart and base-notes contribute much to the scent in the top notes.\n\nCitrus and ginger scents are common top notes.\n\nThey are also called the \"heart notes\".\n\nThe scent of a perfume that emerges just prior to when the top notes dissipate. The middle note compounds form the \"heart\" or main body of a perfume and emerge in the middle of the perfume's dispersion process. They serve to mask the often unpleasant initial impression of base notes, which become more pleasant with time. Not surprisingly, the scent of middle note compounds is usually more mellow and \"rounded\". Scents from this note class appear anywhere from twenty minutes to one hour after the application of a perfume. \n\nLavender and rose scents are typical middle notes.\n\nThe scent of a perfume that appears close to the departure of the middle notes. The base and middle notes together are the main theme of a perfume. Base notes bring depth and solidity to a perfume. Compounds of this class are often the fixatives used to hold and boost the strength of the lighter top and middle notes. Consisting of large, heavy molecules that evaporate slowly, compounds of this class of scents are typically rich and \"deep\" and are usually not perceived until 30 minutes after the application of the perfume or during the period of perfume dry-down. \n\nSome base notes can still be detectable in excess of twenty-four hours after application, particularly the animalic and musk notes.\n\n"}
{"id": "44390264", "url": "https://en.wikipedia.org/wiki?curid=44390264", "title": "OrCam device", "text": "OrCam device\n\nOrCam Technologies Ltd. is an Israeli-based company producing wearable artificial intelligence space. OrCam develops and manufactures assistive technology devices for individuals who are visually impaired, partially sighted, blind, print disabilities, or have other disabilities. OrCam headquarters is located in Jerusalem operating under the company name OrCam Technologies Ltd.\n\nOrCam Technologies Ltd was founded in 2010 by Professor Amnon Shashua and Ziv Aviram. Before co-founding OrCam, the two in 1999 co-founded Mobileye, an Israeli company that develops vision-based advanced driver-assistance systems (ADAS) providing warnings for collision prevention and mitigation, which was acquired by Intel for $15.3 billion in 2017.\nOrCam launched OrCam MyEye in 2013 after years of development and testing, and began selling it commercially in 2015. In its early years, the company raised $22 million, $6 million of which from Intel Capital. In March 2017, OrCam had raised $41 million in capital, making it worth $600 million. Today, OrCam has over 150 employees, is headquartered in Jerusalem, and has offices in New York, Toronto, and London.\n\nOrCam Technologies Ltd has created three devices; OrCam MyEye 2.0, OrCam MyEye 1, and OrCam MyReader.\n\nOrCam My Eye 2.0: \n\nJAMA Ophthalmology:\nIn 2016 JAMA Ophthalmology conducted a study involving 12 legally blind participants to evaluate the usefulness of a portable artificial vision device (OrCam) for patients with low vision. The results showed that the OrCam device improved the patient’s ability to perform tasks simulating those of daily living, such as reading a message on an electronic device, a newspaper article or a menu.\n\nWills Eye:\nWills Eye was a clinical study designed to measure the impact of the OrCam device on the quality of life of patients with End-stage Glaucoma. The conclusion was that OrCam, a novel artificial vision device using a mini-camera mounted on eyeglasses, allowed legally blind patients with end-stage glaucoma to read independently, subsequently improving their quality of life.\n\n\n"}
{"id": "452363", "url": "https://en.wikipedia.org/wiki?curid=452363", "title": "Permanent way (history)", "text": "Permanent way (history)\n\nThe permanent way is the elements of railway lines: generally the pairs of rails typically laid on the sleepers (\"ties\" in American parlance) embedded in ballast, intended to carry the ordinary trains of a railway. It is described as permanent way because in the earlier days of railway construction, contractors often laid a temporary track to transport spoil and materials about the site; when this work was substantially completed, the temporary track was taken up and the permanent way installed.\n\nThe earliest tracks consisted of wooden rails on transverse wooden sleepers, which helped maintain the spacing of the rails. Various developments followed, with cast iron plates laid on top of the wooden rails and later wrought iron plates or wrought iron angle plates (angle iron as L-shaped plate rails). Rails were also individually fixed to rows of stone blocks, without any cross ties to maintain correct separation. This system also led to problems, as the blocks could individually move. The first version of Isambard Kingdom Brunel's broad gauge system used rails laid on longitudinal sleepers whose rail gauge and elevation were pinned down by being tied to piles (conceptually akin to a pile bridge), but this arrangement was expensive and Brunel soon replaced it with what became the classic broad gauge track, in which the piles were forgone and transoms, similar to sleepers, maintained the rail gauge. Today, most rail track uses the standard system of rail and sleepers; ladder track is used in a few applications.\n\nDevelopments in manufacturing technologies has led to changes to the design, manufacture and installation of rails, sleepers and the means of attachments. Cast iron rails, long, began to be used in the 1790s and by 1820, long wrought iron rails were in use. The first steel rails were made in 1857 and standard rail lengths increased over time from . Rails were typically specified by units of weight per linear length and these also increased. Railway sleepers were traditionally made of Creosote-treated hardwoods and this continued through to modern times. Continuous welded rail was introduced into Britain in the mid 1960s and this was followed by the introduction of concrete sleepers.\n\nThe earliest use of a railway track seems to have been in connection with mining in Germany in the 12th century. Mine passageways were usually wet and muddy, and moving barrows of ore along them was extremely difficult. Improvements were made by laying timber planks so that wheeled containers could be dragged along by manpower. By the 16th century, the difficulty of keeping the wagon running straight had been solved by having a pin going into a gap between the planks. Georg Agricola describes box-shaped carts, called \"dogs\", about half as large again as a wheelbarrow, fitted with a blunt vertical pin and wooden rollers running on iron axles. An Elizabethan era example of this has been discovered at Silvergill in Cumbria, England, and they were probably also in use in the nearby Mines Royal of Grasmere, Newlands and Caldbeck. Where space permitted round-section wooden tracks to take trucks with flanged wheels were installed: a painting from 1544 by the Flemish artist Lucas Gassel shows a coppermine with rails of this type emerging from an adit.\n\nA different system was developed in England, probably in the late 16th century, near Broseley for conveying coal from mines, sometimes drift mines down the side of the Severn Gorge to the River Severn. This, probably a rope-hauled incline plane, had existed 'long before' 1605. This probably preceded the Wollaton Wagonway of 1604, which has hitherto been regarded as the first.\n\nIn Shropshire, the gauge was usually narrow, to enable the wagons to be taken underground in drift mines. However, by far the greatest number of wagonways were near Newcastle upon Tyne, where a single wagon was hauled by a horse on a wagonway of about the modern standard gauge. These took coal from the pithead down to a staithe, where the coal was loaded into river boats called keels.\n\nWear of the timber rails was a problem. They could be renewed by turning them over, but had to be regularly replaced. Sometimes, the rail was made in two parts, so that the top portion could easily be replaced when worn out. The rails were held together by wooden sleepers, covered with ballast to provide a surface for the horse to walk on.\n\nCast iron strips could be laid on top of timber rails, and the use of such materials probably occurred in 1738, but there are claims that this technology went back to 1716. In 1767, Ketley ironworks began producing cast iron plates, which were fixed to the top of wooden rails with nails, to provide a more durable running surface. This construct was known as strap-iron rail (or strap rail) and was widely used on pre-steam railways in the United States. Although relatively cheap and quick to build, they were unsuited to heavy loads and required 'excessive maintenance'. Train wheels rolling over the spikes loosened them, allowing the rail to break free and curve upwards sufficiently that a car wheel could get beneath it and force the end of the rail up through the floor of the car, writhing and twisting, endangering passengers. These broken rails became known as \"snake heads\".\nWhen wrought iron became available, wrought iron plates provided an even more durable surface. The rails had projecting lugs (or ears) with a hole to enable them to be fixed to the underlying wooden rail.\n\nAn alternative, developed by John Curr of Sheffield, the manager of the Duke of Norfolk's colliery there. This had a L-shaped rail, so that the flange was on the rail rather than on the wheel. This was also used by Benjamin Outram of Butterley Ironworks and William Jessop (who became a partner in them in 1790). These were used to transport goods for relatively short distances down to canals, though Curr's ran between the manor colliery and Sheffield town. These rails are referred to as plates, and the railway is sometimes called a plateway. The term \"platelayer\" also derives from this origin. In theory, the unflanged wheels could have been used on ordinary highways, but in practice this was probably rarely done, because the wagon wheels were so narrow that they would have dug into the road surface.\n\nThe system found wide adoption in Britain. Often, the plates were mounted on stone blocks, and sometimes without sleepers, but that was liable to cause the rails to spread apart, increasing the gauge. Railways of this kind were widely used in south Wales, particularly to transport limestone down to the ironworks, and then to take the iron to a canal, sometimes several miles away, which took the products to market. The rails were at first made of cast iron, typically in lengths of , spanning between stone blocks.\n\nThe stone blocks had been assumed to be permanent, but experience quickly showed that they settled and gradually moved under traffic, creating chaotic track geometry and causing derailments. Another problem was that the running surface was liable to become obstructed by stones, displaced from the ballast. An alternative was to use an iron tie bar to keep the rails to the proper gauge, incorporating a shoe in which the rail was fixed.\n\nAn example of this was the Penydarren or Merthyr tramway. This was used by Richard Trevithick to demonstrate a pioneer locomotive in 1804, using one of his high pressure steam engines, but the engine was so heavy that it broke many of the rails.\n\nCast iron edge rails were used by Thomas Dadford junior when building the Beaufort and Blaenavon lines to the Monmouthshire canal in 1793. These were rectangular, in width with a depth of and in length, and required flanges on the wagon wheels. The same year, Benjamin Outram used edge rails on the Cromford Canal. T-shaped beams were used by William Jessop on the Loughborough-Nanpantan line in 1794, and his sons used I-shaped beams in 1813–15 on a railway from Grantham to Belvoir Castle. Samples of these rails are held in the Science Museum, London.\n\nA short-lived alternative was the \"fish-bellied\" profile, first used by Thomas Barnes (1765–1801) at Walker Colliery, near Newcastle in 1798, which enabled rails to have a longer span between blocks. These were T-section edge rails, three feet long and laid on transverse stone sleepers. These were still made of cast iron.\n\nThe earliest rails had square butt joints, which were weak and difficult to keep in alignment. George Stephenson introduced lapped joints, which maintained their alignment quite well.\n\nThe breakthrough came when John Birkinshaw of Bedlington Ironworks in Northumberland developed rolled wrought iron rails in 1820 in lengths, as used for the Stockton and Darlington Railway. This was strong enough to bear the weight of a locomotive and of a train of wagons (or carriages) pulled by it. This marks the beginning of the modern rail era. This system was instantly successful, although some false starts took place. Some early rails were made in a T cross section, but the lack of metal at the foot limited the bending strength of the rail, which has to act as a beam between supports.\n\nAs metal technologies improved, these wrought iron rails were made progressively somewhat longer, and with a heavier, and therefore stronger, cross-section. By providing more metal in the foot of the rail, a stronger beam was created, achieving much better strength and stiffness, and a section was created similar to the bullhead rail section still visible today. This was expensive, however, and the promoters of early railways struggled with decisions about the appropriate weight (and therefore strength, and cost) of their rails.\n\nAt first, the rail section was almost symmetrical top-to-bottom, and was described as a double-headed rail. The intention was to invert the rail after the top surface had become worn, but rails tend to develop chair gall, an attrition of the rail where it is supported in the chairs, and this would have made running on the former bottom surface impossibly noisy and irregular. It was better to provide the extra metal on the top surface and gain extra wear there without the need to invert the rail at its half life.\n\nMany railways preferred a flat bottom rail section, where the rails could be laid directly on the sleepers, representing a marked cost saving. Indenting of the sleeper was the problem; where the traffic was heavy, it became necessary to provide a sole plate under the rails to spread the load on the tie, partly vitiating the cost saving. However, in main line situations, this form found almost universal adoption in North America and Australia, and in much of continental Europe. The United Kingdom persisted with bullhead rail in main line use, with widespread introduction of flat-bottom rail only starting in about 1947.\n\nThe first rails made from steel were made in 1857, when Robert Forester Mushet remelted scrap steel from an abortive Bessemer trial, in crucibles at Ebbw Vale ironworks, and were laid experimentally at Derby railway station on the Midland Railway in England. The rails proved far more durable than the iron rails they replaced and remained in use until 1873. Henry Bessemer supplied 500 tons of steel blooms to the London and North Western Railway's rail mill at Crewe in 1860. Several other companies began producing steel rails in the following years. The transition to steel rails was hastened by the introduction of open hearth steelmaking. William Siemens set up his Landore steelworks partly to supply rail to the Great Western Railway. A boom in rail production followed, but a banking crisis in America slowed the rate at which railways were built there and orders to British rail producers. The British iron and steel industry went into a recession, which particularly affected the wrought iron sector. When demand for rails began to grow again, it was largely for steel rails, which were more durable than those of iron.\n\nTimber sleepers, that is transverse beams supporting the two rails that form the track, replaced the individual stone blocks formerly used. This system has the major advantage that maintenance adjustments to the track geometry did not disrupt the all-important track gauge. The alignment of the track could be adjusted by sluing it bodily, without loss of gauge. Softwood was widely used, but its life was limited if it was not treated with preservative, and some railways set up creosoting plants for the purpose. Creosote-treated hardwood is now widely used in North America and elsewhere.\n\nBy now relatively long (perhaps 20 ft) wrought iron rails supported in chairs on timber cross-sleepers, were in use – a track form recognisable today in older track.\n\nSteel sleepers were tried as an alternative to timber; Acworth writing in 1889 describes the production of steel sleepers on the London & North Western Railway, and there is an illustration showing rolled channel section (shallow upturned \"U\" shapes) with no shaped ends, and with three-part forged chairs riveted direct. However steel sleepers seem not to have enjoyed widespread adoption until about 1995. Their dominant usage now is for life extension of existing track on secondary routes. They have a significant advantage on weak formations and poor ballast conditions, as the bearing area is at a high level, immediately under the rail seat.\n\nThe early cast iron rails of the 18th century and before used integral fixings for nailing or bolting to the railroad ties. Strap rails introduced in the late 18th century, of cast and later rolled iron were nailed to wooden supports via countersunk holes in the metal. The introduction of rolled rail profiles in the 1820s such as the \"single flanged T parallel rail\" and later \"double flanged T parallel rail\" required the use of chairs, keys to hold the rail, and bolts or spikes to fix the chair. The \"flat bottomed rail\" invented by Robert L. Stevens in 1830 was initially spiked directly to wooden sleepers, later tie plates were used to spread the load and also keep the rail in gauge with inbuilt shoulders in the plate. Outside North America a wide variety of spring based fastening systems were later introduced in combination with baseplates and flat bottomed rail, these are now ubiquitous on main line high speed railways.\n\nThe track was originally laid direct on the ground, but this quickly proved unsatisfactory and some form of ballast was essential, to spread the load and to retain the track in its proper position. The natural ground is rarely strong enough to accept the loading from locomotives without excessive settlement, and a layer of ballast under the sleeper reduces the bearing pressure on the ground. The ballast surrounding the sleepers also tends to keep them in place and resists displacement.\n\nThe ballast was usually some locally available mineral product, such as gravel or reject material from coal and iron mining activities. The Great North of Scotland Railway used river gravel – round pebbles. In later years the ash from steam engines was used and slag (a by-product of steel making).\n\nThe early railways were almost exclusively local concerns involved with conveying minerals to some waterway; for them the gauge of the track was adopted to suit the wagons intended to be used, and it was typically in the range 4 ft to 4 ft 8½ in, and at first there was no idea of the need for any conformity with the gauge of other lines. When the first public railways developed, George Stephenson's skilful innovation meant that his railways were dominant and the gauge he used was therefore the most widespread. As early notions of linking up different railway systems evolved, this gauge secured general adoption. It is more or less an accident of history that this gauge – which suited the wagons already in use at the colliery where George Stephenson had been an engine man – became the British standard gauge: it was exported to most of Europe and North America.\n\nReference is sometimes made to the \"gauge\" of ruts in stone roadways at ancient sites such as Pompeii, and these are often asserted to be about the same as Stephenson's gauge. Of course the ruts were made by the wheels of carts, and the carts were of a sensible size for horse-drawn carts prior to the industrial era, pretty much the same as the size of the pre-railway carts at the colliery where Stephenson worked: that is the only connection.\n\nWhen Isambard Kingdom Brunel conceived the Great Western Railway (GWR), he sought an improved design for his railway track and accepted none of the previous received wisdom without challenge. The 4 ft 8½in gauge had been fine for small mineral trucks on a horse-drawn tramway, but he wanted something more stable for his high speed railway. The large diameter wheels used in stage coaches gave better ride quality over rough ground, and Brunel originally intended to have his passenger carriages carried in the same way – on large diameter wheels placed outside the bodies of the carriages. To achieve this he needed a wider track gauge and he settled on the famous broad gauge. (It was later eased to 7 ft 0¼in). When the time came to build the passenger carriages, they were designed conventionally with smaller wheels under the bodies after all, but with a seven-foot track gauge the bodies could be much wider than on the standard gauge. His original intention to have the wheels outside the width of the bodies was abandoned.\n\nBrunel also looked at novel track forms, and decided to use a continuously supported rail. Using longitudinal timbers under each rail, he achieved a smoother profile while not requiring such a strong rail section, and he used a shallow bridge rail for the purpose. The wider, flat foot also meant that the chair needed by the bullhead section could be dispensed with. The longitudinal timbers needed to be kept at the proper spacing to retain the gauge correctly, and Brunel achieved this by using timber transoms – transverse spacers – and iron tie-bars. The whole assembly was referred to as the baulk road – railwaymen usually call their track a road. Initially, Brunel had the track tied down to timber piles to prevent lateral movement and bounce, but he had overlooked the fact that the made ground, on which his track was supported between piles, would settle. The piles remained stable and the ground between them settled so that his track soon had an unpleasant undulation, and he had to have the piles severed, so that the track could settle more or less uniformly. A variant of the baulk road can still be seen today on many older under-bridges where no ballast was provided. The design varies considerably, but in many cases longitudinal timbers are supported directly on the cross-girders, with transoms and tiebars to retain the gauge, but of course with modern rails and base-plates or chairs. The longitudinal sleepers are somewhat similar to modern-day Ladder track.\n\nThe group of railways that had Brunel as their engineer were successful and the broad gauge track spread throughout the west of England, South Wales, and the West Midlands. But, as the British railway network spread, the incompatibility of the two systems became a serious blockage, as a wagon could not be sent from one system to the other without transshipping the goods by hand. A Gauge Commission was appointed to determine national policy. The Broad Gauge was technically superior but conversion of the standard gauge routes to broad would have meant reconstructing every tunnel, bridge and station platform, whereas universal adoption of the standard gauge only required the progressive conversion of the track itself. The broad gauge was doomed, and no further independent broad gauge lines could be built.\n\nThe existing broad gauge routes could continue, but as they had no development potential it was only a matter of time before they were eventually converted to standard. In the meantime, an extensive mileage of mixed gauge track was installed, where each line had three rails to accommodate trains of either gauge. There were some instances of \"mixed gauge trains\" being run, where wagons of each gauge were run in a single train. The legacy of the broad gauge can still be seen where there seems to be an unnecessarily wide space between station platforms.\n\nAt the beginning of the twentieth century, the form of British track had converged on the use of wrought iron bullhead rails supported in cast iron chairs on timber sleepers, laid in some form of ballast. In North America, the standard was T-rails and tie plates fastened to timber crossties with cut spikes. Many railways were using very light rails and, as locomotive weights and speeds increased, these became inadequate. Consequently, on main lines, the rails in use were made progressively heavier (and stronger). Metallurgical processes improved and better rails, including some of steel, came into use. From a maintenance point of view, the rail joints were the source of most of the work, and as steel-making techniques improved it became possible to roll steel rails of increased length – reducing the number of joints per mile. The standard length became 30 ft (9 144mm), then 45 ft (13 716mm) and finally 60 ft (18 288mm) rails became the norm. For main line use, the standard rail section became the 95BH section, weighing 95 lb per yard (47.13 kg per metre). For secondary routes, a lighter 85BH (42.16 kg per metre) section was used.\n\nFlat bottom rails were still seen as undesirable for British main line railway use, despite their successful use in North America, although some lightly operated British railways used them, generally spiked direct to the sleepers. Under heavy usage, they indent the sleepers severely and the incremental cost of a base-plate appeared at this early date, to rule the flat bottom section out.\n\nTimber sleepers were expensive and not durable, and the railways’ engineers had strong – and conflicting – views about the best wood species and the best preservative treatments. The railways moved towards standardisation on a softwood sleeper preserved by pressure injection of creosote, measuring 8 ft 6in (2 591 mm) long by 10in (254mm) by 5in (127mm). Chairs were secured to the sleepers by trenails (steel spikes driven through a timber sleeve) or three chair-screws on first class routes. The GWR alone among the main line railways kept to its own standard, the 00 rail at 97½ lb/yd (48.365 kg per metre), and with two fangbolts securing each chair to the sleeper, with the head of the bolt under the sleeper and a nut above the chair—more secure but much more difficult to adjust.\n\nSome experiments were made before 1945 with reinforced concrete sleepers, in most cases with bullhead chairs mounted on them. This was in response to the very high price of the best (most durable) timber, but reinforced concrete sleepers were never successful in main line use. Concrete pots were also used in sidings; they are sometimes called twin-block sleepers, and consisted of two concrete blocks each mounted with a chair, and an angle iron connecting them and retaining the gauge.\n\nAt the end of the Second World War in 1945, the British railways were worn out, having been patched up following war damage without the availability of much new material. The country was economically in a weak situation also, and for nearly a decade after the war, materials – especially steel and timber – were in very short supply. Labour too was seriously restricted in availability.\n\nThe railway companies became persuaded that the traditional bullhead forms of track needed revision, and after some experimentation a new flat bottom rail format was adopted. The British Standard sections were unsuitable and a new profile, a 109 lb/yard rail, was made the new standard. In 60 ft lengths, laid on steel baseplates on softwood sleepers, it was to be the universal standard. The fastenings were to be of a resilient steel type, and for secondary routes a 98 lb/yd rail was adopted. Regional variations still persisted, and hardwood sleepers and Mills clip fastenings were favoured on the Eastern Region, for example.\n\nThe new designs were successful, but they introduced many challenges, especially as the availability of experienced track maintenance staff became acutely difficult, and poorly maintained flat bottom track seemed more difficult to keep in good order than poorly maintained bullhead track. The greater stiffness of flat-bottom was an advantage, but it tended to straighten out between the joints on curves; and flat bottom’s rigidity led to high vertical impact forces at badly maintained joints and this resulted in high volumes of fatigue fractures at the joints. Moreover, the elastic rail fastenings had little resistance to rail creep – the propensity of the rails to move gradually in the direction of traffic, and the workload of pulling back the rails to regulate the joints was surprisingly high.\n\nMuch of the work of maintaining the track was at the joints, especially as the stiff rails became dipped, and the joint sleepers took a hammering. Pre-war experiments with long welded rail lengths were built upon, and in the years from 1960 long rail lengths were installed, at first on hardwood sleepers but soon on concrete sleepers. For example, the first long welded rail (almost ) on the UK's East Coast Main Line was laid in 1957, just south of Carlton-on-Trent, resting on rubber pads to resist rail creep. In this pioneering stage, some catastrophic mistakes in detailed design were made, but from about 1968 continuous welded rail became a reliable standard for universal installation on main and secondary routes. The form adopted used pre-stressed concrete sleepers and a 110A rail section – a slight improvement on the 109 rails previously used – the A was to distinguish it from the British Standard 110 lb/yd rail section, which was unsuitable. Rail fastenings eventually converged onto a proprietary spring clip made by the Pandrol company which was the exclusive form of fastening in Britain for about 30 years.\n\nThe welded track was to be laid on six to twelve inches (15 to 30 centimetres) of crushed stone ballast, although this was not always achieved, and the bearing capacity of the formation was not always taken into account, leading to some spectacular formation failures.\n\nA further enhancement to the rail profile produced the 113A section, which was the universal standard until about 1998; detail improvements to the sleepers and ballast profile completed the picture and the general form of the track had stabilised. This format is now in place over 99% of the first-class main lines in Britain, although the CEN60 (60 kg/m) rail section was introduced in the UK during the 1990s. This has a wider rail foot and is taller than the 113A section so is incompatible with standard sleepers.\n\nTrack renewal trains have now replaced labour-intensive permanent way gangs. Long welded rail was hard to install manually. An early demonstration of mechanised track-laying with two lengths of long welded rail took place on the Fighting Cocks branch in 1958. The two lengths were loaded on ten wagons, attached to the existing track by a steel rope and drawn back at a minute. As the train moved back, the old rails were levered out and the new ones dropped into the chairs. A hoist on the rear wagon dropped the last part of the rail into place.\n\nAs stated, the general track gauge in Britain was . In the later 1950s, general track maintenance standards deteriorated rapidly due to manning difficulties, and freight train speeds increased on some routes. Freight trains consisted almost entirely of short wheelbase (10 ft) four-wheeled wagons carried on a very stiff elliptical leaf spring suspension, and these wagons showed an alarmingly rapid rate of increase of derailment events. Anyone standing at the lineside could watch a freight train pass at speed and observe several of the wagons weaving and swaying alarmingly even on good track, and derailment occurred when any poor track was encountered.\n\nThe dynamic behaviour of the wagons was the problem, but the solution adopted was to reduce the permitted speed of the wagons to 45 mph, and to reduce the track gauge by one-eighth of an inch, to 4 ft 8⅜in (1432mm) for new installations of continuously welded track on concrete sleepers. Of course the long life cycle of the track meant that this conversion process would take 30 years or more to complete. However, the basis of the gauge narrowing was mistaken. The idea seems to have been to reduce the free space for lateral movement of the wagons, so that they would be \"contained\" to run in a straight line. In fact, railway vehicles are not contained by the flanges of the wheels except in \"very\" sharp curves, and in normal running the steering effect due to the conicity of the wheels is dominant. In reducing the track gauge, the effective conicity is increased – worsened – and the tendency of the wagons to yaw and roll was increased. Many derailments took place on relatively new continuously welded rail track, and often such a derailment destroyed about a mile of the new track, as the freight train might take that distance to stop; the concrete sleepers were not robust under a derailed wagon's wheels.\n\nThe effect reduced as the wagon fleet was modernised (and other effects took first place) and the track gauge for new track was quietly restored to . Of course the vast majority of the track on main lines is still, as installed, at the tighter gauge, and it will be several decades before the gauge change is complete.\n\nTerminology is difficult for \"switches and crossings\" (S&C) previously \"points and crossings\", or \"fittings\".\n\nEarly S&C allowed only a very slow speed on the subsidiary route (the \"turnout\"), so geometrical design was not too important. Many older s&c units had a loose joint at the heel so that the switch rail could turn to close to the stock rail or open from it. When the switch rail was closed, a reasonable alignment was secured; when it was open, no wheel could run on it so it did not matter.\n\nAs speeds rose, this was no longer feasible and the switch rails were fixed at the heel end, and their flexibility enabled the toe end to open and close. Manufacture of the switch rails was a complex process, and that of the crossings even more so. Speeds on the subsidiary route were rarely higher than 20 mph except in very special designs, and great ingenuity was employed to give a good ride to vehicles passing through at speed on the main line. A difficulty was the common crossing where continuous support to wheels passing was difficult, and the point rail was planed to protect it from direct impact in the facing direction, so that a designed irregularity in support was introduced.\n\nAs faster speeds were required, more configurations of s&c were designed, and a very large number of components, each specific to only one type of s&c, was required. At faster speeds on the turnout road, the divergence from the main route is much more gradual, and therefore a very considerable length of planning of the switch rail is required.\n\nAbout 1971, this trend was reversed with the so-called vertical s&c, in which the rails were held vertical, rather than at the customary 1 in 20 inclination. With other simplifications, this considerably reduced the stockholding required for a wide range of s&c speeds, although the vertical rail imposes a loss of the steering effect and the ride through new vertical s&c is often irregular.\n\nContinuous Welded Rail (CWR) was developed in response to the observation that the bulk of track maintenance work takes place at the joints. As steel production and manufacturing processes improved, the rail lengths installed were progressively increased, and the logical extension of this would be to eliminate the joints altogether.\n\nA major obstacle to doing so is thermal expansion: the rails expand in higher temperatures. Without joints, there is no room for the rails to expand; as the rails get warmer, they will develop an enormous force in \"trying\" to expand. If prevented from expanding, they develop a force of 1.7 tonnes (17 kN) for every 1 degree Celsius of temperature change in a practical rail section.\n\nIf a small cube of metal is compressed between the jaws of a press, it will contract—that is it will be squashed somewhat—and a very large force can be resisted by it without ultimate failure. However, if a long piece of metal of the same cross section is compressed, it will deform sideways into a bow shape; the process is called buckling, and the compressive force it can withstand is very much less.\n\nIf the long thin piece of metal could be constrained to prevent it from buckling (e.g. by being contained inside a tube) then it can resist a much higher compressive force. If the rails can be constrained in a similar way, they can be prevented from buckling. The weight of the track resists buckling upwards, so buckling is most likely to take place laterally. This is prevented by:\n\n\nIf the rail is held so that it cannot expand at all, then there is no limit on the length of rail that can be handled. The expansive force in a one-foot length of rail at a certain temperature is the same as in a length of rail. Early continuous welded rail was installed in limited lengths only because of technological limitations. However at the end of the CWR section where it abutted older, ordinary jointed track, that track would be unable to resist the expansive force and the jointed track might be forced to buckle. To prevent that, special expansion switches, sometimes called breathers, were installed. The expansion switches could accommodate a considerable expansive movement—typically four inches (100mm) or so—in the end section of the CWR without passing the movement on to the jointed track.\n\nThe CWR is installed and fastened down at an optimum temperature, to ensure that the highest possible expansive force is limited. This temperature is called the stress-free temperature, and in the UK it is . It is in the upper range of ordinary outdoor temperatures, and the actual installation work tends to be done at cooler temperatures. Originally the rails were physically heated to the stress-free temperature with propane gas heaters; they were then rattled with hand bars to eliminate any binding, preventing even expansion, and then clipped down. Since about 1963 however hydraulic jacks are used to physically stretch the rails while they are supported on temporary rollers. By stretching the rails to the length they would be if they were at the stress-free temperature, then there is no need to heat them; they can just be clipped down before the jacks are released.\n\nThe CWR rails are made by welding ordinary rails together. For many years, rails could only be made in lengths of up to 60 ft (18m 288mm) in Britain, and the factory welding process made them into 600, 900 or 1200 ft lengths, depending on the factory. The process used was a flash-butt process in which high electrical currents are used to soften the rail end, and the ends are then forced together by rams. The flash-butt process is very reliable, providing that the factory ensured good geometry of the rail ends.\n\nThe long rails could be conveyed to site by a special train, and unloaded on to the ground (by chaining the end in position and pulling the train out from underneath the rails). The long rails had to be welded together (or to adjacent track) using a site welding process; and, after initial experimentation, the proprietary Thermit welding process was used. This was an alumino-thermic process in which a powder 'portion' was ignited; the aluminium was the fuel and a metallurgically appropriate composition of molten steel descended into the gap between the rail ends, contained in refractory moulds.\n\nThe original SmW process was very sensitive to operator skill, and as the welding was usually the final process before returning the track to traffic, time pressure was sometimes applied resulting in unwanted improper welds. The improved SkV process was less sensitive and over the years weld quality improved.\n\nWhilst jointed track has suffered buckles in the past; the fish-plates need to be removed and greased annually (the requirement was relaxed to bi-annually in 1993) and where this was forgotten or where ballast conditions were especially weak, buckling took place in hot weather. In addition, if rails were allowed to creep, it was always possible that several successive joints closed up, so that the expansion gap was lost, with inevitable results at the onset of hot weather.\n\n"}
{"id": "44380976", "url": "https://en.wikipedia.org/wiki?curid=44380976", "title": "Pete Flint", "text": "Pete Flint\n\nPete Flint (born July 25, 1974) is a British internet entrepreneur and investor based in San Francisco. He is the founder and former chairman and CEO of Trulia. Formerly he was part of the founding team at lastminute.com where he was responsible for their highly successful growth and online marketing strategy. He is the most successful British entrepreneur based in the San Francisco Bay Area. He currently is a managing partner at NFX, the early stage venture capital fund focused on Network Effect Businesses.\n\nFlint earned a first class bachelor’s degree and master's in physics from Oxford University and an MBA from the Stanford Graduate School of Business.\n\nShortly after graduating from Oxford University, Flint joined Brent Hoberman and Martha Lane Fox to start lastminute.com. He had previously worked alongside Hoberman at LineOne, a joint venture between New International and British Telecom, where Flint worked for less than a year.\n\nFlint spent five years at lastminute.com helping to scale the company from a business plan to a public company with 2,000 employees and operations in 11 countries. Flint left lastminute.com to attend Stanford Graduate School of Business in 2003. lastminute.com was acquired by Sabre Group for $1.1 billion in cash in 2005.\n\nFlint conceived of Trulia in 2004 while he was between the first and second year of his MBA course at Stanford while looking for off campus housing. He observed that consumer usage to research real estate was growing rapidly, yet all the major websites in the category were not focused on delivering a compelling consumer experience and failed to provide compelling advertising products to tap into the billions of dollars that were being spent on real estate newspaper classified advertising. Flint wrote the original business plan while at Stanford. Flint recruited a team of Stanford students to turn Flint’s concept into a company, of the original students that worked on the project, Sami Inkinen joined Flint to incorporate the company in June 2005.\n\nFlint was the driving force behind Trulia’s innovative product, revenue and user growth strategies that enabled it to navigate the 2008 financial and housing collapse and go on to become one of the most popular real estate mobile apps and websites, spending zero dollars on consumer marketing by the time it went public.\n\nFlint scaled Trulia to become one of the leading online real estate companies in the US, raising $33 million in venture capital from Accel Partners and Sequoia Capital, taking the company public with an IPO on the NYSE in September 2012. Trulia merged with Zillow in a $3.5 billion in 2014 to create the world's largest real estate company.\n\nFlint has been an active angel investor and advisor to consumer Internet and network effect businesses. In December 2016 it was announced that he joined NFX as a managing partner. NFX is an early stage venture capital fund based in the San Francisco Bay Area and focused on Network Effect Businesses.\n\nFlint supports a number of charitable causes and is a founder and the co-chair of the leading British Tech not-for-profit Community GBx in the San Francisco Bay Area, which brings together the leading British technology executives, investors and entrepreneurs. He is also a founding board member of the U.S.-based, fast-growing New Story Charity, which builds sustainable communities in the places most in need.\n\nA frequent speaker at technology and entrepreneurship conferences, Flint is featured regularly on CNBC, FOX, Bloomberg, WSJ, and The New York Times. He also is a guest lecturer at Stanford's Graduate School of Business.\n\nIn 2014, Flint was named Northern California EY Entrepreneur Of The Year. He has also been names one of Fortune Magazine's top tech disrupters, most admired CEO in the San Francisco Bay Area according to the San Francisco Business Times, and one of Forbes Magazine's Most Powerful CEOs Under 40. Flint was named one of the 100 most influential people in real estate by Inman magazine in 2008-2014 and one of the most powerful people in real estate in Swanepoel Power 200. In 2016, Flint was named a Henry Crown Fellow at The Aspen Institute.\n\n"}
{"id": "23083", "url": "https://en.wikipedia.org/wiki?curid=23083", "title": "Playing card", "text": "Playing card\n\nA playing card is a piece of specially prepared heavy paper, thin cardboard, plastic-coated paper, cotton-paper blend, or thin plastic, marked with distinguishing motifs and used as one of a set for playing card games. Playing cards are typically palm-sized for convenient handling, and were first invented in China during the Tang dynasty.\n\nPlaying cards may have been invented during the Tang dynasty around the 9th century AD as a result of the usage of woodblock printing technology. The first possible reference to card games comes from a 9th-century text known as the \"Collection of Miscellanea at Duyang\", written by Tang dynasty writer Su E. It describes Princess Tongchang, daughter of Emperor Yizong of Tang, playing the \"leaf game\" in 868 with members of the Wei clan, the family of the princess' husband. The first known book on the \"leaf\" game was called the \"Yezi Gexi\" and allegedly written by a Tang woman. It received commentary by writers of subsequent dynasties. The Song dynasty (960–1279) scholar Ouyang Xiu (1007–1072) asserts that the \"leaf\" game existed at least since the mid-Tang dynasty and associated its invention with the development of printed sheets as a writing medium. However, Ouyang also claims that the \"leaves\" were pages of a book used in a board game played with dice, and that the rules of the game were lost by 1067.\n\nOther games revolving around alcoholic drinking involved using playing cards of a sort from the Tang dynasty onward. However, these cards did not contain suits or numbers. Instead, they were printed with instructions or forfeits for whomever drew them.\n\nThe earliest dated instance of a game involving cards with suits and numerals occurred on 17 July 1294 when \"Yan Sengzhu and Zheng Pig-Dog were caught playing cards [zhi pai] and that wood blocks for printing them had been impounded, together with nine of the actual cards.\"\n\nWilliam Henry Wilkinson suggests that the first cards may have been actual paper currency which doubled as both the tools of gaming and the stakes being played for, similar to trading card games. Using paper money was inconvenient and risky so they were substituted by play money known as \"money cards\". One of the earliest games in which we know the rules is \"madiao\", a trick-taking game, which dates to the Ming Dynasty (1368–1644). 15th-century scholar Lu Rong described it is as being played with 38 \"money cards\" divided into four suits: 9 in coins, 9 in strings of coins (which may have been misinterpreted as sticks from crude drawings), 9 in myriads (of coins or of strings), and 11 in tens of myriads (a myriad is 10,000). The two latter suits had \"Water Margin\" characters instead of pips on them with Chinese characters to mark their rank and suit. The suit of coins is in reverse order with 9 of coins being the lowest going up to 1 of coins as the high card.\n\nDespite the wide variety of patterns, the suits show a uniformity of structure. Every suit contains twelve cards with the top two usually being the court cards of king and vizier and the bottom ten being pip cards. Half the suits use reverse ranking for their pip cards. There are many motifs for the suit pips but some include coins, clubs, jugs, and swords which resemble later Mamluk and Latin suits. Michael Dummett speculated that Mamluk cards may have descended from an earlier deck which consisted of 48 cards divided into four suits each with ten pip cards and two court cards. \n\nBy the 11th century, playing cards were spreading throughout the Asian continent and later came into Egypt. The oldest surviving cards in the world are four fragments found in the Keir Collection and one in the Benaki Museum. They are dated to the 12th and 13th centuries (late Fatimid, Ayyubid, and early Mamluk periods).\n\nA near complete pack of Mamluk playing cards dating to the 15th century and of similar appearance to the fragments above was discovered by Leo Aryeh Mayer in the Topkapı Palace, Istanbul, in 1939. It is not a complete set and is actually composed of three different packs, probably to replace missing cards. The Topkapı pack originally contained 52 cards comprising four suits: polo-sticks, coins, swords, and cups. Each suit contained ten pip cards and three court cards, called \"malik\" (king), \"nā'ib malik\" (viceroy or deputy king), and \"thānī nā'ib\" (second or under-deputy). The \"thānī nā'ib\" is a non-existent title so it may not have been in the earliest versions; without this rank, the Mamluk suits would structurally be the same as a Ganjifa suit. In fact, the word \"Kanjifah\" appears in Arabic on the king of swords and is still used in parts of the Middle East to describe modern playing cards. Influence from further east can explain why the Mamluks, most of whom were Central Asian Turkic Kipchaks, called their cups \"tuman\" which means myriad in Turkic, Mongolian and Jurchen languages. Wilkinson postulated that the cups may have been derived from inverting the Chinese and Jurchen ideogram for myriad ().\n\nThe Mamluk court cards showed abstract designs or calligraphy not depicting persons possibly due to religious proscription in Sunni Islam, though they did bear the ranks on the cards. \"Nā'ib\" would be borrowed into French (\"nahipi\"), Italian (\"naibi\"), and Spanish (\"naipes\"), the latter word still in common usage. Panels on the pip cards in two suits show they had a reverse ranking, a feature found in madiao, \"ganjifa\", and old European card games like ombre, tarot, and maw.\n\nA fragment of two uncut sheets of Moorish-styled cards of a similar but plainer style were found in Spain and dated to the early 15th century.\n\nExport of these cards (from Cairo, Alexandria, and Damascus), ceased after the fall of the Mamluks in the 16th century. The rules to play these games are lost but they are believed to be plain trick games without trumps.\n\nFour-suited playing cards are first attested in Southern Europe in 1365, and are likely derived from the Mamluk suits of cups, coins, swords, and polo-sticks, which are still used in traditional Latin decks. As polo was an obscure sport to Europeans then, the polo-sticks became batons or cudgels. Their presence is attested in Catalonia in 1371, 1377 in Switzerland, and 1380 in many locations including Florence and Paris. Wide use of playing cards in Europe can, with some certainty, be traced from 1377 onward.\n\nIn the account books of Johanna, Duchess of Brabant and Wenceslaus I, Duke of Luxembourg, an entry dated May 14, 1379 reads: \"Given to Monsieur and Madame four peters, two forms, value eight and a half moutons, wherewith to buy a pack of cards\". In his book of accounts for 1392 or 1393, Charles or Charbot Poupart, treasurer of the household of Charles VI of France, records payment for the painting of three sets of cards.\n\nFrom about 1418 to 1450 professional card makers in Ulm, Nuremberg, and Augsburg created printed decks. Playing cards even competed with devotional images as the most common uses for woodcuts in this period. Most early woodcuts of all types were coloured after printing, either by hand or, from about 1450 onwards, stencils. These 15th-century playing cards were probably painted. The Flemish Hunting Deck, held by the Metropolitan Museum of Art is the oldest complete set of ordinary playing cards made in Europe from the 15th century.\n\nAs cards spread from Italy to Germanic countries, the Latin suits were replaced with the suits of leaves (or shields), hearts (or roses), bells, and acorns, and a combination of Latin and Germanic suit pictures and names resulted in the French suits of (clovers), (tiles), (hearts), and (pikes) around 1480. The \"trèfle\" (clover) was probably derived from the acorn and the (pike) from the leaf of the German suits. The names and \"spade\", however, may have derived from the sword () of the Italian suits. In England, the French suits were eventually used, although the earliest packs circulating may have had Latin suits. This may account for why the English called the clovers \"clubs\" and the pikes \"spades\".\n\nIn the late 14th century, Europeans changed the Mamluk court cards to represent European royalty and attendants. In a description from 1377, the earliest courts were originally a seated \"king\", an upper marshal that held his suit symbol up, and a lower marshal that held it down. The latter two correspond with the \"ober\" and \"unter\" cards found in German and Swiss playing cards. The Italians and Iberians replaced the / system with the \"Knight\" and \" or \" before 1390, perhaps to make the cards more visually distinguishable. In England, the lowest court card was called the \"knave\" which originally meant \"male child\" (compare German ), so in this context the character could represent the \"prince\", son to the king and queen; the meaning \"servant\" developed later. Queens appeared sporadically in packs as early as 1377, especially in Germany. Although the Germans abandoned the queen before the 1500s, the French permanently picked it up and placed it under the king. Packs of 56 cards containing in each suit a king, queen, knight, and knave (as in tarot) were once common in the 15th century.\n\nDuring the mid 16th century, Portuguese traders introduced playing cards to Japan. The first indigenous Japanese deck was the named after the period.\n\nPacks with corner and edge indices (i.e. the value of the card printed at the corner(s) of the card) enabled players to hold their cards close together in a fan with one hand (instead of the two hands previously used). The first such pack known with Latin suits was printed by Infirerra and dated 1693, but this feature was commonly used only from the end of the 18th century. The first American-manufactured (French) deck with this innovation was the Saladee's Patent, printed by Samuel Hart in 1864. In 1870, he and his cousins at Lawrence & Cohen followed up with the Squeezers, the first cards with indices that had a large diffusion.\n\nThis was followed by the innovation of reversible court cards. This invention is attributed to a French card maker of Agen in 1745. But the French government, which controlled the design of playing cards, prohibited the printing of cards with this innovation. In central Europe (Trappola cards) and Italy (Tarocco Bolognese) the innovation was adopted during the second half of the 18th century. In Great Britain, the pack with reversible court cards was patented in 1799 by Edmund Ludlow and Ann Wilcox. The French pack with this design was printed around 1802 by Thomas Wheeler.\n\nSharp corners wear out more quickly, and could possibly reveal the card's value, so they were replaced with rounded corners. Before the mid-19th century, British, American, and French players preferred blank backs. The need to hide wear and tear and to discourage writing on the back led cards to have designs, pictures, photos, or advertising on the reverse.\n\nThe United States introduced the joker into the deck. It was devised for the game of euchre, which spread from Europe to America beginning shortly after the American Revolutionary War. In euchre, the highest trump card is the Jack of the trump suit, called the \"right bower\" (from the German \"\"); the second-highest trump, the \"left bower\", is the jack of the suit of the same color as trumps. The joker was invented c. 1860 as a third trump, the \"imperial\" or \"best bower\", which ranked higher than the other two \"bowers\". The name of the card is believed to derive from \"juker\", a variant name for euchre. The earliest reference to a joker functioning as a wild card dates to 1875 with a variation of poker.\n\nContemporary playing cards are grouped into three broad categories based on the suits they use: French, Latin, and Germanic. Latin suits are used in the closely related Spanish and Italian formats. The Swiss-German suits are distinct enough to merit their subcategory. Excluding jokers and tarot trumps, the French 52-card deck preserves the number of cards in the original Mamluk deck, while Latin and Germanic decks average fewer. Latin decks usually drop the higher-valued pip cards, while Germanic decks drop the lower-valued ones.\n\nWithin suits, there are regional or national variations called \"standard patterns.\" Because these patterns are in the public domain, this allows multiple card manufacturers to recreate them. Pattern differences are most easily found in the face cards but the number of cards per deck, the use of numeric indices, or even minor shape and arrangement differences of the pips can be used to distinguish them. Some patterns have been around for hundreds of years. Jokers are not part of any pattern as they are a relatively recent invention and lack any standardized appearance so each publisher usually puts their own trademarked illustration into their decks. The wide variation of jokers has turned them into collectible items. Any card that bore the stamp duty like the ace of spades in England or the ace of clubs in France are also collectible as that is where the manufacturer's logo is usually placed.\n\nUsually the cards have their pips printed only in the upper left corner assuming holding them with right hand. Such design may be uncomfortable for left-handed people who may prefer all four corners of the card to be used.\n\nFrench decks come in a variety of patterns and deck sizes. The 52-card deck is the most popular deck and includes 13 ranks of each suit with reversible \"court\" or face cards. Each suit includes an ace, depicting a single symbol of its suit, a king, queen, and jack, each depicted with a symbol of their suit; and ranks two through ten, with each card depicting that number of pips of its suit. As well as these 52 cards, commercial packs often include between one and six jokers, most often two.\n\nDecks with less than 52 cards are known as stripped decks. The piquet pack has all values from 2 through 6 in each suit removed for a total of 32 cards. It is popular in France, the Low Countries, Central Europe and Russia and is used to play piquet, belote, bezique and skat. It is also used in the Sri Lankan, whist-based game known as \"omi\". Forty-card French suited packs are common in northwest Italy; these remove the 8s through 10s like Latin suited decks. 24 card decks, removing 2s through 8s are also sold in Austria and Bavaria to play schnapsen.\n\nA pinochle deck consists of two copies of each of the 9, 10, jack, queen, king, and ace cards of all four suits. It thus comprises just 48 cards per deck.\n\nThe 78 card tarot nouveau adds the knight card between queens and jacks along with 21 numbered trumps and the unnumbered Fool.\n\nThe Unicode standard for text encoding on computers defines 8 characters for card suits in the Miscellaneous Symbols block, at U+2660–2667. Unicode 7.0 added a unified pack for French-suited tarot nouveau's trump cards and the 52 cards of the modern French pack, with 4 knights, together with a character for \"Playing Card Back\" and black, red, and white jokers in the block U+1F0A0–1F0FF.\n\n\n\n\n\n\n\n\nPlaying card societies (collectors and researchers)\n\nHistory of playing cards\nPlaying card iconography\nMuseums, Institutes and Organisations\n\nPlaying card collections online\n"}
{"id": "64212", "url": "https://en.wikipedia.org/wiki?curid=64212", "title": "Potassium nitrate", "text": "Potassium nitrate\n\nPotassium nitrate is a chemical compound with the chemical formula KNO. It is an ionic salt of potassium ions K and nitrate ions NO, and is therefore an alkali metal nitrate.\n\nIt occurs in nature as a mineral, niter. It is a source of nitrogen, from which it derives its name. Potassium nitrate is one of several nitrogen-containing compounds collectively referred to as saltpeter or saltpetre.\n\nMajor uses of potassium nitrate are in fertilizers, tree stump removal, rocket propellants and fireworks. It is one of the major constituents of gunpowder (black powder) and has been used since the Middle Ages as a food preservative.\n\nPotassium nitrate, because of its early and global use and production, has many names. Hebrew and Egyptian words for it had the consonants n-t-r, indicating likely cognation in the Greek \"nitron\", which was Latinised to \"nitrum\" or \"nitrium\". Thence Old French had \"niter\" and Middle English \"nitre\". By the 15th century, Europeans referred to it as \"saltpeter\" and later as \"nitrate of potash,\" as the chemistry of the compound was more fully understood.\n\nThe Arabs called it \"Chinese snow\" ( '). It was called \"Chinese salt\" by the Iranians/Persians or \"salt from Chinese salt marshes\" ( ').\n\nPotassium nitrate has an orthorhombic crystal structure at room temperature, which transforms to a trigonal system at .\n\nPotassium nitrate is moderately soluble in water, but its solubility increases with temperature (see infobox). The aqueous solution is almost neutral, exhibiting pH 6.2 at for a 10% solution of commercial powder. It is not very hygroscopic, absorbing about 0.03% water in 80% relative humidity over 50 days. It is insoluble in alcohol and is not poisonous; it can react explosively with reducing agents, but it is not explosive on its own.\n\nBetween , potassium nitrate reaches a temperature dependent equilibrium with potassium nitrite:\n\nThe earliest known complete purification process for potassium nitrate was outlined in 1270 by the chemist and engineer Hasan al-Rammah of Syria in his book \"al-Furusiyya wa al-Manasib al-Harbiyya\" (\"The Book of Military Horsemanship and Ingenious War Devices\"). In this book, al-Rammah describes first the purification of \"barud\" (crude saltpeter mineral) by boiling it with minimal water and using only the hot solution, then the use of potassium carbonate (in the form of wood ashes) to remove calcium and magnesium by precipitation of their carbonates from this solution, leaving a solution of purified potassium nitrate, which could then be dried. This was used for the manufacture of gunpowder and explosive devices. The terminology used by al-Rammah indicated a Chinese origin for the gunpowder weapons about which he wrote.\n\nAt least as far back as 1845, Chilean saltpeter deposits were exploited in Chile and California, USA.\n\nA major natural source of potassium nitrate was the deposits crystallizing from cave walls and the accumulations of bat guano in caves. Extraction is accomplished by immersing the guano in water for a day, filtering, and harvesting the crystals in the filtered water. Traditionally, guano was the source used in Laos for the manufacture of gunpowder for \"Bang Fai\" rockets.\n\nPerhaps the most exhaustive discussion of the production of this material is the 1862 LeConte text. He was writing with the express purpose of increasing production in the Confederate States to support their needs during the American Civil War. Since he was calling for the assistance of rural farming communities, the descriptions and instructions are both simple and explicit. He details the \"French Method\", along with several variations, as well as a \"Swiss method\". N.B. Many references have been made to a method using only straw and urine, but there is no such method in this work.\n\nNiter-beds are prepared by mixing manure with either mortar or wood ashes, common earth and organic materials such as straw to give porosity to a compost pile typically high, wide, and long. The heap was usually under a cover from the rain, kept moist with urine, turned often to accelerate the decomposition, then finally leached with water after approximately one year, to remove the soluble calcium nitrate which was then converted to potassium nitrate by filtering through potash.\n\nLeConte describes a process using only urine and not dung, referring to it as the \"Swiss method\". Urine is collected directly, in a sandpit under a stable. The sand itself is dug out and leached for nitrates which were then converted to potassium nitrate using potash, as above.\n\nFrom 1903 until the World War I era, potassium nitrate for black powder and fertilizer was produced on an industrial scale from nitric acid produced using the Birkeland–Eyde process, which used an electric arc to oxidize nitrogen from the air. During World War I the newly industrialized Haber process (1913) was combined with the Ostwald process after 1915, allowing Germany to produce nitric acid for the war after being cut off from its supplies of mineral sodium nitrates from Chile (see nitratite).\n\nPotassium nitrate can be made by combining ammonium nitrate and potassium hydroxide.\n\nAn alternative way of producing potassium nitrate without a by-product of ammonia is to combine ammonium nitrate, found in instant ice packs, and potassium chloride, easily obtained as a sodium-free salt substitute.\n\nPotassium nitrate can also be produced by neutralizing nitric acid with potassium hydroxide. This reaction is highly exothermic.\n\nOn industrial scale it is prepared by the double displacement reaction between sodium nitrate and potassium chloride.\n\nPotassium nitrate has a wide variety of uses, largely as a source of nitrate.\n\nHistorically, nitric acid was produced by combining sulfuric acid with nitrates such as saltpeter. In modern times this is reversed: nitrates are produced from nitric acid produced via the Ostwald process.\n\nThe most famous use of potassium nitrate is probably as the oxidizer in blackpowder. From the most ancient times through the late 1880s, blackpowder provided the explosive power for all the world's firearms. After that time, small arms and large artillery increasingly began to depend on cordite, a smokeless powder. Blackpowder remains in use today in black powder rocket motors, but also in combination with other fuels like sugars in \"rocket candy\". It is also used in fireworks such as smoke bombs. It is also added to cigarettes to maintain an even burn of the tobacco and is used to ensure complete combustion of paper cartridges for cap and ball revolvers. It can also be heated to several hundred degrees to be used for niter bluing, which is less durable than other forms of protective oxidation, but allows for specific and often beautiful coloration of steel parts, such as screws, pins, and other small parts of firearms.\n\nIn the process of food preservation, potassium nitrate has been a common ingredient of salted meat since the Middle Ages, but its use has been mostly discontinued because of inconsistent results compared to more modern nitrate and nitrite compounds. Even so, saltpeter is still used in some food applications, such as \"charcuterie\" and the brine used to make corned beef. When used as a food additive in the European Union, the compound is referred to as E252; it is also approved for use as a food additive in the United States and Australia and New Zealand (where it is listed under its INS number 252). Although nitrate salts have been suspected of producing the carcinogen nitrosamine, in the United States both sodium and potassium nitrates and nitrites have been added to meats since 1925, though it is generally accepted that solid muscle products do not require the addition of nitrate for safety reasons.\n\nIn West African cuisine, potassium nitrate (saltpetre) is widely used as a thickening agent in soups and stews such as okra soup and isi ewu. It is also used to soften food and reduce cooking time when boiling beans and tough meat. Saltpetre is also an essential ingredient in making special porridges, such as \"kunun kanwa\" literally translated from the Hausa language as 'saltpetre porridge'.\nIn the Shetland Islands (UK) it is used in the curing of mutton to make \"reestit\" mutton, a local delicacy.\n\nPotassium nitrate is used in fertilizers as a source of nitrogen and potassium – two of the macronutrients for plants. When used by itself, it has an NPK rating of 13-0-44.\n\n\n\nPotassium nitrate was once thought to induce impotence, and is still rumored to be in institutional food (such as military fare) as an anaphrodisiac; however, there is no scientific evidence for such properties.\n\n\n\n"}
{"id": "22036768", "url": "https://en.wikipedia.org/wiki?curid=22036768", "title": "Process duct work", "text": "Process duct work\n\nProcess duct work conveys large volumes of hot, dusty air from processing equipment to mills, baghouses to other process equipment. Process duct work may be round or rectangular. Although round duct work costs more to fabricate than rectangular duct work, it requires fewer stiffeners and is favored in many applications over rectangular ductwork.\n\nThe air in process duct work may be at ambient conditions or may operate at up to . Process ductwork varies in size from 2 ft diameter to 20 ft diameter or to perhaps 20 ft by 40 ft rectangular.\n\nLarge process ductwork may fill with dust, depending on slope, to up to 30% of cross section, which can weigh 2 to 4 tons per linear foot.\n\nRound ductwork is subject to duct suction collapse, and requires stiffeners to minimize this, but is more efficient in material than rectangular duct work.\n\nThere are no comprehensive, design references for process duct work design. The ASCE reference for the design of power plant duct design gives some general guidance on duct design, but does not specifically give designers sufficient information to design process duct work.\n\nStructural process ductwork carries large volumes of high temperature, dusty air, between pieces of process equipment. The design of this ductwork requires an understanding of the interaction of heat softening of metals, potential effects of dust buildup in large ductwork, and structural design principles. There are two basic shapes for structural process ductwork: rectangular and round. Rectangular ductwork is covered by the ASCE \"The Structural Design of Air & Gas Ducts for Process Power Stations and Industrial Applications\".\n\nIn the practical design of primarily round structural process ductwork in the cement, lime and lead industries, the duct size involved ranges from 18 inches (45 cm) to 30 feet (10 m). The air temperature may vary from ambient to 1000 °F (515 °C). Process ductwork is subject to large loads due to dust buildup, fan suction pressure, wind, and earthquake forces. 30 ft diameter process ductwork may cost $7,000 per ton. Failure to properly integrate design forces may lead to catastrophic duct collapse. Overdesign of ductwork is expensive.\n\nThe structural design of ductwork plate is based on buckling of the plate element. Round ductwork plate design is based on diameter to duct plate thickness ratios, and the allowable stresses are contained in multiple references such as \"US Steel Plate\", ASME/ANSI STS-1,SMNACA, \"Tubular Steel Structures\", and other references. In actuality round ductwork bent in bending is approximately 30% stronger than a similar shape in compression, however one uses the same allowable stresses in bending as we do for compression.\n\nRound ducts require typical stiffeners at roughly 3 diameter spacing, or roughly 20 ft. O.C. for wind ovaling and fabrication and truck shipping requirements. Round ducts, larger than in diameter (1/4\" plate) require support ring stiffeners. Smaller-diameter ducts may not require support ring stiffeners, but may be designed with saddle supports. When stiffener rings are required they are traditionally designed based on \"Roark\", although this reference is quite conservative.\n\nRound duct elbow allowable stresses are lower than the allowable stresses for straight duct by a K factor = 1.65/(h 2/3power) where [h = t (duct) * R (elbow) /(r(duct)*r (duct). This equation, or similar equations is found in \"Tubular Steel Structures\" section 9.9.\n\nRectangular ductwork design properties is based on width-to-thickness ratios. This is simplified, normally to width=t/16, from corner elements or corner angle stiffeners, although in reality, the entire duct top & side plate does participate, somewhat in duct section properties.\n\nDuct logic is the process of planning for duct thermal movement, combined with planning to minimize duct dust dropout.\n\nDucts move with changes in internal temperature. Ducts are assumed to have the same temperature as their internal gasses, which may be up to 900 °F. If the internal duct temperature exceeds 1000 °F, refractory lining is used to minimize the duct surface temperature. At 1000 °F, ducts may grow approximately 5/8 inch per 10 feet of length. This movement must be carefully planned for, with cloth (or metal) expansion joints at each equipment flange, and one joint per each straight section of ductwork.\n\nSloping ductwork at or above the duct dust angle of repose will minimize dust buildup. Therefore many ducts carrying high dust loads slope at 30 degrees, or steeper.\n\nTo minimize pressure loss in duct elbows, the typical elbow radius is 1 1/2 times the duct diameter. In cases where this elbow radius is not feasible, turning vanes are added to the duct.\n\nProcess ductwork is often large (6-foot diameter to 18-foot diameter), carrying large volumes of hot dirty gasses, at velocities of 3000 to 4500 feet per minute. The fans used to move these gasses are also large, 250 to 4000 horsepower. Therefor minimizing duct pressure drop by minimizing turbulence at elbows and transitions is of importance. Duct elbow radius is usually 1 1/2 to 2 times the duct size. The side slopes of transitions are typically 10 to 30 degrees.\n\nNote: the duct gas velocity is chosen to minimize duct dust dropout. Cement and lime plant duct velocity at normal operations is 3000 to 3200 foot per minute, lead plant velocities are 4000 to 4500 foot per minute, as the dust is heavier. Other industries, such as grain have lower gas velocities. Higher duct gas velocity may require more powerful fans than lower duct velocities.\n\n\nFor cement plant and Lime plant process ductwork, duct loads are a combination of:\nFor duct sloping 0 degrees to 30 degrees, duct internal dust is 25% of duct cross section. For duct sloping 30 degrees to 45 degrees duct dust loads are reduced to 15% of cross section, plus internal duct coating loads. For ducts sloping 45 degrees to 85 degrees,duct internal dust is 5% of duct cross section, plus internal duct coating loads. For ducts sloping over 85 degrees.\nBecause of the potential for high dust loading, most process ductwork is run at a 30 to 45 degree slope.\n\n2a) Duct dust loading in non-process ducts (2-foot diameter and smaller), such as conveyor venting ducts are sometimes run horizontally and can be filled to 100% of cross section.\n\n2b) Power plant internal duct dust loads are coordinated with the client, and are sometimes used at 1 to 2 foot of internal ash loadings.\n\n3) Duct internal, coating dust loads, which sometimes are used as a 2\" (50 mm) coating of dust on the internal perimeter.\n\n4) Duct suction pressure loads. Most process duct loads have design pressures of 25 inches (600 mm) to 40 inches (1000 mm) of water pressure. This suction pressure operates to cause suction pressure collapse on the duct side walls. Also this pressure operates perpendicular to the duct \"expansion joints\" to create an additional load on the duct supports that adds to dead, and live loads. Please note: duct pressure loads vary with temperature, as the gas density varies with temperature. A duct pressure of 25 inches of H2O, at room temperature may become 12 inches to 6 inches at duct operating pressures.\n\n5) Duct wind loads\n\n6) Duct Seismic loads\n\n7) Duct Snow loads, normally inconsequential, as snow will melt quickly unless the plant is in shutdown mode.\n\n8) Top of duct dust loads, often used as zero, since plant dust generation is much less now, than in the past.\n\n9) Duct suction pressure loads, act perpendicular to end of duct cross section, and can be significant. For a duct designed for 25\" of water at a startup temperature of 70 degree F, on an 8 foot in diameter duct, this is equal to 8000 pounds at each end of the duct.\n\nThe majority of cement plant process ductwork is round. This is because the round duct shape does not bend between circumferential stiffeners. Therefor bending stiffeners are not required, and round ductwork requires fewer and lighter intermediate stiffeners than rectangular ductwork. Round cement plant duct stiffeners are sometimes about 5% duct plate weight. Rectangular cement plant duct stiffeners are 15 to 20% times duct plate weight. Power plant ductwork is often larger. power plant ductwork is usually rectangular, with stiffener weights of 50% (or more) times duct plate weight. (this is based on personal experience, and my vary with loads, duct size, and industry standards)\n\nLarge, round process ductwork is usually fabricated from 1/4-inch (6 mm) mild steel plate, with ovaling stiffening rings at 15 to 20 ft (5 to 6 M) on center, regardless of diameter. These lengths allow for resistance to wind ovaling and resistance to out of round when shipping by truck. This also works well with fabricator equipment.\n\nThe typical intermediate rings are designed for wind bending stresses, reduced as required by the yield stress reduction at working temperatures. The typical rings are fabricated from rolled steel plate, angles or tees's welded together to create the ring cross section required. Rings are fabricated from any combination of plate, tee or W shape that the shop can roll. Rings are usually mild carbon steel, ASTM A36 plate, or equivalent. The location of ring butt welds should preferably be offset 15 degrees(+/-)from point of maximum stress to minimize the effect of weld porosity on weld allowable stress.\n\nSee US Steel Plate, volume II for empirical ring spacing, and wind bending stress:\nSpacing = Ls = 60 sqrt [Do (ft) * t plate (in) /wind pressure (psf)]\nSection = p * L (spacing, ft) * Do (ft) * Do (ft)/Fb (20,000 at ambient T)\nThis reference is older, but a good starting point for duct design.\n\nSMACNA, (2ND Edition) chapter 4 has many useful formulas for round ducts, allowable stresses, ring spacing, effect of dust, ice,and live loads. The basic factor of safety for SMACNA, 3, is larger than typically used on typical structural engineering projects, of 1.6.\nUnder SMACNA the critical ring spacing for rings is L = 1.25 * D (ft) sqrt (D(ft)/t(inches)), which is similar to tubular steel structures, L = 3.13 * R sqrt (R/t). In effect, using Spacing = 60 sqrt [Do (ft) * t plate (in) /wind pressure (psf)] is conservative.\n\nAllowable bending and compression stress in ducts can come from several sources.\n\nSee API 560 for design of wind ovaling stiffeners\n\nSee Tubular Steel structures, chapter 2, 9 & 12 for the allowable stresses for thin, round ducts, their allowable stresses, elbows, elbow softening coefficients, and some procedures for the design of duct support rings. These allowable stresses can be verified with select review of chapters of US Steel Plate, Blodgett Design of plate structures, Roark & Young, or API 650.\n\nRound duct support rings are spaced, often at three diameters, or as require at up to about 50 ft centers (14 m). At this spacing the main support rings are designed for the sum of suction pressure stresses & support bending moments.\n\nRound ductwork allowable compressive stress is = 662 /(d/t) +339 * Fy (tubular steel structures, chapter 2). Other reference use similar equations.\n\nDuctwork typical cement plant pressure drops are: 60% to 80% of high temperature process duct work pressure drop occurs in the process equipment, baghouses, mills and cyclones. Since motor 1 (one) horsepower cost roughly $1000/year (US$) (2005), duct efficiency is important. Minimizing duct pressure drop can reduce plan operating costs. most ductwork, non-equipment pressure drop occurs at transitions and changes of directions (elbows). The bests way to minimize duct pressure drop or to minimize plant operating costs, is to use elbows with an elbow radius to duct radius exceeding 1.5. (For a 15-foot duct, the elbow radius would therefore equal, or exceed, 22.5 ft.)\n\nProcess duct pressure drops (US practice) are usually measured in inches of water. A typical duct operates at about - 25 inches (160 psf.) total suction pressure, with roughly 75% of the pressure loss in the bag house, and 10% of pressure lost in duct friction, and 15% (nominal)lost in elbow turbulence. A major consideration of duct design is to minimize duct pressure losses, turbulence, as poor duct geometry, increases turbulence, and increases plant electrical usage.\n\nRound duct work suction pressure collapse, in ducts over 6 feet in diameter, is prevented with rings at supports, and roughly 3 diameter centers.\n\nRound duct support rings are traditionally designed from the formula's found in Roark & Young. However this reference is based on point loads on rings, while actual duct ring loads are based on almost uniform bottom dust. Therefore these formulars can be shown with Ram, or other analysis methods to have conservatism factor of roughly 2 above the stresses given In Roark. The duct ring force dead, live and dust forces need to be combined with suction pressure stresses. Suction pressure forces concentrate on the rings, as they are the stiffest element present.\n\nRound ductwork elbow allowable stresses are reduced due to the elbow curvature. Various references give similar results for this reduction. Tubular steel structures, Section 9.9 gives the (Beskin) reduction factor of K= 1.65/(h (2/3 power)) where h= t (plate) *R(elbow)/ r (duct) (where suction pressures are smaller). This K reduces the I factor of the duct I effective = I/K.\n\nRound duct rings are fabricated from rolled tees, angles, or plates, welded into the shape required. Typically these are designed with ASTM A-36 properties.\n\nTypical duct round plate factor of safety (traditional factor of safety) should be 1.6, because duct plate bending, and buckling is mostly controlled by typical intermediate ring design.\n\nTypical intermediate ring factor of safety should be 1.6, because there is ample evidence in various codes, (API 360, etc.) that intermediate rings designed for wind ovaling and suction pressure combinations are safe.\n\nTypical main support ring factor of safety, if designed by \"Roark\" formulas should be 1.6, (If constructed to the Roark normal 1% out of round standard tolerance) because it can be shown by various methods that these formulas are at least a factor of two, above three D duct ring analysis results etc..\n\nTypical duct elbow factor of safety should be above 1.6, because it can be difficult to show that shipping out of round for elbows corresponds to the normal 1% out of round standard tolerance. (various code and reference notes).\n\nRound structural tubes are sometimes used to support and contain conveyors transporting coal, lead concentrate, or other dusty material over county roads, plant access roads, or river barge loading facilities. When tubes are used for these purposes they may be 10'-6\" to 12 foot diameter, and up to 250 foot long, using up to 1/2\" plate and ovaling ring stiffeners at 8 foot (to 20 foot centers). On one such project My firm added L8x8x3/4 at the top 45 degree location to stiffen the plate near the point of maximum stress for tubes (as per Timoshenko, and others).\n\nSome vendor provide conveyor galleries for the same purpose.\n\nRectangular cement plant ductwork is often 1/4\" (6 mm) duct plate, with stiffeners spaced at about 2'-6\", depending on suction pressure and temperature. Thinner plate requires a closer stiffener spacing. The stiffeners are usually considered pinned end. Power plant ductwork can be 5/16\" thick duct plate, with \"fixed end\" W stiffeners at roughly 2'-5\" spacing. Because rectangular duct plate bends, stiffeners are required at reasonably close spacing. Duct plate 3/16\", or thinner, may dishpan, or make noise, and should be avoided.\n\nRectangular duct section properties are calculated from the distance between the upper to lower duct corners of the ductwork The flanges areas are based on the size of corner angles plus duct plate width based on the plate thickness ratio of 16*t. (see AISC structural duct design below) For section properties the \"web\" plate is ignored.\n\nThe typical stiffener spacing for cement plant duct work is usually based on duct plate bending M = W * L * L / 8. This is because using a fixed-fixed condition requires difficult to design plate attachments. Power plant, and other larger ductwork, usually goes thru the expense of creating \"fixed End\" corner moment. all stiffeners for rectangular ductwork requires consideration of lateral torsional bracing stiffeners.\n\nDucts are usually designed as if the duct plate and stiffener temperatures match the internal duct gas temperatures. For mild carbon steels (ASTM A36) temperatures, the design yield stress ratio at 300°F is 84% of room temperature stress. At 500°F, the design yield stress ratio is 77% of room temperature stress. At 700°F, the design yield stress ratio is about 71% of room temperature stress. Temperatures above 800°F may cause mild carbon steel to warp. This is because, in this temperature range, the crystal lattice structure of mild carbon steel changes with temperatures above about 800 degrees F (reference, US Steel Plate, elevated temperature steel).\n\nFor ductwork operating above 800 degrees F, duct plate material should resist warping. Either Core-ten or ASTM A304 stainless steel may be used for duct plate between 800°F and 1200°F, Core-ten plate is less expensive than stainless steel.\n\nCorten steels have essentially the same yield stress ratios as Corten through 700°F. At 900°F, the yield stress ratio is 63%. At 1100°F, the yield stress ratio is 58% (AISC tables). Corten steels should not be used above 1100°F.\n\nUnless the duct and its stiffeners are insulated, the stiffeners can be designed in ASTM A36 steels, even at a duct temperature of 1000°F. This is because the stiffener temperature is cooler than the duct gas temperature by several hundred degrees (F). Duct stiffener temperatures are assumed to drop about 100°F per inch of depth (when uninsulated) (no reference available).\n\nAs reducing the loss of heat at plants has changed over the years, ductwork now connects more pieces of equipment than ever before. Care needs to be taken to avoid condensation of moisture in plant ductwork. Once condensation occurs, the condensation may absorb CO2, other components in the gas stream, and become corrosive on low carbon steel. Methods to avoid this problem may include\n\nSulfuric acid attack may require stainless steel ducts, fiberglass ducts, etc.\n\nMany plant exhaust gasses contain dusts with high wear potential. Typically wear resistant steels are not useful in resisting duct wear, particularly at higher temperatures. Wear resistant steel ducts are hard to fabricate, and refractory coatings are usually less expensive than wear resistant steel ductwork. Each industry may have different approaches to resist duct wear.\n\nCement plant clinker dust is more abrasive than sand. In high temperature ducts, or ducts with wear potential, 2 1/2-inch refractory, is often anchored to the duct plate with V anchors at 6\" O.C. (+/-) to resist a) temperature, or b) wear at elbows or a combination of these effects. Occasionally ceramic tiles or ceramic mortars are anchored to ductwork to resist temperature and wear.\n\nGrain plant hulls are also very abrasive. Sometimes plastic liners are used to resist wear in grain facilities, where temperatures are lower than in mineral processing facilities.\n\nDuct segments are typically separated with metal or fabric expansion joints. These joints are designed and detailed for the duct suction pressure, temperatures, and movements between duct segments. Fabric joints are often chosen to separate the duct segments because they usually cost 40% less than metal joints. Also metal joints place an additional loads onto duct segments. Metal joints prefer axial movements, and provide significant lateral loads onto duct segments. fabric joints cost $100 to $200 per square foot of joint (2010). Metal joints can cost twice this amount.\n\nfabric expansion duct forces are assumed to be 0 #/inch. Metal expansion joint forces for metal joints a 24-inch diameter duct are on the order of 850#/ inch of movement for axial spring rate, and 32,500 #/inch for lateral movement. These coefficients will vary with duct size, joint thickness, and becomes larger for rectangular ducts (based on one recent job).\n\nFabric expansion joint life is about 5 years under field conditions. Many plants prefer access platforms near the joints for replacing the joint fabric.\n\nCurrently software is available to model ductwork in 3D. This software needs to be used with care, as the design rules for width to thickness and elbow softening coefficients, etc., may not be input into the design program.\n\nIt is easy to draw ducts in 3D without correct dimensioning. Drawings should be laid out with:\n\n\nSpecial duct loading conditions may occur outside of dead, live, dust and temperature conditions. Ductwork associated with coal mills, coke grinding facilities, and to some extent grain processing facilities, may be subject to explosive dusts. Ductwork designed for explosive dust is typically designed for 50 psi internal pressure, and will typically have one explosion relief one vent per duct section. the likelihood of a dust explosion on an indirect coal mill system is 100%, over time. This can generate a plum of fire 5 ft. to 15 ft. in diameter, and 20 ft. to 30 ft. long. Therefore access to areas surrounding explosion vents shall limit personal access with locked access.\n\nDucts are shipped from fabricating facility to job sites on trucks, by rail, or on barges in lengths accommodating the mode of transport, often in 20 foot sections. These sections are connected with flanges, or weld straps. Flanges are provided at expansion joints, or to join low stress duct sections. Flanges may be difficult to design for the duct plate forces. Flange gaskets add flexibility to the flanges that make their ability to carry forces problematic. Therefore, weld straps (short steel straps) are commonly used for higher stress duct plate connections.\n\nA close look at the fixed duct support photo shows several properties or round ring supports. There are stiffeners at roughly 60 degrees on center. This duct ring is fabricated from two rolled WTs, welded at the center. This is a smaller duct, with light loads, so that the bottom flange was slightly modified by support clearance requirements. A small gap is shown for placing the duct PTFE slide bearing, although a fixed support could also be inserted in this gap. In the background of this photo is a duct flange. The duct flange normally has 3/4\" bolts at 6\" nominal; spacing. Duct flange angle thickness needs to be designed for duct plate tensile stresses, as flanges will bend. 5/16\" or 3/8\" angle thicknesses are common. \nSee above photo of round duct elbows, transitions, and stiffeners. The duct elbow radius is from 1 1/2 to 2 times the duct diameter. The round duct has ovaling, and shipping rings at 20 foot nominal spacing, and larger support rings at supports. The Y split has suction stiffeners at the duct intersection. Note the 3000 HP fan inlet transition and stack inlet transition also shown in this photo. \n\nThe adjacent photo also shows several principles of process ductwork. It shows a large baghouse inlet ductwork. The inlet duct is tapered to minimize dust dropout. A shallow taper such at this also reduces pressure losses when changing duct diameters. Note the rectangular duct ring spacing is roughly 2'-6\" on center. The round duct is stiffened near each branch duct.\n\nThere are several references for process duct work. These references are worked together to review duct design processes. Other references are often used for duct design, but they give similar results. Finite element design of process duct work is possible, but a requirement of design theory and allowable stresses is required to properly interpret the finite element model.\n\n\nCement, lime and lead industry accepted dust loads (for structural loading) are:\nProcess ductwork is intended to convey large volumes of dust. some of this dust will settle to the bottom of the duct during power outages and normal operation.\n\nThe percentage of duct cross section filled with dust is often assumed to be as follows:\n\n\nTo minimize the buildup of dust, each material has a minimum carrying velocity, lime = about 2800 fpm., cement about 3200 fpm, and lead dust about 4200 fpm.\n\nDust density depends on industry, Normally these are: cement dust density = 94 pcf, lime industry = 50 pcf, lead oxide dust = 200 pcf.\n\nDuct Wear: High temperature ductwork often carries large volumes of hot abrasive dust. Often the design temperature of the duct, or the abrasiveness of the dust, prevents the use of abrasive resisting steels. In these cases refractory can be anchored inside the duct, or abrasive resisting tiles, with weld nuts, are welded to the inside of the ductwork.\nDuct Thermal Movement\n\nDuct steels expand with temperature. Each type of steel may have a different coefficient of thermal expansion, typical mild carbon steels expand with the coefficient of 0.0000065 (See AISC).\n"}
{"id": "57893865", "url": "https://en.wikipedia.org/wiki?curid=57893865", "title": "Robotic magnetic navigation", "text": "Robotic magnetic navigation\n\nRobotic magnetic navigation (RMN) (also called remote magnetic navigation) uses robotic technology to direct magnetic fields which control the movement of magnetic-tipped endovascular catheters into and through the chambers of the heart during cardiac catheterization procedures.\n\nBecause the human heart beats during ablation procedures, catheter stability can be affected by navigation technique. Magnetic fields created by RMN technology guide the tip of a catheter using a “pull” mechanism of action (as opposed to “push” with manual catheter navigation). Magnetic catheter navigation has been associated with greater catheter stability.\n\nAs of 2015 there were two robotic catherization systems on the market for atrial fibrilation; one of them used magnetic guidance.\n\nAfter long-term follow up, RMN navigation has been associated with better procedural and clinical outcomes for AF ablation when compared with manual catheter navigation for cardiac ablation.\n\nRMN has been shown to be safe and effective for cardiac catheter ablation in various patient populations with ventricular tachycardia.\n"}
{"id": "2725261", "url": "https://en.wikipedia.org/wiki?curid=2725261", "title": "Roche Diagnostics", "text": "Roche Diagnostics\n\nRoche Diagnostics is a diagnostic division of Hoffmann-La Roche which manufactures equipment and reagents for research and medical diagnostic applications. Internally, it is organized into five major business areas: Roche Applied Science, Roche Professional Diagnostics, Roche Diabetes Care, Roche Molecular Diagnostics and Roche Tissue Diagnostics (Ventana). The main location for Roche Professional Diagnostics is in Rotkreuz, Switzerland.\n\nAll business areas except Roche Applied Science focus on health care applications, targeting either physicians, hospitals and clinics, or consumers. Applied Science targets research settings in academia and pharmaceutical and biotechnology industries.\n\nThe firm was named by \"Fortune\" magazine as one of the top 100 companies to work for in 2013 and 2014, citing that employee amenities \"include an on-site medical clinic and fitness center, a $30,000 budget for intramural sports, and health insurance plans tiered to income levels.\"\n\n"}
{"id": "11172915", "url": "https://en.wikipedia.org/wiki?curid=11172915", "title": "Rotary stage", "text": "Rotary stage\n\nA rotary stage is a component of a motion system used to restrict an object to a single axis of rotation. The terms rotary table or rotation stage are often used interchangeably with rotary stage. All rotary stages consist of a platform and a base, joined by some form of guide in such a way that the platform is restricted to rotation about a single axis with respect to the base. In common usage, the term rotary stage may or may not also include the mechanism by which the angular position of the platform is controlled relative to the base.\n\nIn three-dimensional space, an object may either rotate about, or translate along, any of three axes. Thus, the object is said to have six degrees of freedom (3 rotational and 3 translational). A rotary stage exhibits only one degree of freedom (rotation about one axis). In other words, rotary stages operate by physically restricting 3 axes of translation and 2 axes of rotation.\n\nRotary stages consist of a platform that moves relative to a base. The platform and base are joined by some form of bearing which restricts motion of the platform to rotation about a single axis. A variety of different styles of bearings are used, each with benefits and drawbacks, making them more appropriate for some applications than for others.\n\nA plain bearing is simply two surfaces sliding against each other. Typically, a circular step on the platform mates snugly with a circular depression in the base allowing free rotation while minimizing side to side motion. A rotary stage built with this type of bearing is usually only used for coarse positioning and is adjusted manually simply by turning the platform. Index marks on either the base or the platform are often provided, allowing for somewhat repeatable positioning of the platform relative to the base.\n\nThis type of rotary stage includes ball bearing stages, crossed roller bearing stages, and possibly others. Any of a number of different rolling-element bearings may be employed. Typically, a pair of bearings is used and they are preloaded to take up any slack which could result in the stage platform lifting relative to the base.\n\nSome rotary stages are operated simply by turning the platform by hand. The platform may have index marks for setting different angular positions relative to the base. A locking mechanism may be provided to fix the platform to the base at the desired position.\n\nFor more precise position control, a worm drive may be used. A worm wheel is fixed to the rotating platform and meshes with a worm in the base. Rotation of the worm via a manual control knob causes the platform to rotate with respect to the base. Index marks on both the control knob and the platform can be used to locate the platform very precisely and repeatably with respect to the base.\n\nReplacing the manual control knob in the above manual worm drive scenario a stepper motor allows positioning of the rotary stage to be automated. A stepper motor rotates in fixed increments or steps. The number of steps moved is controlled by the stepper motor controller. In this sense, the stepper motor behaves much like an indexed control knob.\n\nA DC motor may also be used in place of a manual control knob. A DC motor does not move in fixed increments. Therefore, an alternate means is required to determine stage position. An encoder may be attached to the DC motor and used to report the angular position of the motor to the motor controller, allowing a motion controller to reliably and repeatably move the stage to set positions.\n\nWhen precise angular positioning over only a small total angle is required, a linear actuator (either manual, or motorized) may be used. Typically, the range of motion possible is only 10° to 20° of rotation. The linear actuator presses against a contact surface fixed to the stage platform such that extension or retraction of the actuator causes the platform to rotate. The stage platform is sprung against the actuator tip so that the contact surface stays in contact with the actuator tip when the actuator retracts.\n\n"}
{"id": "53910621", "url": "https://en.wikipedia.org/wiki?curid=53910621", "title": "Ryan Disraeli", "text": "Ryan Disraeli\n\nRyan Disraeli is an American digital entrepreneur and the co-founder of TeleSign. Raised in San Diego, California, Disraeli attended the University of Southern California (USC), where he studied business at USC Marshall School of Business. In 2018, Disraeli was awarded the USC Marshall School of Business Alumni Entrepreneur of the year award. \n\nIn 2005, Disraeli, while he was a sophomore at USC, cofounded an online security service company, TeleSign, that protects online websites and users. Disraeli worked as a vice president of the company for fraud services and has helped expand TeleSign's communications platform-as-a-service (CPaaS) to any developer capable of invoking a REST Application Programming Interface (API). Disraeli helped TeleSign raise $78 Million dollars in funding and grow to annual revenues of more than $100 Million and hundreds of employees across the globe. \n\nDisraeli regularly provides commentary in the area of behavioral biometrics and Multi-factor authentication. He also regularly comments on online security and fraud-related issues. In 2017, Disraeli was named to Forbes 30 under 30 for enterprise technology. Also in 2017, TeleSign was acquired for 230 million dollars by Belgacom ICS.\n\n"}
{"id": "41545973", "url": "https://en.wikipedia.org/wiki?curid=41545973", "title": "SOTA Mapping Project", "text": "SOTA Mapping Project\n\nSOTA Mapping Project (SMP) is a website (www.sotamaps.org) offering mapping resources for radio amateurs participating in the Summits On The Air (SOTA) awards program. It aims to provide comprehensive mapping information in graphical form based on Google Maps on summits included in the program, for participants in the program as well as for the general user.\n\nThe site is built and maintained by a small group of radio amateurs with interests in SOTA, hill walking and mountaineering, mapping and open source programming. They also maintain a similar site (www.iotamaps.org) for the Islands on the air (IOTA) award scheme.\n\nThe site has five main mapping pages, each providing different sets of functionality.\n\nThis is the page most often used, since it is referenced directly by links from each of the SOTAWatch Summit pages. In addition to being able to view various SOTA Associations, Regions and individual summits, the page offers the exporting or downloading of such data in GPX (for import into a GPS device) or KML (for importing into Google Earth) formats.\n\nThe Range page provides tools with which the user can perform various distance-measurement tasks. The most often used of these is the \"range\" facility itself, which will find summits within a certain distance of some reference point. The user can input a central location in one of several forms - latitude/longitude, Maidenhead (\"Grid\") Locator, or an address or place-name - and can then choose one of three range-types within which the summits should be found:\n\nAll summits so found will be presented in a list and also on the map. Export of data for the summits found is also provided.\n\nThe Tracks Page allows the user to view, draw, or upload tracks or walking routes leading to SOTA summits. The three main sets of options are:\nExport of tracks data for the summits is also provided.\n\nPresents the latest SOTAWatch Alerts - data are refreshed once every minute. Each Alert as it arrives is presented in the listview on the right-hand side of the main map.\n\nPresents the latest SOTAWatch Spots - data are refreshed once every minute. Each Spot as it arrives is presented in the listview on the right-hand side of the main map. Clicking on an entry in the list will cause the map to navigate to the summit mentioned in the Spot and an information window will open, listing all Spots reported for that summit within the time-frame of the query.\n\nEach of the five mapping pages, in addition to their specific functionality, also feature a set of map tools common to all. These are situated at the top of the map area in each of the mapping pages.\n\n"}
{"id": "304612", "url": "https://en.wikipedia.org/wiki?curid=304612", "title": "Safety valve", "text": "Safety valve\n\nA safety valve is a valve that acts as a fail-safe. An example of safety valve is a pressure relief valve (PRV), which automatically releases a substance from a boiler, pressure vessel, or other system, when the pressure or temperature exceeds preset limits. Pilot-operated relief valves are a specialized type of pressure safety valve. A leak tight, lower cost, single emergency use option would be a rupture disk.\n\nSafety valves were first developed for use on steam boilers during the Industrial Revolution. Early boilers operating without them were prone to explosion unless carefully operated.\n\nVacuum safety valves (or combined pressure/vacuum safety valves) are used to prevent a tank from collapsing while it is being emptied, or when cold rinse water is used after hot CIP (clean-in-place) or SIP (sterilization-in-place) procedures.\nWhen sizing a vacuum safety valve, the calculation method is not defined in any norm, particularly in the hot CIP / cold water scenario, but some manufacturers have developed sizing simulations.\n\nThe earliest and simplest safety valve was used on a 1679 steam digester and utilized a weight to retain the steam pressure (this design is still commonly used on pressure cookers); however, these were easily tampered with or accidentally released. On the Stockton and Darlington Railway, the safety valve tended to go off when the engine hit a bump in the track. A valve less sensitive to sudden accelerations used a spring to contain the steam pressure, but these (based on a Salter spring balance) could still be screwed down to increase the pressure beyond design limits. This dangerous practice was sometimes used to marginally increase the performance of a steam engine. In 1856, John Ramsbottom invented a tamper-proof spring safety valve that became universal on railways. The Ramsbottom valve consisted of two plug-type valves connected to each other by a spring-laden pivoting arm, with one valve element on either side of the pivot. Any adjustment made to one of valves in an attempt to increase its operating pressure would cause the other valve to be lifted off its seat, regardless of how the adjustment was attempted. The pivot point on the arm was not symmetrically between the valves, so any tightening of the spring would cause one of the valves to lift. Only by removing and diassembling the entire valve assembly could its operating pressure be adjusted, making impromptu 'tying down' of the valve by locomotive crews in search of more power impossible. The pivoting arm was commonly extended into a handle shape lead back into the locomotive cab, allowing crews to 'rock' both valves off their seats to confirm they were set and operating correctly.\n\nSafety valves also evolved to protect equipment such as pressure vessels (fired or not) and heat exchangers. The term safety valve should be limited to compressible fluid applications (gas, vapor, or steam).\n\nThe two general types of protection encountered in industry are \"thermal protection\" and \"flow protection\".\n\nFor liquid-packed vessels, thermal relief valves are generally characterized by the relatively small size of the valve necessary to provide protection from excess pressure caused by thermal expansion. In this case a small valve is adequate because most liquids are nearly incompressible, and so a relatively small amount of fluid discharged through the relief valve will produce a substantial reduction in pressure.\n\nFlow protection is characterized by safety valves that are considerably larger than those mounted for thermal protection. They are generally sized for use in situations where significant quantities of gas or high volumes of liquid must be quickly discharged in order to protect the integrity of the vessel or pipeline. This protection can alternatively be achieved by installing a high integrity pressure protection system (HIPPS).\n\nIn the petroleum refining, petrochemical, chemical manufacturing, natural gas processing, power generation, food, drinks, cosmetics and pharmaceuticals industries, the term safety valve is associated with the terms pressure relief valve (PRV), pressure safety valve (PSV) and relief valve.\nThe generic term is Pressure relief valve (PRV) or pressure safety valve (PSV) It should be noted that PRVs and PSVs are not the same thing, despite what many people think; the difference is that PSVs have a manual lever to open the valve in case of emergency.\n\nRV, SV and SRV are spring-operated (even spring-loaded). LPSV and VPSV are spring-operated or weight-loaded.\n\nIn most countries, industries are legally required to protect pressure vessels and other equipment by using relief valves. Also, in most countries, equipment design codes such as those provided by the ASME, API and other organizations like ISO (ISO 4126) must be complied with. These codes include design standards for relief valves and schedules for periodic inspection and testing after valves have been removed by the company engineer.\n\nToday, the food, drinks, cosmetics, pharmaceuticals and fine chemicals industries call for hygienic safety valves, fully drainable and Cleanable-In-Place. Most are made of stainless steel; the hygienic norms are mainly 3A in the USA and EHEDG in Europe.\n\nThe first safety valve was invented by Denis Papin for his steam digester, an early pressure cooker rather than an engine. A weight acting through a lever held down a circular plug valve in the steam vessel. By using a \"steelyard\" lever a smaller weight was required, also the pressure could easily be regulated by sliding the same weight back and forth along the lever arm. Papin retained the same design for his 1707 steam pump. Early safety valves were regarded as one of the engineman's controls and required continuous attention, according to the load on the engine. In a famous early explosion at Greenwich in 1803, one of Trevithick's high-pressure stationary engines exploded when the boy trained to operate the engine left it to catch eels in the river, without first releasing the safety valve from its working load. By 1806, Trevithick was fitting pairs of safety valves, one external valve for the driver's adjustment and one sealed inside the boiler with a fixed weight. This was unadjustable and released at a higher pressure, intended as a guarantee of safety.\n\nWhen used on locomotives, these valves would rattle and leak, releasing near-continuous puffs of waste steam.\n\nAlthough the lever safety valve was convenient, it was too sensitive to the motion of a steam locomotive. Early steam locomotives therefore used a simpler arrangement of weights stacked directly upon the valve. This required a smaller valve area, so as to keep the weight manageable, which sometimes proved inadequate to vent the pressure of an unattended boiler, leading to explosions. An even greater hazard was the ease with which such a valve could be tied down, so as to increase the pressure and thus power of the engine, at further risk of explosion.\n\nAlthough deadweight safety valves had a short lifetime on steam locomotives, they remained in use on stationary boilers for as long as steam power remained.\n\nWeighted valves were sensitive to bouncing from the rough riding of early locomotives. One solution was to use a lightweight spring rather than a weight. This was the invention of Timothy Hackworth on his \"Royal George\" of 1828. Owing to the limited metallurgy of the period, Hackworth's first spring valves used an accordion-like stack of multiple leaf springs.\n\nThese direct-acting spring valves could be adjusted by tightening the nuts retaining the spring. To avoid tampering, they were often shrouded in tall brass casings which also vented the steam away from the locomotive crew.\n\nThe Salter coil spring spring balance for weighing, was first made in Britain by around 1770. This used the newly developed spring steels to make a powerful but compact spring in one piece. Once again by using the lever mechanism, such a spring balance could be applied to the considerable force of a boiler safety valve.\n\nThe spring balance valve also acted as a pressure gauge. This was useful as previous pressure gauges were unwieldy mercury manometers and the Bourdon gauge had yet to be invented.\n\nThe risk of firemen tying down the safety valve remained. This was encouraged by them being fitted with easily adjustable wing nuts, the practice of adjusting the boiler's working pressure via the safety valve being an accepted behaviour well into the 1850s. It was later common with Salter valves for them to be fitted in pairs, one adjustable and often calibrated for use as a gauge, the other sealed inside a locked cover to prevent tampering.\n\nPaired valves were often adjusted to slightly different pressures too, a small valve as a control measure and the lockable valve made larger and permanenently set to a higher pressure, as a safeguard. Some designs, such as one by Sinclair for the Eastern Counties Railway in 1859, had the valve spring with pressure scale behind the dome, facing the cab, and the locked valve ahead of the dome, out of reach of interference.\n\nIn 1855, John Ramsbottom, later locomotive superintendent of the LNWR, described a new form of safety valve intended to improve reliability and especially to be tamper-resistant. A pair of plug valves were used, held down by a common spring-loaded lever between them with a single central spring. This lever was characteristically extended rearwards, often reaching into the cab on early locomotives. Rather than discouraging the use of the spring lever by the fireman, Ramsbottom's valve encouraged this. Rocking the lever freed up the valves alternately and checked that neither was sticking in its seat. Even if the fireman held the lever down and increased the force on the rear valve, there was a corresponding reduction of force on the forward valve.\n\nVarious forms of Ramsbottom valve were produced. Some were separate fittings to the boiler, through separate penetrations. Others were contained in a U-shaped housing fastened to a single opening in the boiler shell. As boiler diameter increased, some forms were even set inside the boiler shell, with the springs housed in a recess inside and only the valves and balance lever protruding outside. These had obvious drawbacks for easy maintenance.\n\nA drawback to the Ramsbottom type was its complexity. Poor maintenance or mis-assembly of the linkage between the spring and the valves could lead to a valve that no longer opened correctly under pressure. The valves could be held against their seats and fail to open or, even worse, to allow the valve to open but insufficiently to vent steam at an adequate rate and so not being an obvious and noticeable fault. Mis-assembly of just this nature led to a fatal boiler explosion in 1909 at Cardiff on the Rhymney Railway, even though the boiler was almost new, at only eight months old.\n\ns were introduced around 1866. A bellcrank arrangement reduced the strain (percentage extension) of the spring, thus maintaining a more constant force. They were used by the L&Y & NER.\n\nAll of the preceding safety valve designs opened gradually and had a tendency to leak a \"feather\" of steam as they approached \"blowing-off\", even though this was below the pressure. When they opened they also did so partially at first and didn't vent steam quickly until the boiler was well over pressure.\nThe quick-opening \"pop\" valve was a solution to this. Their construction was simple: the existing circular plug valve was changed to an inverted \"top hat\" shape, with an enlarged upper diameter. They fitted into a stepped seat of two matching diameters. When closed, the steam pressure acted only on the crown of the top hat, and was balanced by the spring force. Once the valve opened a little, steam could pass the lower seat and began to act on the larger brim. This greater area overwhelmed the spring force and the valve flew completely open with a \"pop\". Escaping steam on this larger diameter also held the valve open until pressure had dropped \"below\" that at which it originally opened, providing hysteresis.\n\nThese valves coincided with a change in firing behaviour. Rather than demonstrating their virility by always showing a feather at the valve, firemen now tried to \"avoid\" noisy blowing off, especially around stations or under the large roof of a major station. This was mostly at the behest of stationmasters, but firemen also realised that any blowing off through a pop valve wasted several pounds of boiler pressure; estimated at 20 psi lost and 16 lbs or more of shovelled coal.\n\nPop valves derived from Adams's patent design of 1873, with an extended lip. R L Ross's valves were patented in 1902 & 1904. They were more popular in America at first, but widespread from 1920s.\nAlthough showy polished brass covers over safety valves had been a feature of steam locomotives since Stephenson's day, the only railway to maintain this tradition into the era of pop valves was the GWR, with their distinctive tapered brass safety valve bonnets and copper-capped chimneys.\n\nDevelopments in high-pressure water-tube boilers for marine use placed more demands on safety valves. Valves of greater capacity were required, to vent safely the high steam-generating capacity of these large boilers. As the force on their valves increased, the issue of the spring's increasing stiffness as its load increased (like the Naylor valve) became more critical. The need to reduced valve feathering became even more important with high-pressure boilers, as this represented both a loss of distilled feedwater and also a scouring of the valve seats, leading to wear.\n\nHigh-lift safety valves are direct-loaded spring types, although the spring does not bear directly on the valve, but on a guide-rod valve stem. The valve is beneath the base of the stem, the spring rests on a flange some height above this. The increased space between the valve itself and the spring seat allows the valve to lift higher, further clear of the seat. This gives a steam flow through the valve equivalent to a valve one and a half or twice as large (depending on detail design).\n\nThe Cockburn Improved High Lift design has similar features to the Ross pop type. The exhaust steam is partially trapped on its way out and acts on the base of the spring seat, increasing the lift force on the valve and holding the valve further open.\n\nTo optimise the flow through a given diameter of valve, the full-bore design is used. This has a servo action, where steam through a narrow control passage is allowed though if it passes a small control valve. This steam is then not exhausted, but is passed to a piston that is used to open the main valve.\n\nThere are safety valves known as PSV's and can be connected to pressure gauges (usually with a 1/2\" BSP fitting). These allow a resistance of pressure to be applied to limit the pressure forced on the gauge tube, resulting in prevention of over pressurisation. the matter that has been injected into the gauge, if over pressurised, will be diverted through a pipe in the safety valve, and shall be driven away from the gauge.\n\nThere is a wide range of safety valves having many different applications and performance criteria in different areas. In addition, national standards are set for many kinds of safety valves.\n\n\n\nSafety valves are required on water heaters, where they prevent disaster in certain configurations in the event that a thermostat should fail. There are still occasional, spectacular failures of older water heaters that lack this equipment. Houses can be leveled by the force of the blast.\n\nPressure cookers are cooking pots with a pressure-proof lid. Cooking at pressure allows the temperature to rise above the normal boiling point of water (100 degrees Celsius at sea level), which speeds up the cooking and makes it more thorough.\n\nPressure cookers usually have two safety valves to prevent explosions. On older designs, one is a nozzle upon which a weight sits. The other is a sealed rubber grommet which is ejected in a controlled explosion if the first valve gets blocked. On newer generation pressure cookers, if the steam vent gets blocked, a safety spring will eject excess pressure and if that fails, the gasket will expand and release excess pressure downwards between the lid and the pan. Also, newer generation pressure cookers have a safety interlock which locks the lid when internal pressure exceeds atmospheric pressure, to prevent accidents from a sudden release of very hot steam, food and liquid, which would happen if the lid were to be removed when the pan is still slightly pressurised inside (however, the lid will be very hard or impossible to open when the pan is still pressurised).\n\nThe term \"safety valve\" is also used metaphorically.\n\n\n"}
{"id": "12465411", "url": "https://en.wikipedia.org/wiki?curid=12465411", "title": "Silicon Gorge", "text": "Silicon Gorge\n\nThe Silicon Gorge refers to the numerous high-tech and research companies, in the triangle of Bristol, Swindon and Gloucester, in England. It is ranked fifth of such areas in Europe, and is named after the Avon Gorge. \n\nBath is home to a number of high-tech companies ranging from fabless semiconductor designers to eCommerce retailers. Many companies have been started by ex-employees of companies such as Future Publishing and IPL, two long standing employers in the area.\n\n\n\nBristol is part of the Silicon Gorge, along with Gloucester and Swindon and hosts a number of high-tech and creative industries including research group HP Labs and animation studio Aardman Animations. The cluster of high-tech electronics industries began when Fairchild Semiconductor located a design office in Bristol in 1972. Bristol also has the strongest digital media supply chain in England, outside London and has been pinpointed as a \"hot spring\" for innovation on the McKinsey/World Economic Forum innovation map.\n\n\n\n\n\n\n\nCities sometimes associated with the region: \n\n\n"}
{"id": "1216801", "url": "https://en.wikipedia.org/wiki?curid=1216801", "title": "Simplified Technical English", "text": "Simplified Technical English\n\nASD-STE100 Simplified Technical English, or Simplified English, is the original name of a controlled language specification originally developed for aerospace industry maintenance manuals. It is a carefully limited and standardized subset of English. It is now officially known under its trademarked name as \"Simplified Technical English\" (STE). STE is regulated for use in the aerospace and defense industries, but other industries have used it as a basis for their own controlled English standards.\n\nFirst attempts towards an STE specification were made as early as the 1930s and 1970s with Basic English and Caterpillar Fundamental English. \n\nIn 1979 aerospace documentation was written in American English (Boeing, Douglas, Lockheed, etc.), in British English (Hawker Siddeley, British Aircraft Corporation, etc.) and by companies whose native language was not English (Fokker, Aeritalia, Aerospatiale, and some of the companies that formed Airbus at the time). There were also European airlines that had to translate parts of their maintenance documentation into other languages for their local mechanics.\n\nThis led the European Airline industry to approach AECMA (European Association of Aerospace Industries) to ask manufacturers to investigate the possibility of using a controlled form of English. In 1983, after an investigation into the different types of controlled languages that existed in other industries, the AECMA decided to produce its own controlled English. The AIA (Aerospace Industries Association of America) was also invited to participate in this development.\n\nThe result of this collaborative work was a guide, known as the AECMA Simplified English Guide. After a merger of AECMA with two other associations to form ASD in 2004, the specification changed its name to become ASD Simplified Technical English, Specification ASD-STE100.\n\nSimplified Technical English is claimed to:\nHowever, these claims come mostly from those who have invested in developing it, implementing it or supporting it. In the absence of third-party endorsement or published scientific studies, such claims should be considered unconfirmed.\nThe Simplified Technical English specification consists of two parts:\n\nThe Writing Rules differentiate between two types of topics: procedure and description. The Writing Rules also specify restrictions on grammar and style usage.\nFor example, they require writers to:\n\n\nThe dictionary includes entries of both approved and unapproved words. The approved words can only be used according to their specified meaning. For example, the word \"close\" can only be used in one of two meanings:\n\n\nThe verb can express \"close a door\" or \"close a circuit\", but cannot be used in other senses (for example \"to close the meeting\" or \"to close a business\"). The adjective \"close\" appears in the Dictionary as an unapproved word with the suggested approved alternative \"near\". So STE does not allow \"do not go close to the landing gear\", but it does allow \"do not go near the landing gear\". In addition to basic STE vocabulary listed in the Dictionary, Section 1, \"Words\", gives explicit guidelines for adding technical names and technical verbs that writers need to describe technical information. For example, words or phrases such as \"overhead panel\", \"grease\", \"propeller\", \"to ream\", and \"to drill\" are not listed in the Dictionary, but qualify as approved terms under the guidelines in Part 1, Section 1 (specifically, Writing Rules 1.5 and 1.10).\n\n\"Simplified English\" is sometimes used as a generic term for a controlled language. The aerospace and defense standard started as an industry-regulated writing standard for aerospace maintenance documentation, but has become mandatory for an increasing number of military land vehicle, sea vehicle and weapons programs as well. Although it was not intended for use as a general writing standard, it has been successfully adopted by other industries and for a wide range of document types. The US government’s Plain English lacks the strict vocabulary restrictions of the aerospace standard, but represents an attempt at a more general writing standard.\n\nThe regulated aerospace standard was formerly called \"AECMA Simplified English\", because AECMA originally created the standard in the 1980s. In 2005, AECMA was subsumed by the Aerospace and Defence Industries Association of Europe (ASD), which renamed its standard to \"ASD Simplified Technical English\" or \"STE\". STE is defined by the specification \"ASD-STE100\", which is maintained by the Simplified Technical English Maintenance Group (STEMG). The specification contains a set of restrictions on the grammar and style of procedural and descriptive text. It also contains a dictionary of approx. 875 approved general words. Writers are given guidelines for adding technical names and technical verbs to their documentation.\n\nSTE is mandated by several commercial and military specifications that control the style and content of maintenance documentation, most notably ASD S1000D.\n\nBoeing has developed a Simplified English Checker to assist during development. The linguistic-based checker uses a sophisticated 350-rule English grammar and parser, which is augmented with special functions that check for violations of the Simplified English standard. \n\nHyperSTE is a plugin tool offered by Etteplan to check content for adherence to the rules and grammar of the specification.\n\nA free copy of the official ASD-STE100 Specification can be downloaded through the ASD-STE100 website. Over 3,600 copies of Issue 6 of the specification were distributed. Issue 7 of the ASD-STE100 specification was released in January 2017. This standard is released every 3 years. The following is an extract from a page of the ASD-STE100 Dictionary:\n\nAn explanation of the four columns:\n\n"}
{"id": "51216978", "url": "https://en.wikipedia.org/wiki?curid=51216978", "title": "Solar-powered waste compacting bin", "text": "Solar-powered waste compacting bin\n\nA solar-powered waste compactor is a smart device that reads a waste bin's fill-level in real-time and triggers an automatic compaction of the waste, effectively increasing the bin's capacity by up to 5-8 times. The compaction mechanism runs on a battery, which is charged by the solar panel. Fully charged, the battery reserve lasts for approximately 3-4 weeks depending on the compaction frequency and usage patterns.\n\nSolar-powered waste compactors are typically connected to a remote software platform through wireless 2G/3G networks. The platform enables waste collection managers to access real-time data analytics and route optimization.\n\nSolar-powered compactors are primarily used in high foot traffic areas such as town centers, shopping malls, amusement parks, beaches, transit stations and sports stadiums.\n\nSome of the benefits of using solar-powered waste compactors include:\n\n\n"}
{"id": "43298828", "url": "https://en.wikipedia.org/wiki?curid=43298828", "title": "Stephen Bernard Dorsey", "text": "Stephen Bernard Dorsey\n\nStephen Bernard Dorsey is a serial entrepreneur who founded several companies, two of which he built into international leaders among manufacturers of word processors. He currently is the CEO of babyTEL, an Internet Telephony Service Provider.\n\nDorsey was born in Montreal, Quebec, Canada on July 10, 1937 and is a Massachusetts Institute of Technology-trained engineer.\n\nHis son Dan Dorsey, as part of his preparation to film a video about his father, made a March 2017 visit to an exhibit at The Personal Computer Museum by curator Syd Bolton that focused on MICOM.\n\nIn 1967, Dorsey founded AES Data Inc. in Montreal, developing industrial remote-control applications. In 1972, AES launched the AES 90 computer, years before Microsoft and Apple were companies. Marketed as a \"word processing system,\" the AES 90 was really the world's first general-purpose personal computer. AES went on to grow to more than $200 million in annual sales. The Word Processor was the precursor to Microsoft Word, Apple and other software based business applications. \n\nDorsey launched his next venture, Micom Co., after selling his stake in AES in 1975, again selling word processing and office automation systems, but adding telecommunications equipment. Having directed Micom to $200 million in revenue, Dorsey sold it to Philips NV in 1984 and founded Voice & Data Systems in 1991. \n\nVoice & Data Systems (VDS) found success in deploying cutting-edge VoIP and fax-over-data network technology to phone companies internationally and in providing unified messaging solutions to some of the world's largest financial firms.\n\nMicom became known for its ads, often run on the back pages of popular-in-their-day computer industry publications, with the slogan \"Concentrate. Because it's cheaper!\" The initial ads showed oranges and what resembled a can of frozen orange juice, with a \"brand name\" of Micom. After adding variations, they began advertising on television. The focus of the ads, within their telecommunications products, was their line of Concentrators.\nTheir print ads included the trademarked phrase \"MICOM: MicroComputers for DataCommunications(tm)\"\n\n"}
{"id": "61867", "url": "https://en.wikipedia.org/wiki?curid=61867", "title": "Trombe wall", "text": "Trombe wall\n\nA Trombe wall is a passive solar building design where a wall is built on the winter sun side of a building with a glass external layer and a high heat capacity internal layer separated by a layer of air. Light close to UV in the electromagnetic spectrum passes through the glass almost unhindered then is absorbed by the wall that then re-radiates in the far infrared spectrum which does not pass back through the glass easily, hence heating the inside of the building. Trombe walls are commonly used to absorb heat during sunlit hours of winter then slowly release the heat over night. The essential idea was first explored by Edward S. Morse and patented by him in 1881. In the 1960s it was fully developed as an architectural element by French engineer Félix Trombe and architect Jacques Michel.\n\nTrombe walls work on the basic greenhouse principle that heat from the sun in the form of near-visible shorter-wavelength higher-energy ultraviolet radiation passes through glass largely unimpeded.\n\nWhen this radiation strikes objects the energy is absorbed and then re-emitted in the form of longer-wavelength infra-red radiation that does not pass through glass as readily. Hence heat becomes trapped and builds up in an enclosed structure with high internal heat capacity and glass surfaces that face the sun\n\nHow effectively objects absorb and shed radiant heat depends on a number of factors; how dark they are in colour, how directly the surface of the object is opposed to the angle of the radiation striking it, how matte or reflective its surface is, the heat capacity of the object, and the surface conductivity of the object.\nFor Trombe walls to work effectively they are made from high heat capacity materials such as concrete or water, whose surface is dark and matte in colour and placed in direct opposition to the sun striking them.\n\nThe clearer the glass in front of a Trombe wall appears in the UV spectrum the more short wave radiation will penetrate and the more reflective or non transparent the glass appears in the infra-red spectrum, the less re-emitted heat will escape.\n\nTrombe walls may be constructed with or without internal vents. Non-vented walls rely on conduction through the wall to heat the space behind the wall, while vented walls allow the user to actively or passively circulate room air past the heated side of the wall for more immediate heating. Vented Trombe walls may use passively or actively controllable flaps to prevent convection in the undesired direction, as when the wall cools at night in winter or heats during the day in summer. In climates that have higher summer temperatures Trombe walls may also be designed with external vents to improve the shedding of heat at night.\nVented walls offer the advantage of being able to shed more heat earlier in the evening when it is more commonly required while higher heat capacity non-vented walls offer the advantage of improved overall diurnal stability. Views differ among the passive solar community as to which is more advantageous.\n\nA simplistic rule of thumb that is often used when designing dense masonry walls is that heat will be absorbed and lost at around two hours per inch.\n\nCommon modifications to the Trombe wall include:\n\nThere is a misconception that Trombe walls must be full height, completely blocking light and direct solar gain into the adjacent living space rather converting it into absorbed radiant heat that is then re-emitted as the room cools. In reality Trombe walls can be built to whatever height suits the needs of the home owner, their reduced height simply reducing the solar absorption area and increasing direct light and heat gain area. Half-height Trombe walls are a relatively simple solution that can greatly enhance the solar storage capacity of a passive solar home, whilst still allowing for views to the sun's winter direction.\n\nHalf-height Trombe walls function in the same way as a full height wall. They are commonly constructed around 4-6 inches (100-150 mm) from the inner window surface, allowing a gap large enough for curtains or blinds to reduce heat loss on winter nights and heat gain on summer days.\n\nIn Ladakh, India, the Ladakh Project is designing Trombe walls that complement Ladakh's traditional architecture and has promoted building them in Ladakhi homes. This has shown Ladhakis a clean, reliable alternative to fire as a source of heat. The traditional fuel, dung, burns poorly and offers poor relief from the bitter winter temperatures. The smoldering dung produces significant amounts of smoke that fouls the air and causes many health problems. Trombe walls offer relief from both the cold and the smoke. Ladakh receives about 320 days of sun annually, and the traditional building materials — stone and mud brick — provide the heat capacity needed for heat storage in a Trombe wall.\n\nThe Druk White Lotus School in Ladakh uses Trombe walls and as part of \"a model of appropriate design and development\".\n\n\n"}
{"id": "6553728", "url": "https://en.wikipedia.org/wiki?curid=6553728", "title": "Trompe", "text": "Trompe\n\nA trompe is a water-powered air compressor, commonly used before the advent of the electric-powered compressor. A trompe is somewhat like an airlift pump working in reverse.\n\nTrompes were used to provide compressed air for bloomery furnaces in Catalonia and the USA. The presence of a trompe is a signature attribute of a Catalan forge, a type of bloomery furnace.\n\nTrompes can be enormous. At Canadian Hydro Developers' Ragged Chute facility in New Liskeard, Ontario, water falls down a shaft deep and across to generate compressed air for mining equipment and ventilation.\n\nTrompes are very simple devices. They consist of four main parts: \"water-supply pipe\" or \"shaft\" with an \"air-inlet\" inside it, \"water outflow pipe\", \"separation chamber\" and \"takeoff\" \"air-pipe\". The vertical pipe or shaft goes down from higher point to a separation chamber, a pipe, that is typically narrower than previous one is coming away from that chamber allows the water to exit at a lower level, and another pipe (air-pipe) coming from the chamber allows the compressed air to exit as needed.\n\nWater rushing down the vertical pipe falls through a constriction.\nThe constriction produces a lower pressure because of the venturi effect, and an external port allows air to be sucked in thus creating a constant air supply. \nThe air forms bubbles in the pipe.\nAs the bubbles go down the pipe they are pressurized proportionally to the hydraulic head, which is the height of the column of water in the pipe. \nThe compressed air rises to the top of the separation chamber.\nThe separation chamber has a compressed-air takeoff pipe, and the compressed air can be used as a power source.\n\nThe energy of the falling water creates negative pressure inside the pipe that is compensated by the air from the outside atmosphere provided through inlet. The air is compressed by surrounding water pressure (which increases under a column due to the discharge to atmospheric pressure). The pressure of the air delivered cannot exceed the hydraulic head of the discharge pipe of the separation chamber. \n\nLarge trompes were often situated at high waterfalls so that ample head was available.\n(However, trompes can raise the water, via siphon-effect, nearly to 70% of its initial elevation.) The Ragged Chute plant on the Montreal River near the town of Cobalt, Ontario, is a trompe and tourist attraction. It is now owned by Canadian Hydro and exists beside a modern hydroelectric plant.\n\nCompressed air from a trompe is at the temperature of the water, and its partial pressure of water vapor is that of the dewpoint of the water's temperature. If the water is cool, the compressed air can be made very dry by passing it through pipes that are warmer than the water. Often, ordinary outside air can warm the pipes enough to produce dry, cool compressed air.\n\nToday, trompes constructed of plastic pipe are being used to provide aeration for mine drainage treatment. In this application, mine water is used to drive the trompe and the compressed air that is generated is used to oxygenate the mine water and to drive off excess dissolved carbon dioxide that may be present thus raising the pH of the water being treated.\n\n\n"}
{"id": "17894942", "url": "https://en.wikipedia.org/wiki?curid=17894942", "title": "Ultra-large-scale systems", "text": "Ultra-large-scale systems\n\nUltra-large-scale system (ULSS) is a term used in fields including Computer Science, Software Engineering and Systems Engineering to refer to software intensive systems with unprecedented amounts of hardware, lines of source code, numbers of users, and volumes of data. The scale of these systems gives rise to many problems: they will be developed and used by many stakeholders across multiple organizations, often with conflicting purposes and needs; they will be constructed from heterogeneous parts with complex dependencies and emergent properties; they will be continuously evolving; and software, hardware and human failures will be the norm, not the exception. The term 'ultra-large-scale system' was introduced by Northrop and others to describe challenges facing the United States Department of Defense. The term has subsequently been used to discuss challenges in many areas, including the computerization of financial markets. The term 'ultra-large-scale system' (ULSS) is sometimes used interchangeably with the term 'large-scale complex IT system' (LSCITS). These two terms were introduced at similar times to describe similar problems, the former being coined in the USA and the latter in the UK.\n\nThe term ultra-large-scale system was introduced in a 2006 report from the Software Engineering Institute at Carnegie Mellon University authored by Linda Northrop and colleagues. The report explained that software intensive systems are reaching unprecedented scales (by measures including lines of code; numbers of users and stakeholders; purposes the system is put to; amounts of data stored, accessed, manipulated, and refined; numbers of connections and interdependencies among components; and numbers of hardware elements). When systems become ultra-large-scale, traditional approaches to engineering and management will no longer be adequate. The report argues that the problem is no longer of engineering systems or system of systems, but of engineering \"socio-technical ecosystems\".\n\nIn 2013, Linda Northrop and her team conducted a talk to review outcome of the 2006 study and the reality of 2013. In summary, the talk concluded that (a) ULS systems are in the midst of society and the changes to current social fabric and institutions are significant; (b) The 2006 original research team was probably too conservative in their report; (c) Recent technologies have exacerbated the pace of scale growth; and (d) There are great opportunities.\n\nAt a similar time to the publication of the report by Northrop and others, a research and training initiative was being initiated in the UK on Large-scale Complex IT Systems. Many of the challenges recognized in this initiative were the same as, or were similar to those recognized as the challenges of ultra-large-scale systems. Greg Goth quotes Dave Cliff, director of the UK initiative as saying \"The ULSS proposal and the LSCITS proposal were written entirely independently, yet we came to very similar conclusions about what needs to be done and about how to do it\". A difference pointed out by Ian Sommerville is that the UK initiative began with a 5 to 10 year vision, while that of Northrop and her co-authors was much longer term. This seems to have led to there being two slightly different perspectives on ultra-large-scale systems. For example, Richard Gabriel's perspective is that ultra-large-scale systems are desirable but currently impossible to build due to limitations in the fields of software design and systems engineering. On the other hand, Ian Sommerville's perspective is that ultra-large-scale systems are already emerging (for example in air traffic control), the key problem being not how to achieve them but how to ensure they are adequately engineered.\n\nUltra-large-scale systems hold the characteristics of systems of systems (systems that have: operationally independent sub-systems; managerially independent components and sub-systems; evolutionary development; emergent behavior; and geographic distribution). But in addition to these, the Northrop report argues that a ULSS will:\n\nThe Northrop report states that \"the sheer scale of ULS systems will change everything. ULS systems will necessarily be decentralized in a variety of ways, developed and used by a wide variety of stakeholders with conflicting needs, evolving continuously, and constructed from heterogeneous parts. People will not just be users of a ULS system; they will be elements of the system. The realities of software and hardware failures will be fundamentally integrated into the design and operation of ULS systems. The acquisition of a ULS system will be simultaneous with its operation and will require new methods for control. In ULS systems, these characteristics will dominate. Consequently, ULS systems will place unprecedented demands on software acquisition, production, deployment, management, documentation, usage, and evolution practices.\"\n\nThe term ultra-large-scale system was introduced by Northrop and others to discuss challenges faced by the United States Department of Defense in engineering software intensive systems. In 2008 Greg Goth wrote that although Northrop’s report focused on the US military’s future requirements, \"its description of how the fundamental principles of software design will change in a global economy … is finding wide appeal\". The term is now used to discuss problems in several domains.\n\nThe Northrop report argued that \"the U.S. Department of Defense (DoD) has a goal of information dominance … this goal depends on increasingly complex systems characterized by thousands of platforms, sensors, decision nodes, weapons, and warfighters connected through heterogeneous wired and wireless networks. … These systems will push far beyond the size of today's systems by every measure … They will be ultra-large-scale systems.\"\n\nFollowing the flash crash, Cliff and Northrop have argued \"The very high degree of interconnectedness in the global markets means that entire trading systems, implemented and managed separately by independent organizations, can rightfully be considered as significant constituent entities in the larger global super-system. … The sheer number of human agents and computer systems connected within the global financial-markets system-of-systems is so large that it is an instance of an ultra-large-scale system, and that largeness-of-scale has significant effects on the nature of the system\".\n\nKevin Sullivan has stated that the US healthcare system is \"clearly an ultra-large-scale system\" and that building national scale cyber-infrastructure for healthcare \"demands not just a rigorous, modern software and systems engineering effort, but an approach at the cutting edge of our understanding of information processing systems and their development and deployment in complex socio-technical environments\".\n\nOther domains said to be seeing the rise of ultra-large-scale systems include government, transport systems (for example air traffic control systems), energy distribution systems (for example smart grids) and large enterprises.\n\nFundamental gaps in our current understanding of software and software development at the scale of ULS systems present profound impediments to the technically and economically effective achievement of significant gains in core system functionality. These gaps are strategic, not tactical. They are unlikely to be addressed adequately by incremental research within established categories. Rather, we require a broad new conception of both the nature of such systems and new ideas for how to develop them. We will need to look at them differently, not just as systems or systems of systems, but as socio-technical ecosystems. We will face fundamental challenges in the design and evolution, orchestration and control, and monitoring and assessment of ULS systems. These challenges require breakthrough research.\n\nThe Northrop report proposed a ULS systems research agenda for an interdisciplinary portfolio of research in at least the following areas:\n\nHuman interaction – People are key participants in ULS systems. Many problems in complex systems today stem from failures at the individual and organizational level. Understanding ULS system behavior will depend on the view that humans are elements of a socially constituted computational process. This research involves anthropologists, sociologists, and social scientists conducting detailed socio-technical analyses of user interactions in the field, with the goal of understanding how to construct and evolve such socio-technical systems effectively.\n\nComputational emergence – ULS systems must satisfy the needs of participants at multiple levels of an organization. These participants will often behave opportunistically to meet their own objectives. Some aspects of ULS systems will be \"programmed\" by properly incentivizing and constraining behavior rather than by explicitly prescribing. This research area explores the use of methods and tools based on economics and game theory (e.g., mechanism design) to ensure globally optimal ULS system behavior by exploiting the strategic self-interests of the system’s constituencies. This research area also includes exploring metaheuristics and digital evolution to augment the cognitive limits of human designers, so they can manage ongoing ULS system adaptation more effectively.\n\nDesign – Current design theory, methods, notations, tools, and practices and the acquisition methods that support them are inadequate to design ULS systems effectively. This research area broadens the traditional technology-centric definition of design to include people and organizations; social, cognitive, and economic considerations; and design structures such as design rules and government policies. It involves research in support of designing ULS systems from all of these points of view and at many levels of abstraction, from the hardware to the software to the people and organizations in which they work.\n\nComputational engineering – New approaches will be required to enable intellectual control at an entirely new level of scope and scale for system analysis, design, and operation.ULS systems will be defined in many languages, each with its own abstractions and semantic structures. This research area focuses on evolving the expressiveness of representations to accommodate this semantic diversity. Because the complexity of ULS systems will challenge human comprehension, this area also focuses on providing automated support for computing the behavior of components and their compositions in systems and for maintaining desired properties as ULS systems evolve.\n\nAdaptive system infrastructure – ULS systems require an infrastructure that permits organizations in distributed locations to work in parallel to develop, select, deploy, and evolve system components. This research area investigates integrated development environments and runtime platforms that support the decentralized nature of ULS systems. This research also focuses on technologies, methods, and theories that will enable ULS systems to be developed in their deployment environments.\n\nAdaptable and predictable system quality – ULS systems will be long-running and must operate robustly in environments fraught with failures, overloads, and attacks. These systems must maintain robustness in the presence of adaptations that are not centrally controlled or authorized.\n\nManaging traditional qualities such as security, performance, reliability, and usability is necessary but not sufficient to meet the challenges of ULS systems. This research area focuses on how to maintain quality in a ULS system in the face of continuous change, ongoing failures, and attacks. It also includes identifying, predicting, and controlling new indicators of system health (akin to the U. S. gross domestic product) that are needed because of the scale of ULS systems.\n\nPolicy, acquisition, and management – Policy and management frameworks for ULS systems must address organizational, technical, and operational policies at all levels. Rules and policies must be developed and automated to enable fast and effective local action while preserving global capabilities. This research area focuses on transforming acquisition policies and processes to accommodate the rapid and continuous evolution of ULS systems by treating suppliers and supply chains as intrinsic and essential components of a ULS system.\n\nThe proposed research does not supplant current, important software research but rather significantly expands its horizons. Moreover, because it is focused on systems of the future, the SEI team purposely avoided couching descriptions in terms of today’s technology. The envisioned outcome of the proposed research is a spectrum of technologies and methods for developing these systems of the future, with national-security, economic, and societal benefits that extend far beyond ULS systems themselves.\n\nThe UK’s research programme in Large-scale Complex IT Systems has been concerned with issues around ULSS development and considers that an LSCITS (Large-scale complex IT system) shares many of the characteristics of a ULSS.\n\n\n"}
{"id": "52456475", "url": "https://en.wikipedia.org/wiki?curid=52456475", "title": "Unity Systems", "text": "Unity Systems\n\nUnity Systems was a home automation company. It released in 1985 the Unity Home Manager which was one of the earliest home automation systems. It featured a green monochrome touchscreen display with options such as temperature settings, floor plans, lighting control, the sprinkler system, HVAC control, security and general maintenance settings. In the 1990s, After over a decade of operation, Unity Systems was shut down.\n\n"}
