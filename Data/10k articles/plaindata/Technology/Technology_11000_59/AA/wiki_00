{"id": "3388149", "url": "https://en.wikipedia.org/wiki?curid=3388149", "title": "Absorption wavemeter", "text": "Absorption wavemeter\n\nAn absorption wavemeter is a simple electronic instrument used to measure the frequency of radio waves. It is an older method of measuring frequency, widely used from the birth of radio in the early 20th century until the 1970s, when the development of inexpensive frequency counters, which have far greater accuracy, made it largely obsolete. A wavemeter consists of an adjustable resonant circuit calibrated in frequency, with a meter or other means to measure the voltage or current in the circuit. When adjusted to resonance with the unknown frequency, the resonant circuit absorbs energy, which is indicated by a dip on the meter. Then the frequency can be read from the dial. \n\nWavemeters are used for frequency measurements that do not require high accuracy, such as checking that a radio transmitter is operating within its correct frequency band, or checking for harmonics in the output. Many radio amateurs keep them as a simple way to check their output frequency. Similar devices can be made for detection of mobile phones. As an alternative, a dip meter can be used.\n\nThere are two categories of wavemeters: \"transmission wavemeters\", which have an input and an output port and are inserted into the signal path, or \"absorption wavemeters\", which are loosely coupled to the radio frequency source and absorb energy from it.\n\nThe most simple form of the device is a variable capacitor with a coil wired across its terminals. Attached to one the terminals of the LC circuit is a diode, then between the end of the diode not wired to the LC circuit and the terminal of the LC circuit not bearing the diode is wired a ceramic decoupling capacitor. Finally a galvanometer is wired to the terminals of the decoupling capacitor.\n\nThe device will be sensitive to strong sources of radiowaves at the frequency at which the LC circuit is resonant.\n\nThis is given by\nformula_1\n\nWhen the device is exposed to an RF field which is at the resonant frequency a DC voltage will appear on the terminals on the left hand side. The coil is often outside the case of the unit so it can be brought close to the object being probed.\n\nAt the higher frequencies it is not possible to use lumped components for the tuned circuit. Instead methods such as stripline or resonant cavities are used. One design for ultra high frequencies (UHF) and super high frequencies (SHF) is a resonant λ/4 (quarter wave) rod which can vary in length. Another design for X-band (10 GHz) is a resonant cavity which can be changed in length.\n\nAs an alternative for UHF, Lecher transmission lines can be used. It is possible to measure roughly the frequency of a transmitter using Lecher lines.\n\n"}
{"id": "233195", "url": "https://en.wikipedia.org/wiki?curid=233195", "title": "Air compressor", "text": "Air compressor\n\nAn air compressor is a device that converts power (using an electric motor, diesel or gasoline engine, etc.) into potential energy stored in pressurized air (i.e., compressed air). By one of several methods, an air compressor forces more and more air into a storage tank, increasing the pressure. When tank pressure reaches its engineered upper limit the air compressor shuts off. The compressed air, then, is held in the tank until called into use. The energy contained in the compressed air can be used for a variety of applications, utilizing the kinetic energy of the air as it is released and the tank depressurizes. When tank pressure reaches its lower limit, the air compressor turns on again and re-pressurizes the tank.\nAn air compressor must be differentiated from pump because it works for any gas/air and pump work on liquid.\n\nCompressors can be classified according to the pressure delivered:\n\nThey can also be classified according to the design and principle of operation:\n\n\nThere are numerous methods of air compression, divided into either positive-displacement or roto-dynamic types.\n\nPositive-displacement compressors work by forcing air in a chamber whose volume is decreased to compress the air. Once the maximum pressure is reached, a port or valve opens and air is discharged into the outlet system from the compression chamber. Common types of positive displacement compressors are\n\nDynamic displacement air compressors include centrifugal compressors and axial compressors. In these types, a rotating component imparts its kinetic energy to the air which is eventually converted into pressure energy. These use centrifugal force generated by a spinning impeller to accelerate and then decelerate captured air, which pressurizes it.\n\nDue to adiabatic heating, air compressors require some method of disposing of waste heat. Generally this is some form of air- or water-cooling, although some (particularly rotary type) compressors may be cooled by oil (that is then in turn air- or water-cooled). The atmospheric changes are also considered during cooling of compressors. The type of cooling is determined by considering the factors such as inlet temperature, ambient temperature, power of the compressor and area of application. There is no single type of compressor that could be used for any application.\n\nAir compressors have many uses, including: supplying high-pressure clean air to fill gas cylinders, supplying moderate-pressure clean air to a submerged surface supplied diver, supplying moderate-pressure clean air for driving some office and school building pneumatic HVAC control system valves, supplying a large amount of moderate-pressure air to power pneumatic tools, such as jackhammers, filling high pressure air tanks (HPA), for filling tires, and to produce large volumes of moderate-pressure air for large-scale industrial processes (such as oxidation for petroleum coking or cement plant bag house purge systems).\n\nMost air compressors either are reciprocating piston type, rotary vane or rotary screw. Centrifugal compressors are common in very large applications, while rotary screw, scroll, and reciprocating air compressors are favored for smaller, portable applications. \n\nThere are two main types of air-compressor pumps: oil-injected and oil-less. The oil-less system has more technical development, but is more expensive, louder and lasts for less time than oil-lubed pumps. The oil-less system also delivers air of better quality.\n\nAir compressors are designed to utilize a variety of power sources. While gas/diesel-powered and electric air compressors are among the most popular, air compressors that utilize vehicle engines, power-take-off, or hydraulic ports are also commonly used in mobile applications.\n\nThe power of a compressor is measured in HP (horsepower) and CFM (cubic feet per minute of intake air).\nThe gallon size of the tank specifies the volume of compressed air (in reserve) available. Gas/diesel powered compressors are widely used in remote areas with problematic access to electricity. They are noisy and require ventilation for exhaust gases. Electric powered compressors are widely used in production, workshops and garages with permanent access to electricity. Common workshop/garage compressors are 110-120 Volt or 230-240 Volt. Compressor tank shapes are: \"pancake\", \"twin tank\", \"horizontal\", and \"vertical\". Depending on a size and purpose compressors can be stationary or portable.\n\nTo ensure all compressor types run efficiently with no leaks, it is imperative to perform routine maintenance, such as monitoring and replacing air compressor fittings. It is suggested that air compressor owners perform daily inspections of their equipment, such as:\n\n"}
{"id": "22128220", "url": "https://en.wikipedia.org/wiki?curid=22128220", "title": "Baffle (heat transfer)", "text": "Baffle (heat transfer)\n\nBaffles are flow-directing or obstructing vanes or panels used in some industrial process vessels (tanks), such as shell and tube heat exchangers, chemical reactors, and static mixers. Baffles are an integral part of the shell and tube heat exchanger design. A baffle is designed to support tube bundles and direct the flow of fluids for maximum efficiency. Baffle design and tolerances for heat exchangers are discussed in the standards of the Tubular Exchanger Manufacturers Association (TEMA).\n\nThe main roles of a baffle in a shell and tube heat exchanger are to:\n\nIn a static mixer, baffles are used to minimize tangential component of velocity which cause vortex formation thus promote mixing. \n\nIn a chemical reactor, baffles are often attached to the interior walls to promote mixing and thus increase heat transfer and possibly chemical reaction rates.\n\nImplementation of baffles is decided on the basis of size, cost and their ability to lend support to the tube bundles and direct \n\nAs mentioned, baffles deal with the concern of support and fluid direction in heat exchangers. In this way it is vital that they are spaced correctly at installation. The minimum baffle spacing is the greater of 50.8 mm or one fifth of the inner shell diameter. The maximum baffle spacing is dependent on material and size of tubes. The Tubular Exchanger Manufacturers Association sets out guidelines. There are also segments with a \"no tubes in window\" design that affects the acceptable spacing within the design. An important design consideration is that no recirculation zones or dead spots form – both of which are counterproductive to effective heat transfer.\n\n"}
{"id": "568963", "url": "https://en.wikipedia.org/wiki?curid=568963", "title": "Basement", "text": "Basement\n\nA basement or cellar is one or more floors of a building that are either completely or partially below the ground floor. They are generally used as a utility space for a building where such items as the boiler, water heater, breaker panel or fuse box, car park, and air-conditioning system are located; so also are amenities such as the electrical distribution system, and cable television distribution point. However, in cities with high property prices such as London, basements are often fitted out to a high standard and used as living space.\n\nIn British English, the word \"basement\" is used for underground floors of, for example, department stores, but the word is only used with houses when the space below their ground floor is habitable, with windows and (usually) its own access. The word \"cellar\" or \"cellars\" is used to apply to the whole underground level or to any large underground room. A \"subcellar\" is a cellar that lies further underneath.\n\nA basement can be used in almost exactly the same manner as an additional above-ground floor of a house or other building. However, the use of basements depends largely on factors specific to a particular geographical area such as climate, soil, seismic activity, building technology, and real estate economics.\n\nBasements in small buildings such as single-family detached houses are rare in wet climates such as Great Britain and Ireland where flooding can be a problem, though they may be used on larger structures. However, basements are considered standard on all but the smallest new buildings in many places with temperate continental climates such as the American Midwest and the Canadian Prairies where a concrete foundation below the frost line is needed in any case, to prevent a building from shifting during the freeze-thaw cycle. Basements are much easier to construct in areas with relatively soft soils and may be foregone in places where the soil is too compact for easy excavation. Their use may be restricted in earthquake zones, because of the possibility of the upper floors collapsing into the basement; on the other hand, they may be required in tornado-prone areas as a shelter against violent winds. Adding a basement can also reduce heating and cooling costs as it is a form of earth sheltering, and a way to reduce a building's surface area-to-volume ratio. The housing density of an area may also influence whether or not a basement is considered necessary.\n\nHistorically, basements have become much easier to build (in developed countries) since the industrialization of home building. Large powered excavation machines such as backhoes and front-end loaders have reduced the time and manpower needed to dig a basement dramatically as compared to digging by hand with a spade, although this method may still be used in the developing world.\n\nFor most of its early history, the basement took one of two forms. It could be little more than a cellar, or it could be a section of a building containing rooms and spaces similar to those of the rest of the structure, as in the case of basement flats and basement offices.\n\nHowever, beginning with the development of large, mid-priced suburban homes in the 1950s, the basement, as a space in its own right, gradually took hold. Initially, it was typically a large, concrete-floored space, accessed by indoor stairs, with exposed columns and beams along the walls and ceilings, or sometimes, walls of poured concrete or concrete cinder block.\n\nA daylight basement is contained in a house where at least part of the floor goes above ground to provide reasonably-sized windows. Generally, the floor's ceiling should be enough above ground to provide nearly full-size windows. Some daylight basements are located on slopes, such that one portion of the floor is at-grade with the land. A walk-out basement almost always results from this.\n\nMost daylight basements naturally result from raised bungalows and at-grade walk-out basements. However, there are instances where the terrain dips enough from one side to another to allow for 3/4 to full-size windows, with the actual floor remaining below grade.\n\nIn most parts of North America, it is legal to set up apartments and legal bedrooms in daylight basements, whether or not the entire basement is above grade.\n\nDaylight basements can be used for several purposes—as a garage, as maintenance rooms, or as living space. The buried portion is often used for storage, laundry room, hot water tanks, and HVAC.\n\nDaylight basement homes typically appraise higher than standard-basement homes, since they include more viable living spaces. In some parts of the US, however, the appraisal for daylight basement space is half that of ground and above ground level square footage. Designs accommodated include split-foyer and split-level homes. Garages on both levels are sometimes possible. As with any multilevel home, there are savings on roofing and foundations.\n\nA walk-out basement is any basement that is partially underground but nonetheless allows egress directly outdoors and has floating walls. This can either be through a stairwell leading above ground, or a door directly outside if a portion of the basement is completely at or above grade.\n\nMany walk-out basements are also daylight basements. The only exceptions are when the entire basement is nearly entirely underground, and a stair well leads up nearly a floors worth of vertical height to lead to the outdoors. \n\nGenerally, basements with only an emergency exit well do not count as walk-out. Walk-out basements with at-grade doors on one side typically are worth a lot more, but are more costly to construct since the foundation is still constructed to reach below the frost line. At-grade walk-out basements are on the door-side often used as livable space for the house, with the buried portion used for utilities and storage.\n\nA subbasement is a floor below the basement floor. In the homes where there is any type of basement mentioned above such as a look-out basement, all of the volume of subbasements from floor to ceiling are located well below ground. Therefore, subbasements have no windows nor an outside door. In the homes that have subbasements, all of basement can be used as part of main home where people relax and do recreational things while all of subbasement can be used for storage.\n\nAccording to the international \"Oxford Dictionary of English\", a finished fully underground cellar is a room below ground level in a house that is often used for the storage of wine or coal; it may also refer to the stock of wine itself. A cellar is intended to remain at a constant cool (not freezing) temperature all year round and usually has either a small window/opening or some form of air ventilation (air/draught bricks, etc.) in order to help eliminate damp or stale air. Cellars are more common in the UK in older houses, with most terraced housing built during late 19th and early 20th centuries having cellars. These were important shelters from air raids during World War II. In parts of North America that are prone to tornadoes (e.g. Tornado Alley), cellars still serve as shelter in the event of a direct hit on the house from a tornado or other storm damage caused by strong winds.\n\nExcept for Britain, Australia and New Zealand, cellars are popular in most western countries. In the UK, almost all new homes built since the 1960s have no cellar or basement due to the extra cost of digging down further into the sub-soil and a requirement for much deeper foundations and waterproof tanking. The reverse has recently become common, where the impact of smaller home-footprints has led to roof-space being utilised for further living space and now many new homes are built with third-floor living accommodation. For this reason, especially where lofts have been converted into living space, people tend to use garages for the storage of food freezers, tools, bicycles, garden and outdoor equipment. The majority of continental European houses have cellars, although a large proportion of people live in apartments or flats rather than houses. In North America, cellars usually are found in rural or older homes on the coasts and in the South. However, full basements are commonplace in new houses in the Canadian and American Midwest and other areas subject to tornado activity or requiring foundations below the frost line.\n\nAn underground crawl space (as the name implies) is a type of basement in which one cannot stand up—the height may be as little as one foot (30 cm), and the surface is often soil. Crawl spaces offer a convenient access to pipes, substructures and a variety of other areas that may be difficult or expensive to access otherwise. While a crawl space cannot be used as living space, it can be used as storage, often for infrequently used items. Care must be taken in doing so, however, as water from the damp ground, water vapour (entering from crawl space vents), and moisture seeping through porous concrete can create a perfect environment for mould/mildew to form on any surface in the crawl space, especially cardboard boxes, wood floors and surfaces, drywall and some types of insulation.\n\nHealth and safety issues must be considered when installing a crawl space. As air warms in a home, it rises and leaves through the upper regions of the house, much in the same way that air moves through a chimney. This phenomenon, called the \"stack effect\", causes the home to suck air up from the crawl space into the main area of the home. Mould spores, decomposition odours, and fecal material from dust mites in the crawl space can come up with the air, aggravating asthma and other breathing problems, and creating a variety of health concerns.\n\nIt is usually desirable to finish a crawl space with a plastic vapour barrier that will not support mould growth or allow humidity from the earth into the crawl space. This helps insulate the crawl space and discourages the habitation of insects and vermin by breaking the ecological chain in which insects feed off the mould and vermin feed on the insects, as well as creating a physical inorganic barrier that deters entrance into the space. Vapour barriers can end at the wall or be run up the wall and fastened to provide even more protection against moisture infiltration. Some pest control agencies recommend against covering the walls, as it complicates their job of inspection and spraying. Almost unheard of as late as the 1990s, vapour barriers are becoming increasingly popular in recent years. In fact, the more general area of conditioned vs. unconditioned crawl spaces has seen much research over the last decade.\n\nAlternatively, some find it desirable to create a \"breathing home\" with ample air flow, rather than \"finish\" a crawl space. There are contrary opinions as to what is healthier with many suggesting that vapor barriers simply create a new space where mould and mildew can flourish, trapping moisture below it and still creating a problem inside the home.\n\nStructurally, for houses, the basement walls typically form the foundation. In warmer climates, some houses do not have basements because they are not necessary (although many still prefer them). In colder climates, the foundation must be below the frost line. Unless constructed in very cold climates, the frost line is not so deep as to justify an entire level below the ground, although it is usually deep enough that a basement is the assumed standard. In places with oddly stratified soil substrata or high water tables, such as most of Texas, Oklahoma, Arkansas, and areas within of the Gulf of Mexico, basements are usually not financially feasible unless the building is a large apartment or commercial structure.\n\nExcavation using a backhoe or excavator is commonly used to dig a basement. If shelf rock is discovered, the need for blasting may be cost prohibitive. Basement walls may need to have the surrounding earth backfilled around them to return the soil to grade. A water stop, some gravel and a french drain may need to be used to prevent water from entering the basement at the bottom of the wall. Walls below grade may need to be sealed with an impervious coating (such as tar) to prevent water seepage. A polyethylene of about 6 mil (visqueen) serves as a water barrier underneath the basement.\n\nSome designs elect to simply leave a crawl space under the house, rather than a full basement due to structural challenges. Most other designs justify further excavations to create a full-height basement, sufficient for another level of living space. Even so, basements in Canada and the northern United States were typically only in height, rather than the standard full of the main floors. Older homes may have even lower basement heights as the basement walls were concrete block and thus, could be customized to any height. Modern builders offer higher basements as an option. The cost of the additional depth of excavation is usually quite expensive. Thus, houses almost certainly never have multi-storey basements though basements heights are a frequent choice among new home buyers. For large office or apartment buildings in prime locations, the cost of land may justify multi-storey basement parking garages.\n\nThe concrete floor in most basements is structurally not part of the foundation; only the basement walls are. If there are posts supporting a main floor beam to form a post and beam system, these posts typically go right through the basement floor to a footing underneath the basement floor. It is the footing that supports the post and the footing is part of the house foundation. Load-bearing wood-stud walls rest directly on the concrete floor. Under the concrete floor is typically gravel or crushed stone to facilitate draining. The floor is typically four inches (100 mm) thick and it rests on top of the foundation footings. The floor is typically sloped towards a drain point, in case of leaks.\n\nModern construction for basement walls typically falls into one of two categories: they will be made of poured-in-place concrete using concrete forms with a concrete pump, or they will use concrete masonry units (block walls). Rock may also be used, but is less common. In monolithic architecture, large parts of the building are made of concrete; in insulating concrete form construction, the concrete walls may be hidden with an exterior finish or siding. Inside the structure, a single Lally column, steel basement jack, wooden column or support post may hold up the floor above in a small basement. A series of these supports may be necessary for large basements; many basements have the support columns exposed.\n\nSince warm air rises, basements are typically cooler than the rest of the house. In summer, this makes basements damp, due to the higher relative humidity. Dehumidifiers are recommended. In winter, additional heating, such as a fireplace or baseboard heaters may be required. A well-defined central heating system may minimize this requirement. Heating ducts typically run in the ceiling of the basement (since there is not an empty floor below to run the ducts). Ducts extending from the ceiling down to the floor help heat the cold floors of the basement. Older or cheaper systems may simply have the heating vent in the ceiling of the basement.\n\nThe finished floor is typically raised off the concrete basement floor. In countries such as Canada, laminate flooring is an exception: It is typically separated from the concrete by only a thin foam underlay. Radiant heating systems may be embedded within the concrete floor. Even if unfinished and unoccupied, basements are heated in order to ensure relative warmth of the floor above, and to prevent water supply pipes, drains, etc. from freezing and bursting in winter. It is recommended that the basement walls be insulated to the frost line. In Canada, the walls of a finished basement are typically insulated to the floor with vapor barriers to prevent moisture transmission. However, a finished basement should avoid wood or wood-laminate flooring, and metal framing and other moisture resistant products should be used. Finished basements can be costly to maintain due to deterioration of waterproofing materials or lateral earth movement etc. Below-ground structures will never be as dry as one above ground, and measures must be taken to circulate air and dehumidify the area.\n\nBasement floor drains that connect to sanitary sewers need to be filled regularly to prevent the trap from drying out and sewer gas from escaping into the basement. The drain trap can be topped up automatically by the condensation from air conditioners or high-efficiency furnaces. A small tube from another downpipe is sometimes used to keep the trap from drying out. Health Canada advocates the use of special radon gas traps for floor drains that lead to soil or to a sealed sump pump. In areas where storm and sanitary sewers are combined, and there is the risk of flooding and sewage backing up, backwater valves in all basement drains may be mandated by code and definitely are recommended even if not mandated.\n\nThe main water cut-off valve is usually in the basement. Basements often have \"clean outs\" for the sanitary and storm sewers, where these pipes can be accessed. The storm sewer access is only needed where the weeping tiles drain into the storm sewers.\n\nOther than with walk-out or look-out basements, windows in basements require a well and are below grade. A clear window well cover may be required to keep the window wells from accumulating rain water. There should be drains in the window well, connected to the foundation drains.\nIf the water table outside the basement is above the height of the basement floor, then the foundation drains or the weeping tiles outside the footings may be insufficient to keep the basement dry. A sump pump may be required. It can be located anywhere and is simply in a well that is deeper than the basement floor.\n\nEven with functioning sump pumps or low water tables, basements may become wet after rainfall, due to improper drainage. The ground next to the basement must be graded such that water flows away from the basement wall. Downspouts from roof gutters should drain freely into the storm sewer or directed away from the house. Downspouts should not be connected to the foundation draintiles. If the draintiles become clogged by leaves or debris from the rain gutters, the roof water would cause basement flooding through the draintile. Damp-proofing or waterproofing materials are typically applied to outside of the basement wall. It is virtually impossible to make a concrete wall waterproof, over the long run, so drainage is the key. There are draining membranes that can be applied to the outside of the basement that create channels for water against the basement wall to flow to the foundation drains.\n\nWhere drainage is inadequate, waterproofing may be needed. There are numerous ways to waterproof a basement, but most systems fall into one of three categories:\n\n\nThe waterproofing system can be applied to the inside or the outside walls of a basement. When waterproofing existing basements it is much cheaper to waterproof the basement on the inside. Waterproofing on the outside requires the expense of excavation, but does offer a number of advantages for a homeowner over the long term. Among them are:\n\nThe unfinished design, found principally in spaces larger than the traditional cellar, is common in residences throughout the U.S. and Canada. One usually finds within it a water heater, various pipes running along the ceiling and downwards to the floor, and sometimes a workbench, a freezer or refrigerator, or a washer/dryer set. Boxes of various materials, and objects unneeded in the rest of the house, are also often stored there; in this regard, the unfinished basement takes the place both of the cellar and of the attic. Home workshops are often located in the basement, since sawdust, metal chips, and other mess or noise are less of a nuisance there. The basement can contain all of these objects and still be considered to be \"unfinished\", as they are either mostly or entirely functional in purpose.\n\nIn this case the space has been designed, either during construction or at a later point by the owners, to function as a fully habitable addition to the house. Frequently most or all of the basement is used as a recreation room or living room, but it is not uncommon as well to find there (either instead of or alongside the living/recreation room) a guest bedroom or teenager's room, a bathroom, a home office, a home gym, a home theater, a basement bar, a sauna, and one or more closets. Occasionally a part of the basement is unfurnished and is used for storage, a workshop, and/or a laundry room; when this is the case the water heater and furnace will also often be located there, although in some cases the entire basement is finished, and the water heater and furnace are boxed off into a closet.\n\nThe main point of distinction between this type of basement and the two others lies in its being either entirely unmodified (unlike the finished basement) beyond the addition of furniture, recreational objects and appliances, and/or exercise equipment on the bare floor, or slightly modified through the installation (besides any or all of the aforementioned items) of loose carpet and perhaps simple light fixtures. In both cases, the objects found there—many of which could be found in a finished basement as well—might include the following: weight sets and other exercise equipment; the boom boxes or entertainment systems used during exercise; musical instruments (which are not in storage, as they would technically be in an unfinished basement; an assembled drum set would be the most easily identified of these); football tables, chairs, couches and entertainment appliances of lesser quality than those in the rest of the house; refrigerators, stand-alone freezers, and microwaves (the first and the second being also sometimes used as supplementary storage units in an unfinished basement); and sports pennants and/or other types of posters which are attached to the walls.\n\nAs the description suggests, this type of basement, which also might be called \"half-finished\", is likely used by teenagers and children. The entire family might utilize a work-out area. It is also common to have a secondary (or primary) home office in a partially finished basement, as well as a workbench and/or a space for laundry appliances.\n\nToilets and showers sometimes exist in this variety of basement, as many North American basements are designed to allow for their installation.\n\nIn London the construction of finished retrofit basements is big business with a large number of projects in the 100–200 square meter bracket. There are a smaller number of projects in the 200–500 square meter bracket under construction. It is also not unusual to see multi-level retrofit basements. These are considerable works of civil engineering and require some skill and intuitive understanding as well as good engineering. Some of the more grandiose of these basement projects have been widely reported in the national media, notably the \"Witanhurst\" project in the Highgate area of London. and the huge iceberg-like homes which are beginning to be constructed in prime London areas such as Kensington and Chelsea.\n\nHospitals often place their nuclear chemistry and radiation therapy and diagnostic resources in basements to utilize the shielding from the earth.\n\nIn Canada, historically the basement area was excluded from advertised square footage of a house as it was not part of the living space. For example, a \"2,000-square-foot bungalow\" would, in reality, have of floor space. More recently, finished space has become increasingly acceptable as a measure which includes the developed basement areas of a home. Due to fire code requirements, most jurisdictions require an emergency egress (through either egress-style windows, or, in the case of a walk-out basement, a door) to include the basement square footage as living space.\n\n\n"}
{"id": "23582948", "url": "https://en.wikipedia.org/wiki?curid=23582948", "title": "Brake to Vacate", "text": "Brake to Vacate\n\nBrake to Vacate (BTV) is additional software planned by Airbus for incorporation on its line of airliners, intended to reduce runway overruns. A more tangible benefit is the increased ability to exit the runway at a specified turnoff point. The European Aviation Safety Agency certified the system, initially for use on the Airbus A380, in 2009. The second Airbus product to incorporate BTV will be the Airbus A320 family, which is much more widely used around the world than the A380. However, an A320 BTV system would be more modest, since its flight computer does not incorporate the extensive electronic architecture of the A380.\n\nThe BTV is a subset of the airliner's auto-flight computer, and allows that computer to be programmed for a pre-selected stopping distance. It indicates (and to some extent directs) which combination of brakes and thrust reversers are required to achieve that distance.\n\nThe BTV uses the airliner's existing warning systems to alert the crew if unsafe conditions exist. If the system computes that the runway is too short when wet, an amber message appears in the primary flight display. If it computes that the runway is too short even under dry surface conditions, \"RWY TOO SHORT\" (in red letters) is flashed on the primary flight display, accompanied by an aural signal.\n\nWhen the aircraft landing gear is firmly on the runway surface, the BTV combines audio and visual prompts to the flight crew in order to achieve the calculated required deceleration to achieve the designated turnoff point. If BTV senses that the aircraft will overrun the runway end, it automatically applies maximum wheel braking, and it sends an aural message (to the flight crew) to apply maximum reverse thrust, along with a red message in the primary flight display. It continues to call out \"keep max reverse\" until the computer figures the desired turnoff point can be achieved at a safe turnoff speed. If the taxiway departs the runway at 90 degrees, the BTV automatically disconnects when the aircraft groundspeed reduces to 10 knots. If the taxiway is a rapid-exit taxiway, the BTV automatically disconnects at 40 to 50 knots groundspeed (depending on conditions).\n\nProponents of the system point out that using BTV will reduce wear on brakes and tires (estimated at 20% reduction over present wear rates), less time that the aircraft spends within the active area of the runway, and enhanced ability to predict required cool-down time on the brakes (and thus to better control minimum gate turnaround time).\n\nThe greatest potential advantage of the system is its ability to predict whether an aircraft will be able to stop safely on a specified runway. The pilot selects the runway and enters the reported surface conditions (wind direction, windspeed, wet or dry, cleared or slushy), and the computer uses stored runway information, computed aircraft weights and required approach speeds, and computed flare/touchdown characteristics to predict whether the stopping point will lie outside the runway's endpoint.\n\nThe flight crew inputs the selected runway, and the reported runway surface conditions. The flight computer updates its calculations using predicted speed and wind conditions until the airplane is 500 feet (150 m) above the surface, after which it uses actual speed and wind inputs.\n\nThe BTV concept was born in a 1998 Ph.D. thesis by French engineer Fabrice Villaumé, who became head of Airbus' BTV program and who holds patents on the process. Between 2002 and 2006 the computer routines were worked out, and the process was first tested on an Airbus A340,\n\nAirbus initially announced its plan to make the BTV system available by 2007, but the production bottlenecks that Airbus encountered in meeting the initial A380 delivery schedules apparently pushed back development of several such planned improvements. As of 2009 the company is seeking certification initially on the A380, after which it will announce planned incorporation schedules for other aircraft in the Airbus airliner product line. In 2006 Airbus indicated that following its introduction on the A380, the BTV \"[would] be followed by retrofits available on all of other Airbus aircraft families.\" However, as of 2009 the company is indicating only that it will work on incorporating the feature on new A320s, and no definite availability schedule has been announced.\n\nIn 2005 Airbus also announced its intention to incorporate BTV into the upcoming Airbus A350. However, since then that project has suffered several redesign phases, and it is not clear whether the BTV will still be offered when the A350 does come to market.\n\nAs of 2018, the system is indeed incorporated in the Airbus A350.\n"}
{"id": "75885", "url": "https://en.wikipedia.org/wiki?curid=75885", "title": "Content management system", "text": "Content management system\n\nA content management system (CMS) manages the creation and modification of digital content. It typically supports multiple users in a collaborative environment.\n\nCMS features vary widely. Most CMSs include Web-based publishing, format management, history editing and version control, indexing, search, and retrieval. By their nature, content management systems support the separation of content and presentation.\n\nA web content management system (WCM or WCMS) is a CMS designed to support the management of the content of Web pages. Most popular CMSs are also WCMSs. Web content includes text and embedded graphics, photos, video, audio, maps, and program code (e.g., for applications) that displays content or interacts with the user.\n\nSuch a content management system (CMS) typically has two major components:\nDigital asset management systems are another type of CMS. They manage content with clearly defined author or ownership, such as documents, movies, pictures, phone numbers, and scientific data. Companies also use CMSs to store, control, revise, and publish documentation.\n\nBased on market share statistics, the most popular content management system is WordPress, used by over 28% of all websites on the internet, and by 59% of all websites using a known content management system, followed by Joomla and Drupal.\n\nContent management systems typically provide the following features:\n\n\n\n\n"}
{"id": "1093599", "url": "https://en.wikipedia.org/wiki?curid=1093599", "title": "Derailment", "text": "Derailment\n\nA derailment occurs when a vehicle such as a train runs off its rails. This does not necessarily mean that it leaves its track. Although many derailments are minor, all result in temporary disruption of the proper operation of the railway system, and they are potentially seriously hazardous to human health and safety. Usually, the derailment of a train can be caused by a collision with another object, an operational error, the mechanical failure of tracks, such as broken rails, or the mechanical failure of the wheels. In emergency situations, deliberate derailment with derails or catch points is sometimes used to prevent a more serious accident.\n\nRailroad wrecks in the 19th century were sensational, and the newspapers claimed them to be due to human failure or the consequence of corporate greed. It took railroads several decades to improve train-control practices and adopt safety devices sufficient to make railroad travel truly safe. Very few passengers were killed in train wrecks in the US before 1853. The early trains ran slowly and made short trips, night travel was rare, and there were not many of them in operation. While trains were convenient for travel and for transporting goods, they had become a greater danger over the years as their speed had increased. While fatal railway accidents occurred about once a year previously, there was a sudden 800 percent increase in accidents in 1853. Some railroad accidents were caused by human error, but other causes included derailment, explosions on board, equipment failures, and bridge collapses. Thereafter, the rate of accidents returned to its former level.\n\nBoiler explosions had been noted in locomotive-type fire tube boilers when the top of the firebox (called the crown sheet) failed. This had to be covered with a significant layer of water at all times or the heat of the fire would weaken it to the point of failure, even at normal working pressures. Low water levels in the boiler when traversing a significant grade could expose parts of the crown sheet. Even a well-maintained firebox could fail explosively if the water level in the boiler was allowed to fall far enough to leave the top plate of the firebox uncovered. Due to the constant expansion and contraction of the firebox a form of \"stress corrosion\" could also take place at the ends of the firebox plates. This corrosion was accelerated by poor water quality and the build-up of boiler scale. A fuel explosion within the confines of the firebox (actually the ignition of unburned gases caused by an inappropriate air/fuel mixture) could also damage the pressurized boiler tubes and interior shell, potentially triggering a structural failure. The majority of locomotive explosions were found to be related to these circumstances, and constant attention to the engine was found to be the best defense against catastrophe.\n\nOn 6 January 1853, the Boston & Maine express, traveling from Boston to Lawrence, MA, derailed at forty miles an hour when an axle broke, and the single coach went down an embankment breaking in two. Only one person was killed, the eleven-year-old son of President-elect Franklin Pierce, who was also on board but was only badly bruised. A few days later, on 23 January 1853, at Glen Rock, PA, the conductor B.A. Stells was lost after the caboose was detached from its train cars in a forest during a blizzard. The body of the man and the car were not found until Spring. On 6 May 1853, a New Haven Railroad train ran through an open drawbridge at Norwalk, CT and plunged into the Norwalk River. Forty-six passengers were crushed to death or drowned. This was the first major railroad drawbridge accident. \n\nOn 17 July 1856, at Fort Washington, PA, there was one of the most infamous train wrecks to ever occur in the US and the deadliest in the world up to that time. Known as the Great Train Wreck, two North Pennsylvania Railroad trains, one of them carrying 1,500 Sunday School children to a picnic, collided. Upon impact, the boiler of the passenger train exploded and the train carrying the children derailed. Fifty-nine people were instantly killed, and dozens more died from their injuries. The conductor of the passenger train committed suicide the same day, although he was later absolved of any responsibility. \n\nFinally in this sampling, on 11 May 1858, at Utica, NY, two New York Central trains, a westbound freight and the eastbound Cincinnati Express, passed on parallel tracks on a forty-foot wooden trestle over Sauquoit Creek. It collapsed under their combined weight, utterly destroying the passenger train, killing nine and injuring 55 persons.\n\nDuring the 19th century derailments were commonplace, but progressively improved safety measures have resulted in a stable lower level of such incidents. In the US, derailments have dropped dramatically since 1980 from over 3,000 annually (1980) to 1,000 or so in 1986, to about 500 in 2010.\n\nDerailments result from one or more of a number of distinct causes; these may be classified as:\n\n\nA traditional track structure consists of two rails, fixed at a designated distance apart (known as the track gauge), and supported on transverse sleepers (ties). Some advanced track structures support the rails on a concrete or asphalt slab. The running surface of the rails is required to be practically continuous and of the proper geometrical layout.\n\nIn the event of a broken or cracked rail, the rail running surface may be disrupted if a piece has fallen out, or become lodged in an incorrect location, or if a large gap between the remaining rail sections arises. 170 broken (not cracked) rails were reported on Network Rail in the UK in 2008, down from a peak of 988 in 1998/1999.\n\n\nDerailment may take place due to excessive gauge widening (sometimes known as road spread), in which the sleepers or other fastenings fail to maintain the proper gauge. In lightly engineered track where rails are spiked (dogged) to timber sleepers, spike hold failure may result in rotation outwards of a rail, usually under the aggravating action of crabbing of bogies (trucks) on curves.\n\nThe mechanism of gauge widening is usually gradual and relatively slow, but if it is undetected, the final failure often takes place under the effect of some additional factor, such as excess speed, poorly maintained running gear on a vehicle, misalignment of rails, and extreme traction effects (such as high propelling forces). The crabbing effect referred to above is more marked in dry conditions, when the coefficient of friction at the wheel to rail interface is high.\n\nThe running gear — wheelsets, bogies (trucks), and suspension — may fail. The most common historical failure mode is collapse of plain bearings due to deficient lubrication, and failure of leaf springs; wheel tyres are also prone to failure due to metallurgical crack propagation.\n\nModern technologies have reduced the incidence of these failures considerably, both by design (specially the elimination of plain bearings) and intervention (non-destructive testing in service).\n\nIf a vertical, lateral, or crosslevel irregularity is cyclic and takes place at a wavelength corresponding to the natural frequency of certain vehicles traversing the route section, there is a risk of resonant harmonic oscillation in the vehicles, leading to extreme improper movement and possibly derailment. This is most hazardous when a cyclic roll is set up by crosslevel variations, but vertical cyclical errors also can result in vehicles lifting off the track; this is especially the case when the vehicles are in the tare (empty) condition, and if the suspension is not designed to have appropriate characteristics. The last condition applies if the suspension springing has a stiffness optimised for the loaded condition, or for a compromise loading condition, so that it is too stiff in the tare situation.\n\nThe vehicle wheelsets become momentarily unloaded vertically so that the guidance required from the flanges or wheel tread contact is inadequate.\n\nA special case is heat related buckling: in hot weather the rail steel expands. This is managed by stressing continuously welded rails (they are tensioned mechanically to be stress neutral at a moderate temperature) and by providing proper expansion gaps at joints and ensuring that fishplates are properly lubricated. In addition, lateral restraint is provided by an adequate ballast shoulder. If any of these measures are inadequate, the track may buckle; a large lateral distortion takes place, which trains are unable to negotiate. (In nine years 2000/1 to 2008/9 there were 429 track buckle incidents in Great Britain).\n\nJunctions and other changes of routing on railways are generally made by means of points (switches — movable sections capable of changing the onward route of vehicles). In the early days of railways these were moved independently by local staff. Accidents — usually collisions — took place when staff forgot which route the points were set for, or overlooked the approach of a train on a conflicting route. If the points were not correctly set for either route — set in mid-stroke — it is possible for a train passing to derail.\n\nThe first concentration of levers for signals and points brought together for operation was at Bricklayer's Arms Junction in south-east London in the period 1843-1844. The signal control location (forerunner of the signalbox) was enhanced by the provision of interlocking (preventing a clear signal being set for a route that was not available) in 1856.\n\nTo prevent the unintended movement of freight vehicles from sidings to running lines, and other analogous improper movements, trap points and derails are provided at the exit from the sidings. In some cases these are provided at the convergence of running lines. It occasionally happens that a driver incorrectly believes s/he has authority to proceed over the trap points, or that the signaller improperly gives such permission; this results in derailment. The resulting derailment does not always fully protect the other line: a trap point derailment at speed may well result in considerable damage and obstruction, and even a single vehicle may obstruct the clear line.\n\nIf a train collides with a massive object, it is clear that derailment of the proper running of vehicle wheels on the track may take place. Although very large obstructions are imagined, it has been known for a cow straying on to the line to derail a passenger train at speed such as occurred in the Polmont rail accident.\n\nThe most common obstructions encountered are road vehicles at level crossings (grade crossings); malicious persons sometimes place materials on the rails, and in some cases relatively small objects cause a derailment by guiding one wheel over the rail (rather than by gross collision).\n\nDerailment has also been brought about in situations of war or other conflict, such as during hostility by Native Americans, and more especially during periods when military personnel and materiel was being moved by rail.\n\nThe handling of a train can also cause derailments. The vehicles of a train are connected by couplings; in the early days of railways these were short lengths of chain (\"loose couplings\") that connected adjacent vehicles with considerable slack. Even with later improvements there may be a considerable slack between the traction situation (power unit pulling the couplings tight), and power unit braking (locomotive applying brakes and compressing buffers throughout the train). This results in coupling surge.\n\nMore sophisticated technologies in use nowadays generally employ couplings that have no loose slack, although there is elastic movement at the couplings; continuous braking is provided, so that every vehicle on the train has brakes controlled by the driver. Generally this uses compressed air as a control medium, and there is a measurable time lag as the signal (to apply or release brakes) propagates along the train.\n\nIf a train driver applies the train brakes suddenly and severely, the front part of the train is subject to braking forces first. (Where only the locomotive has braking, this effect is obviously more extreme). The rear part of the train may overrun the front part, and in cases where coupling condition is imperfect, the resultant sudden closing up (an effect referred to as a \"run-in\") may result in a vehicle in tare condition (an empty freight vehicle) being lifted momentarily, and leaving the track.\n\nThis effect was relatively common in the nineteenth century.\n\nOn curved sections, the longitudinal (traction or braking) forces between vehicles have a component inward or outward respectively on the curve. In extreme situations these lateral forces may be enough to encourage derailment.\n\nA special case of train handling problems is overspeed on sharp curves. This generally arises when a driver fails to slow the train for a sharp curved section in a route that otherwise has higher speed conditions. In the extreme this results in the train entering a curve at a speed at which it cannot negotiate the curve, and gross derailment takes place. It takes roughly 6.976 × 10 N to completely derail a train traveling at such circumstances. The specific mechanism of this may involve bodily tipping (rotation) but is likely to involve disruption of the track structure and derailment as the primary failure event, followed by overturning.\n\nAn example of speeding on a curve would be the May 2015 Philadelphia train derailment involving an Amtrak train traveling at 106 mph (171 km/h), twice the maximum allowable speed of 50 mph (80 km/h).\n\nThe guidance system of practical railway vehicles relies on the steering effect of the conicity of the wheel treads on moderate curves (down to a radius of about 500 m, or about 1,500 feet). On sharper curves flange contact takes place, and the guiding effect of the flange relies on a vertical force (the vehicle weight).\n\nA flange climbing derailment can result if the relationship between these forces, L/V, is excessive. The lateral force L results not only from centrifugal effects, but a large component is from the crabbing of a wheelset which has a non-zero angle of attack during running with flange contact. The L/V excess can result from wheel unloading, or from improper rail or wheel tread profiles. The physics of this is more fully described below, in the section \"wheel-rail interaction\".\n\nWheel unloading can be caused by twist in the track. This can arise if the cant (crosslevel, or superelevation) of the track varies considerably over the wheelbase of a vehicle, and the vehicle suspension is very stiff in torsion. In the quasi-static situation it may arise in extreme cases of poor load distribution, or on extreme cant at low speed.\n\nIf a rail has been subject to extreme sidewear, or a wheel flange has been worn to an improper angle, it is possible for the L/V ratio to exceed the value that the flange angle can resist.\n\nIf weld repair of side-worn switches is undertaken, it is possible for poor workmanship to produce a ramp in the profile in the facing direction, that deflects an approaching wheel flange on to the rail head.\n\nIn extreme situations, the infrastructure may be grossly distorted or even absent; this may arise from earthworks movement (embankment slips and washouts), earthquake and other major terrestrial disruption, deficient protection during work processes etc.\n\nNearly all practical railway systems use wheels fixed to a common axle: the wheels on both sides rotate in unison. Tramcars requiring low floor levels are the exception, but much benefit in vehicle guidance is lost by having unlinked wheels.\n\nThe benefit of linked wheels derives from the conicity of the wheel treads—the wheel treads are not cylindrical, but conical. On idealised straight track, a wheelset would run centrally, midway between the rails.\n\nThe example shown here uses a right-curving section of track. The focus is on the left-side wheel, which is more involved with the forces critical to guiding the railcar through the curve.\n\nDiagram 1 below shows the wheel and rail with the wheelset running straight and central on the track. The wheelset is running away from the observer. (Note that the rail is shown inclined inwards; this is done on modern track to match the rail head profile to the wheel tread profile.)\n\nDiagram 2 shows the wheelset displaced to the left, due to curvature of the track or a geometrical irregularity. The left wheel (shown here) is now running on a slightly larger diameter; the right wheel opposite has moved to the left as well, towards the centre of the track, and is running on a slightly smaller diameter. As the two wheels rotate at the same rate, the forward speed of the left wheel is a little faster than the forward speed of the right wheel. This causes the wheelset to curve to the right, correcting the displacement. This takes place without flange contact; the wheelsets steer themselves on moderate curves without any flange contact.\n\nThe sharper the curve, the greater the lateral displacement necessary to achieve the curving. On a very sharp curve (typically less than about 500 m or 1,500 feet radius) the width of the wheel tread is not enough to achieve the necessary steering effect, and the wheel flange contacts the face of the high rail.\n\nDiagram 3 shows the running of wheelsets in a bogie or a four-wheeled vehicle. The wheelset is not running parallel to the track: it is constrained by the bogie frame and suspension, and it is yawing to the outside of the curve; that is, its natural rolling direction would lead along a less sharply curved path than the actual curve of the track.\n\nThe angle between the natural path and the actual path is called the angle of attack (or the yaw angle). As the wheelset rolls forward, it is forced to slide across the railhead by the flange contact. The whole wheelset is forced to do this, so the wheel on the low rail is also forced to slide across its rail.\n\nThis sliding requires a considerable force to make it happen, and the friction force resisting the sliding is designated \"L\", the lateral force. The wheelset applies a force L outwards to the rails, and the rails apply a force L inwards to the wheels. Note that this is quite independent of \"centrifugal force\". However at higher speeds the centrifugal force is added to the friction force to make L.\n\nThe load (vertical force) on the outer wheel is designated V, so that in Diagram 4 the two forces L and V are shown.\n\nThe steel-to-steel contact has a coefficient of friction that may be as high as 0.5 in dry conditions, so that the lateral force may be up to 0.5 of the vertical wheel load.\n\nDuring this flange contact, the wheel on the high rail is experiencing the lateral force L, towards the outside of the curve. As the wheel rotates, the flange tends to climb up the flange angle. It is held down by the vertical load on the wheel V, so that if L/V exceeds the trigonometrical tangent of the flange contact angle, climbing will take place. The wheel flange will climb to the rail head where there is no lateral resistance in rolling movement, and a flange climbing derailment usually takes place. In Diagram 5 the flange contact angle is quite steep, and flange climbing is unlikely. However, if the rail head is side-worn (side-cut) or the flange is worn, as shown in Diagram 6 the contact angle is much flatter and flange climbing is more likely.\n\nOnce the wheel flange has completely climbed onto the rail head, there is no lateral restraint, and the wheelset is likely to follow the yaw angle, resulting in the wheel dropping outside the rail. An L/V ratio greater than 0.6 is considered to be hazardous.\n\nIt is emphasised that this is a much simplified description of the physics; complicating factors are creep, actual wheel and rail profiles, dynamic effects, stiffness of longitudinal restraint at axleboxes, and the lateral component of longitudinal (traction and braking) forces.\n\nFollowing a derailment, it is naturally necessary to replace the vehicle on the track. If there is no significant track damage that may be all that is needed. However, when trains in normal running derail at speed, a considerable length of track may be damaged or destroyed; far worse secondary damage may be caused if a bridge is encountered.\n\nWith simple wagon derailments where the final position is close to the proper track location, it is usually possible to pull the derailed wheelsets back on to the track using rerailing ramps; these are metal blocks designed to fit over the rails and to provide a rising path back to the track. A locomotive is usually used to pull the wagon.\n\nIf the derailed vehicle is further from the track, or its configuration (such as a high centre of gravity or a very short wheelbase) make the use of ramps impossible, jacks may be used. In its crudest form, the process involves lifting the vehicle frame and then allowing it to fall off the jack towards the track. This may need to be repeated.\n\nA more sophisticated process involves a controlled process using slewing jacks in addition. Photographs of early locomotives often indicate one or more jacks carried on the frame of the locomotive for the purpose, presumed to be a frequent occurrence.\n\nWhen more complex rerailing work is needed, various combinations of cable and pulley systems may be used, or the use of one or more rail-borne cranes to lift a locomotive bodily. In special cases road cranes are used, as these have greater lifting and reach capacity, if road access to the site is feasible.\n\nIn extreme circumstances, a derailed vehicle in an awkward location may be scrapped and cut up on site, or simply abandoned as non-salvageable.\n\n\"Note: there is a large list of railway accidents in general at Lists of rail accidents.\"\n\nIn the Hatfield rail crash in England in 2000, which killed four people, rolling contact fatigue had resulted in multiple gauge corner cracking in the surface; 300 such cracks were subsequently found at the site. The rail cracked under a high speed passenger train, which derailed.\n\nIn the earlier Hither Green rail crash, a triangular segment of rail at a joint became displaced, and lodged in the joint; it derailed a passenger train and 49 persons died. Poor maintenance on an intensively operated section of route was the cause.\n\nIn the Eschede train disaster in Germany, a high speed passenger train derailed in 1998, killing 101 people. The primary cause was the fracture from metal fatigue of a wheel tyre; the train failed to negotiate two sets of points and struck the pier of an overbridge. It was the most serious railway accident in Germany, and also the most serious on any high speed (over ) line. Ultrasonic testing had failed to reveal the incipient fracture.\n\nIn 1967 in the UK there were four derailments due to buckling of continuously welded track (\"cwr\"): at Lichfield on 10 June, an empty carflat train (a train of flat cars for transporting automobiles); on 13 June an express passenger train was derailed at Somerton; on 15 July a freightliner train (container train) was derailed at Lamington; and on 23 July an express passenger train was derailed at Sandy. The official report was not entirely conclusive as to the causes, but it observed that the annual total of buckling distortions was 48 in 1969, having been in single figures in every previous year, and that [heat-related] distortions per 1,000 miles per annum were 10.42 for cwr and 2.98 for jointed track in 1969, having been a maximum of 1.78 and 1.21 in the previous ten years. 90% of the distortions could be attributed to one of the following:\n\nIn the Connington South rail crash on 5 March 1967 in England, a signaller moved the points immediately in front of an approaching train. Mechanical signalling was in force at the location, and it was believed that he improperly replaced the signal protecting the points to danger just as the locomotive passed it. This released the locking on the points and he moved them to lead to a loop line with a low speed restriction. The train, travelling at , was unable to negotiate the points in that position and five persons died.\n\nA passenger train was derailed in the Polmont rail accident in the UK in 1984 upon hitting a cow at speed; the train formation had the locomotive at the rear (propelling) with a light driving-trailer vehicle leading. The cow had strayed on to the line from adjacent agricultural land, due to deficient fencing. 13 persons died in the resulting derailment. However this was thought to be the first occurrence from this cause (in the UK) since 1948.\n\nThe Salisbury rail crash took place on 1 July 1906; a first class only special boat train from Stonehousepool, Plymouth England, ran through Salisbury station at about ; there was a sharp curve of ten chains (660 feet, 200 m) radius and a speed restriction to . The locomotive overturned bodily and struck the vehicles of a milk train on the adjacent line. 28 people were killed. The driver was sober and normally reliable, but had not driven a non-stopping train through Salisbury before.\n\nThere have been several other derailments in the UK due to trains entering speed-restricted sections of track at excessive speed; the causes have generally been inattention by the driver due to alcohol, fatigue or other causes. Prominent cases were the Nuneaton rail crash in 1975 (temporary speed restriction in force due to trackwork, warning sign illumination failed), the Morpeth accident in 1984 (express passenger sleeping car train took restricted sharp curve at full speed; alcohol a factor; no fatalities due to the improved crashworthiness of the vehicles)\n\n"}
{"id": "16110500", "url": "https://en.wikipedia.org/wiki?curid=16110500", "title": "Direct carbon fuel cell", "text": "Direct carbon fuel cell\n\nA Direct Carbon Fuel Cell (DCFC) is a fuel cell that uses a carbon rich material as a fuel such as bio-mass or coal. The cell produces energy by combining carbon and oxygen, which releases carbon dioxide as a by-product. It also called coal fuel cells (CFCs), carbon-air fuel cells (CAFCs), direct carbon/coal fuel cells (DCFCs), and DC-SOFC.\n\nThe total reaction of the cell is C + O → CO. \nThe process in half cell notation:\n\nDespite this release of carbon dioxide, the direct carbon fuel cell is more environmentally friendly than traditional carbon burning techniques. Due to its higher efficiency, it requires less carbon to produce the same amount of energy. Also, because pure carbon dioxide is emitted, carbon capture techniques are much cheaper than for conventional power stations. Utilized carbon can be in the form of coal, coke, char, or a non-fossilized source of carbon.\n\nAt least four types of DCFC exist:\n\nOverall reaction in the solid oxide electrolyte based DCFC\n\nAnode reaction\n\n<Direct electrochemical oxidation path>\n\nC + 2O → CO + 4e\n\nC + O → CO+ 2e\n\n<Indirect electrochemical oxidation path>\n\nCO + O → CO + 2e\n\n<Boudouard reaction:indirect chemical reaction path>\n\nC + CO → 2CO\n\nCathode reaction\n\nO + 4e → 2O\n\n\n\n"}
{"id": "3666937", "url": "https://en.wikipedia.org/wiki?curid=3666937", "title": "First flush", "text": "First flush\n\nFirst flush is the initial surface runoff of a rainstorm. During this phase, water pollution entering storm drains in areas with high proportions of impervious surfaces is typically more concentrated compared to the remainder of the storm. Consequently, these high concentrations of urban runoff result in high levels of pollutants discharged from storm sewers to surface waters.\n\nThe term \"first flush effect\" refers to rapid changes in water quality (pollutant concentration or load) that occur after early season rains. Soil and vegetation particles wash into streams; sediments and other accumulated organic particles on the river bed are re-suspended, and dissolved substances from soil and shallow groundwater can be flushed into streams. Recent research has shown that this effect has not been observed in relatively pervious areas.\n\nThe term is often also used to address the first flood after a dry period, which is supposed to contain higher concentrations than a subsequent one. This is referred to as \"first flush flood.\" There are various definitions of the first flush phenomenon.\n\nStorm water runoff in a combined sewer produces a first foul flush with a suspension of accumulated sanitary solids from the sewer in addition to pollutants from surface runoff. Inflow may produce a foul flush effect in sanitary sewers if flows peak during wet weather. As flow rates increase above average, a relatively small percentage of the total flow contains a disproportionately large percentage of the total pollutant mass associated with overall flow volume through the peak flow event. Sewer solids deposition during low flow periods and subsequent resuspension during peak flow events is the major pollutant source for the first-flush combined-sewer overflow (CSO) phenomenon.\n\nSanitary sewage solids can either go through the system or settle out in laminar flow portions of the sewer to be available for washout during peak flows. The wetted perimeter of sewers may also be colonized by biofilm nourished by soluble sanitary wastes. Hydraulic design is the underlying reason for solids deposition in sewers. Combined sewers sized for peak runoff events expected once a decade can carry up to 1,000 times the average sanitary flow. Less dramatically oversized sewers are common in new developments and near the upstream end of collection systems. Suspended solids may accumulate when low-flow fluid velocities generate insufficient turbulence. Solids deposition is greatest where velocities are low during dry weather. In large combined sewers it may be impossible to attain sanitary sewage velocities generating sufficient turbulence to keep solids suspended during dry weather.\n\nBiofilm and previously deposited solids may be scoured and re-entrained during peak flow turbulence. The high pollution load in wastewater at the beginning of a runoff event occurs when increased flow rate erodes accumulated sewer sediment. Erosion of sediments in sewers can release pollutants in concentrations exceeding levels found in contributing sources. The initial highly polluting foul flush is released at the start of wet weather flow during speedy erosion of a weak layer of highly concentrated surficial sediment bed-load. When conditions favor dry-weather solids deposition, the first foul flush may contain as much as 30 percent of the annual total suspended solids discharged to a combined sewer system. Combined sewer suspended solids concentrations of several thousand milligrams per liter (mg/L) may be observed during the first foul flush.\n\nPollutant concentration levels are influenced by the age and condition of the collection system and the amount of infiltration/inflow in comparison to the sanitary flow. Pollutant concentration peaks depend on size and slope of the piping system, time interval between storms, and solids accumulation in the collection system. Steeper sewer gradients and pipe bottom shapes that maintain high velocity flow during low-flow conditions will reduce sediment accumulation in sewers; and periodic sewer flushing of individual lines during dry weather may move accumulated solids to the wastewater treatment plant before stormwater runoff causes simultaneous peak flow in the entire collection system.\n\nBecause the reference of the first flush is not always clear, the terms \"concentration-based first flush\" (CBFF) and \"mass-based first flush\" (MBFF) have been introduced.\n\nApart from this definition, there are a number of rating parameters in literature to determine the occurrence of a first flush.\n\nIn the context of rainwater harvesting, a first flush diverter is a simple device that is designed to protect a storage cistern from contamination by first flush runoff. This leads to a higher quality of water captured, and less silting of the cistern over time in dusty areas. The diverted first flush water is used for irrigation or other purposes in a fashion similar to greywater. Although many commercial versions are available, these devices are frequently constructed of spare pipe when the cistern is initially installed or thereafter. See \"Texas Manual on Rainwater Harvesting\" for calculations on sizing.\n\n\n"}
{"id": "57077104", "url": "https://en.wikipedia.org/wiki?curid=57077104", "title": "Flatiron School", "text": "Flatiron School\n\nFlatiron School is an education organization founded in 2012 by Adam Enbar and Avi Flombaum. The organization is based in New York City and teaches software engineering, computer programming, and data science. Until late 2017, Flatiron School's courses were taught at its campus in Manhattan or online. Since being acquired by WeWork in October 2017, the school has expanded to five different cities in two countries, having opened campuses in Washington, D.C., Brooklyn, London, and Houston.\n\nFlatiron School was founded in 2012 by Adam Enbar and Avi Flombaum. It received two rounds of funding in 2014 and 2015.\n\nIn 2017, the New York State Attorney fined Flatiron $375,000 after it was found to be operating without a licence and making misleading statements about the employment status and earning potential of graduates. They claimed a 98.5% employment rate but this included apprentices and freelance workers, while the claimed average salary of $74,447 only included graduates in full time employment.\n\nFlatiron School was acquired by WeWork, a collaborative workspace company, in October 2017.\n"}
{"id": "11107908", "url": "https://en.wikipedia.org/wiki?curid=11107908", "title": "Giant Campus", "text": "Giant Campus\n\nGiant Campus, Inc. is a defunct Seattle-based online education company. \n\nGiant Campus specializied in technology-focused courses for students, grades 9 through 12, as well as for members of the United States military and their families. Giant Campus was geared towards developing original course content, mapping to both the Washington Career and Technical Education (WA CTE) and Texas Essential Knowledge and Skills (TEKS) frameworks in business innovation, computer science, and digital arts.\nGiant Campus was founded in 1997, by president and CEO, Pete Findley. The idea came when Findley was a business school student at the University of Washington, and he created a detailed business plan for a network of summer camps that would teach middle and high school kids new media skills—including multimedia and web design on college campuses. This project earned him first place in a statewide business plan competition, along with $15,000 in seed money to launch Giant Campus. Within five years, Giant Campus was running Cybercamps at more than 70 colleges, including programs in China, Hong Kong, Korea and Saudi Arabia.\n\nIn 2008, Findley was named Ernst & Young Entrepreneur of the Year in the Pacific Northwest.\n"}
{"id": "3925786", "url": "https://en.wikipedia.org/wiki?curid=3925786", "title": "Helikon vortex separation process", "text": "Helikon vortex separation process\n\nThe Helikon vortex separation process is an aerodynamic uranium enrichment process designed around a device called a vortex tube. Paul Dirac thought of the idea for isotope separation and tried creating such a device in 1934 in the lab of Peter Kapitza at Cambridge. Other methods of separation were more practical at that time, but this method was designed and used in South Africa for producing reactor fuel with a uranium-235 content of around 3–5%, and 80–93% enriched uranium for use in nuclear weapons. The Uranium Enrichment Corporation of South Africa, Ltd. (UCOR) developed the process, operating a facility at Pelindaba (known as the 'Y' plant) to produce hundreds of kilograms of HEU. Aerodynamic enrichment processes require large amounts of electricity and are not generally considered economically competitive because of high energy consumption and substantial requirements for removal of waste heat. The South African enrichment plant has apparently been closed.\n\nIn the vortex separation process a mixture of uranium hexafluoride gas and hydrogen is injected tangentially into a tube at one end through nozzles or holes, at velocities close to the speed of sound. The tube tapers to a small exit aperture at one or both ends. This tangential injection of gas results in a spiral or vortex motion within the tube, and two gas streams are withdrawn at opposite ends of the vortex tube; centrifugal force providing the isotopic separation. The spiral swirling flow decays downstream of the feed inlet due to friction at the tube wall. Consequently, the inside diameter of the tube is typically tapered to reduce decay in the swirling flow velocity. This process is characterized by a separating element with a very small stage cut (the ratio of product flow to feed flow) of about 1/20, and high process-operating pressures.\n\nDue to the extremely difficult plumbing required to link stages together, the design was developed into a cascade design technique (dubbed Helikon), in which 20 separation stages are combined into one module, and all 20 stages share a common pair of axial-flow compressors. A basic requirement for the success of this method is that the axial-flow compressors successfully transmit parallel streams of different isotopic compositions without significant mixing. A typical Helikon module consists of a large cylindrical steel vessel housing the 20 separator assemblies, along with two compressors (one mounted on each end), and two water-cooled heat exchangers.\n\nAdvantages of this process are a lack of criticality concerns due to the highly diluted feedstock and suitability for batch processing. This means Helikon-type plants can be relatively small, making the technology a nuclear proliferation concern.\n\n\n"}
{"id": "57333683", "url": "https://en.wikipedia.org/wiki?curid=57333683", "title": "History of Airbus", "text": "History of Airbus\n\nToday's Airbus SE is the product of international consolidation in the European aerospace industry tracing back to the formation of the \"Airbus Industrie GIE\" consortium in 1970. In 2000, the European Aeronautic Defence and Space Company (EADS) NV was established. In addition to other subsidiaries pertaining to security and space activities, EADS owned 100% of the pre-existing Eurocopter SA, established in 1992, as well as 80% of Airbus Industrie GIE. In 2001, Airbus Industrie GIE was reorganised as Airbus SAS, a simplified joint-stock company. In 2006, EADS acquired BAE Systems's remaining 20% of Airbus. EADS NV was renamed Airbus Group NV and SE in 2014, and 2015, respectively. Due to the dominance of the Airbus SAS division within Airbus Group SE, these parent and subsidiary companies were merged in January 2017, keeping the name of the parent company. The company was given its present name in April 2017.\n\n\"Airbus Industrie\" began as a consortium of European aviation firms formed to compete with American companies such as Boeing, McDonnell Douglas, and Lockheed.\n\nWhile many European aircraft were innovative, even the most successful had small production runs. Factors favouring American aircraft manufacturers included: the size of the United States which made air transport popular; a 1942 Anglo-American agreement entrusting transport aircraft production to the US; and the World War II legacy of \"a profitable, vigorous, powerful and structured aeronautical industry\" in America.\n\nThe European industry began to accept, along with their governments, that collaboration was required to develop such an aircraft and to compete with the more powerful US manufacturers. Negotiations began over a European collaborative approach and at the 1965 Paris Air Show the major European airlines informally discussed their requirements for a new \"Airbus\" capable of transporting 100 or more passengers over short to medium distances at a low cost. The same year Hawker Siddeley (at the urging of the UK government) teamed with Breguet and Nord to study Airbus designs. The Hawker Siddeley/Breguet/Nord group's HBN 100 became the basis for the continuation of the project. By 1966 the partners were Sud Aviation, later Aérospatiale (France), Arbeitsgemeinschaft Airbus, later Deutsche Airbus (West Germany) and Hawker Siddeley (UK). A request for funding was made to the three governments in October 1966. On 25 July 1967, the three governments agreed to proceed with the proposal.\n\nIn the two years following this agreement, both the British and French governments expressed doubts about the project. The memorandum of understanding had stated that 75 orders must be achieved by 31 July 1968. The French government threatened to withdraw from the project due to its concern over funding all of the Airbus A300, Concorde and the Dassault Mercure concurrently, but was persuaded to maintain its support. With its own concerns at the A300B proposal in December 1968, and fearing it would not recoup its investment due to lack of sales, the British government withdrew on 10 April 1969. West Germany took this opportunity to increase its share of the project to 50%. Given the participation by Hawker Siddeley up to that point, France and West Germany were reluctant to take over its wing design. Thus the British company was allowed to continue as a privileged subcontractor. Hawker Siddeley invested GB£35 million in tooling and, requiring more capital, received a GB£35 million loan from the West German government.\n\nAirbus Industrie was formally established as a \"Groupement d'Intérêt Économique\" (Economic Interest Group or GIE) on 18 December 1970. It had been formed by a government initiative between France, West Germany and the UK that originated in 1967. Its initial shareholders were the French company Aérospatiale and the West German company Deutsche Airbus, each owning a 50% share. The name \"Airbus\" was taken from a non-proprietary term used by the airline industry in the 1960s to refer to a commercial aircraft of a certain size and range, for this term was acceptable to the French linguistically. Aérospatiale and Deutsche Airbus each took a 36.5% share of production work, Hawker Siddeley 20% and the Dutch company Fokker-VFW 7%. Each company would deliver its sections as fully equipped, ready-to-fly items. In October 1971 the Spanish company CASA acquired a 4.2% share of Airbus Industrie, with Aérospatiale and Deutsche Airbus reducing their stakes to 47.9%. In January 1979 British Aerospace, which had absorbed Hawker Siddeley in 1977, acquired a 20% share of Airbus Industrie. The majority shareholders reduced their shares to 37.9%, while CASA retained its 4.2%.\n\nThe Airbus A300 was to be the first aircraft to be developed, manufactured and marketed by Airbus. By early 1967 the \"A300\" label began to be applied to a proposed 320 seat, twin engined airliner. Following the 1967 tri-government agreement, Roger Béteille was appointed technical director of the A300 development project. Béteille developed a division of labour that would be the basis of Airbus' production for years to come: France would manufacture the cockpit, flight control and the lower centre section of the fuselage; Hawker Siddeley, whose Trident technology had impressed him, was to manufacture the wings; West Germany should make the forward and rear fuselage sections, as well as the upper centre section; the Dutch would make the flaps and spoilers; finally Spain (yet to become a full partner) would make the horizontal tailplane. On 26 September 1967 the West German, French and British governments signed a Memorandum of Understanding in London which allowed continued development studies. This also confirmed Sud Aviation as the \"lead company\", that France and the UK would each have a 37.5% work share with West Germany taking 25%, and that Rolls-Royce would manufacture the engines.\n\nIn the face of lukewarm support from airlines for a 300+ seat Airbus A300, the partners submitted the A250 proposal, later becoming the A300B, a 250-seat airliner powered by pre-existing engines. This dramatically reduced development costs, as the Rolls-Royce RB207 to be used in the A300 represented a large proportion of the costs. The RB207 had also suffered difficulties and delays, since Rolls-Royce was concentrating its efforts on the development of another jet engine, the RB211, for the Lockheed L-1011 and Rolls-Royce entering into administration due to bankruptcy in 1971. The A300B was smaller but lighter and more economical than its three-engined American rivals.\n\nIn 1972, the A300 made its maiden flight; its first production model, the A300B2, entered service in 1974. However, the launch of the A300 was largely overshadowed by the similarly timed supersonic aircraft Concorde. Initially the success of the consortium was poor, but orders for the aircraft picked up, due in part to the marketing skills used by Airbus CEO Bernard Lathière, targeting airlines in America and Asia. By 1979 the consortium had 256 orders for A300, and Airbus had launched a more advanced aircraft, the A310, in the previous year. It was the launch of the A320 in 1987 that guaranteed the status of Airbus as a major player in the aircraft market – the aircraft had over 400 orders before it first flew, compared to 15 for the A300 in 1972.\n\nThe Eurocopter SA was formed in 1992, through the merger of the helicopter divisions of Aérospatiale and DASA. The company's heritage traces back to Blériot and Lioré et Olivier in France and to Messerschmitt and Focke-Wulf in Germany.\n\nIn June 1997, British Aerospace Defence Managing Director John Weston commented \"Europe... is supporting three times the number of contractors on less than half the budget of the U.S.\" European governments wished to see the merger of their defence manufacturers into a single entity, a European Aerospace and Defence Company.\n\nAs early as 1995 the German aerospace and defence company DaimlerChrysler Aerospace (DASA) and its British counterpart British Aerospace were said to be eager to create a transnational aerospace and defence company. The two companies envisaged including the French company Aérospatiale, the other major European aerospace company, but only after its privatisation. The first stage of this integration was seen as the transformation of Airbus from a consortium of British Aerospace, DASA, Aérospatiale and Construcciones Aeronáuticas SA into an integrated company; in this aim BAe and DASA were united against the various objections of Aérospatiale. As well as Airbus, British Aerospace and DASA were partners in the Panavia Tornado and Eurofighter Typhoon aircraft projects. Merger discussions began between British Aerospace and DASA in July 1998, just as French participation became more likely with the announcement that Aérospatiale was to merge with Matra and emerge with a diluted French government shareholding. A merger was agreed between British Aerospace Chairman Richard Evans and DASA CEO Jürgen Schrempp in December 1998. However, when the British General Electric Company put its defence electronics business Marconi Electronic Systems (MES) up for sale on 22 December 1998, British Aerospace abandoned the DASA merger in favour of purchasing its British rival. The merger of British Aerospace and MES to form BAE Systems was announced on 19 January 1999 and completed on 30 November. Evans stated in 2004 that his fear was that an American defence contractor would acquire MES and challenge both British Aerospace and DASA.\n\nDASA and the Spanish aircraft company Construcciones Aeronáuticas SA agreed to merge with the signature of a memorandum of understanding on 11 June 1999. On 14 October 1999 DASA agreed to merge with Aérospatiale-Matra to create the European Aeronautic Defence and Space Company. 10 July 2000 was \"Day One\" for the new company, which became the world's second-largest aerospace company after Boeing and the second-largest European arms manufacturer after BAE Systems.\n\nIn January 2001 Airbus Industrie was transformed from an inherently inefficient consortium structure to a formal joint stock company, with legal and tax procedures being finalised on 11 July. Both EADS and BAE transferred ownership of their Airbus factories to the new Airbus SAS in return for 80 % and 20 % shares in the new company respectively. In April 2001 EADS agreed to merge its missile businesses with those of BAE Systems and Alenia Marconi Systems (BAE/Finmeccanica) to form MBDA. EADS took a 37.5 % share of the new company that was formally established in December 2001 and which thus became the world's second-largest missile manufacturer.\n\nOn 16 June 2003 EADS acquired BAE's 25 % share in Astrium, the satellite and space system manufacturer, to become the sole owner. EADS paid £84 million, however due to the lossmaking status of the company BAE invested an equal amount for \"restructuring\". It was subsequently renamed EADS Astrium, and had the divisions Astrium Satellites, Astrium Space Transportation and Astrium Services.\n\nIn November 2003, EADS announced that it was considering working with Japanese companies, and the Japanese METI, to develop a hypersonic airliner intended to be a larger, faster, and quieter, replacement for the Concorde, which was retired in October the same year.\n\nDespite repeated suggestions as early as 2000 that BAE Systems wished to sell its 20 % share of Airbus, the possibility was consistently denied by the company. However, on 6 April 2006 BBC News reported that it was indeed to sell its stake, then \"conservatively valued\" at £2.4 billion. Due to the slow pace of informal negotiations, BAE exercised its put option, which saw investment bank Rothschild appointed to give an independent valuation. Six days after this process began, Airbus announced delays to the A380 with significant effects on the value of Airbus shares. On 2 June 2006 Rothschild valued BAE's share at £1.87 billion, well below BAE's, analysts' and even EADS' expectations. The BAE board recommended that the company proceed with the sale and on 4 October 2006 shareholders voted in favour; the sale was completed on 13 October making EADS the sole shareholder of Airbus.\n\nIn March 2007 EADS Defence and Security Systems division was awarded an eight-year, £200m contract to provide the IT infrastructure for the FiReControl project in the UK.\nThe retention of production and engineering assets by the partner companies in effect made Airbus Industrie a sales and marketing company. This arrangement led to inefficiencies due to the inherent conflicts of interest that the four partner companies faced; they were both GIE shareholders of, and subcontractors to, the consortium. The companies collaborated on development of the Airbus range, but guarded the financial details of their own production activities and sought to maximise the transfer prices of their sub-assemblies. It was becoming clear that Airbus was no longer a temporary collaboration to produce a single plane as per its original mission statement, it had become a long term brand for the development of further aircraft. By the late 1980s work had begun on a pair of new medium-sized aircraft, the biggest to be produced at this point under the Airbus name, the Airbus A330 and the Airbus A340.\nIn the early 1990s the then Airbus CEO Jean Pierson argued that the GIE should be abandoned and Airbus established as a conventional company. However, the difficulties of integrating and valuing the assets of four companies, as well as legal issues, delayed the initiative. In December 1998, when it was reported that British Aerospace and DASA were close to merging, Aérospatiale paralysed negotiations on the Airbus conversion; the French company feared the combined BAe/DASA, which would own 57.9% of Airbus, would dominate the company and it insisted on a 50/50 split. However, the issue was resolved in January 1999 when BAe abandoned talks with DASA in favour of merging with Marconi Electronic Systems to become BAE Systems. Then in 2000 three of the four partner companies (DaimlerChrysler Aerospace, successor to Deutsche Airbus; Aérospatiale-Matra, successor to Sud-Aviation; and CASA) merged to form EADS, simplifying the process. EADS now owned Airbus France, Airbus Deutschland and Airbus España, and thus 80% of Airbus Industrie. BAE Systems and EADS transferred their production assets to the new company, Airbus , in return for shareholdings in that company.\n\nIn mid-1988 a group of Airbus engineers led by Jean Roeder began working in secret on the development of an ultra-high-capacity airliner (UHCA), both to complete its own range of products and to break the dominance that Boeing had enjoyed in this market segment since the early 1970s with its 747. The project was announced at the 1990 Farnborough Air Show, with the stated goal of 15% lower operating costs than the 747-400. Airbus organised four teams of designers, one from each of its partners (Aérospatiale, DaimlerChrysler Aerospace, British Aerospace, CASA) to propose new technologies for its future aircraft designs. In June 1994 Airbus began developing its own very large airliner, then designated as A3XX. Airbus considered several designs, including an odd side-by-side combination of two fuselages from the Airbus A340, which was Airbus's largest jet at the time. Airbus refined its design, targeting a 15% to 20% reduction in operating costs over the existing Boeing 747–400. The A3XX design converged on a double-decker layout that provided more passenger volume than a traditional single-deck design.\n\nFive A380s were built for testing and demonstration purposes. The first A380 was unveiled at a ceremony in Toulouse on 18 January 2005, and its maiden flight took place on 27 April 2005. After successfully landing three hours and 54 minutes later, chief test pilot Jacques Rosay said flying the A380 had been \"like handling a bicycle\". On 1 December 2005, the A380 achieved its maximum design speed of Mach 0.96. On 10 January 2006, the A380 made its first transatlantic flight to Medellín in Colombia.\n\nThe Airbus A380 was delayed in October 2006 due to the use of incompatible software used to design the aircraft. Primarily, the Toulouse assembly plant used the latest version 5 of CATIA (made by Dassault), while the design centre at the Hamburg factory were using the older and incompatible version 4. The result was that the 530 km of cables wiring throughout the aircraft had to be completely redesigned. Although no orders had been cancelled, Airbus still had to pay millions in late-delivery penalties.\n\nThe first aircraft delivered was to Singapore Airlines on 15 October 2007 and entered service on 25 October 2007 with an inaugural flight between Singapore and Sydney. Two months later Singapore Airlines CEO Chew Choong Seng said that the A380 was performing better than both the airline and Airbus had anticipated, burning 20% less fuel per passenger than the airline's existing 747-400 fleet. Emirates was the second airline to take delivery of the A380 on 28 July 2008 and started flights between Dubai and New York on 1 August 2008. Qantas followed on 19 September 2008, starting flights between Melbourne and Los Angeles on 20 October 2008.\n\nIn 2003, Airbus and the Kaskol Group created an Airbus Engineering centre in Russia, which started with 30 engineers and since has emerged as a model of success for Airbus’ globalisation strategy. It was the first engineering facility to open in Europe outside the company's home countries. Equipped with state-of-the-art communications equipment and linked with Airbus engineering sites in France and Germany, the facility performs extensive work in disciplines such as fuselage structure, stress, system installation and design. In 2011, the centre employs some 200 engineers who have completed over 30 large-scale projects for the A320, the A330/A340 and the A380 programmes. Russian engineers also performed more than half of all design work on the A330-200F freighter, with its activity related to fuselage structure design, floor grids installation and junctions design. The centre currently is involved in the A320neo Sharklets design development and numerous design works for the A350 XWB programme.\n\nOn 6 April 2006 BAE Systems planned to sell its 20% share in Airbus, then \"conservatively valued\" at €3.5 billion (US$4.17 billion). Analysts suggested the move to make partnerships with U.S. firms more feasible, in both financial and political terms. BAE originally sought to agree on a price with EADS through an informal process. Due to lengthy negotiations and disagreements over price, BAE exercised its put option, which saw investment bank Rothschild appointed to give an independent valuation.\n\nIn June 2006 Airbus was embroiled in significant international controversy over an announcement of further delays in the delivery of its A380. Following the announcement the value of associated stock plunged by up to 25% in a matter of days, although it soon recovered afterwards. Allegations of insider trading on the part of Noël Forgeard, CEO of EADS, its majority corporate parent, promptly followed. The loss of associated value was of grave concern to BAE, press described a \"furious row\" between BAE and EADS, with BAE believing the announcement was designed to depress the value of its share. A French shareholder group filed a class action lawsuit against EADS for failing to inform investors of the financial implications of the A380 delays while airlines awaiting deliveries demanded compensation. As a result, EADS chief Noël Forgeard and Airbus CEO Gustav Humbert resigned on 2 July 2006.\n\nOn 2 July 2006 Rothschild valued BAE's stake at £1.9 billion (€2.75 billion), well below the expectation of BAE, analysts, and even EADS. On 5 July BAE appointed independent auditors to investigate how the value of its share of Airbus had fallen from the original estimates to the Rothschild valuation; however in September 2006 BAE agreed to the sale of its stake in Airbus to EADS for £1.87 billion (€2.75 billion, $3.53 billion), pending BAE shareholder approval. On 4 October shareholders voted in favour of the sale, leaving Airbus entirely owned by EADS.\n\nOn 29 February 2008, the United States Air Force awarded a $35 billion contract for aerial refueling tankers (the KC-45) to Northrop Grumman, with EADS as a major subcontractor. The contract, one of the largest created by the Department of Defence, is initially valued at $35 billion but has the potential to grow to $100 billion.\n\nUnder the contract, Northrop Grumman and EADS would build a fleet of 179 planes, based on the existing Airbus A330, to provide in-air refueling to military aircraft, from fighter jets to cargo planes. While final assembly of the craft would take place at an Airbus plant near Mobile, Alabama, parts would come from suppliers across the globe.\n\nHowever, the award was protested by Boeing, the other bidder on the project, which was upheld by the GAO. In response to the new contest, on 8 March 2010, Northrop Grumman announced it was abandoning its bid for the new contract, with its CEO stating that the revised bid requirement favored Boeing. On 20 April 2010, EADS announced it was re-entering the competition and intended to enter a bid with the KC-45.\n\nEADS reported a 763 million euros loss for 2009 as a result of a 1.8 billion euros charge on the troubled Airbus A400M project and a 240 million euros charge related to the A380.\n\nOn 12 September 2012 it was reported that BAE and EADS were in discussions regarding a possible merger. In the event of the merger, BAE shareholders would own 40 % and EADS 60 % of the new enlarged organisation. A key French EADS shareholder Lagardere asked EADS to rethink the proposed merger plan as the conditions were unsatisfactory. The bosses of BAE Systems and EADS issued a joint statement seeking political support for their proposed 35 billion euro (US$45 billion) merger from the British, French and German governments; and reiterated that the combination is borne out of opportunity, not necessity and the new company would be greater than the sum of its parts. It was reported on 10 October 2012, that the merger between BAE Systems and EADS had been called off.\n\nIn January 2014, EADS was reorganised as \"Airbus Group NV\", with three divisions (Airbus, Airbus Defence and Space, and Airbus Helicopters. On 27 May 2015 the company became a \"Societas Europaea\" (SE) (Latin: European Company), having been a \"Naamloze vennootschap\" (public limited company). In September 2016, Airbus Group announced that it would merge with its largest division, Airbus SAS, into a new entity and introduce a single Airbus brand, the merge to take effect on 1 January 2017. The group reorganized under the brand name of \"Airbus\" in January 2017. The subsidiaries Airbus Helicopters and Airbus Defence and Space became operating divisions of the same company. Airbus Group SE changed its legal name to \"Airbus SE\" at its 2017 annual meeting on 12 April 2017.\n\nOn 9 October 2006 Christian Streiff, Humbert's successor, resigned due to differences with parent company EADS over the amount of independence he would be granted in implementing his reorganisation plan for Airbus. He was succeeded by EADS co-CEO Louis Gallois, bringing Airbus under more direct control of its parent company.\n\nOn 28 February 2007, CEO Louis Gallois announced the company's restructuring plans. Entitled Power, the plan would see 10,000 jobs cut over four years; 4,300 in France, 3,700 in Germany, 1,600 in the UK and 400 in Spain. 5,000 of the 10,000 would be at subcontractors. Plants at Saint Nazaire, Varel and Laupheim face sell off or closure, while Meaulte, Nordenham and Filton are \"open to investors\". As of 16 September 2008 the Laupheim plant has been sold to a Thales-Diehl consortium to form Diehl Aerospace and while the design activities at Filton have been retained, the manufacturing operations have been sold to British company GKN. The announcements resulted in Airbus unions in France and Germany threatening strike action.\n\nAt the 2011 Paris Air Show, Airbus received total orders valued at about $72.2 billion for 730 aircraft, representing a new record in the civil aviation industry. The A320neo (\"new engine option\") model, announced in December 2010, received 667 orders; this, together with previous orders, resulted in a total of 1029 orders within six months of launch date, creating another industry record.\n\nIn January 2016 Airbus announced it has signed a tentative agreement with Iran to sell 118 Airbus aircraft along with a comprehensive civil aviation cooperation package as a part of the implementation of the Joint Comprehensive Plan of Action (JCPOA). Boeing has also announced its will to sell 80 jets directly to Iran Air as part of a proposed deal worth up to $17.6bn.\n\nHowever, In early July 2016, US House of Representatives passed amendments that would block US Department of Treasury funds from granting export licences or reexport of passenger commercial aircraft. Boeing reacted that if its deal with Iran is blocked by the US Congress, all other companies that supply to its rivals should be prohibited as well. Airbus, too, has said that it requires US's approval to export airliners to Iran, because parts of its aircraft are made in the US.\n\nThe deal between Iran Air and Airbus was finally implemented, and the first new purchased Airbus aircraft, an A321, landed in Tehran's International Mehr Abad Airport on January 12, 2017; Airbus stated that the delivery has been in full compliance with the JCPOA and US government Office of Foreign Assets Control licenses.\n\nOn 30 June, Airbus said its airliner sales team would now report directly to Tom Enders and by-pass Fabrice Bregier, which will lead programs, support and services, engineering, manufacturing, procurement and quality while Enders will lead sales and marketing.\n\nOn 16 October, Airbus and Bombardier Aerospace announced a partnership on the CSeries program, with Airbus acquiring a 50.01% majority stake, Bombardier keeping 31% and Investissement Québec 19%, to expand in an estimated market of more than 6,000 new 100-150 seat aircraft over 20 years; Airbus’ supply chain expertise should save production costs but headquarters and assembly remain in Québec while U.S. customers would benefit from a second Final Assembly Line in Mobile, Alabama.\n\nIn the fall, Der Spiegel investigated systematic corruption and improper intermediates usage in past sales and questioned whether Enders can survive the scandal as he did not react quickly enough, then Handelsblatt reported the French government wants to control Airbus again and Bregier wants to get Enders fired to gain his position.\n\nSales chief John Leahy was supposed to retire at the end of 2017 to be replaced by his deputy Kiran Rao.\nA few weeks before the switch, Rao told Airbus CEO Tom Enders that he was no longer available.\nAfter investigations into alleged bribery, Enders is taking personal responsibility for the sales organization compliance.\nLeahy should stay till a successor is found, maybe Frenchmen Eric Schulz, aerospace engineer and president of Rolls-Royce plc Civil Aerospace or Christian Scherer, CEO of ATR Aircraft, strategist and salesman.\nSchulz should replace Leahy on 25 January.\n\nIn November, Paul Eremenko, Airbus's CTO, quit after two years. French unions held him for responsible for the job cuts made at a French research facility nearby Paris. Tom Enders, Airbus chief executive, was counting on Paul Eremenko to create a radically different approach to R&D.\n\nOn 28 November 2017, Airbus announced a partnership with Rolls-Royce plc and Siemens to develop the E-Fan X hybrid-electric aircraft demonstrator, to fly in 2020.\n\nEnders CEO mandate runs until April 2019 and his reconduction is decided by Airbus' board, but in December he said would not seek a third mandate while he was in April as he is pressured over corruption investigations.\nOn 15 December, the Board confirmed Enders will not stay beyond April 2019 and announced that in February 2018 Brégier will be replaced by Guillaume Faury, currently Airbus Helicopters CEO.\nThe renewal should have been disclosed in early 2018, the media hype accelerated its timing but not the decision.\nWhen told by the board that he would not succeed Enders as CEO, Bregier chose to leave.\nBesides Enders, Bregier, Leahy and Eremenko, engineering chief Charles Champion is retiring at the end of 2017, Airbus North America chairman Allan McArtor is leaving, as is the unit's CEO Barry Eccleston to be replaced by Jeff Knittel, CEO of lessor CIT Aerospace.\nHead of military aircraft Fernando Alonso, civil aircraft division COO Tom Williams and head of programs Didier Evrard are also nearing retirement.\n\nFor 2017, Airbus announced it received 1,109 net orders from 44 customers in 2017, and delivered 718 aircraft to 85 customers: 558 A320 Family (including 181 A320neo); 67 A330s; 78 A350 XWBs and 15 A380s.\n\nAs the UK leaves the EU, Airbus has been approached by at least seven governments (France, Germany, Spain, U.S., China, Mexico and South Korea) for wing production competitiveness.\nCurrently produced in Broughton and designed in Filton since 1970, wing production employs 15,000 people, which is over 10% of Airbus staff.\nComposite wings will be needed to lower fuel burn by 30% for the A320 replacement.\nTom Enders later promised the U.K. government Business Secretary Greg Clark that Airbus would retain its British operations “long into the future’’ and see the U.K. as a “home country and a competitive place to invest.’’ \n\nAirbus will designate a new CEO to succeed Enders by the end of 2018, which will be submitted to shareholders at the spring 2019 annual meeting, with planemaking boss and former Eurocopter head Guillaume Faury as the main internal candidate.\nAirbus chief financial officer Harald Wilhelm will quit when Enders will leave in 2019.\n\nOn 15 May, in its EU appeal ruling, the WTO concluded that the A380 and A350 received improper subsidies through repayable launch aids or low interest rates which could have been avoided, and Airbus agreed to correct those violations.\n\nOn 13 September, Eric Schulz left the Chief Commercial Officer role for personal reasons and was replaced by Christian Scherer, CEO of ATR since October 2016.\nAs Schulz was previously head of Rolls-Royce plc civil engines, currently suffering problems, airlines could had a skewed opinion.\n\nOn 8 October, the Board of directors selected Guillaume Faury to succeed Tom Enders as Airbus CEO from 10 April 2019.\nOn 21 November, Airbus appointed Michael Schöllhorn, COO for BSH Home Appliances GmbH, \nto succeed Tom Williams as Chief Operating Officer (COO) for Airbus Commercial Aircraft from 1 February 2019, and Dominik Asam, CFO of Infineon Technologies, to succeed Harald Wilhelm as Chief Financial Officer\nfrom 10 April 2019.\n\nAirbus is in tight competition with Boeing every year for aircraft orders although Airbus has secured over 50% of aircraft orders in the decade since 2003.\n\nAirbus won a greater share of orders in 2003 and 2004. In 2005, Airbus achieved 1111 (1055 net) orders, compared to 1029 (net of 1002) for the same year at rival Boeing However, Boeing won 55% of 2005 orders proportioned by value; and in the following year Boeing won more orders by both measures. Airbus in 2006 achieved its second best year ever in its entire 35-year history in terms of the number of orders it received, 824, second only to the previous year. Airbus plans to increase production of A320 airliners to reach 40 per month by 2012, at a time when Boeing is increasing monthly 737 production from 31.5 to 35 per month.\n\nRegarding operational aircraft, there were 7,264 Airbus aircraft operational at April 2013. Although Airbus secured over 50% of aircraft orders in the decade since 2003, the number of Boeing aircraft still in operation at April 2013 still exceeded Airbus by 21% because Airbus made a late entry into the market, 1972 vs. 1958 for Boeing; this lead is diminishing as older aircraft are progressively retired.\n\nThough both manufacturers have a broad product range in various segments from single-aisle to wide-body, their aircraft do not always compete head-to-head. Instead they respond with models slightly smaller or bigger than the other in order to plug any holes in demand and achieve a better edge. The A380, for example, is designed to be larger than the 747. The A350XWB competes with the high end of the 787 and the low end of the 777. The A320 is bigger than the 737-700 but smaller than the 737–800. The A321 is bigger than the 737–900 but smaller than the previous 757-200. Airlines see this as a benefit since they get a more complete product range, from 100 seats to 500 seats, than if both companies offered identical aircraft.\nIn recent years the Boeing 777 has outsold its Airbus counterparts, which include the A340 family as well as the A330-300. The smaller A330-200 competes with the 767, outselling its Boeing counterpart in recent years. The A380 is anticipated to further reduce sales of the Boeing 747, gaining Airbus a share of the market in very large aircraft, though frequent delays in the A380 programme have caused several customers to consider the refreshed 747–8. Airbus has also proposed the A350 XWB to compete with the Boeing 787 Dreamliner, after being under great pressure from airlines to produce a competing model.\n\nIn 2015, Airbus Group said it was establishing an R&D center and venture capital fund in Silicon Valley. Airbus CEO Fabrice Bregier stated: \"What is the weakness of a big group like Airbus when we talk about innovation? We believe that we have better ideas than the rest of the world. We believe that we know because we control the technologies and platforms. The world has shown us in the car industry, the space industry and the hi-tech industry that this is not true. And we need to be open to others' ideas and others' innovations,\"\n\nAirbus Group CEO Tom Enders stated that \"The only way to do it for big companies is really to create spaces outside of the main business where we allow and where we incentivize experimentation... That is what we have started to do but there is no manual... It is a little bit of trial and error. We all feel challenged by what the Internet companies are doing.\"\n\nSix months after launch, the Airbus Group Venture fund in Silicon Valley became fully operational in January 2016.\n\nEmblems of Airbus Industrie GIE (1970–2000) and Airbus SAS (2001–2016), until the latter on 1 January 2017 merged with its parent company, Airbus Group SE:\nEmblems of the European Aeronautic Defence and Space Company NV (2000–2014), Airbus Group NV (2014–2015) and Airbus Group SE (2015–2017):\n\n\n\n\"The Airbus story so far\", Airbus.com\n"}
{"id": "42466294", "url": "https://en.wikipedia.org/wiki?curid=42466294", "title": "Hitit Computer Services", "text": "Hitit Computer Services\n\nHitit Computer Services is a private company that sells commercial IT systems for the travel industry. Its headquarters is in the ITU Arı Technopolis science park on the campus of Istanbul Technical University.\n\nHitit was established as an IT company by executives from Turkish Airlines in 1994. The first program sold by the company was Crane FF which became the most common loyalty management system, by number of airline users. Hitit introduced its new generation airline reservations system in 2004. The company opened an office in Dubai due to an increasing number of airline customers in the Middle East. Hitit is one of Turkey’s biggest software exporters: with 83% of its customerbase being overseas and 92% of its revenues coming from exports,around $5million-8$million revenue, it became the 3rd largest service exporter of 2011.\n\nThe company had two divisions until 8 April 2014 when the Loyalty Division of Hitit was acquired by the Amadeus IT Group. The Airline Division of Hitit remains independent and continues to sell software to airlines and travel companies from Europe, Middle East, Africa and Asia. With 318% growth rate, the company was in Deloitte's Technology Fast 500 EMEA list in 2013, and it is also an International Air Transport Association Strategic Partner. Today, the company sells products covering a range from passenger service systems to financial, operational and loyalty management systems. Recently, Pakistan International Airlines (PIAC) has acquired the services provided by HITIT in a step to solve the issues it previously encountered by using SABRE system.\n\n"}
{"id": "33855541", "url": "https://en.wikipedia.org/wiki?curid=33855541", "title": "Housing and Building Research Institute", "text": "Housing and Building Research Institute\n\nHousing and Building Research Institute (HBRI) is an autonomous organization under Government of Bangladesh Ministry of Housing and Public Works.\n\nThe institute was established through the passing of the Housing and Building Research Institutes Act 2016.\n\nThe Institute has a total of 153 personnel including 33 qualified research personnel. All activities including R & D programme are now being executed through the following Divisions and Sections:\n\n\n"}
{"id": "282350", "url": "https://en.wikipedia.org/wiki?curid=282350", "title": "Hygrometer", "text": "Hygrometer\n\nA hygrometer () is an instrument used for measuring the amount of humidity and water vapour in the atmosphere, in soil, or in confined spaces. Humidity measurement instruments usually rely on measurements of some other quantity such as temperature, pressure, mass, a mechanical or electrical change in a substance as moisture is absorbed. By calibration and calculation, these measured quantities can lead to a measurement of humidity. Modern electronic devices use temperature of condensation (called the dew point), or changes in electrical capacitance or resistance to measure humidity differences. The first crude hygrometer was invented by the Italian Renaissance polymath Leonardo da Vinci in 1480 and a more modern version was created by Swiss polymath Johann Heinrich Lambert in 1755.\n\nThe maximum amount of water vapour that can be held in a given volume of air (saturation) varies greatly by temperature; cold air can hold less mass of water per unit volume than hot air. Temperature can change humidity. Most instruments respond to (or are calibrated to read) relative humidity (RH), which is the amount of water relative to the maximum at a particular temperature expressed as per cent.\n\nPrototype hygrometers were devised and developed during the Western Han dynasty in Ancient China to study weather. The Chinese used a bar of charcoal and a lump of earth: its dry weight was taken, then compared with its damp weight after being exposed in the air. The differences in weight were used to tally the humidity level.\n\nOther techniques were applied using mass to measure humidity, such as when the air was dry, the bar of charcoal would be light, while when the air was humid, the bar of charcoal would be heavy. By hanging a lump of earth and a bar of charcoal on the two ends of a staff separately and adding a fixed lifting string on the middle point to make the staff horizontal in dry air, an ancient hygrometer was made.\n\nThe metal-paper coil hygrometer is useful for giving a dial indication of humidity changes. It appears most often in inexpensive devices, and its accuracy is limited, with variations of 10% or more. In these devices, water vapour is absorbed by a salt-impregnated paper strip attached to a metal coil, causing the coil to change shape. These changes (analogous to those in a bimetallic thermometer) cause an indication on a dial. There is usually a metal needle on the front of the gauge that will change where it points to.\n\nThese devices use a human or animal hair under some tension. The hair is hygroscopic (tending toward retaining moisture); its length changes with humidity, and the length change may be magnified by a mechanism and indicated on a dial or scale. In the late 17th century, such devices were called by some scientists \"hygroscopes\"; that word is no longer in current use, but \"hygroscopic\" and \"hygroscopy\", which derive from it, still are. The traditional folk art device known as a weather house works on this principle. Whale bone and other materials may be used in place of hair.\n\nIn 1783, Swiss physicist and geologist Horace Bénédict de Saussure built the first hair-tension hygrometer using human hair.\n\nA psychrometer, or a Wet-and-dry-bulb thermometer, consists of two thermometers, one that is dry and one that is kept moist with distilled water on a sock or wick. At temperatures above the freezing point of water, evaporation of water from the wick lowers the temperature, so that the wet-bulb thermometer usually shows a lower temperature than that of the dry-bulb thermometer. When the air temperature is below freezing, however, the wet-bulb is covered with a thin coating of ice and may be warmer than the dry bulb.\nRelative humidity (RH) is computed from the ambient temperature, shown by the dry-bulb thermometer and the difference in temperatures as shown by the wet-bulb and dry-bulb thermometers. Relative humidity can also be determined by locating the intersection of the wet and dry-bulb temperatures on a psychrometric chart. The two thermometers coincide when the air is fully saturated, and the greater the difference the drier the air. Psychrometers are commonly used in meteorology, and in the HVAC industry for proper refrigerant charging of residential and commercial air conditioning systems.\n\nA sling psychrometer, which uses thermometers attached to a handle or length of rope and spun in the air for about one minute, is sometimes used for field measurements, but is being replaced by more convenient electronic sensors. A whirling psychrometer uses the same principle, but the two thermometers are fitted into a device that resembles a ratchet or football rattle.\n\nDew point is the temperature at which a sample of moist air (or any other water vapour) at constant pressure reaches water vapor saturation. At this saturation temperature, further cooling results in condensation of water. Chilled mirror dewpoint hygrometers are some of the most precise instruments commonly available. They use a chilled mirror and optoelectronic mechanism to detect condensation on the mirror's surface. The temperature of the mirror is controlled by electronic feedback to maintain a dynamic equilibrium between evaporation and condensation, thus closely measuring the dew point temperature. An accuracy of 0.2 °C is attainable with these devices, which correlates at typical office environments to a relative humidity accuracy of about ±1.2%. These devices need frequent cleaning, a skilled operator and periodic calibration to attain these levels of accuracy. Even so, they are prone to heavy drifting in environments where smoke or otherwise impure air may be present.\n\nMore recently, spectroscopic chilled-mirrors have been introduced. Using this method, the dew point is determined with spectroscopic light detection which ascertains the nature of the condensation. This method avoids many of the pitfalls of the previous chilled-mirrors and is capable of operating drift free.\n\nFor applications where cost, space, or fragility are relevant, other types of electronic sensors are used, at the price of a lower accuracy. In capacitive hygrometers, the effect of humidity on the dielectric constant of a polymer or metal oxide material is measured. With calibration, these sensors have an accuracy of ±2% RH in the range 5–95% RH. Without calibration, the accuracy is 2 to 3 times worse. Capacitive sensors are robust against effects such as condensation and temporary high temperatures. Capacitive sensors are subject to contamination, drift and aging effects, but they are suitable for many applications.\n\nIn resistive hygrometers, the change in electrical resistance of a material due to humidity is measured. Typical materials are salts and conductive polymers. Resistive sensors are less sensitive than capacitive sensors – the change in material properties is less, so they require more complex circuitry. The material properties also tend to depend both on humidity and temperature, which means in practice that the sensor must be combined with a temperature sensor. The accuracy and robustness against condensation vary depending on the chosen resistive material. Robust, condensation-resistant sensors exist with an accuracy of up to ±3% RH (relative humidity).\n\nIn thermal hygrometers, the change in thermal conductivity of air due to humidity is measured. These sensors measure absolute humidity rather than relative humidity.\n\nA Gravimetric hygrometer measures the mass of an air sample compared to an equal volume of dry air. This is considered the most accurate primary method to determine the moisture content of the air. National standards based on this type of measurement have been developed in US, UK, EU and Japan. The inconvenience of using this device means that it is usually only used to calibrate less accurate instruments, called Transfer Standards.\n\nAside from greenhouses and industrial spaces, hygrometers are also used in some incubators, saunas, humidors and museums. They are also used in the care of wooden musical instruments such as pianos, guitars, violins, and harps which can be damaged by improper humidity conditions. In residential settings, hygrometers are used to assist in humidity control (too low humidity can damage human skin and body, while too high humidity favors growth of mildew and dust mite). Hygrometers are also used in the coating industry because the application of paint and other coatings may be very sensitive to humidity and dew point. With a growing demand on the amount of measurements taken the psychrometer is now replaced by a dewpoint gauge known as a dewcheck. These devices make measurements a lot faster but are often not allowed in explosive environments.\n\nHumidity measurement is among the more difficult problems in basic metrology. According to the WMO Guide, \"The achievable accuracies (for humidity determination) listed in the table refer to good quality instruments that are well operated and maintained. In practice, these are not easy to achieve.\" Two thermometers can be compared by immersing them both in an insulated vessel of water (or alcohol, for temperatures below the freezing point of water) and stirring vigorously to minimize temperature variations. A high-quality liquid-in-glass thermometer if handled with care should remain stable for some years. Hygrometers must be calibrated in air, which is a much less effective heat transfer medium than is water, and many types are subject to drift so need regular recalibration. A further difficulty is that most hygrometers sense relative humidity rather than the absolute amount of water present, but relative humidity is a function of both temperature and absolute moisture content, so small temperature variations within the air in a test chamber will translate into relative humidity variations.\n\nIn a cold and humid environment, sublimation of ice may occur on the sensor head, whether it is a hair, dew cell, mirror, capacitance sensing element, or dry-bulb thermometer of an aspiration psychrometer. The ice on the probe matches the reading to the saturation humidity with respect to ice at that temperature, i.e. the frost point. However, a conventional hygrometer is unable to measure properly above the frost point, and the only way to go around this fundamental problem is to use a heated humidity probe.\n\nAccurate calibration of the thermometers used is fundamental to precise humidity determination by the wet-dry method. The thermometers must be protected from radiant heat and must have a sufficiently high flow of air over the wet bulb for the most accurate results. One of the most precise types of wet-dry bulb psychrometer was invented in the late 19th century by Adolph Richard Aßmann (1845–1918); in English-language references the device is usually spelled \"Assmann psychrometer.\" In this device, each thermometer is suspended within a vertical tube of polished metal, and that tube is in turn suspended within a second metal tube of slightly larger diameter; these double tubes serve to isolate the thermometers from radiant heating. Air is drawn through the tubes with a fan that is driven by a clockwork mechanism to ensure a consistent speed (some modern versions use an electric fan with electronic speed control). According to Middleton, 1966, \"an essential point is that air is drawn between the concentric tubes, as well as through the inner one.\"\n\nIt is very challenging, particularly at low relative humidity, to obtain the maximal theoretical depression of the wet-bulb temperature; an Australian study in the late 1990s found that liquid-in-glass wet-bulb thermometers were warmer than theory predicted even when considerable precautions were taken; these could lead to RH value readings that are 2 to 5 percent points too high.\n\nOne solution sometimes used for accurate humidity measurement when the air temperature is below freezing is to use a thermostatically-controlled electric heater to raise the temperature of outside air to above freezing. In this arrangement, a fan draws outside air past (1) a thermometer to measure the ambient dry-bulb temperature, (2) the heating element, (3) a second thermometer to measure the dry-bulb temperature of the heated air, then finally (4) a wet-bulb thermometer. According to the World Meteorological Organization Guide, \"The principle of the heated psychrometer is that the water vapour content of an air mass does not change if it is heated. This property may be exploited to the advantage of the psychrometer by avoiding the need to maintain an ice bulb under freezing conditions.\".\n\nSince the humidity of the ambient air is calculated indirectly from three temperature measurements, in such a device accurate thermometer calibration is even more important than for a two-bulb configuration.\n\nVarious researchers have investigated the use of saturated salt solutions for calibrating hygrometers. Slushy mixtures of certain pure salts and distilled water have the property that they maintain an approximately constant humidity in a closed container. A saturated table salt (Sodium Chloride) bath will eventually give a reading of approximately 75%. Other salts have other equilibrium humidity levels: Lithium Chloride ~11%; Magnesium Chloride ~33%; Potassium Carbonate ~43%; Potassium Sulfate ~97%. Salt solutions will vary somewhat in humidity with temperature and they can take relatively long times to come to equilibrium, but their ease of use compensates somewhat for these disadvantages in low precision applications, such as checking mechanical and electronic hygrometers.\n\n\n"}
{"id": "160529", "url": "https://en.wikipedia.org/wiki?curid=160529", "title": "Illegal prime", "text": "Illegal prime\n\nAn illegal prime is a prime number that represents information whose possession or distribution is forbidden in some legal jurisdictions. One of the first illegal primes was found in 2001. When interpreted in a particular way, it describes a computer program that bypasses the digital rights management scheme used on DVDs. Distribution of such a program in the United States is illegal under the Digital Millennium Copyright Act. An illegal prime is a kind of illegal number.\n\nOne of the earliest illegal prime numbers was generated in March 2001 by Phil Carmody. Its binary representation corresponds to a compressed version of the C source code of a computer program implementing the DeCSS decryption algorithm, which can be used by a computer to circumvent a DVD's copy protection.\n\nProtests against the indictment of DeCSS author Jon Lech Johansen and legislation prohibiting publication of DeCSS code took many forms. One of them was the representation of the illegal code in a form that had an \"intrinsically archivable\" quality. Since the bits making up a computer program also represent a number, the plan was for the number to have some special property that would make it archivable and publishable (one method was to print it on a T-shirt). The primality of a number is a fundamental property of number theory and is therefore not dependent on legal definitions of any particular jurisdiction.\n\nThe large prime database of The Prime Pages website records the top 20 primes of various special forms; one of them is proof of primality using the elliptic curve primality proving (ECPP) algorithm. Thus, if the number were large enough and proved prime using ECPP, it would be published.\n\nSpecifically, Carmody applied Dirichlet's theorem to several prime candidates of the form \n\"k\"·256 + \"b\", where \"k\" was the decimal representation of the original compressed file. Multiplying by a power of 256 adds as many trailing null characters to the gzip file as indicated in the exponent which would still result in the DeCSS C code when unzipped.\n\nOf those prime candidates, several were identified as probable prime using the open source program OpenPFGW, and one of them was proved prime using the ECPP algorithm implemented by the Titanix software. Even at the time of discovery in 2001, this 1401-digit number, of the form \"k\"·256 + 2083, was too small to be mentioned, so Carmody created a 1905-digit prime, of the form \"k\"·256 + 99, that was the tenth largest prime found using ECPP, a remarkable achievement by itself and worthy of being published on the lists of the highest prime numbers. In a way, by having this number independently published for a completely unrelated reason to the DeCSS code, he had been able to evade legal responsibility for the original software.\n\nFollowing this, Carmody discovered a 1811-digit prime -- this one being non-compressed, directly executable machine language in the ELF format for Linux i386, implementing the same DeCSS functionality.\n\n\n"}
{"id": "4410085", "url": "https://en.wikipedia.org/wiki?curid=4410085", "title": "Intermediate eXperimental Vehicle", "text": "Intermediate eXperimental Vehicle\n\nThe Intermediate eXperimental Vehicle (IXV) is a European Space Agency (ESA) experimental suborbital re-entry vehicle. It was developed to serve as a prototype spaceplane to validate the ESA's work in the field of reusable launchers.\n\nThe European Space Agency has a program called Future Launchers Preparatory Programme (FLPP), which made a call for submissions for a reusable spaceplane. One of the submissions was by the Italian Space Agency, that presented their own Programme for Reusable In-orbit Demonstrator in Europe (PRIDE program) which went ahead to develop the prototype named Intermediate eXperimental Vehicle (IXV) and the consequential Space Rider that inherits technology from its prototype IXV.\n\nOn 11 February 2015, the IXV conducted its first 100-minute space flight, successfully completing its mission upon landing intact on the surface of the Pacific Ocean. The vehicle holds the distinction of being the first ever lifting body to perform full atmospheric reentry from orbital speed. Past missions have flight tested either winged bodies, which are highly controllable but also very complex and costly, or capsules, which are difficult to control but offer less complexity and lower cost. \n\nDuring the 1980s and 1990s, there was significant international interest in the development of reusable launch platforms and reusable spacecraft, particularly in respect to spaceplanes; perhaps the most high-profile examples of these being the American Space Shuttle and Soviet Buran programmes. The national space agencies of European nations, such as France's Centre National d'Études Spatiales (CNES) and Germany's German Aerospace Center (DLR), worked on their own designs during this era, the most prominent of these to emerge being the Hermes spaceplane. Development of the Hermes programme, which was backed by the European Space Agency (ESA) for several years, was ultimately terminated in 1992 prior to any flights being performed in favour of a partnership arrangement with the Russian Aviation and Space Agency (RKA) to use the existing Soyuz spacecraft instead.\n\nWhile work on the development of the Hermes vehicle was cancelled during the early 1990s, the ESA maintained its strategic long term objective to indigenously develop and eventually deploy similar reusable space vehicles. Accordingly, in support of this goal, the ESA embarked upon a series of design studies on different experimental vehicle concepts as well as to refine and improve technologies deemed critical to future reentry vehicles. In order to test and further develop the technologies and concepts produced by these studies, there was a clear needs to accumulate practical flight experience with reentry systems, as well as to maintain and expand upon international cooperation in the fields of space transportation, exploration and science. Out of these desires emerged the Future Launchers Preparatory Programme (FLPP), an ESA-headed initiative conceived and championed by a number of its member states, which provided a framework for addressing the challenges and development of the technology associated with reentry vehicles.\n\nIt was recognised that, in order for significant progress to be made, FLPP would require the production and testing of a prototype reentry vehicle that drew on their existing research, technologies, and designs. By adopting a step-by-step approach using a series of test vehicles prior to the development of a wider series of production vehicles, this approach was seen to reduce the risk and to allow for the integration of progressively more sophisticated developments out of from the early relatively low-cost missions. \n\nIn line with this determination, during early 2005, the Intermediate eXperimental Vehicle (IXV) project was formally initiated by the Italian Space Agency and the Italian Aerospace Research Centre under an Italian programme named PRIDE (Programme for Reusable In-orbit Demonstrator in Europe) Their main industrial contractor was Next Generation Launcher Prime SpA (NGLP) in Italy. The latter organisation is a joint venture entity comprising two major European aerospace companies, Astrium and Finmeccanica. The PRIDE programme had the support of various national space agencies, including the European Space Research and Technology Centre, Italian Space Agency (ASI), French space agency CNES, and Germany's DLR; by November 2006, the IXV was supported by 11 Member States: Austria, Belgium, France, Germany, Ireland, Italy, Portugal, Spain, Sweden, Switzerland and The Netherlands. Of these, Italy emerged as the principal financial backer of the IXV programme.\n\nThe IXV project also benefitted from and harnessed much of the research data and operational principles from many of the previously conducted studies, especially from the successful Atmospheric Reentry Demonstrator (ARD), which was test-flown during 1998. Early on, during the mission definition and design maturity stages of the project, thorough comparisons were conducted again between existing ESA and national concepts against shared criteria, aimed at evaluating the experiment requirements (technology and systems), programme requirements (technology readiness, development schedule and cost) and risk mitigation (feasibility, maturity, robustness and growth potential). The selected baseline design, a slender lifting body configuration, drew primarily upon the CNES-led \"Pre-X\" the ESA's ARD vehicles. Development work quickly proceeded through the preliminary design definition phase, reaching a system requirements review by mid-2007.\n\nOn 18 December 2009, the ESA announced the signing of a contract with Thales Alenia Space, valued at , to cover 18 months of preliminary IXV work. In 2011, the total estimated cost for the IXV project was reportedly .\n\nDuring late 2012, the IXV's subsonic parachute system was tested at the Yuma Proving Ground in Arizona, United States. Shortly thereafter, a series of water impact tests were conducted at Consiglio Nazionale delle Ricerche's INSEAN research tank near Rome, Italy.\n\nOn 21 June 2013, an IXV test vehicle was dropped from an altitude of in the Salto di Quirra range off Sardinia, Italy. The purpose of this test-drop was to validate the vehicle's water-landing system, including the subsonic parachute, flotation balloons, and beacon deployment. A small anomaly was encountered during the inflation of the balloons, however, all of the other systems performed as expected. Following the drop-test, the vehicle was retrieved for further analysis. On 23 June 2014, the recovery ship \"Nos Aries\" conducted a training exercise involving a single IXV test article off the coast of Tuscany.\n\nDuring June 2014, the IXV test vehicle arrived at the ESTEC Technical Centre in Noordwijk, the Netherlands, to undergo a test campaign to confirm its flight readiness in anticipation of a flight on a Vega rocket, which was by that point scheduled to occurring during November of that year.\n\nThe Intermediate eXperimental Vehicle (IXV) is a prototype unmanned reusable spaceplane —and the precursor of the next model called Space Rider. According to the ESA, the \"Intermediate\" part of its name is due to the shape of the vehicle not necessarily being representative of the envisioned follow-on production spacecraft. It possesses a lifting body arrangement which lacks wings of any sort; the size and shape is balanced between the need to maximise internal volume to accommodate experimental payloads while keeping within the mass limits of the Vega launcher and favourable centre of gravity. The vehicle purposefully includes several key technologies of interest to the ESA, including its thermal protection system and the presence of active aerodynamic control surfaces. Control and manoeuvrability of the IXV is provided by a combination of these aerodynamic surfaces (comprising a pair of movable flaps) and thrusters throughout its full flight regime, which includes flying at hypersonic speeds.\n\nA key role for the IXV is the gaining of data and experience in aerodynamically-controlled reentry, which has been claimed by the ESA to represent significant advance on earlier ballistic and quasi-ballistic techniques previously employed. Throughout each mission, representative reentry performance data is recorded in order to investigate aerothermodynamic phenomena and to validate system design tools and ground verification methods, which in turn supports future design efforts. Reentry is accomplished in a nose-high attitude, similar to the NASA-operated Space Shuttle; during this phase of flight, manoeuvring of the spaceplane is accomplished by rolling out-of-plane and then lifting in that direction, akin to a conventional aircraft. Landing is accomplished by an arrangement of parachutes, which are ejected during the descent through the top of the vehicle; additionally, seconds prior to landing, a series of airbags are inflate to soften the landing.\n\nAnother key ESA objective for the IXV was the verification of both its structure and its advanced thermal protection measures, specifically their performance during the challenging conditions present during reentry. The underside is covered by ceramic thermal protection panels composed of a blend of carbon fiber and silicon carbide directly fixed to the spaceplane's structure, while ablative materials comprising a cork and silicon-based composite material coat the vehicle's upper surfaces. The airframe was based on a traditional hot-structure/cold-structure arrangement; relying upon a combination of advanced ceramic and metallic assemblies, insulating materials, as well as the effective design of assorted attachments, junctions and seals; the role played by advanced navigation and control techniques was also deemed to be of a high importance. The IVX is supported on-orbit by a separate manoeuvring and support module, which is largely similar to the Resource Module that had been intended for use by the cancelled Hermes shuttle. The avionics of the IVX are controlled by a LEON2-FT microprocessor and are interconnected by a MIL-STD-1553B serial bus.\n\nAs an experimental vehicle primarily intended to gather data, various assorted sensors and monitoring equipment are present and operational throughout the full length of the flight in order to gather data to support the evaluation effort, including the verification of the vehicle's critical reentry technologies. The recorded data covers various elements of the IVX's flight, including its guidance, navigation and control systems, such as Vehicle Model Identification (VMI) measurements for post-flight reconstruction of the spacecraft's dynamic behaviour and environment, as well as the mandatory core experiments regarding its reentry technologies. Additionally, the IXV will typically carry complementary passenger experiments which, while not having been directly necessary to its mission success, serve to increase the vehicle's return on investment; according to the ESA, in excess of 50 such proposals had been received from a mixture of European industries, research institutes and universities, many having benefits to future launcher programmes (such as potential additional methods for guidance, navigation, control, structural health monitoring, and thermal protection) space exploration and scientific value. Throughout each mission, telemetry is broadcast to ground controllers to monitor the vehicle's progress; however, phenomenon such as the build-up of plasma around the spaceplane during its re-entry has been known to block radio signals.\n\nThe IXV is the precursor of the next model named Space Rider, also developed under the Italian PRIDE programme for ESA.\n\nDuring 2011, it was reported that the IEV was planned to conduct its maiden flight as early as 2013; however, the vehicle was later rescheduled to perform its first launch using the newly developed Vega launcher during late 2014. This initial launch window was ultimately missed due to unresolved range safety concerns.\n\nFollowing some delays, on 11 February 2015, the IXV was successfully launched into orbit by a Vega rocket as part of the VV04 mission. Having launched at 08:40am local time, the spaceplane separated from the Vega launch vehicle at 333 km altitude and ascended to 412 km, after which it commenced a controlled descent towards beginning its reentry at 120 km altitude, travelling at a recorded speed of 7.5 km/s, identical as to a typical re-entry path to be flown by low Earth orbit (LEO) spacecraft. Following re-entry, the IXV glided over the Pacific Ocean prior to the opening of its landing parachutes, which were deployed in order to slow down the craft's descent, having flown over 7300 km from the beginning of its reentry. The vehicle descended to the surface of Pacific Ocean, where it was subsequently recovered by the \"Nos Aries\" ship; analysis of both the spacecraft itself and recorded mission data took place. Jean-Jacques Dordain, then-director general of the ESA, stated of the mission: \"It couldn't have been better, but the mission itself is not yet over...it will move the frontiers of knowledge further back concerning aerodynamics, thermal issues, and guidance and navigation of such a vehicle — this lifting body\".\n\nFollowing on from the completion of the reportedly 'flawless' test flight, ESA officials decided that an additional test flight should be performed during the 2019-2020 timeframe. During this mission, the IXV is envisioned to land in a different manner, descending directly onto a runway instead of performing a splashdown landing as before; this approach is to be achieved either via the installation of a parafoil, or by the adoption of landing gear. The planning for the second spaceflight was originally to begin during March 2015, while design work on the modified vehicle was to commence during mid 2015.\n\nIn the ESA December 2016 Science Budget funding was approved by the Ministerial Council for the next IXV flight in the form of the commercialised Space Rider mini shuttle. Subject to design reviews in 2018 and 2019 a full size mockup will be dropped from balloon in 2019 and will have a first flight atop a Vega-C in 2020/2021 and then conduct approximately 5 science flights at 6-12 month intervals before becoming commercially available from 2025 at a cost of $40,000 per kg of payload for launch, operation and return to Earth. The Space Rider mini shuttle will have a length of between 4 and 5 meters, a payload capacity of 800 kg, a total mass of 2,400 kg and endurance of 2-6 month missions at a 400 km orbit before returning to Earth and being reflown within 4 months. The Vega-C rockets 4th stage payload dispenser AVUM acts as the service module for the shuttle providing orbital manoeuvring and braking, power and communications before being jettisoned for re-entry. The AVUM service module replaces the integrated IXV Propulsion Module and frees 0.8 m of internal space in the vehicle for a payload bay. The Space Rider is similar in shape and operation to the US Airforce X-37B but half the X37's length and a fifth the X37's mass and payload capacity making it the smallest and lightest spaceplane to ever fly. Payload doors will be opened on achieving orbit exposing instruments and experiments to space before being closed for landing.\n\n\n\n"}
{"id": "20033505", "url": "https://en.wikipedia.org/wiki?curid=20033505", "title": "Iron–hydrogen resistor", "text": "Iron–hydrogen resistor\n\nAn iron–hydrogen resistor consists of a hydrogen-filled glass bulb (similar to a light bulb), in which an iron wire is located. This resistor has a positive temperature coefficient of resistance. This characteristic made it useful for stabilizing circuits against fluctuations in power supply voltages. This device is often called a \"barretter\" because of its similarity to the barretter used for detection of radio signals. The modern successor to the iron–hydrogen resistor is the current source. \n\nWhen the current increases the temperature will increase. The higher temperature leads to a higher electrical resistance, opposing the increase in current. The hydrogen gas protects the iron against oxidation, and also enhances the effect since the solubility of hydrogen in iron increases as temperature increases, resulting in higher resistance.\n\nIron–hydrogen resistors were used in the early vacuum tube systems in series with the tube heaters, to stabilize the heater circuit current against fluctuating supply voltage. In 1930s Europe it was popular to combine them in the same glass envelope with an NTC-type thermistor made of UO until 1936, known as \"Urdox resistor\" and acting as an inrush current limiter for the series heater strings of domestic ac/dc tube radios.\n\n\n"}
{"id": "1431965", "url": "https://en.wikipedia.org/wiki?curid=1431965", "title": "L'Arrivée d'un train en gare de La Ciotat", "text": "L'Arrivée d'un train en gare de La Ciotat\n\nL'arrivée d'un train en gare de La Ciotat (translated from French into English as The Arrival of a Train at La Ciotat Station, Arrival of a Train at La Ciotat (US) and The Arrival of the Mail Train, and in the United Kingdom the film is known as Train Pulling into a Station) is an 1896 French short black-and-white silent documentary film directed and produced by Auguste and Louis Lumière. Contrary to myth, it was not shown at the Lumières' first public film screening on 28 December 1895 in Paris, France: the programme of ten films shown that day makes no mention of it. Its first public showing took place in January 1896.\n\nThis 50-second silent film shows the entry of a train pulled by a steam locomotive into the gare de La Ciotat, the train station of the French coastal town of La Ciotat. Like most of the early Lumière films, \"L'arrivée d'un train en gare de La Ciotat\" consists of a single, unedited view illustrating an aspect of everyday life. There is no apparent intentional camera movement, and the film consists of one continuous real-time shot.\n\nThis 50-second movie was filmed in La Ciotat, Bouches-du-Rhône, France. It was filmed by means of the Cinématographe, an all-in-one camera, which also serves as a printer and film projector. As with all early Lumière movies, this film was made in a 35 mm format with an aspect ratio of 1.33:1.\n\nThe film is associated with an urban legend well known in the world of cinema. The story goes that when the film was first shown, the audience was so overwhelmed by the moving image of a life-sized train coming directly at them that people screamed and ran to the back of the room. Hellmuth Karasek in the German magazine \"Der Spiegel\" wrote that the film \"had a particularly lasting impact; yes, it caused fear, terror, even panic.\" However, some have doubted the veracity of this incident such as film scholar and historian in his essay, \"Lumiere's Arrival of the Train: Cinema's Founding Myth\". Others such as theorist Benjamin H. Bratton have speculated that the alleged reaction may have been caused by the projection being mistaken for a camera obscura by the audience which at the time would have been the only other technique to produce a naturalistic moving image. Whether or not it actually happened, the film undoubtedly astonished people unaccustomed to the illusion created by moving images.\n\nThis film is interesting because it contains the first example of several common cinematic techniques: camera angle, long shot, medium shot, close-up, and forced perspective.\n\nIt is evident from their films, taken as a whole, that the Lumière brothers knew what the effect of their choice of camera placement would be. They placed the camera on the platform to produce a dramatic increase in the size of the arriving train. The train arrives from a distant point and bears down on the viewer, finally crossing the lower edge of the screen.\n\nA significant aspect of the film is that it illustrates the use of the long shot to establish the setting of the film, followed by a medium shot, and then a close-up. As the camera is static for the entire film, the effect of these various \"shots\" is achieved by the movement of the subject alone. Nonetheless, it is these different types of shots are clearly illustrated here, and later film makers moved their cameras to achieve these shots.\n\nWhat most film histories leave out is that the Lumière Brothers were trying to achieve a 3D image even prior to this first-ever public exhibition of motion pictures. Louis Lumière eventually re-shot \"L'Arrivée d’un Train\" with a stereoscopic film camera and exhibited it (along with a series of other 3D shorts) at a 1935 meeting of the French Academy of Science. Given the contradictory accounts that plague early cinema and pre-cinema accounts, it's plausible that early cinema historians conflated the audience reactions at these separate screenings of \"L'Arrivée d’un Train\". The intense audience reaction fits better with the latter exhibition, when the train apparently \"was\" actually coming out of the screen at the audience. But due to the fact that the 3D film never took off commercially as the conventional 2D version did, including such details would not make for a compelling myth.\n\nThe short has been featured in a number of film collections including \"Landmarks of Early Film volume 1\". A screening of the film was depicted in the 2011 film \"Hugo\". The scene of the train pulling in was placed at #100 on Channel 4's two-part documentary \"The 100 Greatest Scary Moments\".\n\nIn March 2017, the film was encoded into DNA by Yaniv Erlich and Dina Zielinski.\n\n"}
{"id": "370465", "url": "https://en.wikipedia.org/wiki?curid=370465", "title": "Limitations on exclusive rights: Computer programs", "text": "Limitations on exclusive rights: Computer programs\n\nLimitations on exclusive rights: Computer programs is the title of the current form of section 117 of the U.S. Copyright Act (17 U.S.C. § 117). In United States copyright law, it provides users with certain adaptation rights for computer software that they own.\n\nThe current form of section 117 is the result of a recommendation by CONTU, the National Commission on New Technological Uses of Copyrighted Works. The U.S. Congress established CONTU to study and make recommendations on modifying the 1976 Copyright Act to deal with new technologies, particularly computer software, that Congress had not addressed when it passed the 1976 Act. CONTU operated from 1975 to 1978, and its principal recommendation to Congress was to revise the wording of section 117. Its report stated:\n\nBecause the placement of a work into a computer is the preparation of a copy, the law should provide that persons in rightful possession of copies of programs be able to use them freely without fear of exposure to copyright liability. Obviously, creators, lessors, licensors, and vendors of copies of programs intend that they be used by their customers, so that rightful users would but rarely need a legal shield against potential copyright problems. It is easy to imagine, however, a situation in which the copyright owner might desire, for good reason or none at all, to force a lawful owner or possessor of a copy to stop using a particular program. One who rightfully possesses a copy of a program, therefore, should be provided with a legal right to copy it to that extent which will permit its use by that possessor. This would include the right to load it into a computer and to prepare archival copies of it to guard against destruction or damage by mechanical or electrical failure. But this permission would not extend to other copies of the program. Thus, one could not, for example, make archival copies of a program and later sell some while retaining some for use. The sale of a copy of a program by a rightful possessor to another must be of all rights in the program, thus creating a new rightful possessor and destroying that status as regards the seller.\n\nThe revisions recommended by CONTU were approved with one important change. Instead of \"rightful possessor\" of a computer program Congress used the word \"owner\" of a computer program. It is not clear why this change was made. This one change resulted in a state of affairs in which software vendors began to take the position that customers do not own their software but rather only \"license\" it. The courts have split on whether the assertion in software agreements that the customer does not own the software, and has only a right to use it in accordance with the license agreement, is legally enforceable.\n\nSection 117 is a limitation on the rights granted to holders of copyright on computer programs. The limitation allows the owner of a particular copy of a copyrighted computer program to make copies or adaptations of the program for any of several reasons:\n\n\nThe law allows any copies that are created for the above purposes to be transferred when the software is sold, only along with the copy made to prepare them. Adaptations made can not be transferred without permission from the copyright holder.\n\nWhile it is not part of section 117, it is also lawful to reverse engineer software for compatibility purposes. Sec. 103(f) of the DMCA (17 U.S.C. § 1201 (f)) says that a person who is in legal possession of a program, is permitted to reverse-engineer and circumvent its protection against copying if this is necessary in order to achieve \"interoperability\" - a term broadly covering other devices and programs being able to interact with it, make use of it, and to use and transfer data to and from it, in useful ways. A limited exemption exists that allows the knowledge thus gained to be shared and used for interoperability purposes.\n\nMore generally, it has been held that reverse engineering is a fair use. In \"Sega v. Accolade\", the Ninth Circuit held that making copies in the course of reverse engineering is a fair use, when it is the only way to get access to the \"ideas and functional elements\" in the copyrighted code, and when \"there is a legitimate reason for seeking such access.\"\n\n\n"}
{"id": "47808473", "url": "https://en.wikipedia.org/wiki?curid=47808473", "title": "Link Electronics", "text": "Link Electronics\n\nLink Electronics Ltd. was a major UK industrial and broadcast television equipment manufacturer and systems integrator in the 1970s and 1980s. The company was founded by John Tanner and David Mann, who began manufacturing television cameras in 1966.\nLink was mainly known for its range of broadcast television cameras, but was also a manufacturer of outside broadcast (OB) vehicles, including the famous BBC \"Type 5\". Link also produced a wide range of ancillary studio equipment, such as distribution amplifiers, measuring sets and test signal generators.\n\nLink started as an industrial camera manufacturer but soon moved into broadcast equipment when the BBC approached it to develop a successor to the commercially successful EMI 2001, when EMI's own design for the 2001's successor, the 2005, failed to meet expected standards when launched around 1975. The poor performance of this camera considering its development cost led to EMI exiting the broadcast camera industry. A similar fate befell Link around 10–15 years later upon the release of the Link 130 (further down this page).\n\nThe Type 109 was a broadcast quality black and white camera mainly used as a caption scanner or simple tele-cine.\n\nThe Link-NEC 100 was the companion camera to the Type 130 and designed in conjunction with NEC. It had a triax interface unit and could be used stand alone, via a radio link or with a CCU via triax cable. it shared a common architecture with the 130 by using the same 18mm tubes and both where fully automatic for set-up and used the same CCU (Camera Control Unit), OCP (operational Control Unit) and MSU (Master Setup Unit) \n\nThe 110 was Link's first attempt at a colour broadcast camera and around 200 cameras were manufactured. Styling was based on the famous EMI 2001 colour camera but at an economical price, including what some claim to be a very flimsy casing that was not of rugged design.\nThe camera consisted of a closed body and an internal lens from a range of manufacturers, like the EMI 2001, leading to similar claims that the design was \"boxy\". The camera was capable of both studio and outside broadcast use and at the BBC found its way into TC6, TC8 and several presentation studios at Television Centre, BBC Lime Grove, BBC Bristol, the Open University studio in Milton Keynes, BBC Wood Norton Training Centre and onto several BBC outside broadcast vehicles. ITV company Thames Television also used them at their Euston road studios from around 1979, replacing their Marconi MkVIIs (with RCA TK-47s at Teddington a few years later replacing the EMI 2001s there).\nThe 110 was seen as a good, cheap, modern option which was lightweight and easier to carry compared to the much older and heavier EMI 2001 camera.\n\nThe Link 120 was a portable camera system that consisted of the camera head and a portable electronics box that then connected to the CCU via standard TV36 multicore cable. The Type 120 was intended to be able to 'clip' into a studio frame to allow easy conversion from either a handheld camera on location or to a large lens camera at sports events in studio. An advert in the Guild of Television Cameramen magazine ZERB notes that there are three options. the Type 121 shoulder mount (with long 3 inch viewfinder) Type 122 with 3 inch viewfinder, Type 123 in sports camera body and 5 inch viewfinder and the Type 124 with full studio lens and 7 inch viewfinder. The Link 125 camera was the next step to purpose designed studio camera.\n\nThe Link 125 camera was purchased in quantity by the BBC and deployed to most of the studios at Television Centre, Pebble Mill in Birmingham and Belfast as well as several other BBC studios. It was also the camera of choice at Limehouse Television. In addition, ITV company Television South (TVS) used the model in its Maidstone studios, which were still in use by those studios when they were sold off as an independent studio facility following the loss of TVS's franchise at the end of 1992.\nThe 125 was a well thought out and well-built studio and OB camera developed from the Link 120 portable camera system. It also contained a comprehensive communications system and used a camera control unit (CCU) based around the 110's but updated with auto black, white, iris and centering functions.\nThe BBC preferred to use a mid range Schneider-Kreuznach lens as it gave good zoom angles. Some believed it produced one of the best images for a pickup tube camera, others believed it could never match the quality of the EMI 2001. Many BBC users felt that the image was soft and not easy to focus, it also produced an unusual image effect that formed the shape of a 'teardrop'\nThe Link 125 was the most successful of all of Links cameras and the last studio to use them was BBC Pebble Mill which decommissioned the last four in 1994 from the news studio having upgraded to Sony BVP-375's and Ikegami HL-55's a few years before. These cameras have become collectable vintage items. \n\nThe 130 was designed in the mid 1980s as a high tech modern camera with aspirations of quality and high tech design through the use of microprocessors for full auto setup. Unfortunately it soon became apparent that there were several hardware and/or software errors made during the design and manufacture of this model, which seriously affected the manufacturer's reputation with its broadcast customer base, much in the same way the 2005 had done for EMI around 15 years before.\nThis was compounded by competition from high quality camera systems from Japanese manufacturers including Sony, Ikegami and Hitachi and by the Dutch manufacturer Philips. Also at this time RCA had started to use CCDs in its cameras which produced what was considered a superior picture to the then prevailing technology of camera pickup tubes, which necessitated not so many regular adjustments in their setup procedures.\n\nIn the mid-1980s the BBC was designing the Type 6 Scanner and had chosen the Link 100 and 130 and had several camera channels for testing. Prototypes had been sent to Israel, Sweden and Australia. The BBC Link 130 cameras were initially installed in Elstree Studio A along with some NEC MNC-100 lightweight cameras. Sadly, they proved to be very unreliable. They had been around for a few years in development and the idea was to use studio A and BBC Glasgow as test beds to try to make them work. (John Wardle (Head of BBC Engineering) stated that 42 130s were ordered but only 11 delivered - BBC Glasgow was the studio to receive the first batch, much to the frustration of those who used them.)\n\nUnfortunately as the camera suffered from poor design and many software bugs (that would leave the camera's automatic lineup software switched on and the cue light happening to cause picture interference), the BBC engineering department felt that these faults wouldn't be fixed quickly and opted for Thomson Cameras. As a result, this act allegedly caused the final collapse of Link, which was declared bankrupt. UEI Group then briefly purchased the Company, after investing many hundreds of thousands of pounds in the development of the camera only for the poor design not being rectified in the design or prototype stages giving Link a poor reputation after several technically and commercially successful models. The company then transferred to the ownership of Quantel.\n\nThere are several units in private collections and until the early 2000s several casings were to be seen at the National Science and Media Museum in Bradford as part of a public display.\nLink produced a full range of sync pulse generators, colour bar generators, video distribution amps and various other pieces of industrial and broadcast television equipment.\n\nIncluding -\n\nA Plumbicon tube condition tester\n\nA Colour Syncrohnising Pulse Generator including mixed sysnc and blanking, line and field, PAL Subcarrier and PAL ident\n\nA PAL Ecncoder with picture input and three composite outputs. Optional modules could produce Black and Burst signal and Colour Bars.\n\n8 Module rack from and space for a 260 Series PSU.\n\nWorked with the 251 Sync Generator.\n\n260 Ramp Generator\n261 RGB Colour Bar Generator\n262 PAL Black and Burst Generator\n263 Grille and Dot Generator\n264 Step Staircase Generator\n265 PLUGE Generator\n268 Phase Locked Sweep Generator\n271 6In to 1Out Selector panel\n272 PAL burst Adder\n\nA universally mains powered video/sync distribution amp with six individual isolated outputs. A rack frame was available to contain up to 8 modules\n\n290 Single Channel Delay and 291 Triple channel Delay that where housed in the 280 type chassis.\n\nGenerates a 5 step Staircase pattern, Peak White and Black signals and PAL sub-carriers.\n\nA devise for checking the signal quality of a video signal\n\nLink 700 series intercomm where custom built systems using computerized FET cross-point switching with selector panels and feeds to and from VT's and cameras.\n\nSpecially designed for EFP (Electronic Field Production) applications. It consisted of three control panel's (Director, Vision Engineer and Sound Engineer) which had access to each other as well as a maximum of four cameras and an auxiliary feed (production sound).\n\n760 1In - 8Out Audio Distribution amplifier\n762 Rear Connector Assembly\n\nA telephone exchange system capable of working with up to 10n external lines and up to five operating positions, each position having a keypad with an illuminated push button for each line and a buzzer that sounded for 2 seconds when an incoming call was received.\n\nPYE TVT had supplied the BBC with its television OB vehicles from the mid 1960s until the early 1980s. When the Type 5 was being designed Link offered a design that provided a full facility unit with up to eight cameras (although BBC OBs chose the Philips LDK-5, not a Link camera) at least 20 were supplied to the BBC and five to ITV and foreign customers. One Type 5 (London 6) was donated to the Science Museum.\n\nFollowing its bankruptcy, UK digital broadcast equipment manufacture Quantel, famous for the Paintbox graphics system, acquired the company and used it to develop a facilities integration and design company known as Quantel-Link. This company continued to design and manufacture outside broadcast vehicles to many national broadcasters including YTE in Finland as well as companies in France, Germany and other European broadcasters.\n\n"}
{"id": "2923328", "url": "https://en.wikipedia.org/wiki?curid=2923328", "title": "Locks-and-keys", "text": "Locks-and-keys\n\nLocks-and-keys is a solution to dangling pointers in computer programming languages.\n\nThe locks-and-keys approach represents pointers as ordered pairs (key, address) where the key is an integer value. Heap-dynamic variables are represented as the storage for the variable plus a cell for an integer lock value. When a variable is allocated, a \"lock value\" is created and placed both into the variable's cell and into the pointer's key cell. Every access to the pointer compares these two values, and access is allowed only if the values match.\n\nWhen a variable is deallocated, the key of its pointer is modified to hold a value different from the variable's cell. From then on, any attempt to dereference the pointer can be flagged as an error. Since copying a pointer also copies its cell value, changing the key of the ordered pair safely disables all copies of the pointer.\n\n"}
{"id": "1663053", "url": "https://en.wikipedia.org/wiki?curid=1663053", "title": "MIMOS", "text": "MIMOS\n\nMIMOS Berhad (or MIMOS) is a research and development centre in Kuala Lumpur, Malaysia under purview of the Malaysian Ministry of Science, Technology and Innovation (MOSTI).\n\nThe company was founded as the Malaysian Institute of Microelectronic Systems in 1985.\n\n"}
{"id": "19775547", "url": "https://en.wikipedia.org/wiki?curid=19775547", "title": "Ministry of Science, Technology and Higher Education", "text": "Ministry of Science, Technology and Higher Education\n\nThe Ministry of Science, Technology and Higher Education ( or \"MCTES\") is a Portuguese government ministry.\n\n"}
{"id": "32302176", "url": "https://en.wikipedia.org/wiki?curid=32302176", "title": "Nanochannel glass materials", "text": "Nanochannel glass materials\n\nNanochannel glass materials are an experimental mask technology that is an alternate method for fabricating nanostructures, although optical lithography is the predominant patterning technique.\n\nNanochannel glass materials are complex glass structures containing large numbers of parallel hollow channels. In its simplest form, the hollow channels are arranged in geometric arrays with packing densities as great as 10 channels/cm. Channel dimensions are controllable from micrometers to tens of nanometers, while retaining excellent channel uniformity. Exact replicas of the channel glass can be made from a variety of materials. This is a low cost method for creating identical structures with nanoscale features in large numbers.\n\nThese materials have high density of uniform channels with diameters from 15 micrometres to 15 nanometers. These are rigid structures with serviceable temperatures to at least 300 °C, with potential up to 1000 °C. Furthermore, these are optically transparent photonic structures with high degree of reproducibility.\n\nThese can be used as a material for chromatographic columns, unidirectional conductors, Microchannel plate and nonlinear optical devices. Other uses are as masks for semiconductor development, including ion implantation, optical lithography, and reactive ion etching.\n\n\n"}
{"id": "20485972", "url": "https://en.wikipedia.org/wiki?curid=20485972", "title": "OSAMI-E", "text": "OSAMI-E\n\nThe research project OSAMI-E is the Spanish subproject of the European ITEA \"2\" project OSAMI (Open Source AMbient Intelligence).\n\nThe aim of the international project OSAMI is the design of a basic, widely applicable SOA-oriented component platform, its development, test and its provision as open source software. The project consists of a number of national sub-projects, each focussing on a certain field of application.\n\nOSGi and Web Services forms the technical basis of the OSAMI platform in order to implement distributed, dynamically configurable, vendor-neutral and device-independent solutions.\n\nThe Spanish sub-project OSAMI-E, funded by the Spanish Ministry of Industry, Tourism and Commerce, contributes to different transversal areas such as engineering, architecture, tools and security and with demonstrators in the fields of education, geographic information systems, sensor networks, digital home and mobile services.\n\n\nThe main objective of OSAMI is to connect technologically vertical markets on the basis of an open platform and, hence, to facilitate the market entry for small and medium-sized enterprises (SME). \n\n\n\n\n"}
{"id": "37043632", "url": "https://en.wikipedia.org/wiki?curid=37043632", "title": "OU Campus", "text": "OU Campus\n\nOU Campus is a web content management system (CMS) for colleges, universities, and other higher education institutions. \n\nOU Campus was launched in 1999 using the cloud computing, software as a service (SaaS) product delivery model. It was developed by OmniUpdate, a privately owned company headquartered in Camarillo, CA. OmniUpdate was started by software developers Tom Nalevanko, Lance Merker, and Yves Lempereur (developer of BinHex 4.0).\n\nThe product uses a push technology. OU Campus can be accessed via the cloud - also known as software as a service (SaaS). The underlying platform of OU Campus is developed in Java, and uses XSLT 3.0 for template transformation. OU Campus uses open standards, including XML, HTML, and CSS and is extensible through APIs. As a decoupled system, OU Campus works with any server-side technology (e.g., PHP, ASP, .NET, ColdFusion) and for templates uses XHTML/HTML5 and CSS, as well as XML/XSL. The templates are customizable so that developers can create sites using popular web development techniques including Responsive web design to adapt to all screen sizes and devices. OU Campus does not store page content in a database; rather, each page is stored as an XML file on the CMS server. It is compatible with Internet Explorer, Firefox, Google Chrome, and Safari. OU Campus' push technology architecture requires a separate database for some applications that use interactive content such as forms and surveys. These applications can be handled through the Live Delivery Platform (LDP), which provides a server-side module for plug-in application architecture. LDP is accessed and managed through OU Campus.\n\n\n"}
{"id": "4687372", "url": "https://en.wikipedia.org/wiki?curid=4687372", "title": "OpenStax CNX", "text": "OpenStax CNX\n\nOpenStax CNX, formerly called Connexions, is a global repository of educational content provided by volunteers. The open source platform is provided and maintained by OpenStax, which is based at Rice University. The collection is available free of charge, can be remixed and edited, and is available for download in various digital formats.\n\nFounded as Connexions in 1999 by Richard Baraniuk, OpenStax CNX is based on the philosophy that scholarly and educational content can and should be shared, re-used and recombined, interconnected and continually enriched. As such, it was one of the first Open Educational Resources (OER) initiatives along with projects such as MIT OpenCourseWare and the Public Library of Science. The materials in Connexions are available under a CC BY Creative Commons license, which means that content can be used, adapted, and remixed, as long as attribution is provided.\n\nOpenStax CNX contains educational materials at all levels—from children to college students to professionals—organized in small modules (pages) that can be connected into larger collections (books). Material is authored by people from all walks of life. Much content is created by university professors, but the collection also contains very popular music content created by a part-time music teacher.\n\nOpenStax CNX material is translated into many languages, aided by the open-content licensing.\n\nTo ensure the legal reusability of content, OpenStax CNX requires authors to license materials they publish under the Creative Commons Attribution License (presently, version 4.0). Under this license, the author retains the right to be credited (attributed) wherever the content is reused. The author grants others the right to copy, distribute, and display the work, and to derive works based on it, as long as the author is credited.\n\n\nThree key factors enable the collaborative environment in OpenStax CNX:\n\nThe Connexions project was started in 1999 and initially supported by individuals and Rice University. That support has been substantially supplemented by grants from the William and Flora Hewlett Foundation.\n\n\n"}
{"id": "50585274", "url": "https://en.wikipedia.org/wiki?curid=50585274", "title": "Pakistan-China Fiber Optic Project", "text": "Pakistan-China Fiber Optic Project\n\nThe Pakistan-China Fiber Optic Project is a 820 kilometer long optical fiber cable laid down between the Khunjerab Pass on the China-Pakistan border and the city of Rawalpindi. It has been constructed as part of the China–Pakistan Economic Corridor at an estimated cost of $44 million. Groundbreaking on the project took place on May 19, 2016 in the city of Gilgit. The line was first envisaged in 2009, with Pakistan and China signing an agreement in 2013 to implement the project . However the project was not implemented until being included as part of the China Pakistan Economic Corridor. Placement of the line will take an estimated 2 years, and will bring 3G and 4G connectivity to the Gilgit-Baltistan region. \n\nThe line will connect the Transit Europe-Asia Terrestrial Cable Network with that of Pakistan, which currently transmits its telecom and internet traffic through four undersea fiber optic cables, with another three undersea fiber optic cables under construction.\n\n466.54 kilometers of the route will be located in Gilgit-Baltistan, while 287.66 kilometers will be laid in Khyber Pakhtunkhwa province, 47.56 kilometers will be in Punjab province, and 18.2 kilometers in Islamabad Capital Territory.\nThe project is financed by the Exim Bank of China at a concessionary interest rate of 2%, versus the 1.6% typical of other CPEC infrastructure projects.\n\nThe project has been completed and inaugurated in July 2018.\n"}
{"id": "8778209", "url": "https://en.wikipedia.org/wiki?curid=8778209", "title": "Pastry blender", "text": "Pastry blender\n\nA pastry blender is a cooking utensil used to mix a hard (solid) fat into flour in order to make pastries. The tool is usually made of narrow metal strips or wires attached to a handle, and is used by pressing down on the items to be mixed (known as \"cutting in\"). It is also used to break these fats (shortening, butter, lard) into smaller pieces. The blending of fat into flour at this stage impacts the amount of water that will be needed to bind the pastry into a dough.\n\nThere are several alternatives to using a pastry blender. One is to work the fat into the flour with the fingertips, though this requires having a good \"touch\" and knowing just how lightly to work the fat into the flour without melting it. Another is to use two table knives like a pair of scissors, one knife in each hand. A third is to use a food processor.\n\n"}
{"id": "9606667", "url": "https://en.wikipedia.org/wiki?curid=9606667", "title": "Pervious concrete", "text": "Pervious concrete\n\nPervious concrete (also called porous concrete, permeable concrete, no fines concrete and porous pavement) is a special type of concrete with a high porosity used for concrete flatwork applications that allows water from precipitation and other sources to pass directly through, thereby reducing the runoff from a site and allowing groundwater recharge.\n\nPervious concrete is made using large aggregates with little to no fine aggregates. The concrete paste then coats the aggregates and allows water to pass through the concrete slab. Pervious concrete is traditionally used in parking areas, areas with light traffic, residential streets, pedestrian walkways, and greenhouses. It is an important application for sustainable construction and is one of many low impact development techniques used by builders to protect water quality.\n\nPervious concrete was first used in the 1800s in Europe as pavement surfacing and load bearing walls. Cost efficiency was the main motive due to a decreased amount of cement. It became popular again in the 1920s for two storey homes in Scotland and England. It became increasingly viable in Europe after WWII due to the scarcity of cement. It did not become as popular in the US until the 1970s.\nIn India it became popular in 2000.\n\nThe proper utilization of pervious concrete is a recognized Best Management Practice by the U.S. Environmental Protection Agency (EPA) for providing first flush pollution control and stormwater management. As regulations further limit stormwater runoff, it is becoming more expensive for property owners to develop real estate, due to the size and expense of the necessary drainage systems. Pervious concrete lowers the sites SCS Curve Number by retaining stormwater on site. This allows the planner/designer to achieve pre-development stormwater goals for pavement intense projects. Pervious concrete reduces the runoff from paved areas, which reduces the need for separate stormwater retention ponds and allows the use of smaller capacity storm sewers. This allows property owners to develop a larger area of available property at a lower cost. Pervious concrete also naturally filters storm water and can reduce pollutant loads entering into streams, ponds and rivers.\n\nPervious concrete functions like a storm water infiltration basin and allows the storm water to infiltrate the soil over a large area, thus facilitating recharge of precious groundwater supplies locally. All of these benefits lead to more effective land use. Pervious concrete can also reduce the impact of development on trees. A pervious concrete pavement allows the transfer of both water and air to root systems allowing trees to flourish even in highly developed areas.\n\nPervious concrete consists of cement, coarse aggregate and water with little to no fine aggregates. The addition of a small amount of sand will increase the strength. The mixture has a water-to-cement ratio of 0.28 to 0.40 with a void content of 15 to 25 percent.\n\nThe correct quantity of water in the concrete is critical. A low water to cement ratio will increase the strength of the concrete, but too little water may cause surface failure. A proper water content gives the mixture a wet-metallic appearance. As this concrete is sensitive to water content, the mixture should be field checked. Entrained air may be measured by a Rapid Air system, where the concrete is stained black and sections are analyzed under a microscope.\n\nA common flatwork form has riser strips on top such that the screed is 3/8-1/2 in. (9 to 12 mm) above final pavement elevation. Mechanical screeds are preferable to manual. The riser strips are removed to guide compaction. Immediately after screeding, the concrete is compacted to improve the bond and smooth the surface. Excessive compaction of pervious concrete results in higher compressive strength, but lower porosity (and thus lower permeability).\n\nJointing varies little from other concrete slabs. Joints are tooled with a rolling jointing tool prior to curing or saw cut after curing. Curing consists of covering concrete with 6 mil. plastic sheeting within 20 minutes of concrete discharge. However, this contributes to a substantial amount of waste sent to landfills. Alternatively, preconditioned absorptive lightweight aggregate as well as internal curing admixture (ICA) have been used to effectively cure pervious concrete without waste generation.\n\nPervious concrete has a common strength of though strengths up to can be reached. There is no standardized test for compressive strength. Acceptance is based on the unit weight of a sample of poured concrete using ASTM standard no. C1688. An acceptable tolerance for the density is plus or minus of the design density. Slump and air content tests are not applicable to pervious concrete because of the unique composition. The designer of a storm water management plan should ensure that the pervious concrete is functioning properly through visual observation of its drainage characteristics prior to opening of the facility.\n\nConcerns over the resistance to the freeze-thaw cycle have limited the use of pervious concrete in cold weather environments. The rate of freezing in most applications is dictated by the local climate. Entrained air may help protect the paste like in normal concrete. The addition of a small amount of fine aggregate to the mixture increases the durability of the pervious concrete. Avoiding saturation during the freeze cycle is the key to the longevity of the concrete. Related, having a well prepared 8 to 24 inch (200 to 600 mm) sub-base and drainage will reduce the possibility of freeze-thaw damage.\n\nTo prevent reduction in permeability, pervious concrete needs to be cleaned regularly. Cleaning can be accomplished through wetting the surface of the concrete and vacuum sweeping.\n\n\n"}
{"id": "9489179", "url": "https://en.wikipedia.org/wiki?curid=9489179", "title": "Pocasset Manufacturing Company", "text": "Pocasset Manufacturing Company\n\nPocasset Manufacturing Company was a cotton textile mill located in Fall River, Massachusetts. It was located just west of Main Street across the second falls of the Quequechan River. It was organized on August 15, 1821, with $100,000 in capital. The mill began operation in 1822, with Samuel Rodman of New Bedford as the principal owner. Oliver Chace, served as the mill's agent until 1837. Nathaniel Briggs Borden was named clerk and treasurer.\n\nThe Pocasset Mills were the site of the origin of the Great Fall River Fire of 1928. The mills were destroyed along with a vast portion of the city's business district.\n\nIn the company's early years, it primarily constructed mills for their various tenants. The first development scheme was intended to enlarge the grist mill, but with Oliver Chace, the grist mill was razed for the erection of a new mill for the production of textiles. The old fulling mill remained. Referred to as the \"Bridge Mill\", the new mill was constructed from stone with three stories, and measured 100 feet long by 40 feet wide, with a large ell. It was located west of Main Street, immediately north of the Fall River stream.\n\nIn 1821, the machinery firm of Harris, Hawes & Company occupied two floors of a building put up for their use by the Pocasset Company. The basement was still used as a grist mill and they also built a water-wheel to raise the water to a convenient level for laundry. The \"Bridge Mill\" and the fulling mill were destroyed in the Great Fire of 1843. The company built the original Granite Block here a few years after the fire. It became the center of business activity in Fall River for many years.\n\nIn 1825, the stone Satinet Mill was erected. It was partly occupied by the first calico printing business in Fall River, set up by Andrew Robeson. The south half was occupied by John and Jesse Eddy for the manufacture of satinets. It was made 3-5 story building made of Fall River granite. It was demolished in 1846 for construction of the new Pocasset Mill #1.\n\nWest of the printing mill, the Quequechan Mill was built in 1826. It was called the \"New Pocasset\" and was leased for a yarn mill. Considered an extremely large mill for its time, it was a five story stone building that was 319 ft long and 48 feet wide. It contained 16,392 spindles and 492 looms. It was in the Quequechan Mill that Holder Borden set up the cloth printing business that would later become the American Print Works. This mill was demolished about 1880 for expansion of Pocasset Mills #2 and #3.\n\nAlso in 1825, the Watuppa Reservoir Company, was incorporated under a special statute by the Massachusetts and Rhode Island state legislature. It was authorized to make reservations of water in the ponds by erecting a dam to raise the water by two feet. Nathaniel Briggs Borden as a member of this private entity, enabled Pocasset Manufacturing to take advantage of the increase in the river flow speed to allow them to build more mills for lease. Previously use of land and waterfalls were controlled by Troy Cotton & Woolen Manufactory. The Watuppa Reservoir Company built a dam below the Troy Company dam.\n\nIn 1826 the Pocasset constructed still another stone building which was known as the Massasoit Mill and later called the Watuppa Mill. This mill had 9,000 spindles, 224 looms This mill was later operated as Pocasset Mill #4. It was destroyed by fire along with the main plant in February 1928.\n\nIn 1847, the Pocasset Mill #1 commenced running. Built on the site of the Satinet Mill, it was a five story stone building that was 208 ft long and 75 feet wide. It was the first of the so-called \"wide\" mills and contained 20,352 spindles and 422 looms. The machinery was run by three turbine wheels, which were later supplemented by a Corliss engine fed by a steam plant with eighteen condensers. The building had its own fire apparatus including pumps and sprinklers. The Pocasset Mill made sheetings and shirtings.\n\nBy 1877, the company employed 550 and owned fifty four tenements. The number of stockholders increased to twenty one. In 1899, a complete electric light plant was installed. In 1905 the Pocasset replaced 16,000 mule spinners with 13,000 frame spindles. That same year, the company also acquired the mill of the Fall River Manufacturing Company, operating it as Mill #5. By 1917, the Poccaset Manufacturing Company was capitalized at $1,200,000 and had a capacity of 123,000 spindles and 2,874 looms. It produced sateens, twills and plain cloths.\n\nThe mills operated until 1926, and were destroyed by fire in 1928 during dismantling. The site was later occupied by a bus terminal and parking lot until the early 1960s, when the property was taken by the Commonwealth of Massachusetts for construction of Interstate 195. A portion of the property of the Pocasset Manufacturing Company is now occupied by the Fall River Chamber of Commerce.\n\n"}
{"id": "18648165", "url": "https://en.wikipedia.org/wiki?curid=18648165", "title": "Process corners", "text": "Process corners\n\nIn semiconductor manufacturing, a process corner is an example of a design-of-experiments (DoE) technique that refers to a variation of fabrication parameters used in applying an integrated circuit design to a semiconductor wafer. Process corners represent the extremes of these parameter variations within which a circuit that has been etched onto the wafer must function correctly. A circuit running on devices fabricated at these process corners may run slower or faster than specified and at lower or higher temperatures and voltages, but if the circuit does not function at all at any of these process extremes the design is considered to have inadequate design margin.\n\nIn order to verify the robustness of an integrated circuit design, semiconductor manufacturers will fabricate corner lots, which are groups of wafers that have had process parameters adjusted according to these extremes, and will then test the devices made from these special wafers at varying increments of environmental conditions, such as voltage, clock frequency, and temperature, applied in combination (two or sometimes all three together) in a process called characterization. The results of these tests are plotted using a graphing technique known as a shmoo plot that indicates clearly the boundary limit beyond which a device begins to fail for a given combination of these environmental conditions.\n\nCorner-lot analysis is most effective in digital electronics because of the direct effect of process variations on the speed of transistor switching during transitions from one logic state to another, which is not relevant for analog circuits, such as amplifiers.\n\nIn Very-Large-Scale Integration (VLSI) integrated circuit microprocessor design and semiconductor fabrication, a process corner represents a three or six sigma variation from nominal doping concentrations (and other parameters) in transistors on a silicon wafer. This variation can cause significant changes in the duty cycle and slew rate of digital signals, and can sometimes result in catastrophic failure of the entire system.\n\nVariation may occur for many reasons, such as minor changes in the humidity or temperature changes in the clean-room when wafers are transported, or due to the position of the die relative to the center of the wafer.\n\nWhen working in the schematic domain, we usually only work with front end of line (FEOL) process corners as these corners will affect the performance of devices. But there is an orthogonal set of process parameters that affect back end of line (BEOL) parasitics.\n\nOne naming convention for process corners is to use two-letter designators, where the first letter refers to the N-channel MOSFET (NMOS) corner, and the second letter refers to the P channel (PMOS) corner. In this naming convention, three corners exist: typical, fast and slow. Fast and slow corners exhibit carrier mobilities that are higher and lower than normal, respectively. For example, a corner designated as FS denotes fast NFETs and slow PFETs.\n\nThere are therefore five possible corners: typical-typical (TT) (not really a corner of an n vs. p mobility graph, but called a corner, anyway), fast-fast (FF), slow-slow (SS), fast-slow (FS), and slow-fast (SF). The first three corners (TT, FF, SS) are called even corners, because both types of devices are affected evenly, and generally do not adversely affect the logical correctness of the circuit. The resulting devices can function at slower or faster clock frequencies, and are often binned as such. The last two corners (FS, SF) are called \"skewed\" corners, and are cause for concern. This is because one type of FET will switch much faster than the other, and this form of imbalanced switching can cause one edge of the output to have much less slew than the other edge. Latching devices may then record incorrect values in the logic chain.\n\nIn addition to the FETs themselves, there are more on-chip variation (OCV) effects that manifest themselves at smaller technology nodes. These include process, voltage and temperature (PVT) variation effects on on-chip interconnect, as well as via structures.\n\nExtraction tools often have a nominal corner to reflect the nominal cross section of the process target. Then the corners cbest and cworst were created to model the smallest and largest cross sections that are in the allowed process variation. A simple thought experiment shows that the smallest cross section with the largest vertical spacing will produce the smallest coupling capacitance. CMOS Digital circuits were more sensitive to capacitance than resistance so this variation was initially acceptable. As processes evolved and resistance of wiring became more critical, the additional rcbest and rcworst were created to model the minimum and maximum cross sectional areas for resistance. But the one change is that cross sectional resistance is not dependent on oxide thickness (vertical spacing between wires) so for rcbest the largest is used and for rcworst the smallest is used.\n\nTo combat these variation effects, modern technology processes often supply SPICE or BSIM simulation models for all (or, at the least, TT, FS, and SF) process corners, which enables circuit designers to detect corner skew effects before the design is laid out, as well as post-layout (through parasitics extraction), before it is taped out.\n\n"}
{"id": "2328120", "url": "https://en.wikipedia.org/wiki?curid=2328120", "title": "Proton-exchange membrane", "text": "Proton-exchange membrane\n\nA proton-exchange membrane, or polymer-electrolyte membrane (PEM), is a semipermeable membrane generally made from ionomers and designed to conduct protons while acting as an electronic insulator and reactant barrier, e.g. to oxygen and hydrogen gas. This is their essential function when incorporated into a membrane electrode assembly (MEA) of a proton-exchange membrane fuel cell or of a proton-exchange membrane electrolyser: separation of reactants and transport of protons while blocking a direct electronic pathway through the membrane.\n\nPEMs can be made from either pure polymer membranes or from composite membranes, where other materials are embedded in a polymer matrix. One of the most common and commercially available PEM materials is the fluoropolymer (PFSA) Nafion, a DuPont product.\n\nProton-exchange membranes are primarily characterized by proton conductivity (σ), methanol permeability (\"P\"), and thermal stability.\n\nPEM fuel cells use a solid polymer membrane (a thin plastic film) as the electrolyte. This polymer is permeable to protons when it is saturated with water, but it does not conduct electrons.\n\nProton-exchange membrane fuel cells (PEMFCs) are believed to be the most promising type of fuel cell to act as the vehicular power source replacement for gasoline and diesel internal combustion engines. They are being considered for automobile applications because they typically have a low operating temperature (~80 °C) and a rapid start-up time, including from frozen conditions. PEMFCs operate at 40–60% efficiency and can vary the output to match the demands. First used in the 1960s for the NASA Gemini program, PEMFCs are currently being developed and demonstrated from ~100 kW cars to a 59 MW power plant.\n\nPEMFCs contain advantages over other types of fuel cells such as solid oxide fuel cells (SOFC). PEMFCs operate at a lower temperature, are lighter and more compact, which makes them ideal for applications such as cars.\nHowever, some disadvantages are: the ~80 °C operating temperature is too low for cogeneration like in SOFCs, and that the electrolyte for PEMFCs must be water-saturated. However, some fuel-cell cars, including the Toyota Mirai, operate without humidifiers, relying on rapid water generation and the high rate of back-diffusion through thin membranes to maintain the hydration of the membrane, as well as the ionomer in the catalyst layers. High-temperature PEMFCs operate between 100 °C and 200 °C, potentially offering benefits to electrode kinetics and heat management, and better tolerance to fuel impurities, particularly CO in reformate. These improvements potentially could lead to higher overall system efficiencies. However, these gains have yet to be realized, as the gold-standard perfluorinated sulfonic acid (PFSA) membranes lose function rapidly at 100 °C and above if hydration drops below ~100%, and begin to creep in this temperature range, resulting in localized thinning and overall lower system lifetimes. As a result, new anhydrous proton conductors, such as protic organic ionic plastic crystals (POIPCs) and protic ionic liquids, are actively studied for the development of suitable PEMs.\n\nThe fuel for the PEMFC is hydrogen, and the charge carrier is the hydrogen ion (proton). At the anode, the hydrogen molecule is split into hydrogen ions (protons) and electrons. The hydrogen ions permeate across the electrolyte to the cathode, while the electrons flow through an external circuit and produce electric power. Oxygen, usually in the form of air, is supplied to the cathode and combines with the electrons and the hydrogen ions to produce water. The reactions at the electrodes are as follows:\n\nThe theoretical exothermic potential is +1.23 V overall.\n\nIn 2014, Andre Geim of the University of Manchester published initial results on atom thick monolayers of graphene and boron nitride which allowed only protons to pass the material.\n\nPEM fuel cells have been used to power everything from cars to drones. 3,000 fuel cell cars will be sold or leased in 2016 globally, with 30,000 intended for 2017. Ballard Power Systems has developed a completely viable commercial market supplying forklifts.\n\n"}
{"id": "4121300", "url": "https://en.wikipedia.org/wiki?curid=4121300", "title": "Refuse-derived fuel", "text": "Refuse-derived fuel\n\nRefuse-derived fuel (RDF) is a fuel produced from various types of waste such as municipal solid waste (MSW), industrial waste or commercial waste.\n\nThe World Business Council for Sustainable Development provides a definition:\n\n\"“Selected waste and by-products with recoverable calorific value can be used as fuels in a cement kiln, replacing a portion of conventional fossil fuels, like coal, if they meet strict specifications. Sometimes they can only be used after pre-processing to provide ‘tailor-made’ fuels for the cement process“\"\n\nRDF consists largely of combustible components of such waste, as non recyclable plastics (not including PVC), paper cardboard, labels, and other corrugated materials. These fractions are separated by different processing steps, such as screening, air classification, ballistic separation, separation of ferrous and non ferrous materials, glass, stones and other foreign materials and shredding into a uniform grain size, or also pelletized in order to produce a homogeneous material which can be used as substitute for fossil fuels in e.g. cement plants, lime plants, coal fired power plants or as reduction agent in steel furnaces. RDF can be also further specified into e.g. tyre derived fuels (TDF) from used tyres, or solid recovered fuels (SRF)\n\nOthers describe the properties, such as:\n\nThere is no universal exact classification or specification which is used for such materials. Even legislative authorities have not yet established any exact guidelines on the type and composition of alternative fuels. The first approaches towards classification or specification are to be found in the Federal Republic of Germany (Bundesgütegemeinschaft für Sekundärbrennstoffe) as well as at European level (European Recovered Fuel Organisation). These approaches which are initiated primarily by the producers of alternative fuels, follow a correct approach: Only through an exactly defined standardisation in the composition of such materials can both production and utilisation be uniform worldwide.\n\nFirst approaches towards alternative fuel classification:\n\nSolid recovered fuels are part of RDF in the fact that it is produced to reach a standard such as CEN/343 ANAS. A comprehensive review is now available on SRF / RDF production, quality standards and thermal recovery, including statistics on European SRF quality.\n\nIn the fifties of the last century tyres were used for the first time as refuse derived fuel in the cement industry. Continuous use of various waste-derived alternative fuels then followed in the mid-eighties with “Brennstoff aus Müll“ (BRAM) – fuel from waste – in the Westphalian cement industry in Germany.\n\nAt that time the thought of cost reduction through replacement of fossil fuels was the priority as considerable competition pressure weighed down on the industry. Since the eighties the German Cement Works Association (Verein Deutscher Zementwerke e.V. (VDZ, Düsseldorf)) has been documenting the use of alternative fuels in the Federal German cement industry. In 1987 less than 5% of fossil fuels were replaced by refuse derived fuels, in 2015 its use increased to almost 62%.\n\nRefuse derived Fuels are used in a wide range of specialized waste to energy facilities, which are using processed refuse derived fuels with lower calorific values of 8-14MJ/kg in grain sizes of up to 500 mm to produce electricity and thermal energy (heat/steam) for district heating systems or industrial uses.\n\nMaterials such as glass and metals are removed during the treatment processing since they are non-combustible. The metal is removed using a magnet and the glass using mechanical screening. After that, an air knife is used to separate the light materials from the heavy ones. The light materials have higher calorific value and they create the final RDF. The heavy materials will usually continue to a landfill. The residual material can be sold in its processed form (depending on the process treatment) as a plain mixture or it may be compressed into pellet fuel, bricks or logs and used for other purposes either stand-alone or in a recursive recycling process.\n\nRDF is extracted from municipal solid waste and other waste using a mix of mechanical and/or biological treatment methods.\n\nThe production of RDF may involve the following steps:\n\n\nRDF can be used in a variety of ways to produce electricity. It can be used alongside traditional sources of fuel in coal power plants. In Europe RDF can be used in the cement kiln industry, where the strict standards of the Waste Incineration Directive are met. RDF can also be fed into plasma arc gasification modules & pyrolysis plants. Where the RDF is capable of being combusted cleanly or in compliance with the Kyoto Protocol, RDF can provide a funding source where unused carbon credits are sold on the open market via a carbon exchange. However, the use of municipal waste contracts and the bankability of these solutions is still a relatively new concept, thus RDF's financial advantage may be debatable. The European market for the production of RDF have been grown fast due to e.g. the European landfill directive, or landfill taxes e.g. in UK and Ireland. Refuse derived fuel (RDF) exports from the UK to Europe and beyond are expected to have reached 3.3 million tonnes in 2015, representing a near-500,000 tonnes increase on the previous year.\n\nThe biomass fraction of RDF and SRF has a monetary value under multiple greenhouse gas protocols, such as the European Union Emissions Trading Scheme and the Renewable Obligation Certificate program in the United Kingdom. Biomass is considered to be carbon-neutral since the liberated from the combustion of biomass is recycled in plants. The combusted biomass fraction of RDF/SRF is used by stationary combustion operators to reduce their overall reported emissions.\n\nSeveral methods have been developed by the European CEN 343 working group to determine the biomass fraction of RDF/SRF. The initial two methods developed (CEN/TS 15440) were the manual sorting method and the selective dissolution method. Since each method suffered from limitations in properly characterizing the biomass fraction, an alternative method was developed using the principles of radiocarbon dating. A technical review (CEN/TR 15591:2007) outlining the carbon-14 method was published in 2007, and a technical standard of the carbon dating method (CEN/TS 15747:2008) was published in 2008. In the United States, there is already an equivalent carbon-14 method under the standard method ASTM D6866.\n\nAlthough carbon-14 dating can determine the biomass fraction of RDF/SRF, it cannot determine directly the biomass calorific value. Determining the calorific value is important for green certificate programs such as the Renewable Obligation Certificate program. These programs award certificates based on the energy produced from biomass. Several research papers, including the one commissioned by the Renewable Energy Association in the UK, have been published that demonstrate how the carbon-14 result can be used to calculate the biomass calorific value.\n\nThe first full-scale waste-to-energy facility in the US was the Arnold O. Chantland Resource Recovery Plant, built in 1975 located in Ames, Iowa. This plant also produces RDF that is sent to a local power plant for supplemental fuel.\n\nThe city of Manchester, in the north west of England, is in the process of awarding a contract for the use of RDF which will be produced by proposed mechanical biological treatment facilities as part of a huge PFI contract. The Greater Manchester Waste Disposal Authority has recently announced there is significant market interest in initial bids for the use of RDF which is projected to be produced in tonnages up to 900,000 tonnes per annum.\n\nDuring spring 2008 Bollnäs Ovanåkers Renhållnings AB (BORAB) in Sweden, started their new waste-to-energy plant. Municipal solid waste as well as industrial waste is turned into refuse-derived fuel. The 70,000-80,000 tonnes RDF that is produced per annum is used to power the nearby BFB-plant, which provides the citizens of Bollnäs with electricity and district heating.\n\nIn late March 2017, Israel launched its own RDF plant at the Hiriya Recycling Park; which daily will intake about 1,500 tonnes of household waste, which will amount to around half a million tonnes of waste each year,with an estimated production of 500 tonnes of RDF daily. The plant is part of Israel's \"diligent effort to improve and advance waste management in Israel.\"\n"}
{"id": "143498", "url": "https://en.wikipedia.org/wiki?curid=143498", "title": "Salting (food)", "text": "Salting (food)\n\nSalting is the preservation of food with dry edible salt. It is related to pickling in general and more specifically to brining (preparing food with brine, that is, salty water) and is one form of curing. It is one of the oldest methods of preserving food, and two historically significant salt-cured foods are salted fish (usually dried and salted cod or salted herring) and salt-cured meat (such as bacon). Vegetables such as runner beans and cabbage are also often preserved in this manner.\n\nSalting is used because most bacteria, fungi and other potentially pathogenic organisms cannot survive in a highly salty environment, due to the hypertonic nature of salt. Any living cell in such an environment will become dehydrated through osmosis and die or become temporarily inactivated.\n\nIt was discovered in the 19th century that salt mixed with nitrates (saltpeter) would color meats red, rather than grey, and consumers at that time then strongly preferred the red-colored meat. The food hence preserved stays healthy and fresh for days avoiding bacterial decay. \nJewish and Muslim dietary laws require the removal of blood from freshly slaughtered meat. Salt and brine are used for the purpose in both traditions, but salting is more common in Kosher Shechita (where it is all but required) than in Halal Dhabiha (as in most cases, draining alone will suffice).\n\n"}
{"id": "246507", "url": "https://en.wikipedia.org/wiki?curid=246507", "title": "Silage", "text": "Silage\n\nSilage is fermented, high-moisture stored fodder which can be fed to cattle, sheep and other such ruminants (cud-chewing animals) or used as a biofuel feedstock for anaerobic digesters. It is fermented and stored in a process called \"ensilage\", \"ensiling\" or \"silaging\", and is usually made from grass crops, including maize, sorghum or other cereals, using the entire green plant (not just the grain). Silage can be made from many field crops, and special terms may be used depending on type: \"oatlage\" for oats, \"haylage\" for alfalfa; but see below for the different British use of the term \"haylage\".\n\nSilage is made by one or more of the following methods: placing cut green vegetation in a silo or pit; piling the vegetation in a large heap and compressing it down so as to purge as much oxygen as possible, then covering it with a plastic sheet; or by wrapping large round bales tightly in plastic film.\n\nThe crops most often used for ensilage are the ordinary grasses, clovers, alfalfa, vetches, oats, rye and maize. Many crops have ensilaging potential, including potatoes and various weeds, notably spurrey such as \"Spergula arvensis\". Silage must be made from plant material with a suitable moisture content: about 50% to 60% depending on the means of storage, the degree of compression, and the amount of water that will be lost in storage, but not exceeding 75%. Weather during harvest need not be as fair and dry as when harvesting for drying. For corn, harvest begins when the whole-plant moisture is at a suitable level, ideally a few days before it is ripe. For pasture-type crops, the grass is mowed and allowed to wilt for a day or so until the moisture content drops to a suitable level. Ideally the crop is mowed when in full flower, and deposited in the silo on the day of its cutting.\n\nAfter harvesting, crops are shredded to pieces about long. The material is spread in uniform layers over the floor of the silo, and closely packed. When the silo is filled or the stack built, a layer of straw or some other dry porous substance may be spread over the surface. In the silo the pressure of the material, when chaffed, excludes air from all but the top layer; in the case of the stack extra pressure is applied by weights in order to prevent excessive heating.\n\nForage harvesters collect and chop the plant material, and deposit it in trucks or wagons. These forage harvesters can be either tractor-drawn or self-propelled. Harvesters blow the chaff into the wagon through a chute at the rear or side of the machine. Chaff may also be emptied into a bagger, which puts the silage into a large plastic bag that is laid out on the ground.\n\nIn North America, Australia, northwestern Europe, and frequently in New Zealand, silage is placed in large heaps on the ground and rolled by tractor to push out the air, then wrapped in plastic covers held down by reused tires or tire ring walls.\n\nIn New Zealand and Northern Europe, the silo or \"pit\" is often a bunker built into the side of a bank, usually made out of concrete or old wooden railroad ties (railway sleepers). The chopped grass can then be dumped in at the top, to be drawn from the bottom in winter. This requires considerable effort to compress the stack in the silo to cure it properly. Again, the pit is covered with plastic sheet and weighed down with tire weights.\n\nIn an alternative method, the cut vegetation is baled, making \"balage\" (North America) or \"silage bales\" (UK). The grass or other forage is cut and partly dried until it contains 30–40% moisture (much drier than bulk silage, but too damp to be stored as dry hay). It is then made into large bales which are wrapped tightly in plastic to exclude air. The plastic may wrap the whole of each cylindrical or cuboid bale, or be wrapped around only the curved sides of a cylindrical bale, leaving the ends uncovered. In this case, the bales are placed tightly end to end on the ground, making a long continuous \"sausage\" of silage, often at the side of a field. The wrapping may be performed by a bale wrapper, while the baled silage is handled using a bale handler or a front-loader, either impaling the bale on a flap, or by using a special grab. The flaps do not hole the bales.\n\nIn the UK, baled silage is most often made in round bales about 4 feet by 4 feet, individually wrapped with four to six layers of \"bale wrap plastic\" (black, white or green 25-micrometre stretch film). The percentage of dry matter can vary from about 20% dry matter upwards. The continuous \"sausage\" referred to above is made with a special machine which wraps the bales as they are pushed through a rotating hoop which applies the bale wrap to the outside of the bales (round or square) in a continuous wrap. The machine places the bales on the ground after wrapping by moving forward slowly during the wrapping process.\n\nHaylage refers to high dry matter silage of around 45% to 75%. Horse haylage is usually 55% to 75% dry matter, made in small bales or larger bales.\nHandling of wrapped bales is most often with some type of gripper that squeezes the plastic-covered bale between two metal parts to avoid puncturing the plastic. Simple fixed versions are available for round bales which are made of two shaped pipes or tubes spaced apart to slide under the sides of the bale, but when lifted will not let it slip through. Often used on the tractor rear three-point linkage, they incorporate a trip tipping mechanism which can flip the bales over on to the flat side/end for storage on the thickest plastic layers.\n\nSilage undergoes anaerobic fermentation, which starts about 48 hours after the silo is filled, and converts sugars to acids. Fermentation is essentially complete after about two weeks.\n\nBefore anaerobic fermentation starts, there is an aerobic phase in which the trapped oxygen is consumed. How closely the fodder is packed determines the nature of the resulting silage by regulating the chemical reactions that occur in the stack. When closely packed, the supply of oxygen is limited, and the attendant acid fermentation brings about decomposition of the carbohydrates present into acetic, butyric and lactic acids. This product is named sour silage. If, on the other hand, the fodder is unchaffed and loosely packed, or the silo is built gradually, oxidation proceeds more rapidly and the temperature rises; if the mass is compressed when the temperature is , the action ceases and sweet silage results. The nitrogenous ingredients of the fodder also change: in making sour silage as much as one-third of the albuminoids may be converted into amino and ammonium compounds; in making sweet silage a smaller proportion is changed, but they become less digestible. If the fermentation process is poorly managed, sour silage acquires an unpleasant odour due to excess production of ammonia or butyric acid (the latter is responsible for the smell of rancid butter).\n\nIn the past, the fermentation was conducted by indigenous microorganisms, but, today, some bulk silage is inoculated with specific microorganisms to speed fermentation or improve the resulting silage. Silage inoculants contain one or more strains of lactic acid bacteria, and the most common is \"Lactobacillus plantarum\". Other bacteria used include \"Lactobacillus buchneri\", \"Enterococcus faecium\" and \"Pediococcus\" species.\n\nRyegrasses have high sugars and respond to nitrogen fertiliser better than any other grass species. These two qualities have made ryegrass the most popular grass for silage making for the last sixty years. There are three ryegrasses in seed form and commonly used: Italian, Perennial and Hybrid.\n\nThe fermentation process of silo or pit silage releases liquid. Silo effluent is corrosive. It can also contaminate water sources unless collected and treated. The high nutrient content can lead to eutrophication (hypertrophication), the growth of bacterial or algal blooms.\n\nPlastic sheeting used for sealing pit or baled silage needs proper disposal, and some areas have recycling schemes for it. Traditionally, farms have burned silage plastics; however odor and smoke concerns have led certain communities to restrict that practice.\n\nSilage must be firmly packed to minimize the oxygen content, or it will spoil.\nSilage goes through four major stages in a silo:\n\nSilos are potentially hazardous: deaths may occur in the process of filling and maintaining them, and several safety precautions are necessary. There is a risk of injury by machinery or from falls. When a silo is filled, fine dust particles in the air can become explosive because of their large aggregate surface area. Also, fermentation presents respiratory hazards. The ensiling process produces \"silo gas\" during the early stages of the fermentation process. Silage gas contains nitric oxide (NO), which will react with oxygen (O) in the air to form nitrogen dioxide (NO), which is toxic. Lack of oxygen inside the silo can cause asphyxiation. Molds that grow when air reaches cured silage can cause organic dust toxic syndrome. Collapsing silage from large bunker silos has caused deaths. Silage itself poses no special danger.\n\nEnsilage can be substituted for root crops. Bulk silage is commonly fed to dairy cattle, while baled silage tends to be used for beef cattle, sheep and horses. The advantages of silage as animal feed are several: \n\nUsing the same technique as the process for making sauerkraut, green fodder was preserved for animals in parts of Germany since the start of the 19th century. This gained the attention of a French agriculturist, Auguste Goffart of Sologne, near Orléans, who published a book in 1877 which described the experiences of preserving green crops in silos. Goffart's experience attracted considerable attention. The conditions of dairy farming in the USA suited the ensiling of green corn fodder, and was soon adopted by New England farmers. Francis Morris of Maryland prepared the first silage produced in America in 1876. The favourable results obtained in the U.S. led to the introduction of the system in the United Kingdom, where Thomas Kirby first introduced the process for British dairy herds.\n\nEarly silos were made of stone or concrete either above or below ground, but it is recognized that air may be sufficiently excluded in a tightly pressed stack, though in this case a few inches of the fodder round the sides is generally useless owing to mildew. In the U.S. structures were typically constructed of wooden cylinders to 35 or 40 ft. in depth.\n\nIn the early days of mechanized agriculture, stalks were cut and collected manually using a knife and horsedrawn wagon, and fed into a stationary machine called a \"silo filler\" that chopped the stalks and blew them up a narrow tube to the top of a tower silo.\n\nSilage may be used for anaerobic digestion.\n\n\n\n"}
{"id": "1592159", "url": "https://en.wikipedia.org/wiki?curid=1592159", "title": "Sindicato Nacional de Trabajadores de la Industria de Alimentos", "text": "Sindicato Nacional de Trabajadores de la Industria de Alimentos\n\nThe National Union of Food Industry Workers (, SINALTRAINAL) is a Colombian food industry trade union.\n\nThe group has repeatedly tried to form unions in Colombia for workers of Panamco, a Colombian Coca-Cola bottling company, and have documentation of many members or leaders being murdered, kidnapped, and tortured by right-wing paramilitary groups such as the AUC in order to prevent unionisation. They are a central focus of the ongoing Coca-Cola boycott movement prevalent across college campuses worldwide (see criticism of Coca-Cola).\n\n\n"}
{"id": "9002673", "url": "https://en.wikipedia.org/wiki?curid=9002673", "title": "Soak testing", "text": "Soak testing\n\nSoak testing involves testing a system with a typical production load, over a continuous availability period, to validate system behavior under production use.\n\nIt may be required to extrapolate the results, if not possible to conduct such an extended test. For example, if the system is required to process 10,000 transactions over 100 hours, it may be possible to complete processing the same 10,000 transactions in a shorter duration (say 50 hours) as representative (and conservative estimate) of the actual production use. A good soak test would also include the ability to simulate peak loads as opposed to just average loads. If manipulating the load over specific periods of time is not possible, alternatively (and conservatively) allow the system to run at peak production loads for the duration of the test.\n\nFor example, in software testing, a system may behave exactly as expected when tested for one hour. However, when it is tested for three hours, problems such as memory leaks cause the system to fail or behave unexpectedly.\n\nSoak tests are used primarily to check the reaction of a subject under test under a possible simulated environment for a given duration and for a given threshold. Observations made during the soak test are used to improve the characteristics of the subject under further tests.\n\nIn electronics, soak testing may involve testing a system up to or above its maximum ratings for a long period of time. Some companies may soak test a product for a period of many months, while also applying external stresses such as elevated temperatures.\n\nThis falls under load testing.\n"}
{"id": "49755666", "url": "https://en.wikipedia.org/wiki?curid=49755666", "title": "Software architectural style", "text": "Software architectural style\n\nA software architectural style is characterized by a set of features that make the software architecture uniquely identifiable. Software architectural styles generally provide a high level direction for solutions unlike software patterns which are focused on solving one or more specific problems. A software architectural style usually consists of one or more software patterns.\n"}
{"id": "6887194", "url": "https://en.wikipedia.org/wiki?curid=6887194", "title": "SoundBug", "text": "SoundBug\n\nSoundbug is a small speaker that can turn a resonant flat surface into a flat panel speaker. The Soundbug is attached to a smooth resonant surface, this surface then acts as speaker. The tone differs depending on the surface (wood, metal, glass etc.). The Soundbug was developed by FeONIC Technology (formerly Newlands Scientific), a commercial research and development company specialising in magnetostrictive audio products as a spin-off from Hull University. FeONIC used the same technology for Whispering Windows, which is able to resonate shop windows.\n\nSoundbug transmits the audio signal to the flat surface by way of a small piece of Terfenol-D, which is a magnetostrictive alloy of rare earth metals and iron. This material, when stimulated with an electrical input, causes the Terfenol to expand slightly, resulting in sound output with a very small amplitude. Once attached to a flat surface, Soundbug will translate electric signals into mechanical energy, causing the flat surface to vibrate and broadcast the sound.\n\n"}
{"id": "3728892", "url": "https://en.wikipedia.org/wiki?curid=3728892", "title": "Space elevators in fiction", "text": "Space elevators in fiction\n\nThis is a list of occurrences of space elevators in fiction. Some depictions were made before the space elevator concept became fully established.\n\n\n\n\n\n\n"}
{"id": "28117173", "url": "https://en.wikipedia.org/wiki?curid=28117173", "title": "Submarine mines in United States harbor defense", "text": "Submarine mines in United States harbor defense\n\nThe modern era of defending American harbors with controlled mines or submarine mines (also called \"naval mines\" and originally referred to as \"torpedoes\") began in the post-Civil War period, and was a major part of US harbor defenses from circa 1900 to 1947.\n\nIn 1866, the United States Army Corps of Engineers established the Engineer School of Application at Willets Point, New York. The first commander of this School, Major Henry Larcom Abbot, was almost single-handedly responsible for designing and supervising the program of research and development that defined the strategy and tactics for the mine defense of American harbors. Abbot experimented with underwater explosives, fuzes, cabling, and electrical equipment for over a decade before publishing the first manuals on the use of mines in coast defense in 1876–77.\n\nIn 1886 the Endicott Board made its report on harbor defense, with sweeping recommendations for new minefield and gun defenses. This initiated a vast expansion in the building of modern forts, the installation of new guns, and the preparation of mine defenses at newly created Artillery Districts, designated Coast Defense Commands in 1913, defending major seaports. \n\nMines were planted at 28 harbors during the Spanish–American War, and many lessons were learned. In 1901 the responsibility for underwater mines shifted from the Corps of Engineers to the Artillery Corps, and was a founding responsibility of the Coast Artillery Corps in 1907. From about 1900 until 1946 the mine defense program grew, until upwards of 10,000 controlled mines were maintained by the Coast Artillery Corps. In 1904 the first US Army mine planters were built, small vessels used to emplace and retrieve controlled mines. In July 1918, the U.S. Army Mine Planter Service (AMPS) was established to maintain U.S. mine defenses, replacing civilian manning of the mine planters.\n\nIn 1943 the buoyant mines were replaced with ground mines, which rested on the seabed. This was due to the deployment of minefields in World War II, which resulted in deep-draft ships fouling the mine cables while passing through deactivated minefields.\n\nControlled mines were anchored to the bottom of a harbor, either sitting on the bottom itself (ground mines) or floating (buoyant mines) at depths which could vary widely, from about 20 to 250 feet (6–75 m). These mines were fired electrically through a vast network of underwater electrical cables at each protected harbor. Mines could be set to explode on contact or be triggered by the operator, based on reports of the position of enemy ships. The networks of cables terminated on shore in massive concrete bunkers called mine casemates (see photo, below right), that were usually buried beneath protective coverings of earth.\n\nThe mine casemate housed electrical generators, batteries, control panels, and troops that were used to test the readiness of the mines and to fire them when needed. The map of Boston Harbor's mine fields (below right) shows the harbor mine defenses consisting of 30 groups of mines, with 19 mines per group. Each mine was normally loaded with of TNT. So in Boston's case, a total of 57 tons of explosives guarded the harbor.\n\nEach protected harbor also maintained a small fleet of mine planters and tenders that were used to plant the mines in precise patterns, haul them back up periodically to check their condition (or to remove them back to the shore for maintenance), and then plant them again. Each of these harbors also had on-shore facilities to store the mines (called \"torpedo storehouses\") and the TNT used to fill them, rail systems to load and transport the mines (which often weighed over each when loaded), and to test and repair the electrical cables. Fire control structures were also built that were used first to observe the mine-planting process and fix location of each mine and second to track attacking ships, reporting when specific mines should be detonated (known as \"observed fire\"). The preferred method of using the mines was to set them to detonate a set period of time after they had been touched or tipped, avoiding the need for observers to spot each target ship.\n\nEach mine casemate controlled about 150 to 300 mines (depending on the harbor defended), arranged in groups of 19. The mines in a group were generally laid about apart, in lines running across the channel being protected, with or less on either end of the line. This meant that one group of mines could protect a total distance of about . About of cable were required to connect one group of 19 mines to its distribution box, with the connecting cables radiating out from the box in a hub-and-spoke fashion. Actually, the distribution box was usually located well behind the line of mines, so the pattern looked more like a 19-armed candelabrum with the box at its base. If more groups of mines were needed, multiple mine casemates were generally built and equipped. For example, Boston had three mine casemates, with two originally at Fort Dawes on Deer Island (northern channels) and Fort Strong on Long Island (southern channels). Later in World War II, Fort Strong's mine casemate was deactivated and control of the southern mines passed to Fort Warren on Georges Island.\n\nThe shore cable from the underwater distribution box of each mine group ran back to a cable hut on the shore near the mine casemate, and from there to the casemate itself, where it was connected to a mine control panel (see photo at left, below). These panels were located in the casemate's operating room, pictured at left-center in the plan shown at right. (This is the plan of the Ft. Strong mine casemate, the one shown in the photo directly above it.)\n\nIn addition to these firing controls, the mine casemate contained one or more electric generators and a large bank of electric storage batteries. The generators produced direct current, which was used to signal the readiness of the mines. An interrupter was used to convert a portion of this current to AC which was used to fire the mines; the distinct current supply was a safety feature. The casemate also had a number of telephone lines, keeping it in touch with remotely located mine observation and fire-control positions, with the mine commander, and with the gun batteries and searchlights that covered the mine fields. The casemate also contained switching and diagnostic equipment used to test cable integrity and the functionality of the individual and group mine detonation switches.\n\nFor example, as the largest mine casemate in Boston Harbor controlled 15 mine groups (285 mines), it would have mounted 15 of these mine control panels, plus many more related rack-mount devices for controlling the casemate's generators, inverters, and battery systems.\n\nThe mines could be fired in three ways, listed here in order of tactical preference:\n\nDelayed contact fire was preferred because it was thought that the mine would first be dragged underneath the target and then fired after a few seconds' delay, once it had had the chance to contact a more lightly protected portion of the target's bottom. This type of fire required the casemate troops to hear the bell and see the accompanying signal light that indicated when a given mine was tipped, wait several seconds, and then throw the firing switch for that mine's group into \"fire\" position to detonate the mine.\n\nIn contact fire mode, each mine in the group was set to explode when contacted or tipped by a ship; in observed fire mode, mines were fired in a fashion similar to that used under the fire control system for the coast artillery batteries defending the harbor: distant observers took bearings on targets though spotting telescopes, and these bearings were used to plot the target's position.\n\nIt was suggested that contact fire or delayed contact fire was most useful in situations when more than one enemy ship was approaching the mine field, or was approaching quite rapidly, which made observed fire more difficult, or under conditions of poor visibility. On the other hand, these types of fire made it much more dangerous for any friendly shipping that might be near the mine field.\n\nExtensive on-shore facilities, as well as a small fleet of mine-planting boats, supported each mine casemate. For Fort Strong in Boston Harbor, these facilities are illustrated in the map shown at right.\n\nThe mine wharf was where mine planters tied up to load or unload their cargo of mines and connecting cable. This wharf was equipped with a heavy lifting crane. From the wharf, returning mines were carried by the mine tramway (a rail system) to the torpedo storehouse (\"torpedo\" was an old alternate term for \"mine\"). This storehouse was the largest building in the mine complex, and was used to store the mines, on large racks, when they had been pulled from the water for testing, repair, or storage (for example, when peace had broken out). Loaded mines were also stored in the underground magazines of those gun batteries which were disarmed about 1925, at the top of the bluff on the northern end of the fort. These magazines were reached by following the tramway off the map towards the lower left, where it ran up a gradual slope past the \"reservoir\" (water tower).\n\nAnother branch of the tramway lead to the loading room, where the TNT charges for the mines were loaded into or unloaded from the mines. A very small TNT storehouse was immediately adjacent to the loading room. Only one box of TNT – , or half of the load for an average mine – was supposed to be taken from this storehouse at a time for loading.\n\nAnother branch of the tramway track lead to the cable tanks, large concrete tanks filled with seawater pumped from the harbor and used for insulation and conductivity testing of the many miles of electrical cable that were used for mine operations. The torpedo storehouse had its own smaller tanks that were used for submergence testing of the mine casings and their fuzes, which were inserted into the casings through watertight plugs.\n\nThe mine casemate (pictured in the photo above and in the plan drawing) is also shown on the map. Its coordinates () indicate the approximate center of the structure. All traces of the other mine facilities were destroyed by the City of Boston in the 1990s during the redevelopment of the northern end of the fort and its parade ground for use as a summer camp for city children.\n\n\n\n"}
{"id": "253836", "url": "https://en.wikipedia.org/wiki?curid=253836", "title": "Surround sound", "text": "Surround sound\n\nSurround sound is a technique for enriching the fidelity of sound reproduction by using multiple audio channels from speakers that surround the listener (surround channels). Its first application was in movie theaters. Prior to surround sound, theater sound systems commonly had three \"screen channels\" of sound, from loudspeakers located in front of the audience at the left, center, and right. Surround sound adds one or more channels from loudspeakers behind the listener, able to create the sensation of sound coming from any horizontal direction 360° around the listener. Surround sound formats vary in reproduction and recording methods along with the number and positioning of additional channels. The most common surround sound specification, the ITU's 5.1 standard, calls for 6 speakers: Center (C) in front of the listener, Left (L) and Right (R) at angles of 60° on either side of the center, and Left Surround (LS) and Right Surround (RS) at angles of 100–120°, plus a subwoofer whose position is not critical.\n\nSurround sound typically has a listener location or sweet spot where the audio effects work best, and presents a fixed or forward perspective of the sound field to the listener at this location. The technique enhances the perception of sound spatialization by exploiting sound localization; a listener's ability to identify the location or origin of a detected sound in direction and distance. This is achieved by using multiple discrete audio channels routed to an array of loudspeakers.\n\nThough cinema and soundtracks represent the major uses of surround techniques, its scope of application is broader than that as surround sound permits creation of an audio-environment for all sorts of purposes. Multichannel audio techniques may be used to reproduce contents as varied as music, speech, natural or synthetic sounds for cinema, television, broadcasting, or computers. In terms of music content for example, a live performance may use multichannel techniques in the context of an open-air concert, of a musical theatre or for broadcasting; for a film specific techniques are adapted to movie theater, or to home (e.g. home cinema systems). The narrative space is also a content that can be enhanced through multichannel techniques. This applies mainly to cinema narratives, for example the speech of the characters of a film, but may also be applied to plays for theatre, to a conference, or to integrate voice-based comments in an archeological site or monument. For example, an exhibition may be enhanced with topical ambient sound of water, birds, train or machine noise. Topical natural sounds may also be used in educational applications. Other fields of application include video game consoles, personal computers and other platforms. In such applications, the content would typically be synthetic noise produced by the computer device in interaction with its user. Significant work has also been done using surround sound for enhanced situation awareness in military and public safety application.\n\nCommercial surround sound media include videocassettes, DVDs, and SDTV broadcasts encoded as compressed Dolby Digital and DTS, and lossless audio such as DTS HD Master Audio and Dolby TrueHD on HDTV Blu-ray Disc and HD DVD, which are identical to the studio master. Other commercial formats include the competing DVD-Audio (DVD-A) and Super Audio CD (SACD) formats, and MP3 Surround. Cinema 5.1 surround formats include Dolby Digital and DTS. Sony Dynamic Digital Sound (SDDS) is an 8 channel cinema configuration which features 5 independent audio channels across the front with two independent surround channels, and a Low-frequency effects channel. Traditional 7.1 surround speaker configuration introduces two additional rear speakers to the conventional 5.1 arrangement, for a total of four surround channels and three front channels, to create a more 360° sound field.\n\nMost surround sound recordings are created by film production companies or video game producers; however some consumer camcorders have such capability either built-in or available separately. Surround sound technologies can also be used in music to enable new methods of artistic expression. After the failure of quadraphonic audio in the 1970s, multichannel music has slowly been reintroduced since 1999 with the help of SACD and DVD-Audio formats. Some AV receivers, stereophonic systems, and computer soundcards contain integral digital signal processors and/or digital audio processors to simulate surround sound from a stereophonic source (see fake stereo).\n\nIn 1967, the rock group Pink Floyd performed the first-ever surround sound concert at \"Games for May\", a lavish affair at London’s Queen Elizabeth Hall where the band debuted its custom-made quadraphonic speaker system. The control device they had made, the Azimuth Co-ordinator, is now displayed at London's Victoria and Albert Museum, as part of their Theatre Collections gallery.\n\nThe first documented use of surround sound was in 1940, for the Disney studio's animated film \"Fantasia\". Walt Disney was inspired by Nikolai Rimsky-Korsakov's operatic piece \"Flight of the Bumblebee\" to have a bumblebee featured in his musical \"Fantasia\" and also sound as if it was flying in all parts of the theatre. The initial multichannel audio application was called 'Fantasound', comprising three audio channels and speakers. The sound was diffused throughout the cinema, controlled by an engineer using some 54 loudspeakers. The surround sound was achieved using the sum and the difference of the phase of the sound. However, this experimental use of surround sound was excluded from the film in later showings. In 1952, \"surround sound\" successfully reappeared with the film \"This is Cinerama\", using discrete seven-channel sound, and the race to develop other surround sound methods took off.\n\nIn the 1950s, the German composer Karlheinz Stockhausen experimented with and produced ground-breaking electronic compositions such as \"Gesang der Jünglinge\" and \"Kontakte\", the latter using fully discrete and rotating quadraphonic sounds generated with industrial electronic equipment in Herbert Eimert's studio at the \"Westdeutscher Rundfunk\" (WDR). Edgar Varese's Poeme Electronique, created for the Iannis Xenakis-designed Philips Pavilion at the 1958 Brussels World's Fair, also utilised spatial audio with 425 loudspeakers used to move sound throughout the pavilion.\n\nIn 1957, working with artist Jordan Belson, Henry Jacobs produced Vortex: Experiments in Sound and Light - a series of concerts featuring new music, including some of Jacobs' own, and that of Karlheinz Stockhausen, and many others - taking place in the Morrison Planetarium in Golden Gate Park, San Francisco. Sound designers commonly regard this as the origin of the (now standard) concept of \"surround sound.\" The program was popular, and Jacobs and Belson were invited to reproduce it at the 1958 World Expo in Brussels.\nThere are also many other composers that created ground-breaking surround sound works in the same time period.\n\nIn 1978, a concept devised by Max Bell for Dolby Laboratories called \"split surround\" was tested with the movie \"Superman\". This led to the 70mm stereo surround release of \"Apocalypse Now\", which became one of the first formal releases in cinemas with three channels in the front and two in the rear. There were typically five speakers behind the screens of 70mm-capable cinemas, but only the Left, Center and Right were used full-frequency, while Center-Left and Center-Right were only used for bass-frequencies (as it is currently common). The \"Apocalypse Now\" encoder/decoder was designed by Michael Karagosian, also for Dolby Laboratories. The surround mix was produced by an Oscar-winning crew led by Walter Murch for American Zoetrope. The format was also deployed in 1982 with the stereo surround release of \"Blade Runner\".\n\nThe 5.1 version of surround sound originated in 1987 at the famous French Cabaret Moulin Rouge. A French engineer, Dominique Bertrand used a mixing board specially designed in cooperation with Solid State Logic, based on 5000 series and including six channels. Respectively: A left, B right, C centre, D left rear, E right rear, F bass. The same engineer had already achieved a 3.1 system in 1974, for the International Summit of Francophone States in Dakar, Senegal.\n\nSurround sound is created in several ways. The first and simplest method is using a surround sound recording technique—capturing two distinct stereo images, one for the front and one for the back or by using a dedicated setup, e.g. an augmented Decca tree —and/or mixing-in surround sound for playback on an audio system using speakers encircling the listener to play audio from different directions. A second approach is processing the audio with psychoacoustic sound localization methods to simulate a two-dimensional (2-D) sound field with headphones. A third approach, based on Huygens' principle, attempts reconstructing the recorded sound field wave fronts within the listening space; an \"audio hologram\" form. One form, wave field synthesis (WFS), produces a sound field with an even error field over the entire area. Commercial WFS systems, currently marketed by companies \"sonic emotion\" and \"Iosono\", require many loudspeakers and significant computing power.\n\nThe Ambisonics form, also based on Huygens' principle, gives an exact sound reconstruction at the central point; less accurate away from center point. There are many free and commercial software programs available for Ambisonics, which dominates most of the consumer market, especially musicians using electronic and computer music. Moreover, Ambisonics products are the standard in surround sound hardware sold by Meridian Audio. In its simplest form, Ambisonics consumes few resources, however this is not true for recent developments, such as Near Field Compensated Higher Order Ambisonics. Some years ago it was shown that, in the limit, WFS and Ambisonics converge.\n\nFinally, surround sound can also be achieved by mastering level, from stereophonic sources as with Penteo, which uses Digital Signal Processing analysis of a stereo recording to parse out individual sounds to component panorama positions, then positions them, accordingly, into a five-channel field. However, there are more ways to create surround sound out of stereo, for instance with the routines based on QS and SQ for encoding Quad sound, where instruments were divided over 4 speakers in the studio. This way of creating surround with software routines is normally referred to as \"upmixing,\", which was particularly successful on the Sansui QSD-series decoders that had a mode where it mapped the L ↔ R stereo onto an ∩ arc.\n\nIn most cases, surround sound systems rely on the mapping of each source channel to its own loudspeaker. Matrix systems recover the number and content of the source channels and apply them to their respective loudspeakers. With discrete surround sound, the transmission medium allows for (at least) the same number of channels of source and destination; however, one-to-one, channel-to-speaker, mapping is not the only way of transmitting surround sound signals.\n\nThe transmitted signal might encode the information (defining the original sound field) to a greater or lesser extent; the surround sound information is rendered for replay by a decoder generating the number and configuration of loudspeaker feeds for the number of speakers available for replay – one renders a sound field as produced by a set of speakers, analogously to rendering in computer graphics. This \"replay device independent\" encoding is analogous to encoding and decoding an Adobe PostScript file, where the file describes the page, and is rendered per the output device's resolution capacity. The Ambisonics and WFS systems use audio rendering; the Meridian Lossless Packing contains elements of this capability\n\nThere are many alternative setups available for a surround sound experience, with a 3-2 (3 front, 2 back speakers and a Low Frequency Effects channel) configuration (more commonly referred to as 5.1 surround) being the standard for most surround sound applications, including cinema, television and consumer applications. This is a compromise between the ideal image creation of a room and that of practicality and compatibility with two-channel stereo. Because most surround sound mixes are produced for 5.1 surround (6 channels), larger setups require matrixes or processors to feed the additional speakers.\n\nThe standard surround setup consists of three front speakers LCR (left, center and right), two surround speakers LS and RS (left and right surround respectively) and a subwoofer for the Low Frequency Effects (LFE) channel, that is low-pass filtered at 120 Hz. The angles between the speakers have been standardized by the ITU (International Telecommunication Union) recommendation 775 and AES (Audio Engineering Society) as follows: 60 degrees between the L and R channels (allows for two-channel stereo compatibility) with the center speaker directly in front of the listener. The Surround channels are placed 100-120 degrees from the center channel, with the subwoofer’s positioning not being critical due to the low directional factor of frequencies below 120 Hz. The ITU standard also allows for additional surround speakers, that need to be distributed evenly between 60 and 150 degrees.\n\nSurround mixes of more or less channels are acceptable, if they are compatible, as described by the ITU-R BS. 775-1, with 5.1 surround. The 3-1 channel setup (consisting of one monophonic surround channel) is such a case, where both LS and RS are fed by the monophonic signal at an attenuated level of -3 dB.\n\nThe function of the center channel is to anchor the signal so that any central panned images do not shift when a listener is moving or is sitting away from the sweet spot. The center channel also prevents any timbral modifications from occurring, which is typical for 2-channel stereo, due to phase differences at the two ears of a listener. The centre channel is especially used in films and television, with dialogue primarily feeding the center channel. The function of the center channel can either be of a monophonic nature (as with dialogue) or it can be used in combination with the left and right channels for true three-channel stereo. Motion Pictures tend to use the center channel for monophonic purposes with stereo being reserved purely for the left and right channels. Surround microphones techniques have however been developed that fully use the potential of three-channel stereo.\n\nIn 5.1 surround, phantom images between the front speakers are quite accurate, with images towards the back and especially to the sides being unstable. The localisation of a virtual source, based on level differences between two loudspeakers to the side of a listener, shows great inconsistency across the standardised 5.1 setup, also being largely affected by movement away from the reference position. 5.1 surround is therefore limited in its ability to convey 3D sound, making the surround channels more appropriate for ambience or effects.)\n\n7.1 channel surround is another setup, most commonly used in large cinemas, that is compatible with 5.1 surround, though it is not stated in the ITU-standards. 7.1 channel surround adds two additional channels, center-left (CL) and center-right (CR) to the 5.1 surround setup, with the speakers situated 15 degrees off centre from the listener. This convention is used to cover an increased angle between the front loudspeakers as a product of a larger screen.\n\nMost 2-channel stereophonic microphone techniques are compatible with a 3-channel setup (LCR), as many of these techniques already contain a center microphone or microphone pair. Microphone techniques for LCR should, however, try to obtain greater channel separation to prevent conflicting phantom images between L/C and L/R for example. Specialised techniques have therefore been developed for 3-channel stereo. Surround microphone techniques largely depend on the setup used, therefore being biased towards the 5.1 surround setup, as this is the standard.\n\nSurround recording techniques can be differentiated into those that use single arrays of microphones placed in close proximity, and those treating front and rear channels with separate arrays. Close arrays present more accurate phantom images, whereas separate treatment of rear channels is usually used for ambience. For accurate depiction of an acoustic environment, such as a halls, side reflections are essential. Appropriate microphone techniques should therefore be used, if room impression is important. Although the reproduction of side images are very unstable in the 5.1 surround setup, room impressions can still be accurately presented.\n\nSome microphone techniques used for coverage of three front channels, include double-stereo techniques, INA-3 (Ideal Cardioid Arrangement), the Decca Tree setup and the OCT (Optimum Cardioid Triangle). Surround techniques are largely based on 3-channel techniques with additional microphones used for the surround channels. A distinguishing factor for the pickup of the front channels in surround is that less reverberation should be picked up, as the surround microphones will be responsible for the pickup of reverberation. Cardioid, hypercardioid, or supercardioid polar patterns will therefore often replace omnidirectional polar patterns for surround recordings. To compensate for the lost low-end of directional (pressure gradient) microphones, additional omnidirectional (pressure microphones), exhibiting an extended low-end response, can be added. The microphone’s output is usually low-pass filtered.\nA simple surround microphone configuration involves the use of a front array in combination with two backward-facing omnidirectional room microphones placed about 10–15 meters away from the front array. If echoes are notable, the front array can be delayed appropriately. Alternatively, backward facing cardioid microphones can be placed closer to the front array for a similar reverberation pickup.\n\nThe INA-5 (Ideal Cardioid Arrangement) is a surround microphone array that uses five cardioid microphones resembling the angles of the standardised surround loudspeaker configuration defined by the ITU Rec. 775. Dimensions between the front three microphone as well as the polar patterns of the microphones can be changed for different pickup angles and ambient response. This technique therefore allows for great flexibility.\n\nA well established microphone array is the Fukada Tree, which is a modified variant of the Decca Tree stereo technique. The array consists of 5 spaced cardioid microphones, 3 front microphones resembling a Decca Tree and two surround microphones. Two additional omnidirectional outriggers can be added to enlarge the perceived size of the orchestra and/or to better integrate the front and surround channels. The L, R, LS and RS microphones should be placed in a square formation, with L/R and LS/RS angled at 45 degrees and 135 degrees from the center microphone respectively. Spacing between these microphones should be about 1.8 meters. This square formation is responsible for the room impressions. The center channel is placed a meter in front of the L and R channels, producing a strong center image. The surround microphones are usually placed at the critical distance (where the direct and reverberant field is equal), with the full array usually situated several meters above and behind the conductor.\n\nThe NHK (Japanese broadcasting company) developed an alternative technique also involving 5 cardioid microphones. Here a baffle is used for separation between the front left and right channels, which are 30 cm apart. Outrigger omnidirectional microphones, low-pass filtered at 250 Hz, are spaced 3 meters apart in line with the L and R cardioids. These compensate for the bass roll-off of the cardioid microphones and also add expansiveness. A 3-meter spaced microphone pair, situated 2–3 meters behind front array, is used for the surround channels. The centre channel is again placed slightly forward, with the L/R and LS/RS again angled at 45 and 135 degrees respectively.\n\nThe OCT-Surround (Optimum Cardioid Triangle-Surround) microphone array is an augmented technique of the stereo OCT technique using the same front array with added surround microphones.The front array is designed for minimum crosstalk, with the front left and right microphones having supercardioid polar patterns and angled at 90 degrees relative to the center microphone. It is important that high quality small diaphragm microphones are used for the L and R channels to reduce off-axis coloration. Equalization can also be used to flatten the response of the supercardioid microphones to signals coming in at up to about 30 degrees from the front of the array. The center channel is placed slightly forward. The surround microphones are backwards facing cardioid microphones, that are placed 40 cm back from the L and R microphones. The L, R, LS and RS microphones pick up early reflections from both the sides and the back of an acoustic venue, therefore giving significant room impressions. Spacing between the L and R microphones can be varied to obtain the required stereo width.\n\nSpecialized microphone arrays have been developed for recording purely the ambience of a space. These arrays are used in combination with suitable front arrays, or can be added to above mentioned surround techniques. The Hamasaki square (also proposed by NHK) is a well established microphone array used for the pickup of hall ambience. Four figure-eight microphones are arranged in a square, ideally placed far away and high up in the hall. Spacing between the microphones should be between 1–3 meters. The microphones nulls (zero pickup point) are set to face the main sound source with positive polarities outward facing, therefore very effectively minimizing the direct sound pickup as well as echoes from the back of the hall The back two microphones are mixed to the surround channels, with the front two channels being mixed in combination with the front array into L and R.\n\nAnother ambient technique is the IRT (Institut für Rundfunktechnik) cross. Here, four cardioid microphones, 90 degrees relative to one another, are placed in square formation, separated by 21–25 cm. The front two microphones should be positioned 45 degrees off axis from the sound source. This technique therefore resembles back to back near-coincident stereo pairs. The microphones outputs are fed to the L, R and LS, RS channels. The disadvantage of this approach is that direct sound pickup is quite significant.\n\nMany recordings do not require pickup of side reflections. For Live Pop music concerts a more appropriate array for the pickup of ambience is the cardioid trapezium. All four cardioid microphones are backward facing and angled at 60 degrees from one another, therefore similar to a semi-circle. This is effective for the pickup of audience and ambience.\n\nAll the above-mentioned microphone arrays take up considerable space, making them quite ineffective for field recordings. In this respect, the double MS (Mid Side) technique is quite advantageous. This array uses back to back cardioid microphones, one facing forward, the other backwards, combined with either one or two figure-eight microphone. Different channels are obtained by sum and difference of the figure-eight and cardioid patterns. When using only one figure-eight microphone, the double MS technique is extremely compact and therefore also perfectly compatible with monophonic playback. This technique also allows for postproduction changes of the pickup angle.\n\nSurround replay systems may make use of \"bass management\", the fundamental principle of which is that bass content in the incoming signal, irrespective of channel, should be directed only to loudspeakers capable of handling it, whether the latter are the main system loudspeakers or one or more special low-frequency speakers called subwoofers.\n\nThere is a notation difference before and after the bass management system. Before the bass management system there is a Low Frequency Effects (LFE) channel. After the bass management system there is a subwoofer signal. A common misunderstanding is the belief that the LFE channel is the \"subwoofer channel\". The bass management system may direct bass to one or more subwoofers (if present) from \"any\" channel, not just from the LFE channel. Also, if there is no subwoofer speaker present then the bass management system can direct the LFE channel to one or more of the main speakers.\n\nBecause the \"low-frequency effects\" channel requires only a fraction of the bandwidth of the other audio channels, it is referred to as the \".1\" channel; for example \"5.1\" or \"7.1\".\n\nThe LFE channel is a source of some confusion in surround sound. It was originally developed to carry extremely low \"sub-bass\" cinematic sound effects (with commercial subwoofers sometimes going down to 30 Hz, e.g., the loud rumble of thunder or explosions) on their own channel. This allowed theaters to control the volume of these effects to suit the particular cinema's acoustic environment and sound reproduction system. Independent control of the sub-bass effects also reduced the problem of intermodulation distortion in analog movie sound reproduction. A \"sub-woofer\" capable of playing back frequencies as low as 5 Hz was developed by a small speaker manufacturer in Florida. It utilized a propellor design and required a large cabinet to move sub-sonic air mass.\n\nIn the original movie theater implementation, the LFE was a separate channel fed to one or more subwoofers. Home replay systems, however, may not have a separate subwoofer, so modern home surround decoders and systems often include a bass management system that allows bass on any channel (main or LFE) to be fed only to the loudspeakers that can handle low-frequency signals. The salient point here is that the LFE channel is not the \"subwoofer channel\"; there may be no subwoofer and, if there is, it may be handling a good deal more than effects.\n\nSome record labels such as Telarc and Chesky have argued that LFE channels are not needed in a modern digital multichannel entertainment system. They argue that all available channels have a full-frequency range and, as such, there is no need for an LFE in surround music production, because all the frequencies are available in all the main channels. These labels sometimes use the LFE channel to carry a height channel, underlining its redundancy for its original purpose. The label BIS generally uses a 5.0 channel mix.\n\nChannel notation indicates the number of discrete channels encoded in the audio signal, not necessarily the number of channels reproduced for playback. The number of playback channels can be increased by using matrix decoding. The number of playback channels may also differ from the number of speakers used to reproduce them if one or more channels drives a group of speakers. Notation represents the number of channels, not the number of speakers.\n\nThe first digit in \"5.1\" is the number of full range channels. The \".1\" reflects the limited frequency range of the LFE channel.\n\nFor example, two stereo speakers with no LFE channel = 2.0\n<br>\n5 full-range channels + 1 LFE channel = 5.1\n\nAn alternative notation shows the number of full-range channels in front of the listener, separated by a slash from the number of full-range channels beside or behind the listener, with a decimal point marking the number of limited-range LFE channels.\n\nE.g. 3 front channels + 2 side channels + an LFE channel = 3/2.1\n\nThe notation can be expanded to include Matrix Decoders. Dolby Digital EX, for example, has a sixth full-range channel incorporated into the two rear channels with a matrix. This is expressed:\n\n3 front channels + 2 rear channels + 3 channels reproduced in the rear in total + 1 LFE channel = 3/2:3.1\n\nThe term stereo, although popularised in reference to two channel audio, historically also referred to surround sound, as it strictly means \"solid\" (three-dimensional) sound. However this is no longer common usage and \"stereo sound\" almost exclusively means two channels, left and right.\n\nIn accordance with ANSI/CEA-863-A\n\nIn 2002, Dolby premiered a master of We Were Soldiers which featured a Sonic Whole Overhead Sound soundtrack. This mix included a new ceiling-mounted height channel.\n\nAmbisonics is a recording and playback technique using multichannel mixing that can be used live or in the studio and which recreates the soundfield as it existed in the space, in contrast to traditional surround systems, which can only create illusion of the soundfield if the listener is located in a very narrow sweetspot between speakers. Any number of speakers in any physical arrangement can be used to recreate a sound field. With 6 or more speakers arranged around a listener, a 3-dimensional (\"periphonic\", or full-sphere) sound field can be presented. Ambisonics was invented by Michael Gerzon.\n\nBinaural recording is a method of recording sound that uses two microphones, arranged with the intent to create the 3-D stereo experience of being present in the room with the performers or instruments. The idea of a three dimensional or \"internal\" form of sound has developed into technology for stethoscopes creating \"in-head\" acoustics and IMAX movies creating a three dimensional acoustic experience.\n\nPanAmbio combines a stereo dipole and crosstalk cancellation in front and a second set behind the listener (total of four speakers) for 360° 2D surround reproduction. Four channel recordings, especially those containing binaural cues, create speaker-binaural surround sound. 5.1 channel recordings, including movie DVDs, are compatible by mixing C-channel content to the front speaker pair. 6.1 can be played by mixing SC to the back pair.\n\nSeveral speaker configurations are commonly used for consumer equipment. The order and identifiers are those specified for the channel mask in the standard uncompressed WAV file format (which contains a raw multichannel PCM stream) and are used according to the same specification for most PC connectible digital sound hardware and PC operating systems capable of handling multiple channels. While it is possible to build any speaker configuration, there is little commercial movie or music content for alternative speaker configurations. However, source channels can be remixed for the speaker channels using a matrix table specifying how much of each content channel is played through each speaker channel.\nAny channel configuration may include a low frequency effects (LFE) channel (the channel played through the subwoofer.) This would make the configuration \".1\" instead of \".0\". Most modern multichannel mixes contain an LFE.\n\n7.1 surround sound is a popular format in theaters & Home cinema including Blu-rays with Dolby and DTS being major players\n\n10.2 is the surround sound format developed by THX creator Tomlinson Holman of TMH Labs and University of Southern California (schools of Cinema/Television and Engineering). Developed along with Chris Kyriakakis of the USC Viterbi School of Engineering, \"10.2\" refers to the format's promotional slogan: \"Twice as good as 5.1\". Advocates of 10.2 argue that it is the audio equivalent of IMAX.\n\n11.1 sound is supported by BARCO with installations in theaters worldwide.\n\n22.2 is the surround sound component of Ultra High Definition Television, developed by NHK Science & Technical Research Laboratories. As its name suggests, it uses 24 speakers. These are arranged in three layers: A middle layer of ten speakers, an upper layer of nine speakers, and a lower layer of three speakers and two sub-woofers. The system was demonstrated at Expo 2005, Aichi, Japan, the NAB Shows 2006 and 2009, Las Vegas, and the IBC trade shows 2006 and 2008, Amsterdam, Netherlands.\n\n\n"}
{"id": "2607361", "url": "https://en.wikipedia.org/wiki?curid=2607361", "title": "Talkback (recording)", "text": "Talkback (recording)\n\nIn sound recording, a talkback system is the intercom used in recording studios and production control rooms (PCRs) in television studios to enable personnel to communicate with people in the recording area or booth. While the control room can hear the person in the booth over the studio microphones, the person in the booth hears the control room over a PA, monitor speaker, in their headphones or Interruptible feedback (IFB) earpiece. Take numbers, reference data, and sometimes count-ins or remarks are also \"stamped\" onto recordings through talkback, similar to a clapperboard. \n\nThe audio quality of talkback systems is usually markedly lower than that of studio microphones and speakers, coming from a simple microphone (which may be omnidirectional or unidirectional) built or plugged into the audio mixer, and with its sound often compressed. Since talkback is usually edited out of master recordings, high fidelity isn't essential, and studios tend to cut budget corners when possible. Compression allows comments from around the control room to be audible.\n\nOccasionally instructions and comments from talkback systems do appear in studio recordings, notably in records by The Beach Boys, The Beatles, Spoon, and Bob Dylan. They frequently turn up in bootleg or \"sessions\" records.\n"}
{"id": "23159492", "url": "https://en.wikipedia.org/wiki?curid=23159492", "title": "UMA Today", "text": "UMA Today\n\nUMA Today is an international consortium of companies joined together to lead the adoption of 3GPP UMA technology around the world.\n\nUMA is the commercial name for the global 3GPP Generic Access Network (GAN) standard for fixed-mobile convergence (FMC). UMA enables secure, scalable access to mobile voice, data and IMS services over broadband IP access networks. By deploying UMA, mobile operators can deliver a number of compelling FMC services. The most well-known applications of UMA include dual-mode Wi-Fi/cellular phones. Leading operators around the world have embraced UMA as the foundation for their FMC strategies, including France Telecom/Orange, T-Mobile (USA), Rogers Wireless, TeliaSonera and Cincinnati Bell.\n\nUMA Today publishes the UMA Today Magazine hosts Webinars, sends news alerts to a subscription list and is involved in other industry activity to promote UMA technology. In addition, UMA Today co-sponsored the UMA Innovation Awards with Orange/France Telecom in 2008 and 2009. \n\nAs of October 2010, T-Mobile USA announced it was using Kineto Wireless Smart Wi-Fi technology as the enabling technology for its Wi-Fi Calling service offer.\n\n"}
{"id": "48160061", "url": "https://en.wikipedia.org/wiki?curid=48160061", "title": "Unbalanced circuit", "text": "Unbalanced circuit\n\nIn electrical engineering, an unbalanced circuit is one in which the transmission properties between the ports of the circuit are different for the two poles of each port. It is usually taken to mean that one pole of each port is bonded to a common potential (single-ended signalling) but more complex topologies are possible. This common point is commonly called \"ground\" or \"earth\" but it may well not actually be connected to electrical ground at all.\n\nThe figure shows two versions of a simple low-pass filter, unbalanced version (A) and balanced version (B). Both circuits have exactly the same effect as filters, they have the same transfer function. However, on the unbalanced circuit, the bottom pole of the input port is connected directly to the bottom pole of the output port. Thus, the impedance between the top poles is greater than the impedance between the bottom poles from input to output. For a circuit to be balanced the impedance of the top leg must be the same as the impedance of the bottom leg so that the transmission paths are identical. To achieve this, the inductor in the balanced version is split into two equal inductors, each with half the original inductance.\n\nThe figure shows the circuit of a typical tuned amplifier. The lower pole of the input port is connected directly to the lower pole of the output port. This connection also forms the negative rail of the supply voltage. This scheme is typical of many electronic circuits that are not required to have differential inputs or outputs. An example of a circuit that does not follow this pattern is the differential amplifier.\n\nThe basic advantage of using an unbalanced circuit topology, as compared to an equivalent balanced circuit, is that far fewer components are required. The difficulties come when a port of the circuit is to be connected to a transmission line or to an external device that requires differential input or output. Many transmission lines are intrinsically an unbalanced format such as the widely used coaxial cable. In such cases the circuit can be directly connected to the line. However, connecting an unbalanced circuit to, for instance, a twisted pair line, which is an intrinsically balanced format, makes the line susceptible to common-mode interference. \n\nFor this reason, balanced lines are normally driven from balanced circuits. One option is to redesign the circuit so that it is in a balanced format. If that is not possible or desirable, a balun, a device for converting between balanced and unbalanced formats, may be used.\n\n"}
{"id": "24504297", "url": "https://en.wikipedia.org/wiki?curid=24504297", "title": "Women's Engineering Society", "text": "Women's Engineering Society\n\nThe Women's Engineering Society is a United Kingdom professional learned society and networking body for women engineers, scientists and technologists.\n\nThe society was formed in 1919, after the First World War, during which many women had taken up roles in engineering to replace men who were involved in the military effort. There had been an attitude among employers and trades unions that denied women jobs and training in engineering. While it had been seen as necessary to bring women into engineering to fill the gap left by men joining the armed forces, government, employers and trades unions were against the continuing employment of women after the war.\n\nThis led a group of women, including Lady Katherine Parsons and her daughter Rachel Parsons, also Verena Holmes who would become the first female member of the Institution of Mechanical Engineers to form the Women's Engineering Society, with the aim of enabling women to gain training, jobs and acceptance. There is a parallel with the difficulties faced by women in medicine in the 19th century. The Society's first Secretary was Caroline Haslett.\n\nThe society has an archive documenting women's status in engineering and provides an insight into women's changing role in society. The archive is hosted by the IET.\n\nThe society celebrated its 95th year in 2014 with the launch of National Women in Engineering Day on 23 June 2014.\n\nThe Society is a company limited by guarantee 162096 registered at Companies House and it a charity 1008913 registered at the Charity Commission.\n\nSociety members have advised the UK government on evolving employment practices for women. Constituted as a professional society with membership grades based on qualification and experience, the society promotes the study and practice of engineering and allied sciences among women.\n\nWES is represented by groups. The work of the groups focuses on:\n\nThe society's journal \"The Woman Engineer\" contained technical articles in its early years. It now contains articles which give engineers a view of work in engineering disciplines and celebrates the achievements of women.\n\nThe Women's Engineering Society holds an annual conference, a student conference and regional workshops and networking events.\n\nIn 2014 WES set up an outreach programme called Magnificent Women (and their flying machines) which replicates the work that women did during the First World War in making aircraft wings, and this is aimed at secondary school girls.\n\nWES members often volunteer in schools to inspire girls to take up engineering and allied science careers. In 1969, President Verena Holmes left a legacy to fund an annual lecture to inspire school girls. Run by the Verena Holmes Trust, the first lecture tour was in 1969 during the first UK Women in Engineering Year.\n\nMembers provided the 'technical women power' for the WISE Buses that were launched following the WISE Year in 1984. They continue to undertake activities in schools, often through the UK STEM Ambassador scheme.\n\nMentorSET is a mentoring scheme for engineers, inspired by the WES President Petra Gratton (née Godwin) in 2000. The scheme was a collaborative project between WES and the national network of women scientists (AWISE). The philosophy was to enable women to joining a bespoke mentoring scheme to help them progress in their career and to support them back into engineering after a career break. MentorSET has previously been funded by DTI, the UK Resource Centre for Women in SET, and BAE Systems. In 2015 the MentorSET programme was relaunched with funding from DECC, now BEIS and Women in Nuclear and is now relevant to women working in science and technology as well as engineering. \n\nMembers are drawn from women who have entered the profession through routes varying from traditional apprenticeship to higher education leading to graduate and further degrees. The participation of male engineers in the society is encouraged.\n\nCurrent membership exceeds 1000 individuals and over 35 corporate and education partners.\n\n"}
{"id": "20142339", "url": "https://en.wikipedia.org/wiki?curid=20142339", "title": "WriteOnline", "text": "WriteOnline\n\nWriteOnline is an online word processor from Crick Software that incorporates writing support tools and is designed for schools and colleges.\n\nWriteOnline is a WYSIWYG, page-view word processor which includes software speech, word prediction, Wordbars and a graphic organiser.\n\nWriteOnline includes features to make it accessible for users with visual impairments, and also includes a switch accessible onscreen keyboard for users unable to use a standard keyboard.\n\nWriteOnline is a Java application. It is available in UK English and US English.\n\n\n\n"}
