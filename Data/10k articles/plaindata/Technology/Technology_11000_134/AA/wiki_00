{"id": "13292275", "url": "https://en.wikipedia.org/wiki?curid=13292275", "title": "Analytics in higher education", "text": "Analytics in higher education\n\nAcademic analytics is basically defined as the process of evaluating and analysing organisational data received from university systems for reporting and decision making reasons (Campbell, & Oblinger, 2007). According to Campbell & Oblinger (2007), accrediting agencies, governments, parents and students are all calling for the adoption of new modern and efficient ways of improving and monitoring student success. This has ushered the higher education system into an era characterised by increased scrutiny from the various stakeholders. For instance, the Bradley review acknowledges that benchmarking activities such as student engagement serve as indicators for gauging the institution’s quality (Commonwealth Government of Australia, 2008). \n\nIncreased competition, accreditation, assessment and regulation are the major factors encouraging the adoption of analytics in higher education. Although institutions of higher learning gather a lot of vital data that can significantly aid in solving problems like attrition and retention, the collected data is not being analysed adequately and hence translated into useful data (Goldstein, 2005.) Subsequently, higher education leadership are forced to make critical and vital decisions based on inadequate information that could be achieved by properly utilising and analysing the available data (Norris, Leonard, & strategic Initiatives Inc., 2008). This gives rise to strategic problems. This setback also depicts itself at the tactical level. Learning and teaching at institutions of higher education if often a diverse and complex experience. Each and every teacher, student or course is quite different. However, LMS is tasked with taking care of them all. LMS is at the centre of academic analytics. It records each and every student and staff’s information and results in a click within the system. When this crucial information is added, compared and contrasted with different enterprise information systems provides the institution with a vast array of useful information that can be harvested to gain a competitive edge (Dawson & McWilliam, 2008; Goldstein, 2005; Heathcoate & Dawson, 2005).\n\nIn order to retrieve meaningful information from institution sources i.e. LMS, the information has to be correctly interpreted against a basis of educational efficiency, and this action requires analysis from people with learning and teaching skills. Therefore, a collaborative approach is required from both the people guarding the data and those who will interpret it, otherwise the data will remain to be a total waste (Baepler & Murdoch, 2010). Decision making at its most basic level is based on presumption or intuition (a person can make conclusions and decisions based on experience without having to do data analysis) (Siemens & Long, 2011). However, a lot of decisions made at institutions of higher learning are too vital to be based on anecdote, presumption or intuition since significant decisions need to be backed by data and facts.\n\nAnalytics, which is often termed as business intelligence, has come out as new software and hardware that enables businesses to gather and analyse large amounts of information or data. The analytics process is made up of gathering, analysing, data manipulation and employing the results to answer critical questions such as ‘why’. Analytics was first applied in the admissions department in higher education institutions. The institutions normally used some formulas to choose students from a large pool of applicants. These formulas drew their information from high school transcripts and standardized test scores. In today’s world, analytics is commonly used in administrative units such as fund raising and admissions. The use and application of academic analytics is meant to grow due to the ever-increasing concerns about student success and accountability. Academic analytics primarily marries complex and vast data with predictive modelling and statistical techniques to better decision making. Current academic analytics initiatives are bent to use data to predict students experiencing difficulty (Arnold, & Pistilli, 2012, April). This allows advisors and faculty members to intervene by tailoring procedures which will meet the student’s learning needs (Arnold, 2010). As such, academic analytics possesses the ability to improve learning, student success and teaching. Analytics has become a valuable tool for institutions because of its ability to predict, model and improve decision making.\n\nAnalysis is made up of five basic steps: capture, report, predict, act and refine\nCapture: All analytic efforts are centred on data. Consequently, academic analytics can be rooted in data from various sources such as a CMS, and financial systems (Campbell, Finnegan, & Collins, 2006). Additionally, the data comes in various different formats for example spread sheets. Also, data can be got from the institution’s external environment. To capture data, academic analytics needs to determine the type of available data, methods of harnessing it and the formats it is in. \nReport: After the data has been captured and stored in a central location, analysts will examine the data, perform queries, identify patterns, trends and exceptions depicted by the data. The standard deviation and mean (descriptive statistics) are mostly generated.\n\nPredict: After analysing the warehoused data through the use of statistics, a predictive model is developed. These models vary depending on the question nature and type of data. To develop a probability, these models employ statistical regression concepts and techniques. Predictions are made after the use of statistical algorithms.\n\nAct: The major goal and aim of analytics is to enable the institution to take actions based on the probabilities and predictions made. These actions might vary from invention to information. The interventions to address problems might be in the form of a personal email, phone call or an automated contact from faculty advisors about study resources and skills, such as office hours or help sessions. Undoubtedly, institutions have to come up with appropriate mechanisms for impact measurement; such as did the students actually respond or attend the help sessions when invited.\nRefine: Academic analytics should also be made up of a process aimed at self-improvement. Statistics processes should be continually updated since the measurement of project impacts is not a one-time static effort but rather a continual effort. For instance, admission analytics should be updated or revised yearly. \n\nAnalytics affects executive officers, students, faculty members, IT staff and student affairs staff. Whereas students will be keen to know academic analytics will affect their grades, faculty members will be interested in finding out how the information and data can be appropriated for other purposes (Pistilli, Arnold & Bethune, 2012). Moreover, the institution staff will be focussed on finding how the analysis will enable them to effectively accomplish their jobs while the institution president will be focussed on freshman retention and increase in graduation rates.\n\nAnalytics have been criticised for various reasons such as profiling. Their main use is to profile students into successful and unsuccessful categories. However, some individuals argue that profiling of students tends to bias people’s behaviours and expectations (Ferguson, 2012). Additionally, there is no clear guidelines on which profiling issues should be prohibited or allowed in institutions of higher learning.\n\n\nArnold, K. E. (2010). Signals: Applying Academic Analytics. Educause Quarterly, 33(1), n1. (accountability)\n\nArnold, K. E., & Pistilli, M. D. (2012, April). Course Signals at Purdue: Using learning analytics to increase student success. In Proceedings of the 2nd International Conference on Learning Analytics and Knowledge (pp. 267-270). ACM.\n\nBaepler, P., & Murdoch, C. J. (2010). Academic analytics and data mining in higher education. International Journal for the Scholarship of Teaching and Learning, 4(2), 17.\n\nCampbell, J. P., & Oblinger, D. G. (2007). Academic Analytics. Educause Article.\n\nCampbell, J. P., Finnegan, C., & Collins, B. (2006). Academic analytics: Using the CMS as an early warning system. In WebCT impact conference.\nCommonwealth Government of Australia. (2008). Review of Australian Higher Education o. Document Number)\n\nDawson, S., & McWilliam, E. (2008). Investigating the application of IT generated data as an indicator of learning and teaching performance: Queensland University of Technology and the University of British Columbia. (A. L. a. T. Council o. Document Number)\n\nFerguson, R. (2012). Learning analytics: drivers, developments and challenges. International Journal of Technology Enhanced Learning, 4(5), 304-317.\nGoldstein, P. (2005). Academic analytics: The uses of management information and technology in Higher Education o. Document Number)\n\nHeathcoate, L., & Dawson, S. (2005). Data Mining for Evaluation, Benchmarking and Reflective Practice in a LMS. E-Learn 2005: World conference on E-Learning in corporate, government, healthcare and higher education.\n\nNorris, D. M., Leonard, J., & Strategic Initiatives Inc. (2008). What Every Campus Leader Needs to Know About Analytics o. Document Number)\n\nPistilli, M. D., Arnold, K., & Bethune, M. (2012). Signals: Using academic analytics to promote student success. EDUCAUSE Review Online, 1-8.\n\nSiemens, G., & Long, P. (2011). Penetrating the fog: Analytics in learning and education. Educause Review, 46(5), 30-32.\n"}
{"id": "11323169", "url": "https://en.wikipedia.org/wiki?curid=11323169", "title": "Application delivery controller", "text": "Application delivery controller\n\nAn application delivery controller (ADC) is a computer network device in a datacenter, often part of an application delivery network (ADN), that helps perform common tasks, such as those done by web accelerators to remove load from the web servers themselves. Many also provide load balancing. ADCs are often placed in the DMZ, between the outer firewall or router and a web farm.\n\n A common misconception is that an ADC is an advanced load-balancer. This is not an adequate description. In fact, an ADC includes many OSI layer 3-7 services including load-balancing. Other features commonly found in most ADCs include SSL offload, Web Application Firewall, NAT64, DNS64, and proxy/reverse proxy to name a few. \n\nThere are many type of ADC vendors. Software-only ADC vendors such as Avi Networks, Buoyant, NGINX, Snapt Inc & Pulse Secure. Traditional ADC vendors such as A10 Networks, F5 Networks, Citrix Systems & Kemp Technologies. As the market evolves there will be an increasing amount of vendors offering cloud-integrated and software-centric versus hardware based.\n\nFirst generation ADCs, starting around 2004, offered simple application acceleration and load balancing. In 2006, ADCs began to mature when they began featuring advanced applications services such as compression, caching, connection multiplexing, traffic shaping, application layer security, SSL offload and content switching combined with services like server load balancing in an integrated services framework that optimized and secured business critical application flows.\n\nApplication acceleration products were available from many companies by 2007, for example F5 Networks, Juniper Networks, Microsoft, KEMP Technologies, AVANU WebMux, Snapt Inc, and aiScaler. Cisco Systems offered application delivery controllers, until leaving the market in 2012. Market leaders like F5 Networks, Radware and Citrix had been gaining market share from Cisco in previous years.\n\nThe ADC market segment became fragmented into two general areas: 1) general network optimization and 2) application/framework specific optimization. Both types of devices improve performance, but the latter is usually more aware of optimization strategies that work best with a particular application framework, focusing on ASP.NET or AJAX applications, for example.\n\nIn 2005, a market research firm estimated the ADC market at US$ 727 million, with major vendors including F5 Networks and Cisco Systems. In 2012, Cisco Systems lost market share to companies like F5, Citrix, Radware, aiScaler and A10 Networks . F5 has since gone on to become the market leader within the ADC space.\n\nIn 2015, Brocade Communications Systems acquired the SteelApp business unit of Riverbed Technologies, Inc. to expand on its ADC offerings and consolidate its position in the Software Defined Data Center/Software Defined Network arena. Brocade renamed SteelApp to Brocade Virtual Traffic Manager (aka VTM).\n\nIn 2016, Webscale launched their Cloud-based Application Delivery Platform, with the included software-defined ADC to integrate seamlessly with the application and utilize cloud infrastructure to improve performance, availability and security \n"}
{"id": "37232595", "url": "https://en.wikipedia.org/wiki?curid=37232595", "title": "Architectural ironmongery", "text": "Architectural ironmongery\n\nArchitectural Ironmongery or (Architectural Hardware) is a term used for the manufacture and wholesale distributor of items made from iron, steel, brass, aluminium or other metals, including plastics, for use in all types of buildings. The Architectural Ironmongery range includes door handles, closers, locks, cylinder pulls and hinges (\"door furniture\"), window fittings, cupboard fittings, iron railings, handrails, balustrades, switches and sockets.\n\nThe term is sometimes used to distinguish between these items and retail of consumer goods sold in ironmongers' shops or hardware stores.\n\nUse of ironware in buildings has a long tradition, with local blacksmiths producing items for use in houses, churches and other buildings. During the Industrial Revolution, mass production of ironmongery became more widespread, though businesses often remained regionally focused. For example, in the UK, Laidlaw was founded in Manchester in 1876; Derby-based Bennetts Ironmongery can trace its history back to 1734; William Tonks & Sons was established in Leeds in 1789; and Quiggins served the Victorian era Liverpool market. The West Midlands region saw several well-known businesses established: Parker Winder & Achurch started in Birmingham in 1836, J Legge in Willenhall in 1881, and William Newton in Wolverhampton in 1750 (relocating to Birmingham in the 1820s).\n\nAfter the Second World War, the industry began to consolidate. For example, the Newton and Tonks businesses merged in 1970, acquired Legge in 1988 and Laidlaw in 1993, and were then taken over by Ingersoll Rand in 1997, and are today part of Ingersoll Rand Security Technologies, A & H Brass was established in Harrow, North-West London in 1981 (later relocating to Edgware Road in 1984).\n\nThe Guild of Architectural Ironmongers was established in 1961 to promote standards in the business of architectural ironmongery. It manages an industry accreditation scheme, GuildMark, and runs an education programme, including a three-year diploma course and a Registered Architectural Ironmonger (RegAI) scheme.\n\n"}
{"id": "52876222", "url": "https://en.wikipedia.org/wiki?curid=52876222", "title": "Artisanal food", "text": "Artisanal food\n\nArtisanal food encompasses breads, cheeses, fruit preserves, cured meats, beverages, oils, and vinegars that are made by hand using traditional methods by skilled craftworkers, known as food artisans. The foodstuff material from farmers and backyard growers can include fruit, grains and flours, milks for cheese, cured meats, fish, beverages, oils, and vinegars. The movement is focused on providing farm to fork type foods with locally sourced products that benefit the consumer, small scale growers and producers, and the local economy.\n\nFood artisans produce foods and edible foodstuffs that are not mass produced, but rather made by hand. These include cheeses, breads and baked goods, charcuterie and other foods that involve preservation or fermentation, home preservation or canning processes, and fruit preserves, cured meats, beverages, oils, and vinegars. \nFermentation or otherwise controlling the preservation environment for beneficial microorganisms can be utilized for vinegars, cheeses, cured meats, wine, oolong tea, kimchi and other examples. \nAn artisan food item is usually developed and produced over a long period of time and consumed relatively close to where the food is created.\n\nIn 2009, the Food Safety Enhancement Act was proposed and passed the House of Representatives, but did not pass. The measure was renegotiated and became known as the Food Safety Modernization Act (FSMA). On 4 January 2011, President Barack Obama signed the bill into law.\n\nSenator Jon Tester (D-MT) and Senator Kay Hagan (D-NC) introduced two amendments to the FSMA that removed local food growers and food processors from federal oversight. These growers and producers would remain under the jurisdiction of state and local health and sanitation laws, rules, and regulations.\n\nAs of 2016, there was not a published official standard or definition for artisan foods. A good working definition can be gleaned from the Tester-Hagen Amendment that stated artisanal food producers are constrained to: \"make less than $500,000 a year and sell greater than 50% of their products direct to consumers in the same state and within a 400-mile radius\".\n\nThe advertising and marketing industries have latched on to the trendy word \"artisanal\" and now have artisanal products on supermarket shelves and offerings from local fast food chains. Dunkin' Donuts came out with an \"artisanal\" bagel, Domino's Pizza dished out an \"artisanal\" pizza, Tostitos served up \"artisanal\" chips, McDonald's offered an \"artisan\" bun, Wendy's introduced an \"artisan\" egg sandwich, and Subway provided \"sandwich artisans\" to prepare lunch.\n\nFarmers' markets, either temporary or permanent, are a tremendous resource for consumers to procure artisanal foods. They exist in many communities in the United States, Canada, the United Kingdom, and throughout the European Union countries.\n\n\n\n\n\n"}
{"id": "1203072", "url": "https://en.wikipedia.org/wiki?curid=1203072", "title": "Backscatter X-ray", "text": "Backscatter X-ray\n\nBackscatter X-ray is an advanced X-ray imaging technology. Traditional X-ray machines detect hard and soft materials by the variation in x-ray intensity transmitted through the target. In contrast, backscatter X-ray detects the radiation that reflects from the target. It has potential applications where less-destructive examination is required, and can operate even if only one side of the target is available for examination.\n\nThe technology is one of two types of whole-body imaging technologies that have been used to perform full-body scans of airline passengers to detect hidden weapons, tools, liquids, narcotics, currency, and other contraband. A competing technology is millimeter wave scanner. One can refer to an airport security machine of this type as a \"body scanner\", \"whole body imager (WBI)\", \"security scanner\" or \"naked scanner\".\n\nIn the United States, the FAA Modernization and Reform Act of 2012 required that all full-body scanners operated in airports by the Transportation Security Administration use \"Automated Target Recognition\" software, which replaces the picture of a nude body with the cartoon-like representation. As a result of this law, all backscatter X-ray machines formerly in use by the Transportation Security Administration were removed from airports by May 2013, since the agency said the vendor (Rapiscan) did not meet their contractual deadline to implement the software.\n\nIn the European Union, backscatter X-ray screening of airline passengers was banned in 2012 to protect passenger safety, and the deployment at Manchester Airport was removed.\n\nBackscatter technology is based on the Compton scattering effect of X-rays, a form of ionizing radiation. Unlike a traditional X-ray machine, which relies on the transmission of X-rays through the object, backscatter X-ray detects the radiation that reflects from the object and forms an image. The backscatter pattern is dependent on the material property and is good for imaging organic material.\n\nIn contrast to millimeter wave scanners, which create a 3D image, backscatter X-ray scanners will typically only create a 2D image. For airport screening, images are taken from both sides of the human body.\n\nBackscatter X-ray was first applied in a commercial low-dose personnel scanning system by Dr. Steven W. Smith. Smith developed the Secure 1000 whole-body scanner in 1992 and then sold the device and associated patents to Rapiscan Systems, who now manufactures and distributes the device.\n\nThe following companies manufacture commercial backscatter X-ray devices that are used in security scanning applications with price ranging from $250,000 to $2,000,000:\n\nSome backscatter X-ray scanners can scan much larger objects, such as trucks and containers. This scan is much faster than a physical search and could potentially allow a larger percentage of shipping to be checked for smuggled items, weapons, drugs, or people.\n\nThe \"Z Backscatter Van\", or ZBV, from AS&E is a mobile backscatter X-ray machine in a van, which \"from the outside looks like an ordinary delivery van, allowing it to blend in to urban and other landscapes\". It is being promoted as a means of examining the contents of vehicles, containers, and dumpsters. Unlike many of the truck-based scanners, it has no \"arm\" that deploys from the van to be able to scan other vehicles passing through, therefore allowing larger items and objects to be scanned in comparison to the limitations in width and height of \"scanning arms\".\n\nOther companies in the industry are Smiths Detection and Rapiscan.\n\nThere are also gamma-ray-based systems coming to market, like the MVACIS.\n\nIn May 2011, the Electronic Privacy Information Center filed suit against the United States Department of Homeland Security (DHS) under the Freedom of Information Act, claiming that DHS had withheld nearly 1000 pages of documents related to the Z backscatter vans and other mobile backscatter devices.\n\nSince in addition to weapons, these machines are designed to be capable of detecting drugs, currency and contraband, which have no direct effect on airport security and passenger safety, some have argued that the use of these full body scanners is a violation of the 4th Amendment to the United States Constitution and can be construed as an illegal search and seizure.\n\nBackscatter x-ray technology has been proposed as an alternative to personal searches at airport and other security checkpoints easily penetrating clothing to reveal concealed weapons. It raises privacy concerns about what is seen by the person viewing the scan. Some worry that viewing the image violates confidential medical information, such as the fact a passenger uses a colostomy bag, has a missing limb or wears a prosthesis, or is transgender.\n\nThe ACLU and the Electronic Privacy Information Center are opposed to this use of the technology. The ACLU refers to backscatter x-rays as a \"virtual strip search\". According to the Transportation Security Administration (TSA), in one trial 79 percent of the public opted to try backscatter over the traditional pat-down in secondary screening.\n\nIt is \"possible for backscatter X-raying to produce photo-quality images of what's going on beneath our clothes\", thus, many software implementations of the scan have been designed to distort private areas. According to the TSA, further distortion is used in the Phoenix airport's trial system where photo-quality images are replaced by chalk outlines. The TSA has also commented that screening procedures such as having the screener viewing the image located far away from the person being screened could be a possibility.\n\nIn light of this, some journalists have expressed concern that this blurring may allow people to carry weapons or certain explosives aboard by attaching the object or substance to their genitals.\n\nThe British newspaper \"The Guardian\" has revealed concern among British officials that the use of such scanners to scan children may be illegal under the Protection of Children Act 1978, which prohibits the creation and distribution of indecent images of children. This concern may delay the introduction of routine backscatter scanning in UK airports, which had been planned in response to the attempted Christmas Day 2009 attack on Northwest Airlines Flight 253.\n\nThe Fiqh Council of North America have also issued the following fatwa in relation to full-body scanners:\nIn August 2010, it was reported that U.S. Marshals (part of the Department of Justice), saved thousands of images from a low resolution MM wave scanner: This machine does not show details of human anatomy, and is a different kind of machine from the one used in airports. TSA, part of the Department of Homeland Security, said that its scanners do not save images and that the scanners do not have the capability to save images when they are installed in airports, but later admitted that the scanners are required to be capable of saving images for the purpose of evaluation, training and testing.\n\nUnlike cell phone signals, or millimeter-wave scanners, the energy being emitted by a backscatter X-ray is a type of ionizing radiation that breaks chemical bonds. Ionizing radiation is considered carcinogenic even in very small doses but at the doses used in airport scanners this effect is believed to be negligible for an individual. If 1 million people were exposed to 520 scans in one year, one study estimated that roughly four additional cancers would occur due to the scanner, in contrast to the 600 additional cancers that would occur from the higher levels of radiation during flight.\n\nSince the scanners do not have a medical purpose, the United States Food and Drug Administration (FDA) does not need to subject them to the same safety evaluations as medical X-rays. However, the FDA has created a webpage comparing known estimates of the radiation from backscatter X-ray body scanners to that of other known sources, which cites various reasons they deem the technology to be safe.\n\nFour professors at the University of California, San Francisco, among them members of NAS and an expert in cancer and imaging, in an April 2010 letter to the presidential science and technology advisor raised several concerns about the validity of the indirect comparisons the Food and Drug Administration used in evaluating the safety of backscatter x-ray machines. They argued that the effective dose is higher than claimed by the TSA and the body scanner manufacturers because the dose was calculated as if distributed throughout the whole body, whereas most of the radiation is absorbed in the skin and tissues immediately underneath. Other professors from the radiology department at UCSF disagree with the claims of the signing four professors.\n\nThe UCSF professors requested that additional data be made public detailing the specific data regarding sensitive areas, such as the skin and certain organs, as well as data on the special (high-risk) population. In October 2010, the FDA and TSA responded to these concerns. The letter cites reports which show that the specific dose to the skin is some 89,000 times lower than the annual limit to the skin established by the NCRP. Regarding the UCSF concerns over the high-risk population to sensitive organs, the letter states that such an individual \"would have to receive more than 1,000 screenings to begin to approach the annual limit\".\n\nJohn Sedat, the principal author of the UCSF letter, responded in November 2010 that the White House's claim that full-body scanners pose no health risks to air travelers is in \"error,\" adding that the White House statement has \"many misconceptions, and we will write a careful answer pointing out their errors.\"\n\nIn a December 2, 2010 letter to the House of Representatives, Dr. Steven Smith, inventor of the body scanner in 1991, stated that the concerns of Brenner and UCSF regarding the skin dose of backscatter scanners is incorrect and the result of a confusion between dose and imaging penetration. Smith demonstrated this difference with two experiments using plastic (with a similar rate of absorption as body tissue), copper (the image subject), and an x-ray scanner. The dose-penetration experiment shows that plastic samples absorb 5% and 50% of the beam intensity respectively, whereas the imaging penetration experiment shows that plastic samples reduce the image darkness by 23% and 50% respectively. Dr. Smith states that those who calculate high skin dosage have incorrectly used the shallow imaging penetration value of a few millimeters (~), whereas the actual dosage is calculated by the deeper dose penetration.\n\nThe TSA has also made public various independent safety assessments of the Secure 1000 Backscatter X-ray Scanner.\n\nRadiation safety authorities including the National Council on Radiation Protection and Measurements, The Health Physics Society and the American College of Radiology, have stated that there is no specific evidence that full-body scans are unsafe. The Secure 1000 Backscatter X-ray scanner was developed in 1992 by Dr. Steve Smith. The scanner has been studied extensively for almost 20 years by the leading independent radiation safety authorities in the United States. Experimental and epidemiological data do not support the proposition, however, that there is a threshold dose of radiation below which there is no increased risk of cancer.\n\nThe UK Health Protection Agency has completed an analysis of the X-ray dose from backscatter scanners and has written that the dose is extremely low and \"about the same as people receive from background radiation in an hour\".\n\nThe Health Physics Society (HPS) reports that a person undergoing a backscatter scan receives approximately 0.05 μSv (or 0.005 mrems) of radiation; American Science and Engineering Inc. reports 0.09 μSv (0.009 mrems). At the high altitudes typical of commercial flights, naturally occurring cosmic radiation is considerably higher than at ground level. The radiation dose for a six-hour flight is 20 μSv (2 mrems)—200 to 400 times larger than a backscatter scan. The U.S. Nuclear Regulatory Agency limits radiation exposure to the public to less than 100 mrem (1 mSv) per year from nuclear power plants. While this is not specifically for airline-associated radiation, the limit is an effective proxy for understanding what level is deemed safe by a regulatory agency.\n\nAccording to a draft standard on the United States FDA website, the allowable dose from a scan would be 0.1 μSv, and that report uses a model whereby a 0.01 μSv dose increases an individual's risk of death by cancer during his or her lifetime by . Since the dose limit is ten times higher than 0.01 μSv, their model would predict one additional cancer death per 200 million scans. Since the airports in the UK handled 218 million passengers in 2009, if all passengers in the UK were scanned at the maximum dosage, then each year this would produce on average one additional cancer death (since there would be 200 million scans per year that the scanners were in operation), though usually each death would not occur in the same year as the particular scan that caused it, since the cancer may take years to grow. In addition, additional people would be given cancer but would die from other causes.\n\nThere may not yet be evidence of hereditary effects of x-rays administered by backscatter scanners, but backscatter scanners use the same kind of x-ray photons as are produced in medical x-ray machines but expose the subject at a considerably lower dose, so it is possible that the results from medical radiology may be relevant, at least until a study is done of any effects specific to backscatter x-ray machines. Fathers exposed to medical diagnostic x-rays are more likely to have infants who contract leukemia, especially if exposure is closer to conception or includes two or more X-rays of the lower gastrointestinal (GI) tract or lower abdomen. In medical radiography the x-ray beam is adjusted to expose only the area of which an image is required, so that generally shielding is applied to the patient to avoid exposing the gonads, whereas in an airport backscatter scan, the testicles of men and boys will be deliberately subjected to the direct beam in order to check for weapons in the underpants, and some radiation will also reach the ovaries of female subjects. A linear dose-response relationship has been observed between x-ray dose and double-strand breaks in DNA in human sperm.\n\nExtrapolations of cancer risk from minuscule exposures to radiation across large populations, however, are not supported by analysis by the National Council on Radiation Protection (NCRP). On May 26, 2010 NCRP issued a press release to address such comments about full body scanners that are compliant with ANSI N43.17. In Commentary No.16 issued on May 26, 2010, it reads as follows:\nAccording to NCRP, the use of statistical extrapolations that predict 1 death for every 200 million persons scanned for example (as above) is an unrealistic over-estimation.\n\nOther scientists at Columbia University have made the following statements in support of the safety of body scanners:\nFurthermore, other scientists claim the health effects of backscatter are well understood whereas those from millimeter wave scanners are not:\nExperts evaluating backscatter x-ray machine technology have also argued that defects in the machines, damage from normal wear-and-tear, or software errors could focus an intense dose of radiation on just one spot of the body. For example, Dr. Peter Rez, a professor of physics at Arizona State University, has said, \"The thing that worries me the most, is not what happens if the machine works as advertised, but what happens if it doesn't,\" adding that a potential malfunction of the machine could increase the radiation dose.\n\nThe designers and manufacturers of backscatter X-ray scanners claim that the scanners are designed to prevent the occurrence of these kinds of errors. The scanners' safety requirements include fail-safe controls and multiple overlapping interlocks. These features, combined with fault analysis, ensure that failure of any subsystem results in non-operation of the x-ray generator to prevent accidental exposures. In the United States, the TSA requires that certification to the ANSI N43.17 safety standard is performed by a third party and not by the manufacturer themselves.\n\nThe European Commission issued a report stating that backscatter x-ray scanners pose no known health risk, and that \"assuming all other conditions equal\", that backscatter x-ray scanners, which expose people to ionizing radiation, should not be used when millimeter-wave scanners that \"have less effects on the human body\" are available.\n\nHowever, the European Commission report provides no data substantiating its claim that \"all other conditions are equal\". One area where backscatter X-ray scanners can provide better performance than MM wave scanners, for example, is in the inspection of the shoes, groin and armpit regions of the body.\n\nIn a study published in the \"Archives of Internal Medicine\" on March 28, 2011 researchers at the University of California \"calculated that fully implementing backscatter scanners would not significantly increase the lifetime risk of cancer for travelers.\" The researchers calculated that for every 100 million passengers who flew seven one-way flights, there would be one additional cancer.\n\nIn March 2012, scientist and blogger Jonathan Corbett demonstrated the ineffectiveness of the technology by publishing a viral video showing how he was able to get a metal box through backscatter x-ray and millimeter wave scanners (including the currently-used \"Automated Target Recognition\" scanners) in two US airports. In April 2012, Corbett released a second video interviewing a TSA screener, who described firearms and simulated explosives passing through the scanners during internal testing and training.\n\nBackscatter scanners installed by the TSA until 2013 were unable to screen adequately for security threats inside hats and head coverings, casts, prosthetics and loose clothing. This technology limitation of current scanners often requires these persons to undergo additional screening by hand or other methods and can cause additional delay or feelings of harassment.\n\nThe next generation of backscatter scanners are able to screen these types of clothing, according to manufacturers; however, these machines are not currently in use in public airports.\n\nIn Germany, field tests on more than 800,000 passengers over a 10-month trial period concluded that scanners were effective, but not ready to be deployed in German airports due to a high rate of false alarms. The Italian Civil Aviation Authority removed scanners from airports after conducting a study that revealed them to be inaccurate and inconvenient. The European Commission decided to effectively ban backscatter machines. In a 2011 staff report by Republican Members of Congress about the TSA, airport body scanners were described as \"ineffective\" and \"easily thwarted.\"\n\nIn the US, manufacturers of security related equipment can apply for protection under the SAFETY act, which limits their financial liability in product liability cases to the amount of their insurance coverage. The Rapiscan Secure 1000 was listed in 2006.\n\nIn the US, an X-ray system can be considered to comply with requirements for general purpose security screening of humans if the device complies with American National Standards Institute (ANSI) Standard #N43.17.\n\nIn the most general sense, N43.17 states that a device can be used for general purpose security screening of humans if the dose to the subject is less than 25 μrems (0.25 μSv) per examination and complies with other requirements of the standard. Twenty-five micro-Rem is equal to the amount of background radiation every human is exposed to (from the air and soil) at sea level every 1.5 hours and is also equal to the radiation exposure from cosmic rays when travelling in an airplane at altitude for 2 minutes.\n\nMany types of X-ray systems can be designed to comply with ANSI N43.17 including transmission X-ray, backscatter X-ray and gamma ray systems. Not all backscatter X-ray devices necessarily comply with ANSI N43.17; only the manufacturer or end user can confirm compliance of a particular product to the standard.\n\nANSI standards use a standard of measurement algorithm called \"effective dose\" that considers the different exposure of all parts of the body and then weights them differently. The interior of the human body is given more weight in this survey, and the exterior, including the skin organ, are given less weight.\n\nSome people wish to prevent either the loss of privacy or the possibility of health problems or genetic damage that might be associated with being subjected to a backscatter X-ray scan. One company sells X-ray absorbing underwear which is said to have X-ray absorption equivalent to of lead. Another product, Flying Pasties, \"are designed to obscure the most private parts of the human body when entering full body airport scanners,\" but their description does not seem to claim any protection from the X-ray beam penetrating the body of the person being scanned.\n\n\n"}
{"id": "6964698", "url": "https://en.wikipedia.org/wiki?curid=6964698", "title": "C4ISRNET", "text": "C4ISRNET\n\nCISRNET (previously \"CISR\", or \"C4ISR: The Journal of Net-Centric Warfare\") is a publication covering emerging issues and trends in global military transformation and network centric warfare technologies, products and services for federal government managers, defense, and industry. It is published nine times per year.\n\n\"CISRNET\" was established in 2002. The magazine is published by Sightline Media Group, which was a part of Gannett Company (NYSE:GCI). \nAs part of the spinoff of digital and broadcasting properties in 2015, Gannett spun off these properties to Tegna. In March 2016, Tegna sold Sightline Media Group to Regent, a Los Angeles-based private equity firm controlled by investor Michael Reinstein.\n\"CISRNET\"'s headquarters is in Tysons, Virginia.\n\nFrom its website: \"CISRNET\" is the premier content destination for defense and government communities to stay connected to technology and network innovations to ensure information dominance. Defense and Intelligence officials rely on \"CISRNET\" for information on advanced weapons platforms, sensor systems, and command and control centers that provide information advantage, battlefield dominance, speed of command and mission effectiveness. \"CISRNET\" is a digital brand with a magazine, offering unparalleled focus and coverage for the network-based defense community.\n\nThe 16th annual \"CISRNET\" Conference is May 3, 2017, at the Renaissance Arlington Capital View.\n"}
{"id": "8303458", "url": "https://en.wikipedia.org/wiki?curid=8303458", "title": "Chaser bin", "text": "Chaser bin\n\nA chaser bin, also called grain cart or (grain) auger wagon, is a trailer towed by a tractor with a built-in auger conveyor system, usually with a large capacity (from several hundred to over 1000 bushels; around is average).\n\nThe typical setup of a chaser bin is a cross auger, which feeds the folding unload auger, which in turn empties the contents into waiting Grain hopper trailers or mother bins.\n\nThe bins range in size, from 12T to 38T in most cases. Single axle bins usually can only handle a maximum of 20T of grain, with excess weight placing undue strain on the axle, potentially causing failure, especially on rough terrain. Bins above a capacity of 20T, tend to feature a \"walking beam\" chassis. This chassis design features an independent axle setup, allowing the bin to smoothly travel across ditches and divets, as well as more easily access fields where the road is higher than the field level. It also allows the load to be evenly distributed along the length of the chassis, further reducing the risk of axle failure when traveling over rough terrain, particularly with a full bin.\n\nThe design of the bins usually follows the same pattern regardless of capacity, with smooth flowing curves to allow the grain to easily unload via the cross auger. This also helps prevent the grain sticking to the walls of the bin, preventing corrosion.\n\nThe cross auger is smaller than the main unload auger, which allows the unload auger to expel the grain at a constant rate. The bins can also feature cross auger cut-offs, which allow the operator to choke the feed rate to the unload auger if it starts struggling. Unload augers can empty a full bin in a matter of minutes, with 15\" augers unloading at a rate of 6T per minute, and 19\" augers unloading at 10T per minute. \nChaser bins are typically used to transport harvested grain or corn over fields from a header to a road train or other hauling device which is used to cover larger distances over roads. The use of a chaser bin allows the harvester to operate continuously, eliminating the need to stop and unload.\n\nThese bins usually require tractors with large power outputs and are popular on the generally larger and more open fields of the United States and Australia, though usage in Europe is increasing.\n\nMother bins, also known as \"field bins\" in Queensland, offer the farmer a convenient storage location while harvesting. They offer a large storage capacity ranging from 55T to 130T. They are usually located on a paddock road at the entrance to the paddock being harvested, and allowing chaser bins to unload into them and offering truck drivers a more convenient location to fill grain into their semi-trailers. This also assists in reducing unnecessary soil compaction, by reducing the number of road trains on a given paddock.\n\nMother bins are not to be confused with chaser bins. Although they both hold and unload grain in the same manner, mother bins, are not designed to be used as chaser bins. They are only to be moved when empty, or if needed at a maximum of 25% full on smooth terrain only. They are designed to be a static collection point only.\n"}
{"id": "12337075", "url": "https://en.wikipedia.org/wiki?curid=12337075", "title": "Code-O-Graph", "text": "Code-O-Graph\n\nThe Code-O-Graph is a field cipher device and identifier from the Captain Midnight radio serial. In the story line they were used by agents of the Secret Squadron, a paramilitary organization headed by Captain Midnight. In addition to their use as devices in the radio program, they were offered as radio premiums for listeners. As radio premiums, listeners sent in an Ovaltine proof of purchase with their names and addresses.\n\nThe Code-O-Graph incorporates a cipher disk, a cryptological device, but the letters in the message were substituted by numbers, probably to avoid ambiguity for broadcast messages. On the radio show, usually once a week, a message would be sent that could be deciphered by listeners. Each message, only a few words long, was invariably a clue to the upcoming episode.\n\nThe first Code-O-Graph, called the \"Mystery Dial\" unit,was introduced in 1941, as a device to enable Secret Squadron agents in the field to send and receive secure messages. It was in badge form, as were the next three models. The front of the badge displayed the number and cipher alphabet scales. The reverse had two windows, one labeled \"Master Code\"; the other, \"Super Code.\" each window was used for cipher key settings. As an example, if the cipher was designated as \"Master Code 3,\" it meant that the movable rotor was to spun so that the number 3 would appear in the window labeled \"Master Code.\" This setting would align the number and cipher alphabet scales correctly to decipher a message.\n\nThe second Code-O-Graph was the \"Photo-Matic\" unit. The badge had a space for a picture of the owner, to make it a photo-ID badge.\n\nThe advent of World War II affected the Code-O-Graph availability: the two previous models were made of brass, and the attack on Pearl Harbor, which propelled the United States into World War II, caused the U.S. Government to impose restrictions on manufacturing materials. Copper and brass were considered critical materials, and most of the materials were diverted to war activities. This precluded brass being used to manufacture novelties like radio premiums.\n\nThe Photo-Matic Code-O-Graph, although not distributed until 1942, was manufactured prior to the Pearl Harbor attack. Since it, and its predecessor, were undated, the newer Code-O-Graph was used for the 1943 and 1944 seasons as well as the 1942, making it the Code-O-Graph with the longest service life. The cipher setting scheme was similar to the 1941 Mystery Dial model, but there was only one cipher setting window, labeled \"Master Code.\"\n\nBy late 1944, Ovaltine scraped together enough material to manufacture another Code-O-Graph, the 1945 \"Magni-Magic\" Code-O-Graph. This unit used stamped sheet steel for the badge bodies, painted with \"gold\" paint to look similar to the earlier brass badges. The cipher disk element was made of injection-molded plastic, with the center of the disk knob in the form of a magnifying lens. The production was limited, and it was the only model of which supplies were completely exhausted.\n\nThe 1945 model altered the cipher-key setting scheme. The new method was to align one of the letters on the alphabet scale to a numeral on the number scale. For instance, the \"Master Code X-15\" setting meant that the letter X would be moved until it was next to the number 15 on the number scale. The advantage with the new scheme was that a total of 676 possible key setting combinations could be used. The disadvantage was that each key compromised one letter-number pair.\n\nThe 1946 model was the \"Mirro-Flash\" unit, the first postwar Code-O-Graph, and the last in badge form. Since the war had ended, the new badge was made of stamped sheet brass, and the plastic \"dial\" element had a small circular mirror for signaling by heliography. It used the same cipher-key setting methodology as its immediate predecessor. This practice was used for the 1947 and 1949 models as well.\n\nThe 1947 model was the first in non-badge form. It was in the shape of a police-style whistle, with the cipher elements along one side. It was called the \"Whistle Code-O-Graph,\" possibly the least imaginative name of the series.\n\nThe 1948 model, the \"Mirro-Magic\" unit, was a circular product, manufactured of brass, aluminum, plastic, and steel. The cipher letters and numbers could only be seen one at a time through windows on the front. The design had the cipher alphabet and number disks coupled by friction, and there was often slippage when trying to decipher a message. Unlike any other Code-O-Graph, the cipher-key settings utilized a pointer on the back, and a number scale from 1 through 26. Each would increment the positioning of the two scales.\n\nThe last Code-O-Graph was the \"Key-O-Matic\" unit. Possibly to compensate for the slippage of the 1948 unit's elements, the cipher alphabet and number scales were placed on interlocking gears, preventing any slippage. Resetting the cipher elements utilized a small key that was inserted into slots over one of the gears, which could be disengaged, using the key and a leaf spring as a simple clutch mechanism.\n\nA Code-O-Graph-like secret decoder ring was featured in the 1983 movie \"A Christmas Story\" as an object of both joy and frustration.\nRichard Farina - Been Down So Long It Looks Like Up To Me (1966) \n- talks about it in Chapter one, listing it first in the list of the \"only possessions and necessities of his life.\" (p. 4, in the 1983 Penguin edition)\n\n"}
{"id": "424487", "url": "https://en.wikipedia.org/wiki?curid=424487", "title": "Compressive stress", "text": "Compressive stress\n\nIn long, slender structural elements — such as columns or truss bars — an increase of compressive force \"F\" leads to structural failure due to buckling at lower stress than the compressive strength.\nCompressive stress has stress units (force per unit area), usually with negative values to indicate the compaction. However, in geotechnical engineering, compressive stress is represented with positive values.\n\nCompressive stress is defined in the same way as the tensile stress but it has negative values so as to express the compression since dL has the opposite direction. ( L is the length of the object.)\n\nCompression stress= -( F/A)\n\nWhere F= Force applied on the object. \n\nA= Area of cross section of the object. \n"}
{"id": "28861619", "url": "https://en.wikipedia.org/wiki?curid=28861619", "title": "Craftsman furniture", "text": "Craftsman furniture\n\n\"Craftsman furniture\" refers to the Arts and Crafts Movement style furniture of Gustav Stickley's Craftsman Workshops.\n\nStickley began making American Craftsman furniture in 1900, though he did not change the name of his firm to the Craftsman Workshops until 1903. It was sometimes popularly referred to as Mission Style Furniture, a term which Stickley despised. The company ceased making furniture in 1916.\n\n"}
{"id": "40217584", "url": "https://en.wikipedia.org/wiki?curid=40217584", "title": "Daisy grubber", "text": "Daisy grubber\n\nA daisy grubber is a garden tool that is used to pull out roots. It is effective because it can pull out deep roots yet cause little or no disturbance to the surrounding soil.\n"}
{"id": "9008", "url": "https://en.wikipedia.org/wiki?curid=9008", "title": "Debit card", "text": "Debit card\n\nA debit card (also known as a bank card, plastic card or check card) is a plastic payment card that can be used instead of cash when making purchases. It is similar to a credit card, but unlike a credit card, the money comes directly from the user's bank account when performing a transaction.\n\nSome cards might carry a stored value with which a payment is made, while most relay a message to the cardholder's bank to withdraw funds from a payer's designated bank account. In some cases, the primary account number is assigned exclusively for use on the Internet and there is no physical card.\n\nIn many countries, the use of debit cards has become so widespread that their volume has overtaken or entirely replaced cheques and, in some instances, cash transactions. The development of debit cards, unlike credit cards and charge cards, has generally been country specific resulting in a number of different systems around the world, which were often incompatible. Since the mid-2000s, a number of initiatives have allowed debit cards issued in one country to be used in other countries and allowed their use for internet and phone purchases.\n\nUnlike credit and charge cards, payments using a debit card are immediately transferred from the cardholder's designated bank account, instead of them paying the money back at a later date.\n\nDebit cards usually also allow for instant withdrawal of cash, acting as an ATM card for withdrawing cash. Merchants may also offer cashback facilities to customers, where a customer can withdraw cash along with their purchase.\n\nThere are currently three ways that debit card transactions are EFTPOS (also known as \"online debit\" or \"PIN debit\"), offline debit (also known as \"signature debit\"), and the Electronic Purse Card System. One physical card can include the functions of all three types, so that it can be used in a number of different circumstances.\n\nAlthough the four largest bank card issuers (American Express, Discover Card, MasterCard, and Visa) all offer debit cards, there are many other types of debit card, each accepted only within a particular country or region, for example Switch (now: Maestro) and Solo in the United Kingdom, Interac in Canada, Carte Bleue in France, EC electronic cash (formerly Eurocheque) in Germany, UnionPay in China, RuPay in India and EFTPOS cards in Australia and New Zealand. The need for cross-border compatibility and the advent of the euro recently led to many of these card networks (such as Switzerland's \"EC direkt,\" Austria's \"Bankomatkasse,\" and Switch in the United Kingdom) being re-branded with the internationally recognized Maestro logo, which is part of the MasterCard brand. Some debit cards are dual branded with the logo of the (former) national card as well as Maestro (for example, EC cards in Germany, Switch and Solo in the UK, Pinpas cards in the Netherlands, Bancontact cards in Belgium, etc.). The use of a debit card system allows operators to package their product more effectively while monitoring customer spending.\n\nOnline debit cards require electronic authorization of every transaction and the debits are reflected in the user’s account immediately. The transaction may be additionally secured with the personal identification number (PIN) authentication system; some online cards require such authentication for every transaction, essentially becoming enhanced automatic teller machine (ATM) cards.\n\nOne difficulty with using online debit cards is the necessity of an electronic authorization device at the point of sale (POS) and sometimes also a separate PINpad to enter the PIN, although this is becoming commonplace for all card transactions in many countries.\n\nOverall, the online debit card is generally viewed as superior to the offline debit card because of its more secure authentication system and live status, which alleviates problems with processing lag on transactions that may only issue online debit cards. Some on-line debit systems are using the normal authentication processes of Internet banking to provide real-time online debit transactions.\n\nOffline debit cards have the logos of major credit cards (for example, Visa or MasterCard) or major debit cards (for example, Maestro in the United Kingdom and other countries, but not the United States) and are used at the point of sale like a credit card (with payer's signature). This type of debit card may be subject to a daily limit, and/or a maximum limit equal to the current/checking account balance from which it draws funds. Transactions conducted with offline debit cards require 2–3 days to be reflected on users’ account balances.\n\nIn some countries and with some banks and merchant service organizations, a \"credit\" or offline debit transaction is without cost to the purchaser beyond the face value of the transaction, while a fee may be charged for a \"debit\" or online debit transaction (although it is often absorbed by the retailer). Other differences are that online debit purchasers may opt to withdraw cash in addition to the amount of the debit purchase (if the merchant supports that functionality); also, from the merchant's standpoint, the merchant pays lower fees on online debit transaction as compared to \"credit\" (offline).\n\nSmart-card-based electronic purse systems (in which value is stored on the card chip, not in an externally recorded account, so that machines accepting the card need no network connectivity) are in use throughout Europe since the mid-1990s, most notably in Germany (Geldkarte), Austria (Quick Wertkarte), the Netherlands (Chipknip), Belgium (Proton), Switzerland (CASH) and France (Moneo, which is usually carried by a debit card). In Austria and Germany, almost all current bank cards now include electronic purses, whereas the electronic purse has been recently phased out in the Netherlands.\n\nPrepaid debit cards that can be reloaded are also called reloadable debit cards.\n\nThe primary market for prepaid debit cards has traditionally been unbanked people; that is, people who do not use banks or credit unions for their financial transactions. But prepaid cards also appeal to other users attracted by their advantages.\n\nAdvantages of prepaid debit cards include being safer than carrying cash, worldwide functionality due to Visa and MasterCard merchant acceptance, not having to worry about paying a credit card bill or going into debt, the opportunity for anyone over the age of 18 to apply and be accepted without regard to credit quality, and the option to directly deposit paychecks and government benefits onto the card for free.\n\nIf the card provider offers an insecure website for letting you check the card's balance, this could give an attacker access to the card information.\nIf you lose the card, and have not somehow registered it, you likely lose the money. If a provider has technical issues, the money might not be accessible when you need it. Some companies' payment systems do not appear to accept prepaid debit cards. And there is a risk that prolific use of prepaid debit cards could lead data provider companies to miscategorize you in unfortunate ways.\n\nSome of the first companies to enter this market were: MiCash, RushCard, Netspend, and Green Dot who gained market share as a result of being first to market. However, since 1999, there have been several new providers, such as TransCash, 247card, iKobo. These prepaid card companies offer a number of benefits, such as money remittance services, card-to-card transfers, and the ability to apply without a social security number.\n\nIn 2009 a company called PEX Card launched a corporate expense card service aimed at business users.\n\nAs of 2017, many other companies also offer the cards.\n\nAs of 2013, several city governments (including Oakland, California and Chicago, Illinois) are now offering prepaid debit cards, either as part of a municipal ID card (for people such as illegal immigrants who are unable to obtain a state driver's license or DMV ID card) in the case of Oakland, or in conjunction with a prepaid transit pass (Chicago). These cards have been heavily criticized for their higher-than-average fees, including some (such as a flat fee added onto every purchase made with the card) that similar products offered by Green Dot and American Express do not have.\n\nThe U.S. federal government uses prepaid debit cards to make benefits payments to people who do not have bank accounts. In 2008, the U.S. Treasury Department paired with Comerica Bank to offer the Direct Express Debit MasterCard prepaid debit card.\n\nIn July 2013, the Association of Government Accountants released a report on government use of prepaid cards, concluding that such programs offer a number of advantages to governments and those who receive payments on a prepaid card rather than by check. The prepaid card programs benefit payments largely for cost savings they offer and provide easier access to cash for recipients, as well as increased security. The report also advises that governments should consider replacing any remaining cheque-based payments with prepaid card programs in order to realize substantial savings for taxpayers, as well as benefits for payees.\n\nIn January 2016, the UK government introduced fee-free basic bank accounts for all, having a significant impact on the prepaid industry, including the departure of a number of firms.\n\nConsumer protections vary, depending on the network used. Visa and MasterCard, for instance, prohibit minimum and maximum purchase sizes, surcharges, and arbitrary security procedures on the part of merchants. Merchants are usually charged higher transaction fees for credit transactions, since debit network transactions are less likely to be fraudulent. This may lead them to \"steer\" customers to debit transactions. Consumers disputing charges may find it easier to do so with a credit card, since the money will not immediately leave their control. Fraudulent charges on a debit card can also cause problems with a checking account because the money is withdrawn immediately and may thus result in an overdraft or bounced checks. In some cases debit card-issuing banks will promptly refund any disputed charges until the matter can be settled, and in some jurisdictions the consumer liability for unauthorized charges is the same for both debit and credit cards.\n\nIn some countries, like India and Sweden, the consumer protection is the same regardless of the network used. Some banks set minimum and maximum purchase sizes, mostly for online-only cards. However, this has nothing to do with the card networks, but rather with the bank's judgement of the person's age and credit records. Any fees that the customers have to pay to the bank are the same regardless of whether the transaction is conducted as a credit or as a debit transaction, so there is no advantage for the customers to choose one transaction mode over another. Shops may add surcharges to the price of the goods or services in accordance with laws allowing them to do so. Banks consider the purchases as having been made at the moment when the card was swiped, regardless of when the purchase settlement was made. Regardless of which transaction type was used, the purchase may result in an overdraft because the money is considered to have left the account at the moment of the card swiping.\n\nDebit cards and secured credit cards are popular among college students who have not yet established a credit history. Debit cards may also be used by expatriated workers to send money home to their families holding an affiliated debit card.\n\nTo the consumer, a debit transaction is perceived as occurring in real-time; \"i.e.\" the money is withdrawn from their account immediately following the authorization request from the merchant, which in many countries, is the case when making an online debit purchase. However, when a purchase is made using the \"credit\" (offline debit) option, the transaction merely places an authorization hold on the customer's account; funds are not actually withdrawn until the transaction is reconciled and hard-posted to the customer's account, usually a few days later. However, the previous sentence applies to all kinds of transaction types, at least when using a card issued by a European bank. This is in contrast to a typical credit card transaction; though it can also have a lag time of a few days before the transaction is posted to the account, it can be many days to a month or more before the consumer makes repayment with actual money.\n\nBecause of this, in the case of a benign or malicious error by the merchant or bank, a debit transaction may cause more serious problems (for example, money not accessible; overdrawn account) than in the case of a credit card transaction (for example, credit not accessible; over credit limit). This is especially true in the United States, where check fraud is a crime in every state, but exceeding your credit limit is not.\n\nDebit cards may also be used on the Internet either with or without using a PIN. Internet transactions may be conducted in either online or offline mode, although shops accepting online-only cards are rare in some countries (such as Sweden), while they are common in other countries (such as the Netherlands). For a comparison, PayPal offers the customer to use an online-only Maestro card if the customer enters a Dutch address of residence, but not if the same customer enters a Swedish address of residence.\n\nInternet purchases can be authenticated by the consumer entering their PIN if the merchant has enabled a secure online PIN pad, in which case the transaction is conducted in debit mode. Otherwise, transactions may be conducted in either credit or debit mode (which is sometimes, but not always, indicated on the receipt), and this has nothing to do with whether the transaction was conducted in online or offline mode, since both credit and debit transactions may be conducted in both modes.\n\nIn some countries, banks tend to levy a small fee for each debit card transaction. In some countries (for example, the UK) the merchants bear all the costs and customers are not charged. There are many people who routinely use debit cards for all transactions, no matter how small. Some (small) retailers refuse to accept debit cards for small transactions, where paying the transaction fee would absorb the profit margin on the sale, making the transaction uneconomic for the retailer.\n\nThe banks in Angola issue by official regulation only one brand of debit cards: Multicaixa, which is also the brand name of the one and only network of ATMs and POS terminals.\n\nArCa (Armenian Card) - a national system of debit (ArCa Debit and ArCa Classic) and credit (ArCa Gold, ArCa Business, ArCA Platinum, ArCa Affinity and ArCa Co-branded) cards popular in the Republic of Armenia. Established in 2000 by 17 largest Armenian banks.\n\nDebit cards in Australia are called different names depending on the issuing bank: Commonwealth Bank of Australia: Keycard; Westpac Banking Corporation: Handycard; National Australia Bank: FlexiCard; ANZ Bank: Access card; Bendigo Bank: Cashcard.\n\nEFTPOS is very popular in Australia and has been operating there since the 1980s. EFTPOS-enabled cards are accepted at almost all swipe terminals able to accept credit cards, regardless of the bank that issued the card, including Maestro cards issued by foreign banks, with most businesses accepting them, with 450,000 point of sale terminals.\n\nEFTPOS cards can also be used to deposit and withdraw cash over the counter at Australia Post outlets participating in GiroPost, just as if the transaction was conducted at a bank branch, even if the bank branch is closed. Electronic transactions in Australia are generally processed via the Telstra Argent and Optus Transact Plus network - which has recently superseded the old Transcend network in the last few years. Most early keycards were only usable for EFTPOS and at ATM or bank branches, whilst the new debit card system works in the same way as a credit card, except it will only use funds in the specified bank account. This means that, among other advantages, the new system is suitable for electronic purchases without a delay of two to four days for bank-to-bank money transfers.\n\nAustralia operates both electronic credit card transaction authorization and traditional EFTPOS debit card authorization systems, the difference between the two being that EFTPOS transactions are authorized by a personal identification number (PIN) while credit card transactions can additionally be authorized using a contactless payment mechanism. If the user fails to enter the correct pin three times, the consequences range from the card being locked out for a minimum 24-hour period, a phone call or trip to the branch to reactivate with a new PIN, the card being cut up by the merchant, or in the case of an ATM, being kept inside the machine, both of which require a new card to be ordered.\n\nGenerally credit card transaction costs are borne by the merchant with no fee applied to the end user (although a direct consumer surcharge of 0.5 - 3% is not uncommon) while EFTPOS transactions cost the consumer an applicable withdrawal fee charged by their bank.\n\nThe introduction of Visa and MasterCard debit cards along with regulation in the settlement fees charged by the operators of both EFTPOS and credit cards by the Reserve Bank has seen a continuation in the increasing ubiquity of credit card use among Australians and a general decline in the profile of EFTPOS. However, the regulation of settlement fees also removed the ability of banks, who typically provide merchant services to retailers on behalf of Visa or MasterCard, from stopping those retailers charging extra fees to take payment by credit card instead of cash or EFTPOS.\n\nIn Bahrain debit cards are under Benefit, the interbanking network for Bahrain. Benefit is also accepted in other countries though, mainly GCC, similar to the Saudi Payments Network and the Kuwaiti KNET.\n\nIn Brazil debit cards are called \"cartão de débito\" (singular) and got popular from 2008 and on. In 2013, the 100 millionth Brazilian debit card was issued. Debit cards replaced cheques, common until the first decade of the 2000s.\n\nToday, the majority of the financial transactions (like shopping, etc.) are made using debit cards (and this system is quickly replacing cash payments). Nowadays, the majority of debit payments are processed using a card + pin combination, and almost every card comes with a chip to make transactions.\n\nThe major debit card vendors in Brazil are Visa (with Visa Electron cards) and MasterCard (with Maestro cards), as well as local brand Elo.\n\nIn Bulgaria, debit cards are accepted in almost all stores and shops, as well as in most of the hotels and restaurants in the bigger cities. Smaller restaurants or small shops often accept cash only. All Bulgarian banks can provide debit cards when you open a bank account, for maintenance costs. Usually, it is free to use debit cards on ATMs owned by the issuing bank are free of charge, and they can also be used on the ATMs of other banks for a small fee (3-10 times cheaper than using a credit card). The most common cards in Bulgaria are Maestro and Visa Electron, accepted everywhere together with Visa and MasterCard.\n\nCanada has a nationwide EFTPOS system, called Interac Direct Payment (IDP). Since being introduced in 1994, IDP has become the most popular payment method in the country. Previously, debit cards have been in use for ABM usage since the late 1970s, with credit unions in Saskatchewan and Alberta introducing the first card-based, networked ATMs beginning in June 1977. Debit cards, which could be used anywhere a credit card was accepted, were first introduced in Canada by Saskatchewan Credit Unions in 1982. In the early 1990s, pilot projects were conducted among Canada's six largest banks to gauge security, accuracy and feasibility of the Interac system. Slowly in the later half of the 1990s, it was estimated that approximately 50% of retailers offered Interac as a source of payment. Retailers, many small transaction retailers like coffee shops, resisted offering IDP to promote faster service. In 2009, 99% of retailers offer IDP as an alternative payment form.\n\nIn Canada, the debit card is sometimes referred to as a \"bank card\". It is a client card issued by a bank that provides access to funds and other bank account transactions, such as transferring funds, checking balances, paying bills, etc., as well as point of purchase transactions connected on the Interac network. Since its national launch in 1994, Interac Direct Payment has become so widespread that, as of 2001, more transactions in Canada were completed using debit cards than cash. This popularity may be partially attributable to two main factors: the convenience of not having to carry cash, and the availability of automated bank machines (ABMs) and direct payment merchants on the network.\n\nDebit cards may be considered similar to stored-value cards in that they represent a finite amount of money owed by the card issuer to the holder. They are different in that stored-value cards are generally anonymous and are only usable at the issuer, while debit cards are generally associated with an individual's bank account and can be used anywhere on the Interac network.\n\nIn Canada, the bank cards can be used at POS and ABMs. Interac Online has also been introduced in recent years allowing clients of most major Canadian banks to use their debit cards for online payment with certain merchants as well. Certain financial institutions also allow their clients to use their debit cards in the United States on the NYCE network.\n\nConsumers in Canada are protected under a voluntary code entered into by all providers of debit card services, The Canadian Code of Practice for Consumer Debit Card Services (sometimes called the \"Debit Card Code\"). Adherence to the Code is overseen by the Financial Consumer Agency of Canada (FCAC), which investigates consumer complaints.\n\nAccording to the FCAC website, revisions to the code that came into effect in 2005 put the onus on the financial institution to prove that a consumer was responsible for a disputed transaction, and also place a limit on the number of days that an account can be frozen during the financial institution's investigation of a transaction.\n\nChile has an EFTPOS system called \"Redcompra\" (Purchase Network) which is currently used in at least 23,000 establishments throughout the country. Goods may be purchased using this system at most supermarkets, retail stores, pubs and restaurants in major urban centers. Chilean banks issue Maestro, Visa Electron and Visa Debit cards.\n\nColombia has a system called Redeban-Multicolor and Credibanco Visa which are currently used in at least 23,000 establishments throughout the country. Goods may be purchased using this system at most supermarkets, retail stores, pubs and restaurants in major urban centers. Colombian debit cards are Maestro (pin), Visa Electron (pin), Visa Debit (as credit) and MasterCard-Debit (as credit).\n\nThe Danish debit card Dankort is ubiquitous in Denmark. It was introduced on 1 September 1983, and despite the initial transactions being paper-based, the Dankort quickly won widespread acceptance. By 1985 the first EFTPOS terminals were introduced, and 1985 was also the year when the number of Dankort transactions first exceeded 1 million. Today Dankort is primarily issued as a multicard combining the national Dankort with the more internationally recognized Visa (denoted simply as a \"Visa/Dankort\" card). In September 2008, 4 million cards have been issued, of which three million cards were Visa/Dankort cards. It is also possible to get a Visa Electron debit card and MasterCard.\n\n\nMost daily customer transactions are carried out with debit cards or online giro/electronic bill payment, although credit cards and cash are accepted. Checks are no longer used. Prior to European standardization, Finland had a national standard (\"pankkikortti\"). Physically, a \"pankkikortti\" was the same as an international credit card, and the same card imprinters and slips were used for \"pankkikortti\" and credit cards, but the cards were not accepted abroad. This has now been replaced by the Visa and MasterCard debit card systems, and Finnish cards can be used elsewhere in the European Union and the world.\n\nAn electronic purse system, with a chipped card, was introduced, but did not gain much traction.\n\nSigning a payment offline entails incurring debt, thus offline payment is not available to minors. However, online transactions are permitted, and since almost all stores have electronic terminals, today also minors can use debit cards. Previously, only cash withdrawal from ATMs was available to minors (\"automaattikortti\" or Visa).\n\nCarte Bancaire (CB), the national payment scheme, in 2008, had 57.5 million cards carrying its logo and 7.76 billion transactions (POS and ATM) were processed through the e-rsb network (135 transactions per card mostly debit or deferred debit). Most CB cards are debit cards, either debit or deferred debit. Less than 10% of CB cards were credit cards.\n\nBanks in France usually charge annual fees for debit cards (despite card payments being very cost efficient for the banks), yet they do not charge personal customers for checkbooks or processing checks (despite checks being very costly for the banks). This imbalance dates from the unilateral introduction in France of Chip and PIN debit cards in the early 1990s, when the cost of this technology was much higher than it is now. Credit cards of the type found in the United Kingdom and United States are unusual in France and the closest equivalent is the deferred debit card, which operates like a normal debit card, except that all purchase transactions are postponed until the end of the month, thereby giving the customer between 1 and 31 days of \"interest-free\" credit.\n\nThe annual fee for a deferred debit card is around €10 more than for one with immediate debit. Most France debit cards are branded with the Carte Bleue logo, which assures acceptance throughout France. Most card holders choose to pay around €5 more in their annual fee to additionally have a Visa or a MasterCard logo on their Carte Bleue, so that the card is accepted internationally. A Carte Bleue without a Visa or a MasterCard logo is often known as a \"Carte Bleue Nationale\" and a Carte Bleue with a Visa or a MasterCard logo is known as a \"Carte Bleue Internationale\", or more frequently, simply called a \"Visa\" or \"MasterCard\".\n\nMany smaller merchants in France refuse to accept debit cards for transactions under a certain amount because of the minimum fee charged by merchants' banks per transaction (this minimum amount varies from €5 to €15.25, or in some rare cases even more). But more and more merchants accept debit cards for small amounts, due to the massive daily use of debit card nowadays. Merchants in France do not differentiate between debit and credit cards, and so both have equal acceptance. It is legal in France to set a minimum amount to transactions, but the merchants must display it clearly.\n\nIn January 2016, 57.2% of all the debits cards in France also had a contactless payment chip . The maximum amount per transaction is set to €20 and the maximum amount of all contactless payments per day is between 50 and €100 depending on the bank.\n\nAccording to French law, banks are liable for any transaction made with a copy of the original card and for any transaction made without a card (on the phone or on the Internet), so banks have to pay back any fraudulent transaction to the card holder if the previous criteria are met. Fighting card fraud is therefore more interesting for banks. As a consequence, French banks websites usually propose an \"e-card\" service (\"electronic (bank) card\"), where a new virtual card is created and linked to a physical card. Such virtual card can be used only once and for the maximum amount given by the card holder. If the virtual card number is intercepted or used to try to get a higher amount than expected, the transaction is blocked.\n\nDebit cards have enjoyed wide acceptance in Germany for years. Facilities already existed before EFTPOS became popular with the Eurocheque card, an authorization system initially developed for paper checks where, in addition to signing the actual check, customers also needed to show the card alongside the check as a security measure. Those cards could also be used at ATMs and for card-based electronic funds transfer (called Girocard) with PIN entry. These are now the only functions of such cards: the Eurocheque system (along with the brand) was abandoned in 2002 during the transition from the Deutsche Mark to the euro. As of 2005, most stores and petrol outlets have EFTPOS facilities. Processing fees are paid by the businesses, which leads to some business owners refusing debit card payments for sales totalling less than a certain amount, usually 5 or 10 euro.\n\nTo avoid the processing fees, many businesses resorted to using direct debit, which is then called \"electronic\" direct debit (, abbr. \"ELV\"). The point-of-sale terminal reads the bank sort code and account number from the card but instead of handling the transaction through the Girocard network it simply prints a form, which the customer signs to authorise the debit note. However, this method also avoids any verification or payment guarantee provided by the network. Further, customers can return debit notes by notifying their bank without giving a reason. This means that the beneficiary bears the risk of fraud and illiquidity. Some business mitigate the risk by consulting a proprietary blacklist or by switching to Girocard for higher transaction amounts.\n\nAround 2000, an Electronic Purse Card was introduced, dubbed Geldkarte (\"money card\"). It makes use of the smart card chip on the front of the standard issue debit card. This chip can be charged with up to 200 euro, and is advertised as a means of making medium to very small payments, even down to several euros or cent payments. The key factor here is that no processing fees are deducted by banks. It did not gain the popularity its inventors had hoped for. However, this could change as this chip is now used as means of age verification at cigarette vending machines, which has been mandatory since January 2007. Furthermore, some payment discounts are being offered (\"e.g.\" a 10% reduction for public transport fares) when paying with \"Geldkarte\". The \"Geldkarte\" payment lacks all security measures, since it does not require the user to enter a PIN or sign a sales slip: the loss of a \"Geldkarte\" is similar to the loss of a wallet or purse - anyone who finds it can then use their find to pay for their own purchases.\n\nGuinée Bissau\n\nPlease, see below on \"UEMOA\"\n\nDebit card usage surged in Greece after the introduction of Capital Controls in 2015.\n\nMost bank cards in Hong Kong for saving / current accounts are equipped with EPS and UnionPay, which function as a debit card and can be used at merchants for purchases, where funds are withdrawn from the associated account immediately.\n\nEPS is a Hong Kong only system and is widely accepted in merchants and government departments. However, as UnionPay cards are accepted more widely overseas, consumers can use the UnionPay functionality of the bank card to make purchases directly from the bank account.\n\nVisa debit cards are uncommon in Hong Kong. The British banking firm HSBC's subsidiary Hang Seng Bank's Enjoy card and American firm Citibank's ATM Visa are two of the Visa debit cards available in Hong Kong.\n\nDebit cards usage in Hong Kong is relatively low, as the credit card penetration rate is high in Hong Kong. In Q1 2017, there are near 20 million credit cards in circulation, about 3 times the adult population. There are 145800 thousand transaction made by credit cards but only 34001 thousand transactions made by debit cards.\n\nIn Hungary debit cards are far more common and popular than credit cards. Many Hungarians even refer to their debit card (\"betéti kártya\") mistakenly using the word for credit card (\"hitelkártya\").\n\nAfter the demonetization by current government there has been a surge in cashless transactions, so nowadays you could find card acceptance in most places. The debit card was mostly used for ATM transactions. RBI has announced that fees are not justified so transactions have no processing fees. Almost half of Indian debit and credit card users use Rupay card. Some Indian banks issue Visa debit cards, though some banks (like SBI and Citibank India) also issue Maestro cards. The debit card transactions are routed through Rupay (mostly),Visa or MasterCard networks in India and overseas rather than directly via the issuing bank.\n\nThe National Payments Corporation of India (NPCI) has launched a new card called RuPay. It is similar to Singapore's NETS and Mainland China's UnionPay\nForeign-owned brands issuing Indonesian debit cards include Visa, Maestro, MasterCard, and MEPS. Domestically-owned debit card networks operating in Indonesia include Debit BCA (and its Prima network's counterpart, Prima Debit) and Mandiri Debit.\n\nIraq's two biggest state-owned banks, Rafidain Bank and Rasheed Bank, together with the \"Iraqi Electronic Payment System (IEPS)\" have established a company called International Smart Card, which has developed a national credit card called 'Qi Card', which they have issued since 2008. According to the company's website: 'after less than two years of the initial launch of the Qi card solution, we have hit 1.6 million cardholder with the potential to issue 2 million cards by the end of 2010, issuing about 100,000 card monthly is a testament to the huge success of the Qi card solution. Parallel to this will be the expansion into retail stores through a network of points of sales of about 30,000 units by 2015'\n\nToday, Irish debit cards are exclusively Chip and PIN and almost entirely Visa Debit. These can be used anywhere the Visa logo is seen and in much the same way as a credit card. MasterCard debit is also used by a small minority of institutions and operates in a very similar manner.\n\nIrish debit cards are normally multi-functional and combine ATM card facilities. The cards are also sometimes used for authenticating transactions together with a card reader for 2-factor authentication on online banking.\n\nThe majority of Irish Visa Debit cards are also enabled for contactless payment for small, frequent transactions (with a maximum value of €15 or €30). Three consecutive contactless transactions are allowed, after which, the card software will refuse contactless transactions until a standard Chip and PIN transaction has been completed and the counter resets. This measure was put in place to minimise issuers' exposure to fraudulent charges.\n\nThe cards are usually processed online, but some cards can also be processed offline depending on the rules applied by the card issuer.\n\nA number of card issuers also provide prepaid debit card accounts primarily for use as gift cards / vouchers or for added security and anonymity online. These may be disposable or reloadable and are usually either Visa or MasterCard branded.\n\nPrevious system (defunct since 28 February 2014):\n\nLaser was launched by the Irish banks in 1996 as an extension of the existing ATM and Cheque guarantee card systems that had existed for many years. When the service was added, it became possible to make payments with a multifunctional card that combined ATM, cheque and debit card and international ATM facilities through MasterCard Cirrus or Visa Plus and sometimes the British Link ATM system. Their functionality was similar to the British Switch card.\n\nThe system first launched as a swipe & sign card and could be used in Ireland in much the same way as a credit card and were compatible standard card terminals (online or offline, although they were usually processed online). They could also be used in cardholder-not-present transactions over the phone, by mail or on the internet or for processing recurring payments. Laser also offered 'cash back' facilities where customers could ask retailers (where offered) for an amount of cash along with their transaction. This service allowed retailers to reduce volumes of cash in tills and allowed consumers to avoid having to use ATMs. Laser adopted EMV 'Chip and PIN' security in 2002 in common with other credit and debit cards right across Europe. In 2005, some banks issued customers with Lasers cards that were cobranded with Maestro. This allowed them to be used in POS terminals overseas, internet transactions were usually restricted to sites that specifically accepted Laser.\n\nSince 2006, Irish banks have progressively replaced Laser with international schemes, primarily Visa Debit and by 28 February 2014 the Laser Card system had been withdrawn entirely and is no longer accepted by retailers.\n\nThe Israel bank card system is somewhat confusing to newcomers, comprising a blend of features taken from different types of cards. What may be referred to as a credit card, is most likely to be a deferred debit card on an associated bank current account, the most common type of card in Israel, somewhat like the situation in France, though the term \"debit card\" is not in common usage. Cards are nearly universally called \"cartis ashrai\" (כרטיס אשראי), literally, \"credit card\", a term which may bely the card's characteristics. Its main feature may be a direct link to a connected bank account (through which they are mostly issued), with the total value of the transactions made on the card being debited from the bank account in full on a regular date once a month, without the option to carry the balance over; indeed certain types of transactions (such as online and/or foreign currency) may be debited directly from the connected bank account at the time of the transaction. Any such limited credit enjoyed is a result of the customer's assets and credibility with the bank, and not granted by the credit card company. The card usually enables immediate ATM cash withdrawals & balance inquiries (as debit cards do), instalment & deferred charge interest free transactions offered by merchants (also applicable in Brazil), interest bearing instalment plans/deferred charge/revolving credit which is transaction specific at the point of sale (though granted by the issuer, hence the interest), and a variety of automated/upon request types of credit schemes including loans, some of which revolve or resemble the extended payment options sometimes offered by charge cards.\n\nThus the \"true\" debit card is not so common in Israel, though it has existed since 1994. It is offered by two credit companies in Israel: One is ICC, short for \"Israeli Credit Cards\" (referred to as \"CAL\", an acronym formed from its abbreviation in Hebrew), which issues it in the form of a Visa Electron card valid only in Israel. It is offered mainly through the Israel Post (post office) bank (which is not allowed, by regulation, to offer any type of credit) or through Israel Discount Bank, its main owner (where it is branded as \"Discount Money Key\" card). This branded Israel Discount Bank branded debit card also offered as valid worldwide card, either as Visa Electron or MasterCard Debit cards. The second & more common debit card is offered by the Isracard consortium to its affiliate banks and is branded \"Direct\". It is valid only in Israel, under its local & unique - though immensely popular - private label brand, as \"Isracard Direct\" (which was known as \"Electro Cheque\" until 2002 and while the local brand Isracard is often viewed as a MasterCard for local use only). Since 2006, Isracard has also offered an international version, branded \"MasterCard Direct\", which is less common. These two debit card brands operate offline in Israel (meaning the transaction operates under the credit cards systems & debited officially from the cardholder account only few days later, after being processed - though reflected on the current account immediately). In 2014 the Isracard Direct card (a.k.a. the valid only in Israel version) was relaunched as Isracash, though the former subbrand still being marketed - & replaced ICC Visa Electron as Israel Post bank debit card.\n\nOverall, banks routinely offer deferred debit cards to their new customers, with \"true\" debit cards usually offered only to those who cannot obtain credit. These latter cards are not attractive to the average customer since they attract both a monthly fee from the credit company and a bank account fee for each day's debits. Isracard Direct is by far more common than the ICC Visa Electron debit card. Banks who issue mainly Visa cards will rather offer electronic use, mandate authorized transaction only, unembossed version of Visa Electron deferred debit cards (branded as \"Visa Basic\" or \"Visa Classic\") to its customers - sometimes even in the form of revolving credit card.\n\nCredit/debit card transactions in Israel are not PIN based (other than at ATMs) and it is only in recent years that EMV chip smart cards have begun to be issued, with the Bank of Israel ordering the banks and credit card companies - in 2013 - to switch customers to credit cards with the EMV security standard within 3.5 years.\n\nDebit cards are quite popular in Italy. There are both classic and prepaid cards. The main classic debit card in Italy is Bancomat/PagoBancomat: this kind of card is issued by Italian banks. Bancomat is the commercial brand for the cash withdrawal circuit, while PagoBancomat is used for POS transactions. Unlike other European countries such as UK, only a few Italian banks are issuing Visa/MasterCard debit cards (such as Intesa Sanpaolo NextCard). The main international debit circuit used by Italian banks is Mastercard's Maestro: for this reason almost every debit card issued in Italy has both PagoBancomat and Maestro logos, with Bancomat/PagoBancomat being used in Italy and the Maestro circuit when abroad. Sometimes, instead of using the Maestro circuit, the Bancomat/PagoBancomat debit card is issued along with V-Pay or Visa Electron logos, or sometimes with credit card functions (so you get a dual-mode card). In this last case, only the credit-card mode is allowed for abroad/Internet transactions, while the debit card mode is used only in Italy. The most popular prepaid debit card is \"Postepay\". It is issued by Poste italiane S.p.A., and usually runs on the Visa Electron circuit, but there are some versions that run on MasterCard. It can be used on Poste Italiane's ATMs (Postamat) and on Visa's Electron-compatible bank ATMs all over the world. It has no fees when used on the Internet and in POS-based transactions. Other cards are issued by other companies, such as Vodafone CashCard, Banca Popolare di Milano's Carta Jeans and Carta Moneta Online.\n\nIn Japan people usually use their , originally intended only for use with cash machines, as debit cards. The debit functionality of these cards is usually referred to as , and only cash cards from certain banks can be used. A cash card has the same size as a Visa/MasterCard. As identification, the user will have to enter his or her four-digit PIN when paying. J-Debit was started in Japan on March 6, 2000. However, J-Debit has not been that popular since then.\n\nSuruga Bank began service of Japan's first Visa Debit in 2006. Rakuten Bank, formally known as Ebank, offers a Visa debit card.\n\nResona Bank and The Bank of Tokyo-Mitsubishi UFJ bank also offer a Visa branded debit card.\n\nIn Kuwait, all banks provide a debit card to their account holders. This card is branded as KNET, which is the central switch in Kuwait. KNET card transactions are free for both customer and the merchant and therefore KNET debit cards are used for low valued transactions as well. KNET cards are mostly co-branded as Maestro or Visa Electron which makes it possible to use the same card outside Kuwait on any terminal supporting these payment schemes.\n\nIn Malaysia, the local debit card network is operated by the Malaysian Electronic Clearing Corporation (MyClear), which had taken over the scheme from MEPS in 2008. The new name for the local debit card in Malaysia is MyDebit, which was previously known as either bankcard or e-debit. Debit cards in Malaysia are now issued on a combo basis where the card has both the local debit card payment application as well as having that of an International scheme (Visa or MasterCard). All newly issued MyDebit combo cards with Visa or MasterCard have the contactless payment feature. The same card also acts as the ATM card for cash withdrawals.\n\nMali\n\nPlease, see below on \"UEMOA\"\n\nIn Mexico, many companies use a type of debit card called a payroll card (tarjeta de nómina), in which they deposit their employee's payrolls, instead of paying them in cash or through checks. This method is preferred in many places because it is a much safer and secure alternative compared to the more traditional forms of payment.\n\nIn the Netherlands using EFTPOS is known as \"pinnen\" (pinning), a term derived from the use of a personal identification number (PIN). PINs are also used for ATM transactions, and the term is used interchangeably by many people, although it was introduced as a marketing brand for EFTPOS. The system was launched in 1987, and in 2010 there were 258,585 terminals throughout the country, including mobile terminals used by delivery services and on markets. All banks offer a debit card suitable for EFTPOS with current accounts.\n\nPIN transactions are usually free to the customer, but the retailer is charged per-transaction and monthly fees. Equens, an association with all major banks as its members, runs the system, and until August 2005 also charged for it. Responding to allegations of monopoly abuse, it has handed over contractual responsibilities to its member banks through who now offer competing contracts. The system is organised through a special banking association Currence set up specifically to coordinate access to payment systems in the Netherlands. Interpay, a legal predecessor of Equens, was fined €47 million in 2004, but the fine was later dropped, and a related fine for banks was lowered from €17 million to €14 million. Per-transaction fees are between 5-10 eurocents, depending on volume.\n\nCredit card use in the Netherlands is very low, and most credit cards cannot be used with EFTPOS, or charge very high fees to the customer. Debit cards can often, though not always, be used in the entire EU for EFTPOS. Most debit cards are Mastercard Maestro cards. Visa's V Pay cards are also accepted at most locations.\nIn 2011 spending money using debit cards rose to 83 billion euro whilst cash spending dropped to 51 billion euro and creditcard spending grew to 5 billion.\n\nElectronic Purse Cards (called Chipknip) were introduced in 1996, but have never become very popular. The system was abolished at the end of 2014.\n\nEFTPOS (electronic fund transfer at point of sale) in New Zealand is highly popular. In 2006, 70 percent of all retail transactions were made by Eftpos, with an average of 306 Eftpos transaction being made per person. At the same time, there were 125,000 Eftpos terminals in operation (one for every 30 people), and 5.1 million Eftpos cards in circulation (1.27 per capita).\n\nThe system involves the merchant swiping (or inserting) the customer's card and entering the purchase amount. Point of sale systems with integrated EFTPOS often sent the purchase total to the terminal and the customer swipes their own card. The customer then selects the account they wish to use: Current/Cheque (CHQ), Savings (SAV), or Credit Card (CRD), before entering in their PIN. After a short processing time in which the terminal contacts the EFTPOS network and the bank, the transaction is approved (or declined) and a receipt is printed. The EFTPOS system is used for credit cards as well, with a customer selecting Credit Card and entering their PIN, or for older credit cards without loaded PIN, pressing OK and signing their receipt with identification through matching signatures. Fixed EFTPOS terminals in most businesses utilise the public switched telephone network to contact the EFTPOS network, either utilising dedicated phone lines or sharing the merchant's voice line (especially in smaller businesses). The uptake of broadband internet in the 21st century has seen some terminals move to internet protocol connections.\n\nVirtually all retail outlets have EFTPOS facilities, so much that retailers without EFTPOS have to advertise so. In addition, an increasing number of mobile operator, such as taxis, stall holders and pizza deliverers have mobile EFTPOS systems. The system is made up of two primary networks: EFTPOS NZ, which is owned by VeriFone and Paymark Limited (formerly Electronic Transaction Services Limited), which is owned by ANZ Bank New Zealand, ASB Bank, Westpac and the Bank of New Zealand. The two networks are intertwined and highly sophisticated and secure, able to handle huge volumes of transactions during busy periods such as the lead-up to Christmas: on 24 December 2012, the Paymark network alone recorded an average of 132 transactions per second between 12:00 and 13:00. Network failures are rare, but when they occur they cause massive disruption, resulting in major delays and loss of income for businesses. Most businesses have to resort to manual \"zip-zap\" swipe machines in such case. Newer POS-based terminals have the ability to \"capture\" transactions in the event of a communications break-down - instead of entering a PIN, the customer signs their receipt and the transaction is approved on a matching signature, The transaction details are stored and sent for processing once the connection to the network is restored. A notable example of this occurs on the Cook Strait ferries, where in the middle of Cook Strait there is no mobile phone reception to connect to the EFTPOS network.\n\nDepending on the user's bank, a fee may be charged for use of EFTPOS. Most youth accounts (the minimum age to obtain an Eftpos card from most banks in New Zealand is 13 years) and an increasing number of 'electronic transaction accounts' do not attract fees for electronic transactions, meaning the use of Eftpos by younger generations has become ubiquitous and subsequently cash use has become rare. Typically merchants don't pay fees for transactions, most only having to pay for the equipment rental.\n\nOne of the disadvantages of New Zealand's well-established EFTPOS system is that it is incompatible with overseas systems and non-face-to-face purchases. In response to this, many banks since 2005 have introduced international debit cards such as Maestro and Visa Debit which work online and overseas as well as on the New Zealand EFTPOS system.\n\nNiger\n\nPlease, see below on \"UEMOA\"\n\nIn the Philippines, all three national ATM network consortia offer proprietary PIN debit. This was first offered by Express Payment System in 1987, followed by Megalink with Paylink in 1993 then BancNet with the Point-of-Sale in 1994.\n\nExpress Payment System or EPS was the pioneer provider, having launched the service in 1987 on behalf of the Bank of the Philippine Islands. The EPS service has subsequently been extended in late 2005 to include the other Expressnet members: Banco de Oro and Land Bank of the Philippines. They currently operate 10,000 terminals for their cardholders.\n\nMegalink launched Paylink EFTPOS system in 1993. Terminal services are provided by Equitable Card Network on behalf of the consortium. Service is available in 2,000 terminals, mostly in Metro Manila.\n\nBancNet introduced their point of sale system in 1994 as the first consortium-operated EFTPOS service in the country. The service is available in over 1,400 locations throughout the Philippines, including second and third-class municipalities. In 2005, BancNet signed a Memorandum of Agreement to serve as the local gateway for China UnionPay, the sole ATM switch in the People's Republic of China. This will allow the estimated 1.0 billion Chinese ATM cardholders to use the BancNet ATMs and the EFTPOS in all participating merchants.\n\nVisa debit cards are issued by Union Bank of the Philippines (e-Wallet & eon), Chinatrust, Equicom Savings Bank (Key Card & Cash Card), Banco De Oro, HSBC, HSBC Savings Bank, Sterling Bank of Asia (Visa ShopNPay prepaid and debit cards)& EastWest Bank. Union Bank of the Philippines cards, EastWest Visa Debit Card, Equicom Savings Bank & Sterling Bank of Asia EMV cards which can also be used for internet purchases. Sterling Bank of Asia has released its first line of prepaid and debit Visa cards with EMV chip.\n\nMasterCard debit cards are issued by Banco de Oro, Security Bank (Cashlink & Cash Card) & Smart Communications (Smart Money) tied up with Banco De Oro. MasterCard Electronic cards are issued by BPI (Express Cash) and Security Bank (CashLink Plus).\n\nOriginally, all Visa and MasterCard based debit cards in the Philippines are non-embossed and are marked either for \"Electronic Use Only\" (Visa/MasterCard) or \"Valid only where MasterCard Electronic is Accepted\" (MasterCard Electronic). However, EastWest Bank started to offer embossed Visa Debit Cards without the for \"Electronic Use Only\" mark. Paypass Debit MasterCard from other banks also have embossed labels without the for \"Electronic Use Only\" mark. Unlike credit cards issued by some banks, these Visa and MasterCard-branded debit cards do not feature EMV chips, hence they can only be read by the machines through swiping.\n\nBy March 21, 2016, BDO has started issuing sets of Debit MasterCards having the EMV chip and is the first Philippine bank to have it. This is a response to the BSP's monitor of the EMV shift progress in the country. By 2017, all Debit Cards in the country should have an EMV chip on it.\n\nIn Poland, the first system of electronic payments was operated by Orbis, which later was changed to PolCard in 1991 (which also issued its own cards) and then that system was bought by First Data Poland Holding SA. In the mid-1990s international brands such as Visa, MasterCard, and the unembossed Visa Electron or Maestro were introduced.\nVisa Electron and Maestro work as a standard debit cards: the transactions are debited instantly, although it may happen on some occasions that a transaction is processed with some delay (hours, up to one day). These cards do not possess the options that credit cards have.\n\nIn the late 2000s contactless cards started to be introduced. The first technology to be used was MasterCard PayPass, later joined by Visa's payWave. This payment method is now universal and accepted almost everywhere. In an everyday use this payment method is always called Paypass.\nAlmost all business and stores in Poland accept debit and credit cards.\n\nIn the mid-2010s Polish banks started to replace unembossed cards with embossed electronic cards such as Debit MasterCard and Visa Debit, allowing the customers to own a card that has all qualities of a credit card (given that credit cards are not popular in Poland).\n\nThere are also some banks that do not possess an identification system to allow customers to order debit cards online.\n\nIn Portugal, debit cards are accepted almost everywhere: ATMs, stores, and so on. The most commonly accepted are Visa and MasterCard, or the unembossed Visa Electron or Maestro. Regarding Internet payments debit cards cannot be used for transfers, due to its unsafeness, so banks recommend the use of 'MBnet', a pre-registered safe system that creates a virtual card with a pre-selected credit limit. All the card system is regulated by SIBS, the institution created by Portuguese banks to manage all the regulations and communication processes proply. SIBS' shareholders are all the 27 banks operating in Portugal.\n\nIn addition to Visa, MasterCard and American Express, there are some local payment systems based in general on smart card technology.\n\n\nNearly every transaction, regardless of brand or system, is processed as an immediate debit transaction. Non-debit transactions within these systems have spending limits that are strictly limited when compared with typical Visa or MasterCard accounts.\n\nIn Saudi Arabia, all debit card transactions are routed through Saudi Payments Network (SPAN), the only electronic payment system in the Kingdom and all banks are required by the Saudi Arabian Monetary Agency (SAMA) to issue cards fully compatible with the network. It connects all point of sale (POS) terminals throughout the country to a central payment switch which in turn re-routes the financial transactions to the card issuer, local bank, Visa, Amex or MasterCard.\n\nAs well as its use for debit cards, the network is also used for ATM and credit card transactions.\n\nAll Serbian banks issue debit cards. Since August 2018, all owners of transactional accounts in Serbian dinars are automatically issued a debit card of the national brand \"DinaCard\". Other brands (VISA, MasterCard and Maestro) are more popular, better accepted and more secure, but must be requested specifically as additional cards. Debit cards are used for cash withdrawal at ATMs as well as store transactions.\n\nSingapore's debit service is managed by the Network for Electronic Transfers (NETS), founded by Singapore’s leading banks and shareholders namely DBS, Keppel Bank, OCBC and its associates, OUB, IBS, POSB, Tat Lee Bank and UOB in 1985 as a result of a need for a centralised e-Payment operator.\n\nHowever, due to the banking restructuring and mergers, the local banks remaining were UOB, OCBC, DBS-POSB as the shareholders of NETS with Standard Chartered Bank to offer NETS to their customers. However, DBS and POSB customers can use their network atms on their own and not be shared with UOB, OCBC or SCB (StanChart). The mega failure of 5 July 2010 of POSB-DBS ATM Networks (about 97,000 machines) made the government to rethink the shared ATM system again as it affected the NETS system too.\n\nIn 2010, in line with the mandatory EMV system, Local Singapore Banks started to reissue their Debit Visa/MasterCard branded debit cards with EMV Chip compliant ones to replace the magnetic stripe system. Banks involved included NETS Members of POSB-DBS, UOB-OCBC-SCB along with the SharedATM alliance (NON-NETS) of HSBC, Citibank, State Bank of India, and Maybank. Standard Chartered Bank (SCB) is also a SharedATM alliance member. Non branded cards of POSB and Maybank local ATM Cards are kept without a chip but have a Plus or Maestro sign which can be used to withdraw cash locally or overseas.\n\nMaybank Debit MasterCards can be used in Malaysia just like a normal ATM or Debit MEPS card.\n\nSingapore also uses the e-purse systems of NETS CASHCARD and the CEPAS wave system by EZ-Link and NETS.\n\nDebit cards are accepted in a relatively larger amount of stores, both large and small in Spain. Banks often offer debit cards for small fees in connection with a chequing account. These cards are used more often than credit cards at ATMs because it is a cheaper alternative.\n\nMost banks issue major-brand debit cards that can be used internationally such as Visa, MasterCard and JCB, often with contactless functionality. Payments at brick-and-mortar stores generally require a signature except for contactless payments.\n\nA separate, local debit system, known as Smart Pay, can be used by the majority of debit and ATM cards, even major-brand cards. This system is available only in Taiwan and a few locations in Japan as of 2016. Non-contactless payments require a PIN instead of a signature. Cards from a few banks support contactless payment with Smart Pay.\n\nDebit cards are widely accepted from different debit card issuers including the Network International local subsidiary of Emirates Bank.\n\nIn the UK debit cards (an integrated EFTPOS system) are an established part of the retail market and are widely accepted both by bricks and mortar stores and by internet stores. The term EFTPOS is not widely used by the public; debit card is the generic term used. Debit cards commonly issued are Debit MasterCard and Visa Debit, with Maestro, Visa Electron and UnionPay also in circulation. Banks do not charge customers for EFTPOS transactions in the UK, but some retailers make small charges, particularly where the transaction amount in question is small. The UK has converted all debit cards in circulation to Chip and PIN (except for Chip and Signature cards issued to people with certain disabilities and non-reloadable prepaid cards), based on the EMV standard, to increase transaction security; however, PINs are not required for Internet transactions (though some banks employ additional security measures for online transactions such as Verified by Visa and MasterCard Secure Code), nor for most contactless transactions.\n\nIn the United Kingdom, banks started to issue debit cards in the mid-1980s in a bid to reduce the number of cheques being used at the point of sale, which are costly for the banks to process; the first bank to do so was Barclays with the \"Barclays Connect\" card. As in most countries, fees paid by merchants in the United Kingdom to accept credit cards are a percentage of the transaction amount, which funds card holders' interest-free credit periods as well as incentive schemes such as points or cashback. For consumer credit cards issued within the EEA, the interchange fee is capped at 0.3%, with a cap of 0.2% for debit cards, although the merchant acquirers may charge the merchant a higher fee. Although merchants won the right through The Credit Cards (Price Discrimination) Order 1990 to charge customers different prices according to the payment method, few merchants in the UK charge less for payment by debit card than by credit card, the most notable exceptions being budget airlines and travel agents. Most debit cards in the UK lack the advantages offered to holders of UK-issued credit cards, such as free incentives (points, cashback etc. (the Tesco Bank debit card being one exception)), interest-free credit and protection against defaulting merchants under Section 75 of the Consumer Credit Act 1974. Almost all establishments in the United Kingdom that accept credit cards also accept debit cards, but a minority of merchants, for cost reasons, accept debit cards and not credit cards.\n\nIt is the West Africa Economic and Monetary Union federating eight countries: Benin, Burkina Faso, Côte d'Ivoire, Guinée Bissau, Mali, Niger, Senegal and Togo.\n\nGIM-UEMOA is the regional switch féderating more than 120 members (banks, microfinances, electronic money issuers, etc.). All interbank cards transactions between banks in the same country or between banks in two different countries UEMOA zone are routed and cleared by GIM-UEMOA. The settlement is done on Central Bank RTGS.\n\nGIM-UEMOA also provides some processing products and services to more than 50 banks in UEMOA zone and out of UEMOA zone.\n\nIn the U.S., EFTPOS is universally referred to simply as \"debit\". The largest pre-paid debit card company is Green Dot Corporation, by market capitalization. The same interbank networks that operate the ATM network also operate the POS network. Most interbank networks, such as Pulse, NYCE, MAC, Tyme, SHAZAM, STAR, and so on, are regional and do not overlap, however, most ATM/POS networks have agreements to accept each other's cards. This means that cards issued by one network will typically work anywhere they accept ATM/POS cards for payment. For example, a NYCE card will work at a Pulse POS terminal or ATM, and vice versa. Debit cards in the United States are usually issued with a Visa, MasterCard, Discover or American Express logo allowing use of their signature-based networks.\n\nU.S. Federal law caps the liability of a U.S. debit card user in case of loss or theft at $50 USD if the loss or theft is reported to the issuing bank in two business days after the customer notices the loss. Most banks will, however, set this limit to $0 for debit cards issued to their customers which are linked to their checking or savings account.\n\nThe fees charged to merchants for offline debit purchases vs. the lack of fees charged to merchants for processing online debit purchases and paper checks have prompted some major merchants in the U.S. to file lawsuits against debit-card transaction processors, such as Visa and MasterCard. In 2003, Visa and MasterCard agreed to settle the largest of these lawsuits for $2 billion and $1 billion respectively.\n\nSome consumers prefer \"credit\" transactions because of the lack of a fee charged to the consumer/purchaser. A few debit cards in the U.S. offer rewards for using \"credit\". However, since \"credit\" transactions cost more for merchants, many terminals at PIN-accepting merchant locations now make the \"credit\" function more difficult to access. For example, if you swipe a debit card at Wal-Mart or Ross in the U.S., you are immediately presented with the PIN screen for online debit. To use offline debit you must press \"cancel\" to exit the PIN screen, and then press \"credit\" on the next screen.\n\nAs a result of the Dodd–Frank Wall Street Reform and Consumer Protection Act, U.S. merchants can now set a minimum purchase amount for credit card transactions, as long as it does not to exceed $10.\n\nIn the United States, an FSA debit card only allow medical expenses. It is used by some banks for withdrawals from their FSAs, medical savings accounts (MSA), and health savings accounts (HSA) as well. They have Visa or MasterCard logos, but cannot be used as \"debit cards\", only as \"credit cards\". Furthermore, they are not accepted by all merchants that accept debit and credit cards, but only by those that specifically accept FSA debit cards. Merchant codes and product codes are used at the point of sale (required by law by certain merchants by certain states in the US) to restrict sales if they do not qualify. Because of the extra checking and documenting that goes on, later, the statement can be used to substantiate these purchases for tax deductions. In the occasional instance that a qualifying purchase is rejected, another form of payment must be used (a check or payment from another account and a claim for reimbursement later). In the more likely case that non-qualifying items are accepted, the consumer is technically still responsible, and the discrepancy could be revealed during an audit. A small but growing segment of the debit card business in the U.S. involves access to tax-favored spending accounts such as FSAs, HRAs, and HSAs. Most of these debit cards are for medical expenses, though a few are also issued for dependent care and transportation expenses.\n\nTraditionally, FSAs (the oldest of these accounts) were accessed only through claims for reimbursement after incurring, and often paying, an out-of-pocket expense; this often happens after the funds have already been deducted from the employee's paycheck. (FSAs are usually funded by payroll deduction.) The only method permitted by the Internal Revenue Service (IRS) to avoid this \"double-dipping\" for medical FSAs and HRAs is through accurate and auditable reporting on the tax return. Statements on the debit card that say \"for medical uses only\" are invalid for several reasons: (1) The merchant and issuing banks have no way of quickly determining whether the entire purchase qualifies for the customer's type of tax benefit; (2) the customer also has no quick way of knowing; often has mixed purchases by necessity or convenience; and can easily make mistakes; (3) extra contractual clauses between the customer and issuing bank would cross-over into the payment processing standards, creating additional confusion (for example if a customer was penalized for accidentally purchasing a non-qualifying item, it would undercut the potential savings advantages of the account). Therefore, using the card exclusively for qualifying purchases may be convenient for the customer, but it has nothing to do with how the card can actually be used. If the bank rejects a transaction, for instance, because it is not at a recognized drug store, then it would be causing harm and confusion to the cardholder. In the United States, not all medical service or supply stores are capable of providing the correct information so an FSA debit card issuer can honor every transaction-if rejected or documentation is not deemed enough to satisfy regulations, cardholders may have to send in forms manually.\n\nDebit cards are accepted in a relatively large number of stores, both large and small in Uruguay; but their use has so far remained low as compared to credit cards at ATMs. Since August 2014, with the Financial Inclusion Law coming into force, end consumers obtain a 4% VAT deduction for using debit cards in their purchases.\n\nThere has been a lack of cash due to the venezuelan economical crisis and thus the demand and use of debit cards has increased greatly during the last years. One of the reasons why a noticeable percentage of businesses has been closed is due to a lack of payment terminals. The most used brands are Maestro and Visa Electron.\n"}
{"id": "456550", "url": "https://en.wikipedia.org/wiki?curid=456550", "title": "Design language", "text": "Design language\n\nA design language or design vocabulary is an overarching scheme or style that guides the design of a complement of products or architectural settings. Designers wishing to give their suite of products a unique but consistent look and feel define a design language for it, which can describe choices for design aspects such as materials, colour schemes, shapes, patterns, textures, or layouts. They then follow the scheme in the design of each object in the suite.\n\nUsually, design languages are not rigorously defined; the designer basically makes one thing in a similar manner as another. In other cases, they are followed strictly, so that the products gain a strong thematic quality. For example, although there is a great variety of unusual chess set designs, the pieces within a set are usually thematically consistent.\n\nSometimes, designers encourage others to follow their design languages when decorating or accessorizing. In the context of graphical user interfaces, for example, human interface guidelines can be thought of as design languages for applications.\n\nIn automobiles, the design language is often in the grille design. For instance, many BMW vehicles share a design language, including front-end styling consisting of a split kidney and four circular headlights. Some manufacturers have appropriated design language cues from rival firms.\n\nIn software architecture, design languages are related to architecture description languages. The most well known design language is Unified Modeling Language.\n\n\n"}
{"id": "10868200", "url": "https://en.wikipedia.org/wiki?curid=10868200", "title": "Downhole safety valve", "text": "Downhole safety valve\n\nA downhole safety valve refers to a component on an oil and gas well, which acts as a failsafe to prevent the uncontrolled release of reservoir fluids in the event of a worst-case-scenario surface disaster. It is almost always installed as a vital component on the completion.\n\nThese valves are commonly uni-directional flapper valves which open downwards such that the flow of wellbore fluids tries to push it shut, while pressure from the surface pushes it open. This means that when closed, it will isolate the reservoir fluids from the surface.\n\nMost downhole safety valves are controlled hydraulically from the surface, meaning they are opened using a hydraulic connection linked directly to a well control panel. When hydraulic pressure is applied down a control line, the hydraulic pressure forces a sleeve within the valve to slide downwards. This movement compresses a large spring and pushes the flapper downwards to open the valve. When hydraulic pressure is removed, the spring pushes the sleeve back up and causes the flapper to shut. In this way, it is failsafe and will isolate the wellbore in the event of a loss of the wellhead. The full designation for a typical valve is 'tubing retrievable, surface controlled, subsurface safety valve', abbreviated to TR-SCSSV.\n\nThe location of the downhole safety valve within the completion is a precisely determined parameter intended to optimise safety. There are arguments against it either being too high or too low in the well and so the final depth is a compromise of all factors. MMS regulations state that the valve must be placed no less than 100' below the mudline.\n\nThe further down the well the DHSV is located, the greater the potential inventory of hydrocarbons above it when closed. This means that in the event of loss of containment at surface, there is more fluid to be spilled causing environmental damage, or in the worst case, more fuel for a fire. Therefore, placing the valve higher limits this hazard.\n\nAnother reason relates to the hydraulic control line. Hydraulic pressure is required to keep the valve open as part of the failsafe design. However, if the valve is too far down the well, then the weight of the hydraulic fluid alone may apply sufficient pressure to keep the valve open, even with the loss of surface pressurisation.\n\nAs part of the role of the DHSV to isolate the surface from wellbore fluids, it is necessary for the valve to be positioned away from the well where it could potentially come to harm. This implies that it must be placed subsurface in all circumstances, i.e. in offshore wells, not above the seabed. There is also the risk of cratering in the event of a catastrophic loss of the topside facility. The valve is specifically placed below the maximum depth where cratering is expected to be a risk.\n\nIf there is a risk of methane hydrate (clathrate) plugs forming as the pressure changes through the valve due to Joule–Thomson cooling, then this is a reason to keep it low, where the rock is warmer than an appropriately-calculated temperature.\n\nMost downhole safety valves installed as part of the completion design are classed as \"tubing retrievable\". This means that they are installed as a component of the completion string and run in during completion. Retrieving the valve, should it malfunction, requires a workover. The full name for this most common type of downhole safety valve is a Tubing Retrievable Surface Controlled Sub-Surface Valve, shortened in completion diagrams to TRSCSSV.\n\nIf a tubing retrievable valve fails, rather than go to the expense of a workover, a \"wireline retrievable\" valve may be used instead. This type of valve can fit inside the production tubing and is deployed on wireline after the old valve has been straddled open.\n\nThe importance of DHSVs is undisputed. Graphic images of oil wells in Kuwait on fire after the First Gulf War after their wellheads were removed, demonstrate the perils of not using the components (at the time, they were deemed unnecessary because they were onshore wells). It is, however, not a direct legal requirement in many places. In the United Kingdom, no law mandates the use of DHSVs. However, the 1974 Health & Safety at Work Act requires that measures are taken to ensure that the uncontrolled release of wellbore fluids is prevented even in the worst case. The brilliance of the act is that it does not issue prescriptive guideline for how to achieve the goal of health and safety, but merely sets out the requirement that the goal be achieved. It is up to the oil companies to decide how to achieve it and DHSVs are an important component of that decision. As such, although not a legal requirement, it is company policy for many operators in the UKCS.\n\nWhile the DHSV isolates the production tubing, a loss of integrity could allow wellbore fluid to bypass the valve and escape to surface through the annulus. For wells using gas lift, it may be a requirement to install a safety valve in the 'A' annulus of the well to ensure that the surface is protected from a loss of annulus containment. However, these valves are not as common and they are not necessarily installed at the same position in the well, meaning it is possible that fluids could snake their way around the valves to surface.\n\n"}
{"id": "25521752", "url": "https://en.wikipedia.org/wiki?curid=25521752", "title": "Edinburgh Calotype Club", "text": "Edinburgh Calotype Club\n\nThe Edinburgh Calotype Club (1843 – c.1850s) of Scotland was the first photographic club in the world. Its members consisted of pioneering photographers primarily from Edinburgh and St Andrews. The efforts of the Club's members resulted in the production of two of the world's earliest assembled photographic albums, consisting of more than 300 images.\n\nThe group was formed after the introduction of calotype photography to Edinburgh gentlemen by David Brewster, then Principal of St Andrews University, and also a close friend of the inventor of the calotype process, Henry Fox Talbot. Talbot sent Brewster examples of his work well before publishing on his findings, and it was Brewster who suggested that Talbot only patent his invention in England, and not Scotland, which eventually allowed for the club's formation.\n\nTalbot sent Brewster examples of his calotype photography, but Brewster had to turn to a colleague at St Andrews, the Professor of Chemistry Dr John Adamson, in order to discover how to reproduce his friend's process. Although John Adamson was the first person in Scotland to use calotype photography, it was his brother, Robert, who was to take up photography as a passion and a profession, eventually establishing the country's first photographic studio, Hill & Adamson, with painter and pioneering photographer David Octavius Hill.\n\nA visit from James Montgomery, who was studying in Edinburgh to enter the Faculty of Advocates, and a group of friends who were interested in Brewster's and John Adamson's reproduction of the calotype process, allegedly led to the formation of the Edinburgh Calotype Club itself.\n\nThe membership of the Club was composed of professional gentlemen from a variety of backgrounds - including clerics, academics and physicians - in both Edinburgh and St Andrews. Meetings of the club are described as being generally informal, and retrospective on the Club from an 1874 edition of \"The British Journal of Photography\" states that had \"neither laws, office-bearers, or formalities of any kind\":\n\n\"The meetings were held periodically at the houses of the members alternately, and generally each took the form of a breakfast, although when some greater step than ordinary had been made in advance it was generally honoured by being introduced to the members at a formal dinner.\"\n\nDespite the lack of records, the names of eight members have been traced, and the albums themselves include the names of five members as well as a number of associates.\n\nThe club's membership included many notable figures of the time, particularly from Edinburgh and St Andrews, including:\n\nThe Edinburgh Calotype Club continued meeting until sometime in the 1850s; although the exact date when it ceased to exist is not known, curators at the National Library of Scotland suggest that it was likely around the mid 1850s, \"when the albumen and collodion processes superseded the calotype... The Edinburgh Calotype Club had, in a sense, outlived its usefulness.\" The development of newer photographic technologies meant that photography was opened to a wider audience, and \"spread like wildfire over the country.\"\n\nSome members of the Club, in particular David Brewster, George Moir and Cosmo Innes, went on to become active in the later Photographic Society of Scotland that was founded in 1856. Brewster became the President of the Photographic Society of Scotland, Moir one of its two vice presidents, and Innes a council member.\n\n"}
{"id": "3599365", "url": "https://en.wikipedia.org/wiki?curid=3599365", "title": "Endrin", "text": "Endrin\n\nEndrin is an organochloride with the chemical formula CHClO that was first produced in 1950 by Shell and Velsicol Chemical Corporation. It was primarily used as an insecticide, as well as a rodenticide and piscicide. It is a colourless, odorless solid, although commercial samples are often off-white. Endrin was manufactured as an emulsifiable solution known commercially as Endrex. The compound became infamous as a persistent organic pollutant and for this reason it is banned in many countries.\n\nIn the environment endrin exists as either endrin aldehyde or endrin ketone and can be found mainly in bottom sediments of bodies of water. Exposure to endrin can occur by inhalation, ingestion of substances containing the compound, or skin contact. Upon entering the body, it can be stored in body fats and can act as a neurotoxin on the central nervous system, which can cause convulsions, seizures, or even death.\n\nAlthough endrin is not currently classified as a mutagen, nor as a human carcinogen, it is still a toxic chemical in other ways with detrimental effects. Due to these toxic effects, the manufacturers cancelled all use of endrin in the United States by 1991. Food import concerns have been raised because some countries may have still been using endrin as a pesticide.\n\nJ. Hyman & Company first developed endrin in 1950. Shell International was licensed in the United States and in the Netherlands to produce it. Velsicol was the other producer in the Netherlands. Endrin was used globally until the early 1970s. Due to its toxicity, it was banned or severely restricted in many countries. In 1982, Shell discontinued its manufacturing.\n\nIn 1962, an estimated 2.3-4.5 million kilograms of endrin were sold by Shell in the USA. In 1970, Japan imported 72,000 kilograms of endrin. From 1963 until 1972, Bali used 171 to 10,700 kilograms of endrin annually for the production of rice paddies until endrin use was discontinued in 1972. Taiwan reported to show higher levels of organochlorine pesticides including endrin in soil samples of paddy fields, compared to other Asian countries such as Thailand and Vietnam. During the 1950s-1970s over two million kilograms of organochlorine pesticides were estimated of having been be released into the environment per year. Endrin was banned in the United States on October 10, 1984. Taiwan banned endrin's use as a pesticide in 1971 and regulated it as a toxic chemical in 1989.\n\nIn May 2004, the Stockholm Convention on Persistent Organic Pollutants came into effect and listed endrin as one of the 12 initial persistent organic pollutants (POPs) that have been causing adverse effects on humans and the environment. The convention requires the participating parties to take measures to eliminate or restrict the production of POPs.\n\nThe synthesis of endrin begins with the condensation of hexachlorocyclopentadiene with vinyl chloride. The product is then dehydrochlorinated. Following reaction with cyclopentadiene, isodrin is formed. Epoxide formation by adding either peracetic acid or perbenzoic acid to the isodrin is the final step in synthesizing endrin.\n\nEndrin is a stereoisomer of dieldrin with comparable properties, though endrin degrades more easily\n\nEndrin was formulated as emulsifiable concentrates (ECs), wettable powders (WPs), granules, field strength dusts (FSDs), and pastes. The product could then be applied by aircraft or by handheld sprayers in its various formulations.\n\nEndrin has been used primarily as an agricultural insecticide on tobacco, apple trees, cotton, sugar cane, rice, cereal, and grains. It is effective against a variety of species, including cotton bollworms, corn borers, cut worms and grass hoppers. In addition, endrin has been employed as a rodenticide and avicide. In Malaysia, fish farms used a solution of endrin as a piscicide to rid mine pools and fish ponds of all fish prior to restocking.\n\nA study conducted from 1981 to 1983 in the US aimed to determine endrin's effects on non-target organisms when applied as a rodenticide in orchards. Most wildlife in and around the orchard was found to have endrin exposure, with endrin toxicity accounting for more than 24% of bird deaths recorded. Endrin was eventually banned in the US on October 10, 1984.\n\nExposure to endrin can occur by inhalation, ingestion of substances containing the compound, or by skin contact. In addition to inhalation and skin contact, infants can be exposed by ingesting the breast milk of an exposed woman. \"In utero\", fetuses are exposed by way of the placenta if the mother has been exposed.\n\nUpon entering the body, endrin metabolizes into \"anti\"-12-hydroxyendrin and other metabolites, which can be expelled in the urine and feces. Both \"anti\"-12-hydroxyendrin and its metabolite, 12-ketoendrin, are likely responsible for the toxicity of endrin. The rapid metabolism of endrin into these metabolites makes detection of endrin itself difficult unless exposure is very high.\n\nSymptoms of endrin poisoning include headache, dizziness, nervousness, confusion, nausea, vomiting, and convulsions. Acute endrin poisoning in humans affects primarily the central nervous system. There, it can act as a neurotoxin that blocks the activity of inhibitory neurotransmitters. In cases of acute exposure, this may result in seizures, or even death. Because endrin can be stored in body fats, acute endrin poisoning can lead to recurrent seizures when stressors induce the release of endrin back into the body, even months after the initial exposure is terminated.\n\nPeople occupationally exposed to endrin may experience abnormal EEG readings even if they exhibit none of the clinical symptoms, possibly due to injury to the brain stem. These readings show bilateral synchronous theta waves with synchronous spike-and-wave complexes. EEG readings can take up to one month to return to normal.\n\nThough endrin exposure has not been found to adversely affect fertility in mammals, an increase in fetal mortality has been observed in mice, rats, and mallard ducks. In those animals that have survived gestation, developmental abnormalities have been observed, particularly in rodents whose mothers were exposed to endrin early in pregnancy. In hamsters, the number of cases of fused ribs, cleft palate, open eyes, webbed feet, and meningoencephaloceles have increased. Along with open eyes and cleft palate, mice have developed with fused ribs and exencephaly. Skeletal abnormalities in rodents have also been reported.\n\nHigher doses of endrin have been found to cause the following in rodents: renal tubular necrosis; inflammation of the liver, fatty liver, and liver necrosis; possible kidney degradation; and a decrease in body weight and body weight gain.\n\nEndrin is very toxic to aquatic organisms, namely fish, aquatic invertebrates, and phytoplankton. It was found to remain in the tissues of infected fish for up to one month.\n\nFrom July 14 to September 26, 1984, an outbreak of endrin poisoning occurred in 21 villages in and around Talagang, a subdistrict of the Punjab province of Pakistan. Eighty percent of the 194 known cases were children under the age of 15. Poisoned individuals had seizures along with vomiting, pulmonary congestion, and hypoxia, leaving 19 people dead. Some individuals had low grade fevers (37.8 °C/100 °F, axillary) following seizures. The more seriously affected had less vomiting, but higher temperatures than people who were less affected. Most patients could be controlled in under two hours using diazepam, phenobarbital, and atropine, though the more seriously affected patients required general anesthesia. Recovery took up to two days. Following treatment, patients reported not remembering their seizures. The outbreak affected both men and women equally.\n\nBased on the demographics of the affected individuals and their area of residence, the outbreak was likely caused by endrin contamination of food. As members of these villages rarely had contact with one another, investigators determined that contaminated sugar shipped to the villages was the most probable cause, though no credible evidence was found to support this. Around this time, endrin was being used by cotton and sugar cane farmers in the Punjab region. A number of truck drivers stated that they had used the same trucks to deliver endrin to farmers and to pick up crops for Talagang, possibly leading to contamination.\n\nInsecticides like dieldrin and endrin have been shown to persist for decades in the environment. A definitive detection of the residues was not possible until 1971 when mass spectrometer started being used as a detector in gas chromatography. Detection of these chemicals in the environment has been reported across the world up to 2005, even though the frequency of reported cases are low due to its relatively small-scale use and very low concentrations.\n\nEndrin regularly enters the environment when applied to crops or when rain washes it off. It has been found in water, sediments, atmospheric air and biotic environment, even after uses have been stopped. Organochlorine pesticides strongly resist degradation, are poorly soluble in water but highly soluble in lipids, which is called lipophilic. This leads to bioaccumulation in fatty tissues of organisms, mainly those dwelling in water. A high bioconcentration factor of 1335-10,000 has been reported in fish. Endrin binds very strongly to organic matter in soil and aquatic sediments due to their high adsorption coefficient, making it less likely to leach into groundwater, even though contaminated groundwater samples have been found. In 2009, EPA released data indicating that the endrin in soil could last up to 14 years or more. The extent of endrin's persistence depends highly on local conditions. For example, high temperature (230 °C) or intense sunlight leads to more rapid breakdown of endrin into endrin ketone and endrin aldehyde, however, this breakdown is less than 5%.\n\nIn the United States, endrin was mainly disposed in land until U.S. federal regulations were applied in 1987 on land disposal of wastes containing endrin. Primary methods of endrin disappearance from soil are volatilization and photodecomposition. Under ultraviolet light, endrin forms δ-ketoendrin and International Programme on Chemical Safety (IPCS) claims that in intense summer sun, about 50% of endrin is isomerized to δ-ketoendrin in 7 days. In anaerobic conditions microbial degradation by fungi and bacteria takes place to form the same major end product.\n\nHazardous Substances Data Bank (HSDB) lists reductive dechlorination and incineration for field disposal of small quantities of endrin. In reductive dechlorination, endrin's chlorine atoms were completely replaced with hydrogen atoms, which is suspected to be more environmentally acceptable. Even though endrin binds very strongly to soil, phytoremediation has been proposed by group of Japanese scientists using crops in the Cucurbitaceae family. , exact mechanisms behind the plant uptake of endrin have not been understood. Research in uptake mechanisms and factors that influence the uptake is needed for practical application.\n\nIn the United States, endrin has been regulated by the EPA. It set a freshwater acute criterion of 0.086 µg/L and a chronic criterion of 0.036 µg/L. In saltwater, the numbers are acute 0.037 and chronic 0.0023 µg/L.\nThe human health contaminate criterion for water plus organism is 0.059 µg/L.\nThe drinking water limit (maximum contaminant level) is set to 2 ppb.\nUse of endrin in fisheries has been advised against due to the zero tolerance of endrin levels in food products.\nFor occupational exposures to endrin, OSHA and NIOSH have set exposure limits at 0.1 mg/m.\n\nThe WHO lists Endrin as an obsolete pesticide in its 'Classification of Pesticides by Hazard' and did not assign any hazard class per the Globally Harmonized System of Classification and Labelling of Chemicals.\n\nTaiwan is not a party to the Stockholm Convention as of 2015, but has drafted its own \"National Implementation Plan of the Stockholm Convention on Persistent Organic Pollutants\" which was approved by the Executive Yuan on April 2008. The Central Competent Authorities of Taiwan sets the limit of 20 mg/kg for soil pollution control. For marine environment quality, standards of 0.002 mg/L has been set. For occupational exposures to endrin, warning has been given that the contact with skin, eyes, and mucous membranes can contribute to the overall exposure.\n\n\n"}
{"id": "2916580", "url": "https://en.wikipedia.org/wiki?curid=2916580", "title": "Ethylene glycol dinitrate", "text": "Ethylene glycol dinitrate\n\nEthylene glycol dinitrate (EGDN,NGc), also known as nitroglycol, is a chemical compound a colorless, oily explosive liquid obtained by nitrating ethylene glycol. It is similar to nitroglycerin in both manufacture and properties, though it is more volatile and less viscous.\n\nPure EGDN was first produced by the Belgian chemist Louis Henry (1834–1913) in 1870 by dropping a small amount of ethylene glycol into a mixture of nitric and sulfuric acids cooled to 0 °C. The previous year, August Kekulé had produced EGDN by the nitration of ethylene, but this was actually contaminated with beta-nitroethyl nitrate.\n\nOther investigators preparing NGc before publication in 1926 of Rinkenbach's work included: Champion (1871), Neff (1899) & Wieland & Sakellarios (1920), Dautriche, Hough & Oehme.\n\nThe American chemist William Henry Rinkenbach (1894–1965) prepared EGDN by nitrating purified glycol obtained by fractioning the commercial product under pressure of 40mm Hg, and at a temperature of 120°. For this 20g of middle fraction of purified glycol was gradually added to mixture of 70g nitric acid and 130g sulfuric acid, maintaining the temperature at 23°. The resulting 49g of crude product was washed with 300ml of water to obtain 39.6g of purified product. The low yield so obtained could be improved by maintaining a lower temperature and using a different nitrating acid mixture.\n\n1) \"Direct Nitration of Glycol\" is carried out in exactly the same manner, with the same apparatus, and with the same mixed acids as nitration of glycerine. \nIn the test nitration of anhydrous glycol (100g) with 625g of mixed acid 40% & 60% at 10-12°, the yield was 222g and it dropped to 218g when the temp was raised to 29-30°. \nWhen 500g of mixed acid 50% & 50% was used at 10-12°, the yield increased to 229g. \nIn commercial nitration, the yields obtained from 100 kg anhydrous glycol and 625 kg of mixed acid containing 41%, 58% & water 1% were 222.2 kg of NGc at nitrating temp of 10-12° and only 218.3 kg at 29-30°. This means 90.6% of theory, as compared to 93.6% with NG.\n\nor through the reaction of ethylene oxide and dinitrogen pentoxide:\n\n2) \"Direct Production of NGc from Gaseous Ethylene\". \n3) \"Preparation of NGc from Ethylene Oxide\".\n4) \"Preparation of NGc by method of Messing\" from ethylene through chlorohydrin & ethylene oxide.\n5) \"Preparation of NGc by duPont method\".\n\nEthylene glycol dinitrate is a colorless volatile liquid when in pure state, but is yellowish when impure.\n\nMolar weight 152.07, N 18.42%, OB to CO 0%, OB to CO +21%; colorless volatile liquid when in pure state; yellowish liquid in crude state; sp gr 1.488 at 20/4° or 1.480 at 25°; n_D 1.4452 at 25° or 1.4472 at 20°; freezing point -22.75° (versus +13.1° for NG); frozen point given in is -22.3°; boiling point 199° at 760mm Hg (with decomposition).\n\n\"Brisance\" by lead block compression (Hess crusher test) is 30.0 mm, versus 18.5 mm for NG and 16 mm for TNT (misleading, needs to give exact density and mass of explosive (25 or 50 g). Brisance by sand test, determined in mixtures with 40% kieselguhr, gave for NGc mixtures slightly higher results then with those containing NG.\n\nWhen ethylene glycol dinitrate is rapidly heated to 215 °C, it explodes; this is preceded by partial decomposition similar to that of nitroglycerin. EGDN has a slightly higher brisance than nitroglycerin.\n\nEthylene glycol dinitrate reacts violently with potassium hydroxide, yielding ethylene glycol and potassium nitrate:\n\nEGDN was used in manufacturing explosives to lower the freezing point of nitroglycerin, in order to produce dynamite for use in colder weather. Due to its volatility it was used as a detection taggant in some plastic explosives, e.g. Semtex, to allow more reliable explosive detection, until 1995 when it was replaced by dimethyldinitrobutane. It is considerably more stable than glyceryl trinitrate owing to the lack of secondary hydroxyl groups in the precursor polyol.\n\nLike other organic nitrates, ethylene glycol dinitrate is a vasodilator.\n\n\n"}
{"id": "4250665", "url": "https://en.wikipedia.org/wiki?curid=4250665", "title": "Forges de Syam", "text": "Forges de Syam\n\nThe Forges de Syam (Syam Forges) are forge works and sheet metal mills at the confluence of the rivers Ain and Saine to the south of Champagnole in the French département of Jura. \n\nThe forges were still active in 2005, and still used some machinery from the 19th century, when the buildings were erected.\nA tilt hammer mentioned 1757 and 1788 has been working as early as 1763, according to the record of a law case of 1763. It is reasonable to imagine it as part of the region's expansion in the metal-working industry in the 16th century, after establishment of the first smelting furnaces in the Saône valley.\n\nThe basic business was making scythes. These had a good reputation, and the making of them was a finely honed skill. In the 18th century, only a few forges had mastered the technique of using a mechanical hammer. So much was this a problem that French producers couldn't keep up with demand for scythes. They were imported, particularly from Styria, in the south-east of modern Austria, beyond the Tyrol. It was possible to develop this very skilled craft in the Jura owing to the expertise of immigrant workers from the Tyrol (just beyond the far end of Switzerland).\n\nIn conjunction with this skill, extensive woodland on the Jura mountains around the town provided the raw material for charcoal, a form of carbon free of sulphur and other impurities that make most coal unsuitable for iron working. Extensive woodland was an asset, as it takes many trees to make a relatively small amount of iron goods. In 1763, the owners, the Péry family, produced more than 15,000 scythes and more than 60,000 iron tires for the wheels of vehicles. \n\nAfter the French Revolution, the production of good-quality scythes provided an important opportunity for investment. Other industrialists of the Jura, of the Vosges and Alsace began production, contributing to Charles-Joseph Péry declaring himself bankrupt on 24 July 1810.\n\nThe unit was bought the same year by Claude Jobez (1745–1830), of Morez. He had already made a fortune from selling clocks from the Franche-Comté region in Paris, and from financing several iron workshops. Also in 1810, Etienne Monnier who had married Adélaïde, the daughter of Claude Jobez in 1800, invested in the company alongside his father-in-law and the latter's son, Emmanuel. Between 1811 and 1820, they built a new factory downstream from the primitive tilt hammer. This included a novelty for France at the time, a reverberatory furnace. From 1820, these works produced 400 tons of goods each year, which doubled by 1840.\n\nCompetition from coke-smelted cast iron from the United Kingdom—which was cheaper than charcoal-produced iron goods at Syam—hurt the business. Alphonse Jobez, the son of Emmanuel, set up a nail works in 1864, which gave new life to an enterprise that had neared collapse. The workforce increased from 40 to 70.\n\nFrom 1825, Emmanuel Jobez developed the project of building a Palladian villa, the Château de Syam to replace the old house beside the original forge. He didn't see its completion, having died by accident in 1828. Alphonse, his son, took up the cause of Fourierist theory. He applied it at Syam by creating a \"cité ouvrière\" (compare Saltaire), adjoining the factory, encouraging the setting up of a school and a dispensary. He added a post office with telegraph in 1885. \n\nIn parallel to this, he introduced exotic livestock to the estate farm and other lands he owned. A cultivated man, Alphonse also fitted out a library of 30,000 volumes on the first floor (American second floor) of the villa. Alphonse's granddaughter, who had married the son of Sadi-Carnot in 1910, would often stay at Syam.\n\nThe nail works closed in 1914. Syam limited its range of products, specializing in those where there was limited competition. This enabled it to survive the two World Wars.\n\nIn 1945, the firm of UMAS, from Arc-et-Senans, a specialist in making files, became the main stakeholder in Syam. The group went bankrupt in 1976. From 1969, a workforce would come from Morocco, from the village of El Hajjyenne.\n\nIn 1976, the forges were sold to Experton-Revollier, a group from Isère. Modernization was necessary, as there was no travelling crane or electric motor-driven machinery (instead of belt transmission). Improvements went hand in hand with the retention of the steel rolling mill, the last of its type in France and one of the last in Europe.\n\nToday, this remnant of the iron industry of the 19th century feeds the market in short-run products, particularly in the fields of locksmiths' work, motor cars and lifts (elevators).\n\n\n"}
{"id": "54055676", "url": "https://en.wikipedia.org/wiki?curid=54055676", "title": "Frank-Kamenetskii theory", "text": "Frank-Kamenetskii theory\n\nIn combustion, Frank-Kamenetskii theory explains the thermal explosion of a homogeneous mixture of reactants, kept inside a closed vessel with constant temperature walls. It is named after a Russian scientist David A. Frank-Kamenetskii, who along with Nikolay Semenov developed the theory in the 1930s.\n\nConsider a vessel maintained at a constant temperature formula_1, containing a homogeneous reacting mixture. Let the characteristic size of the vessel be formula_2. Since the mixture is homogeneous, the density formula_3 is constant. During the initial period of ignition, the consumption of reactant concentration is negligible (see formula_4 and formula_5 below), thus the explosion is governed only by the energy equation. Assuming a one-step global reaction formula_6, where formula_7 is the amount of heat released per unit mass of fuel consumed, and reaction rate governed by Arrhenius law, the energy equation becomes\n\nwhere\n\nThe non-dimensional activation energy formula_16 and the heat-release parameter formula_17 are\n\nThe characteristic heat conduction time across the vessel is formula_19, the characteristic fuel consumption time is formula_20 and the characteristic explosion/ignition time is formula_21. Note should be made that in combustion process, typically formula_22 so that formula_23. Therefore, formula_24, i.e., the fuel is consumed at much longer times when compared with ignition time, the fuel consumption is essentially negligible to study ignition/explosion. That is the reason the fuel concentration is assumed to same as the initial fuel concentration formula_13. The non-dimensional scales are\n\nwhere formula_27 is the Damköhler number and formula_28 is the spatial coordinate with origin at the center, formula_29 for planar slab, formula_30 for cylindrical vessel and formula_31 for spherical vessel. With this scale, the equation becomes\n\nSince formula_33, the exponential term can be linearized formula_34, hence\n\nBefore Frank-Kamenetskii, his doctoral advisor Nikolay Semyonov (or Semenov) proposed a thermal explosion theory with a simple model i.e., he assumed a linear function for heat conduction process instead of Laplacian operator. Semenov's equation reads as\n\nFor formula_37, the system explodes since the exponential term dominates. For formula_38, the system goes to a steady state, the system doesn't explode. In particular, Semenov found the critical Damköhler number, which is called as Frank-Kamenetskii parameter formula_39(where formula_40) as a critical point where the system changes from steady state to explosive state. For formula_41, the solution is\n\nAt time formula_43, the system explodes.\n\nThe only parameter which characterizes the explosion is the Damköhler number formula_27. When formula_27 is very high, conduction time is longer than the chemical reaction time and the system explodes with high temperature since there is not enough time for conduction to remove the heat. On the other hand, when formula_27 is very low, heat conduction time is much faster than the chemical reaction time, such that all the heat produced by the chemical reaction is immediately conducted to the wall, thus there is no explosion, it goes to an almost steady state, Amable Liñán coined this mode as slowly reacting mode. At a critical Damköhler number formula_47 the system goes from slowly reacting mode to explosive mode. Therefore, formula_48, the system is in steady state. Instead of solving the full problem to find this formula_47, Frank-Kamenetskii solved the steady state problem for various Damköhler number until the critical value, beyond which no steady solution exists. So the problem to be solved is\n\nwith boundary conditions\n\nthe second condition is due to the symmetry of the vessel. The above equation is special case of Liouville–Bratu–Gelfand equation in mathematics.\n\nFor planar vessel, there is an exact solution. Here formula_29, then\n\nIf the transformations formula_54 and formula_55, where formula_56 is the maximum temperature which occurs at formula_57 due to symmetry, are introduced\n\nIntegrating once and using the second boundary condition, the equation becomes\n\nand integrating again\n\nThe above equation is the exact solution, but formula_56 maximum temperature is unknown, but we have not used the boundary condition of the wall yet. Thus using the wall boundary condition formula_62 at formula_63, the maximum temperature is obtained from an implicit expression,\n\nCritical formula_47 is obtained by finding the maximum point of the equation(look figure), i.e., formula_66 at formula_47.\n\nSo the critical Frank-Kamentskii parameter is formula_70. The system has no steady state( or explodes) for formula_71 and for formula_72, the system goes to a steady state with very slow reaction.\n\nFor cylindrical vessel, there is an exact solution. Though Frank-Kamentskii used numerical integration assuming there is no explicit solution, Paul L. Chambré provided an exact soution in 1952. Here formula_30, then\n\nIf the transformations formula_75 and formula_76 are introduced\n\nThe general solution is formula_78. But formula_79 from the symmetry condition at the centre. Writing back in original variable, the equation reads,\n\nBut the original equation multiplied by formula_81 is\n\nNow subtracting the last two equation from one another leads to\n\nThis equation is easy to solve because it involves only the derivatives, so letting formula_84 transforms the equation\n\nThis is a Bernoulli differential equation of order formula_86, a type of Riccati equation. The solution is\n\nIntegrating once again, we have formula_88 where formula_89. We have used already one boundary condition, there is one more boundary condition left, but with two constants formula_90. It turns out formula_91 and formula_12 are related to each other, which is obtained by substituting the above solution into the starting equation we arrive at formula_93. Therefore, the solution is\n\nNow if we use the other boundary condition formula_95, we get an equation for formula_12 as formula_97. The maximum value of formula_27 for which solution is possible is when formula_99, so the critical Frank-Kamentskii parameter is formula_100. The system has no steady state( or explodes) for formula_101 and for formula_102, the system goes to a steady state with very slow reaction. The maximum temperature formula_56 occurs at formula_57\n\nFor each value of formula_27, we have two values of formula_56 since formula_12 is multi-valued. The maximum critical temperature is formula_109.\n\nFor spherical vessel, there is no known explicit solution, so Frank-Kamenetskii used numerical methods to find the critical value.Here formula_31, then\n\nIf the transformations formula_54 and formula_55, where formula_56 is the maximum temperature which occurs at formula_57 due to symmetry, are introduced\n\nThe above equation is nothing but Emden–Chandrasekhar equation, which appears in astrophysics describing isothermal gas sphere. Unlike planar and cylindrical case, the spherical vessel has infinitely many solutions for formula_48 oscillating about the point formula_118, instead of just two solutions, which was shown by Israel Gelfand. The lowest branch will be chosen to explain explosive behavior.\n\nFrom numerical solution, it is found that the critical Frank-Kamenetskii parameter is formula_119. The system has no steady state( or explodes) for formula_120 and for formula_121, the system goes to a steady state with very slow reaction. The maximum temperature formula_56 occurs at formula_57 and maximum critical temperature is formula_124.\n\nFor vessels which are not symmetric about the center(for example rectangular vessel), the problem involves solving a nonlinear partial differential equation instead of a nonlinear ordinary differential equation, which can be solved only through numerical methods in most cases. The equation is\n\nwith boundary condition formula_62 on the bounding surfaces.\n\nSince the model assumes homogeneous mixture, the theory is well applicable to study the explosive behavior of solid fuels (spontaneous ignition of bio fuels, organic materials, garbage, etc.,). This is also used to design explosives and fire crackers. The theory predicted critical values accurately for low conductivity fluids/solids with high conductivity thin walled containers.\n\n\n"}
{"id": "33731132", "url": "https://en.wikipedia.org/wiki?curid=33731132", "title": "Gas metal arc welding", "text": "Gas metal arc welding\n\nGas metal arc welding (GMAW), sometimes referred to by its subtypes metal inert gas (MIG) welding or metal active gas (MAG) welding, is a welding process in which an electric arc forms between a consumable wire electrode and the workpiece metal(s), which heats the workpiece metal(s), causing them to melt and join. Along with the wire electrode, a shielding gas feeds through the welding gun, which shields the process from contaminants in the air. \n\nThe process can be semi-automatic or automatic. A constant voltage, direct current power source is most commonly used with GMAW, but constant current systems, as well as alternating current, can be used. There are four primary methods of metal transfer in GMAW, called globular, short-circuiting, spray, and pulsed-spray, each of which has distinct properties and corresponding advantages and limitations.\n\nOriginally developed in the 1940s for welding aluminium and other non-ferrous materials, GMAW was soon applied to steels because it provided faster welding time compared to other welding processes. The cost of inert gas limited its use in steels until several years later, when the use of semi-inert gases such as carbon dioxide became common. Further developments during the 1950s and 1960s gave the process more versatility and as a result, it became a highly used industrial process. Today, GMAW is the most common industrial welding process, preferred for its versatility, speed and the relative ease of adapting the process to robotic automation. Unlike welding processes that do not employ a shielding gas, such as shielded metal arc welding, it is rarely used outdoors or in other areas of moving air. A related process, flux cored arc welding, often does not use a shielding gas, but instead employs an electrode wire that is hollow and filled with flux.\n\nThe principles of gas metal arc welding began to be understood in the early 19th century, after Humphry Davy discovered the short pulsed electric arcs in 1800. Vasily Petrov independently produced the continuous electric arc in 1802 (followed by Davy after 1808). It was not until the 1880s that the technology became developed with the aim of industrial usage. At first, carbon electrodes were used in carbon arc welding. By 1890, metal electrodes had been invented by Nikolay Slavyanov and C. L. Coffin. In 1920, an early predecessor of GMAW was invented by P. O. Nobel of General Electric. It used direct current with a bare electrode wire and used arc voltage to regulate the feed rate. It did not use a shielding gas to protect the weld, as developments in welding atmospheres did not take place until later that decade. In 1926 another forerunner of GMAW was released, but it was not suitable for practical use.\n\nIn 1948, GMAW was developed by the Battelle Memorial Institute. It used a smaller diameter electrode and a constant voltage power source developed by H. E. Kennedy. It offered a high deposition rate, but the high cost of inert gases limited its use to non-ferrous materials and prevented cost savings. In 1953, the use of carbon dioxide as a welding atmosphere was developed, and it quickly gained popularity in GMAW, since it made welding steel more economical. In 1958 and 1959, the short-arc variation of GMAW was released, which increased welding versatility and made the welding of thin materials possible while relying on smaller electrode wires and more advanced power supplies. It quickly became the most popular GMAW variation.\n\nThe spray-arc transfer variation was developed in the early 1960s, when experimenters added small amounts of oxygen to inert gases. More recently, pulsed current has been applied, giving rise to a new method called the pulsed spray-arc variation.\n\nGMAW is one of the most popular welding methods, especially in industrial environments. It is used extensively by the sheet metal industry and the automobile industry. There, the method is often used for arc spot welding, replacing riveting or resistance spot welding. It is also popular for automated welding, where robots handle the workpieces and the welding gun to accelerate manufacturing. GMAW can be difficult to perform well outdoors, since drafts can dissipate the shielding gas and allow contaminants into the weld; flux cored arc welding is better suited for outdoor use such as in construction. Likewise, GMAW's use of a shielding gas does not lend itself to underwater welding, which is more commonly performed via shielded metal arc welding, flux cored arc welding, or gas tungsten arc welding.\n\nTo perform gas metal arc welding, the basic necessary equipment is a welding gun, a wire feed unit, a welding power supply, a welding electrode wire, and a shielding gas supply.\n\nThe typical GMAW welding gun has a number of key parts—a control switch, a contact tip, a power cable, a gas nozzle, an electrode conduit and liner, and a gas hose. The control switch, or trigger, when pressed by the operator, initiates the wire feed, electric power, and the shielding gas flow, causing an electric arc to be struck. The contact tip, normally made of copper and sometimes chemically treated to reduce spatter, is connected to the welding power source through the power cable and transmits the electrical energy to the electrode while directing it to the weld area. It must be firmly secured and properly sized, since it must allow the electrode to pass while maintaining electrical contact. On the way to the contact tip, the wire is protected and guided by the electrode conduit and liner, which help prevent buckling and maintain an uninterrupted wire feed. The gas nozzle directs the shielding gas evenly into the welding zone. Inconsistent flow may not adequately protect the weld area. Larger nozzles provide greater shielding gas flow, which is useful for high current welding operations that develop a larger molten weld pool. A gas hose from the tanks of shielding gas supplies the gas to the nozzle. Sometimes, a water hose is also built into the welding gun, cooling the gun in high heat operations.\n\nThe wire feed unit supplies the electrode to the work, driving it through the conduit and on to the contact tip. Most models provide the wire at a constant feed rate, but more advanced machines can vary the feed rate in response to the arc length and voltage. Some wire feeders can reach feed rates as high as 30.5 m/min (1200 in/min), but feed rates for semiautomatic GMAW typically range from 2 to 10 m/min (75 – 400 in/min).\n\nThe most common electrode holder is a semiautomatic air-cooled holder. Compressed air circulates through it to maintain moderate temperatures. It is used with lower current levels for welding lap or butt joints. The second most common type of electrode holder is semiautomatic water-cooled, where the only difference is that water takes the place of air. It uses higher current levels for welding T or corner joints. The third typical holder type is a water cooled automatic electrode holder—which is typically used with automated equipment.\n\nMost applications of gas metal arc welding use a constant voltage power supply. As a result, any change in arc length (which is directly related to voltage) results in a large change in heat input and current. A shorter arc length causes a much greater heat input, which makes the wire electrode melt more quickly and thereby restore the original arc length. This helps operators keep the arc length consistent even when manually welding with hand-held welding guns. To achieve a similar effect, sometimes a constant current power source is used in combination with an arc voltage-controlled wire feed unit. In this case, a change in arc length makes the wire feed rate adjust to maintain a relatively constant arc length. In rare circumstances, a constant current power source and a constant wire feed rate unit might be coupled, especially for the welding of metals with high thermal conductivities, such as aluminum. This grants the operator additional control over the heat input into the weld, but requires significant skill to perform successfully.\n\nAlternating current is rarely used with GMAW; instead, direct current is employed and the electrode is generally positively charged. Since the anode tends to have a greater heat concentration, this results in faster melting of the feed wire, which increases weld penetration and welding speed. The polarity can be reversed only when special emissive-coated electrode wires are used, but since these are not popular, a negatively charged electrode is rarely employed.\n\nElectrode selection is based primarily on the composition of the metal being welded, the process variation being used, joint design and the material surface conditions. Electrode selection greatly influences the mechanical properties of the weld and is a key factor of weld quality. In general the finished weld metal should have mechanical properties similar to those of the base material with no defects such as discontinuities, entrained contaminants or porosity within the weld. To achieve these goals a wide variety of electrodes exist. All commercially available electrodes contain deoxidizing metals such as silicon, manganese, titanium and aluminum in small percentages to help prevent oxygen porosity. Some contain denitriding metals such as titanium and zirconium to avoid nitrogen porosity. Depending on the process variation and base material being welded the diameters of the electrodes used in GMAW typically range from 0.7 to 2.4 mm (0.028 – 0.095 in) but can be as large as 4 mm (0.16 in). The smallest electrodes, generally up to 1.14 mm (0.045 in) are associated with the short-circuiting metal transfer process, while the most common spray-transfer process mode electrodes are usually at least 0.9 mm (0.035 in).\n\nShielding gases are necessary for gas metal arc welding to protect the welding area from atmospheric gases such as nitrogen and oxygen, which can cause fusion defects, porosity, and weld metal embrittlement if they come in contact with the electrode, the arc, or the welding metal. This problem is common to all arc welding processes; for example, in the older Shielded-Metal Arc Welding process (SMAW), the electrode is coated with a solid flux which evolves a protective cloud of carbon dioxide when melted by the arc. In GMAW, however, the electrode wire does not have a flux coating, and a separate shielding gas is employed to protect the weld. This eliminates slag, the hard residue from the flux that builds up after welding and must be chipped off to reveal the completed weld.\n\nThe choice of a shielding gas depends on several factors, most importantly the type of material being welded and the process variation being used. Pure inert gases such as argon and helium are only used for nonferrous welding; with steel they do not provide adequate weld penetration (argon) or cause an erratic arc and encourage spatter (with helium). Pure carbon dioxide, on the other hand, allows for deep penetration welds but encourages oxide formation, which adversely affects the mechanical properties of the weld. lts low cost makes it an attractive choice, but because of the reactivity of the arc plasma, spatter is unavoidable and welding thin materials is difficult. As a result, argon and carbon dioxide are frequently mixed in a 75%/25% to 90%/10% mixture. Generally, in short circuit GMAW, higher carbon dioxide content increases the weld heat and energy when all other weld parameters (volts, current, electrode type and diameter) are held the same. As the carbon dioxide content increases over 20%, spray transfer GMAW becomes increasingly problematic, especially with smaller electrode diameters.\n\nArgon is also commonly mixed with other gases, oxygen, helium, hydrogen and nitrogen. The addition of up to 5% oxygen (like the higher concentrations of carbon dioxide mentioned above) can be helpful in welding stainless steel, however, in most applications carbon dioxide is preferred. Increased oxygen makes the shielding gas oxidize the electrode, which can lead to porosity in the deposit if the electrode does not contain sufficient deoxidizers. Excessive oxygen, especially when used in application for which it is not prescribed, can lead to brittleness in the heat affected zone. Argon-helium mixtures are extremely inert, and can be used on nonferrous materials. A helium concentration of 50–75% raises the required voltage and increases the heat in the arc, due to helium's higher ionization temperature. Hydrogen is sometimes added to argon in small concentrations (up to about 5%) for welding nickel and thick stainless steel workpieces. In higher concentrations (up to 25% hydrogen), it may be used for welding conductive materials such as copper. However, it should not be used on steel, aluminum or magnesium because it can cause porosity and hydrogen embrittlement.\n\nShielding gas mixtures of three or more gases are also available. Mixtures of argon, carbon dioxide and oxygen are marketed for welding steels. Other mixtures add a small amount of helium to argon-oxygen combinations, these mixtures are claimed to allow higher arc voltages and welding speed. Helium also sometimes serves as the base gas, with small amounts of argon and carbon dioxide added. However, because it is less dense than air, helium is less effective at shielding the weld than argon—which is denser than air. It also can lead to arc stability and penetration issues, and increased spatter, due to its much more energetic arc plasma. Helium is also substantially more expensive than other shielding gases. Other specialized and often proprietary gas mixtures claim even greater benefits for specific applications.\n\nThe desirable rate of shielding-gas flow depends primarily on weld geometry, speed, current, the type of gas, and the metal transfer mode. Welding flat surfaces requires higher flow than welding grooved materials, since gas disperses more quickly. Faster welding speeds, in general, mean that more gas must be supplied to provide adequate coverage. Additionally, higher current requires greater flow, and generally, more helium is required to provide adequate coverage than if argon is used. Perhaps most importantly, the four primary variations of GMAW have differing shielding gas flow requirements—for the small weld pools of the short circuiting and pulsed spray modes, about 10 L/min (20 ft³/h) is generally suitable, whereas for globular transfer, around 15 L/min (30 ft³/h) is preferred. The spray transfer variation normally requires more shielding-gas flow because of its higher heat input and thus larger weld pool. Typical gas-flow amounts are approximately 20–25 L/min (40–50 ft³/h).\n\nGMAW has also been used as a low-cost method to 3-D print metal objects. Various open source 3-D printers have been developed to use GMAW. Such components fabricated from aluminum compete with more traditionally manufactured components on mechanical strength. By forming a bad weld on the first layer, GMAW 3-D printed parts can be removed from the substrate with a hammer.\n\nFor most of its applications gas metal arc welding is a fairly simple welding process to learn requiring no more than a week or two to master basic welding technique. Even when welding is performed by well-trained operators weld quality can fluctuate since it depends on a number of external factors. All GMAW is dangerous, though perhaps less so than some other welding methods, such as shielded metal arc welding.\n\nGMAW's basic technique is uncomplicated, with most individuals able to achieve reasonable proficiency in a few weeks, assuming proper training and sufficient practice. As much of the process is automated, GMAW relieves the weldor (operator) of the burden of maintaining a precise arc length, as well as feeding filler metal into the weld puddle, coordinated operations that are required in other manual welding processes, such as shielded metal arc. GMAW requires only that the weldor guide the gun with proper position and orientation along the area being welded, as well as periodically clean the gun's gas nozzle to remove spatter buildup. Additional skill includes knowing how to adjust the welder so the voltage, wire feed rate and gas flow rate are correct for the materials being welded and the wire size being employed.\n\nMaintaining a relatively constant contact tip-to-work distance (the \"stick-out\" distance) is important. Excessive stick-out distance may cause the wire electrode to prematurely melt, causing a sputtering arc, and may also cause the shielding gas to rapidly disperse, degrading the quality of the weld. In contrast, insufficient stick-out may increase the rate at which spatter builds up inside the gun's nozzle and in extreme cases, may cause damage to the gun's contact tip. Stick-out distance varies for different GMAW weld processes and applications.\n\nThe orientation of the gun relative to the weldment is also important. It should be held so as to bisect the angle between the workpieces; that is, at 45 degrees for a fillet weld and 90 degrees for welding a flat surface. The travel angle, or lead angle, is the angle of the gun with respect to the direction of travel, and it should generally remain approximately vertical. However, the desirable angle changes somewhat depending on the type of shielding gas used—with pure inert gases, the bottom of the torch is often slightly in front of the upper section, while the opposite is true when the welding atmosphere is carbon dioxide.\n\nPosition welding, that is, welding vertical or overhead joints, may require the use of a weaving technique to assure proper weld deposition and penetration. In position welding, gravity tends to cause molten metal to run out of the puddle, resulting in cratering and undercutting, two conditions that produce a weak weld. Weaving constantly moves the fusion zone around so as to limit the amount of metal deposited at any one point. Surface tension then assists in keeping the molten metal in the puddle until it is able to solidify. Development of position welding skill takes some experience, but is usually soon mastered.\n\nTwo of the most prevalent quality problems in GMAW are dross and porosity. If not controlled, they can lead to weaker, less ductile welds. Dross is an especially common problem in aluminium GMAW welds, normally coming from particles of aluminium oxide or aluminum nitride present in the electrode or base materials. Electrodes and workpieces must be brushed with a wire brush or chemically treated to remove oxides on the surface. Any oxygen in contact with the weld pool, whether from the atmosphere or the shielding gas, causes dross as well. As a result, sufficient flow of inert shielding gases is necessary, and welding in moving air should be avoided.\n\nIn GMAW the primary cause of porosity is gas entrapment in the weld pool, which occurs when the metal solidifies before the gas escapes. The gas can come from impurities in the shielding gas or on the workpiece, as well as from an excessively long or violent arc. Generally, the amount of gas entrapped is directly related to the cooling rate of the weld pool. Because of its higher thermal conductivity, aluminum welds are especially susceptible to greater cooling rates and thus additional porosity. To reduce it, the workpiece and electrode should be clean, the welding speed diminished and the current set high enough to provide sufficient heat input and stable metal transfer but low enough that the arc remains steady. Preheating can also help reduce the cooling rate in some cases by reducing the temperature gradient between the weld area and the base metal.\n\nArc welding in any form can be dangerous if proper precautions are not taken. Since GMAW employs an electric arc, weldors must wear suitable protective clothing, including heavy gloves and protective long sleeve jackets, to minimize exposure to the arc itself, as well as intense heat, sparks and hot metal. The intense ultraviolet radiation of the arc may cause sunburn-like damage to exposed skin, as well a condition known as arc eye, an inflammation of the cornea, or in cases of prolonged exposure, irreversible damage to the eye's retina. Conventional welding helmets contain dark face plates to prevent this exposure. Newer helmet designs feature a liquid crystal-type face plate that self-darkens upon exposure to the arc. Transparent welding curtains, made of a polyvinyl chloride plastic film, are often used to shield nearby workers and bystanders from exposure to the arc.\n\nWelders are often exposed to hazardous gases and airborne particulate matter. GMAW produces smoke containing particles of various types of oxides, and the size of the particles tends to influence the toxicity of the fumes. Smaller particles present greater danger. Concentrations of carbon dioxide and ozone can prove dangerous if ventilation is inadequate. Other precautions include keeping combustible materials away from the workplace, and having a working fire extinguisher nearby.\n\nThe three transfer modes in GMAW are globular, short-circuiting, and spray. There are a few recognized variations of these three transfer modes including modified short-circuiting and pulsed-spray.\n\nGMAW with globular metal transfer is considered the least desirable of the three major GMAW variations, because of its tendency to produce high heat, a poor weld surface, and spatter. The method was originally developed as a cost efficient way to weld steel using GMAW, because this variation uses carbon dioxide, a less expensive shielding gas than argon. Adding to its economic advantage was its high deposition rate, allowing welding speeds of up to 110 mm/s (250 in/min). As the weld is made, a ball of molten metal from the electrode tends to build up on the end of the electrode, often in irregular shapes with a larger diameter than the electrode itself. When the droplet finally detaches either by gravity or short circuiting, it falls to the workpiece, leaving an uneven surface and often causing spatter. As a result of the large molten droplet, the process is generally limited to flat and horizontal welding positions, requires thicker workpieces, and results in a larger weld pool.\n\nFurther developments in welding steel with GMAW led to a variation known as short-circuit transfer (SCT) or short-arc GMAW, in which the current is lower than for the globular method. As a result of the lower current, the heat input for the short-arc variation is considerably reduced, making it possible to weld thinner materials while decreasing the amount of distortion and residual stress in the weld area. As in globular welding, molten droplets form on the tip of the electrode, but instead of dropping to the weld pool, they bridge the gap between the electrode and the weld pool as a result of the lower wire feed rate. This causes a short circuit and extinguishes the arc, but it is quickly reignited after the surface tension of the weld pool pulls the molten metal bead off the electrode tip. This process is repeated about 100 times per second, making the arc appear constant to the human eye. This type of metal transfer provides better weld quality and less spatter than the globular variation, and allows for welding in all positions, albeit with slower deposition of weld material. Setting the weld process parameters (volts, amps and wire feed rate) within a relatively narrow band is critical to maintaining a stable arc: generally between 100 and 200 amperes at 17 to 22 volts for most applications. Also, using short-arc transfer can result in lack of fusion and insufficient penetration when welding thicker materials, due to the lower arc energy and rapidly freezing weld pool. Like the globular variation, it can only be used on ferrous metals.\n\nFor thin materials, Cold Metal Transfer (CMT) is used by reducing the current when a short circuit is registered, producing many drops per second. CMT can be used for aluminum.\n\nSpray transfer GMAW was the first metal transfer method used in GMAW, and well-suited to welding aluminium and stainless steel while employing an inert shielding gas. In this GMAW process, the weld electrode metal is rapidly passed along the stable electric arc from the electrode to the workpiece, essentially eliminating spatter and resulting in a high-quality weld finish. As the current and voltage increases beyond the range of short circuit transfer the weld electrode metal transfer transitions from larger globules through small droplets to a vaporized stream at the highest energies. Since this vaporized spray transfer variation of the GMAW weld process requires higher voltage and current than short circuit transfer, and as a result of the higher heat input and larger weld pool area (for a given weld electrode diameter), it is generally used only on workpieces of thicknesses above about 6.4 mm (0.25 in).\n\nAlso, because of the large weld pool, it is often limited to flat and horizontal welding positions and sometimes also used for vertical-down welds. It is generally not practical for root pass welds. When a smaller electrode is used in conjunction with lower heat input, its versatility increases. The maximum deposition rate for spray arc GMAW is relatively high—about 600 mm/s (150 in/min).\n\nA variation of the spray transfer mode, pulse-spray is based on the principles of spray transfer but uses a pulsing current to melt the filler wire and allow one small molten droplet to fall with each pulse. The pulses allow the average current to be lower, decreasing the overall heat input and thereby decreasing the size of the weld pool and heat-affected zone while making it possible to weld thin workpieces. The pulse provides a stable arc and no spatter, since no short-circuiting takes place. This also makes the process suitable for nearly all metals, and thicker electrode wire can be used as well. The smaller weld pool gives the variation greater versatility, making it possible to weld in all positions. In comparison with short arc GMAW, this method has a somewhat slower maximum speed (85 mm/s or 200 in/min) and the process also requires that the shielding gas be primarily argon with a low carbon dioxide concentration. Additionally, it requires a special power source capable of providing current pulses with a frequency between 30 and 400 pulses per second. However, the method has gained popularity, since it requires lower heat input and can be used to weld thin workpieces, as well as nonferrous materials.\n\nFlux-cored, self-shielding or gasless wire-fed welding had been developed for simplicity and portability. This avoids the gas system of conventional GMAW and uses a cored wire containing a solid flux. This flux vaporises during welding and produces a plume of shielding gas. Although described as a 'flux', this compound has little activity and acts mostly as an inert shield. The wire is of slightly larger diameter than for a comparable gas-shielded weld, to allow room for the flux. The smallest available is 0.8 mm diameter, compared to 0.6 mm for solid wire. The shield vapor is slightly active, rather than inert, so the process is always MAGS but not MIG (inert gas shield). This limits the process to steel and not aluminium.\n\nVaporising the additional flux requires greater heat in the wire, so these gasless machines operate as DCEP, rather than the DCEN usually used for GMAW to give deeper penetration. DCEP, or DC Electrode Positive, makes the welding wire into the positively-charged anode, which is the hotter side of the arc. Provided that it is switchable from DCEN to DCEP, a gas-shielded wire-feed machine may also be used for flux-cored wire.\n\nFlux-cored wire is considered to have some advantages for outdoor welding on-site, as the shielding gas plume is less likely to be blown away in a wind than shield gas from a conventional nozzle. A slight drawback is that, like SMAW (stick) welding, there may be some flux deposited over the weld bead, requiring more of a cleaning process between passes.\n\nFlux-cored welding machines are most popular at the hobbyist level, as the machines are slightly simpler but mainly because they avoid the cost of providing shield gas, either through a rented cylinder or with the high cost of disposable cylinders.\n\n\n\n"}
{"id": "17001801", "url": "https://en.wikipedia.org/wiki?curid=17001801", "title": "Hitachi Cable Manchester", "text": "Hitachi Cable Manchester\n\nHitachi Cable Manchester, Inc. (HCM) is a manufacturer of premises telecommunications, optical fiber and electronics cables headquartered in Manchester, New Hampshire, and part of the Hitachi Cable division of electronics conglomerate Hitachi.\n\nCategory 6A UTP, Category 6A F/UTP, Category 6 UTP, Category 5e UTP, Category 5e and 6 outdoor, Category 6 GoldLAN hybrid, Category 5e GoldLAN hybrid, Category 5e power sum multi pair, Category 3 power sum multi pair, composite, ScTP, under carpet, and UTP cable.\n\nIndoor interconnect, indoor multi-unit, indoor single-unit, indoor loose tube no-gel, indoor/outdoor central tube, indoor/outdoor loose tube, indoor/outdoor tight buffered multimode/singlemode, outside plant armored, and outside plant loose tube fiber optic cable.\n\nCoaxial, general purpose 24AWG, HDMI, IEEE 1284, IEEE 1394, Infiniband, probe, SCSI, Serial ATA, USB, and VGA electronic round cable.\n\nMulticolor planar, microzip planar, PVC ribbonized MCX, SCSI micro quick twist, shielded, and wide pitch planar ribbon cable.\n\nHitachi Cable, parent entity of Hitachi Cable Manchester\n\n"}
{"id": "46517872", "url": "https://en.wikipedia.org/wiki?curid=46517872", "title": "Intermittent inductive automatic train stop", "text": "Intermittent inductive automatic train stop\n\nThe intermittent inductive automatic train stop (also referred to as IIATS or just automatic train stop or ATS) is a train protection system used in North American mainline railroad and rapid transit systems. It makes use of magnetic reluctance to trigger a passing train to take some sort of action. The system was developed in the 1920s by the General Railway Signal Company as an improvement on existing mechanical train stop systems and saw limited adoption before being overtaken by more advanced cab signaling and automatic train control systems. The system remains in use after having been introduced in the 1920s.\n\nThe technology works by having the state of a track mounted \"shoe\" read by a receiver mounted to a truck on the leading locomotive or car. In the standard implementation the shoe is mounted to the ties a few inches outside the right hand running rail, although in theory the shoe could be mounted anywhere on the ties. The system is binary with the shoe presenting either an \"on\" or \"off\" state to the receiver. In order to be failsafe when the shoe is energized it presents an \"off\" state to the receiver, while the non-energized state presents an \"on\" state which triggers an action. This allows things like permanent speed restrictions or other hazards to be protected by non-active devices.\n\nThe receiver consists of a two coil electromagnet carefully aligned to pass about 1.5 inches above the surface of the inductor shoe. The inductor shoe consists of two metal plates set into a streamlined housing designed to deflect impacts of debris or misaligned receivers. The metal plates are connected through a choke circuit in the body of the shoe. When the choke circuit is open magnetic flux in the receiver's primary coil is able to induce a voltage in the receiver's secondary coil which in turn triggers an action in the locomotive. When the circuit is closed the choke eliminates the magnetic field and the voltage induced by it allowing the locomotive to pass without activation. Where unconditional activation was desired specially shaped metal plates could be used in place of a fully functional shoe, however the design of the system can result in accidental activations when the train passes over switches or other metal objects in the track area.\n\nThe most common use case for the ATS system was to alert the railroad engineer of an impending hazard and if the alert was not acknowledged, stop the train by means of a full service application of the brakes. When attached to signals the shoe would be energized when the signal was displaying a \"Clear\" indication. Any other signal indication would de-energize the shoe and trigger an alarm in the cab. If the engineer did not cancel the alarm within 5–8 seconds a penalty brake application would be initiated and could not be reset until the train came to a complete stop. Unlike mechanical train stops or other train stop systems, IIATS was not generally used to automatically stop a train if it passed a stop signal and in practice could not be used for this purpose as the shoes were placed only a few feet from the signal they protected and would not present sufficient braking distance for the train to stop.\n\nOn Bi-directionally signaled lines two \"shoes\" would be needed, one for each direction of travel as locomotives would only have a sensor to detect the shoes on one side of the train. The receivers can also be designed for easy removal to prevent damage when operating in non-equipped territory or to cut costs when only a small portion of the railroad requires ATS equipped locomotives. \"Inert\" inductors are sometimes placed in advance of certain speed restrictions as an alert or at engine terminals to test the functionality of the ATS system.\n\nOn a few light rail lines IIATS has been employed in a manner similar to mechanical train stops, stopping the train if it passes an absolute stop signal. It is useful where light rail shares tracks with mainline railroad trains as mechanical trips may be damaged by or interfere with freight operations and because light rail vehicles can be brought to a stop much more quickly than a mainline railroad train without requiring complex signal overlaps\n\nStarting in the 1930s the US Interstate Commerce Commission, in its role as a federal railroad regulator, encouraged railroads to adopt new safety technologies to decrease the rate of railroad accidents. IIATS was offered by the General Railway Signal Company of Rochester, NY as one such technology and it was adopted by the New York Central railroad for use on its high speed Water Level Route between New York and Chicago and on a number of other lines. The Southern Railway also chose to adopt ATS on most of its main lines eventually covering 2700 route miles. In addition the Chicago and North Western Railway installed the system on some of its Chicago area commuter lines.\n\nAfter the Naperville train disaster caused by a missed signal, the ICC required additional technical safety systems for any train traveling at or above 80 mph with the rule taking effect in 1951. Those railroads still interested in high speed operations IIATS met the minimum ICC requirements with a lower cost compared to other cab signaling or automatic train control systems, however with rail travel facing increased competition from cars and airplanes most railroads simply choose to accept the new speed limit. Only the Atchison, Topeka and Santa Fe choose to fully equip its Chicago to Los Angeles and Los Angeles to San Diego main lines in support of the Super Chief and other premier high speed trains.\n\nIIATS installations reached their peak in 1954 with a total of 8650 road miles, 14400 track miles, and 3850 locomotives equipped with the system. However, with the collapse of long distance passenger rail travel and the general North American railroad industry malaise in 1971, the bankrupt Penn Central was permitted to remove IIATS from its Water Level Route along with the Southern and other railroads with test or pilot IIATS systems. Even the ATSF and successor BNSF were gradually allowed by regulators to remove IIATS from parts of previously equipped lines due to the reduced passenger traffic. At the dawn of the 21st century the only IIATS equipped lines were the MetroLink and Coaster line between San Diego and Fullerton, parts of the former ATSF Super Chief route in California, Arizona, New Mexico, Colorado, Kansas and Missouri and the former Chicago and North Western Railway North Line, Northwest Line out of Chicago operated by Union Pacific on behalf of Metra\n\nWhen the New Jersey Transit RiverLINE opened in 2004 it featured a new IIATS system. This is a light rail systems running on shared track with main line freight traffic and IIATS is used to enforce a full stop at equipped signals instead of as a warning system.\n\n"}
{"id": "6979989", "url": "https://en.wikipedia.org/wiki?curid=6979989", "title": "International Council of Societies of Industrial Design", "text": "International Council of Societies of Industrial Design\n\nThe International Council of Societies of Industrial Design (Icsid) was founded in 1957 from a group of international organizations focused on industrial design. Today Icsid is a worldwide society that promotes better design around the world, and has since been renamed the \"World Design Organization\" in January 2017. Today, WDO includes over 140 member organizations in more than 40 nations, representing an estimated 150,000 designers.\n\nThe primary aim of the association is to advance the discipline of industrial design at an international level. To do this, WDO undertakes a number of initiatives of global appeal to support the effectiveness of industrial design in an attempt to address the needs and aspirations of people around the world, to improve the quality of life, as well as help to improve the economy of nations throughout the world. WDO holds United Nations Special Consultative Status which empowers them to make change, and works towards addressing the United Nations Sustainable Development Goals. \n\n first presented the idea to form a society to represent the industrial designers internationally at the Institut d’Esthetique Industrielle’s international congress in 1953. The International Council of Societies of Industrial Designers was formally founded at a meeting in London on June 29, 1957. The name of Icsid demonstrates the spirit which is to protect the interests of practicing designers and to ensure global standards of design. The individuals first elected officials to the Executive Board therefore did not act upon personal conviction, but represented the voice of society members and the international design community.\n\nThe organization then officially registered in Paris and set up their headquarters there. Icsid’s early goals were to help public awareness of industrial designers, to raise the standard of design by setting standards for training and education, and to encourage cooperation between industrial designers worldwide. To do this, in 1959 Icsid held the first Congress and General Assembly in Stockholm, Sweden. At this first Congress the Icsid Constitution was officially adopted, along with the first definition of industrial design which may be found on their website (please see external references). During this Congress, Icsid's official name was changed from the \"International Council of Societies of Industrial Designers\" to the \"International Council of Societies of Industrial Design\" to reflect that the organization would involve itself beyond matters of professional practice.\n\nThroughout Icsid had continued to grow and now has members from all over the world in both capitalist and non-capitalist countries. Icsid has now hosted the Congress in places such as Venice, Paris, Vienna, Montreal, Slovenia, Glasgow, Taipei, Toronto, Sydney, Kyoto and London.\n\nIn 1963, Icsid was granted special status with UNESCO, with whom Icsid continues to work on many projects, using design for the betterment of the human condition. As their humanitarian interests grew, Icsid decided to create a new type of conference that would join industrial designers in a host country to study a problem of both regional and international significance. This new conference held in Minsk in 1971, became the first Icsid Interdesign seminar. These seminars provided opportunities for professional development of mid-career practicing designers, and to allow them to focus their abilities on resolving issues of international significance. This first Interdesign conference and the ones that followed, consolidated Icsid’s position as a driving force of international collaboration.\n\nIn 1971, the 7th Congress of the International Council of Societies of Industrial Design (ICSID), organised by the Agrupació de Disseny Industrial del Foment de les Arts Decoratives (ADI/FAD) in Eivissa (Ibiza), turned out to be an event without precedent in Spain: a process of socialisation and an example of how the energy of collaborative work, vitality, intellectual reflection and leisure can be placed at the service of dialogue-based projects with the capacity to generate imaginative approaches and structure new behavioural patterns.\n\nIn 1974, the Icsid Secretariat moved from Paris, France to Brussels, Belgium, moving onto Helsinki, Finland, and in 2005, it settled to Montreal, Quebec, Canada where it currently resides.\n\nIn the 1980s, collaboration became even more important so a joint Icsid/Icograda/IFI Congress was held in Helsinki. The impetus for this joint conference was a direct recommendation made by Icsid members to explore closer ties with other world design organizations. At their General Assemblies, all participants unanimously approved a directive to investigate options for a closer working relationship in the future. These organizations then joined with UNESCO to bring together doctors, industrial and graphic designers, and assistants to develop basic furniture for rural health centers, packaging, transport, refrigeration, and injection of vaccines and the design of data collecting devices for field use.\n\nIn 2003, Icsid and Icograda ratified an agreement between both organizations during their respective General Assemblies to form the \"International Design Alliance\", a multidisciplinary partnership that supports design. In 2008, the IDA partners welcomed a third member, IFI (International Federation of Interior Architects/Designers). Together in 2011, all three partners held a historical joint Congress in Taipei, Taiwan called the IDA Congress. The alliance was terminated in November 2013.\n\nIn 2017, in January the Icsid officially became the World Design Organization. (WDO)\n\nThe WDO Executive Board is the governing body that directs and supports the WDO's mission and vision. It is composed of international industrial design professionals selected by WDO members during the General Assembly. A total of 11 members make up the Executive Board: President, President-elect, and 9 Executive Board members. Each board member serves a two-year term and may be re-elected for a second term, but cannot run for a third term, unless running for President-Elect.\n\nThe President-elect is designated as the future President of WDO in the following term.\n\nA treasurer is appointed by the President for each new term. The Treasurer's role is to keep record of Icsid's finances throughout the two-year term.\n\nThe Senate is composed of past Presidents of the WDO Executive Board, acting as an advisory board. Its eldest member is Kenji Ekuan (Icsid President 1975-1977), an industrial designer most-renowned for creating the soy sauce bottle design. The most current past President serves as the Convenor of the Senate until the next General Assembly, Prof. Soon-in Lee (Icsid President 2011-2013) is the current convenor.\n\nRegional Advisors are former Icsid Executive Board Members who are appointed to represent Icsid through regional events and strengthen the organization's global presence.\n\nThe Secretariat handles daily operations and carries out decisions made by the Executive Board during the term.\n\nThe Secretariat office have been previously located in Paris, Brussels, and Helsinki. It is currently located in Montreal, Quebec, Canada since 2005.\n\nWDO Members are professional associations, promotional societies, educational institutions, government bodies, corporations and institutions, which aim to contribute to the development of the profession of industrial design. These societies collaborate to establish an international platform through which design institutions worldwide can stay in touch, share common interests and new experiences, and be heard as a powerful voice.\n\nSince its first Congress in 1959 held in Stockholm, Sweden, the World Design Congress was the WDO's most notable biennial international industrial design event. A host city is chosen through a bidding process, followed by a program in which keynote speakers, panellists and workshops are developed during the three-day event. The Congress was open to all those interested in industrial design, including WDO members.\n\nIn 2011, the International Design Alliance have agreed to replace their individual Congresses to a joint biennale Congress under the multidisciplinary banner of the IDA Congress. A joint model was previously held every six years since 1981, starting in Helsinki, Finland. The joint Congresses were then followed by Amsterdam, The Netherlands in 1987; Glasgow, Scotland in 1993; Sydney, Australia in 1999; and Copenhagen, Prague in 2005.\n\nTaipei, Taiwan was selected as the host city for the inaugural 2011 IDA Congress. Each organisation still holds their individual General Assemblies after the IDA Congress. After the Taipei congress, the International Design Alliance unveiled a new branding identity for its biennial congress which each host city will be able to use and personalise their identity to showcase their unique cultures.\n\nThe WDO General Assembly is an WDO member-only event in which member organizations discuss refinements to the Constitution and By-laws, elect a new executive board for the upcoming term, ratify new members, and update the membership on the work done in the past term through the previous executive board's direction.\n\nUnder the WDO's direction, the World Design Capital® Project is a biennial designation given to a city who has proven through the use of design in revitalizing urban areas whilst also promoting innovative design. Through this initiative, the city of Torino, Italy was designated as the first World Design Capital in 2008. With the help of WDO, a yearly program is conceived to engage the city in signature World Design Capital events including galas and exhibitions.\n\nSeveral processes are performed prior to designating the city, including a call for submissions, an evaluation of the submissions, a shortlist of cities, a city visit and a final selection process. A selection committee is appointed to shortlist the cities and the WDC Organising Committee chooses the designated city.\n\nThe current World Design Capital network:\n\nThe World Design Impact Prize was developed to award a socially-responsible design project or initiative, in which design proves or could potentially prove to enrich or improve quality of life on a social, environmental, cultural or economic scale. This biennial award aims to promote and recognise projects that are industrial design driven and use design for social good.\n\nThe inaugural World Design Impact Prize was awarded in Helsinki at the World Design Capital (WDC) International Design Gala in February 2012. It was presented to the Community Cooker Project from Nairobi.\n\nThe second cycle of the World Design Impact Prize was launched on 9 April 2013 with a new identity and a call for public suggestions. The International Review Panel was composed of five international representatives including three WDO Members and two experienced professionals in the field of design for social good\n\nThe nominations period for the WDO Membership was from 29 June – 29 July 2013 and the nominated projects were announced on 10 September 2013. 26 projects from 15 countries completed applications and are showcased on the World Design Impact Prize website.\n\nThe Review Panel collectively shortlisted 7 projects, which were announced at the WDO General Assembly on 18 November 2013 in Montreal (Canada). The quality of the projects nominated in the second cycle pushed the Review Panel to select 7 projects, instead of 6 as originally intended. The shortlisted projects (in alphabetical order) were:\n\nThese projects were selected on the criteria of impact, innovation, context and ease of use and because they promoted an expanded view of the field of industrial design.\n\nThree finalists will be announced on 27 January 2014 and the award will be presented at the World Design Capital® International Design Gala in Cape Town (South Africa) in February 2014.\n\nAn intensive two-week workshop, Interdesign is a collaboration between international designers and local experts to discuss a design issue and seek solutions for implementation. Since 1971, the first Interdesign was held in Minsk, USSR (now Belarus), where thirty designers worked on the product-oriented subject of bread making.\n\nIn 2014, WDO Interdesign under the theme of 'Humanising Metropolis' will take place in Mumbai (India), on 5–19 February. Following the call for applications, the participants list is to be announced in the mid-December 2013. 2014 Icsid Interdesign Mumbai event page\n\nThis annual day of observance is held every 29 June, WDO's anniversary, for industrial designers, academics, design students and non-designers impacted by the power if industrial design on a daily basis. This day of observance promotes awareness and the celebration of industrial design, encouraging WDO Members and non-members to hold events such as exhibitions, panels, workshops, open houses and other social functions.\n\nFrom 2009-2011, an annual student poster competition was held for WDO's Educational student members to participate; this activity concluded once the official World Industrial Design Day logo was introduced by WDO. Each year, a theme is determined by the WDO Board of Directors and announced months in advance of 29 June in order for companies, associations, design schools and design agencies to plan their activities and outreach.\n\nThe WDO World Design Talks were established in 2016 and consist or a series of workshops \"that address local challenges with global relevance—such as rapid urbanization, climate change, and migration—from a design perspective.\" The wisdom that comes out of the World Design Talks helps define who the WDO are and address their relationship to the United Nations Sustainable Development Goals. \n\n\n"}
{"id": "11710822", "url": "https://en.wikipedia.org/wiki?curid=11710822", "title": "Iron puddler", "text": "Iron puddler\n\nAn iron puddler or (often merely puddler) is an occupation in iron manufacturing. The process of puddling was the occupation's chief responsibility. Puddling was an improved process to convert pig iron into wrought iron with the use of a reverberatory furnace.\n\nWorking as a two-man crew, a puddler and helper could produce about 3300lb (1500kg) of iron in a 12-hour shift. The strenuous labor, heat and fumes caused puddlers to have a short life expectancy, with most dying in their 30s. Puddling was never automated because the puddler had to sense when the balls had \"come to nature.\"\n\nJames J. Davis, who was born in Tredegar, Wales, emigrated to the United States where he later became a prominent figure in government, serving as a U.S. Senator from Pennsylvania, and as U.S. Secretary of Labor of Labor under three consecutive Presidents. His book, \"The Iron Puddler\", describing his early experiences as a puddler was ghostwritten by C. L. Edson.\n"}
{"id": "2079927", "url": "https://en.wikipedia.org/wiki?curid=2079927", "title": "Irrigation sprinkler", "text": "Irrigation sprinkler\n\nAn Irrigation sprinkler (also known as a water sprinkler or simply a sprinkler) is a device used to irrigate agricultural crops, lawns, landscapes, golf courses, and other areas. They are also used for cooling and for the control of airborne dust. Sprinkler irrigation is the method of applying water to a controlled manner in that is similar to rainfall. The water is distributed through a network that may consist of pumps, valves, pipes, and sprinklers. Irrigation sprinklers can be used for residential, industrial, and agricultural usage.\n\nHigher pressure sprinklers that themselves move in a circle are driven by a ball drive, gear drive, or impact mechanism (impact sprinklers). These can be designed to rotate in a full or partial circle.\n\nRainguns are similar to impact sprinkler, except that they generally operate at very high pressures of 40 to 130 lbf/in² (275 to 900 kPa) and flows of 50 to 1200 US gal/min (3 to 76 L/s), usually with nozzle diameters in the range of 0.5 to 1.9 inches (10 to 50 mm). In addition to irrigation, guns are used for industrial applications such as dust suppression and logging.\n\nMany irrigation sprinklers are buried in the ground along with their supporting plumbing, although above ground and moving sprinklers are also common. Most irrigation sprinklers operate through electric and hydraulic technology and are grouped together in zones that can be collectively turned on and off by actuating a solenoid-controlled valve.\n\nHome lawn sprinklers vary widely in their size, cost, and complexity. They include impact sprinklers, oscillating sprinklers, drip sprinklers, underground sprinkler systems, and portable sprinklers. Permanently installed systems may often operate on timers or other automated processes. They are occasionally installed with retractable heads for aesthetic and practical reasons, reducing damage during lawn mowing. These types of systems usually can be programmed to automatically start on a set time and day each week.\n\nSmall portable sprinklers can be temporarily placed on lawns if additional watering is needed or if no permanent system is in place. These are often attached to an outdoor water faucet and are placed for a short period of time. Other systems may be professionally installed permanently in the ground and are attached permanently to a home's plumbing system. \n\nAn antique sprinkler developed by Nomad called a 'set-and-forget tractor sprinkler' was used in Australia in the 1950s. Water pressure ensured that the sprinkler slowly moved across a lawn.\n\nThe first use of sprinklers by farmers was some form of home and golf course type sprinklers. These ad hoc systems, while doing the job of the buried pipes and fixed sprinkler heads, interfered with cultivation and were expensive to maintain. In the 1950s a firm based in Portland, Oregon \"Stout-Wyss Irrigation System\", developed the rolling pipe type irrigation system for farms that has become the most popular type for farmers irrigating large fields. With this system large wheels attached to the large pipes with sprinkler heads move slowly across the field.\n\nUnderground sprinklers function through means of basic electronic and hydraulic technology. This valve and all of the sprinklers that will be activated by this valve are known as a zone. Upon activation, the solenoid, which sits on top of the valve is magnetized lifting a small stainless steel plunger in its center. By doing this, the activated (or raised) plunger allows air to escape from the top of a rubber diaphragm located in the center of the valve. Water that has been charged and waiting on the bottom of this same diaphragm now has the higher pressure and lifts the diaphragm. This pressurized water is then allowed to escape down stream of the valve through a series of pipes, usually made of PVC (higher pressure commercial systems) or polyethylene pipe (for typically lower pressure residential systems). At the end of these pipes and flush to ground level (typically) are pre measured and spaced out sprinklers. These sprinklers can be fixed spray heads that have a set pattern and generally spray between 1.5–2m (7–15 ft.), full rotating sprinklers that can spray a broken stream of water from 6–12m (20–40 ft.), or small drip emitters that release a slow, steady drip of water on more delicate plants such as flowers and shrubs. use of indigenous materials also recommended.\n\nIn 2017, it was reported that use of common garden hoses in combination with spray nozzles may generate aerosols containing droplets smaller than 10 μm, which can be inhaled by nearby people. Water stagnating in a hose between uses, especially when warmed by the sun, can host the growth and interaction of Legionella and free-living amoebae (FLA) as biofilms on the inner surface of the hose. Clinical cases of Legionnaires' disease or Pontiac fever have been found to be associated with inhalation of garden hose aerosols containing Legionella bacteria. The report provides measured microbial densities resulting from controlled hose conditions in order to quantify the human health risks. The densities of Legionella spp. identified in two types of hoses were found to be similar to those reported during legionellosis outbreaks from other causes. It is proposed that the risk could be mitigated by draining hoses after use.\n\n"}
{"id": "915295", "url": "https://en.wikipedia.org/wiki?curid=915295", "title": "Kangaroo care", "text": "Kangaroo care\n\nKangaroo care or kangaroo mother care (KMC), sometimes called skin-to-skin care, is a technique of newborn care where babies are kept skin-to-skin with a parent, typically their mother. It is most commonly used for low birth-weight preterm babies, who are more likely to suffer from hypothermia, while admitted to a neonatal unit to keep the baby warm and support early breastfeeding.\n\nKangaroo care, named for the similarity to how certain marsupials carry their young, was initially developed in the 1970s to care for preterm infants in countries where incubators were either unavailable or unreliable. There is evidence that it is effective in reducing both infant mortality and the risk of hospital-acquired infection, and increasing rates of breastfeeding and weight gain.\n\nSkin-to-skin care is also used to describe the technique of placing full-term newborns very soon after birth on the bare chest of their mother or father. This also improves rates of breastfeeding and can lead to improved stability of the heart and breathing rate of the baby.\n\nKangaroo care seeks to provide restored closeness of the newborn with family members by placing the infant in direct skin-to-skin contact with one of them. This ensures physiological and psychological warmth and bonding. The parent's stable body temperature helps to regulate the neonate's temperature more smoothly than an incubator, and allows for readily accessible breastfeeding when the mother holds the baby this way.\n\nWhile this model of infant care is substantially different from the typical Western neonatal intensive-care unit (NICU) procedures, the two are not mutually exclusive, and it is estimated that more than 200 neonatal intensive care units practice kangaroo care today. One recent survey found that 82 percent of neonatal intensive care units use kangaroo care in the United States today.\n\nKangaroo Care is likely the most widely used term in the United States for skin-to-skin contact. Gene Cranston Anderson may have been the first to coin the term Kangaroo Care in the USA. The defining feature of this is however for skin-to-skin contact, commonly abbreviated as SSC, also STS. This is used synonymously with \"skin-to-skin care\". Dr Nils Bergman, one of the founders of the Kangaroo Mother Care Movement, argues that since skin-to-skin contact is a place of care, not a kind of care in itself, skin-to-skin contact should be the preferred term.\n\nKangaroo Mother Care is a broader package of care defined by the World Health Organization. Kangaroo Mother Care originally referred only to care of low birth weight and preterm infants, and is defined as a care strategy including three main components: kangaroo position, kangaroo nutrition and kangaroo discharge.\nKangaroo position means direct skin-to-skin contact between mother and baby, but can include father, other family member or surrogate. The infant should be upright on the chest, and the airway secured with safe technique. (The term Kangaroo Mother Care is commonly used to mean skin-to-skin contact, despite its definition from the WHO as including a broader strategy). Kangaroo nutrition implies exclusive breastfeeding, with additional support as required but with the aim of achieving ultimately exclusive breastfeeding. Kangaroo discharge requires that the infant is sent home early, meaning as soon as the mother is breastfeeding and able to provide all basic care herself. In Colombia in 1985 this took place at weights around 1000g, with oxygen cylinders for home use; the reason was that overcrowding in their hospital meant that three babies in an incubator would result in potentially lethal cross-infections. An essential part of this is close follow-up, and access to daily visits.\n\nPeter de Chateau in Sweden first described studies of \"early contact\" with mother and baby at birth in 1976, articles do not describe specifically that this was skin-to-skin contact. Klaus and Kennell did very similar work in the USA, more well known in the context of early maternal-infant bonding. The first reported use of the term \"skin-to-skin contact\" is by Thomson in 1979 and quotes the work of de Chateau in its rationale. This is contemporary or even precedes the origins of Kangaroo Mother Care in Bogota, Colombia. This latter did however make the concept more widely known.\n\nIn 1978, due to increasing morbidity and mortality rates in the Instituto Materno Infantil NICU in Bogotá, Colombia, Dr. Edgar Rey Sanabria, Professor of Neonatology at Department of Paediatry - Universidad Nacional de Colombia, introduced a method to alleviate the shortage of caregivers and lack of resources. He suggested that mothers have continuous skin-to-skin contact with their low birth weight babies to keep them warm and to give exclusive breastfeeding as needed. This freed up overcrowded incubator space and care givers.\n\nAnother feature of kangaroo care was early discharge in the kangaroo position despite prematurity. It has proven successful in improving survival rates of premature and low birth weight newborns and in lowering the risks of nosocomial infection, severe illness, and lower respiratory tract disease. It also increased exclusive breastfeeding and for a longer duration and improved maternal satisfaction and confidence.\n\nDr Rey and Dr Martinez published their results in 1981 in Spanish, and used the term Kangaroo Mother Method. This was brought to the attention of English speaking health professionals in an article by Whitelaw and Sleath in 1985. Gene Cranston Anderson and Susan Ludington were instrumental in introducing this to North America.\n\n\"Kangaroo Mother Care\" as a term was first defined at a meeting of some 30 interested researchers, attending a meeting convened by Dr Adriano Cattaneo and colleagues in November 1996 in Trieste, Italy, together with the WHO represented by Dr Jelka Zupan.\nAn International Network of Kangaroo Mother Care (INK) was convened at the Trieste meeting and has overseen workshops and conferences every two years. After Trieste, meetings were held in Bogota Colombia 1998, Yogyakarta Indonesia 2000, Cape Town South Africa 2002, Rio de Janeiro Brazil 2004, Cleveland USA 2006, Uppsala Sweden 2008, Quebec Canada 2010, Ahmedabad India 2012, and Kigali Rwanda 2014; the meeting in 2016 planned for Trieste Italy.\n\nAn informal steering committee coordinates these meetings: (alphabetically, current) Nils Bergman, Adriano Cattaneo, Nathalie Charpak, Kerstin Hedberg-Nyqvist, Ochi Ibe, Susan Ludington, Socorro Mendoza, Mantoa Mokrachane, Juan Gabriel Ruiz, Réjean Tessier, Rekha Udani.\nSusan Ludington maintains a \"KC BIB\" (bibliography) on behalf of INK, endeavouring to be a complete inventory of any and all publications relevant to Kangaroo Mother Care. This is also broken down in an analysis of 120 charts, in which specific outcomes are collated.\n\nThe International Kangaroo Care Awareness Day has been celebrated worldwide on May 15 since 2011. It is a day to increase awareness to enhance the practice of Kangaroo Care in NICUS, Post Partum, Labor and Delivery, and any hospital unit that has babies up to 3 months of age.\n\nIn primates, early skin-to-skin contact is part of a universal reproductive behaviour, and early separation is used as a research modality to test the harmful effects on early development. Research suggests that for all mammals, the maternal environment (or place of care) is the primary requirement for regulation of all physiological needs (homeostasis), maternal absence leads to dysregulation and adaptation to adversity.\n\nIn mainstream clinical medicine, Kangaroo Mother Care is used as an adjunct to advanced technology that requires maternal infant separation. However, skin-to-skin contact may have a better scientific rationale than the incubator. All other supportive technology can be provided as part of care to extremely low birth weight babies during skin-to-skin contact, and appears to produce a better effect.\n\nBased on the scientific rationale, it has been suggested that skin-to-skin contact should be initiated immediately, to avoid the harmful effects of separation (Bergman Curationis). In terms of classification and proper defining for research purposes, the following aspects that categorise and define skin-to-skin contact have been proposed:\n\n\nSafe technique should ensure that obstructive apnoea cannot occur. Since the mother must be able to sleep to provide adequate dose, this requires keeping the airway safely open, and close containment to mother’s bare chest using a garment, various of these are described in the WHO guidelines.\n\nMother should be the primary provider of skin-to-skin contact, as only she can breastfeed. However, it is almost always necessary that father should also provide skin-to-skin contact to achieve adequate dose; other family members can also be used. Since skin-to-skin contact is basic to early bonding and attachment, it should probably not be done by hospital staff and other surrogates.\n\nIn 2016 a Cochrane review, \"Kangaroo mother care to reduce morbidity and mortality in low birthweight infants\", was published bringing together data from 21 studies including 3042 low birth-weight babies (less than at birth). This review shows that babies provided kangaroo mother care have a reduced risk of death, hospital-acquired infection, and low body temperature (hypothermia); it is also associated with increased weight gain, growth in length, and rates of breastfeeding.\n\nA further Cochrane review on \"Early skin-to-skin contact for mothers and their healthy babies\", updated in 2015, provides clinical support for the scientific rationale but looks at evidence for early skin-to-skin contact for healthy babies. The available evidence shows that early skin-to-skin contact is associated with increased rates of breastfeeding, and some evidence of improved physiological outcomes (early stability of the heart rate and breathing) for the babies.\nA randomized controlled trial published in 2004 reports that babies born between 1200 and 2200g became physiologically stable in skin-to-skin contact starting from birth, compared to similar babies in incubators. In another randomized controlled trial conducted in Ethiopia, survival improved when skin-to-skin contact was started before 6 hours of age.\n\nWhile Kangaroo Mother Care generally implies care of low birth weight and preterm infants, skin-to-skin contact should be regarded as normal and basic for all newly born humans. The original research by Thomson showed increased breastfeeding rates when skin-to-skin contact started at birth, and when early breastfeeding was encouraged every two hours. Currently, the impact of skin-to-skin contact on breastfeeding is the scientific rationale for Step 4 of the Baby Friendly Hospital Initiative (BFHI), which requires help to \"initiate breastfeeding within one hour of birth\".\n\nOriginally babies who are eligible for kangaroo care include pre-term infants weighing less than , and breathing independently. Cardiopulmonary monitoring, oximetry, supplemental oxygen or nasal (continuous positive airway pressure) ventilation, intravenous infusions, and monitor leads do not prevent kangaroo care. In fact, babies who are in kangaroo care tend to be less prone to apnea and bradycardia and have stabilization of oxygen needs.\n\nDuring the early 1990s, the concept was advocated in North America for premature babies in NICU and later for full term babies. Research has been done in developed countries but there is a lag in implementation of kangaroo care due to ready access of incubators and technology.\n\nRestrictions for eligibility to receive skin-to-skin contact are becoming fewer, the main constraint has probably been caregiver confidence and experience.\n\nIn kangaroo care, the baby wears only a small diaper and a hat and is placed in a flexed (fetal position) with maximum skin-to-skin contact on parent's chest. The baby is secured with a wrap that goes around the naked torso of the adult, providing the baby with proper support and positioning (maintain flexion), constant containment without pressure points or creases, and protecting from air drafts (thermoregulation). If it is cold, the parent may wear a shirt or hospital gown with an opening to the front and a blanket over the wrap for the baby.\n\nThe tight bundling is enough to stimulate the baby: vestibular stimulation from the parent's breathing and chest movement, auditory stimulation from the parent's voice and natural sounds of breathing and the heartbeat, touch by the skin of the parent, the wrap, and the natural tendency to hold the baby. All this stimulation is important for the baby’s development.\n\n\"Birth Kangaroo Care\" places the baby in kangaroo care with the mother within one minute after birth and up to the first feeding. The American Academy of Pediatrics recommends this practice, with minimal disruption for babies that don't require life support. The baby's head must be dried immediately after birth and then the baby is placed with a hat on the mother's chest. Measurements, etc. are performed after the first feeding. According to the US Institute of Kangaroo Care, healthy babies should maintain skin-to-skin contact method for about 3 months so that both baby and mother are established in breastfeeding and have achieved physiological recovery from the birth process.\n\nFor premature babies, this method can be used continuously around the clock or for sessions of no less than one hour in duration (the length of one full sleep cycle.) It can be started as soon as the baby is stabilized, so it may be at birth or within hours, days, or weeks after birth.\n\nKangaroo care is different from the practice of babywearing. In kangaroo care, the adult and the baby are skin-to-skin and chest-to-chest, securing the position of the baby with a stretchy wrap, and it is practiced to provide developmental care to premature babies for 6 months and full-term newborns for 3 months. In babywearing the adult and the child are fully clothed, the child may be in the front or back of the adult, can be done with many different types of carriers and slings, and is commonly practiced with infants and toddlers.\n\nSkin-to-skin contact is effective in reducing pain in infants during painful procedures. There appears to be no difference between mothers and others who provide skin-to-skin contact during medical treatments.\n\nKangaroo care is beneficial for parents because it promotes attachment and bonding, improves parental confidence, and helps to promote increased milk production and breastfeeding success.\n\nA recent study found that the psychological benefits of kangaroo care for parents of preterm infants are fairly extensive. Research shows that the use of kangaroo care is linked to lower parental anxiety levels. Amount of parental anxiety is related to parental age, family income, and socioeconomic status. Older parents may have additional life experience or resilience, leading to decreased anxiety. Higher socioeconomic status may contribute to less anxiety as well. These factors may lead to lower levels of baseline anxiety, and therefore a further decrease of anxiety following kangaroo care. Factors such as gestational age, parental gender, and marital status did not appear to affect parental anxiety. Kangaroo care was shown to decrease anxiety scores in both mothers and fathers, also unrelated to parents’ marital status.\n\nKangaroo care has also lead to greater confidence in parenting skills. Parents who used kangaroo care displayed higher confidence in their ability to care for their child. Kangaroo care has been shown to positively impact breastfeeding as well, with mothers producing larger amounts of milk for longer periods of time. Overall, kangaroo care has many important benefits for parents as well as infants.\n\nBoth preterm and full term infants benefit from skin to skin contact for the first few weeks of life with the baby's father as well. The new baby is familiar with the father's voice and it is believed that contact with the father helps the infant to stabilize and promotes father to infant bonding. If the infant's mother had a caesarean birth, the father can hold their baby in skin-to-skin contact while the mother recovers from the anesthetic.\n\nKangaroo care arguably offers the most benefits for pre-term and low-birth-weight infants, who experience more normalized temperature, heart rate, and respiratory rate, increased weight gain, and fewer hospital-acquired infections. Additionally, studies suggest that preterm infants who experience kangaroo care have improved cognitive development, decreased stress levels, reduced pain responses, normalized growth, and positive effects on motor development. Kangaroo care also helps to improve sleep patterns of infants, and may be a good intervention for colic. Earlier discharge from hospital is also a possible outcome Finally, kangaroo care helps to promote frequent breastfeeding, and can enhance mother-infant bonding. Evidence from a recent systematic review supports the use of kangaroo mother care as a substitute for conventional neonatal care in settings where resources are limited.\"\n\nAccording to some authorities there is a growing body of evidence that suggests that early skin-to-skin contact of mother and baby stimulates breast feeding behavior in the baby. Newborn infants who are immediately placed on their mother’s skin have a natural instinct to latch on to the breast and start nursing, typically within one hour of being born. It is thought that immediate skin-to-skin contact provides a form of imprinting that makes subsequent feeding significantly easier. The World Health Organization reports that in addition to more successful breastfeeding, skin-to-skin contact between a mother and her newborn baby immediately after delivery also reduces crying, improves mother to infant interaction, and keeps baby warm. According to studies quoted by UNICEF, babies have been observed to naturally follow a unique process which leads to a first breastfeed. After birth, babies who are placed skin to skin on their mothers chest will:\n\n\nProviding that there are no interruptions, all babies are said to follow this process and it is suggested that trying to rush the process or interruptions such as removing the baby to weigh or measure is counter-productive and may lead to problems at subsequent breastfeeds.\n\nFor mothers with low milk supply, increasing skin-to-skin contact is recommended, as it promotes more frequent feeding and stimulates the milk ejection reflex, prompting the body to produce more milk.\n\nKangaroo care often results in reduced hospital stays, reduced need for expensive healthcare technology, increased parental involvement and teaching opportunities, and better use of healthcare dollars.\n\nOverall, kangaroo care helps to reduce morbidity and mortality, improves parent satisfaction, provides opportunities for teaching during postnatal follow-up visits, and decreases hospital-associated costs.\n\nKangaroo care \"is an effective and safe alternative to conventional neonatal care for LBW infants, mainly in resource-limited countries.\" Kangaroo Mother Care reduces mortality, and also morbidity in resource limited settings, though further studies are needed.\n\nThe main controversy among proponents of Kangaroo Mother Care relates to eligibility to initiate kangaroo position: in the original Rey & Martinez model and as described in the WHO guidelines, the infant should be stable to \"tolerate skin-to-skin contact\". From a biological and neuroscience perspective, others argue that it is separation from mother that causes the instability,\n\nRegarding ‘kangaroo nutrition’ there is little controversy, with accumulating evidence for the benefits of breastfeeding as such, and evidence that even preterm infants can exclusively breastfeed.\n\nFurther controversy concerns the ‘early discharge’, which is defended by the Fundación Canguro, in Bogota, Colombia, and reported in evidence from a Cochrane review.\n\nThe International Kangaroo Care Awareness Day is celebrated on May 15 since 2011. It is a day to increase awareness, education, and celebration to enhance the practice of kangaroo care globally. Healthcare professionals, parents, volunteers around the world show their support, in their own way, for improving Kangaroo Care practice to benefit babies, parents, and society at large.\n\n\n"}
{"id": "1565519", "url": "https://en.wikipedia.org/wiki?curid=1565519", "title": "Larder", "text": "Larder\n\nA larder is a cool area for storing food prior to use. Larders were commonplace in houses before the widespread use of the refrigerator, but only amongst the middle classes.\n\n\nIn the northern hemisphere, most houses would be arranged to have their larder and kitchen on the north or west side of the house, where it received the least amount of sun. In Australia and New Zealand, larders were placed on the south or east sides of the house for the same reason.\n\nMany larders have small unglazed windows with the window opening covered in fine mesh. This allows free circulation of air without allowing flies to enter. Many larders also have tiled or painted walls to simplify cleaning. Older larders, and especially those in larger houses, have hooks in the ceiling to hang joints of meat or game. Others have insulated containers for ice, anticipating the future development of refrigerators.\n\nA pantry may contain a , a term used in Derbyshire and Yorkshire, to denote a stone slab or shelf used to keep food cool in the days before refrigeration was domestically available. In the late medieval hall, a thrawl would have been appropriate to a larder. In a large or moderately large nineteenth-century house, all these rooms would have been placed as low in the building as possible, or as convenient, in order to use the mass of the ground to retain a low summer temperature. For this reason, a buttery was usually called the cellar by this stage.\n\nVery few modern houses have larders, since this need is now satisfied by refrigerators, freezers, and the convenience of modern grocery stores that eliminate the need to store food for long periods.\n\nMiddle English (denoting a store of meat): from Old French lardier, from medieval Latin lardarium, from laridum\n\n\"In the past, and in many peasant societies, the pig has been a vital source of food for the winter: it can be salted and preserved, and traditionally you can eat every part of it except its squeak. This is reflected in the word larder, which in origin is a place for storing bacon. It comes from the French word meaning ‘bacon’ that also gave us lard (Middle English), and the lardon (Late Middle English), a cube or chunk of bacon.\"\n\nIn medieval households the word \"larder\" referred both to an office responsible for fish, jams, and meat, as well as to the room where these commodities were kept. It was headed by a \"larderer\". The Scots term for larder was \"spence\", and so in Scotland larderers (also pantlers and cellarers) were known as \"spencers\". This is one of the derivations of the modern surname.\n\nThe office generally was subordinated to the kitchen and existed as a separate office only in larger households. It was closely connected with other offices of the kitchen, such as the saucery and the scullery.\n\nLarders were used in the Indus River Valley to store bones of goats, oxen, and sheep. These larders were made of large clay pots.\n\n"}
{"id": "3007169", "url": "https://en.wikipedia.org/wiki?curid=3007169", "title": "Lasso of Truth", "text": "Lasso of Truth\n\nThe Lasso of Truth is a weapon wielded by DC Comics superheroine Wonder Woman, Princess Diana of Themyscira. It is also known as the Magic Lasso or the Lasso of Hestia. It was created by William Moulton Marston, inventor of the lie detector, as an allegory for feminine charm, but it later became more popular as a device to extract truth from people.\n\nThe lariat forces anyone it captures into submission; compelling its captives to obey the wielder of the lasso and tell the truth.\nWilliam Moulton Marston created Wonder Woman but he also worked, in the period before, during and after World War I, on understanding and perfecting the systolic blood-pressure test while working on his Ph.D. in psychology at Harvard University. Blood pressure was one of several elements measured in the polygraph tests that were being perfected since as far back as Italian criminologist Cesare Lombroso, in 1895. Marston's wife, psychologist and lawyer Elizabeth Holloway Marston, one of his inspirations for the Wonder Woman character, also played a key role in his lie detector research.\n\nBut the lie detector had nothing to do with Marston's creation of the Magic Lasso. Wonder Woman's \"Magic Lasso\" or \"Golden Lasso\" was the direct result of their research into emotions and was more about submission than truth. So Marston created the Magic Lasso as an allegory for feminine charm and the compliant effect it has on people. The idea behind feminine allure was that submission to a pleasant controller (instead of a harsh one) was more pleasant and therefore made it more likely that people would submit.\n\nIn a 1997 academic article, psychologist Geoffry Bunn \"incorrectly\" reinforces a correlation between the lasso and the systolic blood-pressure test, stating:\n\nThe lasso was formed from Aphrodite's girdle, which made it indestructible and its magical properties were granted by the Goddess herself. The powers forced whomever was bound within it to obey the commands of whomever held the other end. This effect could be used on larger groups of people, although this reduced its efficiency. In addition to being unbreakable, the lasso was also infinitely elastic.\n\nDiana coated it in special Amazon chemicals that allowed it to transform her civilian clothes into Wonder Woman's garb. Diana demonstrated a remarkable level of skill with the lasso, performing such feats as twirling it to create air currents (upon which she could float) and spinning it to emit certain frequencies that disrupted spells.\n\nIn the post-\"Crisis\" George Pérez reboot the lasso was forged by the god Hephaestus from the Golden Girdle of Gaea that was once worn by Antiope, sister of Hippolyta. It is so strong that not even Hercules can break it and is given to Diana after Hippolyta consults the Goddesses. Originally the Magic Lasso was given to Wonder Woman when she returned to Paradise Island. Then William Moulton Marston later retconned the origin story in Wonder Woman #1 when it was shown that Wonder Woman got it just before she left Paradise Island.\n\nEmpowered by the fires of Hestia, the lasso forces anyone held by it to tell the absolute truth. Furthermore, simple physical contact with the lasso can be enough to have this effect such as when Barbara Ann Minerva attempted to swindle it from Diana, but was forced to confess her intentions when she held the lasso. It is also infinitely long, and can lengthen depending on its user's desire. The fires are said to even be able to cure insanity, as they did in the case of Ares, God of War, when he attempted to incite World War III. He renounced his plan when the lasso showed him that such a war would not only destroy all life on Earth as he wished, but also any potential worshippers he sought to gain from it. The lasso possesses incredible strength and is virtually unbreakable. One story even showed Wonder Woman using the lasso to contain the explosion of two atom bombs. Unable to stop the American bombs that would set off a Russian doomsday machine she wrapped the bombs in her lasso and let the bombs explode. It has easily held beings with tremendous superhuman strength such as Superman, Captain Marvel, who has the strength of Hercules and the Power of Zeus, and Power Girl, as well as gods such as Ares and Heracles. (In several Pre-Crisis stories, it was even capable of binding Wonder Woman herself on the occasions she was caught, sometimes by Gunther.) It is shown that Wonder Woman still has her powers even if bound by the lasso.\n\nThe only times it has ever been shown to break was when truth itself was challenged. For example, in \"JLA\" the lasso broke when she refused to believe the confession it wrought from Rama Khan of Jarhanpur. Elsewhere, when the backwards-thinking monster Bizarro was caught in \"\", he was horrified by the very idea of truth. As the antithesis of reason and logic he was able to break the lasso. The fairy tale villainess, Queen of Fables, who has the power to bring any fictional or non-true character to life, and is herself \"fictional\", had power over the lasso by bringing fictional characters to life and having her non-true minions break it. It is worth noting that Wonder Woman had in fact hoped to win simply by lassoing her and let its powers of truth destroy the fairy tale villain.\n\nThe magic lasso has subsequently been shown to produce a wide array of effects. When battling the entity Decay, Wonder Woman used the lasso's link to Gaia, the Greek Goddess of the Earth, as a circuit between the earth and the monster, pumping the entity of death with life-giving energies that destroyed the creature. Diana herself stated that the lasso's connection to Gaea also constantly renews its user with these energies. Wonder Woman has also used it to create a ring of protective fire around people to protect them from Circe's bestiamorphs. The lasso's energies are also shown to be capable of destroying beings forcibly resurrected by the rings of the Black Lantern Corps. As the goddess of truth, Diana also used it to take memories of Donna Troy and restore her to life. In Pre-Crisis comics, the lasso also had the power to effectively control those who were bound within it.\n\nIn the mini-comic enclosed with the release of the Kenner Super Powers figure of Wonder Woman, the Amazing Amazon ensnares a mind-controlled Superman with her lasso, preventing him from destroying the Washington Monument. Superman is unable to resist the powers of the lasso as Wonder Woman renders him unconscious. Later, Wonder Woman uses her lasso on Brainiac and commands the villain to release Superman from his mind control.\n\nIn later Post-Crisis comics, the power of truth was written as innate to Wonder Woman herself, with the lasso merely a focus of that power. A storyline in the Morrison-era JLA comics by Joe Kelly depicted the lasso as an archetypal manifestation of universal truth, and, once broken (like when Wonder Woman doubted the truth that it was revealing to her because she didn't like it), disrupted the underlying truth of reality itself. With the lasso broken, reality came to be dictated by whatever people believed to be the case, starting with older beliefs and extending to beliefs that were held by various individuals in the present. This resulted in Earth becoming the center of the universe for two weeks, Earth becoming flat for several hours, the moon turning into cheese for a time, Kyle Rayner assuming a Hal Jordan-like appearance (many people still saw Hal as 'the' Green Lantern), and Batman fading in and out of existence due to his 'urban legend' status (meaning that people weren't sure if he even existed). This allegorical interpretation is often ignored in later stories and by much of fandom, as the lasso was long established as magically unable to break, and was never before stated to be the ultimate representation of truth. During her adventures with the Justice League team of superheroes Diana eventually battled a villain named Amazo who was able to duplicate aspects of the lasso for his own use.\n\nDuring her current tenure as writer for Wonder Woman, Gail Simone has further explored the nature of the Lasso of Truth, describing it as \"a deadly weapon, that not only binds you, and follows its mistress’ commands, the damned thing can see into your soul.\"\n\nThis lasso should not be confused with the lasso of the current Wonder Girl, Cassie Sandsmark. That lasso, given to her by Ares, has the power to shock a target with \"Zeus' lightning if Cassandra ropes her target and becomes angry with them. Donna Troy also wields a mystical lasso of her own called the Lasso of Persuasion, which has the ability to persuade anyone within its confines to do Donna's bidding if her willpower is greater than theirs.\n\nSimilarly, the character Bizarra also has a magic lasso, the difference being that her lasso forces one to tell lies.\n\nDespite Wonder Woman's lasso being mystical in origin, in \"\", shows that Batman apparently has reverse-engineered the Amazo technology, which aids duplicating the lasso's capabilities artificially. During \"Endgame\", when the Joker uses a toxin to turn the Justice League against Batman, Batman is able to immobilise Diana using the 'blind of veils', essentially a Lasso of Lies that was woven by Hephaestus after he created the original Lasso by inverting the original weave. Allegedly created using the wool from the sheep used by Odysseus and his men to escape the blind cyclops, it took Batman two years to acquire on the supernatural black market, incorporating it into a suit of armor specifically designed to stand up to the Justice League, with the blind of veils trapping Diana in an illusion where she has killed Batman.\n\nIn the Elseworlds tale \"Red Son\", Wonder Woman was subdued and restrained in her own lasso by the Soviet terrorist incarnation of Batman. In order to free herself and rescue Superman from Lex Luthor's deadly red sun lamps, Wonder Woman snapped the cords of her \"indestructible\" lasso. The shock of the incident appeared to age Diana, leaving her grey-haired, frail, and unable to speak.\n\nThe lasso appears in the film \"\", where Wonder Woman makes her first live action theatrical film appearance. It is used in the battle against Doomsday to tie the monster down until Superman stabs him with the kryptonite spear. \n\nIn addition, it appears in the 2017 Wonder Woman film, in which it is also called \"Lasso of Hestia\". This film's depiction of the golden lasso is partially used as an Indiana Jones-style whip. Diana uses it for many battles, but prior to this, it is also used to interrogate Steve Trevor, with Menalippe noting that, \"it is pointless & painful to resist.\"\n\nIn \"Justice League\", the Lasso primarily appears in the use for truth, such as when Aquaman sits on it, causing him to confess his true feelings for the other members of the League and even his attraction to Diana herself; When Superman needs to be reminded who he is and finally, as a grappling tool when first battling Steppenwolf. The Lasso of Hestia is not used as a weapon in the Justice League film.\n\nThe lasso features in the 1970s live-action \"Wonder Woman\" series. In season one the lasso had the power to compel those bound to tell the truth. Beginning with the second season, it also had the power to cause selective amnesia. The lasso appeared to be able to expand and contract, as in the comic books; instead of being a cord of several links at her waist, it is indefinitely longer and sturdier when used to lasso people or being thrown. In season two, with the updated costume, the lasso is even shorter and more like fabric, and only about twenty feet long, unless used to lasso a person or object. It was significantly longer and heavier when in use.\n\nIn the \"Super Friends\" animated series, the lasso possessed the ability to follow the telepathic commands of Wonder Woman, physically moving on its own to accomplish tasks. The ability is never displayed in the comics, although it is hinted that without her tiara, Wonder Woman cannot fully utilize the lasso's ability. In \"Super Friends\", Wonder Woman was typically displayed using the lasso as a tool for accomplishing feats of strength, leaving it unclear to what extent Wonder Woman herself possessed great strength or the lasso itself performed the feats. In addition, its truth-compelling power was used in the \"Challenge of the Super Friends\" episode \"Sinbad and the Space Pirates\". Superman found himself snared by the lasso, but he manages to tie the controlled Wonder Woman as well. In that situation, Superman forces her to confess whether he is her enemy or friend and the truth of her friendship with him forced from Wonder Woman broke the pirates' power over her. This power was also used in \"The World's Greatest Super Friends\" episode \"Space Knights of Camelon\".\n\nOn \"\", in the episode \"The Fear\", Wonder Woman suggests using the lasso to get a confession out of one of Scarecrow's victims, though Professor Jonathan Crane (out of costume) warns her against it for fear of trauma.\n\nIn the \"Justice League\" animated series, the lasso is \"only\" used as an exceptionally long, flexible, and unbreakable rope. In \"Justice League Unlimited\" however, Wonder Woman's lasso was officially portrayed as being able to compel the truth. This ability was finally unleashed in the episode \"The Balance\" by Wonder Woman's mother Queen Hippolyta who revealed that Diana had stolen the uniform before being told of its full capabilities. Upon touching the star on the tiara, various parts of the Wonder Woman costume began to temporarily glow such as the tiara, bracelets, belt and lasso. It was after this that Diana discovered that the lasso could compel truth. However, in the series, Diana only used the truth powers of the lasso once, on the demon Abnegazar to learn the location of Felix Faust, an event that occurred in the same episode.\n\nIt was used in the \"Injustice\" video game.\n\n"}
{"id": "39265475", "url": "https://en.wikipedia.org/wiki?curid=39265475", "title": "List of Austrian inventions and discoveries", "text": "List of Austrian inventions and discoveries\n\nAustrian inventions and discoveries are objects, processes or techniques invented or discovered partially or entirely by a person born in Austria. In some cases, their Austrianess is determined by the fact that they were born in Austria, of non-Austrian people working in the country. Often, things discovered for the first time are also called \"inventions\", and in many cases, there is no clear line between the two.\n\nThe following is a list of inventions or discoveries generally believed to be Austrian:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "44372880", "url": "https://en.wikipedia.org/wiki?curid=44372880", "title": "List of handheld game consoles", "text": "List of handheld game consoles\n\nThis is a list of handheld game consoles, portable video game consoles with a built-in screen and game controls and separate games. It does not include PDAs, smartphones, or tablet computers; while those devices are often capable of playing games, they are not primarily video game consoles.\n\nFor handheld dedicated consoles, which only play games built into the system, see list of dedicated consoles.\n\nFor home video game consoles, see List of home video game consoles.\n\n"}
{"id": "13525252", "url": "https://en.wikipedia.org/wiki?curid=13525252", "title": "Masaya Nakamura (businessman)", "text": "Masaya Nakamura (businessman)\n\nNakamura stepped down as Namco's CEO in 2002 and took a ceremonial role in the company's management, three years before Namco merged with Bandai, forming Bandai Namco Holdings. Nakamura was awarded the \"Order of the Rising Sun, Gold Rays with Rosette\" by the Japanese government in 2007 for his contributions to Japanese industry.\n\nNakamura was born on 24 December 1925. He graduated from Yokohama Institute of Technology in 1948, having studied shipbuilding. In 1955, in the wake of Japan's economic recovery from World War II, he then founded Nakamura Seisakusho, a company which created kiddie rides for department stores. In one such business deal, Nakamura secured a deal with the department store chain Mitsukoshi in the early 1960s to install a ride on the building's rooftop. The ride was very popular and Mitsukoshi commissioned Nakamura's company to install similar rides across all their stores. The company was renamed Nakamura Manufacturing Company, and later shortened to Namco. \n\nThe company soon expanded from mechanical amusement rides to arcade games in the 1970s. Some of the company's first arcade games used electromechanical projection technology such as \"F-1\" (1976). Nakamura saw potential in the rising video game sector and hired a number of software engineers to allow the company develop its own, as well as testing the games himself for their amusement value. He also purchased Atari, Inc.'s failing Japan division from Nolan Bushnell for $500,000, beating rival Sega's offer of $80,000; the purchase allowed Nakamura's company to distribute Atari's games for a decade, and inspired the company to their own designs. Popular games, such as \"Galaxian\" (1979) and \"Pac-Man\" (1980), were introduced during the late 70s and through the 80s. \"Pac-Man\" was developed by one of Nakamura's new hires, Toru Iwatani, with Nakamura having suggested the \"Pac\" name as a shortened form of \"pakku\", the sound the character made while it ate dots and ghosts on the screen. \"Pac-Man\" became the company's runaway hit, leading to several sequels including \"Ms. Pac-Man\" (1981). As of 2016, \"Pac-Man\" is one of the highest-grossing arcade games of all time, earning $3.5 billion by 1990 or $7.68 billion adjusted for 2016 inflation, according to \"US Gamer\". Nakamura said in a 1983 interview that though he did expect it to be successful, \"I never thought it would be this big\". Nakamura is considered to have been one of the first to identify the potential of \"screen addiction\" due to how much younger people would play \"Pac-Man\" and his company's other games. Namco also invested in the handheld and console gaming market, which took off in the late 80s, with the success of \"Ridge Racer\" (1993) and \"Tekken\" (1994). Because of his vision in developing arcade games, Nakamura is often considered \"the father of Pac-Man\", and credited as one of the instrumental people behind Japan's video game industry.\n\nNakamura also led Namco to manage a chain of now-defunct theme parks across Japan. After Namco bought the film studio Nikkatsu in 1993, Nakamura became involved in film production and was credited as executive producer on a number of Nikkatsu movies. In 2002, Nakamura stepped down as CEO, taking on a more ceremonial role in the company's management. Namco later merged with Bandai to form Bandai Namco Holdings in 2005, with Nakamura retaining an honorary position in its entertainment arm, Bandai Namco Entertainment. At this point, Nakamura was Japan's 68th richest person.\n\nThe Japanese government awarded Nakamura the \"Order of the Rising Sun, Gold Rays with Rosette\" in their 2007 Spring Conferment of Decorations for his contribution to Japanese industry. Nakamura was part of the inaugural inductees into the International Video Game Hall of Fame in Ottumwa, Iowa, celebrated during The Big Bang Gaming Extravaganza on 5–8 August 2010. Nakamura acknowledged his induction via a video feed. The arcade game \"Pac-Man\" was also inducted into the Hall of Fame during these events, which coincided with the celebration of the 30th anniversary of the game's release.\n\nNakamura died on 22 January 2017 at the age of 91. The announcement of his death was made by Bandai Namco on 30 January, requesting respect for his family's privacy. His funeral and wake were held privately, but the company plans to offer a separate public memorial service for Nakamura.\n"}
{"id": "17699472", "url": "https://en.wikipedia.org/wiki?curid=17699472", "title": "Membrane electrode assembly", "text": "Membrane electrode assembly\n\nA membrane electrode assembly (MEA) is an assembled stack of proton exchange membranes (PEM) or alkali anion exchange membrane (AAEM), catalyst and flat plate electrode used in fuel cells and electrolyzers.\n\nThe PEM is sandwiched between two electrodes which have the catalyst embedded in them. The electrodes are electrically insulated from each other by the PEM. These two electrodes make up the anode and cathode respectively.\n\nThe PEM is a fluoropolymer (PFSA) proton permeable but electrical insulator barrier. This barrier allows the transport of the protons from the anode to the cathode through the membrane but forces the electrons to travel around a conductive path to the cathode. The most commonly used Nafion PEMs are Nafion XL, 112, 115, 117, and 1110.\n\nThe electrodes are heat pressed onto the PEM. Commonly used materials for these electrodes are carbon cloth or carbon fiber papers. NuVant produces a carbon cloth called ELAT which maximizes gas transport to the PEM as well as moves water vapor away from the PEM. Imbedding ELAT with noble metal catalyst allows this carbon cloth to also act as the electrode. Many other different methods and procedures also exist for the production of MEAs which are quite similar between fuel cells and electrolyzers.\n\nPlatinum is one of the most commonly used catalysts, however other platinum group metals are also used. Ruthenium and platinum are often used together, if carbon monoxide (CO) is a product of the electro-chemical reaction as CO poisons the PEM and impacts the efficiency of the fuel cell. Due to the high cost of these and other similar materials, research is being undertaken to develop catalysts that use lower cost materials as the high costs are still a hindering factor in the widespread economical acceptance of fuel cell technology.\n\nCurrent service life is 7,300 hours under cycling conditions, while at the same time reducing platinum group metal loading to 0.2 mg/cm2.\n\nAt this time most companies manufacturing MEAs specialize solely in high volume production, such as W. L. Gore & Associates, Johnson Matthey, 3M, WUTenergy and Ion Power. However, there are many companies which produce custom or low quantity MEAs, allowing different shapes, catalysts or membranes to be evaluated as well, which include FuelCellStore, FuelCellsEtc, HIAT gGmbH and many others.\n\n"}
{"id": "4817184", "url": "https://en.wikipedia.org/wiki?curid=4817184", "title": "Network Load Balancing", "text": "Network Load Balancing\n\nNetwork load balancing (commonly referred to as dual-WAN routing or multihoming) is the ability to balance traffic across two WAN links without using complex routing protocols like BGP.\n\nThis capability balances network sessions like Web, email, etc. over multiple connections in order to spread out the amount of bandwidth used by each LAN user, thus increasing the total amount of bandwidth available. For example, a user has a single WAN connection to the Internet operating at 1.5Mbit/s. They wish to add a second broadband (cable, DSL, wireless, etc.) connection operating at 2.5Mbit/s. This would provide them with a total of 4Mbit/s of bandwidth when balancing sessions.\n\nSession balancing does just that, it balances sessions across each WAN link. When Web browsers connect to the Internet, they commonly open multiple sessions, one for the text, another for an image, another for some other image, etc. These sessions can be balanced across the available connections. An FTP application only uses a single session so it is not balanced; however if a secondary FTP connection is made, then it may be balanced so that the traffic is distributed across two of the various connections and thus provides an overall increase in throughput.\n\nAdditionally, network load balancing is commonly used to provide network redundancy so that in the event of a WAN link outage, access to network resources is still available via the secondary link(s). Redundancy is a key requirement for business continuity plans and generally used in conjunction with critical applications like VPNs and VoIP.\n\nFinally, most network load balancing systems also incorporate the ability to balance both outbound and inbound traffic. Inbound load balancing is generally performed via dynamic DNS which can either be built into the system, or provided by an external service or system. Having the dynamic DNS service within the system is generally thought to be better from a cost savings and overall control point of view.\n\nMicrosoft has also purchased a technology that it renamed Network Load Balancing (NLB) that allows for efficient utilization of multiple network cards.\nMS NLB can be configured in unicast or in multicast mode where in multicast mode you can enable IGMP snooping.\n\nMS NLB was introduced for the first time in Windows NT server to spread traffic over multiple hosts without the need for a hardware based load balancer, e.g. when you host a busy web-server application where a single host wouldn't be able to manage all the traffic. And in more recent applications it would be used in Windows clusters for Hyper-V or Microsoft SQL Server\n\nIn unicast mode MS NLB reassigns the stations MAC address (which applies to the clusters IP address) to a virtual MAC address and all NIC's in the NLB cluster use this same MAC address. This setup will cause all incoming traffic for the cluster to be flooded to all ports of the switch as unknown unicast frames: even to hosts that are not joining in the cluster. To keep flooding minimal you would need to use a dedicated VLAN for the cluster.\n\nAnother option is to make NLB in multicast mode. The unicast IPv4 address of the cluster is linked to a multicast MAC address. The hosts in the cluster will never send traffic to the switch using this MAC address with the cluster IPv4 address so one would need to create a static ARP entry on the router (layer 3) in the attached network. Not all vendors will allow you to create an ARP entry where you use a unicast IP address and a multicast MAC address. Cisco publishes some examples how to set up MS NLB on Catalyst switches running IOS and these same examples can be used for switches from many other vendors. As with NLB in unicast mode: incoming traffic towards the cluster will be flooded to all ports in the switch/VLAN and not all vendors support this setup. To limit the flooding, MS NLB now supports IGMP which should lead to the switches learning which ports are actually using the multicast address, but it doesn' always lead to the desired result. For example, Dell PowerConnect multi-layer switches officially don't support MS NLB in multicast. Even though it does work, it will lead to high CPU utilization - affecting (other) traffic in the switch and on other switches one might have other limitations such that the switch to which the NLB NICs are connected can't be the same switch that does the IP routing.\n\nWhen multiple servers are joined to create a cluster. Clusters can use network load balancing whereby simultaneous cluster request are distributed between cluster servers.\n\nRound-robin DNS records is one form of cluster load balancing. It works by creating multiple host records (usually A and/or AAAA) for one machine. As clients make requests, DNS rotates through its list of records.\n\nIn addition to the before mentioned, to configure a terminal server cluster, one needs a load-balancing technology such as Network Load Balancing (NLB) or DNS round robin. A load balancing solution will distribute client connections to each of the terminal servers.\n\nTerminal Server Session Directory is a feature that allows users to easily and automatically reconnect to a disconnected session in a load balanced Terminal Server farm. The session directory keeps a list of sessions indexed by username and server name. This enables a user, after disconnecting a session, to reconnect to the correct Terminal Server where the disconnected session resides in order to resume working in that session.\nThis reconnection will work even if the user connects from a different client computer. \n\n"}
{"id": "1470131", "url": "https://en.wikipedia.org/wiki?curid=1470131", "title": "Number Assignment Module", "text": "Number Assignment Module\n\nNumber Assignment Module (NAM) - The NAM is the electronic memory in the cellular phone that stores the telephone number, International mobile subscriber identity and an Electronic Serial Number. Phones with dual- or multi-NAM features offer users the option of registering the phone with a local number in more than one market.\n"}
{"id": "2534136", "url": "https://en.wikipedia.org/wiki?curid=2534136", "title": "Pressure piling", "text": "Pressure piling\n\nPressure piling describes phenomena related to combustion of gases in a tube or long vessel. As the flame front propagates along the tube, the unburned gases ahead of the front are compressed, and hence heated. The amount of compression varies depending on the geometry and can range from twice to eight times the initial pressure. Where multiple vessels are connected by piping, ignition of gases in one vessel and pressure piling may result in a deflagration to detonation transition and very large explosion pressure.\n\nIn electrical equipment in hazardous areas, if two electrical enclosures are connected by a conduit, an explosion of a gas in one of the compartments travels through the conduit into the next enclosure. The pressure of the 'primary' explosion together with the pressure from the 'secondary' explosion in the other compartment produces one huge explosion that the equipment cannot handle. Heat, arcs or sparks escape from the equipment and ignite any gas or vapour that may be around.\n\nOperators avoid this by not using conduits to join classified equipment together and by using barrier glands on cables going into the enclosure. This ensures that compartments remain separate at all times.\n"}
{"id": "15418785", "url": "https://en.wikipedia.org/wiki?curid=15418785", "title": "Professional network service", "text": "Professional network service\n\nA professional network service (or, in an Internet context, simply professional network) is a type of social network service that is focused solely on interactions and relationships of a business nature rather than including personal, nonbusiness interactions.\n\nA professional network service is used by business individuals to establish and maintain professional contacts and a way to either find work or get ahead in career as well as gain resources and opportunities for networking. According to LinkedIn managing director Clifford Rosenberg in an interview by AAP in 2010, \"[t]his is really a call to action for professionals to re-address their use of social networks and begin to reap as many rewards from networking professionally as they do personally.\" Businesses mostly depend on resources and information outside company and in order to get what they need, they need to reach out and professionally network to others, such as employees or clients as well as potential opportunities.\n\n\"Nardi, Whittaker and Schwarz (2002) point at three main tasks that they believe networkers need to attend in order to keep a successful professional (intentional) network:\nbuilding a network, maintaining the network and activating selected contacts. They stress that networkers need to continue to add new contacts to their network in order to access as many resources as possible, and to maintain their network through staying in touch with their contacts. This is so that the contacts are easy to activate when the networker has work that needs to be done.\"\n\nBy using a professional network service, businesses are able to keep all of their networks up-to-date, in order, and help figure out the best way to efficiently get in touch with each of them. A service that can do all that helps relieve some of the stress when trying to get things done.\n\nNot all professional network services are online sites that help promote a business. There are services that connect the user to other services that help promote the business other than online sites, such as phone/Internet companies that provide services and companies that specifically are designed to do all of the promoting, online and in person, for a business.\n\nIn 1997, professional network services started up throughout the world and continue to grow. The first recognizable site to combine all features, such as create profiles, add friends, and search friends, was SixDegrees.com. According to Boyd and Ellison's article, \"Social Network Sites: Definition, History, and Scholarship\", \"[f]rom 1997 to 2001, a number of community tools began supporting various combinations of profiles and publicly articulated Friends\". Boyd and Ellison go on to say that the next wave began with Ryze.com in 2001. It was introduced as a new way \"to help people leverage their business networks\".\n\nQuite a lot of work is put into a professional network service, such as the amount of hours that go into them and the type of people they work for, as well as the business model of it all, such as the professional interaction and the multiple services they deal with.\n\nSome professional network services do not only help promote the business, but can also help in connecting to other people. Those services may include a specific phone and/or Internet company or a company that helps to connect with other businesses. According to the Society for New Communications Research (SNCR), there are at least nine online professional networks that are being used.\n\nKaplan and Haenlein go on to discuss the five points about using media for companies. They say you need to choose carefully, pick the application or make your own, ensure activity alignment, integrate a media plan, and allow access for all.\n\n\"Choosing the right medium for any given purpose depends on the target group to be reached and the message to be communicated. On one hand, each Social Media application usually attracts a certain group of people and firms should be active wherever their customers are present. On the other hand, there may be situations whereby certain features are necessary to ensure effective communication, and these features are only offered by one specific application.\"\n\n\"Sometimes you may decide to rely on various Social Media, or a set of different applications within the same group, in order to have the largest possible reach.\" \"Using different contact channels can be a worthwhile and profitable strategy.\" According to the Society for New Communications Research at Harvard University \"the average professional belongs to 3-5 online networks for business use, and LinkedIn, Facebook and Twitter are among the top used.\"\n\nSocial media and traditional media are \"both part of the same: your corporate image\" in the customers' eyes.\n\n\"...once the firm has decided to utilize Social Media applications, it is worth checking that all employees may actually access them.\" According to the SNCR \"the convergence of Internet, mobile, and social media has taken significant shape as professionals rely on anywhere access to information, relationships and networks.\"\n\n\"Half of respondents report participating in 3 to 5 online professional networks. Another three in ten participate in 6 or more professional networks.\" \"Popular social networks are now being used frequently as Professional Communities. More than nine in ten respondents indicated that they use LinkedIn and half reported using Facebook. Twitter and blogs were frequently listed as 'professional networks'.\"\n\nAccording to Michael Rappa's article, Business models on the web\", \"a business model is the method of doing business by which a company can sustain itself – that is, generate revenue. The business model spells-out how a company makes money by specifying where it is positioned in the value chain.\" Rappa mentions that there are at least nine basic categories in which a business model can be separated from. Those categories are brokerage, advertising, infomediary, merchant, manufacturer, affiliate, community, subscription, and utility. \"...a firm may combine several different models as part of its overall Internet business strategy.\" At first, Flickr started off as a way to mainstream public relations.\n\nWhen it comes to the social impact that professional network services have on today's society, it has proved to increase activity. According to the SNCR, \"[t]hree quarters of respondents rely on professional networks to support business decisions. Reliance has increased for essentially all respondents over the past three years. Younger (20–35) and older professionals (55+) are more active users of social tools than middle aged professionals. There are more people collaborating outside their company wall than within their organizational intranet.\"\n\nSince the internet and social media are a part of this \"world where consumers can speak so freely with each other and businesses have increasingly less control over the information available about them in cyberspace\", most firms and businesses are uncomfortable with all of the freedom. According to Kaplan and Haenlein's article, \"Users of the world, unite! The challenges and opportunities of Social Media\", businesses are pushed aside and are only able to sit back and watch as their customers publicly post comments, which may or may not be well written.\n\n"}
{"id": "40298480", "url": "https://en.wikipedia.org/wiki?curid=40298480", "title": "Propagation time", "text": "Propagation time\n\nIn digital circuits, propagation time is the delay of the basic inverter of a given family. Thus, it measures the speed at which such family can operate.\n\n"}
{"id": "34936221", "url": "https://en.wikipedia.org/wiki?curid=34936221", "title": "SAP Information Interchange OnDemand", "text": "SAP Information Interchange OnDemand\n\nThe SAP Information Interchange OnDemand technology is no longer available.\n\nSAP Information Interchange OnDemand (FKA. SAP Information Interchange by Crossgate) is a B2B / EDI solution powered by the SAP AG that allows companies to exchange electronic documents (like purchase orders, forecasts, invoices, delivery notes and many more) via their SAP applications.\n\nElectronic documents are delivered through a centralized canonical B2B repository. Integration with SAP applications is available through direct Intermediate Document IDoc/Remote Function Call (RFC) application interfaces, in combination with SAP Process Integration as a middleware layer, and via direct consumption of Enterprise Services.\n\nThe German company Crossgate AG launched their so-called “Business Ready Network” which supports standards-based, custom EDI and B2B integration of business partners, clients and suppliers. In order to simplify business network enablement, Crossgate had created pre-packaged Services for specific business processes, industries and SAP applications. In 2008 SAP became a shareholder in Crossgate. \nLater the two companies furthered their partnership by entering into a global reseller agreement that allowed SAP to offer their customers Crossgate’s B2B Content Engine as an SAP Solution Extension. \nIn 2011 SAP acquired Crossgate with the goal to “instantly Connect SAP Customers and Their Business Partners for Networking at Enterprise Level”. \nThe Solution was renamed from \"SAP Information Interchange by Crossgate\" to SAP Information Interchange OnDemand.\n\nSAP Information Interchange OnDemand is a document exchange service that is operated in a cloud computing environment. It utilizes the EDI/B2B Transaction Server SAP II that has been developed by Crossgate AG as foundation. On top of that it incorporates connections to SAP's NetWeaver platform for the service which makes integrating with SAP users that already run NetWeaver very simple. Companies without SAP NetWeaver can activate the service using IDocs or RFC. The service requires subscription and is offered as a fixed cost solution (fixed license fee, fixed cost managed services).\n\n"}
{"id": "11961734", "url": "https://en.wikipedia.org/wiki?curid=11961734", "title": "SIA Ltd", "text": "SIA Ltd\n\nSIA Ltd was a UK Limited company specialising in Geographic Information System (GIS) software and services. The company offices were based in London, England.\n\nThe company was bought by Hometrack in July 2007. It was based near the Eccleston Square Hotel, off the A3213.\n\n\n\n\n"}
{"id": "42356699", "url": "https://en.wikipedia.org/wiki?curid=42356699", "title": "SME One Asia Awards", "text": "SME One Asia Awards\n\nSME One Asia Awards is an event that is organised by The APF Group in Singapore. Currently in its fourth Year, it has a total of 350 nominations and 191 winners, and has at least 1000 attendees for the awards dinner which is held at Marina Bay Sands every year.\n\nThe SME One Asia Awards was established in 2011 by the APF Group Pte Ltd to recognise the businesses and their leaders’ achievements of the Small and Medium Enterprises in Singapore, particularly the companies that have implemented the business practices that contribute to the development of people, society and the environment. Through these developments, the awards promote sustainable growth in Singapore as well as overseas.\n\nThe SME One Asia Awards targets SMEs with more than S$1 million and less than S$100 million in yearly revenue. Currently, SMEs are an important part in Asia, as they help generate economic growth in Asia. Furthermore, the awards are built on a branding and publicity programme dedicated to raising the profiles and awareness of its winners.\n\nThe SME One Asia Awards is also a platform in which socially responsible business leaders are given the opportunity to connect with each other, share business ideas and practices as well as growing to become an active community of socially responsible leaders.\nIn 2012, the SME One Asia Awards ranked and judged 120 submissions, which included nominations from China, Cambodia, Philippines, Malaysia and Indonesia, of which 66 companies were selected as winners across the six award categories.\n\nIn 2013, the SME One Asia Awards spanned eleven industries with 27.3% of the companies coming from the service industry, 4.6% of the companies coming from the information technology sector, 1.5% of the companies came from the entertainment and tourism sectors, another 1.5%of the companies came from agriculture industry, 16.7% came from the construction industry, 18.2% of the companies came from distribution and wholesale sector, 4.6% came from the electronics and engineering sector, 6.1% of the companies came from the manufacturing industry, a further 6.1% of the companies were in the F&B sector, 3%of the companies came from the logistics and transport industry and last but not least, 10.6% of the companies came from the real estate sector.\n\nThis award comprises six categories, defined by the number of years the participating company has been in operation. The Distinguished Awards, for companies that have been in operation for more than 15 years. The Prominent Awards are for companies who have been in operation for more than 9 years. The Notable Awards are for companies who have been in operation for more than 5 years. The Emerging Awards are for companies who have been in operation for more than 1 year but less than 4 years. Last but not least, the Foreign Enterprise Award is given to foreign companies with a local presence and the Overseas Enterprise Award which is given to overseas businesses.\n\nIn 2012, there was the introduction of the special awards category. The awards were designed to acknowledge companies with significant contributions. The Avant -Garde award is awarded to companies with creative business innovation. There is also the Hoffen award which is awarded to companies which inspire community development.\n\nThe Avant-Garde Award recognises companies that display business innovation and creativity, as well as companies who have effectively introduced inspiring, new and revolutionary ideas to their business models, products and/or services. Furthermore, the implementation of the ideas has to be well defined and must contribute to the company’s overall business performance. In addition to that, the idea has to be original and surprising and must be able to capture imagination of people as well as the industry. One Winner of this Award was BreadTalk Group, who won it in 2012\n\nThe Hoffen Award recognises the faith and commitment of companies that bring new life and revival to living spaces, as well as honouring the companies which contribute to the betterment of the environment and people’s lives. This award is meant to inspire people and social groups to make a difference, and for businesses to go beyond profit making. One winner of this Award was The Fullerton Heritage who won it in 2012.\n\nThe participating companies undergo both quantitative and qualitative assessment from a panel of judges. The judging criterion comprises the Companies’ Sales Turnover, their Brand Value, their Profit Margin / Profit per Staff, Entrepreneur Spirit / Business Model, and whether there is Responsible Leadership.\n\nThe SMEs that have been nominated for this award will have to undergo a selection process with advice and assessment from a panel of Judges/Advisors. In addition to that, the appointed panel of judges will provide insights and opinions that would add a critical dimension that will help to develop SMEs across the region. Furthermore, impartiality and confidentiality of the judging and auditing processes is ensured as an independent panel of judges/advisors from various associations, institutions and industry leaders are selected.\n\nThe inaugural event was held at Marina Bay Sands Grand Ballroom on 28 October 2011. It attracted 110 nominations, of which 60 companies were winners. There were 1100 guests who attended the gala awards ceremony presided over by the Guest of Honour, Mr Seah Kian Peng who presented the award trophies. Several ambassadors from various countries in Asia were present to witness the ceremony.\n\nIn 2012, the second Awards Dinner was held at Marina Bay Sands, with 65 winners from 9 categories with a total of 1130 guests attending the event. The guests include the winners and their guests, as well as business owners and foreign delegates. Furthermore, the guest of honour for that year was Mr Seah Kian Peng.\n\nIn 2013, the third Awards Dinner was held at Marina Bay Sands, with a total of 1180 guests attending the event, the guests include the Winners and their guests, business owners, distinguished guests, ambassadors, foreign delegates, members from trade associations, government institutions, sponsors as well as partners, media partners and journalists from the various newspapers.\n\nBelow is the summary of the number of awards given out for each year, including significant winners\n\nFor the SME One Asia Awards Dinner, Channel News Asia was the official media partner for the SME One Asia Awards. Other supporting media partners include The Business Times, The Straits Times and Lianhe Zaobao. Also included are the various write ups that were featured in other publications.\n\nThe SME One Asia Awards 2013 includes participants from Malaysia, Philippines, Indonesia, Cambodia, South Korea and China. Supporting regional partners and their media providers are also invited for the event.\n\nSupporting Organisation: Singapore Indian Chamber of Commerce and Industry (SICCI)\n\nOfficial Knowledge Management Partner: Singapore Institute of Management\n\nOfficial Design Partner: A.S. Louken Group Pte Ltd\n\nOfficial Trophy Co-Sponsor: Royal Selangor International\n\nOfficial Media Partner: CNBC\n\n"}
{"id": "571325", "url": "https://en.wikipedia.org/wiki?curid=571325", "title": "Sample and hold", "text": "Sample and hold\n\nIn electronics, a sample and hold (S/H, also \"follow-and-hold\") circuit is an analog device that samples (captures, takes) the voltage of a continuously varying analog signal and holds (locks, freezes) its value at a constant level for a specified minimum period of time. Sample and hold circuits and related peak detectors are the elementary analog memory devices. They are typically used in analog-to-digital converters to eliminate variations in input signal that can corrupt the conversion process. They are also used in electronic music, for instance to impart a random quality to successively-played notes (for example, in the bass line to Jan Hammer's \"Don't You Know\".)\n\nA typical sample and hold circuit stores electric charge in a capacitor and contains at least one switching device such as a FET (field effect transistor) switch and normally one operational amplifier. To sample the input signal the switch connects the capacitor to the output of a buffer amplifier. The buffer amplifier charges or discharges the capacitor so that the voltage across the capacitor is practically equal, or proportional to, input voltage. In hold mode the switch disconnects the capacitor from the buffer. The capacitor is invariably discharged by its own leakage currents and useful load currents, which makes the circuit inherently volatile, but the loss of voltage (\"voltage drop\") within a specified hold time remains within an acceptable error margin.\n\nSample and hold circuits are used in linear systems. In some kinds of analog-to-digital converters, the input is compared to a voltage generated internally from a digital-to-analog converter (DAC). The circuit tries a series of values and stops converting once the voltages are equal, within some defined error margin. If the input value was permitted to change during this comparison process, the resulting conversion would be inaccurate and possibly unrelated to the true input value. Such successive approximation converters will often incorporate internal sample and hold circuitry. In addition, sample and hold circuits are often used when multiple samples need to be measured at the same time. Each value is sampled and held, using a common sample clock.\n\nFor practically all commercial liquid crystal active matrix displays based on TN, IPS or VA electro-optic LC cells (excluding bi-stable phenomena), each pixel represents a small capacitor, which has to be periodically charged to a level corresponding to the greyscale value (contrast) desired for a picture element. In order to maintain the level during a scanning cycle (frame period), an additional electric capacitor is attached in parallel to each LC pixel to better hold the voltage. A thin-film FET switch is addressed to select a particular LC pixel and charge the picture information for it. In contrast to an S/H in general electronics, there is no output operational amplifier and no electrical signal AO. Instead, the charge on the hold capacitors controls the deformation of the LC molecules and thereby the optical effect as its output. The invention of this concept and its implementation in thin-film technology have been honored with the IEEE Jun-ichi Nishizawa Medal.\n\nDuring a scanning cycle, the picture doesn’t follow the input signal. This does not allow the eye to refresh and can lead to blurring during motion sequences, also the transition is visible between frames because the backlight is constantly illuminated, adding to display motion blur.\n\nTo keep the input voltage as stable as possible, it is essential that the capacitor have very low leakage, and that it not be loaded to any significant degree which calls for a very high input impedance.\n\n\n"}
{"id": "9749044", "url": "https://en.wikipedia.org/wiki?curid=9749044", "title": "Slow design", "text": "Slow design\n\nSlow Design is a branch of the Slow Movement, which began with the concept of Slow Food, a term coined in contrast to \"fast food\". As with every branch of the Slow Movement, the overarching goal of Slow Design is to promote well being for individuals, society, and the natural environment. Slow Design seeks a holistic approach to designing that takes into consideration a wide range of material and social factors as well as the short and long term impacts of the design.\n\nSlow Design refers to the goals and approach of the designer, rather than the object of the design. In this way a Slow Design approach can be used within any design field. The term was probably first coined by Alastair Fuad-Luke in his 2002 paper \"'Slow Design' - a paradigm for living sustainably?\", in which Slow Design is seen as the next step in the development of sustainable design, balancing individual, socio-cultural, and environmental needs.\n\nWhile Fuad-Luke focused on the design of physical products, the concept can be applied to the design of non-material things such as experiences, processes, services, and organizations. In fact, Slow Design may be seen as a path toward the dematerialization required for long-term sustainability as it takes into account the non-material nature of human well being and happiness.\n\nBeth Meredith and Eric Storm attempt to summarize the concept, stating: \n\nSlow Design is a democratic and holistic design approach for creating appropriately tailored solutions for the long-term well being of people and the planet. To this end, Slow Design seeks out positive synergies between the elements in a system, celebrates diversity and regionalism, and cultivates meaningful relationships that add richness to life.\nThe editor and journalist Spencer Bailey has described Slow Design as “design that’s timeless and made to last, and done thoughtfully, with intent, and with care for our planet.”\n\nCommon qualities of Slow Design include:\n\nSlow design is still a relatively new concept of design thinking, and its implications are yet to be fully developed and defined. It could evolve in the following ways:\n\n\n"}
{"id": "18198290", "url": "https://en.wikipedia.org/wiki?curid=18198290", "title": "Soft story building", "text": "Soft story building\n\nA soft story building is a multi-story building in which one or more floors have windows, wide doors, large unobstructed commercial spaces, or other openings in places where a shear wall would normally be required for stability as a matter of earthquake engineering design. A typical soft story building is an apartment building of three or more stories located over a ground level with large openings, such as a parking garage or series of retail businesses with large windows.\n\nBuildings are classified as having a \"soft story\" if that level is less than 70% as stiff as the floor immediately above it, or less than 80% as stiff as the average stiffness of the three floors above it. Soft story buildings are vulnerable to collapse in a moderate to severe earthquake in a phenomenon known as soft story collapse. The inadequately-braced level is relatively less resistant than surrounding floors to lateral earthquake motion, so a disproportionate amount of the building's overall side-to-side drift is focused on that floor. Subject to disproportionate lateral stress, and less able to withstand the stress, the floor becomes a weak point that may suffer structural damage or complete failure, which in turn results in the collapse of the entire building.\n\nSoft story failure was responsible for nearly half of all homes that became uninhabitable in California's Loma Prieta earthquake of 1989, and was projected to cause severe damage and possible destruction of 160,000 homes in the event of a more significant earthquake in the San Francisco Bay Area, California. As of 2009 few such buildings in the area had undergone the relatively inexpensive seismic retrofit to correct the condition. In 2013, San Francisco mandated screening of soft story buildings to determine if retrofitting is necessary, and required that retrofitting be completed by 2017 through 2020.\n\nIn Los Angeles 2016 following San Francisco's Ordinance, the city adopted a similar ordinance targeting soft-story apartment buildings first. [https://ladbs.org/docs/default-source/publications/misc-publications/ordinance_183893.pdf?sfvrsn=6 <nowiki>[4]</nowiki>] This ordinance is to reduce structural damage in the event of an earthquake by reinforcing \"soft-story\" areas with steel structures. A soft-story building is described as existing wood-frame buildings with soft, weak, or open-front walls and existing non-ductile concrete buildings in the ordinance. Most of these buildings were built before 1978 before building codes were changed.\n\nProperty owners are being targeted by the size of their buildings. The first group of ordinances went out May 2, 2016 with 16 or more Units and more than 3 stories. The 2nd is July 22, 2016 with 16 or more Units and 2 Stories. The third is October 17, 2016, with 16 or less units and more than 3 stories. The fourth is January 30, 2017 for 9-15 units. The 5th is May 29, 2017 for 7-8 units. The 6th is August 14, 2017 for 4-6 units. Then on October 30, 2017 Condos and Commercial buildings will receive their orders to comply. Property owners have 2 years from the date of the order to have approved plans. Then 3.5 years from the order to obtain the permit. Total completion and certificate of compliance must be issued by 7 years of the date. Property owners also have the option to demolish building. Plans must be in order within 2 years, and permit within 3.5, and demolished by 7 years.\n"}
{"id": "235893", "url": "https://en.wikipedia.org/wiki?curid=235893", "title": "Submersible pump", "text": "Submersible pump\n\nA submersible pump (or sub pump, electric submersible pump (ESP)) is a device which has a hermetically sealed motor close-coupled to the pump body. The whole assembly is submerged in the fluid to be pumped. The main advantage of this type of pump is that it prevents pump cavitation, a problem associated with a high elevation difference between pump and the fluid surface. Submersible pumps push fluid to the surface as opposed to jet pumps having to pull fluids. Submersibles are more efficient than jet pumps.\n\nCa. 1928 Armenian oil delivery system engineer and inventor Armais Arutunoff successfully installed the first submersible oil pump in an oil field. In 1929, Pleuger Pumps pioneered the design of the submersible turbine pump, the forerunner of the modern multi-stage submersible pump. \n\nElectric submersible pumps are multistage centrifugal pumps operating in a vertical position. Liquids, accelerated by the impeller, lose their kinetic energy in the diffuser where a conversion of kinetic to pressure energy takes place. This is the main operational mechanism of radial and mixed flow pumps.\n\nThe pump shaft is connected to the gas separator or the protector by a mechanical coupling at the bottom of the pump. Fluids\nenter the pump through an intake screen and are lifted by the pump stages. Other parts include the radial bearings (bushings) distributed along the length of the shaft providing radial support to the pump shaft. An optional thrust bearing takes up part of the axial forces arising in the pump but most of those forces are absorbed by the protector’s thrust bearing.\n\nSubmersible pumps are found in many applications. Single stage pumps are used for drainage, sewage pumping, general industrial pumping and slurry pumping. They are also popular with pond filters. Multiple stage submersible pumps are typically lowered down a borehole and most typically used for residential, commercial, municipal and industrial water extraction (abstraction), water wells and in oil wells.\n\nOther uses for submersible pumps include sewage treatment plants, seawater handling, fire fighting (since it is flame retardant cable), water well and deep well drilling, offshore drilling rigs, artificial lifts, mine dewatering, and irrigation systems.\nPumps in electrical hazardous locations used for combustible liquids or for water that may be contaminated with combustible liquids must be designed not to ignite the liquid or vapors.\n\nSubmersible pumps are used in oil production to provide a relatively efficient form of \"artificial lift\", able to operate across a broad range of flow rates and depths. By decreasing the pressure at the bottom of the well (by lowering bottomhole flowing pressure, or increasing drawdown), significantly more oil can be produced from the well when compared with natural production. The pumps are typically electrically powered and referred to as Electrical Submersible Pumps (ESP).\n\nESP systems consist of both surface components (housed in the production facility, for example an oil platform) and sub-surface components (found in the well hole). Surface components include the motor controller (often a variable speed controller), surface cables and transformers. The subsurface components are deployed by attaching to the downhole end of a tubing string, while at the surface, and then lowered into the wellbore along with the tubing.\n\nA high-voltage (3 to 5 kV) alternating-current source at the surface drives the subsurface motor. Until recently, ESPs had been costly to install due to the requirement of an electric cable extending from the source to the motor. This cable had to be wrapped around jointed tubing and connected at each joint. New coiled tubing umbilicals allow for both the piping and electric cable to be deployed with a single conventional coiled tubing unit. Cables for sensor and control data may also be included.\n\nThe subsurface components generally include a pump portion and a motor portion, with the motor downhole from the pump. The motor rotates a shaft that, in turn, rotates pump impellers to lift fluid through production tubing to the surface. These components must reliably work at high temperatures of up to and high pressures of up to , from deep wells of up to deep with high energy requirements of up to 1000 horsepower (750 kW). The pump itself is a multi-stage unit with the number of stages being determined by the operating requirements. Each stage includes an impeller and diffuser. Each impeller is coupled to the rotating shaft and accelerates fluid from near the shaft radially outward. The fluid then enters a non-rotating diffuser, which is not coupled to the shaft and contains vanes that direct fluid back toward the shaft. Pumps come in diameters from 90mm (3.5 inches) to 254mm (10 inches) and vary between and in length. The motor used to drive the pump is typically a three phase, squirrel cage induction motor, with a nameplate power rating in the range 7.5 kW to 560 kW (at 60 Hz).\n\nESP assemblies may also include: seals coupled to the shaft between the motor and pump; screens to reject sand; and fluid separators at the pump intake that separate gas, oil and water. ESPs have dramatically lower efficiencies with significant fractions of gas, greater than about 10% volume at the pump intake, so separating gas from oil prior to the pump can be important. Some ESPs include a water/oil separator which permits water to be reinjected downhole. As some wells produce up to 90% water and fluid lift is a significant cost, reinjecting water before lifting it to the surface can reduce energy consumption and improve economics Given ESPs' high rotational speed of up to 4000 rpm (67 Hz) and tight clearances, they are not very tolerant of solids such as sand.\n\nThere are at least 15 brands of oilfield ESPs used throughout the world.\n\n\"Submersible pump cable\" are designed for use in wet ground or under water, with types specialized for pump environmental conditions.\n\nA submersible pump cable is a specialized product to be used for a submersible pump in a deep well, or in similarly harsh conditions. The cable needed for this type of application must be durable and reliable as the installation location and environment can be extremely restrictive as well as hostile. As such, submersible pump cable can be used in both fresh and salt water. It is also suitable for direct burial and within well castings. A submersible pump cable's area of installation is physically restrictive. Cable manufacturers must keep these factors in mind to achieve the highest possible degree of reliability. The size and shape of submersible pump cable can vary depending on the usage and preference and pumping instrument of the installer. Pump cables are made in single and multiple conductor types and may be flat or round in cross section; some types include control wires as well as power conductors for the pump motor. \nConductors are often color-coded for identification and an overall cable jacket may also be color-coded.\n\nMajor types of cable include:\n\nIn 3&4 Core cable as per right side \"SPC types\" image shown, Plain Copper/Tinned Copper used as conductor. \n\n\n"}
{"id": "22878791", "url": "https://en.wikipedia.org/wiki?curid=22878791", "title": "Supplier risk management", "text": "Supplier risk management\n\nSupplier risk management (SRM) is an evolving discipline in operations management for manufacturers, retailers, financial services companies and government agencies where the organization is highly dependent on suppliers to achieve business objectives. \n\nThe complexity and globally outsourced nature of today’s supply chains combined with the practice of optimization techniques such as lean and just-in-time manufacturing in order to improve efficiency has increased supply chain vulnerabilities to even minor supply disruptions. While these models have allowed companies to reduce overall costs and expand quickly into new markets, they also expose the company to the risk of a supplier suddenly going bankrupt, closing operations, data breach or being acquired. Among the several types of supply disruptions, most severe are those that have a relatively low probability of occurrence with a very high severity of impact when they do occur. While such risks cannot be eliminated, however, its severity can be reduced.\nTo overcome these challenges, companies mitigate supply chain interruptions and reduce risk with strategies and tactics that address supplier-centric risk at multiple stages in the relationship: \n\n\nIn 2008-2009, manufacturers experienced the startling speed at which suppliers can move from stability to shutting down operations. The devastating impact of a crucial supplier failure has moved risk management from add-on service to mission-critical. With a new focus on risk management, manufacturers have seen value whether the economy is stagnant or thriving. \n\nWith a transparent, accessible and comprehensive set of supplier information, manufacturers have been able to monitor suppliers for behavioral changes which contribute to overall stability, including: \n\nChanges in any of these conditions can be defined as parameters for raising an alert. For example, a financially stable supplier may in fact be about to lose it CEO to retirement – which may cause issues within the management team. Early visibility into that change gives the manufacturer time to ensure it doesn’t negatively affect customers. \n\nBased on the criticality of the supplier and the nature of the alert received, the manufacturer can then choose to take necessary action, such as calling or visiting the supplier, increasing monitoring, or moving towards terminating the relationship with the supplier and finding a replacement.\n\nReducing supplier risk can: \n\n\n"}
{"id": "56716477", "url": "https://en.wikipedia.org/wiki?curid=56716477", "title": "T App Folio", "text": "T App Folio\n\nT App Folio is an integrated app for government to citizen provided by Government of Telangana in India. The service, as a part of Mee Seva 2.0, an integrated app that provides services like Mee Seva services, RTA services, fee payments and bill payment services etc. It is available in Telugu and English.\n\nIt was launched on 28 February 2018 by IT minister of Telangana, K. T. Rama Rao.\n\nAround 150 services including the most used services like MeeSeva, RTA services, fee payments and bill payments.\n\nOther informational services like location services like MeeSeva centers, Ration shops, Hy-Fi hotspots are available on the app.\n\nThe platform supports single sign on feature for using multiple services in on go. It is now extended to mobile platform under M-Governance. T app folio is an app with 180 services from various departments bundled into one single app, similar to Government of India’s, UMANG.\n"}
{"id": "2842072", "url": "https://en.wikipedia.org/wiki?curid=2842072", "title": "Tape dispenser", "text": "Tape dispenser\n\nA tape dispenser is an object that holds a roll of tape and has a mechanism at one end to shear the tape. Dispensers vary widely based on the tape they dispense. Abundant and most common, clear tape dispensers (like those used in an office or at home) are commonly made of plastic, and may be disposable. Other dispensers are stationary and may have sophisticated features to control tape usage and improve ergonomics.\n\nPrior to the development of the tape dispenser, 3M's standard clear scotch tape was sold as a roll, and had to be carefully peeled from the end and cut with scissors. To make the product more useful, the scotch tape sales manager at 3M, John Borden, designed the first tape dispenser in 1932, which had a built-in cutting mechanism and would hold the cut end of the tape until its next use.\n\nA handheld dispenser is a variation of handheld tape dispenser used to apply tape to close boxes, etc. Some refer to it as a \"tape gun\".\n\nSome dispensers are small enough so that the dispenser, with the tape in it, can be taken to the point of application for operator ease. The dispenser allows for a convenient cut-off and helps the operator apply (and sometimes helps rub down) the tape.\n\nTabletop or desk dispensers are frequently used to hold the tape and allow the operator to pull off the desired amount, tear the tape off, and take the tape to the job.\n\nTabletop dispensers are available with electrical assists to dispense and cut pressure-sensitive tape to a predetermined length. They are often used in an industrial setting to increase productivity along manufacturing or assembly lines. They eliminate the need to manually measure and cut each individual piece of tape on high volumes of product or packaging. By automating this process, automatic tape dispensers reduce material waste caused by human error. They also reduce the time needed to cut each piece of tape, therefore reducing labor costs and increasing productivity.\n\nSome taping machinery is semi-automatic: the operator takes an object and puts it in or through a machine which automatically applies the tape. This helps save time and controls the consumption of tape.\n\nFully automatic equipment is available which does not require an operator. All functions can be automated.\n\nHigh speed packaging machinery is an example of highly automated equipment.\n\nGummed (water activated) tape dispensers measure, dispense, moisten, and cut gummed or water-activated adhesive tape. This tape is often composed of a paper backing and adhesive glue that is unable to adhere until it is \"activated\" by contact with water. To perform this step, gummed dispensers often employ a water bottle and wetting brush to moisten each piece of tape as it is dispensed. Many gummed dispensers feature a heater, which is mounted over the feed area to maintain the dispenser’s water temperature. These heaters ensure maximum wetting, and are ideal in cold climates. Gummed tape dispensers are often used in packaging or shipping departments for closing corrugated boxes.\n\n"}
{"id": "46397207", "url": "https://en.wikipedia.org/wiki?curid=46397207", "title": "Universal Test Specification Language", "text": "Universal Test Specification Language\n\nUniversal Test Specification Language (UTSL) is a programming language used to describe ASIC tests in a format that leads to an automated translation of the test specification into an executable test code. UTSL is platform independent and provided a code generation interface for a specific platform is available, UTSL code can be translated into the programming language of a specific Automatic Test Equipment (ATE).\n\nIncreased complexity of ASICs leads to requirements of more complex test programs with longer development times. An automated test program generation could simplify and speed up this process. Teradyne Inc. together with Robert Bosch GmbH agreed to develop a concept and a tool chain for an automated test-program generation. To achieve this a tester independent programming language was required. Hence, UTSL, a programming language that enables detailed description of tests that can be translated into the ATE specific programming language was developed. The ATE manufacturers need to provide a Test Program Generator that uses the UTSL test description as inputs and generates the ATE-specific test code with optimal resource mapping and better practice program code.\n\nAs long as the ATE manufacturer provides with the test program generator that can use UTSL as an input the cumbersome task of translating a test program from one platform to another can be significantly simplified. In other words, the task of rewriting of the test programs for a specific platform can be replaced by the automatically generating the code from the UTSL based test specification. Prerequisite for this is that the UTSL description of tests is sufficiently detailed with definition of the test technique as well as the description of all the necessary inputs and outputs.\n\nBeing a platform independent programming language, UTSL allows the engineers to read, analyse and modify the tests in the test specification regardless of the ATE at which the testing of the ASIC will be done. UTSL is based on C# and allows procedural programming and is class oriented. The classes contain sub-classes which in term have their sub-classes.\n<br>\nUTSL contains high amount of commands and test-functions. It also allows the usage of commonly known high level programming language syntax elements such as \"if/then/else\" and etc.\n\nUTSL is a C# like language where the test are defined as blocks of code. Simple tests such a forcing current and measuring voltage or vice versa can be written in UTSL and with the means of the ATE (Automatic Test Equipment) specific code generator translated into testable code (\"see the picture1\").\n\nUTSL allows the user to set the instruments ranges and clamps in order to guarantee the measurement precision and to prevent the measurements from exceeding the instrument clamp values. The current UTSL capabilities can cover c.a. 70% of the required test specification for ASIC testing. For the remaining 30% one could use the option of writing comments in an informal form as it was done in the past.\n\nUTSL supports language features such as:\n\nFurthermore, specialized classes for testing were added:\n\nUTSL also supports the unitls and scales wherever floating point numbers are used. This is essential for a language that describes a test program where the values can be returned as \"V, mV, uV, A, mA, uA\" and etc.\n\nAlso more complex tests such as serial communications with ASIC that require write and/or read to and from register can be implemented using UTSL.\nThe example below shows a test where a certain trim code is written to a register and based on the trim code the internal regulator steps in voltage which is read back (\"see the picture2\").\n\nAdditionally, UTSL allows the user to define the state of the instrument i.e. connected to the pin, or disconnected from the pin.\n"}
{"id": "756197", "url": "https://en.wikipedia.org/wiki?curid=756197", "title": "Vector signal analyzer", "text": "Vector signal analyzer\n\nA vector signal analyzer is an instrument that measures the magnitude and phase of the input signal at a single frequency within the IF bandwidth of the instrument. The primary use is to make in-channel measurements, such as error vector magnitude, code domain power, and spectral flatness, on known signals.\n\nVector signal analyzers are useful in measuring and demodulating digitally modulated signals like W-CDMA, LTE, and WLAN. These measurements are used to determine the quality of modulation and can be used for design validation and compliance testing of electronic devices.\n\nThe vector signal analyzer spectrum analysis process typically has a down-convert & digitizing stage and a DSP & display stage.\n\nA vector signal analyzer operates by first down converting the signal spectra by using superheterodyne techniques.\nA portion of the input signal spectrum is down-converted (using a voltage-controlled oscillator and a mixer) to the center frequency of a band-pass filter. The use of a voltage-controlled oscillator allows for consideration of different carrier frequencies.\n\nAfter the conversion to an intermediate frequency, the signal is filtered in order to band-limit the signal and prevent aliasing. The signal is then digitized using an analog-to-digital converter. Sampling rate is often varied in relation to the frequency span under consideration.\n\nOnce the signal is digitized, it is separated into quadrature and in-phase components using a quadrature detector, which is typically implemented with a discrete Hilbert transform. Several measurements are made and displayed using these signal components and various DSP processes, such as the ones below\n\nSignal Spectrum from FFT \n\nConstellation Diagram\n\nError Vector Magnitude\n\nTypical vector signal analyzer displays feature the spectrum of the signal measured within the IF bandwidth, a constellation diagram of the demodulated signal, error vector magnitude measurements, and a time domain plot of the signal. Many more measurement results can be displayed depending on the type of modulation being used (symbol decoding, MIMO measurements, radio frame summary, etc.).\n\n"}
{"id": "43356578", "url": "https://en.wikipedia.org/wiki?curid=43356578", "title": "Yabacon Valley", "text": "Yabacon Valley\n\nYabacon Valley (YV) is a nickname for an area within Yaba. Yaba is a suburb of Lagos, Nigeria and located at the mainland of the Lagos. This area is already growing as Nigeria’s technology hub and cluster of hundreds of banking institutions, educational institutions, technology and startup companies which steadily attracts angel investors, venture capitalists, enthusiasts and media people from all over the world. This cluster is the major reason many technology firms are considering opening up shops in Yaba. Close to the region is the Lagos Lagoon which lies on its south-western side, it empties into the Atlantic via Lagos Harbor, a main channel through the heart of the city, 0.5 km to 1 km wide and 10 km long. The term originally was born from an unintentional act of an absent mind, manipulating the Silicon Valley name to create a nick version for this cluster while writing a story title. More so, due to the lagoon near this region, the term Silicon lagoon has also been used to refer to this cluster although this is yet to stick and its origin or creator is unknown.\n\nRegardless of the tech clusters in Africa, Yabacon Valley (YV) or Silicon lagoon — whichever name you chose to call it — continues to be a leading hub for high-tech innovation and development, buoyed by the country’s budding, technology-savvy middle-class and massive online population of 45 million internet users. Nigeria’s fledgling technology start-up scene is witnessing a flurry of activities in the frame of new investment drive, acquisitions, strategic partnerships as well as plans to establish more incubation centers.\n\nGeographically, Yabacon Valley encompasses all Yaba Local Council Development area which was carved out of the old Lagos Mainland Local Government created in 1977 as a separate local government following the national reform of Local government in September 1976. The Lagos Mainland carved out of Lagos city council which administered the Lagos Metropolitan city consists of Lagos Island and Lagos Mainland. So with the creation of three more Local government on 27 August 1991, the former Lagos mainland was re-constituted with Surulere carved out of it. The present Yaba Local Council Area has the look of an urban setting; some areas however mirror rural features and these areas are simply referred to as blighted areas like Makoko and Iwaya. Nonetheless, Yabacon Valley is a development, a commercial nerve center for all regardless of their political and cultural affiliations.\n\nIt is not known if the term \"Yabacon Valley\" has been mentioned or used in reference of this tech cluster in the past, but its first published use is credited to The Business Aim, an online publishing platform with focus on business, strategy, innovation, startup culture and everything new. According to Google search results, the name was first used by \"Blaise Aboh\", then an award-winning editor, now Data Analysis/Design Lead and Founding Partner at Orodata Science Nigeria a Civic technology organization as part of a title for an article on reasons for the arrival of startup accelerators, and also to announce the emergence of a new player named Passion Incubator in Yaba technology ecosystem. The article is dated 13 March 2014. The term is still not widely known however it is been used in conversations among the geeks, players and enthusiasts in the tech ecosystem especially on technology focused blogs and social media. There's still controversy surrounding the name, arguments and comments made on article a popular Nigerian technology blog Techcabal confirms this as few are disgruntled and think a better nickname can be created. They feel the name ought to be something else without the ‘con’ and the ‘valley’ arguing why Nigeria must mimic the west in almost everything. But there are many who actually like the name. Weekly in conversations, this name pops up and there are those who are of the opinion that if the name is disliked so much, it should not be brought up regularly.\n\nLate 2015, a tech blog's community post declared Yabacon Valley is dead. Long live Yaba Right'. The conversation was about how the name Yabacon Valley was not a good branding of the tech cluster, and why other suggestions like 'Yaba Right' or 'Yaba District' or even 'Silicon Yaba' were more suitable names. The Executive Director of Paradigm Initiative Nigeria (PIN) a social enterprise, Gbenga Sesan in a comment said that 'If something wasn't broken, why fix it'? That the whole thing was but a fruitless argument. He was of the opinion that when the time was right, the right name would come, that 'instead of looking for a new name by all means, let all cities focus on real work without assumptions'. It must be noted that one person alone does not have the power to name a country, let alone a district. It comes from careful or careless deliberations, negotiation, discuss, or in the case of YV; incessant community debates and arguments over its origin, originality and suitability. \nYabacon is a portmanteau of ‘Yaba’ the Lagos suburb and ‘Silicon’, a chemical element used to create most semiconductors commercially for electronic computers. Although there are technology companies in this area, there are no companies involved in the making of semiconductors since the cluster is still at its green stage and Nigeria is yet to advance to the technology level of manufacturing electronics. Thus, the name is just a sheer but unconscious imitation of America's Silicon valley. The name Yabacon Valley (YV) is alive although proclaimed dead because the Yaba technology community made it a continuous topic of discussion on blogs, social media and community posts. The name was an ordinary creation of a creative writer while penning the title of an article. The tech community made it the monster it is today.\n\nThe population of Yabacon Valley is between 200 and 300 thousand approximately. It is assumed that females outnumber males in the majority of the localities in Yabacon Valley due to concentration of tertiary institutions in the area.\n\nNorth of this area is Shiro Street down to other side of Morocco road towards the roundabout. This also includes Abule- Ijesha South - Muritala Muhammed way from Jibowu to Wright street junction. To the East is the Lagos lagoon, Onike, Onitiri, Makoko, Iwaya, University of Lagos communities also overlooking lagoon. To the West; a descent of 3rd mainland bridge to Wright Street to Murtala Muhammed way to include Total services station at that junction.\n\nYaba is a part of Lagos Nigeria with many small towns and communities such as Onyigbo, Ebute metta, Makoko, Sabo, Akoka, Abule Ijesha, Onike, Jibowu, and Iwaya among others.\n\nYaba Local council development area as it is today has its secretariat at 198, Herbert Macaulay Street, carved out of the old Lagos Mainland local government which was created in 1977 as a separate Local government following the national reform of Local government in September 1976. Lagos Mainland carved out of Lagos city council which administered the Lagos Metropolitan city; this consists of Lagos Island and Lagos Mainland. So with the creation of three more Local government on 27 August 1991, the former lagos mainland was re-constituted with Surulere carved out of it.\nYaba Local council Development area as one of the Thirty Seven (37) newly created Local council Development areas was created out of Lagos Mainland by the administration of Senator Bola Ahmed Tinubu after the state assembly passed a law creating new local council Development areas.\nThe present Yaba local council area wears an urban setting; some part however mirrored rural features and these areas are simply referred to as blighted areas like Makoko and Iwaya. Nonetheless, Yaba Local council development area is the commercial nerve centre for all regardless of their political and cultural affiliations.\n\n\"Perhaps the strongest push responsible for this industrial powerhouse and cluster is the increase in demand of products leveraging technology for growth\".\n\nYabacon valley has two of Nigeria’s leading educational institutions in Yaba College of Technology (Yabatech) and University of Lagos. Both higher institutions are known to be of higher learning standards and have some of the country’s best brains as lecturers. There of course are the less seasoned institutions which are prestigious in their own right – the federal technical college Akoka (a teacher training institute) and Queens’ college one of Africa’s best post primary educational institutions.\nHistorical significance, landmarks and memorials. Yaba also has notable landmarks like the statue of Herbert Macaulay the father of nationalism in Nigeria. Many other landmarks such as Tejuosho Market, E-center, National library of Nigeria, Yabatech among other notable landmarks there are also historically significant landmarks, such as the tallest building on the mainland Corner stone house, the first filling station in Nigeria - (Total at Sabo) and many more.\n\nYaba has one of the fastest expanding middle class groups in Nigeria as a whole. This is why developers fall over themselves to renovate old buildings or expand them out rightly to create apartments for working class folks. Real estate boom is in no small way an issue to consider when middle-class families demand for accommodation.\n\nCrime rate in most parts of yaba is low. Perhaps the heavy presence of security details around the place deters criminals or could be that the ever busy nature of the place seems to inconvenience criminals. Also there is the possibility that many so called criminals may consider productive work readily available for idle hands to be more rewarding whatever the case crime rate is relatively low.\n\nLagos island is the nerve center of commerce and the service based industry in Nigeria as a whole. Living in Yaba keeps you close to the Victoria island, Ikoyi and Lekki and thereby reduces your transportation expenses, stress due to heavy traffic jams which has found common place in these aforementioned areas.\n\nThe local economy in this part of Lagos is rapidly expanding. Eateries, banks, hotels, retail stores, insurance companies, night clubs are booming implying a lot of opportunities both for investors and employment seekers. The expansion of this area is rapid and many companies are setting up so as to not miss out of its financial benefits.\n\nIn 2011, Wennovation Hub in partnership with African Leadership Forum started incubating startups in the tech ecosystem but not much noise was made till 2012 when Bosun Tijani a social innovator & entrepreneur and a group of individuals set out to help animate a community of change agents who believe in building a strong base for Nigeria through technology. They were certain they could help accelerate a movement of people who are driven by the need to disrupt the status quo in Nigeria through smart application of technology. The name was later changed to Cc-HUB, and it became Nigeria’s first startup incubator. With investment and support in cash and kind from organisations such as the Indigo Trust, Omidyar Network, MainOne Cable Company and the Lagos State government, it soon gained momentum and proceeded to install a fibre-optic-powered information superhighway. In 2011, former banker Seun Onigbinde co-founded BudgIT, a fiscal transparency project, on the third floor of CC Hub’s six-storey building in Yaba. As one of the first early-stage startups to benefit from CC Hub’s incubation drive in 2011, it received $5,000 of its $90,000 seed funding from billionaire businessman Tony Elumelu. Big names like Konga, eCommerce company valued at approximately $200 million as after raising $20 million in Series C rounds, arrived in 2013, while Africa Internet Group which has $469 million in 4 Rounds from six investors transferred six of its companies to Yaba in 2014. In same 2014 BudgIT received $400,000 grant from Omidyar. Mid 2016, Andela – a Nigerian-founded talent accelerator for programmers that has campuses in Lagos, Nairobi and New York – received $24 million in investment from the Chan Zuckerberg Initiative. In 2015 Hotels.ng, which claims to be the largest hotel booking site in Nigeria secured $1.2 million in funding from Omidyar Network to expand its listings across Africa.\n\nA number of startups including Iroko – the biggest digital retailer of Nollywood worldwide with total funding of $40 million – and Paystack, an alternative e-payments company which raised $1.3 million investment in December 2016 from international investors Tencent, Comcast Ventures and more, have offices outside the cluster but still intermingle with those from Yabacon Valley.\n\nA day after Mark Zuckerberg, the founder of Facebook, a social media company with $350 billion market valuation walked on the streets of Yaba in Lagos Nigeria, local and international media went agog as to why Yaba is 'Nigeria's Silicon Valley' after all. In Zuckerberg’s first visit to Africa, his first stop was at Co-Creation Hub (CcHub) in Yaba, Lagos, ground zero for start-ups in order to listen, learn and take ideas back to California on how Facebook can better support tech development and entrepreneurship across Africa. In a Facebook post he said; \"the energy here is amazing and I’m excited to learn as much as I can, I’m looking forward to meeting more people in Nigeria\" after he met and interacted with kids at a summer coding camp in CCHub, developers and entrepreneurs in the startup ecosystem in Nigeria. Then he went to Andela where his wife foundation Chan Zuckerberg foundation had made an investment months back. The visit was part of a series of global town hall meetings.\n\n"}
