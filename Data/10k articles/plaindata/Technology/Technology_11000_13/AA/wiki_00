{"id": "3260411", "url": "https://en.wikipedia.org/wiki?curid=3260411", "title": "Alexander Bogomolny", "text": "Alexander Bogomolny\n\nAlexander Bogomolny (January 4, 1948July 7, 2018) was an Israeli American mathematician. He was known for creating and maintaining the mathematically themed educational website \"Cut-the-Knot\" for the Mathematical Association of America (MAA) Online. He wrote extensively about arithmetic, probability, algebra, geometry, trigononometry and mathematical games. He was a pioneer in mathematical education on the internet, having started \"Cut-the-Knot\" in October 1996.\n\nBogomolny got his master's degree in mathematics from Moscow State University in 1971. From 1971 to 1974 he was a Junior research fellow at the Moscow Institute of Electronics and Mathematics in Moscow. He emigrated to Israel and became a senior programmer at Lake Kinneret Research Laboratory in Tiberias, Israel (19741977) and a software consultant at Ben Gurion University in Negev, Be’er Sheva, Israel (19761977). From 1976 to 1983 he was a Senior Instructor and researcher at Hebrew University in Jerusalem. He got his Ph.D. at Hebrew University in 1981. His dissertation was \"A New Numerical Solution for the Stamp Problem\" and his thesis advisor was Gregory I. Eskin. In 1981 and 1982 he was also a Visiting Professor at Ohio State University where he taught undergraduate mathematics.\n\nFrom 1984 to 1989 he was a Professor of Mathematics at the University of Iowa. From July 1987 to November 1990 he was Vice President of Software Development at CompuDoc, Inc.\n\nCut-the-knot (CTC) was a free, advertisement-funded educational website which Bogomolny maintained from 1996 to 2017. It was devoted to popular exposition of various topics in mathematics. The site was designed for teachers, children and parents, and anyone else curious about mathematics, with an eye to educating, encouraging interest, and provoking curiosity. Its name is a reference to the legend of Alexander the Great's solution to the Gordian knot.\n\nCTC won more than 20 awards from scientific and educational publications, including a \"Scientific American\" Web Award in 2003, the \"Encyclopædia Britannica\"s Internet Guide Award, and \"Science\"s NetWatch award.\n\nThe site was remarkably prolific and contained extensive analysis of many of the classic problems in recreational mathematics including the Apollonian gasket, Napoleon's theorem, logarithmic spirals, The Prisoner of Benda, the Pitot theorem, and the monkey and the coconuts problem. Once, in a remarkable \"tour de force\", CTC published 122 proofs of the Pythagorean theorem.\n\nBogomolny did indeed entertain but his deeper goal was to educate. He wrote a manifesto for CTC in which he said that \"Judging Mathematics by its pragmatic value is like judging symphony by the weight of its score.\" He describes the site as \"a resource that would help learn, if not math itself, then, at least, ways to appreciate its beauty.\" And he wonders why it is acceptable among otherwise well-educated people \"to confess a dislike and misunderstanding of Mathematics as a whole.\"\n\nMany mathematical ideas are illustrated by applets. CTK wiki (powered by PmWiki) extends the main site with additional mathematical content, especially that with more complicated formulae than available on the main site.\n\nBogomolny had to leave academia because he had an uncorrectable hearing problem and was practically deaf in latter years.\n\n"}
{"id": "46213053", "url": "https://en.wikipedia.org/wiki?curid=46213053", "title": "Ammoniaphone", "text": "Ammoniaphone\n\nAmmoniaphone was a voice improvement device invented by Dr. Carter Moffat. It was introduced in 1886.\n\nMoffat thought that the explanation that so many good singers came from Italy must have something to do with the air. When given the opportunity to travel to Italy (according to a newspaper clipping to solve a chemical problem of industrial importance – an assignment he completed so successfully that he was awarded a diploma of honor and a gold medal by the Italian government) he set about testing the air in Italy to reveal its secrets. He discovered that, along with traces of essential oils from herbs, Italian air contained considerable amounts of \"peroxide of hydrogen\", and \"free ammonia\". He then created a device that could reproduce this mixture and administer it in appropriate doses.\n\nAmmoniaphone M812 is on permanent display at the Swedish Museum of Performing Arts / Scenkonstmuseet.\n"}
{"id": "2382486", "url": "https://en.wikipedia.org/wiki?curid=2382486", "title": "AutoAnalyzer", "text": "AutoAnalyzer\n\nThe AutoAnalyzer is an automated analyzer using a flow technique called continuous flow analysis (CFA), first made by the Technicon Corporation. The instrument was invented in 1957 by Leonard Skeggs, PhD and commercialized by Jack Whitehead's Technicon Corporation. The first applications were for clinical analysis, but methods for industrial analysis soon followed. The design is based on separating a continuously flowing stream with air bubbles.\n\nIn continuous flow analysis (CFA) a continuous stream of material is divided by air bubbles into discrete segments in which chemical reactions occur. The continuous stream of liquid samples and reagents are combined and transported in tubing and mixing coils. The tubing passes the samples from one apparatus to the other with each apparatus performing different functions, such as distillation, dialysis, extraction, ion exchange, heating, incubation, and subsequent recording of a signal. An essential principle of the system is the introduction of air bubbles. The air bubbles segment each sample into discrete packets and act as a barrier between packets to prevent cross contamination as they travel down the length of the tubing. The air bubbles also assist mixing by creating turbulent flow (bolus flow), and provide operators with a quick and easy check of the flow characteristics of the liquid. Samples and standards are treated in an exactly identical manner as they travel the length of the tubing, eliminating the necessity of a steady state signal, however, since the presence of bubbles create an almost square wave profile, bringing the system to steady state does not significantly decrease throughput ( third generation CFA analyzers average 90 or more samples per hour) and is desirable in that steady state signals (chemical equilibrium) are more accurate and reproducible.\n\nA continuous flow analyzer (CFA) consists of different modules including a sampler, pump, mixing coils, optional sample treatments (dialysis, distillation, heating, etc.), a detector, and data generator. Most continuous flow analyzers depend on color reactions using a flow through photometer, however, also methods have been developed that use ISE, flame photometry, ICAP, fluorometry, and so forth.\n\nFlow injection analysis (FIA), was introduced in 1975 by Ruzicka and Hansen, The first generation of FIA technology, termed flow injection (FI), was inspired by the AutoAnalyzer technique invented by Skeggs in early 1950s. While Skeggs' AutoAnalyzer uses air segmentation to separate a flowing stream into numerous discrete segments to establish a long train of individual samples moving through a flow channel, FIA systems separate each sample from subsequent sample with a carrier reagent. While the AutoAnalyzer mixes sample homogeneously with reagents, in all FIA techniques sample and reagents are merged to form a concentration gradient that yields analysis results. \n\nFIA methods can be used for both fast reactions as well as slow reactions. For slow reactions, a heater is often utilized. The reaction does not need to reach completion since all samples and standards are given the same period to react. For typical assays commonly measured with FIA (e.g., nitrite, nitrate, ammonia, phosphate) it is not uncommon to have a throughput of 60-120 samples per hour. \n\nFIA methods are limited by the amount of time necessary to obtain a measurable signal since travel time through the tubing tends to broaden peaks to the point where samples can merge with each other. As a general rule, FIA methods should not be used if an adequate signal cannot be obtained within two minutes, and preferably less than one. Reactions that need longer reaction times should be segmented. However,considering the number of FIA publications and wide variety of uses of FIA for serial assays, the \"one minute\" time limitation does not seem to be a serious limitation for most real life assays. Yet, assays based on slow chemical reactions have to be carried either in stopped flow mode ( SIA) or by segmenting the flow. \nOI Analytical, in its gas diffusion amperometric total cyanide method, uses a segmented flow injection analysis technique that allows reaction times of up to 10 minutes by flow injection analysis.\n\nTechnicon experimented with FIA long before it was championed by Ruzicka and Hansen. Andres Ferrari reported that analysis was possible without bubbles if flow rates were increased and tubing diameters decreased. In fact, Skegg's first attempts at the auto analyzer did not segment. Technicon chose to not pursue FIA because it increased reagent consumption and the cost of analysis.\n\nThe second generation of the FIA technique, called sequential injection analysis (SIA), was conceived in 1990 by Ruzicka and Marshal, and has been further developed and miniaturized over the course of the following decade. It uses flow programming instead of the continuous flow regime (as used by CFA and FIA), that allows the flow rate and flow direction to be tailored to the need of individual steps of analytical protocol. Reactants are mixed by flow reversals and a measurement is carried out while the reaction mixture is arrested within the detector by stopping the flow. Microminiaturized chromatography is carried out on microcolumns that are automatically renewed by microfluidic manipulations. The discrete pumping and metering of microliter sample and reagent volumes used in SI only generates waste per each sample injection. The enormous volume of FI and SI literature documents the versatility of FI and SI and their usefulness for routine assays (in soil, water, environmental, biochemical and biotechnological assays) has demonstrated their potential to be used as a versatile research tool.\n\nIn medical testing applications and industrial samples with high concentrations or interfering material, there is often a dialyzer module in the instrument in which the analyte permeates through a dialysis membrane into a separate flow path going on to further analysis. The purpose of a dialyzer is to separate the analyte from interfering substances such as protein, whose large molecules do not go through the dialysis membrane but go to a separate waste stream. The reagents, sample and reagent volumes, flow rates, and other aspects of the instrument analysis depend on which analyte is being measured. The autoanalyzer is also a very small machine\n\nPreviously a chart recorder and more recently a data logger or personal computer records the detector output as a function of time so that each sample output appears as a peak whose height depends on the analyte level in the sample.\n\nTechnicon sold its business to Revlon in 1980 who later sold the company to separate clinical (Bayer) and industrial (Bran+Luebbe - now SEAL Analytical) buyers in 1987. At the time, industrial applications accounted for about 20% of CFA machines sold.\n\nIn 1974 Ruzicka and Hansen carried out in Denmark and in Brasil initial experiments on a competitive technique, that they termed flow injection analysis (FIA). Since then the technique found worldwide use in research and routine applications, and was further modified through miniaturization and by replacing continuous flow with computer controlled programmable flow.\n\nDuring the 1960s industrial laboratories were hesitant to use the autoanalyzer. Acceptance by regulatory agencies eventually came about by demonstration that the techniques are no different from a recording spectrophotometer with reagents and samples added at the exact chemical ratios as traditionally accepted manual methods.\n\nThe best known of Technicon's CFA instruments are the AutoAnalyzer II (introduced 1970), the Sequential Multiple Analyzer (SMA, 1969), and the Sequential Multiple Analyzer with Computer (SMAC, 1974). The Autoanalyzer II (AAII) is the instrument that most EPA methods were written on and reference. The AAII is a second generation segmented flow analyzer that uses 2 millimeter ID glass tubing and pumps reagent at flow rates of 2 - 3 milliliters per minute. Typical sample throughput for the AAII is 30 - 60 samples per hour. Third generation segmented flow analyzers were proposed in the literature, but not developed commercially until Alpkem introduced the RFA 300 in 1984. The RFA 300 pumps at flow rates less than 1 milliliter per minute through 1 millimeter ID glass mixing coils. Throughput on the RFA can approach 360 samples per hour, but averages closer to 90 samples per hour on most environmental tests. In 1986, Technicon (Bran+Luebbe) introduced its own microflow TRAACS-800 system.\n\nBran+Luebbe continued to manufacture the AutoAnalyzer II and TRAACS, a micro-flow analyzer for environmental and other samples, introduced the AutoAnalyzer 3 in 1997 and the QuAAtro in 2004. The Bran+Luebbe CFA business was bought by SEAL Analytical in 2006 and they continue to manufacture, sell and support the AutoAnalyzer II/3 and QuAAtro CFA systems, as well as Discrete Analyzers.\n\nAnd there are other manufacturers of CFA instruments.\n\nSkalar Inc., subsidiary of Skalar Analytical, founded in 1965, which has its head office in Breda (NL), is since its founding an independent company, fully owned by its personnel. Development in robotic analyzers, TOC and TN equipment, and monitors has extended the product lines of its long life SAN++ Continuous Flow Analyzers. Software packages for data acquisition and analyzer control are also in house products, running with latest software demands and handles all analyzer hardware combinations.\n\nAstoria-Pacific International, for example, was founded in 1990 by Raymond Pavitt, who previously owned Alpkem. Based in Clackamas, Oregon, U.S.A., Astoria-Pacific manufactures its own micro-flow systems. Its products include the Astoria Analyzer lines for Environmental and Industrial applications; the SPOTCHECK Analyzer for Neonatal screening; and FASPac (Flow Analysis Software Package) for data acquisition and computer interface.\n\nFIAlab Instruments, Inc., in Seattle, Washington, also manufactures several analyzer systems.\n\nAlpkem was purchased by Perstorp, and then later by OI Analytical in College Station Texas. OI Analytical manufactures the only segmented flow analyzer that uses polymeric tubing in place of glass mixing coils. OI is also the only major instrument manufacturer that provides segmented flow analysis (SFA) and flow injection analysis (FIA) options on the same platform.\n\nAutoAnalyzers were used mainly for routine repetitive medical laboratory analyses, but they had been replaced during the last years more and more by discrete working systems which allow lower reagent consumption. These instruments typically determine levels of albumin, alkaline phosphatase, aspartate transaminase (AST), blood urea nitrogen, bilirubin, calcium, cholesterol, creatinine, glucose, inorganic phosphorus, proteins, and uric acid in blood serum or other bodily samples. AutoAnalyzers automate repetitive sample analysis steps which would otherwise be done manually by a technician, for such medical tests as the ones mentioned previously. This way, an AutoAnalyzer can analyze hundreds of samples every day with one operating technician. Early AutoAnalyzer instruments each tested multiple samples sequentially for individual analytes. Later model AutoAnalyzers such as the SMAC tested for multiple analytes simultaneously in the samples.\n\nIn 1959 a competitive system of analysis was introduced by Hans Baruch of Research Specialties Company. That system became known as Discrete Sample Analysis and was represented by an instrument known as the \"Robot Chemist.\" Over the years the Discrete Sample Analysis method slowly replaced the Continuous Flow system in the clinical laboratory.\n\nThe first industrial applications - mainly for water, soil extracts and fertilizer - used the same hardware and techniques as clinical methods, but from the mid-1970s special techniques and modules were developed so that by 1990 it was possible to perform solvent extraction, distillation, on-line filtration and UV digestion in the continuously flowing stream. In 2005 about two thirds of systems sold worldwide were for water analysis of all kinds, ranging from sub-ppb levels of nutrients in seawater to much higher levels in waste water; other common applications are for soil, plant, tobacco, food, fertilizer and wine analysis.\n\nAutoAnalyzers are still used for a few clinical applications such as neonatal screening or Anti-D, but the majority of instruments are now used for industrial and environmental work. Standardized methods published by the ASTM (ASTM International), the US Environmental Protection Agency (EPA) as well as the International Organization for Standardization (ISO) for environmental analytes such as nitrite, nitrate, ammonia, cyanide, and phenol. Autoanalyzers are also commonly used in soil testing laboratories, fertilizer analysis, process control, seawater analysis, air contaminants, and tobacco leaf analysis.\n\nTechnicon published method sheets for a wide range of analyses and a few of these are listed below. These methods and later methods are available from SEAL Analytical. Method lists for manufacturers instruments are readily available on their websites.\n\n"}
{"id": "42782787", "url": "https://en.wikipedia.org/wiki?curid=42782787", "title": "Canta (vehicle)", "text": "Canta (vehicle)\n\nThe Canta is a two-seat microcar from the Netherlands specifically created for disabled drivers. It was developed in 1995 by Waaijenberg together with the Delft University of Technology. In addition to the standard petrol-engined production models, an electric Canta was designed for the German market but it has remained at the prototype stage. In the Netherlands, it is classified as a mobility aid because the width of the vehicle is only 1.10 metres, thus it may - unlike larger microcars - be used on cycle paths as well as sidewalks and footpaths; in addition a driver's license is not required.\n\nEach Canta is built with the adaptations required by the customer, such as locating controls and switches on the left or right side, or all on the steering wheel. Seats have a wide range of adjustment and can swivel to ease entry and exit. Instead of having a driver's seat, the \"Ride-in Canta\" () model is designed so that a wheelchair user can enter by a ramp at the rear and drive while seated in their wheelchair. The rear of the vehicle is lowered to the ground when parked and is raised pneumatically when the engine is started.\n\n\"De Canta danst!\" is a TV co-production by Viewpoint Productions and NTR in cooperation with the Dutch National Ballet that choreographically features the microcar. It is part of \"Het Nationale Canta ballet\", which in turn is an initiative of writer Karin Spaink, radio producer Bert Kommerij and actress-director Maartje Nevejan. It was choreographed by Ernst Meisner of the Dutch National Ballet, with the music written by composer Robin Rimbaud. The project was financially supported by Stichting Doen and the SNS REAAL Fund. The performances took place on 28 June 2012 at the Westergasfabriek.\n"}
{"id": "12352696", "url": "https://en.wikipedia.org/wiki?curid=12352696", "title": "Channel router", "text": "Channel router\n\nA channel router is a specific variety of router for integrated circuits. Normally using two layers of interconnect, it must connect the specified pins on the top and bottom of the channel. Specified nets must also be brought out to the left and right of the channel, but may be brought out in any order. The height of the channel is not specified - the router computes what height is needed.\n\nThe \"density\" of a channel, defined for every \"x\" within the channel, is the number of nets that appear on both the left and right of a vertical line at that \"x\". The maximum density is a lower bound on the height of the channel. A \"cyclic constraint\" occurs when two pins occur in the same column (but with different orders) in at least two columns. In the example shown, nets 1 and 3 suffer from cyclic constraints. This can only be solved by \"doglegs\" as shown on net 1 of the example.\n\nChannel routers were one of the first forms of routers for integrated circuits, and were heavily used for many years, with YACR perhaps the best known program. However, modern chips have many more than 2 interconnect layers. Although the effort was made to extend channel routers to more layers, this approach was never very popular, since it did not work well with over-the-cell routing where pins are not movable. In recent years, area routers have in general taken over.\n"}
{"id": "3638902", "url": "https://en.wikipedia.org/wiki?curid=3638902", "title": "Chimney starter", "text": "Chimney starter\n\nA chimney starter, also called a charcoal chimney, is a device that is used to ignite either lump charcoal or charcoal briquettes. It is usually a steel cylinder about 8\" (20 cm) in diameter and about 12 to 18 inches (30 to 45 cm) tall. Chimney starters have a plate (or grate) with several holes that is placed horizontally inside the cylinder about 3\" (8 cm) from the bottom. The chimney has large holes around its circumference below the grate. This allows air to flow up underneath the charcoal, which rests atop the grate. Chimneys also have handles that are usually insulated. The chimney starter works by placing newspaper underneath the grate and igniting it. This fire rises through the grate and ignites the charcoal. It is commonly used where charcoal lighter fluid, a toxic petroleum derivative, is inappropriate or banned. It is also used when extra charcoals are required while the grill is being used, such as when slowly cooking something for a few hours.\nA type of chimney starter's basic device, named Automatic Dump Type Charcoal Lighter, was invented in the 1960s by Hugh King, Lavaughn Johnson, and Garner Byars of Corinth, Mississippi and marketed under the \"Auto Fire\" label.\n\nA chimney starter is used by placing charcoal (lump charcoal or briquettes) in the chimney stacked atop the grate, then paper (or other fuel) is placed below the grate to ignite the charcoal. Once all the charcoal is burning (glowing red on the bottom and ashed over on the top), the chimney is lifted by its handle and the burning charcoal dumped into the grill.\n\nIf a couple of pieces of burning charcoal are left in the chimney, and it is filled with unignited charcoal, it will quickly ignite the new batch, this time without the aid of fire lighters or paper, and without smoke.\n\nA non-traditional use is to cook directly over or directly underneath the chimney starter, which provides a high-intensity heat-source for flash-searing fish or other foods.\n"}
{"id": "42995689", "url": "https://en.wikipedia.org/wiki?curid=42995689", "title": "Colorplexer", "text": "Colorplexer\n\nColor television as introduced in North America in 1954 is best described as being 'colored' television. The system used the existing black and white signal but with the addition of a component intended only for television receivers designed to show color. By careful application this 'colored' signal was ignored by ordinary TV sets and had negligible effect on the appearance of the black and white image. This meant that color programs were viewable on the many existing black and white receivers which fulfilled a requirement for 'compatibility' desired by the television industry. Once the so called 'composite' video signal containing the color component had been generated it could be handled just as if it were a black and white signal, eliminating the need to replace much of the existing TV infrastructure. Colorplexer was the RCA name for the equipment that created this 'composite' color signal from three separate images each created in the primary colors, Red, Green and Blue supplied by a color video camera. This process was by the standards of the day quite complex and demanded accurate control of all the various parameters involved if an acceptable color image was to be achieved. The simplification afforded by this 'head end' approach became evident and contributed to the gradual acceptance of color programming over the following decades.\n\nThe National Television System Committee, NTSC, standard was the analog television system that was used in most of the North America from 1941 until the mandatory cutover to ATSC in 2009. However, low-power TV stations were permitted to operate with NTSC, for now, but many have since converted to ATSC. This national standard was later adopted (or, in some cases, adapted) in other jurisdictions, such as Japan.\n\nThe \"Second NTSC Standard\" (525/30, 1941 and later) anticipated that the extant monochrome TV system would eventually incorporate a provision for monochrome-compatible color television. The \"First NTSC Standard\" (441/30, pre-1941) had no such expectation, as even the extant motion picture 3-color system, \"Three-Strip\" Technicolor, was then only five years old. The \"Second NTSC Standard\", as revised for color, sometimes called \"EIA RS-170a\", was operational in North America and elsewhere from 1953 until this standard was replaced by ATSC in the early 21st century.\n\nCentral to this revised standard was a mandate for an information stream, at the transmitter, and broadcast to TV sets (receivers), which was independent of whether the signal was monochrome (already in existence since 1941) or color (adopted in 1953).\n\nThis significant mandate was satisfied by an encoding device which came to be known as a Colorplexer.\n\nColorplexer (a portmanteau of \"color\" and \"multiplexer\") was the RCA trade name for its complex electronic device which encoded discrete red, green and blue 3-color images, as from a color camera, into a composite monochrome-compatible color information stream.\n\nIn RCA's recommendation for monochrome-compatible color TV, generally called \"NTSC color\", each color TV source (as, from a CCU) incorporated its own colorplexer, thereby providing the remaining equipment, all of which were presumed to have originated as a monochrome equipment system, with a signal which could be managed (generated, switched, transmitted, received, etcetera) as if the signal was not color at all, but was an ordinary composite monochrome signal.\n\nThis was a strategic decision on RCA's part, and this \"one Colorplexer per color source\" concept became part of RCA's color TV equipment marketing recommendations. While it made each color source significantly more complicated, hence more expensive, it also obviated the need for major changes to a TV station's signal management system, and the cost of signal management (particularly for networks involving widely separated sources and destinations, such as RCA's wholly owned NBC-TV network) was seen as considerably higher in cost than the color signal sources themselves, as otherwise it would have to be changed from a (composite) Y-only management system into a (component) R-, G- and B-management system (thereby effectively tripling the cost of color signal distribution).\n\nUsing today's three-phase electrical system in an analogy, overlaying an R-, G- and B-color TV signal management system upon an existing monochrome TV signal management system would be analogous to requiring public utility power systems to convert from three-phase to nine-phase electricity, an insurmountable cost penalty.\n\nThe \"Second NTSC Standard\" did not specifically mandate RCA's \"one Colorplexer per color source\" recommendation, as long as the signal actually transmitted to the signal's \"end user\" was monochrome-compatible, and this could have been satisfied by an R, G and B signal management system, and a single Colorplexer at the transmitter, and this would have been adequate for small-market TV stations, particularly those with video sources which were co-located at the station's transmitter site. However, the obvious high cost of R-, G- and B-signal management within a large-market TV station, with separate studio and transmitter sites (sources and destinations separated by perhaps one to tens of miles), or particularly within a TV network, with geographically widely separated sources and destinations (sources and destinations separated by perhaps hundreds to thousands of miles), resulted in adoption of RCA's \"one Colorplexer per color source\" recommendation almost universally, and \"particularly\" after Ampex's introduction of color videotape in 1958 (which was \"never\" a component system at all, but was \"always\" inherently a composite system), and Ampex's (and, later, RCA's) color videotape systems became \"essential\" subsystems of multi-time-zone (national, or, indeed, international) network color TV distribution and transmission.\n\nInitially, the instability of the early Colorplexers caused many operational problems as no two Colorplexers were adjusted alike, and these had to be constantly \"tweaked\", as did the video sources themselves. Eventually, Colorplexer stability improved, as did the stability of the video sources, and NTSC color would go on to provide consistently good color, and it did so until 2009, nearly 56 years, a remarkable technological achievement, as, compared with \"Three-Strip\" Technicolor, perhaps the \"exemplar\" for color motion pictures, which lasted only 19 years (from 1936 to 1955).\n\nThe R, G and B primary color signals are passed through a \"matrix\" to derive the luminance signal, Y, which is the monochrome equivalent of the three primary colors.\n\nWith the addition of inputs from the synchronizing generator, which supplies the blanking and composite synch signals, and inputs from the color burst generator, which supplies the 3.579545 MHz color burst and the \"burst gate\" signals, the colorplexer, using an \"encoder\", synthesizes a compatible signal which includes luminance (described earlier) and chrominance (an amplitude-modulated suppressed-carrier signal with \"I\" and \"Q\" in quadrature, and which represents the differences between the color signals and the monochrome signal), the combination of which produces a monochrome-compatible color information stream.\n\nThe \"burst gate\" admits eight cycles of the 3.579545 MHz \"color burst\" and applies this to the \"back porch\" of each horizontal synch pulse (the vertical synch is unaffected). These eight cycles are just enough to supply a color TV receiver with a reference with which it can correct its own 3.579545 MHz local oscillator as to frequency and phase, phase being the most significant aspect of the process of recovering the \"I\" and \"Q\" signals.\n\nThe \"matrix\" adopted by RCA was Y = 0.30R + 0.59G + 0.11B; the three weighting factors were selected such that their sum was 1.0.\n\nAs with \"prior art\" two-color systems, such as pre-1932 Technicolor, the G signal predominates the R signal; and, as with \"prior art\" three-color systems, such as 1932 and later \"Three-Strip\" Technicolor, the G and R signals predominate the B signal.\n\nRCA's color system was developed while \"Three-Strip\" Technicolor was the \"gold standard\", and Eastman Kodak's Eastmancolor would not completely displace \"Three-Strip\" Technicolor for another half-decade. Indeed, RCA's P22 CRT phosphor was intended to mimic Technicolor's dramatic color palette.\n\nIn most practical color systems, including RCA's, the G signal is taken to be the reference as it has the highest resolution. Indeed, in 1932 to 1944 \"Three-Strip\" Technicolor, the image was enhanced by printing a monochrome image which was taken from a 0.5G negative (called the \"key\" image, and hence that color system was really an RGBK system, not unlike graphic arts' YCMK system) on the film's \"blank receiver\" before the color dyes were applied, as an edge enhancement measure.\n\nEdge enhancement is now a part of many electronically based color systems, but in \"Three-Strip\" Technicolor's day, it was accomplished photographically from the G image, the sharpest of the three.\n\nConventional monochrome TV sets will accept this signal as if there were no chrominance or burst signals at all. A monochrome image, Y, with minimal or no defects (such as moiré, etcetera) will be displayed.\n\nThe \"I\", \"Q\" and \"color burst\" signals will be ignored, leaving only the monochrome image.\n\nColor TV sets will accept this signal and will, first, separate the monochrome image, Y, and will, second, decode the \"I\" and \"Q\" signals, using the extracted \"color burst\" 3.579545 MHz signal as a phase reference to decode these signals.\n\nApplying the monochrome image and the decoded \"I\" and \"Q\" signals to the mathematical inverse of the \"matrix\" reverse synthesizes the R, G and B primary color signals, which were applied to a \"shadow mask\" or equivalent TV tube, and which displays a 3-color color image.\n\nhttp://www.americanradiohistory.com/Archive-RCA-Broadcast-News/RCA-77.pdf\n"}
{"id": "376845", "url": "https://en.wikipedia.org/wiki?curid=376845", "title": "Cooper pair", "text": "Cooper pair\n\nIn condensed matter physics, a Cooper pair or BCS pair is a pair of electrons (or other fermions) bound together at low temperatures in a certain manner first described in 1956 by American physicist Leon Cooper. Cooper showed that an arbitrarily small attraction between electrons in a metal can cause a paired state of electrons to have a lower energy than the Fermi energy, which implies that the pair is bound. In conventional superconductors, this attraction is due to the electron–phonon interaction. The Cooper pair state is responsible for superconductivity, as described in the BCS theory developed by John Bardeen, Leon Cooper, and John Schrieffer for which they shared the 1972 Nobel Prize.\n\nAlthough Cooper pairing is a quantum effect, the reason for the pairing can be seen from a simplified classical explanation. An electron in a metal normally behaves as a free particle. The electron is repelled from other electrons due to their negative charge, but it also attracts the positive ions that make up the rigid lattice of the metal. This attraction distorts the ion lattice, moving the ions slightly toward the electron, increasing the positive charge density of the lattice in the vicinity. This positive charge can attract other electrons. At long distances, this attraction between electrons due to the displaced ions can overcome the electrons' repulsion due to their negative charge, and cause them to pair up. The rigorous quantum mechanical explanation shows that the effect is due to electron–phonon interactions, with the phonon being the collective motion of the positively-charged lattice.\n\nThe energy of the pairing interaction is quite weak, of the order of 10 eV, and thermal energy can easily break the pairs. So only at low temperatures, in metal and other substrates, are a significant number of the electrons in Cooper pairs.\n\nThe electrons in a pair are not necessarily close together; because the interaction is long range, paired electrons may still be many hundreds of nanometers apart. This distance is usually greater than the average interelectron distance so that many Cooper pairs can occupy the same space. Electrons have spin-, so they are fermions, but the total spin of a Cooper pair is integer (0 or 1) so it is a composite boson. This means the wave functions are symmetric under particle interchange. Therefore, unlike electrons, multiple Cooper pairs are allowed to be in the same quantum state, which is responsible for the phenomena of superconductivity.\n\nThe BCS theory is also applicable to other fermion systems, such as helium-3. Indeed, Cooper pairing is responsible for the superfluidity of helium-3 at low temperatures. It has also been recently demonstrated that a Cooper pair can comprise two bosons. Here, the pairing is supported by entanglement in an optical lattice.\n\nThe tendency for all the Cooper pairs in a body to \"condense\" into the same ground quantum state is responsible for the peculiar properties of superconductivity.\n\nCooper originally considered only the case of an isolated pair's formation in a metal. When one considers the more realistic state of many electronic pair formations, as is elucidated in the full BCS theory, one finds that the pairing opens a gap in the continuous spectrum of allowed energy states of the electrons, meaning that all excitations of the system must possess some minimum amount of energy. This \"gap to excitations\" leads to superconductivity, since small excitations such as scattering of electrons are forbidden.\nThe gap appears due to many-body effects between electrons feeling the attraction.\n\nR. A. Ogg, Jr., was first to suggest that electrons might act as pairs coupled by lattice vibrations in the material.\n\nThe theory of Cooper pairs is quite general and does not depend on the specific electron-phonon interaction. Condensed matter theorists have proposed pairing mechanisms based on other attractive interactions such as electron–exciton interactions or electron–plasmon interactions. Currently, none of these other pairing interactions has been observed in any material.\n\nIt should be mentioned that Cooper pairing does not involve individual electrons pairing up to form \"quasi-bosons\". The paired states are energetically favored, and electrons go in and out of those states preferentially. This is a fine distinction that John Bardeen makes:\n\nThe mathematical description of the second-order coherence involved here is given by Yang.\n\n"}
{"id": "57793246", "url": "https://en.wikipedia.org/wiki?curid=57793246", "title": "Data center security", "text": "Data center security\n\nData center security is the set of policies, precautions and practices adopted to avoid unauthorized access and manipulation of a data center's resources. The data center houses the enterprise applications and data, hence why providing a proper security system is critical. Denial of service (DoS), theft of confidential information, data alteration, and data loss are some of the common security problems afflicting data center environments.\n\nThe number of reported security attacks, including those affecting Data Centers continues to grow year by year. Attacks and attack tools are becoming more and more sophisticated. On one hand, the expansion of the Internet and the growing complexity of protocols and applications used in Data Centers result in an increasing number of exploitable vulnerabilities. On the other hand, hackers use the openness of the Internet to communicate and develop automated tools that facilitate the identification and exploitation of those vulnerabilities. Attacks are usually also initiated by internal trusted personnel. In fact, studies show that internal attacks tend to be more damaging because of the variety and amount of information available inside organizations.\n\nThe cost of a breach of security can have some devastating consequence both on the company managing the data center and on the customers whose data were a breach. For example, the 2012 breach at Global Payments, a processing vendor for Visa, where 1.5 millions of credit card numbers were stolen, highlights the risks of storing and managing valuable and confidential data. As a result, Global Payments partnership with Visa was terminated and, it was estimated that they lost over $100 million. The consequences of permitting such data breaches can be damaging to an organization of any size. Besides a huge financial loss, the company is affected by data breaches may lose the confidence of their business partners and their customers who provided their data to the breached company.\n\nAccording to the Cost of a Data Breach Survey, in which 49 U.S. companies in 14 different industry sectors participated, they noticed that:\n\nWe realized that negligent and malicious are the main causes of data breach, which is why companies are supposed to take steps to minimize the risks of an attack. Companies belonging to a certain industry such as healthcare, technology, finances, are taking care of large amounts of extremely sensitive data, such as medical records, credit card numbers, social security numbers. Those type companies must be extremely vigilant in order to protect their clients’ data.\n\nAs time passes, people always find inventive ways to bypass the security measures put in place by data centers. Every business related to data centers will have to overcome those challenges and prioritize data center security. The amount of data passed in those data center will only grow year after year via cloud-computing and phones, ergo those responsible to secure that valuable information much assess their risk which includes physical breaches.\n\nThe following are some of the most common threats to Data Centers:\n\n\nCommon vulnerabilities in data centers are related to the following areas:\n\n\nMost attacks on data centers exploit well-known vulnerabilities that are usually discovered an announced months before the first attack takes place. The worms CodeRed, Nimda, and SQL Slammer are examples of exploiting known vulnerabilities that could have been avoided. This clearly indicates that a significant number of exploits on the Internet by using the latest releases of software.\n\nMany systems are shipped with default accounts and passwords, which are exploited for unauthorized access and theft of information, among other threats.\n\nCommon attacks include:\n\nA probe is an attack which is deliberately crafted so that its target detects and reports it with a recognizable “fingerprint” in the report. The attacker then uses the collaborative infrastructure to learn the detector’s location and defensive capabilities from this report. this is more like a reconnaissance activity because it precedes an attack and it's goal is to gain access by discovering information about a system or network.\n\nA denial-of-service attack occurs when legitimate users are unable to access information systems, devices, or other network resources due to the actions of a malicious cyber threat actor. This type of attack can take different forms like for example generating large volume of data to deliberately consume limited resources as bandwidth, CPU cycles, and memory blocks.\n\nThis kind of attack is a particular case of DoS where a large number of systems are compromised and used as source or traffic on a synchronized attack. In this kind of attack, the hacker does not use only one IP address but thousand of them. \n\n\nWhen someone other than an account owner uses privileges associated to a compromised account to access to restricted resources using a valid account or a backdoor.\n\n\nEtymologically, \"Eavesdropping\" means Secretly listen to a conversation. In the wetworking filed, it is an unauthorized interception of information (usernames, passwords) that travels on the network. The most common cases of eavesdropping in Data Centers include intercepting typical and critical user transactions such as logon sessions.\n\n\nThey are both malicious code that produces once executed undesired results on the infected system. The malicious code usually remains in the system until the damage is discovered. The difference between viruses and worms is that, worms are self-replicating malwares that propagate without human intervention while viruses are also self-replicating but they need some kind of human action to infect the system.\n\nThis kind of attack targets the critical components of the Internet infrastructure rather than individual systems or networks.\n\nThese attacks exploit the trust relationships that computer systems have to communicate.\n\nConsists of stealing a legitimate session established between a target and a trusted host. The attacker intercepts the session and makes the target believe it is communicating with the trusted host.\n\nThis kind of attack occurs when a program allocates memory buffer space beyond what it had reserved; it results in memory corruption affecting the data stored in the memory areas that were overflowed.\n\nThis type of attack exploit the vulnerabilities of data link layer protocols and their implementations on layer 2 switching platforms.\n\nThe network security infrastructure includes the security tools used in Data Center to enforce security policies. The tools include packet-filtering technologies such as ACLs, firewalls and intrusion detection systems (IDSs) both network-based and host-based.\n\nACLs are filtering mechanisms explicitly defined based on packet header information to permit or deny traffic on specific interfaces. Generally, an ACL is set up as a list that is applied sequentially on the packets until a match is found. Once the match is found, the associated permit or deny operation is applied. ACLs are used in multiple locations within the Data Center such as the Internet Edge and the intranet server farm. The following describes standard and extended access lists:\n\nStandard ACLs: the simplest type of ACL filtering traffic solely based on source IP addresses. Standard ACLs are typically deployed to control access to network devices for network management or remote access. For example, you can configure a standard ACL in a router to specify which systems are allowed to Telnet to it. Standard ACLs aren't recommended option for traffic filtering due to their lack of granularity. Standard ACLSs are configured with a number between 1 and 99 in Cisco routers.\n\nExtended ACLs: \nExtended ACL filtering decisions can be based on source and destination IP addresses, Lyaer 4 protocol, Layer 4 ports, ICMP message type and code, type of service, and precedence. In Cisco routers, you can define extended ACLs by name or by a number in the 100 to 199 range.\n\nA firewall is a sophisticated filtering device that separates LAN segments, giving each segment a different security level and establishing a security perimeter that controls the traffic flow between segments.Firewalls are most commonly deployed at the Internet Edge where they act as boundary to the internal networks.They are expected to have the following characteristics: Performance: the main goal of a firewall is to separate the secured and the unsecured areas of a network. Firewalls are then post in the primary traffic path potentially exposed to large volumes of data. Hence, performance becomes a natural design factor to ensure that the firewall meets the particular requirements.\n\nApplication support: Another important aspect is the ability of a firewall to control and protect a particular application or protocol, such as Telnet, FTP, and HTTP. The firewall is expected to understand application-level packet exchanges to determine whether packets do follow the application behavior and, if they do not, do deny the traffic.\n\nThere are different types of firewalls based on their packet-processing capabilities and their awareness of application-level information:\n\nIDSs are real-time systems that can detect intruders and suspicious activities and report them to a monitoring system. They are configured to block or mitigate intrusions in progress and eventually immunize the systems from future attacks. They have two fundamental components:\n\n\nCisco Layer 2 switches provide tools to prevent the common Layer 2 attacks (Scanning or Probing, DoS, DDoS, etc.). The following are some security features covered by the Layer 2 Security:\n\n\nThe process of securing a Data Center requires both a comprehensive system-analysis approach and an ongoing process that improves the security levels as the Data Center evolves. The Data Center is constantly evolving as new applications or services become available. Attacks are becoming more sophisticated and more frequent. These trends require a steady evaluation of security readiness.\n\nA key component of the security-readiness evaluation is the policies that govern the application of security in the network including the Data Center. The application includes both the design best practices and the implementation details. As a result, security is often considered as a key component of the main infrastructure requirement. Since a key responsibility of the data centers is to make sure of the availability of the services, data center management systems often consider how its security affects traffic flows, failures, and scalability. Due to the fact that security measures may vary depending on the data center design, the use of unique features, compliance requirements or the company's business goals, there isn't a set of specific measures that cover all possible scenarios.\n\nThere exist in general two types of data center security: the Physical Security and the Virtual Security.\n\nThe physical security of a data center is the set of protocol built-in within the data center facilities in order to prevent any physical damage to the machines storing the data. Those protocols should be able to handle everything ranging from natural disasters to corporate espionage to terrorist attacks.\n\nThe result of a physical attack on a data center could possibly cause data racks being damaged, destroyed or stolen, which means that the data it contains might have been lost or inaccessible. Planning in advanced and putting the right and modern security measures in place can drastically reduce the risks of a physical attack on a data center. For instance, to reduce the risks of leaks caused by workers, it is a good practice to make sure that the employees' backgrounds are well checked, and that nontechnical personnel is not allowed to work with data servers and repositories unless they receive adequate training. Additionally, the security team in place should effectively know what to do using a disaster or a treat to the security of the data center.\n\nTo prevent physical attacks, data centers use techniques such as:\n\n\nVirtual security is security measures put in place by the data centers to prevent remote unauthorized access that will affect the integrity, availability or confidentiality of data stored on servers.\n\nVirtual or network security is a hard task to handle as there exist many ways it could be attacked. The worst part of it is that it is evolving years after years. For instance, an attacker could decide to use a malware (or similar exploits) in order to bypass the various firewalls to access the data. Old systems may as well put security at risk as they do not contain modern methods of data security.\n\nVirtual attacks can be prevented with techniques such as\n\n"}
{"id": "197673", "url": "https://en.wikipedia.org/wiki?curid=197673", "title": "Delay line memory", "text": "Delay line memory\n\nDelay line memory is a form of computer memory, now obsolete, that was used on some of the earliest digital computers. Like many modern forms of electronic computer memory, delay line memory was a refreshable memory, but as opposed to modern random-access memory, delay line memory was sequential-access.\n\nAnalog delay line technology had been used since the 1920s to delay the propagation of analog signals. When a delay line is used as a memory device, an amplifier and a pulse shaper are connected between the output of the delay line and the input. These devices recirculate the signals from the output back into the input, creating a loop that maintains the signal as long as power is applied. The shaper ensures the pulses remain well-formed, removing any degradation due to losses in the medium.\n\nThe memory capacity is determined by dividing the time taken to transmit one bit into the time it takes for data to circulate through the delay line. Early delay-line memory systems had capacities of a few thousand bits, with recirculation times measured in microseconds. To read or write a particular bit stored in such a memory, it is necessary to wait for that bit to circulate through the delay line into the electronics. The delay to read or write any particular bit is no longer than the recirculation time.\n\nUse of a delay line for a computer memory was invented by J. Presper Eckert in the mid-1940s for use in computers such as the EDVAC and the UNIVAC I. Eckert and John Mauchly applied for a patent for a delay line memory system on October 31, 1947; the patent was issued in 1953. This patent focused on mercury delay lines, but it also discussed delay lines made of strings of inductors and capacitors, magnetostrictive delay lines, and delay lines built using rotating disks to transfer data to a read head at one point on the circumference from a write head elsewhere around the circumference.\n\nThe basic concept of the delay line originated with World War II radar research, as a system to reduce clutter from reflections from the ground and other \"fixed\" objects.\n\nA radar system consists principally of an antenna, a transmitter, a receiver, and a display. The antenna is connected to the transmitter, which sends out a brief pulse of radio energy before being disconnected again. The antenna is then connected to the receiver, which amplifies any reflected signals, and sends them to the display. Objects farther from the radar return echos later than those closer to the radar, which the display indicates visually.\n\nNon-moving objects at a fixed distance from the antenna always return a signal after the same delay. This would appear as a fixed spot on the display, making detection of other targets in the area more difficult. Early radars simply aimed their beams away from the ground to avoid the majority of this \"clutter\". This was not an ideal situation; it required careful aiming which was difficult for smaller mobile radars, and didn't remove other sources of clutter-like reflections from features like prominent hills, and in the worst case would allow low-flying enemy aircraft to literally fly \"under the radar\".\n\nTo filter out static objects, two pulses were compared and those with the same delay times were removed. To do this the signal sent from the receiver to the display was split in two, with one path leading directly to the display and the second leading to a delay unit. The delay was carefully tuned to be some multiple of the time between pulses (the pulse repetition frequency). This resulted in the delayed signal from an earlier pulse exiting the delay unit the same time that the signal from a newer pulse was received from the antenna. One of the signals was then inverted, typically the one from the delay, and the two signals were then combined and sent to the display. Any signal that was at the same location was nullified (via destructive interference) by the inverted signal from a previous pulse, leaving only the moving objects on the display.\n\nSeveral different types of delay systems were invented for this purpose, with one common principle being that the information was stored acoustically in a medium. MIT experimented with a number of systems including glass, quartz, steel and lead. The Japanese deployed a system consisting of a quartz element with a powdered glass coating that reduced surface waves that interfered with proper reception. The United States Naval Research Laboratory used steel rods wrapped into a helix, but this was useful only for low frequencies under 1 MHz. Raytheon used a magnesium alloy originally developed for making bells.\n\nThe first practical de-cluttering system based on the concept was developed by J. Presper Eckert at the University of Pennsylvania's Moore School of Electrical Engineering. His solution used a column of mercury with piezo crystal transducers (a combination of speaker and microphone) at either end. Signals from the radar amplifier were sent to the piezo at one end of the tube, which would cause the transducer to pulse and generate a small wave in the mercury. The wave would quickly travel to the far end of the tube, where it would be read back out by the other piezo, inverted, and sent to the display. Careful mechanical arrangement was needed to ensure that the delay time matched the inter-pulse timing of the radar being used.\n\nAll of these systems were suitable for conversion into a computer memory. The key was to recycle the signals within the memory system so they would not disappear after traveling through the delay. This was relatively easy to arrange with simple electronics.\n\nAfter the war, Eckert turned his attention to computer development, which was a topic of some interest at the time. One problem with practical development was the lack of a suitable memory device, and Eckert's work on the radar delays gave him a major advantage over other researchers in this regard.\n\nFor a computer application the timing was still critical, but for a different reason. Conventional computers have a natural \"cycle time\" needed to complete an operation, the start and end of which typically consist of reading or writing memory. Thus the delay lines had to be timed such that the pulses would arrive at the receiver just as the computer was ready to read it. Typically many pulses would be \"in flight\" through the delay, and the computer would count the pulses by comparing to a master clock to find the particular bit it was looking for.\nMercury was used because its acoustic impedance is close to that of the piezoelectric quartz crystals; this minimized the energy loss and the echoes when the signal was transmitted from crystal to medium and back again. The high speed of sound in mercury (1450 m/s) meant that the time needed to wait for a pulse to arrive at the receiving end was less than it would have been with a slower medium, such as air (343.2 m/s), but it also meant that the total number of pulses that could be stored in any reasonably sized column of mercury was limited. Other technical drawbacks of mercury included its weight, its cost, and its toxicity. Moreover, to get the acoustic impedances to match as closely as possible, the mercury had to be kept at a constant temperature. The system heated the mercury to a uniform above-room temperature setting of 40 °C (104 °F), which made servicing the tubes hot and uncomfortable work. (Alan Turing proposed the use of gin as an ultrasonic delay medium, claiming that it had the necessary acoustic properties.)\n\nA considerable amount of engineering was needed to maintain a \"clean\" signal inside the tube. Large transducers were used to generate a very tight \"beam\" of sound that would not touch the walls of the tube, and care had to be taken to eliminate reflections off the far end of the tubes. The tightness of the beam then required considerable tuning to make sure the two piezos were pointed directly at each other. Since the speed of sound changes with temperature the tubes were heated in large ovens to keep them at a precise temperature. Other systems instead adjusted the computer clock rate according to the ambient temperature to achieve the same effect.\n\nEDSAC, the second full scale stored-program digital computer, began operation with 256 35-bit words of memory, stored in 16 delay lines holding 576 bits each (Words in the delay line were 36 bits, but one was unusable in programs because of timing constraints). The memory was later expanded to 512 words by adding a second set of 16 delay lines. In the UNIVAC I the capacity of an individual delay line was smaller, each column stored 120 bits (although the term \"bit\" was not in popular use at the time), requiring seven large memory units with 18 columns each to make up a 1000-word store. Combined with their support circuitry and amplifiers, the memory subsystem formed its own walk-in room. The average access time was about 222 microseconds, which was considerably faster than the mechanical systems used on earlier computers.\n\nCSIRAC, completed in November 1949, also used delay line memory.\n\nA later version of the delay line used metal wires as the storage medium. Transducers were built by applying the magnetostrictive effect; small pieces of a magnetostrictive material, typically nickel, were attached to either side of the end of the wire, inside an electromagnet. When bits from the computer entered the magnets the nickel would contract or expand (based on the polarity) and twist the end of the wire. The resulting torsional wave would then move down the wire just as the sound wave did down the mercury column. In most cases the entire wire was made of the same material.\n\nUnlike the compressive wave, however, the torsional waves are considerably more resistant to problems caused by mechanical imperfections, so much so that the wires could be wound into a loose coil and pinned to a board. Due to their ability to be coiled, the wire-based systems could be built as \"long\" as needed, and tended to hold considerably more data per unit; 1k units were typical on a board only 1 foot square. Of course this also meant that the time needed to find a particular bit was somewhat longer as it traveled through the wire, and access times on the order of 500 microseconds were typical.\n\nDelay line memory was far less expensive and far more reliable per bit than flip-flops made from tubes, and yet far faster than a latching relay. It was used right into the late 1960s, notably on commercial machines like the LEO I, Highgate Wood Telephone Exchange, various Ferranti machines, and the IBM 2848 Display Control. Delay line memory was also used for video memory in early terminals, where one delay line would typically store 4 lines of characters. (4 lines x 40 characters per line x 6 bits per character= 960 bits in one delay line) They were also used very successfully in several models of early desktop electronic calculator, including the Friden EC-130 (1964) and EC-132, the Olivetti Programma 101 desktop programmable calculator introduced in 1965, and the Litton Monroe Epic 2000 and 3000 programmable calculators of 1967.\n\nA similar solution to the magnetostrictive system was to use delay lines made entirely of a piezo material, typically quartz. Current fed into one end of the crystal would generate a compressive wave that would flow to the other end, where it could be read. In effect, piezoelectric delays simply replaced the mercury and transducers of a conventional mercury delay line with a single unit combining both. However these solutions were fairly rare; growing crystals of the required quality in large sizes was not easy, which limited them to small sizes and thus small amounts of data storage.\n\nA better and more widespread use of piezoelectric delays was in European television sets. The European PAL standard for color broadcasts compares the signal from two successive lines of the image in order to avoid color shifting due to small phase shifts. By comparing two lines, one of which is inverted, the shifting is averaged and the resulting signal more closely matches the original signal, even in the presence of interference. In order to compare the two lines, a piezoelectric delay unit which delays the signal by a time that is equal to the duration of each line, 64 µs, is inserted in one of the two signal paths that are compared. In order to produce the required delay in a crystal of convenient size, the delay unit is shaped to reflect the signal multiple times through the crystal, thereby greatly reducing the required size of the crystal and thus producing a small, cube-shaped device.\n\nElectric delay lines are used for shorter delay times (ns to several µs). They consist of a long electric line or are made of discrete inductors and capacitors, which are arranged in a chain. To shorten the total length of the line it can be wound around a metal tube, getting some more capacitance against ground and also more inductance due to the wire windings, which are lying close together.\n\nOther examples are:\n\nAnother way to create a delay time is to implement a delay line in an integrated circuit storage device. This can be done digitally or with a discrete analogue method. The analogue one uses bucket-brigade devices or charge coupled devices (CCD), which transport a stored electric charge stepwise from one end to the other. Both digital and analog methods are bandwidth limited at the upper end to the half of the clock frequency, which determines the steps of transportation.\n\nIn modern computers operating at gigahertz speeds, millimeter differences in the length of conductors in a parallel data bus can cause data-bit skew, which can lead to data corruption or reduced processing performance. This is remedied by making all conductor paths of similar length, delaying the arrival time for what would otherwise be shorter travel distances by using zig-zagging traces.\n\nIncidentally a method to increase delay time slightly is to wrap tape (ideally Kapton) around the windings and then put foil over that, with it connected to ground via resistor to further increase the transmission line effect. This approach also reduces interference to nearby circuits.\n\nAn invention around 2015 mentioned on various forums and later built using more recent components is to use the optical persistence in ZnS:Cu that is quenched by infrared light to store data. This approach uses a large area glowing sign panel mounted on a flat spinning glass or metal sheet, and write/read sensors using green/blue LEDs and infrared diodes for the erase cycle.\nThis approach also allows a comparatively long delay time if cascaded to multiple R/W layers and if signal integrity is verified and replicated can be very reliable considering that it has only one moving part ie the spinning disk albeit with some variation in the time domain due to speedup effects similar to that seen with optical disk drives.\nAlso useful is to position read/write diodes on the back of a sheet of Veroboard to take advantage of small areas with glass beads as lenses and instrumentation amplifiers using quad op-amps for incidental noise rejection.\nThis could have been built as early as 1959 but maybe overlooked due to lack of reliable diodes in the target wavelength.\nIt is possible that it might have been experimented with in the former Soviet Union but this is as yet unconfirmed.\n\n"}
{"id": "2164976", "url": "https://en.wikipedia.org/wiki?curid=2164976", "title": "Dip-pen nanolithography", "text": "Dip-pen nanolithography\n\nDip pen nanolithography (DPN) is a scanning probe lithography technique where an atomic force microscope (AFM) tip is used to create patterns directly on a range of substances with a variety of inks. A common example of this technique is exemplified by the use of alkane thiolates to imprint onto a gold surface. This technique allows surface patterning on scales of under 100 nanometers. DPN is the nanotechnology analog of the dip pen (also called the quill pen), where the tip of an atomic force microscope cantilever acts as a \"pen,\" which is coated with a chemical compound or mixture acting as an \"ink,\" and put in contact with a substrate, the \"paper.\"\n\nDPN enables direct deposition of nanoscale materials onto a substrate in a flexible manner. Recent advances have demonstrated massively parallel patterning using two-dimensional arrays of 55,000 tips. Applications of this technology currently range through chemistry, materials science, and the life sciences, and include such work as ultra high density biological nanoarrays, and additive photomask repair.\n\nThe uncontrollable transfer of a molecular 'ink' from a coated AFM tip to a substrate was first reported by Jaschke and Butt in 1995, but they erroneously concluded that alkanethiols could not be transferred to gold substrates to form stable nanostructures. A research group at Northwestern University led by Chad Mirkin independently studied the process and determined that under the appropriate conditions, molecules could be transferred to a wide variety of surfaces to create stable chemically-adsorbed monolayers in a high resolution lithographic process they termed \"DPN\". Mirkin and his coworkers hold the patents on this process, and the patterning technique has expanded to include liquid \"inks\". It is important to note that \"liquid inks\" are governed by a very different deposition mechanism when compared to \"molecular inks\".\n\nMolecular inks are typically composed of small molecules that are coated onto a DPN tip and are delivered to the surface through a water meniscus. In order to coat the tips, one can either vapor coat the tip or dip the tips into a dilute solution containing the molecular ink. If one dip-coats the tips, the solvent must be removed prior to deposition. The deposition rate of a molecular ink is dependent on the diffusion rate of the molecule, which is different for each molecule. The size of the feature is controlled by the tip/surface dwell-time (ranging from milliseconds to seconds) and the size of the water meniscus, which is determined by the humidity conditions (assuming the tip's radius of curvature is much smaller than the meniscus). \n\n\nLiquid inks can be any material that is liquid at deposition conditions. The liquid deposition properties are determined by the interactions between the liquid and the tip, the liquid and the surface, and the viscosity of the liquid itself. These interactions limit the minimum feature size of the liquid ink to about 1 micrometre, depending on the contact angle of the liquid. Higher viscosities offer greater control over feature size and are desirable. Unlike molecular inks, it is possible to perform multiplexed depositions using a carrier liquid. For example, using a viscous buffer, it is possible to directly deposit multiple proteins simultaneously.\n\n\nIn order to define a good DPN application, it is important to understand what DPN can do that other techniques can't. Direct-write techniques, like contact printing, can pattern multiple biological materials but it cannot create features with subcellular resolution. Many high-resolution lithography methods can pattern at sub-micrometre resolution, but these require high-cost equipment that were not designed for biomolecule deposition and cell culture. Microcontact printing can print biomolecules at ambient conditions, but it cannot pattern multiple materials with nanoscale registry.\n\nThe following are some examples of how DPN is being applied to potential products. \n\n\nDPN is emerging as a powerful research tool for manipulating cells at subcellular resolution\n\n\nDPN is a direct write technique so it can be used for top-down and bottom-up lithography applications. In top-down work, the tips are used to deliver an etch resist to a surface, which is followed by a standard etching process. In bottom-up applications, the material of interest is delivered directly to the surface via the tips.\n\nA heated probe tip version of Dip Pen Lithography has also been demonstrated, thermal Dip Pen Lithography (tDPL), to deposit nanoparticles. Semiconductor, magnetic, metallic, or optically active nanoparticles can be written to a substrate via this method. The particles are suspended in a PMMA or equivalent polymer matrix, and heated by the probe tip until they begin to flow. The probe tip acts as a nano-pen, and can pattern nanoparticles into a programmed structure. Depending on the size of the nanoparticles, resolutions of 78-400 nm were attained. An O plasma etch can be used to remove the PMMA matrix, and in the case of Iron Oxide nanoparticles, further reduce the resolution of lines to 10 nm. Advantages unique to tDPL are that it is a maskless additive process that can achieve very narrow resolutions, it can also easily write many types of nanoparticles without requiring special solution preparation techniques. However there are limitations to this method. The nanoparticles must be smaller than the radius of gyration of the polymer, in the case of PMMA this is about 6 nm. Additionally, as nanoparticles increase in size viscosity increases, slowing the process. For a pure polymer deposition speeds of 200 μm/s are achievable. Adding nanoparticles reduces speeds to 2 μm/s, but is still faster than regular Dip Pen Lithography.\n\nA two dimensional array of (PDMS) deformable transparent pyramid shaped tips are coated with an opaque layer of metal. The metal is then removed from the very tip of the pyramid, leaving an aperture for light to pass through. The array is then scanned across a surface and light is directed to the base of each pyramid via a micromirror array, which funnels the light toward the tip. Depending on the distance between the tips and the surface, light interacts with the surface in a near-field or far-field fashion, allowing sub-diffraction scale features (100 nm features with 400 nm light) or larger features to be fabricated.\n\nThe criticism most often directed at DPN is the patterning speed. The reason for this has more to do with how it is compared to other techniques rather than any inherent weaknesses. For example, the soft lithography method, microcontact printing (μCP), is the current standard for low cost, bench-top micro and nanoscale patterning, so it is easy to understand why DPN is compared directly to microcontact printing. The problem is that the comparisons are usually based upon applications that are strongly suited to μCP, instead of comparing them to some neutral application. μCP has the ability to pattern one material over a large area in a single stamping step, just as photolithography can pattern over a large area in a single exposure. Of course DPN is slow when it is compared to the strength of another technique. DPN is a maskless direct write technique that can be used to create multiple patterns of varying size, shape, and feature resolution, all on a single substrate. No one would try to apply microcontact printing to such a project because then it would never be worth the time and money required to fabricate each master stamp for each new pattern. Even if they did, microcontact printing would not be capable of aligning multiple materials from multiple stamps with nanoscale registry. The best way to understand this misconception is to think about the different ways to apply photolithography and e-beam lithography. No one would try to use e-beam to solve a photolithography problem and then claim e-beam to be \"too slow\". Directly compared to photolithography's large area patterning capabilities, e-beam lithography is slow and yet, e-beam instruments can be found in every lab and nanofab in the world. The reason for this is because e-beam has unique capabilities that cannot be matched by photolithography, just as DPN has unique capabilities that cannot be matched by microcontact printing.\n\nDPN evolved directly from AFM so it is not a surprise that people often assume that any commercial AFM can perform DPN experiments. In fact, DPN does not require an AFM, and an AFM does not necessarily have real DPN capabilities. There is an excellent analogy with scanning electron microscopy (SEM) and electron beam (E-beam) lithography. E-beam evolved directly from SEM technology and both use a focused electron beam, but no one would ever suggest that one could perform modern E-beam lithography experiments on a SEM that lacks the proper lithography hardware and software requirements.\n\nIt is also important to consider one of the unique characteristics of DPN, namely its force independence. With virtually all ink/substrate combinations, the same feature size will be patterned no matter how hard the tip is pressing down against the surface. As long as robust SiN tips are used, there is no need for complicated feedback electronics, no need for lasers, no need for quad photo-diodes, and no need for an AFM.\n"}
{"id": "3830353", "url": "https://en.wikipedia.org/wiki?curid=3830353", "title": "Dot-matrix display", "text": "Dot-matrix display\n\nA dot-matrix display is a display device used to display information on machines, clocks, railway departure indicators and many other devices requiring a simple display device of limited resolution.\n\nThe display consists of a dot matrix of lights or mechanical indicators arranged in a rectangular configuration (other shapes are also possible, although not common) such that by switching on or off selected lights, text or graphics can be displayed. A dot matrix controller converts instructions from a processor into signals which turns on or off lights in the matrix so that the required display is produced.\n\nCommon sizes of dot matrix displays:\n\nOther sizes include:\n\n\n"}
{"id": "2973891", "url": "https://en.wikipedia.org/wiki?curid=2973891", "title": "Duraspark", "text": "Duraspark\n\nThe Duraspark II is a Ford electronic ignition system.\n\nFord Motor Company began using electronic ignitions in 1973 with the Duraspark electronic ignition system and introduced the Duraspark II system in 1976. The biggest change, apart from the control box redesign, was the large distributor cap to handle the increased spark energy.\n\nFord used several models over the years. They were coded by the color of the plastic wire strain relief, or \"grommet\" as it is most often called, in order to make them easy to identify. In addition to the color-coding, the modules may have a keyway molded into the electrical connectors to prevent accidental use in the wrong vehicle.\n\nThe system consists of a magnetic reluctor and pickup in the distributor, with a separate fender mounted ignition module to trigger the coil. Typically, Duraspark II distributors have both mechanical and vacuum advance mechanisms.\n\nCertain 1981–83 models used the EEC-III system, which uses a Dura-Spark III module (brown grommet where wires emerge) and a Dura-Spark II ignition coil. A resistance wire is used in the primary circuit. The distributors in EEC-III (and later) systems eliminate conventional mechanical and vacuum advance mechanisms. All timing is controlled by the engine computer, which is capable of firing the spark plug at any point within a 50-degree range depending on calibration. This increased spark capability requires greater separation of adjacent distributor cap electrodes to prevent cross-fire, another reason for its large-diameter distributor cap. This system is very similar to the systems used at MSD; MSD used the Duraspark during R&D.\n\nThe Duraspark II ignition system is a common upgrade for older Ford cars equipped with a points-type ignition. In most cases, the distributor will interchange with the older-style points distributor. The system is similar to some aftermarket systems and the control module may be easily swapped. Duraspark swaps are easy and can be run by a Duraspark box or and aftermarket box. MSD makes a harness that adapts the Duraspark mag pick up right to an aftermarket box such as the 6AL2. Re-curving a Duraspark is a way to build additional power and economy. Examples of these are available on eBay and Summit Racing.\n\nThe Duraspark module was used by AMC starting in 1978 and continued to be used with AMC's computerized engine control. The Motorcraft Duraspark system replaced the older Prestolite system in 1978.\n\nAMC used the \"blue grommet\" module from 1978 and it continued on the carbureted AMC engines through the Chrysler buyout until 1991 for V8 engines and 1990 for inline 6 engines. In 1982 AMC briefly used the \"yellow double grommet\" module with three connectors in some passenger cars and Jeeps.\n"}
{"id": "7742604", "url": "https://en.wikipedia.org/wiki?curid=7742604", "title": "E-patient", "text": "E-patient\n\nAn e-patient is a health consumer who participates fully in his/her medical care, primarily by gathering information about medical conditions that impact them and their families, using the Internet and other digital tools. The term encompasses those who seek guidance for their own ailments and the friends and family members who go online on their behalf. E-patients report two effects of their health research: \"better health information and services, and different, but not always better, relationships with their doctors.\"\n\nE-patients are active in their care and demonstrate the power of the Participatory Medicine or Health 2.0 / Medicine 2.0. model of care. The \"e\" can stand for \"electronic\" but has also been used to refer to other terms, such as \"equipped\", \"enabled\", \"empowered\" and \"expert\".\n\nThe current state of knowledge on the impact of e-patients on the healthcare system and the quality of care received indicates:\nA 2011 study of European e-patients found that they tended to be \"inquisitive and autonomous\" and that they noted that the number of e-patients in Europe appeared to be rising. A 2012 study found that e-patients uploading videos about their health experienced a loss of privacy, but also positive benefits from social support. Furthermore, a 2017 study utilizing social network analysis found that when e-patients are included in health care conferences, they increase information flow, expand propagation, and deepen engagement in the conversation of Tweets when compared to both physicians and researchers while only making up 1.4% of the stakeholder mix.\n\n\n"}
{"id": "151590", "url": "https://en.wikipedia.org/wiki?curid=151590", "title": "Faraday cage", "text": "Faraday cage\n\nA Faraday cage or Faraday shield is an enclosure used to block electromagnetic fields. A Faraday shield may be formed by a continuous covering of conductive material or in the case of a Faraday cage, by a mesh of such materials. Faraday cages are named after the English scientist Michael Faraday, who invented them in 1836.\n\nA Faraday cage operates because an external electrical field causes the electric charges within the cage's conducting material to be distributed such that they cancel the field's effect in the cage's interior. This phenomenon is used to protect sensitive electronic equipment from external radio frequency interference (RFI). Faraday cages are also used to enclose devices that produce RFI, such as radio transmitters, to prevent their radio waves from interfering with other nearby equipment. They are also used to protect people and equipment against actual electric currents such as lightning strikes and electrostatic discharges, since the enclosing cage conducts current around the outside of the enclosed space and none passes through the interior.\n\nFaraday cages cannot block stable or slowly varying magnetic fields, such as the Earth's magnetic field (a compass will still work inside). To a large degree, though, they shield the interior from external electromagnetic radiation if the conductor is thick enough and any holes are significantly smaller than the wavelength of the radiation. For example, certain computer forensic test procedures of electronic systems that require an environment free of electromagnetic interference can be carried out within a screened room. These rooms are spaces that are completely enclosed by one or more layers of a fine metal mesh or perforated sheet metal. The metal layers are grounded to dissipate any electric currents generated from external or internal electromagnetic fields, and thus they block a large amount of the electromagnetic interference. See also electromagnetic shielding. They provide less attenuation from outgoing transmissions versus incoming: they can shield EMP waves from natural phenomena very effectively, but a tracking device, especially in upper frequencies, may be able to penetrate from within the cage (e.g., some cell phones operate at various radio frequencies so while one cell phone may not work, another one will).\n\nA common misconception is that a Faraday cage provides full blockage or attenuation; this is not true. The reception or transmission of radio waves, a form of electromagnetic radiation, to or from an antenna within a Faraday cage is heavily attenuated or blocked by the cage; however, a Faraday cage has varied attenuation depending on wave form, frequency or distance from receiver/transmitter, and receiver/transmitter power. Near-field high-powered frequency transmissions like HF RFID are more likely to penetrate. Solid cages generally attenuate fields over a broader range of frequencies than mesh cages.\n\nIn 1836, Michael Faraday observed that the excess charge on a charged conductor resided only on its exterior and had no influence on anything enclosed within it. To demonstrate this fact, he built a room coated with metal foil and allowed high-voltage discharges from an electrostatic generator to strike the outside of the room. He used an electroscope to show that there was no electric charge present on the inside of the room's walls.\n\nAlthough this cage effect has been attributed to Michael Faraday's famous ice pail experiments performed in 1843, it was Benjamin Franklin in 1755 who observed the effect by lowering an uncharged cork ball suspended on a silk thread through an opening in an electrically charged metal can. In his words, \"the cork was not attracted to the inside of the can as it would have been to the outside, and though it touched the bottom, yet when drawn out it was not found to be electrified (charged) by that touch, as it would have been by touching the outside. The fact is singular.\" Franklin had discovered the behavior of what we now refer to as a Faraday cage or shield (based on Faraday's later experiments which duplicated Franklin's cork and can).\n\nA continuous Faraday shield is a hollow conductor. Externally or internally applied electromagnetic fields produce forces on the charge carriers (usually electrons) within the conductor; the charges are redistributed accordingly due to electrostatic induction. The redistributed charges greatly reduce the voltage within the surface, to an extent depending on the capacitance, however, full cancellation does not occur.\n\nIf a charge is placed inside an ungrounded Faraday cage, the internal face of the cage becomes charged (in the same manner described for an external charge) to prevent the existence of a field inside the body of the cage, however, this charging of the inner face re-distributes the charges in the body of the cage. This charges the outer face of the cage with a charge equal in sign and magnitude to the one placed inside the cage. Since the internal charge and the inner face cancel each other out, the spread of charges on the outer face is not affected by the position of the internal charge inside the cage. So for all intents and purposes, the cage generates the same DC electric field that it would generate if it were simply affected by the charge placed inside. The same is not true for electromagnetic waves.\n\nIf the cage is grounded, the excess charges will go to the ground instead of the outer face, so the inner face and the inner charge will cancel each other out and the rest of the cage will retain a neutral charge.\n\nEffectiveness of shielding of a static electric field is largely independent of the geometry of the conductive material, however, static magnetic fields can penetrate the shield completely.\n\nIn the case of a varying electromagnetic fields, the faster the variations are (i.e., the higher the frequencies), the better the material resists magnetic field penetration. In this case the shielding also depends on the electrical conductivity, the magnetic properties of the conductive materials used in the cages, as well as their thicknesses.\n\nA good idea of the effectiveness of a Faraday shield can be obtained from considerations of skin depth. With skin depth, the current flowing is mostly in the surface, and decays exponentially with depth through the material. Because a Faraday shield has finite thickness, this determines how well the shield works; a thicker shield can attenuate electromagnetic fields better, and to a lower frequency.\n\nFaraday cages are Faraday shields which have holes in them and are therefore more complex to analyze. Whereas continuous shields essentially attenuate all wavelengths shorter than the skin depth, the holes in a cage may permit shorter wavelengths to pass through or set up \"evanescent fields\" (oscillating fields that do not propagate as EM waves) just beneath the surface. The shorter the wavelength, the better it passes through a mesh of given size. Thus to work well at short wavelengths (i.e., high frequencies), the holes in the cage must be smaller than the wavelength of the incident wave. Faraday cages may therefore be thought of as high pass filters.\n\n\n\n"}
{"id": "16518741", "url": "https://en.wikipedia.org/wiki?curid=16518741", "title": "Focke-Wulf Fw 47", "text": "Focke-Wulf Fw 47\n\nThe Focke-Wulf Fw 47 \"Höhengeier\" (German: \"Vulture\"), known internally to Focke-Wulf as the A 47, was a meteorological aircraft developed in Germany in 1931. It was a parasol-wing monoplane of largely conventional design, unusual only in the expansiveness of its wing area. Tested first by the \"Reichsverband der Deutschen Luftfahrtindustrie\", and then the weather station at Hamburg, the type was ordered into production to equip ten major weather stations around Germany.\n\n"}
{"id": "53931502", "url": "https://en.wikipedia.org/wiki?curid=53931502", "title": "Global Offset Table", "text": "Global Offset Table\n\nA Global Offset Table, or GOT, is a table of addresses stored in the data section.\nIt is used by executed programs to find during runtime addresses of global variables, unknown in compile time.\nThe global offset table is updated in process bootstrap by the dynamic linker.\nOffsets to global variables from dynamic libraries are not known during compile time, this is why they are read from the GOT table during runtime.\n\nThe procedure linkage table contains trampoline code for calling shared functions.\n\n\n"}
{"id": "267538", "url": "https://en.wikipedia.org/wiki?curid=267538", "title": "HAL/S", "text": "HAL/S\n\nHAL/S (\"Houston Avionics Language/Shuttle\") is a real-time aerospace programming language compiler and cross-compiler for avionics applications used by NASA and associated agencies (JPL, etc.). It has been used in many U.S. space projects since 1973 and its most significant use was in the Space Shuttle program (approximately 85% of the Shuttle software is coded in HAL/S). It was designed by Intermetrics in 1972 for NASA and delivered in 1973. HAL/S is written in XPL, a dialect of PL/I. Although HAL/S is designed primarily for programming on-board computers, it is general enough to meet nearly all the needs in the production, verification, and support of aerospace and other real-time applications. According to documentation from 2005, it is been maintained by the HAL/S project of United Space Alliance.\n\nThe three key principles in designing the language were reliability, efficiency, and machine-independence. The language is designed to allow aerospace-related tasks (such as vector/matrix arithmetic) to be accomplished in a way that is easily understandable by people who have spaceflight knowledge, but may not necessarily have proficiency with computer programming.\n\nHAL/S was designed not to include some constructs that were thought to be the cause of errors. For instance, there is no support for dynamic memory allocation. The language provides special support for real-time execution environments. \n\nSome features, such as \"GOTO\" were provided chiefly to ease mechanical translations from other languages. (page 82) \n\nOn the Preface page of the HAL/S Language Specification, it says,\n\"HAL\" was suggested as the name of the new language by Ed Copps, a founding director of Intermetrics, to honor Hal Laning, a colleague at MIT.\n\nA proposal for a NASA standard ground-based version of HAL named HAL/G for \"ground\" was proposed, but the coming emergence of the soon to be named Ada programming language contributed to Intermetrics' lack of interest in continuing this work. Instead, Intermetrics would place emphasis on what would be the \"Red\" finalist which would not be selected.\n\nHost compiler systems have been implemented on an IBM 360/370, Data General Eclipse, and the Modcomp IV/Classic computers. Target computer systems have included IBM 360/370, IBM AP-101 (space shuttle avionics computer), Sperry 1819A/1819B, Data General Nova and Eclipse, CII Mitra 125, Modcomp II and IV, NASA Std. Spacecraft Computer-l and Computer-2, ITEK ATAC 16M (Galileo Project), and since 1978 the RCA CDP1802 COSMAC microprocessor (Galileo Project and others).\n\nHAL/S is a mostly free-form language: statements may begin anywhere on a line and may spill over the next lines, and multiple statements may be fitted onto the same line if required. However, non-space characters in the first column of a program line may have special significance. For instance, the letter 'C' in the first column indicates that the whole line is a comment and should be ignored by the compiler.\n\nOne particularly interesting feature of HAL/S is that it supports, in addition to a normal single line text format, an optional three-line input format in which three source code lines are used for each statement. In this format, the first and third lines are usable for superscripts (exponents) and subscripts (indices). The multi-line format was designed to permit writing of HAL/S code that is similar to mathematical notation. \n\nAs an example, the statement formula_1 could be written in single-line format as:\nExponentiation is denoted by two asterisks, as in PL/I and Fortran. The subscript formula_2 is denoted by a dollar sign,with the subscript expression enclosed in parentheses. The same code fragment could be written in multiple-line format as:\nIn the example, the base line of the statement is indicated by an 'M' in the first column, the exponent line is indicated by an 'E', and the subscript line is indicated by an 'S'.\n\nHAL/S has native support for integers, floating point scalars, vector, matrices, booleans and strings of 8-bit characters, limited to a maximum length of 255. Structured types may be composed using a codice_1 statement.\n\n\n"}
{"id": "58522784", "url": "https://en.wikipedia.org/wiki?curid=58522784", "title": "History of computing in South America", "text": "History of computing in South America\n\nIn 1957, the first digital computer arrived in Chile after the CCU purchased a Univac to be delivered to Valparaiso. The machine was one of the first documented cases in the history of computer science in South America. During the 1970's, Project Cybersyn was created as an ambitious project to implement cybernetic socialism under the short-lived administration of Salvador Allende.\n\nThe Free Software Foundation Latin America exists to promote the use of free software in Latin America. In 2009, FSF founder Richard Stallman visited Buenos Aires during the concurrent Wikimania 2009 conference in order to promote free software. Stallman regularly gives speeches in Spanish and has visited Latin America multiples times since 2009.\n\nIn 2011, the government of Venezuela adopted the GNU/Linux-based operating system Canaima as the default operating system for the Venezuelan public administration.The operating system has gained a strong foothold and is one of the most used Linux distributions in Venezuela, largely because of its incorporation in public schools. It is being used in large scale projects as \"Canaima Educativo\", a project aimed at providing school children with a basic laptop computer with educational software nicknamed Magallanes. Use of Canaima has been presented on international congresses about the use of open standards,\n\nIn 2015, Google announced that they would invest 1 million USD in computer science in Latin America.Amazon has major telescopes in Chile.\n"}
{"id": "51968774", "url": "https://en.wikipedia.org/wiki?curid=51968774", "title": "Itslearning", "text": "Itslearning\n\nItslearning is a digital learning management system developed by the Norwegian company itslearning AS and designed for both lower and higher degrees of education: from kindergartens and primary schools to colleges.\n\nItslearning is among the top 15 LMS for higher education worldwide, with about 1% of the market in Europe.\n\nThe system was created by a group of students at Bergen University College in 1998 as a master's project on the topic of \"virtual classrooms\". A group of teachers asked whether the system could be created, and after receiving a sum of start-up money from the College itslearning was created following year in 1999, and Bergen University College became the first user of the system. The company was first established as \"it:solutions\".\n\nIn 2004, Arne Bergby became CEO of itslearning. As of 2015 Bergby owns 9.16% of the holding company in ownership of itslearning. A U.S. office was opened in 2009 in Massachusetts. In 2013, itslearning acquired SkoleIntra, and in 2015 Fronter. In 2014, 40% of the company was sold to the Swedish private equity fund EQT. The company at the time was valued at 600 million NOK ($73 million).\n\nItslearning experimented and abandoned some markets such as Spain and Italy, where it was difficult to generate revenue.\n\nAs of 2016, the company headquarters are in Bergen, Norway, with offices in Atlanta, Berlin, Boston, Copenhagen, Enschede, Helsinki, Milton Keynes, Malmö, Mulhouse, and Paris.\n\nThe system is an arena for communication and cooperation, as well as a tool for administration, evaluation and following up on students/pupils. Itslearning is specially adapted for schools, and enables publishing of subject schedules, and individual or group messages (as to a class). In project assignments, students can create groups where files may be uploaded, or ideas discussed. It's also possibly to create and publish tests and competitions in the system. Assignments may be published with or without a deadline, and hand-ins may be uploaded digitally, with options for grading the hand-in digitally and displaying the assessment to the student. There is also access to ePortfolio, which allows each member to create a presentation of themselves. Such homepage contains a blog, and possibilities to upload pictures, etc.\n\nItslearning claims to have over four million active users worldwide, mainly in the United States, Norway, Sweden, Denmark, the Netherlands, the United Kingdom, Spain, Italy, France, Germany and Brazil.\nOver 900.000 users exist in Norway alone, one of the few countries where Moodle is not first by market share.\n\nIn Norway, the system is used extensively by several schools and educational institutions: itslearning is used by universities such as Norwegian School of Economics, BI Norwegian Business School, Norwegian School of Information Technology and University of Stavanger. Additionally, it is used by primary, middle and high schools. The county municipalities of Akershus, Buskerud, Hordaland, Hedmark, Nordland, Nord-Trøndelag, Rogaland, Sør-Trøndelag, Vest-Agder and Vestfold all use itslearning in high schools.\n\nSeveral US school districts use itslearning as its school platform, including Germantown Academy, Dallas, Clear Creek, Oslo and Stockholm.\n\nThe few publicly available evaluation of itslearning indicate that its users are satisfied with the system: a survey carried out in 2003 showed that a high percentage of users at Bergen University College were pleased with the system. A second survey carried out in conjunction with a master's degree showed that 67% of students at different faculties at Norwegian University of Science and Technology and Sør-Trøndelag University College are satisfied or very satisfied with itslearning.\n\n\n"}
{"id": "52308828", "url": "https://en.wikipedia.org/wiki?curid=52308828", "title": "List of Fitbit products", "text": "List of Fitbit products\n\n\"This page contains a list of products Fitbit has released.\"\n\nAnnounced on September 17, 2012, the Fitbit Zip is about the size of a United States quarter and tracks steps taken, distance traveled and calories burned. It is able to sync its data wirelessly to supported mobile devices.\n\nReleased in 2017. It is waterproof and can track swimming. The tracker can be worn in a wristband or pendant, or carried in a pocket. The LED lights function similar to the original Flex, with the number of illuminated dots indicating progress toward the set goal. It features \"reminder to move\" alerts and vibrations when a call or text is received.\n\nThe Fitbit Alta was released in February 2016. The wristband offers a full OLED screen that can be tapped for reminders, a clock and smartphone notifications. While not a touch screen, it is interacted with by tapping the band, similar to previous models. The Alta is also able to recognize the type of activity in progress: running, football or walking. \n\nThe Fitbit Alta HR was released in March 2017. It has an added heart rate monitor. It includes the new Sleep Stages feature, which intends to show the stages of sleep, rather than just time asleep as in previous versions. \n\nReleased in 2018, the Fitbit Ace is a variant of the Alta for children 8+ years of age.\n\nThe Fitbit Charge 3 was released in October 2018. It has a heart rate sensor as well as an oxygen saturation (SPO2) sensor. Sleep tracking has been improved from the Charge 2.\nIn November 2018, a special edition of the Fitbit Charge 3 was released featuring \"Fitbit Pay\" as a special feature.\n\nThe Fitbit Ionic was released in late September 2017. Designed to compete with the Apple Watch Series 3, it is the successor to both the Blaze and the Surge. Like the Surge, the Ionic uses built-in GPS, using GLONASS to tap into global satellites and provide better accuracy when recording exercises, with the antenna being integrated into the watch case for a stronger connection. The Ionic also features SmartTrack, which auto-recognizes user activity and records it in the Fitbit app. The Ionic has interchangeable bands, including classic Fitbit bands, leather bands, and perforated bands for a more sport-like appearance, and the release mechanism has been modified to make swapping out bands easier. It is also water-resistant, making it safe to wear when swimming. Many of the Blaze's clock faces return, as do several new clock faces. New to the Ionic is the ability to load apps onto the watch itself such as AccuWeather and Starbucks, as well as an NFC chip that allows the Ionic to be used for credit card purchases at places that allow contactless payment. As a result, the tactile buttons on the Ionic have some new functions. When not in workout mode, the right side buttons now function as shortcuts for the leftmost two apps loaded onto the watch, while a long press on the left side button brings up Fitbit Pay as well as music and quick settings. The Ionic is shipped in three color combinations of the wristband and watch case: Charcoal & Smoke Gray, Slate Blue & Burnt Orange, and Blue Gray & Silver Gray.\n\nIn 2018, the Ionic was updated to Fitbit OS 2.0 alongside the release of the Versa. The most notable change from OS 1.0 is the addition of a new app called Fitbit Today, a much more intuitive and informative dashboard displaying the user's health and fitness data. It is accessible by swiping up from the clock face, while the notification tray is now accessed by swiping down from the clock face. In addition, a long press on the back button now opens up the music controls, payments, and the shortcuts screen, instead of just Fitbit Pay. In July 2018, Fitbit announced the 15+ Best Fitbit OS Apps for Travel, that can be downloaded in Ionic and some are also available in Versa.\n\nReleased in April 2018, it has a square design with round edges, similar to the Apple Watch and Pebble watches. It retains most of the Ionic's features and interface. It is capable of tracking women's menstrual cycles. It does not have built-in GPS like the Ionic, instead using connected GPS like the Blaze. \n\nThere are two variants of the Versa; the standard edition and the Special Edition. The standard Versa comes in three colors: Black, Rose Gold, and Silver. The Special Edition comes in two colors: Rose Gold with a Lavender band, and Graphite with a Charcoal band. They also include woven wristbands.\n\nThe Aria 2 was announced in August 2017 concurrently with the Ionic. The Aria 2 has been re-engineered for greater accuracy and easier Bluetooth setup. The Aria 2 also has personalized face icons and greetings, compatibility with more Wi-Fi networks, and has an increased weight tolerance of up to 400 pounds.\n\nSweatproof wireless earphones by Fitbit. Has noise isolation.\n\nThe Fitbit Tracker was a small black and teal device that could be clipped discreetly onto clothing and worn 24/7. It uses a three-dimensional accelerometer to sense user movement. The Tracker measures steps taken and combines it with user data to calculate distance walked, calories burned, floors climbed and activity duration and intensity. It uses an OLED display to display this and other information such as the battery level. It also measures sleep quality by tracking periods of restlessness, how long it takes the wearer to fall asleep and how long they are actually asleep.\n\nA wireless base station is included to receive data from the Tracker and to charge its battery. When connected to a computer, the base station will upload data to the Fitbit website, where a number of features are available: seeing an overview of physical activity, setting and tracking goals, keeping food and activity logs and interacting with friends. Use of the website is free.\n\nThe Fitbit Classic tracked only steps taken, distance traveled, calories burned, activity intensity and sleep.\n\nAt the TechCrunch50 during the \"Mobile\" session on September 9, 2008, Fitbit received positive reactions during its panel from experts like Rafe Needleman, Tim O'Reilly, and Evan Williams who cited its wearability, price, and lack of subscription fees.\n\nThe Fitbit Ultra was announced on October 3, 2011. The new features included:\n\n\nThe Fitbit Ultra is powered by a small lithium polymer battery.\n\nThe Fitbit Ultra suffered from a small design flaw: the unit had a permanently curved shape in order to clip directly onto any piece of clothing. The plastic used in the unit was not appropriate for the strain experienced at the looped end, and with time would become brittle, and crack. While most users experienced only minor cracking with no effects to the device's function, in a few cases the cracking led to total failure. Fitbit offered replacement or repair of affected units that were under warranty.\n\nAnnounced on September 17, 2012, the Fitbit One is an update to the Fitbit Ultra that uses a more vivid digital display, has a separate clip and a separate charging cable and wireless sync dongle. The Fitbit One and the Fitbit Zip were the first wireless activity trackers to sync using Bluetooth 4.0 or Bluetooth SMART technology. The wireless syncing is currently available on iOS and Android devices such as the iPhone 4S and higher, iPad 3rd generation, iPod touch 5th generation, Samsung Galaxy Note II and higher, Samsung Galaxy S III and higher, LG G2, HTC One, Moto X, and Nexus 4 or higher. Fitbit One can record several daily activities, including but not limited to, number of steps taken, distance traveled on foot, number of floors climbed, calories burned, vigorously active minutes, and sleep efficiency.\n\nIn May 2013, Fitbit released the Fitbit Flex, the first Fitbit tracker worn on the wrist. It tracks movement 24 hours a day, including sleep patterns. It has a simple display of 5 LED lights that indicate the progress toward the goal number of steps walked in a day and vibrates to indicate when the goal has been reached. The sync functions are similar to the Fitbit One and Zip. The Flex is the most water-resistant tracker, though cannot be worn while swimming. It includes a specialized USB charger; the battery lasts 5–7 days, and it takes 1–2 hours to charge.\n\nThe Fitbit Force was announced on October 10, 2013. It has an OLED display that shows time and daily activity. The Force tracks a number of statistics in real-time, including steps taken, distance traveled, calories burned, stairs climbed and active minutes throughout the day. At night, the Force tracks sleep and can wake a user silently with a vibrating alarm.\n\nOn January 13, 2014 it was reported that an unconfirmed number of Fitbit customers had complained about skin irritation after wearing the Force for extended periods of time. Fitbit stated on its website that the company consulted with medical professionals whose assessments are that these irritations are most likely allergic reactions to nickel, a component of the surgical-grade steel or the adhesives used to assemble the Fitbit Force. Fitbit, working with the Consumer Protection Safety Commission, recalled the Fitbit Force on February 20, 2014. On March 12, 2014 the Consumer Product Safety Commission (CPSC) made the recall official. At that time it was revealed that The Fitbit Force had caused about 9,900 injuries. It is no longer for sale on Fitbit's website.\n\nAnnounced in October 2014, the Fitbit Charge is intended as a replacement for the recalled Fitbit Force. It was released in November 2014 for US$130 retail. The Charge's wrist band is textured and has a screen that can display caller ID information from a connected smartphone through the Fitbit app. The Charge automatically tracks users' steps, sleep, flights of stairs (using an altimeter) and an approximation of distance traveled. It tracks steps using a 3 axis accelerometer by tracking forward movement along with upward movements.\n\nAnnounced in October 2014 and released in early January 2015, the Charge HR is similar to the Charge, with an additional heart-rate monitor. With this addition, the 7-day battery life is reduced to 5 days. The Charge HR has the same textured band as the Charge and comes in black, plum, blue, tangerine, pink, and teal colors. The Charge HR band clasp resembles that of a traditional watch instead of the snap-on band of the original Charge, as the band needs to fit tightly for the heart rate feature.\n\nThe Fitbit Charge 2 featured a new multi-sport mode allowing users to start workouts from their Fitbit. Compared to its predecessor it had a larger screen.\n\nAnnounced in October 2014, the Surge was a smartwatch and an activity tracker. It features a heart-rate monitor and the ability to track pace, distance, and elevation using the GPS on the device. The Surge also can send alerts of text and incoming calls from a connected smartphone.\n\nThe Surge was discontinued in late 2017.\n\nReleased in January 2016 the Fitbit Blaze is a smartwatch made to compete with the Apple Watch, Pebble, and Android Wear. The Blaze comes with a colored touchscreen, and exchangeable strap and frame. It can auto-tracking exercises and has a heart-rate monitor. Blaze has connected GPS, meaning it tracks location using the connected smartphone's GPS. It can display notifications, including incoming calls, texts and calendar appointments. The Blaze introduces the Sleep Stages feature.\n\nThe Fitbit Blaze also integrates with Fitstar, Fitbit's website for customized workouts. These workouts can be displayed on the Blaze's screen.\n\nThe Blaze was discontinued in early 2018.\n\nIn April 2012, Fitbit released a weighing scale called the Fitbit Aria. It recognizes users and measures weight, body mass index (BMI) and percentage of body fat of the user. It can keep track of eight individual users and updates information to fitbit.com automatically via Wi-Fi network. The information is also updated to smartphone apps.\n\n"}
{"id": "538871", "url": "https://en.wikipedia.org/wiki?curid=538871", "title": "Live television", "text": "Live television\n\nLive television is a television production broadcast in real-time, as events happen, in the present. In a secondary meaning, it may refer to streaming television over the internet. In most cases live programming is not being recorded as it is shown on TV, but rather was not rehearsed or edited and is being shown only as it was recorded prior to being aired. Shows broadcast live include newscasts, morning shows, awards shows, sports programs, reality programs and, occasionally, episodes of scripted television series.\n\nLive television was more common until the late 1950s, when videotape technology was invented. Because of the prohibitive cost, adoption was slow, and some television shows remained live until the 1970s, such as soap operas. To prevent unforeseen issues, live television programs may be delayed, which allows censors to edit the program. Some programs may be broadcast live in certain time zones and delayed in others.\n\nFrom the early days of television until about 1958, live television was used heavily, except for filmed shows such as \"I Love Lucy\" and \"Gunsmoke\". Although videotape was invented in 1956, it cost $300 per one hour reel ()\nmeaning it was only very gradually adopted. Some genres, such as soap operas, did not completely abandon live broadcasts until the mid-1970s.\n\nIn general, a live television program was more common for broadcasting content produced specifically for commercial television in the early years of the medium, before technologies such as video tape appeared. As video tape recorders (VTR) became more prevalent, many entertainment programs were recorded and edited before broadcasting rather than being shown live.\n\nTelevision networks provide most live television for morning shows with television programs such as: \"Good Morning Britain\", \"BBC Breakfast\", \"This Morning\", etc. broadcast live in the UK; Sunrise live in Australia; Canada AM live in Canada; and \"Today\", \"Good Morning America\", and \"CBS This Morning\" in the U.S., which air live only in the Eastern Time Zone. The only exceptions are \"CBS This Morning - Saturday\" and \"Sunday Today with Willie Geist\", which air live in the Eastern and Central time zones. Spanish-language morning shows (such as \"Despierta America\" and \"Un Nuevo Día\"), unlike their English speaking counterparts, air live in across the mainland U.S. except for viewers in the Pacific time zone, which, along with viewers in Hawaii and Alaska, have tape-delayed shows.\n\nA few daytime talk shows in the U.S. broadcast live before a studio audience in select time zones. Shows such as \"Live with Kelly and Ryan\" and the \"Wendy Williams Show\" air live in the Eastern time zone only, while shows such as ABC's \"The View\" air live in the Eastern and Central time zones. \"The Talk\" on CBS airs live in the Eastern and Central time zones Monday through Thursday. A separate program is taped on Thursday afternoon for airing on Friday. Affiliates in the remaining time zones air these programs on a tape delay. Most other daytime talk shows and late night programs are taped before a live studio audience earlier in the day and edited for later broadcast.\n\nDuring prime time, Miss USA, Miss America and talent shows (such as \"Dancing With The Stars\") air live in the Eastern and Central time zones. Other talent shows (such as \"The Voice\", and \"America's Got Talent\") will pre-record audition rounds and broadcast the live rounds in the Eastern and Central time zones where viewers have the opportunity to vote for their favorite contestants. In particular, until today, several live prime time U.S. entertainment telecasts are still tape-delayed for viewers outside the Eastern and Central time zones.\n\nFor decades, the Academy Awards have been televised live in all U.S. time zones within mainland North America. The advent of the Internet and social media outlets (e.g. Facebook and Twitter, which resulted in numerous \"spoilers\") in the late 2000s, however, prompted several broadcasters to air live awards shows simultaneously across both U.S. coasts, beginning with NBC in 2009 with the Golden Globe Awards. The Primetime Emmy Awards and Billboard Music Awards followed suit in the following decade.\n\nMeanwhile, CBS began offering its affiliates in the Mountain and Pacific time zones an optional live broadcast of the Grammy Awards in 2016 (with stations in the Pacific time zone airing a rebroadcast of the Grammys immediately after the live broadcast during primetime). Beginning 2017, both the Grammys and the Primetime Emmys have been aired to totality live in all U.S. territories, including Alaska and Hawaii. Most other award shows in the U.S. typically air live in the Eastern and Central time zones. Currently, the Tony Awards is the only major awards show that continues the practice of tape delaying to the West Coast.\n\nMost local television station newscasts are broadcast live in the U.S. as they are an essential medium for providing up-to-the-minute weather forecasts and breaking news stories. Broadcast television networks in the United States typically air their evening newscasts live in the Eastern and Central time zones. A separate \"Western Edition\" is broadcast to viewers in the Pacific Time Zone. When a major breaking news event occurs, whether nationally or globally, broadcast television networks will break into regularly scheduled programming and will televise a live \"special report\" in all time zones. Local television stations break into regularly scheduled programming in the event of severe weather warnings or major local breaking news stories that occur within their viewing area.\n\nCable news outlets (such as CNN and Fox News Channel) air continuous live programming during the day, and air rebroadcasts of earlier live shows during the late night hours, except in cases where breaking news occurs. The \"PBS NewsHour\" airs live on PBS stations in the Eastern Time Zone. Sunday morning news programs in the USA such as \"Meet The Press\" on NBC, \"This Week\" on ABC, and \"Fox News Sunday\" air live in the Eastern Time Zone (including a limited number of small markets in the Central Time Zone), while \"CBS Sunday Morning\" and \"Face The Nation\" on CBS air live in the Eastern and Central time zones.\n\nCable outlets (such as CNN and Fox News Channel) incorporate the word \"LIVE\" in their network logo (also known as a digital on-screen graphic) when those networks broadcast live content. Some (but not all) sports cable networks will opt to insert the word \"LIVE\" somewhere on the corner of the screen. With the exception of special breaking news reports and overseas sporting events, broadcast television networks rarely display such a graphic during its live programming. (although NBC did display the word LIVE next to their logo during its Olympic coverage when live content was being broadcast, a practice that is being continued by its sister station: NBCSN)\n\nLocal television station newscasts display time and temperature during their broadcasts, and only display the word LIVE when they air a news report or a live shot on location. Some networks have begun to insert (in addition to the word LIVE) the local time of where that news report is originating from, particularly when that report is airing live via satellite from overseas.\n\nBeginning in 2014, a trend began of harassing female journalists who are broadcasting live, including shouting profane phrases. The most common phrase, \"fuck her right in the pussy\", comes from a viral video on YouTube in which a comedian staged a fake blooper reel that used the phrase. Fans later started using it to interrupt live broadcasts and humiliate journalists. In 2015, a female CityNews journalist confronted a group of young men who had used the phrase; one of them later lost his job after he was identified. The same year, a teen boy kissed a CBC News reporter during a live broadcast, prompting a discussion of what constitutes sexual assault. The teen later apologized and called it a poorly-considered joke; the reporter declined to press charges. In New Zealand, the boyfriend of a TV3 reporter said she was groped after two young men shouted the phrase at her. When her boyfriend confronted them, the men said the news show should have sent a male reporter. One later apologized.\n\nAs of the current decade, major sporting events like the Super Bowl, World Cup and Olympic Games have been broadcast entirely live in all U.S. territories, encompassing both prime time hours of both U.S. coasts, simultaneous with the live global telecasts of these events in accordance with the official international broadcasters of such games.\n\nOther events that air live all across U.S. territories include multi-network coverage of U.S. presidential and congressional elections, U.S. presidential inaugurations, the State of the Union Address, presidential news conferences, Presidential Addresses to the Nation, the Tournament of Roses Parade, and funerals of major national or international public and religious figures. Local television stations air live local election coverage and special events, such as large scale parades, big city marathons, funerals of major local public and religious figures, inauguration ceremonies of big city mayors and governors, installation masses of cardinals or bishops in a major Catholic archdiocese, and pep rallies for a major sports team. In the UK, events such as the State Opening of Parliament are broadcast live.\n\nLive television is often used as a device, even in scripted programming to take advantage of these often to great success in terms of attracting viewers. The NBC live comedy/variety program \"Saturday Night Live\", for example, has been on that network continuously since 1975 and airs live in the Eastern and Central zones (including the Pacific and Mountain zones beginning 2017) during the show's season which runs from October though May.\n\nOn September 25, 1997, NBC aired two separate live broadcasts (for viewers in both West and East Coasts) of an episode of \"ER\", which at the time ranked as the most watched episode of any U.S. medical drama program ever. Many television news programs, particularly local news ones in North America, have also used live television as a device to gain audience viewers by making their programs appear more exciting. With technologies such as production trucks, satellite truck uplinks, a news reporter can report live \"on location\" from anywhere where a story is happening in the city. This technique has attracted criticism for its overuse (like minor car accidents which often have no injuries) and resulting tendency to make stories appear more urgent than they actually are.\n\nThe unedited nature of live television can pose problems for television networks because of the potential for mishaps. To enforce the Federal Communications Commission (FCC) regulations, television networks often broadcast live programs on a slight delay (usually on single-digit seconds only) to give them the ability to censor words and images while keeping the broadcast as \"live\" as possible.\n\nMany events have happened on live television broadcasts that are well-remembered, sometimes because they were part of a major breaking news story already, and always because they happened unexpectedly and before audiences of thousands or millions of viewers.\n\n\n\nAlthough all programs were once live, the use of video tape means that very few television programs in the modern era have ever attempted such a feat. In the U.S., soap operas including \"As the World Turns\" and \"The Edge of Night\" were broadcast live until 1975.\n\nOn rare occasions, a scripted series will do an episode live to attract ratings. In the U.S. and Canada, the episode is occasionally performed twice: once for the east coast which is composed of the Eastern Time Zone and Central Time Zone and again three hours later for the west coast which is composed of the Mountain Time Zone and the Pacific Time Zone unless they have Dish Network or Direct TV who provides the live feed in all states. The most recent scripted series to air all live episodes was \"Undateable\" on NBC during its third season, which aired from October 2015 until January 2016.\n\nNotable examples of shows that have had a live episode include:\n\n\nSince 2000, there have been a number of special films broadcast live. These include the remakes of \"Fail Safe\" (2000) and \"The Quatermass Experiment\" (2005). Some recent examples of live episodic TV series include shows such as Melissa and Joey (2010), Whitney (2011) and Undateable (2014).\n\nA live television advertisement was shown for the first time in 40 years to celebrate the arrival of the new Honda Accord in the United Kingdom. It was broadcast on Channel Four on 29 May 2008 at 20:10 during a special episode of 'Come Dine With Me'. The ad featured skydivers forming the letters of the word Honda over Spain.\n\nMany live television specials were telecast during the pre-videotape era. Among the most successful were the 1955 and 1956 telecasts of \"Peter Pan\", a 1954 musical adaptation of J. M. Barrie's 1904 play, starring Mary Martin, and Cyril Ritchard. This was such a hit that the show was restaged and rebroadcast (this time on videotape) with the same two stars and most of the rest of the cast in 1960, and rerun several times after that. The \"Peter Pan\" telecasts marked the first-ever telecasts of a complete Broadway musical with most of its original cast.\n\nOn December 5, 2013, NBC broadcast a live television special called \"The Sound of Music Live!\" starring Carrie Underwood. This program aired live in the Eastern and Central time zones, and was the first television musical special to air live on NBC in almost fifty years.\n\n\n\n"}
{"id": "37901692", "url": "https://en.wikipedia.org/wiki?curid=37901692", "title": "MEISTeR (Robot)", "text": "MEISTeR (Robot)\n\nThe Maintenance Equipment Integrated System of Telecontrol Robot (MEISTeR) is a service robot by the Mitsubishi Heavy Industries (MHI). It was specifically designed to work at the devastated Fukushima Daiichi Nuclear Power Plant.\n\nIt was presented to the public on December 6, 2012.\n\nAfter the 9.0 earthquake and subsequent tsunami that hit Japan on March 11, 2011, and the destruction of the Fukushima Daiichi Nuclear Power Plant, the industry went on a quest of providing robots able to do dangerous work at the power plant. One such solution was the HAL exoskeleton. However, the HAL exoskeleton required a human pilot. On November 23, 2012, Toshiba presented a Tetrapod which failed during the presentation.\n\nUntil recently, MHI had only create a household communication robot and the MARS-D robot. As a constructor of 20 nuclear plants in Japan, it was natural for the Mitsubishi Heavy Industries to respond to the crisis with their own robot.\n\nThe MEISTeR (German for master) is a twin-armed four-tracked robot. It is based on the Rabot robot and the experimental MARS-D robot, designed as a nuclear plant inspector. It stands , is wide, long, and weights in at . Its robotic arms have seven degrees of freedom(=seven axes) just like a human arm and can lift each. Unlike the MARS-D robot MEISTeR is robust enough to withstand the radiation environment. The remote-controlled robot can attached with a variety of tools to its hands such as cutters and drills, clear obstacles, and pierce through concrete to check radiation levels. A special tool has been developed that can take samples from walls and concrete floors in contaminated areas with a depth up to . It can move at up to and negotiates uneven terrain, including stairsteps up to on its four independently moving tank tracks. It has an expected working time of two hours.\n"}
{"id": "173366", "url": "https://en.wikipedia.org/wiki?curid=173366", "title": "Mechanization", "text": "Mechanization\n\nMechanisation is the process of changing from working largely or exclusively by hand or with animals to doing that work with machinery. In an early engineering text a machine is defined as follows:\n\nIn some fields, mechanization includes the use of hand tools. In modern usage, such as in engineering or economics, mechanization implies machinery more complex than hand tools and would not include simple devices such as an ungeared horse or donkey mill. Devices that cause speed changes or changes to or from reciprocating to rotary motion, using means such as gears, pulleys or sheaves and belts, shafts, cams and cranks, usually are considered machines. After electrification, when most small machinery was no longer hand powered, mechanization was synonymous with motorized machines.\n\nWater wheels date to the Roman period and were used to grind grain and lift irrigation water. Water powered bellows were in use on blast furnaces in China in 31 AD. By the 13th century, water wheels powered sawmills and trip hammers, to full cloth and pound flax and later cotton rags into pulp for making paper. Trip hammers are shown crushing ore in \"De re Metallica\" (1555).\n\nClocks were some of the most complex early mechanical devices. Clock makers were important developers of machine tools including gear and screw cutting machines, and were also involved in the mathematical development of gear designs. Clocks were some of the earliest mass-produced items, beginning around 1830.\n\nWater powered bellows for blast furnaces, used in China in ancient times, were in use in Europe by the 15th century. \"De re Metallica\" contains drawings related to bellows for blast furnaces including a fabrication drawing.\n\nImproved gear designs decreased wear and increased efficiency. Mathematical gear designs were developed in the mid 17th century. French mathematician and engineer Desargues designed and constructed the first mill with epicycloidal teeth ca. 1650. In the 18th century involute gears, another mathematical derived design, came into use. Involute gears are better for meshing gears of different sizes than epicycloidal. Gear cutting machines came into use in the 18th century.\n\nThe Newcomen steam engine was first used, to pump water from a mine, in 1712. John Smeaton introduced metal gears and axles to water wheels in the mid to last half of the 18th century. Smeaton also conducted a scientific investigation into the design of water wheels which led to significant efficiency increases. The Industrial Revolution started mainly with textile machinery, such as the spinning jenny (1764) and water frame (1768).\n\nDemand for metal parts used in textile machinery led to the invention of many machine tools in the late 1700s until the mid-1800s. After the early decades of the 19th century, iron increasingly replaced wood in gearing and shafts in textile machinery. In the 1840s \"self acting\" machine tools were developed. Self-acting tools displaced hand dexterity and allowed one unskilled operator to tend several machines. Machinery was developed to make nails ca. 1810. The Fourdrinier paper machine paper machine for continuous production of paper was patented in 1801, displacing the centuries-old hand method of making individual sheets of paper.\n\nOne of the first mechanical devices used in agriculture was the seed drill invented by Jethro Tull around 1700. The seed drill allowed more uniform spacing of seed and planting depth than hand methods, increasing yields and saving valuable seed. Mechanized agriculture greatly increased in the late eighteenth and early nineteenth centuries with horse drawn reapers and horse powered threshing machines. By the late nineteenth century steam power was applied to threshing and steam tractors appeared. Internal combustion began being used for tractors in the early twentieth century. Threshing and harvesting was originally done with attachments for tractors, but in the 1930s independently powered combine harvesters were in use.\n\nIn the mid to late 19th century, hydraulic and pneumatic devices were able to power various mechanical actions, such as positioning tools or work pieces. Pile drivers and steam hammers are examples for heavy work. In food processing, pneumatic or hydraulic devices could start and stop filling of cans or bottles on a conveyor. Power steering for automobiles uses hydraulic mechanisms, as does practically all earth moving equipment and other construction equipment and many attachments to tractors. Pneumatic (usually compressed air) power is widely used to operate industrial valves.\n\nBy the early 20th century machines developed the ability to perform more complex operations that had previously been done by skilled craftsmen. An example is the glass bottle making machine developed 1905. It replaced highly paid glass blowers and child labor helpers and led to the mass production of glass bottles.\n\nAfter 1900 factories were electrified, and electric motors and controls were used to perform more complicated mechanical operations. This resulted in mechanized processes to manufacture almost all goods.\n\nIn manufacturing, mechanization replaced hand methods of making goods. Prime movers are devices that convert thermal, potential or kinetic energy into mechanical work. Prime movers include internal combustion engines, combustion turbines (jet engines), water wheels and turbines, windmills and wind turbines and steam engines and turbines. Powered transportation equipment such as locomotives, automobiles and trucks and airplanes, is a classification of machinery which includes sub classes by engine type, such as internal combustion, combustion turbine and steam. Inside factories, warehouses, lumber yards and other manufacturing and distribution operations, material handling equipment replaced manual carrying or hand trucks and carts.\n\nMechanized agriculture\n\nIn mining and excavation, power shovels replaced picks and shovels. Rock and ore crushing had been done for centuries by water powered trip hammers, but trip hammers have been replaced by modern ore crushers and ball mills.\n\nBulk material handling systems and equipment are used for a variety of materials including coal, ores, grains, sand, gravel and wood products.\n\nConstruction equipment includes cranes, concrete mixers, concrete pumps, cherry pickers and an assortment of power tools.\n\nPowered machinery today usually means either by electric motor or internal combustion engine. Before the first decade of the 20th century powered usually meant by steam engine, water or wind.\n\nMany of the early machines and machine tools were hand powered, but most changed over to water or steam power by the early 19th century.\n\nBefore electrification, mill and factory power was usually transmitted using a line shaft. Electrification allowed individual machines to each be powered by a separate motor in what is called \"unit drive\". Unit drive allowed factories to be better arranged and allowed different machines to run at different speeds. Unit drive also allowed much higher speeds, which was especially important for machine tools.\n\nA step beyond mechanization is automation. Early production machinery, such as the glass bottle blowing machine (ca. 1890s), required a lot of operator involvement. By the 1920s fully automatic machines, which required much less operator attention, were being used.\n\nSee: Mass production\n\nThe term is also used in the military to refer to the use of tracked armoured vehicles, particularly armoured personnel carriers, to move troops that would otherwise have marched or ridden trucks into combat. In military terminology, \"mechanized\" refers to ground units that can fight from vehicles, while \"motorized\" refers to units that go to battle in vehicles but then dismount and fight without them. Thus, a towed artillery unit is considered motorized while a self-propelled one is mechanized.\n\nWhen we compare the efficiency of a labourer, we see that he has an efficiency of about 1%-5.5% (depending on whether he uses arms, or a combination of arms and legs). Internal combustion engines mostly have an efficiency of about 20%, although large diesel engines, such as those used to power ships, may have efficiencies of nearly 50%. Industrial electric motors have efficiencies up to the low 90% range, before correcting for the conversion efficiency of fuel to electricity of about 35%.\n\nWhen we compare the costs of using an internal combustion engine to a worker to perform work, we notice that an engine can perform more work at a comparative cost. 1 liter of fossil fuel burnt with an IC engine equals about 50 hands of workers operating for 24 hours or 275 arms and legs for 24 hours.\n\nIn addition, the combined work capability of a human is also much lower than that of a machine. An average human worker can provide work good for around 0,9 hp (2.3 MJ per hour) while a machine (depending on the type and size) can provide for far greater amounts of work. For example, it takes more than one and a half hour of hard labour to deliver only one kWh - which a small engine could deliver in less than one hour while burning less than one litre of petroleum fuel. This implies that a gang of 20 to 40 men will require a financial compensation for their work at least equal to the required expended food calories (which is at least 4 to 20 times higher). In most situations, the worker will also want compensation for the lost time, which is easily 96 times greater per day. Even if we assume the real wage cost for the human labour to be at US $1.00/day, an energy cost is generated of about $4.00/kWh. Despite this being a low wage for hard labour, even in some of the countries with the lowest wages, it represents an energy cost that is significantly more expensive than even exotic power sources such as solar photovoltaic panels (and thus even more expensive when compared to wind energy harvesters or luminescent solar concentrators).\n\nFor simplification, one can study mechanization as a series of steps. Many students refer to this series as indicating basic-to-advanced forms of mechanical society.\n\n\n\n"}
{"id": "23611099", "url": "https://en.wikipedia.org/wiki?curid=23611099", "title": "Miller Camera Support Equipment", "text": "Miller Camera Support Equipment\n\nMiller Camera Support Equipment is the commercial name of R. E. Miller Pty Ltd, an independent Australian manufacturer of tripods, fluid heads and various accessories for professional camera systems (dollies, spreaders and other camera supports such as monopods and pedestal systems).\nThe company supplies ENG/EFP equipment to broadcast and government.\n\nMiller is also the distributor of Lowel lighting equipment in Australia.\n\nRobert Eric Miller, an Australian engineer from Sydney, invented the fluid head for motion picture cameras for which he was granted an Australian patent in 1946 and US patent in 1949. He founded Miller Camera Support Equipment in 1954, manufacturing fluid heads and tripods. The same year, he developed the Miller Viscosity Drag. This invention greatly improved image quality in films by applying user-variable dampening to the pan and tilt movements of the camera.\n\nMiller products are widely used in Australia, and since 1958, have been exported to the US, Europe and Asia.\n\nMiller Camera Support Equipment headquarters and manufacturing are based near Sydney, Australia. The company also has service facilities in London and in New York, plus branch offices in Canada, California, Hong Kong, China, and various European countries.\n\n\n\n"}
{"id": "47090617", "url": "https://en.wikipedia.org/wiki?curid=47090617", "title": "Ministry of Electric Power", "text": "Ministry of Electric Power\n\nThe Ministry of Electric Power (; abbreviated MOEP) administers Burma's electric power policies.\n\nThe Ministry is currently led by Win Khaing who is the minister for both MOEE and MOC.\n\nMyanmar Engineering Society has identified at least 39 locations capable of geothermal power production and some of these hydrothermal reservoirs lie quite close to Yangon which is a significant underutilized resource.\n\nFinancing Geothermal projects in Myanmar use an estimated break even power cost of 5.3-8.6 U.S cents/kWh or in Myanmar Kyat 53-86K per kWh. This pegs a non-fluctuating 1$=1000K, which is a main concern for power project funding. The main drawback with depreciation pressures, in the current FX market.\nBetween June 2012 and October 2015, the Myanmar Kyat depreciated by approximately 35%, from 850 down to 1300 against the US Dollar. Local businesses with foreign denominated loans from abroad suddenly found themselves rushing for a strategy to mitigate currency risks. Myanmar’s current lack of available currency hedging solutions presents a real challenge for Geothermal project financing.\n\nOn 1 October 1951, Electricity Supply Board (ESB) was organized under the Ministry of Industry. On 16 March 1972, it was changed as Electric Power Corporation (EPC). On 1 April 1975, the Ministry of Industry was organized as No 1 and No 2, the EPC was composed under the Ministry of No 2 Industry. On 12 April 1985,the Ministry of No 2 Industry was changed as Ministry of Energy, so the EPC was composed under it. On 1 April 1989, the EPC was changed into Myanmar Electric Power Enterprise (MEPE).\n\nOn 15 November 1997,the Ministry of Electrical Power was started organized and there were three departments under it, Department of Electrical Power, Myanmar Electric Power Enterprise and Department of Hydropower. On 15 May 2006, the ministry was divided into No 1 and No 2. On 5 September 2012, they were composed as one Ministry as Ministry of Electrical Power (MOEP).\n\nIn the new Myanmar cabinet of U Htin Kyaw, the MOEP was composed with Ministry of Energy as Ministry of Electricity and Energy (MOEE).\n\n\n"}
{"id": "52206", "url": "https://en.wikipedia.org/wiki?curid=52206", "title": "Nanowire", "text": "Nanowire\n\nA nanowire is a nanostructure, with the diameter of the order of a nanometer (10 meters). It can also be defined as the ratio of the length to width being greater than 1000. Alternatively, nanowires can be defined as structures that have a thickness or diameter constrained to tens of nanometers or less and an unconstrained length. At these scales, quantum mechanical effects are important—which coined the term \"quantum wires\". Many different types of nanowires exist, including superconducting (e.g. YBCO), metallic (e.g. Ni, Pt, Au), semiconducting (e.g. silicon nanowires (SiNWs), InP, GaN) and insulating (e.g. SiO, TiO). Molecular nanowires are composed of repeating molecular units either organic (e.g. DNA) or inorganic (e.g. MoSI).\n\nTypical nanowires exhibit aspect ratios (length-to-width ratio) of 1000 or more. As such they are often referred to as one-dimensional (1-D) materials. Nanowires have many interesting properties that are not seen in bulk or 3-D (three-dimensional) materials. This is because electrons in nanowires are quantum confined laterally and thus occupy energy levels that are different from the traditional continuum of energy levels or bands found in bulk materials.\n\nPeculiar features of this quantum confinement exhibited by certain nanowires manifest themselves in discrete values of the electrical conductance. Such discrete values arise from a quantum mechanical restraint on the number of electrons that can travel through the wire at the nanometer scale. These discrete values are often referred to as the quantum of conductance and are integer multiples of\n\nThey are inverse of the well-known resistance unit \"h/e\", which is roughly equal to 25812.8 ohms, and referred to as the von Klitzing constant \"R\" (after Klaus von Klitzing, the discoverer of exact quantization). Since 1990, a fixed conventional value \"R\" is accepted.\n\nExamples of nanowires include inorganic molecular nanowires (MoSI, LiMoSe), which can have a diameter of 0.9 nm and be hundreds of micrometers long. Other important examples are based on semiconductors such as InP, Si, GaN, etc., dielectrics (e.g. SiO,TiO), or metals (e.g. Ni, Pt).\n\nThere are many applications where nanowires may become important in electronic, opto-electronic and nanoelectromechanical devices, as additives in advanced composites, for metallic interconnects in nanoscale quantum devices, as field-emitters and as leads for biomolecular nanosensors.\n\nThere are two basic approaches to synthesizing nanowires: top-down and bottom-up. A top-down approach reduces a large piece of material to small pieces, by various means such as lithography, milling or thermal oxidation. A bottom-up approach synthesizes the nanowire by combining constituent adatoms. Most synthesis techniques use a bottom-up approach. Initial synthesis via either method may often be followed by a nanowire thermal treatment step, often involving a form of self-limiting oxidation, to fine tune the size and aspect ratio of the structures.\n\nNanowire production uses several common laboratory techniques, including suspension, electrochemical deposition, vapor deposition, and VLS growth. Ion track technology enables growing homogeneous and segmented nanowires down to 8 nm diameter. As nanowire oxidation rate is controlled by diameter, thermal oxidation steps are often applied to tune their morphology.\n\nA suspended nanowire is a wire produced in a high-vacuum chamber held at the longitudinal extremities. Suspended nanowires can be produced by:\n\nA common technique for creating a nanowire is vapor-liquid-solid method (VLS), which was first reported by Wagner and Ellis in 1964 for silicon whiskers with diameters ranging from 100s of nm to 100s of µm. This process can produce high-quality crystalline nanowires of many semiconductor materials, for example, VLS–grown single crystalline silicon nanowires (SiNWs) with smooth surfaces could have excellent properties, such as ultra-large elasticity. This method uses a source material from either laser ablated particles or a feed gas such as silane.\n\nVLS synthesis requires a catalyst. For nanowires, the best catalysts are liquid metal (such as gold) nanoclusters, which can either be self-assembled from a thin film by dewetting, or purchased in colloidal form and deposited on a substrate.\n\nThe source enters these nanoclusters and begins to saturate them. On reaching supersaturation, the source solidifies and grows outward from the nanocluster. Simply turning off the source can adjust the final length of the nanowire. Switching sources while still in the growth phase can create compound nanowires with super-lattices of alternating materials.\n\nA single-step vapour phase reaction at elevated temperature synthesises inorganic nanowires such as MoSI. From another point of view, such nanowires are cluster polymers.\n\nSolution-phase synthesis refers to techniques that grow nanowires in solution. They can produce nanowires of many types of materials. Solution-phase synthesis has the advantage that it can produce very large quantities, compared to other methods. In one technique, the polyol synthesis, ethylene glycol is both solvent and reducing agent. This technique is particularly versatile at producing nanowires of gold, lead, platinum, and silver.\n\nThe supercritical fluid-liquid-solid growth method can be used to synthesize semiconductor nanowires, e.g., Si and Ge. By using metal nanocrystals as seeds, Si and Ge organometallic precursors are fed into a reactor filled with a supercritical organic solvent, such as toluene. Thermolysis results in degradation of the precursor, allowing release of Si or Ge, and dissolution into the metal nanocrystals. As more of the semiconductor solute is added from the supercritical phase (due to a concentration gradient), a solid crystallite precipitates, and a nanowire grows uniaxially from the nanocrystal seed.\n\nNanowires can be also grown without the help of catalysts, which gives an advantage of pure nanowires and minimizes the number of technological steps. The simplest methods to obtain metal oxide nanowires use ordinary heating of the metals, e.g. metal wire heated with battery, by Joule heating in air can be easily done at home.\nThe vast majority of nanowire-formation mechanisms are explained through the use of catalytic nanoparticles, which drive the nanowire growth and are either added intentionally or generated during the growth. However the mechanisms for catalyst-free growth of nanowires (or whiskers) were known from 1950s. Spontaneous nanowire formation by non-catalytic methods were explained by the dislocation present in specific directions or the growth anisotropy of various crystal faces. More recently, after microscopy advancement, the nanowire growth driven by screw dislocations or twin boundaries were demonstrated. The picture on the right shows a single atomic layer growth on the tip of CuO nanowire, observed by in situ TEM microscopy during the non-catalytic synthesis of nanowire.\n\nAn emerging field is to use DNA strands as scaffolds for metallic nanowire synthesis. This method is investigated both for the synthesis of metallic nanowires in electronic components and for biosensing applications, in which they allow the transduction of a DNA strand into a metallic nanowire that can be electrically detected. Typically, ssDNA strands are stretched, whereafter they are decorated with metallic nanoparticles that have been functionalised with short complementary ssDNA strands.\n\nSeveral physical reasons predict that the conductivity of a nanowire will be much less than that of the corresponding bulk material. First, there is scattering from the wire boundaries, whose effect will be very significant whenever the wire width is below the free electron mean free path of the bulk material. In copper, for example, the mean free path is 40 nm. Copper nanowires less than 40 nm wide will shorten the mean free path to the wire width.\n\nNanowires also show other peculiar electrical properties due to their size. Unlike single wall carbon nanotubes, whose motion of electrons can fall under the regime of ballistic transport (meaning the electrons can travel freely from one electrode to the other), nanowire conductivity is strongly influenced by edge effects. The edge effects come from atoms that lay at the nanowire surface and are not fully bonded to neighboring atoms like the atoms within the bulk of the nanowire. The unbonded atoms are often a source of defects within the nanowire, and may cause the nanowire to conduct electricity more poorly than the bulk material. As a nanowire shrinks in size, the surface atoms become more numerous compared to the atoms within the nanowire, and edge effects become more important.\n\nFurthermore, the conductivity can undergo a quantization in energy: i.e. the energy of the electrons going through a nanowire can assume only discrete values, which are multiples of the conductance quantum \"G = 2e/h\" (where \"e\" is the charge of the electron and \"h\" is the Planck constant. See also the Quantum Hall effect).\n\nThe conductivity is hence described as the sum of the transport by separate \"channels\" of different quantized energy levels. The thinner the wire is, the smaller the number of channels available to the transport of electrons.\n\nThis quantization has been demonstrated by measuring the conductivity of a nanowire suspended between two electrodes while pulling it: as its diameter reduces, its conductivity decreases in a stepwise fashion and the plateaus correspond to multiples of G.\n\nThe quantization of conductivity is more pronounced in semiconductors like Si or GaAs than in metals, due to their lower electron density and lower effective mass. It can be observed in 25 nm wide silicon fins, and results in increased threshold voltage. In practical terms, this means that a MOSFET with such nanoscale silicon fins, when used in digital applications, will need a higher gate (control) voltage to switch the transistor on.\n\nTo incorporate nanowire technology into industrial applications, researchers in 2008 developed a method of welding nanowires together: a sacrificial metal nanowire is placed adjacent to the ends of the pieces to be joined (using the manipulators of a scanning electron microscope); then an electric current is applied, which fuses the wire ends. The technique fuses wires as small as 10 nm.\n\nFor nanowires with diameters less than 10 nm, existing welding techniques, which require precise control of the heating mechanism and which may introduce the possibility of damage, will not be practical. Recently scientists discovered that single-crystalline ultrathin gold nanowires with diameters ~3–10 nm can be \"cold-welded\" together within seconds by mechanical contact alone, and under remarkably low applied pressures (unlike macro- and micro-scale cold welding process). High-resolution transmission electron microscopy and in situ measurements reveal that the welds are nearly perfect, with the same crystal orientation, strength and electrical conductivity as the rest of the nanowire. The high quality of the welds is attributed to the nanoscale sample dimensions, oriented-attachment mechanisms and mechanically assisted fast surface diffusion. Nanowire welds were also demonstrated between gold and silver, and silver nanowires (with diameters ~5–15 nm) at near room temperature, indicating that this technique may be generally applicable for ultrathin metallic nanowires. Combined with other nano- and microfabrication technologies, cold welding is anticipated to have potential applications in the future bottom-up assembly of metallic one-dimensional nanostructures.\n\nThe study of nanowire mechanics has boomed since the advent of the Atomic Force Microscope (AFM), and associated technologies which have enabled direct study of the response of the nanowire to an applied load. Specifically, a nanowire can be clamped from one end, and the free end displaced by an AFM tip. In this cantilever geometry, the height of the AFM is precisely known, and the force applied is precisely known. This allows for construction of a force vs. displacement curve, which can be converted to a stress vs. strain curve if the nanowire dimensions are known. From the stress-strain curve, the elastic constant known as the Young’s Modulus can be derived, as well as the toughness, and degree of strain-hardening. Moreover, if this process occurs while simultaneously viewing the nanowire in a scanning electron microscope, the resulting mechanical properties can be directly correlated with the nanowire’s microstructure.\n\nThe elastic component of the stress-strain curve described by the Young’s Modulus, has been reported for nanowires, however the modulus depends very strongly on the microstructure. Thus a complete description of the modulus dependence on diameter is lacking. Analytically, continuum mechanics has been applied to estimate the dependence of modulus on diameter: formula_2 in tension, where formula_3 is the bulk modulus, formula_4 is the thickness of a shell layer in which the modulus is surface dependent and varies from the bulk, formula_5 is the surface modulus, and formula_6 is the diameter. This equation implies that the modulus increases as the diameter decreases. However, various computational methods such as molecular dynamics have predicted that modulus should decrease as diameter decreases. \n\nExperimentally, gold nanowires have been shown to have a Young’s modulus which is effectively diameter independent. Similarly, nano-indentation was applied to study the modulus of silver nanowires, and again the modulus was found to be 88 GPa, very similar to the modulus of bulk Silver (85 GPa) These works demonstrated that the analytically determined modulus dependence seems to be suppressed in nanowire samples where the crystalline structure highly resembles that of the bulk system.\n\nIn contrast, Si solid nanowires have been studied, and shown to have a decreasing modulus with diameter The authors of that work report a Si modulus which is half that of the bulk value, and they suggest that the density of point defects, and or loss of chemical stoichiometry may account for this difference.\n\nThe plastic component of the stress strain curve (or more accurately the onset of plasticity) is described by the yield strength. The strength of a material is increased by decreasing the number of defects in the solid, which occurs naturally in nanomaterials where the volume of the solid is reduced. As a nanowire is shrunk to a single line of atoms, the strength should theoretically increase all the way to the molecular tensile strength. Gold nanowires have been described as ‘ultrahigh strength’ due to the extreme increase in yield strength, approaching the theoretical value of E/10. This huge increase in yield is determined to be due to the lack of dislocations in the solid. Without dislocation motion, a ‘dislocation-starvation’ mechanism is in operation. The material can accordingly experience huge stresses before dislocation motion is possible, and then begins to strain-harden. For these reasons, nanowires (historically described as 'whiskers') have been used extensively in composites for increasing the overall strength of a material. Moreover, nanowires continue to be actively studied, with research aiming to translate enhanced mechanical properties to novel devices in the fields of MEMS or NEMS.\n\nNanowires can be used for transistors. Transistors are used widely as fundamental building element in today's electronic circuits. As predicted by Moore's law, the dimension of transistors is shrinking smaller and smaller into nanoscale. One of the key challenges of building future nanoscale transistors is ensuring good gate control over the channel. Due to the high aspect ratio, if the gate dielectric is wrapped around the nanowire channel, we can get good control of channel electrostatic potential, thereby turning the transistor on and off efficiently.\n\nDue to the unique one-dimensional structure with remarkable optical properties, the nanowire also opens new opportunities for realizing high efficiency photovoltaic devices. Compared with its bulk counterparts, the nanowire solar cells are less sensitive to impurities due to bulk recombination, and thus silicon wafers with lower purity can be used to achieve acceptable efficiency, leading to the a reduction on material consumption.\n\nTo create active electronic elements, the first key step was to chemically dope a semiconductor nanowire. This has already been done to individual nanowires to create p-type and n-type semiconductors.\n\nThe next step was to find a way to create a p–n junction, one of the simplest electronic devices. This was achieved in two ways. The first way was to physically cross a p-type wire over an n-type wire. The second method involved chemically doping a single wire with different dopants along the length. This method created a p-n junction with only one wire.\n\nAfter p-n junctions were built with nanowires, the next logical step was to build logic gates. By connecting several p-n junctions together, researchers have been able to create the basis of all logic circuits: the AND, OR, and NOT gates have all been built from semiconductor nanowire crossings.\n\nIn August 2012, researchers reported constructing the first NAND gate from undoped silicon nanowires. This avoids the problem of how to achieve precision doping of complementary nanocircuits, which is unsolved. They were able to control the Schottky barrier to achieve low-resistance contacts by placing a silicide layer in the metal-silicon interface.\n\nIt is possible that semiconductor nanowire crossings will be important to the future of digital computing. Though there are other uses for nanowires beyond these, the only ones that actually take advantage of physics in the nanometer regime are electronic.\n\nIn addition, nanowires are also being studied for use as photon ballistic waveguides as interconnects in quantum dot/quantum effect well photon logic arrays. Photons travel inside the tube, electrons travel on the outside shell.\n\nWhen two nanowires acting as photon waveguides cross each other the juncture acts as a quantum dot.\n\nConducting nanowires offer the possibility of connecting molecular-scale entities in a molecular computer. Dispersions of conducting nanowires in different polymers are being investigated for use as transparent electrodes for flexible flat-screen displays.\n\nBecause of their high Young's moduli, their use in mechanically enhancing composites is being investigated. Because nanowires appear in bundles, they may be used as tribological additives to improve friction characteristics and reliability of electronic transducers and actuators.\n\nBecause of their high aspect ratio, nanowires are also uniquely suited to dielectrophoretic manipulation, which offers a low-cost, bottom-up approach to integrating suspended dielectric metal oxide nanowires in electronic devices such as UV, water vapor, and ethanol sensors.\n\nNanowire lasers are nano-scaled lasers with potential as optical interconnects and optical data communication on chip. Nanowire lasers are built from III–V semiconductor heterostructures, the high refractive index allows for low optical loss in the nanowire core. Nanowire lasers are subwavelength lasers of only a few hundred nanometers. Nanowire lasers are Fabry–Perot resonator cavities defined by the end facets of the wire with high-reflectivity, recent developments have demonstrated repetition rates greater than 200 GHz offering possibilities for optical chip level communications.\n\nIn an analogous way to FET devices in which the modulation of conductance (flow of electrons/holes) in the semiconductor, between the input (source) and the output (drain) terminals, is controlled by electrostatic potential variation (gate-electrode) of the charge carriers in the device conduction channel, the methodology of a Bio/Chem-FET is based on the detection of the local change in charge density, or so-called “field effect”, that characterizes the recognition event between a target molecule and the surface receptor.\n\nThis change in the surface potential influences the Chem-FET device exactly as a ‘gate’ voltage does, leading to a detectable and measurable change in the device conduction. When these devices are fabricated using semiconductor nanowires as the transistor element the binding of a chemical or biological species to the surface of the sensor can lead to the depletion or accumulation of charge carriers in the \"bulk\" of the nanometer diameter nanowire i.e. (small cross section available for conduction channels). Moreover, the wire, which serves as a tunable conducting channel, is in close contact with the sensing environment of the target, leading to a short response time, along with orders of magnitude increase in the sensitivity of the device as a result of the huge S/V ratio of the nanowires.\n\nWhile several inorganic semiconducting materials such as Si, Ge, and metal oxides (e.g. In2O3, SnO2, ZnO, etc.) have been used for the preparation of nanowires, Si is usually the material of choice when fabricating nanowire FET-based chemo/biosensors.\n\nSeveral examples of the use of silicon nanowire(SiNW) sensing devices include the ultra sensitive, real-time sensing of biomarker proteins for cancer, detection of single virus particles, and the detection of nitro-aromatic explosive materials such as 2,4,6 Tri-nitrotoluene (TNT) in sensitives superior to these of canines.\nSilicon nanowires could also be used in their twisted form, as electromechanical devices, to measure intermolecular forces with great precision.\n\nGenerally, the charges on dissolved molecules and macromolecules are screened by dissolved counterions, since in most cases molecules bound to the devices are separated from the sensor surface by approximately 2–12 nm (the size of the receptor proteins or DNA linkers bound to the sensor surface). As a result of the screening, the electrostatic potential that arises from charges on the analyte molecule decays exponentially toward zero with distance. Thus, for optimal sensing, the Debye length must be carefully selected for nanowire FET measurements.\nOne approach of overcoming this limitation employs fragmentation of the antibody-capturing units and control over surface receptor density, allowing more intimate binding to the nanowire of the target protein. This approach proved useful for dramatically enhancing the sensitivity of cardiac biomarkers (e.g. Troponin) detection directly from serum for the diagnosis of acute myocardial infarction.\n\n"}
{"id": "57363360", "url": "https://en.wikipedia.org/wiki?curid=57363360", "title": "Nanowood", "text": "Nanowood\n\nNanowood is heat-insulating material made from wood that is considered a slightly better insulator than Styrofoam. Unlike Styrofoam, the material is more environmentally friendly and biodegradable. It is considered light, strong and created entirely from stripped-down wood fibers.\n\nThe material was invented by engineer, Liangbing Hu and his team at University of Maryland, College Park. The material \"when exposed to the solar spectrum\" reflected approximately 95% of radiation energy absorbing only approximately 2%. Silica aerogel \"absorbed approximately 20% and transmits approximately 60% of the radiative heat\" according to study authors. Nanowood could potentially save \"billions in energy costs\" according to Tian Li, a team member.\n\n"}
{"id": "6161283", "url": "https://en.wikipedia.org/wiki?curid=6161283", "title": "Nonthermal plasma", "text": "Nonthermal plasma\n\nA nonthermal plasma, cold plasma or non-equilibrium plasma is a plasma which is not in thermodynamic equilibrium, because the electron temperature is much hotter than the temperature of heavy species (ions and neutrals). As only electrons are thermalized, their Maxwell-Boltzmann velocity distribution is very different than the ion velocity distribution. When one of the velocities of a species does not follow a Maxwell-Boltzmann distribution, the plasma is said to be non-Maxwellian.\n\nA kind of common nonthermal plasma is the mercury-vapor gas within a fluorescent lamp, where the \"electron gas\" reaches a temperature of while the rest of the gas, ions and neutral atoms, stays barely above room temperature, so the bulb can even be touched with hands while operating.\n\nIn the context of food processing, a nonthermal plasma (NTP) or cold plasma is specifically an antimicrobial treatment being investigated for application to fruits, vegetables and other foods with fragile surfaces. \nThese foods are either not adequately sanitized or are otherwise unsuitable for treatment with chemicals, heat or other conventional food processing tools. While the applications of nonthermal plasma were initially focused on microbiological disinfection, newer applications such as enzyme inactivation, protein modification and pesticide dissipation are being actively researched. Nonthermal plasma also sees increasing use in the sterilization of teeth and hands, in hand dryers as well as in self-decontaminating filters.\n\nThe term \"cold plasma\" has been recently used as a convenient descriptor to distinguish the one-atmosphere, near room temperature plasma discharges from other plasmas, operating at hundreds or thousands of degrees above ambient (see ). Within the context of food processing the term \"cold\" can potentially engender misleading images of refrigeration requirements as a part of the plasma treatment. However, in practice this confusion has not been an issue. \"Cold plasmas\" may also loosely refer to weakly ionized gases (degree of ionization < 0.01%).\n\nThe nomenclature for nonthermal plasma found in the scientific literature is varied. In some cases, the plasma is referred to by the specific technology used to generate it (\"gliding arc\", \"plasma pencil\", \"plasma needle\", \"plasma jet\", \"dielectric barrier discharge\", \"Piezoelectric direct discharge plasma\", etc.), while other names are more generally descriptive, based on the characteristics of the plasma generated (\"one atmosphere uniform glow discharge plasma\", \"atmospheric plasma\", \"ambient pressure nonthermal discharges\", \"non-equilibrium atmospheric pressure plasmas\", etc.). The two features which distinguish NTP from other mature, industrially applied plasma technologies, is that they are 1) nonthermal and 2) operate at or near atmospheric pressure.\n\nAn emerging field adds the capabilities of nonthermal plasma to dentistry and medicine.\n\nMagnetohydrodynamic power generation, a direct energy conversion method from a hot gas in motion within a magnetic field was developed in the 1960s and 1970s with pulsed MHD generators known as shock tubes, using non-equilibrium plasmas seeded with alkali metal vapors (like caesium, to increase the limited electrical conductivity of gases) heated at a limited temperature of 2000 to 4000 kelvins (to protect walls from thermal erosion) but where electrons were heated at more than 10,000 kelvins.\n\nA particular and unusual case of \"inverse\" nonthermal plasma is the very high temperature plasma produced by the Z machine, where ions are much hotter than electrons.\n\nAerodynamic active flow control solutions involving technological nonthermal weakly ionized plasmas for subsonic, supersonic and hypersonic flight are being studied, as plasma actuators in the field of electrohydrodynamics, and as magnetohydrodynamic converters when magnetic fields are also involved.\n\nStudies conducted in wind tunnels involve most of the time low atmospheric pressure similar to an altitude of 20–50 km, typical of hypersonic flight, where the electrical conductivity of air is higher, hence non-thermal weakly ionized plasmas can be easily produced with a fewer energy expense.\n\nAtmospheric pressure non-thermal plasma can be used to promote chemical reactions. Collisions between hot temperature electrons and cold gas molecules can lead to dissociation reactions and the subsequent formation of radicals. This kind of discharge exhibits reacting properties that are usually seen in high temperature discharge systems. Non-thermal plasma is also used in conjunction with a catalyst to further enchance the chemical conversion of reactants or to alter the products chemical composition.\n\nAmong the different application fields, there are:\n\n\nThe coupling between the two different mechanisms can be done in two different ways:\n\n\nIn the first case the catalytic reactor is placed after the plasma chamber. This means that only the long-lived species can reach the catalyst surface and react, while short-lived radicals, ions and excited species decay in the first part of the reactor. As an example, the oxygen ground state atom O(3P) has a lifetime of about 14 μs in a dry air atmospheric pressure plasma. This means that only a small region of the catalyst is in contact with active radicals. In a such two-stage set-up, the main role of the plasma is to alter the gas composition fed to the catalytic reactor. In a PEC system, synergistic effects are greater since short-lived excited species are formed near the catalyst surface. The way the catalyst is inserted in the PEC reactor influence the overall performance. It can be placed inside the reactor in different ways:\n\n\nPacked bed plasma-catalytic reactor are commonly used for fundamental studies and a scale-up to industrial applications is difficult since the pressure drop increase with the flow rate.\n\nIn a PEC system, the way the catalyst is positioned in relation to the plasma can affect the process in different ways. The catalyst can positively influence the plasma and vice versa resulting in an output that cannot be obtained using each process individually. The synergy that is established is ascribed to different cross effects\n\n\n\nCatalyst effects on plasma are mostly related to the presence of a dielectric material inside the discharge region and do not necessarily require the presence of a catalyst.\n\n"}
{"id": "308056", "url": "https://en.wikipedia.org/wiki?curid=308056", "title": "Overhead projector", "text": "Overhead projector\n\nAn overhead projector is a variant of slide projector that is used to display images to an audience. The name is often abbreviated to OHP.\n\nAn overhead projector works on the same principle as a 35mm slide projector, in which a focusing lens projects light from an illuminated slide onto a projection screen where a real image is formed. However some differences are necessitated by the much larger size of the transparencies used (generally the size of a printed page), and the requirement that the transparency be placed face up (and readable to the presenter). For the latter purpose, the projector includes a mirror just before or after the focusing lens to fold the optical system toward the horizontal. That mirror also accomplishes a reversal of the image in order that the image projected onto the screen corresponds to that of the slide as seen by the presenter looking down at it, rather than a mirror image thereof. Therefore, the transparency is placed face up (toward the mirror and focusing lens), in contrast with a 35mm slide projector or film projector (which lack such a mirror) where the slide's image is non-reversed on the side \"opposite\" the focusing lens.\n\nThe device has sometimes been called a \"Belshazzar\", after Belshazzar's feast (\"In the same hour came forth fingers of a man's hand, and wrote over against the candlestick upon the plaister of the wall of the king's palace: and the king saw the part of the hand that wrote\" ()).\n\nBecause the focusing lens (typically less than 10 cm [4 in] in diameter) is much smaller than the transparency, a crucial role is played by the optical condenser which illuminates the transparency. Since this requires a large optical lens (at least the size of the transparency) but may be of poor optical quality (since the sharpness of the image does not depend on it), a Fresnel lens is employed. The Fresnel lens is located at (or is part of) the glass plate on which the transparency is placed, and serves to redirect most of the light hitting it into a converging cone toward the focusing lens. Without such a condenser at that point, most of the light would miss the focusing lens (or it would have to be very large and prohibitively expensive). Additionally, mirrors or other condensing elements below the Fresnel lens serve to increase the portion of the light bulb's output which reaches the Fresnel lens in the first place. In order to provide sufficient light on the screen, a high intensity bulb is used which must be fan cooled.\n\nOverhead projectors normally include a manual focusing mechanism which raises and lowers the position of the focusing lens (including the folding mirror) in order to adjust the object distance (optical distance between the slide and the lens) to focus at the chosen image distance (distance to the projection screen) given the fixed focal length of the focusing lens. This permits a range of projection distances.\n\nIncreasing (or decreasing) the projection distance increases (or decreases) the focusing system's magnification in order to fit the projection screen in use (or sometimes just to accommodate the room setup). Increasing the projection distance also means that the same amount of light is spread over a larger screen, resulting in a dimmer image. With a change in the projection distance, the focusing must be readjusted for a sharp image. However, the condensing optics (Fresnel lens) is optimized for one particular vertical position of the lens, corresponding to one projection distance. Therefore, when it is focused for a greatly different projection distance, part of the light cone projected by the Fresnel lens towards the focusing lens misses that lens. This has the greatest effect towards the outer edges of the projected image, so that one typically sees either blue or brown fringing at the edge of the screen when the focus is towards an extreme. Using the projector near its recommended projection distance allows a focusing position where this is avoided and the intensity across the screen is approximately uniform.\n\nThe lamp technology of an overhead projector is typically very simple compared to a modern LCD or DLP video projector. Most overheads use an extremely high-power halogen lamp that may consume up to 750 watts. A high-flow blower is required to keep the bulb from melting due to the heat generated, and this blower is often on a timer that keeps it running for a period after the light is extinguished.\n\nFurther, the intense heat accelerates failure of the high intensity lamp, often burning out in less than 100 hours, requiring replacement. In contrast, a modern LCD or DLP projector uses an arc lamp which has a higher luminous efficacy and lasts for thousands of hours. A drawback of that technology is the warm up time required for arc lamps.\n\nOlder overhead projectors used a tubular quartz bulb which was mounted above a bowl-shaped polished reflector. However, because the lamp was suspended above and outside the reflector, a large amount of light was cast to the sides inside the projector body that was wasted, thus requiring a higher power lamp for sufficient screen illumination. More modern overhead projectors use an integrated lamp and conical reflector assembly, allowing the lamp to be located deep within the reflector and sending a greater portion of its light towards the Fresnel lens; this permits using a lower power lamp for the same screen illumination.\n\nA useful innovation for overhead projectors with integrated lamps/reflectors is the quick-swap dual-lamp control, allowing two lamps to be installed in the projector in movable sockets. If one lamp fails during a presentation the presenter can merely move a lever to slide the spare into position and continue with the presentation, without needing to open the projection unit or waiting for the failed bulb to cool before replacing it.\n\nSome ancient projectors like the magic lantern can be regarded as predecessors of the overhead projector. The steganographic mirror possibly came closest to how the overhead projector was used.\n\nGerman Jesuit scholar Athanasius Kircher's 1645 book \"Ars Magna Lucis et Umbrae\" included a description of his invention, the \"Steganographic Mirror\": a primitive projection system with a focusing lens and text or pictures painted on a concave mirror reflecting sunlight, mostly intended for long distance communication. In 1654 Belgian Jesuit mathematician André Tacquet used Kircher's technique to show the journey from China to Belgium of Italian Jesuit missionary Martino Martini. It is unknown how exactly Tacquet used Kircher's system, but it is imaginable that he drew pictures on the projecting mirror while details of the journey were explained.\n\nFrench physicist Edmund Becquerel developed the first known overhead projection apparatus in 1853. It was demonstrated by French instrument maker and inventor Jules Duboscq in 1866.\n\nAn overhead projector designed by American scientist Henry Morton was marketed around 1880 as a \"vertical lantern\".\n\nThe use of transparent sheets for overhead projection, called viewfoils or viewgraphs, was largely developed in the United States.\n\nOverhead projectors were introduced into U.S. military training during World War II. After the war they were used at schools like the U.S. Military Academy.\n\nOverhead projectors were used early on for police work with a cellophane roll over a 9-inch stage, allowing facial characteristics to be rolled across the stage.\n\nAs the demand for projectors grew, Buhl Industries was founded in 1953, and became the leading US contributor for several optical refinements for the overhead projector and its projection lens.\n\nOverhead projectors began to be widely used in schools and businesses in the late 1950s and early 1960s.\n\nIn the late 1950s Roger Appeldorn was challenged by his boss at 3M to find a use for the transparencies that were the waste of their color copy process. Appeldorn developed a process for the projection of transparent sheets that led to 3M’s first marketable transparency film. The Strategic Air Command base in Omaha was one of the first big clients, using circa 20,000 sheets per month. 3M then decided to develop their own overhead projector instead of the one they had been selling until then, which was produced by an outside manufacturer. It took several prototypes before a cost-effective, small and foldable version could be presented on January 15, 1962. It had a new fresnel lens made with a structured-surface plastic, much better than other plastic lenses and much cheaper than glass.\n\nIn 1957, the United States' first Federal Aid to Education program stimulated overhead sales which remained high up to the late 1990s and into the 21st Century.\n\nThe overhead projector facilitates an easy low-cost interactive environment for educators. Teaching materials can be pre-printed on plastic sheets, upon which the educator can directly write using a non-permanent, washable color marking pen. This saves time, since the transparency can be pre-printed and used repetitively, rather than having materials written manually before each class.\n\nThe overhead is typically placed at a comfortable writing height for the educator and allows the educator to face the class, facilitating better communication between the students and teacher. The enlarging features of the projector allow the educator to write in a comfortable small script in a natural writing position rather than writing in an overly large script on a blackboard and having to constantly hold his arm out in midair to write on the blackboard.\n\nWhen the transparency sheet is full of written or drawn material, it can simply be replaced with a new, fresh sheet with more pre-printed material, again saving class time vs a blackboard that would need to be erased and teaching materials rewritten by the educator. Following the class period, the transparencies are easily restored to their original unused state by washing off with soap and water.\n\nIn the early 1980s–1990s, overhead projectors were used as part of a classroom computer display/projection system. A liquid-crystal panel mounted in a plastic frame was placed on top of the overhead projector and connected to the video output of the computer, often splitting off the normal monitor output. A cooling fan in the frame of the LCD panel would blow cooling air across the LCD to prevent overheating that would fog the image.\n\nThe first of these LCD panels were monochrome-only, and could display NTSC video output such as from an Apple II computer or VCR. In the late 1980s color models became available, capable of \"thousands\" of colors (16-bit color), for the color Macintosh and VGA PCs. The displays were never particularly fast to refresh or update, resulting in the smearing of fast-moving images, but it was acceptable when nothing else was available.\n\nThe Do-It-Yourself community has started using this idea to make low-cost home theater projectors. By removing the casing and backlight assembly of a common LCD monitor, one can use the exposed LCD screen in conjunction with the overhead projector to project the contents of the LCD screen to the wall at a much lower cost than with standard LCD projectors. Due to the mirroring of the image in the head of the overhead projector, the image on the wall is \"re-flipped\" to where it would be if one was looking at the LCD screen normally.\n\nOverhead projectors were once a common fixture in most classrooms and business conference rooms, but today are slowly being replaced by document cameras, dedicated computer projection systems and interactive whiteboards. Such systems allow the presenter to project video directly from a computer file, typically produced using software such as Microsoft PowerPoint and LibreOffice. Such presentations can also include animations, interactive components, or even video clips, with ease of paging between slides. The relatively expensive printing or photocopying of color transparencies is eliminated.\n\nThe primary reason for this gradual replacement is the deeply ingrained use of computing technology in modern society and the inability of overheads to easily support the features that modern users demand. While an overhead can display static images fairly well, it performs poorly at displaying moving images. The LCD video display panels that were once used as an add-on to an overhead projector have become obsolete, with that combination of display technology and projection optics now optimally integrated into a modern video projector.\n\nThe standards of users have also increased, so that a dim, fuzzy overhead projection that is too bright in the center and too dim around the edges is no longer acceptable. The optical focus, linearity, brightness and clarity of an overhead generally cannot match that of a video projector . Video projectors use extremely small picture generation mechanisms, allowing for precision optics that far exceed the plastic fresnel lens' optical performance. They also include additional optics that eliminate the \"hotspot\" in the center of the screen directly above the light source, so that the brightness is uniform everywhere on the projection screen.\n\nCritics feel that there are some downsides as these technologies are more prone to failure and have a much steeper learning curve for the user than a standard overhead projector.\n\n\n"}
{"id": "26576217", "url": "https://en.wikipedia.org/wiki?curid=26576217", "title": "Patrick Whitehouse", "text": "Patrick Whitehouse\n\nPatrick Bruce Whitehouse OBE (1922–1993) was one of the pioneers of railway preservation, when he helped save the Talyllyn Railway in 1951. He also led the restoration to working order of several of Britain's steam locomotives after they were replaced by diesel locomotion in the 1960s.\n\nPatrick Bruce Whitehouse was born in Warwick, the son of Cecil Whitehouse who was the co-owner of the family construction firm B. Whitehouse and Sons based in Birmingham, West Midlands and his wife Phyllis (nee Bucknell), who was a descendant of the founding family of the shipping line that became the Ellerman & Bucknell Steamship Company.\n\nEducated at Warwick School, on graduation he was due to join the family firm, but the outbreak of World War II meant that he tried to follow his ambition of becoming a pilot. With less than perfect eyesight, the Royal Air Force approved his training as a navigator, which he completed in Canada under the Empire Air Training Scheme. Returning to the UK he was posted to No. 15 Squadron RAF, completing three tours each of 30 flights over Nazi Germany and occupied western-Europe.\n\nAssigned to RAF Transport Command, he was then posted by them to what was consider the less stressful Middle East. Shot down over the Mediterranean, he was the sole survivor of a crew of four, picked-up four days later from his RAF issue rubber dinghy by a Greek freighter. By the time of his demobilisation in 1946, he had reached the rank of Squadron Leader.\n\nPost-WW2, Whitehouse returned to the family firm, initially becoming number three behind his father and uncle. He succeeded his uncle as Chairman, but sold the business in 1964 to Holland Hannen & Cubbits Ltd. This allowed him to serve on their board until 1971, being appointed OBE in the 1960s for his campaign for safety regulations on building sites.\n\nWhitehouse was a member of what later became chronicled as the \"Birmingham Railway Mafia\", a group centred around a core of steam railway enthusiasts who were members of both the Birmingham Locomotive Club and West Midlands branch of the Stephenson Locomotive Society.\n\nNewly married, Whitehouse bought a Rolleiflex camera, and began chronicling the demise of steam and the railways in some of his favourite railway locations in the West Midlands. These were supplemented by photographs from SLS tours organised by the Birmingham Mafia, including one Whitehouse co-arranged on the Ashover Light Railway, Derbyshire. Inspired by H Fayle's book \"Narrow Gauge Railways of Ireland\", Whitehouse and his wife toured Ireland in the summers of the early 1950s, in 1952 accompanied by fellow publisher Ian Allan, with the group especially enjoying chasing trains along the Tralee and Dingle Railway. These tours are also where Whitehouse met fellow enthusiasts who would later play a key role in preservation of the UK's steam railway heritage, including Ivo Peters, Henry S Orbach and Peter Allen, later knighted for his chairmanship of Imperial Chemical Industries. With a good volume and sufficient personal confidence in his own photographic ability, in the mid-1950s Whitehouse submitted a selected portfolio to the Royal Photographic Society, and in 1958 co-authored with John Powell published a book on the Tralee & Dingle.\n\nOn 11 October 1950, as part of the Birmingham Railway Mafia, Whitehouse was one of the attendees at a meeting at the Imperial Hotel, Birmingham, called and chaired by his friend Tom Rolt to save the Talyllyn Railway, a narrow gauge slate railway in mid-Wales. At the end of the meeting, having agreed to form the Talyllyn Railway Preservation Society (TRPS), the world's first railway preservation society, Whitehouse accepted the position of Secretary. He later became vice-president of the society, used his construction knowledge and connections with regards preservation and development of the permanent way, and became a skilled steam engine fireman on the line.\n\nFriends from his time on the Talyllyn soon became enthused by the idea of reviving the Ffestiniog Railway in North Wales, which Whitehouse also became involved in the early stages of. In 1968, a group of business people including Whitehouse made an offer to BR to purchase the Vale of Rheidol Railway, which was turned down by the then governing socialist Labour Party government.\n\nIn 1962/3, B Whitehouse & Sons Ltd had been commissioned to rebuild a bridge and walls at Walsall railway station. With the oncoming introduction of diesel and electric services to the area, and subsequent rationalisation and simplification of the required infrastructure, the BR specification changed on numerous occasions resulting in large cost rises. At a subsequent meeting between Whitehouse and the regional manager Stanley Raymond (who as Sir Stanley Raymond, succeeded Dr Richard Beeching as Chairman of the Board of British Railways), the cost issues were resolved, and Raymond asked if he could help Whitehouse with anything else? Whitehouse responded that he wanted to buy a steam locomotive, which after within the room discussions with his staff, Raymond agreed to.\n\nWhitehouse and fellow Talyllyn member Pat Garland secured Great Western Railway 4500 Class \"Small Prairie\" Tank No.4555 for £750, which included a light overhaul at Swindon Works, a spare boiler, a wagon load of spares and free delivery to Tyseley TMD. Subsequently utilised lightly by BR around the Birmingham area, Whitehouse and his friends wanted to run the locomotive on a GWR branch line. Having originally focused on the already closed GWR Kingswear branch in Devon, with track already being removed they then focused on the Dart Valley Railway from Buckfastleigh to Ashburton. Operated from the outset – as had the Talyllyn and the Ffestiniog – as a commercial railway, in the first year the Dart Valley carried 60,000 passengers at a profit.\n\nWith an agreement already in place to purchase Castle Class No.7029 \" Clun Castle \" from BR for its scrap price of £2,400, in January 1966 Whitehouse and John Evans topped up the fund to allow the transaction to be completed.\n\nPurchased under the Standard Gauge Steam Trust, Whitehouse was instrumental in subsequently securing a lease from BR on a residual area of the soon to be demolished GWR Tyseley roundhouse, encompassing the coaling stage, ash shelter and watering tower. Unable to negotiate at the time a return to the BR mainline with Dr Beeching, these residual MDP elements later became the Birmingham Railway Museum, a site equipped to preserve and maintain main line steam locomotives. There he led a team which restored the LMS Jubilee Class No.5593 \"Kolhapur\". By the 1970s the Birmingham Railway Museum housed up to 15 locomotives, and now leases engines to preservation lines throughout Britain. It is also the home of the \" Shakespeare Express \" on the BR main line from Birmingham to Stratford.\n\nIn 1952, with Tom Rolt undertaking the writing and Whitehouse contributing the photographs and research, Whitehouse co-authored his first book \"Lines of Character\". Five years later he wrote his first solo book \"Narrow Gauge Album\", published by Ian Allan Publishing. Whitehouse became the author or co-author of 53 books on railways, and built up a collection of more than a quarter of a million photographs of British and foreign railways. In the 1980s, his travels in China led to a long-standing friendship with the China Railway Publishing House in Beijing, and a treaty of friendship between Birmingham and the north-eastern city of Changchun, Manchuria.\n\nPublished under the title Millbrook House, the publishers collection of over 250,000 photographs – from both those taken by Whitehouse, and the collections he purchased – became known as the Millbrook House collection. After his death, the majority of the photographs that he took himself known as the Whitehouse Collection was donated to the nation, acquired by the National Railway Museum.\n\nWhitehouse had inherited a Kodak film camera, on which he chronicled his excursions across the UK, Ireland and mainland Europe. In 1956, he produced his first commercial film with John Adams, about the railways of the Isle of Man. Showing the production at a local film club in Birmingham, they met BBC Children's producer Peggy Bacon, who commissioned the pair to present and produce some full-length railway programmes for Children. Whitehouse and John Adams subsequently filmed, produced and co-presented the BBC1 Children's programme \"Railway Roundabout\". Although the films exist still to this day, the programme commentaries and links were live to air and hence not recorded. Film archivist John Huntley estimates the pair were responsible for 137 films, of which around 100 were shown on \"Railway Roundabout\".\n\nHis book, \"China By Rail\" was co-authored by his daughter Maggy Whitehouse. His son Michael Whitehouse is a lawyer specialising in rail transport, and non-executive director of the Rail Freight Group.\n"}
{"id": "50305913", "url": "https://en.wikipedia.org/wiki?curid=50305913", "title": "Platform display", "text": "Platform display\n\nA platform display, destination display or train describer (British English) is supplementing the destination sign on arriving trains giving passengers an advance information. Historically they did only show the next destination and sometimes the type of train. In later usage they were replaced by passenger information display systems (PIDS) allowing for real-time passenger information.\n\nThe first railway stations had only a time table for passenger information. On larger stations the train porters would help passengers to board the correct train matching with their ticket. They were supervised by a station manager that would handle the security requirements for each departing train. The first help in that task was a bell to remind passengers to board the train in time which on smaller stations does also announce the next train. Different directions would then be called out on the platform. At the time that trains grew into mass transport systems this was not enough anymore. The train handling became optimized to allow for less than a minute from arrival to departure at a stop which triggered the usage of loudspeakers and platform displays. The mechanical types were not standardized and every station had its own range of facilities as they seemed useful.\n\nA train describer is originally an additional apparatus at British railways that ensures that the identity of each train is displayed on the signalbox panel together with the indication of that train's presence, usually offering routing information. This routing information would then be passed through to the platform display for passenger information. Technically the train reporting number was pushed from one signal box to the next. A series of interconnected signal boxes form a train describer system (TDS) transferring train describer data to be shown on the respective signalbox panel in a train describer display. The electric relay interlocking boxes were later replaced by electronic control boards where the train indication is just a text element on the video display.\n\nIn a centralized electronic interlocking the current train location and identification is used to predict the arrival at the next stop allowing for countdown clocks for passenger information. The term \"passenger information display\" has widely replaced the term platform display as station design can include different types of information displays - like a departure board in the main hall, a shorter list in the tunnels and an announcement of the next train on each platform side - which all get their information from a central electronic railway control system. Additionally passenger information displays have come into use for bus and tram stops as well where the destination display equipment is technically similar.\n"}
{"id": "4279364", "url": "https://en.wikipedia.org/wiki?curid=4279364", "title": "Primasheet", "text": "Primasheet\n\nPrimasheet is a rubberized sheet explosive material similar to Detasheet. Manufactured by Ensign-Bickford Aerospace & Defense Company Primasheet comes in two varieties: Primasheet 1000 is PETN based and Primasheet 2000 is RDX based. Both are waterproof and are supplied in continuous rolls.\n\nPrimasheet 1000 is PETN based, and contains 65% PETN, 8% nitrocellulose, and 27% plasticizer. Primasheet 1000 is olive green colored, and manufactured in 1.0, 1.5, 2, 3, 4, 5, 6, and 8 mm thicknesses.\n\nPrimasheet 2000 is an RDX-based rubberized sheet explosive. It contains 88.2% RDX with the remainder plasticizer. It is equally as powerful as C4.\n\nA British military explosive, also manufactured by Ensign-Bickford. This is very similar or identical to commercial Primasheet 2000.\n"}
{"id": "2740122", "url": "https://en.wikipedia.org/wiki?curid=2740122", "title": "Prix Guzman", "text": "Prix Guzman\n\nThe Prix Pierre Guzman (Pierre Guzman Prize) was the name given to two prizes, one astronomical and one medical. Both were established by the will of Anne Emilie Clara Goguet (died June 30, 1891), wife of Marc Guzman, and named after her son Pierre Guzman.\n\nThis prize was a sum of 100,000 francs, to be given to a person who succeeded in communicating with a celestial body, other than Mars, and receiving a response. Until this occurred, the will also allowed for the accumulated interest on the 100,000 francs to be given, every five years, to a person who had made significant progress in astronomy. The prize was to be awarded by the French Académie des sciences. Pierre Guzman had been interested in the work of Camille Flammarion, the author of \"La planète Mars et ses conditions d'habitabilité\" (The Planet Mars and Its Conditions of Habitability, 1892). Communication with Mars was specifically exempted as many people believed that Mars was inhabited at the time and communication with that planet would not be a difficult enough challenge. The prize was later announced in 1900 by the French Académie des sciences.\n\nThe five-yearly prize of interest was awarded, starting in 1905, as follows:\n\nNikola Tesla claimed in 1937 that he should receive the prize for \"his discovery relating to the interstellar transmission of energy.\" The prize was awarded to the crew of Apollo 11 in 1969.\n\nThis prize was a sum of 50,000 francs, to be awarded by the French Académie de médecine, to be given to a person who succeeded in developing an effective treatment for the most common forms of heart disease. Until this occurred, the will also allowed for the accumulated interest to be given yearly to someone who had made progress in heart disease.\n\nThe yearly prize of interest was awarded as follows:\n\n"}
{"id": "3089392", "url": "https://en.wikipedia.org/wiki?curid=3089392", "title": "Project RESISTANCE", "text": "Project RESISTANCE\n\nProject RESISTANCE was a domestic espionage operation coordinated under the Domestic Operations Division (DOD) of the CIA. Its purpose was to collect background information on groups around the U.S. that might pose threats to CIA facilities and personnel. From 1967 to 1973, many local police departments, college campus staff members, and other independent informants collaborated with the CIA to keep track of student radical groups that opposed the U.S. government's foreign policies on Vietnam. Project RESISTANCE and its twin program, Project MERRIMAC were both coordinated by the CIA Office of Security. In addition, the twin projects were branch operations that relayed civilian information to their parent program, Operation CHAOS.\n\n\n"}
{"id": "49519822", "url": "https://en.wikipedia.org/wiki?curid=49519822", "title": "RAVPower", "text": "RAVPower\n\nRAVPower is an American consumer electronics brand founded in 2011 and owned by Sunvalleytek, which is a branch of the Chinese owned Sunvalley Group. The company's current product line is primarily focused on battery chargers, including portable powerbanks and solar chargers, shifting away from its previous focus on laptop and cell phone batteries. RAVPower products are sold through its own website and the Amazon Marketplace.\n\nOn December 18, 2015, RAVPower launched its Turbo+ 20100 mAh External Battery, the world's first portable powerbank to include both Qualcomm Quick Charge 3.0 technology and a USB-C port.\n\nPatented by RAVPower, products with iSmart technology automatically detect the optimal charging current for connected devices and adjust to their power needs. This allows for faster and smarter charging for smartphones and tablets, including the Apple iPhone and iPad.\n"}
{"id": "31543246", "url": "https://en.wikipedia.org/wiki?curid=31543246", "title": "RAVe Publications", "text": "RAVe Publications\n\nrAVe [Publications] is a digital news organization that covers the audiovisual industry – both commercial and residential. rAVe, originally Kayye Consulting Inc., was founded in 1998 by Gary Kayye. The company produces e-newsletters, blogs and video to provide opinionated news and commentary on the AV industry. rAVe is tailored to “AV Insiders” (dealers, integrators, consultants, designers, manufacturers, etc.) as opposed to end-users of the products and/or services.\n\nGary Kayye, a prominent member of the AV community, founded Kayye Consulting in 1998. Kayye graduated from the School of Journalism and Mass Communication with an emphasis on marketing and public relations at the University of North Carolina at Chapel Hill (where he serves as an adjunct faculty member). He has been in the industry since 1987 when he started at Extron Electronics before eventually moving to AMX. In addition to consulting, Kayye Consulting worked with AV professionals through training, development and marketing.\n\nIn 2003, Kayye Consulting published its first e-newsletter – Gary Kayye's rAVe – which largely covered commercial AV news. This e-newsletter eventually became the ProAV e-newsletter, and continues to cover the commercial AV sector. In 2004, Kayye Consulting added the HomeAV e-newsletter, in partnership with CEDIA, covering the residential sector.\n\nWith a larger focus on AV industry news coverage, Kayye Consulting Inc. became publicly known as rAVe [Publications] in 2008. rAVe [Publications] is a DBA of Kayye Consulting Inc.\n\nIn 2007, rAVe began using social media to enhance its coverage of the AV community. Beginning at InfoComm 2007 rAVe tweeted about new products, and after a positive response, started using social media as a critical aspect of their coverage. rAVe NOW was launched in 2008 to incorporate all of rAVe's social media efforts.\n\nToday, rAVe has six e-newsletters, two international partners and an extensive social media site in rAVe NOW.\n\nThe rAVe [Publications] website features content from all of its e-newsletters, as well as industry press releases, a job board, educational whitepapers and other educational opportunities, all to benefit members of the AV community.\n\nrAVe publishes six U.S.-based e-newsletters: ProAV, HomeAV, Rental [and Staging], ED [Education], [DS] Digital Signage and GreenAV. rAVe also has two international partners: rAVe Europe and rAVe Asia. rAVe is entirely digital, and there are no print versions of its newsletters. All e-newsletters are opt-in and are delivered with full content via email.\n\nrAVe [Publications] began using Twitter in 2007 at InfoComm, and launched rAVe NOW in 2008. rAVe NOW incorporates all aspects of rAVe's social media presence, including blogs, video, social networks (Facebook, LinkedIn, Twitter, YouTube), tradeshow coverage and the Social Media Blueprint.\n\nThe rAVe NOW blog includes articles from nearly ten bloggers, all experts in the AV industry. These bloggers comprise the rAVe BlogSquad, and cover a variety of aspects of AV. The rAVe BlogSquad includes Gary Kayye, Sara Abrons, Joel Rollins, Lee Distad, Dawn Meade, Jessica Spicer, Johnny Mota, Gina Sansivero and Dave Haynes. Bloggers cover a variety of topics from new products and industry trends to green issues and specialized AV topics.\n\nrAVe began filming and uploading videos at tradeshows so that those who weren’t able to attend could still keep up with product releases and manufacturer news. Over the years, they have expanded to include product introductions, recaps of events, informational text movies covering research, and entertainment series, such as Gary's Rants & rAVes. All videos can be found on the rAVe NOW website, as well as its YouTube channel.\n\nrAVe Original Videos include videos created by rAVe staff, focusing on different aspects of the AV industry. Videos include Gary's Rants & rAVes (a regular segment where rAVe founder Gary Kayye comments on the latest industry trends), as well as informational videos on different aspects of the AV industry.\n\nrAVe NOW uses a variety of social media platforms to connect members of the audiovisual community (both Home and Pro). These social networks not only allow rAVe to post original content in an interactive format, but they allow users to interact with one another to offer advice and insight.\n\nrAVe [Publications] honors manufacturers and companies with “Best of Show” Awards after each of the major tradeshows they cover (CEDIA Expo, InfoComm, Digital Signage Expo). These awards cover a variety of categories and are sent out in the e-newsletter relating to that tradeshow.\n\nExamples of specific awards are “DS [Digital Signage] Champs,\" “Best of CEDIA,” “Best of InfoComm” and “Up and Coming.” The “Up and Coming” award is occasionally given at the beginning of the year to companies and products that rAVe thinks are going to affect the coming year.\n\nrAVe [Publications] also provides a variety of educational literature – researched and written on specialized topics for targeted audiences. In addition to the Social Media Blueprint, rAVe produces educational videos and whitepapers, and gives live presentations at tradeshows and conferences.\n\nThe Social Media Blueprint, originally launched as a stand-alone document written in Feb. 2010, was re-launched in 2011 as a comprehensive website that serves as a digital media guide for members of the AV industry to get involved in B2B social media. The new website features a blog on social media, basic strategies, an overview of social media resources, and tools to help get readers started. The website also features a forum where AV professionals can interact with one another to share their stories and ask questions.\n\n"}
{"id": "56505215", "url": "https://en.wikipedia.org/wiki?curid=56505215", "title": "Range gate pull-off", "text": "Range gate pull-off\n\nRange gate pull-off (RGPO) is an electronic warfare technique used to break radar lock-on. The basic concept is to produce a pulse of radio signal similar to the one that the target radar would produce when it reflects off the aircraft. This second pulse is then increasingly delayed in time so that the radar's range gate begins to follow the false pulse instead of the real reflection, pulling it off the target.\n\nDoppler radars may not use range gates and instead select a single target by narrowly filtering frequencies on either side of the target's initial return. Against these radars, the related velocity gate pull-off can be used. These send a return signal that slowly changes in frequency, rather than time, hoping the radar's velocity gate will be pulled off the target in the same general fashion.\n\nPull-off belongs to the wider family of \"deceptive jamming\" concepts that use details of the target radar to their advantage, rather than attempting to simply overpower the radar's signal. Alternate names for \"pull-off\" include \"stealing\" and \"walk-off\". A related technique is angle deception jamming.\n\nEven the earliest radar systems included a system to highlight a single selected target for further analysis. For instance, the Gun-Laying Mark I, the British Army's first operational radar, used an on-screen cursor known as the \"strobe\" to highlight a single target. This worked by filtering out, or \"gating\", signals that were not within the strobe's short time period, typically a few microseconds, corresponding to a range of a few hundred meters. The signal within the strobe's window was then sent to secondary displays where two operators would determine the azimuth and elevation of that single target, by keeping its \"blip\" centered in their displays. Similar systems were used by many radars by the mid-war period.\n\nBy the end of the war, many experiments were being carried out on automatic target following, or radar lock-on. In these systems, the operator would select a target using the strobe, and then circuits in the radar would automatically track the target in azimuth and elevation. This eliminated the need for the additional operators. Since the target's range would continue to change as it moved, the circuitry also attempted to keep the strobe centered in range. Some systems automated even the strobing; the AI Mark V was designed for single-seat fighter aircraft where the pilot would be too busy to adjust the strobe, and instead had a second system to sweep the strobe through a wide range and then lock onto the first signal it saw.\n\nIn the post-war era the circuitry that produced the strobe and filtered out other returns became more widely known as a range gate.\n\nWhile testing a late-war radar design, the AI Mk. IX, a serious problem with the auto-follow system was found.\n\nWhile this system was being tested, Bomber Command was pressing the Air Ministry to use \"window\", better known today as chaff, as a radar countermeasure. Fighter Command pointed out that the Germans could easily copy the system and use it against England, potentially re-opening The Blitz. It was suggested that the AI Mk. IX would ignore window because it deccelerated rapidly after it was dropped, and would thus quickly pass out of the range gates and not be tracked. But exactly the opposite occurred in testing; the radar unerringly locked onto the window and the target disappeared from the display.\n\nRange gate pull-off is essentially an electronic version of window. Instead of producing the secondary return by dropping a packet of foil reflectors, the second return is created by a transponder in the target aircraft. The transponder initially responds as rapidly as possible to the radar's signal, producing a second blip that overlaps the original. Over a period of time it increasingly delays the return so that it falls \"behind\" the radar signal in time. The goal is to delay the signal so it counters the aircraft's motion, leaving a signal at what appears to be a (nearly) fixed location in space. If the radar was locked on to the aircraft, it may remain locked to this second pulse as the aircraft moves away from the original location. Eventually, the aircraft will fall outside the range gate and disappear, while the radar continues tracking the false signal. Thus, the false signal is said to pull the range gate off the target.\n\nThe system can be further improved through detailed control of the returned signal. One improvement is to have another signal sent out at \"zero time\", immediately on reception. This may require the transponder to listen to the radar's signal and determine its pulse repetition frequency so it can properly time the response to be exactly the same as the radar's reflection. This signal is then added to the one from the reflected radar signal, making the target signal stronger. The system then begins sending the second signal as well, at exactly the same strength as this doubled-up return, while at the same time reducing the false reflection as the second signal moves away in time. The result on the radar display is that the real signal fades back to its original smaller value, while the fake second signal remains a constant stronger strength. This makes the real return look like the fake.\n\nOne problem with a simple RGPO system is that the false signals will always appear further away from the radar, due to the slight delay in the transponder. On a plan-position indicator, the false signal will appear as a second dot at increasing distances from the first, which the operator can then manually strobe to regain lock. This can be offset by having the transponder store how rapidly the radar signals are being received, and then estimating the pulse repetition frequency so that it can predict future pulses. Then it can send its signal before the next pulse, allowing it to pull off in either direction and making manual re-lock more difficult.\n\nDoppler radars directly measure the target's velocity via the Doppler effect. In typical early implementations, the received signal was amplified and then sent into a bank of narrow-band filters, each one corresponding to a particular target velocity.\n\nIf an RGPO jammer responds to such a signal by sending out the same frequency it received, this additional signal will be sent into the same filter, adding to the original signal and making it stronger. It the transponder instead responds at a fixed frequency, it will fall into a different filter and can be easily distinguished. In either case, the original target return remains locked-on.\n\nModifying a transponder to deal with Doppler radars is easy, it simply requires it to be able to adjust its frequency. In this case, the system initially responds at the same frequency as the original signal, and then increasingly shifts the frequency over time in a manner similar to the RGPO case. This will cause a second signal to appear in adjacent filters, with no way to know which is the original. Since the frequency can be easily adjusted up or down, it does not have the added complication seen in RGPOs that want to pull-off in either direction.\n\nPulse-Doppler radars use both pulse timing and Doppler shifting to track targets, so by varying both the frequency and return timing, these can be pulled off as well. Such a transponder will continue to work against non-Doppler radars as well, as these generally have wide frequency response and continue to see the signal as long as its frequency shift does not become significant.\n\nThe effectiveness of the pull-off can be reduced if the radar changes its pulse repetition frequency, thereby making it difficult for the transponder to continue smoothly delaying the fake signal. Frequency agility has the same effect, as the transponder cannot guess what frequency to send out the fake signals on until it hears the one from the radar.\n\nDenying this capability means the signal from the transponder can only respond to signals after hearing them on its receiver. These signals will always represent returns from greater distances than the jammer aircraft. Pulse-to-pulse comparison techniques, like moving target indication, can be used to filter out these sorts of returns as they appear on the radar to be slower-moving targets.\n\n"}
{"id": "35161367", "url": "https://en.wikipedia.org/wiki?curid=35161367", "title": "Register memory architecture", "text": "Register memory architecture\n\nIn computer engineering, a register–memory architecture is an instruction set architecture that allows operations to be performed on (or from) memory, as well as registers. If the architecture allows all operands to be in memory or in registers, or in combinations, it is called a \"register plus memory\" architecture.\n\nIn a register–memory approach one of the operands for ADD operation may be in memory, while the other is in a register. This differs from a load/store architecture (used by RISC designs such as MIPS) in which both operands for an ADD operation must be in registers before the ADD.\n\nExamples of register memory architecture are IBM System/360, its successors, and Intel x86. Examples of register plus memory architecture are VAX and the Motorola 68000 family.\n\n"}
{"id": "7720731", "url": "https://en.wikipedia.org/wiki?curid=7720731", "title": "Roger L. Easton", "text": "Roger L. Easton\n\nRoger Lee Easton, Sr. (April 30, 1921 – May 8, 2014) was an American scientist/physicist who was the principal inventor and designer of the Global Positioning System, along with Ivan A. Getting and Bradford Parkinson. He was born in Craftsbury, Vermont.\n\nIn 1955, Easton co-wrote the Naval Research Laboratory's Project Vanguard proposal for a U.S. satellite program in competition with two other proposals, including a proposal from the U.S. Army prepared by Wernher Von Braun. The Eisenhower Administration selected Project Vanguard. In 1957, Easton invented the Minitrack tracking system to determine the Vanguard satellite's orbit. When Sputnik I was launched, Easton extended the system to actively follow unknown orbiting satellites.\n\nIn 1959, he designed the Naval Space Surveillance (NAVSPASUR) system. The Naval Space Surveillance System became the first system to detect and track all types of Earth-orbiting objects. It goes through the 33rd parallel, which is effectively coast to coast of the US.\n\nLater in his career at NRL, Easton conceived, patented, and led the development of essential enabling technologies for the United States Global Positioning System (GPS). During the 1960s and early 1970s he developed a time-based navigational system with passive ranging, circular orbits, and space-borne high precision clocks placed in satellites. The idea was tested with four experimental satellites: TIMATION I and II (in 1967 and 1969) and Navigation Technology Satellites (NTS) 1 and 2 (in 1974 and 1977). NTS-2 was the first satellite to transmit GPS signals.\n\nEaston was born in Craftsbury, Vermont, to Dr. Frank B. Easton, a physician, and Della Donnocker, a school teacher. He graduated from Middlebury College in 1943. He also attended the University of Michigan for 1 semester before joining the Naval Research Laboratory in 1943. At the Naval Research Laboratory he worked in the Radio Division on radar beacons and blind-landing systems. Easton also worked in the laboratory's Rocket-Sonde Branch which was dealing with space related research.\n\nEaston retired in 1980. In 1986, Easton ran for Governor and served 3 terms on the Board of the New Hampshire Electric Cooperative.\n\nEaston died on May 8, 2014, at the age of 93.\n\nGeorge W. Bush awarded Easton the National Medal of Technology for his \"extensive pioneering achievements in spacecraft tracking, navigation and timing technology that led to the development of the NAVSTAR-Global Positioning System (GPS)\" in 2006. The National Medal of Technology is the highest honor awarded for technology.\nOn March 31, 2010, Easton was inducted into the National Inventors Hall of Fame and presented the NIHF Medal of Honor for the development of TIMed navigATION (TIMATION - U.S. Patent 3,789,409) that provided both accurate position and precise time to terrestrial based observers, an important foundation for contemporary Global Positioning Systems.\n\nDuring his career at the Naval Research Laboratory, Easton was awarded:\n"}
{"id": "240105", "url": "https://en.wikipedia.org/wiki?curid=240105", "title": "Room temperature", "text": "Room temperature\n\nColloquially, room temperature is the range of air temperatures that most people prefer for indoor settings, which feel comfortable when wearing typical indoor clothing. As a medical definition, the range generally considered suitable for human occupancy is between 15 degrees Celsius (59 degrees Fahrenheit) and , though human comfort can extend beyond this range depending on humidity, air circulation and other factors. In certain fields, like science and engineering, and within a particular context, room temperature can mean different agreed-on ranges. In contrast, ambient temperature is the actual temperature of the air in any particular place, as measured by a thermometer. It may be very different from usual room temperature, for example an unheated room in winter.\n\n\"The American Heritage Dictionary of the English Language\" identifies room temperature as around , and the Oxford English Dictionary claims that it is \"conventionally taken as about \".\n\nOwing to variations in humidity and likely clothing, recommendations for summer and winter may vary; a suggested typical range for summer is , with that for winter being , although by other considerations the maximum should be below  – and to avoid sick building syndrome, below .\n\nSome studies have suggested that thermal comfort preferences of men and women may differ significantly, with women on average preferring higher ambient temperatures.\n\nThe World Health Organization's standard for comfortable warmth is for normal, healthy adults who are appropriately dressed. For those with respiratory problems or allergies, they recommend no less than , and for the sick, disabled, very old or very young, a minimum of .\n\nTemperature ranges are defined as \"room temperature\" for certain products and processes in industry, science, and consumer goods. For instance, for the shipping and storage of pharmaceuticals, the United States Pharmacopeia-National Formulary (USP-NF) defines \"controlled room temperature\" as between , with excursions between allowed, provided the mean kinetic temperature does not exceed . The European Pharmacopoeia defines it as being simply , and the Japanese Pharmacopeia defines \"ordinary temperature\" as , with room temperature being .\n\nPeople traditionally serve red wine at room temperature. This practice dates from before central heating, when room temperature in wine-drinking countries was considerably lower than it is today, usually in the range between and . The advice is therefore to serve the wine at, at most, about .\n\n"}
{"id": "21280291", "url": "https://en.wikipedia.org/wiki?curid=21280291", "title": "School of Everything", "text": "School of Everything\n\nSchool of Everything is an internet startup company founded in 2006 and based in London, UK. The stated purpose of School of Everything is to \"connect people who can teach with people who want to learn\".\n\nSchool of Everything was founded by Peter Brownell, Andy Gibson, Mary Harrington, Dougald Hine and Paul Miller. The site was funded by the Young Foundation, amongst others, and won a UK Catalyst award for the social use of technology and a New Statesman New Media Award 2008. In 2010, School of Everything was chosen by Becta and the Department for Business, Innovation and Skills as its new platform for adult informal learning in the UK.\n\nVia the website, learners are able to search for teachers in their area, and, similarly teachers can search for learners. Registration is free, with the site catering for teachers who charge for their lessons as well as those who offer lessons for free, or as part of a skill swap.\n\n\n"}
{"id": "27658643", "url": "https://en.wikipedia.org/wiki?curid=27658643", "title": "Scientific Working Group – Imaging Technology", "text": "Scientific Working Group – Imaging Technology\n\nThe Scientific Working Group on Imaging Technology was convened by the Federal Bureau of Investigation in 1997 to provide guidance to law enforcement agencies and others in the criminal justice system regarding the best practices for photography, videography, and video and image analysis. This group was terminated in 2015.\n\nAs technology has advanced through the years, law enforcement has needed to stay abreast of emerging technological advances and use these in the investigation of crime. A factor that is considered when new technology is used in these investigations is the determination of whether the use of that new technology will be admissible in court. The judicial system in the United States currently has two standards used in the determination of admissibility of testimony regarding scientific evidence; the Daubert Standard and the Frye Standard. These standards guide the courts in the admissibility of testimony derived from the use of new technologies and scientific techniques. The Federal Bureau of Investigation (FBI), seeking to address possible admissibility issues with such testimony, established Scientific Working Groups starting with the Scientific Working Group on DNA Analysis and Methods (SWGDAM) in 1988. The goal of these groups is to open lines of communication between law enforcement agencies and forensic laboratories around the world while providing guidance on the use of new and innovative technologies and techniques. This guidance can lead to admissibility of evidence and/or testimony, provided proper methods in the collection of evidence and its analysis are employed. In 2009, the National Academy of Sciences released a report entitled, \"Strengthening Forensic Science in the United States: A Path Forward.\" This report addresses many topics including challenges and disparities facing the forensic science community, standardization, certification of practitioners and accreditation of their respective entities, problems related to the interpretation of forensic evidence, the need for research, and the admission of forensic science evidence in litigation. This report mentions the Scientific Working Groups and their role in forensic science.\n\nThe history of imaging technology (photography) can be said to extend back to the times of Chinese philosopher Mo-Ti (470-390 B.C.) who described the principles behind the precursor to the camera obscura. Since that time, advances in imaging technology include the discovery of chemical photographic processes in the 19th century and the use of electronic imaging technology that includes analog video cameras and digital video and still cameras. By the mid 1990s, it was apparent that technologically advanced camera systems such as these were being adopted for use in the criminal justice system. This led the FBI to convene a meeting of individuals working in the field of forensic imaging from federal, state, local, and foreign law enforcement, and the U.S. military, during the summer of 1997. As a result of this meeting, the Technical Working Group on Imaging Technology was formed from a core group of the meeting’s participants. This group later became the Scientific Working Group on Imaging Technology (SWGIT).\nPrior to the inception of SWGIT, some law enforcement agencies began adopting digital imaging technology. Due to the lack of guidelines or standards, some of these agencies attempted to replace all their film cameras with substandard digital cameras, only to find that the equipment they had purchased was not capable of accomplishing the mission for which they were intended. At that time only low resolution digital cameras were deemed affordable by some law enforcement agencies. Some of these agencies were forced to rethink their photography procedures and reverted to the use of film cameras or replaced their low-resolution digital cameras with higher quality, more expensive equipment. Also lacking at this early stage was guidance on how to store and archive digital image files. When SWGIT was formed, it was tasked with providing guidance to law enforcement and others in the criminal justice system by releasing documents that describe the best practices and guidelines for the use of imaging technology, to include these concerns and many others. This group was terminated in 2015.\n\nDuring its existence, SWGIT provided information on the appropriate use of various imaging technologies including both established and new. This was accomplished through the release of documents such as the SWGIT Best Practices documents. As changes in technology occurred, these documents were updated. Over the course of its existence, SWGIT collaborated with other Scientific Working Groups to address imaging concerns within their respective disciplines. SWGIT published over 20 documents that dealt specifically with imaging technology. SWGIT also co-published documents with the Scientific Working Group on Digital Evidence (SWGDE) that had a component or components dealing with imaging technology. SWGIT also provided imaging technology guidance and input for documents from the Scientific Working Group on Friction Ridge Analysis, Study and Technology (SWGFAST), the Scientific Working Group for Forensic Document Examination (SWGDOC)5, and the Scientific Working Group on Shoeprint and Tire Tread Evidence (SWGTREAD). SWGIT assisted the American Society of Crime Lab Directors/Laboratory Accreditation Board (ASCLD/LAB) in the writing of definitions and standards for the accreditation of Digital and Multimedia Evidence sections of crime laboratories. \nIn addition to releasing documents, SWGIT members disseminated best practices for law enforcement professionals where imaging technology was concerned. This was carried out by attending and lecturing at meetings and conferences of various forensic organizations that included:\n\nThe SWGIT membership consisted of approximately fifty scientists, photographers, instructors, and managers from more than two dozen federal, state, and local law enforcement agencies, as well as from the academic and research communities. The membership elected its officers from within.\nSWGIT was composed of the Executive Committee, four standing subcommittees, and ad hoc subcommittees appointed on an as-needed basis. The standing subcommittees were: Image Analysis, Forensic Photography, Video, and Outreach. This group was terminated in 2015.\n\nThe following court cases have conducted Daubert v. Merrell Dow Pharm., Inc., 509 U.S. 579 (1993) hearings in which SWGIT best practice documents have been cited as accepted protocol, methodology, and as generally accepted techniques in the forensic community:\n\n\nThis group was unfunded by the FBI in 2015.\n\n"}
{"id": "48516569", "url": "https://en.wikipedia.org/wiki?curid=48516569", "title": "SemaConnect", "text": "SemaConnect\n\nSemaConnect is an electric vehicle infrastructure company located in Bowie, Maryland founded in 2008 by entrepreneur, Mahi Reddy.\n\nSemaConnect is a developer and producer of smart networked Electric Vehicle charging stations and sophisticated electric vehicle software for station owners and EV drivers. The company launched in 2008 with the first ChargePro 620 edition and EV software called SemaCharge.\n\nSemaConnect installations and accomplishments include successful deployments of over 150 ChargePro charging stations in Maryland, Virginia and Washington, D.C. In 2011, SemaConnect announced the largest commercial order of EV Charging tations when they partnered with 350Green for the national launch of 1,500 charging stations across the U.S. at major retail locations including Walgreens and Simon Properties. In July 2015, SemaConnect raised $15 million in order to expand their electric car network and customer support services.\n\nThe ChargePro charging station uses the North American standard SAEJ1772 connector for Level 2 charging. The ChargePro charging station is designed for installation in commercial properties including municipal, parking, multifamily, hotel, office and retail locations. The ChargePro can have a single or double head unit, and can come with or without a cable management system.\n\n\n\nOfficial website\n"}
{"id": "16591877", "url": "https://en.wikipedia.org/wiki?curid=16591877", "title": "Strem Chemicals", "text": "Strem Chemicals\n\nStrem Chemicals, Inc. is an employee-owned company specializing in fine chemicals in Newburyport, Massachusetts, United States. It was established in 1964 by Michael Strem, who remains president. \n\nWhile Michael Strem was a graduate student, he spent time in Dr. Irving Wender's laboratory learning to synthesize and use cobalt carbonyl. They discussed setting up a chemical company, which resulted in Strem Chemicals. Later, cobalt carbonyl became Strem Chemical's first commercial product.\n\nStrem Chemicals manufactures and markets specialty chemicals of high purity, provides custom synthesis and cGMP manufacturing services, and supplies about 4,500 specialty products in the area of metals, inorganics, organometallics and nanomaterials.\n\nStrem Chemicals supports the American Chemical Society Award for Distinguished Service in the Advancement of Inorganic Chemistry and the Canadian Society for Chemistry Award for Pure or Applied Chemistry.\n"}
{"id": "9944817", "url": "https://en.wikipedia.org/wiki?curid=9944817", "title": "Totem and Ore", "text": "Totem and Ore\n\nTotem and Ore is a book written by B Wongar in 2006. It is an A4 coffee table format book which has a number of photos which show the effect of the British nuclear testing on the aboriginal people of Australia during the 1950s and 60s. There is also a foreword which talks about the devastation of nuclear weapons in Australia and recent conflicts.\n\nThe Australian Aborigines are the only people who have lived through a dual nuclear tragedy, the mining of uranium and the subsequent British nuclear testing, both of which took place on tribal land. The photographs of Totem and Ore collection tell what it was like to be at the forefront of the tragedy.\n\n"}
{"id": "44245060", "url": "https://en.wikipedia.org/wiki?curid=44245060", "title": "Ulrich Spiesshofer", "text": "Ulrich Spiesshofer\n\nUlrich Spiesshofer (born 26 March 1964), is the chief executive officer of the ABB Group, a leading power and automation technology company, headquartered in Zurich, Switzerland. He took up the role on 15 September 2013, succeeding Joe Hogan. Previously, Spiesshofer had headed ABB’s Discrete Automation and Motion division, which includes the company’s robotics, power conversion, motors and drives businesses.\n\nSpiesshofer was born in Aalen, in the southwestern German state of Baden-Wuerttemberg. He holds a PhD in economics as well as a Master’s degree in business administration and engineering from the University of Stuttgart.\n\nFrom 1991 to 2002, Spiesshofer worked for AT Kearney management consultants, rising to become the managing director of AT Kearney International, during which time he ran consulting businesses in industries including oil and gas, utilities, telecommunications and automotive, in Europe, Asia and the Americas. He then spent three years as a senior partner and global head of the operations practice at Roland Berger Strategy Consultants in Switzerland.\n\nSpiesshofer joined ABB in 2005 as Executive Committee member responsible for strategy development. In 2010, he was appointed Head of Discrete Automation and Motion, one of ABB’s five divisions. There, he led the acquisition of Baldor Electric, the largest maker of industrial motors in North America. It was ABB’s biggest acquisition to date with a purchase price of $4.2 billion.\n\nSpiesshofer is an avid skier, sailor and amateur musician, accomplished at the clarinet, saxophone and accordion. He lives with his wife and two children in Zurich.\n"}
{"id": "35365676", "url": "https://en.wikipedia.org/wiki?curid=35365676", "title": "UniFirst", "text": "UniFirst\n\nUniFirst Corporation is a uniform rental company based in Wilmington, Massachusetts, United States, that manufactures, sells, and rents uniforms and protective clothing. UniFirst employs more than 14,000 people and has over 250 facilities in the United States, Canada, and Europe, including customer service centers, nuclear decontamination facilities, cleanroom locations, distribution centers, and manufacturing plants.\n\nUniFirst was founded in 1936 by the Croatti family, under the name of National Overall Dry Cleaning Company. The company began in a horse barn that had been converted into a makeshift laundry and its equipment consisted of a single washing machine and a delivery truck. It served Boston area factory workers and other laborers, whose heavy soiled work clothing needed to be cleaned frequently. The National Overall Dry Cleaning Company was incorporated in Massachusetts on October 6, 1950.\n\nIn the 1980s, UniFirst was sued by residents of Woburn, Massachusetts in a class-action lawsuit. The residents alleged that Unifirst, along with two other firms, had released pollution that had leaked into the water supply, and that this was a cause of increased instances of leukemia in the town. UniFirst settled with the residents without going to trial, for a sum of one million dollars. This episode was featured in the non-fiction book \"A Civil Action\" by Jonathan Harr. As of 2009, UniFirst's enivronmental record has improved; it has received awards for its water treatment processes from the Missouri Water Environment Association and the Water and Wastewater Utility Special Service Division of Austin, Texas, among others.\n\nIn 1991 Ronald Croatti became the chief executive officer of the company. He continued his rise in 1995, when he became the company president, and again in 2002, when he became the chairman of the board. In 2011, UniFirst featured in an episode of the reality television series \"Undercover Boss\".\n\nIn May 2017, Ronald Croatti passed away and Steven S. Sintros became President & CEO.\n\nUniFirst supplies uniforms and protective clothing, as well as restroom and cleaning products such as floor mats, mops, air fresheners and soap. Products that it manufactures in-house include work shirts, work pants, outerwear, and flame-resistant work apparel. It also manufactures a majority of the garments it places in rental programs.\n\nUniFirst subsidiary companies include Green Guard, UniTech Services Group, and UniClean. Green Guard is a corporate supplier of first aid equipment; UniTech provides laundering and decontamination services to the nuclear industry; and UniClean supplies clothing and services related to cleanrooms. UniFirst also has a Canadian uniform rental subsidiary called UniFirst Canada.\n\n\n"}
{"id": "43495360", "url": "https://en.wikipedia.org/wiki?curid=43495360", "title": "Unified Diagnostic Services", "text": "Unified Diagnostic Services\n\nUnified Diagnostic Services (UDS) is a diagnostic communication protocol in the electronic control unit (ECU) environment within the automotive electronics, which is specified in the ISO 14229-1. It is derived from ISO 14230-3 (KWP2000) and ISO 15765-3 (Diagnostic Communication over Controller Area Network (DoCAN)). Unified in this context means that it is an international and not a company-specific standard. By now this communication protocol is used in almost all new ECUs made by Tier 1 suppliers of Original Equipment Manufacturer (OEM). These ECUs control a wide range of functions in vehicles including electronic fuel injection (EFI), engine control, the transmission, anti-lock braking system, door locks, braking, and more.\n\nThe diagnostic tool contacts all control units installed in a vehicle, which have UDS services enabled. In contrary to the CAN protocol, which only uses the first and second layers of the OSI model, UDS services utilize the fifth and seventh layers of the OSI model. The Service ID (SID) and the parameters associated with the services are contained in the 8 data bytes of a message frame issued from the diagnostic tool. \n\nModern vehicles have a diagnostic interface for off-board diagnostics, which makes it possible to connect a computer (client) or diagnostics tool, which is referred to as tester, to the bus system of the vehicle. Thus, the messages defined in UDS can be sent to the controllers which must provide the predetermined UDS services. This makes it possible to interrogate the fault memory of the individual control units or to update them with a new firmware.\n\nSID (Service Identifier)\n\n"}
