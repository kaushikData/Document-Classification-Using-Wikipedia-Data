{"id": "1680877", "url": "https://en.wikipedia.org/wiki?curid=1680877", "title": "Air filter", "text": "Air filter\n\nA particulate air filter is a device composed of fibrous or porous materials which removes solid particulates such as dust, pollen, mold, and bacteria from the air. Filters containing an absorbent or catalyst such as charcoal (carbon) may also remove odors and gaseous pollutants such as volatile organic compounds or ozone. Air filters are used in applications where air quality is important, notably in building ventilation systems and in engines.\n\nSome buildings, as well as aircraft and other human-made environments (e.g., satellites and space shuttles) use foam, pleated paper, or spun fiberglass filter elements. Another method, air ionizers, use fibers or elements with a static electric charge, which attract dust particles. The air intakes of internal combustion engines and air compressors tend to use either paper, foam, or cotton filters. Oil bath filters have fallen out of favor. The technology of air intake filters of gas turbines has improved significantly in recent years, due to improvements in the aerodynamics and fluid dynamics of the air-compressor part of the gas turbines.\n\nHEPA filters (high-efficiency particulate air filters) remove at least 99.97% of particles that are 3 micrometres in diameter, and efficiently remove both larger and smaller particles.\n\nThe cabin air filter is typically a pleated-paper filter that is placed in the outside-air intake for the vehicle's passenger compartment. Some of these filters are rectangular and similar in shape to the combustion air filter. Others are uniquely shaped to fit the available space of particular vehicles' outside-air intakes.\n\nThe first automaker to include a disposable filter to clean the ventilation system was the Nash Motors \"Weather Eye\", introduced in 1940.\n\nBeing a relatively recent addition to automobile equipment, this filter is often overlooked. Clogged or dirty cabin air filters can significantly reduce airflow from the cabin vents, as well as introduce allergens into the cabin air stream, and since the cabin air temperature depends upon the flow rate of the air passing through the heater core, the evaporator or both, they can greatly reduce the effectiveness of the vehicle's air conditioning and the heating performance. The poor performance of these filters is obscured by manufacturers by not using the minimum efficiency reporting value (MERV) rating system. Some people mistakenly believe that some of these are HEPA filters.\n\nThe combustion air filter prevents abrasive particulate matter from entering the engine's cylinders, where it would cause mechanical wear and oil contamination.\n\nMost fuel injected vehicles use a pleated paper filter element in the form of a flat panel. This filter is usually placed inside a plastic box connected to the throttle body with duct work. Older vehicles that use carburetors or throttle body fuel injection typically use a cylindrical air filter, usually between and in diameter. This is positioned above or beside the carburetor or throttle body, usually in a metal or plastic container which may incorporate ducting to provide cool and/or warm inlet air, and secured with a metal or plastic lid. The overall unit (filter and housing together) is called the air cleaner.\n\nPleated paper filter elements are the nearly exclusive choice for automobile engine air cleaners, because they are efficient, easy to service, and cost-effective. The \"paper\" term is somewhat misleading, as the filter media are considerably different from papers used for writing or packaging, etc. There is a persistent belief among tuners, fomented by advertising for aftermarket non-paper replacement filters, that paper filters flow poorly and thus restrict engine performance. In fact, as long as a pleated-paper filter is sized appropriately for the airflow volumes encountered in a particular application, such filters present only trivial restriction to flow until the filter has become significantly clogged with dirt. Construction equipment engines also use this. The reason is that the paper is bent in zig-zag shape, and the total area of the paper is very large, in the range of 50 times of the air opening.\n\nOil-wetted polyurethane foam elements are used in some aftermarket replacement automobile air filters. Foam was in the past widely used in air cleaners on small engines on lawnmowers and other power equipment, but automotive-type paper filter elements have largely supplanted oil-wetted foam in these applications. Foam filters are still commonly used on air compressors for air tools up to 5Hp. Depending on the grade and thickness of foam employed, an oil-wetted foam filter element can offer minimal airflow restriction or very high dirt capacity, the latter property making foam filters a popular choice in off-road rallying and other motorsport applications where high levels of dust will be encountered. Due to the way dust is captured on foam filters, large amounts may be trapped without measurable change in airflow restriction.\n\nOiled cotton gauze is employed in a growing number of aftermarket automotive air filters marketed as high-performance items. In the past, cotton gauze saw limited use in original-equipment automotive air filters. However, since the introduction of the Abarth SS versions, the Fiat subsidiary supplies cotton gauze air filters as OE filters.\n\nStainless steel mesh is another example of medium which allow more air to pass through. \nStainless steel mesh comes with different mesh counts, offering different filtration standards. \nIn an extreme modified engine lacking in space for a cone based air filter, some will opt to install a simple stainless steel mesh over the turbo to ensure no particles enter the engine via the turbo.\n\nAn oil bath air cleaner consists of a sump containing a pool of oil, and an insert which is filled with fiber, mesh, foam, or another coarse filter media. When the cleaner is assembled, the media-containing body of the insert sits a short distance above the surface of the oil pool. The rim of the insert overlaps the rim of the sump. This arrangement forms a labyrinthine path through which the air must travel in a series of U-turns: up through the gap between the rims of the insert and the sump, down through the gap between the outer wall of the insert and the inner wall of the sump, and up through the filter media in the body of the insert. This U-turn takes the air at high velocity across the surface of the oil pool. Larger and heavier dust and dirt particles in the air cannot make the turn due to their inertia, so they fall into the oil and settle to the bottom of the base bowl. Lighter and smaller particles are trapped by the filtration media in the insert, which is wetted by oil droplets aspirated there into by normal airflow.\nOil bath air cleaners were very widely used in automotive and small engine applications until the widespread industry adoption of the paper filter in the early 1960s. Such cleaners are still used in off-road equipment where very high levels of dust are encountered, for oil bath air cleaners can sequester a great deal of dirt relative to their overall size without loss of filtration efficiency or airflow. However, the liquid oil makes cleaning and servicing such air cleaners messy and inconvenient, they must be relatively large to avoid excessive restriction at high airflow rates, and they tend to increase exhaust emissions of unburned hydrocarbons due to oil aspiration when used on spark-ignition engines.\n\nIn the early 20th century (about 1900 to 1930), water bath air cleaners were used in some applications (cars, trucks, tractors, and portable and stationary engines). They worked on roughly the same principles as oil bath air cleaners. For example, the original Fordson tractor had a water bath air cleaner. By the 1940s, oil bath designs had displaced water bath designs because of better filtering performance.\n\nBulk solids handling involves the transport of solids (mechanical transport, pneumatic transport) which may be in a powder form. Many industries are handling bulk solids (mining industries, chemical industries, food industries) which requires the treatment of air streams escaping the process so that fine particles are not emitted, for regulatory reasons or economical reasons (loss of materials). As a consequence, air filters are positioned at many places in the process, especially at the reception of pneumatic conveying lines where the quantity of air is important and the load in fine particle quite important. Filters can also be placed at any point of air exchange in the process to avoid that pollutants enter the process, which is particularly true in pharmaceuticals and food industries. The physical phenomena involved in catching particles with a filter are mainly inertial and diffusional\n\nEuropean normalization standards recognize the following filter classes:\n\n"}
{"id": "47320077", "url": "https://en.wikipedia.org/wiki?curid=47320077", "title": "Alternative finance", "text": "Alternative finance\n\nAlternative finance refers to financial channels, processes, and instruments that have emerged outside of the traditional finance system such as regulated banks and capital markets. Examples of alternative financing activities through 'online marketplaces' are reward-based crowdfunding, equity crowdfunding, peer-to-peer consumer and business lending, invoice trading third party payment platforms. Alternative finance instruments include cryptocurrencies such as Bitcoin, SME mini-bond, social impact bond, community shares, private placement and other 'shadow banking' mechanisms. Alternative finance differs to traditional banking or capital market finance through technology-enabled 'disintermediation', which means utilising third party capital by connecting fundraisers directly with funders, in turn, reducing transactional costs and improve market efficiency.\n\nAlternative finance has grown into a considerable global industry in recent years following the financial crisis according to various reports, particularly for small and medium enterprises. For instance, the European online alternative finance market is estimated to have reached nearly €3bn in 2014 and is projected to reach €7bn in 2015. For the United Kingdom, according to the University of Cambridge and Nesta, the UK online alternative finance market reached £1.74bn in 2014. In comparison, the alternative finance markets in France and Germany reached €154m and €140m respectively in 2014.\n\nAlternative finance activities such as equity crowdfunding and peer-to-peer lending are now regulated by the Financial Conduct Authority in the United Kingdom from 1 April 2014. Peer-to-peer lending investment will be eligible for an Innovative Finance ISA from 2016. In the US, under the Title II of the JOBS Act, accredited investors are allowed to invest on equity crowdfunding platforms from September 2013. The SEC then announced the updated and expanded Regulation A mandated by the Title IV of the JOBS Act to allow non-accredited investors to participate in equity crowdfunding.\n\n"}
{"id": "14345757", "url": "https://en.wikipedia.org/wiki?curid=14345757", "title": "Back-up ring", "text": "Back-up ring\n\nA back-up ring is a rigid ring that holds an elastomeric seal or plastic (such as Polyethylene) connection to its designed shape and in its correct place. Back up rings are commonly used with O-rings, lip seals, and as reciprocating shaft seals. They are also used for piping connections joining two different materials - typically one flexible and one rigid.\n\nWhen sealing the piston inside a pneumatic cylinder, a soft and flexible material is required to prevent leakage, but those same properties may leave the seal material vulnerable to being pulled out of its seat and then pinched or torn in the narrow space between piston and cylinder wall. If the joint cannot be redesigned, or a more resistant elastomer used, then the solution may be direct reinforcement with a stiffer material - in the form of a hard inner ring in this case. A second function can be to hold the elastomer in place while a machine is being assembled, as geometry may prevent the seal from being directly checked after assembly.\n\nContoured back-ups are routinely produced in a 90 durometer nitrile. This provides sufficient elasticity to permit stretching over the major diameter of a piston and then snapping back into the gland groove cut into the piston. 90 durometer has sufficient hardness to resist extrusion of the softer elastomeric O-ring performing the actual sealing function against the high pressure liquid or gas.\n\nBack-up rings are used in Polyethylene (PE) pipe connections, such as HDPE pipes, where they are connected to steel pipe or pipe fittings. \n\nThe back-up ring is matched to the steel pipe flange specification and is placed behind the HDPE flange adapter - a end piece on the HDPE pipe that looks much like a steel flange at the end of the HDPE pipe. As the flange adapter does not have much structural strength, the back-up ring supports and compresses the HDPE flange adapter against the steel flange and creates an effective connection between the two piping materials.\n\n"}
{"id": "34147555", "url": "https://en.wikipedia.org/wiki?curid=34147555", "title": "Blinders (poultry)", "text": "Blinders (poultry)\n\nBlinders, also known as peepers, are devices fitted to, or through, the beaks of poultry to block their forward vision and assist in the control of feather pecking, cannibalism and sometimes egg-eating. A patent for the devices was filed as early as 1935.\nThey are used primarily for game birds, pheasant and quail, but also for turkeys and laying hens. Blinders are opaque and prevent forward vision, unlike similar devices called spectacles which have transparent lenses. Blinders work by reducing the accuracy of pecking at the feathers or body of another bird, rather than spectacles which have coloured lenses and allow the bird to see forwards but alter the perceived colour, particularly of blood. Blinders are held in position with a circlip arrangement or lugs into the nares of the bird, or a pin which pierces through the nasal septum. They can be made of metal (aluminium), neoprene or plastic, and are often brightly coloured making it easy to identify birds which have lost the device. Some versions have a hole in the centre of each of the blinders, thereby allowing restricted forward vision.\n\nIn pheasants, blinders have been shown to reduce the incidence of birds pecking each other and damage to the feathers or skin.\n\nIn laying hens, blinders have been shown to reduce feather pecking, improve food utilisation (due to less spillage) and increase egg production.\n\nBlinders which require a pin to pierce the nasal septum and hold the device in position almost certainly cause pain to the bird. In the UK, the use of these devices is illegal on welfare grounds. The Department for Environment, Food and Rural Affairs in their Codes of Recommendations for the Welfare of Livestock: Laying Hens, states: \"The Welfare of Livestock (Prohibited Operations) Regulations 1982 (S.I. 1982 No.1884) prohibits ...the fitting of any appliance which has the object or effect of limiting vision to a bird by a method involving the penetration or other mutilation of the nasal septum.\"\n\nStudies on pin-less blinders indicate these devices are at least temporarily distressful to the birds. In pheasants, fitting blinders causes an increase in head shaking and scratching, and increases in damage to the beak and nostrils of the bird. Fitting pin-less blinders to laying hens leads to reduced activity, increased resting, adjustment problems in feeding, stereotypic head shaking and protracted displacement neck preening for a month after fitting. In another study on laying hens, mortality was greater among hens wearing blinders compared to hens that had been beak-trimmed.\n\n\n"}
{"id": "18125774", "url": "https://en.wikipedia.org/wiki?curid=18125774", "title": "Bowl feeder", "text": "Bowl feeder\n\nVibratory bowl feeders are common devices used to feed individual component parts for assembly on industrial production lines. They are used when a randomly sorted bulk package of small components must be fed into another machine one-by-one, oriented in a particular direction.\n\nVibratory feeders rely on the mechanical behaviour of a part, such that when gently shaken down a conveyor chute that is shaped to fit the part, they will gradually be shaken so that they are all aligned. They thus leave the feeder's conveyor one-by-one, all in the same orientation. This conveyor then leads directly to the following assembly or packing machine.\n\nOrientation relies on the shape and mechanical behaviour of an object, particular the position of its centre of mass in relation to its centre of volume. It thus works well for parts such as machine screws, with rotational symmetry and a clear asymmetry to one heavy end. It does not work for entirely symmetrical shapes, or where orientation depends on a feature such as colour. The ramps within a bowl feeder are specifically designed for each part, although the core mechanism is re-used across different parts.\n\nThe exit orientation of a bowl feeder depends on the part's shape and mass distribution. Where this is not the orientation needed for the following assembly step, a feeder is often followed by a twisted conveyor that turns the part over, as needed.\n\nWith increasing integration across an entire production process, the need for feeders is sometimes reduced by supplying the components on tape packages or similar, that keep them oriented the same way during shipping and storage. These are most common in fields such as electronics, where components must be used in a particular orientation, but this cannot be detected mechanically.\n\nVibratory feeders, commonly known as a bowl feeder, are self-contained devices, consisting of a specially shaped bowl designed to orient the parts to a specific orientation. A vibrating drive unit, upon which the bowl is mounted and a variable-amplitude control box controls the bowl feeder. Usually included is an out feed accumulation track (linear or gravity) to convey parts along and discharge into the assembly machine comes in many shapes and sizes. The drive unit, available in piezoelectric, electromagnetic and pneumatic drives, vibrates the bowl, forcing the parts to move up a circular, inclined track. The tooling (hand made) is designed to sort and orient the parts in to a consistent, repeatable position. The track length, width, and depth are carefully chosen to suit each application, component shape and size. Special bowl and track coatings are applied according to shape size and material of the component which aids traction, damage to the product and lower acoustic levels. A variable speed control box is used for controlling the vibration speed of the bowl feeder, and can control the flow of parts to the out feed track via sensors.\n\nVibratory feeders are utilized by all industries including, the pharmaceutical, automotive, electronic, cosmetic, food, Fast Moving Consumable Goods (FMCG), packaging and metalworking industries. It also serves other industries such as glass, foundry, steel, construction, recycling, pulp and paper, and plastics. Vibratory feeders offer a cost-effective alternative to manual labour, saving manufacturer’s time and labour costs. Several factors must be considered when selecting a parts feeder, including the industry, application, material properties and product volume.\n\nHistory/Inventor\n\n\n\n\n"}
{"id": "515999", "url": "https://en.wikipedia.org/wiki?curid=515999", "title": "Brokaw bandgap reference", "text": "Brokaw bandgap reference\n\nBrokaw bandgap reference is a voltage reference circuit widely used in integrated circuits, with an output voltage around 1.25 V with low temperature dependence. This particular circuit is one type of a bandgap voltage reference, named after Paul Brokaw, the author of its first publication.\nLike all temperature-independent bandgap references, the circuit maintains an internal voltage source that has a positive temperature coefficient and another internal voltage source that has a negative temperature coefficient. By summing the two together, the temperature dependence can be canceled. Additionally, either of the two internal sources can be used as a temperature sensor.\n\nIn the Brokaw bandgap reference, the circuit uses negative feedback (e.g., an operational amplifier) to force a constant current through two bipolar transistors with different emitter areas. By the Ebers–Moll model of a transistor,\nThe circuit output is the sum of the base–emitter voltage difference with one of the base–emitter voltages. With proper component choices, the two opposing temperature coefficients will cancel each other exactly and the output will have no temperature dependence.\n\n\n"}
{"id": "287134", "url": "https://en.wikipedia.org/wiki?curid=287134", "title": "Coleco Adam", "text": "Coleco Adam\n\nThe Coleco Adam is a home computer, and expansion for the ColecoVision (port 3), released in 1983 by American toy and video game manufacturer Coleco Industries, Inc.. It was an attempt to follow on the success of the company's ColecoVision video game console. The Adam was not very successful, partly because of early production problems, and was discontinued in early 1985.\n\nColeco announced the Adam at the Summer Consumer Electronics Show (CES) in June 1983, and executives predicted sales of 500,000 by Christmas 1983. From the time of the computer's introduction to the time of its shipment, the price increased, from USD $525 to $725.\n\nThe Adam announcement received favorable press coverage. Competitors such as Commodore and Atari almost immediately announced similar computer-printer bundles. The company announced an extensive marketing campaign, with television commercials for \"boys age 8 to 16 and their fathers ... the two groups that really fuel computer purchases\", and print advertisements in nontechnical publications like \"Time\" and \"People\".\n\nThe \"Boston Phoenix\", observing that Adam's $600 price was comparable to the lowest price for a letter-quality printer alone, stated \"a nice trick if they can do it!\" It was a trick; the computers were shown behind tinted glass that hid the fact that they were hand-made and had non-working tape drives. In June Coleco promised to ship the computer by August. In August it promised to ship a half million Adams by Christmas, but missed shipping dates of 1 September, 15 September, 1 October, and 15 October. \"Ahoy!\" reported that Coleco had not shipped by early October because of various problems. Each month of delay could mean losing the opportunity to sell 100,000 units, the magazine reported, adding that missing the Christmas season would result in \"inestimable losses\". CEO Arnold Greenberg promised in late September to ship by \"mid-October\", but claimed that Adam was \"not, primarily, a Christmas item\". The printer was the main cause of the delays; after it failed to function properly at demonstrations, by November \"InfoWorld\" reported on \"growing skepticism\" about its reliability, speed, and noise.\n\nGreenberg refused to say how many units he expected Coleco to ship by the end of the year. The company did not ship review units to magazines planning to publish reviews before Christmas, stating that all were going to dealers, but admitted that it would not meet the company's goal of shipping 400,000 computers by the end of the year; Kmart and JCPenney announced in November that it would not sell the Adam during the Christmas season because of lack of availability. Despite great consumer interest, Coleco shipped only 95,000 units by December, many of which were defective; \"Creative Computing\" later reported that \"the rumored return rate was absolutely alarming\". One store manager stated that five of six sold Adams had been returned, and expected that the sixth would likely be returned after being opened on Christmas. Coleco partnered with Honeywell Information Systems to open up repair chain stores around the nation. By December 1983 the press reported that company executives at a news conference \"fielded questions about Coleco's problems with its highly-publicized new Adam home computer, which has been plagued by production delays and complaints of defects\", with the company able to fulfill only one third of its Canadian orders for Christmas. Less than 10% of Adam units had defects, the company claimed, \"well below industry standards\".\n\nAn analyst stated in early 1984 that the company had\n\nBy March 1984 John J. Anderson declared Adam as having caused for Coleco \"a trail of broken promises, unfulfilled expectations, and extremely skittish stockholders.\" On January 2, 1985, after continuing complaints about Adam failures and low sales, Coleco announced that it was discontinuing the Adam and would be selling off its inventory.\nColeco revealed that it lost $35 million in late 1983 (the time of the Adam's launch), along with a loss of $13.4 million in the first 9 months of 1984. Coleco did not reveal which company they were selling the inventory to, but stated that they had worked with this partner before. No final sales numbers were revealed of the Adam computer and Expansion, but one analyst estimated that Coleco had sold 350,000 Adams in 1983 and 1984.\n\nIn its favor, the Adam had a large software library from the start. It was derived from and compatible with the ColecoVision's software and accessories, and, in addition, the popular CP/M operating system was available as an option. Its price gave a complete system: an 80 kB RAM computer, tape drive, letter-quality printer, and software including the \"\" video game. The IBM PCjr sold for $669 but included no peripherals, and although the popular Commodore 64 sold for around $200, its price was not much lower after the purchase of a printer, tape or disk drive, and software.\n\nLike many home computers of its day, the Adam was intended to use a television set for its display. The SmartWriter electronic typewriter loaded when the system was turned on. In this mode, the system operated just like a typewriter, printing letters as soon as the user typed them. Pressing the Escape/WP key put SmartWriter into word processor mode, which functioned similarly to a modern word processor.\n\nUnlike other home computers at the time, the Adam did not have its BASIC interpreter permanently stored in ROM. Instead, it featured a built-in electronic typewriter and word processor, SmartWriter, as well as the Elementary Operating System (EOS) OS kernel and the 8kB OS-7 ColecoVision operating system. The SmartBASIC interpreter was delivered on a proprietary-format Digital Data Pack tape cassette.\n\nA less expensive version of the Adam plugged into a ColecoVision, which delivered on one of ColecoVision's launch commitments that owners would one day be able to upgrade their game system to a fully featured computer system.\n\nMany early Adams were defective. An author of the computer's manual reported receiving \"300 calls on Christmas week\" from owners with problems; \"some callers who were on their fourth or fifth Adam\", he said. Defective computers at the time could be repaired only by mailing it to Coleco in Connecticut. Despite improving product quality and the Honeywell repair partnership, the company could not improve the computer's poor reputation. Problems included:\n\n\nColecovision software that was not built in were mostly on ROM cartridges, with Adam Calculator, Personal Checkbook, and Smart Filer programs also being on floppy disk.\n\n\nTo showcase the machine at the June 1983 CES, Coleco decided to demonstrate a port of its ColecoVision conversion of \"Donkey Kong\" on the system. Nintendo was in the midst of negotiating a deal with Atari to license its Famicom for distribution outside Japan, and the final signing would have been done at CES. Atari had exclusive rights to \"Donkey Kong\" for home computers (as Coleco had for game consoles), and when Atari saw that Coleco was showing \"Donkey Kong\" on a computer, its proposed deal with Nintendo was delayed. Coleco had to agree not to sell the Adam version of \"Donkey Kong\". Ultimately, it had no bearing on the Atari/Nintendo deal, as Atari's CEO Ray Kassar was fired the next month and the proposal went nowhere, with Nintendo deciding to market its system on its own.\n\n\"BYTE\" reported in September 1983 that the Adam's introduction had \"dominated\" the Consumer Electronics Show in Chicago. Citing its $599 price, bundled hardware, and compatibility with ColecoVision and CP/M software, the magazine compared the Adam's potential impact on the home-computer industry to that of the Osborne 1. \"Ahoy!\" reported in January 1984 that \"Early indications were that the Adam would be a runaway best seller\" but the delays, technical problems, and Coleco's reputation as a toy company \"should combine to keep buyers away in droves\", and predicted that \"there is no reason to think that the Adam will topple the C-64 from the catbird seat\".\n\n\"The Washington Post\"s T. R. Reid gave \"an 'A' for ingenuity [but] would have to stretch to give Adam a gentleman's 'C' for performance\" in January 1984. While praising the keyboard and SmartWriter's ease of use, and calling the data pack \"a reasonable compromise\", he described the documentation as \"wholly inadequate\" and \"generally inexcusable\". \"A more serious flaw with Adam is in the hardware\", Reid said, citing defects in a data pack and both the printer and a replacement, and the computer's unusability without a working printer. He concluded that \"I'd dearly like to\" recommend the Adam, but \"for the time being, though, I'd advise you to proceed with caution\", including confirming that the computer worked before leaving the store. \"Popular Mechanics\" in February 1984 was more favorable. Calling the bundle \"the most revolutionary concept in how to design and sell a home computer that we have seen\", it also praised the keyboard and SmartWriter. While citing flaws such as the \"slow and very noisy printer\", the magazine concluded that \"Adam competes with and overpowers everything else in its class\", inferior only to the IBM PC and Apple IIe.\n\n\"Compute!\"s March 1984 review also approved of the Adam's prepackaged, all-in-one nature and called the keyboard \"impressive\", but cited widespread reports of hardware failures. \"BYTE\"s April 1984 review was much harsher, stating that \"It is often said that if something sounds too good to be true, it probably is. The Coleco Adam is no exception to this rule\". It called the tape-drive technology \"impressive\", and approved of the keyboard, but reported several cases of data errors and deletions when using the tape drives, a buggy word processor, and a BASIC manual that was \"the worst I have ever seen\". The reviewer reported that he was waiting for his fifth Adam after four previous systems malfunctioned in two months; only the keyboard did not fail. Surmising that \"the computer was apparently rushed into production\", he advised \"don't buy an Adam—yet. Wait until Coleco fixes all of the Adam's bugs and delivers on all of its promises\", and concluded \"Coleco is [apparently] betting the whole company on the Adam and it's not yet clear that it's going to win that bet\".\n\nThe Adam received some good reviews based on the quality of its keyboard and printer, and offered competitive sound and graphics. Its BASIC interpreter, called SmartBASIC, was largely compatible with Applesoft BASIC, which meant that many type-in programs from computer books and magazines would work with the Adam with little or no modification.\n\nSales were weak, especially after the technical issues became obvious. Coleco lost $35 million in the fourth quarter of 1984 as returns flooded in. Officially, Coleco blamed \"manuals which did not offer the first-time user adequate assistance.\" Coleco reintroduced Adam with a new instruction manual, lower price, and a $500 college scholarship along with each unit for use by a young child (with $125 paid for each completed year of college). Fewer than 100,000 units ultimately sold.\n\nNew York City advertising firm, Ketchum Advertising, won the assignment of promoting the computer. The agency staffed up to handle the work, and the prestige, of the new business, but agency executives were caught off-guard, when they opened the \"New York Times\" January 1985 morning edition, to read, with no previous warning, that Coleco was abandoning the computer.\n\nThe Adam was permanently discontinued in 1985, less than two years after its introduction.\n\nA group of Adam enthusiasts have been gathering every year since 1989 for an event called AdamCon.\n\nThird-party developers contributed to the overall success of the ADAM after Coleco abandoned the ADAM. Developers such as Orphanware, In House Reps, Thomas Electronics, Oasis Pensive, Eve, E&T, Micro Innovations, Microfox Technologies and others added multiple-density disk drives, memory expanders, speech synthesizers, serial cards, printer cards, IDE cards and other hardware so the ADAM could follow other computers into a newer modern age. In 2012, most of this hardware is still available to upgrade the ADAM.\n\n\n\n"}
{"id": "20153971", "url": "https://en.wikipedia.org/wiki?curid=20153971", "title": "Correlated double sampling", "text": "Correlated double sampling\n\nCorrelated double sampling (CDS\n) is a method to measure electrical values such as voltages or currents that allows removing an undesired offset. It is used often when measuring sensor outputs. The output of the sensor is measured twice: once in a known condition and once in an unknown condition. The value measured from the known condition is then subtracted from the unknown condition to generate a value with a known relation to the physical quantity being measured.\n\nThis is commonly used in switched capacitor operational amplifiers to effectively double the gain of the charge sharing opamp, while adding an extra phase.\n\nWhen used in imagers, correlated double sampling is a noise reduction technique in which the Reference Voltage of the pixel (i.e., the pixel’s voltage after it is reset) is removed from the Signal Voltage of the pixel (i.e., the pixel’s voltage at the end of integration) at the end of each integration period.\n\n"}
{"id": "20483650", "url": "https://en.wikipedia.org/wiki?curid=20483650", "title": "Dynamic accumulator", "text": "Dynamic accumulator\n\nDynamic accumulators is a term used in the permaculture and organic farming literature to indicate plants that gather certain minerals or nutrients from the soil and store them in a more bioavailable form and in high concentration in their tissues, then used as fertilizer or just to improve the mulch. \n\nWhile this idea is quite common and often taprooted plants are used for this reason in companion planting, there is no scientific data supporting it, and the definition itself varies quite depending on the author. The closest thing with a proven scientific base are hyperaccumulators.\n\nThe first to use the term \"dynamic accumulator\" in the above definition was probably Robert Kourik in his book \"Designing and Maintaining Your Edible Landscape—Naturally\" (1986). After him many important permaculturists have used dynamic accumulators in their design systems and methods, such as Eric Toensmeier, Dave Jake and Toby Hemenway.\n\n\n Jake, Dave (2005). Edible Forest Gardens. Chelsea Green Publishing Company. p. 186. .\nJump up ^ Hemenway, Toby (2009). Gaia's Garden - A Guide To Home Scale Permaculture. Chelsea Green Publishing Company. p. 97. .\n\n"}
{"id": "48223694", "url": "https://en.wikipedia.org/wiki?curid=48223694", "title": "EpiBone", "text": "EpiBone\n\nEpiBone is a biomedical engineering company that is developing technology to create bone tissue from a patient's mesenchymal stem cells \"in vitro\" for use in bone grafts. The company was founded by Nina Tandon.\n"}
{"id": "36726699", "url": "https://en.wikipedia.org/wiki?curid=36726699", "title": "Fault reporting", "text": "Fault reporting\n\n'Fault Reporting is a maintenance concept that increases operational availability and that reduces operating cost through three mechanisms.\n\nThis is a prerequisite for Condition-based maintenance.\n\nActive redundancy can be integrated with fault reporting to reduce down time to a few minutes per year.\n\nFormal maintenance philosophies are required by organizations whose primary responsibility is to ensure systems are ready when expected, such as space agencies and military.\n\nLabor-intensive planned maintenance began during the rise of the industrial revolution. This depends upon periodic diagnostic evaluation based upon calendar dates, distance, or use. The intent is to accomplish diagnostic evaluations that indicate when maintenance is required to prevent inconvenience and safety issues that will occur when critical equipment failures occur during use.\n\nThe electronic revolution allowed inexpensive sensors and controls to be integrated into most equipment. This includes diagnostic indicators, fluid sensors, temperature sensors, ignition sensors, exhaust monitoring, voltage sensors, and similar monitoring equipment that indicates when maintenance is required. Sensor displays are often located in inaccessible locations that cannot be observed during normal operation. Labor-intensive periodic maintenance is often required to inspect indicators.\n\nSome organizations have eliminated most labor-intensive periodic maintenance and diagnostic down time by implementing designs that bring all sensor status to fault indicators located in close proximity to users.\n\nMaintenance requires three actions.\n\nFault discovery requires diagnostic maintenance, which requires system down time and labor costs.\n\nDown time and cost requirements associated with diagnostics are eliminated for every item that satisfies the following criteria.\n\nFault reporting is an optional feature that can be forwarded to remote displays using simple configuration setting in all modern computing equipment. The system level of reporting that is appropriate for Condition Based Maintenance are critical, alert, and emergency, which indicate software termination due to failure. Specific failure reporting, like interface failure, can be integrated into applications linked with these reporting systems. There is no development cost if these are incorporated into designs.\n\nOther kinds of fault reporting involves painting green, yellow, and red zones onto temperature gages, pressure gages, flow gages, vibration sensors, strain gages, and similar sensors. Remote viewing can be implemented using a video camera.\n\nThe historical approach for Fault discovery is periodic diagnostic testing, which eliminates the following operational availability penalty.\n\nFault reporting eliminates maintenance costs associated manual diagnostic testing.\n\nLabor is eliminated in redundant designs by using the fault discovery and fault isolation functions to automatically reconfigure equipment for degraded operation.\n\nMaintenance savings can be re-allocated to upgrades and improvements that increase organizational competitiveness.\n\nFaults that do not trigger a sustained requirement for fault isolation and fault recovery actions should not be displayed for management action.\n\nAs an example, lighting up a fault indicator in situations where human intervention is not required will induce breakage by causing maintenance personnel to perform work when nothing is already broken.\n\nAs another example, enabling fault reporting for Internet network packet delivery failure will increase network loading when the network is already busy, and that will cause total network outage.\n\n"}
{"id": "2168608", "url": "https://en.wikipedia.org/wiki?curid=2168608", "title": "Flash pasteurization", "text": "Flash pasteurization\n\nFlash pasteurization, also called \"high-temperature short-time\" (HTST) processing, is a method of heat pasteurization of perishable beverages like fruit and vegetable juices, beer, kosher wine, and some dairy products such as milk. Compared with other pasteurization processes, it maintains color and flavor better, but some cheeses were found to have varying responses to the process.\n\nFlash pasteurization is performed to kill spoilage microorganisms prior to filling containers, in order to make the products safer and to extend their shelf life compared to the unpasteurised foodstuff. For example, one manufacturer gives the shelf life of its pasteurized milk as 12 months. It must be used in conjunction with sterile fill technology (similar to aseptic processing) to prevent post-pasteurization contamination.\n\nThe liquid moves in a controlled, continuous flow while subjected to temperatures of 71.5 °C (160 °F) to 74 °C (165 °F), for about 15 to 30 seconds Followed by rapid cooling from 4 °C (39.2 °F) to 5.5 °C (42 °F).\n\nThe standard US protocol for flash pasteurization of milk, 71.7 °C (161 °F) for 15 seconds in order to kill \"Coxiella burnetii\" (the most heat-resistant pathogenic germ found in raw milk), was introduced in 1933, and results in five \"log reduction value\" (99.999%) or greater reduction in harmful bacteria. An early adopter of pasteurization was Tropicana Products, which has used the method since the 1950s. The juice company Odwalla switched from non-pasteurized to flash-pasteurized juices in 1996 after tainted unpasteurized apple juice containing sickened many children and killed one.\n\n"}
{"id": "22406494", "url": "https://en.wikipedia.org/wiki?curid=22406494", "title": "Fresnel–Arago laws", "text": "Fresnel–Arago laws\n\nThe Fresnel–Arago laws are three laws which summarise some of the more important properties of interference between light of different states of polarization. Augustin-Jean Fresnel and François Arago, both discovered the laws, which bear their name.\n\nThe laws are as follows:\n\n\nOne may understand this more clearly when considering two waves, given by the form formula_1 and formula_2, where the boldface indicates that the relevant quantity is a vector, interfering. We know that the intensity of light goes as the electric field squared (in fact, formula_3, where the angled brackets denote a time average), and so we just add the fields before squaring them. Extensive algebra yields an interference term in the intensity of the resultant wave, namely: \nformula_4, where formula_5 represents the phase difference arising from a combined path length and initial phase-angle difference.\n\nNow it can be seen that if formula_6 is perpendicular to formula_7 (as in the case of the first Fresnel–Arago law), formula_8 and there is no interference. On the other hand, if formula_6 is parallel to formula_7 (as in the case of the second Fresnel–Arago law), the interference term produces a variation in the light intensity corresponding to formula_11. Finally, if natural light is decomposed into orthogonal linear polarizations (as in the third Fresnel–Arago law), these states are incoherent, meaning that the phase difference formula_12 will be fluctuating so quickly and randomly that after time-averaging we have formula_13, so again formula_8 and there is no interference (even if formula_6 is rotated so that it is parallel to formula_7).\n"}
{"id": "13237178", "url": "https://en.wikipedia.org/wiki?curid=13237178", "title": "G.8261", "text": "G.8261\n\nITU-T Recommendation G.8261/Y.1361 (formerly G.pactiming) \"Timing and Synchronization Aspects in Packet Networks\" specifies the upper limits of allowable network jitter and wander, the minimum requirements that network equipment at the TDM interfaces at the boundary of these packet networks can tolerate, and the minimum requirements for the synchronization function of network equipment.\n\nPacket networks have been inherently asynchronous. However, as the communications industry moves toward an all IP core and edge network, there is a need to provide synchronization functionality to traditional TDM-based applications. This is essential for the interworking with PSTN. The goal is provide a Primary Reference Clock (PRC) traceable clock for the TDM applications.\n\n"}
{"id": "3350640", "url": "https://en.wikipedia.org/wiki?curid=3350640", "title": "GIS in archaeology", "text": "GIS in archaeology\n\nGIS or Geographic Information Systems has been an important tool in archaeology since the early 1990s. Indeed, archaeologists were early adopters, users, and developers of GIS and GIScience, Geographic Information Science. The combination of GIS and archaeology has been considered a perfect match, since archaeology often involves the study of the spatial dimension of human behavior over time, and all archaeology carries a spatial component.\n\nSince archaeology looks at the unfolding of historical events through geography, time and culture, the results of archaeological studies are rich in spatial information. GIS is adept at processing these large volumes of data, especially that which is geographically referenced. It is a cost effective, accurate and fast tool. The tools made available through GIS help in data collection, its storage and retrieval, its manipulation for customized circumstances and, finally, the display of the data so that it is visually comprehensible by the user. The most important aspect of GIS in archaeology lies, however, not in its use as a pure map-making tool, but in its capability to merge and analyse different types of data in order to create new information. The use of GIS in archaeology has changed not only the way archaeologists acquire and visualise data, but also the way in which archaeologists think about space itself. GIS has therefore become more of a science than an objective tool.\n\nSurvey and documentation are important to preservation and archaeology, and GIS makes this research and fieldwork efficient and precise. Research done using GIS capabilities is used as a decision making tool to prevent loss of relevant information that could impact archaeological sites and studies. It is a significant tool that contributes to regional planning and for cultural resource management to protect resources that are valuable through the acquisition and maintenance of data about historical sites.\n\nIn archaeology, GIS increases the ability to map and record data when it is used directly at the excavation site. This allows for immediate access to the data collected for analysis and visualization as an isolated study or it can be incorporated with other relevant data sources to help understand the site and its findings better.\n\nThe ability of GIS to model and predict likely archaeological sites is used by companies that are involved with utilizing vast tracts of land resources like the Department of Transportation. Section 106 of the National Preservation Act specifically requires historical sites as well as others to be assessed for impact through federally funded projects. Using GIS to assess archaeological sites that may exist or be of importance can be identified through predictive modeling. These studies and results are then used by the management to make relevant decisions and plan for future development. GIS makes this process less time consuming and more precise.\n\nThere are different processes and GIS functionalities that are used in archaeological research. Intrasite spatial analysis or distributional analysis of the information on the site helps in understanding the formation, process of change and in documentation of the site. This leads to research, analysis and conclusions. The old methods utilized for this provide limited exposure to the site and provide only a small picture of patterns over broad spaces. Predictive modeling is used through data acquisition like that of hydrography and hypsography to develop models along with archaeological data for better analysis. Point data in GIS is used to focus on point locations and to analyze trends in data sets or to interpolate scattered points. Density mapping is done for the analysis of location trends and interpolation is done to aid surface findings through the creation of surfaces through point data and is used to find occupied levels in a site. Aerial data is more commonly used. It focuses on the landscape and the region and helps interpret archaeological sites in their context and settings. Aerial data is analyzed through predictive modeling which is used to predict location of sites and material in a region. It is based on the available knowledge, method of prediction and on the actual results. This is used primarily in cultural resource management.\n\nGIS are able to store, manipulate and combine multiple data sets, making complex analyses of the landscape possible. Catchment analysis is the analysis of catchment areas, the region surrounding the site accessible with a given expenditure of time or effort. Viewshed analysis is the study of what regions surrounding the site are visible from that site. This has been used to interpret the relationship of sites to their social landscape. Simulation is a simplified representation of reality, attempting to model phenomena by identifying key variables and their interactions. This is used to think through problem formulation, as a means of testing hypothetical predictions, and also as a means to generate data.\n\nIn recent years, it has become clear that archaeologists will only be able to harvest the full potential of GIS or any other spatial technology if they become aware of the specific pitfalls and potentials inherent in the archaeological data and research process. Archaeoinformation science attempts to uncover and explore spatial and temporal patterns and properties in archaeology. Research towards a uniquely archaeological approach to information processing produces quantitative methods and computer software specifically geared towards archaeological problem solving and understanding.\n\n\n"}
{"id": "18039334", "url": "https://en.wikipedia.org/wiki?curid=18039334", "title": "Global Standards Collaboration", "text": "Global Standards Collaboration\n\nThe Global Standards Collaboration (GSC) started life as The \"Inter-regional Telecommunications Standards conference (ITSC) in 1990. This was an initiative of the T1 Committee of the United States who invited the other founding partner organizations ITU-T, ETSI and the Japanese TTC to the first ISC Meeting in Fredericksburg, VA. The goal was set by the “spirit of Melbourne”, stemming from a CCITT Plenary Assembly, to find a way of co-operation between Participating Standards Organizations (PSOs) from different regions of the world in order to facilitate global standardization within the ITU (International Telecommunication Union). The ITSC focussed its work on fixed telecommunications networks.\n\nAfter a few years, the ITSC, as a conference, became too big and was therefore reduced to GSC, the Global Standards Collaboration, with delegations limited to a reasonable size (10 maximum). The first GSC meeting was held in 1994 in Melbourne, Australia. Around the same time, it was decided to expand the scope to cover Radio Communications with the addition of a parallel set of Global Radio Standardization (RAST) meeting, the first one being hosted by ETSI in Nice in October 1994.\n\nIn the course of the time, the Australian, Canadian, Korean and Chinese national standards organizations joined bringing the number of Participating Standards Organizations to ten. Observers from other standards-related organizations and Fora are also invited to participate.\n\nIn November 2001, it was decided to rename GSC as GTSC (where T = Telecoms) and RAST as GRSC (where R = Radio) and to use the term GSC an overall \"umbrella\" for the combined plenary sessions.\n\nGSC meets approximately once every year. GSC-16 was held in Halifax, Nova Scotia, from October 31 to November 3, 2011 hosted by the ICT Standards Advisory Council of Canada (ISACC).\n\n\nAccording to the participating standards organizations, an open standard fulfills the following conditions:\n\n\n"}
{"id": "8300479", "url": "https://en.wikipedia.org/wiki?curid=8300479", "title": "Heat torch", "text": "Heat torch\n\nA heat torch is a tool or device that is used to heat up a substance quickly, whether it is air, metal, plastic, or other materials. Heat torches typically provide a way to quickly heat a concentrated area of material for uses such as molding, metallurgy, hardening, and solidification.\n\nHeat torches can be found in various medical instruments such as blood analyzers. In one application, a heat torch was installed inside a blood analyzer to make film cuvettes in real time, allowing blood to be stored and tested on board the instrument.\n\nCommonly referred to as a soldering torch, heat torches used in jewelry making are often fueled by butane, propane, MAPP gas, or a mixture of propane and oxygen. Heat torches are more effective at working with certain metals, such as sterling, gold, and copper, because they are able to heat these metals to a higher degree than traditional soldering irons and soldering guns.\n\nHeat torches have a wide range of use in manufacturing industries such as high-capacity staking, curing, drying, heat-shrinking, air heaters, sterilization, adhesive activation, air scrubbing and air knives.\n\nSpecialized plasma heat torches can be used in construction to heat up soil, a technique that has proven to be useful for providing more solid footing for buildings and structures. This is still a developing technology, but has proven to be successful in prototype testing.\n\n"}
{"id": "52082745", "url": "https://en.wikipedia.org/wiki?curid=52082745", "title": "Hive Connected Home", "text": "Hive Connected Home\n\nHive is a trademark owned by Centrica Hive Limited that produces smart home devices. It is one of the largest connected home providers in the UK and, as of May 2018, the company had more than 1,000,000 customers.\n\nUsing the Hive app and website, customers can control a range of internet-connected devices, from a thermostat, to lights, smart plugs, motion sensors, and window and door sensors.\n\nBefore the creation of the Hive brand, British Gas had been trialling a remote control heating service, starting in September 2011, using technology from AlertMe. This led to the launch of the Remote Heating Control service in 2012 paving the way for the later Hive-branded smart heating system.\n\nHive was established when parent company Centrica, owner of British Gas, formed its Connected Homes sector in 2012. The year after it was founded, the company launched Hive Active Heating on October 14, 2013, a smart thermostat that allows customers to control heating and hot water in their homes via the company's website or app. The system can be installed in customers' homes by a British Gas engineer.\n\nIn February 2015, British Gas purchased the connected home firm AlertMe in a deal worth £65 million. This allowed Hive to build its devices using the Honeycomb platform developed by AlertMe, which facilitates the capability for users to control all their smart home devices through an app. This meant Hive could more closely compete with similar platforms used by rivals such as HomeKit, Project Brillo and Nest Labs.\n\nHive Active Heating 2, the second edition of the company's smart heating system, was released in July 2015. The new model was upgraded to include two additional features. The holiday mode setting allows customers to set a given temperature while away from home. The boost function allows customers to immediately turn heating and hot water on for a period of up to six hours.\n\nThe smart thermostat was designed by Silicon Valley-based, Swiss designer Yves Béhar, who won Design Miami Design Visionary Award in 2015. It was praised for its sleek look, with critics saying it was an improvement on the company's first active heating model, which was criticised for having a dull aesthetic. However, in the same year, complaints about the lack of data security led to changes in the software. The company improved data encryption as a result of an investigation by \"Which?\" magazine.\n\nThe company also paired up with Dulux Paint to provide accessory frames for the thermostat in a range of different colours.\n\nIn 2015 the company announced future plans to use the Honeycomb platform to support and allow integration with third-party products including a Samsung camera.\n\nThe company launched two new products in January, 2016. The Active Plug enables users to attach devices, monitor whether they are on or off, remotely-controlling them using a mobile device or laptop. The Window or Door Sensor allows customers to remotely monitor when windows or doors in their home are opened or closed.\n\nThe company released the Motion Sensor on February 9, 2016. Customers can use it to keep track of movement within their homes remotely.\n\nOn June 13, 2016, the company launched the Active Light ,supplied by the Aurora Group, which users can control remotely to turn the bulb on and off and program lighting schedules.\n\nOn August 11, 2016, the company announced a partnership with IFTTT (If This, Then That) to allow the smart thermostat to connect to 300 additional products and services through expanded recipe-based automation.\n\nIn September 2016 Hive announced it will partner with Amazon Echo, allowing customers to use Alexa voice control to activate lighting, plugs and heating.\n\nOn September 27, 2016, the company launched two new smart LED bulbs that can be controlled to alter the colour and tone of the bulbs.\n\nEnergy efficiency targets set by UK energy industry regulator Ofgem aim to convert 53 million homes and small businesses to use smart gas and electricity meters by the end of 2020. Since this was announced, there has been an increase in partnerships between smart thermostat companies and utility companies.\n\nThe company is the only UK-based, connected homes organisation that provides an end-to-end service, including the creation and installation of devices while also providing customer support on an ongoing basis.\n\nBeginning with 15 staff members, by 2015 this had grown to 200. The company offers services in the UK, the Republic of Ireland and North America.\n\nThe main devices offered include a smart thermostat, lightbulbs, motion sensors for doors and windows, and what the company refers to as a smart plug - all of which operate on an interconnected system that links to a central hub connected to the internet.\n\nDevices can be controlled via the hub, either by using the company website or smartphone app (available on iOS and Android). Users can turn devices on and off both remotely and manually.\n\nIn August 2015, a Which? investigation revealed that the Hive app was a \"burglars dream\" because it was sending data that was unencrypted, including heating schedules and away settings, posing a security risk to customers should their Wi-Fi be tapped into. The company was informed about the Which? investigation in May 2015 and immediately began encrypting previously vulnerable data.\n\nThere was criticism of Hive because users could only operate devices designed for the company's system, instead of integrating multiple platforms like companies such as nCube do. However, in August 2016 it was announced the partnership with IFTTT will allow for cross-platform collaboration.\n\nIn February 2016, a glitch in the system caused the temperatures in some customers homes to rise to 32 degrees, and led to 30 out of Hive's 300,000 customers to contact the company. Following customer concerns over things such as high energy bills and the risk of fire, the company advised customers to reset their thermostats.\n"}
{"id": "744907", "url": "https://en.wikipedia.org/wiki?curid=744907", "title": "Hole punch", "text": "Hole punch\n\nA hole punch (also known as a hole puncher) most commonly refers to an office tool that is used to create holes in sheets of paper, often for the purpose of collecting the sheets in a binder or folder. The term can also refer to tools of different construction from one designed for paper, such as a those used for leather goods (generally called a leather punch), for cloth, for thin plastic sheeting, and for variations of sheet metal, such as aluminum siding or metal air ducts.\n\nA typical hole punch, whether a single or multiple hole punch, has a long lever which is used to push a bladed cylinder, the punch, straight through one or more sheets of paper and then through a close fitting hole in the die. As the vertical travel distance of the cylinder is only a few millimeters, it can be positioned within a centimeter of the lever fulcrum. For low volume hole punches, the resulting lever need not be more than for sufficient force.\n\nTwo paper guides are needed to line up the paper: one opposite where the paper is inserted, to set the margin distance, and one on an adjacent side.\n\nHole punches for industrial volumes (hundreds of sheets) feature very long lever arms, but function identically.\n\nAnother mechanism uses hollowed drills which are lowered by a screwing action into the paper. The paper is cut and forced up into the shaft of the drill to be later discarded as tightly packed columns of waste paper. This method allows a small machine to cut industrial volumes of paper with little effort.\n\nThe most common standard dimension and location of filing holes punched in paper is International Standard ISO 838. Two holes with a diameter of 6±0.5 mm are punched into the paper. The centers of these holes are 80±0.5 mm apart and have a distance of 12±1 mm to the nearest edge of the paper. The holes are located symmetrically in relation to the axis of the sheet or document.\n\nAny paper format that is at least 100 mm high (e.g. ISO A7 and larger) can be filed using this system. A printed document with a margin of 20–25 mm will accommodate ISO 838 filing holes.\n\nA four-hole extension is also commonly used. The two middle holes are punched in accordance with ISO 838, and so paper punched with the four holes can be filed in binders that are consistent with ISO 838. The two additional holes are located 80 mm above and below these. The use of two additional holes provides more stability. This extension is sometimes referred to as the \"888\" system, because of the three 8-cm gaps between the holes. Some 2-hole punches have an \"888\" marking on their paper guide, to assist punching all four holes into A4 paper.\n\nFor USA Legal Size paper format [8½ by 14 inches (215 by 356 mm)] traditionally 4 holes has been used in the past and still in use today but not as common as its sibling the standard 3 holes (see below). The 4 holes are preferred due to the extra-long length of 14-inch side of the paper where the 4 holes would be placed. Binders with 4 rings gives the paper better support in the binder. Were the documents only punched with 3 holes, this would allow sagging of the paper at the top part of the binder above the top ring. The 4 holes are positioned symmetrically with centers apart.\n\nIn regions that use the US Letter paper format [8½ by 11 inches (215 by 280 mm)]; United States, Canada, and in part Mexico and the Philippines), a three-hole standard is widely used. The holes are positioned symmetrically, with the centers apart. The diameter of the holes varies between manufacturers, with typical values being . The 5/16 value is most commonly used, as it allows for looser tolerances in both ring binder and paper punching. The distance of the hole center to the paper edge also varies, with being a typical value. Unlike ISO 838, this 3-hole system appears to have no well-established official specification, and is a de facto standard established by custom and tradition. It can only be applied to paper formats that are at least high.\n\nAnother standard also occasionally used in the US is a \"filebinder\" system. Its two holes are positioned symmetrically, with the centers apart.\n\nIn Sweden, a four-hole national standard is almost exclusively used. The centers of the holes are 21 mm, 70 mm and 21 mm apart. The guides help keep the paper in a straight line.\n\nThe official name of this four-hole system is \"triohålning\", since it was adapted to the \"Trio binder\" which was awarded Swedish patent in 1890. The binder's inventor, Andreas Tengwall, supposedly named it after a consortium consisting of himself and two companions, i.e. a trio.\nThe binder can be opened at any place while holding the papers in place, as the inner holes have guide pins from one side, the outer holes have pins from the other side.\n\nSingle hole punches are often used to punch a ticket, which indicates it has been used, or to make confetti when creating scrapbooks and other paper crafts. For applications needing a variety of hole shapes, a ticket punch may be used. A single hole punch differs from a ticket punch in having a shorter reach and no choice of hole shape.\n\nIn the United States, single hole punches are often used to punch holes through playing cards, marking them as \"used\" or \"canceled\". This helps cut down on cheating by eliminating any cards that may have been tainted by players. Paper drilling is also popular for this purpose.\n\nSingle hole punches are widely used in the UK Civil Service where papers are punched with a single hole in the upper left, and secured using a treasury tag.\n\nA related office tool is the eyelet punch. This is a single-hole punch which also crimps a metal fastening loop around the hole. It is used to permanently secure a few sheets of paper together which must not be separated or modified.\n\nA similar tool, generally known as a holing pincer, is used in animal husbandry. A common application is to attach an ear tag to a livestock animal.\n\nMultiple hole punches typically make between one and eight holes at one time, the placement of which matches the spacing of the rings in a binder. For example, the filofax system uses six holes in two groups of three. In much of the world, two-hole and four-hole punches consistent with ISO 838 are the norm.\n\nIn the US, the three-hole punch is most common. Less frequently seen is the two-hole filebinder punch.\n\nIn Japan, loose leaf in A4 and JIS B5 sizes (for binders) usually has 30 and 26 holes respectively according to the standard JIS Z 8303 (section 11); which specifies holes of 6±0.5mm of diameter, with their centers every 9.5±1mm, and a distance of 6.5±0.5mm from the center of the holes to the edge of the paper with the additional restriction that the holes must be placed in positions symmetric to the axis across the middle of the page.\n\nTo prepare documents for comb binding there are special 19-hole punches for letter paper and 23-hole punches for A4 paper. The holes are usually rectangular in shape, to accommodate the plastic binding combs. Specialized punches are also used for the similar but incompatible coil binding process.\n\nThere are office models available for the perforation of 1 to 150 sheets of paper, and industrial models for up to 470 sheets. Most multiple-hole and many single-hole punches accumulate the waste paper circles (chads) in a chamber, which must be periodically emptied in order to allow the continued operation of the punch. For large stacks of paper, a process of drilling may work better than punching.\n\nPaper drills are machines similar to a drill press that use hollow drill bits to drill through stacks of paper. The hollow bit design allows the chads to be ejected during drilling. Paper drills in the United States are most commonly either single-hole or three-hole in construction.\n\nThe origins of the hole punch date back to Germany via Matthias Theel, where two early patents for a device designed to \"punch holes in paper\" have since been discovered. Friedrich Soennecken filed his patent on November 14, 1886, for his \"Papierlocher für Sammelmappen\".\n\nA Google Doodle was used on 14 November 2017 to celebrate the 131st anniversary of the hole punch.\n\n\n"}
{"id": "57825741", "url": "https://en.wikipedia.org/wiki?curid=57825741", "title": "ISO 9846", "text": "ISO 9846\n\nISO 9846, \"Solar energy -- Calibration of a pyranometer using a pyrheliometer,\" is the ISO standard for the calibration of a pyranometer using a pyrheliometer.\n"}
{"id": "1188604", "url": "https://en.wikipedia.org/wiki?curid=1188604", "title": "Interactive kiosk", "text": "Interactive kiosk\n\nAn interactive kiosk is a computer terminal featuring specialized hardware and software that provides access to information and applications for communication, commerce, entertainment, or education.\n\nEarly interactive kiosks sometimes resembled telephone booths, but have been embraced by retail, food service and hospitality to improve customer service. Interactive kiosks are typically placed in high foot traffic settings such as shops, hotel lobbies or airports.\n\nIntegration of technology allows kiosks to perform a wide range of functions, evolving into self-service kiosks. For example, kiosks may enable users to order from a shop's catalogue when items are not in stock, check out a library book, look up information about products, issue a hotel key card, enter a public utility bill account number in order to perform an online transaction, or collect cash in exchange for merchandise. Customised components such as coin hoppers, bill acceptors, card readers and thermal printers enable kiosks to meet the owner's specialised needs.\n\nThe first self-service, interactive kiosk was developed in 1977 at the University of Illinois at Urbana–Champaign by a pre-med student, Murray Lappe. The content was created on the PLATO computer system and accessible by plasma touch screen interface. The plasma display panel was invented at the University of Illinois by Donald L. Bitzer. Lappe's kiosk, called The Plato Hotline allowed students and visitors to find movies, maps, directories, bus schedules, extracurricular activities and courses. When it debuted in the U of Illinois Student Union in April 1977, more than 30,000 students, teachers and visitors stood in line during its first 6 weeks, to try their hand at a \"personal computer\" for the first time.\n\nThe first successful network of interactive kiosks used for commercial purposes was a project developed by the shoe retailer Florsheim Shoe Co., led by their executive VP, Harry Bock, installed circa 1985. The interactive kiosk was created, manufactured and customized by ByVideo Inc. of Sunnyvale, CA. The network of over 600 kiosks provided images and video promotion for customers who wished to purchase shoes that were not available in the retail location. Style, size and color could be selected, and the product paid for on the kiosk itself. The transaction was sent to the Florsheim mainframe in St, Louis, MO, via dialup lines, for next-day home or store delivery via Federal Express. The hardware (including microcomputer, display system, touchscreen) were designed and built by ByVideo, while other components (like the CRT, floppy disk, printer, keyboard and physical housing) were sourced from other vendors. The videodisc material was created quarterly by ByVideo at Florsheim's direction, in ByVideo's state-of-the-art video production facility in CA. This kiosk network operated for over 6 years in Florsheim retail locations.\n\nIn 1991, the first commercial kiosk with internet connection was displayed at Comdex. The application was for locating missing children. The first true documentation of a kiosk was the 1995 report by Los Alamos National Laboratory which detailed what the interactive kiosk consisted of. This was first announced on comp.infosystems.kiosks by Arthur the original usenet moderator.\n\nIn 1997, KioskCom was launched to provide a tradeshow for organizations looking to deploy interactive self-service kiosks, and continues to provide these services to this day. These tradeshows occur twice a year, and offer companies education and demonstrations for successful self-service deployments.\n\nThe first company to launch a statewide interactive kiosk program was Imperial Multimedia in 2007. Imperial Multimedia installed interactive kiosks in 31 of Virginia's State Parks and these electronic kiosks included park overviews, printable maps, waypoints, points of interest, video tours of trails, and emergency information.\n\nToday's kiosks bring together the classic vending machine with high-tech communications and complex robotic and mechanical internals. Such interactive kiosks can include self-checkout lanes, e-ticketing, information and wayfinding, and vending. Electronic kiosks have become a larger part of the retail landscape as users embrace technology in their daily lives. One such example of a strong retail kiosk business is Redbox, a movie rental kiosk company. Redbox was formerly a subsidiary of Outerwall, Inc., another well-known company popular for Coinstar kiosks.\n\nThe aesthetic and functional design of interactive kiosks is a key element that drives user adoption, overall up-time and affordability. There are many factors to consider when designing an interactive kiosk including:\n\nSeveral countries have already implemented nationwide installation of kiosks for various purposes. One example of such large scale installations can be found in the United Kingdom, where thousands of special-purpose kiosks are now available to aid job-seekers in finding employment.\n\nThe United States Department of Homeland Security has created immigration kiosks where visitors register when they enter the United States. There are also Exit kiosks where visitors register when they leave the U.S.\n\nInternally the U.S. government has institutions such as the Postal Service which use HR kiosks for their disconnected employees to update their training as well as monitor and maintain their benefits.\n\nIn India, digital kiosks are used for various purposes, such as payment of bills.\n\nIt is estimated that over 1,200,000 kiosk terminals exist in the U.S. and Canada alone.\n\nGroups who use kiosks in their business environment include: Delta Airlines, United Airlines, JetBlue Airways, GTAA, Future Shop, The Home Depot, Target Corporation, and Wal-Mart.\n\nThe tele kiosk can be considered the technical successor to the telephone booth, a publicly accessible set of devices that are used for communication. These can include email, fax, SMS, as well as standard telephone service. The Telekiosk is rarely seen anymore.\n\nTelekiosks gradually appeared around the United Kingdom in the first years of the 21st century. Some are situated in shopping centres and transport terminals, with the intention of providing detailed local information. Others are in public places, including motorway service areas and airports.\n\nThe International Telecommunications Union is promoting the use of the telekiosk in Africa and parts of Asia where local people do not have access to communications technology. In part this work addresses the \"digital divide\" between rich and poor nations. There are, however, great practical benefits. The scheme in Bhutan aims to provide an E-Post system, whereby messages are relayed by telephone, then delivered by hand to rural areas, easing the problems of transporting letters across the countryside. Health, agricultural and educational information is also available.\n\nThe financial services kiosk can provide the ability for customers to perform transactions that may normally require a bank teller and may be more complex and longer to perform than desired at an ATM. These are sometimes to referred to as \"bank-in-a-box\" and the first prime example would be the Vcom units deployed in 7-11 in U.S.\n\nThese units are generally referred to 'multi-function financial service kiosks' and the first iteration was back in late 1990s with the VCOM product deployed in Southland (7-Eleven) convenience stores. Check-cashing, bill-payment and even dispensing cashcards. New multi-function machines have been deployed in \"c-store\" markets supported by Speedway and others.\n\nBy 2010, the largest bill pay kiosk network is AT&T for the phone customers which allows them customers to pay their phone bill. Verizon and Sprint have similar units for their customers\n\nAn interactive kiosk which allows users to print pictures from their digital images. The marquee example began with Kodak who had at one point had over 100,000 units up and running in the U.S. Many of these units were customized PC's with an LCD which would then print to central printer in Customer service. Two major classes of photo kiosks exist:\n\nDigital Order Stations -- This type of photo kiosk exists within retail locations and allows users to place orders for prints and photographic products. Products typically get produced in-store by a digital minilab, or at another location to be shipped directly to the consumer, or back to the store to be picked up at a later time. Digital Order Stations may or may not support instant printing, and typically do not handle payments.\n\nInstant Print Stations - This type of photo kiosk uses internal printers to instantly create photographic prints for a self serve paying customer. Often located in public locations (hotels, schools, airports), Instant Print Stations handle payments. Often such systems will only print 4x6 inch prints, although popular dye sublimation photo printers as of 2008 allow for 4x6, 5x7, 8x10, 8x12. It's more a matter of resupply labor economics and chassis size.\n\nAn Internet kiosk is a terminal that provides public Internet access. Internet kiosks sometimes resemble telephone booths, and are typically placed in settings such as hotel lobbies, long-term care facilities, medical waiting rooms, apartment complex offices, or airports for fast access to e-mail or web pages. Internet kiosks sometimes have a bill acceptor or a credit card swipe, and nearly always have a computer keyboard, a mouse (or a fixed trackball which is more robust), and a monitor.\n\nSome Internet kiosks are based on a payment model similar to vending machines or Internet café, while others are free. A common arrangement with pay-for-use kiosks has the owner of the Internet kiosk enter into a partnership with the owner of its location, paying either a flat rate for rental of the floor space or a percentage of the monthly revenue generated by the machine.\n\nOne of the first companies in North America to develop and deploy internet kiosks with touch screens via user login and password was Streetspace Inc. based out of San Francisco, California. Starting in 1999 they deployed internet kiosks across locations inside cafes, restaurants and record shops in Berkeley, California. Streetspace was also one of the first companies to roll-out targeted advertising and services to these internet kiosks based on the location of the kiosk and the profile of the user when they logged into the terminal.\n\nInternet kiosks have been the subject of hacker activity. Hackers will download spyware and catch user activity via keystroke logging. Other hackers have installed hardware keystroke logging devices that capture user activity.\n\nBusinesses that provide Internet kiosks are encouraged to use special Internet kiosk software and management procedures to reduce exposure to liability.\n\nMany amusement parks such as Disney have unattended outdoor ticketing kiosks. Amtrak has automated self-service ticketing kiosks. Busch Gardens uses kiosks for amusement parks. Lisbon Oceanarium has self-servive ticketing kiosks by Partteam & Oemkiosks. Cruise ships use ticketing kiosks for passengers. Check-in Kiosks for auto rental companies such as Alamo and National have had national deployments.\nThe ticket halls of train stations and metro stations have ticketing kiosks that sell transit passes, train tickets, transit tickets, and train passes.\n\nMany movie theater chains have specialized ticket machines that allow their customers to purchase tickets and/or pick up tickets that were purchased online. Radiant and Fujitsu have been involved in this segment.\n\nA new way to order in-cafe from tablet kiosks. Kiosks are available in addition to cashier stations so that wait time is reduced for all guests. The kiosk is highly visual and includes a product builder to assist with order accuracy and customization.\n\nAn example of a vending kiosk is that of the DVD rental kiosks manufactured by several manufacturers, where a user can rent a DVD, secured by credit card for $1 per day.\n\nOne of the larger DVD Vending Kiosks companies is Outerwall, Inc who owns and operates the Redbox kiosks throughout North America. They also operate Coinstar kiosks as well.\n\nA visitor management and security kiosk can facilitate the visitor check in process at businesses, schools, and other controlled access environments. These systems can check against blacklists, run criminal background checks, and print access badges for visitors. School security concerns in the United States have led to an increase in these types of kiosks to screen and track visitors.\n\nMany shopping malls, hospitals, airports and other large public buildings use interactive kiosks to allow visitors to navigate in buildings. Harris County Hospital District, Baptist Hospital of Miami, the Children's Hospital of Philadelphia and the Cayuga Medical Center are but a few medical centers utilizing interactive touch screen kiosks with a building directory and wayfinding solution.\n\nHospitals and medical clinics are looking to kiosks to allow patients to perform routine activities. Kiosks that allow patients to check in for their scheduled appointments and update their personal demographics reduce the need to line up and interact with a registration clerk. In areas where patients must make a co-pay, kiosks will also collect payment. As the requirements for documentation, waivers and consent increase, kiosks with integrated signature capture devices are able to present the documentation to the patient and collect their signature. A business case for registration and check-in kiosks is built around:\nA large community hospital has been able to reduce their registration staff by 30%, improve data quality, and shorten lineups.\n\nMuseums, historical sites, national parks and other tourist/visitor attractions often engage kiosks as a method for conveying information about a particular exhibit or site. Kiosks allows guests to read about - or view video of - particular artifacts or areas at their own pace and in an interactive manner, learning more about those areas that interest them most. The Rockwell Museum in New York City uses touchscreen tablets to provide visitors with accessible and relevant labels for a particular exhibit. The Penn State All sports museum employs interactive kiosks to display up to date information about past and current Penn State athletes and sports teams. The Ellis Island National Museum of Immigration now boasts a citizen test available for visitors to take online via an informational kiosk. Additional kiosk displays include a \"Threads of Migration\" interactive exhibition featuring three touch-screen kiosks as part of \"The Journey: New Eras of Immigration\" section, which covers immigration since 1954.\n\nVideo kiosk integrates video conferencing and collaboraton capabilities to help users run video calls or conferences with available operators, view content or exchange messages. Video kiosks are often used in banking or telemedicine for improved customer service. \n\nReliability is an important consideration, and as a result many specialised kiosk software applications have been developed for the industry. These applications interface with the bill acceptor and credit card swipe, meter time, prevent users from changing the configuration of software or downloading computer viruses and allow the kiosk owner to see revenue. Threats to reliability come from vulnerabilities to hacking, allowing access to the OS, and the need for a session or hardware restart.\n\nThe kiosk industry is divided into three segments: kiosk hardware, kiosk software, and kiosk application. Kiosk software locks down your operating system (be it Apple, Windows, Android, or Linux or ChromeOS) to restrict access and/or functionality of a kiosk hardware device. This allows for users to interact with an application that serves a self-service purpose such as those mentioned above. \nHistorically electronic kiosks though are standalone enclosures which accept user input, integrate many devices, include a software GUI application and remote monitoring and are deployed widely across all industry verticals. This is considered \"Kiosk Hardware\" within the kiosk industry.\n\nPOS-related \"kiosks\" are \"lane busting\" check-outs such as seen at large retailers like Home Depot and Kroger.\n\nSimple touchscreen terminals or panel-pcs are another segment and enjoy most of their footprint in POS retail applications and typically facing the employee. Terminals include NCR Advantage (740x terminal) and the IBM Anyplace computer terminal. These units are considered \"kiosks\" only in functionality delivered and typically only incorporate touchscreen, bar code scanner and/or magnetic stripe reader.\n\nMarket segments for kiosk and self-service terminal manufacturers include photo kiosks, government, airlines, internet, music, retail loyalty, HR and financial services, just to name some.\n\nThis segment includes healthcare patient check-in and \"take a number\" type custom flow. Devices range from simple ticket dispense to biometrics (fingerprint readers) for patient check-in.\n\nThe basic application of kiosks in the hotel industry is to reduce waiting time for guests at check-in/checkout and relieve the reception desk. Usually, hotel kiosks are located in the lobby and are integrated or interface with the hotel's property management system. The machines allow guests to fill in and sign a registration card, select a room, issue hotel key cards, check for extra offers or upgrades and book and pay for them. \nIn retail, clients can place online orders in store for home delivery, avoid queuing in fast food restaurants and issue library books. Generally, kiosks are seen as an enhancement to a retail or hospitality offer rather than a replacing staff members.\n\n"}
{"id": "1890199", "url": "https://en.wikipedia.org/wiki?curid=1890199", "title": "Interactive whiteboard", "text": "Interactive whiteboard\n\nAn interactive whiteboard (IWB) is a large interactive display in the form factor of a whiteboard. It can either be a standalone touchscreen computer used independently to perform tasks and operations, or a connectable apparatus used as a touchpad to control computers from a projector. They are used in a variety of settings, including classrooms at all levels of education, in corporate board rooms and work groups, in training rooms for professional sports coaching, in broadcasting studios, and others.\n\nThe first interactive whiteboards were designed and manufactured for use in the office. They were developed by PARC around 1990. This board was used in small group meetings and round-tables. \n\nThe interactive whiteboard industry was expected to reach sales of US$1 billion worldwide by 2008; one of every seven classrooms in the world was expected to feature an interactive whiteboard by 2011 according to market research by Futuresource Consulting. In 2004, 26% of British primary classrooms had interactive whiteboards. The Becta Harnessing Technology Schools Survey 2007 indicated that 98% of secondary and 100% of primary schools had IWBs. By 2008, the average numbers of interactive whiteboards rose in both primary schools (18 compared with just over six in 2005, and eight in the 2007 survey) and secondary schools (38, compared with 18 in 2005 and 22 in 2007).\n\nAn interactive whiteboard (IWB) device can either be a standalone computer or a large, functioning touchpad for computers to use.\n\nA device driver is usually installed on the attached computer so that the interactive whiteboard can act as a Human Input Device (HID), like a mouse. The computer's video output is connected to a digital projector so that images may be projected on the interactive whiteboard surface.\n\nThe user then calibrates the whiteboard image by matching the position of the projected image in reference to the whiteboard using a pointer as necessary. After this, the pointer or other device may be used to activate programs, buttons and menus from the whiteboard itself, just as one would ordinarily do with a mouse. If text input is required, user can invoke an on-screen keyboard or, if the whiteboard software provides for this, utilize handwriting recognition. This makes it unnecessary to go to the computer keyboard to enter text.\n\nThus, an IWB emulates both a mouse and a keyboard. The user can conduct a presentation or a class almost exclusively from the whiteboard.\n\nIn addition, most IWBs are supplied with software that provides tools and features specifically designed to maximize interaction opportunities. These generally include the ability to create virtual versions of paper flipcharts, pen and highlighter options, and possibly even virtual rulers, protractors, and compasses—instruments that would be used in traditional classroom teaching.\n\nUses for interactive whiteboards may include:\n\nThe majority of IWBs sold globally involve one of four forms of interaction between the user and the content projected on the whiteboard. These are an infrared scan technology, a resistive, touch-based board, an electromagnetic pen and associated software, and an ultrasonic pen.\n\nAn infrared interactive whiteboard is a large interactive display that connects to a computer and projector. The board is typically mounted to a wall or floor stand. Movement of the user's finger, pen, or other pointer over the image projected on the whiteboard is captured by its interference with infrared light at the surface of the whiteboard.\nWhen the whiteboard surface is pressed, software triangulates the location of the marker or stylus. Infrared IWBs may be made of any material, no dry-erase markers are involved, and may be found in many settings, including various levels of classroom education, corporate boardrooms, training or activity rooms for organizations, professional sports coaching facilities, and broadcasting studios.\n\nA touch-based IWB also involves a simple pointing device. In this case, the material of the board is important. In the most common resistive system, a membrane stretched over the surface deforms under pressure to make contact with a conducting backplate. The touch point location can then be determined electronically and registered as a mouse event. For example, when a finger is pressed on the surface, it is registered as the equivalent of the left mouse click. Again, such a board requires no special instruments. This leads to the claim of resistive systems manufacturers that such a whiteboard is easy and natural to use. It is, however, heavily dependent on the construction of the board itself.\n\nAn electromagnetic pen-based interactive whiteboard involves an array of wires embedded behind the solid board surface that interacts with a coil in the stylus tip to determine the horizontal and vertical coordinates of the stylus. The pen itself usually is passive, i.e., it contains no batteries or other power source; it alters the electrical signals produced by the board. For instance, when close to the surface of the board, the mouse pointer can be sensed, giving the board \"mouse-over\" capabilities. When it is pressed in against the board in one way, the board activates a switch in the pen to signal a mouse click to the computer; pressed in another way, contact with the board signals a click of the right mouse-button. Like a scaled-up version of a graphics tablet used by professional digital artists and designers, an electromagnetic IWB can emulate mouse actions accurately, will not malfunction if a user leans on the board, and can potentially handle multiple inputs.\n\nThis technology uses infrared light and ultrasound positioning technology. The technology works in a similar way to lightning in a thunderstorm by computing the time difference between the speed of light and the speed of sound. An infrared IWB is also available in a portable format. After moving the set-up to a new location, the system acquires connection to the computer with a simple re-calibration of the projected image — again using the electronic pen. The device or bar scans a bracketed area (usually 3m by 1.5m, giving a whiteboard that is 110\" wide). Typically, multiple brackets can be added, providing for users at different sites to share the same virtual whiteboard.\n\nA portable IR pen-based whiteboard works on a variety of surfaces — an existing whiteboard, a flat wall, even a chalkboard with dry-erase paint, transforms those surface into an interactive whiteboard. No battery is required for USB signal receiver and the unit can be mounted to the ceiling if a permanent solution is required. Made of a tiny and lightweight material, the PIWB is easy to transport.\n\nA Wii-based IR system was invented by Johnny Chung Lee, PhD. in 2007. Lee claimed that the system \"[m]akes a technology available to a much wider percentage of the population\" (Speaking at TED, April 2008) by using an ordinary Wii remote control as a pointer and the IR camera on the front of the remote control as tracking device sensing light from an IR light pen. Lee produced several videos on YouTube about this system to demonstrate its operability, flexibility, and ease of use, and pointing out its modest price — the most inexpensive part is the infrared LED of the pen. This is an approach with a shallow learning curve since the gaming system is already familiar to many. A large programming support community may be available, both in opensource and commercial offerings.) However, the system cannot be used near direct sunlight, nor can it share the software of manufacturers of the IWB-types already mentioned. Certain considerations about the Bluetooth connection of the light pen also apply. Two lines of sight are involved (the controller and the pen) in the case of rear-projection case. unlike many others.)\n\nAn interactive projector IWB involves a CMOS camera built into the projector, so that the projector produces the IWB image, but also detects the position of an active IR light pen when it contacts the surface where the projected image. This solution, developed in 2007 and patented in 2010 by U.S. manufacturer Boxlight, like the other IR whiteboard systems, can suffer from potential problems caused by 'line of sight' between the pen and the projector/receiver and, like them also, does not provide mouse-over capability found in other solutions.\n\nIn some classrooms, interactive whiteboards have replaced traditional whiteboards or flipcharts, or video/media systems such as a DVD player and TV combination. Even where traditional boards are used, the IWB often supplements them by connecting to a school network digital video distribution system. In other cases, IWBs interact with online shared annotation and drawing environments such as interactive vector based graphical websites.\n\nBrief instructional blocks can be recorded for review by students — they will see the exact presentation that occurred in the classroom with the teacher's audio input. This can help transform learning and instruction.\n\nMany companies and projects now focus on creating supplemental instructional materials specifically designed for interactive whiteboards. Electrokite out of Boston, MA, for example, will have the first complete curriculum for schools and districts.\n\nOne recent use of the IWB is in shared reading lessons. Mimic books, for instance, allow teachers to project children's books onto the interactive whiteboard with book-like interactivity.\n\nDixons City Academy in the North of England was the first non college or university learning environment to make use of interactive whiteboards after the school's then principal Sir John Lewis showed a keen interest in the developing technology. An interactive whiteboard can now be found in every classroom of the school.\n\nSome manufacturers also provide classroom response systems as an integrated part of their interactive whiteboard products. Handheld 'clickers' operating via Infrared or Radio signals, for example, offer basic multiple choice and polling options. More sophisticated clickers offer text and numeric responses and can export an analysis of student performance for subsequent review.\n\nBy combining classroom response with an interactive whiteboard system, teachers can present material and receive feedback from students in order to direct instruction more effectively or else to carry out formal assessments. For example, a student may both solve a puzzle involving math concepts on the interactive whiteboard and later demonstrate his or her knowledge on a test delivered via the classroom response system. Some classroom response software can organize and develop activities and tests aligned with State standards.\n\nThere are now several studies revealing contradictory conclusions about the effect of the use of IWBs is effective on student learning. A compilation of this research is available.\n\nAccording to the findings of a study conducted by the London Institute of Education with the funding of the DfES evaluated the educational and operational effectiveness of the London Challenge element of the adoption of the use of interactive whiteboards in the London area under a program called \"the Schools Whiteboard Expansion project.\" At Key Stage 3, interactive whiteboards here associated with little significant impact on student performance in Mathematics and English and only a slight improvement in Science. In the same schools, at Key Stage 4, use of interactive whiteboards was found to have negative effects for Mathematics and Science, but positive effects for English. The authors cite several possible causes for the Key Stage 4 findings, including: a Type II statistical error, disruption to teaching methods leading to reduced pupil performance when IWBs were installed, or a non-random deployment decision of IWB installation resulting in a skew of the data.\n\nAt the same time, there is evidence of improved performance gains with the use of interactive whiteboards. The BECTA (UK) commissioned a study into the impact of Interactive Whiteboards over a two-year period. This study showed a very significant learning gains, particularly with second cohorts of students, where they benefited from the teacher's experience with the device.\n\nBetween 2003 and 2004, The DfES Primary Schools Whiteboard Expansion project (PSWE) provided substantial funding to 21 local authorities for the acquisition and use of interactive whiteboards in UK primary schools. The BECTA-sponsored study investigated the impact of this investment with 20 local authorities, using data for 7272 learners in 97 schools.\n\nVariables considered in the research included length of exposure to interactive whiteboard technology, the age of pupils (down to individual birthdays), gender, special needs, entitlement to free schools meals and other socio-economic groupings. The implementation and impacts of the project were evaluated by a team at Manchester Metropolitan University, led by Professor Bridget Somekh. To date it is the largest and longest study conducted into the impact of interactive whiteboards.\n\nThe principal finding of this large-scale study was that, \"[w]hen teachers have used an interactive whiteboard for a considerable period of time (by the autumn of 2006 for at least two years) its use becomes embedded in their pedagogy as a mediating artefact for their interactions with their pupils, and pupils’ interactions with one another.\" The authors of the study argued that \"mediating interactivity\" is a sound concept, offering \"a ... theoretical explanation for the way in which the multi-level modelling (MLM) analyses link the length of time pupils have been taught with interactive whiteboards to greater progress in national test scores year on year.\"\"\n\nThe research showed that interactive whiteboard technology led to consistent gains across all key stages and subjects with increasingly significant impact on the second cohorts, indicating that embedding of the technology into the classroom and teacher experience with the technology are key factors.\n\nGains were measured in ‘months progress’ against standard measures of attainment over the two-year study period.\n\nIn infant classes, ages 5–7:\n\nThere was also clear evidence of similar impacts in Key stage two – ages 7 – 11\nThere was no adverse impact observed at any level.\n\nGlover & Miller conducted a study on the pedagogic impact of interactive whiteboards in a secondary school. They found that although interactive whiteboards are theoretically more than a computer if it is only being used as an adjunct to teaching its potential remains unrealized. The authors’ research was primarily to ascertain the extent and type of use in the classroom.\nIn order to determine if any change in pedagogy or teaching strategies was taking place the researchers conducted a detailed questionnaire. The authors found that the teachers used the IWBs in one of three ways; as an aid to efficiency, as an extension device, and as a transformative device. They noted that teachers’ use of the technology was not primarily affected by training, access, or software availability. When used as a transformative device (approximately 10% of teachers taking part in the study) the impact on pedagogy was transformative.\n\nIn recent times, manufacturers of IWB technology have been setting up various online support communities for teachers and educational institutions deploying the use of the interactive whiteboards in learning environments. Such websites regularly contribute research findings and administer free whiteboard lessons to promote widespread use of interactive whiteboards in classrooms.\n\nSome of the benefits of using interactive whiteboards include:\n\nAccording to a June 11, 2010 \"Washington Post\" article:\n\nThe same article also quotes Larry Cuban, education professor emeritus at Stanford University:\n\nAn article posted on the National Association of Secondary School Principals web site details pros & cons of interactive whiteboards.\nA report on interactive whiteboards from London's Institute of Education said:\n\n\nThe report highlighted the following issues:\n\n\nThere are a number of literature reviews, findings and papers on the use of interactive whiteboards in the classroom:\n\n\nInteractive whiteboards may use one of several types of sensing technology to track interaction on the screen surface: resistive, electromagnetic, infrared optical, laser, ultra-sonic, and camera-based (optical).\n\n\n\n\n\nPermanent markers and use of regular dry erase markers can create problems on some interactive whiteboard surfaces, because interactive whiteboard surfaces are most often melamine, which is a porous, painted surface that can absorb marker ink. Punctures, dents and other damage to surfaces are also a risk.\n\nSome educators have found that use of interactive whiteboards reinforces an age-old teaching method—teacher speaks, students listen. This teaching model is contrary to many modern instructional models, such as the Madeline Hunter-derived instructional delivery model.\n\nInteractive whiteboards are generally available in two forms: front projection and rear projection.\n\n\nSome manufacturers also provide an option to raise and lower the display to accommodate users of different heights.\n\nSome manufacturers offer short-throw projection systems in which a projector with a special wide angle lens is mounted much closer to the interactive whiteboard surface and projects down at an angle of around 45 degrees. These vastly reduce the shadow effects of traditional front-projection systems and eliminate any chance for a user to see the projector beam. The risk of projector theft, which is problematic for some school districts, is reduced by integrating the projector with the interactive whiteboard.\n\nSome manufacturers have provided a unified system where the whiteboards, short throw projection system and audio system are all combined into a single unit which can be set at different heights and enable young children and those in wheelchairs to access all areas of the board. Reduced installation costs make these short-throw projection systems cost effective.\n\nIn most cases, the touch surface must be initially calibrated with the display image. This process involves displaying a sequence of dots or crosses on the touch surface and having the user select these dots either with a stylus or their finger. This process is called alignment, calibration, or orientation. Fixed installations with projectors and boards bolted to roof and wall greatly reduce or eliminate the need to calibrate.\n\nA few interactive whiteboards can automatically detect projected images during a different type of calibration. The technology was developed by Mitsubishi Electric Research Laboratories Inc and is disclosed in patent 7,001,023. The computer projects a Gray Code sequence of white and black bars on the touch surface and light sensitive sensors behind the touch surface detect the light passing through the touch surface. This sequence allows the computer to align the touch surface with the display; however, it has the disadvantage of having tiny fiber-sized \"dead spots\" in the resistive touch surface where the light sensors are present. The \"dead spots\" are so small that touches in that area are still presented to the computer properly.\n\nAnother system involves having a light sensor built into the projector and facing the screen. As the projector generates its calibration image (a process called \"training\"), it detects the change in light reflected from the black border and the white surface. In this manner it can uniquely compute all the linear matrix transform coefficients.\n\nYet another system includes a camera built into the handheld pen, with human imperceptible targets injected into the image stream sent to the projector or display, containing positioning information, where the camera detects that information and calculates position accordingly, requiring no calibration at all. Such a technology and system is integrated into penveu, and is further disclosed in patent 8,217,997\n\nA variety of accessories is available for interactive whiteboards:\n"}
{"id": "2534015", "url": "https://en.wikipedia.org/wiki?curid=2534015", "title": "Jade Raymond", "text": "Jade Raymond\n\nJade Raymond (born 28 August 1975) is a Canadian video game producer and executive, and the founder of Ubisoft Toronto and Motive Studios.\n\nJade Raymond was born 28 August 1975 in Montreal She graduated from St. George's School of Montreal in 1992 and Marianopolis College in 1994. She received a Bachelor of Science degree from McGill University in 1998, where she majored in computer science. Her first post-university job was as a programmer for Sony, where she eventually helped in the creation of Sony Online's first Research and Development group. This led to Electronic Arts where she worked as a producer on \"The Sims Online\". From 2003-2004, Raymond joined the G4 program \"The Electric Playground\" as a part-time correspondent, working with Victor Lucas, Tommy Tallarico and Julie Stoffer. In 2004, she started working for Ubisoft Montreal, where she led the creation of the first \"Assassin's Creed game.\" Raymond went on to become executive producer on \"Assassin's Creed II\", and was executive producer of new IP at Ubisoft Montreal, which included \"Watch Dogs\" and \"The Mighty Quest for Epic Loot\".\n\nIn January 2010, Raymond moved to Toronto to build a new studio for Ubisoft in the role of managing director. Raymond is also on the Board of Directors of WIFT-T, an organization dedicated the advancement of women across film, television and screen-based industries.\n\nOn 20 October 2014, Ubisoft and Raymond announced they were parting ways. Raymond did not announce at the time what she would be moving on to.\n\nIn July 2015, Raymond announced that she had joined Electronic Arts and formed Motive Studios, based in Montreal. She was also to be in charge of Visceral Games studio, located in California, where she worked with games designer and writer Amy Hennig on Star Wars games and also develop new original IP, before the company was shut down by EA.\n\nOn 22 October 2018, it was confirmed that Jade Raymond left EA to pursue new opportunities.\n"}
{"id": "17210883", "url": "https://en.wikipedia.org/wiki?curid=17210883", "title": "Louis XVI style", "text": "Louis XVI style\n\nLouis XVI style, also called Louis Seize, is a style of architecture, furniture, decoration and art which developed in France during the 19-year reign of Louis XVI (1774–1793), just before the French Revolution. It saw the final phase of the baroque style as well as the birth of French neoclassicism. The style was a reaction against the elaborate ornament of the preceding baroque period. It was inspired in part by the discoveries of ancient Roman paintings, sculpture and architecture in Herculaneum and Pompeii. Its features included the straight column, the simplicity of the post-and-lintel, the architrave of the Greek temple. It also expressed the Rousseau-inspired values of returning to nature and the view of nature as an idealized and wild but still orderly and inherently worthy model for the arts to follow.\n\nNotable architects of the period included Victor Louis (1731-1811) who completed the theater of Bordeaux (1780), The Odeon Theater in Paris (1779-1782) was built by Marie-Joseph Peyre (1730-1785) and Charles de Wailly (1729-1798). François-Joseph Bélanger completed the Chateau de Bagatelle in just sixty-three days to win a bet for its builder, the King's brother. Another period landmark was the belvedere of the Petit Trianon, built by Rchard Mique. The most characteristic building of the late Louis XVI residential style is the Hôtel de Salm in Paris (Now the Palais de la Légion d'Honneur, built by Pierre Rousseau in 1751-83.\n\nSuperbly crafted desks and cabinets were created for the Palace of Versailles and other royal residences by cabinetmakers Jean-Henri Riesener and David Roentgen, using inlays of fine woods, particularly mahogany, decorated with gilded bronze and mother of pearl. Equally fine sets of chairs and tables were made by Jean-Henri Riesener and Georges Jacob.\n\nThe Royal tapestry works of Gobelins, Aubusson and Beauvais Tapestry continued to make large tapestries, but an increasing part of their business was the manufacture of upholstery for the new sets of chairs, sofas and other furnishings for the royal residences and nobility. Wallpaper also became an important part of interior design, thanks to new processes developed by Reveillon.\n\nThe Lous XVI style was a reaction to and transition the French Baroque style, which had dominated French architecture, decoration and art since the mid-17th century, and partly from a desire to establish a new \"Beau idéal\", or ideal of beauty, based on the purity and grandeur of the art of the Ancient Romans and Greeks. In 1754 The French engraver, painter and art critic Charles-Nicolas Cochin denounced the curves and undulations of the predominant rocaille style: \"Don't torture without reason those things which could be straight, and come back to the good sense which is the beginning of good taste.\"\n\nLouis XVI himself showed little enthusiasm for art or architecture. He left the management of these to Charles-Claude Flahaut de la Billaderie, the Count of Angiviller, who was made Director General of Buildings, Gardens, Arts, Academies and Royal Manufactories. Angeviller, for financial reasons, postponed a grand enlargement of the Palace of Versailles, but completed the new Château de Compiègne (1751–83), begun by Louis XV, and decorated it from 1782 to 1786. The King's principal architectural addition to Versailles was the new library on the first floor (begun 1774). He was much more generous to Queen Marie-Antoinette; she redecorated the Grand Apartments of the Queen at Versailles in 1785, and carried out important works on her apartments at the Palace of Fontainebleau and Compiegne, as well as new apartments in the Tuileries Palace. The King also gave the Queen the Petit Trianon at Versailles, and in 1785 bought a new chateau for her at St. Cloud.\n\nClassicism, based Roman and Greek models had been used in French architecture since the time of Louis XIV; he rejected a plan by Gian Lorenzo Bernini for a baroque facade of the Louvre, and chose instead a classical facade with a colonnade and pediment. The architects of Louis XIV, Jules Hardouin-Mansart and Jacques Lemercier, turned away from the gothic and renaissance style and used a baroque version of the Roman dome on the new churches at Val-de-Grace and Les Invalides. Louis XV and his chief architects, Jacques Ange Gabriel and Jacques-Germain Soufflot continued the style of architecture based upon symmetry and the straight line. Gabriel created the ensemble of classical buildings around the Place de la Concorde while Soufflot designed the Panthéon (1758–90) on the Roman model.\n\nAn influential building from the late Louis XV period was the Petit Trianon at Versailles (1762-1764), by Jacques Ange Gabriel, built for the mistress of the King, Madame Pompadour. Its cubic form, symmetric facade and Corinthian peristyle, similar to the villas of Palladio, made it model for the following Louis XVI style.\n\nAnother notable influence on the style was the architecture of the Renaissance architect Palladio, which influenced the building of country houses in England, as well as the French architect Claude-Nicolas Ledoux (1736-1806). Palladio's ideas were the inspiration for the Château de Louveciennes, and its neoclassical music pavilion (1770–71) built by Claude Nicolas Ledoux for the mistress of Louis XV, Madame du Barry. The pavilion is cubic in form, with a facade of four pilasters supporting the architrave and the pilaster of the terrace. It became the model for similar houses under Louis XVI.\n\nNotable monuments of Louis XVI civil architecture include the Hotel de la Monnaie in Paris (1771–76) by Jacques Denis Antoine, as well as the Palais de Justice in Paris by the same architect; and the theater of Besançon (1775) and the Chateau de Benouville in the Calvados, both by Ledoux. The latter building has geometric architecture, a flat ceiling, and a portico in the colossal order of corinthian columns. The École de Chirurgie, or School of Surgery in Paris by Jacques Gondoin (1769) adapted the forms of the neoclassical town house, with a court of honor placed between a pavilion with a colonnade on the street and the main building. He also added a peristyle and another floor above the columns, and transformed he entrance to the courtyard into a miniature triumphal arch.\nTheaters in Paris and Bordeaux were prominent examples of the new style. The architect Victor Louis (1731-1811) completed the theater of Bordeaux (1780); its majestic stairway was a forerunner of the stairway of the Paris Opera Garnier. In 1791, in the midst of the French Revolution, he completed the Comedie Francaise. The Odeon Theater in Paris (1779-1782) was built by Marie-Joseph Peyre (1730-1785) and Charles de Wailly (1729-1798). It featured a portico in the form of a covered gallery and columns in advance of the facade.\n\nOne of the best-known buildings of the period is the small Chateau de Bagatelle, (1777) designed and built by François-Joseph Bélanger for the Comte d'Artois, Louis XVI's brother.. The small chateau was designed and completed in just sixty three days, to win a bet with Marie Antoinette that he could build a chateau in less than three months. Marie Antoinette had a similar small neoclassical belvedere created by architect Richard Mique, who had also designed her picturesque rustic village in the gardens. it was completed in 1789, the year of the French Revolution. \nAnother unusual architectural project was the transformation of the Palais Royal in the heart of Paris, into a grand shopping mall. In 1781 the Duc de Chartres, needing money, commissioned architect Victor Louis to create an arcade of shops, cafes and clubs on the ground floor. In 1788 he added a covered cirque in the center, a covered promenade and space for concerts and entertainments, with a trellis roof supported by seventy-two ionic columns.\n\nThe most characteristic building of the late Louis XVI residential style is the Hôtel de Salm in Paris (Now the Palais de la Légion d'Honneur, built by Pierre Rousseau in 1751-83. The facade is distinguished by its simplicity and purity, and its harmony and balance. A colonnade of corinthian columns supports the entablement of the rotunda, which is surmounted by statues. The facade is also animated by busts of roman emperors in niches, and sculptures in relief above the windows of the semicircular central avant-corps.\n\nThe Pantheon, designed by Jacques Germain Soufflot as the Church of Sainte-Geneviève and begun in 1757 under Louis XV, was the most prominent example of religious architecture under construction during the period. It replaced the colossal columns modeled after those of the Church of the Jesu and St. Peter's in Rome with slender, graceful corinthian columns supporting a continuous entablement. The plan was also classical; the long nave with a vaulted ceiling was replaced by a Greek cross, with the dome in the center. Soufflot employed novel engineering techniques to support the dome; a system of contreforts and arches, and the use of iron bars to support the stone structure. The building was begun in 1764 but not completed until 1790, after the Revolution.\n\nAnother important church completed in the Louis XVI period was Saint-Philippe-du-Roule (1768-1784) by Jean-François-Thérèse Chalgrin. It was one of the last churches finished before the Revolution. The church is inspired by paleo-Christian architecture; it features massive columns and a pediment, and an interior with vaulted ceiling that suggests a vast Roman basilica.\nThe architect Claude-Nicolas Ledoux specialized in designing functional buildings in greatly simplified the classical style. Examples included his simplified neoclassical design for the customs barrier at La Villette in Paris (1785-1789), with its classical facade and rotunda. He was especially known for his project for the Royal Saltworks at Arc-et-Senans (1775–79) This was a model industrial site, in an elliptical shape, with the house of the factory director in the center, with a rustic neoclassical colonnade, surrounded by the workshops, storerooms and offices in concentric rings.\n\nEtienne-Louis Boullée (1728-1799) was another visionary architect of the period; his projects, never built, included a monument to Isaac Newton (1784) in the form of an immense dome, with a perforated top allowing the light to enter, giving the impression of a sky full of stars. His project for an enlargement of the Royal Library (1785) was even more dramatic, with a gigantic arch sheltering the collection of books. While none of his projects were ever built, the images were widely published and inspired architects of the period to look outside the traditional forms.\nThe Louis XVI style of decoration marked the triumph of neo-classicism, which had been underway in Europe since 1770. It reflected the murals and designs found in the early archeological excavations in Herculaneum and Pompei, and the travels of groups of artists to Greece and Asia Minor. The \"Taste Pompeiian\" was followed by \"the taste Entruscan\". Motifs in interior decoration included arabesques and grotesques on the Pompeiian model. Bas-reliefs in the Greek and Roman style were popular, often in the form of rectangual friezes in bronze on furniture, or stucco, marble, molded stucco, baked earth, or simply painted in tromp l'oeil over doors. Other popular motifs included garlands of oak leaves or olive leaves; interlaced flowers, ribbons or vines; crowns of roses, flaming torches, horns of plenty, and particularly vases from which emerged flowers or vines.\n\nIn the early part of the reign of Louis XVI, interior decoration was designed to overwhelm the viewer with its scale, majesty and opulence. Grand halls served multiple purposes, for theater entertainments, balls, or banquets. An example of the early Louis XVI style is the dining room of the Chateau de Maisons, rebuilt between 1777-1782 by François-Joseph Bélanger for the Comte d'Artois, the brother of Louis XVI. This dining room, inspired by grand style of Louis XIV and Louis XV. It features columns of the colossal order, inches, frontons, consoles, sculpture in relief, and a gigantic fireplace.\n\nLater in the reign, the tendency shifed to smaller, more intimate and comfortable salons, studies, dining rooms and boudoirs, as the Cabinet Doré of Marie Antoinette at the Palace of Versailles (1783) and the boudoir of Marie Antoinette at Fontainebleau, in the Pompeiian style (1785). The Pompeiian style featured mythical animals, such as sphinxes and griffons, horns of plenty, and vases of flowers mounted on tripods. The style also was frequently used in friezes and cameos, in medallions and in white on blue Wedgewood porcelain. In the laster years of the Louis XVI style, the decorative panels were divided into often geometric divisions, either circles or octagons,\n\nLouis XVI style furniture, particularly the furniture made for the royal palaces, is among the most finely-crafted and valuable ever produced in France. Much of it was produced at the \"Garde-Meuble du Roi\", the royal furniture worship, directed by Francois II Foliot (1748-1808). produced at the Among the notable craftsmen of the period were Georges Jacob, who made a suite of sofas and chairs for the apartments of Marie Antoinette at Versailles and for those of the Comte d'Artois, the King's brother, at the Temple. Oak, mahogany and walnut were the woods most commonly used. The chairs of the early period made for Marie Antoinette were richly decorated gilded carvings, usually with floral patterns. The chairs and sofas were usually upholstered in satin, with more elaborate medallions embroidered in silk attached. Later in the period, more exotic themes, often taken from popular theater productions in Paris, appeared in decoration of furniture,{ These included Chinese, Arabesque, and Etruscan figures. A variety of specialized pieces of furniture were created. These included lightweight chairs for men sitting at gambling tables, and specialized chairs for boudoirs, dressing rooms, libraries, and antechambers.\n\nThe beds, especially in the \"chambers de parade\" or ceremonial bedrooms of the royal palaces, were of extraordinary proportions, and were usually separated from the rest of the room by a balustrade. reached monumental proportions. These beds were termed \"à la Duchesse\", and featured an ornate canopy over the bed. The sculpted and gilded wood frame of the silk embroidered canopy over the bed of Marie Antoinette at Fontainebleau, installed in 1787, was so heavy that two additional columns were placed under it at night avoid its collapse. \nThe craft of the \"Ebenist\", or cabinet-maker, was considered separate from that of other furniture-makers. About a third of the \"Ebenists\" in Paris were of foreign origin, either second-generation immigrants from Belgium and the Netherlands or first-generation from the Rhineland. The latter group included some of the most famous craftsmen, including Jean-Henri Riesener, who became a master in 1768, and David Roentgen. They received special protection and patronage from Marie-Antoinette, who admired German craftsmanship.\n\nSeveral new varieties of the furniture were introduced, including a commode in the form of half-moon form, and the \"commode dessert\", which had a door in the front with shelves on either side. The \"commode bonheur-du-jour\" was a dressing table for a boudoir, with a small armoire on top, with either a mirror or a curtain. The \"table a la Tronchin\" was a table with a built-in-shelf which could be raised by a mechanism for reading. some of the furniture was small and designed to be easily moved, to quickly re-earrange salons. These included the \"table bouillotte\", a small round table with four legs and drawer.\n\nThe tables and cabinets were usually decorated with sculpted and gilded bronze ornament, often in the forms of stylized roses, knotted ribbons, or pine cones. The surfaces were frequently inlaid with plaques of different colored exotic woods or mother-of-pearl, forming either a checkerboard pattern, a pattern of cubes, or more intricate designs. The Commodes Sometimes the wood was dyed to achieve the color contrast, or pieces of wood were laid with the grain in different directions. Rissener were especially known for their richly ornamented surfaces. David Roentgen was particularly famous for his desks, which featured a variety of mechanical features as well as superb woodwork.\n\nThe royal tapestry workshop of Gobelins Manufactory continued to produce high-quality large works for royal residences and the nobility, but tastes had changed. The immense tapestries celebrating historical events were largely out of style. Instead of creating new designs, the manufactories of Gobelins, Beauvais and Aubusson recycled old designs, such as the \"Metamorphoses\" of Boucher. An increasing amount of the work was the creation of designs, especially polychrome floral patterns, for the upholstery of the royal furniture. The other two major tapestry workshops, Aubusson and the Beauvais Manufactory, also oriented their work primarily to furniture upholstery. \nHand-painted wallpaper had been used since in the 16th century for interior decoration, followed by wood block prints. French aristocrats often used tapestries in the major rooms, but in the antechambers and lesser rooms they often used painted or printed of painted paper designs imported from China, India, and especially England. In 1765, the French government placed a heavy tax on imported wallpaper, stimulating French production. During the reign of Louis XVI, the largest French enterprise for making wallpaper was created Jean-Baptiste Réveillon, In 1784 they received the title of Royal Manufactory, opened a large depot near the Tuilieries Palace, and hired a group of noted artists and illustrators, including the son of the painter Boucher, to design wallpaper. They also soon developed a process for printing the wallpaper in long rolls. He also made the colorful paper that covered the balloon that made the first manned flight in 1783. Their factory in the Faubourg Saint-Antoine . became one of the largest in Paris, and was an early target of demonstrations at the beginning of the French Revolution.\n\nAnother popular style that developed during the period was the decoration of rooms with panoramic scenes, composed of a number of painted or printed panels put together. These were commonly used in boudoirs and bath chambers. The salon of the pavilion of the Countess of Provence in Montreuil, and the country \"Cottage\" of the Prince de Condé at Chantilly had a similar panorama installed in 1775.\n\nAnother popular form of decoration was printed fine cotton, with elaborate arabesques and floral patterns. The most famous variety was \"toile de Jouy\". The fabric was made with wood block prints, was usually white and red or blue and red, was used to cover beds, for curtains, and for the covers of furniture. Another important industry was that of the manufacture of silk products. The best quality silk was made in Lyon, and was sold to Catherine the Great of Russia, Frederick the Great of Prussia, and other royal clients. Lampas silk coverings with motifs of arabesques and medallions covered the walls of the billiards room of Marie-Antoinette in 1779, and thereafter became fashionable in Paris residences.\n\nThe most famous painter of the later French baroque was François Boucher, who perfectly captured the spirit and style of the period. After his death in 1770, shortly before the beginning of the reign of Louis XVI, he had no real successor in the baroque style. The end of the reign of Louis XV also brought to prominence the first artist to paint in the neoclassical style, Joseph Marie Vien, who painted scenes of Rome inspired by the discoveries at Herculaneum and Pompeii. Vien became the last holder of the title of first painter of the King, which he held from 1789 to 1791. Jean Peyron was another neoclassicist in the early Louis XVI reign. Élisabeth Vigée Le Brun was noted for her portraits of the royal family and nobility, including of Marie Antoinette and her children. The most prominent neoclassicist by far was Jacques-Louis David whose works well before the revolution expressed the Roman virtues of noble and grave simplicity. His major early works included \"Belisarius asking for alms\" (1781),\"The Agony of Andromique\" (1783), and especially Oath of the Horatii (1784), exalting the willingness of Roman soldiers to give their lives for the nation. The painting was so popular when shown at the Salon of 1785 that David was permitted to establish his studio in Louvre, a particular honor for artists. This painting became a model of the style that dominated French art during and after the Revolution.\n\nSculpture evolved from the more animated forms of the baroque to the more serene neoclassical style. The sculptors who were most prominent in the period included Étienne Maurice Falconet, who created table sculptures on classical and romantic themes for many Parisian salons, as well as the famous statue of Peter the Great on horseback for St Petersburg. Another notable portrait sculptor was Augustin Pajou, who also made statues of Greek and Roman gods, illustrating virtues; his statue of Mercury represented commerce. The most celebrated portrait sculptor was Jean-Antoine Houdon, known for his busts of leading figures of the period, including, in 1790, in the midst of the Revolution, Louis XVI himself. Louis-Simon Boizot was prominent for making busts of the nobility, including Marie Antoinette, but also for modeling figures for the Sevres porcelain factory, which became better known than his more formal sculpture. Examples include his \"The Toilet of Madame\", made of hard-paste porcelain, mounted on a plaque of marble and gilded bronze. \nMusical tastes at court were guided by Marie Antoinette. The Queen played the harp and sang, and had been, in Vienna, a student of Christoph Willibald Gluck. Her favorite composers were Gluck and Grétry, and she regularly attended concerts at the Academy of Music and the \"Concert Sprituel\", a society created to support new religious music. Gluck came to Paris in December 1776 for performances of his opera \"Iphigenie en Tauride\", and remained to compose seven operas. However his opera, Echo et Narcisse in 1779, was a failure, and he departed Paris, never to return.\n\nMozart came to Paris in 1778, where he conducted two symphonies, including the \"Parisian\" and gave music lessons to members of the nobility, as did Hayden. The members of the new Masonic movement in Paris were particularly active in sponsoring music; they commissioned Hayden in 1786-85 to write the \"Symphonies Parisienees\".\n\n"}
{"id": "13976698", "url": "https://en.wikipedia.org/wiki?curid=13976698", "title": "Maersk Drilling", "text": "Maersk Drilling\n\nMaersk Drilling is a drilling rig operator based in Copenhagen, Denmark. It is a subsidiary of the A.P. Moller – Maersk Group, established in 1972. Maersk Drilling is one of the A.P. Moller – Maersk Group's core businesses.\n\nMaersk Drilling owns 24 rigs. The fleet includes five Ultra-Harsh jack-ups, four XL Enhanced jack-ups (including one newbuild which is set for delivery in late 2016), seven Harsh environment jack-ups, four semi-submersibles and four Ultra deepwater drillships. \nMaersk Drilling is among others a market leader in challenging Norwegian jack-up market with a market share of 7 out of 12 rigs (2016).\n\nMaersk Drilling is part of the A.P. Moller - Maersk Group - a worldwide conglomerate with 89,000 employees and offices in 135 countries, headquartered in Copenhagen, Denmark. The company is listed on the Copenhagen Stock Exchange. On 21 June 1972 Mærsk Storm Drilling Company and Atlantic Pacific Marine Corporation were established with the purpose of purchasing two semi-subs and two barge rigs. These were the very early days of Maersk Drilling. Today, Maersk Drilling’s fleet includes some of the industries youngest and most advanced harsh environment jack-up rigs, jack-ups rigs, Deepwater semi-submersibles and Ultra deepwater drillships.\n\nEgyptian Drilling Company (EDC) was a 50/50 joint venture between Maersk Drilling and the Egyptian General Petroleum Corporation until 2017, whereafter it has been 100% owned by the Egyptian General Petroleum Corporation. EDC owns and operates a fleet of more than 60 land rigs and five jack-up rigs in the Middle East.\n\n\nMaersk Drilling's fleet includes: \n\nMaersk Drilling has invested more than USD 8.0 bn total in eight highly advanced newbuilds delivered between 2014-2016. The newbuilds includes 4 Ultra deepwater drillships and 4 XL Enhanced jack-ups. \n\nHaving taking delivery of four state-of-the-art XLE Jack-Ups, Maersk Intrepid, Maersk Interceptor and Maersk Integrator, Maersk Invincible, Maersk Drilling's has cemented its position as an industry leader in the North Sea, with the most advanced Jack-Up drilling rigs in existence. All of the newbuilds have been customized to handle demanding and complex well drilling operations in the North Sea.\n\nMaersk Drilling is a considerable player in the ultra deepwater market with four newbuilding drillships. The drillships are purpose-built for safe operations under extreme conditions.\n\nThe design and capacities of the new drillships include additional features for high efficiency operation. Featuring dual derrick and large subsea work and storage areas, the design allows for efficient well construction and field development activities through parallel and offline activities.\n\nThe 228-metre long drillships will be able to operate at water depths up to 3,650 m (12,000 ft) and will be capable of drilling wells of more than 12,000 m (40,000 ft) deep. With their advanced positioning control systems, the ships automatically maintain a fixed position in severe weather conditions with waves up to 11 metres high and wind speeds up to 26 metres per second.\n"}
{"id": "709958", "url": "https://en.wikipedia.org/wiki?curid=709958", "title": "Matrioshka brain", "text": "Matrioshka brain\n\nA matrioshka brain is a hypothetical megastructure proposed by Robert Bradbury, based on the Dyson sphere, of immense computational capacity. It is an example of a Class B stellar engine, employing the entire energy output of a star to drive computer systems. \nThis concept derives its name from the nesting Russian Matryoshka dolls.\nThe concept was deployed by its inventor, Robert Bradbury, in the anthology \"Year Million: Science at the Far Edge of Knowledge\", and attracted interest from reviewers in the \"Los Angeles Times\" and the \"Wall Street Journal\".\n\nThe concept of a matrioshka brain comes from the idea of using Dyson spheres to power an enormous, star-sized computer. The term \"matrioshka brain\" originates from matryoshka dolls, which are wooden Russian nesting dolls. Matrioshka brains are composed of several Dyson spheres nested inside one another, the same way that matryoshka dolls are composed of multiple nested doll components. The innermost Dyson sphere of the matrioshka brain would draw energy directly from the star it surrounds and give off large amounts of waste heat while computing at a high temperature. The next surrounding Dyson sphere would absorb this waste heat and use it for its computational purposes, all while giving off waste heat of its own. This heat would be absorbed by the next sphere, and so on, with each sphere giving off less heat than the one before it. For this reason, matrioshka brains with more nested Dyson spheres would tend to be more efficient, as they would waste less heat energy. The inner shells could run at nearly the same temperature as the star itself, while the outer ones would be close to the temperature of interstellar space. The engineering requirements and resources needed for this would be enormous.\n\nThe term \"matrioshka brain\" was invented by Robert Bradbury as an alternative to the \"Jupiter brain\"—a concept similar to the matrioshka brain, but on a smaller planetary scale and optimized for minimal signal propagation delay. A matrioshka brain design is concentrated on sheer capacity and the maximum amount of energy extracted from its source star, while a Jupiter brain is optimized for computational speed.\n\nSome possible uses of such an immense computational resource have been proposed. One idea suggested by Charles Stross, in his novel \"Accelerando\", would be to use it to run perfect simulations or uploads of human minds into virtual reality spaces supported by the Matrioshka brain. Stross even went so far as to suggest that a sufficiently powerful species utilizing enough raw processing power could launch attacks upon, and manipulate, the structure of the universe itself.\nIn \"Godplayers\" (2005), Damien Broderick surmises that a matrioshka brain would allow simulating entire alternate universes.\nThe futurist and transhumanist author Anders Sandberg wrote an essay speculating on implications of computing on the massive scale of machines such as the matrioshka brain, published by the Institute for Ethics and Emerging Technologies.\nMatrioshka brains and other megastructures are a common theme in the fictional Orion's Arm universe where they are used by superintelligences as processing nodes connected via artificial wormholes.\n\n\n"}
{"id": "42159582", "url": "https://en.wikipedia.org/wiki?curid=42159582", "title": "Mobiles for development", "text": "Mobiles for development\n\nMobiles for Development (M4D), a more specific iteration of Information and Communication Technologies for Development (ICT4D), refers to the use of mobile technologies in global development strategies. Focusing on the fields of international and socioeconomic development and human rights, M4D relies on the theory that increased access to mobile devices acts as an integral cornerstone in the promotion of overall societal development.\n\nOnce viewed as an item of luxury and privilege, mobile phones and devices have become a near necessity throughout the developed and developing world alike. According to a 2007 United Nations study, over two thirds of the world’s mobile phones are owned and utilized within developing countries. With less-developed wired infrastructure and the high cost associated with its modernization and implementation, the adoption of cellular technologies can be attributed to a necessary leapfrogging of traditional telephony and communication technologies. In addition, the unsound and undependable electrical infrastructure of many developing countries does not cater well to mass hardwired ICT adoption. The portability, battery power, and flexibility of mobile technologies is well suited to the common pursuits and lifestyles of those residing in the developing world.\n\nThis mass adoption of ICTs and mobile phones as well the increased quality and expanse of signal coverage within many developing countries has led to increased academic, socioeconomic, and political attention as the various impacts of the M4D movement continue to expand. In addition to the predictable developmental outcomes of mobile adoption including increased economic agency, unforeseen progress has been experienced in the forms of individual empowerment, female agency, as well as familial and community growth.\n\nThe opportunities for effective mass mobilization and aggregation of information and data offered by developmental movements utilizing cellular telephones and other mobile devices such as tablets have been widely featured in the mass media and academia. Literature on this matter is being steadily produced as developing countries continue to adopt mobile technologies at a remarkable rate.\n\nRecent developments in mobile communication and computation technologies have led to the expansion of mobile phone, smartphone, tablet computer, and netbook ownership. Typically marketed to the developed world as supplementary to standard laptops and desktop computers, these electronic products often offer lower price points to the consumer. This lower price point caters well to developing countries and their rapidly evolving markets for ICT expansion and adoption. These mobile devices come equipped with basic mobile communications hard and software such as WiFi and 3G services which allow users to connect to the Internet via mobile and wireless networks without having to secure a landline or an expensive broadband connection via DSL, cable Internet or fiber optics. This leapfrogging movement towards the acceptance and implementation of mobile technologies made the Internet and modern digital telecommunications more accessible to people, particularly those in emerging markets and developing countries.\n\nAccording to International Telecommunication Union, mobile ICTs have emerged as the primary form of technology that will offer a bridge within the digital divide. Data collected by the ITU shows the trajectory of mobile technologies as their adoption out-paces and even replaces the adoption of desktop computers and standard laptops.\n\nThe International Telecommunication Union estimates that as of 2013, approximately 6.8 billion mobile-cellular subscriptions are held worldwide, 5.2 billion of which are held in developing countries. These numbers stand in stark contrast to the penetration rate of fixed-telephone subscriptions which stand at approximately 1.2 billion worldwide, a small margin over half of which belong to the developing world.\n\nThe following table displays key ICT indicators for developed and developing countries as well as world totals from 2010 to 2013:\n\nThe trends displayed in the table above are further supported by the successful sales reports of technology companies selling mobile technologies in emerging markets within developing countries. Some multinational computer manufacturers like Acer and Lenovo are marketing more affordable netbooks to emerging markets such as those in China, Indonesia and India.\n\nOriginally a concept used in reference to economic growth theories and industrialization, leapfrogging has more recently been used in the context of sustainable development for developing countries within world development theories. Technological leapfrogging refers to the acceleration of development through the skipping of low-grade, less efficient, and more costly technologies and industries in favour of the direct adoption of more effective and advanced technologies.\n\nIn the case of M4D, the trajectory of the rapid mass adoption of mobile technologies can be attributed to the \"mobile leapfrog effect\", whereby many developing countries have been seen to bypass traditional routes of wired telephony and broadband infrastructure and development, opting instead for the immediate appropriation of wireless cellular and broadband technology. This mobile leapfrogging can be attributed to the lengthy process and high cost of wired infrastructure implementation. Additionally, as seen in mobile adoption in the Arab States, wired infrastructure that predates mobile technology is often old, outdated, and incapable of data transmission, \"the basic requirement for implementing Digital Subscriber Line (DSL) services\".\n\nMobile ICT platforms and their wide adoption in developing countries serves as an ideal example of leapfrog technology and current practices of leapfrogging within the sustainable development of developing countries. By enabling developing countries to “leapfrog” over legacy technologies of the wired telephone and Internet service of the 20th century and embrace the mobile technologies of the 21st century, opportunities to bridge the Digital divide have been argued to become more prevalent.\n\nThe term ‘mobile hack’ refers to the use-based practices of mobile technology that goes beyond the use intended by the creators of the device. Often used in order to circumvent the high cost of mobile ownership and use, mobile hacks have become common practice in many developing countries. Mobile hacks have also proven to be widespread in the adoption of mobile technologies in developing countries. Device sharing, the ‘Missed Call’ technique, and the transferring of mobile credits has allowed for individuals who were previously isolated from digital communication the opportunity to economically engage in the networked community.\n\nDevice sharing, most commonly the sharing of mobile telephones, refers to the practice of sharing a mobile device between a number of individuals either within a family or a community.\n\nThe 'Missed call' technique refers to the establishment of a code that works around connection charges by utilizing a specified number of rings prior to cutting a call. This method avoids call charge and uses discreet codes to convey messages. This technique goes against the profit models of mobile service providers and allows for individuals to communicate messages in a cost-effective manner.\n\nThrough the transfer of mobile credits, mobile device owners are able to use credits of an amount specified by their service provider as a means of monetary transaction. By sending credits to another's mobile device, an exchange of cash can be made in situations that it is necessitated.\n\nAccording to the ITU's report \"Measuring the Information Society,\" mobile phones and other mobile ICT devices are seen to be replacing standard laptops and desktop computers as the main platform for Internet and ICT access and use. This increased access to and use of mobile phones offers individuals a handheld communication platform that can assist in expanding the level of citizen agency in the development of local and international social, economic, and political endeavours. It also offers individuals the opportunity to form social, economic, and political communities, regardless of geographic location, and provides an at-hand device that allows individuals to fight against human rights abuses from the ground. Organizations such as Digital Democracy (Dd) and the Democratic Voice of Burma offers users the ability to report, compile, and disseminate news and information about human rights violations in order to effectuate global attention and action.\n\nIn the establishment and fostering of mobile citizens within developing countries, a \"bottom of the pyramid\" corporate approach has been encouraged in order to \"convert poverty into a business opportunity that benefits everyone.\"\n\n\nAccording to a report by InfoDev, “[h]ealth conditions in rural areas are generally poorer, and access to information, services, and supplies is most limited.” With the rapid worldwide adoption of mobile technology, a range of health-related areas such as the improvement of public health information dissemination, the facilitation of remote consultation, diagnosis, and treatment, the sharing of a patient’s health information between health professional, and the monitoring and increased efficiency of public health systems have adopted and benefited from mobile based practices.\n\nStudies have suggested that the use of mobile capabilities such as text message reminders regarding dosage information and the increased communication between health professionals has allowed for increased effectiveness in treatment and control of disease. Cases of mobile technology being effectively piloted and utilized in the area of public health exhibits the promise of M4D programs and practices in the spheres of health prevention and medical care for the world’s developing nations.\n\nThe following lists a sampling of programs utilizing M4D strategies for the improvement of public health in developing countries around the world:\n\n\nThe ability to mobilize data aggregation to the mobile-carrying public offers NGOs a valuable resource for their efforts for social, political, economic, and environmental justice. According to a study published by the Vodafone Group Foundation and the UN Foundation Partnership, of a sample of over 500 NGOs, “eighty-six per cent used mobiles, with 99 per cent characterizing its utility positively, with one-quarter of those citing it as a ‘revolutionary’ technology and another one-third calling it indispensable for their work.”. The benefits perceived by M4D initiatives include their \"ability to mobilize and aggregate information and data more effectively and to a wider audience\" as well as their time-saving qualities in the fields of social justice, environmental conservation, global health and humanitarian assistance.\n\nThe following lists organizations engaging in M4D strategies and programming:\n\n\nThe potential for the expansion and replication of M4D projects has been recognized as vital to the overall success of this development practice. The sharing and exchange of information and technical advancements can allow for easier and less costly adoption in other developing countries however, many of the organizations creating and implementing M4D projects act within 'innovation silos'. This siloing of information threatens to create and solidify boundaries between organizations and mobile development projects.\n\nThe environmental implications of increased mobile usage can be seen in the form of the large electronic waste dumps found in many of the developing countries meant to benefit from M4D programs and policies.\n\nIn addition, it has been argued that the introduction of mobile information and communication technologies could result in the proliferation of the Matthew effect, whereby the \"rich get richer.\" In the case of mobile adoption, the inequalities of wealth considered encompass both economic and knowledge-based wealth. Despite the benefits attributed to the adoption and use of mobile ICTs for development purposes, the information and communication resources in question have been initially created and adopted within already developed countries.\n\nAlso problematic is the potential for mobile hardware and software development in developing countries to become a purely for-profit endeavour. As stated by a UN Foundation-Vodafone Group Foundation Partnership publication, \"the potential for scaling up 'mobile for good' initiatives may come with identifying commercial incentives.\" The free and open source software applications that have been developed and implemented by various NGOs in developing countries as well as the ad-hoc communal use of mobile devices could be threatened by the prospective monetization of the mass markets available in the developing world.\n\n"}
{"id": "9313361", "url": "https://en.wikipedia.org/wiki?curid=9313361", "title": "NASBA (molecular biology)", "text": "NASBA (molecular biology)\n\nNucleic acid sequence based amplification (NASBA) is a method in molecular biology which is used to amplify RNA sequences.\n\nNASBA was developed by J Compton in 1991, who defined it as \"a primer-dependent technology that can be used for the continuous amplification of nucleic acids in a single mixture at one temperature\". Immediately after the invention of NASBA it was used for the rapid diagnosis and quantification of HIV-1 in patient sera. Although RNA can also be amplified by PCR using a reverse transcriptase (in order to synthesize a complementary DNA strand as a template), NASBA's main advantage is that it works at isothermic conditions – usually at a constant temperature of 41 °C. NASBA can be used in medical diagnostics as an alternative to PCR that is quicker and more sensitive in some circumstances.\n\nExplained briefly, NASBA works as follows:\n\nThe NASBA technique has been used to develop rapid diagnostic tests for several pathogenic viruses with single-stranded RNA genomes, e.g. influenza A, foot-and-mouth disease virus, severe acute respiratory syndrome (SARS)-associated coronavirus, human bocavirus (HBoV) and also parasites like \"Trypanosoma brucei\".\n"}
{"id": "30784853", "url": "https://en.wikipedia.org/wiki?curid=30784853", "title": "NanoIntegris", "text": "NanoIntegris\n\nNanoIntegris is a nanotechnology company based in Boisbriand, Quebec specializing in the production of enriched, single-walled carbon nanotubes. In 2012, NanoIntegris was acquired by Raymor Industries, a large-scale producer of single-wall carbon nanotubes using the plasma torch process.\n\nThe proprietary technology through which NanoIntegris creates their products spun out of the Hersam Research Group at Northwestern University.\n\nThe process through which these technologies emerged is called Density Gradient Ultracentrifugation (DGU). DGU has been used for some time in biological and medical applications but Dr. Mark Hersam utilized this process with carbon nanotubes which allowed for those nanotubes with semi-conductive properties to be separated from those with conductive properties. While the DGU method was the first one to convincingly produce high-purity semiconducting carbon nanotubes, the rotation speeds involved limit the amount of liquid, and thus nanotubes, that can be processed with this technology. NanoIntegris has recently licensed a new process using selective wrapping of semiconducting nanotubes with conjugated polymers. This method is scalable thus enables the supply of this material in large quantities for commercial applications.\n\nEnriched Semiconducting carbon nanotubes (sc-SWCNT) using either a density-gradient ultracentrifugation (DGU) or a polymer-wrapping (conjugated polymer extraction(CPE)) method. While the DGU method is used to disperse and enrich sc-SWCNT in an aqueous solution, the CPE method disperses and enriches sc-SWCNT in non-polar aromatic solvents\n\nEnriched Conducting carbon nanotubes\n\nHighly graphitized single-wall carbon nanotubes grown using an industrial scale plasma torch. Nanotubes grown using a plasma torch display diameters, lengths and purity levels comparable to the arc and laser method. The nanotubes measure between 1 and 1.5 nm in diameter and between 0.3-5 microns in length.\n\nHighly purified carbon nanotubes. Carbon impurities and metal catalysts impurities below 3% and 1.5% respectively.\n\n1-4+ layer graphene sheets obtained by liquid exfoliation of graphite\n\nSmall-diameter single-walled carbon nanotubes\n\nField-Effect Transistors\n\nBoth Wang and Engel have found that NanoIntegris separated nanotubes \"hold great potential for thin-film transistors and display applications\" compared to standard carbon nanotubes. More recently, nanotube-based thin film transistors have been printed using inkjet or gravure methods on a variety of flexible substrates including polyimide and polyethylene (PET) and transparent substrates such as glass. These p-type thin film transistors reliably exhibit high-mobilities (> 10 cm^2/V/s) and ON/OFF ratios (> 10^3) and threshold voltages below 5 V. Nanotube-enabled thin-film transistors thus offer high mobility and current density, low power consumption as well as environmental stability and especially mechanical flexibility. Hysterisis in the current-voltage curves as well as variability in the threshold voltage are issues that remain to be solved on the way to nanotube-enabled OTFT backplanes for flexible displays.\n\nTransparent Conductors\n\nAdditionally, the ability to distinguish semiconducting from conducting nanotubes was found to have an effect on conductive films.\n\nOrganic Light-Emitting Diodes\n\nOrganic Light-Emitting Diodes (OLEDs) can be made on a larger scale and at a lower cost using separated carbon nanotubes.\n\nHigh Frequency Devices\n\nBy using high-purity, semiconducting nanotubes, scientists have been able to achieve \"record...operating frequencies above 80 GHz.\"\n\nChongwu Zhou\n\nMark Hersam\n\nBruce Weisman\n\nCraig E. Banks\n\nFwu-Shan Sheu\n\nPeter John Burke\n\nSaiful I. Khondaker\n\nMartin Pumera\n\nAchim Hartschuh\n\nLu-Chang Qin\n\nPhaedon Avouris,\n\nRalph Krupke\n\nJong-Hyun Ahn\n\nPartha Hazra\n\nLain-Jong Li\n\nMenachem Elimelech\n\nChad D. Vecitis\n\nJonas I. Goldsmith\n\nSamuel Graham\n\nRobert C. Haddon\n"}
{"id": "2740204", "url": "https://en.wikipedia.org/wiki?curid=2740204", "title": "Nanoelectrochemistry", "text": "Nanoelectrochemistry\n\nNanoelectrochemistry is a branch of electrochemistry that investigates the electrical and electrochemical properties of materials at the nanometer size regime. Nanoelectrochemistry plays significant role in the fabrication of various sensors, and devices for detecting molecules at very low concentrations.\n\nNanoelectrodes are tiny electrodes made of metals or semiconducting materials having typical dimensions of 1-100 nm.\n\n"}
{"id": "57540601", "url": "https://en.wikipedia.org/wiki?curid=57540601", "title": "Ohmic heating (food processing)", "text": "Ohmic heating (food processing)\n\nOhmic heating (joule heating, resistance heating, or electroconductive heating) generates heat by passage of electrical current through food which resists the flow of electricity. Heat is generated rapidly and uniformly in the liquid matrix as well as in particulates, and thus producing a high quality sterile product that can be aseptically filled. Electrical energy is linearly translated to thermal energy as electrical conductivity increases, and this is the key process parameter that affects heating uniformity and heating rate. This heating method is best for foods that contain particulates suspended in a weak salt containing medium due to their high resistance properties. Ohmic heating is beneficial due to its ability to inactivate microorganisms through thermal and non-thermal cellular damage. This method can also inactivate antinutritional factors thereby maintaining nutritional and sensory properties. However, ohmic heating is limited by viscosity, electrical conductivity, and fouling deposits. Although ohmic heating has not yet been approved by the Food and Drug Administration (FDA) for commercial use, this method has many potential applications, ranging from cooking to fermentation.\n\nThere are different configurations for continuous ohmic heating systems, but the most basic process is outlined in Figure 1. A power supply or generator is needed to produce electrical current. Electrodes, in direct contact with food, pass electric current through the matrix. The distance between the electrodes can be adjusted to achieve the optimum electrical field strength.\n\nThe generator creates the electrical current which flows to the first electrode and passes through the food product placed in the electrode gap. The food product resists the flow of current causing internal heating. The current continues to flow to the second electrode and back to the power source to close the circuit. The insulator caps around the electrodes controls the environment within the system.\n\nThe electrical field strength and the residence time are the key process parameters which affect heat generation.\n\nThe ideal foods for ohmic heating are viscous with particulates.\n\nThe efficiency by which electricity is converted to heat depends upon on salt, water, and fat content due to their thermal conductivity and resistance factors. In particulate foods, the particles heat up faster than the liquid matrix due to higher resistance to electricity and matching conductivity can contribute to uniform heating. This prevents overheating of the liquid matrix while particles receive sufficient heat processing. Table 1 shows the electrical conductivity values of certain foods to display the effect of composition and salt concentration. The high electrical conductivity values represent a larger number of ionic compounds suspended in the product, which is directly proportional to the rate of heating. This value is increased in the presence of polar compounds, like acids and salts, but decreased with nonpolar compounds, like fats. Electrical conductivity of food materials generally increases with temperature, and can change if there are structural changes caused during heating such as gelatinization of starch. Density, pH, and specific heat of various components in a food matrix can also influence heating rate.\n\nBenefits of Ohmic heating include: uniform and rapid heating (>1°Cs), less cooking time, better energy efficiency, lower capital cost, and volumetric heating as compared to aseptic processing, canning, and PEF. Volumetric heating allows internal heating instead of transferring heat from a secondary medium. This results in the production of safe, high quality food with minimal changes to structural, nutritional, and organoleptic properties of food. Heat transfer is uniform to reach areas of food that are harder to heat. Less fouling accumulates on the electrodes as compared to other heating methods. Ohmic heating also requires less cleaning and maintenance, resulting in an environmentally cautious heating method.\n\nMicrobial inactivation in ohmic heating is achieved by both thermal and non-thermal cellular damage from the electrical field. This method destroys microorganisms due to electroporation of cell membranes, membrane rupture, and cell lysis. In electroporation, excessive leakage of ions and intramolecular components results in cell death. In membrane rupture, cells swell due to an increase in moisture diffusion across the cell membrane. Pronounced disruption and decomposition of cell walls and cytoplasmic membranes causes cells to lyse.\n\nDecreased processing times in ohmic heating maintains nutritional and sensory properties of foods. Ohmic heating inactivates antinutritional factors like lipoxigenase (LOX), polyphenoloxidase (PPO), and pectinase due to the removal of active metallic groups in enzymes by the electrical field. Similar to other heating methods, ohmic heating causes gelatinization of starches, melting of fats, and protein agglutination. Water-soluble nutrients are maintained in the suspension liquid allowing for no loss of nutritional value if the liquid is consumed.\n\nOhmic heating is limited by viscosity, electrical conductivity, and fouling deposits. The density of particles within the suspension liquid can limit the degree of processing. A higher viscosity fluid will provide more resistance to heating, allowing the mixture to heat up quicker than low viscosity products.\n\nA food product’s electrical conductivity is a function of temperature, frequency, and product composition. This may be increased by adding ionic compounds, or decreased by adding non-polar constituents. Changes in electrical conductivity limit ohmic heating as it is difficult to model the thermal process when temperature increases in multi-component foods.\n\nFouling deposits on the surface of electrodes is caused by direct contact of food surface with electrodes. Formation of deposit layers by thermal denaturation of proteins or redox reactions can create additional electrical resistance. Consequentially, under- or over- processing can occur: over-processing near the electrodes; under-processing creating cold spots near the middle of the circuit. Cold spots signify the possibility of spoilage organisms in food resulting in a shortened shelf‐life and foodborne pathogens resulting in consumer illness. At lower frequencies (less than 50 Hz) there is a risk of corrosion with stainless steel electrodes that leach into the food matrix. Higher frequencies (greater than 10 kHz) decrease both corrosion and fouling. Although ohmic heating is limited by fouling deposits, it is less apparent as compared to other thermal heating methods.\n\nThe potential applications of ohmic heating range from cooking, thawing, blanching, peeling, evaporation, extraction, dehydration, and fermentation. These allow for ohmic heating to pasteurize particulate foods for hot filling, pre-heat products prior to canning, and aseptically process ready-to-eat meals and refrigerated foods. Prospective examples are outlined in Table 2 as this food processing method has not been commercially approved by the FDA. Since there is currently insufficient data on electrical conductivities for solid foods, it is difficult to prove the high quality and safe process design for ohmic heating. Additionally, a successful 12D reduction for \"C. botulinum\" prevention has yet to be validated.\n"}
{"id": "28381575", "url": "https://en.wikipedia.org/wiki?curid=28381575", "title": "Partial stroke testing", "text": "Partial stroke testing\n\nPartial stroke testing (or PST) is a technique used in a control system to allow the user to test a percentage of the possible failure modes of a shut down valve without the need to physically close the valve. PST is used to assist in determining that the safety function will operate on demand. PST is most often used on high integrity emergency shutdown valves (ESDVs) in applications where closing the valve will have a high cost burden yet proving the integrity of the valve is essential to maintaining a safe facility. In addition to ESDVs PST is also used on high integrity pressure protection systems or HIPPS. Partial stroke testing is not a replacement for the need to fully stroke valves as proof testing is still a mandatory requirement.\n\nPartial stroke testing is an accepted petroleum industry standard technique and is also quantified in detail by regulatory bodies such as the International Electrotechnical Commission (IEC) and the Instrument Society of Automation (ISA). The following are the standards appropriate to these bodies.\n\nThese standards define the requirements for safety related systems and describe how to quantify the performance of PST systems\n\nIEC61508 adapts a safety life cycle approach to the management of plant safety. During the design phase of this life cycle of a safety system the required safety performance level is determined using techniques such as Markov analysis, FMEA, fault tree analysis and Hazop. These techniques allow the user to determine the potential frequency and consequence of hazardous activities and to quantify the level of risk. A common method for this quantification is the safety integrity level. This is quantified from one to four with level four being the most hazardous.\n\nOnce the SIL level is determined this specifies the required performance level of the safety systems during the operational phase of the plant. The metric for measuring the performance of a safety function is called the average probability of failure on demand (or PFD) and this correlates to the SIL level as follows\n\nOne method of calculating the PFD for a basic safety function with no redundancy is using the formula\n\nWhere:\n\nThe proof test coverage is a measure of how effective the partial stroke test is and the higher the PTC the greater the effect of the test.\n\nThe benefits of using PST are not limited to simply the safety performance but gains can also be made in the production performance of a plant and the capital cost of a plant. These are summarised as follows\n\nGains can be made in the following areas by the use of PST.\n\nThere are a number of areas where production efficiency can be improved by the successful implementation of a PST system.\n\nThe main drawback of all PST systems is the increased probability of causing an accidental activation of the safety system thus causing a plant shutdown, this is the primary concern of PST systems by operators and for this reason many PST system remain dormant after installation. Different techniques mitigate for this issue in different manners but all systems have an inherent risk\n\nIn addition in some cases, a PST cannot be performed due to the limitations inherent in the process or the valve being used. Further, as the PST introduces a disturbance into the process or system, it may not be appropriate for some processes or systems that are sensitive to disturbances.\n\nFinally, a PST cannot always differentiate between different faults or failures within the valve and actuator assembly thus limiting the diagnostic capability.\n\nThere are a number of different techniques available for partial stroke testing and the selection of the most appropriate technique depends on the main benefits the operator is trying to gain.\n\nMechanical jammers are devices where a device is inserted into the valve and actuator assembly that physically prevents the valve from moving past a certain point. These are used in cases where accidentally shutting the valve would have severe consequences, or any application where the end user prefers a mechanical device.\n\nTypical benefits of this type of device are as follows:\n\n\nHowever, opinions differ whether these devices are suitable for functional safety systems as the safety function is offline for the duration of the test.\n\nModern mechanical PST devices may be automated.\n\nExamples of this kind of device include direct interface products that mount between the valve and the actuator and may use cams fitted to the valve stem. \nAn example of such a mechanical PST system:\n\nOther methods include adjustable actuator end stops.\n\nThe basic principle behind partial stroke testing is that the valve is moved to a predetermined position in order to determine the performance of the shut down valve. This led to the adaptation of pneumatic positioners used on flow control valve for use in partial stroke testing. These systems are often suitable for use on shutdown valves up to and including SIL3.\nThe main benefits are : \n\nThe main benefit of these systems is that positioners are common equipment on plants and thus operators are familiar with the operation of these systems, however the primary drawback is the increased risk of spurious trip caused by the introduction of additional control components that are not normally used on on/off valves. These systems are however limited to use on pneumatically actuated valves.\n\nThese systems use an electrical switch to de-energise the solenoid valve and use an electrical relay attached to the actuator to re-energise the solenoid coil when the desired PST point is reached.\n\nElectronic control systems use a configurable electronic module that connects between the supply from the ESD system and the solenoid valve. In order to perform a test the timer de-energises the solenoid valve to simulate a shutdown and re-energises the soleniod when the required degree of partial stroke is reached. These systems are fundamentally a miniature PLC dedicated to the testing of the valve.\n\nDue to their nature these devices do not actually form part of the safety function and are therefore 100% fail safe. With the addition of a pressure sensor and/or a position sensor for feedback timer systems are also capable of providing intelligent diagnostics in order to diagnose the performance of all components including the valve, actuator and solenoid valves.\n\nIn addition timers are capable of operating with any type of fluid power actuator and can also be used with subsea valves where the solenoid valve is located top-side.\n\nAnother technique is to embed the control electronics into a solenoid valve enclosure removing the need for additional control boxes. In addition there is no need to change the control schematic as no dedicated components are required.\n\n"}
{"id": "13142265", "url": "https://en.wikipedia.org/wiki?curid=13142265", "title": "Payment terminal", "text": "Payment terminal\n\nA payment terminal, also known as a point of sale terminal, credit card terminal, EFTPOS terminal (or a PDQ terminal which stands for \"Process Data Quickly\"), is a device which interfaces with payment cards to make electronic funds transfers.\n\nThere are various types of terminals available to merchants, although most have the same basic purpose and functions. They allow a merchant to insert, swipe, or manually enter the required credit/debit card information, as well as to accept NFC Contactless transactions, and to transmit this data to the merchant service provider for authorization and finally, to transfer funds to the merchant.\n\nHigher end models not only process credit and debit cards but also serve as a comprehensive customer engagement screen at the checkout. Common features include but are not limited to the ability to process gift cards, cheques, contactless and mobile wallet payments. Some are also programmed to accept store loyalty cards or allow the customer to use the pin pad to enter their information (ex. phone number) to redeem points. POS screens also allow retailors to advertise near the register when the terminal isn't being used. During the checkout process, many terminals are set to display a list of items purchased along with the running total. Other times, this functionality may be turned off or it may be used to supplement a dedicated screen that lists items being purchased. Some stores also use the terminal for customers to view and agree to the terms of a product warranty. Like ATMs, many POS terminals are also equipped raised tactile buttons and an earphone jack which allow the blind to audibly finish the payment process.\n\nThe majority of card terminals transmit data over a standard telephone line or an internet connection. Some also have the ability to cache transactional data to be transmitted to the gateway processor when a connection becomes available; the major drawback to this is that immediate authorization is not available at the time the card was processed, which can subsequently result in failed payments. Wireless terminals transmit card data using Bluetooth, Wi-Fi, cellular, or even satellite networks in remote areas and onboard airplanes.\nA merchant can replace the functionality of dedicated credit card terminal hardware using a terminal application running on a PC or mobile device, such as a smartphone. They usually work with dedicated hardware readers that can transfer magnetic stripe data to the application, while there are also some that also work with smart cards (using technology such as EMV), although this is rarely seen on smartphone readers. In case the necessary hardware is unavailable, these applications usually support manual entry of the card number and other data. In addition, more and more devices are beginning to offer built-in RFID or NFC technology to accommodate contactless or mobile device payment methods, often without requiring additional external hardware.\n\nBy moving to the use of card terminals to directly capture card information instead of manually entering in card details, merchants benefit from the efficiency of decreased transaction processing times. In terms of security, major card terminal manufacturers usually offer software allowing end to end card data encryption. Still, there have been some cases of POS pin pad malware. In countries such as the US, where magnetic stripe cards aren't fully phased out, there have also been incidence of skimming at card terminals. That said, stand alone payment terminals are seen as superior to register attached payment methods because they don't require store cashers to take possession of the customer's card.\n\nPrior to the development of payment terminals, merchants would use Manual Imprinters (also known as ZipZap machines) to capture the information from the embossed information on a credit card onto a paper slips with carbon paper copies. These paper slips had to be taken to the bank for processing. This was a cumbersome and time consuming process.\n\nPoint of sale terminals emerged in 1979, when Visa introduced a bulky electronic data capturing terminal which was the first payment terminal. In the same year magnetic stripes were added to credit cards for the first time. This allowed card information to be captured electronically and led to the development of payment terminals.\nOne of the first companies to produce dedicated payment terminals was Verifone. It started in 1981 in Hawaii as a small electronic company. In 1983 they introduced the ZON terminal series which would become the standard for modern payment terminals.\n\nHungarian born George Wallner in Sydney, Australia founded rival Hypercom in 1978 and in 1982 started producing dedicated payment terminals. It went on to dominate the south pacific region. In 1988 the company signed a deal with American Express to provide its terminals to them in the US. To consolidate the deal, Hypercom moved its head office from Australia to Arizona in the US. It then faced head to head competition with VeriFone on its home market.\n\nOver a decade later in 1994 Lipman Electronic Engineering, Ltd. was established in Israel. Lipman manufactured the Nurit line of processing terminals. Because of Verifone’s already firm place in the payment processing industry when Lipman was established, Lipman targeted an untapped niche in the processing industry. While, Lipman holds about a 10% share in wired credit card terminals, they are the undisputed leader with more than 95% share in wireless processing terminals in the late 1990s.\n\nVerifone would later acquired both of these major rivals, acquiring Lipman in 2006 and the payment part of the Hypercom business including its brand in 2011.\n\nIn 1980 Jean-Jacques Poutrel and Michel Malhouitre in 1980 established Ingenico in France and developed their first payment terminal in 1984. Its Barcelona-based R&D unit would lead the development of payment terminals for the next decade. Ingenico, through a number of acquisitions, would dominate the European market for payment terminals. They acquired French based Bull and UK based De La Rue payment terminal activity as well as German Epos in 2001, but entered into the global market with their 2013 acquisition of Indias second-largest POS gateway, EBS (E-Billing Solutions).\nInitially, information was captured from the magnetic strip on the back of the card, by swiping the card through the terminal. In the late 1990s, this started to be replaced by smart cards where the chip was embedded in the card. This was done for added security and required the card to be inserted into the credit card terminal. In the late 1990s and early 2000s contactless payment systems were introduced and the payment terminals were updated to include the ability to read this these cards using near field communication (NFC) technology.\n\n\nThere are 3 main global players who offer both a wide range of payment terminals, sell worldwide, and continue to develop to the latest international payment industry standards. In most countries terminals are provided to merchants via a multitude distributors that support and pre-configure devices to operate with local payment networks or financial institutions.\n\n\n"}
{"id": "18534037", "url": "https://en.wikipedia.org/wiki?curid=18534037", "title": "Pickling salt", "text": "Pickling salt\n\nPickling salt is a salt that is used mainly for canning and manufacturing pickles. It is sodium chloride, as is table salt, but unlike most brands of table salt, it does not contain iodine or any anti caking products added. A widely circulated legend suggested that iodisation caused the brine of pickles to change color. This is false; however, some anti-caking agents are known to collect at the bottom of the jars, a minor aesthetic issue. Pickling salt is very fine-grained, to speed up dissolving in water to create a brine, so it is useful for solutions needing salt.\n\nPickling salt can be used for things other than pickling. It can be used in place of table salt, although it can cake. A solution to this would be to add a few grains of rice to the salt, or to bake it (draws the moisture out), and then break it apart. Pickling salt sticks well to food, so it can be used in place of popcorn salt, which also has fine grains.\n"}
{"id": "33195859", "url": "https://en.wikipedia.org/wiki?curid=33195859", "title": "Portable Modular Data Center", "text": "Portable Modular Data Center\n\nThe Portable Modular Data Center (PMDC) is a portable data center solution built into a standard 20, 40, or 53-foot intermodal container (shipping container) manufactured and marketed by IBM. IBM states that a PMDC cost 30% less to design and build than a traditional data center with cooling equipment.\n\nThe Portable Modular Data Center loaded with computer equipment can be transported using standard shipping methods. The PMDC is weather resistant and insulated, and can be placed in environments like tundra or the desert.\n\n\n"}
{"id": "33535658", "url": "https://en.wikipedia.org/wiki?curid=33535658", "title": "Potato planter", "text": "Potato planter\n\nA potato planter is a farm implement for sowing seed potatoes.\n\nA manual planter is sometimes called a bell planter, which may have two farm hands sitting on the back whilst taking potatoes from a hopper. The length between potatoes is tolled by a bell, at the sound of which potatoes are thrown down tubes. \n\nAn automatic planter is hitched behind a farm tractor with a three-point linkage and towed. Cups lift seed potatoes from a hopper and drop them in tubes, planting up to eight drills at a time.\n"}
{"id": "1202336", "url": "https://en.wikipedia.org/wiki?curid=1202336", "title": "Power window", "text": "Power window\n\nPower windows or electric windows are automobile windows which can be raised and lowered by pressing a button or switch, as opposed to using a crank handle.\n\nPackard had introduced hydraulic window lifts (power windows in fall of 1940, for its new 1941 Packard 180 series cars. This was a hydro-electric system. In 1941, the Ford Motor Company followed with the first power windows on the Lincoln Custom (only the limousine and seven-passenger sedans). Cadillac had a straight-electric divider window (but not side windows) on their series 75.\n\nPower assists originated in the need and desire to move convertible body-style tops up and down by some means other than human effort. The earliest power assists were vacuum-operated and were offered on Chrysler Corporation vehicles, particularly the low-cost Plymouth convertibles in the late 1930s. \n\nShortly before World War II, General Motors developed a central hydraulic pump for working convertible tops. This system was introduced on 1942 convertibles built by GM. Previously, GM had used a vacuum system which did not have the power to handle increasingly larger and complex (four side-windows vs. only two) convertible top mechanisms. \n\nChief Engineer of the Buick Division, Charles A. Chayne, \"...had introduced an electrically controlled hydraulic system into the 1946 Buick convertibles that provided fingertip operation of the top, door windows, and front seat adjustment\". These systems were based on major hydraulic advances made in military weapons in preparation for World War II. \n\nThe \"Hydro-Electric\" system (windows, front seat adjustment and convertible top) was standard on 1947 model year. The seat and window assist system became available on GM closed cars (standard on some Cadillac Series 75 models and all Series 60 Specials, commonly called \"Fleetwood\" beginning with the 1948). The full system was standard only on the high-end GM convertibles made by Oldsmobile, Buick, and Cadillac. It was only available as a package; that is, power assisted windows, front seat, and convertible top (where applicable). This feature can be identified in 1948 and later General Motors model numbers with an \"X\" at the end, such as the 1951 Cadillac Sixty Special sedan, model 6019X. The electrically operated hydraulic pump system was shared by Hudson and Packard for their 1948 through 1950 models. The driver's door contained four buttons in addition to the remaining individual windows.\n\nFord also had a similar electro-hydraulic system on higher-end convertibles. Mercury and Ford Sportsman convertibles (with wood trim) were equipped with power windows on four windows from 1946 through 1948 and Mercury and Lincoln by 1951. These systems were used by other luxury car models (Imperial and Packard) until Chrysler introduced the all-electric operation on the 1951 Imperial. The availability of power windows increased with the use of small, high-torque electric motors. General Motors also followed with full electric operation in 1954. This included four-way and then six-way seats, which were introduced in 1956. Chevrolet introduced the oddity of power front windows (only) in the 1954 model. Ford also introduced full four-door power windows in sedans in 1954. The full-sized 1955 Nash \"Airflyte\" models featured optional power windows.\n\nElectrically-operated vent windows were available as early as 1956 on the Continental Mark II. The 1960s Cadillac Fleetwood came standard with power front and rear vent windows, in addition to standard power side windows, for a total of eight power window controls on the driver's door panel. \n\nModern heavy-duty highway tractors frequently have an option for power window controls; however, these are generally what is referred to as \"straight air\". That is, the compressed air system used for air brakes is also used for the windows. These types of trucks have long used compressed air cylinders for seat height adjustment. In a similar fashion to the electro-hydraulic system, the compressed air is merely released to lower the window and/or seat. The compressed air is then admitted to the respective cylinder to raise the window or seat. \n\nIn a typical auto/light truck installation, there is an individual switch at each window and a set of switches in the driver's door or a-frame pillar, so the driver can operate all the windows. These switches took on many different appearances, from heavy chrome plate to inexpensive plastic. \n\nHowever, some models like Saab, Volvo, Mazda and Holden have used switches located in the center console, where they are accessible to all the occupants. In this case, the door-mounted switches can be omitted. This also removes the need to produce separate door components and wiring for left and right-hand drive variants.\n\nPower windows are usually inoperable when the car is not running. This is primarily a security feature. It would be a simple thing to allow electric power windows to be operable when the ignition is turned off, however it would also make the car much easier to steal. Some systems offer the compromise of leaving power applied to the windows until a passenger door is opened at which time the window power is removed.\n\nHydraulic drive systems could lower the windows at rest, since pressure from the hydraulic system was merely released to lower the window. Raising the windows required an electrically operated pump to operate and introduce pressure at the appropriate cylinder. These systems also required pressure lines to each cylinder (in the doors, as well as on certain cars, to the power seat and a power operated convertible top). Because of the complexity, the system could also leak fluid. \n\nMany modern cars have a time delay feature, first introduced by Cadillac in the 1980s, called \"retained accessory power\". This allows operation of the windows and some other accessories for ten minutes or so after the engine is stopped. Another feature is the \"express-down\" window, which allows the window to be fully lowered with one tap on the switch, as opposed to holding the switch down until the window retracts. Many luxury vehicles during the 1990s expanded on this feature, to include \"express-up\" on the driver's window, and recently, some manufacturers have added the feature on all window switches for all passengers' convenience. This is done by activating the switch until a \"click\" response is felt.\n\nPower windows have become so common that by 2008, some automakers eliminated hand crank windows from all their models. So many vehicles now have power windows that some people no longer understand the (formerly) common sign from another driver of using their hand to simulate moving a window crank to indicate that they wish to speak with someone when stopped at a light or in a parking lot. The 2008 Audi RS4 sold in Europe, however, still has roll-up windows for the rear doors although its counterpart sold in the U.S. has power windows for all doors.\n\nPower windows have come under some scrutiny after several fatal accidents in which children's necks have become trapped, leading to suffocation. Some designs place the switch in a location on a hand rest where it can be accidentally triggered by a child climbing to place his or her head out of the window. To prevent this, many vehicles feature a driver-controlled lockout switch, preventing rear-seat passengers (usually smaller children) from accidentally triggering the switches. This also prevents children from playing with them and pets riding with their heads out windows from activating the power window switch. \n\nStarting with the 2008 model year, U.S. government regulations required automakers to install safety mechanisms to improve child safety. However, the rules do not prevent all potential injuries to a hand, finger, or even a child's head, if someone deliberately holds the switch when the window is closing. In 2009, the U.S. auto safety administration tentatively decided against requiring all cars to have automatic reversing power windows if they sense an obstruction while closing. Proposed requirements concern automatic (\"one-touch up\") window systems, but most vehicles with this feature already have automatic-reversing. The federal government made a written contract that all automakers should make the lever switches (as opposed to the rocker and toggle switches) standard on all new vehicles by 1 October 2010.\n\n"}
{"id": "6977357", "url": "https://en.wikipedia.org/wiki?curid=6977357", "title": "Reboiler", "text": "Reboiler\n\nReboilers are heat exchangers typically used to provide heat to the bottom of industrial distillation columns. They boil the liquid from the bottom of a distillation column to generate vapors which are returned to the column to drive the distillation separation. The heat supplied to the column by the reboiler at the bottom of the column is removed by the condenser at the top of the column.\n\nProper reboiler operation is vital to effective distillation. In a typical classical distillation column, all the vapor driving the separation comes from the reboiler. The reboiler receives a liquid stream from the column bottom and may partially or completely vaporize that stream. Steam usually provides the heat required for the vaporization.\n\nThe most critical element of reboiler design is the selection of the proper type of reboiler for a specific service. Most reboilers are of the shell and tube heat exchanger type and normally steam is used as the heat source in such reboilers. However, other heat transfer fluids like hot oil or Dowtherm (TM) may be used. Fuel-fired furnaces may also be used as reboilers in some cases.\n\nCommonly used heat exchanger type reboilers are:\n\nKettle reboilers (Image 1) are very simple and reliable. They may require pumping of the column bottoms liquid into the kettle, or there may be sufficient liquid head to deliver the liquid into the reboiler. In this reboiler type, steam flows through the tube bundle and exits as condensate. The liquid from the bottom of the tower, commonly called the bottoms, flows through the shell side. There is a retaining wall or overflow weir separating the tube bundle from the reboiler section where the residual reboiled liquid (called the bottoms product) is withdrawn, so that the tube bundle is kept covered with liquid and reduce the amount of low-boiling compounds in the bottoms product.\n\nThermosyphon reboilers (Image 2) do not require pumping of the column bottoms liquid into the reboiler. Natural circulation is obtained by using the density difference between the reboiler inlet column bottoms liquid and the reboiler outlet liquid-vapor mixture to provide sufficient liquid head to deliver the tower bottoms into the reboiler. Thermosyphon reboilers (also known as calandrias) are more complex than kettle reboilers and require more attention from the plant operators. There are many types of thermosyphon reboilers including vertical, horizontal, once-through or recirculating.\n\nFired heaters (Image 3), also known as furnaces, may be used as a distillation column reboiler. A pump is required to circulate the column bottoms through the heat transfer tubes in the furnace's convection and radiant sections. The heat source for the fired heater reboiler may be either fuel gas or fuel oil.\n\nA forced circulation reboiler (Image 4) uses a pump to circulate the column bottoms liquid through the reboilers. This is useful if the reboiler must be located far from the column, or if the bottoms product is extremely viscous.\n\nSome fluids are temperature sensitive such as those subject to polymerization by contact with high temperature heat transfer tube walls. High liquid recirculation rates are used to reduce tube wall temperatures, thereby reducing polymerization on the tube and associated fouling.\n\n\n\n"}
{"id": "5190562", "url": "https://en.wikipedia.org/wiki?curid=5190562", "title": "Robin Li", "text": "Robin Li\n\nRobin Li or Li Yanhong (; born 17 November 1968) is a Chinese Internet entrepreneur, co-founder of the search engine Baidu, and one of the richest people in China, with a net worth of US$18.5 billion as of October 2017. Li is a member of the 12th Chinese People's Political Consultative Conference.\n\nLi studied information management at Peking University and computer science at the State University of New York at Buffalo. In 2000 he founded Baidu with Eric Xu. Li has been CEO of Baidu since January 2004. The company was listed on NASDAQ on August 5, 2005. Li was included as one of the \"15 Asian Scientists To Watch\" by \"Asian Scientist Magazine\" on 15 May 2011.\n\nOn August 29, 2014, Robin Li was appointed by the United Nations Secretary General, Mr. Ban Ki-Moon, as co-chair of the Independent Expert Advisory Group on Data Revolution for Sustainable Development.\n\nLi was born in Shanxi Province, China, where he spent most of his childhood. Both of his parents were factory workers. Li was the fourth of five children, and the only boy. \n\nHe enrolled at Peking University where he studied information management and earned a Bachelor of Science degree. In the fall of 1991, Li went to the University at Buffalo, The State University of New York in the US to study for a doctorate in computer science. He received his Master in Computer Science degree in 1994 after deciding not to continue with the PhD.\n\nIn 1994, Li joined IDD Information Services, a New Jersey division of Dow Jones and Company, where he helped develop a software program for the online edition of \"The Wall Street Journal\". He also worked on improving algorithms for search engines. He remained at IDD Information Services from May 1994 to June 1997. In 1996, while at IDD, Li developed the Rankdex site-scoring algorithm for search engine page ranking, which was awarded a U.S. patent. He later used this technology for the Baidu search engine.\n\nLi worked as a staff engineer for Infoseek, a pioneer internet search engine company, from July 1997 to December 1999. An achievement of his was the picture search function used by Go.com. Since founding Baidu in January 2000, Li has turned the company into the largest Chinese search engine, with over 80% market share by search query, and the second largest independent search engine in the world. On 5 August 2005, Baidu successfully completed its IPO on NASDAQ, and in 2007 became the first Chinese company to be included in the NASDAQ-100 Index. He appeared in CNN Money's annual list of \"50 people who matter now\" in 2007.\n\nIn 2001, he was named one of the \"Chinese Top Ten Innovative Pioneers\" In 2002 and 2003, he was consecutively named one of \"IT Ten Famous Persons\". In April 2004, he was named the second session of \"Chinese Software Ten Outstanding Young Persons\". On 23 August 2005, he was named the twelfth session of \"ASEAN Youth Award\". On 28 December 2005 he was named one of the \"CCTV 2005 Chinese Economic Figures of The Year\" On 10 December 2006 he was named 2006's \"World's Best Business Leader\" by the \"American Business Weekly\".\n\nLi is married to Dongmin Ma, who also works for Baidu. They have four children, and live in Beijing, China.\n\n"}
{"id": "3832180", "url": "https://en.wikipedia.org/wiki?curid=3832180", "title": "Sequoia Voting Systems", "text": "Sequoia Voting Systems\n\nSequoia Voting Systems was a California-based company that is one of the largest providers of electronic voting systems in the U.S., having offices in Oakland, Denver and New York City. Some of its major competitors were Premier Election Solutions (formerly Diebold Election Systems) and Election Systems & Software.\n\nIt was acquired by the Canadian company Dominion Voting Systems on June 4, 2010. At the time it had contracts for 300 jurisdictions in 16 states through its BPS, WinEDS, Edge, Edge2, Advantage, Insight, InsightPlus and 400C systems.\n\nSequoia Voting Systems began as Mathematical Systems Corporation of Anaheim, California, the developers of a punched-card voting system that served as an alternative to the Votomatic. Some time around 1970, Diamond National Corporation (the holding company that grew from the Diamond Match Company) acquired the company. In the 1970s, Diamond National became Diamond International, which was acquired and reorganized by Jefferson Smurfit, an Irish printing conglomerate, producing Smurfit Diamond Packaging Corporation. Diamond spun off its punched-card voting business in 1983 as Sequoia Pacific Systems Corporation.\n\nIn 1984, Sequoia purchased the voting machine business of AVM Corporation (the former Automatic Voting Machine Corporation) and was reorganized as Sequoia Voting Systems. AVM had its roots in a number of voting machine companies founded in the 1890s, but by the 1980s, most of its business was in other fields. Nonetheless, in the late 1950s, AVM had begun investing in the development of electronic voting machines. By the time Sequoia bought the AVM voting business, the AVM Automatic Voting Computer (AVC) was ready for market. Under Sequoia ownership, the AVC was certified for use in several states in 1986 and 1987, and with sleek new packaging, it went to market as the Sequoia AVC Advantage DRE voting machine in 1990. Business Week considered the AVC Advantage to be one of the high points in industrial design for the decade of the 1990s and credited it with turning the company around.\nIn late 1997, Sequoia obtained the intellectual property rights to the Optech line of from Business Records Corporation. This transfer was a consequence of antitrust action taken by the United States Department of Justice when American Information Systems merged with the Election Services Division of Business Records Corporation to form Election Systems & Software. After this merger ES&S retained the right to sell and service Optech scanners to existing customers; as a result, the ES&S Optech IV-C and the Sequoia Optech 400-C, for example, are essentially the same device.\n\nIn early 2002 De La Rue, a British currency paper printing and security company took over ownership from Smurfit for $23 million. After losing money for several years, on March 8, 2005, Sequoia was acquired by Smartmatic, a multi-national technology company which had developed advanced election systems, voting machines included. Thereafter Smartmatic assigned a major portion of its development and managerial teams, dedicated to revamping some of Sequoia's old-fashioned, legacy voting machines, and replacing their technology with \"avant-garde\" proprietary features and developments, which resulted in new, high-tech products. As a result, Sequoia sold many new-generation election products and experienced a healthy financial resurrection during the fiscal years of 2006 and 2007. However, in November 2007, following a verdict by the CFIUS, Smartmatic was ordered to sell Sequoia, which it did to its Sequoia managers having U.S. citizenship.\n\nOn August 3, 2007, California Secretary of State Debra Bowen withdrew approval and granted conditional reapproval to Sequoia Voting Systems optical scan and DRE voting machines after a \"review of the voting machines certified for use in California in March 2007\" found \"significant security weaknesses throughout the Sequoia system\" and \"pervasive structural weaknesses\" which raise \"serious questions as to whether the Sequoia software can be relied upon to protect the integrity of elections.\"\n\nA 2007 investigative report by Dan Rather charged Sequoia with deliberately supplying poor quality punch-card ballots to Palm Beach County, Florida for the 2000 election. According to former Sequoia employees, the ballots for Palm Beach County were produced with paper and manufacturing processes that were outside of normal specifications. This supposedly caused all of the problems with \"hanging chads\". When quality problems were found, Sequoia management ordered the production workers to ignore them. One worker speculated that the object was to discredit punch-card ballots and thus promote sales of electronic voting machines.\nAfter the 2000 election problems, Florida required its counties to replace punch-card voting systems with touch-screen systems, some of which were purchased from Sequoia. However, there were some major problems with touch-screen systems, and in 2007 Florida ordered the counties to replace them with optical-scan systems by 1 July 2008. Sequoia offered to buy back its machines for $1 each. This offer was rejected.\n\nIn early 2008, New Jersey election officials announced that they planned to send one or more Sequoia Advantage voting machines to Professors Edward Felten and Andrew Appel of Princeton University for analysis. Felten and Appel are computer scientists interested in security issues, especially in regard to electronic voting systems. In March 2008, Sequoia sent an e-mail to Professor Felten asserting that allowing him to examine Sequoia voting machines would violate the license agreement between Sequoia and the county which bought them, and also that Sequoia would take legal action \"to stop... non-compliant analysis... publication of Sequoia software... or any other infringement of our intellectual property.\"\nThis action sparked outrage among computer technology activists. Author and digital rights activist Cory Doctorow commented \"It's hard to imagine a stupider legal threat.\"\n\nShortly after this, Sequoia's corporate Web site was hacked. Ironically, the hack was first discovered by Ed Felten. Sequoia took its Web site down on 20 March and removed the \"intrusive content\".\n\nOn June 2006, Sequoia Voting Systems, along with Diebold and ES&S, were sued by a small, virtually unknown New Jersey technology company called 'Avante', alleging infringement of two of its patents covering DREs (Direct Recording Electronic voting machines) and Optical Scanners. The lawsuit demands that the three companies a) are prohibited permanently to sell all their “infringing” equipment; b) recall all “infringing” equipment; c) destroy or deliver to Avante the “infringing” equipment; and d) award “infringement” damages to Avante including treble damages for \"willful infringement\". Sequoia Voting Systems, in particular, was sued for its Edge, Advantage, 400C, VeriVote Printer (VVPAT) and Insight machines (that is, for all of its products except one). The other two companies were sued for almost all of their products.\n\nIn April 2008, competitor Hart InterCivic attempted a hostile takeover of Sequoia. Court documents unearthed at this time revealed that Smartmatic still retained some financial control over several aspects of Sequoia. At the time, Smartmatic held a $2 million note from SVS Holdings, Inc., the management team which purchased the company from Smartmatic. In accordance to the acquisition contract, Smartmatic also retains ownership of intellectual property rights for some of Sequoia's currently deployed election products in the United States, and holds the right to negotiate overseas non-compete agreements.\n\nThe CEO and President of Sequoia and SVS Holdings is Jack Blaine, a former Smartmatic executive. During a conference call with company employees, Blaine admitted that SVS/Sequoia did not control the intellectual property of some of its novel products, which belongs to Smartmatic.\nThese arrangements were purportedly agreed upon under the scrutiny and approval of the Committee on Foreign Investment in the United States (CFIUS) of the U.S. Treasury Department, which had been investigating whether there were any ties between Sequoia, Smartmatic, and the government of Venezuela. CFIUS dropped the investigation when Smartmatic agreed to divest Sequoia, in a deal whereby all of Sequoia's shares were sold off to SVS Holdings for an undisclosed price.\n\nAmong other bidders, Smartmatic and Sequoia were competitors for the contract to provide voting machines and services to the 2010 national elections in the Philippines, one of the largest contracts ever in the voting technology industry. In the bidding process Sequoia was disqualified early, while Smartmatic was declared the winner.\n\nOn June 4, 2010 Dominion Voting Systems, a previously little-known Canadian company engaged in manufacturing electronic voting hardware and optical scanners, acquired Sequoia Voting Systems. \nHowever, in Feb. 2014 Sequoia filed a bankruptcy petition under Chapter 11 of the bankruptcy code.\n\n\n"}
{"id": "34544099", "url": "https://en.wikipedia.org/wiki?curid=34544099", "title": "Skylight", "text": "Skylight\n\nA skylight is a light-transmitting structure that forms all or part of the roof space of a building for daylighting purposes.\n\nOpen skylights were used in Ancient Roman architecture, such as the oculus of the Pantheon. Glazed 'closed' skylights have been in use since the Industrial Revolution made advances in glass production manufacturing. Mass production units since the mid-20th century have brought skylights to many uses and contexts. Energy conservation has brought new motivation, design innovation, transmission options, and efficiency rating systems for skylights.\n\nPrior to the Industrial Revolution, it was France that probably had the leading technology in architectural glass. One of the earliest forms of the glass skylight can be seen at the Palace of Versailles in the Galerie des Batailles, which was added onto the existing palace by Louis Philippe in the year 1830. Another form that displays early sky lighting technology is the Halle aux blés (Paris) built in 1763-67. This form of natural overhead lighting allowed for illumination while decoration could cover the entire interior wall, and it is the option least obstructed by other buildings. This means that sky lighting as we know it, in many forms today, was probably pioneered in France during the early 18th century or late 17th century. According to architectural glass, the earliest functional skylights would have been formed by either glass casting, crown glass (window), cylinder blown sheet, and machine drawn cylinder sheet, or fourcault process. \n\nSkylighting types include roof windows, unit skylights, tubular daylighting devices (TDDs), sloped glazing, and custom skylights. Uses include: \n\nAn unglazed hole in a roof.\nA fixed skylight consists of a structural perimeter frame supporting glazing infill (the light-transmitting portion, which is made primarily of glass or plastic). A fixed skylight is non-operable, meaning there is no ventilation.\nAn operable (venting) unit skylight uses a hinged sash attached to and supported by the frame. When within reach of the occupants, this type is also called a roof window.\n\nA retractable skylight rolls - on a set of tracks - off the frame, so that the interior of the facility is entirely open to the outdoors, i.e., not impeded by a hinged skylight. The terms \"retractable skylight\" and \"retractable roof\" are often used interchangeably, though \"skylight\" implies a degree of transparency.\n\nActive daylighting uses a tubular daylighting device—TDD. It is a roof-mounted fixed unit skylight element, condensing sunlight, distributed by a light conveying optic conduit to a light diffusing element. Being small in diameter, they can be used for daylighting smaller spaces such as hallways, and bounce light in darker corners of spaces. TDDs harvest daylight through a roof-mounted dome with diameters ranging from about 10 inches for residential applications to 22 inches for commercial buildings. Made from acrylic or polycarbonate formulated to block ultraviolet rays, the dome captures and redirects light rays into an aluminum tubing system that resembles ductwork. \nImage:Skylight on the roof terrace of Liverpool Central Library (2).JPG|TDD skylight on the roof terrace of Liverpool Central Library\n\nSloped glazing differs from other “skylights” in that one assembly contains multiple infill panels in a framing system, usually designed for a specific project and installed in sections on site.\n\nPavement lights are walk-on skylights. They are set into sidewalks, open areas, and well-lit interior floors.\n\nPrism lights are sometimes used as skylights; they redirect the light passing through.\n\nSkylights are widely used in designing daylighting for residential, public, and commercial buildings. Increased daylighting can result in less electrical lighting use and smaller sized window glazing (sidelighting), saving energy, lowering costs, and reducing environmental impacts. Daylighting can cut lighting energy use in some buildings by up to 80%.\n\nToplighting (skylights) works well with sidelighting (windows) to maximize daylighting: \n\nEven on overcast days, toplighting from skylights is three to ten times more efficient than sidelighting.\n\nMany recent advances in both glass and plastic infill systems have greatly benefited all skylight types. Some advances increase thermal performance, some are focused on preserving and utilizing daylight potential, and some are designed to enhance strength, durability, fire resistance and other performance measures.\n\nContemporary skylights using glass infill (windows) typically use sealed insulating glass units (IGU) made with two panes of glass. These types of products are NFRC-ratable for visible transmittance. Assemblies with three panes can sometimes be cost-justified in the coldest climate zones, but they lose some light by adding the third layer of glass.\n\nGlass units typically include at least one low emissivity (Low-E) coating applied to one or more glass surfaces to reduce the U-factor and especially SHGC by suppressing radiant heat flow. Many varieties of Low-E coatings also reduce daylight potential to different degrees. High purity inert gas is frequently used in the space(s) between panes, and advances in thermally efficient glass spacing and supporting elements can further improve thermal performance of glass-glazed skylight assemblies.\n\nPlastic glazing infill is commonly used in many skylights and TDDs. These assemblies typically contain thermally formed domes, but molded shapes are not uncommon. Domed skylights are typically used on low slope roofs. The dome shape allows for shedding of water and burning embers.\n\nPlastics used in skylights are UV stabilized and may feature other advances to improve thermal properties. Lack of accepted standards for measuring light transmittance is a disadvantage for comparing and choosing skylights with plastic glazing.\n\nAcrylic is the most common plastic glazing used for dome skylights. However, Polycarbonate and Copolyester materials are also used as glazing, where additional properties such as impact resistance may be required.\n\nNFRC — rating for visible transmittance\n\nU-factor — expresses the heat loss performance of any building assembly.\n\nSHGC—Solar Heat Gain Coefficient — measures the assembly’s transfer of heat from outside to inside that is caused by sunlight.\n\nThese properties are labeled in the U.S. as a decimal between zero and one, with lower numbers indicating lower heat transfer rates. Depending on the geographic region, optimal U-factor and SHGC performance will vary. In the sunny southern climate zones, a lower SHGC is more important than lower U-factor. In the cooler northern climate zones, lower U-factor is more important, and higher SHGC can be justified.\n\nIn selection of skylights, a balance is sought between low U-factor and optimal SHGC values, while preserving enough daylight supply to minimize artificial light use. Automatic light sensing controls for electric lighting maximize energy savings.\n\nA study concluded that students have significantly higher test scores in classrooms that optimize daylighting, than classrooms that do not. Other studies show that daylight positively affects physiological and psychological well-being, which can increase productivity in many contexts, such as sales in retail spaces.\n\nIn terms of cost savings, U.S. DOE reported that many commercial buildings can reduce total energy costs by up to one-third through the optimal use of daylighting. The majority of commercial warehouses and 'big box stores' built in recent years have used skylights extensively for energy/costs savings.\n\n"}
{"id": "14382459", "url": "https://en.wikipedia.org/wiki?curid=14382459", "title": "Split attention effect", "text": "Split attention effect\n\nThe split-attention effect is a learning effect inherent within some poorly designed instructional materials. It is apparent when the same modality (e.g. visual) is used for various types of information within the same display. To learn from these materials, learners must split their attention between these materials to understand and use the materials provided.\n\nConsider the graphic below from Tarmizi and Sweller (1988). They used these graphics to compare the learning that takes place given split attention conditions. Each is a possibility of how one might arrange graphical material within a lesson. Ward and Sweller (1990) advise instructional designers to be careful when they direct a learner's attention. Sweller and his associates found that learners had difficulty following some worked examples if they included diagrams that were separated from their formulas. In several studies and over a variety of experiments, they found that learners using integrated diagrams were better able to process that information, and significantly improved their performance relative to their peers (Ward & Sweller, 1990; Chandler & Sweller, 1991; Chandler & Sweller, 1992).\n\nThe split-attention effect is not limited to geometry, Chandler and Sweller (1991) found that this effect extends to a variety of other disciplines and is simply a limitation of human information processing. This overload is the result of high visual cognitive load due to poor instructional design. By simply integrating formulas with diagrams, learners found it easier to integrate and process both forms of visual information and in turn they performed significantly better (Chandler & Sweller, 1991; Chandler & Sweller, 1992).\n\nThe left example produces split attention, however the right example enhances learning because it guides the learner's attention through the worked example. The split-attention effect is an important form of extraneous cognitive load that instructional designers should avoid.\n\nChandler and Sweller (1992) found through empirical study that the integration of text and diagrams reduces cognitive load and facilitates learning. They found that this effect is evident, when learners are required to split their attention between different sources of information (e.g., text and diagrams).\n\nSplit attention is important evidence of the cognitive load theory (that the working memory load of instructional materials is important in the design of instructional materials). Chandler and Sweller (1992) found that students viewing integrated instruction spent less time processing the materials and outperformed students in the split attention condition.\n\nMoreno and Mayer (2000) found evidence for auditory split attention when they tested learners with both ambient environmental sounds and music as they learned from instructional materials. Animation is processed in a visual channel but must be converted to the auditory channel. The extraneous cognitive load imposed by music or environmental sounds were not conducive to learning.\n\nThese researchers studied learners that learned within an environment that had additional (extraneous) environmental sounds or music and found that learners performed significantly poorer on retention and transfer tests (Moreno, 2001).\n\n\n"}
{"id": "2147274", "url": "https://en.wikipedia.org/wiki?curid=2147274", "title": "Supercritical fluid extraction", "text": "Supercritical fluid extraction\n\nSupercritical Fluid Extraction (SFE) is the process of separating one component (the extractant) from another (the matrix) using supercritical fluids as the extracting solvent. Extraction is usually from a solid matrix, but can also be from liquids. SFE can be used as a sample preparation step for analytical purposes, or on a larger scale to either strip unwanted material from a product (e.g. decaffeination) or collect a desired product (e.g. essential oils). These essential oils can include limonene and other straight solvents. Carbon dioxide (CO) is the most used supercritical fluid, sometimes modified by co-solvents such as ethanol or methanol. Extraction conditions for supercritical carbon dioxide are above the critical temperature of 31 °C and critical pressure of 74 bar. Addition of modifiers may slightly alter this. The discussion below will mainly refer to extraction with CO, except where specified.\n\nThe properties of the supercritical fluid can be altered by varying the pressure and temperature, allowing selective extraction. For example, volatile oils can be extracted from a plant with low pressures (100 bar), whereas liquid extraction would also remove lipids. Lipids can be removed using pure CO at higher pressures, and then phospholipids can be removed by adding ethanol to the solvent. The same principle can be used to extract polyphenols and unsaturated fatty acids separately from wine wastes.\n\nExtraction is a diffusion-based process, in which the solvent is required to diffuse into the matrix and the extracted material to diffuse out of the matrix into the solvent. Diffusivities are much faster in supercritical fluids than in liquids, and therefore extraction can occur faster. In addition, due to the lack of surface tension and negligible viscosities compared to liquids, the solvent can penetrate more into the matrix inaccessible to liquids. An extraction using an organic liquid may take several hours, whereas supercritical fluid extraction can be completed in 10 to 60 minutes.\n\nThe requirement for high pressures increases the cost compared to conventional liquid extraction, so SFE will only be used where there are significant advantages. Carbon dioxide itself is non-polar, and has somewhat limited dissolving power, so cannot always be used as a solvent on its own, particularly for polar solutes. The use of modifiers increases the range of materials which can be extracted. Food grade modifiers such as ethanol can often be used, and can also help in the collection of the extracted material, but reduces some of the benefits of using a solvent which is gaseous at room temperature.\n\nThe system must contain a pump for the CO, a pressure cell to contain the sample, a means of maintaining pressure in the system and a collecting vessel. The liquid is pumped to a heating zone, where it is heated to supercritical conditions. It then passes into the extraction vessel, where it rapidly diffuses into the solid matrix and dissolves the material to be extracted. The dissolved material is swept from the extraction cell into a separator at lower pressure, and the extracted material settles out. The CO can then be cooled, re-compressed and recycled, or discharged to atmosphere.\n\nCarbon dioxide () is usually pumped as a liquid, usually below 5 °C (41 °F) and a pressure of about 50 bar. The solvent is pumped as a liquid as it is then almost incompressible; if it were pumped as a supercritical fluid, much of the pump stroke would be \"used up\" in compressing the fluid, rather than pumping it. For small scale extractions (up to a few grams / minute), reciprocating pumps or syringe pumps are often used. For larger scale extractions, diaphragm pumps are most common. The pump heads will usually require cooling, and the CO will also be cooled before entering the pump.\n\nPressure vessels can range from simple tubing to more sophisticated purpose built vessels with quick release fittings. The pressure requirement is at least 74 bar, and most extractions are conducted at under 350 bar. However, sometimes higher pressures will be needed, such as extraction of vegetable oils, where pressures of 800 bar are sometimes required for complete miscibility of the two phases.\n\nThe vessel must be equipped with a means of heating. It can be placed inside an oven for small vessels, or an oil or electrically heated jacket for larger vessels. Care must be taken if rubber seals are used on the vessel, as the supercritical carbon dioxide may dissolve in the rubber, causing swelling, and the rubber will rupture on depressurization. \n\nThe pressure in the system must be maintained from the pump right through the pressure vessel. In smaller systems (up to about 10 mL / min) a simple restrictor can be used. This can be either a capillary tube cut to length, or a needle valve which can be adjusted to maintain pressure at different flow rates. In larger systems a back pressure regulator will be used, which maintains pressure upstream of the regulator by means of a spring, compressed air, or electronically driven valve. Whichever is used, heating must be supplied, as the adiabatic expansion of the CO results in significant cooling. This is problematic if water or other extracted material is present in the sample, as this may freeze in the restrictor or valve and cause blockages.\n\nThe supercritical solvent is passed into a vessel at lower pressure than the extraction vessel. The density, and hence dissolving power, of supercritical fluids varies sharply with pressure, and hence the solubility in the lower density CO is much lower, and the material precipitates for collection. It is possible to fractionate the dissolved material using a series of vessels at reducing pressure. The CO can be recycled or depressurized to atmospheric pressure and vented. For analytical SFE, the pressure is usually dropped to atmospheric, and the now gaseous carbon dioxide bubbled through a solvent to trap the precipitated components.\n\nThis is an important aspect. The fluid is cooled before pumping to maintain liquid conditions, then heated after pressurization. As the fluid is expanded into the separator, heat must be provided to prevent excessive cooling. For small scale extractions, such as for analytical purposes, it is usually sufficient to pre-heat the fluid in a length of tubing inside the oven containing the extraction cell. The restrictor can be electrically heated, or even heated with a hairdryer. For larger systems, the energy required during each stage of the process can be calculated using the thermodynamic properties of the supercritical fluid.\n\nThere are two essential steps to SFE, transport (by diffusion or otherwise) from with the solid particles to the surface, and dissolution in the supercritical fluid. Other factors, such as diffusion into the particle by the SF and reversible release such as desorption from an active site are sometimes significant, but not dealt with in detail here. Figure 2 shows the stages during extraction from a spherical particle where at the start of the extraction the level of extractant is equal across the whole sphere (Fig. 2a). As extraction commences, material is initially extracted from the edge of the sphere, and the concentration in the center is unchanged (Fig 2b). As the extraction progresses, the concentration in the center of the sphere drops as the extractant diffuses towards the edge of the sphere (Figure 2c).\n\nThe relative rates of diffusion and dissolution are illustrated by two extreme cases in Figure 3. Figure 3a shows a case where dissolution is fast relative to diffusion. The material is carried away from the edge faster than it can diffuse from the center, so the concentration at the edge drops to zero. The material is carried away as fast as it arrives at the surface, and the extraction is completely diffusion limited. Here the rate of extraction can be increased by increasing diffusion rate, for example raising the temperature, but not by increasing the flow rate of the solvent. Figure 3b shows a case where solubility is low relative to diffusion. The extractant is able to diffuse to the edge faster than it can be carried away by the solvent, and the concentration profile is flat. In this case, the extraction rate can be increased by increasing the rate of dissolution, for example by increasing flow rate of the solvent. \n\nThe extraction curve of % recovery against time can be used to elucidate the type of extraction occurring. Figure 4(a) shows a typical diffusion controlled curve. The extraction is initially rapid, until the concentration at the surface drops to zero, and the rate then becomes much slower. The % extracted eventually approaches 100%. Figure 4(b) shows a curve for a solubility limited extraction. The extraction rate is almost constant, and only flattens off towards the end of the extraction. Figure 4(c) shows a curve where there are significant matrix effects, where there is some sort of reversible interaction with the matrix, such as desorption from an active site. The recovery flattens off, and if the 100% value is not known, then it is hard to tell that extraction is less than complete.\n\nThe optimum will depend on the purpose of the extraction. For an analytical extraction to determine, say, antioxidant content of a polymer, then the essential factors are complete extraction in the shortest time. However, for production of an essential oil extract from a plant, then quantity of CO used will be a significant cost, and \"complete\" extraction not required, a yield of 70 - 80% perhaps being sufficient to provide economic returns. In another case, the selectivity may be more important, and a reduced rate of extraction will be preferable if it provides greater discrimination. Therefore, few comments can be made which are universally applicable. However, some general principles are outlined below.\n\nThis can be achieved by increasing the temperature, swelling the matrix, or reducing the particle size. Matrix swelling can sometimes be increased by increasing the pressure of the solvent, and by adding modifiers to the solvent. Some polymers and elastomers in particular are swelled dramatically by CO, with diffusion being increased by several orders of magnitude in some cases.\n\nGenerally, higher pressure will increase solubility. The effect of temperature is less certain, as close to the critical point, increasing the temperature causes decreases in density, and hence dissolving power. At pressures well above the critical pressure, solubility is likely to increase with temperature. Addition of low levels of modifiers (sometimes called entrainers), such as methanol and ethanol, can also significantly increase solubility, particularly of more polar compounds.\n\nThe flow rate of supercritical carbon dioxide should be measured in terms of mass flow rather than by volume because the density of the changes according to the temperature both before entering the pump heads and during compression. Coriolis flow meters are best used to achieve such flow confirmation. To maximize the rate of extraction, the flow rate should be high enough for the extraction to be completely diffusion limited (but this will be very wasteful of solvent). However, to minimize the amount of solvent used, the extraction should be completely solubility limited (which will take a very long time). Flow rate must therefore be determined depending on the competing factors of time and solvent costs, and also capital costs of pumps, heaters and heat exchangers. The optimum flow rate will probably be somewhere in the region where both solubility and diffusion are significant factors.\n\n\n"}
{"id": "51483280", "url": "https://en.wikipedia.org/wiki?curid=51483280", "title": "Sustainable Urban Mobility Plan", "text": "Sustainable Urban Mobility Plan\n\nA Sustainable Urban Mobility Plan (SUMP) is a planning concept applied by local and regional authorities for strategic mobility planning. It encourages a shift towards more sustainable transport modes and supports the integration and balanced development of all modes. A SUMP is instrumental in solving urban transport problems and reaching local and higher-level environmental, social, and economic objectives.\n\nIn 2009, the European Commission adopted the Action Plan on Urban Mobility, which proposes measures to encourage and help local, regional and national authorities in achieving their goals for sustainable urban mobility. Also the 2011 Transport White Paper “Roadmap to a Single European Transport Area - Towards a competitive and resource efficient transport system” advises cities to develop Sustainable Urban Mobility Plans. The European Commission adopted the Urban Mobility Package “Together towards competitive and resource-efficient urban mobility” in December 2013. Sustainable urban mobility planning is emphasized in the Urban Mobility Package alongside urban freight distribution, urban access regulations, deployment of intelligent transportation system (ITS) solutions in urban areas and road traffic safety. In its annex, the Urban Mobility Package includes a comprehensive definition and explanation of the SUMP concept which was developed based on a discussion and exchange process between planning experts and stakeholders across the European Union.\n\nA Sustainable Urban Mobility Plan addresses all modes and forms of urban and regional transport. It aims to provide sustainable and high-quality transport and mobility in the agglomeration and enhance its accessibility. Instead of addressing the needs of the administrative area only, a SUMP regards the entire urban area including its commuter hinterland. A SUMP integrates technical, infrastructure, policy, and soft measures to improve performance and cost-effectiveness. It aims to meet the basic mobility needs of all users. The SUMP concept emphasizes aspects of participatory planning, vertical and horizontal integration, and mechanisms for monitoring, evaluation and quality control.\n\n\nExternal links\n\n"}
{"id": "5012810", "url": "https://en.wikipedia.org/wiki?curid=5012810", "title": "Synchronous learning", "text": "Synchronous learning\n\nSynchronous learning refers to a learning event in which a group of students are engaging in learning at the same time. Before learning technology allowed for synchronous learning environments, most online education took place through asynchronous learning methods. Since synchronous tools that can be used for education have become available, many people are turning to them as a way to help decrease the challenges associated with transactional distance that occurs in online education. Several case studies\nthat found that students are able to develop a sense of community over online synchronous communication platforms.\n\nWhile many online educational programs started out as and with the advent of web conferencing tools, people can learn at the same time in different places as well. For example, use of instant messaging or live chat, webinars and video conferencing allow for students and teachers to collaborate and learn in real time.\n\nA lecture is an example of synchronous learning in a face-to-face environment, because learners and teachers are all in the same place at the same time. Another example of a synchronous learning event would involve students watching a live web stream of a class, while simultaneously taking part in a discussion. Synchronous learning can be facilitated by having students and instructors participate in a class via a web conferencing tool. These synchronous experiences can be designed to develop and strengthen instructor-student and student-student relationships, which can be a challenge in distance learning programs.\n\nSynchronous communication in distance education began long before the advent of the use of computers in synchronous learning. After the very early days of distance education, when students and instructors communicated asynchronously via the post office, synchronous forms of communication in distance education emerged with broadcast radio and television. However, it was not until the 1980s, with video-conferencing and interactive television, that students could ask questions and discuss concepts while seeing participants in a synchronous setting. Manifestations of interactive multimedia, the Internet, access to Web-based resources, to synchronous and asynchronous forms of computer mediated communication followed in the 1990s (Bernard, et al., 2005; Simonson, et al., 2012, p. 37).\n\n\n"}
{"id": "1135942", "url": "https://en.wikipedia.org/wiki?curid=1135942", "title": "Technician", "text": "Technician\n\nA technician is a worker in a field of technology who is proficient in the relevant skill and technique, with a relatively practical understanding of the theoretical principles.\n\nThe term technician covers many different specialisations. These include:\n\n\nIn the UK a shortage of skilled technicians in the science, engineering and technology sectors has led to various campaigns to encourage more people to become technicians and to promote the role of technician.\n\n4. Report for TheNextHint\n\n"}
{"id": "1926240", "url": "https://en.wikipedia.org/wiki?curid=1926240", "title": "Teletext", "text": "Teletext\n\nTeletext (or broadcast teletext) is a television information retrieval service created in the United Kingdom in the early 1970s by the Philips Lead Designer for VDUs, John Adams. Teletext is a means of sending pages of text and simple geometric shapes from mosaic blocks to a VBI decoder equipped television screen by use of a number of reserved vertical blanking interval lines that together form the dark band dividing pictures horizontally on the television screen. It offers a range of text-based information, typically including news, weather and TV schedules. Paged subtitle (or closed captioning) information is also transmitted within the television signal.\n\nIt is closely linked to the PAL broadcast system used in Europe. Other teletext systems have been developed to work with the SECAM and NTSC systems, but teletext failed to gain widespread acceptance in North America and other areas where NTSC is used. In contrast, teletext is nearly ubiquitous across Europe as well as some other regions, with most major broadcasters providing a teletext service. Common teletext services include TV schedules, regularly updated current affairs and sport news, simple games (such as quizzes) and subtitles (or closed captioning).\n\nTeletext is broadcast in numbered \"pages.\" For example, a list of news headlines might appear on page 110; a teletext user would type \"110\" into the TV's remote control to view this page.\nThe broadcaster constantly sends out pages in sequence. There will typically be a delay of a few seconds from requesting the page and it being broadcast and displayed, the time being entirely dependent in the number of pages being broadcast.\nMore sophisticated receivers use a buffer memory to store some or all of the teletext pages as they are broadcast, allowing instant display from the buffer.\n\nThis basic architecture separates from other digital information systems, such as the internet, whereby pages are 'requested' and then 'sent' to the user – a method not possible given the one-way nature of broadcast teletext.\nUnlike the Internet, teletext is broadcast, so it does not slow down further as the number of users increase, although the greater number of pages, the longer one is likely to wait for each to be found in the cycle. For this reason, some pages (e.g. common index pages) are broadcast more than once in each cycle.\n\nTeletext proved to be a reliable text news service during events such as the September 11 terrorist attacks, during which the webpages of major news sites became inaccessible because of the high demand. Teletext is also used for carrying special packets interpreted by TVs and video recorders, containing information about subjects such as channels and programming.\n\nAlthough the term \"teletext\" tends to be used to refer to the PAL-based system, or variants, the recent availability of digital television has led to more advanced systems being provided that perform the same task, such as MHEG-5 in the UK, and Multimedia Home Platform.\n\nTeletext is a means of sending text and simple geometric shapes to a properly equipped television screen by use of one of the \"vertical blanking interval\" lines that together form the dark band dividing pictures horizontally on the television screen. \nTransmitting and displaying subtitles was relatively easy. It requires limited bandwidth; at a rate of perhaps a few words per second. However, it was found that by combining even a slow data rate with a suitable memory, whole pages of information could be sent and stored in the TV for later recall.\n\nIn the early 1970s work was in progress in Britain to develop such a system. The goal was to provide UK rural homes with electronic hardware that could download pages of up-to-date news, reports, facts and figures targeting U.K. agriculture. The original idea was the brainchild of Philips (CAL) Laboratories in 1970.\n\nIn 1971, CAL engineer John Adams created a design and proposal for UK broadcasters. His configuration contained all the fundamental elements of classic Teletext including pages of 24 rows with 40 characters each, page selection, sub pages of information and vertical blanking interval data transmission.\n\nA major objective for Adams during the concept development stage was to make Teletext affordable to the home user. \nIn reality, there was no scope to make an economic Teletext system with 1971 technology. However, as low cost was essential to the project's long term success, this obstacle had to be overcome.\n\nMeanwhile, the General Post Office (GPO), whose telecommunications division later became British Telecom, had been researching a similar concept since the late 1960s, known as Viewdata. Unlike Teledata which was a one-way service carried in the existing TV signal, Viewdata was a two-way system using telephones. Since the Post Office owned the telephones, this was considered to be an excellent way to drive more customers to use the phones.\n\nIn 1972 the BBC demonstrated their system, now known as Ceefax (\"see facts\", the departmental stationery used the \"Cx\" logo), on various news shows. The Independent Television Authority (ITA) announced their own service in 1973, known as ORACLE (Optional Reception of Announcements by Coded Line Electronics). Not to be outdone, the GPO immediately announced a 1200/75 baud videotext service under the name Prestel.\n\nThe first teletext test transmissions were made by Ceefax in 1973. After proliferation of the BBC system in the UK, it was adopted in Europe and standardised as World System Teletext (WST). The World Wide Web began to take over some of the functions of teletext from the late 1990s and many broadcasters have ceased broadcast of teletext—CNN in 2006 and the BBC in 2012. In the UK the decline of teletext has been hastened by the introduction of digital television, though an aspect of teletext continues in closed captioning. In other countries the system is still widely used on standard-definition DVB broadcasts.\n\nA number of teletext services have been syndicated to web viewers, which mimic the look and feel of broadcast teletext. RSS feeds of news and information from the BBC are presented in Ceefax format in the web viewer at PagesFromCeefax.net.\n\nIn 2016, the Teefax teletext service was launched in the United Kingdom to coverage by the BBC, ITV and others. Using a Raspberry Pi computer card as a set-top box, it feeds its service to standard televisions. Teefax content is a mix of crowdsourcing, syndication and contributions from media professionals who contributed heavily to broadcast teletext services. Teefax is also syndicated to a web viewer.\n\nThe systems were originally incompatible; Ceefax displayed pages of 24 lines with 32 characters each, while ORACLE offered pages of 22 lines with 40 characters each. In other ways the standards overlapped; for instance, both used 7-bit ASCII characters and other basic details. In 1974 all the services agreed a standard for displaying the information. The display would be a simple grid of text, with some \"graphics characters\" for constructing simple graphics. The standard did not define the delivery system, so both Viewdata-like and Teledata-like services could at least share the TV-side hardware (which at that point in time was quite expensive).\n\nFollowing test transmissions in 1973–74, towards the end of 1974 the BBC news department put together an editorial team of nine, including and led by Editor Colin McIntyre, to develop a news and information service. Initially limited to 30 pages, the Ceefax service was later expanded to 100 pages and was launched formally in 1976. It was followed quickly by ORACLE and Prestel. Wireless World magazine ran a series of articles between November 1975 and June 1976 describing the design and construction of a teletext decoder using mainly TTL devices; however, development was limited until the first TV sets with built-in decoders started appearing in 1977.\n\nBy 1982 there were two million such sets, and by the mid-1980s they were available as an option for almost every European TV set, typically by means of a plug in circuit board. It took another decade before the decoders became a standard feature on almost all sets with a screen size above 15 inches (teletext is still usually only an option for smaller \"portable\" sets). From the mid-1980s both Ceefax and ORACLE were broadcasting several hundred pages on every channel, slowly changing them throughout the day.\n\nThe \"Broadcast Teletext Specification\" was published in September 1976 jointly by the IBA, the BBC and the British Radio Equipment Manufacturers' Association. The new standard also made the term \"teletext\" generic, describing any such system. The standard was internationalised as World System Teletext (WST), formalised as an international standard by CCIR in 1986 as CCIR Teletext System B.\n\nIn the early 1980s a number of higher extension levels were envisaged for the specification, based on ideas then being promoted for worldwide videotex standards (telephone dial-up services offering a similar mix of text and graphics).\n\nThe most common implementation is Level 1.5, that supports languages other than English. Virtually any TV sold in Europe since the 1990s has support for this level.\nAround the year 2000 some station adopted Level 2.5 teletext / Hi-Text, that allowed for a larger color palette and higher resolution graphics.\n\nThe proposed higher content levels included geometrically-specified graphics (Level 4), and higher-resolution photographic-type images (Level 5), to be conveyed using the same underlying mechanism at the transport layer. No TV sets currently implement the two most sophisticated levels.\n\nThe Mullard SAA5050 was a character generator chip used in the UK teletext-equipped television sets. \nIn addition to the UK version, several variants of the chip existed with slightly different character sets for particular localizations and/or languages. These had part numbers SAA5051 (German), SAA5052 (Swedish), SAA5053 (Italian), SAA5054 (Belgian), SAA5055 (U.S. ASCII), SAA5056 (Hebrew) and SAA5057 (Cyrillic).\n\nThe type of decoder circuitry is sometimes marked on televisions as CCT (Computer-Controlled Teletext), or ECCT (Enhanced Computer-Controlled Teletext).\n\nBesides the hardware implementations, it's also possible to decode teletext using a PC and video capture or DVB board.\n\nAdoption in the United States was hampered due to a lack of a single teletext standard and consumer resistance to the high initial price of teletext decoders. Throughout the period of analogue broadcasting, teletext or other similar technologies in the US were practically non-existent, with the only technologies resembling such existing in the country being closed captioning, TV Guide On Screen, and Extended Data Services (XDS).\n\nNABTS was originally developed as a protocol by the Canadian Department of Communications, with their industry partner Norpak, for the Telidon system. It was similar to the European World System Teletext (WST, aka CCIR Teletext System B), but differences between the European and North American television standards and the greater flexibility of the Telidon standard led to the creation of a new delivery mechanism that was tuned for speed.\n\nFirst demonstrated in the United States in 1978, NABTS was the standard used for both CBS's ExtraVision and NBC's very short-lived NBC Teletext services in the mid-1980s.\n\nStation KSL-TV in Salt Lake City, Utah premiered a teletext service using Ceefax. They were followed by television network CBS, which carried out preliminary tests on both the British Teletext and the rival French Antiope system.\n\nOne of the most prominent providers was the \"Electra\" teletext service, using World System Teletext (WST), broadcast from the early 1980s on American cable channel WTBS.\nElectra also carried another teletext service on its higher-numbered pages, a service called \"Tempo\". \"Tempo\" mainly carried sports (and other miscellaneous) information on its pages.\nAt the time of Electra's closing in 1993, it was the only existing teletext service in the USA.\n\nA few other services were offered by some large-market TV stations in the US throughout the 1980s, such as Metrotext from KTTV in Los Angeles and KeyFax from WFLD in Chicago.\nDespite this, the system never caught on in the USA partly due to EIA-608 being deployed for captioning before Teletext was introduced and the higher cost of Teletext receivers.\n\nIn the 1980s a similar system called Telidon was developed in Canada by the Department of Communications. It used a simple graphics language that would allow a more complex circuit in the TV to decode not only characters, but graphics as well. To do this, the graphic was encoded as a series of instructions (graphics primitives) like \"polyline\" which was represented as the characters PL followed by a string of digits for the X and Y values of the points on the line. This system was referred to as PDI (Picture Description Instructions). Later improved versions of Telidon were developed into NAPLPS.\n\nAlthough there were numerous attempts to introduce NAPLPS services in North America, none of these was successful and eventually shut down. A number of special-purpose systems lived on for some time, similar to Prestel's lingering death, but the widespread rollout of internet access in the 1990s ended these efforts.\n\nBesides the US and UK developments, a number of similar teletext services were developed in other countries, some of which attempted to address the limitations of the initial British-developed system.\n\nIn France, where the SECAM standard is used in television broadcasting, a teletext system was developed in the late 1970s under the name \"Antiope\". It had a higher data rate and was capable of dynamic page sizes, allowing more sophisticated graphics. It was phased out in favour of standard teletext in 1991.\n\nThe Acorn BBC Micro's default graphics mode (mode 7) was based on Teletext display, and the computer could be used to create and serve Teletext-style pages over a modem connection. With a suitable adapter, the computer could receive and display teletext pages, as well as software over the BBC's Ceefax service, for a time. The Philips P2000 homecomputer's video logic was also based on a chip designed to provide teletext services in TVs.\n\nPrestel was a British information-retrieval system based on teletext protocols. However, it was essentially a different system, using a modem and the phone system to transmit and receive the data, comparable to systems such as France's Minitel. The modem was asymmetric, with data sent at 75-bit/s, and received at 1200-bit/s. This two-way nature allowed pages to be served on request, in contrast to the TV-based systems' sequential rolling method. It also meant that a limited number of extra services were available such as booking event or train tickets and a limited amount of online banking.\n\nTeletext information is broadcast in the vertical blanking interval between image frames in a broadcast television signal, in numbered \"pages.\"\n\nTeletext allows up to eight 'magazines' to be broadcast, identified by the first digit of the three-digit page number (1–8). Within each magazine there may theoretically be up to 256 pages at a given time, numbered in hexadecimal and prefixed with the magazine number – for example magazine 2 may contain pages numbered 200-2FF. In practice, however, non-decimal page numbers are rarely used as domestic teletext receivers will not have options to select hex values A-F, with such numbered pages only occasionally used for 'special' pages of interest to the broadcaster and not intended for public view.\n\nThe broadcaster constantly sends out pages in sequence in one of two modes: Serial mode broadcasts every page sequentially whilst parallel mode divides VBI lines amongst the magazines, enabling one page from each magazine to be broadcast simultaneously. There will typically be a delay of a few seconds from requesting the page and it being broadcast and displayed, the time being entirely dependent in the number of pages being broadcast in the magazine (parallel mode) or in total (serial mode) and the number of VBI lines allocated. In parallel mode, therefore, some magazines will load faster than others.\n\nMore sophisticated systems use a buffer memory to store some or all of the teletext pages as they are broadcast, allowing instant display from the buffer.\n\nThe greater number of pages, the longer one is likely to wait for each to be found in the cycle. For this reason, some pages (e.g. common index pages) are broadcast more than once in each cycle.\n\nA standard PAL signal contains 625 lines of video data per screen, broken into two \"fields\" containing half the lines of the whole image, divided as every odd line, then every even line number. Lines near the top of the screen are used to synchronize the display to the signal, and are not seen on-screen. CEPT1 hides the data in these lines, where they are not visible, using lines 6–22 on the first field and 318–335 on the second field. The system does not have to use all of these lines; a unique pattern of bits allows the decoder to identify which lines contain data. Some teletext services use a great number of lines, others, for reasons of bandwidth and technical issues, use fewer.\n\nTeletext in the PAL B system can use the VBI lines 6–22 in first half image and 318–334 in the other to transmit 360 data bits including clock run-in and framing code during the active video period at a rate of using binary NRZ line coding. The amplitude for a \"0\" is black level ±2% and a \"1\" is 66±6% of the difference between black and peak white level. The clock run in consist of of \"10\" and the framing code is \"11100100\". The two last bits of the clock-run in shall start within from the negative flank of the line synchronization pulse.\n\nThe rate is , i.e. the TV line frequency. Thus 625 * 25 * 444 = 6 937 500 Hz. Each bit will then be 144 ns long. The bandwidth amplitude is 50% at 3.5 MHz and 0% at 6 MHz.\nIf the horizontal sync pulse during the vertical synchronization starts in the middle of horizontal scan line. Then first interlace frame will be sent, otherwise if vertical synchronization let the full video line complete the second interlace frame is sent.\n\nLike EIA-608 bits are transmitted in the order of LSB to MSB with odd parity coding of 7-bit character codes. However unlike EIA-608, the digital DVB version is transmitted the same way. For single bit error recovery during transmission, the packet address (page row and magazine numbers) and header bytes (page number, subtitle flag, etc.) use hamming code 8/4 with extended packets (header extensions) using hamming 24/18, which basically doubles the bits used.\n\nThe commonly used standard B uses a fixed PAL subtitling bandwidth of 8600 (7680 without page/packet header) bits/s per field for a maximum of 32 characters per line per caption (maximum 3 captions – lines 19 – 21) for a 25 frame broadcast. While the bandwidth is greater than EIA-608, so is the error rate with more bits encoded per field. Subtitling packets use a lot of non-boxed spacing to control the horizontal positioning of a caption and to pad out the fixed packet. The vertical caption position is determined by the packet address.\n\nIn the case of the Ceefax and ORACLE systems and their successors in the UK, the teletext signal is transmitted as part of the ordinary analogue TV signal but concealed from view in the Vertical Blanking Interval (VBI) television lines which do not carry picture information. The teletext signal is digitally coded as 45-byte packets, so the resulting rate is 7,175 bits per second per used lines (41 7-bit 'bytes' per line, on each of 25 frames per second).\n\nA teletext page comprises one or more \"frames\", each containing a screen-full of text. The pages are sent out one after the other in a continual loop. When the user requests a particular page the decoder simply waits for it to be sent, and then captures it for display. In order to keep the delays reasonably short, services typically only transmit a few hundred frames in total. Even with this limited number, waits can be up to 30 seconds, although teletext broadcasters can control the speed and priority with which various pages are broadcast.\n\nModern television sets, however, usually have a built-in memory, often for a few thousand different pages. This way, the teletext decoder captures every page sent out and stores it in memory, so when a page is requested by the user it can be loaded directly from memory instead of having to wait for the page to be transmitted. When the page is transmitted again, the television checks if the page in memory is still up-to-date and updates it if necessary.\n\nThe text can be displayed instead of the television image, or superimposed on it (a mode commonly called \"mix\"). Some pages, such as subtitles (closed captioning), are \"in-vision\", meaning that text is displayed in a block on the screen covering part of the television image.\n\nThe original standard provides a mono spaced 40×24 character grid. Characters are sent using a 7-bit codec, with an 8th bit employed for error detection. The standard was improved in 1976 to allow for improved appearance and the ability to individually select the color of each character from a palette of 8. The proposed higher resolution Level 2 (1981) was not adopted in Britain (in-vision services from Ceefax & ORACLE did use it at various times however, though even this was ceased by the BBC in 1996), although transmission rates were doubled from two to four lines a frame.\n\nWhile the basic teletext format has remained unchanged in more than 30 years, a number of improvements and additions have been made.\n\nA closely related service is the Video Program System (VPS), introduced in Germany in 1985. Like teletext, this signal is also broadcast in the vertical blanking interval. It consists only of 32 bits of data, primarily the date and time for which the broadcast of the currently running TV programme was originally scheduled. Video recorders can use this information (instead of a simple timer) in order to automatically record a scheduled programme, even if the broadcast time changes after the user programmes the VCR. VPS also provides a PAUSE code; broadcasters can use it to mark interruptions and pause the recorders, however advertisement-financed broadcasters tend not to use it during their ad breaks.\nVPS (line 16) definition is now included in the PDC standard from ETSI.\n\nSome TV channels offer a service called interactive teletext to remedy some of the shortcomings of standard teletext. To use interactive teletext, the user calls a special telephone number with a regular telephone.\nA computer then instructs the user to go to a certain teletext page which has been assigned to the customer for that session. Usually the page initially contains a menu with options and the user chooses among the options using the buttons on the telephone. When a choice has been made, the selected page is immediately broadcast and can be viewed by the user. This is in contrast with usual teletext where the customer has to wait for the selected page to be broadcast, because the pages are broadcast sequentially. This technology enables teletext to be used for games, chat, access to databases etc. It allows one to overcome the limitations on the number of available pages. On the other hand, only a limited number of users can use the service at the same time, since one page is allocated per user. Some channels solve this by taking into account where the user is geographically calling from and by broadcasting different teletext pages in different geographical regions. In that way, two different users can be assigned the same page number at the same time as long as they do not receive the TV signals from the same source. Another drawback to the technology is the privacy concerns in that many users can see what a user is doing because the interactive pages are received by all viewers. Also, the user usually has to pay for the telephone call to the TV station. For these reasons, these services have since been superseded by the World Wide Web.\n\nWith the advent of digital television some countries adopted the misnomer \"digital teletext\" which, despite the previous teletext standard's digital nature, uses an interpreted binary language, such as MHEG-5 and Multimedia Home Platform (MHP).\n\nOthers use the same teletext streams as before on DVB transmissions, due to the DVB-TXT and DVB-VBI sub-standards.\nThose allow the emulation of analogue teletext on digital TV platforms, directly on the TV or set-top box, or via analog output, reproducing the vertical blanking interval data in which Teletext is carried.\n\nA number of broadcast authorities have recently ceased the transmission of teletext services.\n\n\n\"(Subtitling still continues to use Teletext in these three countries with some providers switching to using image based DVB subtitling for HD broadcasts. New Zealand solely uses DVB subtitling on terrestrial transmissions despite Teletext still being used on internal SDI links.)\"\n\n\n"}
{"id": "1033036", "url": "https://en.wikipedia.org/wiki?curid=1033036", "title": "Thin film", "text": "Thin film\n\nA thin film is a layer of material ranging from fractions of a nanometer (monolayer) to several micrometers in thickness. The controlled synthesis of materials as thin films (a process referred\nto as deposition) is a fundamental step in many applications. A familiar example is the household mirror, which typically has a thin metal coating on the back of a sheet of glass to form a reflective interface. The process of silvering was once commonly used to produce mirrors, while more recently the metal layer is deposited using techniques such as sputtering. Advances in thin film deposition techniques during the 20th century have enabled a wide range of technological breakthroughs in areas such as magnetic recording media, electronic semiconductor devices, LEDs, optical coatings (such as antireflective coatings), hard coatings on cutting tools, and for both energy generation (e.g. thin-film solar cells) and storage (thin-film batteries). It is also being applied to\npharmaceuticals, via thin-film drug delivery. A stack of thin films is called a multilayer.\n\nIn addition to their applied interest, thin films play an important role in the development and study of materials with new and\nunique properties. Examples include multiferroic materials, and superlattices that allow the study of\nquantum confinement by creating two-dimensional electron states.\n\nThe act of applying a thin film to a surface is \"thin-film deposition\" – any technique for depositing a thin film of material onto a substrate or onto previously deposited layers. \"Thin\" is a relative term, but most deposition techniques control layer thickness within a few tens of nanometres. Molecular beam epitaxy, Langmuir-Blodgett method and atomic layer deposition allow a single layer of atoms or molecules to be deposited at a time.\n\nIt is useful in the manufacture of optics (for reflective, anti-reflective coatings or self-cleaning glass, for instance), electronics (layers of insulators, semiconductors, and conductors form integrated circuits), packaging (i.e., aluminium-coated PET film), and in contemporary art (see the work of Larry Bell). Similar processes are sometimes used where thickness is not important: for instance, the purification of copper by electroplating, and the deposition of silicon and enriched uranium by a CVD-like process after gas-phase processing.\n\nDeposition techniques fall into two broad categories, depending on whether the process is primarily chemical or physical.\n\nHere, a fluid precursor undergoes a chemical change at a solid surface, leaving a solid layer. An everyday example is the formation of soot on a cool object when it is placed inside a flame. Since the fluid surrounds the solid object, deposition happens on every surface, with little regard to direction; thin films from chemical deposition techniques tend to be \"conformal\", rather than \"directional\".\n\nChemical deposition is further categorized by the phase of the precursor:\n\nPlating relies on liquid precursors, often a solution of water with a salt of the metal to be deposited. Some plating processes are driven entirely by reagents in the solution (usually for noble metals), but by far the most commercially important process is electroplating. It was not commonly used in semiconductor processing for many years, but has seen a resurgence with more widespread use of chemical-mechanical polishing techniques.\n\nChemical solution deposition (CSD) or chemical bath deposition (CBD) uses a liquid precursor, usually a solution of organometallic powders dissolved in an organic solvent. This is a relatively inexpensive, simple thin-film process that produces stoichiometrically accurate crystalline phases. This technique is also known as the sol-gel method because the 'sol' (or solution) gradually evolves towards the formation of a gel-like diphasic system.\n\nLangmuir-Blodgett method uses molecules floating on top of an aqueous subphase. The packing density of molecules is controlled, and the packed monolayer is transferred on a solid substrate by controlled withdrawal of the solid substrate from the subphase. This allows creating thin films of various molecules such as nanoparticles, polymers and lipids with controlled particle packing density and layer thickness.\n\nSpin coating or spin casting, uses a liquid precursor, or sol-gel precursor deposited onto a smooth, flat substrate which is subsequently spun at a high velocity to centrifugally spread the solution over the substrate. The speed at which the solution is spun and the viscosity of the sol determine the ultimate thickness of the deposited film. Repeated depositions can be carried out to increase the thickness of films as desired. Thermal treatment is often carried out in order to crystallize the amorphous spin coated film. Such crystalline films can exhibit certain preferred orientations after crystallization on single crystal substrates.\n\nDip coating is similar to spin coating in that a liquid precursor or sol-gel precursor is deposited on a substrate, but in this case the substrate is completely submerged in the solution and then withdrawn under controlled conditions. By controlling the withdrawal speed, the evaporation conditions (principally the humidity, temperature) and the volatility/viscosity of the solvent, the film thickness, homogeneity and nanoscopic morphology are controlled. There are two evaporation regimes: the capillary zone at very low withdrawal speeds, and the draining zone at faster evaporation speeds.\nChemical vapor deposition (CVD) generally uses a gas-phase precursor, often a halide or hydride of the element to be deposited. In the case of MOCVD, an organometallic gas is used. Commercial techniques often use very low pressures of precursor gas.\n\nPlasma enhanced CVD (PECVD) uses an ionized vapor, or plasma, as a precursor. Unlike the soot example above, commercial PECVD relies on electromagnetic means (electric current, microwave excitation), rather than a chemical-reaction, to produce a plasma.\n\nAtomic layer deposition (ALD) uses gaseous precursor to deposit conformal thin films one layer at a time. The process is split up into two half reactions, run in sequence and repeated for each layer, in order to ensure total layer saturation before beginning the next layer. Therefore, one reactant is deposited first, and then the second reactant is deposited, during which a chemical reaction occurs on the substrate, forming the desired composition. As a result of the stepwise, the process is slower than CVD, however it can be run at low temperatures, unlike CVD.\n\nPhysical deposition uses mechanical, electromechanical or thermodynamic means to produce a thin film of solid. An everyday example is the formation of frost. Since most engineering materials are held together by relatively high energies, and chemical reactions are not used to store these energies, commercial physical deposition systems tend to require a low-pressure vapor environment to function properly; most can be classified as physical vapor deposition (PVD).\n\nThe material to be deposited is placed in an energetic, entropic environment, so that particles of material escape its surface. Facing this source is a cooler surface which draws energy from these particles as they arrive, allowing them to form a solid layer. The whole system is kept in a vacuum deposition chamber, to allow the particles to travel as freely as possible. Since particles tend to follow a straight path, films deposited by physical means are commonly \"directional\", rather than \"conformal\".\n\nExamples of physical deposition include:\nA thermal evaporator that uses an electric resistance heater to melt the material and raise its vapor pressure to a useful range. This is done in a high vacuum, both to allow the vapor to reach the substrate without reacting with or scattering against other gas-phase atoms in the chamber, and reduce the incorporation of impurities from the residual gas in the vacuum chamber. Obviously, only materials with a much higher vapor pressure than the heating element can be deposited without contamination of the film. Molecular beam epitaxy is a particularly sophisticated form of thermal evaporation.\n\nAn electron beam evaporator fires a high-energy beam from an electron gun to boil a small spot of material; since the heating is not uniform, lower vapor pressure materials can be deposited. The beam is usually bent through an angle of 270° in order to ensure that the gun filament is not directly exposed to the evaporant flux. Typical deposition rates for electron beam evaporation range from 1 to 10 nanometres per second.\n\nIn molecular beam epitaxy (MBE), slow streams of an element can be directed at the substrate, so that material deposits one atomic layer at a time. Compounds such as gallium arsenide are usually deposited by repeatedly applying a layer of one element (i.e., gallium), then a layer of the other (i.e., arsenic), so that the process is chemical, as well as physical; this is known also as atomic layer deposition. The beam of material can be generated by either physical means (that is, by a furnace) or by a chemical reaction (chemical beam epitaxy).\n\nSputtering relies on a plasma (usually a noble gas, such as argon) to knock material from a \"target\" a few atoms at a time. The target can be kept at a relatively low temperature, since the process is not one of evaporation, making this one of the most flexible deposition techniques. It is especially useful for compounds or mixtures, where different components would otherwise tend to evaporate at different rates. Note, sputtering's step coverage is more or less conformal. It is also widely used in the optical media. The manufacturing of all formats of CD, DVD, and BD are done with the help of this technique. It is a fast technique and also it provides a good thickness control. Presently, nitrogen and oxygen gases are also being used in sputtering.\n\nPulsed laser deposition systems work by an ablation process. Pulses of focused laser light vaporize the surface of the target material and convert it to plasma; this plasma usually reverts to a gas before it reaches the substrate.\n\nCathodic arc deposition (arc-PVD) which is a kind of ion beam deposition where an electrical arc is created that literally blasts ions from the cathode. The arc has an extremely high power density resulting in a high level of ionization (30–100%), multiply charged ions, neutral particles, clusters and macro-particles (droplets). If a reactive gas is introduced during the evaporation process, dissociation, ionization and excitation can occur during interaction with the ion flux and a compound film will be deposited.\n\nElectrohydrodynamic deposition (electrospray deposition) is a relatively new process of thin-film deposition. The liquid to be deposited, either in the form of nanoparticle solution or simply a solution, is fed to a small capillary nozzle (usually metallic) which is connected to a high voltage. The substrate on which the film has to be deposited is connected to ground. Through the influence of electric field, the liquid coming out of the nozzle takes a conical shape (Taylor cone) and at the apex of the cone a thin jet emanates which disintegrates into very fine and small positively charged droplets under the influence of Rayleigh charge limit. The droplets keep getting smaller and smaller and ultimately get deposited on the substrate as a uniform thin layer.\n\nFrank-van-der-Merwe (\"layer-by-layer\"). In this growth mode the adsorbate-surface and adsorbate-adsorbate interactions are balanced. This type of growth requires lattice matching, and hence considered an \"ideal\" growth mechanism.\n\nStranski–Krastanov growth (\"joint islands\" or \"layer-plus-island\"). In this growth mode the adsorbate-surface interactions are stronger than adsorbate-adsorbate interactions.\n\nVolmer-Weber (\"isolated islands\"). In this growth mode the adsorbate-adsorbate interactions are stronger than adsorbate-surface interactions, hence \"islands\" are formed right away.\n\nA subset of thin-film deposition processes and applications is focused on the so-called epitaxial growth of materials, \nthe deposition of crystalline thin films that grow following the crystalline structure of the substrate. The term epitaxy comes from the Greek roots epi (ἐπί), meaning \"above\", and taxis (τάξις), meaning \"an ordered manner\". It can be translated as \"arranging upon\".\n\nThe term homoepitaxy refers to the specific case in which a film of the same material is grown on a crystalline\nsubstrate. This technology is used, for instance, to grow a film which is more pure than the substrate, has a lower density\nof defects, and to fabricate layers having different doping levels. Heteroepitaxy refers to the case in which the film being deposited is different than the substrate.\n\nTechniques used for epitaxial growth of thin films include molecular beam epitaxy, chemical vapor deposition,\nand pulsed laser deposition.\n\nThe usage of thin films for decorative coatings probably represents their oldest application. This encompasses ca. 100 nm thin gold leafs that were already used in ancient India more than 5000 years ago. It may also be understood as any form of painting, although this kind of work is generally considered as an arts craft rather than an engineering or scientific discipline. Today, thin-film materials of variable thickness and high refractive index like titanium dioxide are often applied for decorative coatings on glass for instance, causing a rainbow-color appearance like oil on water. In addition, intransparent gold-colored surfaces may either be prepared by sputtering of gold or titanium nitride.\n\nThese layers serve in both reflective and refractive systems. Large-area (reflective) mirrors became available during the 19th century and were produced by sputtering of metallic silver or aluminum on glass. Refractive lenses for optical instruments like cameras and microscopes typically exhibit aberrations, i.e. non-ideal refractive behavior. While large sets of lenses had to be lined up along the optical path previously, nowadays, the coating of optical lenses with transparent multilayers of titanium dioxide, silicon nitride or silicon oxide etc. may correct these aberrations. A well-known example for the progress in optical systems by thin-film technology is represented by the only a few mm wide lens in smart phone cameras. Other examples are given by anti-reflection coatings on eyeglasses or solar panels.\n\nThin films are often deposited to protect an underlying work piece from external influences. The protection may operate by minimizing the contact with the exterior medium in order to reduce the diffusion from the medium to the work piece or vice versa. For instance, plastic lemonade bottles are frequently coated by anti-diffusion layers to avoid the out-diffusion of CO, into which carbonic acid decomposes that was introduced into the beverage under high pressure. Another example is represented by thin TiN films in microelectronic chips separating electrically conducting aluminum lines from the embedding insulator SiO in order to suppress the formation of AlO. Often, thin films serve as protection against abrasion between mechanically moving parts. Examples for the latter application are diamond-like carbon (DLC) layers used in car engines or thin films made of nanocomposites.\n\nThin layers from elemental metals like copper, aluminum, gold or silver etc. and alloys have found numerous applications in electrical devices. Due to their high electrical conductivity they are able to transport electrical currents or supply voltages. Thin metal layers serve in conventional electrical system, for instance, as Cu layers on printed circuit boards, as the outer ground conductor in coaxial cables and various other forms like sensors etc. A major field of application became their use in integrated circuits, where the electrical network among active and passive devices like transistors and capacitors etc. is built up from thin Al or Cu layers. These layers dispose of thicknesses in the range of a few 100 nm up to a few µm, and they are often embedded into a few nm thin titanium nitride layers in order to block a chemical reaction with the surrounding dielectric like SiO. The figure shows a micrograph of a laterally structured TiN/Al/TiN metal stack in a microelectronic chip.\n\nThin-film technologies are also being developed as a means of substantially reducing the cost of solar cells. The rationale for this is thin-film solar cells are cheaper to manufacture owing to their reduced material costs, energy costs, handling costs and capital costs. This is especially represented in the use of printed electronics (roll-to-roll) processes. Other thin-film technologies, that are still in an early stage of ongoing research or with limited commercial availability, are often classified as emerging or third generation photovoltaic cells and include, organic, dye-sensitized, and polymer solar cells, as well as quantum dot, copper zinc tin sulfide, nanocrystal and perovskite solar cells.\n\nThin-film printing technology is being used to apply solid-state lithium polymers to a variety of substrates to create unique batteries for specialized applications. Thin-film batteries can be deposited directly onto chips or chip packages in any shape or size. Flexible batteries can be made by printing onto plastic, thin metal foil, or paper.\n\n"}
{"id": "17484717", "url": "https://en.wikipedia.org/wiki?curid=17484717", "title": "Ultrasonic nozzle", "text": "Ultrasonic nozzle\n\nUltrasonic nozzles are a type of spray nozzle that uses high frequency vibration produced by piezoelectric transducers acting upon the nozzle tip that will create capillary waves in a liquid film. Once the amplitude of the capillary waves reaches a critical height (due to the power level supplied by the generator), they become too tall to support themselves and tiny droplets fall off the tip of each wave resulting in atomization.\n\nThe primary factors influencing the initial droplet size produced are frequency of vibration, surface tension, and viscosity of the liquid. Frequencies are commonly in the range of 20–180 kHz, beyond the range of human hearing, where the highest frequencies produce the smallest drop size.\n\nIn 1962 Dr. Robert Lang followed up on this work, essentially proving a correlation between his atomized droplet size relative to Rayleigh's liquid wavelength. Ultrasonic nozzles were first commercialized by Dr. Harvey L. Berger.\nSubsequent uses of the technology include coating blood collection tubes, spraying flux onto printed circuit boards, coating implantable drug eluting stents and balloon/catheters, Float glass manufacturing coatings, anti-microbial coatings onto food, precision semiconductor coatings and alternative energy coatings for solar cell and fuel cell manufacturing, among others.\n\nPharmaceuticals such as Sirolimus (Rapamycin) and Paclitaxel used with or without a polymer is coated on the surface of drug eluting stents (DES) and drug eluting balloons (DEB). These devices benefit greatly from ultrasonic spray nozzles for their ability to apply coatings with little to no loss. Medical devices such as DES and DEB because of their small size, require very narrow spray patterns, a low-velocity atomized spray and low-pressure air.\n\nResearch has shown that ultrasonic nozzles can be effectively used to manufacture Proton exchange membrane fuel cells. The inks typically used are a platinum-carbon suspension, wherein the platinum acts as a catalyst inside the cell. Traditional methods to apply the catalyst to the proton exchange membrane typically involve screen printing or doctor-blades. However, this method can have undesirable cell performance due to the tendency of the catalyst to form agglomerations resulting in non-uniform gas flow in the cell and prohibiting the catalyst from being fully exposed and running the risk that the solvent or carrier liquid may be absorbed into the membrane, both of which impeded proton exchange efficiency. When ultrasonic nozzles are used, the spray can be made to be as dry as necessary by the nature of the small and uniform droplet size, by varying the distance the droplets travel and by applying low heat to the substrate such that the droplets dry in the air before reaching the substrate. Process engineers have finer control over these types of variables as opposed to other technologies. Additionally, because the ultrasonic nozzle imparts energy to the suspension just prior to and during atomization, possible agglomerates in the suspension are broken up resulting in homogenous distribution of the catalyst, resulting in higher efficiency of the catalyst and in turn, the fuel cell.\n\nUltrasonic spray nozzle technology has been used to create films of indium tin oxide (ITO) in the formation of transparent conductive films (TCF). ITO has excellent transparency and low sheet resistance, however it is a scarce material and prone to cracking, which does not make it a good candidate for the new flexible TCFs. Graphene on the other hand can be made into a flexible film, extremely conductive and has high transparency. Ag nanowires (AgNWs) when combined with Graphene has been reported to be a promising superior TCF alternative to ITO. Prior studies focus on spin and bar coating methods which are not suitable for large area TCFs. A multi-step process utilizing ultrasonic spray of graphene oxide and conventional spray of AgNWs followed by a hydrazine vapor reduction, followed by the application of polymethylmethacrylate (PMMA) topcoat resulted in a peelable TCF that can be scaled to a large size.\n\nCNT thin films are used as alternative materials to create transparent conducting films (TCO layers) for touch panel displays or other glass substrates, as well as organic solar cell active layers.\n\nMicroelectromechanical systems (MEMs) are small microfabricated devices that combine electrical and mechanical components. Devices vary in size from below one micron to millimeters in size, functioning individually or in arrays to sense, control, and activate mechanical processes on the micro scale. Examples include pressure sensors, accelerometers, and microengines. Fabrication of MEMs involves depositing a uniform layer of photoresist onto the Si wafer. Photoresist has traditionally been applied to wafers in IC manufacturing using a spin coating technique. In complex MEMs devices that have etched areas with high aspect ratios, it can be difficult to achieve uniform coverage along the top, side walls, and bottoms of deep grooves and trenches using spin coating techniques due to the high rate of spin needed to remove excess liquid. Ultrasonic spray techniques are used to spray uniform coatings of photoresist onto high aspect ratio MEMs devices and can minimize usage and overspray of photoresist.\n\nThe non-clogging nature of ultrasonic nozzles, the small and uniform droplet size created by them and the fact that the spray plume can be shaped by tightly controlled air shaping devices make the application quite successful in wave soldering processes. The viscosity of nearly all fluxes on the market fit well within the capabilities of the technology. In soldering, \"no-clean\" flux is highly preferred. But if excessive quantities are applied the process will result in corrosive residues on the bottom of the circuit assembly.\n\nPhotovoltaic and dye-sensitized solar technology both need the application of liquids and coatings during the manufacturing process. With most of these substances being very expensive, any losses due to over-spray or quality control are minimized with the use of ultrasonic nozzles. In efforts to reduce the manufacturing costs of solar cell, traditionally done using the batch-based phosphoryl chloride or POCl method, it has been shown that using ultrasonic nozzles to lay a thin aqueous-based film onto silicon wafers can effectively be used as a diffusion process to create N-type layers with uniform surface resistance.\n\nUltrasonic spray pyrolysis is a chemical vapor deposition (CVD) method utilized in the formation of a variety of materials in thin film or nanoparticle form. Precursor materials are often fabricated through sol-gel methods and examples include the formation of aqueous silver nitrate, synthesis of zirconia particles, and fabrication of solid oxide fuel cell SOFC cathodes.\nAn atomized spray produced from an ultrasonic nozzle is subjected to a heated substrate typically ranging from 300–400 degrees C. Due to the high temperatures of the spray chamber, extensions to the ultrasonic nozzle (as pictured and labeled – High Temperature Ultrasonic Nozzle) such as a removable tip (tip is hidden under the vortex air shroud labeled #2) have been designed to be subjected to high temperatures while protecting the body (labeled #1) of the ultrasonic nozzle that contains temperature sensitive piezoelectric elements, typically outside of the spray chamber or by other means of isolation.\n\nBerger, Harvey L. Ultrasonic Liquid Atomization: Theory and Application. 2nd ed. Hyde Park: Partrige Hill, 2006. 1-177.\n\nLefebvre, Arthur, Atomization and Sprays, Hemisphere, 1989, \n\n"}
