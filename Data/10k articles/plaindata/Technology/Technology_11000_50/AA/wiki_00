{"id": "838142", "url": "https://en.wikipedia.org/wiki?curid=838142", "title": "Addressing mode", "text": "Addressing mode\n\nAddressing modes are an aspect of the instruction set architecture in most central processing unit (CPU) designs. The various addressing modes that are defined in a given instruction set architecture define how machine language instructions in that architecture identify the operand(s) of each instruction. An addressing mode specifies how to calculate the effective memory address of an operand by using information held in registers and/or constants contained within a machine instruction or elsewhere.\n\nIn computer programming, addressing modes are primarily of interest to compiler writers and to those who write in assembly languages.\n\nNote that there is no generally accepted way of naming the various addressing modes. In particular, different authors and computer manufacturers may give different names to the same addressing mode, or the same names to different addressing modes. Furthermore, an addressing mode which, in one given architecture, is treated as a single addressing mode may represent functionality that, in another architecture, is covered by two or more addressing modes. For example, some complex instruction set computer (CISC) architectures, such as the Digital Equipment Corporation (DEC) VAX, treat registers and literal or immediate constants as just another addressing mode. Others, such as the IBM System/360 and its successors, and most reduced instruction set computer (RISC) designs, encode this information within the instruction. Thus, the latter machines have three distinct instruction codes for copying one register to another, copying a literal constant into a register, and copying the contents of a memory location into a register, while the VAX has only a single \"MOV\" instruction.\n\nThe term \"addressing mode\" is itself subject to different interpretations: either \"memory address calculation mode\" or \"operand accessing mode\". Under the first interpretation, instructions that do not read from memory or write to memory (such as \"add literal to register\") are considered not to have an \"addressing mode\". The second interpretation allows for machines such as VAX which use operand mode bits to allow for a register or for a literal operand. Only the first interpretation applies to instructions such as \"load effective address\".\n\nThe addressing modes listed below are divided into code addressing and data addressing. Most computer architectures maintain this distinction, but there are, or have been, some architectures which allow (almost) all addressing modes to be used in any context.\n\nThe instructions shown below are purely representative in order to illustrate the addressing modes, and do not necessarily reflect the mnemonics used by any particular computer.\n\nDifferent computer architectures vary greatly as to the number of addressing modes they provide in hardware. There are some benefits to eliminating complex addressing modes and using only one or a few simpler addressing modes, even though it requires a few extra instructions, and perhaps an extra register. It has proven much easier to design pipelined CPUs if the only addressing modes available are simple ones.\n\nMost RISC architectures have only about five simple addressing modes, while CISC architectures such as the DEC VAX have over a dozen addressing modes, some of which are quite complicated. The IBM System/360 architecture had only three addressing modes; a few more have been added for the System/390.\n\nWhen there are only a few addressing modes, the particular addressing mode required is usually encoded within the instruction code\n(e.g. IBM System/360 and successors, most RISC). But when there are lots of addressing modes, a specific field is often set aside in the instruction to specify the addressing mode. The DEC VAX allowed multiple memory operands for almost all instructions, and so reserved the first few bits of each operand specifier to indicate the addressing mode for that particular operand.\nKeeping the addressing mode specifier bits separate from the opcode operation bits produces an orthogonal instruction set.\n\nEven on a computer with many addressing modes, measurements of actual programs indicate that the simple addressing modes listed below account for some 90% or more of all addressing modes used. Since most such measurements are based on code generated from high-level languages by compilers, this reflects to some extent the limitations of the compilers being used.\n\nSome instruction set architectures, such as Intel x86 and IBM/360 and its successors, have a load effective address instruction. This performs a calculation of the effective operand address, but instead of acting on that memory location, it loads the address that would have been accessed into a register. This can be useful when passing the address of an array element to a subroutine. It may also be a slightly sneaky way of doing more calculations than normal in one instruction; for example, using such an instruction with the addressing mode \"base+index+offset\" (detailed below) allows one to add two registers and a constant together in one instruction.\n\n |jump| address |\n\nThe effective address for an absolute instruction address is the address parameter itself with no modifications.\n\n |jump| offset | jump relative\n\nThe effective address for a PC-relative instruction address is the offset parameter added to the address of the next instruction. This offset is usually signed to allow reference to code both before and after the instruction.\n\nThis is particularly useful in connection with jumps, because typical jumps are to nearby instructions (in a high-level language most if or while statements are reasonably short). Measurements of actual programs suggest that an 8 or 10 bit offset is large enough for some 90% of conditional jumps (roughly ±128 or ±512 bytes).\n\nAnother advantage of PC-relative addressing is that the code may be position-independent, i.e. it can be loaded anywhere in memory without the need to adjust any addresses.\n\nSome versions of this addressing mode may be conditional referring to two registers (\"jump if reg1=reg2\"),\none register (\"jump unless reg1=0\") or no registers, implicitly referring to some previously-set bit in the status register. See also conditional execution below.\n\n |jumpVia| reg |\n\nThe effective address for a Register indirect instruction is the address in the specified register. For example, (A7) to access the content of address register A7.\n\nThe effect is to transfer control to the instruction whose address is in the specified register.\n\nMany RISC machines, as well as the CISC IBM System/360 and successors, have subroutine call instructions that place the return address in an address register—the register-indirect addressing mode is used to return from that subroutine call.\n\n | nop | execute the following instruction\n\nThe CPU, after executing a sequential instruction, immediately executes the following instruction.\n\nSequential execution is not considered to be an addressing mode on some computers.\n\nMost instructions on most CPU architectures are sequential instructions.\nBecause most instructions are sequential instructions, CPU designers often add features that deliberately sacrifice performance on the other instructions—branch instructions—in order to make these sequential instructions run faster.\n\nConditional branches load the PC with one of 2 possible results, depending on the condition—most CPU architectures use some other addressing mode for the \"taken\" branch, and sequential execution for the \"not taken\" branch.\n\nMany features in modern CPUs -- instruction prefetch and more complex pipelineing, out-of-order execution, etc. -- maintain the illusion that each instruction finishes before the next one begins, giving the same final results, even though that's not exactly what happens internally.\n\nEach \"basic block\" of such sequential instructions exhibits both temporal and spatial locality of reference.\n\nCPUs that do not use sequential execution with a program counter are extremely rare. In some CPUs, each instruction always specifies the address of next instruction. Such CPUs have an instruction pointer that holds that specified address; it is not a program counter because there is no provision for incrementing it. Such CPUs include some drum memory computers such as the IBM 650, the SECD machine, and the RTX 32P.\n\nOther computing architectures go much further, attempting to bypass the von Neumann bottleneck using a variety of alternatives to the program counter.\n\nSome computer architectures have conditional instructions (such as ARM, but no longer for all instructions in 64-bit mode) or conditional load instructions (such as x86) which can in some cases make conditional branches unnecessary and avoid flushing the instruction pipeline. An instruction such as a 'compare' is used to set a condition code, and subsequent instructions include a test on that condition code to see whether they are obeyed or ignored.\n\n |skipEQ| reg1| reg2| skip the next instruction if reg1=reg2\n\nSkip addressing may be considered a special kind of PC-relative addressing mode with a fixed \"+1\" offset. Like PC-relative addressing, some CPUs have versions of this addressing mode that only refer to one register (\"skip if reg1=0\") or no registers, implicitly referring to some previously-set bit in the status register. Other CPUs have a version that selects a specific bit in a specific byte to test (\"skip if bit 7 of reg12 is 0\").\n\nUnlike all other conditional branches, a \"skip\" instruction never needs to flush the instruction pipeline, though it may need to cause the next instruction to be ignored.\n\n | mul | reg1| reg2| reg3| reg1 := reg2 * reg3;\n\nThis \"addressing mode\" does not have an effective address and is not considered to be an addressing mode on some computers.\n\nIn this example, all the operands are in registers, and the result is placed in a register.\n\nThis is sometimes referred to as 'base plus displacement'\n\nThe offset is usually a signed 16-bit value (though the 80386 expanded it to 32 bits).\n\nIf the offset is zero, this becomes an example of \"register indirect\" addressing; the effective address is just the value in the base register.\n\nOn many RISC machines, register 0 is fixed at the value zero. If register 0 is used as the base register, this becomes an example of \"absolute addressing\". However, only a small portion of memory can be accessed (64 kilobytes, if the offset is 16 bits).\n\nThe 16-bit offset may seem very small in relation to the size of current computer memories (which is why the 80386 expanded it to 32-bit). It could be worse: IBM System/360 mainframes only have an unsigned 12-bit offset. However, the principle of locality of reference applies: over a short time span, most of the data items a program wants to access are fairly close to each other.\n\nThis addressing mode is closely related to the indexed absolute addressing mode.\n\n\"Example 1\":\nWithin a subroutine a programmer will mainly be interested in the parameters and the local variables, which will rarely exceed 64 KB, for which one base register (the frame pointer) suffices. If this routine is a class method in an object-oriented language, then a second base register is needed which points at the attributes for the current object (this or self in some high level languages).\n\n\"Example 2\":\nIf the base register contains the address of a composite type (a record or structure), the offset can be used to select a field from that record (most records/structures are less than 32 kB in size).\n\n | add | reg1| reg2| constant | reg1 := reg2 + constant;\n\nThis \"addressing mode\" does not have an effective address, and is not considered to be an addressing mode on some computers.\n\nThe constant might be signed or unsigned. For example, codice_1 to move the immediate hex value of \"FEEDABBA\" into register D0.\n\nInstead of using an operand from memory, the value of the operand is held within the instruction itself. On the DEC VAX machine, the literal operand sizes could be 6, 8, 16, or 32 bits long.\n\nAndrew Tanenbaum showed that 98% of all the constants in a program would fit in 13 bits (see RISC design philosophy).\n\n | clear carry bit |\nThe implied addressing mode, also called the implicit addressing mode (X86 assembly language), does not explicitly specify an effective address for either the source or the destination (or sometimes both).\n\nEither the source (if any) or destination effective address (or sometimes both) is implied by the opcode.\n\nImplied addressing was quite common on older computers (up to mid-1970s). Such computers typically had only a single register in which arithmetic could be performed—the accumulator. Such accumulator machines implicitly reference that accumulator in almost every instruction. For example, the operation < a := b + c; > can be done using the sequence < load b; add c; store a; > -- the destination (the accumulator) is implied in every \"load\" and \"add\" instruction; the source (the accumulator) is implied in every \"store\" instruction.\n\nLater computers generally had more than one general purpose register or RAM location which could be the source or destination or both for arithmetic—and so later computers need some other addressing mode to specify the source and destination of arithmetic.\n\nAmong the x86 instructions, some use implicit registers for one of the operands or results (multiplication, division, counting conditional jump).\n\nMany computers (such as x86 and AVR) have one special-purpose register called the stack pointer which is implicitly incremented or decremented when pushing or popping data from the stack, and the source or destination effective address is (implicitly) the address stored in that stack pointer.\n\nMany 32-bit computers (such as 68000, ARM, or PowerPC) have more than one register which could be used as a stack pointer—and so use the \"register autoincrement indirect\" addressing mode to specify which of those registers should be used when pushing or popping data from a stack.\n\nSome current computer architectures (e.g. IBM/390 and Intel Pentium) contain some instructions with implicit operands in order to maintain backwards compatibility with earlier designs.\n\nOn many computers, instructions that flip the user/system mode bit, the interrupt-enable bit, etc. implicitly specify the special register that holds those bits. This simplifies the hardware necessary to trap those instructions in order to meet the Popek and Goldberg virtualization requirements—on such a system, the trap logic does not need to look at any operand (or at the final effective address), but only at the opcode.\n\nA few CPUs have been designed where every operand is always implicitly specified in every instruction -- zero-operand CPUs.\n\n | load | reg | address |\n\nThis requires space in an instruction for quite a large address. It is often available on CISC machines which have variable-length instructions, such as x86.\n\nSome RISC machines have a special \"Load Upper Literal\" instruction which places a 16- or 20-bit constant in the top half of a register. That can then be used as the base register in a base-plus-offset addressing mode which supplies the low-order 16 or 12 bits. The combination allows a full 32-bit address.\n\n | load | reg |index| address |\n\nThis also requires space in an instruction for quite a large address. The address could be the start of an array or vector, and the index could select the particular array element required. The processor may scale the index register to allow for the size of each array element.\n\nNote that this is more or less the same as base-plus-offset addressing mode, except that the offset in this case is large enough to address any memory location.\n\n\"Example 1\":\nWithin a subroutine, a programmer may define a string as a local constant or a static variable.\nThe address of the string is stored in the literal address in the instruction.\nThe offset—which character of the string to use on this iteration of a loop—is stored in the index register.\n\n\"Example 2\":\nA programmer may define several large arrays as globals or as class variables.\nThe start of the array is stored in the literal address (perhaps modified at program-load time by a relocating loader) of the instruction that references it.\nThe offset—which item from the array to use on this iteration of a loop—is stored in the index register.\nOften the instructions in a loop re-use the same register for the loop counter and the offsets of several arrays.\n\n | load | reg | base|index|\n\nThe base register could contain the start address of an array or vector, and the index could select the particular array element required. The processor may scale the index register to allow for the size of each array element. This could be used for accessing elements of an array passed as a parameter.\n\n | load | reg | base|index| offset |\n\nThe base register could contain the start address of an array or vector of records, the index could select the particular record required, and the offset could select a field within that record. The processor may scale the index register to allow for the size of each array element.\n\n | load | reg | base|index|\n\nThe base register could contain the start address of an array or vector data structure, and the index could contain the offset of the one particular array element required.\n\nThis addressing mode dynamically scales the value in the index register to allow for the size of each array element, e.g. if the array elements are double precision floating-point numbers occupying 8 bytes each then the value in the index register is multiplied by 8 before being used in the effective address calculation. The scale factor is normally restricted to being a power of two, so that shifting rather than multiplication can be used.\n\n | load | reg1 | base|\n\nA few computers have this as a distinct addressing mode. Many computers just use \"base plus offset\" with an offset value of 0. For example, (A7)\n\n | load | reg | base |\n\nAfter determining the effective address, the value in the base register is incremented by the size of the data item that is to be accessed. For example, (A7)+ would access the content of the address register A7, then increase the address pointer of A7 by 1 (usually 1 word). Within a loop, this addressing mode can be used to step through all the elements of an array or vector.\n\nIn high-level languages it is often thought to be a good idea that functions which return a result should not have side effects (lack of side effects makes program understanding and validation much easier). This addressing mode has a side effect in that the base register is altered. If the subsequent memory access causes an error (e.g. page fault, bus error, address error) leading to an interrupt, then restarting the instruction becomes much more problematic since one or more registers may need to be set back to the state they were in before the instruction originally started.\n\nThere have been at least two computer architectures which have had implementation problems with regard to recovery from interrupts when this addressing mode is used:\n\n | load | reg | base|\n\nBefore determining the effective address, the value in the base register is decremented by the size of the data item which is to be accessed.\n\nWithin a loop, this addressing mode can be used to step backwards through all the elements of an array or vector. A stack can be implemented by using this mode in conjunction with the previous addressing mode (autoincrement).\n\nSee the discussion of side-effects under the autoincrement addressing mode.\n\nAny of the addressing modes mentioned in this article could have an extra bit to indicate indirect addressing, i.e. the address calculated using some mode is in fact the address of a location (typically a complete word) which contains the actual effective address.\n\nIndirect addressing may be used for code or data. It can make implementation of \"pointers\", \"references\", or \"handles\" much easier, and can also make it easier to call subroutines which are not otherwise addressable. Indirect addressing does carry a performance penalty due to the extra memory access involved.\n\nSome early minicomputers (e.g. DEC PDP-8, Data General Nova) had only a few registers and only a limited addressing range (8 bits). Hence the use of memory indirect addressing was almost the only way of referring to any significant amount of memory.\n\n | load | reg1 | base=PC | offset |\n\nThe PC-relative addressing mode can be used to load a register with a value stored in program memory a short distance away from the current instruction. It can be seen as a special case of the \"base plus offset\" addressing mode, one that selects the program counter (PC) as the \"base register\".\n\nThere are a few CPUs that support PC-relative data references. Such CPUs include:\n\nThe x86-64 architecture and the 64-bit ARMv8-A architecture have PC-relative addressing modes, called \"RIP-relative\" in x86-64 and \"literal\" in ARMv8-A. The Motorola 6809 also supports a PC-relative addressing mode.\n\nThe PDP-11 architecture, the VAX architecture, and the 32-bit ARM architectures support PC-relative addressing by having the PC in the register file.\n\nWhen this addressing mode is used, the compiler typically places the constants in a literal pool immediately before or immediately after the subroutine that uses them, to prevent accidentally executing those constants as instructions.\n\nThis addressing mode, which always fetches data from memory or stores data to memory and then sequentially falls through to execute the next instruction (the effective address points to data), should not be confused with \"PC-relative branch\" which does not fetch data from or store data to memory, but instead branches to some other instruction at the given offset (the effective address points to an executable instruction).\n\nThe addressing modes listed here were used in the 1950–1980 period, but are no longer available on most current computers.\nThis list is by no means complete; there have been many other interesting and peculiar addressing modes used from time to time, e.g. absolute-minus-logical-OR of two or three index registers.\n\nIf the word size is larger than the address, then the word referenced for memory-indirect addressing could itself have an indirect flag set to indicate another memory indirect cycle. This flag is referred to as an indirection bit, and the resulting pointer is a tagged pointer, the indirection bit tagging whether it is a direct pointer or an indirect pointer. Care is needed to ensure that a chain of indirect addresses does not refer to itself; if it does, one can get an infinite loop while trying to resolve an address.\n\nThe IBM 1620, the Data General Nova, the HP 2100 series, and the NAR 2 each have such a multi-level memory indirect, and could enter such an infinite address calculation loop.\nThe memory indirect addressing mode on the Nova influenced the invention of indirect threaded code.\n\nThe DEC PDP-10 computer with 18-bit addresses and 36-bit words allowed multi-level indirect addressing with the possibility of using an index register at each stage as well.\n\nOn some computers, the registers were regarded as occupying the first 8 or 16 words of memory (e.g. ICL 1900, DEC PDP-10). This meant that there was no need for a separate \"add register to register\" instruction – one could just use the \"add memory to register\" instruction.\n\nIn the case of early models of the PDP-10, which did not have any cache memory, a tight inner loop loaded into the first few words of memory (where the fast registers were addressable if installed) ran much faster than it would have in magnetic core memory.\n\nLater models of the DEC PDP-11 series mapped the registers onto addresses in the input/output area, but this was primarily intended to allow remote diagnostics. Confusingly, the 16-bit registers were mapped onto consecutive 8-bit byte addresses.\n\nThe DEC PDP-8 minicomputer had eight special locations (at addresses 8 through 15). When accessed via memory indirect addressing, these locations would automatically increment after use. This made it easy to step through memory in a loop without needing to use any registers to handle the steps.\n\nThe Data General Nova minicomputer had 16 special memory locations at addresses 16 through 31. When accessed via memory indirect addressing, 16 through 23 would automatically increment before use, and 24 through 31 would automatically decrement before use.\n\nThe Data General Nova, Motorola 6800 family, and MOS Technology 6502 family of processors had very few internal registers. Arithmetic and logical instructions were mostly performed against values in memory as opposed to internal registers. As a result, many instructions required a two-byte (16-bit) location to memory. Given that opcodes on these processors were only one byte (8 bits) in length, memory addresses could make up a significant part of code size.\n\nDesigners of these processors included a partial remedy known as \"zero page\" addressing. The initial 256 bytes of memory ($0000 – $00FF; a.k.a., page \"0\") could be accessed using a one-byte absolute or indexed memory address. This reduced instruction execution time by one clock cycle and instruction length by one byte. By storing often-used data in this region, programs could be made smaller and faster.\n\nAs a result, the zero page was used similarly to a register file. On many systems, however, this resulted in high utilization of the zero page memory area by the operating system and user programs, which limited its use since free space was limited.\n\nThe zero page address mode was enhanced in several late model 8-bit processors, including the WDC 65816, the CSG 65CE02, and the Motorola 6809. The new mode, known as \"direct page\" addressing, added the ability to move the 256-byte zero page memory window from the start of memory (offset address $0000) to a new location within the first 64 KB of memory.\n\nThe CSG 65CE02 allowed the direct page to be moved to any 256-byte boundary within the first 64 KB of memory by storing an 8-bit offset value in the new base page (B) register. The Motorola 6809 could do the same with its direct page (DP) register. The WDC 65816 went a step further and allowed the direct page to be moved to any location within the first 64 KB of memory by storing a 16-bit offset value in the new direct (D) register.\n\nAs a result, a greater number of programs were able to utilize the enhanced direct page addressing mode versus legacy processors that only included the zero page addressing mode.\n\nThis is similar to scaled index addressing, except that the instruction has two extra operands (typically constants), and the hardware checks that the index value is between these bounds.\n\nAnother variation uses vector descriptors to hold the bounds; this makes it easy to implement dynamically allocated arrays and still have full bounds checking.\n\nThe DEC PDP-10 computer used 36-bit words. It had special instructions which allowed memory to be treated as a sequence of bytes (bytes could be any size from 1 bit to 36 bits). A one-word sequence descriptor in memory, called a \"byte pointer\", held the current word address within the sequence, a bit position within a word, and the size of each byte.\n\nInstructions existed to load and store bytes via this descriptor, and to increment the descriptor to point at the next byte (bytes were not split across word boundaries). Much DEC software used five 7-bit bytes per word (plain ASCII characters), with one bit per word unused. Implementations of C had to use four 9-bit bytes per word, since the 'malloc' function in C assumes that the size of an \"int\" is some multiple of the size of a \"char\"; the actual multiple is determined by the system-dependent compile-time operator sizeof.\n\nThe Elliott 503, the Elliott 803, and the Apollo Guidance Computer only used absolute addressing, and did not have any index registers.\nThus, indirect jumps, or jumps through registers, were not supported in the instruction set. Instead, it could be instructed to \"add the contents of the current memory word to the next instruction\". Adding a small value to the next instruction to be executed could, for example, change a codice_2 into a codice_3, thus creating the effect of an indexed jump. Note that the instruction is modified on-the-fly and remains unchanged in memory, i.e. it is not self-modifying code. If the value being added to the next instruction was large enough, it could modify the opcode of that instruction as well as or instead of the address.\n\n"}
{"id": "5050389", "url": "https://en.wikipedia.org/wiki?curid=5050389", "title": "Advanced very-high-resolution radiometer", "text": "Advanced very-high-resolution radiometer\n\nAdvanced very-high-resolution radiometer (AVHRR) instruments are a type of space-borne sensor that measure the reflectance of the Earth in five spectral bands that are relatively wide by today's standards. Most AVHRR instruments are or have been carried by the National Oceanic and Atmospheric Administration (NOAA) family of polar orbiting platforms (POES). The first two are centered on the red (0.6 micrometres, 500 THz) and near-infrared (0.9 micrometres, 300 THz) regions, the third one is located around 3.5 micrometres, and the last two sample the thermal radiation emitted by the planet, around 11 and 12 micrometres, respectively. The NOAA satellite has equator crossing times of 0730 and 1930 local solar time.\n\nThe first AVHRR instrument was a four-channel radiometer, while the latest version (known as AVHRR/3, first carried on the NOAA-15 platform launched in May 1998) acquires data in a 6th channel located at 1.6 micrometres.\n\nNOAA has at least two polar-orbiting meteorological satellites in orbit at all times, with one satellite crossing the equator in the early morning and early evening and the other crossing the equator in the afternoon and late evening. The primary sensor on board both satellites is the AVHRR instrument. Morning-satellite data are most commonly used for land studies, while data from both satellites are used for atmosphere and ocean studies. Together they provide twice-daily global coverage, and ensure that data for any region of the earth are no more than six hours old. The swath width, the width of the area on the Earth's surface that the satellite can \"see\", is approximately 2,500 kilometers (~1,540 mi). The satellites orbit between 833 or 870 kilometers (+/− 19 kilometers, 516–541 miles) above the surface of the Earth.\n\nThe highest ground resolution that can be obtained from the current AVHRR instruments is , which means that the satellite records discrete information for areas on the ground that are 1.1 × 1.1 kilometers. This smallest recorded unit is called a pixel. AVHRR data have been collected continuously since 1981.\n\nThe primary purpose of these instruments is to monitor clouds and to measure the thermal emission of the Earth. These sensors have proven useful for a number of other applications, however, including the surveillance of land surfaces, ocean state, aerosols, etc. AVHRR data are particularly relevant to study climate change and environmental degradation because of the comparatively long records of data already accumulated (over 20 years). The main difficulty associated with these investigations is to properly deal with the many limitations of these instruments, especially in the early period (sensor calibration, orbital drift, limited spectral and directional sampling, etc.).\n\nThe AVHRR instrument also flies on the MetOp series of satellites. The three planned MetOp satellites are part of the EUMETSAT Polar System (EPS) run by EUMETSAT.\n\nRemote sensing applications of the AVHRR sensor are based on validation (matchup) techniques of co-located ground observations and satellite observations. Alternatively, radiative transfer calculations are performed. There are specialized codes which allow simulation of the AVHRR observable brightness temperatures and radiances in near infrared and infrared channels.\n\nPrior to launch, the visible channels (Ch. 1 and 2) of AVHRR sensors are calibrated by the instrument manufacturer, ITT, Aerospace/Communications Division, and are traceable to NIST standards. The calibration relationship between electronic digital count response (C) of the sensor and the albedo (A) of the calibration target are linearly regressed:\n\nwhere S and I are the slope and intercept (respectively) of the calibration regression [NOAA KLM]. However, the highly accurate prelaunch calibration will degrade during launch and transit to orbit as well as during the operational life of the instrument [Molling et al., 2010]. Halthore et al. [2008] note that sensor degradation is mainly caused by thermal cycling, outgassing in the filters, damage from higher energy radiation (such as ultraviolet (UV)), and condensation of outgassed gases onto sensitive surfaces.\n\nOne major design fault of AVHRR instruments is that they lack the capability to perform accurate, onboard calibrations once on orbit [NOAA KLM]. Thus, post-launch on-orbit calibration activities (known as vicarious calibration methods) must be performed to update and ensure the accuracy of retrieved radiances and the subsequent products derived from these values [Xiong et al., 2010]. Numerous studies have been performed to update the calibration coefficients and provide more accurate retrievals versus using the pre-launch calibration.\n\nRao and Chen [1995] use the Libyan Desert as a radiometrically stable calibration target to derive relative annual degradation rates for Channels 1 and 2 for AVHRR sensors on board the NOAA -7, -9, and -11 satellites. Additionally, with an aircraft field campaign over the White Sands desert site in New Mexico, USA [See Smith et al., 1988], an absolute calibration for NOAA-9 was transferred from a well calibrated spectrometer on board a U-2 aircraft flying at an altitude of ~18 km in a congruent path with the NOAA-9 satellite above. After being corrected for the relative degradation, the absolute calibration of NOAA-9 is then passed onto NOAA −7 and −11 via a linear relationship using Libyan Desert observations that are restricted to similar viewing geometries as well as dates in the same calendar month [Rao and Chen, 1995], and any sensor degradation is corrected for by adjusting the slope (as a function of days after launch) between the albedo and digital count signal recorded [Rao and Chen, 1999].\n\nIn another similar method using surface targets, Loeb [1997] uses spatiotemporal uniform ice surfaces in Greenland and Antarctica to produce second-order polynomial reflectance calibration curves as a function of solar zenith angle; calibrated NOAA-9 near-nadir reflectances are used to generate the curves that can then derive the calibrations for other AHVRRs in orbit (e.g. NOAA-11, -12, and -14).\n\nIt was found that the ratio of calibration coefficients derived by Loeb [1997] and Rao and Chen [1995] are independent of solar zenith angle, thus implying that the NOAA-9-derived calibration curves provide an accurate relation between the solar zenith angle and observed reflectance over Greenland and Antarctica.\n\nIwabuchi [2003] employed a method to calibrate NOAA-11 and -14 that uses clear-sky ocean and stratus cloud reflectance observations in a region of the NW Pacific Ocean and radiative transfer calculations of a theoretical molecular atmosphere to calibrate AVHRR Ch. 1. Using a month of clear-sky observations over the ocean, an initial minimum guess to the calibration slope is made. An iterative method is then used to achieve the optimal slope values for Ch. 1 with slope corrections adjusting for uncertainties in ocean reflectance, water vapor, ozone, and noise. Ch. 2 is then subsequently calibrated under the condition that the stratus cloud optical thickness in both channels must be the same (spectrally uniform in the visible) if their calibrations are correct [Iwabuchi, 2003].\n\nA more contemporary calibration method for AVHRR uses the on-orbit calibration capabilities of the VIS/IR channels of MODIS. Vermote and Saleous [2006] present a methodology that uses MODIS to characterize the BRDF of an invariant desert site. Due to differences in the spectral bands used for the instruments' channels, spectral translation equations were derived to accurately transfer the calibration accounting for these differences. Finally, the ratio of AVHRR observed to that modeled from the MODIS observation is used to determine the sensor degradation and adjust the calibration accordingly.\n\nMethods for extending the calibration and record continuity also make use of similar calibration activities [Heidinger et al., 2010].\n\nIn the discussion thus far, methods have been posed that can calibrate individual or are limited to a few AVHRR sensors. However, one major challenge from a climate point of view is the need for record continuity spanning 30+ years of three generations of AVHRR instruments as well as more contemporary sensors such as MODIS and VIIRS. Several artifacts may exist in the nominal AVHRR calibration, and even in updated calibrations, that cause a discontinuity in the long-term radiance record constructed from multiple satellites [Cao et al., 2008].\n\nBrest and Rossow [1992], and the updated methodology [Brest et al., 1997], put forth a robust method for calibration monitoring of individual sensors and normalization of all sensors to a common standard. The International Satellite Cloud Climatology Project (ISCCP) method begins with the detection of clouds and corrections for ozone, Rayleigh scatter, and seasonal variations in irradiance to produce surface reflectances. Monthly histograms of surface reflectance are then produced for various surface types, and various histogram limits are then applied as a filter to the original sensor observations and ultimately aggregated to produce a global, cloud free surface reflectance.\n\nAfter filtering, the global maps are segregated into monthly mean SURFACE, two bi-weekly SURFACE, and a mean TOTAL reflectance maps. The monthly mean SURFACE reflectance maps are used to detect long-term trends in calibration. The bi-weekly SURFACE maps are compared to each other and are used to detect short-term changes in calibration.\n\nFinally, the TOTAL maps are used to detect and assess bias in the processing methodology. The target histograms are also examined, as changes in mode reflectances and in population are likely the result of changes in calibration.\n\nLong-term record continuity is achieved by the normalization between two sensors. First, observations from the operational time period overlap of two sensors are processed. Next, the two global SURFACE maps are compared via a scatter plot. Additionally, observations are corrected for changes in solar zenith angle caused by orbital drift. Ultimately, a line is fit to determine the overall long-term drift in calibration, and, after a sensor is corrected for drift, normalization is performed on observations that occur during the same operational period [Brest et al., 1997].\n\nAnother recent method for the absolute calibration of the AHVRR record makes use of the contemporary MODIS sensor onboard NASA's TERRA and AQUA satellites. The MODIS instrument has high calibration accuracy and can track its own radiometric changes due to the inclusion of an onboard calibration system for the VIS/NIR spectral region [MCST]. The following method utilizes the high accuracy of MODIS to absolutely calibrate AVHRRs via simultaneous nadir overpasses (SNOs) of both MODIS/AVHRR and AVHRR/AVHRR satellite pairs as well as MODIS-characterized surface reflectances for a Libyan Desert target and Dome-C in Antarctica [Heidinger et al., 2010]. Ultimately, each individual calibration event available (MODIS/AVHRR SNO, Dome C, Libyan Desert, or AVHRR/AVHRR SNO) is used to provide a calibration slope time series for a given AVHRR sensor. Heidinger et al. [2010] use a second-order polynomial from a least-squares fit to determine the time series.\n\nThe first step involves using a radiative transfer model that will convert observed MODIS scenes into those that a perfectly calibrated AVHRR would see. For MODIS/AVHRR SNO occurrences, it was determined that the ratio of AVHRR to MODIS radiances in both Ch1 and Ch2 are modeled well by a second-order polynomial of the radio of MODIS reflectances in channels 17 and 18. Channels 17 and 18 are located in a spectral region (0.94mm) sensitive to atmospheric water vapor, a quantity that affects the accurate calibration of AVHRR Ch. 2. Using the Ch17 to Ch 18 ratio, an accurate guess at the total precipitable water (TPW) is obtained to further increase the accuracy of MODIS to AVHRR SNO calibrations. The Libyan Desert and Dome-C calibration sites are used when MODIS/AVHRR SNOs do not occur. Here, the AVHRR to MODIS ratio of reflectances is modeled as a third-order polynomial using the natural logarithm of TWP from the NCEP reanalysis. Using these two methods, monthly calibration slopes are generated with a linear fit forced through the origin of the adjusted MODIS reflectances versus AVHRR counts.\n\nTo extend the MODIS reference back for AVHRRs prior to the MODIS era (pre-2000), Heidinger et al. [2010] use the stable Earth targets of Dome C in Antarctica and the Libyan Desert. MODIS mean nadir reflectances over the target are determined and are plotted versus the solar zenith angle. The counts for AVHRR observations at a given solar zenith angle and corresponding MODIS reflectance, corrected for TWP, are then used to determine what AVHRR value would be provided it had the MODIS calibration. The calibration slope is now calculated.\n\nOne final method used by Heidinger et al. [2010] for extending the MODIS calibration back to AVHRRs that operated outside of the MODIS era is through direct AVHRR/AVHRR SNOs. Here, the counts from AVHRRs are plotted and a regression forced through the origin calculated. This regression is used to transfer the accurate calibration of one AVHRRs reflectances to the counts of an un-calibrated AVHRR and produce appropriate calibration slopes. These AVHRR/AVHRR SNOs do not provide an absolute calibration point themselves; rather they act as anchors for the relative calibration between AVHRRs that can be used to transfer the ultimate MODIS calibration.\n\nOperational experience with the MODIS sensor onboard NASA's Terra and Aqua led to the development of AVHRR's follow-on, VIIRS. VIIRS is currently operating on board the NPOESS Preparatory Project and NOAA-20 satellites.\n\nBrest, C.L. and W.B. Rossow. 1992. Radiometric calibration and monitoring of NOAA AVHRR data for ISCCP. International Journal of Remote Sensing. Vol. 13. pp. 235–273.\n\nBrest, C.L. et al. 1997. Update of Radiance Calibrations for ISCCP. Journal of Atmospheric and Oceanic Technology. Vol 14. pp. 1091–1109.\n\nCao, C. et al. 2008. Assessing the consistency of AVHRR and MODIS L1B reflectance for generating Fundamental Climate Data Records. Journal of Geophysical Research. Vol. 113. D09114. doi: 10.1029/2007JD009363.\n\nHalthore, R. et al. 2008. Role of Aerosol Absorption in Satellite Sensor Calibration. IEEE Geoscience and Remote Sensing Letters. Vol. 5. pp. 157–161.\n\nHeidinger, A. K. et al. 2002. Using Moderate Resolution Imaging Spectrometer (MODIS) to calibrate Advanced Very High Resolution Radiometer reflectance channels. Journal of Geophysical Research. Vol. 107. doi: 10.1029/2001JD002035.\n\nHeidinger, A.K. et al. 2010. Deriving an inter-sensor consistent calibration for the AVHRR solar reflectance data record. International Journal of Remote Sensing. Vol. 31. pp. 6493–6517.\n\nIwabuchi, H. 2003. Calibration of the visible and near-infrared channels of NOAA-11 and NOAA-14 AVHRRs by using reflections from molecular atmosphere and stratus cloud. International Journal of Remote Sensing. Vol. 24. pp. 5367–5378.\n\nLoeb, N.G. 1997. In-flight calibration of NOAA AVHRR visible and near-IR bands over Greenland and Antarctica. International Journal of Remote Sensing. Vol. 18. pp. 477–490.\n\nMCST. MODIS Level 1B Algorithm Theoretical Basis Document, Version 3. Goddard Space Flight Center. Greenbelt, MD. December 2005.\n\nMolling, C.C. et al. 2010. Calibrations for AVHRR channels 1 and 2: review and path towards consensus. International Journal of Remote Sensing. Vol. 31. pp. 6519–6540.\n\nNOAA KLM User’s Guide with NOAA-N, -N’ Supplement. NOAA NESDIS NCDC. Asheville, NC. February 2009.\n\nRao, C.R.N. and J. Chen. 1995. Inter-satellite calibration linkages for the visible and near-infrared channels of the Advanced Very High Resolution Radiometer on the NOAA-7, −9, and −11 spacecraft. International Journal of Remote Sensing. Vol. 16. pp. 1931–1942.\n\nRao, C.R.N. and J. Chen. 1999. Revised post-launch calibration of the visible and near-infrared channels of the Advanced Very High Resolution Radiometer on the NOAA-14 spacecraft. International Journal of Remote Sensing. Vol. 20. pp. 3485–3491.\n\nSmith, G.R. et al. 1988. Calibration of the Solar Channels of the NOAA-9 AVHRR Using High Altitude Aircraft Measurements. Journal of Atmospheric and Oceanic Technology. Vol. 5. pp. 631–639.\n\nVermote, E.F. and N.Z. Saleous. 2006. Calibration of NOAA16 AVHRR over a desert site using MODIS data. Remote Sensing of Environment. Vol. 105. pp. 214–220.\n\nXiong, X. et al. 2010. On-Orbit Calibration and Performance of Aqua MODIS Reflective Solar Bands. IEEE Transactions on Geoscience and Remote Sensing. Vol 48. pp. 535–546.\n\n"}
{"id": "1965327", "url": "https://en.wikipedia.org/wiki?curid=1965327", "title": "Alberger process", "text": "Alberger process\n\nThe Alberger process is a method of producing salt.\n\nIt involves mechanical evaporation and uses an open evaporating pan and steam energy. This results in a three-dimensional \"cup-shaped\" flake salt, which has low bulk density, high solubility and good adhesiveness. The extremely low bulk density makes the salt highly prized in the fast-food industry due to its lower sodium content and stronger flavor for a given volume.\n\nCargill operates a plant in St. Clair, Michigan that is the only plant in the United States that makes such salt.\n\nThe method was patented by Charles L. Weil on June 8, 1915.\n"}
{"id": "25134863", "url": "https://en.wikipedia.org/wiki?curid=25134863", "title": "Auto-text", "text": "Auto-text\n\nAuto-text is a portion of a text preexisting in the computer memory, available as a supplement to newly composed documents, and suggested to the document author by software. A block of Auto-text can contain a few letters, words, sentences or paragraphs. It can be chosen by the document author via menu or be offered automatically after typing specific words or letters (word prediction or text prediction), or be added to the document automatically after typing specific words or letters (word / text completion).\n\nAuto-text saves the time of typists who type many similar documents, or serves as an assistive technology for aiding persons with disabilities. These disabilities may be upper-limb disabilities that slow down movement, or produce pain or fatigue, as well as spelling disorders (e.g. dysgraphia). Persons with speech disabilities who type on augmentative and alternative communication devices may also benefit from Auto-text, since it can speed up their communication.\n\nExamples of software that offer Auto-text:\n"}
{"id": "26466761", "url": "https://en.wikipedia.org/wiki?curid=26466761", "title": "Auzentech", "text": "Auzentech\n\nAuzentech was a Korean computer hardware manufacturer that specialized in high-definition audio equipment and in particular PC sound cards.\n\nAuzentech has its origins in March 2005, when under the company name HDA (HiTeC Digital Audio), the company launched the X-Mystique 7.1, the first consumer add-in sound card to feature Dolby Digital Live.\nInitially only a manufacturer, HDA's products were commercialized worldwide by a network of local distributors, including BlueGears as their vendor in the United States.\n\nIn 2006 the company took distribution into their own hands, ceasing relations with BlueGears, and subsequently changing their brand name to Auzen (a name which originates from \"Audio\" and \"Zen\") and their company name to Auzentech.\n\nSince that time the company continued to incorporate new sound cards into their lineup in an effort to compete in a market dominated by Creative Labs. Auzentech sought to provide customers with features not present in Creative's sound cards at that time, such as real time multi-channel audio encoding and built-in TOSLINK connections. These features enable users to have multichannel realtime audio (like that originating in PC games) over a single digital line, instead of the previously unavoidable three analog lines running from the PC to the speakers. Also present in all Auzentech sound cards are user-replaceable OPAMPs, which offer the possibility to further improve the out-of-the-box quality of analog outputs.\n\nEventually expansion led Auzentech to broaden their range of products to include items such as speakers, microphones and PC cases among others.\n\nAuzentech has recently switched from the C-Media audio processors used in their earlier cards to the X-Fi audio DSP, licensed from their competitor, Creative Labs.\n\nSince late 2013, Auzentech's official web site has been directing to a park page, and their technical support department ceased to provide any service.\n\n\n\n\n\n"}
{"id": "14224240", "url": "https://en.wikipedia.org/wiki?curid=14224240", "title": "Clube do Hardware", "text": "Clube do Hardware\n\nBased in Brazil, Clube do Hardware (meaning \"Hardware Club\" in Brazilian Portuguese) is one of the largest websites about computers in South America and also one of the biggest in the world. According to Alexa, as of 2018, Clube do Hardware is the 185th most accessed website in Brazil.\n\nClube do Hardware publishes tutorials, articles, reviews, and news about computer hardware and has a very active forum where users can discuss technology topics.\n\nIt has a very engaged community, with the largest Brazilian team at the Folding@Home project.\n\nIt was created in 1996 by the Brazilian PC hardware expert Gabriel Torres, initially as a personal webpage on Geocities with the title \"Hardware by Gabriel Torres\" as a portfolio of his work. In 1997 the author registered the domain gabrieltorres.com with the website, changing its name to \"Hardware Site.\" The final name, Clube do Hardware, was adopted in 1999.\n"}
{"id": "15440402", "url": "https://en.wikipedia.org/wiki?curid=15440402", "title": "DESG", "text": "DESG\n\nThe Defence Engineering and Science Group (DESG) is a community of 9,000 engineers and scientists working within the United Kingdom Ministry of Defence Civil Service to equip and support the UK Armed Forces with military hardware.\n\n\nThere are 3 main methods of recruiting for DESG,\n\n \n"}
{"id": "19047741", "url": "https://en.wikipedia.org/wiki?curid=19047741", "title": "Dühring's rule", "text": "Dühring's rule\n\nDühring's rule states that a linear relationship exists between the temperatures at which two solutions exert the same vapour pressure. The rule is often used to compare a pure liquid and a solution at a given concentration.\n\nDühring's plot is a graphical representation of such a relationship, typically with the pure liquid's boiling point along the x-axis and the mixture's boiling point along the y-axis; each line of the graph represents a constant concentration.\n\n"}
{"id": "19409654", "url": "https://en.wikipedia.org/wiki?curid=19409654", "title": "Enhanced avionics system", "text": "Enhanced avionics system\n\nThe enhanced avionics system (or EASy) is an integrated modular avionics suite and cockpit display system used on Dassault Falcon business jets since Falcon 900EX, and later used in other newer Falcon aircraft such as Falcon 2000EX and Falcon 7X.\n\nEASy has been jointly developed by Dassault and Honeywell, and is based on Honeywell Primus Epic.\n\nDassault Aviation started to develop the EASy flight deck concept in the mid-1990s with a goal to have a much better integration of aircraft systems such as FMS.\n\nEASy was first integrated and certificated on Falcon 900EX. The first EASy equipped 900EX was delivered in December 2003. Honeywell Primus Epic base of EASy was then integrated on other business jets and helicopters.\n\nEASy was certified on the Falcon 2000EX in June 2004 with deliveries starting shortly after. Falcon 7X was developed from the ground-up with EASy avionics.\n\nOn October 2008, Dassault announced the launch of EASy phase II program at the annual NBAA meeting in Orlando. EASy phase II include several enhancements to EASy, such as:\n\nEASy Phase II was certified on Falcon 900LX on June 2011 and on Falcon 7X on May 2013\n\nEASy architecture is based on Integrated Modular Avionics. The processing modules are called \"MAU\" (Modular Avionics Units). The core Operating System of EASy is provided by DDCI.\n\n\n"}
{"id": "53145871", "url": "https://en.wikipedia.org/wiki?curid=53145871", "title": "Felix Meyer (industrialist)", "text": "Felix Meyer (industrialist)\n\nFelix Meyer came from a liberal Jewish family with roots traceable to sixteenth-century Westphalia. The brothers Elias and Moses (later Moritz) Meyer established themselves at Aachen, founding a successful textile factory. Moritz Meyer’s son, Eduard, joined the firm, married Esther Pauline Salomon in 1874 and a daughter, Else, was born the same year, followed by five other children, of whom Felix, born in 1875, was the eldest, the others being Georg, Dora, Meta and Erna.\n\nEduard Meyer’s family lived well, in a sixteen-room house, with servants, a large garden and stables. While they were not religiously fervent, they did not convert to Christianity. In later reminiscences, Felix spoke of a happy, even boisterous, childhood, though the upbringing for the boys in the family was severe, involving regular corporal punishment, though he himself proved to be an indulgent father. In particular, he left his daughters free to choose their husbands. A rebel at school, he followed his own father to the extent of undertaking several technical apprenticeships as a dyer, spinner and weaver. He also took jobs in several different companies in Germany, but also in England, before joining his father’s then flagging business at the age of 22.\n\nThe once flourishing firm had not found in Felix’s father an entrepreneural talent to equal the firm’s founder, and as Felix put it, ‘...the money disappeared rapidly since nobody knew how to earn it, but only how to spend it’. Felix Meyer rose to the occasion with innovation. For one thing he invented a twin loom, but the sale of the patent was insufficient to relaunch the factory’s fortunes and in 1905 he convinced his father to sell the mill, taking on the remaining debts himself. In 1906, Felix married Marguerite Darmstaedter, a Belgian national, and they set up home in the Kurbrunnenstrasse, in Aachen. Though the dwelling was modest, it was near the Cologne-Liège railway and there was a shed that Felix used for developing his inventions, which were many and varied, including forays into mechanics, chemical processes and even medical appliances. While the definitive number of patents registered under various names is uncertain, it probably reached several hundred, including new designs of loom, processes for manufacturing multi-coloured yarns or fabrics. Meyer worked quite a lot exploiting the possibilities of cellulose, with applications in the form of artificial limbs, for which he also constructed new joints. In the 1920s and 1930s, Meyer was occupied with the enhanced manufacture of glass tubes, ampoules, etc. and for filling and sealing these. Already in 1906, the newlyweds could evidently make a living from the success of the inventions. The firm Rotawerke was founded in 1909 and thereafter Meyer heeded the earlier business decline of his family in that he did not draw a salary, but simply fed his own pension fund. A daughter, Kläre (later known as Claire), was born in 1907 and another, Margot, in 1909.\n\nThe united German Empire proclaimed in the Hall of Mirrors at Versailles in the aftermath of the Franco-Prussian War, followed its destiny, especially during the reign of William II, towards becoming a vast militaristic-industrial complex and simultaneously largely losing control of its own foreign policy. When the summer of 1914 saw the outbreak of the Great War, this was hailed with enthusiasm by the patriotic German public, and there was no doubt that the course of events would echo those of the rapid victory of 1870. Felix Meyer was a fervent patriot whose spirit was undaunted by defeat or postwar conditions. The debts from his father’s time were finally paid off, and the family and its business prospered. Rotawerke had branches in Belgium and France and business ties to Britain. Meyer was wealthy enough to support young artists.\n\nMeyer remained a German patriot, and refused like other assimilated Jews to believe that the rise to power of the Nazi party constituted a danger. In Meyer’s case, he even returned to Germany in 1938 from a journey to the United States with his wife. That same year, however, events caught up with him, especially after the so-called Kristallnacht of 9–10 November, during which he spent some hours under arrest. This must certainly have been a great shock for Meyer and his wife. He had taken the precaution of carrying a capsule with poison on him and in a letter to his daughters he asked them to show understanding if he or his wife should decide to commit suicide. By the end of the year Meyer had been forced out of the firm under the so-called 'Arianization' process, and its management had been reluctantly taken over for the greater part by his son-in-law John Hennig, husband since 1933 of Meyer’s daughter Kläre, and with her a convert to Catholicism, in Hennig’s case from Lutheranism.\n\nIn February 1939 Meyer was allowed to emigrate legally to Belgium, being married to a Belgian woman, and with the complicity of officials he knew. However, already in May Germany invaded the Netherlands and Belgium and the couple failed in an attempt to escape to France. They were now stranded as German Jews in German-occupied Belgium, a situation which lasted till the liberation by the Allies in 1944.\n\nLater that same year, in October 1939, Meyer's son-in-law Hennig emigrated legally to the Irish Free State to prepare a home there for his wife and family, who joined him a month later. Despite the separation and the many difficulties of these years, Felix Meyer and his wife Marguerite maintained a strongly affectionate relationship with their daughter and her family.\n\nMeyer showed remarkable strength of character and a courage that almost defies belief. His exploits included complaining in person to the Gestapo about illegal detention of Jews. In that strange mental world of ice-cold cruelty and pseudo-bureaucratic meticulousness, Meyer succeeded in having well over a hundred Belgian and foreign Jews released one by one, arguing in detail their cases. He also alleviated the conditions of others, sometimes adducing the rights implied by military decorations awarded prior to the advent of Nazism. A man of the world, Meyer exploited the rivalry between committed Nazis and other military and civilian personnel and perhaps, too, the fear that some of his interlocutors had of facing legal retribution after the war. Meyer’s initiatives solely of benefit to small numbers of people, since he arranged shelter and medical care for hundreds.\n\nAs to Felix Meyer’s humanitarian activities in wartime Belgium, many testimonies are conserved at Jerusalem’s Yad Vashem memorial site.\n\nThe end of hostilities in Europe brought little immediate alleviation in general conditions, though Meyer fought for his humanitarian projects and to retrieve possession of his family firm and eventually met with success, though the struggle contributed to his early death in Brussels on 14 April 1950. He is buried in the Alter Jüdischer Friedhof (Old Jewish Cemetery) Aachen. With him is buried his wife, Esther, who died in 1966.\n\nSome years passed before in 1956 the Hennigs made the decision to return to the Continent, largely for the sake of their daughters’ future, but also to allow Hennig to take control of the Meyer business, now restored to family ownership. A plant of the firm, known as Rota, had been established at Säckingen in Germany, on the southern edge of the Black Forest area. John Hennig took on its management, but the events of the Nazi period were such that the family could not bring itself to refound their family life on German soil. Instead, they took up residence in nearby Basel, Switzerland, Hennig commuting from there over the border daily to manage the firm. The work was not of the kind that Hennig relished, but it ensured the family’s financial stability and was in any case seen by him as a duty to the memory of his revered father-in-law. The firm later became a subsidiary of the Japanese Yokogawa group.\n\n"}
{"id": "21954995", "url": "https://en.wikipedia.org/wiki?curid=21954995", "title": "Financial innovation", "text": "Financial innovation\n\nFinancial innovation is the act of creating new financial instruments as well as new financial technologies, institutions, and markets. Recent financial innovations include hedge funds, private equity, weather derivatives, retail-structured products, exchange-traded funds, multi-family offices, and Islamic bonds (Sukuk). The shadow banking system has spawned an array of financial innovations including mortgage-backed securities products and collateralized debt obligations (CDOs).\n\nThere are three categories of innovation—institutional, product, and process. Institutional innovations relate to the creation of new types of financial firms such as specialist credit card firms like MBNA, discount broking firms such as Charles Schwab, and internet banks. Product innovation relates to new products such as derivatives, securitized assets, and foreign currency mortgages. Process innovations relate to new ways of doing financial business, including online banking and telephone banking.\n\nEconomic theory has much to say about what types of securities should exist, and why some may not exist (why some markets should be \"incomplete\") but little to say about why new types of securities should come into existence.\n\nOne interpretation of the Modigliani-Miller theorem is that taxes and regulation are the only reasons for investors to care what kinds of securities firms issue, whether debt, equity, or something else. The theorem states that the structure of a firm's liabilities should have no bearing on its net worth (absent taxes). The securities may trade at different prices depending on their composition, but they must ultimately add up to the same value.\n\nFurthermore, there should be little demand for specific types of securities. The capital asset pricing model, first developed by Jack L. Treynor and William F. Sharpe, suggests that investors should fully diversify and their portfolios should be a mixture of the \"market\" and a risk-free investment. Investors with different risk/return goals can use leverage to increase the ratio of the market return to the risk-free return in their portfolios. However, Richard Roll argued that this model was incorrect, because investors cannot invest in the entire market. This implies there should be demand for instruments that open up new types of investment opportunities (since this gets investors closer to being able to buy the entire market), but not for instruments that merely repackage existing risks (since investors already have as much exposure to those risks in their portfolio).\n\nIf the world existed as the Arrow-Debreu model posits, then there would be no need for financial innovation. The model assumes that investors are able to purchase securities that pay off if and only if a certain state of the world occurs. Investors can then combine these securities to create portfolios that have whatever payoff they desire. The fundamental theorem of finance states that the price of assembling such a portfolio will be equal to its expected value under the appropriate risk-neutral measure.\n\nTufano (2003) and Duffie and Rahi (1995) provide useful reviews of the literature.\n\nThe extensive literature on principal–agent problems, adverse selection, and information asymmetry points to why investors might prefer some types of securities, such as debt, over others like equity. Myers and Majluf (1984) develop an adverse selection model of equity issuance, in which firms (which are trying to maximize profits for existing shareholders) issue equity only if they are desperate. This was an early article in the pecking order literature, which states that firms prefer to finance investments out of retained earnings first, then debt, and finally equity, because investors are reluctant to trust any firm that needs to issue equity.\n\nDuffie and Rahi also devote a considerable section to examining the utility and efficiency implications of financial innovation. This is also the topic of many of the papers in the special edition of the \"Journal of Economic Theory\" in which theirs is the lead article. The usefulness of spanning the market appears to be limited (or, equivalently, the disutility of incomplete markets is not great).\n\nAllen and Gale (1988) is one of the first papers to endogenize security issuance contingent on financial regulation—specifically, bans on short sales. In these circumstances, they find that the traditional split of cash flows between debt and equity is not optimal, and that state-contingent securities are preferred. Ross (1989) develops a model in which new financial products must overcome marketing and distribution costs. Persons and Warther (1997) studied booms and busts associated with financial innovation.\n\nThe fixed costs of creating liquid markets for new financial instruments appears to be considerable. Black and Scholes (1974) describe some of the difficulties they encountered when trying to market the forerunners to modern index funds. These included regulatory problems, marketing costs, taxes, and fixed costs of management, personnel, and trading. Shiller (2008) describes some of the frustrations involved with creating a market for house price futures.\n\nSome types of financial instrument became prominent after macroeconomic conditions forced investors to be more aware of the need to hedge certain types of risk.\n\n\nFutures, options, and many other types of derivatives have been around for centuries: the Japanese rice futures market started trading around 1730. However, recent decades have seen an explosion use of derivatives and mathematically complicated securitization techniques. From a sociological point of view, some economists argue that mathematical formulas actually change the way that economic agents use and price assets. Economists, rather than acting as a camera taking an objective picture of the way the world works, actively change behavior by providing formulas that let dispersed agents agree on prices for new assets.\n\nMiller (1986) placed great emphasis on the role of taxes and government regulation in stimulating financial innovation. The Modigliani-Miller theorem explicitly considered taxes as a reason to prefer one type of security over another, despite that corporations and investors should be indifferent to capital structure in a fractionless world.\n\nThe development of checking accounts at U.S. banks was in order to avoid punitive taxes on state bank notes that were part of the National Banking Act.\n\nSome investors use total return swaps to convert dividends into capital gains, which are taxed at a lower rate.\n\nMany times, regulators have explicitly discouraged or outlawed trading in certain types of financial securities. In the United States, gambling is mostly illegal, and it can be difficult to tell whether financial contracts are illegal gambling instruments or legitimate tools for investment and risk-sharing. The Commodity Futures Trading Commission (CFTC) is in charge of making this determination. The difficulty that the Chicago Board of Trade faced in attempting to trade futures on stocks and stock indexes is described in Melamed (1996).\n\nIn the United States, Regulation Q drove several types of financial innovation to get around its interest rate ceilings, including eurodollars and NOW accounts.\n\nSome types of financial innovation are driven by improvements in computer and telecommunication technology. For example, Paul Volcker suggested that for most people, the creation of the ATM was a greater financial innovation than asset-backed securitization. Other types of financial innovation affecting the payments system include credit and debit cards and online payment systems like PayPal.\n\nThese types of innovations are notable because they reduce transaction costs. Households need to keep lower cash balances—if the economy exhibits cash-in-advance constraints then these kinds of financial innovations can contribute to greater efficiency. One study of Italian households' use of debit cards found that ownership of an ATM card resulted in benefits worth €17 annually.\n\nThese types of innovations may also affect monetary policy by reducing real household balances. Especially with the increased popularity of online banking, households are able to keep greater percentages of their wealth in non-cash instruments. In a special edition of \"International Finance\" devoted to the interaction of e-commerce and central banking, Goodhart (2000) and Woodford (2000) express confidence in the ability of a central bank to maintain its policy goals by affecting the short-term interest rate even if electronic money has eliminated the demand for central bank liabilities, while Friedman (2000) is less sanguine.\n\nA 2016 PwC report pointed to the \"accelerating pace of technological change\" as the \"most creative force—and also the most destructive—in the financial services ecosystem\".\n\nSome economists argue that financial innovation has little to no productivity benefit: Paul Volcker stated that \"there is little correlation between sophistication of a banking system and productivity growth\", that there is no \"neutral evidence that financial innovation has led to economic growth\", and that financial innovation was a cause of the financial crisis of 2007–2010, while Paul Krugman states that \"the rapid growth in finance since 1980 has largely been a matter of rent-seeking, rather than true productivity\".\n\n\n"}
{"id": "1071636", "url": "https://en.wikipedia.org/wiki?curid=1071636", "title": "Fuel tank", "text": "Fuel tank\n\nA fuel tank (or petrol tank) is a safe container for flammable fluids. Though any storage tank for fuel may be so called, the term is typically applied to part of an engine system in which the fuel is stored and propelled (fuel pump) or released (pressurized gas) into an engine. Fuel tanks range in size and complexity from the small plastic tank of a butane lighter to the multi-chambered cryogenic Space Shuttle external tank.\n\nTypically, a fuel tank must allow or provide the following:\n\n\nPlastic (high-density polyethylene HDPE) as a fuel tank material of construction, while functionally viable in the short term, has a long term potential to become saturated as fuels such as diesel and gasoline permeate the HDPE material.\n\nConsidering the inertia and kinetic energy of fuel in a plastic tank being transported by a vehicle, environmental stress cracking is a definite potential. The flammability of fuel makes stress cracking a possible cause of catastrophic failure. Emergencies aside, HDPE plastic is suitable for short term storage of diesel and gasoline. In the U.S., Underwriters Laboratories approved (UL 142) tanks would be a minimum design consideration.\n\nWhile most tanks are manufactured, some fuel tanks are still fabricated by metal craftsmen or hand-made in the case of bladder-style tanks. These include custom and restoration tanks for automotive, aircraft, motorcycles, boats and even tractors. Construction of fuel tanks follows a series of specific steps. The craftsman generally creates a mockup to determine the accurate size and shape of the tank, usually out of foam board. Next, design issues that affect the structure of the tank are\naddressed - such as where the outlet, drain, fluid level indicator, seams, and baffles go. Then the craftsmen must determine the thickness, temper and alloy of the sheet he will use to make the tank. After the sheet is cut to the shapes needed, various pieces are bent to create the basic shell and/or ends and baffles for the tank. Many fuel tanks' baffles (particularly in aircraft and racecars) contain lightening holes. These flanged holes serve two purposes, they reduce the weight of the tank while adding strength to the baffles. Toward the end of construction, openings are added for the filler neck, fuel pickup, drain, and fuel-level sending unit. Sometimes these holes are created on the flat shell, other times they are added at the end of the fabrication process. Baffles and ends can be riveted into place. The heads of the rivets are frequently brazed or soldered to prevent tank leaks. Ends can then be hemmed in and soldered, or flanged and brazed (and/or sealed with an epoxy-type sealant) or the ends can be flanged and then welded. Once the soldering, brazing or welding is complete, the fuel tank is leak-tested.\n\nThe maximum distance a combustion-engine powered car with a full tank can cover is the product of the tank capacity and its fuel efficiency (as in miles per gallon). While larger tanks increase the maximum distance, they also take up more space and (especially when full) add to the total weight, requiring higher fuel consumption for the same performance. Fuel-tank capacity is therefore the result of a trade-off in design considerations. For most compact cars, the capacity is in the range ; the original model Tata Nano is exceptional with its fuel tank. SUVs and trucks tend to have considerably larger fuel tanks.\n\nFor each new vehicle a specific fuel system is developed, to optimize the use of available space. Moreover, for one car model, different fuel system architectures are developed, depending on the type of the car, the type of fuel (gasoline or diesel), nozzle models, and region.\n\nTwo technologies are used to make fuel tanks for automobiles:\n\n\nModern cars often feature remote opening of the fuel tank fuel filler flap using an electric motor or cable release. For both convenience and security, many modern fuel tanks cannot be opened by hand or otherwise from the outside of the car.\n\nSometimes called the reserve tank is a secondary fuel tank (in many cars/bikes it contains around 15% of the capacity of the primary tank) these are more commonly found on bikes, older cars (some without a fuel gauge) and vehicles designed for long distance or special use. A light on the instrument panel indicates when the fuel level dips below a certain point in the tank. There is no current standard, although some efforts are made to collect this data for all automobiles.\n\nIn vehicles modified for endurance the primary tank (the one that comes with the car) is made into a reserve tank and a larger one installed. Some 4x4 vehicles can be fitted with a secondary (or sub-tank) by the dealership.\n\nThe \"ship in a bottle\" fuel tank is a manufacturing design developed by TI Automotive in Rastatt, Germany wherein all fuel delivery components including the pump, control electronics and most hosing are encased within a blow-molded plastic fuel tank, and named after the traditional ship-in-a-bottle mechanical puzzle. The technique was developed to reduce fuel vapor emissions in response to Partial Zero-Emission Vehicle (PZEV) requirements. The first application was for the 2005 Ford GT.\n\nA \"racing fuel cell\" has a rigid outer shell and flexible inner lining to minimize the potential for punctures in the event of a collision or other mishap resulting in serious damage to the vehicle. It is filled with an open-cell foam core to prevent explosion of vapor in the empty portion of the tank and to minimize sloshing of fuel during competition that may unbalance the vehicle or cause inadequate fuel delivery to the motor (fuel starvation). The designation \"racing\" is often omitted due to familiarity and because this type of gas tank is also used on street vehicles. The omission can lead to confusion with other types of fuel cells. See Fuel cell (disambiguation).\n\nFor safety considerations, in modern cars the fuel tank is usually located ahead of the rear axle, out of the crumple zones of the car.\n\nAutomobiles such as the Ford Pinto or the models that still use the Ford Panther platform (Ford Crown Victoria, Lincoln Town Car, and Mercury Grand Marquis) are notorious for having the fuel tank behind the rear axle. Since 1980 new Ford models corrected this problem and had the fuel tank in front of the rear axle.\n\nGeneral Motors 1973-1987 C/K pickup trucks have the fuel tank located outside the frame. According to the Center for Auto Safety this creates a fire hazard. In automotive applications, improper placement of the fuel tank has led to increased probability of fire in collisions. Circa 1990, General Motors faced over a hundred product liability lawsuits related to fires allegedly caused by GM's decision to place the fuel tanks in its pickup trucks outside the protection of the vehicle's frame. In 1993, reportage on this matter for NBC News created a scandal over vehicles rigged to catch fire for the television camera.\n\nFord's Pinto also sparked controversy for putting the fuel tank in a poorly reinforced area which can cause deadly fires and explosions if the car got into a rear-end collision, costing Ford US$125 million.\n\nLikewise for safety reasons, the filler could no longer be in the middle back of the car in the crumple zone and thus had to be on the side of the car. Which side is a series of trade-offs: driver's side is easier to access, and mechanically simpler for gas cap locks; passenger side is safer (away from passing traffic in roadside fill-ups). Asymmetric sliding doors may also dictate placement and some minivan doors will collide with a fillup in progress.\n\nAircraft typically use three types of fuel tanks: integral, rigid removable, and bladder.\n\n\nFuel tanks have been implicated in aviation disasters, being the cause of the accident or worsening it (fuel tank explosion). For example:\n\n\nIn some areas, an aircraft's fuel tank is also referred to as an \"aircraft fuel cell\".\n\nWater supply systems can have primary or backup power supplied by diesel-fueled generators fed by a small \"day tank\" and a much larger bulk storage fuel tank.\n\nProper design and construction of a fuel tank play a major role in the safety of the system of which the tank is a part. In most cases intact fuel tanks are very safe, as the tank is full of fuel vapour/air mixture that is well above the flammability limits, and thus cannot burn even if an ignition source were present (which is rare).\n\nBunded oil tanks are used for safely storing domestic heating oil and other hazardous materials. Bunding is often required by insurance companies, rather than single skinned oil storage tanks.\n\nSeveral systems, such as BattleJacket and rubber bladders, have been developed and deployed for use in protecting (from explosion caused by enemy fire) the fuel tanks of military vehicles in conflict zones.\n\n\n"}
{"id": "25719198", "url": "https://en.wikipedia.org/wiki?curid=25719198", "title": "Gas collecting tube", "text": "Gas collecting tube\n\nThe characterization gas collecting tube describes an oblong gas-tight container with one valve at either end. Usually such a container has a gauged volume, has a cylindrical shape and is made of glass. Gas collecting tubes are used for science-related purposes; for taking samples of gases.\n\nVersions\n\n\"Capacity\" of gas collecting tubes 150, 250, 350, 500 and 1.000 ml\n\nThe mass density of a gas can be measured by using a gas collecting tube, an analytical balance and an aspirator. The mass and volume of a displaced amount of gas are determined: At atmospheric pressure formula_1, the gas collecting tube is filled with the gas to be investigated and the overall mass formula_2 is measured. Then the aspirator sucks out much of the gas yielding a second overall mass formula_3 measurement. (The difference of masses represents the mass (formula_4) of the extracted amount of gas.) Finally, the nearly evacuated gas collecting tube is allowed to suck in an outgassed liquid, usually previously heated water, which is offered under atmospheric pressure formula_1. The gas collecting tube is weighed for a third and last time containing the liquid yielding the value formula_6. (The difference of masses of the nearly evacuated tube and the liquid-containing tube gives the mass (formula_7) of the sucked-in liquid, that took the place of the extracted amount of gas.) The given mass-density formula_8 of the liquid permits to calculate the displaced volume (formula_9). Thus giving mass formula_10 and volume formula_11 of the extracted amount of gas, consequently accessing its mass density (formula_12) under atmospheric pressure\n\nThe mass density of a fluid can be measured by using a gas collecting tube, an analytical balance and two other fluids of known mass densities, preferably a gas and a liquid (with mass densities formula_13, formula_8). Overview: First, mass measurements get the volume formula_11 and the evacuated mass formula_16 of the gas collecting tube; secondly, these two are used to measure and calculate the mass-density formula_17 of the investigated fluid.\n\nFill the gas collecting tube with one of those fluids of given mass density and measure the overall mass, do the same with the second one giving the two mass values formula_18, formula_19. Consequently for those two fluids, the definition of mass density can be rewritten:\nThese two equations with two unknowns formula_16 and formula_11 can be solved by using elementary algebra:\n\nNow fill the gas collecting tube with the fluid to be investigated. Measure the overall mass formula_2 to calculate the mass of the fluid inside the tube formula_27 yielding the desired mass density formula_12.\n\nIf the gas is a pure gaseous chemical substance (and not a mixture), with the mass density formula_12, then using the ideal gas law permits to calculate the molar mass formula_30 of the gaseous chemical substance: formula_31 (formula_32 represents the universal gas constant, formula_33 the absolute temperature at which the measurements took place)\n\n"}
{"id": "57800775", "url": "https://en.wikipedia.org/wiki?curid=57800775", "title": "Green data center", "text": "Green data center\n\nA green data center, or sustainable data center, is a type of server facility that utilizes energy efficiency technologies. These data centers do not contain obsolete systems such as inactive or underused servers, and instead take advantage of newer and more efficient technologies.\n\nWith the exponential growth and usage of the internet, power consummation in data centers has increased significantly. Due to the resulting environmental impact, rise in public awareness, higher cost of energy and legislative action, an increasing pressure has been placed on companies to follow a green policy. For these reasons, the creation of sustainable data centers has become essential in both an environmental and business sense.\n\nIn recent years, the use of high-performance computing techniques has increased, trading off energy consumption for obtaining increased performances. Industry estimates suggest that the data center sector is consuming around 3-5% of the world’s global energy.\nAccording to the latest AFCOM State of the Data Center survey, 70% of data center providers indicated that the power density (per rack) has significantly increased over 2013. That means that managers have been forced to find new ways to power their data centers, using renewable energy sources such as solar, geo-thermal, and wind. More efficient technologies were developed in order to decrease the power consumption in data centers.\n\nThere are several metrics invented to measure power efficiency of data centers. The power usage effectiveness (PUE) and Carbon Usage Effectiveness (CUE) are two highly used metrics created by The Green Grid (TGG), a global consortium to advancing energy efficiency in data centers.\n\nPUE was invented in 2007 and proposed new guidelines to measure energy usage in data centers.\n\nThis ratio tells us basically how much extra energy a data center needs to maintain the IT equipment for every watt delivered to IT equipment. The best PUE a data center can have is 1 which is the ideal situation with zero extra energy usage.\nAfter the introduction of PUE it is found in various studies that average PUE for the industry was between 2.5 and 3. In more recent studies it is observed that average PUE values fell to around 1.7 by using this framework. As a result, PUE started the shift of the data center industry towards energy efficiency.\n\nUntil now, the PUE is the most used metric for data centers to measure the energy efficiency. However, the reliability of PUE is still discussed by some sources. The main reason of this discussion is that the PUE can change significantly depending on the measurement time and time period.\n\nCarbon usage effectiveness (CUE) is another metric in order to measure the energy usage and sustainability. It is calculated by the following formula\n\nAnother way to express this formula is the product of the Carbon Dioxide Emission Factor (CEF) with the PUE, where CEF is the kg of <chem>CO2</chem> produced for each kilowatt-hour of electricity:\n\nCUE takes into account the carbon emission, so in some ways it may be a more accurate representation of the sustainability of a data center compared to PUE since some data centers can have low PUE scores but still have high carbon emission. Used with PUE, these metrics allow us to examine the energy efficiency in more detail.\n\n Data centers can be certified as green data centers. One of the popular certificats is the Leadership in Energy and Environmental Design (LEED), which is a green rating system for buildings. It is developed by US Green Building Council and has a set of rating systems for different categories. Depending on ratings data centers can get a Silver, Gold or Platinum certification. The Platinum certification is given to data centers with the highest level of environmentally responsible construction with efficient use of resources.\nData centers can be also certified by Energy Star which is a part of an initiative by the U.S. Environmental Protection Agency and the U.S. Department of Energy called the National Data Center Energy Efficiency Information Program. This program certifies buildings and consumer products for their energy efficiency. Only data centers that are in the top 25% energy performance range can receive an Energy Star certification.\n\nThere are several technologies to increase the efficiency and decrease the energy consumption in data centers.\n\nLow power servers are servers which are more energy efficient than current servers used in data centers. These servers use the technology of smartphone computing which tries to balance performance with energy consumption. The first low power servers were launched in 2012 by big IT providers like Dell and HP. When they are used correct, they can be much more efficient than current servers. The low power servers can have a big impact on the data centers because they can decrease the power consumption and capital and operating costs of cooling facilities.\n\nA modular data center is a portable data center which can be placed anywhere where data capacity is needed. In comparison to traditional data centers they are designed for rapid deployment, energy efficiency and high density. Due to the fact that these data centers are ready-made data center-in-a-box, this technology became very popular. For example, the modular data center “HP EcoPod” supports more than 4.000 data centers with a PUE rating of 1.05 in combination with free air cooling.\n\nFree air cooling systems uses outdoor air instead of using traditional data center Computer Room Air Conditioner (CRAC) units. Outdoor air still needs to be filtered and moisturized, however, with this method much less energy is required to cool the data center. Outdoor air temperature is an important issue here and the location of the data center plays a critical role to implement this technology.\n\nIn this method, the rows of the racks are aligned in a way that the back of the servers are facing each other. The created aisles are enclosed in order to capture the air. In hot aisle containment, the heat produced by the servers is trapped inside and pumped to the cooling units. In contrary in cold aisle containment, cold air is pumped to the enclosed aisles whereas the rest of the data center has higher degrees of heat. Both of these containment methods are more effective compared to traditional cooling technologies and can help to reduce the energy consumption and its impact. It is also shown that although it may be harder to implement, hot aisle containments are more effective compared to cold aisle containments.\n\nData centers use electric power and in return the release more than 98% of this electricity as heat energy. The main idea in this case is that produced waste heat can be actively reused. Thus, a data center becomes a district energy system, creating a closed-loop system that has no waste.\nThere are already some examples of data centers around the world which are turning waste heat into heat energy that can be used, such as:\n\nA certain level of humidity is necessary for data centers to work efficiently and prevent damages to devices and servers. There are several ways to produce vapor and ensure humidity. Ultrasonic humidification uses ultrasound frequency to create vapor. This system uses %90 less energy compared to conventional methods (such as resistance steam humidifiers).\n\nThis method uses the idea of heat decreasing by evaporation of water. Two main methods are used in order to realize this idea; usage of evaporation pads and high pressure spray systems. In evaporation pads, air is drawn through pads, which makes water evaporate and air cool. The second most used technique, High pressure spray systems, needs larger areas and consumes more energy because of the usage of pumps. Evaporative cooling is dependent on the geographical location and the season because they affect the moisture level in the air. In general, compared to traditional mechanical cooling systems, evaporative cooling uses significantly less electric.\n"}
{"id": "22847226", "url": "https://en.wikipedia.org/wiki?curid=22847226", "title": "I-net Crystal-Clear", "text": "I-net Crystal-Clear\n\ni-net Clear Reports (formerly known as i-net Crystal-Clear) is a Java-based cross-platform reporting application providing a report designer and a server component to create reports in numerous output formats like PDF, HTML, PS, RTF, XLS, TXT, CSV, SVG, XML, as well as being viewable in a Java applet or Swing component. Application programmers can integrate i-net Clear Reports using the public API which spans over 200 classes. Starting with release 11.0 i-net Clear Reports also supports the .NET programming language and offers a public API for further integration in other products.\n\nin Version 11.0 i-net Crystal-Clear was renamed to i-net Clear Reports. At this time it also gained .NET integration as a major feature.\n\ni-net Crystal-Clear was primarily designed to read Crystal Reports templates. It had to be capable of reading the RPT report format and producing a reasonable output. For exporting and saving reasons a new file format had to be created later on, enabling Crystal-Clear to save the API results back and make them editable by a designer.\n\nIn 2002 the first version of i-net DesignerXML, the report designer, was written using Java Swing. The editing concept is slightly different from some other designers, using a band-oriented report template format, meaning that reports are designed based on rows of data.\n\nThe development efforts has changed in more recent years to a full reporting platform approach, rather than the developer-only, framework-based one.\n\nTrue to its roots, i-net Crystal-Clear still has the ability to read and execute Crystal Reports report templates up until the latest versions of Crystal Reports.\n\nUnlike Crystal Reports, however, the i-net Crystal-Clear report file format has always been an open format. Until version 9.0 it was an XML format. Since version 9, the report file format is in a zip-based format similar to OpenDocument.\n\nBeing a Java application, i-net Clear Reports has the ability to run on a variety of platforms and environments. There is virtually no restriction concerning data sources that can be used, as long as there is a JDBC driver available for accessing the data. For non-JDBC data sources, there are how-to's to write simple mini-drivers. With the new .NET API it got even more flexible and allows integration into .NET based applications.\n\ni-net Clear Reports comes as a standalone server as well as a servlet which can be run on any Java EE application server such as Tomcat, Jetty, IIS, or Apache (via PHP).\n\nFeatures can be added using the public API, or by adding custom JavaBeans which can be added to reports. User defined functions (UDF) extend the formula features of the embedded formula calculation routines.\n\n"}
{"id": "48049760", "url": "https://en.wikipedia.org/wiki?curid=48049760", "title": "Ikranite", "text": "Ikranite\n\nIkranite is a member of the eudialyte group, named after the Shubinov Institute of Crystallography of the Russian Academy of Sciences. It is a cyclosilicate mineral that shows trigonal symmetry with the space group \"R\"3\"m\", and is often seen with a pseudo-hexagonal habit. Ikranite appears as translucent and ranges in color from yellow to a brownish yellow. This mineral ranks a 5 on Mohs Scale of Hardness, though it is considered brittle, exhibiting conchoidal fracture when broken.\n\nThe eudialyte group currently consists of 27 known minerals (see below), all considered rare, with the exception of eudialyte. The list below also includes one of around six unnamed (\"UM\") species listed by Mindat. This group is growing exponentially, with 17 members having been written about since 2000, and the chemical possibility of several thousand species that have yet to be discovered. Eudialyte group members are typically found as small crystals which have a complex crystal structure that is unique in that it consists of both 3- and 9-member SiO tetrahedra rings.\n\nIkranite separates itself from the other members of this group through both its physical and compositional properties. The most prominent of these characteristics is the absence of sodium in its structure, along with the replacement of divalent iron with the trivalent form. This replacement also causes the characteristic color change from the reddish color seen in eudialyte to the yellow brown of ikranite, which can be further examined in its IR spectrum.\n\nIkranite was first discovered on Mount Karnasurt (Kola Peninsula) in an agpaitic pegmatite, in the form of 1–2 cm grains. It is commonly associated with microcline, nepheline, lorenzenite, murmanite, lamprophyllite, and arfvedsonite. Tetranatrolite, and halloysite can also be found with it, though they occur at a later stage.\n\nThe crystal structure of ikranite can be described as a framework of three- and nine- member SiO tetrahedra rings, connected by Ca six-membered rings and Zn (Ti, or Nb) octahedra. Layers are constructed along the c axis as Si-Zr-Si-Ca. This repetition generates 12 layers, equal to ~30Å in size.\n\nDiffering types cations, anions, anionic groups, and water molecules fill any pockets within the framework. A defining feature is the location of the M(3) and M(4) vacancies in the nine-membered rings. These cavities may be occupied with Si in the M(3b) location, Zr in the M(4a), and Zr, Nb, or Ti in the (M4b), though the probability of occupancy is low. The M(2a) and M(2b) locations are also uniquely occupied in ikranite. The M(2a) vacancy is seen occupied by Fe octahedra. Typically holding a five-membered polyhedra with Fe, the M(2b) position is occupied by Na cations. Ikranite also holds a significant amount of water in the space between the rings where an Na molecule is usually found. A distinctive feature is the oxonium groups that can also be found occupying Na sites.\n\nIkranite's general formula thus becomes: (Na,HO)(Ca,Mn,REE)FeZr([ ],Zr)([ ],Si)SiO(O,OH)ClHO\n\n"}
{"id": "16431173", "url": "https://en.wikipedia.org/wiki?curid=16431173", "title": "Image Share", "text": "Image Share\n\nImage Share is a service for sharing images between users during a mobile phone call. It has been specified for use in a 3GPP-compliant cellular network by the GSM Association in the PRD IR.79 Image Share Interoperability Specification.\n\nAccording to the specification, \"The terminal interoperable Image Share service allows users to share Images between them over PS connection with ongoing CS call, thus enhancing and enriching end-users voice communication.\" \n\nAn Image Share session begins by end-users setting up a normal circuit switched (CS) voice call. After the voice call is set up, terminals perform a registration to an IMS core system with a packet switched (PS) connection. Then based on successful capability negotiation between the terminals, the end-user will be presented with an option in terminal UI offering the possibility of sharing one or several images. If this is selected, then these images are transferred between the Image Share software clients located in the mobile phones using the PS connection and the recipient is able to see the images. During this process the normal CS voice session has been ongoing continuously. \n\nImage Share can be seen as a kind of spin-off from the Video Share mobile phone service. Video Share is commercially launched for example by AT&T in USA, but Image Share is not yet available from any mobile operator/service provider. \n\n\nAccording to GSMA press release interoperability between different Image Share clients was successfully tested in a multi-vendor trial in May 2007, including interworking between multiple networks. \n\nNo mobile operator has launched Image Share so far (as of March 2008).\n\n\n"}
{"id": "26526465", "url": "https://en.wikipedia.org/wiki?curid=26526465", "title": "Joint Tactical Ground Station", "text": "Joint Tactical Ground Station\n\nThe Joint Tactical Ground Station (JTAGS) is the United States Army's element to United States Strategic Command's Theater Event System (TES). TES provides an integrated, in-theater, 24-hour overhead non-imaging infrared detection capability for processing and disseminating missile early warning, alerting, and cueing information data to combatant commanders and missile defense assets through the use of stereo processing of the Defense Support Program (DSP) satellite data.\n\nThe Joint Tactical Ground Station (JTAGS) is the Army’s primary system that provides space based integrated, in-theater missile warning. It provides continuous\nprocessing of overhead non-imaging infrared (ONIR) data that is directly down linked from the Defense Support Program (DSP) satellite constellation. This data is\nused to provide near real-time dissemination of warning, alerting and cueing information, to combatant commanders (CCDRs) and ballistic missile defense\nsystems (BMDS), on ballistic missile threats for the protection of military assets, civilian populations, and geopolitical centers. This dissemination is accomplished by\nusing existing communication networks such as the Integrated Broadcast Service (IBS) and Link 16.\n\n\"(U) Receive and process in-theater, direct down-linked data from OPIR (Overhead Persistent Infrared) Sensors in order to disseminate warning, alerting and cueing information on tactical ballistic missiles and other events of interest throughout the theater using existing communication networks.\"\n\nThe Joint Tactical Ground Station Theater Missile Warning system is organized under the 1st Space Company (JTAGS), with 4 forward-stationed Detachments in Europe, CENTCOM, Korea, and Japan. The 1st Space Company falls under the 1st Space Battalion, under the 1st Space Brigade, part of the US Army Space and Missile Defense Command\n\n\n"}
{"id": "332326", "url": "https://en.wikipedia.org/wiki?curid=332326", "title": "Kepone", "text": "Kepone\n\nKepone, also known as chlordecone, is an organochlorine compound and a colourless solid. This compound is an obsolete insecticide related to Mirex and DDT. Its use was so disastrous that it is now prohibited in the western world, but only after many millions of kilograms had been produced. Kepone is a known persistent organic pollutant (POP), classified among the \"dirty dozen\" and banned globally by the Stockholm Convention on Persistent Organic Pollutants as of 2011.\n\nThe LC50 (LC = lethal concentration) is 35 μg/ L for \"Etroplus maculatus\", 0.022–0.095 mg/kg for blue gill and trout. Kepone bioaccumulates in animals by factors up to a million-fold. Workers with repeated exposure suffer severe convulsions resulting from degradation of the synaptic junctions.\n\nKepone has been found to act as an agonist of the GPER (GPR30).\n\nIn the US, kepone was produced by Allied Signal Company and LifeSciences Product Company in Hopewell, Virginia. The improper handling and dumping of the substance into the nearby James River (U.S.) in the 1960s and 1970s drew national attention to its toxic effects on humans and wildlife. The product is similar to DDT and is a degradation product of Mirex. The history of Kepone incidents are reviewed in \"Who's Poisoning America?: Corporate Polluters and Their Victims in the Chemical Age\" (1982). In 2009, Kepone was included in the Stockholm Convention on persistent organic pollutants, which bans its production and use worldwide.\n\nDue to the pollution risks, many businesses and restaurants along the river suffered economic losses. In 1975 Governor Mills Godwin Jr. shut down the James River to fishing for 100 miles, from Richmond to the Chesapeake Bay. This ban remained in effect for 13 years, until efforts to clean up the river began to show results.\n\nThe French island of Martinique is heavily contaminated with kepone, following years of its unrestricted use on banana plantations. Despite a 1990 ban of the substance by France, the economically powerful planter community lobbied intensively to gain the power to continue using kepone until 1993. They had argued that no alternative pesticide was available, which has since been disputed. Similarly, the nearby island of Guadeloupe is also contaminated, but to a lesser extent. Since 2003, local authorities have restricted cultivation of crops because the soil has been seriously contaminated by kepone. Guadeloupe has one of the highest prostate cancer diagnosis rates in the world.\n\n\nKepone is made by dimerizing hexachlorocyclopentadiene and hydrolyzing to a ketone.\n\n"}
{"id": "31754683", "url": "https://en.wikipedia.org/wiki?curid=31754683", "title": "Knowland Group", "text": "Knowland Group\n\nKnowland is a web-based software company that provides business development products and services to the hospitality industry.\n\nKnowland traces its roots back to a small software engineering firm. This firm spent months developing an entirely new kind of hotel readerboard service, heavily reliant on web technology, for a client in the industry. When the client backed out, Knowland’s founders recognized the hospitality industry’s need for a higher standard of market intelligence. They decided to start their own hospitality field research service, and by the fall of 2004 an online readerboard service was launched.\n\nKnowland signed its first client, the Holiday Inn in Arlington, Va., in September 2004. During 2005, the company expanded to over 20 markets, serving a diverse clientele of single and multi-property clients. By the end of 2006, Knowland had over 400 clients in 60 markets and had tracked more than 200,000 events.\n\nIn 2007, Knowland introduced Insight, a tool that generates targeted sales leads with details on each prospect’s event booking history. The company also activated a 7,000 square foot call center to expand its infrastructure for customer support, in-depth contact and event research, and sales activities on behalf of customers. The Event Booking Center employs research and sales support professionals who specialize in cold calling and can become an extension of hotel clients’ sales staff.\n\nKnowland launched Target Net, a meetings management and sales force automation platform, in 2010. In addition to managing events from start to finish, Target Net also generated sales leads and opportunities. This product was discontinued in 2015.\n\nToday Knowland is a provider of intuitive business intelligence products for the hospitality industry and has more than 3,000 client hotels and 50,000 users globally.\n\nKnowland is a provider of business development solutions to the hospitality industry. Target Net, Insight, and Readers are cloud-based and accessible from any computer connected to the Internet.\n\nTarget Net is a salesforce automation and meetings management tool that allows hoteliers to manage their sales funnel from start to finish.\n\nKnowland no longer sells Target Net product, but continues to offer support for clients already using it.\n\nInsight is a search engine and sales lead-generation tool that gives hotel sales teams access to a database of groups that have held millions of meetings at hotels and conference centers.\n\nIn 2014, Knowland released Insight 3.0, an effort to enhance the Insight platform, which transformed the traditional search engine leads database into a customizable meeting intelligence tool for event professionals. Insight 3.0 included new features such as anonymous reviews of groups and their events, the ability to follow groups, properties, and people, and more detailed group profiles.\n\nAfter receiving many unfavorable reviews from clients regarding the changes to Insight for the 3.0 release, Knowland launched Insight 3.1 the following year (2015). Insight 3.1 was designed almost entirely around user feedback. This release leveraged the powerful, and once again enhanced, search capabilities of the tool, putting popular search features in the spotlight. Insight 3.1 made the focus of Knowland technology less about social media-influenced networking and more about the power of its raw, historical data on groups, planners, meetings, and contact information to help clients optimize their sales tactics.\n\nReaders is an online readerboard service that offers daily reporting and data on groups that hold meetings and events at hotels' competitor properties. Readers combines an online tool with Knowland’s version of a traditional readerboard report, in addition to telephone-verified contact information. Each week, approximately 400 field researchers take digital photos of hotel readerboards. Those photos are uploaded each night to the Knowland database and the data made available online to clients the next morning. New groups are constantly being added to the database and detailed profiles are created and updated for each group. In April 2014, Knowland released a new data management process to help maintain the accuracy and freshness of the information provided.\n\nKnowland Group was ranked 26th on Deloitte’s 2010 Technology Fast 500™ list in October 2010. The list identifies the fastest-growing technology, media, telecommunications, life sciences, and clean technology companies in North America. Rankings for the award were based on the fiscal revenue growth from 2005–2009, during which Knowland grew 5,764 percent. Knowland ranked fifth out of 190 companies in the software industry, which was the largest industry recognized on the Fast 500™ list. Knowland ranked first on the Greater Philadelphia Fast 50 list.\n\nTarget Net, Knowland’s sales force automation and meetings management tool, received a 2010 Product of the Year Award from Technology Marketing Corporation’s Customer Interaction Solutions magazine. The award is given annually to companies based on their vision, leadership, and diligence.\n"}
{"id": "7964372", "url": "https://en.wikipedia.org/wiki?curid=7964372", "title": "List of mobile network operators of the Asia Pacific region", "text": "List of mobile network operators of the Asia Pacific region\n\nThis is a list of all mobile phone carriers in the Asia Pacific Region and their respective number of subscribers.\n\n, the penetration rate in Afghanistan was estimated at 86%: 31.28 millions subscribers over a population estimate of 36.37 million. It is estimated that 56% of the subscribers own 2 SIM cards or more and 12% of the subscribers own 4 SIM cards.\n\nThe country's telecom regulator is the Afghanistan Telecommunications Regulatory Authority (ATRA). http://atra.gov.af/en\n, American Samoa has 32,000 subscribers in total, or an 85% penetration rate.\n\nAs of June 2014, Armenia has 3,3 million subscribers in total, and a 120% penetration rate.\n, the number of mobile phone subscriptions in Australia was recorded to be 29.28 million which corresponds to a penetration rate of 129.482% over an estimated population of around 21.8 million.\n\nThe country's telecom regulator is the Australian Communications and Media Authority.\nThe total number of Mobile Phone subscriptions in Bangladesh has reached 155.810 million at the end of September 2018. The country's telecom regulator is the Bangladesh Telecommunication Regulatory Commission (BTRC).\n\nAs of October 2018, total four mobile operators are truly operational. Airtel merged with Robi and Pacific Bangladesh Telecom Limited (ownership: SingTel (44.54%), Pacific Motors (37.95%) and Far East Telecom (17.51%)), which used 'Citycell' and 'Zoom Ultra' brands, is no longer operating commercially. On 19th February 2018 Government has given FDD-LTE license to above three telecom operators. Besides, there are another two LTE licensed operators: Banglalion, Qubee and OLLO; which do not have permission for voice services. AMTOB is a national trade organization representing all mobile telecom operators in Bangladesh.\n\n, the penetration rate in Bhutan was 690,000 subscribers in total, or over 90% penetration rate.\n\n, the penetration rate in Brunei Darussalam was 114%, over a population estimate of over 400,000.\n\nThe country's telecom regulator is the Authority for Info-communications Technology Industry (AITI).\n, the penetration rate in Cambodia was estimated at 69.318% over a population estimate of over 14.7 million.\n\n, China has over 1.3 billion mobile subscribers in total. The country's telecom regulator is the Ministry of Industry and Information Technology (工业和信息化部).\n\n, Cook Islands has 6,700 subscribers in total.\n, the penetration rate in Fiji was estimated at 79.957% over a population estimate of around 0.9 million.\n, the penetration rate in French Polynesia was estimated at 98.7% over a population estimate of around 268.000.<br>\n, Guam has 185,000 subscribers in total.\n\n, the penetration rate in Hong Kong was estimated at 240.8% over a population estimate of over 7.325 million. Hong Kong's telecom regulator is the Office of the Communications Authority (OFCA).\n\n, India has 1.169 Billion subscribers in total or 89.5% penetration rate. The country's telecom regulator is the Telecom Regulatory Authority of India.\n\nNote:\n\n\n\n\n\nIndonesia has 254.792 million subscribers in total (April 2018), or a 142.00% penetration rate (January 2017).\nThe regulatory authority for telecommunication in Indonesia is the Badan Regulasi Telekomunikasi Indonesia.\n\n, Japan has 165,821,800 subscribers in total, or a 112.17% penetration rate.\n, Kyrgyzstan has 6.6 million subscribers in total, a 120% penetration rate.\n\n, Laos mobile penetration was 70.86% with 4.841 millions subscriptions.\n\n, Macau had about 1.969 million subscribers in total. Macau's telecom regulator is Direcção dos Serviços de Correios e Telecomunicações (DSRT).\n\n, the penetration rate in Malaysia was estimated at 144.2%.\n\nThe country's telecom regulator is the Malaysian Communications and Multimedia Commission.\n\n, the penetration rate in Maldives was estimated at 116.456% over a population estimate of around 0.4 million.\n, Marshall Islands has 1000 subscribers in total.\n, Micronesia, Federated States of has 34,000 subscribers in total.\n, the penetration rate in Mongolia was estimated at 80.120% over a population estimate of around 3.1 million.\n, the penetration rate in Myanmar was estimated at 105% over a population estimate of around 51.4 million.\n\n, the penetration rate in Nauru was estimated at 42.909% over a population estimate of around 9,000 people. The World Factbook shows 6,700 subscribers in 2011.\n\n, the penetration rate in Nepal was estimated at 130.24% over a population estimate of around 26.5 million.\n\n, the penetration rate in New Zealand was estimated at 124.326% over a population estimate of around 4.3 million.\nAs of 2008, Niue has about 385 subscribers in total, a 25% penetration rate.\n, Norfolk Island has 131 subscribers in total, a 6.5% penetration rate.\n North Korea has over a million subscribers in total.\n\n, the Northern Mariana Islands have 20,500 subscribers in total.\n\nAs of October 2018, Pakistan has 152.16 million subscribers in total with a mobile density of 73.23%.\n\nThe country's telecom regulator is the PTA (Pakistan Telecommunication Authority).\n\n, Palau has 17,150 subscribers in total.\n\n, the penetration rate in Papua New Guinea was estimated at 47.595% over a population estimate of around 6.2 million.\n, the number of subscribers in the Philippines was estimated at approximately 129.1 million, a 129% penetration rate.\n\nThe country's telecom regulator is the National Telecommunications Commission (NTC).\n\nRussia has 247.9 million subscribers in total, or a 173.6% penetration rate. (3Q 2015).\n\n, Samoa has 85,000 subscribers in total, a 47% penetration rate.\n, the penetration rate in Singapore was estimated at 149.1% over a population estimate of around 5.61 million.\n\n, Solomon Islands has 30,000 subscribers in total.\n, South Korea has 60.55 million subscribers in total.\n\nSri Lanka has 38.85 million subscribers in total, or a 179.3% penetration rate. (September 2018)\n\nThe country's telecom regulator is Telecommunications Regulatory Commission of Sri Lanka (TRCSL).\n\n, the penetration rate in Taiwan was estimated at 105.354% over a population estimate of around 23 million.\n, Tajikistan has an estimated 4.5 million subscribers in total, a 42% penetration rate.\n\nThe Telecom Regulator is the Communications Regulatory Agency (CRA).\n, the penetration rate in Thailand was 158% over a population estimate of around 86.8 million.\n\nMobile subscribers reach 1.54m at End-2017\n, Tonga has 0.03 million subscribers in total.\n, Turkmenistan has 4.44 million subscribers in total or 88.8% penetration rate.\n, Tuvalu has 2,000 subscribers in total.\n, the penetration rate in Uzbekistan was estimated at 81.205% over a population estimate of around 28.1 million.\n\nThe country's telecom regulator is the Uzbekistan Communications and Information Agency's Board (CIAUz).\n\n, Vanuatu has 36,000 subscribers in total.\n, the penetration rate in Vietnam was estimated at 120% over a population estimate of around 91 million.\n\n"}
{"id": "407214", "url": "https://en.wikipedia.org/wiki?curid=407214", "title": "List of proposed future transport", "text": "List of proposed future transport\n\nTransport today is mostly powered by fossil fuel. The reason for this is the ease of use and the existence of mature technologies harnessing this fuel source. Fossil fuels represent a concentrated, relatively compact source of energy. The drawbacks are that they are heavily polluting and rely on limited natural resources. There are many proposals to harness renewable forms of energy, to use fossil fuel more efficiently, or to use human power, or some hybrid of these, to move people and things.\n\nThe list below contains some forms of transport not in general use, but considered as possibilities in the future.\n\n\n"}
{"id": "1516916", "url": "https://en.wikipedia.org/wiki?curid=1516916", "title": "Magnetic core", "text": "Magnetic core\n\nA magnetic core is a piece of magnetic material with a high magnetic permeability used to confine and guide magnetic fields in electrical, electromechanical and magnetic devices such as electromagnets, transformers, electric motors, generators, inductors, magnetic recording heads, and magnetic assemblies. It is made of ferromagnetic metal such as iron, or ferrimagnetic compounds such as ferrites. The high permeability, relative to the surrounding air, causes the magnetic field lines to be concentrated in the core material. The magnetic field is often created by a current-carrying coil of wire around the core.\n\nThe use of a magnetic core can increase the strength of magnetic field in an electromagnetic coil by a factor of several hundred times what it would be without the core. However, magnetic cores have side effects which must be taken into account. In alternating current (AC) devices they cause energy losses, called core losses, due to hysteresis and eddy currents in applications such as transformers and inductors. \"Soft\" magnetic materials with low coercivity and hysteresis, such as silicon steel, or ferrite, are usually used in cores.\n\nAn electric current through a wire wound into a coil creates a magnetic field through the center of the coil, due to Ampere's circuital law. Coils are widely used in electronic components such as electromagnets, inductors, transformers, electric motors and generators. A coil without a magnetic core is called an \"air core\" coil. Adding a piece of ferromagnetic or ferrimagnetic material in the center of the coil can increase the magnetic field by hundreds or thousands of times; this is called a magnetic core. The field of the wire penetrates the core material, magnetizing it, so that the strong magnetic field of the core adds to the field created by the wire. The amount that the magnetic field is increased by the core depends on the magnetic permeability of the core material. Because side effects such as eddy currents and hysteresis can cause frequency-dependent energy losses, different core materials are used for coils used at different frequencies. \n\nIn some cases the losses are undesirable and with very strong fields saturation can be a problem, and an 'air core' is used. A former may still be used; a piece of material, such as plastic or a composite, that may not have any significant magnetic permeability but which simply holds the coils of wires in place.\n\n\"Soft\" (annealed) iron is used in magnetic assemblies, direct current (DC) electromagnets and in some electric motors; and it can create a concentrated field that is as much as 50,000 times more intense than an air core.\n\nIron is desirable to make magnetic cores, as it can withstand high levels of magnetic field without saturating (up to 2.16 teslas at ambient temperature.) Annealed iron is used because, unlike \"hard\" iron, it has low coercivity and so does not remain magnetised when the field is removed, which is often important in applications where the magnetic field is required to be repeatedly switched.\n\nDue to the electrical conductivity of the metal, when a solid one-piece metal core is used in alternating current (AC) applications such as transformers and inductors, the changing magnetic field induces large eddy currents circulating within it, closed loops of electric current in planes perpendicular to the field. The current flowing through the resistance of the metal heats it by Joule heating, causing significant power losses. Therefore, solid iron cores are not used in transformers or inductors, they are replaced by laminated or powdered iron cores, or nonconductive cores like ferrite.\n\nIn order to reduce the eddy current losses mentioned above, most low frequency power transformers and inductors use laminated cores, made of stacks of thin sheets of silicon steel:\n\nLaminated magnetic cores are made of stacks of thin iron sheets coated with an insulating layer, lying as much as possible parallel with the lines of flux. The layers of insulation serve as a barrier to eddy currents, so eddy currents can only flow in narrow loops within the thickness of each single lamination. Since the current in an eddy current loop is proportional to the area of the loop, this prevents most of the current from flowing, reducing eddy currents to a very small level. Since power dissipated is proportional to the square of the current, breaking a large core into narrow laminations reduces the power losses drastically. From this, it can be seen that the thinner the laminations, the lower the eddy current losses.\n\nA small addition of silicon to iron (around 3%) results in a dramatic increase of the resistivity of the metal, up to four times higher. The higher resistivity reduces the eddy currents, so silicon steel is used in transformer cores. Further increase in silicon concentration impairs the steel's mechanical properties, causing difficulties for rolling due to brittleness.\n\nAmong the two types of silicon steel, grain-oriented (GO) and grain non-oriented (GNO), GO is most desirable for magnetic cores. It is anisotropic, offering better magnetic properties than GNO in one direction. As the magnetic field in inductor and transformer cores is always along the same direction, it is an advantage to use grain oriented steel in the preferred orientation. Rotating machines, where the direction of the magnetic field can change, gain no benefit from grain-oriented steel.\n\nA family of specialized alloys exists for magnetic core applications. Examples are mu-metal, permalloy, and supermalloy. They can be manufactured as stampings or as long ribbons for tape wound cores. Some alloys, e.g. Sendust, are manufactured as powder and sintered to shape.\n\nMany materials require careful heat treatment to reach their magnetic properties, and lose them when subjected to mechanical or thermal abuse. For example, the permeability of mu-metal increases about 40 times after annealing in hydrogen atmosphere in a magnetic field; subsequent sharper bends disrupt its grain alignment, leading to localized loss of permeability; this can be regained by repeating the annealing step.\n\nAmorphous metal is a variety of alloys (e.g. Metglas) that are non-crystalline or glassy. These are being used to create high-efficiency transformers. The materials can be highly responsive to magnetic fields for low hysteresis losses, and they can also have lower conductivity to reduce eddy current losses. China is currently making widespread industrial and power grid usage of these transformers for new installations.\n\nPowder cores consist of metal grains mixed with a suitable organic or inorganic binder, and pressed to desired density. Higher density is achieved with higher pressure and lower amount of binder. Higher density cores have higher permeability, but lower resistance and therefore higher losses due to eddy currents. Finer particles allow operation at higher frequencies, as the eddy currents are mostly restricted to within the individual grains. Coating of the particles with an insulating layer, or their separation with a thin layer of a binder, lowers the eddy current losses. Presence of larger particles can degrade high-frequency performance. Permeability is influenced by the spacing between the grains, which form distributed air gap; the less gap, the higher permeability and the less-soft saturation. Due to large difference of densities, even a small amount of binder, weight-wise, can significantly increase the volume and therefore intergrain spacing.\n\nLower permeability materials are better suited for higher frequencies, due to balancing of core and winding losses.\n\nThe surface of the particles is often oxidized and coated with a phosphate layer, to provide them with mutual electrical insulation.\n\nPowdered iron is the cheapest material. It has higher core loss than the more advanced alloys, but this can be compensated for by making the core bigger; it is advantageous where cost is more important than mass and size. Saturation flux of about 1 to 1.5 tesla. Relatively high hysteresis and eddy current loss, operation limited to lower frequencies (approx. below 100 kHz). Used in energy storage inductors, DC output chokes, differential mode chokes, triac regulator chokes, chokes for power factor correction, resonant inductors, and pulse and flyback transformers.\n\nThe binder used is usually epoxy or other organic resin, susceptible to thermal aging. At higher temperatures, typically above 125 °C, the binder degrades and the core magnetic properties may change. With more heat-resistant binders the cores can be used up to 200 °C.\n\nIron powder cores are most commonly available as toroids. Sometimes as E, EI, and rods or blocks, used primarily in high-power and high-current parts.\n\nCarbonyl iron is significantly more expensive than hydrogen-reduced iron.\n\nPowdered cores made of carbonyl iron, a highly pure iron, have high stability of parameters across a wide range of temperatures and magnetic flux levels, with excellent Q factors between 50 kHz and 200 MHz. Carbonyl iron powders are basically constituted of micrometer-size spheres of iron coated in a thin layer of electrical insulation. This is equivalent to a microscopic laminated magnetic circuit (see silicon steel, above), hence reducing the eddy currents, particularly at very high frequencies. Carbonyl iron has lower losses than hydrogen-reduced iron, but also lower permeability.\n\nA popular application of carbonyl iron-based magnetic cores is in high-frequency and broadband inductors and transformers, especially higher power ones.\n\nCarbonyl iron cores are often called \"RF cores\".\n\nThe as-prepared particles, \"E-type\"and have onion-like skin, with concentric shells separated with a gap. They contain significant amount of carbon. They behave as much smaller than what their outer size would suggest. The \"C-type\" particles can be prepared by heating the E-type ones in hydrogen atmosphere at 400 °C for prolonged time, resulting in carbon-free powders.\n\nPowdered cores made of hydrogen reduced iron have higher permeability but lower Q than carbonyl iron. They are used mostly for electromagnetic interference filters and low-frequency chokes, mainly in switched-mode power supplies.\n\nHydrogen-reduced iron cores are often called \"power cores\".\n\nAn alloy of about 2% molybdenum, 81% nickel, and 17% iron. Very low core loss, low hysteresis and therefore low signal distortion. Very good temperature stability. High cost. Maximum saturation flux of about 0.8 tesla. Used in high-Q filters, resonant circuits, loading coils, transformers, chokes, etc.\n\nThe material was first introduced in 1940, used in loading coils to compensate capacitance in long telephone lines. It is usable up to about 200 kHz to 1 MHz, depending on vendor. It is still used in above-ground telephone lines, due to its temperature stability. Underground lines, where temperature is more stable, tend to use ferrite cores due to their lower cost.\n\nAn alloy of about 50–50% of nickel and iron. High energy storage, saturation flux density of about 1.5 tesla. Residual flux density near zero. Used in applications with high DC current bias (line noise filters, or inductors in switching regulators) or where low residual flux density is needed (e.g. pulse and flyback transformers, the high saturation is suitable for unipolar drive), especially where space is constrained. The material is usable up to about 200 kHz.\n\nAn alloy of 6% aluminium, 9% silicon, and 85% iron. Core losses higher than MPP. Very low magnetostriction, makes low audio noise. Loses inductance with increasing temperature, unlike the other materials; can be exploited by combining with other materials as a composite core, for temperature compensation. Saturation flux of about 1 tesla. Good temperature stability. Used in switching power supplies, pulse and flyback transformers, in-line noise filters, swing chokes, and in filters in phase-fired controllers (e.g. dimmers) where low acoustic noise is important.\n\nAbsence of nickel results in easier processing of the material and its lower cost than both high-flux and MPP.\n\nThe material was invented in Japan in 1936. It is usable up to about 500 kHz to 1 MHz, depending on vendor.\n\nA nanocrystalline alloy of a standard iron-boron-silicon alloy, with addition of smaller amounts of copper and niobium. The grain size of the powder reaches down to 10-100 nanometers. The material has very good performance at lower frequencies. It is used in chokes for inverters and in high power applications. It is available under names like e.g. Nanoperm, Vitroperm, Hitperm and Finemet.\n\nFerrite ceramics are used for high-frequency applications. The ferrite materials can be engineered with a wide range of parameters. As ceramics, they are essentially insulators, which prevents eddy currents, although losses such as hysteresis losses can still occur.\n\nA coil not containing a magnetic core is called an \"air core\". This includes coils wound on a plastic or ceramic form in addition to those made of stiff wire that are self-supporting and have air inside them. Air core coils generally have a much lower inductance than similarly sized ferromagnetic core coils, but are used in radio frequency circuits to prevent energy losses called core losses that occur in magnetic cores. The absence of normal core losses permits a higher Q factor, so air core coils are used in high frequency resonant circuits, such as up to a few megahertz. However, losses such as proximity effect and dielectric losses are still present. Air cores are also used when field strengths above around 2 Tesla are required as they are not subject to saturation.\n\nMost commonly made of ferrite or powdered iron, and used in radios especially for tuning an inductor. The coil is wound around the rod, or a coil form with the rod inside. Moving the rod in or out of the coil changes the flux through the coil, and can be used to adjust the inductance. Often the rod is threaded to allow adjustment with a screwdriver. In radio circuits, a blob of wax or resin is used once the inductor has been tuned to prevent the core from moving.\n\nThe presence of the high permeability core increases the inductance, but the magnetic field lines must still pass through the air from one end of the rod to the other. The air path ensures that the inductor remains linear. In this type of inductor radiation occurs at the end of the rod and electromagnetic interference may be a problem in some circumstances.\n\nLike a cylindrical rod but square, rarely used on its own.\nThis type of core is most likely to be found in car ignition coils.\n\n\"U\" and \"C\"-shaped cores are used with \"I\" or another \"C\" or \"U\" core to make a square closed core, the simplest closed core shape. Windings may be put on one or both legs of the core.\nE-shaped core are more symmetric solutions to form a closed magnetic system. Most of the time, the electric circuit is wound around the center leg, whose section area is twice that of each individual outer leg. In 3-phase transformer cores, the legs are of equal size, and all three legs are wound.\n\nSheets of suitable iron stamped out in shapes like the (sans-serif) letters \"E\" and \"I\", are stacked with the \"I\" against the open end of the \"E\" to form a 3-legged structure. Coils can be wound around any leg, but usually the center leg is used. This type of core is frequently used for power transformers, autotransformers, and inductors.\n\nAgain used for iron cores. Similar to using an \"E\" and \"I\" together, a pair of \"E\" cores will accommodate a larger coil former and can produce a larger inductor or transformer. If an air gap is required, the centre leg of the \"E\" is shortened so that the air gap sits in the middle of the coil to minimize fringing and reduce electromagnetic interference.\n\nA planar core consists of two flat pieces of magnetic material, one above and one below the coil. It is typically used with a flat coil that is part of a printed circuit board. This design is excellent for mass production and allows a high power, small volume transformer to be constructed for low cost. It is not as ideal as either a pot core or toroidal core but costs less to produce.\n\nUsually ferrite or similar. This is used for inductors and transformers. The shape of a pot core is round with an internal hollow that almost completely encloses the coil. Usually a pot core is made in two halves which fit together around a coil former (bobbin). This design of core has a shielding effect, preventing radiation and reducing electromagnetic interference.\n\nThis design is based on a toroid (the same shape as a doughnut). The coil is wound through the hole in the torus and around the outside. An ideal coil is distributed evenly all around the circumference of the torus. The symmetry of this geometry creates a magnetic field of circular loops inside the core, and the lack of sharp bends will constrain virtually all of the field to the core material. This not only makes a highly efficient transformer, but also reduces the electromagnetic interference radiated by the coil.\n\nIt is popular for applications where the desirable features are: high specific power per mass and volume, low mains hum, and minimal electromagnetic interference. One such application is the power supply for a hi-fi audio amplifier. The main drawback that limits their use for general purpose applications is the inherent difficulty of winding wire through the center of a torus.\n\nUnlike a split core (a core made of two elements, like a pair of \"E\" cores), specialized machinery is required for automated winding of a toroidal core. Toroids have less audible noise, such as mains hum, because the magnetic forces do not exert bending moment on the core. The core is only in compression or tension, and the circular shape is more stable mechanically.\n\nThe ring is essentially identical in shape and performance to the toroid, except that inductors commonly pass only through the center of the core, without wrapping around the core multiple times.\n\nThe ring core may also be composed of two separate C-shaped hemispheres secured together within a plastic shell, permitting it to be placed on finished cables with large connectors already installed, that would prevent threading the cable through the small inner diameter of a solid ring.\n\nThe A value of a core configuration is frequently specified by manufacturers. The relationship between inductance and A number in the linear portion of the magnetisation curve is defined to be:\n\nwhere n is the number of turns, L is the inductance (e.g. in nH) and A is expressed in inductance per turn squared (e.g. in nH/n).\n\nWhen the core is subjected to a \"changing\" magnetic field, as it is in devices that use AC current such as transformers, inductors, and AC motors and alternators, some of the power that would ideally be transferred through the device is lost in the core, dissipated as heat and sometimes noise. The losses are often described as being in three categories:\n\nWhen the magnetic field through the core changes, the magnetization of the core material changes by expansion and contraction of the tiny magnetic domains it is composed of, due to movement of the domain walls. This process causes losses, because the domain walls get \"snagged\" on defects in the crystal structure and then \"snap\" past them, dissipating energy as heat. This is called hysteresis loss. It can be seen in the graph of the \"B\" field versus the \"H\" field for the material, which has the form of a closed loop. \nThe net energy that flows into the inductor expressed in relationship to the B-H characteristic of the core is shown by the equation\nThis equation shows that the amount of energy lost in the material in one cycle of the applied field is proportional to the area inside the hysteresis loop. Since the energy lost in each cycle is constant, hysteresis power losses increase proportionally with frequency. The final equation for the hysteresis power loss is\n\nIf the core is electrically conductive, the changing magnetic field induces circulating loops of current in it, called eddy currents, due to electromagnetic induction. The loops flow perpendicular to the magnetic field axis. The energy of the currents is dissipated as heat in the resistance of the core material. The power loss is proportional to the area of the loops and inversely proportional to the resistivity of the core material. Eddy current losses can be reduced by making the core out of thin laminations which have an insulating coating, or alternatively, making the core of a magnetic material with high electrical resistance, like ferrite.. Most magnetic cores intended for power converter application use ferrite cores for this reason.\n\nBy definition, this category includes any losses in addition to eddy-current and hysteresis losses. This can also be described as broadening of the hysteresis loop with frequency. Physical mechanisms for anomalous loss include localized eddy-current effects near moving domain walls.\n\nAn equation known as Legg's equation models the magnetic material core loss at low flux densities. The equation has three loss components: hysteresis, residual, and eddy current, and it is given by\n\nwhere\n\n\n\n"}
{"id": "4598960", "url": "https://en.wikipedia.org/wiki?curid=4598960", "title": "Malaysian Centre of Remote Sensing", "text": "Malaysian Centre of Remote Sensing\n\nThe Malaysian Centre of Remote Sensing (MACRES) was a Malaysian remote sensing centre. An agency under the Ministry of Science, Technology and Innovation of Malaysia. Its role was to provide data and solutions for remote sensing applications. The main centre was located in Kuala Lumpur and the ground receiving centre was located in Mentakab, Pahang, Malaysia.\n\n"}
{"id": "35482706", "url": "https://en.wikipedia.org/wiki?curid=35482706", "title": "Ministry of Communications and Multimedia (Malaysia)", "text": "Ministry of Communications and Multimedia (Malaysia)\n\nThe Ministry of Communications and Multimedia (), abbreviated KKMM, is a ministry of the Government of Malaysia that is responsible for communications, multimedia, broadcasting, information, personal data protection, special affairs, media industry, film industry, domain name, postal, courier, mobile service, fixed service, broadband, digital signature, universal service, international broadcasting, content.\n\nThe Ministry is housed in the Sultan Abdul Samad Building in Kuala Lumpur.\n\n\n\n\nThe Ministry of Communications and Multimedia is responsible for administration of several key Acts:\n\n\nUniversal Services Provision:\n\n\n"}
{"id": "38333590", "url": "https://en.wikipedia.org/wiki?curid=38333590", "title": "Morris N Beitman", "text": "Morris N Beitman\n\nBorn on December 1911 in Cook County, Illinois. Started as an engineer with the US Army Signal Corp, he used his experience and abilities to form two career paths. He became a teacher in the Chicago public schools high school system as a radio instructor. Soon after got into technical publishing as Supreme Publications. As publisher his goal was to support the radio and television servicing industry with easy to understand reference materials. He was married to Rose Rissman. \nMorris Beitman received his BS in Mathematics at Illinois Institute of Technology and later served in the military as an engineer with the US Army Signal Corps. He later became an associate member with the Institute of Radio Engineers (now called Institute of Electrical and Electronics Engineers or IEEE). His known pre and post military career was a teacher in the Chicago public high schools as radio instructor.\n\nBy the mid-1930s the growth in ownership of radio receivers in the United States spun off other business opportunities. One of them was the repair of radio and radio-phonograph sets and eventually, television. Hugo Gernsback was an early publisher of repair manuals. Soon others were publishing. John F. Rider in the early 1930s began to compile complete volumes of radio servicing diagrams of many radio manufacturers called the Perpetual Troubleshooter's Manual. In time these servicing manuals became quite large and contained information on radios that were not common or were produced in small quantities. Although Rider's Perpetual Troubleshooter's Manual became a standard reference during the 1930s, the size and bulk of these yearly volumes could become a hindrance. Rider manuals contained information on common and rare models. Service businesses were paying extra for brands they rarely or never encounter. This \"opened the door\" for an alternative path and Morris Beitman was able to exploit this business opportunity.\n\nIn 1940-41 Morris N. Beitman under the name Supreme Publications (located at 328 S. Jefferson St in Chicago) produced a series of books called, \"Most-Often-Needed Radio Diagrams and Servicing Information\" and later \"Most-Often-Needed Television Servicing Information\" for successive years. These books offered radio and television repair businesses a condensed version of the Rider's, \"Perpetual Troubleshooter's Manual\", by only providing models that were common or made in large quantities. The radio series started with 1926-38 models in one volume. Each year after that represented a new volume until 1969 when the last volume was published. The television series started in 1946 and continued into the early 1970s. Their last known location was 1760 Balsam Road, Highland Park, Illinois.\n\nFrom 1940-1960 Supreme published other volume sets for record players, tape recorders, wire recorders and other specialized consumer electronics using the \"most often needed\" title. These references like radios and televisions were in yearly bound sets but were not consistent year-to-year sets. These volumes never reached the quantity, consistency or longevity of the radio or television titles.\n\nBeitman under the Supreme Publication name was author to a number of other publications devoted to radio and television. Dates given are the earliest known date of first publication.\n\n\nLittle can be found on Morris N. Beitman's career activities and Supreme Publications. It is also possible that Supreme Publications was a side business for Morris Beitman since he was also a radio instructor in the Chicago high schools. In the 1950s, like John F. Rider, Beitman had to revise many of his publications due to rapid changes in technology used in consumer electronics. In the 1960s his son Hartford assisted him in Supreme Publications by continuing the yearly service volumes for radio and television. Many of Morris Beitman's publications either had no copyright or they had expired. Hartford spent time to straighten these matters and compiled the \"1967-69 Most Often Needed Radio Diagram and Servicing Information\" book. Supreme would stop publishing by the early 1970s. One reason was the declining need for repairing consumer electronics due to the increased reliability and the low cost of electronics (cost less to buy one than repair it). He died on February 1980, in Highland Park, Illinois.\n\nRadio and television servicing is no longer a significant business segment in small or independent business. The rise in interest is in preserving old consumer technology. Interest in Morris Beitman's, \"Most Often Needed Radio Diagrams\" resurfaced in the early 1980s with the rise of restoring antique and collectible radios made before the 1940s. Vintage Radio, founded by Morgan E. McMahon was a publishing company specializing in preserving early radio and television technology. They reprinted \"Most Often Needed 1926-1938 Radio Diagrams and Servicing Information\" and sold it to radio collectors.\n\nFrom 1986-1989, Hartford Beitman and Kristina Hund Beitman made an effort to resurrect the \"Most Often Needed\" series by compiling past servicing information from 1926 to 1950 based on brand name. This series of books was published by A R S Enterprises. No more new copies are available and A R S Enterprises publishing current status has not been found. Hartford Beitman's other books are Recorded Sound (1981), Seattle Seating (1984), Aggressive Investment Marketing (1985), Radio Hobbyist Handbook (1988) and Financial Services Marketing (1990).\n\nSupreme's \"Most Often Needed\" series have become public domain. There are groups interested in technology preservation. These groups through scanning or digitizing the thousands of pages of the series have made them available online for non-commercial use. Radio collecting and restoration books suggest using Supreme's \"Most-Often-Needed\" volumes as a starting source to find information. Preserving old technology is often called \"dead technology\", in other words technology that is no longer advancing or used in mass production.\n\nThe American Radio History website currently offers the complete set of Supreme Publications \"Most Often Need Radio Servicing Information\" and \"Most Often Needed Television Servicing Information\" to be viewed online. Selecting a title and year allows one to scan through the entire manual. For more information use this link: http://www.americanradiohistory.com/Beitman-Manual.htm\n"}
{"id": "30721542", "url": "https://en.wikipedia.org/wiki?curid=30721542", "title": "Nanofountain probe", "text": "Nanofountain probe\n\nNanofountain probe (NFP) is a cantilevered micro-fluidic device terminated in a nanofountain. The embedded microfluidics facilitates rapid and continuous delivery of molecules from the on-chip reservoirs to the fountain tip. When the tip is brought into contact with the substrate, a liquid meniscus forms, providing a path for molecular transport to the substrate. By controlling the geometry of the meniscus through hold time and deposition speed, various inks and biomolecules could be patterned on a surface, with sub 100 nm resolution.\n\nThe advent of dip-pen nanolithography (DPN) in recent years represented a revolution in nanoscale patterning technology. With sub-100-nanometer resolution and an architecture conducive to massive parallelization, DPN is capable of producing large arrays of nanoscale features. As such, conventional DPN and other probe-based techniques are generally limited in their rate of deposition and by the need for repeated re-inking during extended patterning.\n\nTo address these challenges, nanofountain probe was developed by Espinosa et al. where microchannels were embedded in AFM probes to transport ink or bio-molecules from reservoirs to substrates, realizing continuous writing at the nanoscale. Integration of continuous liquid ink feeding within the NFP facilitates more rapid deposition and eliminates the need for repeated dipping, all while preserving the sub-100-nanometer resolution of DPN.\n\nNano fountain probes (NFPs) are fabricated on the wafer-scale using microfabrication techniques allowing for batch fabrication of numerous chips. Through the different generations of devices, design and experimentation improved the device yielding to a robust fabrication process. The highly enhanced feature dimension and shapes is expected to improve the performance in writing and imaging.\n\nNFP is used in the development of a to scale, direct-write nanomanufacturing platform. The platform is capable of constructing complex, highly-functional nanoscale devices from a diverse suite of materials (e.g., nanoparticles, catalysts (increase rate of reaction), biomolecules, and chemical solutions).\nDemonstrated nanopatterning capabilities include:\n\n• Biomolecules (proteins, DNA) for biodetection assays or cell adhesion studies\n\n• Functional nanoparticles for drug delivery studies and nanosystems making (fabrication)\n\n• Catalysts for carbon nanotube growth in nanodevice fabrication\n\n• Thiols for directed self-assembly of nanostructures.\n\nTaking advantage of the unique tip geometry of the NFP nanomaterials are directly injected into live cells with minimal invasiveness. This enables unique studies of nanoparticle-mediated delivery, as well as cellular pathways and toxicity. Whereas typical in vitro studies are limited to cell populations, these broadly-applicable tools enable multifaceted interrogation at a truly single cell level.\n\n"}
{"id": "3286357", "url": "https://en.wikipedia.org/wiki?curid=3286357", "title": "Odor detection threshold", "text": "Odor detection threshold\n\nThe odor detection threshold is the lowest concentration of a certain odor compound that is perceivable by the human sense of smell. The threshold of a chemical compound is determined in part by its shape, polarity, partial charges, and molecular mass. The olfactory mechanisms responsible for a compound's different detection threshold is not well understood. As such, odor thresholds cannot be accurately predicted. Rather, they must be measured through extensive tests using human subjects in laboratory settings.\n\nOptical isomers can have different detection thresholds because their conformations may cause them to be less perceivable for the human nose. It is only in recent years that such compounds were separated on gas chromatographs.\n\n\n\"Odor threshold value (OTV)\" (also \"aroma threshold value (ATV)\", \"Flavor threshold\") is defined as the most minimal concentration of a substance that can be detected by a human nose. Some substances can be detected when their concentration is only few milligrams per 1000 tonnes, which is less than a drop in an Olympic swimming pool. Odor threshold value can be expressed as a concentration in water or concentration in air.\n\nTwo major types of flavor thresholds can be distinguished: the absolute and the difference threshold. The odor detection threshold and the odor recognition threshold are absolute thresholds; the first is the minimum concentration at which an odor can be detected without any requirements to identify or recognize the stimulus, while the second is the minimum concentration at which a stimulus can be identified or recognized.\n\nThe odor threshold value of an odorant is influenced by the medium.\n\nExamples of substances with strong odors:\n\n\nThreshold in a food is dependent upon:\n\nThe concentration of an odor above a food is dependent on its solubility in that food and its vapor pressure and concentration in that food.\n\n"}
{"id": "20059418", "url": "https://en.wikipedia.org/wiki?curid=20059418", "title": "Penalty flag", "text": "Penalty flag\n\nThe penalty flag (or just \"flag\") is a yellow cloth used in several field sports including American football and lacrosse by game officials to identify and sometimes mark the location of penalties or infractions that occur during regular play. It is usually wrapped around a weight, such as sand or beans so it can be thrown accurately over greater distances. Many officials previously weighted flags with ball bearings, but the practice was largely discontinued after a flag thrown by NFL referee Jeff Triplette struck Cleveland Browns offensive tackle Orlando Brown Sr. in the eye during a 1999 game vs. the Jacksonville Jaguars, causing a serious injury to Brown, who later attacked Triplette and threw him to the ground. Brown was forced to sit out three seasons because of the eye injury and settled with the NFL for a reported amount of $25 million.\n\nThe flag is colored orange in Canadian football. NFL penalty flags were colored white until 1965, when the color was changed to yellow. Penalty flags in college football were red until the 1970s.\n\nThe idea for the penalty flag came from Youngstown State coach Dwight Beede and first used in a game against Oklahoma City University on October 17, 1941. Prior to the use of flags, officials used horns and whistles to signal a penalty. Official adoption of the use of the flag occurred at the 1948 American Football Coaches rules session. The National Football League first used flags on September 17, 1948 when the Green Bay Packers played the Boston Yanks.\n\nIn some football leagues, coaches are given a challenge flag of similar construction as a penalty flag. The flag is red in American football and yellow in Canadian football, so it contrasts with the officials' penalty flags. This is thrown by a coach when he wishes to contest (challenge) a referee's decision.\n\n"}
{"id": "23647", "url": "https://en.wikipedia.org/wiki?curid=23647", "title": "Polymerase chain reaction", "text": "Polymerase chain reaction\n\nPolymerase chain reaction (PCR) is a method widely used in molecular biology to make many copies of a specific DNA segment. Using PCR, a single copy (or more) of a DNA sequence is exponentially amplified to generate thousands to millions of more copies of that particular DNA segment. PCR is now a common and often indispensable technique used in medical laboratory and clinical laboratory research for a broad variety of applications including biomedical research and criminal forensics . PCR was developed by Kary Mullis in 1983 while he was an employee of the Cetus Corporation. He was awarded the Nobel Prize in Chemistry in 1993 (along with Michael Smith) for his work in developing the method.\n\nThe vast majority of PCR methods rely on thermal cycling. Thermal cycling exposes reactants to repeated cycles of heating and cooling to permit different temperature-dependent reactions—specifically, DNA melting and enzyme-driven DNA replication. PCR employs two main reagents - primers (which are short single strand DNA fragments known as oligonucleotides that are a complementary sequence to the target DNA region) and a DNA polymerase. In the first step of PCR, the two strands of the DNA double helix are physically separated at a high temperature in a process called DNA melting. In the second step, the temperature is lowered and the primers bind to the complementary sequences of DNA. The two DNA strands then become templates for DNA polymerase to enzymatically assemble a new DNA strand from free nucleotides, the building blocks of DNA. As PCR progresses, the DNA generated is itself used as a template for replication, setting in motion a chain reaction in which the original DNA template is exponentially amplified.\n\nAlmost all PCR applications employ a heat-stable DNA polymerase, such as Taq polymerase, an enzyme originally isolated from the thermophilic bacterium \"Thermus aquaticus\". If the polymerase used was heat-susceptible, it would denature under the high temperatures of the denaturation step. Before the use of Taq polymerase, DNA polymerase had to be manually added every cycle, which was a tedious and costly process. \n\nApplications of the technique include DNA cloning for sequencing, gene cloning and manipulation, gene mutagenesis; construction of DNA-based phylogenies, or functional analysis of genes; diagnosis and monitoring of hereditary diseases; amplification of ancient DNA; analysis of genetic fingerprints for DNA profiling (for example, in forensic science and parentage testing); and detection of pathogens in nucleic acid tests for the diagnosis of infectious diseases.\n\nPCR amplifies a specific region of a DNA strand (the DNA target). Most PCR methods amplify DNA fragments of between 0.1 and 10 kilo base pairs (kbp) in length, although some techniques allow for amplification of fragments up to 40 kbp. The amount of amplified product is determined by the available substrates in the reaction, which become limiting as the reaction progresses.\n\nA basic PCR set-up requires several components and reagents, including:\n\nThe reaction is commonly carried out in a volume of 10–200 μL in small reaction tubes (0.2–0.5 mL volumes) in a thermal cycler. The thermal cycler heats and cools the reaction tubes to achieve the temperatures required at each step of the reaction (see below). Many modern thermal cyclers make use of the Peltier effect, which permits both heating and cooling of the block holding the PCR tubes simply by reversing the electric current. Thin-walled reaction tubes permit favorable thermal conductivity to allow for rapid thermal equilibration. Most thermal cyclers have heated lids to prevent condensation at the top of the reaction tube. Older thermal cyclers lacking a heated lid require a layer of oil on top of the reaction mixture or a ball of wax inside the tube.\n\nTypically, PCR consists of a series of 20–40 repeated temperature changes, called thermal cycles, with each cycle commonly consisting of two or three discrete temperature steps (see figure below). The cycling is often preceded by a single temperature step at a very high temperature (>), and followed by one hold at the end for final product extension or brief storage. The temperatures used and the length of time they are applied in each cycle depend on a variety of parameters, including the enzyme used for DNA synthesis, the concentration of bivalent ions and dNTPs in the reaction, and the melting temperature (\"Tm\") of the primers. The individual steps common to most PCR methods are as follows:\n\n\n\nTo check whether the PCR successfully generated the anticipated DNA target region (also sometimes referred to as the amplimer or amplicon), agarose gel electrophoresis may be employed for size separation of the PCR products. The size(s) of PCR products is determined by comparison with a DNA ladder, a molecular weight marker which contains DNA fragments of known size run on the gel alongside the PCR products.\nAs with other chemical reactions, the reaction rate and efficiency of PCR are affected by limiting factors. Thus, the entire PCR process can further be divided into three stages based on reaction progress:\n\nIn practice, PCR can fail for various reasons, in part due to its sensitivity to contamination causing amplification of spurious DNA products. Because of this, a number of techniques and procedures have been developed for optimizing PCR conditions. Contamination with extraneous DNA is addressed with lab protocols and procedures that separate pre-PCR mixtures from potential DNA contaminants. This usually involves spatial separation of PCR-setup areas from areas for analysis or purification of PCR products, use of disposable plasticware, and thoroughly cleaning the work surface between reaction setups. Primer-design techniques are important in improving PCR product yield and in avoiding the formation of spurious products, and the usage of alternate buffer components or polymerase enzymes can help with amplification of long or otherwise problematic regions of DNA. Addition of reagents, such as formamide, in buffer systems may increase the specificity and yield of PCR. Computer simulations of theoretical PCR results (Electronic PCR) may be performed to assist in primer design.\n\nPCR allows isolation of DNA fragments from genomic DNA by selective amplification of a specific region of DNA. This use of PCR augments many ways, such as generating hybridization probes for Southern or northern hybridization and DNA cloning, which require larger amounts of DNA, representing a specific DNA region. PCR supplies these techniques with high amounts of pure DNA, enabling analysis of DNA samples even from very small amounts of starting material.\n\nOther applications of PCR include DNA sequencing to determine unknown PCR-amplified sequences in which one of the amplification primers may be used in Sanger sequencing, isolation of a DNA sequence to expedite recombinant DNA technologies involving the insertion of a DNA sequence into a plasmid, phage, or cosmid (depending on size) or the genetic material of another organism. Bacterial colonies \"(such as E. coli)\" can be rapidly screened by PCR for correct DNA vector constructs. PCR may also be used for genetic fingerprinting; a forensic technique used to identify a person or organism by comparing experimental DNAs through different PCR-based methods.\n\nSome PCR 'fingerprints' methods have high discriminative power and can be used to identify genetic relationships between individuals, such as parent-child or between siblings, and are used in paternity testing (Fig. 4). This technique may also be used to determine evolutionary relationships among organisms when certain molecular clocks are used (i.e., the 16S rRNA and recA genes of microorganisms).\n\nBecause PCR amplifies the regions of DNA that it targets, PCR can be used to analyze extremely small amounts of sample. This is often critical for forensic analysis, when only a trace amount of DNA is available as evidence. PCR may also be used in the analysis of ancient DNA that is tens of thousands of years old. These PCR-based techniques have been successfully used on animals, such as a forty-thousand-year-old mammoth, and also on human DNA, in applications ranging from the analysis of Egyptian mummies to the identification of a Russian tsar and the body of English king Richard III.\n\nQuantitative PCR or Real Time Quantitative PCR (RT-qPCR) methods allow the estimation of the amount of a given sequence present in a sample—a technique often applied to quantitatively determine levels of gene expression. Quantitative PCR is an established tool for DNA quantification that measures the accumulation of DNA product after each round of PCR amplification.\n\nRT-qPCR allows the quantification and detection of a specific DNA sequence in real time since it measures concentration while the synthesis process is taking place. There are two methods for simultaneous detection and quantification. The first method consists of using fluorescent dyes that are retained nonspecifically in between the double strands. The second method involves probes that code for specific sequences and are fluorescently labeled. Detection of DNA using these methods can only be seen after the hybridization of probes with its complementary DNA takes place. An interesting technique combination is real-time PCR and reverse transcription. This sophisticated technique allows for the quantification of a small quantity of RNA. Through this combined technique, mRNA is converted to cDNA, which is further quantified using qPCR. This technique lowers the possibility of error at the end point of PCR, increasing chances for detection of genes associated with genetic diseases such as cancer. Laboratories use RT-qPCR for the purpose of sensitively measuring gene regulation.\n\nProspective parents can be tested for being genetic carriers, or their children might be tested for actually being affected by a disease. DNA samples for prenatal testing can be obtained by amniocentesis, chorionic villus sampling, or even by the analysis of rare fetal cells circulating in the mother's bloodstream. PCR analysis is also essential to preimplantation genetic diagnosis, where individual cells of a developing embryo are tested for mutations.\n\nPCR allows for rapid and highly specific diagnosis of infectious diseases, including those caused by bacteria or viruses. PCR also permits identification of non-cultivatable or slow-growing microorganisms such as mycobacteria, anaerobic bacteria, or viruses from tissue culture assays and animal models. The basis for PCR diagnostic applications in microbiology is the detection of infectious agents and the discrimination of non-pathogenic from pathogenic strains by virtue of specific genes.\n\nCharacterization and detection of infectious disease organisms have been revolutionized by PCR in the following ways:\n\n\nThe development of PCR-based genetic (or DNA) fingerprinting protocols has seen widespread application in forensics:\n\n\nPCR has been applied to many areas of research in molecular genetics:\n\n\nPCR has a number of advantages. It is fairly simple to understand and to use, and produces results rapidly. The technique is highly sensitive with the potential to produce millions to billions of copies of a specific product for sequencing, cloning, and analysis. qRT-PCR shares the same advantages as the PCR, with an added advantage of quantification of the synthesized product. Therefore, it has its uses to analyze alterations of gene expression levels in tumors, microbes, or other disease states.\n\nPCR is a very powerful and practical research tool. The sequencing of unknown etiologies of many diseases are being figured out by the PCR. The technique can help identify the sequence of previously unknown viruses related to those already known and thus give us a better understanding of the disease itself. If the procedure can be further simplified and sensitive non radiometric detection systems can be developed, the PCR will assume a prominent place in the clinical laboratory for years to come.\n\nOne major limitation of PCR is that prior information about the target sequence is necessary in order to generate the primers that will allow its selective amplification.<ref name=\"10.1038/jid.2013.1\"></ref> This means that, typically, PCR users must know the precise sequence(s) upstream of the target region on each of the two single-stranded templates in order to ensure that the DNA polymerase properly binds to the primer-template hybrids and subsequently generates the entire target region during DNA synthesis.\n\nLike all enzymes, DNA polymerases are also prone to error, which in turn causes mutations in the PCR fragments that are generated.\n\nAnother limitation of PCR is that even the smallest amount of contaminating DNA can be amplified, resulting in misleading or ambiguous results. To minimize the chance of contamination, investigators should reserve separate rooms for reagent preparation, the PCR, and analysis of product. Reagents should be dispensed into single-use aliquots. Pipetters with disposable plungers and extra-long pipette tips should be routinely used.\n\n\n\nA 1971 paper in the \"Journal of Molecular Biology\" by and co-workers in the laboratory of H. Gobind Khorana first described a method of using an enzymatic assay to replicate a short DNA template with primers \"in vitro\". However, this early manifestation of the basic PCR principle did not receive much attention at the time and the invention of the polymerase chain reaction in 1983 is generally credited to Kary Mullis.\n\nWhen Mullis developed the PCR in 1983, he was working in Emeryville, California for Cetus Corporation, one of the first biotechnology companies, where he was responsible for synthesizing short chains of DNA. Mullis has written that he first conceived the idea for PCR while cruising along the Pacific Coast Highway one night in his car. He was playing in his mind with a new way of analyzing changes (mutations) in DNA when he realized that he had instead invented a method of amplifying any DNA region through repeated cycles of duplication driven by DNA polymerase. In \"Scientific American\", Mullis summarized the procedure: \"Beginning with a single molecule of the genetic material DNA, the PCR can generate 100 billion similar molecules in an afternoon. The reaction is easy to execute. It requires no more than a test tube, a few simple reagents, and a source of heat.\" DNA fingerprinting first become used for paternity testing in 1988.\n\nMullis was awarded the Nobel Prize in Chemistry in 1993 for his invention, seven years after he and his colleagues at Cetus first put his proposal to practice. Mullis’s 1985 paper with R. K. Saiki and H. A. Erlich, “Enzymatic Amplification of β-globin Genomic Sequences and Restriction Site Analysis for Diagnosis of Sickle Cell Anemia”—the polymerase chain reaction invention (PCR) -- was honored by a Citation for Chemical Breakthrough Award from the Division of History of Chemistry of the American Chemical Society in 2017.\n\nSome controversies have remained about the intellectual and practical contributions of other scientists to Mullis' work, and whether he had been the sole inventor of the PCR principle (see below).\n\nAt the core of the PCR method is the use of a suitable DNA polymerase able to withstand the high temperatures of > required for separation of the two DNA strands in the DNA double helix after each replication cycle. The DNA polymerases initially employed for in vitro experiments presaging PCR were unable to withstand these high temperatures. So the early procedures for DNA replication were very inefficient and time-consuming, and required large amounts of DNA polymerase and continuous handling throughout the process.\n\nThe discovery in 1976 of Taq polymerase—a DNA polymerase purified from the thermophilic bacterium, \"Thermus aquaticus\", which naturally lives in hot () environments such as hot springs—paved the way for dramatic improvements of the PCR method. The DNA polymerase isolated from \"T. aquaticus\" is stable at high temperatures remaining active even after DNA denaturation, thus obviating the need to add new DNA polymerase after each cycle. This allowed an automated thermocycler-based process for DNA amplification.\n\nThe PCR technique was patented by Kary Mullis and assigned to Cetus Corporation, where Mullis worked when he invented the technique in 1983. The \"Taq\" polymerase enzyme was also covered by patents. There have been several high-profile lawsuits related to the technique, including an unsuccessful lawsuit brought by DuPont. The pharmaceutical company Hoffmann-La Roche purchased the rights to the patents in 1992 and currently holds those that are still protected.\n\nA related patent battle over the Taq polymerase enzyme is still ongoing in several jurisdictions around the world between Roche and Promega. The legal arguments have extended beyond the lives of the original PCR and Taq polymerase patents, which expired on March 28, 2005.\n\n\n"}
{"id": "2596491", "url": "https://en.wikipedia.org/wiki?curid=2596491", "title": "Preference elicitation", "text": "Preference elicitation\n\nPreference elicitation refers to the problem of developing a decision support system capable of generating recommendations to a user, thus assisting in decision making. It is important for such a system to model user's preferences accurately, find hidden preferences and avoid redundancy. This problem is sometimes studied as a computational learning theory problem. Another approach for formulating this problem is a partially observable Markov decision process. The formulation of this problem is also dependent upon the context of the area in which it is studied.\n\nWith the explosion of on-line information new opportunities for finding and using electronic data have been generated, these changes have also brought the task of eliciting useful information to the forefront. Researchers as well as major online catalog companies have come up with algorithms and prototypes of systems that can aid a user to be able to navigate through a complex and huge information space using some information from the user in the form of answers to certain queries or ratings to certain items etc. depending upon the domain of the information space.\n\n\n"}
{"id": "2222150", "url": "https://en.wikipedia.org/wiki?curid=2222150", "title": "Promega", "text": "Promega\n\nPromega Corporation is a manufacturer of enzymes and other products for biotechnology and molecular biology with a portfolio covering the fields of genomics, protein analysis and expression, cellular analysis, drug discovery and genetic identity.\n\nPromega Corporation was founded by Bill Linton in 1978 to provide restriction enzymes for biotechnology. The company now offers more than 3,000 life science products used by scientists, researchers and life science and pharmaceutical companies. Promega has 1,200 employees, including nearly 700 at the headquarters location in Fitchburg, WI. In 2011, sales were approximately $300 million.\n\nThe privately held company has branch offices in 15 countries and more than 50 global distributors serving 100 countries. Promega Corporation also established the first biotechnology joint venture in China (Sino-American Biotechnology Co. in 1985).\n\nThe company has developed an on-site stocking system, which uses radio frequency identification (RFID) linked to the internet to track and manage remote inventory. This resulted in the spin-off company Terso Solutions that specializes in the design and manufacture of small RFID storage units.\n\nThe company's portfolio began with products for genomics researchers and now includes cloning systems, luciferase reporters, and amplification products as well as the original restriction and modifying enzymes. The portfolio of amplification products includes the GoTaq family of polymerases and buffers and the Plexor quantitative PCR system.\n\nThe company is one of two main suppliers of systems for genetic identification based on DNA analysis using short tandem repeats (STRs). Promega was the first company to provide kits for STR analysis of single loci. Along with Applied Biosystems, Promega participated with the FBI and other crime labs in validating STR loci that would eventually be selected as the core loci for the COmbined DNA Index System (CODIS), used for forensic DNA testing in North America.\n\nThe Promega PowerPlex STR systems were the first commercially available systems for STR analysis that contained all of the CODIS loci.\n\nThe company was an early supplier in the cell-free protein synthesis field and is continuing to develop its portfolio in this area.\n\nPromega offers a range of products for cell biology and drug discovery, many of which are built upon bioluminescence technology. Assays for drug discovery are used globally and include biochemical and cell-based assays. In 2010, Promega launched a custom assay services business for biologics and small molecule drug development.\n\nThe company's bioluminescence assays, DNA and RNA purification chemistries, and HaloTag technologies integrate with the high-throughput automated systems found in many laboratories. Some of this integration has occurred through collaboration with instrument manufacturers.\n\nThe company also sells their own Maxwell RSC and Maxwell RSC 48 Systems, bench-top automated purification systems for low and middle throughput research and diagnostic laboratories.\n\nPromega GloMax Luminometers are supplied with preinstalled protocols that allow researchers to perform multiplex bioluminescent assays. The luminometers with injection systems are available for use with dual-reporter assays like the Dual-Luciferase systems.\nThe Y-Chromosome Deletion Detection System from Promega also carries the CE Mark for use as an in vitro diagnostic device in the European Union.\n\n"}
{"id": "1049596", "url": "https://en.wikipedia.org/wiki?curid=1049596", "title": "Proton-exchange membrane fuel cell", "text": "Proton-exchange membrane fuel cell\n\nProton-exchange membrane fuel cells, also known as polymer electrolyte membrane (PEM) fuel cells (PEMFC), are a type of fuel cell being developed mainly for transport applications, as well as for stationary fuel-cell applications and portable fuel-cell applications. Their distinguishing features include lower temperature/pressure ranges (50 to 100 °C) and a special proton-conducting polymer electrolyte membrane. PEMFCs generate electricity and operate on the opposite principle to PEM electrolysis, which consumes electricity. They are a leading candidate to replace the aging alkaline fuel-cell technology, which was used in the Space Shuttle.\n\nPEMFCs are built out of membrane electrode assemblies (MEA) which include the electrodes, electrolyte, catalyst, and gas diffusion layers. An ink of catalyst, carbon, and electrode are sprayed or painted onto the solid electrolyte and carbon paper is hot pressed on either side to protect the inside of the cell and also act as electrodes. The pivotal part of the cell is the triple phase boundary (TPB) where the electrolyte, catalyst, and reactants mix and thus where the cell reactions actually occur. Importantly, the membrane must not be electrically conductive so the half reactions do not mix. Operating temperatures above 100 °C are desired so the water byproduct becomes steam and water management becomes less critical in cell design.\n\nA proton exchange membrane fuel cell transforms the chemical energy liberated during the electrochemical reaction of hydrogen and oxygen to electrical energy, as opposed to the direct combustion of hydrogen and oxygen gases to produce thermal energy.\n\nA stream of hydrogen is delivered to the anode side of the MEA. At the anode side it is catalytically split into protons and electrons. This oxidation half-cell reaction or hydrogen oxidation reaction (HOR) is represented by:<br>\nAt the anode: \nThe newly formed protons permeate through the polymer electrolyte membrane to the cathode side. The electrons travel along an external load circuit to the cathode side of the MEA, thus creating the current output of the fuel cell.\nMeanwhile, a stream of oxygen is delivered to the cathode side of the MEA. At the cathode side oxygen molecules react with the protons permeating through the polymer electrolyte membrane and the electrons arriving through the external circuit to form water molecules. This reduction half-cell reaction or oxygen reduction reaction (ORR) is represented by:\n\nAt the cathode: \n<br>\nOverall reaction: \n\nThe reversible reaction is expressed in the equation and shows the reincorporation of the hydrogen protons and electrons together with the oxygen molecule and the formation of one water molecule. The potentials in each case are given with respect to the standard hydrogen electrode.\n\nTo function, the membrane must conduct hydrogen ions (protons) but not electrons as this would in effect \"short circuit\" the fuel cell. The membrane must also not allow either gas to pass to the other side of the cell, a problem known as gas crossover. Finally, the membrane must be resistant to the reducing environment at the cathode as well as the harsh oxidative environment at the anode.\n\nSplitting of the hydrogen molecule is relatively easy by using a platinum catalyst. Unfortunately however, splitting the oxygen molecule is more difficult, and this causes significant electric losses. An appropriate catalyst material for this process has not been discovered, and platinum is the best option.\n\nA cheaper alternative to platinum is Cerium(IV) oxide catalyst used by the research group of professor Vladimír Matolín in the development of PEMFC.\n\nThe PEMFC is a prime candidate for vehicle and other mobile applications of all sizes down to mobile phones, because of its compactness. However, the water management is crucial to performance: too much water will flood the membrane, too little will dry it; in both cases, power output will drop. Water management is a very difficult subject in PEM systems, primarily because water in the membrane is attracted toward the cathode of the cell through polarization. A wide variety of solutions for managing the water exist including integration of electroosmotic pumps. Furthermore, the platinum catalyst on the membrane is easily poisoned by carbon monoxide (no more than one part per million is usually acceptable) and the membrane is sensitive to things like metal ions, which can be introduced by corrosion of metallic bipolar plates, metallic components in the fuel cell system or from contaminants in the fuel/oxidant.\n\nPEM systems that use reformed methanol were proposed, as in Daimler Chrysler Necar 5; reforming methanol, i.e. making it react to obtain hydrogen, is however a very complicated process, that requires also purification from the carbon monoxide the reaction produces. A platinum-ruthenium catalyst is necessary as some carbon monoxide will unavoidably reach the membrane. The level should not exceed 10 parts per million. Furthermore, the start-up times of such a reformer reactor are of about half an hour. Alternatively, methanol, and some other biofuels can be fed to a PEM fuel cell directly without being reformed, thus making a direct methanol fuel cell (DMFC). These devices operate with limited success.\n\nThe most commonly used membrane is Nafion by Chemours, which relies on liquid water humidification of the membrane to transport protons. This implies that it is not feasible to use temperatures above 80 to 90 °C, since the membrane would dry. Other, more recent membrane types, based on polybenzimidazole (PBI) or phosphoric acid, can reach up to 220 °C without using any water management: higher temperature allow for better efficiencies, power densities, ease of cooling (because of larger allowable temperature differences), reduced sensitivity to carbon monoxide poisoning and better controllability (because of absence of water management issues in the membrane); however, these recent types are not as common. PBI can be doped with phosphoric or sulfuric acid and the conductivity scales with amount of doping and temperature. At high temperatures, it is difficult to keep Nafion hydrated, but this acid doped material does not use water as a medium for proton conduction. It also exhibits better mechanical properties, higher strength, than Nafion and is cheaper. However, acid leaching is a considerable issue and processing, mixing with catalyst to form ink, has proved tricky. Aromatic polymers, such as PEEK, are far cheaper than Teflon (PTFE and backbone of Nafion) and their polar character leads to hydration that is less temperature dependent than Nafion. However, PEEK is far less ionically conductive than Nafion and thus is a less favorable electrolyte choice. Recently, protic ionic liquids and protic organic ionic plastic crystals have been shown as promising alternative electrolyte materials for high temperature (100–200 °C) PEMFCs.\n\nAn electrode typically consists of carbon support, Pt particles, Nafion ionomer, and/or Teflon binder. The carbon support functions as an electrical conductor; the Pt particles are reaction sites; the ionomer provides paths for proton conduction, and the Teflon binder increases the hydrophobicity of the electrode to minimize potential flooding. In order to enable the electrochemical reactions at the electrodes, protons, electrons and the reactant gases (hydrogen or oxygen) must gain access to the surface of the catalyst in the electrodes, while the product water, which can be in either liquid or gaseous phase, or both phases, must be able to permeate from the catalyst to the gas outlet. These properties are typically realized by porous composites of polymer electrolyte binder (ionomer) and catalyst nanoparticles supported on carbon particles. Typically platinum is used as the catalyst for the electrochemical reactions at the anode and cathode, while nanoparticles realize high surface to weight ratios (as further described below) reducing the amount of the costly platinum. The polymer electrolyte binder provides the ionic conductivity, while the carbon support of the catalyst improves the electric conductivity and enables low platinum metal loading. The electric conductivity in the composite electrodes is typically more than 40 times higher as the proton conductivity.\n\nThe GDL electrically connects the catalyst and current collector. It must be porous, electrically conductive, and thin. The reactants must be able to reach the catalyst, but conductivity and porosity can act as opposing forces. Optimally, the GDL should be composed of about one third Nafion or 15% PTFE. The carbon particles used in the GDL can be larger than those employed in the catalyst because surface area is not the most important variable in this layer. GDL should be around 15-35 µm thick to balance needed porosity with mechanical strength. Often, an intermediate porous layer is added between the GDL and catalyst layer to ease the transitions between the large pores in the GDL and small porosity in the catalyst layer. Since a primary function of the GDL is to help remove water, a product, flooding can occur when water effectively blocks the GDL. This limits the reactants ability to access the catalyst and significantly decreases performance. Teflon can be coated onto the GDL to limit the possibility of flooding. Several microscopic variables are analyzed in the GDLS such as: porosity, tortuosity and permeability. These variables have incidence over the behavior of the fuel cells. Two major GDL substrates, carbon cloth and paper, were compared, indicative of different cell performances.\n\nThe maximal theoretical efficiency applying the Gibbs free energy equation ΔG=-237.13 kJ/mol and using the heating value of Hydrogen (ΔH=-285.84 kJ/mol) is 83% at 298 K.\nformula_1\n\nThe practical efficiency of a PEM's is in the range of 50–60% .\nMain factors that create losses are:\n\nMetal-organic frameworks (MOFs) are a relatively new class of porous, highly crystalline materials that consist of metal nodes connected by organic linkers. Due to the simplicity of manipulating or substituting the metal centers and ligands, there are a virtually limitless number of possible combinations, which is attractive from a design standpoint. MOFs exhibit many unique properties due to their tunable pore sizes, thermal stability, high volume capacities, large surface areas, and desirable electrochemical characteristics. Among their many diverse uses, MOFs are promising candidates for clean energy applications such as hydrogen storage, gas separations, supercapacitors, Li-ion batteries, solar cells, and fuel cells. Within the field of fuel cell research, MOFs are being studied as potential electrolyte materials and electrode catalysts that could someday replace traditional polymer membranes and Pt catalysts, respectively.\n\nAs electrolyte materials, the inclusion of MOFs seems at first counter-intuitive. Fuel cell membranes generally have low porosity to prevent fuel crossover and loss of voltage between the anode and cathode. Additionally, membranes tend to have low crystallinity because the transport of ions is more favorable in disordered materials. On the other hand, pores can be filled with additional ion carriers that ultimately enhance the ionic conductivity of the system and high crystallinity makes the design process less complex.\n\nThe general requirements of a good electrolyte for PEMFCs are: high proton conductivity (>10 S/cm for practical applications) to enable proton transport between electrodes, good chemical and thermal stability under fuel cell operating conditions (environmental humidity, variable temperatures, resistance to poisonous species, etc.), low cost, ability to be processed into thin-films, and overall compatibility with other cell components. While polymeric materials are currently the preferred choice of proton-conducting membrane, they require humidification for adequate performance and can sometimes physically degrade due to hydrations effects, thereby causing losses of efficiency. As mentioned, Nafion is also limited by a dehydration temperature of < 100 °C, which can lead to slower reaction kinetics, poor cost efficiency, and CO poisoning of Pt electrode catalysts. Conversely, MOFs have shown encouraging proton conductivities in both low and high temperature regimes as well as over a wide range of humidity conditions. Below 100 °C and under hydration, the presence of hydrogen bonding and solvent water molecules aid in proton transport, whereas anhydrous conditions are suitable for temperatures above 100 °C. MOFs also have the distinct advantage of exhibiting proton conductivity by the framework itself in addition to the inclusion of charge carries (i.e., water, acids, etc.) into their pores.\n\nA low temperature example is work by Kitagawa, et al. who used a two-dimensional oxalate-bridged anionic layer framework as the host and introduced ammonium cations and adipic acid molecules into the pores to increase proton concentration. The result was one of the first instances of a MOF showing “superprotonic” conductivity (8 x 10 S/cm) at 25 °C and 98% relative humidity (RH). They later found that increasing the hydrophilic nature of the cations introduced into the pores could enhance proton conductivity even more. In this low temperature regime that is dependent on degree of hydration, it has also been shown that proton conductivity is heavily dependent on humidity levels.\n\nA high temperature anhydrous example is PCMOF2, which consists of sodium ions coordinated to a trisulfonated benzene derivative. To improve performance and allow for higher operating temperatures, water can be replaced as the proton carrier by less volatile imidazole or triazole molecules within the pores. The maximum temperature achieved was 150 °C with an optimum conductivity of 5 x 10 S/cm, which is lower than other current electrolyte membranes. However, this model holds promise for its temperature regime, anhydrous conditions, and ability to control the quantity of guest molecules within the pores, all of which allowed for the tunability of proton conductivity. Additionally, the triazole-loaded PCMOF2 was incorporated into a H/air membrane-electrode assembly and achieved an open circuit voltage of 1.18 V at 100 °C that was stable for 72 hours and managed to remain gas tight throughout testing. This was the first instance that proved MOFs could actually be implemented into functioning fuel cells, and the moderate potential difference showed that fuel crossover due to porosity was not an issue.\n\nTo date, the highest proton conductivity achieved for a MOF electrolyte is 4.2 x 10 S/cm at 25 °C under humid conditions (98% RH), which is competitive with Nafion. Some recent experiments have even successfully produced thin-film MOF membranes instead of the traditional bulk samples or single crystals, which is crucial for their industrial applicability. Once MOFs are able to consistently achieve sufficient conductivity levels, mechanical strength, water stability, and simple processing, they have the potential to play an important role in PEMFCs in the near future.\n\nMOFs have also been targeted as potential replacements of platinum group metal (PGM) materials for electrode catalysts, although this research is still in the early stages of development. In PEMFCs, the oxygen reduction reaction (ORR) at the Pt cathode is significantly slower than the fuel oxidation reaction at the anode, and thus non-PGM and metal-free catalysts are being investigated as alternatives. The high volumetric density, large pore surface areas, and openness of metal-ion sites in MOFs make them ideal candidates for catalyst precursors. Despite promising catalytic abilities, the durability of these proposed MOF-based catalysts is currently less than desirable and the ORR mechanism in this context is still not completely understood.\n\nMuch of the current research on catalysts for PEM fuel cells can be classified as having one of following main objectives:\n\n\nExamples of these approaches are given in the following sections.\n\nAs mentioned above, platinum is by far the most effective element used for PEM fuel cell catalysts, and nearly all current PEM fuel cells use platinum particles on porous carbon supports to catalyze both hydrogen oxidation and oxygen reduction. However, due to their high cost, current Pt/C catalysts are not feasible for commercialization. The U.S. Department of Energy estimates that platinum-based catalysts will need to use roughly four times less platinum than is used in current PEM fuel cell designs in order to represent a realistic alternative to internal combustion engines. Consequently, one main goal of catalyst design for PEM fuel cells is to increase the catalytic activity of platinum by a factor of four so that only one-fourth as much of the precious metal is necessary to achieve similar performance.\n\nOne method of increasing the performance of platinum catalysts is to optimize the size and shape of the platinum particles. Decreasing the particles’ size alone increases the total surface area of catalyst available to participate in reactions per volume of platinum used, but recent studies have demonstrated additional ways to make further improvements to catalytic performance. For example, one study reports that high-index facets of platinum nanoparticles (that is Miller indexes with large integers, such as Pt (730)) provide a greater density of reactive sites for oxygen reduction than typical platinum nanoparticles.\n\nSince the most common and effective catalyst, platinum, is extremely expensive, alternative processing is necessary to maximize surface area and minimize loading. Deposition of nanosized Pt particles onto carbon powder (Pt/C) provides a large Pt surface area while the carbon allows for electrical connection between the catalyst and the rest of the cell. Platinum is so effective because it has high activity and bonds to the hydrogen just strongly enough to facilitate electron transfer but not inhibit the hydrogen from continuing to move around the cell. However, platinum is less active in the cathode oxygen reduction reaction. This necessitates the use of more platinum, increasing the cell’s expense and thus feasibility. Many potential catalyst choices are ruled out because of the extreme acidity of the cell.\n\nThe most effective ways of achieving the nanoscale Pt on carbon powder, which is currently the best option, are through vacuum deposition, sputtering, and electrodeposition. The platinum particles are deposited onto carbon paper that is permeated with PTFE. However, there is an optimal thinness to this catalyst layer, which limits the lower cost limit. Below 4 nm, Pt will form islands on the paper, limiting its activity. Above this thickness, the Pt will coat the carbon and be an effective catalyst. To further complicate things, Nafion cannot be infiltrated beyond 10 um, so using more Pt than this is an unnecessary expense. Thus the amount and shape of the catalyst is limited by the constraints of other materials.\n\nA second method of increasing the catalytic activity of platinum is to alloy it with other metals. For example, it was recently shown that the PtNi(111) surface has a higher oxygen reduction activity than pure Pt(111) by a factor of ten. The authors attribute this dramatic performance increase to modifications to the electronic structure of the surface, reducing its tendency to bond to oxygen-containing ionic species present in PEM fuel cells and hence increasing the number of available sites for oxygen adsorption and reduction.\n\nFurther efficiencies can be realized using an Ultrasonic nozzle to apply the platinum catalyst to the electrolyte layer or to carbon paper under atmospheric conditions resulting in high efficiency spray. Studies have shown that due to the uniform size of the droplets created by this type of spray, due to the high transfer efficiency of the technology, due to the non-clogging nature of the nozzle and finally due to the fact that the ultrasonic energy de-agglomerates the suspension just before atomization, fuel cells MEA's manufactured this way have a greater homogeneity in the final MEA, and the gas flow through the cell is more uniform, maximizing the efficiency of the platinum in the MEA.\nRecent studies using inkjet printing to deposit the catalyst over the membrane have also shown high catalyst utilization due to the reduced thickness of the deposited catalyst layers.\nVery recently, a new class of ORR electrocatalysts have been introduced in the case of Pt-M (M-Fe and Co) systems with an ordered intermetallic core encapsulated within a Pt-rich shell. These intermetallic core-shell (IMCS) nanocatalysts were found to exhibit an enhanced activity and most importantly, an extended durability compared to many previous designs. While the observed enhancement in the activities is ascribed to a strained lattice, the authors report that their findings on the degradation kinetics establish that the extended catalytic durability is attributable to a sustained atomic order.\n\nAnother practical approach is through multi-layer configuration catalyst layer. This idea was promoted by the highly spatial variation of reaction rate through the catalyst layer, which is measured by a new dimensionless parameter, h.\n\nThe other popular approach to improving catalyst performance is to reduce its sensitivity to impurities in the fuel source, especially carbon monoxide (CO). Presently, pure hydrogen gas is becoming economical to mass-produce by electrolysis. However, at the moment hydrogen gas is produced by steam reforming light hydrocarbons, a process which produces a mixture of gases that also contains CO (1–3%), CO (19–25%), and N (25%). Even tens of parts per million of CO can poison a pure platinum catalyst, so increasing platinum’s resistance to CO is an active area of research.\n\nFor example, one study reported that cube-shaped platinum nanoparticles with (100) facets displayed a fourfold increase in oxygen reduction activity compared to randomly faceted platinum nanoparticles of similar size. The authors concluded that the (111) facets of the randomly shaped nanoparticles bonded more strongly to sulfate ions than the (100) facets, reducing the number of catalytic sites open to oxygen molecules. The nanocubes they synthesized, in contrast, had almost exclusively (100) facets, which are known to interact with sulfate more weakly. As a result, a greater fraction of the surface area of those particles was available for the reduction of oxygen, boosting the catalyst’s oxygen reduction activity.\n\nIn addition, researchers have been investigating ways of reducing the CO content of hydrogen fuel before it enters a fuel cell as a possible way to avoid poisoning the catalysts. One recent study revealed that ruthenium-platinum core–shell nanoparticles are particularly effective at oxidizing CO to form CO, a much less harmful fuel contaminant. The mechanism that produces this effect is conceptually similar to that described for PtNi above: the ruthenium core of the particle alters the electronic structure of the platinum surface, rendering it better able to catalyze the oxidation of CO.\n\nThe challenge for the viability of PEM fuel cells today still remains in their cost and stability. The high cost can in large part be attributed to the use of the precious metal of platinum in the catalyst layer of PEM cells. The electrocatalyst currently accounts for nearly half of the fuel cell stack cost. Although the Pt loading of PEM fuel cells has been reduced by two orders of magnitude over the past decade, further reduction is necessary to make the technology economically viable for commercialization. Whereas some research efforts aim to address this issue by improving the electrocatalytic activity of Pt-based catalysts, an alternative is to eliminate the use of Pt altogether by developing a non-platinum-group-metal (non-PGM) cathode catalyst whose performance rivals that of Pt-based technologies. The U.S. Department of Energy has been setting milestones for the development of fuel cells, targeting a durability of 5000 hours and a non-PGM catalyst ORR volumetric activity of 300 A cm \n\nPromising alternatives to Pt-based catalysts are Metal/Nitrogen/ Carbon-catalysts (M/N/C-catalysts). To achieve high power density, or output of power over surface area of the cell, a volumetric activity of at least 1/10 that of Pt-based catalysts must be met, along with good mass transport properties. While M/N/C-catalysts still demonstrate poorer volumetric activities than Pt-based catalysts, the reduced costs of such catalysts allows for greater loading to compensate. However, increasing the loading of M/N/C-catalysts also renders the catalytic layer thicker, impairing its mass transport properties. In other words, H, O, protons, and electrons have greater difficulty in migrating through the catalytic layer, decreasing the voltage output of the cell. While high microporosity of the M/N/C catalytic network results in high volumetric activity, improved mass transport properties are instead associated to macroporosity of the network. These M/N/C materials are synthesized using high temperature pyrolysis and other high temperature treatments of precursors containing the metal, nitrogen, and carbon.\n\nRecently, researchers have developed a Fe/N/C catalyst derived from iron (II) acetate (FeAc), phenanthroline (Phen), and a metal-organic-framework (MOF) host. The MOF is a Zn(II) zeolitic imidazolate framework (ZIF) called ZIF-8, which demonstrates a high microporous surface area and high nitrogen content conducive to ORR activity. The power density of the FeAc/Phen/ZIF-8-catalyst was found to be 0.75 W cm at 0.6 V. This value is a significant improvement over the maximal 0.37 W cm power density of previous M/N/C-catalysts and is much closer to matching the typical value of 1-1.2 W cm for Pt-based catalysts with a Pt loading of 0.3 mg cm. The catalyst also demonstrated a volumetric activity of 230 A cm, the highest value for non-PGM catalysts to date, approaching the U.S. Department of Energy milestone.\n\nWhile the power density achieved by the novel FeAc/Phen/ZIF-8-catalyst is promising, its durability remains inadequate for commercial application. It is reported that the best durability exhibited by this catalyst still had a 15% drop in current density over 100 hours in H/air. Hence while the Fe-based non-PGM catalysts rival Pt-based catalysts in their electrocatalytic activity, there is still much work to be done in understanding their degradation mechanisms and improving their durability.\n\nThe major application of PEM fuel cells focuses on transportation primarily because of their potential impact on the environment, e.g. the control of emission of the green house gases (GHG). Other applications include distributed/stationary and portable power generation. Most major motor companies work solely on PEM fuel cells due to their high power density and excellent dynamic characteristics as compared with other types of fuel cells.\nDue to their light weight, PEMFCs are most suited for transportation applications. PEMFCs for buses, which use compressed hydrogen for fuel, can operate at up to 40% efficiency. Generally PEMFCs are implemented on buses over smaller cars because of the available volume to house the system and store the fuel. Technical issues for transportation involve incorporation of PEMs into current vehicle technology and updating energy systems. Full fuel cell vehicles are not advantageous if hydrogen is sourced from fossil fuels; however, they become beneficial when implemented as hybrids. There is potential for PEMFCs to be used for stationary power generation, where they provide 5 kW at 30% efficiency; however, they run into competition with other types of fuel cells, mainly SOFCs and MCFCs. Whereas PEMFCs generally require high purity hydrogen for operation, other fuel cell types can run on methane and are thus more flexible systems. Therefore, PEMFCs are best for small scale systems until economically scalable pure hydrogen is available. Furthermore, PEMFCs have the possibility of replacing batteries for portable electronics, though integration of the hydrogen supply is a technical challenge particularly without a convenient location to store it within the device.\n\nBefore the invention of PEM fuel cells, existing fuel cell types such as solid-oxide fuel cells were only applied in extreme conditions. Such fuel cells also required very expensive materials and could only be used for stationary applications due to their size. These issues were addressed by the PEM fuel cell. The PEM fuel cell was invented in the early 1960s by Willard Thomas Grubb and Leonard Niedrach of General Electric. Initially, sulfonated polystyrene membranes were used for electrolytes, but they were replaced in 1966 by Nafion ionomer, which proved to be superior in performance and durability to sulfonated polystyrene.\n\nPEM fuel cells were used in the NASA Gemini series of spacecraft, but they were replaced by Alkaline fuel cells in the Apollo program and in the Space shuttle.\n\nParallel with Pratt and Whitney Aircraft, General Electric developed the first proton exchange membrane fuel cells (PEMFCs) for the Gemini space missions in the early 1960s. The first mission to use PEMFCs was Gemini V. However, the Apollo space missions and subsequent Apollo-Soyuz, Skylab and Space Shuttle missions used fuel cells based on Bacon's design, developed by Pratt and Whitney Aircraft.\n\nExtremely expensive materials were used and the fuel cells required very pure hydrogen and oxygen. Early fuel cells tended to require inconveniently high operating temperatures that were a problem in many applications. However, fuel cells were seen to be desirable due to the large amounts of fuel available (hydrogen and oxygen).\n\nDespite their success in space programs, fuel cell systems were limited to space missions and other special applications, where high cost could be tolerated. It was not until the late 1980s and early 1990s that fuel cells became a real option for wider application base. Several pivotal innovations, such as low platinum catalyst loading and thin film electrodes, drove the cost of fuel cells down, making development of PEMFC systems more realistic. However, there is significant debate as to whether hydrogen fuel cells will be a realistic technology for use in automobiles or other vehicles. (See hydrogen economy.) A large part of PEMFC production is for the Toyota Mirai. The US Department of Energy estimates a 2016 price at $53/kW if 500,000 units per year were made.\n\nIn the U.S, the major companies are Plug Power located in upstate New York and Nuvera Fuel Cells in Massachusetts. Ford Motor and General Motor also have significant amount of fuel cell activities.\n\nIn Canada, Ballard is the largest followed by Hydrogenics.\n\nIn Europe, there are ElringKlinger in Germany and PowerCell in Sweden.\n\nIn China, companies include Re-fire, SinoHytec, Horizon, Bing Energy, and Zehe Energy.\n\n\n"}
{"id": "15530023", "url": "https://en.wikipedia.org/wiki?curid=15530023", "title": "QFX (file format)", "text": "QFX (file format)\n\nQFX is Intuit's proprietary version of the standard OFX financial interchange file format. QFX is used in Intuit's \"Web Connect\" and \"Direct Connect\" features to transmit and receive financial information over the internet.\n\nA QFX file is a standard OFX file with additional fields to support a licensing fee paid by institutions to Intuit. In contrast, the standard OFX format is a free and open standard. Intuit's Quicken software will only import QFX files where the providing institution has paid the fee and in some cases passed quality tests, otherwise giving the error message \"Quicken is currently unable to verify the financial institution information for this download\". The same error message is given for Quicken users attempting to import previously downloaded QFX files with non-supported versions of Quicken (i.e. Quicken 2011 and earlier).\n\n"}
{"id": "3568704", "url": "https://en.wikipedia.org/wiki?curid=3568704", "title": "Radio receiver design", "text": "Radio receiver design\n\nRadio receiver design includes the electronic design of different components of a radio receiver which processes the radio frequency signal from an antenna in order to produce usable information such as audio. The complexity of a modern receiver and the possible range of circuitry and methods employed are more generally covered in electronics and communications engineering. The term \"radio receiver\" is understood in this article to mean any device which is intended to receive a radio signal in order to generate useful information from the signal, most notably a recreation of the so-called baseband signal (such as audio) which modulated the radio signal at the time of transmission in a communications or broadcast system.\n\nDesign of a radio receiver must consider several fundamental criteria to produce a practical result. The main criteria are gain, selectivity, sensitivity, and stability. The receiver must contain a detector to recover the information initially impressed on the radio carrier signal, a process called modulation. \n\nGain is required because the signal intercepted by an antenna will have a very low power level, on the order of femtowatts. To produce an audible signal in a pair of headphones requires this signal to be amplified a trillion-fold or more. The magnitudes of the required gain are so great that the logarithmic unit decibel is preferred - a gain of 1 trillion times the power is 120 decibels, which is a value achieved by many common receivers. Gain is provided by one or more amplifier stages in a receiver design; some of the gain is applied at the radio-frequency part of the system, and the rest at the frequencies used by the recovered information (audio, video, or data signals). \n\nSelectivity is the ability to \"tune in\" to just one station of the many that may be transmitting at any given time. An adjustable bandpass filter is a typical stage of a receiver. A receiver may include several stages of bandpass filters to provide sufficient selectivity. Additionally, the receiver design must provide immunity from spurious signals that may be generated within the receiver that would interfere with the desired signal. Broadcasting transmitters in any given area are assigned frequencies so that receivers can properly select the desired transmission; this is a key factor limiting the number of transmitting stations that can operate in a given area. \n\nSensitivity is the ability to recover the signal from the background noise. Noise is generated in the path between transmitter and receiver, but is also significantly generated in the receiver's own circuits. Inherently, any circuit above absolute zero generates some random noise that adds to the desired signals. In some cases, atmospheric noise is far greater than that produced in the receiver's own circuits, but in some designs, measures such as cryogenic cooling are applied to some stages of the receiver, to prevent signals from being obscured by thermal noise. A very good receiver design may have a noise figure of only a few times the theoretical minimum for the operating temperature and desired signal bandwidth. The objective is to produce a signal-to-noise ratio of the recovered signal sufficient for the intended purpose. This ratio is also often expressed in decibels. A signal-to-noise ratio of 10 dB (signal 10 times as powerful as noise) might be usable for voice communications by experienced operators, but a receiver intended for high-fidelity music reproduction might require 50 dB or higher signal-to-noise ratio. \n\nStability is required in at least two senses. Frequency stability; the receiver must stay \"tuned\" to the incoming radio signal and must not \"drift\" with time or temperature. Additionally, the great magnitude of gain generated must be carefully controlled so that spurious emissions are not produced within the receiver. These would lead to distortion of the recovered information, or, at worst, may radiate signals that interfere with other receivers. \n\nThe detector stage recovers the information from the radio-frequency signal, and produces the sound, video, or data that was impressed on the carrier wave initially. Detectors may be as simple as an \"envelope\" detector for amplitude modulation, or may be more complex circuits for more recently developed techniques such as frequency-hopping spread spectrum. \n\nWhile not fundamental to a receiver, automatic gain control is a great convenience to the user, since it automatically compensates for changing received signal levels or different levels produced by different transmitters. \n\nMany different approaches and fundamental receiver \"block diagrams\" have developed to address these several, sometimes contradictory, factors. Once these technical objectives have been achieved, the remaining design process is still complicated by considerations of economics, patent rights, and even fashion. \n\nA crystal radio uses no active parts: it is powered only by the radio signal itself, whose detected power feeds headphones in order to be audible at all. In order to achieve even a minimal sensitivity, a crystal radio is limited to low frequencies using a large antenna (usually a long wire). It relies on detection using some sort of semiconductor diode such as the original cat's-whisker diode discovered long before the development of modern semiconductors.\nA crystal receiver is very simple and can be easy to make or even improvise, for example, the foxhole radio. However, the crystal radio needs a strong RF signal and a long antenna to operate. It displays poor selectivity since it only has one tuned circuit.\n\nThe tuned radio frequency receiver (TRF) consists of a radio frequency amplifier having one or more stages all tuned to the desired reception frequency. This is followed by a detector, typically an envelope detector using a diode, followed by audio amplification. This was developed after the invention of the triode vacuum tube, greatly improving the reception of radio signals using electronic amplification which had not previously been available. The greatly improved selectivity of the superheterodyne receiver overtook the TRF design in almost all applications, however the TRF design was still used as late as the 1960s among the cheaper \"transistor radios\" of that era.\n\nThe reflex receiver was a design from the early 20th century which consists of a single-stage TRF receiver but which used the same amplifying tube to also amplify the audio signal after it had been detected. This was in an era where each tube was a major cost (and consumer of electrical power) so that a substantial increase in the number of passive elements would be seen as preferable to including an additional tube. The design tends to be rather unstable, and is obsolete.\n\nThe regenerative receiver also had its heyday at the time where adding an active element (vacuum tube) was considered costly. In order to increase the gain of the receiver, positive feedback was used in its single RF amplifier stage; this also increased the selectivity of the receiver well beyond what would be expected from a single tuned circuit. The amount of feedback was quite critical in determining the resulting gain and had to be carefully adjusted by the radio operator. Increasing the feedback beyond a point caused the stage to oscillate at the frequency it was tuned to.\n\nSelf-oscillation reduced the quality of its reception of an AM (voice) radio signal but made it useful as a CW (Morse code) receiver. The beat signal between the oscillation and the radio signal would produce an audio \"beeping\" sound. The oscillation of the regenerative receiver could also be a source of local interference. An improved design known as the super-regenerative receiver improved the performance by allowing an oscillation to build up which was then \"quenched\", with that cycle repeating at a rapid (ultrasonic) rate. From the accompanying schematic for a practical regenerative receiver, one can appreciate its simplicity in relation to a multi-stage TRF receiver, while able to achieve the same level of amplification through the use of positive feedback.\n\nIn the Direct conversion receiver, the signals from the antenna are only tuned by a single tuned circuit before entering a mixer where they are mixed with a signal from a local oscillator which is tuned to the carrier wave frequency of the transmitted signal. This is unlike the superheterodyne design, where the local oscillator is at an offset frequency. The output of this mixer is thus audio frequency, which is passed through a low pass filter into an audio amplifier which may drive a speaker.\n\nFor receiving CW (morse code) the local oscillator is tuned to a frequency slightly different from that of the transmitter in order to turn the received signal into an audible \"beep.\"\n\n\nPractically all modern receivers are of the superheterodyne design. The RF signal from the antenna may have one stage of amplification to improve the receiver's noise figure, although at lower frequencies this is typically omitted. The RF signal enters a mixer, along with the output of the local oscillator, in order to produce a so-called intermediate frequency (IF) signal. An early optimization of the superheterodyne was to combine the local oscillator and mixer into a single stage called \"converter\". The local oscillator is tuned to a frequency somewhat higher (or lower) than the intended reception frequency so that the IF signal will be at a particular frequency where it is further amplified in a narrow-band multistage amplifier. Tuning the receiver involves changing the frequency of the local oscillator, with further processing of the signal (especially in relation to increasing the receiver) conveniently done at a single frequency (the IF frequency) thus requiring no further tuning for different stations.\n\nHere we show block diagrams for typical superheterodyne receivers for AM and FM broadcast respectively. This particular FM design uses a modern phase locked loop detector, unlike the frequency \"discriminator\" or ratio detector used in earlier FM receivers.\nFor single conversion superheterodyne AM receivers designed for medium wave (AM broadcast) the IF is commonly 455 kHz. Most superheterodyne receivers designed for broadcast FM (88 - 108 MHz) use an IF of 10.7 MHz. TV receivers often use intermediate frequencies of about 40 MHz. Some modern multiband receivers actually convert lower frequency bands first to a much higher frequency (VHF) after which a second mixer with a tunable local oscillator and a second IF stage process the signal as above.\n\nSoftware-defined radio (SDR) is a radio communication system where components, that have been traditionally implemented in hardware (e.g. mixers, filters, amplifiers, modulators/demodulators, detectors, etc.) are instead implemented by means of software on a personal computer or embedded system. While the concept of SDR is not new, the rapidly evolving capabilities of digital electronics render practical many processes which used to be only theoretically possible.\n\n\n"}
{"id": "8665001", "url": "https://en.wikipedia.org/wiki?curid=8665001", "title": "Raymarine Marine Electronics", "text": "Raymarine Marine Electronics\n\nRaymarine is a manufacturer of electronic equipment for marine use. The company targets both recreational and light commercial markets with their products, which include:\n\n\nThe company was founded by a leveraged buyout of the consumer electronics branch of Raytheon corporation. With subsequent acquisitions it is now a major supplier of marine electronics. As of May 14, 2010 Raymarine is part of FLIR Systems. The Raymarine brand has been on the market for over 80 years. Within this time, their product range has included visual navigation information equipment. At the moment, Raymarine is one of the global leaders in this niche. Their products work with performance sensors that operate along with intelligence operating systems. Raymarine has got global service network that operates in over 80 countries all over the world.\n\n\n"}
{"id": "35127635", "url": "https://en.wikipedia.org/wiki?curid=35127635", "title": "Seed-counting machine", "text": "Seed-counting machine\n\nSeed counting machines (Seed counters) count seeds for research and packaging purposes. The machines typically provide total counts of seeds, or batch sizes for packaging.\n\nTraditionally, the seed packaging industry packed seeds by weight using weighing scales.\nSeeds were packed by weight while sold as units.In order to assure the correct quantity of seeds, seed distributors added a safety margin to the packed weight.This safety margin results in loss of revenue.\nThe seed counter is a uniquely appropriate solution for items that are sold by units as opposed to weight.\n"}
{"id": "55274673", "url": "https://en.wikipedia.org/wiki?curid=55274673", "title": "Single colour reflectometry", "text": "Single colour reflectometry\n\nSingle colour reflectometry (SCORE), formerly known as imaging Reflectometric Interferometry (iRIf) and 1-lambda Reflectometry, is a physical method based on interference of monochromatic light at thin films, which is used to investigate (bio-)molecular interactions. The obtained binding curves using SCORE provide detailed information on kinetics and thermodynamics of the observed interaction(s) as well as on concentrations of the used analytes. These data can be relevant for pharmaceutical screening and drug design, biosensors and other biomedical applications, diagnostics, and cell-based assays.\n\nThe underlying principle corresponds to that of the Fabry-Pérot interferometer, which is also the underlying principle for the white-light interferometry.\n\nMonochromatic light is illuminated vertically on the rear side of a transparent multi-layer substrate. The partial beams of the monochromatic light are transmitted and reflected at each interphase of the multi-layer system. Superimposition of the reflected beams result in destructive or constructive interference (depending on wavelength of the used light and the used substrate/multi-layer system materials) that can be detected in an intensity change of the reflected light using a photodiode, CCD, or CMOS element.\n\nThe sensitive layer on top of the multi-layer system can be (bio-)chemically modified with receptor molecules, e.g. antibodies. Binding of specific ligands to the immobilised receptor molecules results in a change refractive index n and physical thickness d of the sensitive layer. The product of n and d results in the optical thickness (n*d) of the sensitive layer.\n\nMonitoring the change of the reflected intensity of the used light over time results in binding curves that provide information on:\n\nCompared to bio-layer interferometry, which monitors the change of the interference pattern of reflected white light, SCORE only monitors the intensity change of the reflected light using a photodiode, CCD, or CMOS element. Thus, it is possible to analyse not only a single interaction but high-density arrays with up to 10,000 interactions per cm.\nCompared to surface plasmon resonance (SPR), which penetration depth is limited by the evanescent field, SCORE is limited by the coherence length of the light source, which is typically a few micrometers. This is especially relevant when investigating whole cell assays. Also, SCORE (as well as BLI) is not influenced by temperature fluctuations during the measurement, while SPR needs thermostabilisation.\n\nSCORE is especially used as detection method in bio- and chemosensors. It is a label-free technique like Reflectometric interference spectroscopy (RIfS), Bio-layer Interferometry (BLI) and Surface plasmon resonance (SPR), which allows time-resolved observation of binding events on the sensor surface without the use of fluorescence or radioactive labels.\n\nThe SCORE technology is commercialised by Biametrics GmbH, a service provider and instrument manufacturer with headquarters in Tübingen, Germany.\n\n\n\n"}
{"id": "47763930", "url": "https://en.wikipedia.org/wiki?curid=47763930", "title": "Specification for human interface for semiconductor manufacturing equipment", "text": "Specification for human interface for semiconductor manufacturing equipment\n\nThis specification is usually called SEMI E95-0200 standard. It was originally published in February 2000, and the latest technical revision is SEMI E95-1101.\n\nThis standard addresses the area of processing content with the direct intention of developing common software standards, so that problems involving operator training, operation specifications, and efficient development can be resolved more easily.\n\nSemiconductor Equipment and Materials International\n"}
{"id": "47050949", "url": "https://en.wikipedia.org/wiki?curid=47050949", "title": "SportVU", "text": "SportVU\n\nSportVU is a camera system hung from the rafters that collects data at a rate of 25 times per second and follows the ball and every player on the court. SportVU provides in-depth statistics, including real-time player and ball positioning through sophisticated software and statistical algorithms. SportVU's speed, distance, and possession data provide key insights and analysis points.\n\nSportVU player tracking delivers statistical information via real-time X, Y positioning of players and X, Y, Z positioning of the ball. By leveraging this data, STATS is able to present performance metrics for players and teams to leverage.\n\nWith a foundation in soccer, STATS expanded the core SportVU technology into basketball beginning with the 2010-2011 NBA season. Today, STATS is the Official Tracking partner of the NBA. The NBA is using these statistics captured by SportVU on NBA.com and NBATV as well as in arenas across the country to provide fans with insight. SportVU statistics are also being utilized by every team in the league to support their pro analytics and aid in player development.\n\nSportVU was created in 2005 by Israeli scientists, Gal Oz and Miky Tamir, who had a background in missile tracking and advanced optical recognition. They had previously used some of that same science to track soccer matches in Israel.\n\nSportVU was featured at national trade shows NAB 2007, in Las Vegas, and International Broadcasting Convention 2007, in Amsterdam. Shortly after, in 2008, SportVU was acquired by STATS LLC, one of the largest sports statistics companies in the world. STATS would go on to center SportVU efforts on basketball. During the 2009 NBA Finals in Orlando, STATS successfully demoed their SportVU technology for NBA executives. At the start of the 2010-2011 NBA season, four teams were contracted to use SportVU, the Dallas Mavericks, Houston Rockets, Oklahoma City Thunder and San Antonio Spurs.\n\nSportVU converted their tracking system from delayed processing to real-time data delivery during the 2011-2012 NBA season. NBA analytics teams would now be able to utilize tracking data during games. At the start of the 2012-2013 season, 10 teams were using SportVU.\n\nSince the 2013-2014 NBA season, the SportVU camera system has been installed and in operation in all NBA arenas. In that same year, STATS added the ICE analytics platform to organize, display and analyze SportVU data, used by every team in the NBA. NBA team, Toronto Raptors, shared with sports blog, Grantland, their progress with the use of SportVU's new algorithms, which identify new events (touches, dribbles, and passes) and play types (drives, isolations, post ups, and ball screens). The Raptors Analytics Team was able to create a graphical user interface to play video footage of the play from the X-Y coordinates retrieved from SportVU.\n\nIn 2016, STATS and the NBA met an agreement to extend SportVU tracking data to many more media outlets around the world including ESPN, NBA on TNT, and Bleacher Report.\n\nBeginning in the 2016-2017 season, STATS would serve as France's Ligue de Football Professionnel's official data and tracking provider. STATS would use SportVU to provide various football data and statistics.\n\nAfter the NBA's full adoption of SportVU tracking technology in 2013, many statisticians and data scientists have utilized tools such as machine learning to provide more complex statistics from the tracking data. At the 10th annual MIT Sloan Sports Analytics Conference in 2016, STATS's own Director of Data Science and his team was awarded the grand prize for their contributions to a research paper concerning the prediction of shot outcomes in tennis. This was one of the many research papers completed for the MIT Sloan Sports Analytics Conference that used SportVU data, including Kirk Goldsberry's paper in 2014 concerning the prediction of points and evaluation of decision-making in basketball.\n"}
{"id": "1277505", "url": "https://en.wikipedia.org/wiki?curid=1277505", "title": "Steam explosion", "text": "Steam explosion\n\nA steam explosion is an explosion caused by violent boiling or flashing of water into steam, occurring when water is either superheated, rapidly heated by fine hot debris produced within it, or heated by the interaction of molten metals (as in a fuel–coolant interaction, or FCI, of molten nuclear-reactor fuel rods with water in a nuclear reactor core following a core-meltdown). Pressure vessels, such as pressurized water (nuclear) reactors, that operate above atmospheric pressure can also provide the conditions for a steam explosion. The water changes from a liquid to a gas with extreme speed, increasing dramatically in volume. A steam explosion sprays steam and boiling-hot water and the hot medium that heated it in all directions (if not otherwise confined, e.g. by the walls of a container), creating a danger of scalding and burning.\n\nSteam explosions are not normally chemical explosions, although a number of substances react chemically with steam (for example, zirconium and superheated graphite react with steam and air respectively to give off hydrogen, which burns violently in air) so that chemical explosions and fires may follow. Some steam explosions appear to be special kinds of boiling liquid expanding vapor explosion (BLEVE), and rely on the release of stored superheat. But many large-scale events, including foundry accidents, show evidence of an energy-release front propagating through the material (see description of FCI below), where the forces create fragments and mix the hot phase into the cold volatile one; and the rapid heat transfer at the front sustains the propagation.\n\nIf a steam explosion occurs in a confined tank of water due to rapid heating of the water, the pressure wave and rapidly expanding steam can cause severe water hammer. This was the mechanism that, in Idaho, USA, in 1961, caused the SL-1 nuclear reactor vessel to jump over in the air when it was destroyed by a criticality accident. In the case of SL-1, the fuel and fuel elements vaporized from instantaneous overheating.\n\nEvents of this general type are also possible if the fuel and fuel elements of a liquid-cooled nuclear reactor gradually melt. Such explosions are known as fuel–coolant interactions (FCI). In these events the passage of the pressure wave through the predispersed material creates flow forces which further fragment the melt, resulting in rapid heat transfer, and thus sustaining the wave. Much of the physical destruction in the Chernobyl disaster, a graphite-moderated, light-water-cooled RBMK-1000 reactor, is thought to have been due to such a steam explosion.\n\nIn a nuclear meltdown, the most severe outcome of a steam explosion is early containment failure. Two possibilities are the ejection at high pressure of molten fuel into the containment, causing rapid heating; or an in-vessel steam explosion causing ejection of a missile (such as the upper head) into, and through, the containment. Less dramatic but still significant is that the molten mass of fuel and reactor core melts through the floor of the reactor building and reaches ground water; a steam explosion might occur, but the debris would probably be contained, and would in fact, being dispersed, probably be more easily cooled. See WASH-1400 for details.\n\nSteam explosions are often encountered where hot lava meets sea water. Such an occurrence is also called a littoral explosion. A dangerous steam explosion can also be created when liquid water encounters hot, molten metal. As the water explodes into steam, it splashes the burning hot liquid metal along with it, causing an extreme risk of severe burns to anyone located nearby and creating a fire hazard.\n\nA water vapor explosion creates a high volume of gas without producing environmentally harmful leftovers. The controlled explosion of water has been used for generating steam in power stations and in modern types of steam turbines. Newer steam engines use heated oil to force drops of water to explode and create high pressure in a controlled chamber. The pressure is then used to run a turbine or a converted combustion engine. Hot oil and water explosions are becoming particularly popular in concentrated solar generators, because the water can be separated from the oil in a closed loop without any external energy. Water explosion is considered to be environmentally friendly if the heat is generated by a renewable resource.\n\nA cooking technique called flash boiling uses a small amount of water to quicken the process of boiling. For example, this technique can be used to melt a slice of cheese onto a hamburger patty. The cheese slice is placed on top of the meat on a hot surface such as a frying pan, and a small quantity of cold water is thrown onto the surface near the patty. A vessel (such as a pot or frying-pan cover) is then used to quickly seal the steam-flash reaction, dispersing much of the steamed water on the cheese and patty. This results in a large release of heat, transferred via vaporized water condensing back into a liquid (a principle also utilized in refrigerator and freezer production).\n\nHigh steam generation rates are possible under other circumstances, such as boiler-drum failure, or at a quench front (for example when water re-enters a hot dry boiler). Though potentially damaging, they are usually less energetic than events in which the hot (\"fuel\") phase is molten and so can be finely fragmented within the volatile (\"coolant\") phase. Some examples follow:\n\nSteam explosions are naturally produced by certain volcanoes, especially stratovolcanoes, and are a major cause of human fatalities in volcanic eruptions.\n\nThe 1986 Chernobyl nuclear disaster in Soviet Union was feared to cause major steam explosion (and resulting Europe-wide fallout) upon melting the lava-like nuclear fuel through the reactor's basement towards contact with residue fire-fighting water and groundwater. The threat was averted by frantic tunneling underneath the reactor in order to pump out water and reinforce underlying soil with concrete.\n\nWhen a pressurized container such as the waterside of a steam boiler ruptures, it is always followed by some degree of steam explosion. A common operating temperature and pressure for a marine boiler is around 950 P.S.I. (6.55 MPa) and 850 °F (454 °C) at the outlet of the superheater. A steam boiler has an interface of steam and water in the steam drum, which is where the water is finally evaporating due to the heat input, usually oil-fired burners. When a water tube fails due to any of a variety of reasons, it causes the water in the boiler to expand out of the opening into the furnace area that is only a few P.S.I. above atmospheric pressure. This will likely extinguish all fires and expands over the large surface area on the sides of the boiler. To decrease the likelihood of a devastating explosion, boilers have gone from the \"fire-tube\" designs, where the heat was added by passing hot gases through tubes in a body of water, to \"water-tube\" boilers that have the water inside of the tubes and the furnace area is around the tubes. Old \"fire-tube\" boilers often failed due to poor build quality or lack of maintenance (such as corrosion of the fire tubes, or fatigue of the boiler shell due to constant expansion and contraction). A failure of fire tubes forces large volumes of high pressure, high temperature steam back down the fire tubes in a fraction of a second and often blows the burners off the front of the boiler, whereas a failure of the pressure vessel surrounding the water would lead to a full and entire evacuation of the boiler's contents in a large steam explosion. On a marine boiler, this would certainly destroy the ship's propulsion plant and possibly the corresponding end of the ship.\n\nIn a more domestic setting, steam explosions can be a result of trying to extinguish burning oil with water. When oil in a pan is on fire, the natural impulse may be to extinguish it with water. However, doing so will cause the water to become superheated by the hot oil. Upon turning to steam, it will disperse upwards and outwards rapidly and violently in a spray also containing the ignited oil. It is for this reason that the correct course of action for dealing with such fires is to either use a damp cloth or a tight lid on the pan; both help deprive the fire of oxygen, and the cloth also serves to cool it down. Alternatively, a non-volatile purpose designed fire retardant agent or simply a fire blanket can be used instead.\n\n\n"}
{"id": "32804414", "url": "https://en.wikipedia.org/wiki?curid=32804414", "title": "Stock pot", "text": "Stock pot\n\nStock pot is a generic name for one of the most common types of cooking pot used worldwide. A stock pot is traditionally used to make stock or broth, which can be the basis for cooking more complex recipes. It is a wide pot with a flat bottom, straight sides, a wide opening to the full diameter of the pot, two handles on the sides, and a lid with a handle on top. \n\nFrench Chef Auguste Escoffier (1846-1935) published \"A Guide to Modern Cookery\" in 1907. On the first page, Escoffier writes, \"stocks are the keynote of culinary structure\" in French cuisine. A stock or broth is made by simmering water for several hours, to continuously cook added foods such as pieces of meat, meat bones, fish or vegetables. The slow simmering process transfers flavours, colours and nutrients to the water, where they blend, and a new ingredient is thus created, the broth or stock.\n\nA broth made with meat or meat bones creates a base with concentrated flavours and aromas, even without the addition of salt or herbs or spices. This is what is referred to as soup base. Stock pots are also used for cooking stews, porridge, boiled foods, steamed shellfish, and a vast variety of recipes.\n\nStock pots have great versatility, and so they are used for many cooking purposes, and occasionally non-cooking purposes. Large stock pots may be used at home to boil clothing, wool or yarn for colour dying, for example. They do not come in standard sizes. The size of the pot is normally given on the manufacturer's label by volume, for example 12 litres.\n\nThe most common materials for manufacturing stock pots are stainless steel, aluminium, copper and enamel (Vitreous enamel) on metal. More expensive types of stock pots have bottoms that are made of layers of different metals, to enhance heat conductivity.\n\nA recent innovation sculpts the pot sides to harness the boiling liquid into a self-stirring pot.\n\n"}
{"id": "1665223", "url": "https://en.wikipedia.org/wiki?curid=1665223", "title": "SunGard", "text": "SunGard\n\nSunGard was an American multinational company based in Wayne, Pennsylvania, which provided software and services to education, financial services, and public sector organizations. It was formed in 1983, as a spin-off of the computer services division of Sun Oil Company. The name of the company originally was an acronym which stood for Sun Guaranteed Access to Recovered Data, a reference to the disaster recovery business it helped pioneer. SunGard was ranked at 480th in the U.S. Fortune 500 list in the year 2012.\n\nIn August 2005 the company was acquired by seven private equity firms and de-listed from the NYSE.\nSunGard was one of the title sponsors of the pro cycling team until the end of 2011.\n\nIn August 2015, FIS announced that it had signed a definitive agreement to acquire SunGard.\n\nSunGard provided software and processing for financial services, K-12 and Higher Education, and the public sector. It also provided continuity-assurance and production data center hosting services, now part of SunGard AS. SunGard had offices in many parts of the world outside of the United States including Paris, South Africa, Tunis, Stockholm, and the United Kingdom.\n\nSunGard has historically grown by acquisition. Past mergers include Performance Pathways, Vericenter, InFlow, Strohl Systems, Comdisco Continuity Services, and Guardian iT PLC on the availability side, Systems & Computer Technology Corp. (SCT) on the higher education side, and Kiodex. Inc., GL Trade, Oshap (Mint, Decalog), TRAX, Carnot AG, Front Capital Systems (Front Arena), Martini, Monis, APT, Dyatron, Reech, VPM, Phase 3 Systems, Infinity, Microhedge, Reconciliation, Automated Securities Clearance India, Opus Renaissance Software Inc., and National Computer Systems Financial Systems Division on the (primarily financial) software and processing side. SunGard has completed more than 150 acquisitions over the past 20 years.\n\nAmong the largest acquisitions were:\n\n\nIn March 2014 SunGard completed the split-off of its Availability Services business, forming the independent company Sungard AS. Andy Stern continues to lead the Sungard AS business.\n\nIn August 2011 Datatel and SunGard's Higher Education group announced the definitive agreement to combine businesses and operate as one company. Datatel’s current chief executive officer, John Speer, will be the chief executive officer of the combined business. Ron Lang, the CEO of SunGard Higher Education, will continue to play an active role in the combined business as vice chairman of the board of directors.\n\nAffiliates of private equity firm Hellman & Friedman LLC will acquire the SunGard Higher Education businesses from SunGard Data Systems Inc. for an aggregate cash purchase price of US$1.775 billion and combine the acquired businesses. The combined company will operate under a new name, Ellucian, as announced at the 2012 annual users' conference.\n\nOn 7 December Hellman & Friedman, Datatel and SunGard announced that the proposed combination of Datatel and SunGard Higher Education had cleared Department of Justice review and the companies are preparing to close the transaction in the first quarter of 2012. In March 2012, the combination was announced, with the new company being called Ellucian.\n\nSunGard's total debt of US$8.08 billion will be alleviated by selling the Higher Education business line, however a projected annual revenue shortage of $580 million can be assumed, resulting in a total revenue projection for the fiscal 2011 of $4.7 billion, suggesting a negative revenue trend.\n\nIn December 2010, The Capita Group plc acquired SunGard Public Sector Holdings Ltd (part of the business servicing the UK public sector) for £86 million. The new business was known as Capita Secure Information Systems; it has now been renamed Capita Secure Managed Services.\n\nFormerly listed on the NYSE (ticker symbol SDS) on August 11, 2005 the company was acquired by a consortium of seven private equity investment firms in a transaction valued at $11.3 billion. The partners in the acquisition were Silver Lake Partners, Bain Capital, Blackstone Group, Goldman Sachs Capital Partners, Kohlberg Kravis Roberts, Providence Equity Partners, and TPG Capital.\n\n"}
{"id": "23520451", "url": "https://en.wikipedia.org/wiki?curid=23520451", "title": "Technoromanticism (book)", "text": "Technoromanticism (book)\n\nTechnoromanticism: Digital Narrative, Holism, and the Romance of the Real is a philosophical book written by Richard Coyne, published in 1999.\n\nIn \"Technoromanticism,\" Coyne shows how narratives about the computer, and high technology in general, are grounded in Enlightenment and romantic traditions. Because of these narratives grounding, discourse about technology is subject to very similar critiques of the Enlightenment and romanticism.\n\nThe plan of the book is divided into three parts: unity, multiplicity, and ineffability. In the introduction, the reader is given a summary glimpse of technoromanticism as an attempt to establish political unity through information, as an attempt to achieve techno-idealism through empirical realism, and as an attempt to achieve a digital utopia. Throughout the work he points out the shortcomings of such endeavors.\n\nIn the first part of the book, Coyne shows how IT narratives attempt to transcend the material realm. He discusses how romanticism in a neoplatonic guise provides the allure and seduction of cyberspace and technologies devoted to creating it. (p. 63)\n\nIn the second part of the book, Coyne discusses how the philosophy of empiricism is foundational to the modeling of space and time for computers.\n\nWhat a computer represents is done through coding and code consists of a symbolic language that can be used to model the world around us. However, for Lacan, what is real is what cannot be symbolized or represented. Thus, for Coyne, the computer strongly suggests the idea that there is a fundamental disconnection from reality through the lens of high technology.\n\n"}
{"id": "26461121", "url": "https://en.wikipedia.org/wiki?curid=26461121", "title": "Techné: Research in Philosophy and Technology", "text": "Techné: Research in Philosophy and Technology\n\nTechné: Research in Philosophy and Technology is a peer-reviewed academic journal with a focus on philosophical analysis of technological systems. The journal was established in 1995 as an electronic journal, under the sponsorship of the Society for Philosophy and Technology. It is published three times per year and online access to the journal is a benefit of membership in the society. The journal is published for the society by the Philosophy Documentation Center. The current editors-in-chief are Neelke Doorn and Diane P. Michelfelder.\n\n"}
{"id": "10789250", "url": "https://en.wikipedia.org/wiki?curid=10789250", "title": "Television standards conversion", "text": "Television standards conversion\n\nTelevision standards conversion is the process of changing one type of television system to another. The most common is from NTSC to PAL or the other way around. This is done so television programs in one nation may be viewed in a nation with a different standard. The video is fed through a video standards converter that changes the video to a different video system.\n\nConverting between different numbers of lines and different frame rates in video pictures is a complex technical problem. However, the international exchange of television programming makes standards conversion necessary and in many cases mandatory.\n\nThe first known case of television systems conversion was in Europe a few years after World War II, mainly with the RTF (France) and the BBC (UK) trying to exchange their 441 line and 405 line programming.\n\nThe problem got worse with the introduction of PAL, SECAM (both 625 lines), and the French 819 line service.\n\nUntil the 1980s, standards conversion was so difficult that 24 frame/s 16 mm or 35 mm film was the preferred medium of programming interchange.\nPerhaps the most technically challenging conversion to make is the PAL to NTSC.\n\nThe two TV standards are for all practical purposes, temporally and spatially incompatible with each other. Aside from the line count being different, converting to a format that requires 60 fields every second from a format that has only 50 fields poses difficulty. Every second, an additional 10 fields must be generated—the converter has to create new frames (from the existing input) in real time.\n\nTV contains many hidden signals. One signal type that is not transferred, except on some very expensive converters, is the closed captioning signal. Teletext signals do not need to be transferred, but the captioning data stream should be if it is technologically possible to do so.\n\nWith HDTV broadcasting, this is less of an issue, for the most part meaning only passing the captioning datastream on to the new source material. However, DVB and ATSC have significantly different captioning datastream types.\n\nInformation theory and the Nyquist–Shannon sampling theorem imply that conversion from one television standard to another will be easier providing:\n\nThe subsampling in a video system is usually expressed as a three part ratio. The three terms of the ratio are: the number of brightness (\"luminance\" \"luma\" or Y) samples, followed by the number of samples of the two color (\"chroma\") components: U/Cb then V/Cr, for each complete sample area.\n\nFor quality comparison, only the ratio between those values is important, so 4:4:4 could easily be called 1:1:1; however, traditionally the value for brightness is always 4, with the rest of the values scaled accordingly.\n\nThe sampling principles above apply to both digital and analog television.\n\nThe \"3:2 pulldown\" conversion process for 24 frame/s film to television (telecine) creates a slight error in the video signal compared to the original film frames. This is one reason why NTSC films viewed on typical home equipment may not appear as smooth as when viewed in a cinema. The phenomenon is particularly apparent during slow, steady camera movements which appear slightly jerky when telecined. This process is commonly referred to as telecine judder.\n\nPAL material in which 2:2:2:2:2:2:2:2:2:2:2:3 pulldown has been applied, suffers from a similar lack of smoothness, though this effect is not usually called telecine judder.\n\nIn effect, every 12th film frame is displayed for the duration of 3 PAL fields (60 milliseconds) whereas the other 11 frames are all displayed for the duration of 2 PAL fields (40 milliseconds). This causes a slight \"hiccup\" in the video about twice a second.\n\nTelevision systems converters must avoid creating telecine judder effects during the conversion process. Avoiding this judder is of economic importance as a substantial amount of NTSC (60 Hz, technically 29.97 frame/s) resolution material that originates from film – will have this problem when converted to PAL or SECAM (both 50 Hz, 25 frame/s).\n\nThis method was used by Ireland to convert 625 line service to 405 line service. It is perhaps the most basic television standard conversion technique. RTÉ used this method during the latter years of its use of the 405 line system.\n\nA standards converter was used to provide the 405 line service, but according to more than one former RTÉ engineering source the converter blew up and afterwards the 405 line service was provided by a 405 line camera pointing at a monitor. This is not the best conversion technique but it can work if one is going from a higher resolution to a lower one – at the same frame rate. Slow phosphors are required on both orthicons.\nThe first video standards converters were analog. That is, a special professional video camera that used a video camera tube would be pointed at a Cathode ray tube video monitor. Both the camera and the monitor could be switched to either NTSC or PAL, to convert both ways. Robert Bosch GmbH's Fernseh Division made a large three rack analog video standards converter. These were the high end converters of the 1960s and 1970s. Image Transform in Universal City, California, used the Fernseh converter and in the 1980s made their own custom digital converter. This was also a larger three-rack device. As digital memory size became larger in smaller packages, converters became the size of a microwave oven. Today one can buy a very small consumer converter for home use.\n\nThe Apollo moon missions (late 1960s, early 1970s) used SSTV as opposed to normal bandwidth television; this was mostly done to save battery power (and transmission bandwidth, since the SSTV video from the Apollo missions was multiplexed with all other voice and telemetry communications from the spacecraft). The camera used only 7 watts of power.\n\nSSTV was used to transmit images from inside Apollo 7, Apollo 8, and Apollo 9, as well as the Apollo 11 Lunar Module television from the Moon; see Apollo TV camera.\n\nLater Apollo missions featured color field sequential cameras that output 60-frame/s video. Each frame corresponded to one of the RGB primary colors. This method is compatible with black and white NTSC, but incompatible with color NTSC. In fact, even NTSC monochrome TV compatibility is marginal. A monochrome set could have reproduced the pictures, but the pictures would have flickered terribly. The camera color video ran at only 10  frame/s. Also, Doppler shift in the lunar signal would have caused pictures to tear and flip. For these reasons, the Apollo moon pictures required special conversion techniques.\n\nThe conversion steps were completely electromechanical, and they took place in nearly real time. First, the downlink station corrected the pictures for Doppler shift. Next, in an analog disc recorder, the downlink station recorded and replayed every video field six times. On the six-track recorder, recording and playback took place simultaneously. After the recorder, analog video processors added the missing components of the NTSC color signal: These components included:\n\nThe conversion delay lasted only some 10 seconds. Then color moon pictures left the downlink station for world distribution.\n\nThis conversion technique may become popular with manufacturers of HDTV --> NTSC and HDTV --> PAL converter boxes for the ongoing global conversion to HDTV.\n\nIn a typical image transmission setup, all stationary images are transmitted at full resolution. Moving pictures possess a lower resolution visually, based on complexity of interframe image content.\n\nWhen one uses Nyquist subsampling as a standards conversion technique, the horizontal and vertical resolution of the material are reduced – this is an excellent method for converting HDTV to standard definition television, but it works very poorly in reverse.\n\nThe Nyquist subsampling method of systems conversion only works for HDTV to Standard Definition Television, so as a standards conversion technology it has a very limited use. Phase Correlation is usually preferred for HDTV to standard definition conversion.\n\nThere is a large difference in frame rate between film (24.0 frames per second) and NTSC (approximately 29.97 frames per second). Unlike the two other most common video formats, PAL and SECAM, this difference cannot be overcome by a simple speed-up, because the required 25% speed-up would be clearly noticeable.\n\nTo convert 24 frame/s film to 29.97 frame/s (presented as 59.94 interlaced fields per second) NTSC, a complex process called \"\" is used, in which every other film frame is duplicated across an additional interlaced field to achieve a framerate of 23.976 (the audio is slowed imperceptibly from the 24 frame/s source to match). This produces irregularities in the sequence of images which some people can perceive as a stutter during slow and steady pans of the camera in the source material. See telecine for more details.\n\nFor viewing native PAL or SECAM material (such as European television series and some European movies) on NTSC equipment, a standards conversion has to take place. There are basically two ways to accomplish this.\n\nWhen converting PAL (625 lines @ 25 frame/s) to NTSC (525 lines @ 30 frame/s), the converter must eliminate 100 lines per frame. The converter must also create five frames per second.\n\nTo reduce the 625-line signal to 525, less expensive converters drop 100 lines. These converters maintain picture fidelity by evenly spacing removed lines. (For example, the system might discard every sixth line from each PAL field. After the 50th discard, this process would stop. By then the system would have passed the viewable area of the field. In the following field, the process would repeat, completing one frame.) To create the five additional frames, the converter repeats every fifth frame.\n\nIf there is little inter-frame motion, this conversion algorithm is fast, inexpensive and effective. Many inexpensive consumer television system converters have employed this technique. Yet in practise, most video features significant inter-frame motion. To reduce conversion artefacts, more modern or expensive equipment may use sophisticated techniques.\n\nThe most basic and literal way to double lines is to repeat each scanline, though the results of this are generally very crude. Linear interpolation use digital interpolation to recreate the missing lines in an interlaced signal, and the resulting quality depends on the technique used. Generally the bob version of linear deinterlacer will only interpolate within a single field, rather than merging information from adjacent fields, to preserve the smoothness of motion, resulting in a frame rate equal to the field rate (i.e. a 60i signal would be converted to 60p.) The former technique in moving areas and the latter in static areas, which improves overall sharpness.\n\nInterfield Interpolation is a technique in which new frames are created by blending adjacent frames, rather than repeating a single frame. This is more complex and computationally expensive than linear interpolation, because it requires the interpolator to have knowledge of the preceding and the following frames to produce an intermediate blended frame. Deinterlacing may also be required in order to produce images which can be interpolated smoothly. Interpolation can also be used to reduce the number of scanlines in the image by averaging the colour and intensity of pixels on neighbouring lines, a technique similar to Bilinear filtering, but applied to only one axis.\n\nThere are simple 2-line and 4 line converters. The 2-line converter creates a new line by comparing two adjacent lines, whereas a 4-line model compares 4 lines to average the 5th. Interfield interpolation \"reduces\" judder, but at the expense of picture smearing. The greater the blending applied to smooth out the judder, the greater the smear caused by blending.\n\nSome more advanced techniques measure the nature and degree of inter-frame motion in the source, and use adaptive algorithms to blend the image based on the results. Some such techniques are known as motion compensation algorithms, and are computationally much more expensive than the simpler techniques, thus requiring more powerful hardware to be effective in real-time conversion.\n\nAdaptive Motion algorithms capitalize on the way the human eye and brain process moving images - in particular, detail is perceived less clearly on moving objects.\n\nAdaptive interpolation requires that the converter analyzes multiple successive fields and to detect the amount and type of motion of different areas of the picture.\n\n\"Adaptive Motion Interpolation has many variations and is commonly found in midrange converters\". The quality and cost is dependent upon the accuracy in analyzing the type and amount of motion, and the selection of the most appropriate algorithm for processing the type of motion.\n\nBlock matching involves dividing the image into mosaic blocks - say perhaps for the sake of explanation, 8x8 pixels. The blocks are then stored in memory. The next field read out is also divided up into the same number and size of mosaic blocks. The converter's computer then goes to work and starts matching up blocks. The blocks that stayed in the same relative position (read: there was no motion in this part of the image) receive relatively little processing.\n\nWhen panning from left to right is taking place (over say 10 fields) it is safe to assume that the 11th field will be similar or very close.\n\nThe technique is highly effective but it does require a tremendous amount of computing power. Consider a block of only 8x8 pixels. For each block, the computer has 64 possible directions and 64 pixels to be matched to the block in the next field. Also consider that the greater the motion, the further out the search must be conducted. Just to find an adjacent block in the next field would entail making a search of 9 blocks. 2 blocks out would require a search and match of 25 blocks - 3 blocks further distant and it grows to 49 etc.\n\nThe type of motion can exponentially compound the compute power required. Consider a rotating object, where a simple straight line motion vector is of little help in predicting where the next block should match. It can quickly be seen that the more inter frame motion introduced, the much greater the processing power required. This is the general concept of block matching. Block match converters can vary widely in price and performance depending on the attention to detail and complexity.\n\nA weird artifact of block matching owes to the size of the block itself. If a moving object is smaller than the mosaic block, consider that it's the entire block that gets moved. In most cases, it's not an issue, but consider a thrown baseball. The ball itself has a high motion vector, but its background that makes up the rest of the block might not have any motion. The background gets transported in the moved block as well, based on the motion vector of the baseball, What you might see is the ball with a small amount of outfield or whatever, tagging along. As it's in motion, the block may be \"soft\" depending upon what additional techniques were used and barely noticeable unless you're looking for it.\n\nBlock matching requires a staggering amount of processing horsepower, but today's microprocessors are making it a viable solution.\n\nPhase correlation is perhaps the most computationally complex of the general algorithms.\n\nPhase correlation's success lies in the fact that it is effective with coping with rapid motion and random motion. Phase correlation does not easily get confused by rotating or twirling objects that confuse most other kinds of systems converters. Phase correlation is elegant as well as technically and conceptually complex. Its successful operation is derived by performing a Fourier transform to each field of video.\n\nA Fast Fourier Transform (FFT) is an algorithm which deals with the transformation of discrete values (in this case image pixels). When applied to a sample of finite values, a Fast Fourier Transform expresses any changes (motion) in terms of frequency components.\n\nSince the result of the FFT represents only the inter-frame changes in terms of frequency distribution, there is far less data that has to be processed in order to calculate the motion vectors.\n\nA digital television adapter, (CECB), or digital-to-analog converter (box), is a device that receives, by means of an antenna, a digital television (DTV) transmission, and converts that signal into an analog television signal that can be received and displayed on an analog television.\n\nThese boxes cheaply convert HDTV (16:9 at 720 or 1080) to (NTSC or PAL at 4:3). Very little is known about the specific conversion technologies used by these converter boxes in the PAL and NTSC zones.\n\nDownconversion is usually required, hence very little image quality loss is perceived by viewers at the recommended viewing distance with most TV sets.\n\nA lot of cross format television conversion is done offline. There are several DVD packages that offer offline PAL <--> NTSC conversion – including cross conversion (technically MPEG <--> DTV) from the myriad of MPEG based web video formats.\n\nCross conversion can use any method commonly in use for TV system format conversion, but typically (in order to reduce complexity and memory use) it is left up to the codec to do the conversion. Most modern DVDs are converted from 525 <--> 625 lines in this way, as it is very economical for most programming that originates at EDTV resolution.\n\n\n"}
{"id": "36625840", "url": "https://en.wikipedia.org/wiki?curid=36625840", "title": "The Internship", "text": "The Internship\n\nThe Internship is a 2013 American comedy film directed by Shawn Levy, written by Vince Vaughn and Jared Stern, and produced by Vaughn and Levy. The film stars Vince Vaughn and Owen Wilson as recently laid-off salesmen who attempt to compete with much younger and more technically skilled applicants for a job at Google. Rose Byrne, Max Minghella, Aasif Mandvi, Josh Brener, Dylan O'Brien, Tobit Raphael, Tiya Sircar, Josh Gad, and Jessica Szohr also star.\n\n\"The Internship\" is the second film with Vaughn and Wilson in the lead roles, after the 2005 film \"Wedding Crashers\"; the two had also both appeared in the 2004 film \"Starsky & Hutch\". This is also the second collaboration of Levy, Vaughn, and Stern after the 2012 film \"The Watch\", and the third of Levy and Wilson after the first two \"Night at the Museum\" films.\n\nReleased on June 7, 2013, the film received mixed reviews from critics and grossed $93 million worldwide.\n\nSalesmen Billy McMahon and Nick Campbell's employer goes out of business, and Billy applies for Google internships on their behalf. They are accepted due to their unorthodox interview answers, despite a lack of relevant experience. They are the only interns not of traditional collegiate age. They will spend the summer competing in teams against other interns in a variety of tasks, and only the members of the winning team will be guaranteed jobs with Google. Billy and Nick are teamed with other interns seen as rejects: Stuart, who is usually engrossed in his smart phone; Yo-Yo, a Filipino-American who was homeschooled by his tiger mother; and Neha, an Indian-American who is an enthusiast of nerd-related kink. The team is led by Lyle, who constantly tries to act hip in order to hide his insecurities. Another intern, Graham, bullies Billy and Nick's team. Mr. Chetty, the head of the internship program, also expresses his doubts about the older men's abilities. Stuart, Yo-Yo, and Neha see Billy and Nick as useless during a task focused on debugging and send them on a wild-goose chase for the fictional Charles Xavier at Stanford University. But later, during a game of Quidditch against Graham's team, Billy rallies his team to a comeback that unifies them as a team, despite ultimately losing after Graham cheats.\n\nWhen the teams are tasked with developing an app, Billy and Nick convince the team to indulge in a wild night out. At a strip club, Neha admits to Billy that, despite her rich fantasy life, she has no real world experience and is nervous. With his support, she decides to stay. Nick gets Yo-Yo to break out of his shell by drinking and receiving lap dances. Encouraged by Billy, Lyle approaches one of the dancers, Marielena, who is also a dance instructor at Google on whom he had developed a crush. She is charmed by him, but another customer challenges Lyle for her attention and they fight, getting the team kicked out. Before sunrise, Stuart learns to appreciate his surroundings while overlooking the Golden Gate Bridge, and Lyle's drunken antics inspire the team to create an app that guards against reckless phone usage while drunk. They win the task by earning the most downloads.\n\nMeanwhile, Nick has been flirting with an executive, Dana, with little success. When he begins attending technical presentations to impress her, he develops an interest in the material. While the teams prepare to staff the technical support hotline, only Billy feels at a loss. A Google employee, \"Headphones,\" who always wears headphones and never socializes, approaches Billy and tells him that the way he interacts with people is special. He tutors Billy on the technical information. Dana agrees to go on a date with Nick, and she invites him in at the end of the evening. During the task, Billy is comfortable with the material, but his team receives no score because he failed to properly log his calls for review. Dejected, Billy leaves Google to pursue a new sales opportunity with his former boss. The final task is announced as a sales challenge. Teams must sign the largest possible company to begin advertising with Google. The team is stunned when Nick tells them that Billy has left, and they declare that they do not want to do the task without him. Nick convinces Billy to return, and Billy leads the team to show a local pizzeria owner how Google can help him interact with potential customers and thereby expand his business, while remaining true to his professional values.\n\nChetty is about to announce that Graham's team have won, when Billy, Nick, and their team arrive to give a dynamic presentation about their new client. Chetty recognizes that although the pizzeria is not a large business, its potential is limitless because it is expanding via technology. Graham protests and is dressed down by Headphones, who turns out to be the head of Google Search. Nick and Billy's team win the challenge and the guaranteed jobs. Graham berates his team, who finally reject him. As the students depart, Nick and Dana are still seeing each other, as are Lyle and Marielena. Stuart and Neha have formed a romantic connection as well with Stuart promising to see her in person rather than texting her, and Yo-Yo asserts himself to his mother. Billy and Nick toast their success.\n\n\nMost of the scenes were filmed in Atlanta, Georgia, and at the Georgia Institute of Technology, which posed as a double for Googleplex. Vaughn came up with the idea after watching a \"60 Minutes\" segment on Google's work culture, and subsequently brought the idea to director Shawn Levy. Google agreed to work with the film producers, with founder Larry Page noting that \"computer science has a marketing problem.\" Google also felt it would help further explain their \"Don't be evil\" mantra. Although Reuters reported that as part of the deal Google asked for \"Creative control\", Levy denied the company was involved with the script, insisting that Google only assisted from a \"technical\" perspective. CNN reported that the studio did give \"some control\" to Google over the depiction of its products. \n\nOn Rotten Tomatoes, the film has an approval rating of 35% based on 164 reviews and an average rating of 4.8/10. The site's critical consensus reads, \"\"The Internship\" weighs down Vince Vaughn and Owen Wilson's comic charisma with a formulaic script and padded running time that leans far too heavily on its stars' easygoing interplay.\" On Metacritic, the film has a score of 42 out of 100 based on 36 critics, indicating \"mixed or average reviews\". Audiences polled by CinemaScore gave the film an average grade \"B+\" on an A+ to F scale.\n\nA majority of the reviewers have derided it for being a feature-length Google commercial. In his review, Ty Burr of \"The Boston Globe\" commented: \"Here’s why Google is so successful: It's figured out a way for Twentieth Century Fox to make a two-hour Google commercial disguised as a summer comedy\". Stephen Rea of \"The Philadelphia Inquirer\" wrote on his review, \"\"The Internship\" itself would be kind of charming, too, if this Google-recruitment film, this 119-minute commercial for Googliness, weren't so downright creepy\". \"The Guardian\"s David Cox described the movie as a \"two-hour corporate video,\" while British film critic Mark Kermode called the film \"the longest advert I've seen in the cinema\". He dismissed it as \"one of the most witless, humourless, vomit-inducing horribly self-satisfied, smug, unfunny comedies I have ever seen\".\n\nAnother critique was that combining Vaughn and Wilson with Google was poorly timed, and that the film would have been much more successful, had it been released on the heels of Vaughn and Wilson's success in 2005's \"Wedding Crashers\". This fact of timing was satirized by a video news story run by \"The Onion\", a satirical newspaper, titled \"The Internship Poised to be Biggest Comedy of 2005\".\n\nMany former Google interns and Google employees noted the accuracy of the company environment depicted in the movie, but also pointed out that the internship process is nothing like that shown in the movie.\n\n\"The Internship\" was released in \"Unrated\" form on DVD and Blu-ray Combo Pack on October 22, 2013. This edition runs 125 minutes and contains profanity and nudity not found in the theatrical release.\n\n"}
{"id": "54691944", "url": "https://en.wikipedia.org/wiki?curid=54691944", "title": "Tunnel construction", "text": "Tunnel construction\n\nTunnels are dug in types of materials varying from soft clay to hard rock. The method of tunnel construction depends on such factors as the ground conditions, the ground water conditions, the length and diameter of the tunnel drive, the depth of the tunnel, the logistics of supporting the tunnel excavation, the final use and shape of the tunnel and appropriate risk management.\n\nThere are three basic types of tunnel construction in common use:\n\nIn 2017 experiences show that city subway underground tunnels cost approximately 500 Mio EUR per kilometer. In Switzerland a kilometer of motorway tunnel was roughly calculated at 300 Mio CHF, at the time 200 Mio Eur. The undersea tunnel between Denmark and Germany is planned for 425 Mio per km, in 2015.\n\nCut-and-cover is a simple method of construction for shallow tunnels where a trench is excavated and roofed over with an overhead support system strong enough to carry the load of what is to be built above the tunnel.\nTwo basic forms of cut-and-cover tunnelling are available:\n\nShallow tunnels are often of the cut-and-cover type (if under water, of the immersed-tube type), while deep tunnels are excavated, often using a tunnelling shield. For intermediate levels, both methods are possible.\n\nLarge cut-and-cover boxes are often used for underground metro stations, such as Canary Wharf tube station in London. This construction form generally has two levels, which allows economical arrangements for ticket hall, station platforms, passenger access and emergency egress, ventilation and smoke control, staff rooms, and equipment rooms. The interior of Canary Wharf station has been likened to an underground cathedral, owing to the sheer size of the excavation. This contrasts with many traditional stations on London Underground, where bored tunnels were used for stations and passenger access. Nevertheless, the original parts of the London Underground network, the Metropolitan and District Railways, were constructed using cut-and-cover. These lines pre-dated electric traction and the proximity to the surface was useful to ventilate the inevitable smoke and steam.\n\nA major disadvantage of cut-and-cover is the widespread disruption generated at the surface level during construction. This, and the availability of electric traction, brought about London Underground's switch to bored tunnels at a deeper level towards the end of the 19th century.\n\nTunnel boring machines (TBMs) and associated back-up systems are used to highly automate the entire tunnelling process, reducing tunnelling costs. In certain predominantly urban applications, tunnel boring is viewed as quick and cost effective alternative to laying surface rails and roads. Expensive compulsory purchase of buildings and land, with potentially lengthy planning inquiries, is eliminated. Disadvantages of TBMs arise from their usually large size – the difficulty of transporting the large TBM to the site of tunnel construction, or (alternatively) the high cost of assembling the TBM on-site, often within the confines of the tunnel being constructed.\n\nThere are a variety of TBM designs that can operate in a variety of conditions, from hard rock to soft water-bearing ground. Some types of TBMs, the bentonite slurry and earth-pressure balance machines, have pressurised compartments at the front end, allowing them to be used in difficult conditions below the water table. This pressurizes the ground ahead of the TBM cutter head to balance the water pressure. The operators work in normal air pressure behind the pressurised compartment, but may occasionally have to enter that compartment to renew or repair the cutters. This requires special precautions, such as local ground treatment or halting the TBM at a position free from water. Despite these difficulties, TBMs are now preferred over the older method of tunnelling in compressed air, with an air lock/decompression chamber some way back from the TBM, which required operators to work in high pressure and go through decompression procedures at the end of their shifts, much like deep-sea divers.\n\nIn February 2010, Aker Wirth delivered a TBM to Switzerland, for the expansion of the Linth–Limmern Power Stations located south of Linthal in the canton of Glarus. The borehole has a diameter of . The four TBMs used for excavating the Gotthard Base Tunnel, in Switzerland, had a diameter of about . A larger TBM was built to bore the Green Heart Tunnel (Dutch: Tunnel Groene Hart) as part of the HSL-Zuid in the Netherlands, with a diameter of . This in turn was superseded by the Madrid M30 ringroad, Spain, and the Chong Ming tunnels in Shanghai, China. All of these machines were built at least partly by Herrenknecht. , the world's largest TBM is \"Big Bertha\", a diameter machine built by Hitachi Zosen Corporation, which is digging the Alaskan Way Viaduct replacement tunnel in Seattle, Washington (US).\n\nClay-kicking is a specialised method developed in the United Kingdom of digging tunnels in strong clay-based soil structures. Unlike previous manual methods of using mattocks which relied on the soil structure to be hard, clay-kicking was relatively silent and hence did not harm soft clay-based structures. The clay-kicker lies on a plank at a 45-degree angle away from the working face and inserts a tool with a cup-like rounded end with the feet. Turning the tool manually, the kicker extracts a section of soil, which is then placed on the waste extract.\n\nUsed in Victorian civil engineering, the method found favour in the renewal of Britain's ancient sewerage systems, by not having to remove all property or infrastructure to create a small tunnel system. During the First World War, the system was used by Royal Engineer tunnelling companies to put mines beneath the German Empire lines. The method was virtually silent and so not susceptible to listening methods of detection.\n\nA temporary access shaft is sometimes necessary during the excavation of a tunnel. They are usually circular and go straight down until they reach the level at which the tunnel is going to be built. A shaft normally has concrete walls and is usually built to be permanent. Once the access shafts are complete, TBMs are lowered to the bottom and excavation can start. Shafts are the main entrance in and out of the tunnel until the project is completed. If a tunnel is going to be long, multiple shafts at various locations may be bored so that entrance to the tunnel is closer to the unexcavated area.\n\nOnce construction is complete, construction access shafts are often used as ventilation shafts, and may also be used as emergency exits.\n\nThe New Austrian Tunneling Method (NATM) was developed in the 1960s and is the best known of a number of engineering practices that use calculated and empirical measurements to provide safe support to the tunnel lining. The main idea of this method is to use the geological stress of the surrounding rock mass to stabilize the tunnel, by allowing a measured relaxation and stress reassignment into the surrounding rock to prevent full loads becoming imposed on the supports. Based on geotechnical measurements, an optimal cross section is computed. The excavation is protected by a layer of sprayed concrete, commonly referred to as shotcrete. Other support measures can include steel arches, rockbolts and mesh. Technological developments in sprayed concrete technology have resulted in steel and polypropylene fibres being added to the concrete mix to improve lining strength. This creates a natural load-bearing ring, which minimizes the rock's deformation.\n\nBy special monitoring the NATM method is flexible, even at surprising changes of the geomechanical rock consistency during the tunneling work. The measured rock properties lead to appropriate tools for tunnel strengthening. In the last decades also soft ground excavations up to became usual.\n\nIn pipe jacking, hydraulic jacks are used to push specially made pipes through the ground behind a TBM or shield. This method is commonly used to create tunnels under existing structures, such as roads or railways. Tunnels constructed by pipe jacking are normally small diameter bores with a maximum size of around .\n\nBox jacking is similar to pipe jacking, but instead of jacking tubes, a box-shaped tunnel is used. Jacked boxes can be a much larger span than a pipe jack, with the span of some box jacks in excess of . A cutting head is normally used at the front of the box being jacked, and spoil removal is normally by excavator from within the box.Recent developments of the Jacked Arch and Jacked deck have enabled longer and larger structures to be installed to close accuracy. The 126m long 20m clear span underpass below the high speed rail lines at Cliffsend in Kent, UK.\n\nThere are also several approaches to underwater tunnels, the two most common being bored tunnels or immersed tubes, examples are Bjørvika Tunnel and Marmaray. Submerged floating tunnels are a novel approach under consideration; however, no such tunnels have been constructed to date.\n\nA new kind of tunnels is used to reduce the environmental impact of motorways or railways: landtunnels. These are not underground tunnels, but built at ground level. The urban area next to the tunnel can be raised with ground or buildings (for instance parking facilities) to improve the integration of the tunnel in the immediate area. A good early example of such a landtunnel is the A2 motorway tunnel at Leidsche Rijn, near the Dutch city of Utrecht.\n\nDuring construction of a tunnel it is often convenient to install a temporary railway, particularly to remove excavated spoil, often narrow gauge so that it can be double track to allow the operation of empty and loaded trains at the same time. The temporary way is replaced by the permanent way at completion, thus explaining the term \"Perway\".\n\nThe vehicles or traffic using a tunnel can outgrow it, requiring replacement or enlargement:\n\nAn open building pit consists of a horizontal and a vertical boundary that keeps groundwater and soil out of the pit. There are several potential alternatives and combinations for (horizontal and vertical) building pit boundaries. The most important difference with cut-and-cover is that the open building pit is muted after tunnel construction; no roof is placed.\n\n"}
{"id": "8117675", "url": "https://en.wikipedia.org/wiki?curid=8117675", "title": "Wallwasher", "text": "Wallwasher\n\nWallwashing is a popular name for a lighting design technique for illumination of large surfaces. It is mainly used with contemporary architecture; in public cultural buildings, museums and galleries; and in landscape lighting. \n\nMost of what one actually sees, entering a room, are the vertical surfaces. The illumination requires asymmetric lighting fixtures, which, in a number of 3 or more in a line, can produce evenly illuminated walls. It is a tool mainly used by lighting designers, to create lighter spaces or making the rooms seem brighter or higher. The technique is similar to the \"horizon\" technique used in theatre lighting.\n"}
{"id": "28414748", "url": "https://en.wikipedia.org/wiki?curid=28414748", "title": "Web-guiding systems", "text": "Web-guiding systems\n\nWeb-guiding systems are used in the converting industry to position flat materials, known as webs, before processing. They are typically positioned just before a critical stage on a converting machine. Each type of web guiding system uses a sensor to monitor the web position for lateral tracking, and each has an actuator to shift the running web mechanically back on course whenever the sensor detects movement away from the set path. Actuators may be pneumatic or hydraulic cylinders, or some kind of electromechanical device. Because the web may be fragile — particularly at its edge — non-contact sensors are used. These sensors may be pneumatic, photoelectric, ultrasonic, or infrared. The system’s controls must put the output signals from the sensors in to a form that can drive the actuator.\n\nWeb guiding systems work at high speed, constantly making small adjustments to maintain the position of the material. The latest systems use digital technology and touch screen operator interfaces to simplify set up. Web guiding systems are used on slitting machines, slitter rewinders, printing presses, coating and laminating machines.\n\nIn 1939, Irwin Fife invented the first web guide in his Garage in Oklahoma City, Oklahoma, solving a newspaper owner’s challenge of keeping paper aligned in his high-speed newspaper press.\n\nActive guiding systems are composed of a sensor, an actuator connected to a guide mechanism, and a controller. The sensor can be any detector, which can reliably pick up the edges of a web. The most common types of sensors are pneumatic (only works with nonporous webs), optical (works well with opaque webs), ultrasonic (works with most material), or paddies (thick webs). Recent developments have introduced to the industry a new sensor technology based on light scattering and spatial filtering that allows the use of an optical sensor to detect the edge of any material. The web must be flat (free of curl) and stable (free of flutter) through the edge sensor. For this and other reasons, the sensor is often placed near a roller. If two sensors are used, the web could be guided to the front edge, back edge, or center. Common active guide systems include steering guide (remotely pivoted guide), displacement guide (offset-pivot guide), unwind guide, and rewind guide.\n\nTension adjustment is necessary due to several mechanical factors: oscillations caused by mechanical misalignments, differing inertial response (lag) of mechanical elements during web acceleration, out-of-round unwind and tension rolls, slipping through nip rolls, and over-aggressive web-guide correction. Several technical process and control issues also affect tension: tension setpoint changes, phase offset on driven rolls, tension bleed from one zone to another, and thermal effect (contraction/expansion) as the substrate passes through various processes. It is impossible to eliminate all factors requiring tension adjustment. Variance in any one factor in a zone necessitates changes in tension control and web speed. Consequently, with coupled tension zone control, jitter is inevitable in a continuous web where the controllers cause a feedback loop.\n\nPrecise control of the system is essential. If the line speed of web is reduced, the amount of lateral displacement error that can be controlled by the steering guide system also decreases. If the input error decreases, the lateral displacement error also becomes smaller. Lateral displacement occurs on the transport web by the air blow from the dryer and the increase of blowing frequency can reduce the lateral displacement.\n"}
