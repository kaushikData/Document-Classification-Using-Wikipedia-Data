{"id": "662399", "url": "https://en.wikipedia.org/wiki?curid=662399", "title": "Affymetrix", "text": "Affymetrix\n\nAffymetrix, Inc. was an American company that manufactured DNA microarrays; it was based in Santa Clara, California, United States. The company was acquired by Thermo Fisher Scientific in March 2016.\n\nThe company was founded by Stephen Fodor in 1992. It began as a unit in Affymax N.V. in 1991 by Fodor's group, which had in the late 1980s developed methods for fabricating DNA microarrays, called \"GeneChip\" according to the Affymetrix trademark, using semiconductor manufacturing techniques. The company's first product, an HIV genotyping GeneChip, was introduced in 1994 and the company went public in 1996. As a result of its pioneering work and the ensuing popularity of microarray products, Affymetrix derived significant benefit from its patent portfolio in this area.\n\nAffymetrix acquired Genetic MicroSystems for slide-based microarrays and scanners,\nNeomorphic for bioinformatics,\nParAllele Bioscience for custom SNP genotyping,\nUSB/Anatrace for biochemical reagents,\neBioscience for flow cytometry,\nand Panomics and True Materials to expand its offering of low to mid-plex applications.\nIn 2000, Perlegen Sciences spun out from Affymetrix to focus on wafer-scale genomics for massive data creation and collection required for characterizing population variance of genomic markers and expression for the drug discovery process.\n\nOn January 8, 2016, Thermo Fisher Scientific announced the acquisition of Affymetrix for approximately $1.3 billion. The acquisition closed on March 31, 2016.\n\nAffymetrix makes quartz chips for analysis of DNA Microarrays called GeneChip arrays. Affymetrix's GeneChip arrays assist researchers in quickly scanning for the presence of particular genes in a biological sample. Within this area, Affymetrix is focused on oligonucleotide microarrays. These microarrays are used to determine which genes exist in a sample by detecting specific pieces of mRNA. A single chip can be used to analyze thousands of genes in one assay. Chips can be used only once.\n\nAffymetrix sells both mass-produced GeneChip arrays intended to match scientifically important parts of human and other animal genomes. Affymetrix manufactures its GeneChip arrays using photolithography. The company also manufactures machinery for high speed analysis of biological samples.\n\nAffymetrix GeneChip Operating Software is a software system for managing Affymetrix microarray data.\n\nCompetitors in the DNA Microarray business include Illumina, GE Healthcare, Applied Biosystems, Beckman Coulter, Eppendorf Biochip Systems, and Agilent. There are also various inexpensive plastic-based technologies under development in small companies and laboratories around the world. It has been widely speculated that mass-produced plastic chips can be produced at lower prices than Affymetrix's quartz chips.\n\nThe Food and Drug Administration in January 2014 cleared a first-of-a-kind whole-genome postnatal blood test that can aid physicians in identifying the underlying genetic cause of developmental delay, intellectual disability, congenital anomalies, or dysmorphic features in children.\nThis test, known as CytoScan Dx Assay, is designed to diagnose these disabilities earlier to expedite appropriate care and support.\n\"About 2 to 3 percent of U.S. children have some sort of intellectual disability, according to the National Institutes of Health.\"\n\n"}
{"id": "20657020", "url": "https://en.wikipedia.org/wiki?curid=20657020", "title": "Anti-nuclear protests", "text": "Anti-nuclear protests\n\nAnti-nuclear protests began on a small scale in the U.S. as early as 1946 in response to Operation Crossroads. Large scale anti-nuclear protests first emerged in the mid-1950s in Japan in the wake of the March 1954 Lucky Dragon Incident. August 1955 saw the first meeting of the World Conference against Atomic and Hydrogen Bombs, which had around 3,000 participants from Japan and other nations. Protests began in Britain in the late 1950s and early 1960s. In the United Kingdom, the first Aldermaston March, organised by the Campaign for Nuclear Disarmament, took place in 1958. In 1961, at the height of the Cold War, about 50,000 women brought together by Women Strike for Peace marched in 60 cities in the United States to demonstrate against nuclear weapons. In 1964, Peace Marches in several Australian capital cities featured \"Ban the Bomb\" placards.\n\nNuclear power became an issue of major public protest in the 1970s and demonstrations in France and West Germany began in 1971. In France, between 1975 and 1977, some 175,000 people protested against nuclear power in ten demonstrations. In West Germany, between February 1975 and April 1979, some 280,000 people were involved in seven demonstrations at nuclear sites. Many mass demonstrations took place in the aftermath of the 1979 Three Mile Island accident and a New York City protest in September 1979 involved two hundred thousand people. Some 120,000 people demonstrated against nuclear power in Bonn, in October 1979. In May 1986, following the Chernobyl disaster, an estimated 150,000 to 200,000 people marched in Rome to protest against the Italian nuclear program, and clashes between anti-nuclear protesters and police became common in West Germany.\n\nIn the early 1980s, the revival of the nuclear arms race triggered large protests about nuclear weapons. In October 1981 half a million people took to the streets in several cities in Italy, more than 250,000 people protested in Bonn, 250,000 demonstrated in London, and 100,000 marched in Brussels. The largest anti-nuclear protest was held on June 12, 1982, when one million people demonstrated in New York City against nuclear weapons. In October 1983, nearly 3 million people across western Europe protested nuclear missile deployments and demanded an end to the arms race; the largest crowd of almost one million people assembled in the Hague in the Netherlands. In Britain, 400,000 people participated in what was probably the largest demonstration in British history.\n\nOn May 1, 2005, 40,000 anti-nuclear/anti-war protesters marched past the United Nations in New York, 60 years after the atomic bombings of Hiroshima and Nagasaki. This was the largest anti-nuclear rally in the U.S. for several decades. In 2005 in Britain, there were many protests about the government's proposal to replace the aging Trident weapons system with a newer model. The largest protest had 100,000 participants. In May 2010, some 25,000 people, including members of peace organizations and 1945 atomic bomb survivors, marched from downtown New York to the United Nations headquarters, calling for the elimination of nuclear weapons.\n\nThe 2011 Japanese nuclear accidents undermined the nuclear power industry's proposed renaissance and revived anti-nuclear passions worldwide, putting governments on the defensive. There were large protests in Germany, India, Japan, Switzerland, and Taiwan.\n\nIn 1964, Peace Marches which featured \"Ban the bomb\" placards, were held in several Australian capital cities.\n\nIn 1972, the anti-nuclear weapons movement maintained a presence in the Pacific, largely in response to French nuclear testing there. Activists, including David McTaggart from Greenpeace, defied the French government by sailing small vessels into the test zone and interrupting the testing program. In Australia, thousands joined protest marches in Adelaide, Melbourne, Brisbane, and Sydney. Scientists issued statements demanding an end to the tests; unions refused to load French ships, service French planes, or carry French mail; and consumers boycotted French products. In Fiji, activists formed an Against Testing on Mururoa organization.\n\nIn November and December 1976, 7,000 people marched through the streets of Australian cities, protesting against uranium mining. The Uranium Moratorium group was formed and it called for a five-year moritorium on uranium mining. In April 1977 the first national demonstration co-ordinated by the Uranium Moratorium brought around 15,000 demonstrators into the streets of Melbourne, 5,000 in Sydney, and smaller numbers elsewhere. A National signature campaign attracted over 250,000 signatures calling for a five-year moratorium. In August, another demonstration brought 50,000 people out nationally and the opposition to uranium mining looked like a potential political force.\n\nOn Palm Sunday 1982, an estimated 100,000 Australians participated in anti-nuclear rallies in the nation's largest cities. Growing year by year, the rallies drew 350,000 participants in 1985. The movement focused on halting Australia's uranium mining and exports, abolishing nuclear weapons, removing foreign military bases from Australia's soil, and creating a nuclear-free Pacific.\n\nOn Dec 17th 2001, 46 Greenpeace activists occupied the Lucas Heights facility to protest the construction of a second research reactor. Protestors gained access to the grounds, the HIFAR reactor, the high-level radioactive waste store and the radio tower. Their protest highlighted the security and environmental risks of the production of nuclear materials and the shipment of radioactive waste from the facility.\n\nIn March 2012, hundreds of anti-nuclear demonstrators converged on the Australian headquarters of global mining giants BHP Billiton and Rio Tinto to mark one year since the Fukushima nuclear disaster. The 500-strong march through southern Melbourne called for an end to uranium mining in Australia. There were also events in Sydney, and in Melbourne the protest included speeches and performances by representatives of the expatriate Japanese community as well as Australia's Indigenous communities, who are worried about the effects of uranium mining near tribal lands.\n\nAs early as 1993 there were local and international protests against the Temelin Nuclear Power Plant's construction. Large grassroots civil disobedience actions took place in 1996 and 1997. These were organized by the so-called Clean Energy Brigades. In September and October 2000, Austrian anti-nuclear protesters demonstrated against the Temelin Nuclear Power Plant and at one stage temporarily blocked all 26 border crossings between Austria and the Czech Republic. The first reactor was finally commissioned in 2000 and the second in 2002.\n\nIn 1971, 15,000 people demonstrated against French plans to locate the first light-water reactor power plant in Bugey. This was the first of a series of mass protests organized at nearly every planned nuclear site in France until the massive demonstration at the Superphénix breeder reactor in Creys-Malvillein in 1977 culminated in violence.\n\nIn France, between 1975 and 1977, some 175,000 people protested against nuclear power in ten demonstrations.\n\nIn January 2004, up to 15,000 anti-nuclear protesters marched in Paris against a new generation of nuclear reactors, the European Pressurised Water Reactor (EPWR).\n\nOn March 17, 2007 simultaneous protests, organised by \"Sortir du nucléaire\", were staged in five French towns to protest construction of EPR plants; Rennes, Lyon, Toulouse, Lille, and Strasbourg.\n\nFollowing the 2011 Fukushima I nuclear accidents, around 1,000 people took part in a protest against nuclear power in Paris on March 20. Most of the protests, however, are focused on the closure of the Fessenheim Nuclear Power Plant, where some 3,800 French and Germans demonstrated on April 8 and April 25.\n\nThousands staged anti-nuclear protests around France, on the eve of the 25th anniversary of Chernobyl and after Japan's Fukushima nuclear disaster, demanding reactors be closed. Protesters' demands were focused on getting France to shut its oldest nuclear power station at Fessenheim, which lies in a densely populated part of France, less than two kilometres from Germany and around 40 kilometres (25 miles) from Switzerland.\n\nAround 2,000 people also protested at the Cattenom nuclear plant, France's second most powerful, in the Mosel region to the northwest of Strasbourg. Protesters in southwestern France staged another demonstration in the form of a mass picnic in front of the Blayais nuclear reactor, also in memory of Chernobyl. In France's northwestern region of Brittany, around 800 people staged a good-humoured march in front of the Brennilis experimental heavy-water atomic plant that was built in the 1960s. It was taken offline in 1985 but its dismantling is still not completed after 25 years.\n\nThree months after the Fukushima nuclear disaster, thousands of anti-nuclear campaigners protested in Paris.\n\nOn June 26, 2011, around 5,000 protesters gathered near Fessenheim nuclear power plant, demanding the plant be shut down immediately. Demonstrators from France and Germany came to Fessenheim and formed a human chain along the road. Protesters claim that the plant is vulnerable to flooding and earthquakes. Fessenheim has become a flashpoint in renewed debate over nuclear safety in France after the Fukushima accident. The plant is operated by French power group EDF.\n\nIn November 2011, thousands of anti-nuclear protesters delayed a train carrying radioactive waste from France to Germany. Many clashes and obstructions made the journey the slowest one since the annual shipments of radioactive waste began in 1995. The shipment, the first since Japan's Fukishima nuclear disaster, faced large protests in France where activists damaged the train tracks. Thousands of people in Germany also interrupted the train's journey, forcing it to proceed at a snail's pace, covering 1,200 kilometers (746 miles) in 109 hours. More than 200 people were reported injured in the protests and several arrests were made.\n\nOn December 5, 2011, nine Greenpeace activists cut through a fence at the Nogent Nuclear Power Plant. They scaled the roof of the domed reactor building and unfurled a \"Safe Nuclear Doesn't Exist\" banner before attracting the attention of security guards. Two activists remained at large for four hours. On the same day, two more campaigners breached the perimeter of the Cruas Nuclear Power Plant, escaping detection for more than 14 hours, while posting videos of their sit-in on the internet.\n\nIn Aquitaine, the local group TchernoBlaye continue to protest against the continued operation of the Blayais Nuclear Power Plant.\n\nOn the first anniversary of the Fukushima nuclear disaster, organisers of French anti-nuclear demonstrations claim 60,000 supporters formed a human chain 230 kilometres long, stretching from Lyon to Avignon.\n\nIn March 2014, police arrested 57 Greenpeace protesters who used a truck to break through security barriers and enter the Fessenheim nuclear power plant in eastern France. The activists hung antinuclear banners, but France’s nuclear safety authority said that the plant’s security had not been compromised. President Hollande has promised to close Fessenheim by 2016, but Greenpeace wants immediate closure.\n\nIn 1971, the town of Wyhl, in Germany, was a proposed site for a nuclear power station. In the years that followed, public opposition steadily mounted, and there were large protests. Television coverage of police dragging away farmers and their wives helped to turn nuclear power into a major issue. In 1975, an administrative court withdrew the construction licence for the plant. The Wyhl experience encouraged the formation of citizen action groups near other planned nuclear sites. Many other anti-nuclear groups formed elsewhere, in support of these local struggles, and some existing citizen action groups widened their aims to include the nuclear issue.\n\nIn West Germany, between February 1975 and April 1979, some 280,000 people were involved in seven demonstrations at nuclear sites. Several site occupations were also attempted. In the aftermath of the Three Mile Island accident in 1979, some 120,000 people attended a demonstration against nuclear power in Bonn.\n\nIn 1981, Germany's largest anti-nuclear power demonstration took place to protest against the construction of the Brokdorf Nuclear Power Plant on the North Sea coast west of Hamburg. Some 100,000 people came face to face with 10,000 police officers. Twenty-one policemen were injured by demonstrators armed with gasoline bombs, sticks, stones and high-powered slingshots.\n\nThe largest anti-nuclear protest was most likely a 1983 nuclear weapons protest in West Berlin which had about 600,000 participants.\n\nIn October 1983, nearly 3 million people across western Europe protested nuclear missile deployments and demanded an end to the arms race. The largest turnout of protesters occurred in West Germany when, on a single day, 400,000 people marched in Bonn, 400,000 in Hamburg, 250,000 in Stuttgart, and 100,000 in West Berlin.\n\nIn May 1986, following the Chernobyl disaster, clashes between anti-nuclear protesters and West German police became common. More than 400 people were injured in mid-May at the site of a nuclear-waste reprocessing plant being built near Wackersdorf. Police \"used water cannons and dropped tear-gas grenades from helicopters to subdue protesters armed with slingshots, crowbars and Molotov cocktails\".\n\nDuring a weekend in October 2008, some 15,000 people disrupted the transport of radioactive nuclear waste from France to a dump in Germany. This was one of the largest such protests in many years and, according to \"Der Spiegel\", it signals a revival of the anti-nuclear movement in Germany. In 2009, the coalition of green parties in the European parliament, who are unanimous in their anti-nuclear position, increased their presence in the parliament from 5.5% to 7.1% (52 seats).\n\nA convoy of 350 farm tractors and 50,000 protesters took part in an anti-nuclear rally in Berlin on September 5, 2009. The marchers demanded that Germany close all nuclear plants by 2020 and close the Gorleben radioactive dump. Gorleben is the focus of the anti-nuclear movement in Germany, which has tried to derail train transports of waste and to destroy or block the approach roads to the site. Two above-ground storage units house 3,500 containers of radioactive sludge and thousands of tonnes of spent fuel rods.\n\nFollowing the Fukushima I nuclear accidents, anti-nuclear opposition intensified in Germany. On 12 March 2011, 60,000 Germans formed a 45-km human chain from Stuttgart to the Neckarwestheim power plant. On 14 March, 110,000 people protested in 450 other German towns, with opinion polls indicating 80% of Germans opposed the government's extension of nuclear power. On March 15, 2011, Angela Merkel said that seven nuclear power plants which went online before 1980 would be temporarily closed and the time would be used to study speedier renewable energy commercialization.\n\nIn March 2011, more than 200,000 people took part in anti-nuclear protests in four large German cities, on the eve of state elections. Organisers called it the biggest anti-nuclear demonstration the country has seen. Thousands of Germans demanding an end to the use of nuclear power took part in nationwide demonstrations on 2 April 2011. About 7,000 people took part in anti-nuclear protests in Bremen. About 3,000 people protested outside of RWE's headquarters in Essen.\n\nThousands of Germans demanding an end to the use of nuclear power took part in nationwide demonstrations on 2 April 2011. About 7,000 people took part in anti-nuclear protests in Bremen. About 3,000 people protested outside of RWE's headquarters in Essen. Other smaller rallies were held elsewhere.\n\nChancellor Angela Merkel's coalition announced on May 30, 2011, that Germany’s 17 nuclear power stations will be shut down by 2022, in a policy reversal following Japan's Fukushima I nuclear accidents. Seven of the German power stations were closed temporarily in March, and they will remain off-line and be permanently decommissioned. An eighth was already off line, and will stay so.\n\nIn November 2011, thousands of anti-nuclear protesters delayed a train carrying radioactive waste from France to Germany. Many clashes and obstructions made the journey the slowest one since the annual shipments of radioactive waste began in 1995. The shipment, the first since Japan's Fukishima nuclear disaster, faced large protests in France where activists damaged the train tracks.\n\nFollowing the March 2011 Fukushima disaster, many are questioning the mass roll-out of new plants in India, including the World Bank, the former Indian Environment Minister, Jairam Ramesh, and the former head of the country's nuclear regulatory body, A. Gopalakrishnan. The massive Jaitapur Nuclear Power Project is the focus of concern — \"931 hectares of farmland will be needed to build the reactors, land that is now home to 10,000 people, their mango orchards, cashew trees and rice fields\" — and it has attracted many protests. Fishermen in the region say their livelihoods will be wiped out.\n\nEnvironmentalists, local farmers and fishermen have been protesting for months over the planned six-reactor nuclear power complex on the plains of Jaitapur, 420 km south of Mumbai. If built, it would be one of the world's largest nuclear power complexes. Protests have escalated in the wake of Japan's Fukushima I nuclear accidents. During two days of violent rallies in April 2011, a local man was killed and dozens were injured.\n\nAs of October 2011, thousands of protesters and villagers living around the Russian-built Koodankulam Nuclear Power Plant in the southern Tamil Nadu province, are blocking highways and staging hunger strikes, preventing further construction work, and demanding its closure as they fear of the disasters like the Environmental impact of nuclear power, Radioactive waste, nuclear accident similar to the releases of radioactivity in March at Japan's Fukushima nuclear disaster.\n\nA Public Interest Litigation (PIL) has also been filed against the government’s civil nuclear program at the apex Supreme Court. The PIL specifically asks for the \"staying of all proposed nuclear power plants till satisfactory safety measures and cost-benefit analyses are completed by independent agencies\".\n\nThe People's Movement Against Nuclear Energy is an anti-nuclear power group in Tamil Nadu, India. The aim of the group is to close the Kudankulam Nuclear Power Plant site and to preserve the largely untouched coastal landscape, as well as educate locals about nuclear power. In March 2012, police said they had arrested nearly 200 anti-nuclear activists who were protesting the restart of work at the long-stalled nuclear power plant. Engineers have resumed working on one of two 1,000-megawatt Koodankulam nuclear reactors a day after the local government gave the green light for the resumption of the Russia-backed project.\n\nIn May 1986, an estimated 150,000 to 200,000 people marched in Rome to protest against the Italian nuclear program, and 50,000 marched in Milan.\n\nIn March 1982 some 200,000 people participated in a nuclear disarmament rally in Hiroshima. In May 1982, 400,000 people demonstrated in Tokyo. In mid-April, 17,000 people protested at two demonstrations in Tokyo against nuclear power.\n\nIn 1982, Chugoku Electric Power Company proposed building a nuclear power plant near Iwaishima, but many residents opposed the idea, and the island’s fishing cooperative voted overwhelmingly against the plans. In January 1983, almost 400 islanders staged a protest march, which was the first of more than 1,000 protests the islanders carried out. Since the Fukushima nuclear disaster in March 2011 there has been wider opposition to construction plans for the plant.\n\nResearch results show that some 95 post-war attempts to site and build nuclear power plants resulted in only 54 completions. Many affected communities \"fought back in highly publicized battles\". Co-ordinated opposition groups, such as the Citizens' Nuclear Information Center and the anti-nuclear newspaper \"Hangenpatsu Shinbun\" have operated since the early 1980s. Cancelled plant orders included:\n\n\nIn May 2006, an international awareness campaign about the dangers of the Rokkasho Reprocessing Plant, Stop Rokkasho, was launched by musician Ryuichi Sakamoto. Greenpeace has opposed the Rokkasho Reprocessing Plant under a campaign called \"Wings of Peace – No more Hiroshima Nagasaki\", since 2002 and has launched a cyberaction to stop the project. Consumers Union of Japan together with 596 organisations and groups participated in a parade on 27 January 2008 in central Tokyo against the Rokkasho Reprocessing Plant. Over 810,000 signatures were collected and handed in to the government on 28 January 2008. Representatives of the protesters, which include fishery associations, consumer cooperatives and surfer groups, handed the petition to the Cabinet Office and the Ministry of Economy, Trade and Industry. Seven consumer organisations have joined in this effort: Consumers Union of Japan, Seikatsu Club Consumer's Co-operative Union, Daichi-o-Mamoru Kai, Green Consumer's Co-operative Union, Consumer's Co-operative Union \"Kirari\", Consumer's Co-operative Miyagi and Pal-system Co-operative Union. In June 2008, several scientists stated that the Rokkasho plant is sited directly above an active geological fault line that could produce a magnitude 8 earthquake. But Japan Nuclear Fuel Limited have stated that there was no reason to fear an earthquake of more than magnitude 6.5 at the site, and that the plant could withstand a 6.9 quake.\n\nThree months after the Fukushima nuclear disaster, thousands of anti-nuclear protesters marched in Japan. Company workers, students, and parents with children rallied across Japan, \"venting their anger at the government's handling of the crisis, carrying flags bearing the words 'No Nukes!' and 'No More Fukushima'.\" Problems in stabilizing the Fukushima I plant have hardened attitudes to nuclear power. As of June 2011, \"more than 80 percent of Japanese now say they are anti-nuclear and distrust government information on radiation\". The ongoing Fukushima crisis may spell the end of nuclear power in Japan, as \"citizen opposition grows and local authorities refuse permission to restart reactors that have undergone safety checks\". Local authorities are skeptical that sufficient safety measures have been taken and are reticent to give their permission – now required by law – to bring suspended nuclear reactors back online. More than 60,000 people in Japan marched in demonstrations in Tokyo, Osaka, Hiroshima and Fukushima on June 11, 2011.\n\nIn July 2011, Japanese mothers, many new to political activism, have started \"taking to the streets to urge the government to protect their children from radiation leaking from the crippled Fukushima No. 1 nuclear plant\". Using social networking media, such as Facebook and Twitter, they have \"organized antinuclear energy rallies nationwide attended by thousands of protesters\".\n\nIn September 2011, anti-nuclear protesters, marching to the beat of drums, \"took to the streets of Tokyo and other cities to mark six months since the March earthquake and tsunami and vent their anger at the government's handling of the nuclear crisis set off by meltdowns at the Fukushima power plant\". An estimated 2,500 people marched past TEPCO headquarters, and created a human chain around the building of the Trade Ministry that oversees the power industry. Protesters called for a complete shutdown of Japanese nuclear power plants and demanded a shift in government policy toward alternative sources of energy. Among the protestors were four young men who started a 10-day hunger strike to bring about change in Japan's nuclear policy.\n\nTens of thousands of people marched in central Tokyo in September 2011, chanting \"Sayonara nuclear power\" and waving banners, to call on Japan's government to abandon atomic energy in the wake of the Fukushima nuclear disaster. Author Kenzaburō Ōe, who won the Nobel Prize for literature in 1994, and has campaigned for pacifist and anti-nuclear causes addressed the crowd. Musician Ryuichi Sakamoto, who composed the score to the movie \"The Last Emperor\" was also among the event's supporters.\n\nThousands of demonstrators took to the streets of Yokohama on the weekend of January 14–15, 2012, to show their support for a nuclear power-free world. The demonstration showed that organized opposition to nuclear power has gained momentum in the wake of the Fukushima nuclear disaster. The most immediate demand was for the protection of rights for those affected by the Fukushima accident, including basic human rights such as health care, living standards and safety.\n\nOn the anniversary of the 11 March 2011 earthquake and tsunami all over Japan protesters called for the abolishment of nuclear power, and the scrapping of nuclear reactors.\n\nIn June 2012, tens of thousands of protesters participated in anti-nuclear power rallies in Tokyo and Osaka, over the government's decision to restart the first idled reactors since the Fukushima disaster, at Oi Nuclear Power Plant in Fukui Prefecture.\n\nFrom the early 1960s New Zealand peace groups CND and the Peace Media organised nationwide anti-nuclear campaigns in protest of atmospheric testing in French Polynesia. These included two large national petitions presented to the New Zealand government which led to a joint New Zealand and Australian Government action to take France to the International Court of Justice (1972). In 1972, Greenpeace and an amalgam of New Zealand peace groups managed to delay nuclear tests by several weeks by trespassing with a ship in the testing zone. During the time, the skipper, David McTaggart, was beaten and severely injured by members of the French military.\n\nOn 1 July 1972, the Canadian ketch \"Vega\", flying the Greenpeace III banner, collided with the French naval minesweeper \"La Paimpolaise\" while in international waters to protest French nuclear weapon tests in the South Pacific.\n\nIn 1973 the New Zealand Peace Media organised an international flotilla of protest yachts including the Fri, Spirit of Peace, Boy Roel, Magic Island and the Tanmure to sail into the test exclusion zone. Also in 1973, New Zealand Prime Minister Norman Kirk as a symbolic act of protest sent two navy frigates, HMNZS Canterbury and HMNZS Otago, to Mururoa. They were accompanied by HMAS \"Supply\", a fleet oiler of the Royal Australian Navy.\n\nIn 1985 the Greenpeace ship \"Rainbow Warrior\" was bombed and sunk by the French DGSE in Auckland, New Zealand, as it prepared for another protest of nuclear testing in French military zones. One crew member, Fernando Pereira of Portugal, photographer, drowned on the sinking ship while attempting to recover his photographic equipment. Two members of DGSE were captured and sentenced, but eventually repatriated to France in a controversial affair.\n\nIn the Philippines, a focal point for protests in the late 1970s and 1980s was the proposed Bataan Nuclear Power Plant, which was built but never operated. The project was criticised for being a potential threat to public health, especially since the plant was located in an earthquake zone.\n\nIn March 2012, environmental conservation groups staged a rally in central Seoul to voice opposition to nuclear power on the first anniversary of the Fukushima nuclear disaster. According to organizers, over 5,000 people attended, and the turnout was one of the biggest in recent memory for an antinuclear demonstration. The rally adopted a declaration demanding that President Lee Myung Bak abandon his policy to promote nuclear power.\n\nIn Spain, in response to a surge in nuclear power plant proposals in the 1960s, a strong anti-nuclear movement emerged in 1973, which ultimately impeded the realisation of most of the projects. On July 14, 1977, in Bilbao, Spain, between 150,000 and 200,000 people protested against the Lemoniz Nuclear Power Plant. This has been called the \"biggest ever anti-nuclear demonstration\".\n\nIn June 2010, Greenpeace anti-nuclear activists invaded Forsmark nuclear power plant to protest the then-plan to remove the government prohibition on building new nuclear power plants. In October 2012, 20 Greenpeace activists scaled the outer perimeter fence of the Ringhals nuclear plant, and there was also an incursion of 50 activists at the Forsmark plant. Greenpeace said that its non-violent actions were protests against the continuing operation of these reactors, which it says are unsafe in European stress tests, and to emphasise that stress tests did nothing to prepare against threats from outside the plant. A report by the Swedish nuclear regulator said that \"the current overall level of protection against sabotage is insufficient\". Although Swedish nuclear power plants have security guards, the police are responsible for emergency response. The report criticised the level of cooperation between nuclear site staff and police in the case of sabotage or attack.\n\nIn May 2011, some 20,000 people turned out for Switzerland's largest anti-nuclear power demonstration in 25 years. Demonstrators marched peacefully near the Beznau Nuclear Power Plant, the oldest in Switzerland, which started operating 40 years ago. Days after the anti-nuclear rally, Cabinet decided to ban the building of new nuclear power reactors. The country’s five existing reactors would be allowed to continue operating, but \"would not be replaced at the end of their life span\".\n\nIn March 2011, around 2,000 anti-nuclear protesters demonstrated in Taiwan for an immediate end to the construction of the island's fourth nuclear power plant. The protesters were also opposed to lifespan extensions for three existing nuclear plants.\n\nIn May 2011, 5,000 people joined an anti-nuclear protest in Taipei City, which had a carnival-like atmosphere, with protesters holding yellow banners and waving sunflowers. This was part of a nationwide \"No Nuke Action\" protest, against construction of the fourth nuclear plant and in favor of a more renewable energy policy.\n\nOn World Environment Day in June 2011, environmental groups demonstrated against Taiwan's nuclear power policy. The Taiwan Environmental Protection Union, together with 13 environmental groups and legislators, gathered in Taipei and protested against the nation’s three operating nuclear power plants and the construction of the fourth plant.\n\nIn March 2012, about 2,000 people staged an anti-nuclear protest in Taiwan's capital following the massive earthquake and tsunami that hit Japan one year ago. The protesters rallied in Taipei to renew calls for a nuclear-free island by taking lessons from Japan's disaster on March 11, 2011. They \"want the government to scrap a plan to operate a newly constructed nuclear power plant – the fourth in densely populated Taiwan\". Scores of aboriginal protesters \"demanded the removal of 100,000 barrels of nuclear waste stored on their Orchid Island, off south-eastern Taiwan. Authorities have failed to find a substitute storage site amid increased awareness of nuclear danger over the past decade\".\n\nIn March 2013, 68,000 Taiwanese protested across major cities against the island’s fourth nuclear power plant, which is under construction. Taiwan’s three existing nuclear plants are near the ocean, and prone to geological fractures, under the island.\n\nActive seismic faults run across the island, and some environmentalists argue Taiwan is unsuited for nuclear plants. Construction of the Lungmen Nuclear Power Plant using the ABWR design has encountered public opposition and a host of delays, and in April 2014 the government decided to halt construction.\n\nIn October 1983, nearly one million people assembled in the Hague to protest nuclear missile deployments and demand an end to the arms race.\n\nThe first Aldermaston March organised by the Campaign for Nuclear Disarmament took place at Easter 1958, when several thousand people marched for four days from Trafalgar Square, London, to the Atomic Weapons Research Establishment close to Aldermaston in Berkshire, England, to demonstrate their opposition to nuclear weapons. The Aldermaston marches continued into the late 1960s when tens of thousands of people took part in the four-day marches.\n\nMany significant anti-nuclear mobilizations in the 1980s occurred at the Greenham Common Women's Peace Camp. It began in September 1981 after a Welsh group called \"Women for Life on Earth\" arrived at Greenham to protest against the decision of the Government to allow cruise missiles to be based there. The women's peace camp attracted significant media attention and \"prompted the creation of other peace camps at more than a dozen sites in Britain and elsewhere in Europe\". In December 1982 some 30,000 women from various peace camps and other peace organisations held a major protest against nuclear weapons on Greenham Common.\n\nOn 1 April 1983, about 70,000 people linked arms to form a human chain between three nuclear weapons centres in Berkshire. The anti-nuclear demonstration stretched for 14 miles along the Kennet Valley.\n\nIn London, in October 1983, more than 300,000 people assembled in Hyde Park. This was \"the largest protest against nuclear weapons in British history\", according to \"The New York Times\".\n\nIn 2005 in Britain, there were many protests about the government's proposal to replace the aging Trident weapons system with a newer model. The largest protest had 100,000 participants and, according to polls, 59 percent of the public opposed the move.\n\nIn October 2008 in the United Kingdom, more than 30 people were arrested during one of the largest anti-nuclear protests at the Atomic Weapons Establishment at Aldermaston for 10 years. The demonstration marked the start of the UN World Disarmament Week and involved about 400 people.\n\nIn October 2011, more than 200 protesters blockaded the Hinkley Point C nuclear power station site. Members of several anti-nuclear groups that are part of the Stop New Nuclear alliance barred access to the site in protest at EDF Energy's plans to renew the site with two new reactors.\n\nIn January 2012, three hundred anti-nuclear protestors took to the streets of Llangefnia, against plans to build a new nuclear power station at Wylfa. The march was organised by a number of organisations, including Pobl Atal Wylfa B, Greenpeace and Cymdeithas yr Iaith, which are supporting farmer Richard Jones who is in dispute with Horizon.\n\nOn March 10, 2012, the first anniversary of the Fukushima nuclear disaster, hundreds of anti-nuclear campaigners formed a symbolic chain around Hinkley Point to express their determined opposition to new nuclear power plants, and to call on the coalition government to abandon its plan for seven other new nuclear plants across the UK. The human chain continued for 24 hours, with the activists blocking the main Hinkley Point entrance.\n\nIn April 2013, thousands of Scottish campaigners, MSPs, and union leaders, rallied against nuclear weapons. The Scrap Trident Coalition wants to see an end to nuclear weapons, and says saved monies should be used for health, education and welfare initiatives. There was also a blockade of the Faslane Naval Base, where Trident missiles are stored.\n\nOn November 1, 1961, at the height of the Cold War, about 50,000 women brought together by Women Strike for Peace marched in 60 cities in the United States to demonstrate against nuclear weapons. It was the largest national women's peace protest of the 20th century.\n\nOn May 2, 1977, 1,414 Clamshell Alliance protesters were arrested at Seabrook Station Nuclear Power Plant.\nThe protesters who were arrested were charged with criminal trespass and asked to post bail ranging from $100 to $500. They refused and were then held in five national guard armories for 12 days. The Seabrook conflict, and role of New Hampshire Governor Meldrim Thomson, received much national media coverage.\n\nThe American public were concerned about the release of radioactive gas from the Three Mile Island accident in 1979 and many mass demonstrations took place across the country in the following months. The largest one was held in New York City in September 1979 and involved two hundred thousand people; speeches were given by Jane Fonda and Ralph Nader.\n\nOn June 3, 1981, Thomas launched the longest running peace vigil in US history at Lafayette Square in Washington, D.C.. He was later joined on the White House Peace Vigil by anti-nuclear activists Concepcion Picciotto and Ellen Benjamin.\n\nOn June 12, 1982, one million people demonstrated in New York City's Central Park against nuclear weapons and for an end to the cold war arms race. It was the largest anti-nuclear protest and the largest political demonstration in American history.\n\nBeginning in 1982, an annual series of Christian peace vigils called the \"Lenten Desert Experience\" were held over a period of several weeks at a time, at the entrance to the Nevada Test Site in the USA. This led to a faith-based aspect of the nuclear disarmament movement and the formation of the anti-nuclear Nevada Desert Experience group.\n\nThe Seneca Women's Encampment for a Future of Peace and Justice was located in Seneca County, New York, adjacent to the Seneca Army Depot. It took place mainly during the summer of 1983. Thousands of women came to participate and rally against nuclear weapons and the \"patriarchal society\" that created and used those weapons. The purpose of the Encampment was to stop the scheduled deployment of Cruise and Pershing II missiles before their suspected shipment from the Seneca Army Depot to Europe that fall. The Encampment continued as an active political presence in the Finger Lakes area for at least 5 more years.\n\nHundreds of people walked from Los Angeles to Washington, D.C. in 1986 in what is referred to as the Great Peace March for Global Nuclear Disarmament. The march took nine months to traverse , advancing approximately fifteen miles per day.\n\nOther notable anti-nuclear protests in the United States have included:\n\nAnti-nuclear protests preceded the shutdown of the Shoreham, Yankee Rowe, Millstone I, Rancho Seco, Maine Yankee, and about a dozen other nuclear power plants.\n\nOn May 1, 2005, 40,000 anti-nuclear/anti-war protesters marched past the United Nations in New York, 60 years after the atomic bombings of Hiroshima and Nagasaki. This was the largest anti-nuclear rally in the U.S. for several decades.\n\nIn 2008 and 2009, there have been protests about, and criticism of, several new nuclear reactor proposals in the United States. There have also been some objections to license renewals for existing nuclear plants.\n\nIn May 2010, some 25,000 people, including members of peace organizations and 1945 atomic bomb survivors, marched for about two kilometers from downtown New York to a square in front of United Nations headquarters, calling for the elimination of nuclear weapons. The march occurred ahead of the opening of the review conference on the Non-Proliferation of Nuclear Weapons Treaty (NPT).\n\nThe anti-nuclear organisation \"Nevada Semipalatinsk\" was formed in 1989 and was one of the first major anti-nuclear groups in the former Soviet Union. It attracted thousands of people to its protests and campaigns which eventually led to the closure of the nuclear test site at Semipalatinsk, in north-east Kazakhstan, in 1991. The Soviet Union conducted over 400 nuclear weapons tests at the Semipalatinsk Test Site between 1949 and 1989. The United Nations believes that one million people were exposed to radiation.\n\n"}
{"id": "39392097", "url": "https://en.wikipedia.org/wiki?curid=39392097", "title": "Atlantic Rim (film)", "text": "Atlantic Rim (film)\n\nAtlantic Rim (also known as Attack from Beneath, Attack from the Atlantic Rim and From the Sea) is a low budget 2013 American science fiction monster film produced by The Asylum and directed by Jared Cohn. Shot in Pensacola, Florida, the film stars Graham Greene, David Chokachi, Treach, and Jackie Moore.\n\nThe film was released direct-to-DVD on June 24, 2013 in the United Kingdom, and on July 9 in the United States. In the tradition of The Asylum's catalog, \"Atlantic Rim\" is a cheap mockbuster of the Warner Bros./Legendary Pictures film \"Pacific Rim\".\n\nFollowing the mysterious disappearance of an oil rig and a reconnaissance mini-submarine in the Gulf of Mexico, scientist Dr. Margaret Adams initiates the Armada Program, which consists of giant robots designed for deep sea rescue. The three robots — piloted by Red, Tracy and Jim — dive nearly 800 fathoms to the sea bed, where they not only discover the mangled remains of the oil rig, but encounter the monster that brought it down. Red pursues the monster, against orders from Admiral Hadley, prompting the Admiral to order every naval fleet on the East Coast to converge on the oil rig's site. Red emerges on a beach to warn the bystanders to leave the area; he is suddenly attacked from behind by the monster as their fight takes its toll on the city. An F-18 Hornet piloted by Spitfire assists Red in taking the monster down. Red, however, is arrested for disobeying a direct order. He is locked in solitary confinement until he is briefly released by Adm. Hadley and later given a medal of honor for his heroic actions, before serving the rest of his confinement.\n\nLater, Adm. Hadley is informed by Sheldon Geise of a top-secret sonar program that discovered the monsters, which are hundreds of millions of years old and lay their eggs on a mixture of crude oil and saltwater. Two eggs have been discovered, one of which hatched into the monster that Red and Spitfire killed. Adm. Hadley orders a search for the other egg, but he is too late, as it has already hatched, with the second monster, much bigger than the first, feeding on the corpse of the first monster and destroying a whole naval fleet before wreaking havoc on the city. As the monster attacks the naval base, Tracy and Jim scramble to spring Red out of solitary before they are picked up by Lt. Wexler. Meanwhile, Geise informs Adm. Hadley that the President has authorized a nuclear strike on the monster, but Adm. Hadley defies that decision and orders everyone to evacuate the base. The monster retreats after a Northrop Grumman B-2 Spirit drops a payload on it. Adm. Hadley is later informed that another egg has hatched off the Atlantic Coast.\n\nDr. Adams gives the trio special \"halo\" headbands that neurally link them to their robots, increasing their reflexes by using their direct body movements instead of joysticks. The system's downside is the pilot feeling pain for every damage the robot takes. After a crash course on the new system, the trio fly their robots to New York City to battle the monster. Following numerous refusals by Adm. Hadley to launch a nuclear strike, Geise orders the \"USS Virginia\" to launch a warhead. Red intercepts the missile and jams its frequency, saving the city from a nuclear holocaust. In retaliation, Geise threatens to shut down the robots, but is quickly subdued by Lt. Wexler, despite shooting the Admiral in the arm. During the battle, Tracy loses consciousness when her neural level goes critical. Jim takes Tracy to safety while Red grabs the warhead and the monster before flying them to the atmosphere. He then kicks the monster to deep space, detonating the warhead in the process and sending him crashing back to Earth. The trio and Adm. Hadley celebrate by heading to the local bar for some tequila shots.\n\n\"Atlantic Rim\" was originally planned to be shot at Naval Air Station Pensacola in Florida, but The Asylum were denied permission to film at the base after a high-ranking official read the script and disagreed with the portrayal of the soldiers. As a result, the production team relocated to a private helicopter airport that served as a stand-in for at least seven locations for the film. The film's script underwent at least nine rewrites during production due to weather conditions and the last-minute relocation.\n\nDave Pace of \"Fangoria\" gave the film two out of four stars, calling it \"a testament to why there aren't many live action giant robot vs. monster movies. It's a very difficult thing to do right and keep the audience on your side. \"ATLANTIC RIM\" manages to be enjoyable as a bit of a goof and works on the 'so bad it's good' level.\" \"Dread Central\" gave the film three out of five stars, describing it as \"the ultimate monster movie about booze-hounding broskis in battle bots saving New York City from a crazy-eyed giant sea beast that frequently appears to be merely a lost animal, confused and irritated that these metal men won't stop hitting it.\"\n\nOn October 5, 2017, it was announced that a sequel had wrapped filming. The film, entitled \"Resurrection\", was released in March 2018.\n\nIt is one of six films featured in Season 12 of \"Mystery Science Theater 3000\", and is the first 21st century film riffed on the show.\n\n"}
{"id": "49483110", "url": "https://en.wikipedia.org/wiki?curid=49483110", "title": "Auto Research Center", "text": "Auto Research Center\n\nAuto Research Center, also known as ARC Indy, is a research and development company with headquarters in Indianapolis, Indiana USA. It was founded as Reynard Motorsports North American headquarters, and became its own company in 2002.\n\nARC was opened in 1998, as the North American headquarters for Reynard Motorsports. Reynard Motorsports was based at Reynard Park, Brackley, England and at that time was the largest race car designer and manufacture in the world, producing cars for both open wheel and sportscar racing. In the USA, the primary effort was on open wheel in Indycar/CART.\n\nARC was initially housed a scale model wind tunnel and 7-post test rig, the engineering design suite, scale model modeling making/building and tech office were all in the same building. While ARC was a public test facility, in reality Reynard had enough customers that the facility was completely booked by Indycar/CART and the British American Racing F1 race team. Reynard Motorsports went into receivership in 2002, owing to many reasons, but primarily an aborted IPO and costly purchase of Riley & Scott race car manufacture. As ARC was an arm of Reynard Motorsports, it found itself on its own without a parent company.\n\nWith staff and a small budget on hand, but no customers, ARC set about to build a NASCAR scale model to entice new customers. The first major customers were NASCAR OEM manufactures, who were impressed with the scale model that had been built. From there Sportscar racing also became a large focus from other customers. Thus, ARC found itself with a new direction.\n\nIn the middle 2000s, ARC expanded its services with the addition of its driveline dyno test rig rapid prototyping and 3d laser scanning. Then, around the start of 2010’s ARC start to further expand its customer base with production OEM scale model testing and commercial semi-trailer truck industry. In 2008 ARC expanded upon its Indianapolis Headquarter, breaking ground for a large expansion. The engineering suite and scale model making services were all moved to the new addition to help handle the growing increase in customers and expansion from on-going customers.\n\nARC timed the commercial trucking market perfectly, as a time when the industry was starting to realize the full importance of aerodynamics, by reducing drag, to save money on fuel costs. ARC took the industry by storm, creating an impact from the start and over time working with almost every major commercial OEM and aftermarket producer. ARC has also become an EPA certified testing facility. After that ARC expanded into CFD, computational fluid dynamics. ARC formed a joint venture with Streamline Solutions to development their own CFD software, ELEMENTS and purchased its own processing cores. This made Streamline Solutions the first and only CFD supplier in the world that owns/operates test facilities to actually validate its own code. ARC also created a skunkworks research and development engineering division to help push ARC’s innovation in the industry further forward. One such product of skunkworks was the creation of pneumatic tires for scale wind tunnel testing; something only F1 teams had IP technology of. Prior to this carbon fiber tires had been primarily in the scale model wind tunnels.\n\nIn 2015 ARC opened its Aerodynamic Bicycle Testing Facility (ABT). This new facility housed in the ARC wind tunnel, further shows the growth the company has taken upon, while ensuring that the customer interest is there in all of ARC’s many solutions and services.\n\nARC continues to keep its commitment to further technology in transportation industries, with new innovates, such as wind tunnel wall simulation that can automatically move, yaw and curve to increase accuracy of aerodynamic results for race and driver simulations. ARC is a global company with offices located on four continents.\n\nThe wind tunnel at ARC was designed with the automotive industry in mind. The tunnel is a 3/4 open plenum configuration with an optional rolling road and primary and secondary boundary layer control treatments. The tunnel nozzle has a cross section that is 2.3 m wide by 2.1 m high. Force measurements are gathered utilizing a 6 degree of freedom balance mounted internally between the tested model a vertical sting. The balance includes an integrated model motion system capable of automated pitch, roll, yaw, and vertical translation with laser measurement feedback. Additional capabilities can be included in the model for incorporation automatically or dependently controlled motors (i.e. wheel steer). In addition to force measurements the tunnel has capabilities for pressure tap measurements, anemometer measurements, and flow visualization including tufts and oil droplets. Beyond the standard test measurements new capabilities have been added for measuring torque and rotation rate, as well as braking controls for wind turbine experiments. \n\n"}
{"id": "7498299", "url": "https://en.wikipedia.org/wiki?curid=7498299", "title": "Bamboo fly rod", "text": "Bamboo fly rod\n\nA bamboo fly rod or a split cane rod is a fly fishing rod that is made from bamboo. The British generally use the term \"split cane.\" In the U.S., most use the term \"bamboo.\" The \"heyday\" of bamboo fly rod production and use was an approximately 75-year period from the 1870-s to the 1950s when fiberglass became the predominant material for fly rods. Nevertheless, bamboo fly rods made from skilled makers continue to be 'state-of-the-art' in performance and are cherished and revered by their owners.\n\nWith more than 1,000 different bamboo species and nearly a hundred different kinds, Tonkin Cane (\"Arundinaria amabilis\" or \"Pseudosasa amabilis\") is most often used for fishing rods, replacing Calcutta cane which was used extensively prior.\nThis bamboo species originally grew on only approximately 190 km² (48,000 acres) up the Sui River in the Tonkin Gulf region of Guangdong Province in China. It is said to be one of the strongest bamboo species because of its high density of fibers. This high density is what the bamboo fly rod maker is after because this gives the rod its strength and flexibility. It also is selected because of its straightness, and well-spaced nodes.\n\nThe bamboo culms are split and shaped into strips of equilateral triangles that taper to precise dimensions. Tolerances are held to .001\". These precise dimensions determine the diameter of the rod when the strips are laminated into a hexagonal, square and sometimes octagonal cross-sections. Many remark that a bamboo rod resembles a pencil in shape. The diameter of the rod or blank is measured every few inches. These measurements make a 'taper', which shows how the rod goes from the fine tip to thick butt section. This is the recipe for the overall performance of the blank. This process, together with the wrapping of the guides with very fine silk thread, varnishing and making of the cork grip and wooden reel seat, can take a craftsman more than forty hours.<ref name=\"Garrison/Carmichael\">Garrison, Everett and Carmichael, Hoagy B. (1997). A Master's Guide To Building A Bamboo Fly Rod. Far Hills, New Jersey: Meadow Run Press.</ref>\n\nPrior to the 1800s, most, if not all, flyfishermen used wooden rods. Some may have used solid bamboo rods, or \"cane poles.\" France, England, China and the U.S. all claim to have been the birthplace of the modern \"split cane\" rod. In the early 1800s, quite a few people began experimenting with splitting the cane and re-gluing in 2,3 and 4 sections. Samuel Phillipe of Easton, Pennsylvania is credited (at least by Americans) as being the first to produce such a multi-sided rod. However, the use of such rods did not become commonplace until after the Civil War when makers and manufacturers sought to supply goods to the expanding nation through the use of railroads and the U.S. Mail.\n\nInitially, the rodmakers were gunsmiths and other craftsmen like H.L Leonard, whom Americans credit with creating the first six-sided rod, the configuration that is still predominant today. Leonard began making rods in 1874, and continued to do so until his death in 1907. Square or quadrate rods were the first rods Leonard attempted to make, but he eventually started making 6 strip or hexagonal rods because of commercial reasons. At that time good quality cane was hard to find. What was available was often full of scorch marks and insect damage. For this reason it was easier to acquire six strips of good quality cane than 4 wider strips for the quadrate rod. Bill Edwards, Clarence \"Sam\" Carlson and Ebenezer Green produced quadrate rods and others even made bamboo rods which had pentagonal and octagonal cross-sections.\n\nHe did not make only the rods, the H.L. Leonard rod company made machinery to produce cane/ bamboo fly rods. The most important of these was the beveler. Some of the greatest fly rod makers learned their craft under Leonard and later opened their own rod shops. The company would continue to make rods for almost eight decades under various ownership, including surviving a fire in 1964 which virtually destroyed the shop. In 1984, the Leonard Rod Company closed its doors. The machinery from the shop, including the beveler, was purchased at auction by Marc Aroner who continues to make rods under his own name using the equipment.\n\nThe Leonard Rod Company found competition early in the game. In 1868, the colorful Thomas Chubb opened a rod manufacturing plant in Thetford Mass. By 1875, he would employ 50 people and market his products through the well known Chubb mail-order catalogs. The company sold not only flyrods, but all the parts necessary to build them. They manufactured and marketed their higher quality rods as \"trademarked rods\" bearing the Chubb star. They sold those and cheaper rods without the trademark through their catalogs, The also sold many unmarked rods to large retailers who would place the retailer's markings on the rod. This letter group of rods were known as \"trade rods.\"\n\nAt the turn of the century, the Thomas Chubb Rod Company was bought out by a group headed by Evander Bartlett, the Montague Rod & Reel Company. This company would become the giant of the bamboo rod making industry. Initially, they continued to produce rods under both the Chubb and Montague names. The Chubbs were often brand, the Montagues were rarely marked. The vast majority were sold as trade rods in the early part of the century. Many unmarked rods of 1900 to 1930 are referred to as Chubb/Momtys today, though other manufacturers like Horrocks Ibbotson and Union Hardware also produced many thousands of unmarked trade rods. The Depression created a need for economic efficiency that saw the closing of the Chubb plant, and consolidation at Montague's main plant in Pelham Mass. The Chubb name disappeared.\n\nAlthough Montague would continue to be the leader in the production of trade rods, Montague began to market their rods with their own decals as the 1920s ended. They made over 50 different models that ranged in price from $3.00 to $35.00 in the 1930s. The Montague Red Wing is the most commonly seen example of the high quality rods seen today. The Rapidan is the most common midlevel, and the Subeam the most common lower level rods seen today.\n\nLeonard and Montague stand as examples of two different levels of bamboo fly rods: the craftsman vs. the large manufacturer. Other well known craftsman type makers were Paul Young, The Edwards Family, F.E, Thomas, LL Dickerson, H.W. Hawes and Bill Phillipson. High quality smaller manufacturers include Winston and Orvis - which still make high quality bamboo rods to this day. Larger manufacturers included high quality Heddon, and South Bend and bargain basement Horrocks Ibbotsons and Union Hardware.\n\nBamboo soon became the preferred material for all fishing rods with Tonkin cane being prized above other species. This continued to 1950 when a trade embargo was imposed on Chinese goods. Due to the resultant shortage of quality bamboo and the concurrent development of synthetic fibers the fabrication of bamboo rods nearly stopped. By the time the embargo ended in the early seventies only a handful of craftsmen were still making bamboo rods. The main reason for bamboo rods regaining their popularity was a result of Everett Garrison together with Hoagy Bix Carmichael publishing bamboo rod building ‘secrets’ in their book \"A Masters guide to building a bamboo fly rod\".\n\nBamboo rods produce a smooth, fluid backcast which provides its own 'damping' effect at the end of the backcast. The forward cast accelerates the line throw through the air with the same 'damping' effect at the beginning of the cast, and then again at the end of the cast as the caster lays the line out over the target water—generally with smooth, precise placement. Expert fisherman and enthusiasts alike have heralded the performance of the bamboo rod as being likened to that of a fine musical instrument. Master craftsman and bamboo innovators such as: H.L. Leonard, E.W. Edwards, Jim Payne (fishing rodmaker) and Everett Garrison have elevated the bamboo fly rod from the realm of sporting goods to that of fine art.\n\nBamboo rods will benefit from extra care from their owners. With a little extra maintenance, a bamboo fly rod can be used for many decades. The rods should be cleaned and stored in a cool, dry place away from sunlight. Owners are encouraged to avoid bending the rod at acute angles when playing a fish and rod sections should be separated when not in use. The extra care required to maintain these natural fiber instruments guaranties a long life of use. For this reason, bamboo fly rods decades or even centuries old, are still valued by anglers today.\n\n\n\n"}
{"id": "47488", "url": "https://en.wikipedia.org/wiki?curid=47488", "title": "Barometer", "text": "Barometer\n\nA barometer is a scientific instrument used in meteorology to measure atmospheric pressure. Pressure tendency can forecast short term changes in the weather. Many measurements of air pressure are used within surface weather analysis to help find surface troughs, high pressure systems and frontal boundaries.\n\nBarometers and pressure altimeters (the most basic and common type of altimeter) are essentially the same instrument, but used for different purposes. An altimeter is intended to be used at different levels matching the corresponding atmospheric pressure to the altitude, while a barometer is kept at the same level and measures subtle pressure changes caused by weather.\n\nThe word \"barometer\" is derived from the , and \"-meter\" from Ancient Greek: μέτρον (“measure”).\n\nAlthough Evangelista Torricelli is universally credited with inventing the barometer in 1643,<ref name=\"http://www.islandnet.com/~see/weather/history/barometerhistory1.htm\"></ref><ref name=\"http://www.barometerfair.com/history_of_the_barometer.htm\"></ref><ref name=\"http://www.juliantrubin.com/bigten/torricellibarometer.html\"></ref> historical documentation also suggests Gasparo Berti, an Italian mathematician and astronomer, unintentionally built a water barometer sometime between 1640 and 1643. French scientist and philosopher René Descartes described the design of an experiment to determine atmospheric pressure as early as 1631, but there is no evidence that he built a working barometer at that time.\n\nOn July 27, 1630, Giovanni Battista Baliani wrote a letter to Galileo Galilei explaining an experiment he had made in which a siphon, led over a hill about twenty-one meters high, failed to work. Galileo responded with an explanation of the phenomenon: he proposed that it was the power of a vacuum that held the water up, and at a certain height the amount of water simply became too much and the force could not hold any more, like a cord that can support only so much weight. This was a restatement of the theory of \"horror vacui\" (\"nature abhors a vacuum\"), which dates to Aristotle, and which Galileo restated as \"resistenza del vacuo\".\n\nGalileo's ideas reached Rome in December 1638 in his \"Discorsi\". Raffaele Magiotti and Gasparo Berti were excited by these ideas, and decided to seek a better way to attempt to produce a vacuum other than with a siphon. Magiotti devised such an experiment, and sometime between 1639 and 1641, Berti (with Magiotti, Athanasius Kircher and Niccolò Zucchi present) carried it out.\n\nFour accounts of Berti's experiment exist, but a simple model of his experiment consisted of filling with water a long tube that had both ends plugged, then standing the tube in a basin already full of water. The bottom end of the tube was opened, and water that had been inside of it poured out into the basin. However, only part of the water in the tube flowed out, and the level of the water inside the tube stayed at an exact level, which happened to be , the same height Baliani and Galileo had observed that was limited by the siphon. What was most important about this experiment was that the lowering water had left a space above it in the tube which had no intermediate contact with air to fill it up. This seemed to suggest the possibility of a vacuum existing in the space above the water.\n\nTorricelli, a friend and student of Galileo, interpreted the results of the experiments in a novel way. He proposed that the weight of the atmosphere, not an attracting force of the vacuum, held the water in the tube. In a letter to Michelangelo Ricci in 1644 concerning the experiments, he wrote:\nMany have said that a vacuum does not exist, others that it does exist in spite of the repugnance of nature and with difficulty; I know of no one who has said that it exists without difficulty and without a resistance from nature. I argued thus: If there can be found a manifest cause from which the resistance can be derived which is felt if we try to make a vacuum, it seems to me foolish to try to attribute to vacuum those operations which follow evidently from some other cause; and so by making some very easy calculations, I found that the cause assigned by me (that is, the weight of the atmosphere) ought by itself alone to offer a greater resistance than it does when we try to produce a vacuum.\n\nIt was traditionally thought (especially by the Aristotelians) that the air did not have weight: that is, that the kilometers of air above the surface did not exert any weight on the bodies below it. Even Galileo had accepted the weightlessness of air as a simple truth. Torricelli questioned that assumption, and instead proposed that air had weight and that it was the latter (not the attracting force of the vacuum) which held (or rather, pushed) up the column of water. He thought that the level the water stayed at (c. 10.3 m) was reflective of the force of the air's weight pushing on it (specifically, pushing on the water in the basin and thus limiting how much water can fall from the tube into it). In other words, he viewed the barometer as a balance, an instrument for measurement (as opposed to merely being an instrument to create a vacuum), and because he was the first to view it this way, he is traditionally considered the inventor of the barometer (in the sense in which we now use the term).\n\nBecause of rumors circulating in Torricelli's gossipy Italian neighborhood, which included that he was engaged in some form of sorcery or witchcraft, Torricelli realized he had to keep his experiment secret to avoid the risk of being arrested. He needed to use a liquid that was heavier than water, and from his previous association and suggestions by Galileo, he deduced by using mercury, a shorter tube could be used. With mercury, which is about 14 times denser than water, a tube only 80 cm was now needed, not 10.5 m.\n\nIn 1646, Blaise Pascal along with Pierre Petit, had repeated and perfected Torricelli's experiment after hearing about it from Marin Mersenne, who himself had been shown the experiment by Torricelli toward the end of 1644. Pascal further devised an experiment to test the Aristotelian proposition that it was vapors from the liquid that filled the space in a barometer. His experiment compared water with wine, and since the latter was considered more \"spiritous\", the Aristotelians expected the wine to stand lower (since more vapors would mean more pushing down on the liquid column). Pascal performed the experiment publicly, inviting the Aristotelians to predict the outcome beforehand. The Aristotelians predicted the wine would stand lower. It did not.\n\nHowever, Pascal went even further to test the mechanical theory. If, as suspected by mechanical philosophers like Torricelli and Pascal, air had weight, the pressure would be less at higher altitudes. Therefore, Pascal wrote to his brother-in-law, Florin Perier, who lived near a mountain called the Puy de Dome, asking him to perform a crucial experiment. Perier was to take a barometer up the Puy de Dome and make measurements along the way of the height of the column of mercury. He was then to compare it to measurements taken at the foot of the mountain to see if those measurements taken higher up were in fact smaller. In September 1648, Perier carefully and meticulously carried out the experiment, and found that Pascal's predictions had been correct. The mercury barometer stood lower the higher one went.\n\nThe concept that decreasing atmospheric pressure predicts stormy weather, postulated by Lucien Vidi, provides the theoretical basis for a weather prediction device called a \"weather glass\" or a \"Goethe barometer\" (named for Johann Wolfgang Von Goethe, the renowned German writer and polymath who developed a simple but effective weather ball barometer using the principles developed by Torricelli). The French name, \"le baromètre Liègeois\", is used by some English speakers. This name reflects the origins of many early weather glasses – the glass blowers of Liège, Belgium.\n\nThe weather ball barometer consists of a glass container with a sealed body, half filled with water. A narrow spout connects to the body below the water level and rises above the water level. The narrow spout is open to the atmosphere. When the air pressure is lower than it was at the time the body was sealed, the water level in the spout will rise above the water level in the body; when the air pressure is higher, the water level in the spout will drop below the water level in the body. A variation of this type of barometer can be easily made at home.\n\nA mercury barometer has a vertical glass tube closed at the top sitting in an open mercury-filled basin at the bottom. The weight of the mercury creates a vacuum in the top of the tube known as Torricellian vacuum. Mercury in the tube adjusts until the weight of the mercury column balances the atmospheric force exerted on the reservoir. High atmospheric pressure places more force on the reservoir, forcing mercury higher in the column. Low pressure allows the mercury to drop to a lower level in the column by lowering the force placed on the reservoir. Since higher temperature levels around the instrument will reduce the density of the mercury, the scale for reading the height of the mercury is adjusted to compensate for this effect. The tube has to be at least as long as the amount dipping in the mercury + head space + the maximum length of the column.\n\nTorricelli documented that the height of the mercury in a barometer changed slightly each day and concluded that this was due to the changing pressure in the atmosphere. He wrote: \"We live submerged at the bottom of an ocean of elementary air, which is known by incontestable experiments to have weight\". Inspired by Torricelli, Otto von Guericke on 5 December 1660 found that air pressure was unusually low and predicted a storm, which occurred the next day.\nThe mercury barometer's design gives rise to the expression of atmospheric pressure in inches or millimeters or feet (torr): the pressure is quoted as the level of the mercury's height in the vertical column. Typically, atmospheric pressure is measured between and of Hg. One atmosphere (1 atm) is equivalent to of mercury.\n\nDesign changes to make the instrument more sensitive, simpler to read, and easier to transport resulted in variations such as the basin, siphon, wheel, cistern, Fortin, multiple folded, stereometric, and balance barometers. Fitzroy barometers combine the standard mercury barometer with a thermometer, as well as a guide of how to interpret pressure changes. Fortin barometers use a variable displacement mercury cistern, usually constructed with a thumbscrew pressing on a leather diaphragm bottom. This compensates for displacement of mercury in the column with varying pressure. To use a Fortin barometer, the level of mercury is set to the zero level before the pressure is read on the column. Some models also employ a valve for closing the cistern, enabling the mercury column to be forced to the top of the column for transport. This prevents water-hammer damage to the column in transit.\n\nOn June 5, 2007, a European Union directive was enacted to restrict the sale of mercury, thus effectively ending the production of new mercury barometers in Europe.\n\nUsing vacuum pump oil the working fluid in a barometer has led to the creation of the new \"World's Tallest Barometer\" in February 2013. The barometer at Portland State University (PSU) uses doubly distilled vacuum pump oil and has a nominal height of about 12.4 m for the oil column height; expected excursions are in the range of ±0.4 m over the course of a year. Vacuum pump oil has very low vapor pressure and it is available in a range of densities; the lowest density vacuum oil was chosen for the PSU barometer to maximize the oil column height.\n\nAn aneroid barometer is an instrument used for measuring pressure as a method that does not involve liquid. Invented in 1844 by French scientist Lucien Vidi, the aneroid barometer uses a small, flexible metal box called an aneroid cell (capsule), which is made from an alloy of beryllium and copper. The evacuated capsule (or usually several capsules, stacked to add up their movements) is prevented from collapsing by a strong spring. Small changes in external air pressure cause the cell to expand or contract. This expansion and contraction drives mechanical levers such that the tiny movements of the capsule are amplified and displayed on the face of the aneroid barometer. Many models include a manually set needle which is used to mark the current measurement so a change can be seen. This type of barometer is common in homes and in recreational boats. It is also used in meteorology, mostly in barographs and as a pressure instrument in radiosondes.\n\nA barograph records a graph of atmospheric pressure.\n\nMicro Electro Mechanical Systems (or MEMS) barometers are extremely small devices between 1 and 100 micrometres in size (0.001 to 0.1 mm). They are created via photolithography or photochemical machining. Typical applications include miniaturized weather stations, electronic barometers and altimeters.\n\nA barometer can also be found in smartphones such as the Samsung Galaxy Nexus, Samsung Galaxy S3-S6, Motorola Xoom, Apple iPhone 6 smartphones, and Timex Expedition WS4 smartwatch, based on MEMS and piezoresistive pressure-sensing technologies. Inclusion of barometers on smartphones was originally intended to provide a faster GPS lock. However, third party researchers were unable to confirm additional GPS accuracy or lock speed due to barometric readings. The researchers suggest that the inclusion of barometers in smartphones may provide a solution to determining a user's elevation, but also suggest that several pitfalls must first be overcome.\n\nThere are many other more unusual types of barometer. From variations on the storm barometer, such as the Collins Patent Table Barometer, to more traditional-looking designs such as Hooke's Otheometer and the Ross Sympiesometer. Some, such as the Shark Oil barometer, work only in a certain temperature range, achieved in warmer climates.\n\nBarometric pressure and the pressure tendency (the change of pressure over time) have been used in weather forecasting since the late 19th century. When used in combination with wind observations, reasonably accurate short-term forecasts can be made. Simultaneous barometric readings from across a network of weather stations allow maps of air pressure to be produced, which were the first form of the modern weather map when created in the 19th century. Isobars, lines of equal pressure, when drawn on such a map, give a contour map showing areas of high and low pressure. Localized high atmospheric pressure acts as a barrier to approaching weather systems, diverting their course. Atmospheric lift caused by low-level wind convergence into the surface brings clouds and sometimes precipitation. The larger the change in pressure, especially if more than 3.5 hPa (0.1 inHg), the greater the change in weather that can be expected. If the pressure drop is rapid, a low pressure system is approaching, and there is a greater chance of rain. Rapid pressure rises, such as in the wake of a cold front, are associated with improving weather conditions, such as clearing skies.\n\nWith falling air pressure, gases trapped within the coal in deep mines can escape more freely. Thus low pressure increases the risk of firedamp accumulating. Collieries therefore keep track of the pressure. In the case of the Trimdon Grange colliery disaster of 1882 the mines inspector drew attention to the records and in the report stated \"the conditions of atmosphere and temperature may be taken to have reached a dangerous point\".\n\nAneroid barometers are used in scuba diving. A submersible pressure gauge is used to keep track of the contents of the diver's air tank. Another gauge is used to measure the hydrostatic pressure, usually expressed as a depth of sea water. Either or both gauges may be replaced with electronic variants or a dive computer.\n\nThe density of mercury will change with increase or decrease in temperature, so a reading must be adjusted for the temperature of the instrument. For this purpose a mercury thermometer is usually mounted on the instrument. Temperature compensation of an aneroid barometer is accomplished by including a bi-metal element in the mechanical linkages. Aneroid barometers sold for domestic use typically have no compensation under the assumption that they will be used within a controlled room temperature range.\n\nAs the air pressure decreases at altitudes above sea level (and increases below sea level) the uncorrected reading of the barometer will depend on its location. The reading is then adjusted to an equivalent sea-level pressure for purposes of reporting. For example, if a barometer located at sea level and under fair weather conditions is moved to an altitude of 1,000 feet (305 m), about 1 inch of mercury (~35 hPa) must be added on to the reading. The barometer readings at the two locations should be the same if there are negligible changes in time, horizontal distance, and temperature. If this were not done, there would be a false indication of an approaching storm at the higher elevation.\n\nAneroid barometers have a mechanical adjustment that allows the equivalent sea level pressure to be read directly and without further adjustment if the instrument is not moved to a different altitude. Setting an aneroid barometer is similar to resetting an analog clock that is not at the correct time. Its dial is rotated so that the current atmospheric pressure from a known accurate and nearby barometer (such as the local weather station) is displayed. No calculation is needed, as the source barometer reading has already been converted to equivalent sea-level pressure, and this is transferred to the barometer being set—regardless of its altitude. Though somewhat rare, a few aneroid barometers intended for monitoring the weather are calibrated to manually adjust for altitude. In this case, knowing \"either\" the altitude or the current atmospheric pressure would be sufficient for future accurate readings.\n\nThe table below shows examples for three locations in the city of San Francisco, California. Note the corrected barometer readings are identical, and based on equivalent sea-level pressure. (Assume a temperature of 15 °C.)\n\nWhen atmospheric pressure is measured by a barometer, the pressure is also referred to as the \"barometric pressure\". Assume a barometer with a cross-sectional area \"A\", a height \"h\", filled with mercury from the bottom at Point B to the top at Point C. The pressure at the bottom of the barometer, Point B, is equal to the atmospheric pressure. The pressure at the very top, Point C, can be taken as zero because there is only mercury vapor above this point and its pressure is very low relative to the atmospheric pressure. Therefore, one can find the atmospheric pressure using the barometer and this equation:\n\nP = ρgh\n\nwhere ρ is the density of mercury, g is the gravitational acceleration, and h is the height of the mercury column above the free surface area. The physical dimensions (length of tube and cross-sectional area of the tube) of the barometer itself have no effect on the height of the fluid column in the tube.\n\nIn thermodynamic calculations, a commonly used pressure unit is the \"standard atmosphere\". This is the pressure resulting from a column of mercury of 760 mm in height at 0 °C. For the density of mercury, use ρ = 13,595 kg/m and for gravitational acceleration use g = 9.807 m/s.\n\nIf water were used (instead of mercury) to meet the standard atmospheric pressure, a water column of roughly 10.3 m (33.8 ft) would be needed.\n\nStandard atmospheric pressure as a function of elevation:\n\nNote: 1 torr = 133.3 Pa = 0.03937 In Hg\n\n\n"}
{"id": "206869", "url": "https://en.wikipedia.org/wiki?curid=206869", "title": "CinemaScope", "text": "CinemaScope\n\nCinemaScope is an anamorphic lens series used, from 1953 to 1967, for shooting widescreen movies. Its creation in 1953 by Spyros P. Skouras, the president of 20th Century Fox, marked the beginning of the modern anamorphic format in both principal photography and movie projection.\n\nThe anamorphic lenses theoretically allowed the process to create an image of up to a 2.66:1 aspect ratio, almost twice as wide as the previously common Academy format's 1.37:1 ratio. Although the technology behind the CinemaScope lens system was made obsolete by later developments, primarily advanced by Panavision, CinemaScope's anamorphic format has continued to this day. In film-industry jargon, the shortened form, 'Scope, is still widely used by both filmmakers and projectionists, although today it generally refers to any 2.35:1, 2.39:1, or 2.40:1 presentation or, sometimes, the use of anamorphic lensing or projection in particular. Bausch & Lomb won a 1954 Oscar for its development of the CinemaScope lens.\n\nFrench inventor Henri Chrétien developed and patented a new film process that he called \"Anamorphoscope\" in 1926. It was this process that would later form the basis for CinemaScope. Chrétien's process was based on lenses that employed an optical trick which produced an image twice as wide as those that were being produced with conventional lenses; this was done using an optical system called \"Hypergonar\", which was the process of compressing (at shoot time) and dilating (at projection time) the image laterally. He attempted to interest the motion picture industry in his invention, but at that time the industry was not sufficiently impressed.\n\nBy 1950, however, cinema attendance seriously declined with the advent of a new competitive rival: television. Yet Cinerama and the early 3D films, both launched in 1952, succeeded at the box office in defying this trend, which in turn persuaded Spyros Skouras, the head of Twentieth Century-Fox, that technical innovation could help to meet the challenge. Skouras tasked Earl Sponable, head of Fox's research department, with devising a new, impressive, projection system, but something that, unlike Cinerama, could be retrofitted to existing theatres at a relatively modest costand then Herbert Brag, Sponable's assistant, remembered Chrétien's \"hypergonar\" lens.\n\nThe optical company Bausch & Lomb was asked to produce a prototype \"anamorphoser\" (later shortened to \"anamorphic\") lens. Meanwhile, Sponable tracked down Professor Chrétien, whose patent for the process had expired, so Fox purchased his existing Hypergonars from him and these lenses were flown to Fox's studios in Hollywood. Test footage shot with these lenses was screened for Skouras, who gave the go-ahead for development of a widescreen process based on Chrétien's invention, which was to be known as \"CinemaScope\".\n\nTwentieth Century-Fox's pre-production of \"The Robe\", originally committed to origination, was halted so that the film could be changed to a CinemaScope production (using Eastmancolor, but processed by Technicolor). Two other CinemaScope productions were also planned: \"How to Marry a Millionaire\" and \"Beneath the Twelve-Mile Reef.\" So that production of these first CinemaScope films could proceed without delay, shooting started using the best three of Chrétien's Hypergonars while Bausch & Lomb continued working on their own versions. The introduction of CinemaScope enabled Fox and other studios to reassert its distinction from the new competitor, television.\n\nChrétien's Hypergonars proved to have significant optical and operational defects (primarily loss-of-squeeze at close camera-to-subject distances, plus the requirement of two camera assistants). Bausch & Lomb, Fox's prime contractor for the production of these lenses, initially produced an improved \"Chrétien-formula\" adapter lens design (CinemaScope Adapter Type I), and subsequently produced a dramatically improved and patented \"Bausch & Lomb formula\" adapter lens design (CinemaScope Adapter Type II). \n\nUltimately \"Bausch & Lomb formula\" \"combined\" lens designs incorporated both the \"prime\" lens and the anamorphic lens in one unit (initially in 35, 40, 50, 75, 100 and 152 mm focal lengths, and later including a 25 mm focal length). These \"combined\" lenses continue to be used to this day, particularly in special effects units. Other manufacturers' lenses are often preferred for so-called \"production\" applications that benefit from significantly lighter weight or lower distortion, or a combination of both characteristics.\n\nCinemaScope was developed to use a separate film for sound (see Audio below), thus enabling the full \"silent\" 1.33:1 aperture to be available for the picture, with a 2:1 anamorphic squeeze applied that would allow an aspect ratio of 2.66:1. When, however, developers found that magnetic stripes could be added to the film to produce a composite picture/sound print, the ratio of the image was reduced to 2.55:1. This reduction was kept to a minimum by reducing the width of the normal KS perforations so that they were nearly square, but of DH height. This was the CinemaScope, or CS, perforation, known colloquially as \"fox-holes\". Later still an optical soundtrack was added, further reducing the aspect ratio to 2.35:1. This change also meant a shift in the optical center of the projected image. All of Fox's CinemaScope films were made using a silent/full aperture for the negatives, as was this studio's practice for all films, whether anamorphic or not.\n\nIn order to better hide so-called \"negative assembly\" splices, the ratio of the image was later changed by others to 2.39:1 and, finally, to 2.40:1. All professional cameras are capable of shooting 2.55:1 (special 'Scope aperture plate) or 2.66:1 (standard \"Full\"/\"Silent\" aperture plate, preferred by many producers and all optical houses), and 2.35:1 or 2.39:1 or 2.40:1 is simply a hard-matted version of the others.\nFox selected \"The Robe\" as the first film to start production in CinemaScope, a project chosen because of its epic nature. During its production, \"How to Marry a Millionaire\" and \"Beneath the 12-Mile Reef\" also went into Cinemascope production. \"Millionaire\" finished production first, before \"The Robe\", but because of its importance, \"The Robe\" was released first.\n\nFox used its influential people to promote CinemaScope. With the success of \"The Robe\" and \"How to Marry a Millionaire,\" the process enjoyed success in Hollywood. Fox licensed the process to many of the major film studios including Columbia, Warner Bros., Universal, MGM and Walt Disney Productions.\n\nWalt Disney Productions was one of the first companies to license the CinemaScope process from Fox. Among the features and shorts they filmed with it, they created the live-action epic \"20,000 Leagues Under the Sea,\" considered one of the best examples of early CinemaScope productions. Walt Disney Productions' \"Toot, Whistle, Plunk and Boom\", which won an Academy Award for Best Short Subject (Cartoons) in 1953, was the first cartoon produced in Cinemascope. The first animated feature film to use CinemaScope was \"Lady and the Tramp\" (1955), also from Walt Disney Productions.\n\nDue to initial uncertainty about whether the process would be adopted widely, a number of films were shot simultaneously with anamorphic and regular lenses. Despite early success with the process, Fox did not shoot every production by this process. They reserved CinemaScope as a trade name for their \"A\" productions, while \"B\" productions in black and white were begun in 1956 at Fox under the trade name, \"RegalScope.\" The latter used the very same optics as CinemaScope, but, usually, a different camera system (such as Mitchell BNCs at TCF-TV studios for RegalScope rather than Fox Studio Cameras at Fox Hills studios for CinemaScope).\n\nFox officials were keen that the sound of their new widescreen film format should be as impressive as the picture, and that meant it should include true stereophonic sound.\n\nPreviously stereo sound in the commercial cinema had always employed separate sound films; Walt Disney's 1940 release \"Fantasia\" had used a three-channel soundtrack played from separate optical film. Early post-war stereo systems used with Cinerama and some 3D films had used multichannel audio played from a separate magnetic film. Fox had initially intended to use 3-channel stereo from magnetic film for CinemaScope.\n\nHowever, Hazard E. Reeves' sound company had devised a method of coating 35 mm stock with magnetic stripes and designed a 3-channel (left, center, right) system based on three stripes, one on each edge of the film outside the perforations, and one between the picture and the perforations in approximately the position of a standard optical soundtrack. Later it was found possible to add a narrower stripe between the picture and perforations on the other side of the film; this fourth track was used for a surround channel, also sometimes known at the time as an \"effects\" channel. In order to avoid hiss on the surround/effects channel from distracting the audience the surround speakers were switched on by a 12 kHz tone recorded on the surround track only while wanted surround program material was present.\n\nThis 4-track magnetic sound system was also used for some non-CinemaScope films; for example \"Fantasia\" was re-released in 1956, 1963, and 1969 with the original Fantasound track transferred to 4-track magnetic.\n\nCinemaScope itself was a response to early \"realism\" processes Cinerama and 3-D. Cinerama was relatively unaffected by CinemaScope, as it was a quality-controlled process that played in select venues, similar to the IMAX films of recent years. 3-D was hurt, however, by studio advertising surrounding CinemaScope's promise that it was the \"miracle you see without glasses.\" Technical difficulties in presentation spelled the true end for 3-D, but studio hype was quick to hail it a \"victory\" for CinemaScope.\n\nIn April 1953, a technique simply now known as \"wide-screen\" appeared and was soon adopted as a standard by all \"flat\" film productions in the US. In this process, a fully exposed 1.37:1 Academy ratio-area is cropped in the projector to a wide-screen aspect ratio by the use of an aperture plate, also known as a soft matte. Most films shot today use this technique, cropping the top and bottom of a 1.37:1 image to produce one at a ratio of 1.85:1.\n\nAware of Fox's upcoming CinemaScope productions, Paramount introduced this technique in March's release of \"Shane\" with the 1.66:1 aspect ratio, although the film was not shot with this ratio originally in mind. Universal-International followed suit in May with a 1.85:1 aspect ratio for \"Thunder Bay\". By summer of 1953, Paramount, Universal, MGM, Columbia, and even Fox's B-unit contractors, under the banner of \"Panoramic Productions\" had switched from filming flat shows in a 1.37:1 format, and used variable flat wide-screen aspect ratios in their filming, which would become the standard of that time.\n\nBy this time Chrétien's 1926 patent on the Hypergonar lens had expired while the fundamental technique that CinemaScope utilised was not patentable because the anamorphoscope had been known for centuries. Anamorphosis had been used in visual media such as Hans Holbein's painting, \"The Ambassadors\" (1533). Some studios thus sought to develop their own systems rather than pay Fox.\n\nIn response to the demands for a higher visual resolution spherical widescreen process, Paramount created an optical process, VistaVision, which shot horizontally on the 35 mm film roll, and then printed down to standard 4-perf vertical 35 mm. Thus, a negative with a finer grain was created and release prints had less grain. The first Paramount film in VistaVision was \"White Christmas\". VistaVision died out for feature production in the late 1950s with the introduction of faster film stocks, but was revived by Industrial Light & Magic in 1975 to create high quality visual effects for \"Star Wars\" and ILM's subsequent film projects.\n\nRKO used the Superscope process in which the standard 35 mm image was cropped and then optically squeezed in post-production to create an anamorphic image on film. Today's Super 35 is a variation of this process.\n\nAnother process called Techniscope was developed by Technicolor Inc. in the early 1960s, using normal 35 mm cameras modified for two perforations per (half) frame instead of the regular four and later converted into an anamorphic print. Techniscope was mostly used in Europe, especially with low-budget films.\n\nMany European countries and studios used the standard anamorphic process for their wide-screen films, identical in technical specifications to CinemaScope, and renamed to avoid the trademarks of Fox. Some of these include Euroscope, Franscope, and Naturama (the latter used by Republic Pictures). In 1953, Warner Bros. also planned to develop an identical anamorphic process called Warnerscope but, after the premiere of CinemaScope, Warner Bros. decided to license it from Fox instead.\n\nAlthough CinemaScope was capable of producing a 2.66:1 image, the addition of magnetic sound tracks for multi-channel sound reduced this to 2.55:1.\n\nThe fact that the image was expanded horizontally when projected meant that there could be visible graininess and brightness problems. To combat this, larger film formats were developed (initially a too-costly 55 mm for \"Carousel\" and \"The King and I\") and then abandoned (both films were eventually reduction printed at 35 mm, although the aspect ratio was kept at 2.55:1). Later Fox re-released \"The King and I\" in the 65/70 mm format. The initial problems with grain and brightness were eventually reduced thanks to improvements in film stock and lenses.\n\nThe CinemaScope lenses were optically flawed, however, by the fixed anamorphic element, which caused the anamorphic effect to gradually drop off as objects approached closer to the lens. The effect was that close-ups would slightly overstretch an actor's face, a problem that was soon referred to as \"the mumps\". This problem was avoided at first by composing wider shots, but as anamorphic technology lost its novelty, directors and cinematographers sought compositional freedom from these limitations. Issues with the lenses also made it difficult to photograph animation using the CinemaScope process. Nevertheless, many animated short films and a few features were filmed in CinemaScope during the 1950s, including Walt Disney's \"Lady and the Tramp\" (1955).\n\nCinemaScope 55 was a large-format version of CinemaScope introduced by Twentieth Century Fox in 1955, which used a film width of 55.625 mm.\n\nFox had introduced the original 35 mm version of CinemaScope in 1953 and it had proved to be commercially successful. But the additional image enlargement needed to fill the new wider screens, which had been installed in theatres for CinemaScope, resulted in visible film grain. A larger film was used to reduce the need for such enlargement. CinemaScope 55 was developed to satisfy this need and was one of three \"high-definition\" film systems introduced in the mid-1950s, the other two being Paramount's VistaVision and the Todd-AO 70 mm film system.\n\nFox determined that a system that produced a frame area approximately 4 times that of the 35mm CinemaScope frame would be the optimal trade-off between performance and cost, and it chose the 55.625 mm film width as satisfying that. Camera negative film had larger grain than the film stocks used for prints, so there was a consistent approach in using a larger frame on the film negative than on prints. Since the image area of a print has to allow for a soundtrack, a camera negative does not. CinemaScope 55 had different frame dimensions for the camera negative and struck prints.\n\nThe negative film had the perforations (of the CS \"Fox-hole\" type) close to the edge of the film and the camera aperture was 1.824\" by 1.430\" (approx. 46 mm x 36 mm), giving an image area of 2.61 sq. inch. This compares to the 0.866\" by 0.732\" (approx. 22 mm x 18.6 mm) frame of a modern anamorphic 35 mm negative, which provides a frame area of 0.64 sq. inch. On the print film, however, there was a smaller frame size of approximately 1.34\" x 1.06\" (34 mm x 27 mm) to allow space for the 6 magnetic soundtracks. Four of these soundtracks (two each side) were outside the perforations, which were further from the edges of the print film than in the negative film; the other two soundtracks were between the perforations and the image. The pull-down for the negative was 8 perforations, while for the smaller frame on the print film, it was 6 perforations. In both cases, however, the frame had an aspect ratio of 1.275:1, which when expanded by a 2:1 anamorphic lens resulted in an image of 2.55:1.\n\nA camera originally built for the obsolete Fox 70 mm \"Grandeur\" film format more than 20 years before was modified to work with the new 55 mm film. Bausch & Lomb, the firm that created the original anamorphic CinemaScope lenses, was contracted by Fox to build new \"Super CinemaScope\" lenses that could cover the larger film frame.\n\nFox shot two of their Rodgers and Hammerstein musical series in CinemaScope 55: \"Carousel\", and \"The King and I\". But it did not make 55 mm release prints for either film; both were released in conventional 35 mm CinemaScope with a limited release of \"The King and I\" being shown in 70 mm.\n\nFox soon discontinued this process, as it was too impractical for theaters to re-equip for 55 mm prints. The company substituted Todd-AO for its wide-gauge production process, having acquired a financial interest in the process from the Mike Todd estate.\n\nSubsequent to the abandonment of CinemaScope 55, Century, which had made the 55/35mm dual-gauge projector for Fox (50 sets were delivered), redesigned this projector head into the present day 70/35mm Model JJ, and Ampex, which had made the 55/35mm dual gauge \"penthouse\" magnetic sound reproducer head specifically for CinemaScope 55, abandoned this product (but six-channel Ampex theater systems persisted, these being re-purposed from 55/35mm to 70mm Todd-AO/35mm CinemaScope).\n\nAlthough commercial 55 mm prints were not made, some 55 mm prints were produced. Samples of these prints reside in the Earl I. Sponable Collection at Columbia University. Several 55/35mm projectors and at least one 55/35mm reproducer are in the hands of collectors.\n\nCinemascope 55 was originally intended to have a six-track stereo soundtrack. The premiere engagement of \"Carousel\" in New York did use one, recorded on magnetic film interlocked with the visual image, as with Cinerama. This proved too impractical, and all other engagements of \"Carousel\" had the standard four-track stereo soundtrack (\"sounded\" on the actual film) as was then used in all CinemaScope releases.\n\nIn 2005 both CinemaScope 55 films were restored from the original 55 mm negatives.\n\nLens manufacturer Panavision was initially founded in late 1953 as a manufacturer of anamorphic lens adapters for movie projectors screening CinemaScope films, capitalizing on the success of the new anamorphic format and filling in the gap created by Bausch and Lomb's inability to mass-produce the needed adapters for movie theaters fast enough. Looking to expand beyond projector lenses, Panavision founder Robert Gottschalk soon improved upon the anamorphic camera lenses by creating a new lens set that included dual rotating anamorphic elements which were interlocked with the lens focus gearing. This innovation allowed the Panavision lenses to keep the plane of focus at a constant anamorphic ratio of 2x, thus avoiding the horizontally-overstretched \"mumps\" effect that afflicted many CinemaScope films. After screening a demo reel comparing the two systems, many U.S. studios adopted the Panavision anamorphic lenses. The Panavision technique was also considered more attractive to the industry because it was more affordable than CinemaScope and was not owned or licensed-out by a rival studio. Confusingly, some studios, particularly MGM, continued to use the CinemaScope credit even though they had switched to Panavision lenses. Virtually all MGM \"CinemaScope\" films after 1958 are actually in Panavision.\n\nBy 1967, even Fox had begun to abandon CinemaScope for Panavision (famously at the demand of Frank Sinatra for \"Von Ryan's Express\"), although a significant amount of the principal photography was actually filmed using CinemaScope lenses. Fox eventually capitulated completely to third-party lenses. \"In Like Flint\" with James Coburn and \"Caprice\" with Doris Day, were Fox's final films in CinemaScope.\n\nFox originally intended CinemaScope films to use magnetic stereo sound only, and although in certain areas, such as Los Angeles and New York City, the vast majority of theaters were equipped for 4-track magnetic sound (4-track magnetic sound achieving nearly 90 percent penetration of theaters in the greater Los Angeles area) the owners of many smaller theaters were dissatisfied with contractually having to install expensive three- or four-track magnetic stereo, and because of the technical nature of sound installations, drive-in theaters had trouble presenting stereophonic sound at all. Due to these conflicts, and because other studios were starting to release anamorphic prints with standard optical soundtracks, Fox revoked their policy of stereo-only presentations in 1957, and added a half-width optical soundtrack, while keeping the magnetic tracks for those theaters that were able to present their films with stereophonic sound. These so-called \"mag-optical\" prints provided a somewhat sub-standard optical sound and were also expensive to produce. It made little economic sense to supply those theaters which had only mono sound systems with an expensive striped print. Eventually Fox, and others, elected to supply the majority of their prints in standard mono optical sound form, with magnetic striped prints reserved for those theaters capable of playing them.\n\nMagnetic-striped prints were expensive to produce; each print cost at least twice as much as a print with a standard optical soundtrack only. Furthermore, these striped prints wore out faster than optical prints and caused more problems in use, such as flakes of oxide clogging the replay heads. Due to these problems, and also because many cinemas never installed the necessary playback equipment, magnetic-sound prints started to be made in small quantities for \"roadshow\" screenings only, with the main release using standard mono optical-sound prints. As time went by roadshow screenings were increasingly made using 70 mm film, and the use of striped 35 mm prints declined further. Many CinemaScope films from the 1960s and 1970s were never released in stereo at all. Finally, the 1976 introduction of Dolby Stereo – which provided similar performance to striped magnetic prints albeit more reliable and at a far lower cost – caused the 4-track magnetic system to become totally obsolete.\n\nThe song \"Stereophonic Sound\" written by Cole Porter for the 1955 Broadway musical \"Silk Stockings\" mentions CinemaScope in the lyrics. The first verse is: \"Today to get the public to attend the picture show/ It’s not enough to advertise a famous star they know/ If you wanna get the crowds to come around/ You gotta have glorious Technicolor/ Breathtaking CinemaScope and stereophonic sound.\" The musical was adapted for film in 1957 and was indeed filmed in CinemaScope. (Although the song refers to Technicolor, the film was actually made in Metrocolor.)\n\nWhile the lens system has been retired for decades, Fox has used the trademark in recent years on at least three films: \"Down with Love\", which was shot with Panavision optics but used the credit as a throwback to the films it references, and the Don Bluth films \"Anastasia\" and \"Titan A.E.\" at Bluth's insistence. Nonetheless, these films are not true CinemaScope as they use modern lenses. CinemaScope's association with anamorphic projection is still so embedded in mass consciousness that all anamorphic prints are now referred to generically as \"'Scope\" prints.\n\nSimilarly, the 2016 release \"La La Land\" was shot on film (not digitally) with Panavision equipment in a 2.55:1 widescreen format, but not true CinemaScope. However, the film's opening credits do say \"Presented in CinemaScope\" (\"presented,\" not \"shot\") as a tribute to 1950s musicals in that format. This credit appears initially in black-and-white and in a narrow format. It then widens to widescreen and dissolves to the old-fashioned CinemaScope logo, in color.\n\nIn the 1963 Jean-Luc Godard film \"Contempt\" (\"Le Mepris\"), filmmaker Fritz Lang makes a disparaging comment about CinemaScope: \"Oh, it wasn't meant for human beings. Just for snakes – and funerals.\" Ironically, \"Contempt\" was shot in , a process with a similar format to CinemaScope.\n\nDuring the production of 1999's \"The Iron Giant,\" director Brad Bird wanted to use CinemaScope on the film's promotion, but Fox disapproved of this, so it was not used. The joke was later realized during the end credits of the 2015 \"Signature Edition\" re-release.\n\nRecently, the two 2018 blockbuster films \"\" and \"Deadpool 2\" were shot in CinemaScope.\n\n\n"}
{"id": "57294217", "url": "https://en.wikipedia.org/wiki?curid=57294217", "title": "CircuitPython", "text": "CircuitPython\n\nCircuitPython is an open source derivative of the MicroPython programming language targeted towards the student and beginner. Development of CircuitPython is supported by Adafruit Industries. It is a software implementation of the Python 3 programming language, written in C. It has been ported to run on several modern microcontrollers.\n\nCircuitPython is a full Python compiler and runtime that runs on the microcontroller hardware. The user is presented with an interactive prompt (the REPL) to execute supported commands immediately. Included are a selection of core Python libraries. CircuitPython includes modules which give the programmer access to the low-level hardware of Adafruit compatible products as well as higher level libraries for beginners.\n\nCircuitPython is a fork of MicroPython, originally created by Damien George. The MicroPython community continues to discuss forks of MicroPython into variants such as CircuitPython.\n\nCircuitPython is targeted to be compliant with CPython, the reference implementation of the Python programming language. Programs written for CircuitPython compatible boards may not run unmodified on other platforms such as the Raspberry Pi.\n\nCircuitPython is currently being used for more projects, especially for wearable technology, when in the past the code may have been done in the Arduino development environment. The language has also seen uptake in making small, handheld video game devices. Developer Chris Young has ported his infrared receive/transmit software to CircuitPython to provide interactivity and to aid those with accessibility issues.\n\nThe user community support includes a Discord chat room and product support forums. There is a published Code of Conduct for the project.\n\nFor the general Python community, Adafruit has supported the Python Foundation for several years.\n\nCircuitPython support was incorporated into the Mu Python Editor.\n\nA Twitter account dedicated to CircuitPython news was established in 2018.\n\nThe Applications Programming Interface (API) is documented in Read the Docs.\n\nTutorials on CircuitPython use, including introductory guides, are available on the Adafruit company learning system.\n\nThe source code for the project is available on GitHub.\n\nThe current stable version is 2.3.1 with support for the Microchip Technology Atmel SAMD21 processor and the ESP8266 microcontroller. Adafruit has announced a major revision, 3.0.0, in alpha with support for the SAMD51 series processor.\n\n"}
{"id": "24823594", "url": "https://en.wikipedia.org/wiki?curid=24823594", "title": "Clap-o-meter", "text": "Clap-o-meter\n\nA clap-o-meter, clapometer or applause meter is a measurement instrument that purports to measure and display the volume of clapping or applause made by an audience. It can be used to indicate the popularity of contestants and decide the result of competitions based on audience popularity. Specific implementations may or may not be based on an actual sound level meters. Clap-o-meters were a popular element in talent shows and television game shows in the 1950s and 1960s, most notably \"Opportunity Knocks\", but have been since been supplanted by other, more sophisticated, methods of measuring audience response.\n\nOne of the first appearances of a clap-o-meter was in 1956, on the British TV game show \"Opportunity Knocks\", developed and presented by Hughie Green. The clap-o-meter itself was a wooden box labelled \"Audience Reaction Indicator\". The prop is now part of the collection of the National Media Museum, in Bradford. Clap-o-meters were used in many other TV shows and at live events.\n\nIn 1989, Green unsuccessfully attempted to sue the New Zealand Broadcasting Corporation for copyright infringement over a similar programme. The clap-o-meter was one of the distinctive features of the format by which Green sought to define it as copyrightable. The courts found that a loose format defined by catchphrases and accessories, such as the clap-o-meter, was not copyrightable.\n\nClap-o-meters continue to be used. They are often regarded as a novelty or item of amusement rather than an accurate method to measure popularity. Even so, they are sometimes used to judge winners in fairly serious competitions such as battle of the bands competitions. In politics, a politician's popularity is sometimes gauged by the applause they achieve when giving speeches. News organisations sometimes use the concept of a clap-o-meter to gauge popularity of a politician or of components of a politician's overall message.\n\nClap-o-meter software is also available for computers and mobile devices. The software uses the device's microphone or audio input to determine the level of applause.\n\nQuite often a clap-o-meter is a complete sham, having no real sound measuring equipment at all. It is, instead, manipulated by a person, based on their estimation of the audience reaction. This is normally done semi-openly, with the audience under little or no illusion that the clap-o-meter is genuine. This was apparently the case on \"Opportunity Knocks\", where the clap-o-meter was not used to actually determine the winners and was disclaimed with the phrase \"Remember, folks! The clap-o-meter is just for fun!\".\n\nA number of alternatives to the clap-o-meter exist. A studio audience can be polled by a simple show of hands, or for more visual impact by having them hold up different coloured cards indicating their vote. They can also be polled by electronic means using individual voting devices with buttons for each option. These options are more accurate than a clap-o-meter but lack the element of excitement generated by frenzied applause.\n\nIn recent years, phone voting has become the main method of deciding popularity in talent shows. This has the advantage of expanding participation to include the full TV audience. It can also be used in programmes which do not have a studio audience. Phone voting can provide a significant source of additional revenue for the broadcasters from the use of premium rate phone numbers.\n\n\n"}
{"id": "9409170", "url": "https://en.wikipedia.org/wiki?curid=9409170", "title": "Command Data Buffer", "text": "Command Data Buffer\n\nCommand Data Buffer (CDB) was a system used by the United States Air Force's Minuteman ICBM force. CDB was a method to transfer targeting information from a Minuteman Launch Control Center to an individual missile by communications lines. Prior to CDB, new missile guidance would have to be physically loaded at the launch facility; the process usually took hours.\n\nThe surviving remnant of the Minuteman Command Control System (MICCS), CDB permitted the rapid, remote, retargeting of the Minuteman III fleet. CDB was operational at all Minuteman III wings by 15 Aug 1977. Minuteman II wings had a similar install, designated Improved Launch Control System, providing the older system the potential for remote retargeting.\n\nCDB was replaced in the late 1990s by the Rapid Execution and Combat Targeting system, currently in use by United States ICBM forces.\n\n"}
{"id": "40993", "url": "https://en.wikipedia.org/wiki?curid=40993", "title": "Data forwarder", "text": "Data forwarder\n\nIn telecommunications, a data forwarder is a device that \nand \n"}
{"id": "1944675", "url": "https://en.wikipedia.org/wiki?curid=1944675", "title": "Educational technology", "text": "Educational technology\n\nEducational technology is \"the study and ethical practice of facilitating learning and improving performance by creating, using, and managing appropriate technological processes and resources\".\n\nEducational technology is the use of both physical hardware and educational theoretic. It encompasses several domains including learning theory, computer-based training, online learning, and where mobile technologies are used, m-learning. Accordingly, there are several discrete aspects to describing the intellectual and technical development of educational technology:\n\nThe Association for Educational Communications and Technology (AECT) defined educational technology as \"the study and ethical practice of facilitating learning and improving performance by creating, using and managing appropriate technological processes and resources\". It denoted instructional technology as \"the theory and practice of design, development, utilization, management, and evaluation of processes and resources for learning\". As such, educational technology refers to all valid and reliable applied education sciences, such as equipment, as well as processes and procedures that are derived from scientific research, and in a given context may refer to theoretical, algorithmic or heuristic processes: it does not necessarily imply physical technology. Educational technology is the process of integrating technology into education in a positive manner that promotes a more diverse learning environment and a way for students to learn how to use technology as well as their common assignments.\n\nEducational technology is an inclusive term for both the material tools \"and\" the theoretical foundations for supporting learning and teaching. Educational technology is not restricted to high technology but is anything that enhances classroom learning in the utilization of blended, face to face, or online learning.\n\nAn educational technologist is someone who is trained in the field of educational technology. Educational technologists try to analyze, design, develop, implement, and evaluate process and tools to enhance learning. While the term \"educational technologist\" is used primarily in the United States, \"learning technologist\" is synonymous and used in the UK as well as Canada.\n\nModern electronic educational technology is an important part of society today. Educational technology encompasses e-learning, instructional technology, information and communication technology (ICT) in education, EdTech, learning technology, multimedia learning, technology-enhanced learning (TEL), computer-based instruction (CBI), computer managed instruction, computer-based training (CBT), computer-assisted instruction or computer-aided instruction (CAI), internet-based training (IBT), flexible learning, web-based training (WBT), online education, digital educational collaboration, distributed learning, computer-mediated communication, cyber-learning, and multi-modal instruction, virtual education, personal learning environments, networked learning, virtual learning environments (VLE) (which are also called learning platforms), m-learning, ubiquitous learning and digital education.\n\nEach of these numerous terms has had its advocates, who point up potential distinctive features. However, many terms and concepts in educational technology have been defined nebulously; for example, Fiedler's review of the literature found a complete lack agreement of the components of a personal learning environment. Moreover, Moore saw these terminologies as emphasizing particular features such as digitization approaches, components or delivery methods rather than being fundamentally dissimilar in concept or principle. For example, m-learning \"emphasizes\" mobility, which allows for altered timing, location, accessibility and context of learning; nevertheless, its purpose and conceptual \"principles\" are those of educational technology.\n\nIn practice, as technology has advanced, the particular \"narrowly defined\" terminological aspect that was initially emphasized by name has blended into the general field of educational technology. Initially, \"virtual learning\" as narrowly defined in a semantic sense implied entering an environmental simulation within a virtual world, for example in treating posttraumatic stress disorder (PTSD). In practice, a \"virtual education course\" refers to any instructional course in which all, or at least a significant portion, is delivered by the Internet. \"Virtual\" is used in that broader way to describe a course that is not taught in a classroom face-to-face but through a substitute mode that can conceptually be associated \"virtually\" with classroom teaching, which means that people do not have to go to the physical classroom to learn. Accordingly, virtual education refers to a form of distance learning in which course content is delivered by various methods such as course management applications, multimedia resources, and videoconferencing. Virtual education and simulated learning opportunities, such as games or dissections, offer opportunities for students to connect classroom content to authentic situations.\n\nEducational content, pervasively embedded in objects, is all around the learner, who may not even be conscious of the learning process.<ref name=\"Teaching English as a Second/Foreign Language in a Ubiquitous Learning Environment: A Guide for ESL/EFL Instructors\"></ref> The combination of adaptive learning, using an individualized interface and materials, which accommodate to an individual, who thus receives personally differentiated instruction, with ubiquitous access to digital resources and learning opportunities in a range of places and at various times, has been termed smart learning. Smart learning is a component of the smart city concept.\n\nHelping people and children learn in ways that are easier, faster, more accurate, or less expensive can be traced back to the emergence of very early tools, such as paintings on cave walls. Various types of abacus have been used. Writing slates and blackboards have been used for at least a millennium. From their introduction, books and pamphlets have held a prominent role in education. From the early twentieth century, duplicating machines such as the mimeograph and Gestetner stencil devices were used to produce short copy runs (typically 10–50 copies) for classroom or home use. The use of media for instructional purposes is generally traced back to the first decade of the 20th century with the introduction of educational films (1900s) and Sidney Pressey's mechanical teaching machines (1920s). The first all multiple choice, large-scale assessment was the Army Alpha, used to assess the intelligence and more specifically the aptitudes of World War I military recruits. Further large-scale use of technologies was employed in training soldiers during and after WWII using films and other mediated materials, such as overhead projectors. The concept of hypertext is traced to the description of memex by Vannevar Bush in 1945.\nSlide projectors were widely used during the 1950s in educational institutional settings. Cuisenaire rods were devised in the 1920s and saw widespread use from the late 1950s.\n\nIn the mid 1960s Stanford University psychology professors Patrick Suppes and Richard C. Atkinson experimented with using computers to teach arithmetic and spelling via Teletypes to elementary school students in the Palo Alto Unified School District in California. Stanford's Education Program for Gifted Youth is descended from those early experiments.\n\nOnline education originated from the University of Illinois in 1960. Although internet would not be created for another nine years, students were able to access class information with linked computer terminals. The first online course was offered in 1986 by the Electronic University Network for DOS and Commodore 64 computers. Computer Assisted Learning eventually offered the first online courses with real interaction. In 2002, MIT began providing online classes free of charge. , approximately 5.5 millions students were taking at least one class online. Currently, one out of three college students takes at least one online course while in college (Promises and pitfalls). At DeVry University, out of all students that are earning a bachelor's degree, 80% earn two-thirds of their requirements online (Promises and Pitfalls). Also in 2014, 2.85 millions students out of 5.8 million students that took courses online, took all of their courses online (Promises and Pitfalls). From this information, it can be concluded that the number of students taking classes online is on the steady increase.\nIn 1971, Ivan Illich published a hugely influential book called, Deschooling Society, in which he envisioned \"learning webs\" as a model for people to network the learning they needed. The 1970s and 1980s saw notable contributions in computer-based learning by Murray Turoff and Starr Roxanne Hiltz at the New Jersey Institute of Technology as well as developments at the University of Guelph in Canada. In the UK, the Council for Educational Technology supported the use of educational technology, in particular administering the government's National Development Programme in Computer Aided Learning (1973–77) and the Microelectronics Education Programme (1980–86).\n\nBy the mid-1980s, accessing course content became possible at many college libraries. In computer-based training (CBT) or computer-based learning (CBL), the learning interaction was between the student and computer drills or micro-world simulations.\n\nDigitized communication and networking in education started in the mid-1980s. Educational institutions began to take advantage of the new medium by offering distance learning courses using computer networking for information. Early e-learning systems, based on computer-based learning/training often replicated autocratic teaching styles whereby the role of the e-learning system was assumed to be for transferring knowledge, as opposed to systems developed later based on computer supported collaborative learning (CSCL), which encouraged the shared development of knowledge.\n\nVideoconferencing was an important forerunner to the educational technologies known today. This work was especially popular with museum education. Even in recent years, videoconferencing has risen in popularity to reach over 20,000 students across the United States and Canada in 2008–2009. Disadvantages of this form of educational technology are readily apparent: image and sound quality is often grainy or pixelated; videoconferencing requires setting up a type of mini-television studio within the museum for broadcast, space becomes an issue; and specialised equipment is required for both the provider and the participant.\n\nThe Open University in Britain and the University of British Columbia (where Web CT, now incorporated into Blackboard Inc., was first developed) began a revolution of using the Internet to deliver learning, making heavy use of web-based training, online distance learning and online discussion between students. Practitioners such as Harasim (1995) put heavy emphasis on the use of learning networks.\n\nWith the advent of World Wide Web in the 1990s, teachers embarked on the method using emerging technologies to employ multi-object oriented sites, which are text-based online virtual reality systems, to create course websites along with simple sets of instructions for its students.\n\nBy 1994, the first online high school had been founded. In 1997, Graziadei described criteria for evaluating products and developing technology-based courses that include being portable, replicable, scalable, affordable, and having a high probability of long-term cost-effectiveness.\n\nImproved Internet functionality enabled new schemes of communication with multimedia or webcams. The National Center for Education Statistics estimate the number of K-12 students enrolled in online distance learning programs increased by 65 percent from 2002 to 2005, with greater flexibility, ease of communication between teacher and student, and quick lecture and assignment feedback.\n\nAccording to a 2008 study conducted by the U.S Department of Education, during the 2006–2007 academic year about 66% of postsecondary public and private schools participating in student financial aid programs offered some distance learning courses; records show 77% of enrollment in for-credit courses with an online component. In 2008, the Council of Europe passed a statement endorsing e-learning's potential to drive equality and education improvements across the EU.\n\nComputer-mediated communication (CMC) is between learners and instructors, mediated by the computer. In contrast, CBT/CBL usually means individualized (self-study) learning, while CMC involves educator/tutor facilitation and requires scenarization of flexible learning activities. In addition, modern ICT provides education with tools for sustaining learning communities and associated knowledge management tasks.\n\nStudents growing up in this digital age have extensive exposure to a variety of media. Major high-tech companies have funded schools to provide them the ability to teach their students through technology.\n\n2015 was the first year that private nonprofit organizations enrolled more online students than for-profits, although public universities still enrolled the highest number of online students. In the fall of 2015, more than 6 million students enrolled in at least one online course.\n\nVarious pedagogical perspectives or learning theories may be considered in designing and interacting with educational technology. E-learning theory examines these approaches. These theoretical perspectives are grouped into three main theoretical schools or philosophical frameworks: behaviorism, cognitivism and constructivism.\n\nThis theoretical framework was developed in the early 20th century based on animal learning experiments by Ivan Pavlov, Edward Thorndike, Edward C. Tolman, Clark L. Hull, and B.F. Skinner. Many psychologists used these results to develop theories of human learning, but modern educators generally see behaviorism as one aspect of a holistic synthesis. Teaching in behaviorism has been linked to training, emphasizing the animal learning experiments. Since behaviorism consists of the view of teaching people how to do something with rewards and punishments, it is related to training people.\n\nB.F. Skinner wrote extensively on improvements of teaching based on his functional analysis of verbal behavior and wrote \"The Technology of Teaching\", an attempt to dispel the myths underlying contemporary education as well as promote his system he called programmed instruction. Ogden Lindsley developed a learning system, named Celeration, that was based on behavior analysis but that substantially differed from Keller's and Skinner's models.\n\nCognitive science underwent significant change in the 1960s and 1970s. While retaining the empirical framework of behaviorism, cognitive psychology theories look beyond behavior to explain brain-based learning by considering how human memory works to promote learning. The Atkinson-Shiffrin memory model and Baddeley's working memory model were established as theoretical frameworks. Computer Science and Information Technology have had a major influence on Cognitive Science theory. The Cognitive concepts of working memory (formerly known as short term memory) and long term memory have been facilitated by research and technology from the field of Computer Science. Another major influence on the field of Cognitive Science is Noam Chomsky. Today researchers are concentrating on topics like cognitive load, information processing and media psychology. These theoretical perspectives influence instructional design.\n\nEducational psychologists distinguish between several types of constructivism: individual (or psychological) constructivism, such as Piaget's theory of cognitive development, and social constructivism. This form of constructivism has a primary focus on how learners construct their own meaning from new information, as they interact with reality and with other learners who bring different perspectives. Constructivist learning environments require students to use their prior knowledge and experiences to formulate new, related, and/or adaptive concepts in learning (Termos, 2012). Under this framework the role of the teacher becomes that of a facilitator, providing guidance so that learners can construct their own knowledge. Constructivist educators must make sure that the prior learning experiences are appropriate and related to the concepts being taught. Jonassen (1997) suggests \"well-structured\" learning environments are useful for novice learners and that \"ill-structured\" environments are only useful for more advanced learners. Educators utilizing a constructivist perspective may emphasize an active learning environment that may incorporate learner centered problem-based learning, project-based learning, and inquiry-based learning, ideally involving real-world scenarios, in which students are actively engaged in critical thinking activities. An illustrative discussion and example can be found in the 1980s deployment of constructivist cognitive learning in computer literacy, which involved programming as an instrument of learning. \"LOGO\", a programming language, embodied an attempt to integrate Piagetan ideas with computers and technology. Initially there were broad, hopeful claims, including \"perhaps the most controversial claim\" that it would \"improve general problem-solving skills\" across disciplines. However, \"LOGO\" programming skills did not consistently yield cognitive benefits. It was \"not as concrete\" as advocates claimed, it privileged \"one form of reasoning over all others,\" and it was difficult to apply the thinking activity to non-\"LOGO\"-based activities. By the late 1980s, \"LOGO\" and other similar programming languages had lost their novelty and dominance and were gradually de-emphasized amid criticisms.\n\nFrom a constructivist approach, the research works on the human learning process as a complex adaptive system developed by Peter Belohlavek showed that it is the concept that the individual has that drives the accommodation process to assimilate new knowledge in the long-term memory, defining learning as an intrinsically freedom-oriented and active process. As a student-centered learning approach, the unicist reflection driven learning installs adaptive knowledge objects in the mind of the learner based on a cyclic process of: “action-reflection-action” to foster an adaptive behavior. \n\nThe extent to which e-learning assists or replaces other learning and teaching approaches is variable, ranging on a continuum from none to fully online distance learning. A variety of descriptive terms have been employed (somewhat inconsistently) to categorize the extent to which technology is used. For example, 'hybrid learning' or 'blended learning' may refer to classroom aids and laptops, or may refer to approaches in which traditional classroom time is reduced but not eliminated, and is replaced with some online learning. 'Distributed learning' may describe either the e-learning component of a hybrid approach, or fully online distance learning environments.\n\nE-learning may either be synchronous or asynchronous. Synchronous learning occurs in real-time, with all participants interacting at the same time, while asynchronous learning is self-paced and allows participants to engage in the exchange of ideas or information without the dependency of other participants′ involvement at the same time.\n\nSynchronous learning refers to the exchange of ideas and information with one or more participants during the same period. Examples are face-to-face discussion, online real-time live teacher instruction and feedback, Skype conversations, and chat rooms or virtual classrooms where everyone is online and working collaboratively at the same time. Since students are working collaboratively, synchronized learning helps students become more open minded because they have to actively listen and learn from their peers. Synchronized learning fosters online awareness and improves many students' writing skills.\n\nAsynchronous learning may use technologies such as learning management systems, email, blogs, wikis, and discussion boards, as well as web-supported textbooks, hypertext documents, audio video courses, and social networking using web 2.0. At the professional educational level, training may include virtual operating rooms. Asynchronous learning is beneficial for students who have health problems or who have child care responsibilities. They have the opportunity to complete their work in a low stress environment and within a more flexible time frame. In \"asynchronous\" online courses, students proceed at their own pace. If they need to listen to a lecture a second time, or think about a question for a while, they may do so without fearing that they will hold back the rest of the class. Through online courses, students can earn their diplomas more quickly, or repeat failed courses without the embarrassment of being in a class with younger students. Students have access to an incredible variety of enrichment courses in online learning, and can participate in college courses, internships, sports, or work and still graduate with their class.\n\nComputer-based training (CBT) refers to self-paced learning activities delivered on a computer or handheld device such as a tablet or smartphone. CBT initially delivered content via CD-ROM, and typically presented content linearly, much like reading an online book or manual. For this reason, CBT is often used to teach static processes, such as using software or completing mathematical equations. Computer-based training is conceptually similar to web-based training (WBT) which are delivered via Internet using a web browser.\n\nAssessing learning in a CBT is often by assessments that can be easily scored by a computer such as multiple choice questions, drag-and-drop, radio button, simulation or other interactive means. Assessments are easily scored and recorded via online software, providing immediate end-user feedback and completion status. Users are often able to print completion records in the form of certificates.\n\nCBTs provide learning stimulus beyond traditional learning methodology from textbook, manual, or classroom-based instruction. CBTs can be a good alternative to printed learning materials since rich media, including videos or animations, can be embedded to enhance the learning.\n\nHelp, CBTs pose some learning challenges. Typically, the creation of effective CBTs requires enormous resources. The software for developing CBTs is often more complex than a subject matter expert or teacher is able to use. The lack of human interaction can limit both the type of content that can be presented and the type of assessment that can be performed, and may need supplementation with online discussion or other interactive elements.\n\nComputer-supported collaborative learning (CSCL) uses instructional methods designed to encourage or require students to work together on learning tasks, allowing social learning. CSCL is similar in concept to the terminology, \"e-learning 2.0\" and \"networked collaborative learning\" (NCL). With Web 2.0 advances, sharing information between multiple people in a network has become much easier and use has increased. One of the main reasons for its usage states that it is \"a breeding ground for creative and engaging educational endeavors.\" Learning takes place through conversations about content and grounded interaction about problems and actions. This collaborative learning differs from instruction in which the instructor is the principal source of knowledge and skills. The neologism \"e-learning 1.0\" refers to direct instruction used in early computer-based learning and training systems (CBL). In contrast to that linear delivery of content, often directly from the instructor's material, CSCL uses social software such as blogs, social media, wikis, podcasts, cloud-based document portals, and discussion groups and virtual worlds. This phenomenon has been referred to as Long Tail Learning. Advocates of social learning claim that one of the best ways to learn something is to teach it to others. Social networks have been used to foster online learning communities around subjects as diverse as test preparation and language education. mobile-assisted language learning (MALL) is the use of handheld computers or cell phones to assist in language learning.\n\nCollaborative apps allow students and teachers to interact while studying. Apps are designed after games, which provide a fun way to revise. When the experience is enjoyable the students become more engaged. Games also usually come with a sense of progression, which can help keep students motivated and consistent while trying to improve.\n\nClassroom 2.0 refers to online multi-user virtual environments (MUVEs) that connect schools across geographical frontiers. Known as \"eTwinning\", computer-supported collaborative learning (CSCL) allows learners in one school to communicate with learners in another that they would not get to know otherwise, enhancing educational outcomes and cultural integration.\n\nFurther, many researchers distinguish between collaborative and cooperative approaches to group learning. For example, Roschelle and Teasley (1995) argue that \"cooperation is accomplished by the division of labour among participants, as an activity where each person is responsible for a portion of the problem solving\", in contrast with collaboration that involves the \"mutual engagement of participants in a coordinated effort to solve the problem together.\"\n\nThis is an instructional strategy in which computer-assisted teaching is integrated with classroom instruction. Students are given basic essential instruction, such as lectures, before class instead of during class. Instructional content is delivered outside of the classroom, often online. This frees up classroom time for teachers to more actively engage with learners.\n\nEducational media and tools can be used for:\n\nNumerous types of physical technology are currently used: digital cameras, video cameras, interactive whiteboard tools, document cameras, electronic media, and LCD projectors. Combinations of these techniques include blogs, collaborative software, ePortfolios, and virtual classrooms.\n\nThe current design of this type of applications includes the evaluation through tools of cognitive analysis that allow to identify which elements optimize the use of these platforms. \nRadio offers a synchronous educational vehicle, while streaming audio over the internet with webcasts and podcasts can be asynchronous. Classroom microphones, often wireless, can enable learners and educators to interact more clearly.\n\nVideo technology has included VHS tapes and DVDs, as well as on-demand and synchronous methods with digital video via server or web-based options such as streamed video and webcams. Telecommuting can connect with speakers and other experts. Interactive digital video games are being used at K-12 and higher education institutions.\n\nCollaborative learning is a group-based learning approach in which learners are mutually engaged in a coordinated fashion to achieve a learning goal or complete a learning task. With recent developments in smartphone technology, the processing powers and storage capabilities of modern mobiles allow for advanced development and use of apps. Many app developers and education experts have been exploring smartphone and tablet apps as a medium for collaborative learning.\n\nComputers and tablets enable learners and educators to access websites as well as applications. Many mobile devices support m-learning.\n\nMobile devices such as clickers and smartphones can be used for interactive audience response feedback. Mobile learning can provide performance support for checking the time, setting reminders, retrieving worksheets, and instruction manuals.\n\nSuch devices as iPads are used for helping disabled (visually impaired or with multiple disabilities) children in communication development as well as in improving physiological activity, according to the iStimulation Practice Report.\n\nGroup webpages, blogs, wikis, and Twitter allow learners and educators to post thoughts, ideas, and comments on a website in an interactive learning environment. Social networking sites are virtual communities for people interested in a particular subject to communicate by voice, chat, instant message, video conference, or blogs. The National School Boards Association found that 96% of students with online access have used social networking technologies, and more than 50% talk online about schoolwork. Social networking encourages collaboration and engagement and can be a motivational tool for self-efficacy amongst students.\n\nWebcams and webcasting have enabled creation of virtual classrooms and virtual learning environment. Webcams are also being used to counter plagiarism and other forms of academic dishonesty that might occur in an e-learning environment.\n\nThere are three types of whiteboards. The initial whiteboards, analogous to blackboards, date from the late 1950s. The term whiteboard is also used metaphorically to refer to virtual whiteboards in which computer software applications simulate whiteboards by allowing writing or drawing. This is a common feature of groupware for virtual meeting, collaboration, and instant messaging. Interactive whiteboards allow learners and instructors to write on the touch screen. The screen markup can be on either a blank whiteboard or any computer screen content. Depending on permission settings, this visual learning can be interactive and participatory, including writing and manipulating images on the interactive whiteboard.\n\nScreencasting allows users to share their screens directly from their browser and make the video available online so that other viewers can stream the video directly. The presenter thus has the ability to show their ideas and flow of thoughts rather than simply explain them as simple text content. In combination with audio and video, the educator can mimic the one-on-one experience of the classroom. Learners have an ability to pause and rewind, to review at their own pace, something a classroom cannot always offer.\n\nA virtual learning environment (VLE), also known as a learning platform, simulates a virtual classroom or meetings by simultaneously mixing several communication technologies. Web conferencing software enables students and instructors to communicate with each other via webcam, microphone, and real-time chatting in a group setting. Participants can raise hands, answer polls or take tests. Students are able to whiteboard and screencast when given rights by the instructor, who sets permission levels for text notes, microphone rights and mouse control.\n\nA virtual classroom provides the opportunity for students to receive direct instruction from a qualified teacher in an interactive environment. Learners can have direct and immediate access to their instructor for instant feedback and direction. The virtual classroom provides a structured schedule of classes, which can be helpful for students who may find the freedom of asynchronous learning to be overwhelming. In addition, the virtual classroom provides a social learning environment that replicates the traditional \"brick and mortar\" classroom. Most virtual classroom applications provide a recording feature. Each class is recorded and stored on a server, which allows for instant playback of any class over the course of the school year. This can be extremely useful for students to retrieve missed material or review concepts for an upcoming exam. Parents and auditors have the conceptual ability to monitor any classroom to ensure that they are satisfied with the education the learner is receiving.\n\nIn higher education especially, a virtual learning environment (VLE) is sometimes combined with a management information system (MIS) to create a managed learning environment, in which all aspects of a course are handled through a consistent user interface throughout the institution. Physical universities and newer online-only colleges offer select academic degrees and certificate programs via the Internet. Some programs require students to attend some campus classes or orientations, but many are delivered completely online. Several universities offer online student support services, such as online advising and registration, e-counseling, online textbook purchases, student governments and student newspapers.\n\nAugmented reality (AR) provides students and teachers the opportunity to create layers of digital information, that includes both virtual world and real world elements, to interact with in real time. There are already a variety of apps which offer a lot of variations and possibilities.\n\nMedia psychology involves the application of theories in psychology to media and is a growing specialty in learning and educational technology.\n\nA learning management system (LMS) is software used for delivering, tracking and managing training and education. It tracks data about attendance, time on task, and student progress. Educators can post announcements, grade assignments, check on course activity, and participate in class discussions. Students can submit their work, read and respond to discussion questions, and take quizzes. An LMS may allow teachers, administrators, students, and permitted additional parties (such as parents, if appropriate) to track various metrics. LMSs range from systems for managing training/educational records to software for distributing courses over the Internet and offering features for online collaboration. The creation and maintenance of comprehensive learning content requires substantial initial and ongoing investments of human labor. Effective translation into other languages and cultural contexts requires even more investment by knowledgeable personnel.\n\nInternet-based learning management systems include Canvas, Blackboard Inc. and Moodle. These types of LMS allow educators to run a learning system partially or fully online, asynchronously or synchronously. Learning Management Systems also offer a non-linear presentation of content and curricular goals, giving students the choice of pace and order of information learned. Blackboard can be used for K-12 education, Higher Education, Business, and Government collaboration. Moodle is a free-to-download Open Source Course Management System that provides blended learning opportunities as well as platforms for distance learning courses. Eliademy is a free cloud-based Course Management System that provides blended learning opportunities as well as platforms for distance learning courses. \n\nA learning content management system (LCMS) is software for author content (courses, reusable content objects). An LCMS may be solely dedicated to producing and publishing content that is hosted on an LMS, or it can host the content itself. The Aviation Industry Computer-Based Training Committee (AICC) specification provides support for content that is hosted separately from the LMS.\n\nA recent trend in LCMSs is to address this issue through crowdsourcing (cf.SlideWiki).\n\nComputer-aided assessment (e-assessment) ranges from automated multiple-choice tests to more sophisticated systems. With some systems, feedback can be geared towards a student's specific mistakes or the computer can navigate the student through a series of questions adapting to what the student appears to have learned or not learned. Formative assessment sifts out the incorrect answers, and these questions are then explained by the teacher. The learner then practices with slight variations of the sifted out questions. The process is completed by summative assessment using a new set of questions that only cover the topics previously taught.\n\nAn electronic performance support system (EPSS) is, according to Barry Raybould, \"a computer-based system that improves worker productivity by providing on-the-job access to integrated information, advice, and learning experiences\".\n\nA training management system or training resource management system is a software designed to optimize instructor-led training management. Similar to an enterprise resource planning (ERP), it is a back office tool which aims at streamlining every aspect of the training process: planning (training plan and budget forecasting), logistics (scheduling and resource management), financials (cost tracking, profitability), reporting, and sales for-profit training providers. A training management system can be used to schedule instructors, venues and equipment through graphical agendas, optimize resource utilization, create a training plan and track remaining budgets, generate reports and share data between different teams.\n\nWhile training management systems focus on managing instructor-led training, they can complete an LMS. In this situation, an LMS will manage e-learning delivery and assessment, while a training management system will manage ILT and back-office budget planning, logistic and reporting.\n\nContent and design architecture issues include pedagogy and learning object re-use. One approach looks at five aspects:\n\nPedagogical elements are defined as structures or units of educational material. They are the educational content that is to be delivered. These units are independent of format, meaning that although the unit may be delivered in various ways, the pedagogical structures themselves are not the textbook, web page, video conference, Podcast, lesson, assignment, multiple choice question, quiz, discussion group or a case study, all of which are possible methods of delivery.\n\nMuch effort has been put into the technical reuse of electronically based teaching materials and in particular creating or re-using learning objects. These are self-contained units that are properly tagged with keywords, or other metadata, and often stored in an XML file format. Creating a course requires putting together a sequence of learning objects. There are both proprietary and open, non-commercial and commercial, peer-reviewed repositories of learning objects such as the Merlot repository. Sharable Content Object Reference Model (SCORM) is a collection of standards and specifications that applies to certain web-based e-learning. Other specifications such as Schools Framework allow for the transporting of learning objects, or for categorizing metadata (LOM).\n\nVarious forms of electronic media can be a feature of preschool life. Although parents report a positive experience, the impact of such use has not been systematically assessed.\n\nThe age when a given child might start using a particular technology such as a cellphone or computer might depend on matching a technological resource to the recipient's developmental capabilities, such as the age-anticipated stages labeled by Swiss psychologist, Jean Piaget. Parameters, such as age-appropriateness, coherence with sought-after values, and concurrent entertainment and educational aspects, have been suggested for choosing media.\nAt the preschool level, technology can be introduced in several ways. At the most basic is the use of computers, tablets, and audio and video resources in classrooms. Additionally, there are many resources available for parents and educators to introduce technology to young children or to use technology to augment lessons and enhance learning. Some options that are age-appropriate are video- or audio- recording of their creations, introducing them to the use of the internet through browsing age-appropriate websites, providing assistive technology to allow differently-abled children to participate with the rest of their peers, educational apps, electronic books, and educational videos . There are many free and paid educational website and apps that are directly targeting the educational needs of preschool children. These include Starfall, ABC mouse , PBS Kids Video, Teachme, and Montessori crosswords. Educational technology in the form of electronic books [109] offer preschool children the option to store and retrieve several books on one device, thus bringing together the traditional action of reading along with the use of educational technology. Educational technology is also thought to improve hand-eye coordination, language skills, visual attention and motivation to complete educational tasks, and allows children to experience things they otherwise wouldn’t . There are several keys to making the most educational use out of introducing technology at the preschool level: technology must be used appropriately, should allow access to learning opportunities, should include the interaction of parents and other adults with the preschool children, and should be developmentally appropriate. . Allowing access to learning opportunities especially for allowing disabled children to have access to learning opportunities, giving bilingual children the opportunity to communicate and learn in more than one language, bringing in more information about STEM subjects, and bringing in images of diversity that may be lacking in the child’s immediate environment.\n\nE-learning is utilized by public K–12 schools in the United States as well as private schools. Some e-learning environments take place in a traditional classroom, others allow students to attend classes from home or other locations. There are several states that are utilizing virtual school platforms for e-learning across the country that continue to increase. Virtual school enables students to log into synchronous learning or asynchronous learning courses anywhere there is an internet connection.\n\nE-learning is increasingly being utilized by students who may not want to go to traditional brick and mortar schools due to severe allergies or other medical issues, fear of school violence and school bullying and students whose parents would like to homeschool but do not feel qualified. Online schools create a haven for students to receive a quality education while almost completely avoiding these common problems. Online charter schools also often are not limited by location, income level or class size in the way brick and mortar charter schools are.\n\nE-learning also has been rising as a supplement to the traditional classroom. Students with special talents or interests outside of the available curricula use e-learning to advance their skills or exceed grade restrictions. Some online institutions connect students with instructors via web conference technology to form a digital classroom.\n\nNational private schools are also available online. These provide the benefits of e-learning to students in states where charter online schools are not available. They also may allow students greater flexibility and exemption from state testing. Some of these schools are available at the high school level and offer college prep courses to students.\n\nVirtual education in K-12 schooling often refers to virtual schools, and in higher education to virtual universities. Virtual schools are \"cybercharter schools\" with innovative administrative models and course delivery technology.\n\nOnline college course enrollment has seen a 29% increase in enrollment with nearly one third of all college students, or an estimated 6.7 million students are currently enrolled in online classes. In 2009, 44 percent of post-secondary students in the USA were taking some or all of their courses online, which was projected to rise to 81 percent by 2014.\n\nAlthough a large proportion of for-profit higher education institutions now offer online classes, only about half of private, non-profit schools do so. Private institutions may become more involved with on-line presentations as the costs decrease. Properly trained staff must also be hired to work with students online. These staff members need to understand the content area, and also be highly trained in the use of the computer and Internet. Online education is rapidly increasing, and online doctoral programs have even developed at leading research universities.\n\nAlthough massive open online courses (MOOCs) may have limitations that preclude them from fully replacing college education, such programs have significantly expanded. MIT, Stanford and Princeton University offer classes to a global audience, but not for college credit. University-level programs, like edX founded by Massachusetts Institute of Technology and Harvard University, offer wide range of disciplines at no charge, while others permit students to audit a course at no charge but require a small fee for accreditation. MOOCs have not had a significant impact on higher education and declined after the initial expansion, but are expected to remain in some form.\n\nCompanies with spread out distribution chains use e-learning for staff training and development and to bring customers information about the latest product developments. Continuing professional development (CPD) can deliver regulatory compliance updates and staff development of valuable workplace skills. For effectiveness and competitive learning performance, scoring systems are designed to give live feedback on decision-making in complex (mobile) learning scenarios.\n\nThere is an important need for recent, reliable, and high-quality health information to be made available to the public as well as in summarized form for public health providers. Providers have indicated the need for automatic notification of the latest research, a single searchable portal of information, and access to grey literature. The Maternal and Child Health (MCH) Library is funded by the U.S. Maternal and Child Health Bureau to screen the latest research and develop automatic notifications to providers through the MCH Alert. Another application in public health is the development of mHealth (use of mobile telecommunication and multimedia into global public health). MHealth has been used to promote prenatal and newborn services, with positive outcomes. In addition, \"Health systems have implemented mHealth programs to facilitate emergency medical responses, point-of-care support, health promotion and data collection.\"\nIn low and middle income countries, mHealth is most frequently used as one-way text messages or phone reminders to promote treatment adherence and gather data.\n\nThere has also been a growing interest in e-learning as a beneficial educational method for students with attention deficit hyperactivity disorder (ADHD). With the growing popularity in e-learning among K-12 and higher education, the opportunity to take online classes is becoming increasingly important for students of all ages. However, students with ADHD and special needs face different learning demands compared to the typical developing learner. This is especially significant considering the dramatic rise in ADHD diagnoses in the last decade among both children and adults. Compared to the traditional face-to-face classroom, e-learning and virtual classrooms require a higher level of executive functions, which is the primary deficit associated with ADHD. Although ADHD is not specifically named in the Rehabilitation Act of 1973, students with ADHD who have symptoms that interfere with their learning or ability may be eligible for assistive technology. Some examples of the resources that may help interest students and adults with ADHD consist of, computer software, brain games, timers, calendars, voice recognition devices, screen magnifiers, and talking books.\n\nWolf lists 12 executive function skills necessary for students to succeed in postsecondary education: plan, set goals, organize, initiate, sustain attention/effort, flexibility, monitor, use feedback, structure, manage time, manage materials, and follow through. These skills, along with strong independent and self-regulated learning, are especially pronounced in the online environment and as many ADHD students suffer from a deficit in one or more of these executive functions, this presents a significant challenge and accessibility barrier to the current e-learning approach.\n\nSome have noted that current e-learning models are moving towards applying a constructivism learning theory that emphasizes a learner-centered environment and postulates that everyone has the ability to construct their own knowledge and meaning through a process of problem solving and discovery. However, some principles of constructivism may not be appropriate for ADHD learners; these principles include active learning, self-monitoring, motivation, and strong focus.\n\nDespite the limitations, students with special needs, including ADHD, have expressed an overall enthusiasm for e-learning and have identified a number e-learning benefits, including: availability of online course notes, materials and additional resources; the ability to work at an independent pace and spend extra time formulating thoughtful responses in class discussions; help in understanding course lecture/content; ability to review lectures multiple times; and enhanced access to and communication with the course instructor.\n\nThe design of e-learning platforms in ways that enable universal access has received attention from several directions, including the World Wide Web Consortium's Web Accessibility Initiative (WAI). WAI provides universal formatting standards for websites so they can remain accessible to people with disabilities. For example, developing or adopting e-learning material can enable accessibility for people with visual impairment. The Perkins School for the Blind offers learning resources tailored for the visually impaired, including webcasts, webinars, downloadable science activities, and an online library that has access to over 40,000 resource materials on blindness and deaf blindness.\n\nOnline education may appear to be a promising alternative for students with physical and sensory disabilities because they get to work at their own pace and in their own home. However, not all online programs are equal when it comes to their resources for students with disabilities. Students with disabilities who wish to enroll in online education must either be able to advocate for themselves and their own rights or have a person who is willing to advocate for them. The American with Disabilities Act states that online programs must provide appropriate accommodations for students with disabilities, but has not specifically defined what that means. \"Once students with disabilities are accepted into an online program, they should prepare to be direct and open about what they need to succeed, experts say\" (Haynie).\n\nEducational technology, particularly in online learning environments, can allow students to use real identity, pseudonym, or anonymous identity during classroom communication. Advantages in anonymizing race, age, and gender are increased student participation and increased cross-cultural communication. Risks include increased cyberbullying, and aggressive or hostile language.\n\nEffective technology use deploys multiple evidence-based strategies concurrently (e.g. adaptive content, frequent testing, immediate feedback, etc.), as do effective teachers. Using computers or other forms of technology can give students practice on core content and skills while the teacher can work with others, conduct assessments, or perform other tasks. Through the use of educational technology, education is able to be individualized for each student allowing for better differentiation and allowing students to work for mastery at their own pace.\n\nModern educational technology can improve access to education, including full degree programs. It enables better integration for non-full-time students, particularly in continuing education, and improved interactions between students and instructors. Learning material can be used for long distance learning and are accessible to a wider audience. Course materials are easy to access. In 2010, 70.3% of American family households had access to the internet. In 2013, according to Canadian Radio Television and Telecommunications Commission Canada, 79% of homes have access to the internet. Students can access and engage with numerous online resources at home. Using online resources can help students spend more time on specific aspects of what they may be learning in school, but at home. Schools like MIT have made certain course materials free online. Although some aspects of a classroom setting are missed by using these resources, they are helpful tools to add additional support to the educational system. The necessity to pay for transport to the educational facility is removed.\n\nStudents appreciate the convenience of e-learning, but report greater engagement in face-to-face learning environments.\n\nAccording to James Kulik, who studies the effectiveness of computers used for instruction, students usually learn more in less time when receiving computer-based instruction and they like classes more and develop more positive attitudes toward computers in computer-based classes. Students can independently solve problems. There are no intrinsic age-based restrictions on difficulty level, i.e. students can go at their own pace. Students editing their written work on word processors improve the quality of their writing. According to some studies, the students are better at critiquing and editing written work that is exchanged over a computer network with students they know. Studies completed in \"computer intensive\" settings found increases in student-centric, cooperative and higher order learning, writing skills, problem solving, and using technology. In addition, attitudes toward technology as a learning tool by parents, students and teachers are also improved.\n\nEmployers' acceptance of online education has risen over time. More than 50% of human resource managers SHRM surveyed for an August 2010 report said that if two candidates with the same level of experience were applying for a job, it would not have any kind of effect whether the candidate's obtained degree was acquired through an online or a traditional school. Seventy-nine percent said they had employed a candidate with an online degree in the past 12 months. However 66% said candidates who get degrees online were not seen as positively as a job applicant with traditional degrees.\n\nThe use of educational apps generally has positive effect on learning. Pre- and post- tests reveal that the use of apps on mobile devices reduces the achievement gap between struggling and average students. Some educational apps improve group work by allowing students to receive feedback on answers and promoting collaboration in solving problems, examples of these apps can be found in the third paragraph. The benefits of app-assisted learning have been exhibited in all age groups. Kindergarten students that use iPads show much higher rates of literacy than non-users. Medical students at University of California Irvine that utilized iPad academically have been reported to score 23% higher on national exams than \nprevious classes that did not.\n\nMany US states spend large sums of money on technology. However, , none were looking at technology return on investment (ROI) to connect expenditures on technology with improved student outcomes.\n\nNew technologies are frequently accompanied by unrealistic hype and promise regarding their transformative power to change education for the better or in allowing better educational opportunities to reach the masses. Examples include silent film, broadcast radio, and television, none of which have maintained much of a foothold in the daily practices of mainstream, formal education. Technology, in and of itself, does not necessarily result in fundamental improvements to educational practice. The focus needs to be on the learner's interaction with technology—not the technology itself. It needs to be recognized as \"ecological\" rather than \"additive\" or \"subtractive\". In this ecological change, one significant change will create total change.\n\nAccording to Branford et al., \"technology does not guarantee effective learning\" and inappropriate use of technology can even hinder it. A University of Washington study of infant vocabulary shows that it is slipping due to educational baby DVDs. Published in the Journal of Pediatrics, a 2007 University of Washington study on the vocabulary of babies surveyed over 1,000 parents in Washington and Minnesota. The study found that for every one hour that babies 8–16 months of age watched DVDs and Videos they knew 6-8 fewer of 90 common baby words than the babies that did not watch them. Andrew Meltzoff, a surveyor in this study states that the result makes sense, that if the baby's 'alert time' is spent in front of DVDs and TV, instead of with people speaking, the babies are not going to get the same linguistic experience. Dr. Dimitri Chistakis, another surveyor reported that the evidence is mounting that baby DVDs are of no value and may be harmful.\n\nAdaptive instructional materials tailor questions to each student's ability and calculate their scores, but this encourages students to work individually rather than socially or collaboratively (Kruse, 2013). Social relationships are important but high-tech environments may compromise the balance of trust, care and respect between teacher and student.\n\nMassively open online courses (MOOCs), although quite popular in discussions of technology and education in developed countries (more so in US), are not a major concern in most developing or low-income countries. One of the stated goals of MOOCs is to provide less fortunate populations (i.e., in developing countries) an opportunity to experience courses with US-style content and structure. However, research shows only 3% of the registrants are from low-income countries and although many courses have thousands of registered students only 5-10% of them complete the course. MOOCs also implies that certain curriculum and teaching methods are superior and this could eventually wash over (or possibly washing out) local educational institutions, cultural norms and educational traditions.\n\nWith the Internet and social media, using educational apps makes the students highly susceptible to distraction and sidetracking. Even though proper use has shown to increase student performances, being distracted would be detrimental. Another disadvantage is increased potential for cheating. Smartphones can be very easy to hide and use inconspicuously, especially if their use is normalized in the classroom. These disadvantages can be managed with strict rules and regulations on mobile phone use.\n\nElectronic devices such as cellphones and computers facilitate rapid access to a stream of sources, each of which may receive cursory attention. Michel Rich, an associate professor at Harvard Medical School and executive director of the center on Media and Child Health in Boston, said of the digital generation, \"Their brains are rewarded not for staying on task, but for jumping to the next thing. The worry is we're raising a generation of kids in front of screens whose brains are going to be wired differently.\" Students have always faced distractions; computers and cellphones are a particular challenge because the stream of data can interfere with focusing and learning. Although these technologies affect adults too, young people may be more influenced by it as their developing brains can easily become habituated to switching tasks and become unaccustomed to sustaining attention. Too much information, coming too rapidly, can overwhelm thinking.\n\nTechnology is \"rapidly and profoundly altering our brains.\" High exposure levels stimulate brain cell alteration and release neurotransmitters, which causes the strengthening of some neural pathways and weakening of others. This leads to heightened stress levels on the brain that, at first, boost energy levels, but, over time, actually augment memory, impair cognition, lead to depression, alter the neural circuitry of the hippocampus, amygdala and prefrontal cortex. These are the brain regions that control mood and thought. If unchecked, the underlying structure of the brain could be altered. Over-stimulation due to technology may begin too young. When children are exposed before the age of seven, important developmental tasks may be delayed, and bad learning habits might develop, which \"deprives children of the exploration and play that they need to develop.\" Media psychology is an emerging specialty field that embraces electronic devices and the sensory behaviors occurring from the use of educational technology in learning.\n\nAccording to Lai, \"the learning environment is a complex system where the interplay and interactions of many things impact the outcome of learning.\" When technology is brought into an educational setting, the pedagogical setting changes in that technology-driven teaching can change the entire meaning of an activity without adequate research validation. If technology monopolizes an activity, students can begin to develop the sense that \"life would scarcely be thinkable without technology.\"\n\nLeo Marx considered the word \"technology\" itself as problematic, susceptible to reification and \"phantom objectivity\", which conceals its fundamental nature as something that is only valuable insofar as it benefits the human condition. Technology ultimately comes down to affecting the relations between people, but this notion is obfuscated when technology is treated as an abstract notion devoid of good and evil. Langdon Winner makes a similar point by arguing that the underdevelopment of the philosophy of technology leaves us with an overly simplistic reduction in our discourse to the supposedly dichotomous notions of the \"making\" versus the \"uses\" of new technologies, and that a narrow focus on \"use\" leads us to believe that all technologies are neutral in moral standing. These critiques would have us ask not, \"How do we maximize the role or advancement of technology in education?\", but, rather, \"What are the social and human consequences of adopting any particular technology?\"\n\nWinner viewed technology as a \"form of life\" that not only aids human activity, but that also represents a powerful force in reshaping that activity and its meaning. For example, the use of robots in the industrial workplace may increase productivity, but they also radically change the process of production itself, thereby redefining what is meant by \"work\" in such a setting. In education, standardized testing has arguably redefined the notions of learning and assessment. We rarely explicitly reflect on how strange a notion it is that a number between, say, 0 and 100 could accurately reflect a person's knowledge about the world. According to Winner, the recurring patterns in everyday life tend to become an unconscious process that we learn to take for granted. Winner writes,\n\nBy far the greatest latitude of choice exists the very first time a particular instrument, system, or technique is introduced. Because choices tend to become strongly fixed in material equipment, economic investment, and social habit, the original flexibility vanishes for all practical purposes once the initial commitments are made. In that sense technological innovations are similar to legislative acts or political foundings that establish a framework for public order that will endure over many generations. (p. 29) \n\nWhen adopting new technologies, there may be one best chance to \"get it right.\" Seymour Papert (p. 32) points out a good example of a (bad) choice that has become strongly fixed in social habit and material equipment: our \"choice\" to use the QWERTY keyboard. The QWERTY arrangement of letters on the keyboard was originally chosen, not because it was the most efficient for typing, but because early typewriters were prone to jam when adjacent keys were struck in quick succession. Now that typing has become a digital process, this is no longer an issue, but the QWERTY arrangement lives on as a social habit, one that is very difficult to change.\n\nNeil Postman endorsed the notion that technology impacts human cultures, including the culture of classrooms, and that this is a consideration even more important than considering the efficiency of a new technology as a tool for teaching. Regarding the computer's impact on education, Postman writes (p. 19):\n\nWhat we need to consider about the computer has nothing to do with its efficiency as a teaching tool. We need to know in what ways it is altering our conception of learning, and how in conjunction with television, it undermines the old idea of school.There is an assumption that technology is inherently interesting so it must be helpful in education; based on research by Daniel Willingham, that is not always the case. He argues that it does not necessarily matter what the technological medium is, but whether or not the content is engaging and utilizes the medium in a beneficial way.\n\nThe concept of the digital divide is a gap between those who have access to digital technologies and those who do not. Access may be associated with age, gender, socio-economic status, education, income, ethnicity, and geography.\n\nAccording to a report by the Electronic Frontier Foundation, large amounts of personal data on children is collected by electronic devices that are distributed in schools in the United States. Often far more information than necessary is collected, uploaded and stored indefinitely. Aside name and date of birth, this information can include the child's browsing history, search terms, location data, contact lists, as well as behavioral information. Parents are not informed or, if informed, have little choice. According to the report, this constant surveillance resulting from educational technology can \"warp children's privacy expectations, lead them to self-censor, and limit their creativity\".\n\nSince technology is not the end goal of education, but rather a means by which it can be accomplished, educators must have a good grasp of the technology and its advantages and disadvantages. Teacher training aims for effective integration of classroom technology.\n\nThe evolving nature of technology may unsettle teachers, who may experience themselves as perpetual novices. Finding quality materials to support classroom objectives is often difficult. Random professional development days are inadequate.\n\nAccording to Jenkins, \"Rather than dealing with each technology in isolation, we would do better to take an ecological approach, thinking about the interrelationship among different communication technologies, the cultural communities that grow up around them, and the activities they support.\" Jenkins also suggested that the traditional school curriculum guided teachers to train students to be autonomous problem solvers. However, today's workers are increasingly asked to work in teams, drawing on different sets of expertise, and collaborating to solve problem. Learning styles and the methods of collecting information have evolved, and \"students often feel locked out of the worlds described in their textbooks through the depersonalized and abstract prose used to describe them\". These twenty-first century skills can be attained through the incorporation and engagement with technology. Changes in instruction and use of technology can also promote a higher level of learning among students with different types of intelligence.\n\nThere are two distinct issues of assessment: the assessment \"of\" educational technology and assessment \"with\" technology.\n\nAssessments \"of\" educational technology have included the \"Follow Through\" project.\n\nEducational assessment \"with\" technology may be either formative assessment or summative assessment. Instructors use both types of assessment to understand student progress and learning in the classroom. Technology has helped teachers create better assessments to help understand where students who are having trouble with the material are having issues.\n\nFormative assessment is more difficult, as the perfect form is ongoing and allows the students to show their learning in different ways depending on their learning styles. Technology has helped some teachers make their formative assessments better, particularly through the use of classroom response systems (CRS). A CRS is a tool in which the students each have a handheld device that partners up with the teacher's computer. The instructor then asks multiple choice or true or false questions and the students answer on their device. Depending on the software used, the answers may then be shown on a graph so students and teacher can see the percentage of students who gave each answer and the teacher can focus on what went wrong.\n\nSummative assessments are more common in classrooms and are usually set up to be more easily graded, as they take the form of tests or projects with specific grading schemes. One huge benefit to tech-based testing is the option to give students immediate feedback on their answers. When students get these responses, they are able to know how they are doing in the class which can help push them to improve or give them confidence that they are doing well. Technology also allows for different kinds of summative assessment, such as digital presentations, videos, or anything else the teacher/students may come up with, which allows different learners to show what they learned more effectively. Teachers can also use technology to post graded assessments online for students to have a better idea of what a good project is.\n\nElectronic assessment uses information technology. It encompasses several potential applications, which may be teacher or student oriented, including educational assessment throughout the continuum of learning, such as computerized classification testing, computerized adaptive testing, student testing, and grading an exam. E-Marking is an examiner led activity closely related to other e-assessment activities such as e-testing, or e-learning which are student led. E-marking allows markers to mark a scanned script or online response on a computer screen rather than on paper.\n\nThere are no restrictions to the types of tests that can use e-marking, with e-marking applications designed to accommodate multiple choice, written, and even video submissions for performance examinations. E-marking software is used by individual educational institutions and can also be rolled out to the participating schools of awarding exam organisations. e-marking has been used to mark many well known high stakes examinations, which in the United Kingdom include A levels and GCSE exams, and in the US includes the SAT test for college admissions. Ofqual reports that e-marking is the main type of marking used for general qualifications in the United Kingdom.\n\nIn 2014, the Scottish Qualifications Authority (SQA) announced that most of the National 5 question papers would be e-marked.\n\nIn June 2015, the Odisha state government in India announced that it planned to use e-marking for all Plus II papers from 2016.\n\nThe importance of self-assessment through tools made available on Educational Technology platforms has been growing. Self-assessment in education technology relies on students analyzing their strengths, weaknesses and areas where improvement is possible to set realistic goals in learning, improve their educational performances and track their progress. One of the unique tools for self-assessment made possible by education technology is Analytics. Analytics is data gathered on the student's activities on the learning platform, drawn into meaningful patterns that leads to a valid conclusion, usually through the medium of data visualization such as graphs. Learning analytics is the field that focus in analyzing and reporting data about student's activities in order to facilitate learning.\n\nThe five key sectors of the e-learning industry are consulting, content, technologies, services and support. Worldwide, e-learning was estimated in 2000 to be over $48 billion according to conservative estimates. Commercial growth has been brisk. In 2014, the worldwide commercial market activity was estimated at $6 billion venture capital over the past five years, with self-paced learning generating $35.6 billion in 2011. North American e-learning generated $23.3 billion in revenue in 2013, with a 9% growth rate in cloud-based authoring tools and learning platforms.\n\nEducational technologists and psychologists apply basic educational and psychological research into an evidence-based applied science (or a technology) of learning or instruction. In research, these professions typically require a graduate degree (Master's, Doctorate, Ph.D., or D.Phil.) in a field related to educational psychology, educational media, experimental psychology, cognitive psychology or, more purely, in the fields of educational, instructional or human performance technology or instructional design. In industry, educational technology is utilized to train students and employees by a wide range of learning and communication practitioners, including instructional designers, technical trainers, technical communication and professional communication specialists, technical writers, and of course primary school and college teachers of all levels. The transformation of educational technology from a cottage industry to a profession is discussed by Shurville et al.\n\n\n"}
{"id": "1227154", "url": "https://en.wikipedia.org/wiki?curid=1227154", "title": "Geodetic airframe", "text": "Geodetic airframe\n\nA geodesic (or geodetic) airframe is a type of construction for the airframes of aircraft developed by British aeronautical engineer Barnes Wallis in the 1930s. Before it was developed by Prof. Schütte for the Schütte Lanz Airship LS 1 in 1909. It makes use of a space frame formed from a spirally crossing basket-weave of load-bearing members. The principle is that two geodesic arcs can be drawn to intersect on a curving surface (the fuselage) in a manner that the torsional load on each cancels out that on the other.\n\nThe \"diagonal rider\" structural element was used by Joshua Humphreys in the first US Navy sail frigates in 1794. Diagonal riders are viewable in the interior hull structure of the preserved \"USS Constitution\" on display in Boston Harbor. The structure was a pioneering example of placing \"non-orthogonal\" structural components within an otherwise conventional structure for its time. As the \"diagonal riders\" were included in these American naval vessels' construction to reduce the problem of hogging in the ship's hull, and did not make up the bulk of the vessel's structure, they do not constitute a completely \"geodetic\" space frame.\n\nCalling any diagonal wood brace (as used on gates, buildings, ships or other structures with cantilevered or diagonal loads) an example of geodesic design is a misnomer. In a geodetic structure, the strength and structural integrity, and indeed the shape, come from the diagonal \"braces\" - the structure does not need the \"bits in between\" for part of its strength (implicit in the name space frame) as does a more conventional wooden structure.\n\nThe earliest-known use of a geodesic airframe design for any aircraft was for the pre-World War I Schütte-Lanz SL1 rigid airship's envelope structure of 1911, with the airship capable of up to a 38.3 km/h (23.8 mph) top airspeed.\n\nThe Latécoère 6 was a French four-engined biplane bomber of the early 1920s. It was of advanced all-metal construction and probably the first aircraft to use geodetic construction. Only one was built.\n\nBarnes Wallis, inspired by his earlier experience with light alloy structures and the use of geodesically-arranged wiring to distribute the lifting loads of the gasbags in the design of the \"R100\" airship, evolved the geodetic construction method (although it is commonly stated, there was no geodetic \"structure\" in \"R100\"). Wallis used the term \"geodetic\" to apply to the airframe and distinguish it from \"geodesic\" which is the proper term for a line on a curved surface, arising from geodesy.\n\nThe system was later used by Wallis's employer, Vickers-Armstrongs in a series of bomber aircraft, the Wellesley, Wellington, Warwick and Windsor. In these aircraft, the fuselage was built up from a number of duralumin alloy channel-beams that were formed into a large framework. Wooden battens were screwed onto the metal, to which the doped linen skin of the aircraft was fixed.\n\nThe metal lattice-work gave a light structure with tremendous strength; any one of the stringers could support some of the load from the opposite side of the aircraft. Blowing out the structure from one side would still leave the load-bearing structure as a whole intact. As a result, Wellingtons with huge areas of framework missing continued to return home when other types would not have survived; the dramatic effect enhanced by the doped fabric skin burning off, leaving the naked frames exposed (see photo). The benefits of the geodesic construction were partly offset by the difficulty of modifying the physical structure of the aircraft to allow for a change in length, profile, wingspan etc.\n\nGeodetic wing and fin structures - taken from the Wellington - were used on the post-war Vickers VC.1 Viking, though with a new fuselage and metal skinned.\n\n\n"}
{"id": "4355840", "url": "https://en.wikipedia.org/wiki?curid=4355840", "title": "Glass-coated wire", "text": "Glass-coated wire\n\nGlass-coating is a process invented in 1924 by G. F. Taylor and converted into production machine by Ulitovski for producing fine glass-coated metal filaments only a few micrometres in diameter.\n\nIn this process, known as the \"Taylor-wire\" or \"microwire process\" or \"Taylor-Ulitovski process\", the metal to be produced in microwire form is held in a glass tube, typically a borosilicate composition, which is closed at one end. This end of the tube is then heated in order to soften the glass to a temperature at which the metal part is in liquid state and the glass can be drawn down to produce a fine glass capillary containing a metal core. In recent years the process was converted to continuous one by continuously feeding the metal drop with new material. Although this process is simple enough it requires a lot of factors to be met at the same time. The continuous flow of metal that is being coated by the glass has to be melted at the same temperature as the glass otherwise there may be consistency problems which could lead to a change in the properties of the wire. This means that metals that have a high melting temperature can not be used because it may prove difficult to match the high melting point of the metal to a high melting point in a glass. The rate at which the metal wire is pulled also has to be monitored due to the fact that a fluctuation in the speed of pulling may cause a difference of width in the wire. Not only does the wire need to be pulled at the same rate but it also needs to be cooled in a stable environment, which is normally conducted by moving the wire through a stream of cooled water or oil. However. there are some apparatuses that can bypass some of these problems by heating the glass and the metal in separate chambers which allows for the use of metals with high melting points. Around the 1950s the Taylor-Ulitovski process was changed to a continuous feeding process of the materials in order to make these wires on a mass production scale.\n\nMetal cores in the range 1 to 120 micrometres with a glass coating a few micrometres in diameter can be readily produced by this method. Glass-coated microwires successfully produced by this method include copper, silver, gold, iron, platinum, and various alloy compositions. It has even proved possible to produce amorphous metal (\"glassy metal\") cores because the cooling rate achievable by this process can be of the order of 1,000,000 kelvins per second. Glass-coated wire receives all of its material properties from its microstructure. The microstructure in turn receives its properties from the rate at which the wire is cooled. The magnetic properties of glass-coated wires also differ greatly from the properties of amorphous wires and cold-drawn wires due to the difference of the internal stresses that are occurring in the wire. When choosing a metal for the wire Fe-rich compositions of metals typically hold an advantage over Co-rich compositions since Co is more expensive and Fe-rich metals have better magnetic properties. The magnetic properties such as the magnetic softness of Fe-rich materials can be improved by annealing the metal while it is under mechanical stresses.\n\nThe glass coating of wires improves the thermal stability of the wire. The wires will remain stable until the glass, in this case Pyrex (borosilicate), begins to soften. Pyrex generally begins to soften around 673 Kelvin, therefore, these wires can be used in coolers or in heaters that operate under the temperature of 673 Kelvin. Not only does the glass coating of the wire provide thermal stability but it also helps to prevent metallic corrosion of the wire.\n\nApplications for microwire include miniature electrical components based on copper-cored microwire. Amorphous metal cores with special magnetic properties can even be employed in such articles as security tags and related devices.Cobalt and iron base alloys are used to produce anti-shoplifting labels and security papers. The glass-coated wire has also proven quite valuable in devices that are used to sense brain tumors and used in medical equipment. The main consumers of glass-coated wire are the medical and automobile industries since glass coated wire is very valuable when it comes to precise sensors.\n\nThe Taylor-Ulitovski process has been proven successful in academic environments however it was never duplicated for high volume mass production. The modified Adar-Bolotinsky process has made it possible to produce micro bonding wire directly from the melt, by casting instead of the traditional drawing, converting this process to a mass production scale. This special manufacturing process also makes it possible to develop RED micro wire, for example RED Copper wire which is unique composite wire with a thin glass-coating and a soft copper core. Glass-coated wire had a huge impact on the LED industry by reducing the cost of interconnect components, specifically using Copper wire instead of Gold. Using the Adar-Bolotinsky process has made it possible to coat these wires with glass which protects it from oxidisation, increasing the shelf and operating life. these improvements have contributed to the current success of LED lighting.\n"}
{"id": "24713216", "url": "https://en.wikipedia.org/wiki?curid=24713216", "title": "Guide rail", "text": "Guide rail\n\nA guide rail is device or mechanism to direct products, vehicles or other objects through a channel, conveyor, roadway or rail system. \nSeveral types of guide rails exist and may be associated with:\n\nMost factories use guide rails convey products and component parts along an assembly line. This conveyor system propels products of various sizes, shapes, and dimensions through the factory over the course of their assembly.\nAccessory to a power tool, such as a straight, swivel or angle jig for a circular saw, and can also referred to as a fence. The guide rail system provides an acute method of cutting material.\n\nGuide rails are part of the inner workings of most elevator and lift shafts, functioning as the vertical, internal track. In the referenced graphic from \"How Elevators Work\", the guide rails (labeled 5) are fixed to two sides of the shaft; one guides the elevator car and the other for the counterweight. In tandem, these rails operate both as stabilization within the shaft during routine use and as a safety system in case of emergency stops.\n\nA guide rail is a system designed to guide vehicles back to the roadway and away from potentially hazardous situations. There is no legal distinction between a guide rail and a guard rail.\nSeveral types of roadway guide rail exist; all are engineered to guide vehicular traffic on roads or bridges. Such systems include W-beam, box beam, cable, and concrete barrier. Each system is intended to guide vehicles back onto the road as opposed to guard them from going off the road into potential danger.\n\nAccording to the US Federal Highway Administration, \"The terms guardrail and guiderail are synonymous, and are used in different regions around the country.\" Guide rail and guard rail are intended to steer and \"guide\" vehicles back onto the road. Since guard rails are designed to keep vehicles safe, and were not designed to guard vehicles from going off the roadway, one could argue that the most technically correct name when referring to road-side rails is guide rail.\nAccording to Cornell University’s Local Roads Program, “Quick Bites” (January 2003), titled \"Guide Rails: Introduction\", “The purpose of guide rail is to protect vehicle occupants from roadside hazards, like drop-offs or fixed objects.”\n\nThe following quote is from a \"District 8-0 FAQ, under Right-of-Way (Areas Along Highways & Roads)\" from Pennsylvania’s Dept of Motor Vehicles, see PA DOT District8 FAQ:\n\n\"Question:\" Do I have a location where guide rail is needed?\n\n\"Answer:\" Motorists should keep their vehicle on the roadway, driving sensibly in accordance with the PA Vehicle Code; however, this goal is not always realized. Motorists may run off the road for many reasons, including driver error in the form of excessive speed, falling asleep, reckless or inattentive driving, or driving under the influence of alcohol or other drugs. A driver may also have mechanical problems or leave the road deliberately to avoid a collision with another vehicle or object. Guide rail (formerly called guard rail) is only installed when warranted and justified. Guide rail warrants are based on the premise that a traffic barrier should be installed only if it reduces the severity of potential crashes.\n\nOn the Sapporo Municipal Subway a central rail guides the train. Translohr and Bombardier Guided Light Transit are also guided by a central guide rail.\n"}
{"id": "13191", "url": "https://en.wikipedia.org/wiki?curid=13191", "title": "HTML", "text": "HTML\n\nHypertext Markup Language (HTML) is the standard markup language for creating web pages and web applications. With Cascading Style Sheets (CSS) and JavaScript, it forms a triad of cornerstone technologies for the World Wide Web.\n\nWeb browsers receive HTML documents from a web server or from local storage and render the documents into multimedia web pages. HTML describes the structure of a web page semantically and originally included cues for the appearance of the document.\n\nHTML elements are the building blocks of HTML pages. With HTML constructs, images and other objects such as interactive forms may be embedded into the rendered page. HTML provides a means to create structured documents by denoting structural semantics for text such as headings, paragraphs, lists, links, quotes and other items. HTML elements are delineated by \"tags\", written using angle brackets. Tags such as and directly introduce content into the page. Other tags such as surround and provide information about document text and may include other tags as sub-elements. Browsers do not display the HTML tags, but use them to interpret the content of the page.\n\nHTML can embed programs written in a scripting language such as JavaScript, which affects the behavior and content of web pages. Inclusion of CSS defines the look and layout of content. The World Wide Web Consortium (W3C), maintainer of both the HTML and the CSS standards, has encouraged the use of CSS over explicit presentational HTML \n\nIn 1980, physicist Tim Berners-Lee, a contractor at CERN, proposed and prototyped ENQUIRE, a system for CERN researchers to use and share documents. In 1989, Berners-Lee wrote a memo proposing an Internet-based hypertext system. Berners-Lee specified HTML and wrote the browser and server software in late 1990. That year, Berners-Lee and CERN data systems engineer Robert Cailliau collaborated on a joint request for funding, but the project was not formally adopted by CERN. In his personal notes from 1990 he listed \"some of the many areas in which hypertext is used\" and put an encyclopedia first.\n\nThe first publicly available description of HTML was a document called \"HTML Tags\", first mentioned on the Internet by Tim Berners-Lee in late 1991. It describes 18 elements comprising the initial, relatively simple design of HTML. Except for the hyperlink tag, these were strongly influenced by SGMLguid, an in-house Standard Generalized Markup Language (SGML)-based documentation format at CERN. Eleven of these elements still exist in HTML 4.\n\nHTML is a markup language that web browsers use to interpret and compose text, images, and other material into visual or audible web pages. Default characteristics for every item of HTML markup are defined in the browser, and these characteristics can be altered or enhanced by the web page designer's additional use of CSS. Many of the text elements are found in the 1988 ISO technical report TR 9537 \"Techniques for using SGML\", which in turn covers the features of early text formatting languages such as that used by the RUNOFF command developed in the early 1960s for the CTSS (Compatible Time-Sharing System) operating system: these formatting commands were derived from the commands used by typesetters to manually format documents. However, the SGML concept of generalized markup is based on elements (nested annotated ranges with attributes) rather than merely print effects, with also the separation of structure and markup; HTML has been progressively moved in this direction with CSS.\n\nBerners-Lee considered HTML to be an application of SGML. It was formally defined as such by the Internet Engineering Task Force (IETF) with the mid-1993 publication of the first proposal for an HTML specification, the \"Hypertext Markup Language (HTML)\" Internet Draft by Berners-Lee and Dan Connolly, which included an SGML Document type definition to define the grammar. The draft expired after six months, but was notable for its acknowledgment of the NCSA Mosaic browser's custom tag for embedding in-line images, reflecting the IETF's philosophy of basing standards on successful prototypes. Similarly, Dave Raggett's competing Internet-Draft, \"HTML+ (Hypertext Markup Format)\", from late 1993, suggested standardizing already-implemented features like tables and fill-out forms.\n\nAfter the HTML and HTML+ drafts expired in early 1994, the IETF created an HTML Working Group, which in 1995 completed \"HTML 2.0\", the first HTML specification intended to be treated as a standard against which future implementations should be based.\n\nFurther development under the auspices of the IETF was stalled by competing interests. the HTML specifications have been maintained, with input from commercial software vendors, by the World Wide Web Consortium (W3C). However, in 2000, HTML also became an international standard (ISO/IEC 15445:2000). HTML 4.01 was published in late 1999, with further errata published through 2001. In 2004, development began on HTML5 in the Web Hypertext Application Technology Working Group (WHATWG), which became a joint deliverable with the W3C in 2008, and completed and standardized on 28 October 2014.\n\n\n\n\n\nXHTML is a separate language that began as a reformulation of HTML 4.01 using XML 1.0. It is no longer being developed as a separate standard.\n\nHTML markup consists of several key components, including those called \"tags\" (and their \"attributes\"), character-based \"data types\", \"character references\" and \"entity references\". HTML tags most commonly come in pairs like and , although some represent \"empty elements\" and so are unpaired, for example . The first tag in such a pair is the \"start tag\", and the second is the \"end tag\" (they are also called \"opening tags\" and \"closing tags\").\n\nAnother important component is the HTML \"document type declaration\", which triggers standards mode rendering.\n\nThe following is an example of the classic \"Hello, World!\" program:\n\n<!DOCTYPE html>\n<html>\n</html>\n\nThe text between and describes the web page, and the text between and is the visible page content. The markup text defines the browser page title.\n\nThe Document Type Declaration is for HTML5. If a declaration is not included, various browsers will revert to \"quirks mode\" for rendering.\n\nHTML documents imply a structure of nested HTML elements. These are indicated in the document by HTML \"tags\", enclosed in angle brackets thus: .\n\nIn the simple, general case, the extent of an element is indicated by a pair of tags: a \"start tag\" and \"end tag\" . The text content of the element, if any, is placed between these tags.\n\nTags may also enclose further tag markup between the start and end, including a mixture of tags and text. This indicates further (nested) elements, as children of the parent element.\n\nThe start tag may also include \"attributes\" within the tag. These indicate other information, such as identifiers for sections within the document, identifiers used to bind style information to the presentation of the document, and for some tags such as the used to embed images, the reference to the image resource.\n\nSome elements, such as the line break , do not permit \"any\" embedded content, either text or further tags. These require only a single empty tag (akin to a start tag) and do not use an end tag.\n\nMany tags, particularly the closing end tag for the very commonly used paragraph element , are optional. An HTML browser or other agent can infer the closure for the end of an element from the context and the structural rules defined by the HTML standard. These rules are complex and not widely understood by most HTML coders.\n\nThe general form of an HTML element is therefore: . Some HTML elements are defined as \"empty elements\" and take the form . Empty elements may enclose no content, for instance, the tag or the inline tag.\nThe name of an HTML element is the name used in the tags.\nNote that the end tag's name is preceded by a slash character, codice_1, and that in empty elements the end tag is neither required nor allowed.\nIf attributes are not mentioned, default values are used in each case.\n\nHeader of the HTML document: . The title is included in the head, for example:\n\n<head>\n</head>\n\nHeadings: HTML headings are defined with the to tags:\n\n<h1>Heading level 1</h1>\n<h2>Heading level 2</h2>\n<h3>Heading level 3</h3>\n<h4>Heading level 4</h4>\n<h5>Heading level 5</h5>\n<h6>Heading level 6</h6>\nParagraphs:<p>Paragraph 1</p> <p>Paragraph 2</p>\nLine breaks: . The difference between and is that codice_2 breaks a line without altering the semantic structure of the page, whereas codice_3 sections the page into paragraphs. Note also that codice_2 is an \"empty element\" in that, although it may have attributes, it can take no content and it may not have an end tag.\n<p>This <br> is a paragraph <br> with <br> line breaks</p>\nThis is a link in HTML. To create a link the tag is used. The codice_5 attribute holds the URL address of the link.\n<a href=\"https://www.wikipedia.org/\">A link to Wikipedia!</a>\nInputs:\nThere are many possible ways a user can give input/s like:\n<input type=\"text\" /> <!-- This is for text input -->\n<input type=\"file\" /> <!-- This is for uploading files -->\n<input type=\"checkbox\" /> <!-- This is for checkboxes -->\n\nComments:\n<!-- This is a comment --> Comments can help in the understanding of the markup and do not display in the webpage.\n\nThere are several types of markup elements used in HTML:\n\nMost of the attributes of an element are name-value pairs, separated by codice_11 and written within the start tag of an element after the element's name. The value may be enclosed in single or double quotes, although values consisting of certain characters can be left unquoted in HTML (but not XHTML). Leaving attribute values unquoted is considered unsafe. In contrast with name-value pair attributes, there are some attributes that affect the element simply by their presence in the start tag of the element, like the codice_12 attribute for the codice_7 element.\n\nThere are several common attributes that may appear in many elements :\n\n\nThe abbreviation element, codice_24, can be used to demonstrate some of these attributes:\n\n<abbr id=\"anId\" class=\"jargon\" style=\"color:purple;\" title=\"Hypertext Markup Language\">HTML</abbr>\n\nThis example displays as HTML; in most browsers, pointing the cursor at the abbreviation should display the title text \"Hypertext Markup Language.\"\n\nMost elements take the language-related attribute codice_25 to specify text direction, such as with \"rtl\" for right-to-left text in, for example, Arabic, Persian or Hebrew.\n\nAs of version 4.0, HTML defines a set of 252 character entity references and a set of 1,114,050 numeric character references, both of which allow individual characters to be written via simple markup, rather than literally. A literal character and its markup counterpart are considered equivalent and are rendered identically.\n\nThe ability to \"escape\" characters in this way allows for the characters codice_26 and codice_27 (when written as codice_28 and codice_29, respectively) to be interpreted as character data, rather than markup. For example, a literal codice_26 normally indicates the start of a tag, and codice_27 normally indicates the start of a character entity reference or numeric character reference; writing it as codice_29 or codice_33 or codice_34 allows codice_27 to be included in the content of an element or in the value of an attribute. The double-quote character (codice_36), when not used to quote an attribute value, must also be escaped as codice_37 or codice_38 or codice_39 when it appears within the attribute value itself. Equivalently, the single-quote character (codice_40), when not used to quote an attribute value, must also be escaped as codice_41 or codice_42 (or as codice_43 in HTML5 or XHTML documents) when it appears within the attribute value itself. If document authors overlook the need to escape such characters, some browsers can be very forgiving and try to use context to guess their intent. The result is still invalid markup, which makes the document less accessible to other browsers and to other user agents that may try to parse the document for search and indexing purposes for example.\n\nEscaping also allows for characters that are not easily typed, or that are not available in the document's character encoding, to be represented within element and attribute content. For example, the acute-accented codice_44 (codice_45), a character typically found only on Western European and South American keyboards, can be written in any HTML document as the entity reference codice_46 or as the numeric references codice_47 or codice_48, using characters that are available on all keyboards and are supported in all character encodings. Unicode character encodings such as UTF-8 are compatible with all modern browsers and allow direct access to almost all the characters of the world's writing systems.\n\nHTML defines several data types for element content, such as script data and stylesheet data, and a plethora of types for attribute values, including IDs, names, URIs, numbers, units of length, languages, media descriptors, colors, character encodings, dates and times, and so on. All of these data types are specializations of character data.\n\nHTML documents are required to start with a Document Type Declaration (informally, a \"doctype\"). In browsers, the doctype helps to define the rendering mode—particularly whether to use quirks mode.\n\nThe original purpose of the doctype was to enable parsing and validation of HTML documents by SGML tools based on the Document Type Definition (DTD). The DTD to which the DOCTYPE refers contains a machine-readable grammar specifying the permitted and prohibited content for a document conforming to such a DTD. Browsers, on the other hand, do not implement HTML as an application of SGML and by consequence do not read the DTD.\n\nHTML5 does not define a DTD; therefore, in HTML5 the doctype declaration is simpler and shorter:\n<!DOCTYPE html>\nAn example of an HTML 4 doctype\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01//EN\" \"https://www.w3.org/TR/html4/strict.dtd\">\nThis declaration references the DTD for the \"strict\" version of HTML 4.01. SGML-based validators read the DTD in order to properly parse the document and to perform validation. In modern browsers, a valid doctype activates standards mode as opposed to quirks mode.\n\nIn addition, HTML 4.01 provides Transitional and Frameset DTDs, as explained below. Transitional type is the most inclusive, incorporating current tags as well as older or \"deprecated\" tags, with the Strict DTD excluding deprecated tags. Frameset has all tags necessary to make frames on a page along with the tags included in transitional type.\n\nSemantic HTML is a way of writing HTML that emphasizes the meaning of the encoded information over its presentation (look). HTML has included semantic markup from its inception, but has also included presentational markup, such as , and tags. There are also the semantically neutral span and div tags. Since the late 1990s, when Cascading Style Sheets were beginning to work in most browsers, web authors have been encouraged to avoid the use of presentational HTML markup with a view to the separation of presentation and content.\n\nIn a 2001 discussion of the Semantic Web, Tim Berners-Lee and others gave examples of ways in which intelligent software \"agents\" may one day automatically crawl the web and find, filter and correlate previously unrelated, published facts for the benefit of human users. Such agents are not commonplace even now, but some of the ideas of Web 2.0, mashups and price comparison websites may be coming close. The main difference between these web application hybrids and Berners-Lee's semantic agents lies in the fact that the current aggregation and hybridization of information is usually designed in by web developers, who already know the web locations and the API semantics of the specific data they wish to mash, compare and combine.\n\nAn important type of web agent that does crawl and read web pages automatically, without prior knowledge of what it might find, is the web crawler or search-engine spider. These software agents are dependent on the semantic clarity of web pages they find as they use various techniques and algorithms to read and index millions of web pages a day and provide web users with search facilities without which the World Wide Web's usefulness would be greatly reduced.\n\nIn order for search-engine spiders to be able to rate the significance of pieces of text they find in HTML documents, and also for those creating mashups and other hybrids as well as for more automated agents as they are developed, the semantic structures that exist in HTML need to be widely and uniformly applied to bring out the meaning of published text.\n\nPresentational markup tags are deprecated in current HTML and XHTML recommendations. The majority of presentational features from previous versions of HTML are no longer allowed as they lead to poorer accessibility, higher cost of site maintenance, and larger document sizes.\n\nGood semantic HTML also improves the accessibility of web documents (see also Web Content Accessibility Guidelines). For example, when a screen reader or audio browser can correctly ascertain the structure of a document, it will not waste the visually impaired user's time by reading out repeated or irrelevant information when it has been marked up correctly.\n\nHTML documents can be delivered by the same means as any other computer file. However, they are most often delivered either by HTTP from a web server or by email.\n\nThe World Wide Web is composed primarily of HTML documents transmitted from web servers to web browsers using the Hypertext Transfer Protocol (HTTP). However, HTTP is used to serve images, sound, and other content, in addition to HTML. To allow the web browser to know how to handle each document it receives, other information is transmitted along with the document. This meta data usually includes the MIME type (e.g., text/html or application/xhtml+xml) and the character encoding (see Character encoding in HTML).\n\nIn modern browsers, the MIME type that is sent with the HTML document may affect how the document is initially interpreted. A document sent with the XHTML MIME type is expected to be well-formed XML; syntax errors may cause the browser to fail to render it. The same document sent with the HTML MIME type might be displayed successfully, since some browsers are more lenient with HTML.\n\nThe W3C recommendations state that XHTML 1.0 documents that follow guidelines set forth in the recommendation's Appendix C may be labeled with either MIME Type. XHTML 1.1 also states that XHTML 1.1 documents should be labeled with either MIME type.\n\nMost graphical email clients allow the use of a subset of HTML (often ill-defined) to provide formatting and semantic markup not available with plain text. This may include typographic information like coloured headings, emphasized and quoted text, inline images and diagrams. Many such clients include both a GUI editor for composing HTML e-mail messages and a rendering engine for displaying them. Use of HTML in e-mail is criticized by some because of compatibility issues, because it can help disguise phishing attacks, because of accessibility issues for blind or visually impaired people, because it can confuse spam filters and because the message size is larger than plain text.\n\nThe most common filename extension for files containing HTML is .html. A common abbreviation of this is .htm, which originated because some early operating systems and file systems, such as DOS and the limitations imposed by FAT data structure, limited file extensions to three letters.\n\nAn HTML Application (HTA; file extension \".hta\") is a Microsoft Windows application that uses HTML and Dynamic HTML in a browser to provide the application's graphical interface. A regular HTML file is confined to the security model of the web browser's security, communicating only to web servers and manipulating only web page objects and site cookies. An HTA runs as a fully trusted application and therefore has more privileges, like creation/editing/removal of files and Windows Registry entries. Because they operate outside the browser's security model, HTAs cannot be executed via HTTP, but must be downloaded (just like an EXE file) and executed from local file system.\n\nSince its inception, HTML and its associated protocols gained acceptance relatively quickly. However, no clear standards existed in the early years of the language. Though its creators originally conceived of HTML as a semantic language devoid of presentation details, practical uses pushed many presentational elements and attributes into the language, driven largely by the various browser vendors. The latest standards surrounding HTML reflect efforts to overcome the sometimes chaotic development of the language and to create a rational foundation for building both meaningful and well-presented documents. To return HTML to its role as a semantic language, the W3C has developed style languages such as CSS and XSL to shoulder the burden of presentation. In conjunction, the HTML specification has slowly reined in the presentational elements.\n\nThere are two axes differentiating various variations of HTML as currently specified: SGML-based HTML versus XML-based HTML (referred to as XHTML) on one axis, and strict versus transitional (loose) versus frameset on the other axis.\n\nOne difference in the latest HTML specifications lies in the distinction between the SGML-based specification and the XML-based specification. The XML-based specification is usually called XHTML to distinguish it clearly from the more traditional definition. However, the root element name continues to be \"html\" even in the XHTML-specified HTML. The W3C intended XHTML 1.0 to be identical to HTML 4.01 except where limitations of XML over the more complex SGML require workarounds. Because XHTML and HTML are closely related, they are sometimes documented in parallel. In such circumstances, some authors conflate the two names as (X)HTML or X(HTML).\n\nLike HTML 4.01, XHTML 1.0 has three sub-specifications: strict, transitional and frameset.\n\nAside from the different opening declarations for a document, the differences between an HTML 4.01 and XHTML 1.0 document—in each of the corresponding DTDs—are largely syntactic. The underlying syntax of HTML allows many shortcuts that XHTML does not, such as elements with optional opening or closing tags, and even empty elements which must not have an end tag. By contrast, XHTML requires all elements to have an opening tag and a closing tag. XHTML, however, also introduces a new shortcut: an XHTML tag may be opened and closed within the same tag, by including a slash before the end of the tag like this: . The introduction of this shorthand, which is not used in the SGML declaration for HTML 4.01, may confuse earlier software unfamiliar with this new convention. A fix for this is to include a space before closing the tag, as such: .\n\nTo understand the subtle differences between HTML and XHTML, consider the transformation of a valid and well-formed XHTML 1.0 document that adheres to Appendix C (see below) into a valid HTML 4.01 document. To make this translation requires the following steps:\n\nThose are the main changes necessary to translate a document from XHTML 1.0 to HTML 4.01. To translate from HTML to XHTML would also require the addition of any omitted opening or closing tags. Whether coding in HTML or XHTML it may just be best to always include the optional tags within an HTML document rather than remembering which tags can be omitted.\n\nA well-formed XHTML document adheres to all the syntax requirements of XML. A valid document adheres to the content specification for XHTML, which describes the document structure.\n\nThe W3C recommends several conventions to ensure an easy migration between HTML and XHTML (see HTML Compatibility Guidelines). The following steps can be applied to XHTML 1.0 documents only:\n\nBy carefully following the W3C's compatibility guidelines, a user agent should be able to interpret the document equally as HTML or XHTML. For documents that are XHTML 1.0 and have been made compatible in this way, the W3C permits them to be served either as HTML (with a codice_52 MIME type), or as XHTML (with an codice_57 or codice_58 MIME type). When delivered as XHTML, browsers should use an XML parser, which adheres strictly to the XML specifications for parsing the document's contents.\n\nHTML 4 defined three different versions of the language: Strict, Transitional (once called Loose) and Frameset. The Strict version is intended for new documents and is considered best practice, while the Transitional and Frameset versions were developed to make it easier to transition documents that conformed to older HTML specification or didn't conform to any specification to a version of HTML 4. The Transitional and Frameset versions allow for presentational markup, which is omitted in the Strict version. Instead, cascading style sheets are encouraged to improve the presentation of HTML documents. Because XHTML 1 only defines an XML syntax for the language defined by HTML 4, the same differences apply to XHTML 1 as well.\n\nThe Transitional version allows the following parts of the vocabulary, which are not included in the Strict version:\n\nThe Frameset version includes everything in the Transitional version, as well as the codice_140 element (used instead of codice_59) and the codice_142 element.\n\nIn addition to the above transitional differences, the frameset specifications (whether XHTML 1.0 or HTML 4.01) specify a different content model, with codice_140 replacing codice_59, that contains either codice_142 elements, or optionally codice_63 with a codice_59.\n\nAs this list demonstrates, the loose versions of the specification are maintained for legacy support. However, contrary to popular misconceptions, the move to XHTML does not imply a removal of this legacy support. Rather the X in XML stands for extensible and the W3C is modularizing the entire specification and opening it up to independent extensions. The primary achievement in the move from XHTML 1.0 to XHTML 1.1 is the modularization of the entire specification. The strict version of HTML is deployed in XHTML 1.1 through a set of modular extensions to the base XHTML 1.1 specification. Likewise, someone looking for the loose (transitional) or frameset specifications will find similar extended XHTML 1.1 support (much of it is contained in the legacy or frame modules). The modularization also allows for separate features to develop on their own timetable. So for example, XHTML 1.1 will allow quicker migration to emerging XML standards such as MathML (a presentational and semantic math language based on XML) and XForms—a new highly advanced web-form technology to replace the existing HTML forms.\n\nIn summary, the HTML 4 specification primarily reined in all the various HTML implementations into a single clearly written specification based on SGML. XHTML 1.0, ported this specification, as is, to the new XML defined specification. Next, XHTML 1.1 takes advantage of the extensible nature of XML and modularizes the whole specification. XHTML 2.0 was intended to be the first step in adding new features to the specification in a standards-body-based approach.\n\nThe WHATWG considers their work as \"living standard\" HTML for what constitutes the state of the art in major browser implementations by Apple (Safari), Microsoft (Edge), Google (Chrome), Mozilla (Firefox), Opera (Opera), and others. HTML5 is specified by the HTML Working Group of the W3C following the W3C process. both specifications are similar and mostly derived from each other, i.e., the work on HTML5 started with an older WHATWG draft, and later the WHATWG \"living standard\" was based on HTML5 drafts in 2011.\n\nHTML lacks some of the features found in earlier hypertext systems, such as source tracking, fat links and others. Even some hypertext features that were in early versions of HTML have been ignored by most popular web browsers until recently, such as the link element and in-browser Web page editing.\n\nSometimes web developers or browser manufacturers remedy these shortcomings. For instance, wikis and content management systems allow surfers to edit the Web pages they visit.\n\nThere are some WYSIWYG editors (What You See Is What You Get), in which the user lays out everything as it is to appear in the HTML document using a graphical user interface (GUI), often similar to word processors. The editor renders the document rather than show the code, so authors do not require extensive knowledge of HTML.\n\nThe WYSIWYG editing model has been criticized, primarily because of the low quality of the generated code; there are voices advocating a change to the WYSIWYM model (What You See Is What You Mean).\n\nWYSIWYG editors remain a controversial topic because of their perceived flaws such as:\n\n\n"}
{"id": "46711992", "url": "https://en.wikipedia.org/wiki?curid=46711992", "title": "IP500 Alliance", "text": "IP500 Alliance\n\nThe IP500 Alliance with seat in Berlin is an international organization of manufacturers of products and systems for the building automation (e.g. Bosch, Honeywell, Siemens, OMRON, TOYOTA TSUSHO, in addition, medium-size enterprises), of system integrators as well as of operators of buildings and industrial plants. A goal of the IP500 Alliance is it to define with the IP500 communication platform a wireless (Wireless), manufacturer-neutral and safe data transfer for large buildings to develop and make available for it a “turn key module”.\n\nThe Alliance was created 2007 in Berlin from OEM´s as Interest Group and could win numerous international cooperation partners, among other things enterprises such as TÜV Rhineland, EBV or VARTA. Since 2011 the IP500® Alliance is an official “None profit” organization, on the basis of a registered association (e.V. after German law).\n\nThe IP500 platform is available as Wireless module (IP500 Solution). The IP500 solution fulfills all technical facilities for a safe, interoperable and trade-spreading networking of smart products and devices for all ranges in the building automation. \nFor the interoperability, which is needed for manufacturer and trade-spreading interaction of the components in a comprehensive building automation, the IP500 solution on basis provides more central, generally imported and accepted industrial standards and standards (e. g. IEEE 802.15.4 for the Wireless chip; IPv6/6LoWPAN; AES128, IPsec in the software network stack; as well as BACnet in the application Layer.\n\nThe IP500 solution is available today to all IP500 alliance members.\n\nThe IP500 solution is the only standard, which is particularly for safety-relevant applications, as smoke, break-down or evacuation developed and which fulfills the necessary international rules (e.g. after EN and VdS) supports. With that fact it guarantees that products and electronic devices in safety-relevant applications can communicate and be steered such as fire protection, break-down or admission via a safe, wireless network with one another. The manufacturers of such products and systems can use the IP500 solution as a before-certified communication solution.\n\nThe IP500 solution is available as platform as module with two a chip solution – those are very robust and efficient wireless chip with a high performance, but very economical Micro controller. In addition comes the extremely energy-saving and flexible software network stack (based on a very flexible and secures meshing of the Net) and custom-made network services for the entire network (incl. access points and router). The IP500 solution as module is extremely small and can easily be integrated in sensors, actuators and other smart devices.\n\nThe software (Network services) serves for the organization of the IP500 of the complete system. By flexible gateway software the network services provide for a very scalable and redundant behavior in wireless networks and for a transparent integration in existing (also proprietary) systems. At the same time they offer a set of management and reporting tools for maintenance and controlling of the IP500 of network.\n\nOperational areas of the IP500 solution in products and systems are beside trade real estate, industrial plants, sports facilities and administration buildings increasingly also Smart Home solutions.\n\nThe IP500 Alliance expects the fact that with the Internet of the things (M2M Communication) and the developments in the context of industry 4,0 the availability of highly efficient wireless nets will increase and increases sensors and actuators as basis of innovative solutions for an intelligent building automation is worldwide used.\n\nA globalization of the IP500 standard is already realized by representatives in the USA, India and Asia (Japan).\n\n"}
{"id": "4081099", "url": "https://en.wikipedia.org/wiki?curid=4081099", "title": "Identity transform", "text": "Identity transform\n\nThe identity transform is a data transformation that copies the source data into the destination data without change.\n\nThe identity transformation is considered an essential process in creating a reusable transformation library. By creating a library of variations of the base identity transformation, a variety of data transformation filters can be easily maintained. These filters can be chained together in a format similar to UNIX shell pipes.\n\nThe \"copy with recursion\" permits, changing little portions of code, produce entire new and different output, filtering or updating the input. Understanding the \"identity by recursion\" we can understand the filters.\n\nThe most frequently cited example of the identity transform (for XSLT version 1.0) is the \"copy.xsl\" transform as expressed in XSLT. This transformation uses the xsl:copy command to perform the identity transformation:\n\nThis template works by matching all attributes (@*) and other nodes (node()), copying each node matched, then applying the identity transformation to all attributes and child nodes of the context node. This recursively descends the element tree and outputs all structures in the same structure they were found in the original file, within the limitations of what information is considered significant in the XPath data model. Since node() matches text, processing instructions, root, and comments, as well as elements, all XML nodes are copied.\n\nA more explicit version of the identity transform is:\n\nThis version is equivalent to the first, but explicitly enumerates the types of XML nodes that it will copy. Both versions copy data that is unnecessary for most XML usage (e.g., comments).\n\nXSLT 3.0 specifies an on-no-match attribute of the xsl:mode instruction that allows the identity transform to be declared rather than implemented as an explicit template rule. Specifically:\n\nis essentially equivalent to the earlier template rules. See the XSLT 3.0 standard's description of shallow-copy for details.\n\nFinally, note that markup details, such as the use of CDATA sections or the order of attributes, are not necessarily preserved in the output, since this information is not part of the XPath data model. To show CDATA markup in the output, the XSLT stylesheet that contains the identity transform template (\"not\" the identity transform template itself) should make use of the attribute called cdata-section-elements.\n\ncdata-section-elements specifies a list of the names of elements whose text node children should be output using CDATA sections.\n\nFor example:\n\nXQuery can define recursive functions. The following example XQuery function copies the input directly to the output without modification.\n\nThe same function can also be achieved using a typeswitch-style transform.\nThe typeswitch transform is sometime preferable since it can easily be modified by simply adding a case statement for any element that needs special processing.\n\nTwo simple and illustrative \"copy all\" transforms.\n\nHere one important note about the XProc identity, is that it can take either one document like this example or a sequence of document as input.\n\nGenerally the identity transform is used as a base on which one can make local modifications.\n\nThe identity transformation can be modified to copy everything from an input tree to an output tree except a given node. For example the following will copy everything from the input to the output except the social security number:\n\nTo call this one would add:\n\n\n"}
{"id": "33688667", "url": "https://en.wikipedia.org/wiki?curid=33688667", "title": "Joint Center for Artificial Photosynthesis", "text": "Joint Center for Artificial Photosynthesis\n\nThe Joint Center for Artificial Photosynthesis (JCAP), founded in 2010, is a (DOE) Energy Innovation Hub whose primary mission is to find a cost-effective method to produce fuels using only sunlight, water, and carbon-dioxide. The program has a budget of $122M over five years, subject to Congressional appropriation. \nThe Director of JCAP is Professor Harry Atwater of Caltech and its two main centers are located at the California Institute of Technology and the Lawrence Berkeley National Laboratory. In addition, JCAP has partners from Stanford University, the University of California at Berkeley, University of California at Santa Barbara, University of California at Irvine, the University of California at San Diego, and Stanford Linear Accelerator. In addition, JCAP also serves as a hub for other solar fuels research teams across the United States, including 20 DOE Energy Frontier Research Center. \n\nIn Obama's 2011 State of the Union address, he mentioned the Joint Center for Artificial Photosynthesis. Specifically, he said, \"We're issuing a challenge. We're telling America's scientists and engineers that if they assemble teams of the best minds in their fields, and focus on the hardest problems in clean energy, we'll fund the Apollo projects of our time. At the California Institute of Technology, they're developing a way to turn sunlight and water into fuel for our cars\". \n\n"}
{"id": "42812539", "url": "https://en.wikipedia.org/wiki?curid=42812539", "title": "José Eisenberg", "text": "José Eisenberg\n\nJosé Eisenberg is an Italian entrepreneur, best known for being the founder and owner of Eisenberg Paris. After several years working in the fashion industry, and creating two companies in the IT industry, Eisenberg founded the José Eisenberg brand in 2000, which was renamed Eisenberg in 2001.\n\nEisenberg was born on 20 August 1945 in a family of Textile Industrialists in Bucarest, Romania, where all the family properties were then confiscated by the Communists, due to the new political regime. He spent his childhood in Romania during the communist post-war era. Eisenberg’s family was strongly influenced by French culture; his father had studied at La Sorbonne in Paris, and his uncle was a French citizen and had fought during The Second World War for France as a Resistant in the Maquis, and was then shot in Paris by the Nazis on the Day of the Liberation of France. At 13, Eisenberg had to leave school and worked as a handyman and newspaper deliverer, among other jobs. He also worked in small art workshops, like at the time of the Renaissance. He passed his Baccalaureate Certificate later on, as an independent candidate.\n\nAt age 21 Eisenberg began working in the fashion industry in Florence, Italy, where he created his design studio as well as a fashion factory. He designed and produced collections for various fashion brands. In 1972, Eisenberg was granted the title of Citizen of Honor for his contribution in industrializing the region of Basilicata in Italy.\n\nIn 1974 went to the US to work in technology. He started working in the emerging Artificial Intelligence field in the US, while continuing to work in the fashion industry. He founded the company \"JE Contrex\" in Boston, where he hired scientists from MIT. Eisenberg was successful in developing functioning technology into artificial intelligence; however, the computer's memory limitation at the time did not allow him to progress. In 1976 he launched a second company, Eisenberg Data Systems (E.D.S.), which designed the first personal computer and the first removable keyboard.\n\nIn 1985 Eisenberg started to work on skin care, creating his own beauty products brand reflecting his idea of a timeless Beauty. A thirteen-year R&D, followed by two years of clinical and medical tests led to what he called the Trio-Molecular Formula. This development became the cornerstone of the Eisenberg brand and is present in all the skincare products of the company.\n\nSubsequent to the discovery of the Trio-Molecular Formula, Eisenberg created the José Eisenberg brand in 2000 and one year later, he changed the name of the brand to Eisenberg. According to Eisenberg, he conceives and creates all the products for his brand. Along with skin care products, Eisenberg has also created a line of perfumes and make-up collections.\n\nEisenberg has three children, two daughters and a son. His son Edmond manages the strategy at Eisenberg Paris.\n\n"}
{"id": "31439191", "url": "https://en.wikipedia.org/wiki?curid=31439191", "title": "Landysh", "text": "Landysh\n\nLandysh (, 'Lily of the Valley', known as Suzuran in Japan) is a floating facility for processing contaminated water produced when decommissioning nuclear submarines. It was built in Russia with funds from Japan as part of an agreement on nuclear arms disposal, but has not left the wharf. Japan requested that Russia send \"Landysh\" to help in the aftermath of the Fukushima I nuclear accidents.\n\nIn 1972 the Convention on the Prevention of Marine Pollution by Dumping of Wastes and Other Matter was held and in 1975 the USSR ratified the agreement to limit the dumping of high-level radioactive wastes in the oceans. In 1983 many Convention members signed a voluntary moratorium on all dumping of radioactive wastes at sea, but the USSR did not sign and continued to dispose of low-level radioactive reactor coolant water from its nuclear submarines. Leaks and intentional releases of radioactive materials from Russian facilities in the Far East prompted Japan to offer financial aid for Russia to build facilities to treat low-level radioactive water in 1994. By 1996 a design for a floating processing facility was accepted and contracts issued to the Tomen Corporation (Japan), Babcock & Wilcox (USA) and the Amurskiy shipyard (Russia). \"Landysh\" was built at the Amurskiy shipyard in Komsomolsk-on-Amur, completed in 1998 and commissioned in 2000. \"Landysh\" remained at Zvezda shipyard in Bolshoy Kamen until 2011. As of May 9, 2011 discussions between Rosatom, the Russian nuclear agency, and Japan concerning the dispatch of \"Landysh\" to Japan were still ongoing. Japanese reluctance to accept Russian assistance may be linked to the Kuril Islands dispute between Russia and Japan.\n\n\"Landysh\" is a barge and must be towed from one location to another. It is long, wide, and has a double hull; its waste-treatment facility has thick concrete walls to prevent spills. It displaces 3,900 tonnes and carries a crew of 46.\n\nThere are conflicting reports about what level of radioactivity can be in the water processed by \"Landysh\"; some sources state that it can only process low-level water whilst other sources state it can handle medium- and low-level water; all sources agree that it can process up 7,000 m per year. There have been questions raised about the effectiveness of the decontamination process, especially regarding the removal of caesium-137. \"Landysh\" uses a combination of filtration, ion exchange and reverse osmosis to remove radioactive material from water. After collecting and concentrating the radioactive materials, they are mixed with cement and placed in 200-litre barrels for further radioactive waste management.\n\n"}
{"id": "41003302", "url": "https://en.wikipedia.org/wiki?curid=41003302", "title": "List of fuel cell manufacturers", "text": "List of fuel cell manufacturers\n\nThis is a partial list of companies currently producing commercially available fuel cell systems for use in residential, commercial, or industrial settings. Fuel cell systems from these manufacturers are currently being used to generate AC or DC electricity, heat, water, or any combination of the three.\n"}
{"id": "22385478", "url": "https://en.wikipedia.org/wiki?curid=22385478", "title": "Lists of spacewalks and moonwalks", "text": "Lists of spacewalks and moonwalks\n\nLists of spacewalks and moonwalks include:\n\nBy date:\n\nBy space station:\n\nOther:\n\n"}
{"id": "41722640", "url": "https://en.wikipedia.org/wiki?curid=41722640", "title": "Low carbon leakage", "text": "Low carbon leakage\n\nLow carbon leakage refers to the phenomenon of a country or a region losing its low carbon industries to another country or region. The underlying low carbon leakage trend can also be identified by looking into clean energy patent distribution around the World. The threat of low carbon leakage to the European Union has been repeatedly expressed by a number of European Politicians such as climate Commissioner Connie Hedegaard, UK Energy Secretary Edward Davey and others.\n\nThe low carbon leakage increases its relevance for the industrial competitiveness as the low carbon economy grows and has reached in 2013 $4 trillion and continues to grow at 4% per year. Not taking relevant part in this growth opportunity is also considered as low carbon leakage.\n\nIn October 2014 E&Y published a report \"European Low Carbon Industries. A Health Check.\" specifically examining the state of the European low carbon sectors. The report lists a wide variety of cases in which \"low carbon leakage\" occurs or could occur.\n\nLow carbon leakage could lead to a significant loss of competitiveness for Europe. According to former German federal minister Trittin: “In reality there is no carbon leakage. The danger of low carbon leakage is much more real.\" I typical example is the solar panel manufacturing that has developed rapidly in China and shrunk in Europe.\n"}
{"id": "18784230", "url": "https://en.wikipedia.org/wiki?curid=18784230", "title": "Mapscape BV", "text": "Mapscape BV\n\nMapscape BV is a Netherlands-based independent business-to-business digital maps service provider. The company was founded in 2007 by Henk Eemers and Ralf Stollenwerk. It is active in the navigation and automotive industry. Mapscape is involved in content aggregation, compilation and testing of digital maps for navigation purposes. The maps are targeted for use in the automotive industry in e.g. online and offline navigation and location-based services.\n\nThe company's automotive customers include BMW and Volkswagen, as well as navigation system suppliers, such as Continental, TomTom, Harman International, Bosch, and Elektrobit. The company uses raw navigation data from companies such as Navteq, TeleAtlas, NavInfo, and MapMyIndia which it compiles into the proprietary formats used by various navigation systems.\n\nFurthermore, Mapscape is involved in testing and certifying a new proposed industry standard automotive map format, called the Navigation Data Standard (NDS) initiative.\n\nIn January 2011, Mapscape was acquired by and is now a wholly owned subsidiary of Chinese digital map supplier NavInfo.\n\n\n"}
{"id": "1708811", "url": "https://en.wikipedia.org/wiki?curid=1708811", "title": "Matt Nagle", "text": "Matt Nagle\n\nMatthew Nagle (October 16, 1979 – July 24, 2007) was the first person to use a brain-computer interface to restore functionality lost due to paralysis. He was a C3 tetraplegic, paralyzed from the neck down after being stabbed.\n\nNagle attended Weymouth High School (Class of 1998). He was an exceptional athlete and a star football player. In 2001, he sustained a stabbing injury while leaving the town’s annual fireworks show near Wessagussett Beach on July 3. He was stabbed and his spinal cord severed when he stepped in to help a friend.\n\nNagle died on July 24, 2007 in Stoughton, Massachusetts from sepsis.\n\nNagle agreed to participate in a clinical trial involving the BrainGate Neural Interface System (developed by Cyberkinetics) out of a desire to again be healthy and lead a normal life, and in hopes that modern medical discoveries could help him. He also hoped that his participation in this Clinical Trial would help improve the lives of people who, like him, suffered injuries or diseases that cause severe motor disabilities.\n\nThe device was implanted on June 22, 2004 by neurosurgeon Gerhard Friehs. A 96-electrode \"Utah Array\" was placed on the surface of his brain over the region of motor cortex that controlled his dominant left hand and arm. A link connected it to the outside of his skull, where it could be connected to a computer. The computer was then trained to recognize Nagle's thought patterns and associate them with movements he was trying to achieve.\n\nWhile he was implanted, Matt could control a computer \"mouse\" cursor, using it then to press buttons that can control TV, check e-mail, and do basically everything that can be done by pressing buttons. He could draw (although the cursor control is not precise) on the screen. He could also send commands to an external prosthetic hand (close and open). The results of the study are published in the journal \"Nature\". Per Food and Drug Administration (FDA) regulations and the study protocol, the BrainGate device was removed from him after approximately one year.\n\nOn June 5, 2008, a grand jury in Norfolk County, Massachusetts indicted on a second-degree murder charge Nagle's attacker Nicholas Cirignano. Cirignano had in 2005 been convicted of Nagle's stabbing and sentenced to nine years' imprisonment. District Attorney William Keating used the state medical examiner's ruling that the stabbing had caused Nagle's eventual death as grounds to seek the murder charge.\n\nOn April 10, 2009, a Superior Court Judge ruled that Cirignano could not be tried for murder, as the jury's verdict from the original assault case had already determined that one of the key components to a murder charge, malice, was negated by excessive force in self-defence. However, the lesser charge of manslaughter could still, in theory, be applied.\n"}
{"id": "16239327", "url": "https://en.wikipedia.org/wiki?curid=16239327", "title": "Mobile Studio", "text": "Mobile Studio\n\nThe National Science Foundation supported Mobile Studio Project, or Mobile Studio, is developing pedagogy and hardware/software which, when connected to a PC (via USB), provides functionality similar to that of laboratory equipment (scope, function generator, power supplies, DMM, etc.) typically associated with an instrumented studio classroom. The Mobile Studio IOBoard is a small, inexpensive hardware platform for use in a home, classroom or remote environment. When coupled with the Mobile Studio Desktop software, the system duplicates a large amount of the hardware often used to teach Electrical Engineering, Computer Engineering, Control Systems, and Physics courses; among others. With the support of several technology companies (ADI, HP, and Maxim) and the National Science Foundation, the Mobile Studio Project is now being utilized to enhance science, math, engineering and technology education around the world. The project's goal is to enable hands-on exploration of science and engineering principles, devices, and systems that have historically been restricted to expensive laboratory facilities.\n\nIn 1999, Rensselaer Polytechnic Institute professor Don Millard started thinking about a way to enable students to perform experiments whenever and wherever they desire — experiments that use an oscilloscope, function generator, digital control and some form of power supply. The project started by looking at commercially available solutions; which were found to be prohibitively expensive since the desire was to keep the cost of the solution similar to the price of an engineering textbook.\n\nThe Academy of Electronic Media Mobile Studio Project's inspiration was drawn from a generation of engineering students that has virtually no tinkering background. Instead of taking apart devices and building things with erector sets, students now manipulate computer software. Additionally, the level of integration is so sophisticated in today’s electronics that even if students did crack them open, it’s not clear how much they would garner from it. Simple circuit boards that stimulated the imagination with their discrete components and space for soldering and tinkering - have since given way to multilayered boards with complex ICs and circuitry too small to see.\n\nThe hardware component of the system is a small printed circuit board, referred to as the IOBoard, which was developed by Jason Coutermarsh (as a part of his Rensselaer degree programs) and Dr. Don Lewis Millard. The IOBoard is populated with the components required to implement an oscilloscope, function generator, spectrum analyzer, voltmeter, and digital input/output control. The hardware connects to a PC via USB, and is powered by the user's PC, eliminating the need for a bulky AC transformer.\n\nThe Mobile Studio Desktop software, designed by Jason Coutermarsh, provides the user with \"benchtop equivalent\" displays that mimic their physical counterparts. In addition to providing standard instrumentation options, the software takes advantage of the processing power of the personal computer, giving the user access to features typically found on high-end equipment, along with the ability to easily save data and screen images.\n\nThe Mobile Studio IOBoard is easily expandable using an on-board daughterboard connector. Nearly all the IOBoard resources can be accessed by a daughterboard, allowing a user to enhance current features or add completely new options. The Mobile Studio Desktop application software is also easily expandable by way of a \"Plug-in\" system. The software automatically finds and loads both new hardware drivers and new features that can be installed at any time after the main application. This ensures that both the hardware and software are never out of date.\n\n\n\n\n"}
{"id": "198570", "url": "https://en.wikipedia.org/wiki?curid=198570", "title": "Oil well", "text": "Oil well\n\nAn oil well is a boring in the Earth that is designed to bring petroleum oil hydrocarbons to the surface. Usually some natural gas is released along with the oil. A well that is designed to produce only gas may be termed a gas well.\n\nThe earliest known oil wells were drilled in China in 347 CE. These wells had depths of up to about and were drilled using bits attached to bamboo poles. The oil was burned to evaporate brine and produce salt. By the 10th century, extensive bamboo pipelines connected oil wells with salt springs. The ancient records of China and Japan are said to contain many allusions to the use of natural gas for lighting and heating. Petroleum was known as \"Burning water\" in Japan in the 7th century.\n\nAccording to Kasem Ajram, petroleum was distilled by the Persian alchemist Muhammad ibn Zakarīya Rāzi (Rhazes) in the 9th century, producing chemicals such as kerosene in the alembic (\"al-ambiq\"), and which was mainly used for kerosene lamps. Arab and Persian chemists also distilled crude oil in order to produce flammable products for military purposes. Through Islamic Spain, distillation became available in Western Europe by the 12th century.\n\nSome sources claim that from the 9th century, oil fields were exploited in the area around modern Baku, Azerbaijan, to produce naphtha for the petroleum industry. These places were described by Marco Polo in the 13th century, who described the output of those oil wells as hundreds of shiploads. When Marco Polo in 1264 visited Baku, on the shores of the Caspian Sea, he saw oil being collected from seeps. He wrote that \"on the confines toward Geirgine there is a fountain from which oil springs in great abundance, in as much as a hundred shiploads might be taken from it at one time.\"\n\nIgnacy Łukasiewicz, a Polish pharmacist and petroleum industry pioneer built one of the world's first modern oil wells in 1854 in Polish village Bóbrka, Krosno County who in 1856 built one of the world's first oil refineries.\n\nIn North America, the first commercial oil well entered operation in Oil Springs, Ontario in 1858, while the first offshore oil well was drilled in 1896 at the Summerland Oil Field on the California Coast.\n\nThe earliest oil wells in modern times were drilled percussively, by repeatedly raising and dropping a cable tool into the earth. In the 20th century, cable tools were largely replaced with rotary drilling, which could drill boreholes to much greater depths and in less time. The record-depth Kola Borehole used non-rotary mud motor drilling to achieve a depth of over .\n\nUntil the 1970s, most oil wells were vertical, although lithological and mechanical imperfections cause most wells to deviate at least slightly from true vertical. However, modern directional drilling technologies allow for strongly deviated wells which can, given sufficient depth and with the proper tools, actually become horizontal. This is of great value as the reservoir rocks which contain hydrocarbons are usually horizontal or nearly horizontal; a horizontal wellbore placed in a production zone has more surface area in the production zone than a vertical well, resulting in a higher production rate. The use of deviated and horizontal drilling has also made it possible to reach reservoirs several kilometers or miles away from the drilling location (extended reach drilling), allowing for the production of hydrocarbons located below locations that are either difficult to place a drilling rig on, environmentally sensitive, or populated.\n\nBefore a well is drilled, a geologic target is identified by a geologist or geophysicist to meet the objectives of the well. \nThe target (the end point of the well) will be matched with a surface location (the starting point of the well), and a trajectory between the two will be designed.\n\nWhen the well path is identified, a team of geoscientists and engineers will develop a set of presumed properties of the subsurface that will be drilled through to reach the target. These properties include pore pressure, fracture gradient, wellbore stability, porosity, permeability, lithology, faults, and clay content. This set of assumptions is used by a well engineering team to perform the casing design and completion design for the well, and then detailed planning, where, for example, the drill bits are selected, a BHA is designed, the drilling fluid is selected, and step-by-step procedures are written to provide instruction for executing the well in a safe and cost-efficient manner.\n\nThe well is created by drilling a hole 12 cm to 1 meter (5 in to 40 in) in diameter into the earth with a drilling rig that rotates a drill string with a bit attached. After the hole is drilled, sections of steel pipe (casing), slightly smaller in diameter than the borehole, are placed in the hole. Cement may be placed between the outside of the casing and the borehole known as the annulus. The casing provides structural integrity to the newly drilled wellbore, in addition to isolating potentially dangerous high pressure zones from each other and from the surface.\n\nWith these zones safely isolated and the formation protected by the casing, the well can be drilled deeper (into potentially more-unstable and violent formations) with a smaller bit, and also cased with a smaller size casing. Modern wells often have two to five sets of subsequently smaller hole sizes drilled inside one another, each cemented with casing.\n\n\nThis process is all facilitated by a drilling rig which contains all necessary equipment to circulate the drilling fluid, hoist and turn the pipe, control downhole, remove cuttings from the drilling fluid, and generate on-site power for these operations.\n\nAfter drilling and casing the well, it must be 'completed'. Completion is the process in which the well is enabled to produce oil or gas.\n\nIn a cased-hole completion, small holes called perforations are made in the portion of the casing which passed through the production zone, to provide a path for the oil to flow from the surrounding rock into the production tubing. In open hole completion, often 'sand screens' or a 'gravel pack' is installed in the last drilled, uncased reservoir section. These maintain structural integrity of the wellbore in the absence of casing, while still allowing flow from the reservoir into the wellbore. Screens also control the migration of formation sands into production tubulars and surface equipment, which can cause washouts and other problems, particularly from unconsolidated sand formations of offshore fields.\n\nAfter a flow path is made, acids and fracturing fluids may be pumped into the well to fracture, clean, or otherwise prepare and stimulate the reservoir rock to optimally produce hydrocarbons into the wellbore. Finally, the area above the reservoir section of the well is packed off inside the casing, and connected to the surface via a smaller diameter pipe called tubing. This arrangement provides a redundant barrier to leaks of hydrocarbons as well as allowing damaged sections to be replaced. Also, the smaller cross-sectional area of the tubing produces reservoir fluids at an increased velocity in order to minimize liquid fallback that would create additional back pressure, and shields the casing from corrosive well fluids.\n\nIn many wells, the natural pressure of the subsurface reservoir is high enough for the oil or gas to flow to the surface. However, this is not always the case, especially in depleted fields where the pressures have been lowered by other producing wells, or in low permeability oil reservoirs. Installing a smaller diameter tubing may be enough to help the production, but artificial lift methods may also be needed. Common solutions include downhole pumps, gas lift, or surface pump jacks. Many new systems in the last ten years have been introduced for well completion. Multiple packer systems with frac ports or port collars in an all in one system have cut completion costs and improved production, especially in the case of horizontal wells. These new systems allow casings to run into the lateral zone with proper packer/frac port placement for optimal hydrocarbon recovery.\n\nThe production stage is the most important stage of a well's life; when the oil and gas are produced. By this time, the oil rigs and workover rigs used to drill and complete the well have moved off the wellbore, and the top is usually outfitted with a collection of valves called a Christmas tree or production tree. These valves regulate pressures, control flows, and allow access to the wellbore in case further completion work is needed. From the outlet valve of the production tree, the flow can be connected to a distribution network of pipelines and tanks to supply the product to refineries, natural gas compressor stations, or oil export terminals.\n\nAs long as the pressure in the reservoir remains high enough, the production tree is all that is required to produce the well. If the pressure depletes and it is considered economically viable, an artificial lift method mentioned in the completions section can be employed.\n\nWorkovers are often necessary in older wells, which may need smaller diameter tubing, scale or paraffin removal, acid matrix jobs, or completing new zones of interest in a shallower reservoir. Such remedial work can be performed using workover rigs – also known as \"pulling units\", \"completion rigs\" or \"service rigs\" – to pull and replace tubing, or by the use of well intervention techniques utilizing coiled tubing. Depending on the type of lift system and wellhead a rod rig or flushby can be used to change a pump without pulling the tubing.\n\nEnhanced recovery methods such as water flooding, steam flooding, or CO flooding may be used to increase reservoir pressure and provide a \"sweep\" effect to push hydrocarbons out of the reservoir. Such methods require the use of injection wells (often chosen from old production wells in a carefully determined pattern), and are used when facing problems with reservoir pressure depletion, high oil viscosity, or can even be employed early in a field's life. In certain cases – depending on the reservoir's geomechanics – reservoir engineers may determine that ultimate recoverable oil may be increased by applying a waterflooding strategy early in the field's development rather than later. Such enhanced recovery techniques are often called \"tertiary recovery\".\n\nA well is said to reach an \"economic limit\" when its most efficient production rate does not cover the operating expenses, including taxes.\n\nThe economic limit for oil and gas wells can be expressed using these formulae:\n\nOil fields:\n\n<math>\n"}
{"id": "34229379", "url": "https://en.wikipedia.org/wiki?curid=34229379", "title": "Oracle Big Data Appliance", "text": "Oracle Big Data Appliance\n\nThe Oracle Big Data Appliance consists of hardware and software from Oracle Corporation sold as a computer appliance. It was announced in 2011, promoted for consolidating and loading unstructured data into Oracle Database software.\n\nOracle announced the Oracle Big Data Appliance on October 3, 2011 at Oracle OpenWorld.\nIt was similar to the Oracle Exadata Database Machine and announced with the Oracle Exalytics Business Intelligence Machine.\n\nThe original hardware components of the appliance consisted of a full rack configuration with 864GB of main memory and 432 TB of storage. A full rack consists of 18 servers nodes each of which had two 6-core Intel processors, 48 GB memory per node (upgradable to 96 GB or 144 GB), 12 x 2TB disks per node, InfiniBand Networking and 10 GbE connectivity.\n\nThe product includes an open-source distribution of Apache Hadoop. Support from Cloudera was announced in January 2012.\n\nThe Oracle NoSQL Database, Oracle Data Integrator with an adapter for Hadoop Oracle Loader for Hadoop, an open source distribution of R, Oracle Linux, and Oracle Java Hotspot Virtual Machine were also mentioned in the announcement..\n\n"}
{"id": "1601998", "url": "https://en.wikipedia.org/wiki?curid=1601998", "title": "Organic field-effect transistor", "text": "Organic field-effect transistor\n\nAn organic field-effect transistor (OFET) is a field-effect transistor using an organic semiconductor in its channel. OFETs can be prepared either by vacuum evaporation of small molecules, by solution-casting of polymers or small molecules, or by mechanical transfer of a peeled single-crystalline organic layer onto a substrate. These devices have been developed to realize low-cost, large-area electronic products and biodegradable electronics. OFETs have been fabricated with various device geometries. The most commonly used device geometry is bottom gate with top drain and source electrodes, because this geometry is similar to the thin-film silicon transistor (TFT) using thermally grown SiO as gate dielectric. Organic polymers, such as poly(methyl-methacrylate) (PMMA), can also be used as dielectric.\n\nIn May 2007, Sony reported the first full-color, video-rate, flexible, all plastic display, in which both the thin-film transistors and the light-emitting pixels were made of organic materials.\n\nThe field-effect transistor (FET) was first proposed by J.E. Lilienfeld, who received a patent for his idea in 1930. He proposed that a field-effect transistor behaves as a capacitor with a conducting channel between a source and a drain electrode. Applied voltage on the gate electrode controls the amount of charge carriers flowing through the system.\nThe first field-effect transistor was designed and prepared in 1960 by Kahng and Atalla using a metal–oxide–semiconductor (MOSFET). However, rising costs of materials and manufacturing, as well as public interest in more environmentally friendly electronics materials have supported development of organic based electronics in more recent years. In 1987, Koezuka and co-workers reported the first organic field-effect transistor based on a polymer of thiophene molecules. The thiophene polymer is a type of conjugated polymer that is able to conduct charge, eliminating the need to use expensive metal oxide semiconductors. Additionally, other conjugated polymers have been shown to have semiconducting properties. OFET design has also improved in the past few decades. Many OFETs are now designed based on the thin-film transistor (TFT) model, which allows the devices to use less conductive materials in their design. Improvement on these models in the past few years have been made to field-effect mobility and on–off current ratios.\n\nOne common feature of OFET materials is the inclusion of an aromatic or otherwise conjugated π-electron system, facilitating the delocalization of orbital wavefunctions. Electron withdrawing groups or donating groups can be attached that facilitate hole or electron transport.\n\nOFETs employing many aromatic and conjugated materials as the active semiconducting layer have been reported, including small molecules such as rubrene, tetracene, pentacene, diindenoperylene, perylenediimides, tetracyanoquinodimethane (TCNQ), and polymers such as polythiophenes (especially poly(3-hexylthiophene) (P3HT)), polyfluorene, polydiacetylene, poly(2,5-thienylene vinylene), poly(p-phenylene vinylene) (PPV).\n\nThe field is very active, with newly synthesized and tested compounds reported weekly in prominent research journals. Many review articles exist documenting the development of these materials.\n\nRubrene-based OFETs show the highest carrier mobility 20–40 cm/(V·s). Another popular OFET material is pentacene, which has been used since the 1980s, but with mobilities 10 to 100 times lower (depending on the substrate) than rubrene. The major problem with pentacene, as well as many other organic conductors, is its rapid oxidation in air to form pentacene-quinone. However if the pentacene is preoxidized, and the thus formed pentacene-quinone is used as the gate insulator, then the mobility can approach the rubrene values. This pentacene oxidation technique is akin to the silicon oxidation used in the silicon electronics.\n\nPolycrystalline tetrathiafulvalene and its analogues result in mobilities in the range 0.1–1.4 cm/(V·s). However, the mobility exceeds 10 cm/(V·s) in solution-grown or vapor-transport-grown single crystalline hexamethylene-tetrathiafulvalene (HMTTF). The ON/OFF voltage is different for devices grown by those two techniques, presumably due to the higher processing temperatures using in the vapor transport grows.\n\nAll the above-mentioned devices are based on p-type conductivity. N-type OFETs are yet poorly developed. They are usually based on perylenediimides or fullerenes or their derivatives, and show electron mobilities below 2 cm/(V·s).\n\nThree essential components of field-effect transistors are the source, the drain and the gate. Field-effect transistors usually operate as a capacitor. They are composed of two plates. One plate works as a conducting channel between two ohmic contacts, which are called the source and the drain contacts. The other plate works to control the charge induced into the channel, and it is called the gate. The direction of the movement of the carriers in the channel is from the source to the drain. Hence the relationship between these three components is that the gate controls the carrier movement from the source to the drain.\n\nWhen this capacitor concept is applied to the device design, various devices can be built up based on the difference in the controller – i.e. the gate. This can be the gate material, the location of the gate with respect to the channel, how the gate is isolated from the channel, and what type of carrier is induced by the gate voltage into channel (such as electrons in an n-channel device, holes in a p-channel device, and both electrons and holes in a double injection device).\nClassified by the properties of the carrier, three types of FETs are shown schematically in Figure 1. They are MOSFET (metal–oxide–semiconductor field-effect transistor), MESFET (metal–semiconductor field-effect transistor) and TFT (thin-film transistor).\n\nThe most prominent and widely used FET in modern microelectronics is the MOSFET. There are different kinds in this category, such as MISFET (metal–insulator–semiconductor field-effect transistor), and IGFET (insulated-gate FET). A schematic of a MISFET is shown in Figure 1a. The source and the drain are connected by a semiconductor and the gate is separated from the channel by a layer of insulator. If there is no bias (potential difference) applied on the gate, the Band bending is induced due to the energy difference of metal conducting band and the semiconductor Fermi level. Therefore a higher concentration of holes is formed on the interface of the semiconductor and the insulator. When an enough positive bias is applied on the gate contact, the bended band becomes flat. If a larger positive bias is applied, the band bending in the opposite direction occurs and the region close to the insulator-semiconductor interface becomes depleted of holes. Then the depleted region is formed. At an even larger positive bias, the band bending becomes so large that the Fermi level at the interface of the semiconductor and the insulator becomes closer to the bottom of the conduction band than to the top of the valence band, therefore, it forms an inversion layer of electrons, providing the conducting channel. Finally, it turns the device on.\n\nThe second type of device is described in Fig.1b. The only difference of this one from the MISFET is that the n-type source and drain are connected by an n-type region. In this case, the depletion region extends all over the n-type channel at zero gate voltage in a normally “off” device (it is similar to the larger positive bias in MISFET case). In the normally “on” device, a portion of the channel is not depleted, and thus leads to passage of a current at zero gate voltage.\n\nThe concept of TFT was first proposed by Paul Weimer in 1962. This is illustrated in Figure 1c. Here the source and drain electrodes are directly deposited onto the conducting channel (a thin layer of semiconductor) then a thin film of insulator is deposited between the semiconductor and the metal gate contact. This structure suggests that there is no depletion region to separate the device from the substrate. If there is zero bias, the electrons are expelled from the surface due to the Fermi-level energy difference of the semiconductor and the metal. This leads to band bending of semiconductor. In this case, there is no carrier movement between the source and drain. When the positive charge is applied, the accumulation of electrons on the interface leads to the bending of the semiconductor in an opposite way and leads to the lowering of the conduction band with regards to the Fermi-level of the semiconductor. Then a highly conductive channel forms at the interface (shown in Figure 2).\n\nOFETs adopt the architecture of TFT. With the development of the conducting polymer, the semiconducting properties of small conjugated molecules have been recognized. The interest in OFETs has grown enormously in the past ten years. The reasons for this surge of interest are manifold. The performance of OFETs, which can compete with that of amorphous silicon (a-Si) TFTs with field-effect mobilities of 0.5–1 cm V s and ON/OFF current ratios (which indicate the ability of the device to shut down) of 10–10, has improved significantly. Currently, thin-film OFET mobility values of 5 cm V s in the case of vacuum-deposited small molecules \nand 0.6 cm V s for solution-processed polymers have been reported. As a result, there is now a greater industrial interest in using OFETs for applications that are currently incompatible with the use of a-Si or other inorganic transistor technologies. One of their main technological attractions is that all the layers of an OFET can be deposited and patterned at room temperature by a combination of low-cost solution-processing and direct-write printing, which makes them ideally suited for realization of low-cost, large-area electronic functions on flexible substrates.\n\nThermally oxidized silicon is a traditional substrate for OFETs where the silicon dioxide serves as the gate insulator. The active FET layer is usually deposited onto this substrate using either (i) thermal evaporation, (ii) coating from organic solution, or (iii) electrostatic lamination. The first two techniques result in polycrystalline active layers; they are much easier to produce, but result in relatively poor transistor performance. Numerous variations of the solution coating technique (ii) are known, including dip-coating, spin-coating, inkjet printing and screen printing. The electrostatic lamination technique is based on manual peeling of a thin layer off a single organic crystal; it results in a superior single-crystalline active layer, yet it is more tedious. The thickness of the gate oxide and the active layer is below one micrometer.\n\nThe carrier transport in OFET is specific for two-dimensional (2D) carrier propagation through the device. Various experimental techniques were used for this study, such as Haynes - Shockley experiment on the transit times of injected carriers, time-of-flight (TOF) experiment for the determination of carrier mobility, pressure-wave propagation experiment for probing electric-field distribution in insulators, organic monolayer experiment for probing orientational dipolar changes, optical time-resolved second harmonic generation (TRM-SHG), etc. Whereas carriers propagate through polycrystalline OFETs in a diffusion-like (trap-limited) manner, they move through the conduction band in the best single-crystalline OFETs.\n\nThe most important parameter of OFET carrier transport is carrier mobility. Its evolution over the years of OFET research is shown in the graph for polycrystalline and single crystalline OFETs. The horizontal lines indicate the comparison guides to the main OFET competitors – amorphous (a-Si) and polycrystalline silicon. The graph reveals that the mobility in polycrystalline OFETs is comparable to that of a-Si whereas mobility in rubrene-based OFETs (20–40 cm/(V·s)) approaches that of best poly-silicon devices.\n\nBecause an electric current flows through such a transistor, it can be used as a light-emitting device, thus integrating current modulation and light emission. In 2003, a German group reported the first organic light-emitting field-effect transistor (OLET). The device structure comprises interdigitated gold source- and drain electrodes and a polycrystalline tetracene thin film. Both, positive charges (holes) as well as negative charges (electrons) are injected from the gold contacts into this layer leading to electroluminescence from the tetracene.\n\n"}
{"id": "3115081", "url": "https://en.wikipedia.org/wiki?curid=3115081", "title": "Phase-change incubator", "text": "Phase-change incubator\n\nThe phase-change incubator is a low-cost, low-maintenance incubator to help test for microorganisms in water supplies. It uses small balls containing a chemical compound that, when heated and then kept insulated, will stay at 37°C (approx. 99°F) for 24 hours. \n\nThis allows cultures to be tested without the need for a laboratory or an expensive portable incubator. Thus it is particularly useful for poor or remote communities.\n\nThe phase-change incubator was developed in the late 1990s by Amy Smith, when she was a graduate student at MIT. Smith has also started a non-profit organization called \"A Drop in the Bucket\" to distribute the incubators and train people to use them for testing water quality.\n\nEmbrace, an organization that came out of Stanford University, is applying a similar concept to design low-cost incubators for premature and low birth weight babies in developing countries.\n\n\n"}
{"id": "55202108", "url": "https://en.wikipedia.org/wiki?curid=55202108", "title": "Playnux", "text": "Playnux\n\nPlaynuxis a gaming hardware company founded in 2014. The first product they released was the Playnux PX a video game console that works with the Windows 10 operating system intended for all existing PC games in Steam, Uplay, Origin, Windows Store, Battle.net and other platforms for PC.\n\nAll systems can be used as development kits, allowing any Playnux owner to also be a developer, without the need for licensing fees.\n\nThe Playnux products competing against the Xbox One, PlayStation 4 and Alienware Alpha, which are game consoles with similar characteristics.\n\nDesktop:\n\n\n"}
{"id": "35788722", "url": "https://en.wikipedia.org/wiki?curid=35788722", "title": "Rapier loom", "text": "Rapier loom\n\nA rapier loom is a shuttleless weaving loom in which the filling yarn is carried through the shed of warp yarns to the other side of the loom by finger-like carriers called rapiers.\n\nA stationary package of yarn is used to supply the weft yarns in the rapier machine. One end of a rapier, a rod or steel tape, carries the weft yarn. The other end of the rapier is connected to the control system. The rapier moves across the width of the fabric, carrying the weft yarn across through the shed to the opposite side. The rapier is then retracted, leaving the new pick in place.\n\nIn some versions of the loom, two rapiers are used, each half the width of the fabric in size. One rapier carries the yarn to the centre of the shed, where the opposing rapier picks up the yarn and carries it the remainder of the way across the shed. The double rapier is used more frequently than the single rapier due to its increased pick insertion speed and ability to weave wider widths of fabric.\n\nThe housing for the rapiers must take up as much space as the width of the machine. To overcome this problem, looms with flexible rapiers have been devised. The flexible rapier can be coiled as it is withdrawn, therefore requiring less storage space. If, however, the rapier is too stiff then it will not coil; If it is too flexible, it will buckle. Rigid and flexible rapier machines operate at speeds ranging from about 200 to 260 ppm, using up to 1,300 metres of weft yarn every minute. They have a noise level similar to that of modern projectile looms. They can produce a wide variety of fabrics ranging from muslin to drapery and upholstery materials.\n\nNewer rapier machines are built with two distinct weaving areas for two separate fabrics. On such machines, one rapier picks up the yarn from the centre, between the two fabrics, and carries it across one weaving area; as it finishes laying that pick, the opposite end of the rapier picks up another yarn from the centre, and the rapier moves in the other direction to lay a pick for the second weaving area, on the other half of the machine. The above figure shows the action on a single width of fabric for a single rigid rapier system, a double rigid rapier system, and a double flexible rapier system .\n\nRapier machines weave more rapidly than most shuttle machines but more slowly than most other projectile machines. An important advantage of rapier machines is their flexibility, which permits the laying of picks of different colours. They also weave yarns of any type of fibre and can weave fabrics up to 110 inches in width without modification.\n\nThe development of the rapier loom began in 1844, when John Smith of Salford was granted a patent on a loom design that eliminated the shuttle typical of earlier models of looms. Subsequent patents were taken out by Phillippe and Maurice in 1855, W.S. Laycock in 1869, and W. Glover in 1874, with rigid rapiers being perfected by O. Hallensleben in 1899. The main breakthrough came in 1922 when John Gabler invented the principle of loop transfer in the middle of the shed. Flexible rapiers of the type used today were proposed in 1925 by the Spanish inventor R.G. Moya, while R. Dewas introduced the idea of grasping the weft at its tip by the giver or a carrier rapier and transferring it to the taker or a receiver in the middle of the shed. It was not until the 1950s and 1960s that rapier weaving became fully commercialized, with loom technology developing rapidly. \n\n"}
{"id": "11669726", "url": "https://en.wikipedia.org/wiki?curid=11669726", "title": "Red Oxx Manufacturing", "text": "Red Oxx Manufacturing\n\nRed Oxx Manufacturing, Inc. is an American manufacturing company which produces travel luggage. The company was founded in 1986 by Jim Markel Sr., a retired Green Beret Captain, and was joined by his son Jim Markel Jr. and Perry Jones, also retired military parachute riggers. In the U.S., the products have won a 'National Award for Luggage'. Also, the Red Oxx Safari Beano bag was Outside Magazine's 2004 Bag of the Year.\n\n"}
{"id": "32922846", "url": "https://en.wikipedia.org/wiki?curid=32922846", "title": "Relocatable buildings", "text": "Relocatable buildings\n\nRelocatable Buildings are partially or completely assembled buildings that are constructed in a building manufacturing facility using a modular construction process. They are designed to be reused or repurposed multiple times and transported to different locations.\nRelocatable buildings can offer more flexibility and a much quicker time to occupancy than conventionally built structures. They are essential in cases where speed, temporary swing space, and the ability to relocate are necessary. These buildings are cost effective, code compliant solutions for many markets.\n\n\nSustainable\n\n\n\nModular buildings can also contribute to LEED requirements in any category site-built construction can, and can even provide an advantage in the areas of Sustainable Sites, Energy and Atmosphere, Materials and Resources, and Indoor Environmental Quality.\nModular construction can also provide an advantage in similar categories in the International Green Construction Code.\n\nRelocatable modular buildings are utilized in any application where a relocatable building can meet a temporary space need. The primary markets served are education, general office, retail, healthcare, construction-site and in-plant offices, security, telecommunications/data/equipment centers, and emergency housing/disaster relief.\n\nCommercial Modular Construction\nPermanent Modular Construction\n\n"}
{"id": "4736556", "url": "https://en.wikipedia.org/wiki?curid=4736556", "title": "Renew", "text": "Renew\n\nRenew is a European four-year project (2004–07) to prove different concepts of fuel production from biomass. Established and funded by the European Unions Sixth Framework Programme in January 2004, the Renew consortium consisted of 33 partners from 9 European countries. More than half of the partners are industrial enterprises, remaining are research and development institutes, with the special expertise needed to perform the individual R&D tasks.\n\nThe project divided into six sub-projects, four of which were dedicated to the optimisation and analysis of the fuel production process and production routes for biofuels from lignocellulosic feedstock, also known as second generation biofuel.\n\n\n\n\n\n\n\n\n\n"}
{"id": "54602583", "url": "https://en.wikipedia.org/wiki?curid=54602583", "title": "Robot Man of Szeged", "text": "Robot Man of Szeged\n\nThe Robot Man of Szeged (Hungarian: \"Szegedi Robotember\", literally \"Robot Human of Szeged\") was an early robot which was developed in Hungary in 1962. It was designed and constructed by Dr. Dániel Muszka of the University of Szeged, the same person who created the Ladybird of Szeged. The robot was made to serve in the Pioneer House of Szeged as a robotic receptionist, greeting and guiding visitors at the entrance.\n\nThe original Robot Man was scrapped since its debut, but Muszka built a replica in 2014.\n\nThe Robot Man was constructed at the request of the Pioneer House of Szeged.\n\nThe robot's name in Hungarian is \"Szegedi Robotember\", literally \"Robot Human of Szeged\", as the Hungarian language never used the word for \"man\" to refer to humans or people in general.\n\nThe Robot Man was a roughly human-sized stationary robot. It was capable of rotating its head in a 100° range using an automobile windscreen wiper motor taken from a Wartburg 311. It had a photosensor built in the front of its torso. The robot's left hand held a panel of 24 buttons with topic labels next to them. The right hand held a transistor radio. The head had two \"eyes\" which could lit up and an antenna on the top.\n\nThe Robot Man's photosensor on its \"stomach\" enabled it to sense when people passed it or stood in front of it, and to \"greet\" them audibly. Pressing an associated button of a topic on the panel held in the robot's left hand made the Robot Man \"talk\" about the desired topic. The radio held in the robot's right hand either played Kossuth Rádió or Petőfi Rádió, the entirety of domestic national broadcasts of the day. During operation, the robot's \"eyes\" were lit and its antenna on top of its head spun.\n\nThe Robot Man's replica was first exhibited at the \"A kibernetika hőskora – avagy volt élet a PC előtt\" (\"The Heroic Age of Cybernetics, or, There was Life Before the PC\") temporary exhibition held at the Museum of Nuclear Energetics of the Paks Nuclear Power Plant at Paks in 2014. The next public appearance was at the Technical Study Stores of the Hungarian Technical and Transportation Museum (MMKM) at Budapest in 2015, as part of the annual Night of the Museums. As of 2017, the replica is one of the exhibit items at the \"MI és a Robot\" (\"AI and the Robot\") temporary robotics exhibition held at the Informatics History Exhibition (ITK) in Szeged.\n\n\n"}
{"id": "41555099", "url": "https://en.wikipedia.org/wiki?curid=41555099", "title": "Spartacus Educational", "text": "Spartacus Educational\n\nSpartacus Educational is a free online encyclopedia with essays and other educational material on a wide variety of historical subjects (including British history and the history of the United States as well as other subjects including World War I, World War II, Russian Revolution, slavery, women's suffrage, Nazi Germany, Spanish Civil War and Cold War). It is used by history teachers and students. \n\nBased in the United Kingdom, Spartacus Educational was established as a book publisher in 1984 by former history teacher John Simkin and Judith Harris. It became an online publisher in September 1997. \n\n"}
{"id": "962171", "url": "https://en.wikipedia.org/wiki?curid=962171", "title": "Stern–Gerlach experiment", "text": "Stern–Gerlach experiment\n\nThe Stern–Gerlach experiment demonstrated that the spatial orientation of angular momentum is quantized, and thus an atomic-scale system having intrinsically quantum properties. In the original experiment, silver atoms were sent through a spatially varying magnetic field, which deflected them before they struck a detector screen, such as a glass slide. Particles with non-zero magnetic moment are deflected, due to the magnetic field gradient, from a straight path. The screen reveals discrete points of accumulation, rather than a continuous distribution, owing to their quantized spin. Historically, this experiment was decisive in convincing physicists of the reality of angular-momentum quantization in all atomic-scale systems.\n\nThe experiment was first conducted by the German physicists Otto Stern and Walter Gerlach in 1922.\n\nThe Stern–Gerlach experiment involves sending a beam of silver atoms through an inhomogeneous magnetic field and observing their deflection.\n\nThe results show that particles possess an intrinsic angular momentum that is closely analogous to the angular momentum of a classically spinning object, but that takes only certain quantized values. Another important result is that only one component of a particle's spin can be measured at one time, meaning that the measurement of the spin along the z-axis destroys information about a particle's spin along the x and y axis.\n\nThe experiment is normally conducted using electrically neutral particles such as silver atoms. This avoids the large deflection in the path of a charged particle moving through a magnetic field and allows spin-dependent effects to dominate.\n\nIf the particle is treated as a classical spinning magnetic dipole, it will precess in a magnetic field because of the torque that the magnetic field exerts on the dipole (see torque-induced precession). If it moves through a homogeneous magnetic field, the forces exerted on opposite ends of the dipole cancel each other out and the trajectory of the particle is unaffected. However, if the magnetic field is inhomogeneous then the force on one end of the dipole will be slightly greater than the opposing force on the other end, so that there is a net force which deflects the particle's trajectory. If the particles were classical spinning objects, one would expect the distribution of their spin angular momentum vectors to be random and continuous. Each particle would be deflected by an amount proportional to its magnetic moment, producing some density distribution on the detector screen. Instead, the particles passing through the Stern–Gerlach apparatus are deflected either up or down by a specific amount. This was a measurement of the quantum observable now known as spin angular momentum, which demonstrated possible outcomes of a measurement where the observable has a discrete set of values or point spectrum.\n\nAlthough some discrete quantum phenomena, such as atomic spectra, were observed much earlier, the Stern–Gerlach experiment allowed scientists to observe separation between discrete quantum states for the first time in the history of science.\n\nBy now, it is known that, theoretically, quantum angular momentum \"of any kind\" has a discrete spectrum, which is sometimes briefly expressed as \"angular momentum is quantized\".\n\nIf the experiment is conducted using charged particles like electrons, there will be a Lorentz force that tends to bend the trajectory in a circle (see cyclotron motion). This force can be cancelled by an electric field of appropriate magnitude oriented transverse to the charged particle's path.i\n\nElectrons are spin- particles. These have only two possible spin angular momentum values measured along any axis, formula_1 or formula_2, a purely quantum mechanical phenomenon. Because its value is always the same, it is regarded as an intrinsic property of electrons, and is sometimes known as \"intrinsic angular momentum\" (to distinguish it from orbital angular momentum, which can vary and depends on the presence of other particles). If one measures the spin along a vertical axis, electrons are described as \"spin up\" or \"spin down\", based on the magnetic moment pointing up or down, respectively.\n\nTo mathematically describe the experiment with spin formula_3 particles, it is easiest to use Dirac's bra–ket notation. As the particles pass through the Stern–Gerlach device, they are deflected either up or down, and observed by the detector which resolves to either spin up or spin down. These are described by the angular momentum quantum number formula_4, which can take on one of the two possible allowed values, either formula_1 or formula_2. The act of observing (measuring) the momentum along the formula_7 axis corresponds to the operator formula_8. In mathematical terms, the initial state of the particles is\nwhere constants formula_10 and formula_11 are complex numbers. This initial state spin can point in any direction. The squares of the absolute values \nformula_12 and formula_13 determine the probabilities that for a system in the initial state formula_14 one of the two possible values of formula_4 is found after the measurement is made. The constants formula_10 and formula_11 must also be normalized in order that the probability of finding either one of the values be unity, that is we must ensure that formula_18. However, this information is not sufficient to determine the values of formula_10 and formula_11, because they are complex numbers. Therefore, the measurement yields only the squared magnitudes of the constants, which are interpreted as probabilities.\n\nIf we link multiple Stern–Gerlach apparatuses (the rectangles containing \"S-G\"), we can clearly see that they do not act as simple selectors, but alter the states observed (as in light polarization). In the three S-G systems shown below, the cross-hatched squares denote the blocking of a given output. The unblocked output is thus the only source for the next S-G apparatus in the sequence.\n\nThe top system demonstrates that when z+ is passed through a second, identical, S-G apparatus, only z+ is seen in the output, as expected.\n\nThe middle system demonstrates that a z+ input can be sorted on the x-axis producing x+ and x- outputs.\n\nThe bottom system feeds the x+ output into a third S-G apparatus and demonstrates that it be sorted on the z-axis to produce z+ and z- outputs. Given that the input to the second S-G apparatus consisted only of z+, while the output of the third contains z- as well as z+, it can be inferred that an S-G apparatus must be altering the states of the particles that pass through it.\n\nThe Stern–Gerlach experiment was conceived by Otto Stern in 1921 and performed by him and Walther Gerlach in Frankfurt in 1922. At the time, Stern was an assistant to Max Born at the University of Frankfurt's Institute for Theoretical Physics, and Gerlach was an assistant at the same university's Institute for Experimental Physics.\n\nAt the time of the experiment, the most prevalent model for describing the atom was the Bohr model, which described electrons as going around the positively charged nucleus only in certain discrete atomic orbitals or energy levels. Since the electron was quantized to be only in certain positions in space, the separation into distinct orbits was referred to as space quantization. The Stern–Gerlach experiment was meant to test the Bohr–Sommerfeld hypothesis that the direction of the angular momentum of a silver atom is quantized.\n\nNote that the experiment was performed several years before Uhlenbeck and Goudsmit formulated their hypothesis of the existence of the electron spin. Even though the result of the Stern−Gerlach experiment has later turned out to be in agreement with the predictions of quantum mechanics for a spin- particle, the experiment should be seen as a corroboration of the Bohr–Sommerfeld theory.\n\nIn 1927, T.E. Phipps and J.B. Taylor reproduced the effect using hydrogen atoms in their ground state, thereby eliminating any doubts that may have been caused by the use of silver atoms. However, in 1926 the non-relativistic Schrödinger equation had incorrectly predicted the magnetic moment of hydrogen to be zero in its ground state. To correct this problem Wolfgang Pauli introduced \"by hand\", so to speak, the 3 Pauli matrices which now bear his name, but which were later shown by Paul Dirac in 1928 to be intrinsic in his relativistic equation.\n\nThe experiment was first performed with an electromagnet that allowed the non-uniform magnetic field to be turned on gradually from a null value. When the field was null, the silver atoms were deposited as a single band on the detecting glass slide. When the field was made stronger, the middle of the band began to widen and eventually to split into two, so that the glass-slide image looked like a lip-print, with an opening in the middle, and closure at either end. In the middle, where the magnetic field was strong enough to split the beam into two, statistically half of the silver atoms had been deflected by the non-uniformity of the field.\n\nThe Stern–Gerlach experiment strongly influenced later developments in modern physics:\n\n\n\n\n"}
{"id": "8562063", "url": "https://en.wikipedia.org/wiki?curid=8562063", "title": "Technomancy", "text": "Technomancy\n\nTechnomancy, also called technomagic, is a term in science fiction and fantasy that refers to a category of magical abilities that affect technology, or to magical powers that are gained through the use of technology.\n\nIt is a portmanteau of \"technology\" and \"-mancy\", a suffix used in magical sciences to refer to specific types of specialization or divination (\"-mancy\" is derived from the Greek \"manteia\", meaning divination).\n\nAn early appearance of the term can be found in Steve Martindale's 1990 short story \"Technomancy\" in the magazine \"Aboriginal Science Fiction\".\n\nTechnomancy is a common theme in certain subgenres of both science fiction and modern-day fantasy fiction, particularly fiction that crosses the sci-fi and fantasy genres, as well as role playing games that take place in similar settings. Strictly speaking, though, it belongs fully to the realm of fantasy since it can be magic that is used on technology that presently exists. It most commonly appears in science fantasy. The term technomancy has been gaining usage on webcomics on the internet, although it is used in a vague sense.\n\nIt is also distinct from what is sometimes called \"magitech\" (technology that uses magic, as used by D.O.L.L.Y. in the comic the \"Wotch\"). Magitech considers magic and science to be two parts of one force. Technomancy has magic affecting science, but not working in the same process. another explanation of technomancy is the combination of necromancy and technology.\n\n\nIn some settings technomancy may be totally scientific in nature in accordance with Arthur C Clarke's third law of prediction:\nExamples of users of this type of technomancy are the Technomages of the Babylon 5 universe; and in Ilium/Olympos, where the supernatural powers of wizards and gods come from an advanced technology.\n\nIn the Mass Effect video game series, many characters gain magic-like powers through technology.\n\nThe 4th Edition of the Shadowrun role-playing game has characters who can interact with the Matrix (the Internet of that setting) without using technology and are referred to as \"Technomancers\", but their abilities stem from a mutation rather than magic. Shadowrun Technomancers are specifically unable to use magic.\n\nThe term Technomancy can be descriptive of the skill of an engineer whose expertise allows him or her to diagnose mechanical problems by observing the machine behavior, in essence listening to the machine to let it tell him what is wrong.\n\nIn \"Overwatch\" the character Sombra can instantly \"hack\" and control any piece of technology within seconds. She summons up a holographic keyboard that she types on that can \"hack\" health packs, other characters ultimates, or even their basic abilities. In the animated short Blizzard created introducing her, she can also be seen gaining control of a giant robotic arm. In both the game and the short she can render herself invisible or teleport.\n\nAnother form of technomancy, sometimes called 'industrial magic', has magical devices operating similarly to technological devices.\n\nThe Harry Potter setting has owl familiars serving as a postal system, animated newspapers and fireplace embers serving as video screens, phantom quills and parchments as speech-recognition software, even flying brooms and orbs as athletic equipment, and so on. The Eberron setting of Dungeons and Dragons has bound elemental spirits powering transportation vehicles. In for example, the crystal is a supernatural being, but his power was used like a computer program. In Dave the Barbarian, crystal balls and magic cauldrons were used like telephones, televisions and computers. In Adam Sandler's film Click, the protagonist received a remote control that could change the reality around himself.\n\n"}
{"id": "30010", "url": "https://en.wikipedia.org/wiki?curid=30010", "title": "Telegraphy", "text": "Telegraphy\n\nTelegraphy (from Ancient Greek: τῆλε, \"têle\", \"at a distance\" and γράφειν, \"gráphein\", \"to write\") is the long-distance transmission of textual or symbolic (as opposed to verbal or audio) messages without the physical exchange of an object bearing the message. Thus semaphore is a method of telegraphy, whereas pigeon post is not.\n\nTelegraphy requires that the method used for encoding the message be known to both sender and receiver. Many methods are designed according to the limits of the signalling medium used. The use of smoke signals, beacons, reflected light signals, and flag semaphore signals are early examples.\n\nIn the 19th century, the harnessing of electricity led to the invention of electrical telegraphy. The advent of radio in the early 20th century brought about radiotelegraphy and other forms of wireless telegraphy. In the Internet age, telegraphic means developed greatly in sophistication and ease of use, with natural language interfaces that hide the underlying code, allowing such technologies as electronic mail and instant messaging.\n\nThe word \"telegraph\" was first coined by the French inventor of the Semaphore line, Claude Chappe, who also coined the word \"semaphore\".\n\nA \"telegraph\" is a device for transmitting and receiving messages over long distances, i.e., for telegraphy. The word \"telegraph\" alone now generally refers to an electrical telegraph.\n\nWireless telegraphy is also known as \"CW\", for continuous wave (a carrier modulated by on-off keying), as opposed to the earlier radio technique of using a spark gap.\n\nContrary to the extensive definition used by Chappe, Morse argued that the term \"telegraph\" can strictly be applied only to systems that transmit \"and\" record messages at a distance. This is to be distinguished from \"semaphore\", which merely transmits messages. Smoke signals, for instance, are to be considered semaphore, not telegraph. According to Morse, telegraph dates only from 1832 when Pavel Schilling invented one of the earliest electrical telegraphs.\n\nA telegraph message sent by an electrical telegraph operator or telegrapher using Morse code (or a printing telegraph operator using plain text) was known as a \"telegram\". A \"cablegram\" was a message sent by a submarine telegraph cable, often shortened to a \"cable\" or a \"wire\". Later, a \"Telex\" was a message sent by a Telex network, a switched network of teleprinters similar to a telephone network.\n\nA \"wire picture\" or \"wire photo\" was a newspaper picture that was sent from a remote location by a facsimile telegraph. A \"diplomatic telegram\", also known as a diplomatic cable, is the term given to a confidential communication between a diplomatic mission and the foreign ministry of its parent country. These continue to be called telegrams or cables regardless of the method used for transmission.\n\nEven though early telegraphic precedents, such as signalling through the lighting of pyres, have existed since ancient times, long-distance telegraphy (transmission of complex messages) started in 1792 in the form of semaphore lines, or optical telegraphs, that sent messages to a distant observer through line-of-sight signals. Commercial electrical telegraphs were introduced from 1837.\n\nThe first telegraphs came in the form of optical telegraph, including the use of smoke signals, beacons, or reflected light, which have existed since ancient times. Early proposals for an optical telegraph system were made to the Royal Society by Robert Hooke in 1684 and were first implemented on an experimental level by Sir Richard Lovell Edgeworth in 1767.\n\nThe first successful semaphore network was invented by Claude Chappe and operated in France from 1793 to 1846.\nDuring 1790–1795, at the height of the French Revolution, France needed a swift and reliable communication system to thwart the war efforts of its enemies. In 1790, the Chappe brothers set about devising a system of communication that would allow the central government to receive intelligence and to transmit orders in the shortest possible time. On 2 March 1791, at 11 am, they sent the message \"si vous réussissez, vous serez bientôt couverts de gloire\" (If you succeed, you will soon bask in glory) between Brulon and Parce, a distance of . The first means used a combination of black and white panels, clocks, telescopes, and codebooks to send their message.\n\nIn 1792, Claude was appointed \"Ingénieur-Télégraphiste\" and charged with establishing a line of stations between Paris and Lille, a distance of 230 kilometres (about 143 miles). It was used to carry dispatches for the war between France and Austria. In 1794, it brought news of a French capture of Condé-sur-l'Escaut from the Austrians less than an hour after it occurred.\n\nThe Prussian system was put into effect in the 1830s. However, they were highly dependent on good weather and daylight to work and even then could accommodate only about two words per minute. The last commercial semaphore link ceased operation in Sweden in 1880. As of 1895, France still operated coastal commercial semaphore telegraph stations, for ship-to-shore communication.\n\nThe first suggestion for using electricity as a means of communication appeared in the \"Scots Magazine\" in 1753. Using one wire for each letter of the alphabet, a message could be transmitted by connecting the wire terminals in turn to an electrostatic machine, and observing the deflection of pith balls at the far end. Telegraphs employing electrostatic attraction were the basis of early experiments in electrical telegraphy in Europe but were abandoned as being impractical and were never developed into a useful communication system.\n\nOne very early experiment in electrical telegraphy was an \"electrochemical telegraph\" created by the German physician, anatomist, and inventor Samuel Thomas von Sömmering in 1809, based on an earlier, less robust design of 1804 by Spanish polymath and scientist Francisco Salva Campillo. Both their designs employed multiple wires (up to 35) in order to visually represent most Latin letters and numerals. Thus, messages could be conveyed electrically up to a few kilometers (in von Sömmering's design), with each of the telegraph receiver's wires immersed in a separate glass tube of acid. As an electric current was applied by the sender representing each character of a message, it would at the recipient's end electrolyse the acid in its corresponding tube, releasing a stream of hydrogen bubbles next to its associated letter or numeral. The telegraph receiver's operator would visually observe the bubbles and could then record the transmitted message, albeit at a very low baud rate.\n\nThe first working telegraph was built by the English inventor Francis Ronalds in 1816 and used static electricity. At the family home on Hammersmith Mall, he set up a complete subterranean system in a 175-yard long trench as well as an eight-mile long overhead telegraph. The lines were connected at both ends to clocks marked with the letters of the alphabet and electrical impulses sent along the wire were used to transmit messages. Offering his invention to the Admiralty in July 1816, it was rejected as \"wholly unnecessary\". His account of the scheme and the possibilities of rapid global communication in \"Descriptions of an Electrical Telegraph and of some other Electrical Apparatus\" was the first published work on electric telegraphy and even described the risk of signal retardation due to induction. Elements of Ronalds' design were utilised in the subsequent commercialisation of the telegraph over 20 years later.\n\nAn early electromagnetic telegraph design was created by Russian diplomat Pavel Schilling in 1832. He set it up in his apartment in St. Petersburg and demonstrated the long-distance transmission of signals by positioning two telegraphs of his invention in two different rooms of his apartment. Schilling was the first to put into practice the idea of a binary system of signal transmissions.\n\nCarl Friedrich Gauss and Wilhelm Weber built the first electromagnetic telegraph used for \"regular\" communication in 1833 in Göttingen, connecting Göttingen Observatory and the Institute of Physics, covering a distance of about 1 km. The setup consisted of a coil that could be moved up and down over the end of two magnetic steel bars. The resulting induction current was transmitted through two wires to the receiver, consisting of a galvanometer. The direction of the current could be reversed by commuting the two wires in a special switch. Therefore, Gauss and Weber chose to encode the alphabet in a binary code, using positive and negative currents as the two states.\n\nTelegraph networks were expensive to build, but financing was readily available, especially from London bankers. By 1852, National systems were in operation in major countries:\n\nThe first commercial electrical telegraph was co-developed by Sir William Fothergill Cooke and Charles Wheatstone. In May 1837, they patented the Cooke and Wheatstone system, which used a number of needles on a board that could be moved to point to letters of the alphabet. The patent recommended a five-needle system, but any number of needles could be used depending on the number of characters it was required to code. A four-needle system was installed between Euston and Camden Town in London on a rail line being constructed by Robert Stephenson between London and Birmingham. It was successfully demonstrated on 25 July 1837. Euston needed to signal to an engine house at Camden Town to start hauling the locomotive up the incline. As at Liverpool, the electric telegraph was in the end rejected in favour of a pneumatic system with whistles.\n\nCooke and Wheatstone had their first commercial success with a system installed on the Great Western Railway over the from Paddington station to West Drayton in 1838, the first commercial telegraph in the world. This was a five-needle, six-wire system. The cables were originally installed underground in a steel conduit. However, the cables soon began to fail as a result of deteriorating insulation and were replaced with uninsulated wires on poles. As an interim measure, a two-needle system was used with three of the remaining working underground wires, which despite using only two needles had a greater number of codes. But when the line was extended to Slough in 1843, a one-needle, two-wire system was installed.\n\nFrom this point, the use of the electric telegraph started to grow on the new railways being built from London. The London and Blackwall Railway (another rope-hauled application) was equipped with the Cooke and Wheatstone telegraph when it opened in 1840, and many others followed. The one-needle telegraph proved highly successful on British railways, and 15,000 sets were still in use at the end of the nineteenth century. Some remained in service in the 1930s. In September 1845, the financier John Lewis Ricardo and Cooke formed the Electric Telegraph Company, the first public telegraphy company in the world. This company bought out the Cooke and Wheatstone patents and solidly established the telegraph business.\n\nAs well as the rapid expansion of the use of the telegraphs along the railways, they soon spread into the field of mass communication with the instruments being installed in post offices across the country. The era of mass personal communication had begun.\n\nAn electrical telegraph was independently developed and patented in the United States in 1837 by Samuel Morse. His assistant, Alfred Vail, developed the Morse code signalling alphabet with Morse. The first telegram in the United States was sent by Morse on 11 January 1838, across two miles (3 km) of wire at Speedwell Ironworks near Morristown, New Jersey, although it was only later, in 1844, that he sent the message \"WHAT HATH GOD WROUGHT\" from the Capitol in Washington to the old Mt. Clare Depot in Baltimore. From then on, commercial telegraphy took off in America with lines linking all the major metropolitan centres on the East Coast within the next decade. The overland telegraph connected the west coast of the continent to the east coast by 24 October 1861, bringing an end to the Pony Express.\n\nThe Morse telegraphic apparatus was officially adopted as the standard for European telegraphy in 1851. Only Great Britain with its extensive overseas empire kept the needle telegraph of Cooke and Wheatstone. In 1858, Morse introduced wired communication to Latin America when he established a telegraph system in Puerto Rico, then a Spanish Colony. The line was inaugurated on March 1, 1859, in a ceremony flanked by the Spanish and American flags.\n\nAnother early system was that of Edward Davy, who demonstrated his in Regent's Park in 1837 and was granted a patent on 4 July 1838. He also developed an electric relay.\n\nTelegraphy was driven by the need to reduce sending costs, either in hand-work per message or by increasing the sending rate. While many experimental systems employing moving pointers and various electrical encodings proved too complicated and unreliable, a successful advance in the sending rate was achieved through the development of telegraphese.\nThe first system that didn't require skilled technicians to operate was Sir Charles Wheatstone's ABC system in 1840 where the letters of the alphabet were arranged around a clock-face, and the signal caused a needle to indicate the letter. This early system required the receiver to be present in real time to record the message and it reached speeds of up to 15 words a minute.\n\nIn 1846, Alexander Bain patented a chemical telegraph in Edinburgh. The signal current made a readable mark on a moving paper tape soaked in a mixture of ammonium nitrate and potassium ferrocyanide, which gave a blue mark when a current was passed through it.\nDavid Edward Hughes invented the printing telegraph in 1855; it used a keyboard of 26 keys for the alphabet and a spinning type wheel that determined the letter being transmitted by the length of time that had elapsed since the previous transmission. The system allowed for automatic recording on the receiving end. The system was very stable and accurate and became the accepted around the world.\n\nThe next improvement was the Baudot code of 1874. French engineer Émile Baudot patented a printing telegraph in which the signals were translated automatically into typographic characters. Each character was assigned a unique code based on the sequence of just five contacts. Operators had to maintain a steady rhythm, and the usual speed of operation was 30 words per minute.\n\nBy this point, reception had been automated, but the speed and accuracy of the transmission were still limited to the skill of the human operator. The first practical automated system was patented by Charles Wheatstone, the original inventor of the telegraph. The message (in Morse code) was typed onto a piece of perforated tape using a keyboard-like device called the 'Stick Punch'. The transmitter automatically ran the tape through and transmitted the message at the then exceptionally high speed of 70 words per minute.\n\nTeleprinters were invented in order to send and receive messages without the need for operators trained in the use of Morse code. A system of two teleprinters, with one operator trained to use a typewriter, replaced two trained Morse code operators. The teleprinter system improved message speed and delivery time, making it possible for messages to be flashed across a country with little manual intervention.\n\nEarly teleprinters used the ITA-1 Baudot code, a five-bit code. This yielded only thirty-two codes, so it was over-defined into two \"shifts\", \"letters\", and \"figures\". An explicit, unshared shift code prefaced each set of letters and figures. In 1901, Baudot's code was modified by Donald Murray and around 1930, the CCITT introduced the International Telegraph Alphabet No. 2 (ITA2) code as an international standard.\nBy 1935, message routing was the last great barrier to full automation. Large telegraphy providers began to develop systems that used telephone-like rotary dialling to connect teletypewriters. These machines were called \"Telex\" (TELegraph EXchange). Telex machines first performed rotary-telephone-style pulse dialling for circuit switching and then sent data by Baudot code. This \"type A\" Telex routing functionally automated message routing.\n\nTelex began in Germany as a research and development program in 1926 that became an operational teleprinter service in 1933. The service was operated by the Reichspost (Reich postal service) and had a speed of 50 baud – approximately 66 words-per-minute.\n\nAt the rate of 45.45 (±0.5%) baud—considered speedy at the time—up to 25 telex channels could share a single long-distance telephone channel by using \"voice frequency telegraphy multiplexing\", making telex the least expensive method of reliable long-distance communication.\n\nAutomatic teleprinter exchange service was introduced into Canada by CPR Telegraphs and CN Telegraph in July 1957, and in 1958, Western Union started to build a Telex network in the United States.\n\nBeginning in 1956, telegrams begun to be transmitted over the Telex network using the standard named Gentex in order to lower the costs for some European telecommunications companies by allowing the sending telegraph station to connect directly to the receiving station.\n\nSoon after the first successful telegraph systems were operational, the possibility of transmitting messages across the sea by way of submarine communications cables was first mooted. One of the primary technical challenges was to insulate the submarine cable sufficiently to prevent the current from leaking out into the water. In 1842, a Scottish surgeon William Montgomerie introduced Gutta-percha, the adhesive juice of the \"Palaquium gutta\" tree, to Europe. Michael Faraday and Wheatstone soon discovered the merits of gutta-percha as an insulator, and in 1845, the latter suggested that it should be employed to cover the wire which was proposed to be laid from Dover to Calais. It was tried on a wire laid across the Rhine between Deutz and Cologne. In 1849, C.V. Walker, electrician to the South Eastern Railway, submerged a two-mile wire coated with gutta-percha off the coast from Folkestone, which was tested successfully.\n\nJohn Watkins Brett, an engineer from Bristol, sought and obtained permission from Louis-Philippe in 1847 to establish telegraphic communication between France and England. The first undersea cable was laid in 1850 and connected London with Paris. After an initial exchange of greetings between Queen Victoria and President Napoleon, it was almost immediately severed by a French fishing vessel. The line was relaid the next year and then followed by connections to Ireland and the Low Countries.\nThe Atlantic Telegraph Company was formed in London in 1856 to undertake to construct a commercial telegraph cable across the Atlantic Ocean. It was successfully completed on 27 July 1866, by the ship SS \"Great Eastern\", captained by Sir James Anderson after many mishaps along the way. Earlier transatlantic submarine cables installations were attempted in 1857, 1858, and 1865. The 1858 cable only operated intermittently for a few days or weeks before it failed. The study of underwater telegraph cables accelerated interest in mathematical analysis of very long transmission lines. An overland telegraph from Britain to India was first connected in 1866 but was unreliable so a submarine telegraph cable was connected in 1870. Several telegraph companies were combined to form the \"Eastern Telegraph Company\" in 1872.\n\nAustralia was first linked to the rest of the world in October 1872 by a submarine telegraph cable at Darwin. This brought news reportage from the rest of the world. The telegraph across the Pacific was completed in 1902, finally encircling the world.\n\nFrom the 1850s until well into the 20th century, British submarine cable systems dominated the world system. This was set out as a formal strategic goal, which became known as the All Red Line. In 1896, there were thirty cable laying ships in the world and twenty-four of them were owned by British companies. In 1892, British companies owned and operated two-thirds of the world's cables and by 1923, their share was still 42.7 percent. During World War I, Britain's telegraph communications were almost completely uninterrupted while it was able to quickly cut Germany's cables worldwide.\n\nIn 1843, Scottish inventor Alexander Bain invented a device that could be considered the first facsimile machine. He called his invention a \"recording telegraph\". Bain's telegraph was able to transmit images by electrical wires. Frederick Bakewell made several improvements on Bain's design and demonstrated a telefax machine. In 1855, an Italian abbot, Giovanni Caselli, also created an electric telegraph that could transmit images. Caselli called his invention \"Pantelegraph\". Pantelegraph was successfully tested and approved for a telegraph line between Paris and Lyon.\n\nIn 1881, English inventor Shelford Bidwell constructed the \"scanning phototelegraph\" that was the first telefax machine to scan any two-dimensional original, not requiring manual plotting or drawing. Around 1900, German physicist Arthur Korn invented the \"\" widespread in continental Europe especially since a widely noticed transmission of a wanted-person photograph from Paris to London in 1908 used until the wider distribution of the radiofax. Its main competitors were the \"Bélinographe\" by Édouard Belin first, then since the 1930s, the \"Hellschreiber\", invented in 1929 by German inventor Rudolf Hell, a pioneer in mechanical image scanning and transmission.\n\nThe late 1880s through to the 1890s saw the discovery and then development of a newly understood phenomenon into a form of wireless telegraphy, called \"Hertzian wave\" wireless telegraphy, radiotelegraphy, or (later) simply \"radio\". Between 1886 and 1888, Heinrich Rudolf Hertz published the results of his experiments where he was able to transmit electromagnetic waves (radio waves) through the air, proving James Clerk Maxwell's 1873 theory of electromagnetic radiation. Many scientists and inventors experimented with this new phenomenon but the general consensus was that these new waves (similar to light) would be just as short range as light, and, therefore, useless for long range communication.\n\nAt the end of 1894, the young Italian inventor Guglielmo Marconi began working on the idea of building a commercial wireless telegraphy system based on the use of Hertzian waves (radio waves), a line of inquiry that he noted other inventors did not seem to be pursuing. Building on the ideas of previous scientists and inventors Marconi re-engineered their apparatus by trial and error attempting to build a radio-based wireless telegraphic system that would function the same as wired telegraphy. He would work on the system through 1895 in his lab and then in field tests making improvements to extend its range. After many breakthroughs, including applying the wired telegraphy concept of grounding the transmitter and receiver, Marconi was able, by early 1896, to transmit radio far beyond the short ranges that had been predicted. Having failed to interest the Italian government, the 22-year-old inventor brought his telegraphy system to Britain in 1896 and met William Preece, a Welshman, who was a major figure in the field and Chief Engineer of the General Post Office. A series of demonstrations for the British government followed—by March 1897, Marconi had transmitted Morse code signals over a distance of about across Salisbury Plain.\n\nOn 13 May 1897, Marconi, assisted by George Kemp, a Cardiff Post Office engineer, transmitted the first wireless signals over water to Lavernock (near Penarth in Wales) from Flat Holm. The message sent was \"ARE YOU READY\". From his Fraserburgh base, he transmitted the first long-distance, cross-country wireless signal to Poldhu in Cornwall. His star rising, he was soon sending signals across The English channel (1899), from shore to ship (1899) and finally across the Atlantic (1901). A study of these demonstrations of radio, with scientists trying to work out how a phenomenon predicted to have a short range could transmit \"over the horizon\", led to the discovery of a radio reflecting layer in the Earth's atmosphere in 1902, later called the ionosphere.\n\nRadiotelegraphy proved effective for rescue work in sea disasters by enabling effective communication between ships and from ship to shore. In 1904, Marconi began the first commercial service to transmit nightly news summaries to subscribing ships, which could incorporate them into their on-board newspapers. A regular transatlantic radio-telegraph service was finally begun on 17 October 1907. Notably, Marconi's apparatus was used to help rescue efforts after the sinking of \"Titanic\". Britain's postmaster-general summed up, referring to the \"Titanic\" disaster, \"Those who have been saved, have been saved through one man, Mr. Marconi...and his marvellous invention.\"\n\nAround 1965, DARPA commissioned a study of decentralised switching systems. Some of the ideas developed in this study provided inspiration for the development of the ARPANET packet switching research network, which later grew to become the public Internet.\n\nAs the PSTN became a digital network, T-carrier \"synchronous\" networks became commonplace in the U.S. A T1 line has a \"frame\" of 193 bits that repeats 8000 times per second. The first bit called the \"sync\" bit alternates between 1 and 0 to identify the start of the frames. The rest of the frame provides 8 bits for each of 24 separate voice or data channels. Customarily, a T-1 link is sent over a balanced twisted pair, isolated with transformers to prevent current flow. Europeans adopted a similar system (E-1) of 32 channels (with one channel for frame synchronisation).\n\nLater, SONET and SDH were adapted to combine carrier channels into groups that could be sent over optic fiber. The capacity of an optic fibre is often extended with wavelength division multiplexing, rather than rerigging new fibre. Rigging several fibres in the same structures as the first fibre is usually easy and inexpensive, and many fibre installations include unused spare \"dark fibre\", \"dark wavelengths\", and unused parts of the SONET frame, so-called \"virtual channels\".\n\nIn 2002, the Internet was used by Kevin Warwick at the University of Reading to communicate neural signals, in purely electronic form, telegraphically between the nervous systems of two humans, potentially opening up a new form of communication combining the Internet and telegraphy.\n\nIn 2006, a well-defined communication channel used for telegraphy was established by the SONET standard OC-768, which sent about 40 gigabits per second.\n\nThe theoretical maximum capacity of an optic fibre is more than 10 bits (one petabit or one quadrillion bits) per second.\n\nSince the Internet operates over any digital transmission medium, further evolution of telegraphic technology will be effectively concealed from users.\n\nE-mail was first invented for CTSS and similar time sharing systems of the era in the mid-1960s. At first, e-mail was possible only between different accounts on the same computer (typically a mainframe). ARPANET allowed different computers to be connected to allow e-mails to be relayed from computer to computer, with the first ARPANET e-mail being sent in 1971. Multics also pioneered instant messaging between computer users in the mid-1970s. With the growth of the Internet, e-mail began to be possible between any two computers with access to the Internet.\n\nVarious private networks like UUNET (founded 1987), the Well (1985), and GEnie (1985) had e-mail from the 1970s, but subscriptions were quite expensive for an individual, US$25 to US$50 per month, just for e-mail. Internet use was then largely limited to government, academia, and other government contractors until the net was opened to commercial use in the 1980s.\n\nBy the early 1990s, modems made e-mail a viable alternative to Telex systems in a business environment. But individual e-mail accounts were not widely available until local Internet service providers were in place, although demand grew rapidly, as e-mail was seen as the Internet's killer app. It allowed anyone to email anyone, whereas previously, different system had been walled off from each other, such that America Online subscribers could email only other America Online subscribers, Compuserve subscribers could email only other Compuserve subscribers, etc. The broad user base created by the demand for e-mail smoothed the way for the rapid acceptance of the World Wide Web in the mid-1990s. Fax machines were another technology that helped displace the telegram.\n\nOn Monday, 12 July 1999, a final telegram was sent from the National Liberty Ship Memorial, the SS \"Jeremiah O'Brien\", in San Francisco Bay to President Bill Clinton in the White House. Officials of Globe Wireless reported that \"The message was 95 words, and it took six or eight minutes to copy it.\" They then transmitted the message to the White House via e-mail. That event was also used to mark the final commercial U.S. ship-to-shore telegraph message transmitted from North America by Globe Wireless, a company founded in 1911. Sent from its wireless station at Half Moon Bay, California, the sign-off message was a repeat of Samuel F. B. Morse's message 155 years earlier, \"What hath God wrought?\"\n\nPrior to the electrical telegraph, ancient civilizations, such as Greece, Egypt, and China, transmitted long-distance information using drumbeats, flame beacons, or light flashes with a heliograph. Later, nearly all information was limited to traveling at the speed of a human or animal. The telegraph freed communication from the time constraints of postal mail and revolutionized the global economy and society. By the end of the 19th century, the telegraph was becoming an increasingly common medium of communication for ordinary people. The telegraph isolated the message (information) from the physical movement of objects or the process.\n\nTelegraphy facilitated the growth of organizations \"in the railroads, consolidated financial and commodity markets, and reduced information costs within and between firms\". This immense growth in the business sectors influenced society to embrace the use of telegrams.\n\nWorldwide telegraphy changed the gathering of information for news reporting. Messages and information would now travel far and wide, and the telegraph demanded a language \"stripped of the local, the regional; and colloquial\", to better facilitate a worldwide media language. Media language had to be standardized, which led to the gradual disappearance of different forms of speech and styles of journalism and storytelling.\n\nNumerous newspapers and news outlets in various countries, such as \"The Daily Telegraph\" in Britain, \"The Telegraph\" in India, \"De Telegraaf\" in the Netherlands, and the Jewish Telegraphic Agency in the US, were given names which include the word \"telegraph\" due to their having received news by means of electric telegraphy. Some of these names are retained even though more sophisticated means are now used.\nA newspaper in Indian state of Tamil Nadu is named as Dhina Thanthi which means daily telegraph.\n\nThe average length of a telegram in the 1900s in the US was 11.93 words; more than half of the messages were 10 words or fewer.\n\nAccording to another study, the mean length of the telegrams sent in the UK before 1950 was 14.6 words or 78.8 characters.\n\nFor German telegrams, the mean length is 11.5 words or 72.4 characters. At the end of the 19th century, the average length of a German telegram was calculated as 14.2 words.\n\n\n\n\n"}
{"id": "28279674", "url": "https://en.wikipedia.org/wiki?curid=28279674", "title": "Teletext systems", "text": "Teletext systems\n\nTeletext (or \"broadcast teletext\") is a television information retrieval service developed in the United Kingdom in the early 1970s. It offers a range of text-based information, typically including national, international and sporting news, weather and TV schedules. Subtitle (or closed captioning) information is also transmitted in the teletext signal, typically on page 888 or 777.\n\nA number of similar teletext services were developed in other countries, some of which attempted to address the limitations of the British-developed system, with its simple graphics and fixed page sizes.\n\nTeletext was created in the United Kingdom in the early 1970s.\nDifferent systems existed, but by the end of the decade they converged, with the creation of the World System Teletext (WST). WST remained in use for analogue broadcasts until 2012.\n\nThe first test transmissions were made by the BBC in 1972–74, with the name \"Ceefax\" (\"see facts\").\nThe Ceefax system went live on 23 September 1974 with thirty pages of information.\nDue to the adoption of a common teletext standard (WST), the Ceefax system ceased in 1976.\nThe name was retained for the service itself, that continued after that year using the WST standard.\n\n\"ORACLE\" was first broadcast on the ITV network in the mid-late 1970s.\nDue to the adoption of a common teletext standard (WST), the ORACLE system ceased in 1976.\nThe name was retained for the service itself, that continued after that year using the WST standard.\n\n\"World System Teletext\" (or \"WST\") is the name of a standard for teletext throughout Europe today.\nAlmost all television sets sold in Europe since the early ’80s have built-in WST-standard teletext decoders as a feature.\n\nIt originally stems from the UK standards developed by the BBC (Ceefax) and the UK Independent Broadcasting Authority (ORACLE) in 1974 for teletext transmission, extended in 1976 as the \"Broadcast Teletext Specification\".\n\nWith some tweaks to allow for alternative national character sets, and adaptations to the NTSC 525-line system as necessary, this was then promoted internationally as \"World System Teletext\".\n\nIt was accepted by CCIR in 1986 as \"CCIR Teletext System B\", one of four recognised standards for teletext worldwide.\n\nIn France, where the SECAM standard is used in television broadcasting, a teletext system was developed in the late 1970s under the name \"Antiope\". It had a higher data rate and was capable of dynamic page sizes, allowing more sophisticated graphics. It was phased out in favour of standard teletext in 1991.\n\nThe CBC ran a teletext service, IRIS, accessible only in Calgary, Toronto and Montreal. It ran from 1983 until about 1986, and used the Canadian-developed Telidon system, which was developed in 1980. Like Antiope, Telidon allowed significantly higher graphic resolution than standard teletext.\n\nAdoption in the United States was hampered due to a lack of a single teletext standard and consumer resistance to the high initial price of teletext decoders. Throughout the period of analogue broadcasting, teletext or other similar technologies in the US were practically non-existent, with the only technologies resembling such existing in the country being closed captioning, TV Guide On Screen, and Extended Data Services (XDS).\n\nA version of the European teletext standard designed to work with the NTSC television standard used in North America was first demonstrated in the US in 1978 by station KSL in Salt Lake City, Utah, premiered a teletext service using Ceefax. They were followed by American television network CBS, which decided to try both the British Ceefax and French Antiope software for preliminary tryouts for a teletext service, using station KMOX (now KMOV) in St. Louis, Missouri as a testing ground.\n\nCBS decided on Antiope and mounted a large market trial in Los Angeles in partnership with NBC and Public Broadcasting Service (PBS) Public television. Services premiered simultaneously on station KNXT (now KCBS-TV), KNBC and KCET in Los Angeles. All three services included an array of local news and information services. KCET's service also included service components for use in schools.\n\nLater, an official North American standard of teletext, called NABTS (North American Broadcast Teletext Specification) was developed in the early 1980s by Norpak, a Canadian company. NABTS provided improved graphic and text capability over WST, but was quite short-lived. This was mainly due to the expensive cost of NABTS decoders, costing in the thousands of dollars upon their release to the public. NABTS, however, was adopted for a short while by American TV networks NBC & CBS throughout the early-to-mid 80s, CBS using it for their short-lived ExtraVision teletext service, which premiered after the early Antiope & Ceefax trials by CBS & KNXT, and NBC, who had a NABTS-based service called NBC Teletext for a very short time in the mid-1980s. NBC discontinued their service in 1985 due to the cost of NABTS decoders not dropping to an affordable level for the consumer public.\n\nThe NABTS protocol received a revival of sorts in the late 90s, when it was used for the datacasting features of WebTV for Windows under Windows 98, and for Intel's now-defunct InterCast service (also for Windows as well), using a proper TV tuner card (such as the ATI All-In-Wonder or Hauppauge's Win-TV).\n\nWST was also used for a short time in the US, with services provided throughout the late 1970s and early 1980s by several regional American TV networks (such as the University of Wisconsin–Madison's \"Infotext\" service in the mid-1980s, which was carried on several TV stations across Wisconsin, and Agtext, provided by Kentucky Educational Television and carried on KET's stations, both services providing agriculturally oriented information) and major-market U.S. TV stations (such as \"Metrotext\", which was formerly carried on station KTTV in Los Angeles, and \"KeyFax\", formerly on WFLD in Chicago).\nPerhaps the most prominent of American teletext providers was the \"Electra\" teletext service, using WST, which was broadcast starting in the early 1980s on the vertical blanking interval (VBI) of the American cable channel WTBS. Electra was owned and operated by Taft Broadcasting and Satellite Syndicated Systems (SSS). Electra ran up until 1993, when it was shut down due to Zenith, the prominent (and only) American TV manufacturer at the time offering teletext features in their sets decided to discontinue such features, as well as a lack of funding and lagging interest in teletext by the American consumer.\n\nZenith manufactured models of television sets in the US in the 1980s, most notably their \"Digital System 3\" line, that had built-in WST teletext decoders as a feature, much like most British/European TV sets. Teletext services in the US like Electra could be received with one of these sets, but these were mostly more expensive higher-end sets offered by Zenith, possibly causing Electra (and American teletext in general) to never catch on with the public.\n\nAustralian company Dick Smith Electronics (DSE) also offered through their US distributors a set-top WST teletext decoder kit. The kit used as its core the same teletext decoding module (manufactured by UK electronics company Mullard) installed in most British TV sets, with additional circuitry to adapt it for American NTSC video, and to utilize it in a separate set-top box.\n\nA significant reason for the demise of American teletext was when Zenith introduced built-in closed captioning decoders in TVs in the early '90s, as mandated by the FCC. It was not practical for Zenith to re-design their TV chassis models that previously had teletext decoder support to have both teletext and closed captioning support. So Zenith decided to drop the teletext features, therefore ending teletext service in the US in the early 1990s, considering Zenith was the only major manufacturer of teletext-equipped sets in the United States.\n\nInterCast was a modern teletext-like system created by Intel in 1996, using a TV tuner card installed in a desktop PC running Windows with the InterCast Viewer software. The software would receive data representing HTML pages via the VBI (Vertical Blanking Interval) of a television channel's video, while displaying in a window in the InterCast software the TV channel itself. The HTML data received would then be displayed in another window in the Intercast software. It usually was extra supplemental information relevant to the TV program being viewed, such as extra clues for the viewer during a murder mystery show, or extra news headlines or extended weather forecasts during a newscast.\n\nNBC, as well as The Weather Channel, CNN and M2 (now MTV2), utilized InterCast technology to complement their programming. InterCast, however, fell into disuse, and Intel discontinued support of InterCast a few years later.\n\nAnother service in the US similar in delivery and content to teletext was the \"WaveTop\" service, provided and operated by the Wavephore Corporation. It used the same types of InterCast-compatible TV tuner cards, and used an application that ran under Windows, like InterCast. In fact, WaveTop software was also bundled with TV tuner cards that had InterCast software bundled with them as well.\n\nHowever, Wavetop was an independent service from InterCast, and wasn't a complementary service to a television program or channel like the latter. In fact, viewing television with a TV card was not possible while the WaveTop software was running, since the software utilized the TV tuner card as a full-time data receiver.\n\nWaveTop provided content from several different providers in the form of HTML pages displayed in the WaveTop software, such as news articles from the New York Times, weather information provided by The Weather Channel, and sports from ESPN. It also delivered short video clips, usually commercials, that could be viewed in the software as well.\n\nWhen it was in operation, WaveTop's data was delivered on the VBI of local public TV stations affiliated with PBS through their PBS National Datacast division, that the WaveTop software tuned the TV card to in order to receive the service.\n\nYet another service in the U.S. that relied on data delivery via the VBI like teletext, was the \"Guide+\" (\"Guide Plus\", also referred to as \"GuidePlus+\" as well) service provided and developed by Gemstar. There were several models of television sets made throughout the 90s by Thomson Consumer Electronics under the RCA and General Electric brands that had built-in Guide+ decoders. Guide+ was an on-screen interactive program guide that provided current TV schedule listings, as well as other information like news headlines. Some Guide+ equipped sets from RCA even had an IR-emitting sensor that could be plugged into the back of the TV, to control a VCR to record programs which could be selected from the on-screen Guide+ listings. In some ways, this was very similar to the Video Programming by Teletext|Video Programming by Teletext (VPT), Video Program System (VPS), and Programme Delivery Control (PDC) features of British/European teletext.\n\nGuide+ was a free service, supported by advertisements displayed on-screen in the Guide+ menu and listing screens, not unlike banner ads displayed on web pages. Guide+ was delivered over the VBI of select local American TV stations.\n\nGuide+ was discontinued by Gemstar in June 2004, and soon afterwards, Thomson dropped the Guide+ features from all RCA and GE television sets made afterward.\n\nHowever, Guide+ in the United States has now been replaced by Gemstar with a similar service (delivered in the same fashion via VBI like Guide+), called \"TV Guide On Screen\". A small number of televisions, DVD recorders, and digital video recorders are now being released with \"TV Guide On Screen\" capabilities. The Guide+ name & service is still used in Europe by Gemstar. (The same service is known in Japan as G-Guide).\n\nSimilar to Guide+ was \"Star Sight\", with its decoders built into TVs manufactured by Zenith, Samsung, Sony, Toshiba, Magnavox, and others. This was an electronic program guide service similar to Guide+, but was a service that relied on monthly subscription fees paid by the user, not from revenue gathered from on-screen advertisements like Guide+. Star Sight discontinued operations on July 21, 2003, due to a lack of subscribers to the service. Star Sight's data was also delivered on the VBI of local PBS stations through the PBS National Datacast division, much like how WaveTop was delivered as mentioned previously in this article.\n"}
{"id": "43245211", "url": "https://en.wikipedia.org/wiki?curid=43245211", "title": "Time base generator", "text": "Time base generator\n\nA time base generator, or timebase, is a special type of function generator, an electronic circuit that generates a varying voltage to produce a particular waveform. Time base generators produce very high frequency sawtooth waves specifically designed to deflect the beam in cathode ray tube (CRT) smoothly across the face of the tube and then return it to its starting position.\n\nTime bases are used by radar systems to determine range to a target, by comparing the current location along the time base to the time of arrival of radio echoes. Analog television systems using CRTs had two time bases, one for deflecting the beam horizontally in a rapid movement, and another pulling it down the screen 60 times per second. Oscilloscopes often have several time bases, but these may be more flexible function generators able to produce many waveforms as well as a simple time base.\n\nA cathode ray tube (CRT) consists of three primary parts, the electron gun that provides a stream of accelerated electrons, the phosphor-covered screen that lights up when the electrons hit it, and the deflection plates that use magnetic or electric fields to deflect the electrons in-flight and allows them to be directed around the screen. It is the ability for the electron stream to be rapidly moved using the deflection plates that allows the CRT to be used to display very rapid signals, like those of a television signal or to be used for radio direction finding (see huff-duff).The electrons are fired from cathode.Cathode is heated up with power supply and electrons as fired through them.The electrons flow is controlled by control grid.It is type of regulator for flow of electrons. \n\nMany signals of interest vary over time at a very rapid rate, but have an underlying periodic nature. Radio signals, for instance, have a base frequency, the carrier, which forms the basis for the signal. Sounds are modulated into the carrier by modifying the signal, either in amplitude (AM), frequency (FM) or similar techniques. To display such a signal on an oscilloscope for examination, it is desirable to have the electron beam sweep across the screen so that the electron beam cycles at the same frequency as the carrier, or some multiple of that base frequency.\n\nThis is the purpose of the time base generator, which is attached to one of the set of deflection plates, normally the X axis, while the amplified output of the radio signal is sent to the other axis, normally Y. The result is a visual re-creation of the original waveform.\n\nA typical radar system broadcasts a short pulse of radio signal and then listens for echoes from distant objects. As the signal travels at the speed of light and has to travel to the target object and back, the distance to the target can be determined by measuring the delay between the broadcast and reception, dividing the speed of light by that time, and then dividing by two (there and back again). As this process occurs very rapidly, a CRT is used to display the signal and look for the echoes.\n\nIn the simplest version of a radar display, today known as an \"A-scope\", a time base generator sweeps the display across the screen so that it reaches one side at the time when the signal has travelled the radar's maximum effective distance. For instance, an early warning radar like Chain Home (CH) might have a maximum range of , a distance that light will travel out and back in 1 millisecond. This would be used with a time base generator that pulls the beam across the CRT once every millisecond, starting the sweep when the broadcast signal ends. Any echoes cause the beam to deflect down (in the case of CH) as it moves across the display.\n\nBy measuring the physical location of the \"blip\" on the CRT, one can determine the range to the target. For instance, if a particular radar has a time base of 1 millisecond, then its maximum range is 150 km. If this is displayed on a four-inch CRT and the blip is measured to be 2 inches from the left side, then the target is 0.5 milliseconds away, or about .\n\nTo ensure the blips would line up properly with a mechanical scale, the time base could be adjusted to start its sweep at a certain time. This could be adjusted manually, or automatically trigged by another signal, normally a greatly attenuated version of the broadcast signal.\n\nLater systems modified the time base to include a second signal that periodically produced blips on the display, providing a clock signal that varied with the time base and thus did not need to be aligned. In UK terminology, these were known as \"strobes\".\n\nTelevision signals consist of a series of still images broadcast in sequence, in the NTSC standard such a \"frame\" is broadcast 30 times a second. Each frame is itself broken down into a series of \"lines\", 525 in the NTSC standard. If one examines a television broadcast on an oscilloscope, it will appear to be a continual sequence of modulated signals broken up by short periods of \"empty\" signal. Each modulated portion carries the analog image for a single line.\n\nTo display the signal, two time bases are used. One sweeps the beam horizontally from left to right at 15,750 times a second, the time it takes for one line to be sent. A second time base causes the beam to scan down the screen 60 times a second, so that each line appears below the last one drawn and then returns to the top. This causes the entire signal of 525 lines to be drawn down the screen, re-creating a 2-dimensional image.\n\nTo ensure the time base began its sweep of the screen at the right time, the signal included several special modulations. With each line there was a brief period, the \"front porch\" and \"back porch\" that caused the signal to go negative briefly. This triggered the horizontal time base to start its sweep across the screen, ensuring that the lines started on the left of the display. A much longer but otherwise similar signal, the vertical blanking interval caused the vertical time base to start, with any lengthy delay causing the time base to trigger.\n\n"}
{"id": "11889483", "url": "https://en.wikipedia.org/wiki?curid=11889483", "title": "Tomato knife", "text": "Tomato knife\n\nA tomato knife is a small serrated kitchen knife designed to slice through tomatoes. The serrated edge allows the knife to penetrate the tomatoes’ skin quickly and with a minimum of pressure without crushing the flesh. Many tomato knives have forked tips that allow the user to lift and move the tomato slices after they have been cut.\n\nSerrations are not required to cut tomatoes – a sharp straight blade works – but the serrations allow the knife to cut tomatoes and other foods even when dull. The reason for this being most of the cutting takes place in the serrations themselves. Some knives have serrations on both sides allowing easy slicing for both left-handed and right-handed users. Bread knives and steak knives are similarly serrated.\n\n\n"}
{"id": "2515660", "url": "https://en.wikipedia.org/wiki?curid=2515660", "title": "Universal polar stereographic coordinate system", "text": "Universal polar stereographic coordinate system\n\nThe universal polar stereographic (UPS) coordinate system is used in conjunction with the universal transverse Mercator (UTM) coordinate system to locate positions on the surface of the earth. Like the UTM coordinate system, the UPS coordinate system uses a metric-based cartesian grid laid out on a conformally projected surface. UPS covers the Earth's polar regions, specifically the areas north of 84°N and south of 80°S, which are not covered by the UTM grids, plus an additional 30 minutes of latitude extending into UTM grid to provide some overlap between the two systems.\n\nIn the polar regions, directions can become complicated, with all geographic north–south lines converging at the poles. The difference between UPS grid north and true north can therefore be anything up to 180°—in some places, grid north is true south, and vice versa. UPS grid north is arbitrarily defined as being along the prime meridian in the Antarctic and the 180th meridian in the Arctic; thus, east and west on the grids when moving directly away from the pole are along the 90°E and 90°W meridians respectively.\n\nAs the name indicates, the UPS system uses a stereographic projection. Specifically, the projection used in the system is a secant version based on an elliptical model of the earth. The scale factor at each pole is adjusted to 0.994 so that the latitude of true scale is 81.11451786859362545° (about 81° 06' 52.3\") North and South. The scale factor inside the regions at latitudes higher than this parallel is too small, whereas the regions at latitudes below this line have scale factors that are too large, reaching 1.0016 at 80° latitude.\n\nThe scale factor at the origin (the poles) is adjusted to minimize the overall distortion of scale within the mapped region. As with the Mercator projection, the region near the tangent (or secant) point on a Stereographic map remains very close to true scale for an angular distance of a few degrees. In the ellipsoidal model, a stereographic projection tangent to the pole has a scale factor of less than 1.003 at 84° latitude and 1.008 at 80° latitude. The adjustment of the scale factor in the UPS projection reduces the average scale distortion over the entire zone.\n\n\n"}
{"id": "57187680", "url": "https://en.wikipedia.org/wiki?curid=57187680", "title": "Volcanoes: Life on the Edge", "text": "Volcanoes: Life on the Edge\n\n\"Volcanoes: Life on the Edge\", Corbis gained a reputation for high quality work. MacUser praised the well-organised interface while Popular Science enjoyed the \"stunning\" photographs. MacWorld gave the game 7.2/10.\n"}
{"id": "319613", "url": "https://en.wikipedia.org/wiki?curid=319613", "title": "Voltage spike", "text": "Voltage spike\n\nIn electrical engineering, spikes are fast, short duration electrical transients in voltage (voltage spikes), current (current spikes), or transferred energy (energy spikes) in an electrical circuit.\n\nFast, short duration electrical transients (overvoltages) in the electric potential of a circuit are typically caused by\n\nIn the design of critical infrastructure and military hardware, one concern is of pulses produced by nuclear explosions, whose nuclear electromagnetic pulses distribute large energies in frequencies from 1 kHz into the gigahertz range through the atmosphere.\n\nThe effect of a voltage spike is to produce a corresponding increase in current (current spike). However some voltage spikes may be created by current sources. Voltage would increase as necessary so that a constant current will flow. Current from a discharging inductor is one example.\n\nFor sensitive electronics, excessive current can flow if this voltage spike exceeds a material's breakdown voltage, or if it causes avalanche breakdown. In semiconductor junctions, excessive electric current may destroy or severely weaken that device. An avalanche diode, transient voltage suppression diode, transil, varistor, overvoltage crowbar, or a range of other overvoltage protective devices can divert (shunt) this transient current thereby minimizing voltage.\n\nVoltage spikes, also known as surges, may be created by a rapid buildup or decay of a magnetic field, which may induce energy into the associated circuit. However voltage spikes can also have more mundane causes such as a fault in a transformer or higher-voltage (primary circuit) power wires falling onto lower-voltage (secondary circuit) power wires as a result of accident or storm damage.\n\nVoltage spikes may be longitudinal (common) mode or metallic (normal or differential) mode. Some equipment damage from surges and spikes can be prevented by use of surge protection equipment. Each type of spike requires selective use of protective equipment. For example, a common mode voltage spike may not even be detected by a protector installed for normal mode transients.\n\nPower increases or decreases which last multiple cycles are called swells or sags, respectively. An uninterrupted voltage increase that lasts more than a minute is called an overvoltage. These are usually caused by malfunctions of the electric power distribution system.\n\n"}
{"id": "2777818", "url": "https://en.wikipedia.org/wiki?curid=2777818", "title": "WABAC machine", "text": "WABAC machine\n\nThe WABAC Machine or Wayback Machine is a fictional time machine from the segment \"Peabody's Improbable History\", a recurring feature of the 1960s cartoon series \"The Rocky and Bullwinkle Show\". The WABAC Machine is a plot device used to transport the characters Mr. Peabody and Sherman back in time to visit important events in human history.\n\nThe WABAC machine was a central element of the \"Peabody's Improbable History\" cartoon segment. The machine was invented by Mr. Peabody, a genius, polymath, and bow tie-wearing beagle, as a birthday gift for his adopted pet boy, Sherman. By allowing them to visit famous historical people or events, the WABAC provided educational adventures for Sherman. At the request of Mr. Peabody (\"Sherman, set the WABAC machine to ...\"), Sherman\nwould set the WABAC controls to a time and place of historical importance, and by walking through a door\nin the WABAC machine, they would be instantly transported there. Examples of places or people visited are\nthe Marquess of Queensberry and the rules of boxing, the imprisonment and memoirs of Casanova, and Jim Bowie and the Bowie knife. The machine apparently later returned Mr. Peabody and Sherman to the present, although the return trip was never shown. The segment traditionally ended with a pun.\n\nThe WABAC has two main quirks. Firstly, it automatically translates all languages into English for their convenience. Secondly (and more critically), the historical figures and situations that they encounter are distorted in some crucial way. The main focus of the shorts is thus the restoration of historical events to their proper course, albeit in a characteristically frivolous and anachronistic way.\n\nEither of the names WABAC or Wayback are in common usage, with the term \"WAYBACK\" explicitly indicated during the segment in which Mr. Peabody and Sherman visit the \"Charge of the Light Brigade\". The precise meaning of the acronym WABAC is unknown. According to Gerard Baldwin, one of the show's directors, the name \"WABAC\" is a reference to the UNIVAC I. Mid-century, large-sized computers often had names that ended in \"AC\" (generally for \"Automatic/Analogue Computer\" or similar), such as ENIAC or UNIVAC. The term \"Wayback\" suggests the common expression \"way back in [some former time].\"\n\nThe concept or term \"Wayback Machine\" has been adopted in popular culture as a convenient way to introduce issues or events of the past, often employing the original line \"Sherman, set the Wayback machine to ...\". This introduction was used by the character Kevin Flynn in the film \"Tron\", for example. As in the original cartoon, the Wayback Machine is often invoked to suggest the audience follow the narrator back to the past. Frequently such visits to the past are trips of nostalgia, remembering times, places, or things of the not-so-distant past.\n\nOne example of popular usage occurred in a 1995 episode of the television show \"NewsRadio\", \"Goofy Ball\" (Season 2, Episode 2), in which station owner Jimmy James (Stephen Root) says: \"Dave, don't mess with a man with a Wayback Machine. I can make it so you were never born.\"\n\nThe Wayback Machine of the Internet Archive was named after the WABAC.\n\nThe WABAC machine is frequently referenced in the \"Stuff You Should Know\" podcast when discussing events in the past.\n\nThe movie studio DreamWorks Animation announced in 2006 and again in 2012 that they were creating an animated movie entitled \"Mr. Peabody & Sherman\", which was released March 7, 2014. The WABAC machine is a central element to the plot. In the movie, the acronym is revealed to be Wavelength Acceleration Bidirectional Asynchronous Controller (WABAC).\n\n\n"}
{"id": "54016126", "url": "https://en.wikipedia.org/wiki?curid=54016126", "title": "Yoola", "text": "Yoola\n\nYoola is a creator-to-consumer media and development company, and a YouTube Multi-Channel Network (MCN) ranked in the top 5 most viewed globally with more than 8 billion monthly views.\nYoola works with international creators and brands, providing them tools and services to develop, grow, and commercialize new and existing content, products, events and experiences.\n\nYoola was founded in 2011 by Artyom (Arik) Geller, Michael Shaposhnikov, Alexander Shaposhnikov and Ilan Troyanovsky, originally focused on creating a single content channel focused on fashion and lifestyle. After gaining viewership, Yoola shifted to become an MCN, working with and managing various other content channels.\n\nYoola’s CEO, Eyal Baumel, was previously co-founder of Bites.tv, a voting-based participation platform that served more than 1 billion questions; created by Fox Sports, Miami Heat, American Music Awards, Billboard Music Awards, Linkin Park, and VegasInsider, as well as influencers such as Justin Bieber, Usher, and Ariana Grande via the Bkstg app, and international TV shows like Master Chef and Big Brother.\n\nYoola has offices in Los Angeles, Beijing, Haifa, Kiev, St. Petersburg and Moscow.\n\nYoola works with over 5,000+ content creators and rights holders to develop, manage, produce and distribute multi-channel content.\n\nAs a distribution platform,Yoola focuses on globalization of content by partnering with creators to penetrate emerging markets (e.g. China and Russia). With teams in Moscow and Beijing and through partnerships with local platforms, networks, influencers and brands, Yoola localizes, manages, promotes, and monetizes content across local leading social and video networks such as Sina Weibo, Tencent, Youku Tudou, Toutiao, Miapoi, Kuaishou, Bilibili, Meipai, Ok.ru and VK (social networking).\n\n"}
