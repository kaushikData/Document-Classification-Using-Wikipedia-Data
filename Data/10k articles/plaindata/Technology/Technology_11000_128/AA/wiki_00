{"id": "11005995", "url": "https://en.wikipedia.org/wiki?curid=11005995", "title": "Agricultural robot", "text": "Agricultural robot\n\nAn agricultural robot is a robot deployed for agricultural purposes. The main area of application of robots in agriculture today is at the harvesting stage. Emerging applications of robots or drones in agriculture include weed control, cloud seeding, planting seeds, harvesting, environmental monitoring and soil analysis.\n\nFruit picking robots, driverless tractor / sprayers, and sheep shearing robots are designed to replace human labor. \nIn most cases, a lot of factors have to be considered (e.g., the size and color of the fruit to be picked) before the commencement of a task. \nRobots can be used for other horticultural tasks such as pruning, weeding, spraying and monitoring. \nRobots can also be used in livestock applications (livestock robotics) such as automatic milking, washing and castrating. Robots like these have many benefits for the agricultural industry, including a higher quality of fresh produce, lower production costs, and a decreased need for manual labor. They can also be used to automate manual tasks, such as weed or bracken spraying, where the use of tractors and other manned vehicles is too dangerous for the operators.\n\nThe mechanical design consists of an end effector, manipulator, and gripper. Several factors must be considered in the design of the manipulator, including the task, economic efficiency, and required motions. The end effector influences the market value of the fruit and the gripper's design is based on the crop that is being harvested.\n\nAn end effector in an agricultural robot is the device found at the end of the robotic arm, used for various agricultural operations. Several different kinds of end effectors have been developed. In an agricultural operation involving grapes in Japan, end effectors are used for harvesting, berry-thinning, spraying, and bagging. Each was designed according to the nature of the task and the shape and size of the target fruit. For instance, the end effectors used for harvesting were designed to grasp, cut, and push the bunches of grapes.\n\nBerry thinning is another operation performed on the grapes, and is used to enhance the market value of the grapes, increase the grapes' size, and facilitate the bunching process. For berry thinning, an end effector consists of an upper, middle, and lower part. The upper part has two plates and a rubber that can open and close. The two plates compress the grapes to cut off the rachis branches and extract the bunch of grapes. The middle part contains a plate of needles, a compression spring, and another plate which has holes spread across its surface. When the two plates compress, the needles punch holes through the grapes. Next, the lower part has a cutting device which can cut the bunch to standardize its length.\n\nFor spraying, the end effector consists of a spray nozzle that is attached to a manipulator. In practice, producers want to ensure that the chemical liquid is evenly distributed across the bunch. Thus, the design allows for an even distribution of the chemical by making the nozzle to move at a constant speed while keeping distance from the target.\n\nThe final step in grape production is the bagging process. The bagging end effector is designed with a bag feeder and two mechanical fingers. In the bagging process, the bag feeder is composed of slits which continuously supply bags to the fingers in an up and down motion. While the bag is being fed to the fingers, two leaf springs that are located on the upper end of the bag hold the bag open. The bags are produced to contain the grapes in bunches. Once the bagging process is complete, the fingers open and release the bag. This shuts the leaf springs, which seals the bag and prevents it from opening again.\n\nThe gripper is a grasping device that is used for harvesting the target crop. Design of the gripper is based on simplicity, low cost, and effectiveness. Thus, the design usually consists of two mechanical fingers that are able to move in synchrony when performing their task. Specifics of the design depend on the task that is being performed. For example, in a procedure that required plants to be cut for harvesting, the gripper was equipped with a sharp blade.\n\nThe manipulator allows the gripper and end effector to navigate through their environment. The manipulator consists of four-bar parallel links that maintain the gripper's position and height. The manipulator also can utilize one, two, or three pneumatic actuators. Pneumatic actuators are motors which produce linear and rotary motion by converting compressed air into energy. The pneumatic actuator is the most effective actuator for agricultural robots because of its high power-weight ratio. The most cost efficient design for the manipulator is the single actuator configuration, yet this is the least flexible option.\n\nThe first development of robotics in agriculture can be dated as early as the 1920s, with research to incorporate automatic vehicle guidance into agriculture beginning to take shape. This research led to the advancements between the 1950s and 60s of autonomous agricultural vehicles. The concept was not perfect however, with the vehicles still needing a cable system to guide their path. Robots in agriculture continued to develop as technologies in other sectors began to develop as well. It was not until the 1980s, following the development of the computer, that machine vision guidance became possible.\n\nOther developments over the years included the harvesting of oranges using a robot both in France and the US.\n\nWhile robots have been incorporated in indoor industrial settings for decades, outdoor robots for the use of agriculture are considered more complex and difficult to develop. This is due to concerns over safety, but also over the complexity of picking crops subject to different environmental factors and unpredictability.\n\nThere are concerns over the amount of labor the agricultural sector needs. With an aging population, Japan is unable to meet the demands of the agricultural labor market. Similarly, the United State currently depends on a large number of immigrant workers, but between the decrease in seasonal farmworkers and increased efforts to stop immigration by the government, they too are unable to meet the demand. Businesses are often forced to let crops rot due to an inability to pick them all by the end of the season. Additionally, there are concerns over the growing population that will need to be fed over the next years. Because of this, there is a large desire to improve agricultural machinery to make it more cost efficient and viable for continued use.\n\nMuch of the current research continues to work towards autonomous agricultural vehicles. This research is based on the advancements made in driver-assist systems and self-driving cars.\n\nWhile robots have already been incorporated in many areas of agricultural farm work, they are still largely missing in the harvest of various crops. This has started to change as companies begin to develop robots that complete more specific tasks on the farm. The biggest concern over robots harvesting crops comes from harvesting soft crops such as strawberries which can easily be damaged or missed entirely. Despite these concerns, progress in this area is being made. According to Gary Wishnatzki, the co-founder of Harvest Croo Robotics, one of their strawberry pickers currently being tested in Florida can \"pick a 25-acre field in just three days and replace a crew of about 30 farm workers\". Similar progress is being made in harvesting apples, grapes, and other crops.\n\nAnother goal being set by agricultural companies involves the collection of data. There are rising concerns over the growing population and the decreasing labor available to feed them. Data collection is being developed as a way to increase productivity on farms. AgriData is currently developing new technology to do just this and help farmers better determine the best time to harvest their crops by scanning fruit trees.\n\nRobots have many fields of application in agriculture. Some examples and prototypes of robots include the Merlin Robot Milker, Rosphere, Harvest Automation, Orange Harvester, lettuce bot, and weeder. One case of a large scale use of robots in farming is the milk bot. It is widespread among British dairy farms because of its efficiency and nonrequirement to move. According to David Gardner (chief executive of the Royal Agricultural Society of England), a robot can complete a complicated task if its repetitive and the robot is allowed to sit in a single place. Furthermore, robots that work on repetitive tasks (e.g. milking) fulfill their role to a consistent and particular standard.\n\nAnother field of application is horticulture. One horticultural application is the development of RV100 by Harvest Automation Inc. RV 100 is designed to transport potted plants in a greenhouse or outdoor setting. The functions of RV100 in handling and organizing potted plants include spacing capabilities, collection, and consolidation. The benefits of using RV100 for this task include high placement accuracy, autonomous outdoor and indoor function, and reduced production costs.\n\n\n"}
{"id": "26572302", "url": "https://en.wikipedia.org/wiki?curid=26572302", "title": "Air Force Satellite Communications", "text": "Air Force Satellite Communications\n\nThe United States military's Air Force Satellite Communications (AFSATCOM) is a network of ground and space systems to allow rapid dissemination of communications to a worldwide audience. AFSATCOM's creation was during the height of the Cold War to guarantee that Emergency Action Messages would be received by Strategic Air Command nuclear forces.\n\nAFSATCOM operations used leased transponders off United States Navy Fleet Satellite Communications (FLTSATCOM) satellites for EAM transmission.\n\n"}
{"id": "34286697", "url": "https://en.wikipedia.org/wiki?curid=34286697", "title": "Alert correlation", "text": "Alert correlation\n\nAlert correlation is a type of long analysis. It focuses on the process of clustering alerts (events), generated by NIDS and HIDS computer systems, to form higher-level pieces of information.\n\nExample of simple alert correlation is grouping invalid login attempts to report single incident like \"10000 invalid login attempts on host X\".\n\n"}
{"id": "37919486", "url": "https://en.wikipedia.org/wiki?curid=37919486", "title": "Anti-griddle", "text": "Anti-griddle\n\nThe anti-griddle is a kitchen appliance that flash freezes or semi-freezes foods placed on its chilled metal top. The device was inspired by a similar appliance used by Grant Achatz in one of his restaurants.\n\nChef and \"Top Chef\" guest judge Grant Achatz used a similar device in his first Chicago restaurant Alinea, which he invented with the help of culinary technologist Philip Preston. The device is about the size of a microwave oven. He collaborated with the company Polyscience to mass-produce the anti-griddle for use at homes and other restaurants.\n\nThe anti-griddle maintains a constant temperature of -30°F (c. -34.4°C) by pumping a refrigerant through a compressor to remove heat from its steel surface. Liquids, oil, and gels generally freeze in 30 to 90 seconds. The finished product has a crunchy outer texture while the inside remains soft or creamy.\n\n"}
{"id": "4280305", "url": "https://en.wikipedia.org/wiki?curid=4280305", "title": "Atomic Energy Act of 1954", "text": "Atomic Energy Act of 1954\n\nThe Atomic Energy Act of 1954, 42 U.S.C. §§ 2011-2021, 2022-2286i, 2296a-2297h-13, is a United States federal law that is, according to the Nuclear Regulatory Commission, \"the fundamental U.S. law on both the civilian and the military uses of nuclear materials.\" It covers the laws for the development, regulation, and disposal of nuclear materials and facilities in the United States.\n\nIt was an amendment to the Atomic Energy Act of 1946 and substantially refined certain aspects of the law, including increased support for the possibility of a civilian nuclear industry. Notably it made it possible for the government to allow private companies to gain technical information (Restricted Data) about nuclear energy production and the production of fissile materials, allowing for greater exchange of information with foreign nations as part of President Dwight D. Eisenhower's Atoms for Peace program, and reversed certain provisions in the 1946 law which had made it impossible to patent processes for generating nuclear energy or fissile materials.\n\nThe H.R. 9757 legislation was passed by the 83rd U.S. Congressional session and signed into law by President Dwight Eisenhower on August 30, 1954.\n\n\n"}
{"id": "7402971", "url": "https://en.wikipedia.org/wiki?curid=7402971", "title": "B-MAC", "text": "B-MAC\n\nB-MAC is a form of analog video encoding, specifically a type of (Multiplexed Analogue Components (MAC) encoding. MAC encoding was designed in the mid 80s for use with Direct Broadcast Satellite systems. Other analog video encoding systems include NTSC, PAL and SECAM. Unlike the FDM method used in those, MAC encoding uses a TDM method. B-MAC was a proprietary MAC encoding used by Scientific-Atlanta for encrypting broadcast video services; the full name was \"Multiple Analogue Component, Type B\".\n\nB-MAC uses teletext-style non-return-to-zero (NRZ) signaling with a capacity of 1.625 Mbit/s. The video and audio/data signals are therefore combined at baseband.\n\nUser base (PAL/NTSC zones)\n\nMAC transmits luminance and chrominance data separately in time rather than separately in frequency (as other analog television formats do, such as composite video).\n\nAudio and Scrambling (selective access)\n\n\n"}
{"id": "3287513", "url": "https://en.wikipedia.org/wiki?curid=3287513", "title": "BBC Research &amp; Development", "text": "BBC Research &amp; Development\n\nBBC Research & Development is the national technical research department of the BBC.\n\nIt has responsibility for researching and developing advanced and emerging media technologies for the benefit of the corporation, and wider UK and European media industries, and is also the technical design authority for a number of major technical infrastructure transformation projects for the UK broadcasting industry.\n\nBBC R&D is part of the wider BBC Design & Engineering, and is led by Andy Conroy, Controller Research & Development. In 2011, the North Lab moved into MediaCityUK in Salford along with several other departments of the BBC, whilst the South Lab remained in White City in London.\n\nThe department as it stands today was formed in 1993 from the merger of the BBC Designs Department and the BBC Research Department. From 2006 to 2008 it was known as Research and Innovation but has since reverted to its original name. BBC Research & Development has made major contributions to broadcast technology, carrying out original research in many areas, and developing items like the peak programme meter (PPM) which became the basis for many world standards.\n\nIt has also been involved in many well known consumer technologies such as teletext, DAB, NICAM and Freeview. It was at the forefront of development of FM radio, stereo FM, and RDS. These innovations have led to Queen's Awards for Innovation in 1969, 1974, 1983, 1987, 1992, 1998, 2001 and 2011.\n\nIn the 1970s, its engineers designed the famous LS3/5A studio monitor for use in outside broadcasting units. Licensed to manufacturers, the loudspeaker sold 100,000 pairs in its 20+ years' life.\n\nIn early 2010 the department had approximately 135 staff based at three locations: White City in London, Kingswood Warren in Kingswood, Surrey, and the R&D (North Lab) at the BBC's Manchester offices at New Broadcasting House, Oxford Road, Manchester. In early 2010 the Kingswood Warren site was vacated and the bulk of the department relocated to Centre House, in White City, London co-locating with the main campus of the BBC in London, whilst a significant number have moved to the new North Lab in MediaCityUK in Salford.\n\nBBC R&D engineers and researchers are currently active on approximately 50 projects, including 7 active national and international collaborative research efforts.\n\nThese include R&D projects built around BBC Redux—the proof of concept for the cross-platform, Flash video-based streaming version of the BBC iPlayer.\n\n\n\n"}
{"id": "45705954", "url": "https://en.wikipedia.org/wiki?curid=45705954", "title": "Bristol Temple Quarter Enterprise Zone", "text": "Bristol Temple Quarter Enterprise Zone\n\nBristol Temple Quarter Enterprise Zone is an enterprise zone in Bristol, England, focused on creative, high-tech and low-carbon industries. Covering an area of , it is based around Bristol Temple Meads railway station, which is being redeveloped by Network Rail. It also contains the area around the existing Temple Quay development, and the Silverthorne Lane and Avon Riverside areas. It includes the site of the planned Bristol Arena, and the site of the University of Bristol's planned Temple Quarter Campus.\n\nThe creation of the zone was announced by Chancellor George Osborne in the Budget of March 2011, and it was launched in 2012. The zone offers streamlined planning controls and reduced business rates. Rates generated by the zone are channelled to five other areas in the region, designated Enterprise Areas. These are Avonmouth (focusing on manufacturing and distribution), Bath (media and publishing), Emersons Green including Bristol and Bath Science Park, Filton (high-tech) and Weston-super-Mare (business services). Development of the enterprise zone is coordinated by West of England Local Enterprise Partnership, Bristol City Council, the Homes and Communities Agency (HCA) and Network Rail.\n\nNetwork Rail is redeveloping the station, in conjunction with its electrification of the Great Western line which will cut the journey time from Temple Meads to London Paddington station to 80 minutes. Station Approach Road will be turned into a public square and the station's main entrance moved to the north side of the station. In January 2015, the council announced changes to the layout of the roads around the station, with the removal of Temple Circus roundabout and provision of better routes for pedestrians and cyclists.\n\nAcross Station Approach from the station, the Temple Gate redevelopment by TCN UK is a creative and digital campus for small and medium-sized enterprises. Redeveloped buildings include Bristol and Exeter House and Temple Studios. The latter opened in 2013. \nEngine Shed is a new use for part of the Old Station, Brunel's original building, in partnership with the University of Bristol. Opened in December 2013, it hosts business incubators, including SETsquared and WebStart Bristol, which support a cluster of hi-tech startup companies. Engine Shed leases its office space to the incubators, who in turn sub-let space on more flexible terms to the companies, which are selected for their high growth potential, and high expected benefits for the regional economy. In December 2015, with the building fully occupied, additional space was made available in the form of Boxworks, which was rapidly constructed by ForwardSpace next to Engine Shed using 20 shipping containers. This was intended to be a temporary solution until an Engine Shed 2 is built. Public consultation for Engine Shed 2 took place in November 2016.\n\nIn November 2016, the University of Bristol announced that it will build a £300 million Temple Quarter Campus for c. 5,000 students, directly to the east of the station. It will replace an empty sorting office building, formerly operated by Royal Mail but derelict since 1997. The campus, which will include a new business school, digital research facilities and a student village, is expected to open in 2021. It will also host Engine Shed 2.\n\nThe zone's biggest development site, sometimes known as 'Arena Island', is south of the station and across the River Avon. Formerly occupied by the Bristol Bath Road depot, the site was acquired by the Homes and Communities Agency. In February 2014 Bristol Council agreed the financing of the arena, which including funding provided by the West of England Local Enterprise Partnership totals £91 million. The winner of the competition to design the arena, the architectural firm Populous, was announced in March 2015. In the same month, the HCA gave the arena site to Bristol City Council.\n\nIn 2013 the HCA agreed to fund an £11 million road bridge over the river, to link Cattle Market Road to the site of the planned arena. Construction of the bridge took place from March to September 2015. It has lanes for cars, bicycles and pedestrians. In March 2016, the bridge was named Brock's Bridge, after William Brock (1830–1907), a local builder and entrepreneur.\n\nTemple Quay is an area of mixed-use development on a site to the northwest of the station, where the station's goods yard was formerly located. The development project was initiated in 1989 by Bristol Development Corporation, who originally called it Quay Point. In 1995, the corporation transferred its rights on the site to English Partnerships, and development started in 1998. The developer was Castlemore Securities. By 2002, the development south of Bristol Floating Harbour was largely complete and a new phase was started on the other side of the harbour, called Temple Quay North. Castlemore went into administration in 2009, but development continued in the hands of the administrator, PricewaterhouseCoopers.\n\nIn 2015, Entrepreneurial Spark, a UK-wide business incubator network, opened a hub on the top floor of the Royal Bank of Scotland's Trinity Quay building in Temple Quay North. Managed by NatWest, the hub provides free space, facilities and guidance for startups.\n\nThe Avon Riverside area extends along the A4 Bath Road, and Bristol's so-called 'media mile', as far as the site of Paintworks, an existing mixed-use development by Verve. Phase III of Paintworks will be the development of a 'creative skills hub' for digital and media businesses, in partnership with Creative Skillset.\n\n"}
{"id": "6804", "url": "https://en.wikipedia.org/wiki?curid=6804", "title": "Charge-coupled device", "text": "Charge-coupled device\n\nA charge-coupled device (CCD) is a device for the movement of electrical charge, usually from within the device to an area where the charge can be manipulated, for example conversion into a digital value. This is achieved by \"shifting\" the signals between stages within the device one at a time. CCDs move charge between capacitive \"bins\" in the device, with the shift allowing for the transfer of charge between bins.\n\nIn recent years CCD has become a major technology for digital imaging. In a CCD image sensor, pixels are represented by p-doped metal-oxide-semiconductors (MOS) capacitors. These capacitors are biased above the threshold for inversion when image acquisition begins, allowing the conversion of incoming photons into electron charges at the semiconductor-oxide interface; the CCD is then used to read out these charges. Although CCDs are not the only technology to allow for light detection, CCD image sensors are widely used in professional, medical, and scientific applications where high-quality image data are required. In applications with less exacting quality demands, such as consumer and professional digital cameras, active pixel sensors, also known as complementary metal-oxide-semiconductors (CMOS) are generally used; the large quality advantage CCDs enjoyed early on has narrowed over time.\n\nThe charge-coupled device was invented in 1969 in the United States at AT&T Bell Labs by Willard Boyle and George E. Smith.\nThe lab was working on semiconductor bubble memory when Boyle and Smith conceived of the design of what they termed, in their notebook, \"Charge 'Bubble' Devices\".\nThe device could be used as a shift register. The essence of the design was the ability to transfer charge along the surface of a semiconductor from one storage capacitor to the next. The concept was similar in principle to the bucket-brigade device (BBD), which was developed at Philips Research Labs during the late 1960s. The first patent () on the application of CCDs to imaging was assigned to Michael Tompsett.\n\nThe initial paper describing the concept listed possible uses as a memory, a delay line, and an imaging device. The first experimental device demonstrating the principle was a row of closely spaced metal squares on an oxidized silicon surface electrically accessed by wire bonds.\n\nThe first working CCD made with integrated circuit technology was a simple 8-bit shift register. This device had input and output circuits and was used to demonstrate its use as a shift register and as a crude eight pixel linear imaging device.\nDevelopment of the device progressed at a rapid rate. By 1971, Bell researchers led by Michael Tompsett were able to capture images with simple linear devices.\nSeveral companies, including Fairchild Semiconductor, RCA and Texas Instruments, picked up on the invention and began development programs. Fairchild's effort, led by ex-Bell researcher Gil Amelio, was the first with commercial devices, and by 1974 had a linear 500-element device and a 2-D 100 x 100 pixel device. Steven Sasson, an electrical engineer working for Kodak, invented the first digital still camera using a Fairchild CCD in 1975. The first KH-11 KENNEN reconnaissance satellite equipped with charge-coupled device array ( pixels) technology for imaging was launched in December 1976. Under the leadership of Kazuo Iwama, Sony also started a large development effort on CCDs involving a significant investment. Eventually, Sony managed to mass-produce CCDs for their camcorders. Before this happened, Iwama died in August 1982; subsequently, a CCD chip was placed on his tombstone to acknowledge his contribution.\n\nIn January 2006, Boyle and Smith were awarded the National Academy of Engineering Charles Stark Draper Prize, and in 2009 they were awarded the Nobel Prize for Physics, for their invention of the CCD concept.\nMichael Tompsett was awarded the 2010 National Medal of Technology and Innovation for pioneering work and electronic technologies including the design and development of the first charge coupled device (CCD) imagers. He was also awarded the 2012 IEEE Edison Medal \"For pioneering contributions to imaging devices including CCD Imagers, cameras and thermal imagers\".\n\nIn a CCD for capturing images, there is a photoactive region (an epitaxial layer of silicon), and a transmission region made out of a shift register (the CCD, properly speaking).\n\nAn image is projected through a lens onto the capacitor array (the photoactive region), causing each capacitor to accumulate an electric charge proportional to the light intensity at that location. A one-dimensional array, used in line-scan cameras, captures a single slice of the image, whereas a two-dimensional array, used in video and still cameras, captures a two-dimensional picture corresponding to the scene projected onto the focal plane of the sensor. Once the array has been exposed to the image, a control circuit causes each capacitor to transfer its contents to its neighbor (operating as a shift register). The last capacitor in the array dumps its charge into a charge amplifier, which converts the charge into a voltage. By repeating this process, the controlling circuit converts the entire contents of the array in the semiconductor to a sequence of voltages. In a digital device, these voltages are then sampled, digitized, and usually stored in memory; in an analog device (such as an analog video camera), they are processed into a continuous analog signal (e.g. by feeding the output of the charge amplifier into a low-pass filter), which is then processed and fed out to other circuits for transmission, recording, or other processing.\n\nBefore the MOS capacitors are exposed to light, they are biased into the depletion region; in n-channel CCDs, the silicon under the bias gate is slightly \"p\"-doped or intrinsic. The gate is then biased at a positive potential, above the threshold for strong inversion, which will eventually result in the creation of a \"n\" channel below the gate as in a MOSFET. However, it takes time to reach this thermal equilibrium: up to hours in high-end scientific cameras cooled at low temperature. Initially after biasing, the holes are pushed far into the substrate, and no mobile electrons are at or near the surface; the CCD thus operates in a non-equilibrium state called deep depletion.\nThen, when electron–hole pairs are generated in the depletion region, they are separated by the electric field, the electrons move toward the surface, and the holes move toward the substrate. Four pair-generation processes can be identified:\n\nThe last three processes are known as dark-current generation, and add noise to the image; they can limit the total usable integration time. The accumulation of electrons at or near the surface can proceed either until image integration is over and charge begins to be transferred, or thermal equilibrium is reached. In this case, the well is said to be full. The maximum capacity of each well is known as the well depth, typically about 10 electrons per pixel.\n\nThe photoactive region of a CCD is, generally, an epitaxial layer of silicon. It is lightly \"p\" doped (usually with boron) and is grown upon a substrate material, often p++. In buried-channel devices, the type of design utilized in most modern CCDs, certain areas of the surface of the silicon are ion implanted with phosphorus, giving them an n-doped designation. This region defines the channel in which the photogenerated charge packets will travel. Simon Sze details the advantages of a buried-channel device:\nThis thin layer (= 0.2–0.3 micron) is fully depleted and the accumulated photogenerated charge is kept away from the surface. This structure has the advantages of higher transfer efficiency and lower dark current, from reduced surface recombination. The penalty is smaller charge capacity, by a factor of 2–3 compared to the surface-channel CCD. The gate oxide, i.e. the capacitor dielectric, is grown on top of the epitaxial layer and substrate.\n\nLater in the process, polysilicon gates are deposited by chemical vapor deposition, patterned with photolithography, and etched in such a way that the separately phased gates lie perpendicular to the channels. The channels are further defined by utilization of the LOCOS process to produce the channel stop region.\n\nChannel stops are thermally grown oxides that serve to isolate the charge packets in one column from those in another. These channel stops are produced before the polysilicon gates are, as the LOCOS process utilizes a high-temperature step that would destroy the gate material. The channel stops are parallel to, and exclusive of, the channel, or \"charge carrying\", regions.\n\nChannel stops often have a p+ doped region underlying them, providing a further barrier to the electrons in the charge packets (this discussion of the physics of CCD devices assumes an electron transfer device, though hole transfer is possible).\n\nThe clocking of the gates, alternately high and low, will forward and reverse bias the diode that is provided by the buried channel (n-doped) and the epitaxial layer (p-doped). This will cause the CCD to deplete, near the p–n junction and will collect and move the charge packets beneath the gates—and within the channels—of the device.\n\nCCD manufacturing and operation can be optimized for different uses. The above process describes a frame transfer CCD. While CCDs may be manufactured on a heavily doped p++ wafer it is also possible to manufacture a device inside p-wells that have been placed on an n-wafer. This second method, reportedly, reduces smear, dark current, and infrared and red response. This method of manufacture is used in the construction of interline-transfer devices.\n\nAnother version of CCD is called a peristaltic CCD. In a peristaltic charge-coupled device, the charge-packet transfer operation is analogous to the peristaltic contraction and dilation of the digestive system. The peristaltic CCD has an additional implant that keeps the charge away from the silicon/silicon dioxide interface and generates a large lateral electric field from one gate to the next. This provides an additional driving force to aid in transfer of the charge packets.\n\nThe CCD image sensors can be implemented in several different architectures. The most common are full-frame, frame-transfer, and interline. The distinguishing characteristic of each of these architectures is their approach to the problem of shuttering.\n\nIn a full-frame device, all of the image area is active, and there is no electronic shutter. A mechanical shutter must be added to this type of sensor or the image smears as the device is clocked or read out.\n\nWith a frame-transfer CCD, half of the silicon area is covered by an opaque mask (typically aluminum). The image can be quickly transferred from the image area to the opaque area or storage region with acceptable smear of a few percent. That image can then be read out slowly from the storage region while a new image is integrating or exposing in the active area. Frame-transfer devices typically do not require a mechanical shutter and were a common architecture for early solid-state broadcast cameras. The downside to the frame-transfer architecture is that it requires twice the silicon real estate of an equivalent full-frame device; hence, it costs roughly twice as much.\n\nThe interline architecture extends this concept one step further and masks every other column of the image sensor for storage. In this device, only one pixel shift has to occur to transfer from image area to storage area; thus, shutter times can be less than a microsecond and smear is essentially eliminated. The advantage is not free, however, as the imaging area is now covered by opaque strips dropping the fill factor to approximately 50 percent and the effective quantum efficiency by an equivalent amount. Modern designs have addressed this deleterious characteristic by adding microlenses on the surface of the device to direct light away from the opaque regions and on the active area. Microlenses can bring the fill factor back up to 90 percent or more depending on pixel size and the overall system's optical design.\n\nThe choice of architecture comes down to one of utility. If the application cannot tolerate an expensive, failure-prone, power-intensive mechanical shutter, an interline device is the right choice. Consumer snap-shot cameras have used interline devices. On the other hand, for those applications that require the best possible light collection and issues of money, power and time are less important, the full-frame device is the right choice. Astronomers tend to prefer full-frame devices. The frame-transfer falls in between and was a common choice before the fill-factor issue of interline devices was addressed. Today, frame-transfer is usually chosen when an interline architecture is not available, such as in a back-illuminated device.\n\nCCDs containing grids of pixels are used in digital cameras, optical scanners, and video cameras as light-sensing devices. They commonly respond to 70 percent of the incident light (meaning a quantum efficiency of about 70 percent) making them far more efficient than photographic film, which captures only about 2 percent of the incident light.\n\nMost common types of CCDs are sensitive to near-infrared light, which allows infrared photography, night-vision devices, and zero lux (or near zero lux) video-recording/photography. For normal silicon-based detectors, the sensitivity is limited to 1.1 μm. One other consequence of their sensitivity to infrared is that infrared from remote controls often appears on CCD-based digital cameras or camcorders if they do not have infrared blockers.\n\nCooling reduces the array's dark current, improving the sensitivity of the CCD to low light intensities, even for ultraviolet and visible wavelengths. Professional observatories often cool their detectors with liquid nitrogen to reduce the dark current, and therefore the thermal noise, to negligible levels.\n\nThe frame transfer CCD imager was the first imaging structure proposed for CCD Imaging by Michael Tompsett at Bell Laboratories. A frame transfer CCD is a specialized CCD, often used in astronomy and some professional video cameras, designed for high exposure efficiency and correctness.\n\nThe normal functioning of a CCD, astronomical or otherwise, can be divided into two phases: exposure and readout. During the first phase, the CCD passively collects incoming photons, storing electrons in its cells. After the exposure time is passed, the cells are read out one line at a time. During the readout phase, cells are shifted down the entire area of the CCD. While they are shifted, they continue to collect light. Thus, if the shifting is not fast enough, errors can result from light that falls on a cell holding charge during the transfer. These errors are referred to as \"vertical smear\" and cause a strong light source to create a vertical line above and below its exact location. In addition, the CCD cannot be used to collect light while it is being read out. Unfortunately, a faster shifting requires a faster readout, and a faster readout can introduce errors in the cell charge measurement, leading to a higher noise level.\n\nA frame transfer CCD solves both problems: it has a shielded, not light sensitive, area containing as many cells as the area exposed to light. Typically, this area is covered by a reflective material such as aluminium. When the exposure time is up, the cells are transferred very rapidly to the hidden area. Here, safe from any incoming light, cells can be read out at any speed one deems necessary to correctly measure the cells' charge. At the same time, the exposed part of the CCD is collecting light again, so no delay occurs between successive exposures.\n\nThe disadvantage of such a CCD is the higher cost: the cell area is basically doubled, and more complex control electronics are needed.\n\nAn intensified charge-coupled device (ICCD) is a CCD that is optically connected to an image intensifier that is mounted in front of the CCD.\n\nAn image intensifier includes three functional elements: a photocathode, a micro-channel plate (MCP) and a phosphor screen. These three elements are mounted one close behind the other in the mentioned sequence. The photons which are coming from the light source fall onto the photocathode, thereby generating photoelectrons. The photoelectrons are accelerated towards the MCP by an electrical control voltage, applied between photocathode and MCP. The electrons are multiplied inside of the MCP and thereafter accelerated towards the phosphor screen. The phosphor screen finally converts the multiplied electrons back to photons which are guided to the CCD by a fiber optic or a lens.\n\nAn image intensifier inherently includes a shutter functionality: If the control voltage between the photocathode and the MCP is reversed, the emitted photoelectrons are not accelerated towards the MCP but return to the photocathode. Thus, no electrons are multiplied and emitted by the MCP, no electrons are going to the phosphor screen and no light is emitted from the image intensifier. In this case no light falls onto the CCD, which means that the shutter is closed. The process of reversing the control voltage at the photocathode is called \"gating\" and therefore ICCDs are also called gateable CCD cameras.\n\nBesides the extremely high sensitivity of ICCD cameras, which enable single photon detection, the gateability is one of the major advantages of the ICCD over the EMCCD cameras. The highest performing ICCD cameras enable shutter times as short as 200 picoseconds.\n\nICCD cameras are in general somewhat higher in price than EMCCD cameras because they need the expensive image intensifier. On the other hand, EMCCD cameras need a cooling system to cool the EMCCD chip down to temperatures around 170 K. This cooling system adds additional costs to the EMCCD camera and often yields heavy condensation problems in the application.\n\nICCDs are used in night vision devices and in various scientific applications.\n\nAn electron-multiplying CCD (EMCCD, also known as an L3Vision CCD, a product commercialized by e2v Ltd., GB, L3CCD or Impactron CCD, a now-discontinued product offered in the past by Texas Instruments) is a charge-coupled device in which a gain register is placed between the shift register and the output amplifier. The gain register is split up into a large number of stages. In each stage, the electrons are multiplied by impact ionization in a similar way to an avalanche diode. The gain probability at every stage of the register is small (\"P\" < 2%), but as the number of elements is large (N > 500), the overall gain can be very high (formula_1), with single input electrons giving many thousands of output electrons. Reading a signal from a CCD gives a noise background, typically a few electrons. In an EMCCD, this noise is superimposed on many thousands of electrons rather than a single electron; the devices' primary advantage is thus their negligible readout noise. It is to be noted that the use of avalanche breakdown for amplification of photo charges had already been described in the in 1973 by George E. Smith/Bell Telephone Laboratories.\n\nEMCCDs show a similar sensitivity to intensified CCDs (ICCDs). However, as with ICCDs, the gain that is applied in the gain register is stochastic and the \"exact\" gain that has been applied to a pixel's charge is impossible to know. At high gains (> 30), this uncertainty has the same effect on the signal-to-noise ratio (SNR) as halving the quantum efficiency (QE) with respect to operation with a gain of unity. However, at very low light levels (where the quantum efficiency is most important), it can be assumed that a pixel either contains an electron — or not. This removes the noise associated with the stochastic multiplication at the risk of counting multiple electrons in the same pixel as a single electron. To avoid multiple counts in one pixel due to coincident photons in this mode of operation, high frame rates are essential. The dispersion in the gain is shown in the graph on the right. For multiplication registers with many elements and large gains it is well modelled by the equation:\n\nformula_2 if formula_3\n\nwhere \"P\" is the probability of getting \"n\" output electrons given \"m\" input electrons and a total mean multiplication register gain of \"g\".\n\nBecause of the lower costs and better resolution, EMCCDs are capable of replacing ICCDs in many applications. ICCDs still have the advantage that they can be gated very fast and thus are useful in applications like range-gated imaging. EMCCD cameras indispensably need a cooling system — using either thermoelectric cooling or liquid nitrogen — to cool the chip down to temperatures in the range of . This cooling system unfortunately adds additional costs to the EMCCD imaging system and may yield condensation problems in the application. However, high-end EMCCD cameras are equipped with a permanent hermetic vacuum system confining the chip to avoid condensation issues.\n\nThe low-light capabilities of EMCCDs find use in astronomy and biomedical research, among other fields. In particular, their low noise at high readout speeds makes them very useful for a variety of astronomical applications involving low light sources and transient events such as lucky imaging of faint stars, high speed photon counting photometry, Fabry-Pérot spectroscopy and high-resolution spectroscopy. More recently, these types of CCDs have broken into the field of biomedical research in low-light applications including small animal imaging, single-molecule imaging, Raman spectroscopy, super resolution microscopy as well as a wide variety of modern fluorescence microscopy techniques thanks to greater SNR in low-light conditions in comparison with traditional CCDs and ICCDs.\n\nIn terms of noise, commercial EMCCD cameras typically have clock-induced charge (CIC) and dark current (dependent on the extent of cooling) that together lead to an effective readout noise ranging from 0.01 to 1 electrons per pixel read. However, recent improvements in EMCCD technology have led to a new generation of cameras capable of producing significantly less CIC, higher charge transfer efficiency and an EM gain 5 times higher than what was previously available. These advances in low-light detection lead to an effective total background noise of 0.001 electrons per pixel read, a noise floor unmatched by any other low-light imaging device.\n\nDue to the high quantum efficiencies of CCDs (for a quantum efficiency of 100%, one count equals one photon), linearity of their outputs, ease of use compared to photographic plates, and a variety of other reasons, CCDs were very rapidly adopted by astronomers for nearly all UV-to-infrared applications.\n\nThermal noise and cosmic rays may alter the pixels in the CCD array. To counter such effects, astronomers take several exposures with the CCD shutter closed and opened. The average of images taken with the shutter closed is necessary to lower the random noise. Once developed, the dark frame average image is then subtracted from the open-shutter image to remove the dark current and other systematic defects (dead pixels, hot pixels, etc.) in the CCD.\n\nThe Hubble Space Telescope, in particular, has a highly developed series of steps (“data reduction pipeline”) to convert the raw CCD data to useful images.\n\nCCD cameras used in astrophotography often require sturdy mounts to cope with vibrations from wind and other sources, along with the tremendous weight of most imaging platforms. To take long exposures of galaxies and nebulae, many astronomers use a technique known as auto-guiding. Most autoguiders use a second CCD chip to monitor deviations during imaging. This chip can rapidly detect errors in tracking and command the mount motors to correct for them.\n\nAn unusual astronomical application of CCDs, called drift-scanning, uses a CCD to make a fixed telescope behave like a tracking telescope and follow the motion of the sky. The charges in the CCD are transferred and read in a direction parallel to the motion of the sky, and at the same speed. In this way, the telescope can image a larger region of the sky than its normal field of view. The Sloan Digital Sky Survey is the most famous example of this, using the technique to a survey of over a quarter of the sky.\n\nIn addition to imagers, CCDs are also used in an array of analytical instrumentation including spectrometers and interferometers.\n\nDigital color cameras generally use a Bayer mask over the CCD. Each square of four pixels has one filtered red, one blue, and two green (the human eye is more sensitive to green than either red or blue). The result of this is that luminance information is collected at every pixel, but the color resolution is lower than the luminance resolution.\n\nBetter color separation can be reached by three-CCD devices (3CCD) and a dichroic beam splitter prism, that splits the image into red, green and blue components. Each of the three CCDs is arranged to respond to a particular color. Many professional video camcorders, and some semi-professional camcorders, use this technique, although developments in competing CMOS technology have made CMOS sensors, both with beam-splitters and bayer filters, increasingly popular in high-end video and digital cinema cameras. Another advantage of 3CCD over a Bayer mask device is higher quantum efficiency (and therefore higher light sensitivity for a given aperture size). This is because in a 3CCD device most of the light entering the aperture is captured by a sensor, while a Bayer mask absorbs a high proportion (about 2/3) of the light falling on each CCD pixel.\n\nFor still scenes, for instance in microscopy, the resolution of a Bayer mask device can be enhanced by microscanning technology. During the process of color co-site sampling, several frames of the scene are produced. Between acquisitions, the sensor is moved in pixel dimensions, so that each point in the visual field is acquired consecutively by elements of the mask that are sensitive to the red, green and blue components of its color. Eventually every pixel in the image has been scanned at least once in each color and the resolution of the three channels become equivalent (the resolutions of red and blue channels are quadrupled while the green channel is doubled).\n\nSensors (CCD / CMOS) come in various sizes, or image sensor formats. These sizes are often referred to with an inch fraction designation such as 1/1.8″ or 2/3″ called the optical format. This measurement actually originates back in the 1950s and the time of Vidicon tubes.\n\nWhen a CCD exposure is long enough, eventually the electrons that collect in the \"bins\" in the brightest part of the image will overflow the bin, resulting in blooming. The structure of the CCD allows the electrons to flow more easily in one direction than another, resulting in vertical streaking.\n\nSome anti-blooming features that can be built into a CCD reduce its sensitivity to light by using some of the pixel area for a drain structure.\nJames M. Early developed a vertical anti-blooming drain that would not detract from the light collection area, and so did not reduce light sensitivity.\n\n"}
{"id": "2924447", "url": "https://en.wikipedia.org/wiki?curid=2924447", "title": "Charles Fredrick Wiesenthal", "text": "Charles Fredrick Wiesenthal\n\nCharles Fredrick Wiesenthal was a German inventor who was awarded the patent for the first known mechanical device for sewing in 1755. One might argue that he invented the sewing machine. He was born in Germany, but was in England at the time of invention. For his invention of a double pointed needle with an eye at one end, he received the British Patent No. 701 (1755). but after in 1830 Barthélemy Thimonnier reinvented the sewing machine.\n"}
{"id": "12277238", "url": "https://en.wikipedia.org/wiki?curid=12277238", "title": "Civic agriculture", "text": "Civic agriculture\n\nCivic Agriculture is the trend towards locally based agriculture and food production that is tightly linked to a community's social and economic development. Civic agriculture is geared towards meeting consumer demands in addition to boosting the local economy in the process through jobs, entrepreneurship, and community sustainability. The term was first coined by Thomas Lyson, professor of Sociology at Cornell, to represent an alternative means of sustainability for rural agricultural communities in the era of industrialized agriculture. Civic agriculture is geared towards fostering a self sustainable local economy through an integral community structure in which the entire community is in some part responsible for their food production.\n\nCivic agriculture is a means by which rural agricultural communities can remain subsistent in a largely industrialized agriculture sector. The term was coined by the late Thomas A. Lyson, Department of Development Sociology, Cornell University, at the 1999 Rural Sociology Society Annual Meeting. In his 2004 book, \"Civic Agriculture: Reconnecting Farm, Food and Community\" Lyson argues that in containing food production for a specific community to that community, one is further connecting the community so that it may be economically independent and socially unified. Lyson expounds on his ideas, arguing that because of the interlocked relationship between the food economy and consumers, people have a civic duty to support important agricultural engagements. In his book, Lyson claims that communities that show an active involvement in civic agriculture aid economic development by supporting their local food production. Thus, in committing to civic agriculture, local communities contribute to an economic growth in the agricultural sector.\n\nManifestations of movement towards Civic Agriculture:\n\nThe basis of civic agriculture is rooted in pre-industrialization farming practices. Farmers today are turning to civic agriculture in order to remain economically viable within an industrialized society and corporate agriculture practices. Civic agriculture promotes the sustainability of the local economy by containing the source and production of food to a particular region. Dependent upon the advanced nature of the civic agriculture production, that region is then reliant upon a small subset of farmers for the majority of their food goods. Thus, that subset of farmers must farm a variety of commodities in order to provide for the region. This practice fosters entrepreneurship within the community by treating farming as an economically viable practice, creates jobs through employment of the local community, and keeps the production and consumption of agriculture in one region making that region economically sustainable within itself.\n\nCivic agriculture connects the community by eliminating the fragmented nature of agriculture production. It reconnects farmers to the community and creates a social connection between the farmers and the community that is dependent upon them. The community is linked together by the prospect of its success being dependent upon the success of the collective. Civic agriculture ensures locally oriented practices that serve as a driving force for the way in which the community operates socially and politically. Socially, the general well being of the community becomes a primary concern when civic agriculture is being practiced.\n\nThomas Lyson was a notable sociologist who spent much of his professional career analyzing the possible impacts and outcomes of civic agriculture. After coining the phrase in 1999, Lyson  used his time as a professor at Cornell University to propose ways that rural communities could support themselves not only by providing food to the community, but also by providing jobs and thus supporting the local, rural economy. Lyson's interest in rural community subsistence stemmed from his time spent traveling the globe, specifically in the back roads of Appalachia. In his novel \"Civic Agriculture: Reconnecting Farm, Food, and Community\", Lyson warns against the increasingly industrial approach being developed in the world of agriculture today as being detrimental to the independent family farm which serves as the backbone of the rural community. Lyson spent a considerable portion of his career exploring the economic opportunities presented before rural communities and the ways in which those opportunities should be utilized in order to ensure the prosperity of the community. In 2013, Lyson created The Lyson Center for Civic Agriculture and Food Systems, a food systems development program in Ithaca, New York. Since 2013, it has been a project within the Center for Transformative Action, a nonprofit organization affiliated with Cornell University. The aim of the center is to provide research oriented solutions to the current problems that exist within our various food systems. The center publishes the \"Journal of Agriculture, Food Systems, and Community Development,\" an open access journal on food systems and food systems research and facilitates the North American Food Systems Network. The Lyson Center also publishes the Sustainable Food Systems Sourcebook.\n\n"}
{"id": "3248252", "url": "https://en.wikipedia.org/wiki?curid=3248252", "title": "Collar (animal)", "text": "Collar (animal)\n\nAn animal collar is a device that attached to the neck of an animal to allow it to be harnessed, tied up or for various other reasons.\n\n\nCollars can be dangerous for pets that live in crates or which might get stuck in tree branches and that is why safety collars have been developed. There is a particular type of safety collar which is intended for both dogs and cats. Breakaway collars are especially designed to prevent the pet from choking or getting stuck because of their collar. They feature a clever design that releases quickly when a small amount of pressure is applied, such as the cat hanging from a tree branch. The clasp will release, which quickly gets the pet out of a possibly desperate situation. However, it is recommended that pets have their collar removed before sleeping in a wired crate.\n\n\n"}
{"id": "13626675", "url": "https://en.wikipedia.org/wiki?curid=13626675", "title": "Core77", "text": "Core77\n\nCore77 is an online design magazine dedicated to the practice and produce of the field of industrial design. It serves as a resource for students, practitioners and fans of the field, as well as a venue for essays and reports on the topic of design in general. Historically, most of the magazine's content has been produced by volunteer contributors.\n\nThe site began as the graduate thesis of Stuart Constantine and Eric Ludlum in their final year at Brooklyn, New York's Pratt Institute. The site was launched in March 1995 and has been updated on a monthly basis since. It was first hosted at Interport, an early ISP in New York City; later it moved to its own domain.\n\nCore77's popularity as a general design destination for the public has grown in recent years, leading to references of the site in \"The New York Times\", \"Fast Company\", and \"PC Magazine\". The site also hosts an annual Core77 Design Awards competition to reward excellence in the field of design.\n\n"}
{"id": "7411939", "url": "https://en.wikipedia.org/wiki?curid=7411939", "title": "Cyanometer", "text": "Cyanometer\n\nA cyanometer (from cyan and -meter) is an instrument for measuring 'blueness', specifically the colour intensity of blue sky. It is attributed to Horace-Bénédict de Saussure and Alexander von Humboldt. It consists of squares of paper dyed in graduated shades of blue and arranged in a color circle or square that can be held up and compared to the color of the sky.\n\nDe Saussure is credited with inventing the cyanometer in 1789. De Saussure's cyanometer had 53 sections, ranging from white to varying shades of blue (dyed with Prussian blue) and then to black, arranged in a circle; he used the device to measure the color of the sky at Geneva, Chamonix, and Mont Blanc. De Saussure concluded, correctly, that the color of the sky was dependent on the amount of suspended particles in the atmosphere.\n\nHumboldt was also an eager user of the cyanometer on his voyages and explorations in South America.\n\nThe blueness of the atmosphere indicates transparency and the amount of water vapour.\n\n\n\n"}
{"id": "1182291", "url": "https://en.wikipedia.org/wiki?curid=1182291", "title": "Design engineer", "text": "Design engineer\n\nA design engineer is a person who may be involved in any of various engineering disciplines including civil, mechanical, electrical, chemical, textiles, aerospace, nuclear, manufacturing, systems, and structural /building/architectural.\nDesign engineers tend to work on products and systems that involve adapting and using complex scientific and mathematical techniques. The emphasis tends to be on utilizing engineering physics and sciences to develop solutions for society.\n\nThe design engineer usually works with a team of engineers and other designers to develop conceptual and detailed designs that ensure a product actually functions, performs and is fit for its purpose. They may work with industrial designers and marketers to develop the product concept and specifications to meet customer needs and may direct the design effort. In many engineering areas, a distinction is made between the design engineer and the planning engineer in design; analysis is important for planning engineers while synthesis is more paramount for design engineers.\n\nWhen the design involves public safety, the design engineer is usually required to be licensed, for example, a professional engineer in the U.S and Canada. There is usually an \"industrial exemption\" for design engineers working on project internal to companies and not delivering professional services directly to the public.\n\nDesign engineers may work in a team along with other designers to create the drawings necessary for prototyping and production, or in the case of buildings, for construction. However, with the advent of CAD and solid modeling software, the design engineers may create the drawings themselves, or perhaps with the help of many corporate service providers.\n\nThe next responsibility of many design engineers is prototyping. A model of the product is created and reviewed. Prototypes are either functional or non-functional. Functional \"alpha\" prototypes are used for testing; non-functional prototypes are used for form and fit checking. Virtual prototyping and hence for any such software solutions may also be used. This stage is where design flaws are found and corrected, and tooling, manufacturing fixtures, and packaging are developed.\n\nOnce the \"alpha\" prototype is finalized after many iterations, the next step is the \"beta\" pre-production prototype. The design engineer, working with an industrial engineer, manufacturing engineer, and quality engineer, reviews an initial run of components and assemblies for design compliance and fabrication/manufacturing methods analysis. This is often determined through statistical process control. Variations in the product are correlated to aspects of the process and eliminated. The most common metric used is the process capability index C. A C of 1.0 is considered the baseline acceptance for full production go-ahead.\n\nThe design engineer may follow the product and make requested changes and corrections throughout the life of the product. This is referred to as \"cradle to grave\" engineering. The design engineer works closely with the manufacturing engineer throughout the product life cycle. \n\nThe design process is an information intensive one, and design engineers have been found to spend 56% of their time engaged in various information behaviours, including 14% actively searching for information. In addition to design engineers' core technical competence, research has demonstrated the critical nature of their personal attributes, project management skills, and cognitive abilities to succeed in the role.\n\nAmongst other more detailed findings, a recent work sampling study found that design engineers spend 62.92% of their time engaged in technical work, 40.37% in social work, and 49.66% in computer-based work. There was considerable overlap between these different types of work, with engineers spending 24.96% of their time engaged in technical and social work, 37.97% in technical and non-social, 15.42% in non-technical and social, and 21.66% in non-technical and non-social.\n\n"}
{"id": "36830389", "url": "https://en.wikipedia.org/wiki?curid=36830389", "title": "Digital Ocean", "text": "Digital Ocean\n\nDigital Ocean, Inc., was a maker of wireless products from 1992 to 1998.\n\nIt was eventually bought by Harris Semiconductor and its assets made part of Harris's Intersil Division, which was spun off in 1999 into Intersil Corporation.\n\nThe company was founded in May 1992 by Jeffery Alholm and headquartered in Lenexa, Kansas. Several contracts with Apple Inc., AT&T, Aironet Wireless Communications (later acquired by Cisco as its wireless LAN division), Harris Semiconductor, the United States Department of Defense, and several others made Digital Ocean the leader in developing and manufacturing state-of-the-art wireless products for the entire line of Apple’s desktop, portable, and pen-based devices. It was a co-developer of the IEEE 802.11 wireless standard and of the industry’s first 802.11 chipset. It developed the Seahorse, arguably the world's first smartphone. In addition, by specializing in rapid, custom development, the company concluded multiple individual development contracts for application specific wireless products in vertical markets. Digital Ocean was granted approximately 20 patents for its development of wireless technologies.\n\nIn 1998 it was sold with its assets to Harris Semiconductor to become part of their Intersil division; Intersil was then spun off from Harris one year later.\n\nStarfish Wireless Access Point for LocalTalk and EtherTalk Macintosh\n\nStarfish with Microcellular Roaming Software \nEnabled seamless roaming.\n\nStarfish II Ethernet Access Point\n\"Business Wire Magazine\" explained, \"The Starfish II connects to wired networks and acts as the access provider for Manta and Digital Ocean's other station products.\"\n\nManta 500EN EtherTalk Wireless Station with AAUI Connection\n\nManta 10BaseT\nWireless network connections at full ethernet speeds.\n\nGrouper Line<ref name=\"InfoWorld Magazine, Vol 14, Issue 52/1, Page 3, 28 Dec 1992 - 4 Jan 1993, Firm Developing Wireless Mac Networks, By Vance McCarthy\"></ref> \nThe Grouper line of products were networking devices that used spread-spectrum radio waves to communicate. Groupers could be attached to any PowerBook or used freestanding with any desktop Mac. Placing one Grouper on a wired network would have it serve as a hub for up to 15 other wireless Groupers. Keeping with Digital Ocean's theme, the collection of networked computers was called a school. Wireless devices could access the Grouper-enabled network from within a 250-foot distance indoors to an 800-foot distance outdoors, and the Grouper only used one-sixtieth the power of other comparable wireless devices.\nWireless local area connectivity for all Newton MessagePads.\nWireless local area connectivity for all Newton MessagePads; included additional port for serial devices, such as wand barcode readers, laser barcode scanners, and printers.\nPackage included Grouper 100MPS+ with Digital Ocean Pen Reader, bundled with AllPen barcode software for wireless data collection.\nThe original Wireless LocalTalk station for Macintosh that mounted directly to the bottom of 100xx PowerBooks.\n\nTarpon All-In-One PDA\nThe Tarpon integrated an Apple Newton and Digital Ocean Grouper with backlighting, a water-resistant and ruggedized case, and built-in wireless LAN, WAN, modem, voice capability for anything from simple peer-to-peer conversations to full telephony, and optional GPS via the PCMCIA slot. The addition of wireless capabilities to Newton-based devices was called \"a major step forward\" because \"without it, they are not really useful.\" The Tarpon began as the SuperTech 2000, but was then further modified and released to the general public under the new name.\n\nSeahorse\nA rugged handheld computer based on the Newton OS 2.0, the Seahorse was backlit, lightweight, and durable, with a variety of integrated communication capabilities: the first CDPD modem ever in a wireless PDA, PCMCIA slot, WLAN modem, and a modular snap-on nose for optional GPS and diffused infrared capabilities. A rugged boot protected Seahorse, while large-capacity, slide-in rechargeable batteries provided around eight hours of continuous use. Together these features made Seahorse a good solution for: remote handheld access to customer service databases, wireless Internet access, precision location applications utilizing handheld GPS systems, and wirelessly accessing corporate and Internet databases to utilize schematics and manuals while in the field.\n\nDigital Ocean began sales of the Seahorse in 1996. Though not its original intention, the Seahorse arguably contained all the intellectual property and engineering in one integrated device to constitute the world’s first smartphone, as defined by four parameters:\n\"Business Wire Magazine\" wrote, \"'The UB-1 provides Seahorse with a fully featured, economical module in an easy to integrate form factor,' said Jeff Alholm, president and chief executive officer of Digital Ocean. 'Combining PCSI's proven cellular voice and data technology with Seahorse's power performance and options makes Seahorse a leading industry choice for users seeking a versatile and affordable handheld computer.'\" As such a groundbreaking product, the Seahorse received the Cellular Telephone Industry Association’s (CTIA) 1996 award for Product of the Year.\n\nSkyway Bridge\n\"Business Wire Magazine\" said, \"Skyway Bridge wirelessly connected Ethernet-compliant networks in multiple buildings at distances up to 20 miles apart and can be used in conjunction with Digital Ocean's in-building wireless LAN products, including Starfish II, Manta II and Grouper. With a bandwidth of 2 Mbps, Skyway Bridge is faster and less expensive than T1 lines.\"\n\nAll Digital Ocean wireless LAN (WLAN) products utilized the company’s patented protocol and software technologies along with a direct-sequence spread spectrum radio, giving Digital Ocean products superb penetration through walls, exceptional range, reliable data transfer, secure transmissions, and excellent throughput, especially when compared to infrared LAN communication. Digital Ocean products required no additional network operating hardware or software, and fully supported AppleTalk protocol services.\n\nDigital Ocean developed and sold a complete family of LocalTalk and ethernet wireless network adapters for use with Macintosh desktops, PowerBooks, and Newtons. In addition, the company’s microcellular roaming permitted virtually unlimited wireless coverage areas. Their Starfish Access Points deployed microcellular roaming over a building or campus, allowing seamless wireless LAN connections over the entire area, similar to cellular telephones.\n\nIn 1996, Digital Ocean partnered with two other companies to provide Apple and PC platforms with their first ability to be on the same wireless LAN network through a single access point. The solution viewed each computer as an agnostic system when accessing the network, which placed all computers on equal footing in regards to their ability to communicate across an enterprise.\n\nIn 1995 and 1996, Digital Ocean entered into three-way development contracts with AT&T/Lucent and Aironet to license Digital Ocean's Media Access Control (MAC) chipset technology in return for access to AT&T/Lucent's semiconductor line, the PHY companion chips for each partner (DSSS and FHSS), plus broad development support from all parties. This work led to one common MAC chip for the three parties. Although branded differently for each partner, this common chip came from one manufacturing line and was then sorted and sold by AT&T/Lucent, Harris Semiconductor/Intersil (which by 1998 had acquired Digital Ocean and its assets), and Aironet/Cisco.\n\nBefore it was ratified as a standard, Digital Ocean was the leader in 802.11 capabilities, placing great emphasis on wireless as the future of communication and technology in general. Digital Ocean's portfolio of around 20 patents weighted heavily in the wireless category. The IEEE had already decided that any standard ratified would need to be at no cost, therefore, as a strategy to maintain their stake in the trajectory of the wireless business, Digital Ocean formed a consortium with its partners to publish an open standard for interoperability among different vendors and their products. They also licensed many of their patents to the emerging 802.11 standard. Some of Digital Ocean's patents included:\n"}
{"id": "8267", "url": "https://en.wikipedia.org/wiki?curid=8267", "title": "Dimensional analysis", "text": "Dimensional analysis\n\nIn engineering and science, dimensional analysis is the analysis of the relationships between different physical quantities by identifying their base quantities (such as length, mass, time, and electric charge) and units of measure (such as miles vs. kilometers, or pounds vs. kilograms) and tracking these dimensions as calculations or comparisons are performed. The conversion of units from one dimensional unit to another is often somewhat complex. Dimensional analysis, or more specifically the factor-label method, also known as the unit-factor method, is a widely used technique for such conversions using the rules of algebra.\n\nThe concept of physical dimension was introduced by Joseph Fourier in 1822. Physical quantities that are of the same kind (also called \"commensurable\") have the same dimension (length, time, mass) and can be directly compared to each other, even if they are originally expressed in differing units of measure (such as yards and meters). If physical quantities have different dimensions (such as length vs. mass), they cannot be expressed in terms of similar units and cannot be compared in quantity (also called \"incommensurable\"). For example, asking whether a kilogram is larger than an hour is meaningless.\n\nAny physically meaningful equation (and any inequality) will have the same dimensions on its left and right sides, a property known as \"dimensional homogeneity\". Checking for dimensional homogeneity is a common application of dimensional analysis, serving as a plausibility check on derived equations and computations. It also serves as a guide and constraint in deriving equations that may describe a physical system in the absence of a more rigorous derivation.\n\nA lot of parameters and the measurements(m) in the physical sciences and engineering are expressed as a concrete number – a numerical quantity(q) and a corresponding dimensional unit. Often a quantity is expressed in terms of several other quantities; for example, speed is a combination of length and time, e.g. 60 miles per hour or 1.4 kilometers per second. Compound relations with \"per\" are expressed with division, e.g. 60 mi/1 h. Other relations can involve multiplication (often shown with a centered dot or juxtaposition), powers (like m for square meters), or combinations thereof.\n\nA set of base units for a system of measurement is a conventionally chosen set of units, none of which can be expressed as a combination of the others, and in terms of which all the remaining units of the system can be expressed. For example, units for length and time are normally chosen as base units. Units for volume, however, can be factored into the base units of length (m), thus they are considered derived or compound units.\n\nSometimes the names of units obscure the fact that they are derived units. For example, a newton (N) is a unit of force, which will have units of mass (kg) times acceleration (m⋅s). The newton is defined as .\n\nPercentages are dimensionless quantities, since they are ratios of two quantities with the same dimensions. In other words, the % sign can be read as \"hundredths\", since .\n\nTaking a derivative with respect to a quantity adds the dimension of the variable one is differentiating with respect to, in the denominator. Thus:\nIn economics, one distinguishes between stocks and flows: a stock has units of \"units\" (say, widgets or dollars), while a flow is a derivative of a stock, and has units of \"units/time\" (say, dollars/year).\n\nIn some contexts, dimensional quantities are expressed as dimensionless quantities or percentages by omitting some dimensions. For example, debt-to-GDP ratios are generally expressed as percentages: total debt outstanding (dimension of currency) divided by annual GDP (dimension of currency) – but one may argue that in comparing a stock to a flow, annual GDP should have dimensions of currency/time (dollars/year, for instance), and thus Debt-to-GDP should have units of years, which indicates that Debt-to-GDP is the number of years needed for a constant GDP to pay the debt, if all GDP is spent on the debt and the debt is otherwise unchanged.\n\nIn dimensional analysis, a ratio which converts one unit of measure into another without changing the quantity is called a conversion factor. For example, kPa and bar are both units of pressure, and . The rules of algebra allow both sides of an equation to be divided by the same expression, so this is equivalent to . Since any quantity can be multiplied by 1 without changing it, the expression \"\" can be used to convert from bars to kPa by multiplying it with the quantity to be converted, including units. For example, because , and bar/bar cancels out, so .\n\nThe most basic rule of dimensional analysis is that of dimensional homogeneity. Only commensurable quantities (physical quantities having the same dimension) may be \"compared,\" \"equated,\" \"added,\" or \"subtracted.\"\nHowever, the dimensions form an abelian group under multiplication, so:\n\nFor example, it makes no sense to ask whether 1 hour is more, the same, or less than 1 kilometer, as these have different dimensions, nor to add 1 hour to 1 kilometer. However, it makes perfect sense to ask whether 1 mile is more, the same, or less than 1 kilometer being the same dimension of physical quantity even though the units are different. On the other hand, if an object travels 100 km in 2 hours, one may divide these and conclude that the object's average speed was 50 km/h.\n\nThe rule implies that in a physically meaningful \"expression\" only quantities of the same dimension can be added, subtracted, or compared. For example, if \"m\", \"m\" and \"L\" denote, respectively, the mass of some man, the mass of a rat and the length of that man, the dimensionally homogeneous expression is meaningful, but the heterogeneous expression is meaningless. However, \"m\"/\"L\" is fine. Thus, dimensional analysis may be used as a sanity check of physical equations: the two sides of any equation must be commensurable or have the same dimensions.\n\nEven when two physical quantities have identical dimensions, it may nevertheless be meaningless to compare or add them. For example, although torque and energy share the dimension , they are fundamentally different physical quantities.\n\nTo compare, add, or subtract quantities with the same dimensions but expressed in different units, the standard procedure is first to convert them all to the same units. For example, to compare 32 metres with 35 yards, use 1 yard = 0.9144 m to convert 35 yards to 32.004 m.\n\nA related principle is that any physical law that accurately describes the real world must be independent of the units used to measure the physical variables. For example, Newton's laws of motion must hold true whether distance is measured in miles or kilometers. This principle gives rise to the form that conversion factors must take between units that measure the same dimension: multiplication by a simple constant. It also ensures equivalence; for example, if two buildings are the same height in feet, then they must be the same height in meters.\n\nThe factor-label method is the sequential application of conversion factors expressed as fractions and arranged so that any dimensional unit appearing in both the numerator and denominator of any of the fractions can be cancelled out until only the desired set of dimensional units is obtained. For example, 10 miles per hour can be converted to meters per second by using a sequence of conversion factors as shown below:\n\nIt can be seen that each conversion factor is equivalent to the value of one. For example, starting with 1 mile = 1609.344 meters and dividing both sides of the equation by 1 mile yields 1 mile / 1 mile = 1609.344 meters / 1 mile, which when simplified yields 1 = 1609.344 meters / 1 mile.\n\nSo, when the units \"mile\" and \"hour\" are cancelled out and the arithmetic is done, 10 miles per hour converts to 4.4704 meters per second.\n\nAs a more complex example, the concentration of nitrogen oxides (i.e., formula_2) in the flue gas from an industrial furnace can be converted to a mass flow rate expressed in grams per hour (i.e., g/h) of formula_3 by using the following information as shown below:\n\n\nAfter canceling out any dimensional units that appear both in the numerators and denominators of the fractions in the above equation, the NO concentration of 10 ppm converts to mass flow rate of 24.63 grams per hour.\n\nThe factor-label method can also be used on any mathematical equation to check whether or not the dimensional units on the left hand side of the equation are the same as the dimensional units on the right hand side of the equation. Having the same units on both sides of an equation does not ensure that the equation is correct, but having different units on the two sides (when expressed in terms of base units) of an equation implies that the equation is wrong.\n\nFor example, check the Universal Gas Law equation of , when:\n\nAs can be seen, when the dimensional units appearing in the numerator and denominator of the equation's right hand side are cancelled out, both sides of the equation have the same dimensional units.\n\nThe factor-label method can convert only unit quantities for which the units are in a linear relationship intersecting at 0. Most units fit this paradigm. An example for which it cannot be used is the conversion between degrees Celsius and kelvins (or degrees Fahrenheit). Between degrees Celsius and kelvins, there is a constant difference rather than a constant ratio, while between degrees Celsius and degrees Fahrenheit there is neither a constant difference nor a constant ratio. There is, however, an affine transform (formula_6, rather than a linear transform formula_7) between them.\n\nFor example, the freezing point of water is 0 °C and 32 °F, and a 5 °C change is the same as a 9 °F change. Thus, to convert from units of Fahrenheit to units of Celsius, one subtracts 32 °F (the offset from the point of reference), divides by 9 °F and multiplies by 5 °C (scales by the ratio of units), and adds 0 °C (the offset from the point of reference). Reversing this yields the formula for obtaining a quantity in units of Celsius from units of Fahrenheit; one could have started with the equivalence between 100 °C and 212 °F, though this would yield the same formula at the end.\n\nHence, to convert the numerical quantity value of a temperature \"T\"[F] in degrees Fahrenheit to a numerical quantity value \"T\"[C] in degrees Celsius, this formula may be used:\n\nTo convert \"T\"[C] in degrees Celsius to \"T\"[F] in degrees Fahrenheit, this formula may be used:\n\nDimensional analysis is most often used in physics and chemistry – and in the mathematics thereof – but finds some applications outside of those fields as well.\n\nA simple application of dimensional analysis to mathematics is in computing the form of the volume of an \"n\"-ball (the solid ball in \"n\" dimensions), or the area of its surface, the \"n\"-sphere: being an \"n\"-dimensional figure, the volume scales as formula_8 while the surface area, being formula_9-dimensional, scales as formula_10 Thus the volume of the \"n\"-ball in terms of the radius is formula_11 for some constant formula_12 Determining the constant takes more involved mathematics, but the form can be deduced and checked by dimensional analysis alone.\n\nIn finance, economics, and accounting, dimensional analysis is most commonly referred to in terms of the distinction between stocks and flows. More generally, dimensional analysis is used in interpreting various financial ratios, economics ratios, and accounting ratios.\n\nIn fluid mechanics, dimensional analysis is performed in order to obtain dimensionless Pi terms or groups. According to the principles of dimensional analysis, any prototype can be described by a series of these terms or groups that describe the behaviour of the system. Using suitable Pi terms or groups, it is possible to develop a similar set of Pi terms for a model that has the same dimensional relationships. In other words, Pi terms provide a shortcut to developing a model representing a certain prototype. Common dimensionless groups in fluid mechanics include:\n\n\nThe origins of dimensional analysis have been disputed by historians. The 19th-century French mathematician Joseph Fourier is generally credited with having made important contributions based on the idea that physical laws like should be independent of the units employed to measure the physical variables. This led to the conclusion that meaningful laws must be homogeneous equations in their various units of measurement, a result which was eventually formalized in the Buckingham π theorem. However, the first application of dimensional analysis has been credited to the Italian scholar François Daviet de Foncenex (1734–1799). It was published in 1761, 61 years before the publication of Fourier's work.\n\nJames Clerk Maxwell played a major role in establishing modern use of dimensional analysis by distinguishing mass, length, and time as fundamental units, while referring to other units as derived. Although Maxwell defined length, time and mass to be \"the three fundamental units\", he also noted that gravitational mass can be derived from length and time by assuming a form of Newton's law of universal gravitation in which the gravitational constant \"G\" is taken as unity, thereby defining . By assuming a form of Coulomb's law in which Coulomb's constant \"k\" is taken as unity, Maxwell then determined that the dimensions of an electrostatic unit of charge were , which, after substituting his equation for mass, results in charge having the same dimensions as mass, viz. .\n\nDimensional analysis is also used to derive relationships between the physical quantities that are involved in a particular phenomenon that one wishes to understand and characterize. It was used for the first time in this way in 1872 by Lord Rayleigh, who was trying to understand why the sky is blue. Rayleigh first published the technique in his 1877 book \"The Theory of Sound\".\n\nThe original meaning of the word \"dimension\", in Fourier's \"Theorie de la Chaleur\", was the numerical value of the exponents of the base units. For example, acceleration was considered to have the dimension 1 with respect to the unit of length, and the dimension −2 with respect to the unit of time. This was slightly changed by Maxwell, who said the dimensions of acceleration are LT, instead of just the exponents.\n\nThe Buckingham π theorem describes how every physically meaningful equation involving \"n\" variables can be equivalently rewritten as an equation of dimensionless parameters, where \"m\" is the rank of the dimensional matrix. Furthermore, and most importantly, it provides a method for computing these dimensionless parameters from the given variables.\n\nA dimensional equation can have the dimensions reduced or eliminated through nondimensionalization, which begins with dimensional analysis, and involves scaling quantities by characteristic units of a system or natural units of nature. This gives insight into the fundamental properties of the system, as illustrated in the examples below.\n\nThe dimension of a physical quantity can be expressed as a product of the basic physical dimensions such as length, mass and time, each raised to a rational power. The \"dimension\" of a physical quantity is more fundamental than some \"scale\" unit used to express the amount of that physical quantity. For example, \"mass\" is a dimension, while the kilogram is a particular scale unit chosen to express a quantity of mass. Except for natural units, the choice of scale is cultural and arbitrary.\n\nThere are many possible choices of basic physical dimensions. The SI standard recommends the usage of the following dimensions and corresponding symbols: length (L), mass (M), time (T), electric current (I), absolute temperature (Θ), amount of substance (N) and luminous intensity (J). The symbols are by convention usually written in roman sans serif typeface. Mathematically, the dimension of the quantity \"Q\" is given by \nwhere \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\" are the dimensional exponents. Other physical quantities could be defined as the base quantities, as long as they form a linearly independent basis. For instance, one could replace the dimension of electrical current (I) of the SI basis with a dimension of electric charge (Q), since Q = IT.\n\nAs examples, the dimension of the physical quantity speed \"v\" is\nand the dimension of the physical quantity force \"F\" is\n\nThe unit chosen to express a physical quantity and its dimension are related, but not identical concepts. The units of a physical quantity are defined by convention and related to some standard; e.g., length may have units of metres, feet, inches, miles or micrometres; but any length always has a dimension of L, no matter what units of length are chosen to express it. Two different units of the same physical quantity have conversion factors that relate them. For example, 1 in = 2.54 cm; in this case (2.54 cm/in) is the conversion factor, which is itself dimensionless. Therefore, multiplying by that conversion factor does not change the dimensions of a physical quantity.\n\nThere are also physicists that have cast doubt on the very existence of incompatible fundamental dimensions of physical quantity, although this does not invalidate the usefulness of dimensional analysis.\n\nThe dimensions that can be formed from a given collection of basic physical dimensions, such as M, L, and T, form an abelian group: The identity is written as 1; , and the inverse to L is 1/L or L. L raised to any rational power \"p\" is a member of the group, having an inverse of L or 1/L. The operation of the group is multiplication, having the usual rules for handling exponents ().\n\nThis group can be described as a vector space over the rational numbers, with for example dimensional symbol MLT corresponding to the vector . When physical measured quantities (be they like-dimensioned or unlike-dimensioned) are multiplied or divided by one other, their dimensional units are likewise multiplied or divided; this corresponds to addition or subtraction in the vector space. When measurable quantities are raised to a rational power, the same is done to the dimensional symbols attached to those quantities; this corresponds to scalar multiplication in the vector space.\n\nA basis for such a vector space of dimensional symbols is called a set of base quantities, and all other vectors are called derived units. As in any vector space, one may choose different bases, which yields different systems of units (e.g., choosing whether the unit for charge is derived from the unit for current, or vice versa).\n\nThe group identity 1, the dimension of dimensionless quantities, corresponds to the origin in this vector space.\n\nThe set of units of the physical quantities involved in a problem correspond to a set of vectors (or a matrix). The nullity describes some number (e.g., \"m\") of ways in which these vectors can be combined to produce a zero vector. These correspond to producing (from the measurements) a number of dimensionless quantities, {π, ..., π}. (In fact these ways completely span the null subspace of another different space, of powers of the measurements.) Every possible way of multiplying (and exponentiating) together the measured quantities to produce something with the same units as some derived quantity \"X\" can be expressed in the general form\n\nConsequently, every possible commensurate equation for the physics of the system can be rewritten in the form\n\nKnowing this restriction can be a powerful tool for obtaining new insight into the system.\n\nThe dimension of physical quantities of interest in mechanics can be expressed in terms of base dimensions M, L, and T – these form a 3-dimensional vector space. This is not the only valid choice of base dimensions, but it is the one most commonly used. For example, one might choose force, length and mass as the base dimensions (as some have done), with associated dimensions F, L, M; this corresponds to a different basis, and one may convert between these representations by a change of basis. The choice of the base set of dimensions is thus a convention, with the benefit of increased utility and familiarity. The choice of base dimensions is not arbitrary, because the dimensions must form a basis: they must span the space, and be linearly independent.\n\nFor example, F, L, M form a set of fundamental dimensions because they form a basis that is equivalent to M, L, T: the former can be expressed as [F = ML/T], L, M, while the latter can be expressed as M, L, [T = (ML/F)].\n\nOn the other hand, length, velocity and time do not form a set of as base dimensions, for two reasons:\n\nDepending on the field of physics, it may be advantageous to choose one or another extended set of dimensional symbols. In electromagnetism, for example, it may be useful to use dimensions of M, L, T, and Q, where Q represents the dimension of electric charge. In thermodynamics, the base set of dimensions is often extended to include a dimension for temperature, Θ. In chemistry, the amount of substance (the number of molecules divided by the Avogadro constant, ≈ ) is defined as a base dimension, N, as well.\nIn the interaction of relativistic plasma with strong laser pulses, a dimensionless relativistic similarity parameter, connected with the symmetry properties of the collisionless Vlasov equation, is constructed from the plasma-, electron- and critical-densities in addition to the electromagnetic vector potential. The choice of the dimensions or even the number of dimensions to be used in different fields of physics is to some extent arbitrary, but consistency in use and ease of communications are common and necessary features.\n\nScalar arguments to transcendental functions such as exponential, trigonometric and logarithmic functions, or to inhomogeneous polynomials, must be dimensionless quantities. (Note: this requirement is somewhat relaxed in Siano's orientational analysis described below, in which the square of certain dimensioned quantities are dimensionless.)\nWhile most mathematical identities about dimensionless numbers translate in a straightforward manner to dimensional quantities, care must be taken with logarithms of ratios: the identity log(a/b) = log a − log b, where the logarithm is taken in any base, holds for dimensionless numbers a and b, but it does \"not\" hold if a and b are dimensional, because in this case the left-hand side is well-defined but the right-hand side is not.\n\nSimilarly, while one can evaluate monomials (\"x\") of dimensional quantities, one cannot evaluate polynomials of mixed degree with dimensionless coefficients on dimensional quantities: for \"x\", the expression (3 m) = 9 m makes sense (as an area), while for \"x\" + \"x\", the expression (3 m) + 3 m = 9 m + 3 m does not make sense.\n\nHowever, polynomials of mixed degree can make sense if the coefficients are suitably chosen physical quantities that are not dimensionless. For example,\n\nThis is the height to which an object rises in time \"t\" if the acceleration of gravity is 32 feet per second per second and the initial upward speed is 500 feet per second. It is not even necessary for \"t\" to be in \"seconds\". For example, suppose \"t\" = 0.01 minutes. Then the first term would be\n\nThe value of a dimensional physical quantity \"Z\" is written as the product of a unit [\"Z\"] within the dimension and a dimensionless numerical factor, \"n\".\n\nWhen like-dimensioned quantities are added or subtracted or compared, it is convenient to express them in consistent units so that the numerical values of these quantities may be directly added or subtracted. But, in concept, there is no problem adding quantities of the same dimension expressed in different units. For example, 1 meter added to 1 foot is a length, but one cannot derive that length by simply adding 1 and 1. A conversion factor, which is a ratio of like-dimensioned quantities and is equal to the dimensionless unity, is needed:\n\nThe factor formula_27 is identical to the dimensionless 1, so multiplying by this conversion factor changes nothing. Then when adding two quantities of like dimension, but expressed in different units, the appropriate conversion factor, which is essentially the dimensionless 1, is used to convert the quantities to identical units so that their numerical values can be added or subtracted.\n\nOnly in this manner is it meaningful to speak of adding like-dimensioned quantities of differing units.\n\nSome discussions of dimensional analysis implicitly describe all quantities as mathematical vectors. (In mathematics scalars are considered a special case of vectors; vectors can be added to or subtracted from other vectors, and, inter alia, multiplied or divided by scalars. If a vector is used to define a position, this assumes an implicit point of reference: an origin. While this is useful and often perfectly adequate, allowing many important errors to be caught, it can fail to model certain aspects of physics. A more rigorous approach requires distinguishing between position and displacement (or moment in time versus duration, or absolute temperature versus temperature change).\n\nConsider points on a line, each with a position with respect to a given origin, and distances among them. Positions and displacements all have units of length, but their meaning is not interchangeable:\nThis illustrates the subtle distinction between \"affine\" quantities (ones modeled by an affine space, such as position) and \"vector\" quantities (ones modeled by a vector space, such as displacement).\n\nProperly then, positions have dimension of \"affine\" length, while displacements have dimension of \"vector\" length. To assign a number to an \"affine\" unit, one must not only choose a unit of measurement, but also a point of reference, while to assign a number to a \"vector\" unit only requires a unit of measurement.\n\nThus some physical quantities are better modeled by vectorial quantities while others tend to require affine representation, and the distinction is reflected in their dimensional analysis.\n\nThis distinction is particularly important in the case of temperature, for which the numeric value of absolute zero is not the origin 0 in some scales. For absolute zero,\nbut for temperature differences,\n(Here °R refers to the Rankine scale, not the Réaumur scale).\nUnit conversion for temperature differences is simply a matter of multiplying by, e.g., 1 °F / 1 K (although the ratio is not a constant value). But because some of these scales have origins that do not correspond to absolute zero, conversion from one temperature scale to another requires accounting for that. As a result, simple dimensional analysis can lead to errors if it is ambiguous whether 1 K means the absolute temperature equal to −272.15 °C, or the temperature difference equal to 1 °C.\n\nSimilar to the issue of a point of reference is the issue of orientation: a displacement in 2 or 3 dimensions is not just a length, but is a length together with a \"direction\". (This issue does not arise in 1 dimension, or rather is equivalent to the distinction between positive and negative.) Thus, to compare or combine two dimensional quantities in a multi-dimensional space, one also needs an orientation: they need to be compared to a frame of reference.\n\nThis leads to the extensions discussed below, namely Huntley's directed dimensions and Siano's orientational analysis.\n\nWhat is the period of oscillation of a mass attached to an ideal linear spring with spring constant suspended in gravity of strength ? That period is the solution for of some dimensionless equation in the variables , , , and .\nThe four quantities have the following dimensions: [T]; [M]; [M/T]; and [L/T]. From these we can form only one dimensionless product of powers of our chosen variables, formula_28 = formula_29 , and putting formula_30 for some dimensionless constant gives the dimensionless equation sought. The dimensionless product of powers of variables is sometimes referred to as a dimensionless group of variables; here the term \"group\" means \"collection\" rather than mathematical group. They are often called dimensionless numbers as well.\n\nNote that the variable does not occur in the group. It is easy to see that it is impossible to form a dimensionless product of powers that combines with , , and , because is the only quantity that involves the dimension L. This implies that in this problem the is irrelevant. Dimensional analysis can sometimes yield strong statements about the \"irrelevance\" of some quantities in a problem, or the need for additional parameters. If we have chosen enough variables to properly describe the problem, then from this argument we can conclude that the period of the mass on the spring is independent of : it is the same on the earth or the moon. The equation demonstrating the existence of a product of powers for our problem can be written in an entirely equivalent way: formula_31, for some dimensionless constant κ (equal to formula_32 from the original dimensionless equation).\n\nWhen faced with a case where dimensional analysis rejects a variable (, here) that one intuitively expects to belong in a physical description of the situation, another possibility is that the rejected variable is in fact relevant, but that some other relevant variable has been omitted, which might combine with the rejected variable to form a dimensionless quantity. That is, however, not the case here.\n\nWhen dimensional analysis yields only one dimensionless group, as here, there are no unknown functions, and the solution is said to be \"complete\" – although it still may involve unknown dimensionless constants, such as .\n\nConsider the case of a vibrating wire of length \"ℓ\" (L) vibrating with an amplitude \"A\" (L). The wire has a linear density \"ρ\" (M/L) and is under tension \"s\" (ML/T), and we want to know the energy \"E\" (ML/T) in the wire. Let \"π\" and \"π\" be two dimensionless products of powers of the variables chosen, given by\n\nThe linear density of the wire is not involved. The two groups found can be combined into an equivalent form as an equation\n\nwhere \"F\" is some unknown function, or, equivalently as\n\nwhere \"f\" is some other unknown function. Here the unknown function implies that our solution is now incomplete, but dimensional analysis has given us something that may not have been obvious: the energy is proportional to the first power of the tension. Barring further analytical analysis, we might proceed to experiments to discover the form for the unknown function \"f\". But our experiments are simpler than in the absence of dimensional analysis. We'd perform none to verify that the energy is proportional to the tension. Or perhaps we might guess that the energy is proportional to \"ℓ\", and so infer that . The power of dimensional analysis as an aid to experiment and forming hypotheses becomes evident.\n\nThe power of dimensional analysis really becomes apparent when it is applied to situations, unlike those given above, that are more complicated, the set of variables involved are not apparent, and the underlying equations hopelessly complex. Consider, for example, a small pebble sitting on the bed of a river. If the river flows fast enough, it will actually raise the pebble and cause it to flow along with the water. At what critical velocity will this occur? Sorting out the guessed variables is not so easy as before. But dimensional analysis can be a powerful aid in understanding problems like this, and is usually the very first tool to be applied to complex problems where the underlying equations and constraints are poorly understood. In such cases, the answer may depend on a dimensionless number such as the Reynolds number, which may be interpreted by dimensional analysis.\n\nConsider the case of a thin, solid, parallel-sided rotating disc of axial thickness \"t\" (L) and radius \"R\" (L). The disc has a density \"ρ\" (M/L), rotates at an angular velocity \"ω\" (T) and this leads to a stress \"S\" (MLT) in the material. There is a theoretical linear elastic solution, given by Lame, to this problem when the disc is thin relative to its radius, the faces of the disc are free to move axially, and the plane stress constitutive relations can be assumed to be valid. As the disc becomes thicker relative to the radius then the plane stress solution breaks down. If the disc is restrained axially on its free faces then a state of plane strain will occur. However, if this is not the case then the state of stress may only be determined though consideration of three-dimensional elasticity and there is no known theoretical solution for this case. An engineer might, therefore, be interested in establishing a relationship between the five variables. Dimensional analysis for this case leads to the following (5 − 3 = 2) non-dimensional groups:\n\nThrough the use of numerical experiments using, for example, the finite element method, the nature of the relationship between the two non-dimensional groups can be obtained as shown in the figure. As this problem only involves two non-dimensional groups, the complete picture is provided in a single plot and this can be used as a design/assessment chart for rotating discs\n\nHuntley has pointed out that it is sometimes productive to refine our concept of dimension. Two possible refinements are:\n\nAs an example of the usefulness of the first refinement, suppose we wish to calculate the distance a cannonball travels when fired with a vertical velocity component formula_36 and a horizontal velocity component formula_37, assuming it is fired on a flat surface. Assuming no use of directed lengths, the quantities of interest are then formula_37, formula_36, both dimensioned as LT, , the distance travelled, having dimension L, and the downward acceleration of gravity, with dimension LT.\n\nWith these four quantities, we may conclude that the equation for the range may be written:\n\nOr dimensionally\n\nfrom which we may deduce that formula_42 and formula_43, which leaves one exponent undetermined. This is to be expected since we have two fundamental dimensions L and T, and four parameters, with one equation.\n\nIf, however, we use directed length dimensions, then formula_37 will be dimensioned as LT, formula_36 as LT, as L and as LT. The dimensional equation becomes:\n\nand we may solve completely as formula_47, formula_48 and formula_49. The increase in deductive power gained by the use of directed length dimensions is apparent.\n\nIn a similar manner, it is sometimes found useful (e.g., in fluid mechanics and thermodynamics) to distinguish between mass as a measure of inertia (inertial mass), and mass as a measure of quantity (substantial mass). For example, consider the derivation of Poiseuille's Law. We wish to find the rate of mass flow of a viscous fluid through a circular pipe. Without drawing distinctions between inertial and substantial mass we may choose as the relevant variables\n\nThere are three fundamental variables so the above five equations will yield two dimensionless variables which we may take to be formula_52 and formula_53 and we may express the dimensional equation as\n\nwhere and are undetermined constants. If we draw a distinction between inertial mass with dimension formula_55 and substantial mass with dimension formula_56, then mass flow rate and density will use substantial mass as the mass parameter, while the pressure gradient and coefficient of viscosity will use inertial mass. We now have four fundamental parameters, and one dimensionless constant, so that the dimensional equation may be written:\n\nwhere now only is an undetermined constant (found to be equal to formula_58 by methods outside of dimensional analysis). This equation may be solved for the mass flow rate to yield Poiseuille's law.\n\nHuntley's extension has some serious drawbacks:\n\n\nIt also is often quite difficult to assign the L, L, L, L, symbols to the physical variables involved in the problem of interest. He invokes a procedure that involves the \"symmetry\" of the physical problem. This is often very difficult to apply reliably: It is unclear as to what parts of the problem that the notion of \"symmetry\" is being invoked. Is it the symmetry of the physical body that forces are acting upon, or to the points, lines or areas at which forces are being applied? What if more than one body is involved with different symmetries?\n\nConsider the spherical bubble attached to a cylindrical tube, where one wants the flow rate of air as a function of the pressure difference in the two parts. What are the Huntley extended dimensions of the viscosity of the air contained in the connected parts? What are the extended dimensions of the pressure of the two parts? Are they the same or different? These difficulties are responsible for the limited application of Huntley's addition to real problems.\n\nAngles are, by convention, considered to be dimensionless variables, and so the use of angles as physical variables in dimensional analysis can give less meaningful results. As an example, consider the projectile problem mentioned above. Suppose that, instead of the x- and y-components of the initial velocity, we had chosen the magnitude of the velocity and the angle at which the projectile was fired. The angle is, by convention, considered to be dimensionless, and the magnitude of a vector has no directional quality, so that no dimensionless variable can be composed of the four variables , , , and . Conventional analysis will correctly give the powers of and , but will give no information concerning the dimensionless angle .\n\nNote that the orientational symbols form a group (the Klein four-group or \"Viergruppe\"). In this system, scalars always have the same orientation as the identity element, independent of the \"symmetry of the problem\". Physical quantities that are vectors have the orientation expected: a force or a velocity in the z-direction has the orientation of . For angles, consider an angle that lies in the z-plane. Form a right triangle in the z-plane with being one of the acute angles. The side of the right triangle adjacent to the angle then has an orientation and the side opposite has an orientation . Then, since we conclude that an angle in the xy-plane must have an orientation , which is not unreasonable. Analogous reasoning forces the conclusion that has orientation while has orientation 1. These are different, so one concludes (correctly), for example, that there are no solutions of physical equations that are of the form , where and are real scalars. Note that an expression such as formula_60 is not dimensionally inconsistent since it is a special case of the sum of angles formula and should properly be written:\n\nwhich for formula_62 and formula_63 yields formula_64. Physical quantities may be expressed as complex numbers (e.g. formula_65) which imply that the complex quantity has an orientation equal to that of the angle it is associated with ( in the above example).\n\nThe assignment of orientational symbols to physical quantities and the requirement that physical equations be orientationally homogeneous can actually be used in a way that is similar to dimensional analysis to derive a little more information about acceptable solutions of physical problems. In this approach one sets up the dimensional equation and solves it as far as one can. If the lowest power of a physical variable is fractional, both sides of the solution is raised to a power such that all powers are integral. This puts it into \"normal form\". The orientational equation is then solved to give a more restrictive condition on the unknown powers of the orientational symbols, arriving at a solution that is more complete than the one that dimensional analysis alone gives. Often the added information is that one of the powers of a certain variable is even or odd.\n\nAs an example, for the projectile problem, using orientational symbols, θ, being in the xy-plane will thus have dimension and the range of the projectile will be of the form:\n\nDimensional homogeneity will now correctly yield and , and orientational homogeneity requires that be an odd integer. In fact the required function of theta will be which is a series of odd powers of .\n\nIt is seen that the Taylor series of and are orientationally homogeneous using the above multiplication table, while expressions like and are not, and are (correctly) deemed unphysical.\n\nIt should be clear that the multiplication rule used for the orientational symbols is not the same as that for the cross product of two vectors. The cross product of two identical vectors is zero, while the product of two identical orientational symbols is the identity element.\n\nThe dimensionless constants that arise in the results obtained, such as the C in the Poiseuille's Law problem and the formula_67 in the spring problems discussed above, come from a more detailed analysis of the underlying physics and often arise from integrating some differential equation. Dimensional analysis itself has little to say about these constants, but it is useful to know that they very often have a magnitude of order unity. This observation can allow one to sometimes make \"back of the envelope\" calculations about the phenomenon of interest, and therefore be able to more efficiently design experiments to measure it, or to judge whether it is important, etc.\n\nParadoxically, dimensional analysis can be a useful tool even if all the parameters in the underlying theory are dimensionless, e.g., lattice models such as the Ising model can be used to study phase transitions and critical phenomena. Such models can be formulated in a purely dimensionless way. As we approach the critical point closer and closer, the distance over which the variables in the lattice model are correlated (the so-called correlation length, formula_68 ) becomes larger and larger. Now, the correlation length is the relevant length scale related to critical phenomena, so one can, e.g., surmise on \"dimensional grounds\" that the non-analytical part of the free energy per lattice site should be formula_69 where formula_70 is the dimension of the lattice.\n\nIt has been argued by some physicists, e.g., M. J. Duff, that the laws of physics are inherently dimensionless. The fact that we have assigned incompatible dimensions to Length, Time and Mass is, according to this point of view, just a matter of convention, borne out of the fact that before the advent of modern physics, there was no way to relate mass, length, and time to each other. The three independent dimensionful constants: \"c\", \"ħ\", and \"G\", in the fundamental equations of physics must then be seen as mere conversion factors to convert Mass, Time and Length into each other.\n\nJust as in the case of critical properties of lattice models, one can recover the results of dimensional analysis in the appropriate scaling limit; e.g., dimensional analysis in mechanics can be derived by reinserting the constants \"ħ\", \"c\", and \"G\" (but we can now consider them to be dimensionless) and demanding that a nonsingular relation between quantities exists in the limit formula_71, formula_72 and formula_73. In problems involving a gravitational field the latter limit should be taken such that the field stays finite.\n\nFollowing are tables of commonly occurring expressions in physics, related to the dimensions of energy, momentum, and force.\n\nIf , where \"c\" is the speed of light and \"ħ\" is the reduced Planck constant, and a suitable fixed unit of energy is chosen, then all quantities of length \"L\", mass \"M\" and time \"T\" can be expressed (dimensionally) as a power of energy \"E\", because length, mass and time can be expressed using speed \"v\", action \"S\", and energy \"E\":\n\nthough speed and action are dimensionless ( and ) – so the only remaining quantity with dimension is energy. In terms of powers of dimensions:\n\nThis is particularly useful in particle physics and high energy physics, in which case the energy unit is the electron volt (eV). Dimensional checks and estimates become very simple in this system.\n\nHowever, if electric charges and currents are involved, another unit to be fixed is for electric charge, normally the electron charge \"e\" though other choices are possible.\n\n\n\n\n\n"}
{"id": "18949797", "url": "https://en.wikipedia.org/wiki?curid=18949797", "title": "File sharing", "text": "File sharing\n\nFile sharing is the practice of distributing or providing access to digital media, such as computer programs, multimedia (audio, images and video), documents or electronic books. File sharing may be achieved in a number of ways. Common methods of storage, transmission and dispersion include manual sharing utilizing removable media, centralized servers on computer networks, World Wide Web-based hyperlinked documents, and the use of distributed peer-to-peer networking.\n\nPeer-to-peer file sharing is based on the peer-to-peer (P2P) application architecture. Shared files on the computers of other users are indexed on directory servers. P2P technology was used by popular services like Napster and Limewire. The most popular protocol for P2P sharing is BitTorrent.\n\nCloud-based file syncing and sharing services implement automated file transfers by updating files from a dedicated sharing directory on each user's networked devices. Files placed in this folder also are typically accessible through a website and mobile app, and can be easily shared with other users for viewing or collaboration. Such services have become popular via consumer-oriented file hosting services such as Dropbox and Google Drive.\n\nrsync is a more traditional program released in 1996 which synchronizes files on a direct machine-to-machine basis.\n\nData synchronization in general can use other approaches to share files, such as distributed filesystems, version control, or mirrors.\n\nFiles were first exchanged on removable media. Computers were able to access remote files using filesystem mounting, bulletin board systems (1978), Usenet (1979), and FTP servers (1985). Internet Relay Chat (1988) and Hotline (1997) enabled users to communicate remotely through chat and to exchange files. The mp3 encoding, which was standardized in 1991 and substantially reduced the size of audio files, grew to widespread use in the late 1990s. In 1998, MP3.com and Audiogalaxy were established, the Digital Millennium Copyright Act was unanimously passed, and the first mp3 player devices were launched.\n\nIn June 1999, Napster was released as an unstructured centralized peer-to-peer system, requiring a central server for indexing and peer discovery. It is generally credited as being the first peer-to-peer file sharing system.\n\nGnutella, eDonkey2000, and Freenet were released in 2000, as MP3.com and Napster were facing litigation. Gnutella, released in March, was the first decentralized file sharing network. In the gnutella network, all connecting software was considered equal, and therefore the network had no central point of failure. In July, Freenet was released and became the first anonymity network. In September the eDonkey2000 client and server software was released.\n\nIn 2001, Kazaa and Poisoned for the Mac was released. Its FastTrack network was distributed, though unlike gnutella, it assigned more traffic to 'supernodes' to increase routing efficiency. The network was proprietary and encrypted, and the Kazaa team made substantial efforts to keep other clients such as Morpheus off of the FastTrack network.\n\nIn July 2001, Napster was sued by several recording companies and lost in A&M Records, Inc. v. Napster, Inc.. In the case of Napster, it has been ruled that an online service provider could not use the \"transitory network transmission\" safe harbor in the DMCA if they had control of the network with a server.\n\nShortly after its loss in court, Napster was shut down to comply with a court order. This drove users to other P2P applications and file sharing continued its growth. The Audiogalaxy Satellite client grew in popularity, and the LimeWire client and BitTorrent protocol were released. Until its decline in 2004, Kazaa was the most popular file sharing program despite bundled malware and legal battles in the Netherlands, Australia, and the United States. In 2002, a Tokyo district court ruling shut down File Rogue, and the Recording Industry Association of America (RIAA) filed a lawsuit that effectively shut down Audiogalaxy.\n\nFrom 2002 through 2003, a number of BitTorrent services were established, including Suprnova.org, isoHunt, TorrentSpy, and The Pirate Bay. In 2002, the RIAA was filing lawsuits against Kazaa users. As a result of such lawsuits, many universities added file sharing regulations in their school administrative codes (though some students managed to circumvent them during after school hours). With the shutdown of eDonkey in 2005, eMule became the dominant client of the eDonkey network. In 2006, police raids took down the Razorback2 eDonkey server and temporarily took down The Pirate Bay.\n\n“The File Sharing Act was launched by Chairman Towns in 2009, this act prohibited the use of applications that allowed individuals to share federal information amongst one another. On the other hand, only specific file sharing application were made available to federal computers” (United States.Congress.House). In 2009, the Pirate Bay trial ended in a guilty verdict for the primary founders of the tracker. The decision was appealed, leading to a second guilty verdict in November 2010. In October 2010, Limewire was forced to shut down following a court order in Arista Records LLC v. Lime Group LLC but the gnutella network remains active through open source clients like Frostwire and gtk-gnutella. Furthermore, multi-protocol file sharing software such as MLDonkey and Shareaza adapted in order to support all the major file sharing protocols, so users no longer had to install and configure multiple file sharing programs.\n\nOn January 19, 2012, the United States Department of Justice shut down the popular domain of Megaupload (established 2005). The file sharing site has claimed to have over 50,000,000 people a day. Kim Dotcom (formerly Kim Schmitz) was arrested with three associates in New Zealand on January 20, 2012 and is awaiting extradition. The case involving the downfall of the world's largest and most popular file sharing site was not well received, with hacker group Anonymous bringing down several sites associated with the take-down. In the following days, other file sharing sites began to cease services; Filesonic blocked public downloads on January 22, with Fileserve following suit on January 23.\n\nIn 2004 there were an estimated 70 million people participating in online file sharing. According to a CBS News poll in 2009, 58% of Americans who follow the file sharing issue, considered it acceptable \"if a person owns the music CD and shares it with a limited number of friends and acquaintances\"; with 18- to 29-year-olds this percentage reached as much as 70%.\n\nIn his survey of file-sharing culture, Caraway (2012) noted that 74.4% of participants believed musicians should accept file sharing as a means for promotion and distribution.\n\nAccording to David Glenn, writing in \"The Chronicle of Higher Education\", \"A majority of economic studies have concluded that file sharing hurts sales\". A literature review by Professor Peter Tschmuck found 22 independent studies on the effects of music file sharing. \"Of these 22 studies, 14 – roughly two-thirds – conclude that unauthorized downloads have a 'negative or even highly negative impact' on recorded music sales. Three of the studies found no significant impact while the remaining five found a positive impact.\"\n\nA study by economists Felix Oberholzer-Gee and Koleman Strumpf in 2004 concluded that music file sharing's effect on sales was \"statistically indistinguishable from zero\". This research was disputed by other economists, most notably Stan Liebowitz, who said Oberholzer-Gee and Strumpf had made multiple assumptions about the music industry \"that are just not correct.\" In June 2010, \"Billboard\" reported that Oberholzer-Gee and Strumpf had \"changed their minds\", now finding \"no more than 20% of the recent decline in sales is due to sharing\". However, citing Nielsen SoundScan as their source, the co-authors maintained that illegal downloading had not deterred people from being original. \"In many creative industries, monetary incentives play a reduced role in motivating authors to remain creative. Data on the supply of new works are consistent with the argument that file sharing did not discourage authors and publishers. Since the advent of file sharing, the production of music, books, and movies has increased sharply.\" Glenn Peoples of \"Billboard\" disputed the underlying data, saying \"SoundScan's number for new releases in any given year represents new commercial titles, not necessarily new creative works.\" The RIAA likewise responded that \"new releases\" and \"new creative works\" are two separate things. \"[T]his figure includes re-releases, new compilations of existing songs, and new digital-only versions of catalog albums. SoundScan has also steadily increased the number of retailers (especially non-traditional retailers) in their sample over the years, better capturing the number of new releases brought to market. What Oberholzer and Strumpf found was better ability to track new album releases, not greater incentive to create them.\"\n\nA 2006 study prepared by Birgitte Andersen and Marion Frenz, published by Industry Canada, was \"unable to discover any direct relationship between P2P file-sharing and CD purchases in Canada\". The results of this survey were similarly criticized by academics and a subsequent revaluation of the same data by Dr. George R. Barker of the Australian National University reached the opposite conclusion. \"In total, 75% of P2P downloaders responded that if P2P were not available they would have purchased either through paid sites only (9%), CDs only (17%) or through CDs and pay sites (49%). Only 25% of people say they would not have bought the music if it were not available on P2P for free.\" Barker thus concludes; \"This clearly suggests P2P network availability is reducing music demand of 75% of music downloaders which is quite contrary to Andersen and Frenz's much published claim.\"\n\nAccording to the 2017 paper \"Estimating displacement rates of copyrighted content in the EU\" by the European Commission, illegal usage increases game sales, stating \"The overall conclusion is that for games, illegal online transactions induce more legal transactions.\"\n\nA paper in journal \"\" found that file sharing decreased the chance of survival for low ranked albums on music charts and increased exposure to albums that were ranked high on the music charts, allowing popular and well known artists to remain on the music charts more often. This had a negative impact for new and less known artists while promoting the work of already popular artists and celebrities.\n\nA more recent study that examined pre-release file sharing of music albums, using BitTorrent software, also discovered positive impacts for \"established and popular artists but not newer and smaller artists.\" According to Robert G. Hammond of North Carolina State University, an album that leaked one month early would see a modest increase in sales. \"This increase in sales is small relative to other factors that have been found to affect album sales.\"\n\n\"File-sharing proponents commonly argue that file sharing democratizes music consumption by 'leveling the playing field' for new/small artists relative to established/popular artists, by allowing artists to have their work heard by a wider audience, lessening the advantage held by established/popular artists in terms of promotional and other support. My results suggest that the opposite is happening, which is consistent with evidence on file-sharing behavior.\"\n\n\"Billboard\" cautioned that this research looked only at the pre-release period and not continuous file sharing following a release date. \"The problem in believing piracy helps sales is deciding where to draw the line between legal and illegal ... Implicit in the study is the fact that both buyers and sellers are required in order for pre-release file sharing to have a positive impact on album sales. Without iTunes, Amazon and Best Buy, file-sharers would be just file sharers rather than purchasers. If you carry out the 'file sharing should be legal' argument to its logical conclusion, today's retailers will be tomorrow's file-sharing services that integrate with their respective cloud storage services.\"\n\nMany argue that file-sharing has forced the owners of entertainment content to make it more widely available legally through fee or advertising on demand on the internet. In a 2011 report by Sandvine showed that Netflix traffic had come to surpass that of Bittorrent.\n\nFile sharing raises copyright issues and has led to many lawsuits. In the United States, some of these lawsuits have even reached the Supreme Court. For example, in \"MGM v. Grokster\", the Supreme Court ruled that the creators of P2P networks can be held liable if their software is marketed as a tool for copyright infringement.\n\nOn the other hand, not all file sharing is illegal. Content in the public domain can be freely shared. Even works covered by copyright can be shared under certain circumstances. For example, some artists, publishers, and record labels grant the public a license for unlimited distribution of certain works, sometimes with conditions, and they advocate free content and file sharing as a promotional tool.\n\n\n"}
{"id": "56951215", "url": "https://en.wikipedia.org/wiki?curid=56951215", "title": "Highlighter (cosmetics)", "text": "Highlighter (cosmetics)\n\nHighlighter is a type of cosmetic product that reflects light. Often used for contouring, it can applied to the face or other parts of the body to brighten the skin on a given area, create the perception of depth and angles. The product can come in a variety of forms, including powder, liquid, cream, gloss, solid stick and jelly.\n\nHighlighters became a significant tool among theater and film actors shooting or performing indoors, where natural light was not available to provide definition of facial features like cheekbones, nose, and jawline. Highlighter also offered the possibility of heightening or diminishing a given feature to suit the character portrayed, as well as aesthetic trends.\n\nMAC Cosmetics's Strobing Cream is credited as the first highlighter product to be made available to commercial consumers.\n\nA long-time tool among makeup artists, highlighter sales saw significant gains among consumers in 2016. Some industry experts have attributed the growth in sales of highlighters and related products to the rise of social media usage, given the popularity of YouTube makeup tutorials and the proliferation of snapshot self-portraits known as selfies.\n\n"}
{"id": "24271197", "url": "https://en.wikipedia.org/wiki?curid=24271197", "title": "IQAN", "text": "IQAN\n\nIQAN is a trademark for electronic control systems for mobile machinery, owned by Parker Hannifin corporation.\n\nThe CAN bus based system IQAN was developed by a small Swedish company around 1990-95. First version was introduced on the market 1995. Shortly after, around 1995 the Swedish company VOAC Hydraulics Co. bought this company and in February 1996 VOAC Hydraulics was acquired by Parker Hannifin. The IQAN-system was integrated into Parkers product range of system components for mobile machinery. The system has been was further developed within the Parker organization.\n\n"}
{"id": "1444272", "url": "https://en.wikipedia.org/wiki?curid=1444272", "title": "Jeton", "text": "Jeton\n\nJetons or jettons are tokens or coin-like medals produced across Europe from the 13th through the 17th centuries. They were produced as counters for use in calculation on a counting board, a lined board similar to an abacus. They also found use as a money substitute in games, similar to modern casino chips or poker chips. \n\nThousands of different jetons exist, mostly of religious and educational designs, as well as portraits, the last of which most resemble coinage, somewhat similar to modern, non-circulation commemorative coins. The spelling \"jeton\" is from the French; the English spell it \"jetton\".\n\nThe Romans similarly used pebbles (in Latin: \"calculi\" \"little stones\", whence English \"calculate\"). Addition is straightforward, and relatively efficient algorithms for multiplication and division were known.\n\nAs Arabic numerals and the zero came into use, \"pen reckoning\" gradually displaced \"counter casting\" as the common accounting method. Jetons for calculation were commonly used in Europe from about 1200 to 1400, and remained in occasional use into the early nineteenth century.\n\nFrom the late 13th century to the end of the 14th century, purpose-made jetons were produced in England, similar in design to contemporary Edwardian pennies. Although they were made of brass they were often pierced or indented at the centre to avoid them being plated with silver and pass them off as real silver coins. By the middle of the 14th century, English jetons were being produced at a larger size, similar to the groat. \n\nThroughout the 15th century, competition from France and the Low Countries ended jeton manufacture in England, but this did not last long. Nuremberg jeton masters initially started by copying counters of their European neighbours, but by the mid 16th century they gained a monopoly by mass-producing cheaper jetons for commercial use. Later – \"counter casting\" being obsolete – the production shifted to jetons for use in games and toys, sometimes copying more or less famous jetons with a political background as the following.\n\nIn the Low Countries, the respective mints in the late Middle Ages in general produced the counters for the official bookkeeping. These mostly show the effigy of the ruler within a flattering text and on the reverse the ruler's escutcheon and the name or city of the accounting office.\n\nDuring the Dutch Revolt (1568–1609) this pattern changed and by both parties, the North in front, about 2,000 different, mostly political, jetons (Dutch: \"Rekenpenning\") were minted depicting the victories, ideals and aims. Specifically in the last quarter of the 16th century, where \"Geuzen\" or \"beggars\" made important military contributions to the Dutch side and bookkeeping was already done without counters, the production in the North was just for propaganda.\n\nThe mints and treasuries of the big estates in Central Europe used their own jetons and then had a number of them struck in gold and silver as New Year gifts for their employees who in turn commissioned jetons with their own mottoes and coats-of-arms. In the sixteenth century, the Czech Royal Treasury bought between two and three thousand pieces at the beginning of each year.\n\nIn the 21st century, Jetons continue to be used in some countries to denominate the substitutes for coins in coin-operated public telephones or vending machines, because automatic valuation of coins by machines is unreliable or impossible due to several factors. They are usually made of metal or hard plastic, and are generally called tokens in English-speaking countries.\n\nIn France and other countries, \"jeton\" is also a small (as a token, so to speak) amount of money paid to members of a society or a legislative chamber each time they are present in a meeting.\n\nIn the German language, the word \"Jeton\" refers specifically to casino tokens.\n\nIn the Polish language, the word \"żeton\", pronounced similarly to French \"jeton\", refers both to tokens used in the vending machines, phones etc., as well to those used in the casinos. The word \"жетон\" has the same use in the Russian language, as does the word \"jeton\" in the Romanian language and \"žetoon\" in the Estonian language.\n\nIn the Hungarian language the word \"zseton\" is (somewhat dated) slang for money, particularly coins.\n\n\n"}
{"id": "20390154", "url": "https://en.wikipedia.org/wiki?curid=20390154", "title": "List of A1 weapons", "text": "List of A1 weapons\n\nThis is a list of weapons or firearms designated A1 or A-1 :\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "489256", "url": "https://en.wikipedia.org/wiki?curid=489256", "title": "List of Canon products", "text": "List of Canon products\n\nThe following provides a partial list of products manufactured under the Canon brand.\n\nOther products manufactured and/or service-rendered under the Canon brand may not appear here. Such products may include office or industrial application devices, wireless LAN products, and semiconductor and precision products.\n\n\n\n\n\nSeiki Kogaku (now Canon) began to develop and subsequently to produce rangefinder cameras with the Kwanon prototype in 1933, based on the Leica II 35mm camera, with separate rangefinder and view finder systems (3 windows). Production began with the Hansa Canon on the Leica III format through WWII. Post war Canon resumed production of pre-war designs in early 1946 with the JII viewfinder and the S1 rangefinder. But in late 1946 they introduced the SII which departed from the Leica design by offering a combined viewfinder/rangefinder system, reducing the windows on the front of the camera to two. However, in most other respects these cameras remained visually similar to the Leica III.\n\n\nIn 1956, Canon departed from the Leica II Style and developed a more contemporary look, along with a Contax style self-timer level to the left of the lens mount. This was the first Canon camera with a swing-open camera back for film loading. Upper end models had a new three-mode viewfinders and winding triggers.\n\n\nCanon partnered with US manufacturer Bell & Howell between 1961–1976 and a few Canon products were sold in the US under the Bell & Howell brand e.g. Canon 7 Rangefinder, Canon EX-EE, and the Canon TX.\n\n\"(See also:)\"\n\nCanon developed and produced the Canon R lens mount for film SLR cameras in 1959. The FL lens mount replaced R-mounts in 1964.\n\nDetails \n\nCanon developed and produced the Canon FL lens-mount standard for film SLR cameras from 1964 to replace the Canon R lens-mount standard. The FD lens mount standard replaced FL-mounts in 1971.\n\nIn 1969 Canon introduced an economy camera/lens system where the rear three elements (in two groups) were built-on-to the camera, and several front element options could be interchanged. This had been used by Zeiss-Ikon in their mid-level cameras of their Contaflex series, and by Kodak in early interchangeable lenses for the top-end Retina series (later going to full lenses). Canon offered four lens options: 35mm f/3.5, 50mm f/1.8, 95mm f/3.5, and 1255mm f/3.5.\n\nThrough the lens metering was center weighted and automatic exposure was shutter speed priority. Only two cameras were offered and the line was not successful.\n\n\nCanon developed and produced the Canon FD lens mount standard for film SLR cameras from 1971 to replace the FL lens mount standard.\nThe FD mount had two variants – original lenses used a breechlock collar to mount whilst later versions used a standard bayonet twist lock with a short twist action.\nThe EF lens mount standard superseded FD-mounts in 1987. Canon ceased to produce FD-mount cameras in 1994.\n\n\n\n\nIn 1987, Canon introduced the EOS Single-lens reflex camera system along with the EF lens-mount standard to replace the 16-year-old FD lens-mount standard; EOS became the sole SLR camera-system used by Canon . Canon also used EOS for its digital SLR cameras. All current film and digital SLR cameras produced by Canon use the EOS autofocus system. Canon introduced this system in 1987 along with the EF lens mount standard. The last non-EOS based SLR camera produced by Canon, the Canon T90 of 1986, is widely regarded as the template for the EOS line of camera bodies, although the T90 employed the older FD lens-mount standard.\n\nFor a detailed list of EOS Film and digital SLR cameras, see \"Canon EOS\".\n\n\"See Canon EOS\"\n\n\n\"See Canon EOS\"\n\n\n\n\n\n\n\n\n\"US names listed\"\n\n\n\n\n\n\n\n\nCanon 7d\n\n\n\n\n\n\n\n\nSpeedlite 300EZ,\nSpeedlite 420EZ,\nSpeedlite 430EZ,\nSpeedlite 540EZ\n\nThe 300T is a layover from the FD system, it was introduced with the FD mount Canon T90, but is compatible in TTL mode with most non-digital EF cameras.\n\nMacro Twin Lite MT-24EX\n, Macro Ring Lite MR-14EX\n, Macro Ring Lite ML-3\n\n\nThe \"iR\" series uses Ultra Fast Rendering (UFR) printing system, and some models use UFR II, a page description language.\n\n\n\n\nStarWriter Jet 300 — a word processor and \"Personal Publishing System\".\n\nBeginning in Spring 1993, Canon produced a series of notebooks with integrated inkjet printers called NoteJet. The initial price for the first-model NoteJet was U.S. $2,499. The NoteJet lineup was eventually discontinued, and computers belonging to the series are valued by collectors.\n\n\nCanon printers are supplied with Canon Advanced Printing Technology (CAPT), a printer driver software-stack developed by Canon. The company claims that its use of data compression reduces their printer's memory requirement, good quality compared to conventional laser printers, and also claim that it increases the data transfer rate when printing high-resolution graphics.\n\n\n\nCanon refers to inkjet printers as \"bubblejets\", hence the frequent BJC-prefix.\n\n\nIn Japan, the models are denoted with a trailing “i”, whereas in the rest of the world they are denoted with a leading “i”. While the 50i corresponds to the i70, for all other corresponding models the numerical model numbers are identical.\nThe “X” denotes models sold under special dispensation by retail outles in Europe.\n\n\n\nSince about 2005 Canon introduced a numbering scheme for some whereby the least significant (non-zero) digit signifies the geographic region (“3” signifying Japan) the device is sold in. This leads to a large number of models, all belonging to the same family, but possibly incompatible to some degree, and also makes it difficult to ascertain whether a device is unique or part of an existing family. The software driver filename will often use the family designation.\n\nSome MP devices have fax capability (MP740).\nR=remote\n\nThe DS700 and DS810 are inkjet printers, all the other models are thermal dye-sublimation printers using ALPS technology.\n\n\n\"See Canon EF lenses for the product line-up\".\n\"See Canon EF-S lenses for the product line-up\".\n\nEF-S lenses are built for APS-C 1.6x crop sensors, so it will only work with models that use this sensor size, such as: Canon EOS Digital Rebel series (300D through 750D and 760D, 100D, and 1000D through 1200D), and newer cameras in the prosumer Canon EOS Digital series (20D through 80D, 20Da, 60Da, 7D, and 7D MkII). When EF-S lenses are used on a 35mm (full frame) camera, the back element will hit the mirror assembly or cause massive amounts of vignetting since the sensor is bigger than the image produced by the lens.\n\n\"See Canon FD lenses for the product line-up\".\n\n\"See Canon FL lenses for the product line-up\".\n\n\n\n\"Note: Even though the tilt-shift and dedicated macro lenses are designated TS-E and MP-E respectively, these lenses are still compatible with the EF mount.\"\n\n\n\nApplications bundled with Canon Digital Cameras and printers include:\n\nCanon TrueType Font Pack is a floppy disk collection of supplementar truetype fonts for some Canon printers of years '90 and useful for Windows 3.1 and 95.\n\nThe fonts contained in the collection was:\n\nCanon TrueType Font Pack is a floppy disk collection of supplementar truetype fonts for some Canon printers of years '90 and useful for Windows 3.1 and 95.\n\nThe fonts contained in the collection was:\n\n\n"}
{"id": "41331239", "url": "https://en.wikipedia.org/wiki?curid=41331239", "title": "List of monitors with QHD display", "text": "List of monitors with QHD display\n\nAs of late 2013, monitors with a QHD (Quad HD, 2560×1440) display are becoming more common. This is a list of monitors that have such displays.\n\nThe following monitors and all-in-one desktop computers have QHD displays:\n\n"}
{"id": "4997399", "url": "https://en.wikipedia.org/wiki?curid=4997399", "title": "Luggage lock", "text": "Luggage lock\n\nA luggage lock is a lock used to prevent luggage from opening by accident, usually with little or no security in mind, although they may serve as a deterrent to potential thieves. They may be built into luggage, or may be external locks such as padlocks or lockable straps. They are typically relatively simple low security locks.\n\nLuggage locks are typically low security locks. The shackles have a small diameter, and are easy to clip using bolt cutters or similar equipment. Luggage locks based on a pin tumbler lock design usually use only three or four pins, making them susceptible to lockpicking, even with tools as simple as a bent paperclip.\n\nIn the United States the Transportation Security Administration (TSA) requires access to luggage without the passenger being present; to allow travelers to lock their luggage they have accepted certain locks which they can open and relock. The TSA recommend that TSA accepted locks be used, as luggage locked using other methods must be opened by force in order to be inspected.\n\nLuggage locks accepted by the TSA can be opened by the authorities using publicly known universal \"master\" keys. Locks using this system are produced to meet standards set by Travel Sentry. Under agreement with the TSA, it is Travel Sentry that sets the standards for these locks and approves each design. Every lock with the Travel Sentry identifying mark (red diamond) is accepted by the TSA.\n\nSome TSA accepted locks feature an indicator which will appear red if opened by a master key, so the owner will know that their bag has been accessed.\n\n"}
{"id": "10806161", "url": "https://en.wikipedia.org/wiki?curid=10806161", "title": "Medical calculator", "text": "Medical calculator\n\nA medical calculator is a type of medical computer software, whose purpose is to allow easy calculation of various scores and indices, presenting the user with a friendly interface that hides the complexity of the formulas. Most offer further helpful information such as result interpretation guides and medical literature references. Generally, such calculators are intended for use by health care professionals, and use by the general public is discouraged.\n\nMedical calculators arose because modern medicine makes frequent use of scores and indices that put physicians' memory and calculation skills to the test. The advent of personal computers, the Internet and Web, and more recently personal digital assistants (PDAs) have formed an environment conducive to their development, spread and use.\n\nPurpose-built devices for specific medical calculations are available from various commercial sources. Pharma-Insight Inc. in Canada is one of the only companies in the world that is able to make custom specific medical calculators built to perform a specific medical calculation to make dosing or other calculation easy. Some of the standard units they make include eGFR, CrCl, BMI, BSA, DAS and many other custom units designed for a specific purpose. There are two ways to make a calculator using an array that looks up an answer based on a large array of data or where the calculator computes the answer using a mathematical equation.\n\nSoftware-based medical calculators are available for various PDA-platforms, including the iPhone, Palm and Pocket PC. Handheld battery powered portable units are available and can be manufactured in smaller quantities than before thanks to OTP (one Time Programmable) chips.\n\nVarious websites are available that provide calculations from a browser based input form.\n"}
{"id": "66924", "url": "https://en.wikipedia.org/wiki?curid=66924", "title": "Memory management", "text": "Memory management\n\nMemory management is a form of resource management applied to computer memory. The essential requirement of memory management is to provide ways to dynamically allocate portions of memory to programs at their request, and free it for reuse when no longer needed. This is critical to any advanced computer system where more than a single process might be underway at any time.\n\nSeveral methods have been devised that increase the effectiveness of memory management. Virtual memory systems separate the memory addresses used by a process from actual physical addresses, allowing separation of processes and increasing the size of the virtual address space beyond the available amount of RAM using paging or swapping to secondary storage. The quality of the virtual memory manager can have an extensive effect on overall system performance.\n\nApplication-level memory management is generally categorized as either automatic memory management, usually involving garbage collection, or manual memory management.\n\nThe task of fulfilling an allocation request consists of locating a block of unused memory of sufficient size. Memory requests are satisfied by allocating portions from a large pool of memory called the \"heap\" or \"free store\". At any given time, some parts of the heap are in use, while some are \"free\" (unused) and thus available for future allocations.\n\nSeveral issues complicate the implementation, such as external fragmentation, which arises when there are many small gaps between allocated memory blocks, which invalidates their use for an allocation request. The allocator's metadata can also inflate the size of (individually) small allocations. This is often managed by chunking. The memory management system must track outstanding allocations to ensure that they do not overlap and that no memory is ever \"lost\" (i.e. that there be no \"memory leak\").\n\nThe specific dynamic memory allocation algorithm implemented can impact performance significantly. A study conducted in 1994 by Digital Equipment Corporation illustrates the overheads involved for a variety of allocators. The lowest average instruction path length required to allocate a single memory slot was 52 (as measured with an instruction level profiler on a variety of software).\n\nSince the precise location of the allocation is not known in advance, the memory is accessed indirectly, usually through a pointer reference. The specific algorithm used to organize the memory area and allocate and deallocate chunks is interlinked with the kernel, and may use any of the following methods:\n\nFixed-size blocks allocation, also called memory pool allocation, uses a free list of fixed-size blocks of memory (often all of the same size). This works well for simple embedded systems where no large objects need to be allocated, but suffers from fragmentation, especially with long memory addresses. However, due to the significantly reduced overhead this method can substantially improve performance for objects that need frequent allocation / de-allocation and is often used in video games.\n\nIn this system, memory is allocated into several pools of memory instead of just one, where each pool represents blocks of memory of a certain power of two in size, or blocks of some other convenient size progression. All blocks of a particular size are kept in a sorted linked list or tree and all new blocks that are formed during allocation are added to their respective memory pools for later use. If a smaller size is requested than is available, the smallest available size is selected and split. One of the resulting parts is selected, and the process repeats until the request is complete. When a block is allocated, the allocator will start with the smallest sufficiently large block to avoid needlessly breaking blocks. When a block is freed, it is compared to its buddy. If they are both free, they are combined and placed in the correspondingly larger-sized buddy-block list.\n\nThis memory allocation mechanism preallocates memory chunks suitable to fit objects of a certain type or size. These chunks are called caches and the allocator only has to keep track of a list of free cache slots. Constructing an object will use the any one of the free cache slots and destructing an object will add a slot back to the free cache slot list. This technique alleviates memory fragmentation and is efficient as there is no need to search for a suitable portion of memory, as any open slot will suffice.\n\nIn many programming language implementations, all variables declared within a procedure (subroutine, or function) are local to that function; the runtime environment for the program automatically allocates memory for these variables on program execution entry to the procedure, and automatically releases that memory when the procedure is exited. Special declarations may allow local variables to retain values between invocations of the procedure, or may allow local variables to be accessed by other procedures. The automatic allocation of local variables makes recursion possible, to a depth limited by available memory. \n\nGarbage collection is a strategy for automatically detecting memory allocated to objects that are no longer usable in a program, and returning that allocated memory to a pool of free memory locations. This method is in contrast to \"manual\" memory management where a programmer explicitly codes memory requests and memory releases in the program. While automatic garbage has the advantages of reducing programmer workload and preventing certain kinds of memory allocation bugs, garbage collection does require memory resources of its own, and can compete with the application program for processor time.\n\nVirtual memory is a method of decoupling the memory organization from the physical hardware. The applications operate on memory via \"virtual addresses\". Each attempt by the application to access a particular virtual memory address results in the virtual memory address being translated to an actual \"physical address\". In this way the addition of virtual memory enables granular control over memory systems and methods of access.\n\nIn virtual memory systems the operating system limits how a process can access the memory. This feature, called memory protection, can be used to disallow a process to read or write to memory that is not allocated to it, preventing malicious or malfunctioning code in one program from interfering with the operation of another.\n\nEven though the memory allocated for specific processes is normally isolated, processes sometimes need to be able to share information. Shared memory is one of the fastest techniques for inter-process communication.\n\nMemory is usually classified by access rate into primary storage and secondary storage. Memory management systems, among other operations, also handle the moving of information between these two levels of memory.\n\n\n\n\n"}
{"id": "1633640", "url": "https://en.wikipedia.org/wiki?curid=1633640", "title": "Modern Marvels", "text": "Modern Marvels\n\nModern Marvels is an American worldwide television series that formerly aired on the History Channel. The program focuses on how technologies affect and are used in modern society. It is among History's first, and longest-running programs, having first aired on History's first day of broadcasting on January 1, 1995, and its last episode aired on April 11, 2015.\n\n\"Modern Marvels\" has produced over 650 one-hour episodes covering various topics involving (to list a few) science, technology, electronics, mechanics, engineering, architecture, industry, mass production, manufacturing, and agriculture. Each episode typically discusses the history and production of several related items; for instance, an episode on distilled spirits discusses the production of bourbon in Kentucky, scotch whisky in Scotland, and tequila in Mexico. To fit the network's format, \"Modern Marvels\" focuses a significant portion of the episode on the history of the subject.\n\nThe show began to premiere new episodes in January 2010, not having done so through all of 2009. In August 2010, History Channel began to air older episodes that had been edited to fit a 30-minute time slot, under the title \"Modern Marvels: Essentials\".\n\nIn October 2011, \"Modern Marvels\" began airing first-run episodes on History 2 (formerly History International) in addition to its main run on History Channel.\n\nReruns of the series currently air on the digital broadcast network Quest.\n\nOccasionally, \"Modern Marvels\" aired a special spin-off called \"Engineering Disasters\". These periodic episodes describe the circumstances of situations in which technology does not work correctly, such as building collapses and airplane crashes, resulting in spectacular (and sometimes fatal) failures. Including an episode on New Orleans and another on the 1970s, 24 original \"Engineering Disasters\" episodes have been on \"Modern Marvels\". The packaging on the box set of \"Engineering Disasters\" 4-20 plus New Orleans describes the series as such: \"Dark clouds with silver linings, \"Modern Marvels: Engineering Disasters\" presents the tragic, yet invaluable, handmaidens of technological progress.\" \n\nDistinct from other History Channel series, the introduction of \"Modern Marvels\" features visuals and sounds of a bolt being turned by a ratchet wrench, followed by a partially computer-generated sequence involving construction workers building and hanging the title.\n\nSeveral narrators were used in the history of the series. The last and longest-running is Max Raphael, who has also narrated other History Channel series, such as \"Command Decisions.\"\n\nThe History Channel has repackaged some episodes that originally aired in other series and stand-alone specials into episodes of \"Modern Marvels\", such as \"Ice Road Truckers\", which originally aired in 2000 as part of the series \"Suicide Missions\". These episodes are not narrated by Raphael.\n\nBruce Nash is credited with creating the series. Don Cambou has acted as executive producer on over 350 episodes for Actuality Productions, the production company behind the series.\n\n\n"}
{"id": "3214453", "url": "https://en.wikipedia.org/wiki?curid=3214453", "title": "Moisture vapor transmission rate", "text": "Moisture vapor transmission rate\n\nMoisture vapor transmission rate (MVTR), also water vapor transmission rate (WVTR), is a measure of the passage of water vapor through a substance.\n\nThere are many industries where moisture control is critical. Moisture sensitive foods and pharmaceuticals are put in packaging with controlled MVTR to achieve the required quality, safety, and shelf life. In clothing, MVTR as a measure of breathability has contributed to greater comfort for wearers of clothing for outdoor activity. The building materials industry also manages the moisture barrier properties in architectural components to ensure the correct moisture levels in the internal spaces of buildings. Optoelectronic devices based on organic material, generally named OLEDs, need an encapsulation with low values of WVTR to guarantee same performances over the device lifetime.\n\nThere are various techniques to measure MVTR, ranging from gravimetric techniques that measure the gain or loss of moisture by mass, to highly sophisticated instrumental techniques that in some designs can measure extremely low transmission rates. Special care has to be taken in measuring porous substances such as fabrics, as some techniques are not appropriate. For very low levels, many techniques do not have adequate resolution. Numerous standard methods are described in ISO, ASTM, BS, DIN etc.—these are quite often industry-specific. Instrument manufacturers are often able to provide test methods developed to fully exploit the specific design which they are selling. The search for the most appropriate instrument is a zealous task which is in itself part of the measurement.\n\nThe conditions under which the measurement is made has a considerable influence on the result. Both the temperature and humidity gradients across the sample need to be measured, controlled and recorded with the result. An MVTR result without specifying these conditions is almost meaningless. Certainly no two results should be compared unless the conditions are known. \nThe most common international unit for the MVTR is g/m²/day. In the USA, g/100in²/day is also in use, which is 0.064516 (approximately 1/15) of the value of g/m²/day units. Typical rates in aluminium foil laminates may be as low as 0.001 g/m²/day, whereas the rate in fabrics can measure up to several thousand g/m²/day.\n\nOften, barrier testing is conducted on a sheet of material. Calculations based on that can be useful when designing completed structures, clothing, and packages. Seams, creases, access points, and heat seals are critical to end-use performance. For example, the glass of a bottle may have an effective total barrier, but the screw cap closure and the closure liner might not. Performance verification and validation of complete containers, structures, or irregular objects is often recommended.\n\nFor the special case of OLEDs, where the levels of allowed permeation are in the 10 g/m²/day level \n, \nthe methods preferred exploit an oxidation of a metal upon the exposure to water\n\n\n\nFor drug filing in the US, USP standards are mandatory and must be performed accordingly.\n\n\n"}
{"id": "1077086", "url": "https://en.wikipedia.org/wiki?curid=1077086", "title": "Monolithic Memories", "text": "Monolithic Memories\n\nMonolithic Memories, Inc. (MMI) produced bipolar PROMs, programmable logic devices, and logic circuits (including 7400 series TTL).\n\nA team of MMI engineers,under the direction of Ze'ev Drori and headed by John Birkner and H. T. Chua, invented the class of devices known as Programmable Array Logic (PAL).\n\nMMI was founded in 1969 by former Fairchild Semiconductor engineer Ze'ev Drori, later the President and CEO of Tesla Motors. In 1987, under the stewardship of President Irwin Federman, it was merged with Advanced Micro Devices in a $442 million stock swap to become the world's largest integrated circuit manufacturer. AMD later spun off their programmable logic division as Vantis, which was then acquired by Lattice Semiconductor.\n\n"}
{"id": "50969004", "url": "https://en.wikipedia.org/wiki?curid=50969004", "title": "Movie theaters in Stockholm", "text": "Movie theaters in Stockholm\n\nThe first attempts in building permanent movie theaters in Stockholm were made in the end of the 1890s when the cinematograph had been demonstrated at General Art and Industrial Exposition of Stockholm in 1897. Prior to the demonstration, only travellers demonstrated the concept of the movie theater as a form of entertainment.\n\nIn 1905, the city had ten movie theaters, and by the end of 1909 the number had risen to 25 permanent movie theaters. The highest number of movie theaters operating at the same time in Stockholm City was 110. In 2009, the number of movie theatres had declined to a dozen or less. The highest number of people visiting movie theaters in Stockholm was 16,8 million, which was noted in 1956.\n\nThe city's oldest movie theater, which still operates today is Zita, which opened under the name \"Vinter-Palatset \"(\"the Winter Palace\") in 1913. The most beautiful movie theater is the Skandia Theater, which opened in 1923 and was built according to architect Gunnar Asplunds plans. It was described by art experts at the time as a masterpiece, and is one of the few remaining movie theaters in the city that can only show one film at a time. The Skandia Theater was renovated to its original condition in 2001, and is protected under the Cultural Environment Protection Act.\n\n"}
{"id": "9117110", "url": "https://en.wikipedia.org/wiki?curid=9117110", "title": "Nair (hair removal)", "text": "Nair (hair removal)\n\nNair is a hair removal product manufactured by Church & Dwight. It was purchased from Carter-Wallace in 2001.\n\nThe brand is mainly known for its depilatories that work by breaking the disulfide bonds of the keratin molecules in hair. This reduces the tensile strength of the keratin so greatly that the hair can be wiped away. Nair's slogans include: \"The Less That You Wear, the More You Need Nair!\"; \"Like Never Before\"; and \"We wear short shorts, Nair for short shorts\". The initial ad for the \"short shorts\" commercial won a Clio. It was based on the 1958 song \"Short Shorts\". Nair is a portmanteau of \"No hair.\"\n\nTwo of the active ingredients are calcium hydroxide (slaked lime) and sodium hydroxide (lye), which raise the pH of hair, chemically breaking it down. Some formulations also contain potassium thioglycolate, which breaks down the disulphide bonds in the hair's keratin thus weakening it enough to be simply wiped away. Products such as Nair often combine softening agents such as mineral oil to help offset the harsh active ingredients.\n\n"}
{"id": "33511110", "url": "https://en.wikipedia.org/wiki?curid=33511110", "title": "Nanofoundry", "text": "Nanofoundry\n\nA nanofoundry is considered to be a foundry that performs on a scale similar to nanotechnology. This concept makes it similar to the role that the nanofactory would play because it is considered to be a factory that operates on that same scale model. The closest thing that nature has to a nanofoundry is the simple biological cell. \n\n\"In silico\" biology attempts to duplicate nature by creating a virtual cell with the complete cycle of metabolism. The idea of creating an artificial cell along with working nanofoundries is highlighted in the phenomena of bioconvergence; which may advance us from the Information Age to the \"Nanotechnology Age.\" Nanofoundries and artificial cells are creating a world where health care, the very definition of \"medicine\", along with life itself is entering a state of transition. This phenomenon is directly in parallel with changes in the procedures used in agriculture, managing our bioresources, ultimately leading up to the \"de facto\" equivalent of bio-engineering entire ecosystems from scratch.\n\nOn a larger scale, materials that appear to be smooth still have an abrasive appearance to them. Using the nanoscale, however, atoms rub off one a time. This creates new challenges for researchers who build their devices that are only 10 atoms wide.\n\nOne of the first nanofoundries has been set up at the University of Madras in Chennai, Tamil Nadu, India. Knowledge about nanotechnology would be converted into useful consumer goods through the usage of nanofoundries. Scientists do not want nanotechnology to be confined to publishing research papers in journals when it could be useful for creating nanotechnology-enhanced consumer products that would be beneficial in our 21st century society. By converting the nanotechnology curriculum of the major universities into a more industry-oriented format, it makes the technology more practical for employers as well as consumers.\n\nThe ability to grow more complex structures with a high ratio allows for drug release devices, biosensors, nanoreactors, and other countless discoveries. During the following decades to come, researchers will scramble to construct the world's first nuclear nanobeam complex. This facility would offer state-of-the-art facilities to a wide range of disciplines; including the conventional sciences.\n\nCommercial manufacturing could easily be scaled up thanks to nanofoundries. Nanofactories will most likely use metal nanoparticles instead of glass, plastic or rare earth minerals that are currently used to make most of our products.\n"}
{"id": "98581", "url": "https://en.wikipedia.org/wiki?curid=98581", "title": "Perfume", "text": "Perfume\n\nPerfume (, ; ) is a mixture of fragrant essential oils or aroma compounds, fixatives and solvents, used to give the human body, animals, food, objects, and living-spaces an agreeable scent.\nIt is usually in liquid form and used to give a pleasant scent to a person's body.\nAncient texts and archaeological excavations show the use of perfumes in some of the earliest human civilizations. Modern perfumery began in the late 19th century with the commercial synthesis of aroma compounds such as vanillin or coumarin, which allowed for the composition of perfumes with smells previously unattainable solely from natural aromatics alone.\n\nThe word \"perfume\" derives from the Latin \"perfumare\", meaning \"to smoke through\". Perfumery, as the art of making perfumes, began in ancient Mesopotamia and Egypt, and was further refined by the Romans and the Arabs.\n\nThe world's first-recorded chemist is considered a woman named Tapputi, a perfume maker mentioned in a cuneiform tablet from the 2nd millennium BC in Mesopotamia. She distilled flowers, oil, and calamus with other aromatics, then filtered and put them back in the still several times.\n\nIn India, perfume and perfumery existed in the Indus civilization (3300 BC – 1300 BC). One of the earliest distillations of Ittar was mentioned in the Hindu Ayurvedic text \"Charaka Samhita\" and \"Sushruta Samhita\".\n\nIn 2003, archaeologists uncovered what are believed to be the world's oldest surviving perfumes in Pyrgos, Cyprus. The perfumes date back more than 4,000 years. They were discovered in an ancient perfumery, a factory housing at least 60 stills, mixing bowls, funnels, and perfume bottles. In ancient times people used herbs and spices, such as almond, coriander, myrtle, conifer resin, and bergamot, as well as flowers. In May 2018, an ancient perfume “Rodo” (Rose) was recreated for the Greek National Archaeological Museum's anniversary show “Countless Aspects of Beauty”, allowing visitors to approach antiquity through their olfaction receptors. \n\nIn the 9th century the Arab chemist Al-Kindi (Alkindus) wrote the \"Book of the Chemistry of Perfume and Distillations\", which contained more than a hundred recipes for fragrant oils, salves, aromatic waters, and substitutes or imitations of costly drugs. The book also described 107 methods and recipes for perfume-making and perfume-making equipment, such as the alembic (which still bears its Arabic name. [from Greek ἄμβιξ, \"cup\", \"beaker\"] described by Synesius in the 4th century).\n\nThe Persian chemist Ibn Sina (also known as Avicenna) introduced the process of extracting oils from flowers by means of distillation, the procedure most commonly used today. He first experimented with the rose. Until his discovery, liquid perfumes consisted of mixtures of oil and crushed herbs or petals, which made a strong blend. Rose water was more delicate, and immediately became popular. Both the raw ingredients and the distillation technology significantly influenced western perfumery and scientific developments, particularly chemistry.\n\nThe art of perfumery was known in western Europe from 1221, taking into account the monks' recipes of Santa Maria delle Vigne or Santa Maria Novella of Florence, Italy. In the east, the Hungarians produced in 1370 a perfume made of scented oils blended in an alcohol solution – best known as Hungary Water – at the behest of Queen Elizabeth of Hungary. The art of perfumery prospered in Renaissance Italy, and in the 16th century the personal perfumer to Catherine de' Medici (1519–1589), Rene the Florentine (Renato il fiorentino), took Italian refinements to France. His laboratory was connected with her apartments by a secret passageway, so that no formulae could be stolen en route. Thanks to Rene, France quickly became one of the European centers of perfume and cosmetics manufacture. Cultivation of flowers for their perfume essence, which had begun in the 14th century, grew into a major industry in the south of France.\n\nBetween the 16th and 17th centuries, perfumes were used primarily by the wealthy to mask body odors resulting from infrequent bathing. Partly due to this patronage, the perfume industry developed. In 1693, Italian barber Giovanni Paolo Feminis created a perfume water called Aqua Admirabilis,\ntoday best known as eau de cologne; his nephew Johann Maria Farina (Giovanni Maria Farina) took over the business in 1732.\n\nBy the 18th century the Grasse region of France, Sicily, and Calabria (in Italy) were growing aromatic plants to provide the growing perfume industry with raw materials. Even today, Italy and France remain the center of European perfume design and trade.\n\nPerfume types reflect the concentration of aromatic compounds in a solvent, which in fine fragrance is typically ethanol or a mix of water and ethanol. Various sources differ considerably in the definitions of perfume types. The intensity and longevity of a perfume is based on the concentration, intensity and longevity of the aromatic compounds, or perfume oils, used. As the percentage of aromatic compounds increases, so does the intensity and longevity of the scent. Specific terms are used to describe a fragrance's approximate concentration by the percent of perfume oil in the volume of the final product. The most widespread terms are:\n\nThere is much confusion over the term \"cologne\", which has three meanings. The first and oldest definition refers to a family of fresh, citrus-based fragrances distilled using extracts from citrus, floral, and woody ingredients. Supposedly these were first developed in the early 18th century in Cologne, Germany, hence the name. This type of \"classical cologne\" describes unisex compositions \"which are basically citrus blends and do not have a perfume parent.\" Examples include Mäurer & Wirtz's \"4711\" (created in 1799), and Guerlain's \"Eau de Cologne Impériale\" (1853).\n\nIn the 20th century, the term took on a second meaning. Fragrance companies began to offer lighter, less concentrated interpretations of their existing perfumes, making their products available to a wider range of customers. Guerlain, for example, offered an Eau de Cologne version of its flagship perfume \"Shalimar\". In contrast to classical colognes, this type of modern cologne is a lighter, diluted, less concentrated interpretation of a more concentrated product, typically a pure parfum. The cologne version is often the lightest concentration from a line of fragrance products.\n\nFinally, the term \"cologne\" has entered the English language as a generic, overarching term to denote a fragrance worn by a man, regardless of its concentration. The actual product worn by a man may technically be an eau de toilette, but he may still say that he \"wears cologne\". A similar problem surrounds the term \"perfume\", which can be used a generic sense to refer to fragrances marketed to women, whether or not the fragrance is actually an extrait.\n\nClassical colognes first appeared in Europe in the 17th century. The first fragrance labeled a \"parfum\" extract with a high concentration of aromatic compounds was Guerlain's \"Jicky\" in 1889. Eau de Toilette appeared alongside parfum around the turn of the century. The EdP concentration and terminology is the most recent. Parfum de toilette and EdP began to appear in the 1970s and gained popularity in the 1980s.\n\nThe wide range in the percentages of aromatic compounds that may be present in each concentration means that the terminology of extrait, EdP, EdT, and EdC is quite imprecise. Although an EdP will often be more concentrated than an EdT and in turn an EdC, this is not always the case. Different perfumeries or perfume houses assign different amounts of oils to each of their perfumes. Therefore, although the oil concentration of a perfume in EdP dilution will necessarily be higher than the same perfume in EdT from within a company's same range, the actual amounts vary among perfume houses. An EdT from one house may have a higher concentration of aromatic compounds than an EdP from another.\n\nFurthermore, some fragrances with the same \"product name\" but having a different \"concentration\" may not only differ in their dilutions, but actually use different perfume oil mixtures altogether. For instance, in order to make the EdT version of a fragrance brighter and fresher than its EdP, the EdT oil may be \"tweaked\" to contain slightly more top notes or fewer base notes. \"Chanel No. 5\" is a good example: its parfum, EdP, EdT, and now-discontinued EdC concentrations are in fact different compositions (the parfum dates to 1921, whereas the EdP was not developed until the 1980s). In some cases, words such as \"extrême\", \"intense\", or \"concentrée\" that might indicate a higher aromatic concentration are actually completely different fragrances, related only because of a similar perfume \"accord\". An example of this is Chanel's \"Pour Monsieur\" and \"Pour Monsieur Concentrée\".\n\nAs a rule of thumb, women's fragrances tend to have higher levels of aromatic compounds than men's fragrances. Fragrances marketed to men are typically sold as EdT or EdC, rarely as EdP or perfume extracts. Women's fragrances used to be common in all levels of concentration, but today are mainly seen in parfum, EdP and EdT concentrations.\n\nPerfume oils are often diluted with a solvent, though this is not always the case, and its necessity is disputed. By far the most common solvent for perfume is oil dilution is an alcohol solution, typically a mixture of ethanol and water or a rectified spirit. Perfume oil can also be diluted by means of neutral-smelling oils such as fractionated coconut oil, or liquid waxes such as jojoba oil.\n\nThe conventional application of pure perfume (parfum extrait) in Western cultures is at pulse points, such as behind the ears, the nape of the neck, and the insides of wrists, elbows and knees, so that the pulse point will warm the perfume and release fragrance continuously. According to perfumer Sophia Grojsman behind the knees is the ideal point to apply perfume in order that the scent may rise. The modern perfume industry encourages the practice of layering fragrance so that it is released in different intensities depending upon the time of the day. Lightly scented products such as bath oil, shower gel, and body lotion are recommended for the morning; eau de toilette is suggested for the afternoon; and perfume applied to the pulse points for evening. Cologne fragrance is released rapidly, lasting around 2 hours. Eau de toilette lasts from 2 to 4 hours, while perfume may last up to six hours.\n\nA variety of factors can influence how fragrance interacts with the wearer's own physiology and affect the perception of the fragrance. Diet is one factor, as eating spicy and fatty foods can increase the intensity of a fragrance.<ref name=\"Fragrance Info / FAQs\"></ref> The use of medications can also impact the character of a fragrance. The relative dryness of the wearer's skin is important, since dry skin will not hold fragrance as long as skin with more oil.\n\nThe precise formulae of commercial perfumes are kept secret. Even if they were widely published, they would be dominated by such complex ingredients and odorants that they would be of little use in providing a guide to the general consumer in description of the \"experience\" of a scent. Nonetheless, connoisseurs of perfume can become extremely skillful at identifying components and origins of scents in the same manner as wine experts.\n\nThe most practical way to start describing a perfume is according to the elements of the \"fragrance notes\" of the scent or the \"family\" it belongs to, all of which affect the overall impression of a perfume from first application to the last lingering hint of scent.\n\nThe trail of scent left behind by a person wearing perfume is called its \"sillage\", after the French word for \"wake\", as in the trail left by a boat in water.\n\nPerfume is described in a musical metaphor as having three sets of \"notes\", making the harmonious scent \"accord\". The notes unfold over time, with the immediate impression of the top note leading to the deeper middle notes, and the base notes gradually appearing as the final stage. These notes are created carefully with knowledge of the evaporation process of the perfume.\nThe scents in the top and middle notes are influenced by the base notes; conversely, the scents of the base notes will be altered by the types of fragrance materials used as middle notes. Manufacturers who publish perfume notes typically do so with the fragrance components presented as a \"fragrance pyramid\", using imaginative and abstract terms for the components listed.\n\nGrouping perfumes can never be a completely objective or final process. Many fragrances contain aspects of different families. Even a perfume designated as \"single flower\", however subtle, will have undertones of other aromatics. \"True\" unitary scents can rarely be found in perfumes as it requires the perfume to exist only as a singular aromatic material.\n\nClassification by olfactive family is a starting point for a description of a perfume, but it cannot by itself denote the specific characteristic of that perfume.\n\nThe traditional classification which emerged around 1900 comprised the following categories:\n\nSince 1945, due to great advances in the technology of perfume creation (i.e., compound design and synthesis) as well as the natural development of styles and tastes, new categories have emerged to describe modern scents:\n\nThe Fragrance wheel is a relatively new classification method that is widely used in retail and in the fragrance industry. The method was created in 1983 by Michael Edwards, a consultant in the perfume industry, who designed his own scheme of fragrance classification. The new scheme was created in order to simplify fragrance classification and naming scheme, as well as to show the relationships between each of the individual classes.\n\nThe five standard families consist of \"Floral\", \"Oriental\", \"Woody\", \"Aromatic Fougère\", and \"Fresh\", with the first four families borrowing from the classic terminology and the last consisting of newer bright and clean smelling citrus and oceanic fragrances that have arrived in the past generation due to improvements in fragrance technology. Each of the families are in turn divided into subgroups and arranged around a wheel.\nIn this classification scheme, \"Chanel No.5\", which is traditionally classified as an aldehydic floral, would be located under the Soft Floral sub-group, and amber scents would be placed within the Oriental group. As a class, chypre perfumes are more difficult to place since they would be located under parts of the Oriental and Woody families. For instance, Guerlain's \"Mitsouko\" is placed under Mossy Woods, but Hermès \"Rouge\", a chypre with more floral character, would be placed under Floral Oriental.\n\nPlants have long been used in perfumery as a source of essential oils and aroma compounds. These aromatics are usually secondary metabolites produced by plants as protection against herbivores, infections, as well as to attract pollinators. Plants are by far the largest source of fragrant compounds used in perfumery. The sources of these compounds may be derived from various parts of a plant. A plant can offer more than one source of aromatics, for instance the aerial portions and seeds of coriander have remarkably different odors from each other. Orange leaves, blossoms, and fruit zest are the respective sources of petitgrain, neroli, and orange oils.\n\n\n\nMany modern perfumes contain synthesized odorants. Synthetics can provide fragrances which are not found in nature. For instance, Calone, a compound of synthetic origin, imparts a fresh ozonous metallic marine scent that is widely used in contemporary perfumes. Synthetic aromatics are often used as an alternate source of compounds that are not easily obtained from natural sources. For example, linalool and coumarin are both naturally occurring compounds that can be inexpensively synthesized from terpenes. Orchid scents (typically \"salicylates\") are usually not obtained directly from the plant itself but are instead synthetically created to match the fragrant compounds found in various orchids.\n\nOne of the most commonly used classes of synthetic aromatics by far are the white musks. These materials are found in all forms of commercial perfumes as a neutral background to the middle notes. These musks are added in large quantities to laundry detergents in order to give washed clothes a lasting \"clean\" scent.\n\nThe majority of the world's synthetic aromatics are created by relatively few . They include:\nEach of these companies patents several processes for the production of aromatic synthetics annually.\n\nNatural and synthetics are used for their different odor characteristics in perfumery\nBefore perfumes can be composed, the odorants used in various perfume compositions must first be obtained. Synthetic odorants are produced through organic synthesis and purified. Odorants from natural sources require the use of various methods to extract the aromatics from the raw materials. The results of the extraction are either essential oils, absolutes, concretes, or butters, depending on the amount of waxes in the extracted product.\n\nAll these techniques will, to a certain extent, distort the odor of the aromatic compounds obtained from the raw materials. This is due to the use of heat, harsh solvents, or through exposure to oxygen in the extraction process which will denature the aromatic compounds, which either change their odor character or renders them odorless.\n\n\nAlthough fragrant extracts are known to the general public as the generic term \"essential oils\", a more specific language is used in the fragrance industry to describe the source, purity, and technique used to obtain a particular fragrant extract.\nOf these extracts, only \"absolutes\", \"essential oils\", and \"tinctures\" are directly used to formulate perfumes.\n\nProducts from different extraction methods are known under different names even though their starting materials are the same. For instance, orange blossoms from \"Citrus aurantium\" that have undergone solvent extraction produces \"orange blossom absolute\" but that which have been steam distilled is known as \"neroli oil\".\n\nPerfume compositions are an important part of many industries ranging from the luxury goods sectors, food services industries, to manufacturers of various household chemicals. The purpose of using perfume or fragrance compositions in these industries is to affect customers through their sense of smell and entice them into purchasing the perfume or perfumed product. As such there is significant interest in producing a perfume formulation that people will find aesthetically pleasing.\n\nThe job of composing perfumes that will be sold is left up to an expert on perfume composition or known in the fragrance industry as the \"perfumer\". They are also sometimes referred to affectionately as a \"Nez\" (French for \"nose\") due to their fine sense of smell and skill in smell composition.\n\nThe composition of a perfume typically begins with a \"brief\" by the perfumer's employer or an outside customer. The customers to the perfumer or their employers, are typically fashion houses or large corporations of various industries. The perfumer will then go through the process of blending multiple perfume mixtures and sell the formulation to the customer, often with modifications of the composition of the perfume. \nThe perfume composition will then be either used to enhance another product as a \"functional fragrance\" (shampoos, make-up, detergents, car interiors, etc.), or marketed and sold directly to the public as a \"fine fragrance\".\n\nAlthough there is no single \"correct\" technique for the formulation of a perfume, there are general guidelines as to how a perfume can be constructed from a concept. Although many ingredients do not contribute to the smell of a perfume, many perfumes include colorants and anti-oxidants to improve the marketability and shelf life of the perfume, respectively.\n\nPerfume oils usually contain tens to hundreds of ingredients and these are typically organized in a perfume for the specific role they will play. These ingredients can be roughly grouped into four groups:\n\nThe top, middle, and base notes of a fragrance may have separate primary scents and supporting ingredients. The perfume's fragrance oils are then blended with ethyl alcohol and water, aged in tanks for several weeks and filtered through processing equipment to, respectively, allow the perfume ingredients in the mixture to stabilize and to remove any sediment and particles before the solution can be filled into the perfume bottles.\n\nInstead of building a perfume from \"ground up\", many modern perfumes and colognes are made using \"fragrance bases\" or simply bases. Each base is essentially modular perfume that is blended from essential oils and aromatic chemicals, and formulated with a simple concept such as \"fresh cut grass\" or \"juicy sour apple\". Many of Guerlain's \"Aqua Allegoria\" line, with their simple fragrance concepts, are good examples of what perfume fragrance bases are like.\n\nThe effort used in developing bases by fragrance companies or individual perfumers may equal that of a marketed perfume, since they are useful in that they are reusable. On top of its reusability, the benefit in using bases for construction are quite numerous:\n\nCreating perfumes through reverse engineering with analytical techniques such as Gas chromatography–mass spectrometry (GC/MS) can reveal the \"general\" formula for any particular perfume. The difficulty of GC/MS analysis arises due to the complexity of a perfume's ingredients. This is particularly due to the presence of natural essential oils and other ingredients consisting of complex chemical mixtures. However, \"anyone armed with good GC/MS equipment and experienced in using this equipment can today, within days, find out a great deal about the formulation of any perfume... customers and competitors can analyze most perfumes more or less precisely.\"\n\nAntique or badly preserved perfumes undergoing this analysis can also be difficult due to the numerous degradation by-products and impurities that may have resulted from breakdown of the odorous compounds. Ingredients and compounds can usually be ruled out or identified using gas chromatograph (GC) smellers, which allow individual chemical components to be identified both through their physical properties and their scent. Reverse engineering of best-selling perfumes in the market is a very common practice in the fragrance industry due to the relative simplicity of operating GC equipment, the pressure to produce marketable fragrances, and the highly lucrative nature of the perfume market.\n\nIt is doubtful whether perfumes qualify as appropriate copyright subject matter under the US Copyright Act. The issue has not yet been addressed by any US court. A perfume's scent is not eligible for trademark protection because the scent serves as the functional purpose of the product.\n\nIn 2006 the Dutch Supreme Court granted copyright protection to Lancôme's perfume \"Tresor\" (\"Lancôme v. Kecofa\"). The French Supreme Court has twice taken the position that perfumes lack the creativity to constitute copyrightable expressions (\"Bsiri-Barbir v. Haarman & Reimer\", 2006; \"Beaute Prestige International v. Senteur Mazal\", 2008).\n\nPerfume ingredients, regardless of natural or synthetic origins, may all cause health or environmental problems when used. Although the areas are under active research, much remains to be learned about the effects of fragrance on human health and the environment.\n\nEvidence in peer-reviewed journals shows that some fragrances can cause asthmatic reactions in some individuals, especially those with severe or atopic asthma. Many fragrance ingredients can also cause headaches, allergic skin reactions or nausea.\n\nIn some cases, an excessive use of perfumes may cause allergic reactions of the skin. For instance, acetophenone, ethyl acetate and acetone while present in many perfumes, are also known or potential respiratory allergens. Nevertheless, this may be misleading, since the harm presented by many of these chemicals (either natural or synthetic) is dependent on environmental conditions and their concentrations in a perfume. For instance, linalool, which is listed as an irritant, causes skin irritation when it degrades to peroxides, however the use of antioxidants in perfumes or reduction in concentrations can prevent this. As well, the furanocoumarin present in natural extracts of grapefruit or celery can cause severe allergic reactions and increase sensitivity to ultraviolet radiation.\n\nSome research on natural aromatics have shown that many contain compounds that cause skin irritation. However some studies, such as IFRA's research claim that opoponax is too dangerous to be used in perfumery, still lack scientific consensus.\nIt is also true that sometimes inhalation alone can cause skin irritation.\nA number of national and international surveys have identified balsam of Peru, often used in perfumes, as being in the \"top five\" allergens most commonly causing patch test reactions in people referred to dermatology clinics. A study in 2001 found that 3.8% of the general population patch tested was allergic to it. Many perfumes contain components identical to balsam of Peru.\n\nBalsam of Peru is used as a marker for perfume allergy. Its presence in a cosmetic is denoted by the INCI term \"Myroxylon pereirae\". Balsam of Peru has been banned by the International Fragrance Association since 1982 from use as a fragrance compound, but may be present as an extract or distillate in other products, where mandatory labelling is not required for usage of 0.4% or less.\n\nThere is scientific evidence that nitro-musks such as musk xylene could cause cancer in some specific animal tests. These reports were evaluated by the EU Scientific Committee for Consumer Safety (SCCS, formerly the SCCNFP ) and musk xylene was found to be safe for continued use in cosmetic products. It is in fact part of the procedures of the Cosmetic Regulation in Europe that materials classified as carcinogens require such a safety evaluation by the authorities to be allowed in cosmetic consumer products.\n\nAlthough other ingredients such as polycyclic synthetic musks, have been reported to be positive in some in-vitro hormone assays, these reports have been reviewed by various authorities. For example, for one of the main polycyclic musks Galaxolide (HHCB) these reviews includes those of the EU Scientific Committee on Consumer Safety, the EU's Priority Substances Review, the EU Scientific Committee on Health and Environmental Risk, and more recently also the US EPA. The outcome of all of these reviews over the past decade or so is that there is no safety concerns for human health. Reviews with similar positive outcome exists for another main polycyclic musk (AHTN) as well for instance on its safe use in cosmetics by the EU.\n\nMany natural aromatics, such as oakmoss absolutes, basil oil, rose oil and many others contain allergens or carcinogenic compounds, the safety of which is either governed by regulations (e.g. allowed methyl eugenol levels in the EU Cosmetics Regulation (Entry102, AnnexIII of the EU Cosmetics Regulation.) or through various limitations set by the International Fragrance Association.\n\nCertain chemicals found in perfume are often toxic, at least for small insects if not for humans. For example, the compound Tricyclodecenyl allyl ether is often found in synthetic perfumes and has insect repellent property.\n\nSynthetic musks are pleasant in smell and relatively inexpensive, as such they are often employed in large quantities to cover the unpleasant scent of laundry detergents and many personal cleaning products. Due to their large-scale use, several types of synthetic musks have been found in human fat and milk, as well as in the sediments and waters of the Great Lakes.\n\nThese pollutants may pose additional health and environmental problems when they enter human and animal diets.\n\nThe demands for aromatic materials such as sandalwood, agarwood, and musk have led to the endangerment of these species, as well as illegal trafficking and harvesting.\n\nThe perfume industry in the US is not directly regulated by the FDA, instead the FDA controls the safety of perfumes through their ingredients and requires that they be tested to the extent that they are \"Generally recognized as safe\" (GRAS). Due to the need for protection of trade secrets, companies rarely give the full listing of ingredients regardless of their effects on health. In Europe, as from 11 March 2005, the mandatory listing of a set of 26 recognized fragrance allergens was enforced. The requirement to list these materials is dependent on the intended use of the final product. The limits above which the allegens are required to be declared are 0.001% for products intended to remain on the skin, and 0.01% for those intended to be rinsed off. This has resulted in many old perfumes like chypres and fougère classes, which require the use of oakmoss extract, being reformulated.\n\nFragrance compounds in perfumes will degrade or break down if improperly stored in the presence of heat, light, oxygen, and extraneous organic materials.\nProper preservation of perfumes involves keeping them away from sources of heat and storing them where they will not be exposed to light. An opened bottle will keep its aroma intact for several years, as long as it is well stored. However, the presence of oxygen in the head space of the bottle and environmental factors will in the long run alter the smell of the fragrance.\n\nPerfumes are best preserved when kept in light-tight aluminium bottles or in their original packaging when not in use, and refrigerated to relatively low temperatures: between 3–7 °C (37–45 °F). Although it is difficult to completely remove oxygen from the headspace of a stored flask of fragrance, opting for spray dispensers instead of rollers and \"open\" bottles will minimize oxygen exposure. Sprays also have the advantage of isolating fragrance inside a bottle and preventing it from mixing with dust, skin, and detritus, which would degrade and alter the quality of a perfume.\n\nThere exist several archives and museums devoted to the preservation of historical perfumes, namely the Osmothèque, which stocks over 3,000 perfumes from the past two millennia in their original formulations. All scents in their collection are preserved in non-actinic glass flasks flushed with argon gas, stored in thermally insulated compartments maintained at in a large vault.\n\n\n\n"}
{"id": "22790367", "url": "https://en.wikipedia.org/wiki?curid=22790367", "title": "Peter Baxandall", "text": "Peter Baxandall\n\nPeter J. Baxandall (1921, Kingston upon Thames, Surrey – 1995, Malvern, Worcestershire) was an English audio engineer and electronics engineer and a pioneer of the use of analog electronics in audio. He is probably best known for what is now called the Baxandall tone control circuit, first published in a paper in \"Wireless World\".\n\nBaxandall attended King's College School in London, then got his BSc in electrical engineering at Cardiff Technical College (1942). He was a radio instructor for the Fleet Air Arm for two years, and then worked for the Telecommunications Research Establishment (at the Circuit Research Division headed by Frederic Calland Williams), later renamed and merged to form the Royal Signals and Radar Establishment, until his retirement in 1971. After retiring he worked as a consultant on various audio projects including loudspeakers, tape duplication, and microphone calibration. During this time he continued to publish, including a \"seminal chapter\" on electrostatic loudspeakers. The Audio Engineering Society made him a Fellow in 1980, and in 1993 awarded him with a Silver Medal for his contributions to the field.\n\nBaxandall's bass and treble circuit, when made public in \"Wireless World\" (1952), \"swept all others before it\". An early version of the design had already won him an award in 1950 (a $25 watch) at the British Sound Recording Association, a predecessor of the Audio Engineering Society. The design is now employed in millions of hi-fi systems (Baxandall received no royalties for his work).\n\nIt exists in several versions—Baxandall's original had two capacitors per potentiometer, but it is possible to use only one at either the treble or bass potentiometers, or both. It finds an application in hi-fi audio equipment and in amplifiers and effects for musical instruments.\n\n"}
{"id": "1358178", "url": "https://en.wikipedia.org/wiki?curid=1358178", "title": "Pulse-Doppler radar", "text": "Pulse-Doppler radar\n\nA pulse-Doppler radar is a radar system that determines the range to a target using pulse-timing techniques, and uses the Doppler effect of the returned signal to determine the target object's velocity. It combines the features of pulse radars and continuous-wave radars, which were formerly separate due to the complexity of the electronics.\n\nPulse-Doppler systems were first widely used on fighter aircraft starting in the 1960s. Earlier radars had used pulse-timing in order to determine range and the angle of the antenna (or similar means) to determine the bearing. However, this only worked when the radar antenna was not pointed down; in that case the reflection off the ground overwhelmed any returns from other objects. As the ground moves at the same speed but opposite direction of the aircraft, Doppler techniques allow the ground return to be filtered out, revealing aircraft and vehicles. This gives pulse-Doppler radars \"look-down/shoot-down\" capability. A secondary advantage in military radar is to reduce the transmitted power while achieving acceptable performance for improved safety of stealthy radar.\n\nPulse-Doppler techniques also find widespread use in meteorological radars, allowing the radar to determine wind speed from the velocity of any precipitation in the air. Pulse-Doppler radar is also the basis of synthetic aperture radar used in radar astronomy, remote sensing and mapping. In air traffic control, they are used for discriminating aircraft from clutter. Besides the above conventional surveillance applications, pulse-Doppler radar has been successfully applied in healthcare, such as fall risk assessment and fall detection, for nursing or clinical purposes.\n\nThe earliest radar systems failed to operate as expected. The reason was traced to Doppler effects that degrade performance of systems not designed to account for moving objects. Fast-moving objects cause a phase-shift on the transmit pulse that can produce signal cancellation. Doppler has maximum detrimental effect on moving target indicator systems, which must use reverse phase shift for Doppler compensation in the detector.\n\nDoppler weather effects (precipitation) were also found to degrade conventional radar and moving target indicator radar, which can mask aircraft reflections. This phenomenon was adapted for use with weather radar in the 1950s after declassification of some World War II systems.\n\nPulse-Doppler radar was developed during World War II to overcome limitations by increasing pulse repetition frequency. This required the development of the klystron, the traveling wave tube, and solid state devices. Pulse-Doppler is incompatible with other high power microwave amplification devices that are not coherent.\n\nEarly examples of military systems include the AN/SPG-51B developed during the 1950s specifically for the purpose of operating in hurricane conditions with no performance degradation.\n\nThe Hughes AN/ASG-18 Fire Control System was a prototype airborne radar/combination system for the planned North American XF-108 Rapier interceptor aircraft for the United States Air Force, and later for the Lockheed YF-12. The US's first pulse-Doppler radar, the system had look-down/shoot-down capability and could track one target at a time.\n\nWeather, chaff, terrain, flying techniques, and stealth are common tactics used to hide aircraft from radar. Pulse-Doppler radar eliminates these weaknesses.\n\nIt became possible to use pulse-Doppler radar on aircraft after digital computers were incorporated in the design. Pulse-Doppler provided look-down/shoot-down capability to support air-to-air missile systems in most modern military aircraft by the mid 1970s.\n\nPulse-Doppler systems measure the range to objects by measuring the elapsed time between sending a pulse of radio energy and receiving a reflection of the object. Radio waves travel at the speed of light, so the distance to the object is the elapsed time multiplied by the speed of light, divided by two - there and back.\n\nPulse-Doppler radar is based on the Doppler effect, where movement in range produces frequency shift on the signal reflected from the target.\n\nRadial velocity is essential for pulse-Doppler radar operation. As the reflector moves between each transmit pulse, the returned signal has a phase difference, or \"phase shift\", from pulse to pulse. This causes the reflector to produce Doppler modulation on the reflected signal.\n\nPulse-Doppler radars exploit this phenomenon to improve performance.\n\nThe amplitude of the successively returning pulse from the same scanned volume is\nwhere\n\nSo\n\nThis allows the radar to separate the reflections from multiple objects located in the same volume of space by separating the objects using a spread spectrum to segregate different signals:\nwhere formula_8 is the phase shift induced by range motion.\n\nRejection speed is selectable on pulse-Doppler aircraft-detection systems so nothing below that speed will be detected. A one degree antenna beam illuminates millions of square feet of terrain at range, and this produces thousands of detections at or below the horizon if Doppler is not used.\n\nPulse-Doppler radar uses the following signal processing criteria to exclude unwanted signals from slow-moving objects. This is also known as clutter rejection. Rejection velocity is usually set just above the prevailing wind speed (10 to 100 mile/hour or 15 to 150 km/hour). The velocity threshold is much lower for weather radar.\n\nIn airborne pulse-Doppler radar, the velocity threshold is offset by the speed of the aircraft relative to the ground.\n\nwhere formula_11 is the angle offset between the antenna position and the aircraft flight trajectory.\n\nSurface reflections appear in almost all radar. Ground clutter generally appears in a circular region within a radius of about near ground-based radar. This distance extends much further in airborne and space radar. Clutter results from radio energy being reflected from the earth surface, buildings, and vegetation. Clutter includes weather in radar intended to detect and report aircraft and spacecraft.\n\nClutter creates a vulnerability region in pulse-amplitude time-domain radar. Non-Doppler radar systems cannot be pointed directly at the ground due to excessive false alarms, which overwhelm computers and operators. Sensitivity must be reduced near clutter to avoid overload. This vulnerability begins in the low-elevation region several beam widths above the horizon, and extends downward. This also exists throughout the volume of moving air associated with weather phenomenon.\n\nPulse-Doppler radar corrects this as follows.\n\nClutter rejection capability of about 60 dB is needed for look-down/shoot-down capability, and pulse-Doppler is the only strategy that can satisfy this requirement. This eliminates vulnerabilities associated with the low-elevation and below-horizon environment.\n\nPulse compression, and moving target indicator (MTI) provide up to 25 dB sub-clutter visibility. MTI antenna beam is aimed above horizon to avoid excessive false alarm rate, which renders systems vulnerable. Aircraft and some missiles exploit this weakness using a technique called flying below the radar to avoid detection (Nap-of-the-earth). This flying technique is ineffective against pulse-Doppler radar.\n\nPulse-Doppler provides an advantage when attempting to detect missiles and low observability aircraft flying near terrain, sea surface, and weather.\n\nAudible Doppler and target size support passive vehicle type classification when identification friend or foe is not available from a transponder signal. Medium pulse repetition frequency (PRF) reflected microwave signals fall between 1,500 and 15,000 cycle per second, which is audible. This means a helicopter sounds like a helicopter, a jet sounds like a jet, and propeller aircraft sound like propellers. Aircraft with no moving parts produce a tone. The actual size of the target can be calculated using the audible signal.\n\nAmbiguity processing is required when target range is above the red line in the graphic, which increases scan time.\n\nScan time is a critical factor for some systems because vehicles moving at or above the speed of sound can travel one mile (1.6 km) every few seconds, like the Exocet, Harpoon, Kitchen, and Air-to-air missile. The maximum time to scan the entire volume of the sky must be on the order of a dozen seconds or less for systems operating in that environment.\n\nPulse-Doppler radar by itself can be too slow to cover the entire volume of space above the horizon unless fan beam is used. This approach is used with the AN/SPS 49(V)5 Very Long Range Air Surveillance Radar, which sacrifices elevation measurement to gain speed.\n\nPulse-Doppler antenna motion must be slow enough so that all the return signals from at least 3 different PRFs can be processed out to the maximum anticipated detection range. This is known as dwell time. Antenna motion for pulse-Doppler must be as slow as radar using MTI.\n\nSearch radar that include pulse-Doppler are usually dual mode because best overall performance is achieved when pulse-Doppler is used for areas with high false alarm rates (horizon or below and weather), while conventional radar will scan faster in free-space where false alarm rate is low (above horizon with clear skies).\n\nThe antenna type is an important consideration for multi-mode radar because undesirable phase shift introduced by the radar antenna can degrade performance measurements for sub-clutter visibility.\n\nThe signal processing enhancement of pulse-Doppler allows small high-speed objects to be detected in close proximity to large slow moving reflectors. To achieve this, the transmitter must be coherent and should produce low phase noise during the detection interval, and the receiver must have large instantaneous dynamic range.\n\n\nPulse-Doppler signal processing also includes ambiguity resolution to identify true range and velocity.\n\n\nThe received signals from multiple PRF are compared to determine true range using the range ambiguity resolution process.\n\n\nThe received signals are also compared using the frequency ambiguity resolution process.\n\n\nThe range resolution is the minimal range separation between two objects traveling at the same speed before the radar can detect two discrete reflections:\n\nThe velocity resolution is the minimal radial velocity difference between two objects traveling at the same range before the radar can detect two discrete reflections:\n\nPulse-Doppler radar has special requirements that must be satisfied to achieve acceptable performance.\n\nPulse-Doppler typically uses medium pulse repetition frequency (PRF) from about 3 kHz to 30 kHz. The range between transmit pulses is 5 km to 50 km.\n\nRange and velocity cannot be measured directly using medium PRF, and ambiguity resolution is required to identify true range and speed. Doppler signals are generally above 1 kHz, which is audible, so audio signals from medium-PRF systems can be used for passive target classification.\n\nRadar systems require angular measurement. Transponders are not normally associated with pulse-Doppler radar, so sidelobe suppression is required for practical operation.\n\nTracking radar systems use angle error to improve accuracy by producing measurements perpendicular to the radar antenna beam. Angular measurements are averaged over a span of time and combined with radial movement to develop information suitable to predict target position for a short time into the future.\n\nThe two angle error techniques used with tracking radar are monopulse and conical scan.\n\n\nPulse-Doppler radar requires a coherent oscillator with very little noise. Phase noise reduces sub-clutter visibility performance by producing apparent motion on stationary objects.\n\nCavity magnetron and crossed-field amplifier are not appropriate because noise introduced by these devices interfere with detection performance. The only amplification devices suitable for pulse-Doppler are klystron, traveling wave tube, and solid state devices.\n\nPulse-Doppler signal processing introduces a phenomenon called scalloping. The name is associated with a series of holes that are scooped-out of the detection performance.\n\nScalloping for pulse-Doppler radar involves blind velocities created by the clutter rejection filter. Every volume of space must be scanned using 3 or more different PRF. A two PRF detection scheme will have detection gaps with a pattern of discrete ranges, each of which has a blind velocity.\n\nRinging artifacts pose a problem with search, detection, and ambiguity resolution in pulse-Doppler radar.\n\nRinging is reduced in two ways.\n\nFirst, the shape of the transmit pulse is adjusted to smooth the leading edge and trailing edge so that RF power is increased and decreased without an abrupt change. This creates a transmit pulse with smooth ends instead of a square wave, which reduces ringing phenomenon that is otherwise associated with target reflection.\n\nSecond, the shape of the receive pulse is adjusted using a window function that minimizes ringing that occurs any time pulses are applied to a filter. In a digital system, this adjusts the phase and/or amplitude of each sample before it is applied to the fast Fourier transform. The Dolph-Chebyshev window is the most effective because it produces a flat processing floor with no ringing that would otherwise cause false alarms.\n\nPulse-Doppler radar is generally limited to mechanically aimed antennas and active phase array.\n\nMechanical RF components, such as wave-guide, can produce Doppler modulation due to phase shift induced by vibration. This introduces a requirement to perform full spectrum operational tests using shake tables that can produce high power mechanical vibration across all anticipated audio frequencies.\n\nDoppler is incompatible with most electronically steered phase-array antenna. This is because the phase-shifter elements in the antenna are non-reciprocal and the phase shift must be adjusted before and after each transmit pulse. Spurious phase shift is produced by the sudden impulse of the phase shift, and settling during the receive period between transmit pulses places Doppler modulation onto stationary clutter. That receive modulation corrupts the measure of performance for sub-clutter visibility. Phase shifter settling time on the order of 50ns is required. Start of receiver sampling needs to be postponed at least 1 phase-shifter settling time-constant (or more) for each 20 dB of sub-clutter visibility.\n\nMost antenna phase shifters operating at PRF above 1 kHz introduce spurious phase shift unless special provisions are made, such as reducing phase shifter settling time to a few dozen nanoseconds.\n\nThe following gives the maximum permissible settling time for antenna phase shift modules.\n\nwhere\n\nThe antenna type and scan performance is a practical consideration for multi-mode radar systems.\n\nChoppy surfaces, like waves and trees, form a diffraction grating suitable for bending microwave signals. Pulse-Doppler can be so sensitive that diffraction from mountains, buildings or wave tops can be used to detect fast moving objects otherwise blocked by solid obstruction along the line of sight. This is a very lossy phenomenon that only becomes possible when radar has significant excess sub-clutter visibility.\n\nRefraction and ducting use transmit frequency at L-band or lower to extend the horizon, which is very different from diffraction. Refraction for over-the-horizon radar uses variable density in the air column above the surface of the earth to bend RF signals. An inversion layer can produce a transient troposphere duct that traps RF signals in a thin layer of air like a wave-guide.\n\nSubclutter visibility involves the maximum ratio of clutter power to target power, which is proportional to dynamic range. This determines performance in heavy weather and near the earth surface.\n\nSubclutter visibility is the ratio of the smallest signal that can be detected in the presence of a larger signal.\n\nA small fast-moving target reflection can be detected in the presence of larger slow-moving clutter reflections when the following is true:\n\nThe pulse-Doppler radar equation can be used to understand trade-offs between different design constraints, like power consumption, detection range, and microwave safety hazards. This is a very simple form of modeling that allows performance to be evaluated in a sterile environment.\n\nThe theoretical range performance is as follows.\n\nwhere\n\nThis equation is derived by combining the radar equation with the noise equation and accounting for in-band noise distribution across multiple detection filters. The value \"D\" is added to the standard radar range equation to account for both pulse-Doppler signal processing and transmitter FM noise reduction.\n\nDetection range is increased proportional to the fourth root of the number of filters for a given power consumption. Alternatively, power consumption is reduced by the number of filers for a given detection range.\n\nPulse-Doppler signal processing integrates all of the energy from all of the individual reflected pulses that enter the filter. This means a pulse-Doppler signal processing system with 1024 elements provides 30.103 dB of improvement due to the type of signal processing that must be used with pulse-Doppler radar. The energy of all of the individual pulses from the object are added together by the filtering process.\n\nSignal processing for a 1024 point filter improves performance by 30.103 dB, assuming compatible transmitter and antenna. This corresponds to 562% increase in maximal distance.\n\nThese improvements are the reason pulse-Doppler is essential for military and astronomy.\n\nPulse-Doppler radar for aircraft detection has two modes.\n\nScan mode involves frequency filtering, amplitude thresholding, and ambiguity resolution. Once a reflection has been detected and resolved, the pulse-Doppler radar automatically transitions to tracking mode for the volume of space surrounding the track.\n\nTrack mode works like a phase-locked loop, where Doppler velocity is compared with the range movement on successive scans. Lock indicates the difference between the two measurements is below a threshold, which can only occur with an object that satisfies Newtonian mechanics. Other types of electronic signals cannot produce a lock. Lock exists in no other type of radar.\n\nThe lock criteria needs to be satisfied during normal operation.\n\nLock eliminates the need for human intervention with the exception of helicopters and electronic jamming.\n\nWeather phenomenon obey adiabatic process associated with air mass and not Newtonian mechanics, so the lock criteria is not normally used for weather radar.\n\nPulse-Doppler signal processing selectively excludes low-velocity reflections so that no detections occurs below a threshold velocity. This eliminates terrain, weather, biologicals, and mechanical jamming with the exception of decoy aircraft.\n\nThe target Doppler signal from the detection is converted from frequency domain back into time domain sound for the operator in track mode on some radar systems. The operator uses this sound for passive target classification, such as recognizing helicopters and electronic jamming.\n\nSpecial consideration is required for aircraft with large moving parts because pulse-Doppler radar operates like a phase-locked loop. Blade tips moving near the speed of sound produce the only signal that can be detected when a helicopter is moving slow near terrain and weather.\n\nHelicopters appears like a rapidly pulsing noise emitter except in a clear environment free from clutter. An audible signal is produced for passive identification of the type of airborne object. Microwave Doppler frequency shift produced by reflector motion falls into the audible sound range for human beings (), which is used for target classification in addition to the kinds of conventional radar display used for that purpose, like A-scope, B-scope, C-scope, and RHI indicator. The human ear may be able to tell the difference better than electronic equipment.\n\nA special mode is required because the Doppler velocity feedback information must be unlinked from radial movement so that the system can transition from scan to track with no lock.\n\nSimilar techniques are required to develop track information for jamming signals and interference that cannot satisfy the lock criteria.\n\nPulse-Doppler radar must be multi-mode to handle aircraft turning and crossing trajectory.\n\nOnce in track mode, pulse-Doppler radar must include a way to modify Doppler filtering for the volume of space surrounding a track when radial velocity falls below the minimum detection velocity. Doppler filter adjustment must be linked with a radar track function to automatically adjust Doppler rejection speed within the volume of space surrounding the track.\n\nTracking will cease without this feature because the target signal will otherwise be rejected by the Doppler filter when radial velocity approaches zero because there is no change in frequency. \n\nMulti-mode operation may also include continuous wave illumination for semi-active radar homing.\n\n\n"}
{"id": "22236319", "url": "https://en.wikipedia.org/wiki?curid=22236319", "title": "Quantum-confined Stark effect", "text": "Quantum-confined Stark effect\n\nThe quantum-confined Stark effect (QCSE) describes the effect of an external electric field upon the light absorption spectrum or emission spectrum of a quantum well (QW). In the absence of an external electric field, electrons and holes within the quantum well may only occupy states within a discrete set of energy subbands. Only a discrete set of frequencies of light may be absorbed or emitted by the system. When an external electric field is applied, the electron states shift to lower energies, while the hole states shift to higher energies. This reduces the permitted light absorption or emission frequencies. Additionally, the external electric field shifts electrons and holes to opposite sides of the well, decreasing the overlap integral, which in turn reduces the recombination efficiency (i.e. fluorescence quantum yield) of the system. \n\nThe spatial separation between the electrons and holes is limited by the presence of the potential barriers around the quantum well, meaning that excitons are able to exist in the system even under the influence of an electric field. The quantum-confined Stark effect is used in QCSE optical modulators, which allow optical communications signals to be switched on and off rapidly.\n\nEven if Quantum Objects (Wells, Dots or Discs, for instance) emit and absorb light generally with higher energies than the band gap of the material, the QCSE may shift the energy to values lower than the gap. This was evidenced recently in the study of quantum discs embedded in a nanowire.\n\nThe shift in absorption lines can be calculated by comparing the energy levels in unbiased and biased quantum wells. It is a simpler task to find the energy levels in the unbiased system, due to its symmetry. If the external electric field is small, it can be treated as a perturbation to the unbiased system and its approximate effect can be found using perturbation theory.\n\nThe potential for a quantum well may be written as\nwhere formula_2 is the width of the well and formula_3 is the height of the potential barriers. The bound states in the well lie at a set of discrete energies, formula_4 and the associated wavefunctions can be written using the envelope function approximation as follows:\nIn this expression, formula_6 is the cross-sectional area of the system, perpendicular to the quantisation direction, formula_7 is a periodic Bloch function for the energy band edge in the bulk semiconductor and formula_8 is a slowly varying envelope function for the system.\n\nIf the quantum well is very deep, it can be approximated by the particle in a box model, in which formula_9. Under this simplified model, analytical expressions for the bound state wavefunctions exist, with the form\nThe energies of the bound states are\nwhere formula_12 is the effective mass of an electron in a given semiconductor.\n\nSupposing the electric field is biased along the z direction,\nthe perturbing Hamiltonian term is\nThe first order correction to the energy levels is zero due to symmetry. \nThe second order correction is, for instance n=1,\nfor electrons.\n\nSimilar calculations can be applied to holes by replacing the electron effective mass with the hole effective mass.\n\nAddition to energy level shift, the DC electric field causes decrease of absorption coefficient. Because electron and hole are forced to opposite direction by the field, the overlap of relating valence and conduction band in transition is decreased. Thus, according to Fermi's golden rule, which says that transition probability is proportional to the overlap, optical transition strength is weakened. Using this, light absorption of materials can be controlled by changing electric field and can be used as an optical modulator.\n\n\n"}
{"id": "1510452", "url": "https://en.wikipedia.org/wiki?curid=1510452", "title": "Roll center", "text": "Roll center\n\nThe roll center of a vehicle is the notional point at which the cornering forces in the suspension are reacted to the vehicle body.\n\nThere are two definitions of roll center. The most commonly used is the geometric (or kinematic) roll center; the Society of Automotive Engineers uses a force-based definition.\n\nThe location of the geometric roll center is solely dictated by the suspension geometry, and can be found using principles of the instant center of rotation. The SAE's definition of the force based roll center is, \"The point in the transverse vertical plane through any pair of wheel centers at which lateral forces may be applied to the sprung mass without producing suspension roll\".\n\nThe lateral location of the roll center is typically at the center-line of the vehicle when the suspension on the left and right sides of the car are mirror images of each other.\n\nThe significance of the roll center can only be appreciated when the vehicle's center of mass is also considered. If there is a difference between the position of the center of mass and the roll center a moment arm is created. When the vehicle experiences angular velocity due to cornering, the size of the moment arm, combined with the stiffness of the springs and anti-roll bars (anti-sway bars in some parts of the world), dictates how much the vehicle will roll. This has other effects too, such as dynamic load transfer.\n\nWhen the vehicle rolls the roll centers migrate. It is this movement of roll centers that vehicle dynamics seek to control and in most cases limit. The rapid movement of roll centers when the system experiences small displacements can lead to stability problems with the vehicle. The roll center height has been shown to affect behavior at the initiation of turns such as nimbleness and initial roll control.\n\nCurrent methods of analyzing individual wheel instant centers have yielded more intuitive results of the effects of non-rolling weight transfer effects. This type of analysis is better known as the lateral-anti method. This is where one takes the individual instant center locations of each corner of the car and then calculates the resultant vertical reaction vector due to lateral force. This value then is taken into account in the calculation of a jacking force and lateral weight transfer. This method works particularly well in circumstances where there are asymmetries in left to right suspension geometry.\n\nThe practical equivalent of the above is to push laterally at the tire contact patch and measure the ratio of the change in vertical load to the horizontal force.\n\n"}
{"id": "24417403", "url": "https://en.wikipedia.org/wiki?curid=24417403", "title": "Scrapheap Challenge", "text": "Scrapheap Challenge\n\nScrapheap Challenge is a British television game show where teams of contestants build a working machine that could do a specific task, using materials available in a scrapheap. The series features teams of four or five given ten hours (based around sunset) to build vehicles, or machines to complete a specific task, such as being a trebuchet, or complete a racecourse whilst acting as a gyroscope. The series ran for eleven series, and originally shown on Channel 4 The format was exported to the United States, where it was known as Junkyard Wars. The US show was also produced by RDF Media, and was originally shown on The Learning Channel. Repeats have aired on another Discovery network, the Science Channel.\n\nA typical episode featured a competition between two 4-person teams, each consisting of three regular members (with one designated the captain), plus an expert in the field related to the particular challenge. The judge for each episode is typically a specialist in (non-scrap) versions of the machine being constructed.\n\nThe challenges are many and varied, usually involving teams constructing a machine to achieve a particular objective. The final showdown usually consisted of either head-to-head races or individually run timed events. Examples of challenges included making a jet car, a bridging machine, a car-crusher, and a machine to fling a British Leyland Mini as far as possible.\n\nAssistant producer Eve Kay had the idea for the show after watching a scene in the movie \"Apollo 13\", where NASA engineers had only a short period of time to construct a carbon dioxide filter out of parts available on the space capsule. The show also draws its inspiration from the 1980s TV series \"The Great Egg Race\".\n\nThe UK show was originally presented by Robert Llewellyn, joined in series 2-4 by producer Cathy Rogers, and in series 5-10 by Lisa Rogers (no relation). For series 11, both hosts were replaced with former \"Scrapheap\" contestant and judge Dick Strawbridge.\n\nSeries 1 was titled \"Scrapheap\", and pitted the same two teams against each other each week. From series 2, the show was renamed \"Scrapheap Challenge\" and featured a knockout tournament between teams drawn from the general public. From series 3, a champion of champions contest was initiated.\n\nSeries 3 and 4 included a single US team in the field. Series 3 had The Nerds, and series 4 had \"The Mulewrights\" brought in at the last minute when one team couldn't travel due to a livestock epidemic. Both US teams made it to the final round.\n\n\nSeries 9 was shown in two halves. The first eight episodes aired between 15 April and 3 June, and the remaining episodes between 4 November and 23 December 2007.\n\nSeries 11 saw Dick Strawbridge (formerly a competitor from series 1-3) return as an expert and judge, replacing Robert Llewellyn. The show was now produced by RDF Bristol and Exec. producer Jane Lomas and series producer John Macnish had to deliver the new series on a fraction of the budget of previous series. As well as a change in presenters the format of the series changed dramatically with teams of 'scrappers' from across the UK competing against Dick Strawbridge's home team known as 'Dick's Diamonds'. At the end of each episode a battle to determine the week's champion scrappers took place in a public venue in front of crowds of cheering fans. Series 11 of the UK show had its world premiere in Australia on ABC2, with episode 1 airing on 1 July 2009.\n\nA spin-off from 2003 that had teams from around the world. Teams had a fixed budget and a month to build a vehicle to compete in a varied set of trials, such as auto \"bowling\", a race through a flooded course, a \"car coaster\", car \"sumo\" and other tests.\n\nThe winners were the Aquaholics from the UK. Hosted by Cathy Rogers and Henry Rollins, it lasted one season.\n\nAnother spin-off series was instituted from 2004 to 2006 and was broadcast following the main series. It was also presented by Robert Llewellyn and Lisa Rogers.\n\nIt involved several teams being allocated a budget and several weeks to construct a road-legal vehicle which, if classified as a car, must pass the Single Vehicle Approval test. However, a few of the teams managed to avoid taking the SVA test, either by using an unmodified car chassis, such as the Chaos Crew in Series 2, who placed the body shell from an ice cream truck onto the unchanged chassis of a Range Rover, or by using their livelihood in order to classify their vehicle as an agricultural show-piece (The Barley Pickers in series 1). The teams then drove to various tests in-convoy across the UK where they were given eight hours at a local scrapheap to modify their vehicles for the testalthough, for all but the final test, the vehicles had to be returned to road-legal condition afterwards. In Series 3, four teams were given vehicles and participated in 5 challenges in the Galloway Forest Park. They had to adapt the vehicles to perform two different tasks in each episode. This series was unusual for Scrapheap in that the build time was added together with the time taken to complete each stage.\n\n\nThe US version of the show was titled Junkyard Wars and was presented by George Gray (season 1) and Tyler Harcott (season 2-5), joined by Cathy Rogers (season 1-3) and Karyn Bryant (season 4-5).\n\n\nFormer North Dakota Governor Ed Schafer was one of many notable people appearing on the show. Schafer was on during the fifth season as a member of the High Flyers which lost to the Jet Doctors in the Fifth Series finale. The first season was filmed in the UK, and included a competition between the grand winner of UK series 3, Megalomaniacs, and the US season 1 champs the Long Brothers (who won).\n\nThe first season of the show was nominated for an Emmy award.\n\nThe show was reformatted as Junkyard Mega-Wars to consist of two regular captains, who each select three people to help in the challenge. The show was presented by Rossi Morreale and Bobbi Sue Luther.\n\n\n\n"}
{"id": "13232596", "url": "https://en.wikipedia.org/wiki?curid=13232596", "title": "Sentient Information Systems", "text": "Sentient Information Systems\n\nSentient Information Systems BV is a Dutch software provider specialized in data mining. The company was founded in 2001 out of the former Sentient Machine Research (SMR) and is located in Amsterdam.\n\nSentient's flagship product is DataDetective, a data mining platform capable of analyzing information from various domains. Users of this tool include several Dutch police departments, hospitals, insurance and media companies.\n\n"}
{"id": "40798759", "url": "https://en.wikipedia.org/wiki?curid=40798759", "title": "Sibyl M. Rock", "text": "Sibyl M. Rock\n\nSibyl Martha Rock (August 1, 1909 – November 17, 1981) was a pioneer in mass spectrometry and computing. Rock was a key person in Consolidated Engineering Corporation's (CEC) mass spectrometry team at a time when mass spectrometers were first being commercialized for use by researchers and scientists. Rock was instrumental in developing mathematical techniques for analyzing the results from mass spectrometers, in developing an analog computer with Clifford Berry for analysis of equations, and in sustaining an ongoing dialog between engineers and customers involved in development of both the mass spectrometer and an early digital computer, CEC's Datatron.\n\nSibyl M. Rock was born on August 1, 1909 in Butte, Montana. Her father was a telephone technician, which may have interested her in technology. She entered the University of California, Los Angeles in 1927, earning a degree in Mathematics in 1931. While at UCLA, she was president of the local chapter of Pi Mu Epsilon, the national mathematics society. She also received a Phi Beta Kappa key.\n\nRock was employed as a \"geophysical computer\" in the petroleum industry, first at Rieber Laboratories, and then in 1938 at Herbert Hoover, Jr.'s newly formed United Geophysical Corporation. She transferred into United's engineering and instrument subsidiary, Consolidated Engineering Corporation, when they began to develop the mass spectrometer as a commercial product. The first 21-101 Mass Spectrometer was delivered in December 1942. Sibyl Rock worked in the CEC research group with employees such as Harold Wiley, manager for Chemical Instruments, Harold Washburn, and Clifford Berry. In 1947, she joined the sales department. In sales, she worked closely with people at chemical and refining companies who were potential customers for the mass spectrometer and early digital computers, identifying their needs and concerns. People such as Seymour Meyerson, who worked for Standard Oil of Indiana, were active contributors to Rock's CEC Users' Group. Rock transferred again in 1952, to the organization's newly formed computer division. As of 1953, she was \"in charge of sales and application functions of the Computer Division\" with the title \"Acting Manager, Application Service\" and was the first female sales engineer of ElectroData Corporation.\n\nBy the mid-1940s Rock had made several outstanding contributions to the field. She devised many of the procedures of mixture analysis, and wrote the computing manuals used by CEC's customers, such as the \"Computing Manual: Analysis of Gas and Liquid Mixtures by Means of the Mass Spectrometer\" (1946). Her work contributed to the creation of standards for instruments and methods in a rapidly developing field.\n\nShe also did basic analysis, with Martin Shepherd of the National Bureau of Standards, of the first Pasadena smog samples. This research produced valuable information about the presence and types of hydrocarbons in Los Angeles air, and about the oxidization of hydrocarbons with ozone and nitrogen oxides.\n\nRock was also instrumental in developing mathematical techniques for analyzing the results from mass spectrometers. In 1946, with Clifford Berry, she developed an analog computer capable of solving multiple simultaneous linear equations, suitable for the analysis of data from mass spectrometers. They patented an analog computer that could efficiently solve a series of 12 simultaneous equations in 12 unknowns.\n\nMass-spectrometer users were developing instruments for their own purposes on a lab-by-lab basis, modifying and experimenting with them at a rapid rate. Feedback from the field and effective communication about needs and problems were critical to CEC's success in building and marketing mass spectrometers. Sibyl Rock was engaged in developing the instruments and the procedures for their use. She was also a key person assisting Consolidated's customers in the use of those instruments and techniques, including the use of Cliff Berry's analog computer.\n\nRock understood the technical aspects of how the instrument worked and the mathematical aspects of the calculations it was supposed to compute. Because she worked closely with CEC's customer base, she also understood the instrumentation needs of CEC's growing list of customers. Rock collaborated with Harold Wiley, manager of chemical instruments, to review both old and new instruments, assessing their operational status and identifying needed improvements. Rock continuously drew upon her knowledge of the latest methods used by customers and their critiques and problem reports about the instrumentation, to improve CEC's products.\n\nClifford Berry encouraged CEC to develop a digital computer. Harry Huskey and Ernst Selmer were involved as lecturers and consultants in the development process. Sibyl Rock worked closely with Ernst Selmer on coding problems for the machine before it even existed in hardware. She also encouraged potential customers to try coding problems for the Datatron, ensuring that the system would perform in ways that would be useful to them. She helped customers with coding and ensured that their problems would run on the \"breadboard\" and \"prototype\" systems.\n\nSibyl Rock once described her work as follows:\n\n'For the past several years I have written Product Specifications for digital computers and related peripheral gear. The engineering design is done by engineers. The Product Specification forms a \"contract\" between engineering and marketing to assure that both interested parties know what is to be built. Because the engineer may be too optimistic as to what he can produce and what reliability he can achieve, and because marketing wants the moon for zero dollars, the product specification writer is needed as a negotiator.'\n\nFormal announcement of the new ElectroData 203 computer took place in February 1954. By 1956, ElectroData became the third largest manufacturer of computers in the world, but lacked the funds necessary to expand successfully. On July 1, 1956 Burroughs Corporation purchased ElectroData Corporation from CEC. The basic architecture remained the same even though the computer was marketed under various names: the CEC 30-201, CEC 30-203, the ElectroData 204 and 205, and the Burroughs 205.\n\nIn 1961, the Datatron 205 was used during the first launch of a Saturn rocket at Cape Canaveral, to analyze real-time guidance data.\n\nRock actively encouraged women and girls to become mathematicians and to take up careers in engineering.\n\nShe died November 17, 1981 in Los Angeles, California.\n\nIn addition to the manuals she wrote, Rock was active in scholarly publishing and coauthored significant papers with several of her colleagues, including:\n\n"}
{"id": "3541334", "url": "https://en.wikipedia.org/wiki?curid=3541334", "title": "Silver service", "text": "Silver service\n\nSilver service (in British English) is a method of foodservice that usually includes serving food at the table. It is a technique of transferring food from a service dish to the guest's plate from the left.\n\nIt is performed by a waiter by using service forks and spoons from the diner's left. In France, it is known as \"service à l'anglaise\" (\"English service\").\n\n\n\nSilver service, like all formal food service, is oriented for a right-handed waiter, left-handed waiters may use their right hand: to serve the food, the waiter stands behind the guest and to the guest's left, holds or supports the platter with their left hand and serves the food with their right hand. It is common for the waiter to hold the serving-fork above the serving-spoon both in the right hand, and use the fingers to manipulate the two as a pincer for picking up, holding and transferring the food. This technique or form requires much practice and dexterity. \n\nA modification of silver service is known as butler service.\n\nIn butler service, the diner helps himself from a serving plate held by the waiter (butler). Traditionally, this type of service was used on Sunday evenings, when the waiting staff had the evening off and the butler helped out at dinner. In France, this kind of service is known as \"service à la francaise\" (\"French service\").\n\n"}
{"id": "25232843", "url": "https://en.wikipedia.org/wiki?curid=25232843", "title": "Suvača", "text": "Suvača\n\nSuvača () in Kikinda, Serbia, is one of the three remaining horse-powered dry mills in the whole of Europe.\n\nSuvača in Kikinda is characteristic of the Vojvodina area of the 19th century. It was built in 1899, and the mill stopped working in 1945. It is located in the western part of town, on the corner of Nemanjina and Moravska streets. The plot of land is . Suvača is a mill for grinding grain that uses the work of horses as its driving force. The mill uses one to five pairs of horses. One pair of horses was able to grind up to of grain per hour. According to tradition, the taste of bread from wheat ground in Suvača was excellent and high quality. In addition to cereals, the mill at Suvača would process pepper and cinnamon.\n\nSuvača consists of three connected parts: the drive space, the mill space, and the miller's apartment. The drive space is the compartment with a pyramid roof where the device is located that runs the mill stones. Although the entire facility called Suvača, Suvača is essentially the part of the building in which the device is located.\n\nThe main building is a multi-pyramid shape where the most important part of Suvača—the circular area where the grinding took place—is located. This section of the building is about in diameter, with a ceiling height in the center of the pyramid of . The roof construction is wooden, and is covered in tile. The building relies on fourteen low and three stubby pillars of bricks. Between the columns, the space is enclosed by wooden slat fencing. On the south side, instead of fences, gates were set up using wooden lattices for the introduction of horses into the building. The miller's apartment consists of three rooms: a sitting room, a kitchen, and a cellar. These rooms are located next to the mill area.\n\nIn 1951, Suvača was placed under state protection, and in 1990 it was proclaimed a Monument of Culture of Exceptional Importance.\n\nIn March 2018 a reconstruction of the Suvača complex began. The entire wooden mechanism of the mill will be conserved and reconstructed. An additional object will be built within the complex. It is planned to serve as a souvenir and gift shop and as the workshop during certain happenings and festivities. Reconstruction is part of the wider cross-border project which includes the reconstructions of the windmills in Orom, near Kanjiža, and Kiskunfélegyháza, in Hungary. The work should be finished by April 2019.\n\n\n"}
{"id": "4870930", "url": "https://en.wikipedia.org/wiki?curid=4870930", "title": "Tips from the Top Floor", "text": "Tips from the Top Floor\n\nDigital Photography Tips from the Top Floor is a podcast which presents tips and tricks about photography, mainly digital photography, in episodes normally ranging\nfrom 5 to 15 minutes in length and covering topics from image composition to post processing.\n\nThere are close to 500 episodes now of the show, both in audio and video format. The show is aimed mainly at beginner digital photographers, though the content is suitable for all levels of photographer. It is produced in English and hosted by Chris Marquardt, a 37-year-old photographer, sound professional, musician and media producer. Some shows are produced along with Leo Laporte's show, The Tech Guy and broadcast on the Premiere Radio Network as well as Sirius XM Satellite Radio.\n\nPodcast Awards: Won the \"Education\" category\n\nPodcast Awards: Nominated for the \"People's choice\" and won the \"Education\" category.\n\nPodcast Awards: Nominated in the \"Education\" category\n\nThe podcast also has a large and active forum, features regular tips for Photoshop users and hands out regular assignments for everyone to participate in.\n\nThe podcast is also the flagship show of the Photocast Network, a community of podcasters producing content related to photography.\n\n"}
{"id": "42918770", "url": "https://en.wikipedia.org/wiki?curid=42918770", "title": "Tiva-C LaunchPad", "text": "Tiva-C LaunchPad\n\nThe Tiva-C ( TM4C) LaunchPads are inexpensive self-contained, single-board microcontrollers, about the size of a credit card, featuring an ARM Cortex-M4F 32-bit CPU operating at 80 to 120 MHz, manufactured by Texas Instruments. \nThe TM4C Series TM4C123G LaunchPad is an upgrade from TI's Stellaris LaunchPad adding support options for motion control PWMs and USB Host functionality. The more recently released TM4C1294 Connected LaunchPad is the first cloud-connected offering in TI's LaunchPad ecosystem and provides a solid foundation for beginning and evaluating embedded IoT designs.\n\nThere are many I/O pins (40 to 80 depending upon version) that have multi-personality. This means that they can be easily configured as digital inputs or outputs, analog inputs and outputs or other functions, allowing a great variety of applications, are just the multiple serial ports have the ability to interface with more items such as test cards or other communication modules, etc. Among those pins there are included the GND and POWER (3.3 V) pins.\n\nThe clock is 80 or 120 MHz (vers based), which makes them 5 to over 7 times faster than the Arduino Uno's 16 MHz ATmega328P microcontroller. As with any Cortex M4, the CPU has some DSP (digital signal processor) instructions, with some limitations. One can do signal processing, for example, sampling a human voice with a good quality, able to be processed in MATLAB. The CPU contains the optional floating-point unit with single-precision floating point operations supported.\n\nThey have an additional USB port which can act as USB host, allowing the connection of multiple devices and the \"Connected\" one has an integrated 10/100 Ethernet MAC+PHY for Internet connectivity. They also have a temperature sensor and on-board LED(s) and RGB LED(s), which allows you to generate various colors by combining the three basic colors (red, blue and green) of the additive color synthesis.\n\nThe Tiva/TM4C LaunchPads come preloaded with software to demonstrate many of the capabilities of the ARM microcontroller and with a quickstart application to get up and running within minutes.\n\nThe LaunchPad's TM4C123GXL CPU (ARM Cortex M4F 32-bit CPU) is capable of speeds up to 80 MHz. The CPU can be run at lower speeds to reduce energy consumption.\n\n\nTiva TM4C1294NCPDTI ARM Cortex M4F 32-bit microcontroller\n\nBoth Tiva C Series LaunchPads conform to a standard for BoosterPack layout defined by Texas Instruments.\n\nTI has a Sensor Hub BoosterPack designed exclusively to fit the TM4C123GXL LaunchPad.\n\nInternet of Things is made possible with the SimpleLink Wi-Fi CC3100 BoosterPack.\n\nA demonstration of LCD driving can be achieved with LaunchPad by fitting a Nokia LCD, graphic display (not included in the kit). However, the LCD interface consumes many of the I/O pins.\n\nAnother solution is the 320 x 240 pixel TFT QVGA display with resistive touch screen Kentec 3.5\" screen working through the SPI bus.\n\nThe LaunchPad comes preloaded with a RGB quickstart application\n\nSupported by TivaWare for C Series software including the USB library and the peripheral driver library\n\nTiva C Series TM4C123G LaunchPad BoosterPack XL Interface, which features stackable headers to expand the capabilities of the Tiva C Series LaunchPad development platform\n\nSupport for the ARM Cortex-M4 DSP instructions is provided via ARM's CMSIS-DSP software package\n\nTivaWare for C Series includes support for FreeRTOS and TI-RTOS\n\nThe LaunchPad LM4F120 and TM3C123G can also be programmed using Energia, an Arduino-like IDE based on the Wiring framework. Energia includes the libraries for the SimpleLink Wi-Fi CC3100 BoosterPack.\n\nSoftware may be written for the LaunchPad using the assembly language or GCC (GNU Cprogramming language) with Energia, a free variant of the Arduino integrated development environment (IDE). A pre-installed bootloader program allows the board to be re-programmed with a standard USB 2.0 port (requiring no special hardware). The board also has ISP and JTAG ports for in-circuit programming and debugging.\n\nThe LaunchPad is becoming popular among hobbyists for its low price of about $13 USD, its flexibility, the availability of free development software, and the ability to reprogram it without using special hardware.\n\nMany fully usable projects have been built using the LaunchPad as a base platform, often with few or no additional parts.\nThere are free plans to convert the LaunchPad into a portable ARM-ISP device for programming a whole family of ARM devices. \n\nSeveral plans available on the web to convert a LaunchPad into an MP3 player. \n\nThere are also available lessons and popular books for learning how to program ARMs using C language, after which the LaunchPad was designed.\n\nThere are as well, related YouTube videos and a wiki about Tiva-C LaunchPad.\n\n\n"}
{"id": "59703", "url": "https://en.wikipedia.org/wiki?curid=59703", "title": "Toshiba", "text": "Toshiba\n\n, commonly known as Toshiba (stylized in all uppercaps) is a Japanese multinational conglomerate headquartered in Tokyo, Japan. Its diversified products and services include information technology and communications equipment and systems, electronic components and materials, power systems, industrial and social infrastructure systems, consumer electronics, household appliances, medical equipment, office equipment, as well as lighting and logistics.\n\nToshiba was founded in 1939 as Tokyo Shibaura Denki K.K. through the merger of Shibaura Seisaku-sho (founded in 1875) and Tokyo Denki (founded in 1890). The company name was officially changed to Toshiba Corporation in 1978. \n\nIn 2018 the company sold Westinghouse, one of its many prior acquisitions, which had included:\n\nToshiba is organized into four groupings: the Digital Products Group, the Electronic Devices Group, the Home Appliances Group and the Social Infrastructure Group. It is listed on the Tokyo Stock Exchange, where it is a constituent of the Nikkei 225 and TOPIX indices, the Osaka Securities Exchange and the Nagoya Stock Exchange. Toshiba is the ninth largest semiconductor manufacturer in the world.\n\nOn 11 April 2017, Toshiba filed unaudited quarterly results because of uncertainties at Westinghouse, which had filed for Chapter 11 bankruptcy protection. Toshiba stated that \"substantial doubt about the company's ability to continue as a going concern exists\".\n\nToshiba was founded in 1939 by the merger of Shibaura Seisakusho (Shibaura Engineering Works) and Tokyo Denki (Tokyo Electric). Shibaura Seisakusho had been founded as Tanaka Seisakusho by Tanaka Hisashige in July 1875 as Japan's first manufacturer of telegraph equipment. In 1904, it was renamed Shibaura Seisakusho. Through the first decades of the 20th century, Shibaura Seisakusho had become a major manufacturer of heavy electrical machinery as Japan modernized during the Meiji Era and became a world industrial power. Tokyo Denki was founded as Hakunetsusha in 1890 and had been Japan's first producer of incandescent electric lamps. It later diversified into the manufacture of other consumer products and in 1899 had been renamed Tokyo Denki. The merger of Shibaura and Tokyo Denki created a new company called Tokyo Shibaura Denki (Tokyo Shibaura Electric) (). It was soon nicknamed Toshiba, but it was not until 1978 that the company was officially renamed Toshiba Corporation.\nThe group expanded rapidly, driven by a combination of organic growth and by acquisitions, buying heavy engineering and primary industry firms in the 1940s and 1950s. Groups created include Toshiba Music Industries/Toshiba EMI (1960), Toshiba International Corporation (1970s) Toshiba Electrical Equipment (1974), Toshiba Chemical (1974), Toshiba Lighting and Technology (1989), Toshiba America Information Systems (1989) and Toshiba Carrier Corporation (1999).\n\nToshiba is responsible for a number of Japanese firsts, including radar (1912), the TAC digital computer (1954), transistor television and microwave oven (1959), color video phone (1971), Japanese word processor (1978), MRI system (1982), laptop personal computer (1986), NAND EEPROM (1991), DVD (1995), the Libretto sub-notebook personal computer (1996) and HD DVD (2005).\n\nIn 1977, Toshiba acquired the Brazilian company Semp (Sociedade Eletromercantil Paulista), subsequently forming Semp Toshiba through the combination of the two companies' South American operations.\n\nIn 1987, Tocibai Machine, a subsidiary of Toshiba, was accused of illegally selling CNC milling machines used to produce very quiet submarine propellers to the Soviet Union in violation of the CoCom agreement, an international embargo on certain countries to COMECON countries. The Toshiba-Kongsberg scandal involved a subsidiary of Toshiba and the Norwegian company Kongsberg Vaapenfabrikk. The incident strained relations between the United States and Japan, and resulted in the arrest and prosecution of two senior executives, as well as the imposition of sanctions on the company by both countries. Senator John Heinz of Pennsylvania said \"What Toshiba and Kongsberg did was ransom the security of the United States for $517 million.\"\n\nIn 2001, Toshiba signed a contract with Orion Electric, one of the world's largest OEM consumer video electronic makers and suppliers, to manufacture and supply finished consumer TV and video products for Toshiba to meet the increasing demand for the North American market. The contract ended in 2008, ending seven years of OEM production with Orion.\n\nIn December 2004, Toshiba quietly announced it would discontinue manufacturing traditional in-house cathode-ray tube (CRT) televisions. In 2006, Toshiba terminated production of in-house plasma TVs. To ensure its future competitiveness in the flat-panel digital television and display market, Toshiba has made a considerable investment in a new kind of display technology called SED. Before World War II, Toshiba was a member of the Mitsui Group zaibatsu (family-controlled vertical monopoly). Today Toshiba is a member of the Mitsui keiretsu (a set of companies with interlocking business relationships and shareholdings), and still has preferential arrangements with Mitsui Bank and the other members of the keiretsu. Membership in a keiretsu has traditionally meant loyalty, both corporate and private, to other members of the keiretsu or allied keiretsu. This loyalty can extend as far as the beer the employees consume, which in Toshiba's case is Asahi.\n\nIn July 2005, BNFL confirmed it planned to sell Westinghouse Electric Company, then estimated to be worth $1.8 billion (£1 billion). The bid attracted interest from several companies including Toshiba, General Electric and Mitsubishi Heavy Industries and when the \"Financial Times\" reported on 23 January 2006 that Toshiba had won the bid, it valued the company's offer at $5 billion (£2.8 billion). The sale of Westinghouse by the Government of the United Kingdom surprised many industry experts, who questioned the wisdom of selling one of the world's largest producers of nuclear reactors shortly before the market for nuclear power was expected to grow substantially; China, the United States and the United Kingdom are all expected to invest heavily in nuclear power. The acquisition of Westinghouse for $5.4 billion was completed on 17 October 2006, with Toshiba obtaining a 77 percent share, and partners The Shaw Group a 20 percent share and Ishikawajima-Harima Heavy Industries Co. Ltd. a 3 percent share.\n\nIn late 2007, Toshiba took over from Discover Card as the sponsor of the top-most screen of One Times Square in New York City. It displays the iconic 60-second New Year's countdown on its screen, as well as messages, greetings, and advertisements for the company.\n\nIn January 2009, Toshiba acquired the HDD business of Fujitsu.\n\nToshiba announced on 16 May 2011, that it had agreed to acquire all of the shares of the Swiss-based advanced-power-meter maker Landis+Gyr for $2.3 billion. In 2010 the company released a series of television models including the WL768, YL863, VL963 designed in collaboration with Danish designer Timothy Jacob Jensen.\nIn April 2012, Toshiba agreed to acquire IBM's point-of-sale business for $850 million, making it the world's largest vendor of point-of-sale systems.\n\nIn July 2012, Toshiba was accused of fixing the prices of LCD panels in the United States at a high level. While such claims were denied by Toshiba, they have agreed to settle alongside several other manufacturers for a total of $571 million.\n\nIn December 2013, Toshiba completed its acquisition of Vijai Electricals Limited plant at Hyderabad and set up its own base for manufacturing of transmission and distribution products (transformers and switchgears) under the Social Infrastructure Group in India as Toshiba Transmission & Distribution Systems (India) Private Limited.\n\nIn January 2014, Toshiba completed its acquisition of OCZ Storage Solutions. OCZ Technology stock was halted on 27 November 2013. OCZ then stated they expected to file a petition for bankruptcy and that Toshiba Corporation had expressed interest in purchasing its assets in a bankruptcy proceeding. On 2 December 2013, OCZ announced Toshiba had agreed to purchase nearly all of OCZ's assets for $35 million. The deal was completed on 21 January 2014 when the assets of OCZ Technology Group became a new independently-operated subsidiary of Toshiba named OCZ Storage Solutions. OCZ Technology Group then changed its name to ZCO Liquidating Corporation; on 18 August 2014, ZCO Liquidating Corporation and its subsidiaries were liquidated. OCZ Storage Solutions was dissolved on 1 April 2016 and absorbed into Toshiba America Electronic Components, Inc., with OCZ becoming a brand of Toshiba.\n\nIn March 2014, Toshiba sued SK Hynix accusing the company for stealing technology of their NAND flash memory.\n\nIn October 2014, Toshiba and United Technologies agreed a deal to expand their joint venture outside Japan.\n\nToshiba announced in early 2015 that they would stop making televisions in its own factories. From 2015 onward, Toshiba televisions will be made by Compal for the U.S., or by Vestel and other manufacturers for the European market.\n\nIn January 2016, Toshiba's security division unveiled a new bundle of services for schools that use its surveillance equipment. The program, which is intended for both K-12 and higher education, includes education discounts, alerts and post-warranty support, among other features, on its IP-based security gear.\n\nAs of March 2016, Toshiba is preparing to start construction on a cutting-edge new semiconductor plant in Japan that will mass-produce chips based on the ultra-dense flash variant. Toshiba expects to spend approximately 360 billion yen, or $3.2 billion, on the project through May 2019.\n\nIn April 2016, Toshiba recalled 100,000 faulty laptop lithium-ion batteries, which are made by Panasonic, that can overheat, posing burn and fire hazards to consumers, according to the U.S. Consumer Product Safety Commission. Toshiba first announced the recall in January, and said it was recalling the batteries in certain Toshiba Notebook computers sold since June 2011.\n\nIn September 2016, Toshiba announced the first wireless power receiver IC using the Qi 1.2.2 specification, developed in association with the Wireless Power Consortium.\n\nIn late December 2016 Toshiba announced losses in the Westinghouse subsidiary from nuclear plant construction would lead to a write-down of several billion dollars.\n\nIn January 2017, a person with direct knowledge of the matter reported that the company plans on making its chip division a separate business.\n\nToshiba first announced in May 2015 that it was investigating an accounting scandal and it might have to revise its profits for the previous three years. On 21 July 2015, CEO Hisao Tanaka announced his resignation amid an accounting scandal that he called \"the most damaging event for our brand in the company's 140-year history\". Profits had been inflated by $1.2 billion over the previous seven years. Eight other senior officials also resigned, including the two previous CEOs. Chairman Masashi Muromachi was appointed acting CEO. Following the scandal, Toshiba Corp. was removed from a stock index showcasing Japan's best companies. That was the second reshuffle of the index, which picks companies with the best operating income, return on equity and market value.\n\nIn September 2015, Toshiba shares fell to their lowest point in two and a half years. The firm said in a statement that its net losses for the quarterly period were 12.3 billion yen ($102m; £66m). The company noted poor performances in its televisions, home appliances and personal computer businesses.\n\nIn December 2015, Muromachi said the episode had wiped about $8 billion off Toshiba's market value. He forecast a record 550 billion yen (about US $4.6 billion) annual loss and warned the company would have to overhaul its TV and computer businesses. Toshiba would not be raising funds for two years, he said. The next week, a company spokesperson announced Toshiba would in early 2016 seek 300 billion yen ($2.5 billion), taking the company's indebtedness to more than 1 trillion yen (about $8.3 billion).\n\nIn May 2016, it was announced that Satoshi Tsunakawa, the former head of Toshiba's medical equipment division, was named CEO. This appointment came after the accounting scandal that occurred.\n\nIn February 2017, Toshiba revealed unaudited details of a 390 billion yen ($3.4 billion) corporate wide loss, mainly arising from its majority owned US based Westinghouse nuclear construction subsidiary which was written down by 712 billion yen ($6.3 billion). On 14 February 2017, Toshiba delayed filing financial results, and chairman Shigenori Shiga, formerly chairman of Westinghouse, resigned.\n\nConstruction delays, regulatory changes and cost overruns at Westinghouse built nuclear facilities Vogtle units 3 and 4 in Waynesboro, Georgia and VC Summer units 2 and 3 in South Carolina, are cited as the main causes of the dramatic fall in Toshiba's financial performance and collapse in share price. Fixed priced construction contracts negotiated by Westinghouse with Georgia Power have left Toshiba with uncharted liabilities that will likely result in the sale of key Toshiba operating subsidiaries to secure the company's future.\n\nWestinghouse filed for Chapter 11 bankruptcy protection on 29 March 2017. It was estimated this would cost 9 billion dollar annual net loss.\n\nOn 11 April 2017, Toshiba filed unaudited quarterly results. Auditors PricewaterhouseCoopers had not signed of the accounts because of uncertainties at Westinghouse. Toshiba stated that \"substantial doubt about the company's ability to continue as a going concern exists\". On 25 April 2017, Toshiba announced its decision to replace its auditor after less than a year. Earlier in April, the company filed twice-delayed business results without an endorsement from auditor PricewaterhouseCoopers (PwC).\n\nOn 20 September 2017, Toshiba's board approved a deal to sell its memory chip business to a group led by Bain Capital for US$18 billion, with financial backing by companies such as Apple, Dell Technologies, Hoya Corporation, Kingston Technology, Seagate Technology, and SK Hynix. On 15 November 2017, Hisense reached a deal to acquire 95% of Toshiba Visual Solutions for US$113.6 million.\n\nLater that month, the company announced that it would pull out of its long-standing sponsorships of the Japanese television programs \"Sazae-san\", \"Nichiyō Gekijo\", and the video screens topping out One Times Square in New York City. The company cited that the value of these placements were reduced by its exit from consumer-oriented lines of business.\n\nOn 6 April 2018 Toshiba announced the completion of the sale of Westinghouse's holding company to Brookfield Business Partners and some partners.\n\nToshiba is headquartered in Minato-ku, Tokyo, Japan and has operations worldwide. It had around 210,000 employees as of 31 March 2012.\n\nToshiba is organised into four main business groupings: the Digital Products Group, the Electronic Devices Group, the Home Appliances Group and the Social Infrastructure Group. In the year ended 31 March 2012, Toshiba had total revenues of , of which 25.2 percent was generated by the Digital Products Group, 24.5 percent by the Electronic Devices Group, 8.7 percent by the Home Appliances Group, 36.6 percent by the Social Infrastructure Group and 5 percent by other activities. In the same year, 45 percent of Toshiba's sales were generated in Japan and 55 percent in the rest of the world.\n\nToshiba has 39 R&D facilities worldwide, which employ around 4,180 people. Toshiba invested a total of in R&D in the year ended 31 March 2012, equivalent to 5.2 percent of sales. Toshiba registered a total of 2,483 patents in the United States in 2011, the fifth-largest number of any company (after IBM, Samsung Electronics, Canon and Panasonic).\n\nToshiba is organized into the following principal business groupings, divisions and subsidiaries:\n\nToshiba has a range of products and services, including air conditioners, consumer electronics (including televisions and DVD and Blu-ray players), control systems (including air-traffic control systems, railway systems, security systems and traffic control systems), electronic point of sale equipment, elevators and escalators, home appliances (including refrigerators and washing machines), IT services, lighting, materials and electronic components, medical equipment (including CT and MRI scanners, ultrasound equipment and X-ray equipment), office equipment, business telecommunication equipment personal computers, semiconductors, power systems (including electricity turbines, fuel cells and nuclear reactors) power transmission and distribution systems, and TFT displays.\n\nIn October 2010, Toshiba unveiled the Toshiba Regza GL1 21\" LED backlit LCD TV glasses-free 3D prototype at CEATEC 2010. This system supports 3D capability without glasses (utilising an integral imaging system of 9 parallax images with vertical lenticular sheet). The retail product was released in December 2010.\n\n4K Ultra HD (3840×2160p) televisions provides four times the resolution of 1080p Full HD televisions. Toshiba's 4K HD LED televisions are powered by a CEVO 4K Quad + dual-core processor.\n\nOn 19 February 2008, Toshiba announced that it would be discontinuing its HD DVD storage format following defeat in a format \"war\" against Blu-ray. The HD DVD format had failed after most of the major US film studios backed the Blu-ray format, which was developed by Sony, Panasonic, Philips and Pioneer Corporation. Conceding the abandonment of HD DVD, Toshiba's President, Atsutoshi Nishida said \"We concluded that a swift decision would be best [and] if we had continued, that would have created problems for consumers, and we simply had no chance to win\".\n\nToshiba continued to supply retailers with machines until the end of March 2008, and continued to provide technical support to the estimated one million people worldwide who owned HD DVD players and recorders. Toshiba announced a new line of stand-alone Blu-ray players as well as drives for PCs and laptops, and subsequently joined the BDA, the industry body which oversees development of the Blu-ray format.\n\nREGZA (Real Expression Guaranteed by Amazing Architecture) is a unified television brand owned and manufactured by Toshiba. In 2010 REGZA name disappeared from the North American market, and from March 2015 new TVs carrying the Toshiba name are designed and produced by Compal Electronics, a Taiwanese company, which Toshiba has licensed its name to. REGZA is also used in Android-based smartphones that were developed by Fujitsu Toshiba Mobile Communications.\n\nIn October 2014, Toshiba released the Chromebook 2, a new version with a thinner profile and a much-improved display. The Chromebook runs exclusively on Chrome OS and gives users free Google Drive storage and access to a collection of apps and extensions at the Chrome Web Store.\n\nIn March 2015, Toshiba announced the development of the first 48-layer, three-dimensional flash memory. The new flash memory is based on a vertical stacking technology that Toshiba calls BiCS (Bit Cost Scaling), stores two bits of data per transistor and can store 128Gbits (16GB) per chip.\n\nToshiba has been judged as making \"low\" efforts to lessen their impact on the environment. In November 2012, they came second from the bottom in Greenpeace's 18th edition of the Guide to Greener Electronics that ranks electronics companies according to their policies on products, energy and sustainable operations. Toshiba received 2.3 of a possible 10 points, with the top company (WIPRO) receiving 7.1 points. \"Zero\" scores were received in the categories \"Clean energy policy advocacy\", \"Use of recycled plastics in products\" and \"Policy and practice on sustainable sourcing of fibres for paper\".\n\nIn 2010, Toshiba reported that all of its new LCD TVs comply with the Energy Star standards and 34 models exceed the requirements by 30% or more.\n\nToshiba also partnered with China's Tsinghua University in 2008 in order to form a research facility to focus on energy conservation and the environment. The new Toshiba Energy and Environment Research Center is located in Beijing where forty students from the university will work to research electric power equipment and new technologies that will help stop the global warming process. Through this partnership, Toshiba hopes to develop products that will better protect the environment and save China. This contract between Tsinghua University and Toshiba originally began in October 2007 when they signed an agreement on joint energy and environment research. The projects that they conduct work to reduce car pollution and to create power systems that don’t negatively affect the environment.\n\nOn 28 December 1970 Toshiba began the construction of unit 3 of the Fukushima Daiichi Nuclear Power Plant which was damaged in the Fukushima I nuclear accidents on 14 March 2011. In April 2011, CEO Norio Sasaki declared nuclear energy would \"remain as a strong option\" even after the Fukushima I nuclear accidents.\n\nIn late 2013, Toshiba (Japan) entered the solar power business in Germany, installing PV systems on apartment buildings.\n\n\n\n for US traded stock\n\n for US traded stock\n"}
{"id": "4288600", "url": "https://en.wikipedia.org/wiki?curid=4288600", "title": "Train speed optimization", "text": "Train speed optimization\n\nTrain speed optimization, also known as Zuglaufoptimierung, is a system that reduces the need for trains to brake and accelerate, resulting in smoother and more efficient operation.\n\nWhile train speed optimization needs some technical infrastructure, it is more of an operational concept than a technical installation. One can relatively easily implement train speed optimization using for instance cab signalling (e.g. using ETCS), but the presence of a cab signalling system does not necessarily mean that it uses train speed optimization. Train speed optimization may also be implemented using conventional signalling.\n\nUsually, trains are allowed to run at the maximum speed the track allows until the distant signal of next occupied block. This is inefficient in many cases, because this way the train comes to a halt in front of the red signal and has to accelerate again from zero.\n\nIf the train slows down much earlier, it reaches, given the right timing, the distant signal just when the home signal switches to green. This way, the train does not need to stop. Thus, wear on the brakes is reduced and the train uses less energy. But the main reason, especially for trains that accelerate slowly, is that the train passes the home signal at high speed, compared to the conventional case where the train often has to accelerate from standstill. This effectively increases track capacity, because the time it takes for the train to run from the distant signal (that has just turned green) to the home signal is often much less than the time it takes for a train to accelerate from the home signal.\n\nFor a train speed optimization system to work, it is necessary to have a signalling system which is capable of displaying several different speeds, for instance 40, 60, 90 km/h and the full line speed, which also requires a train protection system that is able to handle these cases (cab signalling may replace these installations). Further, the track must be equipped with inductive loops that detect the presence of trains with sufficient precision (or other means of detecting the positions of the trains). Finally a computer system is needed that is able to reasonably predict the movements of the trains for the next few minutes.\n\nThe expensive and complicated installations usually only make sense for heavily used routes.\n\nSwiss Federal Railways:\n\n"}
{"id": "1401151", "url": "https://en.wikipedia.org/wiki?curid=1401151", "title": "Train whistle", "text": "Train whistle\n\nA train whistle or air whistle (originally referred to as a steam trumpet) is an audible signaling device on a steam locomotive, used to warn that the train is approaching, and to communicate with rail workers. Modern diesel and electric locomotives primarily use a powerful air horn instead of a whistle as an audible warning device. However, the word \"whistle\" continues to be used by railroaders in referring to such signaling practices as \"whistling off\" (sounding the horn when a train gets underway).\n\nThe need for a whistle on a locomotive exists because trains move on fixed rails and thus are uniquely susceptible to collision. This susceptibility is exacerbated by a train's enormous weight and inertia, which make it difficult to quickly stop when encountering an obstacle. Hence a means of warning others of the approach of a train from a distance is necessary. As train whistles are inexpensive compared to other warning devices, the use of loud and distinct whistles became the preferred solution for railway operators.\n\nSteam whistles were almost always actuated with a pull cord (or sometimes a lever) that permitted proportional (tracker) action, so that some form of \"expression\" could be put into the sound. Many locomotive operators would have their own style of blowing the whistle, and it was often apparent who was operating the locomotive by the sound. Modern locomotives often make use of a push button switch to operate the air horn, eliminating any possibility of altering the horn's volume or pitch.\n\nJohn Holliday describes the history of train whistles as originating in 1832, when a stationmaster suggested, at the opening of the Leicester and Swannington Railway, that the trains should have an audible signaling device. A local musical instrument builder was commissioned to provide a steam-powered whistle, then known as a \"steam trumpet\".\n\nThe article also describes a train collision with a cart, where the driver had blown a horn (steam whistles having not as yet been invented). One account states that Weatherburn, the engine driver, had \"mouthblown his horn\" at the crossing in an attempt to prevent the accident, but that no attention had been paid to this audible warning, perhaps because it had not been heard. Although nobody was injured, the accident was deemed serious enough to warrant George Stephenson’s personal intervention. Stephenson subsequently called a meeting of directors and accepted the suggestion of the company manager, Ashlin Bagster, that a horn or whistle which could be activated by steam should be constructed and fixed to the locomotives. Stephenson later visited a musical instrument maker in Duke Street in Leicester, who, on Stephenson's instructions, constructed a \"steam trumpet\", which was tested in the presence of the Board of Directors ten days later.\n\nStephenson mounted the whistle on the top of the boiler's steam dome, which delivers dry steam to the cylinders for locomotion. The device was apparently about high and had an ever-widening trumpet shape with a diameter at its top or mouth. The company went on to mount similar devices on its other locomotives.\n\nThere is another account that sets the invention of the steam whistle against the actual opening of the line in 1832, rather than associating it with a specific incident.\n\nNorth American steam locomotive whistles have different sounds from one another. They come in many forms, from tiny little single-note shriekers (called \"banshees\" on the Pennsylvania Railroad) to larger plain whistles with deeper tones (a deep, plain train whistle is the \"hooter\" of the Norfolk & Western, used on their A- and Y-class Mallet locomotives). Even more well known were the multi-chime train whistles. Nathan of New York copied and improved Casey Jones's boiler-tube chime whistle by casting the six chambers into a single bell, with open \"steps\" on top to save on casting. This whistle is still considered the \"king of train whistles\". It is the most copied train whistle in the United States, and many railroads' shops cast their own version of it.\n\nAnother very popular American train whistle was, again, a Nathan product. This was a five-note whistle, with a much shorter bell, and therefore, much higher in pitch. This whistle produced a bright G-major 6th chord (GBDEG) and, again, was heavily imitated, copies being made by many different railroads. \n\nThe most popular American chime train whistle was the three-note version. These were either commercially made (Crosby, Lunkenheimer, Star Brass, Hancock Inspirator Co. among others) or shop-made by the railroads themselves. Some famous and very melodious shop-made train whistles were Pennsy's passenger chimes and the Baltimore and Ohio's step-top three chimes. But the most beloved of all three-chime train whistles to the public and railroaders alike were the deep-chorded \"steamboat minor\" long-bells. A well known commercially made chime was Hancock Inspirator Company's three-note step top. These found use on almost every American railroad. Some railroads copied these also, examples being found on the old St. Louis–San Francisco Railway and Illinois Central.\n\nThe Southern Railway made three-chime train whistles. These were all distinctive, having top-mounted levers. They had short-bell three-chimes as well as their (highly copied) long-bell three-chimes on passenger engines, especially their PS4 engines, one of which resides today at the Smithsonian Institution.\n\nTwo-note and four-note train whistles never caught on with North American railroads, with one exception: Canadian National Railway created a large four-chime step-top whistle for limited use on some of their locomotives. These were not common and only a few survive today in the hands of collectors. Otherwise, North American train whistles were of the single-note, three-note, five-note and six-note varieties.\n\nThese are a few American railroads with whistles valued by collectors:\n\nIn the United Kingdom, it is normal for diesel and electric multiple-units and locomotives to have two horns, of different pitches (rather like two-tone emergency road vehicles—police cars, etc.). This has given rise to drivers \"playing\" unofficial combinations of low and high notes. When passing through the local station in the Yorkshire town of Ilkley, drivers soon began to play the first line of the chorus of the folk song, \"On Ilkla Moor Baht 'at\" on their horns, using a series of short blasts: low, h-i-g-h high high, low, high, until the practice was stopped by authorities.\n\nThe first four notes of Beethoven's Fifth Symphony—played during World War II as the Morse code \"V\" (for Victory)—can be sounded on a train horn as three short notes and a longer one, often the last note on the lower-tone horn.\n\nEarly railways, before continuous brakes, had the communication chain or cord from the carriages connected to a \"brake whistle\" on the engine. This was usually of a lower note than the normal whistle used by the driver.\nEngines of Britain's Great Western Railway carried two whistles, one low- and one high-pitched. The high-pitched whistle was for warning of the trains approach and for giving shunting signals. The low-pitched whistle was for sending braking instructions to the crew on the train before the advent of continuous brakes and was retained for the same purpose for goods operations. Some whistle-signals required use of both whistles. Some Great Western \"autocoaches\"—from where the driver operated the steam engine's regulator and brakes, when the engine was propelling one or more autocoaches—still had a whistle connection with the engine's brake whistle, although a gong (much like a tram gong) was fitted at the front of each autocoach and was operated by the driver using a foot treadle.\n\nBack in the days of steam, when assisting engines pushed long goods trains up steep gradients (or \"banks\"), the train would come to a halt at the bottom of the bank. The assisting engine—or \"banker\"—would either be attached to the rear of the train, or just come up against the guard's brake van's buffers. Then the banker's driver would whistle—using a series of long blasts and shorts. This told both the signalman and the driver of the train engine that he was ready. The train engine's driver would reply in similar fashion and, with signals at clear, they would set off in unison. If the banker was coupled to the train, when it reached the top of the bank, the train would stop or come to a crawl for the banker to be uncoupled; if not, the banker's driver would just ease off the regulator, allowing the train to continue on its way, with, of course, a whistled \"goodbye\".\n\nIt is not uncommon for the sound of a train's whistle to propagate for miles; yet vehicle operators still have a difficult time hearing the warning signal due to the vehicle's soundproofing and ambient noise within the cab (such as engine, road, radio, and conversation noises).\n\nThe need to blare a train's whistle loudly to be heard by the driver of a vehicle approaching a grade crossing has become a major disadvantage to the use of train whistles as a safety device and has caused much controversy among those living within earshot of the train's whistle. It has been documented that a train's whistle, when operating on compressed air, driving an exponential horn, has been measured at a higher decibel levels within the homes of nearby residents than within the cab of a vehicle sitting at the grade crossing.\n\nGiven the tonal design of the train whistle, the sound level, how often trains pass through a given community, the number of grade crossings in proximity, and the time of day (night) of occurrence, community residents residing near crossing sometimes feel that train whistles have a serious detrimental effect on the quality of life despite the gain in safety that sounding the horn provides to motorists and pedestrians. However, one Federal Railroad Administration study has shown that the frequency of grade crossing accidents increases in areas where \"quiet zones\" are in effect. The study fails to account for other factors that were also introduced at the same time which may have also accounted for the reduction in accidents during the same period the study measured. For instance, it was during the same period that locomotives began sporting the now crucially important tri-lamp headlight arrangement (\"ditch lights\") and reflector strips similar to those commonly found on highway tractor-trailers. Additionally, the measurements were based on accidents at grade-crossings, which are very low numbers overall to begin with. A grade-crossing that had two accidents during the comparison years, when contrasted with only one accident during the control period, would statistically yield a high percentage-wise improvement in safety, when in reality, it was the difference in only one accident for that grade-crossing.\n\nConversely, there are those who do not object to the train whistle, as they believe it provides an important safety feature. Some people even like the sound of the whistle, as it calls to mind a nostalgic era, as with the riverboats and their steam whistles and calliopes. However, no real studies have been performed by unbiased official entities to measure the real effects such noise has on a community.\n\nQuiet zones are created in municipalities where citizens of the community complain of the noise pollution from the increasing amount of trains which decreases their quality of life. In order to be approved for quiet zones, extensive safety and traffic studies must be conducted. Municipalities and the owners of the tracks must work together to ensure all federal regulations are being met. Quiet zones require improvements which would include installing standard or conventional automatic warning devices such as gates with lights if not already installed Medians must be installed at the railroad crossings to ensure vehicles do not proceed into the opposite lane to go around the gates (4). Once all safety measures are completed train whistles will be silenced at the railroad crossings. Quiet zones silencing train whistles reduces noise pollution but also creates safer railroad crossings wherein the whole community benefits, not just those that live near the tracks.\n\nThe example of the change in frequency in the whistle of a passing train is often used to explain the phenomena of the Doppler effect. It became a common classroom example after the introduction of trains, since at the time they were one of the few objects that moved quickly while sounding a relatively constant note.\n\nTrain whistles are used to communicate with other railroad workers on a train or in the yard. Different combinations of long and short whistles each have their own meaning. They are used to pass instructions, as a safety signal, and to warn of impending movements of a train. Despite the advent of modern radio communication, many of these whistle signals are still used today. (See also Train horn (Common horn signals).)\n\nSignals illustrated below are for North American railroads, \"o\" for short sounds, and \"–\" for longer sounds.\n\nNot all railroads use exactly the same whistle signals or assign the same meanings. Some railroads will use their own variations of the above. A few of the signals are obsolete because the workers they were used to communicate with (such as flagman) are now obsolete.\n\nIn Norway, for example, the following whistle signals are used:\n\n\nIn Finland, the following are some of the signals used:\n\n\n\n"}
{"id": "28312448", "url": "https://en.wikipedia.org/wiki?curid=28312448", "title": "Yellowstone-Teton Clean Energy Coalition", "text": "Yellowstone-Teton Clean Energy Coalition\n\nYellowstone-Teton Clean Energy Coalition's (YTCEC) mission is to displace the use of petroleum in the regional transportation sector, improve air quality through reduced harmful exhaust emissions, and increase energy security and sustainability. This is accomplished primarily through the promotion of alternative fuels and vehicles, integrated transportation systems, and energy conservation strategies and technologies that benefit the public interest by reducing energy consumption, particularly of petroleum based fuels.\n\nAs the sole regional designee of the U.S. Department of Energy's Clean Cities program, Yellowstone-Teton Clean Energy Coalition (YTCEC) functions as Department of Energy's on-the-ground advocate focused on petroleum displacement activities in the Greater Yellowstone Region. Currently consisting of nearly 90 organizations across the country, the Clean Cities program has been responsible for displacing over of petroleum fuel since its inception.\n\nAffiliation with the Clean Cities Program provides YTCEC with access to regional and national support networks in order to bring a much broader perspective to local transportation projects. It also allows for access to unique funding opportunities related to supporting the Clean Cities mission. This affiliation, along with YTCEC's regional standing as a resource and advocate for sustainable and efficient transportation, creates an ideal scenario for YTCEC to function as a clean transportation leader within the Greater Yellowstone Community in order to lessen the detrimental impacts of local transportation.\n\nThe Yellowstone-Teton Clean Energy Coalition was created in a ceremony at Old Faithful in September, 2002 becoming one of some 90 coalitions around the USA designated by the Department of Energy to address the U.S.' dependence on imported crude oil and help find solutions to the nation's energy challenges. Yellowstone National Park serves as the coalition's cornerstone and highest profile stakeholder in collaboration with gateway communities, other parks and agencies and the private sector. One of the few truly rural Clean Cities programs, the Yellowstone-Teton Clean Energy Coalition encompasses southwest Montana, eastern Idaho and western Wyoming.\n\nThe Clean Energy Coalition is a non-profit corporation with a board of 12 directors and administered by a coordinator. More than 100 \"stakeholders\" participate in Clean Energy Coalition events ranging from email information sharing to periodic meetings and special events.\n\nThe Clean Cities program grew out of U.S. Department of Energy initiatives in response to Congress passing the 1992 Energy Policy Act signed by then-President George H.W. Bush. One of the Act's goals is to reduce the amount of petroleum used by the transportation sector by promoting the use of alternative fuels and technologies such as ethanol, biodiesel, natural gas, propane and hybrid electric vehicles (HEVs). The Act mandated that a portion of all vehicles purchased for state and federal government use be Alternative Fuel Vehicles (AFVs), capable of burning one or more of the alternative fuels as well as gasoline.\n\nOne of the few truly rural Clean Cities Coalitions, Yellowstone-Teton Clean Energy Coalition encompasses areas of western Wyoming, southwestern Montana, and Eastern Idaho known as the Greater Yellowstone Ecosystem. The GYA encompasses more than and is the last intact, temperate ecosystem in the world. This ecosystem consists of two national parks (Yellowstone and Grand Teton), six National Forests (Targhee, Custer, Shoshone, Gallatin, Bridger/Teton, Deerlodge) and two U.S. Fish and Wildlife sites (National Elk Refuge, Red Lakes).\n\nThis very diverse region boasts alpine peaks, the largest high elevation lake in North America, mountain deserts, the highest concentration of geo-thermal features in the world, and is home to the complete array of animal species that were present in pre-historic times. Additionally, there are permittees that operate three major ski resorts, a commercial airport, large concessioner operations, and dozens of tour guides and outfitters that provide visitor services. Collectively, the area accommodates over seven million visitors annually. Grand Teton and Yellowstone National Parks are Mandatory Class 1 Areas, as defined by the 1977 Clean Air Act. These areas are subject to anti-degradation practices to maintain their status and protect them from impairment created by manmade air pollution. Notably, the GYA achieved a Clean Cities designation in 2002 as the Yellowstone-Teton Clean Energy Coalition, the only ecosystem to receive such recognition.\n\nAny well-marketed deployment of alternative fuels/advanced vehicle technologies in this region will receive extensive public exposure to the millions of annual visitors, residents, and business owners in the various 'gateway' cities adjacent to the parks. Tremendous potential exists for collaboration across the region to deploy a variety of alternative fuels and advanced technologies. These will decrease regional consumption of petroleum, improve air quality, create a stronger, more diverse energy portfolio for the region, while setting an example for the nation. The opportunity to successfully display these technologies in use in Yellowstone and Grand Teton National Parks will serve to further promote their use in the surrounding communities of Cody, Jackson, Dubois, West Yellowstone, Gardiner, Bozeman, Livingston, and beyond.\n\n"}
{"id": "14635488", "url": "https://en.wikipedia.org/wiki?curid=14635488", "title": "Yokosuka Research Park", "text": "Yokosuka Research Park\n\nYokosuka Research Park (YRP) is an area in Yokosuka City, Japan, where many of the wireless, mobile communications related companies have set up their research and development centers and joint testing facilities. \n\nYRP was constructed during the 1990s near to NTT's Yokosuka Research & Development Center.\n\nYRP is located in Hikari no Oka, Yokosuka City, about 90 minutes by train from central Tokyo. However, there are express through trains with significantly faster access. It is served by YRP Nobi Station on the Keikyu Kurihama Line, with direct services from Shinagawa in Tokyo.\n\n\n\n\n"}
