{"id": "17152824", "url": "https://en.wikipedia.org/wiki?curid=17152824", "title": "6AK5", "text": "6AK5\n\nThe 6AK5 vacuum tube is a miniature 7-pin sharp-cutoff pentode used as RF or IF amplifier especially in high-frequency wide-band applications at frequencies up to 400 MHz.\n\nIt was developed by Bell Labs / Western Electric and used extensively as an I.F. amplifier in World War II radar systems. The tube is notable for its extremely fine grid, and extremely close control grid to cathode spacing, yielding excellent high-frequency performance.\n\nIt is also known as EF95 under its Mullard–Phillips designation and was produced in the former Soviet Union as type 6Zh1P (Russian: 6Ж1П) under the Russian designation system.\n\nA version of this tube with extended ratings was designated 6AK5W and 6Zh1P-EV (Russian: 6Ж1П-ЕВ) respectively.\n\nEven though primarily intended for VHF amplification, the tube has found some use in audio applications as a microphone preamplifier, for instance in the LOMO 19A9 microphone and in guitar effects stompboxes, such as Metasonix TM-7 Scrotum Smasher.\n\nThe 6AK5 was used in some types of radiosonde (weather balloon payloads), in the 1960s and 70s. Because of the non-recoverable nature of the equipment, the single 6AK5 was soldered directly to the printed circuit board and served as the only active device in the circuit, performing the function of a low frequency modulation oscillator and the high frequency carrier oscillator and output stage.\n\n\n"}
{"id": "44076517", "url": "https://en.wikipedia.org/wiki?curid=44076517", "title": "A2 Wind Tunnel", "text": "A2 Wind Tunnel\n\nA2 Wind Tunnel is a full-scale general-purpose open-return wind tunnel located in Mooresville, North Carolina. Created in 2006 by Gary Eaker, the tunnel is able to host a variety of objects including full scale cars, motorcycles, and bicycles.\n\nSome of the more notable entities to utilize testing at A2 Wind Tunnel include Lance Armstrong, Kristin Armstrong, the 2010 Winter Olympics gold medalist U.S. bobsled team with \"NightTrain.\" The tunnel has also been the site of filming for several entities including the band Saving Abel for the music video Drowning (Face Down) in 2009, and a Dale Earnhardt documentary.\n"}
{"id": "49297846", "url": "https://en.wikipedia.org/wiki?curid=49297846", "title": "AISINDO", "text": "AISINDO\n\nAISINDO is the Indonesian chapter of Association for Information Systems or AIS. Association for Information Systems (AIS) is the premier professional association for individuals and organizations who lead the research, teaching, practice, and study of information systems worldwide. AIS has members from over 90 countries, and comprises three regions: Region 1, the Americas; Region 2, Europe, the Middle East, and Africa; and Region 3, Asia and the Pacific. AISINDO was established on December 2, 2013 in Bali and was commemorated by the immediate past president, Prof Doug Vogel of City University of Hong Kong.\nAISINDO aims to:\n1. Promote the Information Systems as a discipline of knowledge, a research area, and organization practice in Indonesia.\n2. Position Information Systems as a leading profession in the service of society in Indonesia.\n3. Lead and promote excellence and a global standard in Information Systems education by adopting local values and context of Indonesia.\n4. Explore and develop Information Systems research topics, theories, technologies.\n5. Facilitate a network development between AISINDO members and AIS members around the worlds.\n6. Provide services and products to meet the diverse needs of members and Information Systems related communities in Indonesia.\n\nThe chapter is hosted by the Department of Information Systems, Institut Teknologi Sepuluh Nopember (ITS), Indonesia.\nWebsite at http://aisindo.org/ \n\nChapter President 2014 - 2018: Tony D. Susanto, Ph.D.\n"}
{"id": "6590236", "url": "https://en.wikipedia.org/wiki?curid=6590236", "title": "Association of Personal Computer User Groups", "text": "Association of Personal Computer User Groups\n\nThe Association of Personal Computer User Groups (APCUG) is a worldwide organization that helps computer user groups by facilitating communications between APCUG member groups, computer hardware and software makers, and hardware and software vendors. A non-profit corporation as designated by the U.S. Internal Revenue Service, APCUG also helps member groups and their officers fulfill their education goals with support materials and shared knowledge and experience.\n\nWhile a large number of member groups in APCUG are oriented towards the Microsoft DOS and Windows operating systems, many member groups have SIGs (Special Interest Groups), Forums, Classes, etc. for Apple and Android devices as well as the Linux OS and Chromebooks. Membership is open to all technology user groups.\n\nAPCUG itself is not a user group; only user groups/clubs themselves are members. APCUG services are offered to group leaders and members.\n\nAPCUG began after a series of meetings and discussions between representatives from various user groups around the country about improving communications between groups and sharing information. The presidents from three user groups—Boston Computer Society, Capital PC User Group, and Houston Area League of PC Users—organized the First Annual User Group Summit meeting at the 1986 Fall Comdex.\n\nAfter that first Summit meeting and subsequent meetings, the leaders of 15 user groups met in Seattle in October 1987 and proposed the formation of an association for the purpose of fostering communication among and between user groups. That proposal was presented before 130 representatives from 50 user groups at the Second Annual User Group Summit Meeting in November 1987 and was unanimously approved.\n\nAPCUG offers many benefits to its member groups, including:\n\nAPCUG2.org contains information about:\n\n"}
{"id": "26696969", "url": "https://en.wikipedia.org/wiki?curid=26696969", "title": "Atari AMY", "text": "Atari AMY\n\nThe Atari AMY (or Amy) was a 64-oscillator additive synthesizer implemented as a single-IC sound chip. It was initially developed as part of a larger effort to develop a 16/32-bit workstation known as Sierra, but this project was cancelled in 1984. For a time, AMY was slated to be included in the Atari 520ST, then an updated version of the Atari 8-bit family, the 65XEM, but development was discontinued. The technology was later sold, but when the new owners started to introduce it as a professional synthesizer, Atari sued, and work on the project ended.\n\nThe AMY was based around a bank of 64 oscillators, which emit sine waves of a specified frequency. The sine waves were created by looking up the amplitude at a given time from a 16-bit table stored in ROM, rather than calculating the amplitude using math hardware. The signals could then be mixed together to perform additive synthesis. The AMY also included a number of ramp generators that could be used to smoothly modify the amplitude or frequency of a given oscillator over a given time. During the design phase, it was believed these would be difficult to implement in hardware, so only eight frequency ramps are included.\n\nSounds were created by selecting one of the oscillators to be the master channel, and then attaching other oscillators and ramps to it, slaved to some multiple of the fundamental frequency. Sound programs then sent the AMY a series of instructions setting the master frequency, and instructions on how quickly to ramp to new values. The output of the multiple oscillators was then summed and sent to the output. The AMY allowed the oscillators to be combined in any fashion, two at a time, to produce up to eight output channels. The output was then converted to analog in a separate (user-provided) digital-to-analog converter.\n\nWhile the additive synth system works well for sounds with a narrow spectrum, it is not useful for wider spectrum sounds like white noise. To fill the need to generate the sounds of explosions, jet engines and similar sounds, AMY also included random noise generators that could be mixed into the master oscillator to randomly shift the output.\n\nThe AMY was particularly useful for digital sound playback given the limited memory and bandwidth resources available at the time. An input sample could be run through a Fast Fourier transform to extract the spectral pattern, and then that pattern could be input to the AMY to set up the oscillators. The result is a highly accurate rendition of the original signal, but reduced to a handful of parameters that could easily be stored. That pattern could then be shifted up or down simply by changing the frequency of the master oscillator, with the slaved oscillators following those changes naturally. In one experiment, telephone-quality voice audio was produced using this method, requiring only 2400 baud of bandwidth.\n\nAmy was developed as an experiment within the Sunnyvale Research Lab (SRL) of Atari, starting in 1983. Amy's system design was based on Hal Alles' experimental work at Bell Labs during the 1970s, which produced a similar system that required several racks of equipment to implement the so-called Alles Machine. Several of Alles' solutions to particularly thorny implementation issues were used in Amy. The Amy team was led by Gary Sikorski, and the primary architects were Scott Foster and Steve Saunders. The single-chip implementation was handled by Sam Nicolino, while Jack Palevich and Tom Zimmerman wrote support software.\n\nAmy was announced in an Atari-internal mailing list in March 1984, with a short description and a June estimated time frame for shipping the first version, the AMY-1, with volume quantities available that December. The first versions would run up to 5 MHz, but a second run improved this to 10 MHz. Spec sheets used 4 and 8 MHz and basic clock speeds, respectively.\n\nThe initial target for Amy was a 16/32-bit computer also being designed by SRL, known as the Atari Sierra. Sierra used Amy for sound and a pair of chips code-named \"Gold\" and \"Silver\" for graphics, and was considering either the intel 286 or Motorola 68000 as its CPU. Sierra was one of several similar projects being carried out in different Atari divisions, although they used different sound hardware.\n\nBy the time the AMY-1 was ready for production, Atari was in disarray. In July, Jack Tramiel bought Atari from Warner Communications and quickly dismantled the majority of Atari's engineering departments. The Sierra project quickly broke up.\n\nEarly news articles and development notes for the 520ST mention the AMY as its sound unit. However, by the time it was released, the AMY had been replaced by the off-the-shelf Yamaha YM2149.\n\nDuring this process, the Amy team persuaded them to adapt it for use in the 8-bit machines. The result was the 65XEM, which combined the existing 65XE with the AMY. First shown publicly at the Consumer Electronics Show in January 1985, the XEM carried a premium $30 to $50 above the basic 65XE, which was also being launched at the same show. \n\nHowever, as the company's focus quickly shifted from the 8-bit line to the new Atari ST's being launched at the same time, the XEM was shunted aside and never released commercially. A few prototypes were built, some of which later found their way into the hands of private collectors.\n\nTramiel later decided to sell off the technology, and reached an agreement with the Millwaukee-based company, Sight & Sound. They developed a new version with 32 oscillators, along with a rack-mount MIDI synthesizer based on it. However, as they were preparing to ship the product, Atari threatened to sue them, and the system never shipped.\n\n\n"}
{"id": "2039", "url": "https://en.wikipedia.org/wiki?curid=2039", "title": "Avionics", "text": "Avionics\n\nAvionics are the electronic systems used on aircraft, artificial satellites, and spacecraft. Avionic systems include communications, navigation, the display and management of multiple systems, and the hundreds of systems that are fitted to aircraft to perform individual functions. These can be as simple as a searchlight for a police helicopter or as complicated as the tactical system for an airborne early warning platform. The term \"avionics\" is a portmanteau of the words \"aviation\" and \"electronics\".\n\nThe term \"avionics\" was coined by the journalist Philip J. Klass as a portmanteau of \"aviation electronics\". Many modern avionics have their origins in World War II wartime developments. For example, autopilot systems that are commonplace today began as specialized systems to help bomber planes fly steadily enough to hit precision targets from high altitudes. Famously, radar was developed in the UK, Germany, and the United States during the same period. Modern avionics is a substantial portion of military aircraft spending. Aircraft like the F‑15E and the now retired F‑14 have roughly 20 percent of their budget spent on avionics. Most modern helicopters now have budget splits of 60/40 in favour of avionics. \n\nThe civilian market has also seen a growth in cost of avionics. Flight control systems (fly-by-wire) and new navigation needs brought on by tighter airspaces, have pushed up development costs. The major change has been the recent boom in consumer flying. As more people begin to use planes as their primary method of transportation, more elaborate methods of controlling aircraft safely in these high restrictive airspaces have been invented.\n\nAvionics plays a heavy role in modernization initiatives like the Federal Aviation Administration's (FAA) Next Generation Air Transportation System project in the United States and the Single European Sky ATM Research (SESAR) initiative in Europe. The Joint Planning and Development Office put forth a roadmap for avionics in six areas:\n\nThe Aircraft Electronics Association reports $1.73 billion avionics sales for the first three quarters of 2017 in business and general aviation, a 4.1% yearly improvement: 73.5% came from North America, forward-fit represented 42.3% while 57.7% were retrofits as the U.S. deadline of Jan. 1, 2020 for mandatory ADS-B out approach.\n\nThe cockpit of an aircraft is a typical location for avionic equipment, including control, monitoring, communication, navigation, weather, and anti-collision systems. The majority of aircraft power their avionics using 14- or 28‑volt DC electrical systems; however, larger, more sophisticated aircraft (such as airliners or military combat aircraft) have AC systems operating at 400 Hz, 115 volts AC. There are several major vendors of flight avionics, including Panasonic Avionics Corporation, Honeywell (which now owns Bendix/King), Universal Avionics Systems Corporation, Rockwell Collins, Thales Group, GE Aviation Systems, Garmin, Raytheon, Parker Hannifin, UTC Aerospace Systems and Avidyne Corporation.\n\nInternational standards for avionics equipment are prepared by the Airlines Electronic Engineering Committee (AEEC) and published by ARINC.\n\nCommunications connect the flight deck to the ground and the flight deck to the passengers. On‑board communications are provided by public-address systems and aircraft intercoms.\n\nThe VHF aviation communication system works on the airband of 118.000 MHz to 136.975 MHz. Each channel is spaced from the adjacent ones by 8.33 kHz in Europe, 25 kHz elsewhere. VHF is also used for line of sight communication such as aircraft-to-aircraft and aircraft-to-ATC. Amplitude modulation (AM) is used, and the conversation is performed in simplex mode. Aircraft communication can also take place using HF (especially for trans-oceanic flights) or satellite communication.\n\nAir navigation is the determination of position and direction on or above the surface of the Earth. Avionics can use satellite navigation systems (such as GPS and WAAS), INS( inertial navigation system), ground-based radio navigation systems (such as VOR or LORAN), or any combination thereof. Navigation systems calculate the position automatically and display it to the flight crew on moving map displays. Older avionics required a pilot or navigator to plot the intersection of signals on a paper map to determine an aircraft's location; modern systems calculate the position automatically and display it to the flight crew on moving map displays.\n\n The first hints of glass cockpits emerged in the 1970s when flight-worthy cathode ray tube (CRT) screens began to replace electromechanical displays, gauges and instruments. A \"glass\" cockpit refers to the use of computer monitors instead of gauges and other analog displays. Aircraft were getting progressively more displays, dials and information dashboards that eventually competed for space and pilot attention. In the 1970s, the average aircraft had more than 100 cockpit instruments and controls.\n\nGlass cockpits started to come into being with the Gulfstream G‑IV private jet in 1985. One of the key challenges in glass cockpits is to balance how much control is automated and how much the pilot should do manually. Generally they try to automate flight operations while keeping the pilot constantly informed.\n\nAircraft have means of automatically controlling flight. Autopilot was first invented by Lawrence Sperry during World War I to fly bomber planes steady enough to hit accurate targets from 25,000 feet. When it was first adopted by the U.S. military, a Honeywell engineer sat in the back seat with bolt cutters to disconnect the autopilot in case of emergency. Nowadays most commercial planes are equipped with aircraft flight control systems in order to reduce pilot error and workload at landing or takeoff.\n\nThe first simple commercial auto-pilots were used to control heading and altitude and had limited authority on things like thrust and flight control surfaces. In helicopters, auto-stabilization was used in a similar way. The first systems were electromechanical. The advent of fly by wire and electro-actuated flight surfaces (rather than the traditional hydraulic) has increased safety. As with displays and instruments, critical devices that were electro-mechanical had a finite life. With safety critical systems, the software is very strictly tested.\n\nTo supplement air traffic control, most large transport aircraft and many smaller ones use a traffic alert and collision avoidance system (TCAS), which can detect the location of nearby aircraft, and provide instructions for avoiding a midair collision. Smaller aircraft may use simpler traffic alerting systems such as TPAS, which are passive (they do not actively interrogate the transponders of other aircraft) and do not provide advisories for conflict resolution.\n\nTo help avoid controlled flight into terrain (CFIT), aircraft use systems such as ground-proximity warning systems (GPWS), which use radar altimeters as a key element. One of the major weaknesses of GPWS is the lack of \"look-ahead\" information, because it only provides altitude above terrain \"look-down\". In order to overcome this weakness, modern aircraft use a terrain awareness warning system (TAWS).\n\nCommercial aircraft cockpit data recorders, commonly known as \"black boxes\", store flight information and audio from the cockpit. They are often recovered from an aircraft after a crash to determine control settings and other parameters during the incident.\n\nWeather systems such as weather radar (typically Arinc 708 on commercial aircraft) and lightning detectors are important for aircraft flying at night or in instrument meteorological conditions, where it is not possible for pilots to see the weather ahead. Heavy precipitation (as sensed by radar) or severe turbulence (as sensed by lightning activity) are both indications of strong convective activity and severe turbulence, and weather systems allow pilots to deviate around these areas.\n\nLightning detectors like the Stormscope or Strikefinder have become inexpensive enough that they are practical for light aircraft. In addition to radar and lightning detection, observations and extended radar pictures (such as NEXRAD) are now available through satellite data connections, allowing pilots to see weather conditions far beyond the range of their own in-flight systems. Modern displays allow weather information to be integrated with moving maps, terrain, and traffic onto a single screen, greatly simplifying navigation.\n\nModern weather systems also include wind shear and turbulence detection and terrain and traffic warning systems. In‑plane weather avionics are especially popular in Africa, India, and other countries where air-travel is a growing market, but ground support is not as well developed.\n\nThere has been a progression towards centralized control of the multiple complex systems fitted to aircraft, including engine monitoring and management. Health and usage monitoring systems (HUMS) are integrated with aircraft management computers to give maintainers early warnings of parts that will need replacement.\n\nThe integrated modular avionics concept proposes an integrated architecture with application software portable across an assembly of common hardware modules. It has been used in fourth generation jet fighters and the latest generation of airliners.\n\nMilitary aircraft have been designed either to deliver a weapon or to be the eyes and ears of other weapon systems. The vast array of sensors available to the military is used for whatever tactical means required. As with aircraft management, the bigger sensor platforms (like the E‑3D, JSTARS, ASTOR, Nimrod MRA4, Merlin HM Mk 1) have mission-management computers.\n\nPolice and EMS aircraft also carry sophisticated tactical sensors.\n\nWhile aircraft communications provide the backbone for safe flight, the tactical systems are designed to withstand the rigors of the battle field. UHF, VHF Tactical (30–88 MHz) and SatCom systems combined with ECCM methods, and cryptography secure the communications. Data links such as Link 11, 16, 22 and BOWMAN, JTRS and even TETRA provide the means of transmitting data (such as images, targeting information etc.).\n\nAirborne radar was one of the first tactical sensors. The benefit of altitude providing range has meant a significant focus on airborne radar technologies. Radars include airborne early warning (AEW), anti-submarine warfare (ASW), and even weather radar (Arinc 708) and ground tracking/proximity radar.\n\nThe military uses radar in fast jets to help pilots fly at low levels. While the civil market has had weather radar for a while, there are strict rules about using it to navigate the aircraft.\n\nDipping sonar fitted to a range of military helicopters allows the helicopter to protect shipping assets from submarines or surface threats. Maritime support aircraft can drop active and passive sonar devices (sonobuoys) and these are also used to determine the location of enemy submarines.\n\nElectro-optic systems include devices such as the head-up display (HUD), forward looking infrared (FLIR), infra-red search and track and other passive infrared devices (Passive infrared sensor). These are all used to provide imagery and information to the flight crew. This imagery is used for everything from search and rescue to navigational aids and target acquisition.\n\nElectronic support measures and defensive aids are used extensively to gather information about threats or possible threats. They can be used to launch devices (in some cases automatically) to counter direct threats against the aircraft. They are also used to determine the state of a threat and identify it.\n\nThe avionics systems in military, commercial and advanced models of civilian aircraft are interconnected using an avionics databus. Common avionics databus protocols, with their primary application, include:\n\n\n\n"}
{"id": "28248098", "url": "https://en.wikipedia.org/wiki?curid=28248098", "title": "Barranquilla's Transmetro", "text": "Barranquilla's Transmetro\n\nTransmetro, officially, Sistema Integrado de Transporte Masivo de Barranquilla y su Área Metropolitana (Spanish for: \"Mass Transit Integrated System of Barranquilla and its Metropolitan Area\"), is a mass transit system that has operated in the city of Barranquilla, Colombia since April 7, 2010.\n\n"}
{"id": "241247", "url": "https://en.wikipedia.org/wiki?curid=241247", "title": "Bellows", "text": "Bellows\n\nA bellows or pair of bellows is a device constructed to furnish a strong blast of air. The simplest type consists of a flexible bag comprising a pair of rigid boards with handles joined by flexible leather sides enclosing an approximately airtight cavity which can be expanded and contracted by operating the handles, and fitted with a valve allowing air to fill the cavity when expanded, and with a tube through which the air is forced out in a stream when the cavity is compressed. It has many applications, in particular blowing on a fire to supply it with air.\n\nThe term \"bellows\" is used by extension for a flexible bag whose volume can be changed by compression or expansion, but not used to deliver air. For example, the light-tight (but not airtight) bag allowing the distance between the lens and film of a folding photographic camera to be varied is called a bellows.\n\n\"Bellows\" is only used in plural. The Old English name for 'bellows' was blǽstbęl(i)g, blást-bęl(i)g 'blast-bag, blowing-bag'; the prefix was dropped and by the eleventh century the simple bęlg, bylg, bylig ('bag') was used. The word is cognate with \"belly\". There are similar words in Old Norse, Swedish, and Danish, but the derivation is not certain. 'Bellows' appears not to be cognate with the apparently similar Latin \"follis\".\n\nSeveral processes, such as metallurgical iron smelting and welding, require so much heat that they could only be developed after the invention, in antiquity, of the bellows. The bellows are used to deliver additional air to the fuel, raising the rate of combustion and therefore the heat output.\n\nVarious kinds of bellows are used in metallurgy:\n\nThe manufacturing of different alloys in China often required a continuous stream of air that was able to be blown over through molten metals. The use of double-action piston bellows was described by Chinese philosopher Lao Tzu around 500 BC to produce a continuous stream of air. The Han Dynasty Chinese mechanical engineer Du Shi (d. 38) is credited with being the first to use double-action piston pumps to apply hydraulic power, through a waterwheel, to operate bellows in metallurgy. His invention was used to operate piston bellows of blast furnaces in order to forge cast iron. The ancient Greeks, ancient Romans, and other civilizations used bellows in bloomery furnaces producing wrought iron. Bellows are also used to send pressurized air in a controlled manner in a fired heater. \n\nIn modern industry, reciprocating bellows are usually replaced with motorized blowers.\n\nDouble-acting piston bellows are a type of bellows used by blacksmiths and smelters to increase the air flow going into the forge, with the property that air is blown out on both strokes of the handle (in contrast to simpler and more common bellows that blow air when the stroke is in one direction and refill the bellows in the other direction). These bellows blow a more constant, and thus stronger, blast than simple bellows. Such bellows existed in China at least since the 5th century BC, when it was invented, and had reached Europe by the 16th century. In 240 BC, The ancient Greek inventor Ctesibius of Alexandria independently invented a double-action piston bellow used to lift water from one level to another. \n\nA piston is enclosed in a rectangular box with a handle coming out one side. The piston edges are covered with feathers, fur, or soft paper to ensure that it is airtight and lubricated. As the piston is pulled, air from one side enters and flows through the nozzle and as it is pushed air enters from the opposite side and flows through the same nozzle.\n\nThese have three leaves. The middle leaf is fixed in place. The bottom leaf is moved up and down. The top leaf can move freely and has a weight on it. The bottom and the middle leaves contain valves, the top one does not. Only the top lung is connected to the spout.\n\nWhen the bottom leaf is moved up, air is pumped from the bottom lung into the top lung. At the same time air is leaving the bellows from the top lung through the spout, but at a slower rate. This inflates the top lung. Next the bottom leaf is moved down to pull fresh air into the bellows. While this happens the weight on the top leaf pushes it down, so air keeps leaving through the spout.\n\nThis design does not increase the amount of air flow going into the forge, but provides a more constant air flow compared to a simple bellows. It also provides more even air flow than two simple bellows pumped alternately or one double-acting piston bellows.\n\n\nThe term \"bellows\" is used by extension for a number of applications that do not involve air transfer.\n\n\n\n"}
{"id": "24104531", "url": "https://en.wikipedia.org/wiki?curid=24104531", "title": "Cleaning card", "text": "Cleaning card\n\nCleaning cards are disposable products designed to clean the interior contact points of a device that facilitates an electronic information transaction (point of sale terminal, automated teller machine, remote deposit check scanners, micr readers, magnetic stripe reader, bill acceptor, bill validator, access control locks, etc.). In order for the cleaning card to work properly in the device, the card resembles or mimics the material of the transaction media – such as a credit card, check, or currency. As the cleaning card is inserted and passed through the device, it will clean components that would normally come in contact with the transaction media such as readers, lenses, read/write chip and pins, belts, rollers, and paths. Cleaning card products are widely accepted and endorsed by device manufacturers and industry professionals. Many have developed their own cleaning cards to better clean their particular devices.\n\nA typical cleaning card is much like a wiper or sponge that can get into areas that are not readily accessible. Typically, the cleaning card has a solid core covered by a soft wipe-like material. The product is then saturated with a cleaning solution recommended by the device manufacturer and then placed in a sealed pouch to maintain the saturation level and cleanliness of the card.\n\nThe cleaning card was originally patented by Stanley H. Eyler and the patent (US#5525417 A) was assigned to his employer, the Clean Team Company. The Clean Team Company later changed its name to KICTeam, Inc., which continues to be the leading manufacturer of cleaning cards. The cleaning card has evolved with the equipment they need to clean. A good example is the bill acceptor. Initially, the bill acceptor was designed for vending machines as a means of selling candy to the public. It includes a device that recognizes that a US one dollar bank note has been inserted. The cleaning card was required to be the same shape as US currency in order to be accepted into the device to clean it. Vending machines began accepting higher denominations as well as having the ability to make change. Specialized sensors were introduced into the bill acceptors to recognize multiple denominations and to only accept media that contained bank note characteristics. The bill acceptor cleaning card was redeveloped to contain magnetic ink and bank note characteristics so as to be accepted by the equipment. The development of bill acceptors for slot machines in the gaming and casino industry required the bill acceptor to be more sophisticated. The bill validators needed to validate currency of multiple denominations up to a one hundred dollar bank note. Fraud was now a critical issue and was addressed by multiple sensors and optics throughout the inserted currency pathway. These sensors and optics were recessed so as to keep currency from running across them with each insertion and wearing down sensitive lenses.\n\nCleaning cards are used in the gaming, wagering, vending, hotel, retail, lottery, petroleum, manufacturing, shipping, auto id, card printing, banking. For example, this includes all places that credit cards or cash are inserted into a machine to make payments.\n\nThe magnetic head inside the POS Terminal is a fixed component and for this reason it only can be cleared by cleaning cards that are flexible enough to clean the leading, center and trailing edges of this round reader head. The cleaning of the magnetic head is very important because it's responsible for the reading of the card and so it decides acceptance or rejection of the inserted card.\nThe cleaning card is not only cleaning the reading area of the magnetic head which is cleared. There is also a cleaning process within the device along the card path. The cleaning in these high dirt build-up areas is especially important and ensures efficient cleaning of the card reader.\n\nChip Cards are also known as Smart Cards and EMV Cards. There are two different types of EMV card readers - friction and landing. Contaminated contacts can result in rejection of the inserted payment or authorization card. The cleaning card with ensures optimal cleaning of chip reading contacts.\n\nMotorized readers are built in, for example ATMs. The credit/debit-card is inserted into the card slot, where the first magnetic head is placed. If a magnetic stripe can be recognized, a shutter will be opened and the card will be transported to the second magnetic head by roles. The card is read, thereby the device knows whether the transaction goes over the micro chip or the magnetic stripe. If no micro chip is placed on the card, the transaction goes directly above the magnetic head. If the data gives the order for a transaction over the micro chip, the card is placed on the chip reading contact and is stopped. The chip contacts, are fitted on the chip and now the transaction begins. If the reading of the magnetic stripe respectively by the micro chip is not possible, the card will be declined.\nA cleaning card for a motorized card reader will need a magnetic stripe built into it to activate the acceptance shutter.\n\nCheck scanners are used by banks or business through remote deposit capture programs to take a digital image of the check and send the information to the bank for deposit. This is where the image of a check on your bank statement originates. If a check scanner is not properly cleaned financial institutions risk increased transaction failures, equipment malfunctions, personnel costs to reconcile poor images, equipment repair or exchange and non-compliance due to poor image quality. A cleaning card designed to clean a specific model of check scanner is run through the device the same way the operator would run a check through the device. The cleaning card makes contact with the optical lenses, micr reader, transport belts and rollers, print heads and clears the check path.\n\nTransactions are any action that has a monetary implication or transfer information from one media to another. The most commonly thought of transactions are the use of credit or debit cards through a card reader of some type. Card readers are also widely used for hotel door locks or access control devices. Another of the most common is a currency transaction via vending, slot machines, or self-checkout kiosk where a bill acceptor takes currency or a currency detector tabulates quantity. Many printers are transaction devices such as cashless ticket printers in the gaming industry.\n\n"}
{"id": "8117924", "url": "https://en.wikipedia.org/wiki?curid=8117924", "title": "Cognitive orthotics", "text": "Cognitive orthotics\n\nCognitive orthotics are software-based personal reminder systems for people with cognitive impairment, such as memory loss. People who can benefit include the elderly, people who have experienced traumatic brain injury, and anyone who experiences memory loss. These devices may be installed on personal digital assistants (PDAs). They may include elements of adaptive programming or artificial intelligence, to accommodate the needs of the individuals more appropriately.\n\nBrand name examples include Autominder and PEAT (Planning and Execution Assistant and Trainer).\n\n"}
{"id": "33176131", "url": "https://en.wikipedia.org/wiki?curid=33176131", "title": "Design classic", "text": "Design classic\n\nA design classic is an industrially manufactured object with timeless aesthetic value. It serves as a standard of its kind and remains up to date regardless of the year of its design.\nWhether a particular object is a design classic might often be debatable and the term is sometimes abused \nbut there exists a body of acknowledged classics of product designs from the 19th and 20th century.\nFor an object to become a design classic requires time, and whatever lasting impact the design has had on society, together with its influence on later designs, play large roles in determining whether something becomes a design classic. Thus, design classics are often strikingly simple, going to the essence, and are described with words like \"iconic\", \"neat\", \"valuable\" or \"having meaning\". for example a vacuum cleaner is often known as a hoover, which was not the vacuum cleaner's name but instead was the surname of its inventor, William Henry Hoover.\n"}
{"id": "61967", "url": "https://en.wikipedia.org/wiki?curid=61967", "title": "Detonator", "text": "Detonator\n\nA detonator, frequently a blasting cap, is a device used to trigger an explosive device. Detonators can be chemically, mechanically, or electrically initiated, the latter two being the most common.\n\nThe commercial use of explosives uses electrical detonators or the capped fuse which is a length of safety fuse to which an ordinary detonator has been crimped. Many detonators' primary explosive is a material called ASA compound. This compound is formed from lead azide, lead styphnate and aluminium and is pressed into place above the base charge, usually TNT or tetryl in military detonators and PETN in commercial detonators.\n\nOther materials such as DDNP (diazo dinitro phenol) are also used as the primary charge to reduce the amount of lead emitted into the atmosphere by mining and quarrying operations. Old detonators used mercury fulminate as the primary, often mixed with potassium chlorate to yield better performance.\n\nA blasting cap is a small sensitive primary explosive device generally used to detonate a larger, more powerful and less sensitive secondary explosive such as TNT, dynamite, or plastic explosive.\n\nBlasting caps come in a variety of types, including non-electric caps, electric caps, and fuse caps. They are used in commercial mining, excavation, and demolition. Electric types are set off by a short burst of current conducted from a blasting machine by a long wire to the cap to ensure safety. Traditional fuse caps have a fuse which is ignited by a flame source, such as a match or a lighter.\n\nThe need for detonators such as blasting caps came from the development of safer explosives. Different explosives require different amounts of energy (their activation energy) to detonate. Most commercial explosives are formulated with a high activation energy, to make them stable and safe to handle so they will not explode if accidentally dropped, mishandled, or exposed to fire. These are called secondary explosives. However they are correspondingly difficult to detonate intentionally, and require a small initiating explosion. This is provided by a detonator.\n\nA detonator contains an easy-to-ignite primary explosive that provides the initial activation energy to start the detonation in the main charge. Explosives commonly used in detonators include mercury fulminate, lead azide, lead styphnate, tetryl, and DDNP. Blasting caps and some detonators are stored separately and not inserted into the main explosive charge until just before use, keeping the main charge safe. \nEarly blasting caps also used silver fulminate, but it has been replaced with cheaper and safer primary explosives.\nSilver azide is still used sometimes, but very rarely due to its high price.\n\nDetonators are hazardous for untrained personnel to handle since they contain primary explosive. They are sometimes not recognized as explosives due to their appearance, leading to injuries.\n\nOrdinary detonators usually take the form of ignition-based explosives. While they are mainly used in commercial operations, ordinary detonators are still used in military operations. This form of detonator is most commonly initiated using safety fuse, and used in non time-critical detonations e.g. conventional munitions disposal. Well known detonators are lead azide, Pb(N), silver azide (AgN) and mercury fulminate [Hg(ONC)].\n\nThere are three categories of electrical detonators: instantaneous electrical detonators (IED), short period delay detonators (SPD) and long period delay detonators (LPD). SPDs are measured in milliseconds and LPDs are measured in seconds. In situations where nanosecond accuracy is required, specifically in the implosion charges in nuclear weapons, exploding-bridgewire detonators are employed. The initial shock wave is created by vaporizing a length of a thin wire by an electric discharge. A new development is a slapper detonator, which uses thin plates accelerated by an electrically exploded wire or foil to deliver the initial shock. It is in use in some modern weapon systems. A variant of this concept is used in mining operations, when the foil is exploded by a laser pulse delivered to the foil by optical fiber.\n\nA non-electric detonator is a shock tube detonator designed to initiate explosions, generally for the purpose of demolition of buildings and for use in the blasting of rock in mines and quarries. Instead of electric wires, a hollow plastic tube delivers the firing impulse to the detonator, making it immune to most of the hazards associated with stray electric current. It consists of a small diameter, three-layer plastic tube coated on the innermost wall with a reactive explosive compound, which, when ignited, propagates a low energy signal, similar to a dust explosion. The reaction travels at approximately 6,500 ft/s (2,000 m/s) along the length of the tubing with minimal disturbance outside of the tube. The design of non-electric detonators incorporates patented technology, including the Cushion Disk (CD) and Delay Ignition Buffer (DIB) to provide reliability and accuracy in all blasting applications. Non-electric detonators was invented by the Swedish company Nitro Nobel in the 1960s and 1970s, under the leadership of Per-Anders Persson, and launched to the demolitions market in 1973. (Nitro Nobel became a part of Dyno Nobel after being sold to Norwegian Dyno Industrier AS in 1986.) Nonel is a contraction of \"Non-electric detonators\".\n\nIn civil mining, electronic detonators have a better precision for delays. Electronic detonators are designed to provide the precise control necessary to produce accurate and consistent blasting results in a variety of blasting applications in the mining, quarrying, and construction industries. Electronic detonators may be programmed in 1-millisecond increments from 1 millisecond to 10,000 milliseconds using the dedicated programming device called the logger.\n\nBenefits:\n\n\nThe oldest and simplest type of cap, fuse caps are a metal cylinder, closed at one end. From the open end inwards, there is first an empty space into which a pyrotechnic fuse is inserted and crimped, then a pyrotechnic ignition mix, a primary explosive, and then the main detonating explosive charge.\n\nThe primary hazard of pyrotechnic blasting caps is that for proper usage, the fuse must be inserted and then crimped into place by crushing the base of the cap around the fuse. If the tool used to crimp the cap is used too close to the explosives, the primary explosive compound can detonate during crimping. A common hazardous practice is crimping caps with one's teeth; an accidental detonation can cause serious injury to the mouth.\n\nFuse type blasting caps are still in active use today. They are the safest type to use around certain types of electromagnetic interference, and they have a built in time delay as the fuse burns down.\n\nSolid pack electric blasting caps use a thin bridgewire in direct contact (hence solid pack) with a primary explosive, which is heated by electric current and causes the detonation of the primary explosive. That primary explosive then detonates a larger charge of secondary explosive.\n\nSome solid pack fuses incorporate a small pyrotechnic delay element, up to a few hundred milliseconds, before the cap fires.\n\nMatch type blasting caps use an electric match (insulating sheet with electrodes on both sides, a thin bridgewire soldered across the sides, all dipped in ignition and output mixes) to initiate the primary explosive, rather than direct contact between the bridgewire and the primary explosive. The match can be manufactured separately from the rest of the cap and only assembled at the end of the process.\n\nMatch type caps are now the most common type found worldwide.\n\nThis type of detonator was invented in the 1940s as part of the Manhattan Project to develop nuclear weapons. The design goal was to produce a detonator which acted very rapidly and predictably. Both Match and Solid Pack type electric caps take a few milliseconds to fire, as the bridgewire heats up and heats the explosive to the point of detonation. Explosive bridgewire or EBW detonators use a higher voltage electric charge and a very thin bridgewire, .04 inch long, .0016 diameter, (1 mm long, 0.04 mm diameter). Instead of heating up the explosive, the EBW detonator wire is heated so quickly by the high firing current that the wire actually vaporizes and explodes due to electric resistance heating. That electrically driven explosion then fires the detonator's initiator explosive (usually PETN).\n\nSome similar detonators use a thin metal foil instead of a wire, but operate in the same manner as true bridgewire detonators.\n\nIn addition to firing very quickly when properly activated, EBW detonators are safe from stray static electricity and other electric current. Enough current and the bridgewire may melt, but it is small enough that it cannot detonate the initiator explosive unless the full high-voltage high-current charge passes through the bridgewire. EBW detonators are used in many civilian applications where radio signals, static electricity, or other electrical hazards might cause accidents with conventional electric detonators.\n\nSlapper detonators are an improvement on EBW detonators. Slappers, instead of directly using the exploding foil to detonate the initiator explosive, use the electrical vaporization of the foil to drive a small circle of insulating material such as PET film or kapton down a circular hole in an additional disc of insulating material. At the far end of that hole is a pellet of conventional initiator explosive.\n\nThe conversion efficiency of energy from electricity into kinetic energy of the flying disk or slapper can be 20–40%.\n\nSince the slapper impacts a wide area, 40 thousandths or (roughly one mm across) of the explosive, rather than a thin line or point as in an exploding foil or bridgewire detonator, the detonation is more regular and requires less energy. Reliable detonation requires raising a minimum volume of explosive to temperatures and pressures at which detonation starts. If energy is deposited at a single point, it can radiate away in the explosive in all directions in rarefaction or expansion waves, and only a small volume is efficiently heated or compressed. The flier disc loses impact energy at its sides to rarefaction waves, but a conical volume of explosive is efficiently shock compressed.\n\nSlapper detonators are used in nuclear weapons. These components require large quantities of energy to initiate, making them extremely unlikely to accidentally discharge.\n\nIn this type, a pulse from a laser passes down an optical fiber to strike and thus initiate a carbon-doped explosive. These initiators are highly reliable. Unintentional initiation is very difficult as the explosive can only be detonated by the attached laser, which is precisely tuned to do so, or a completely independent laser that matches.\n\nThe first blasting cap or detonator was demonstrated in 1745 when British physician and apothecary William Watson, F.R.S., showed that the electric spark of a friction machine could ignite black powder, by way of igniting a flammable substance mixed in with the black powder.\n\nIn 1750, Benjamin Franklin in Philadelphia made a commercial blasting cap consisting of a paper tube full of black powder, with wires leading in both sides and wadding sealing up the ends. The two wires came close but did not touch, so a large electric spark discharge between the two wires would fire the cap.\n\nIn 1832, a hot wire detonator was produced by American chemist Robert Hare, although attempts along similar lines had earlier been attempted by the Italians Volta and Cavallo. Hare constructed his blasting cap by passing a multistrand wire through a charge of gunpowder inside a tin tube; he had cut all but one fine strand of the multistrand wire so that the fine strand would serve as the hot bridgewire. When a strong current from a large battery (which he called a \"deflagrator\" or \"calorimotor\") was passed through the fine strand, it became incandescent and ignited the charge of gunpowder.\n\nIn 1863, Alfred Nobel realized that although nitroglycerin could not be detonated by a fuse, it could be detonated by the explosion of a small charge of gunpowder, which in turn was ignited by a fuse. Within a year, he was adding mercury fulminate to the gunpowder charges of his detonators, and by 1867 he was using small copper capsules of mercury fulminate, triggered by a fuse, to detonate nitroglycerin.\n\nIn 1868, Henry Julius Smith of Boston, Massachusetts, introduced a cap that combined a spark gap ignitor and mercury fulminate, the first electric cap able to detonate dynamite.\n\nIn 1875, H. Julius Smith—and then in 1887, Perry G. Gardner of North Adams, Massachusetts—developed electric detonators that combined a hot wire detonator with mercury fulminate explosive. These were the first generally modern type blasting caps. Modern caps use different explosives and separate primary and secondary explosive charges, but are generally very similar to the Gardner and Smith caps.\n\nH. Julius Smith also invented the first satisfactory portable power supply for igniting blasting caps: a high-voltage magneto that was driven by a rack and pinion, which in turn was driven by a T-handle that was pushed downwards.\n\nElectric match caps were developed in the early 1900s in Germany, and spread to the US in the 1950s when ICI International purchased Atlas Powder Co. These match caps have become the predominant world standard cap type.\n\n\n\n"}
{"id": "26580361", "url": "https://en.wikipedia.org/wiki?curid=26580361", "title": "Digital materialization", "text": "Digital materialization\n\nDigital materialization (DM) \n\ncan loosely be defined as two-way direct communication or conversion between matter and information that enables people to exactly describe, monitor, manipulate and create any arbitrary real object. DM is a general paradigm alongside a specified framework that is suitable for computer processing and includes: holistic, coherent, volumetric modeling systems; symbolic languages that are able to handle infinite degrees of freedom and detail in a compact format; and the direct interaction and/or fabrication of any object at any spatial resolution without the need for “lossy” or intermediate formats.\n\n\"DM\" systems possess the following attributes:\n\n\nSuch an approach can not only be applied to tangible objects but can include the conversion of things such as light and sound to/from information and matter. Systems to digitally materialize light and sound already largely exist now (e.g. photo editing, audio mixing, etc.) and have been quite effective - but the representation, control and creation of tangible matter is poorly support by computational and digital systems.\n\nCommonplace computer-aided design and manufacturing systems currently represent real objects as \"2.5 dimensional\" shells. In contrast, DM proposes a deeper understanding and sophisticated manipulation of matter by directly using rigorous mathematics as complete volumetric descriptions of real objects. By utilizing technologies such as Function representation (FRep) it becomes possible to compactly describe and understand the surface and internal structures or properties of an object at an infinite resolution. Thus models can accurately represent matter across all scales making it possible to capture the complexity and quality of natural and real objects and ideally suited for digital fabrication and other kinds of real world interactions. DM surpasses the previous limitations of static disassociated languages and simple human-made objects, to propose systems that are heterogeneous, interacting directly and more naturally with the complex world.\n\nDigital and computer-based languages and processes, unlike the analogue counterparts, can computationally and spatially describe and control matter in an exact, constructive and accessible manner. However, this requires approaches that can handle the complexity of natural objects and materials.\n\n\n"}
{"id": "2055436", "url": "https://en.wikipedia.org/wiki?curid=2055436", "title": "Digital room correction", "text": "Digital room correction\n\nDigital room correction (or DRC) is a process in the field of acoustics where digital filters designed to ameliorate unfavorable effects of a room's acoustics are applied to the input of a sound reproduction system. Modern room correction systems produce substantial improvements in the time domain and frequency domain response of the sound reproduction system.\n\nThe use of analog filters, such as equalizers, to normalize the frequency response of a playback system has a long history; however, analog filters are very limited in their ability to correct the distortion found in many rooms. Although digital implementations of the equalizers have been available for some time, digital room correction is usually used to refer to the construction of filters which attempt to invert the impulse response of the room and playback system, at least in part. Digital correction systems are able to use acausal filters, and are able to operate with optimal time resolution, optimal frequency resolution, or any desired compromise along the Gabor limit. Digital room correction is a fairly new area of study which has only recently been made possible by the computational power of modern CPUs and DSPs.\n\nThe configuration of a digital room correction system begins with measuring the impulse response of the room at the listening location for each of the loudspeakers. Then, computer software is used to compute a FIR filter, which reverses the effects of the room and linear distortion in the loudspeakers. Finally, the calculated filter is loaded into a computer or other room correction device which applies the filter in real time. Because most room correction filters are acausal, there is some delay. Most DRC systems allow the operator to control the added delay through configurable parameters.\n\nDRC systems are not normally used to create a perfect inversion of the room's response because a perfect correction would only be valid at the location where it was measured: a few millimeters away the arrival times from various reflections will differ and the inversion will be imperfect. The imperfectly corrected signal may end up sounding worse than the uncorrected signal because the acausal filters used in digital room correction may cause pre-echo. Room correction filter calculation systems instead favor a robust approach, and employ sophisticated processing to attempt to produce an inverse filter which will work over a usably large volume, and which avoid producing bad-sounding artifacts outside of that volume, at the expense of peak accuracy at the measurement location.\n\n\n\n\n\n\n"}
{"id": "18490588", "url": "https://en.wikipedia.org/wiki?curid=18490588", "title": "Drill pipe", "text": "Drill pipe\n\nDrill pipe, is hollow, thin-walled, steel or aluminium alloy piping that is used on drilling rigs. It is hollow to allow drilling fluid to be pumped down the hole through the bit and back up the annulus. It comes in a variety of sizes, strengths, and wall thicknesses, but is typically 27 to 32 feet in length (Range 2). Longer lengths, up to 45 feet, exist (Range 3).\n\nDrill stems must be designed to transfer drilling torque for combined lengths that often exceed several miles down into the Earth's crust, and also must be able to resist pressure differentials between inside and outside (or vice versa), and have sufficient strength to suspend the total weight of deeper components. For deep wells this requires tempered steel tubes that are expensive, and owners spend considerable efforts to reuse them after finishing a well. \n\nA used drill stem is inspected on site, or off location. Ultrasonic testing and modified instruments similar to the spherometer are used at inspection sites to identify defects from metal fatigue, in order to preclude fracture of the drill stem during future wellboring. Drill pipe is classed as new (N class), becoming premium (P-class) and finally down to C (C 1 to 3) as fatigue is accumulated and as the outside diameter is worn down by usage. Eventually the drill pipe will be graded as scrap and marked with a red band.\n\nDrill pipe is a portion of the overall drill string. The drill string consists of both drill pipe and the bottom hole assembly (BHA), which is the tubular portion closest to the bit. The BHA will be made of thicker walled heavy weight drill pipe (HWDP) and drill collars, which have a larger outside diameter and provide weight to the drill bit and stiffness to the drilling assembly. Other BHA components can include a mud motor, measurement while drilling (MWD) apparatus, stabilizers, and various specialty downhole tools. The drill stem includes the entire drill string, plus the kelly that imparts rotation and torque to the drill pipe at the top.\n\nSee Drilling rig (petroleum) for a diagram of a drilling rig.\n\nModern drill pipe is made from the welding of at least three separate pieces: box tool joint, pin tool joint, and the tube. The green tubes are received by the drill pipe manufacturer from the steel mill. The ends of the tubes are then upset to increase the cross sectional area of the ends. The tube end may be externally upset (EU), internally upset (IU), or internally and externally upset (IEU). Standard max upset dimensions are specified in API 5DP, but the exact dimensions of the upset are proprietary to the manufacturer. After upsetting, the tube then goes through a heat treating process. Drill pipe steel is commonly quenched and tempered to achieve high yield strengths (135 ksi is a common tube yield strength).\n\nThe tool joints (connectors) are also received by the manufacturer as green tubes. After a quench and temper heat treat, the tool joints are cut into box (female) and pin (male) threads. Tool joints are commonly 120 ksi SMYS, rather than the 135 ksi of the tube. They generally are stiffer than the tube, increasing the likelihood of a fatigue failure at the junction. The lower SMYS on the connection increases the fatigue resistance. Higher strength steels are typically harder and more brittle, making them more susceptible to cracking and subsequent stress crack propagation.\n\nTubes and tool joints are welded using rotary inertia or direct drive friction welding. The tube is held stationary while the tool joint is revolved at high RPMs. The tool joint is the firmly pressed onto the upset end of the tube while the tool joint is rotating. The heat and force during this interaction weld the two together. Once the \"ram horns\" or excess material is removed, the weld line can only be seen under a microscope. Inertia friction welding is the traditional proven method. Direct drive friction welding is controlled and monitored up to 1000 times a second, resulting in a fine quality weld that does not necessary need a full heat treat quench and temper regime.\n"}
{"id": "6547678", "url": "https://en.wikipedia.org/wiki?curid=6547678", "title": "Dual control theory", "text": "Dual control theory\n\nDual control theory is a branch of control theory that deals with the control of systems whose characteristics are initially unknown. It is called \"dual\" because in controlling such a system the controller's objectives are twofold: \nThese two objectives may be partly in conflict.\n\nDual control theory was developed by Alexander Aronovich Fel'dbaum in 1960. He showed that in principle the optimal solution can be found by dynamic programming, but this is often impractical; as a result a number of methods for designing sub-optimal dual controllers have been devised.\n\nTo use an analogy: if you are driving a new car you want to get to your destination cheaply and smoothly, but you also want to see how well the car accelerates, brakes and steers so as to get a better feel for how to drive it, so you will do some test manoeuvers for this purpose. Similarly a dual controller will inject a so-called probing (or exploration) signal into the system that may detract from short-term performance but will improve control in the future.\n\n"}
{"id": "2954809", "url": "https://en.wikipedia.org/wiki?curid=2954809", "title": "Ear tag", "text": "Ear tag\n\nAn ear tag is a plastic or metal object used for identification of domestic livestock and other animals. If the ear tag uses Radio Frequency Identification Device (RFID) technology it is referred to as an electronic ear tag. Electronic ear tags conform to international standards ISO 11784 and ISO 11785 working at 134.2 kHz, as well as ISO/IEC 18000-6C operating in the UHF spectrum. There are other non-standard systems such as Destron working at 125 kHz. Although there are many shapes of ear tags, the main types in current use are as follows:\n\nEach of these except the metal type may carry a RFID chip, which normally carries an electronic version of the same identification number.\n\nAn ear tag usually carries an Animal Identification Number (AIN) or code for the animal, or for its herd or flock. Non electronic ear tags may be simply handwritten for the convenience of the farmer (these are known as \"management tags\"). Alternatively this identification number (ID) may be assigned by an organisation, such as the Meat and Livestock Association (MLA), which is a not-for-profit organisation owned by cattle, sheep and goat producers; funded by a levy on livestock sales with Federal Government input. Electronic tags may also show other information about the animal, including other related identification numbers; such as the Property Identification Code (PIC) for the properties the animals have been located. In the case of MLA's NLIS the movement of certain species of livestock (primarily cattle, goats and sheep) must be recorded in the online database within 24 hours of the movement; and include the PICs of the properties the animals are travelling between. The National Livestock Identification System (NLIS) of Australia regulations require that all cattle be fitted with a RFID device in the form of an ear tag or rumen bolus (a cylindrical object placed in the rumen) before movement from the property and that the movement be reported to the NLIS. However, if animals are tagged for internal purposes in a herd or farm, IDs need not be unique in larger scales. The NLIS now also requires sheep and goats to use an ear tag that has the Property Identification Code inscribed on it. These ear tags and boluses are complemented by transport documents supplied by vendors that are used for identification and tracking. A similar system is used for cattle in the European Union (EU), each bovine animal having a passport document and tag in each ear carrying the same number (see British Cattle Movement Service). Sheep and goats in the EU have a tag in one or both ears carrying the official number of their flock and also for breeding stock an individual number for each animal; one of these tags (usually the left) must have a RFID chip (or the chip may instead be carried in a rumen bolus or on an anklet).\n\nAn ear tag can be applied with an ear tag applicator, however there are also specially-designed tags that can be applied by hand. Depending on the purpose of the tagging, an animal may be tagged on one ear or both. There may be requirements for the placement of ear tags, and care must be taken to ensure they are not placed too close to the edge of the ear pinnae; which may leave the tag vulnerable to being ripped out accidentally. If there exists a national animal identification programme in a country, animals may be tagged on both ears for the sake of increased security and effectiveness, or as a legal requirement. If animals are tagged for private purposes, usually one ear is tagged. Australian sheep and goats are required to have visually readable ear tags printed with a Property Identification Code (PIC). They are complemented by movement documents supplied by consignors that are used for identification and tracking.\n\nVery small ear tags are available for laboratory animals such as mice and rats. They are usually sold with a device that pierces the animal's ear and installs the tag at the same time. Lab animals can also be identified by other methods such as ear punching or marking (also used for livestock; see below), implanted RFID tags (mice are too small to wear an ear tag containing an RFID chip), and dye.\n\nLivestock ear tags were developed in 1799 under the direction of Sir Joseph Banks, President of the Royal Society, for identification of Merino sheep in the flock established for King George III. Matthew Boulton designed and produced the first batch of sheep eartags, and produced subsequent batches, modified according to suggestions received from Banks. The first tags were made of tin.\n\nEar tags were incorporated as breed identification in the United States with the forming of the International Ohio Improved Chester Association as early as 1895, and stipulated in the Articles of Incorporation, as an association animal and breed identification, of the improved Chester White.\n\nAlthough ear tags were developed in Canada as early as 1913 as a means to identify cattle when testing for tuberculosis, the significant increase of use of ear tags appeared with the outbreak of BSE in UK. Today, ear tags in a variety of designs are used throughout the world on many species of animal to ensure traceability, to help prevent theft and to control disease outbreaks.\n\nThe first ear tags were primarily steel with nickel plating. After World War II, larger, flag-like, plastic tags were developed in the United States. Designed to be visible from a distance, these were applied by cutting a slit in the ear and slipping the arrow-shaped head of the tag through it so that the flag would hang from the ear.\n\nIn 1953, the first two-piece, self-piercing plastic ear tag was developed and patented. This tag, which combined the easy application of metal tags with the visibility and colour options of plastic tags, also limited the transfer of blood-borne diseases between animals during the application process.\n\nSome cattle ear tags contain chemicals to repel insects, such as buffalo flies, horseflies, etc. Metal ear tags are used to identify the date of regulation shearing of stud and show sheep. Today, a large number of manufacturers are in competition for the identification of world livestock population .\n\nIn 2004, the U.S. Government asked farmers to use EID or Electronic Identification ear tags on all their cattle. This request was part of the National Animal Identification System (NAIS) spurred by the discovery of the first case of mad cow disease in the United States. Due to poor performance and concern that other people could access their confidential information, only about 30 percent of cattle producers in the United States tried using EID tags using standards based on the low frequency standards, while the UHF standards are being mandated for use in Brazil, Paraguay, and Korea . The United States Department of Agriculture maintains a list of manufacturers approved to sell ear tags in the USA.\n\nEar tags (conventional and electronic) are used in the EU as official ID system for cattle, sheep and goat, in some cases combined with RFID devices\n\nThe International Committee for Animal Recording (ICAR) controls the issue electronic tag numbers under ISO regulation 11784.\n\nThe National Livestock Identification System (NLIS) is Australia's system for tracing cattle, sheep and goats from birth to slaughter.\n\nIn Canada, the Health of Animals Regulations require approved ear tags on all bison, cattle and sheep that leave the farm of origin, except that a bovine may be moved, without a tag, from the farm of origin to a tagging site. RFID (radio frequency identification) tags are used for cattle in Canada and metal as well as RFID tags have been in use for sheep. Mandatory RFID tagging of sheep in Canada (which was previously scheduled to take effect January 1, 2013) will be deferred to some later date.\n\nPigs, cattle and sheep are frequently earmarked with pliers that notch registered owner and/or age marks into the ear. Mares on large horse breeding farms have a plastic tag attached to a neck strap for identification; which preserves their ears free of notches. Dairy cows are sometimes identified with ratchet fastened plastic anklets fitted on the pastern for ready inspection during milking; however NLIS requirements apply to cattle - including both dairy and beef animals. More commonly coloured electrical tape is used as short term ankle identifiers for dairy animals to identify when one teat should not be milked for any reason. Laboratory rodents are often marked with ear tags, ear notches or implantable microchips.\n\nThe National Livestock Identification System (NLIS) Australia, formerly used cattle tail tags for property identification and hormone usage declaration.\n\n\n"}
{"id": "56759819", "url": "https://en.wikipedia.org/wiki?curid=56759819", "title": "Ecomom", "text": "Ecomom\n\nEcomom, founded by Jody Sherman in 2007, is a failed (and reborn) E-commerce startup company offering environmental caring, safe and healthy products for mothers, babies and children. Its head offices are located in Las Vegas and San Francisco. Its sales exceeded $1 million by 2011 and it raised $12 million before January 2013.\n\nEcomom shut down in early 2013, and the founder and CEO, Jody Sherman, became a suicide shortly afterward.\n\nGreenCupboards, an E-commerce company selling earth caring products in terms of homes and businesses, then acquired Ecomom and redirected its company name at etailz, Inc.. The website of Ecomom reopened in the early summer of 2013.\n\nVenture companies like Ecomom collect micro-finance from angel investors. One defect of this financing mode is that the individual investment of personal investors has little effect on the company. Therefore, individuals don't have much motivation to participate in the company affairs and lead it to success.\n\nEcomom adopted a deep discounting policy which offered customers fifty-percent discount. As a result, Ecomom lost money every time it sold products because half the price was lower than the cost price. In addition, Ecomom did not make any limitation to the discount and therefore repeat customers were offered the 50% discount when shopping. Ecomom spent excessively on marketing its products, for example the expenditure for search engine marketing was not proportional to the return of investment. Ecomom did not understand that the abuse of the offered discount would have a negative impact on the company when developing new audiences and markets.\n"}
{"id": "31835614", "url": "https://en.wikipedia.org/wiki?curid=31835614", "title": "Floating liquefied natural gas", "text": "Floating liquefied natural gas\n\nFloating liquefied natural gas (FLNG) refers to water-based liquefied natural gas (LNG) operations employing technologies designed to enable the development of offshore natural gas resources. Floating above an offshore natural gas field, the FLNG facility produces, liquefies, stores and transfers LNG (and potentially LPG and condensate) at sea before carriers ship it directly to markets. The world's first completed FLNG production facility is the PFLNG Satu located in Kanowit gas field off the shore of Sarawak in Malaysia. Petronas is the owner of the platform and first cargo was loaded onto the 150,200-cbm Seri Camellia LNG carrier on 03 April 2017. Multiple other FLNG facilities are in development. Another FLNG facility, developed by Exmar NV using Black & Veatch PRICO(R) technology, passed performance test in October 2016 in Nantong, China. Fortuna FLNG, to be commissioned in 2020, owned by a joint-venture between Ophir Energy and Golar LNG is still under development in Equatorial Guinea, the US$2 billion vessel would be the first to produce its gas in Africa. The agreement between Equatorial Guinea and state-owned GEPetrol, Ophir and OneLNG reconfirms GEPetrol’s participation rights as partners in 20 per cent of the FLNG project.\n\nStudies into offshore LNG production have been conducted since the early 1970s, but it was only in the mid-1990s that significant research backed by experimental development began.\n\nIn 1997, Mobil developed a FLNG production concept based on a large, square structure () with a moonpool in the center,\ncommonly known as \"The Doughnut\".\nThe Mobil proposal was sized to produce LNG per year produced from per year of feed gas, with storage provided on the structure for of LNG and of condensate.\n\nIn 1999, a major study was commissioned as a joint project by Chevron Corporation and several other oil and gas companies. This was closely followed by the so-called 'Azure' research project, conducted by the EU and several oil and gas companies. Both projects made great progress in steel concrete hull design, topside development and LNG transfer systems.\n\nSince the mid-1990s Royal Dutch Shell has been working on its own FLNG technology. This includes engineering and the optimization of its concept related to specific potential project developments in Namibia, Timor Leste/Australia, and Nigeria.\n\nIn July 2009, Royal Dutch Shell signed an agreement with Technip and Samsung allowing for the design, construction and installation of multiple Shell FLNG facilities.\n\nPetrobras invited three consortiums to submit proposals for engineering, procurement and construction contracts for FLNG plants in ultra-deep Santos Basin waters during 2009. A final investment decision was expected in 2011.\n\n, Japan's Inpex planned to leverage FLNG to develop the Abadi gas field in the Masela block of the Timor Sea, with a final investment decision expected by the end of 2013. Late in 2010, Inpex deferred start-up by two years to 2018 and cut its 'first phase' capacity to 2.5 million tons per year (from a previously proposed capacity of 4.5 million tonnes).\n\n, Chevron Corporation was considering an FLNG facility to develop offshore discoveries in the Exmouth Plateau of Western Australia, while in 2011, ExxonMobil was waiting for an appropriate project to launch its FLNG development.\n\nAccording to a presentation given by their engineers at GASTECH 2011, ConocoPhillips aimed to implement a facility by 2016-19, and had completed the quantitative risk analysis of a design that would undergo pre-FEED study during the remainder of 2011.\n\nIn June 2014, GDF Suez and Santos Limited made a decision to halt development on an Australia offshore gas field project that had proposed to use floating LNG platform technology. A part of the decision included the perception that long-term capabilities of North American gas fields due to hydraulic fracturing technologies and increasing Russian export capabilities may adversely affect the profitability of the venture due to competition.\n\nA number of major gas and oil companies are still researching and considering FLNG developments, with several initiatives planned for the future. However, the world's first development of FLNG is Shell's Au$12bn \"Prelude\" FLNG project, offshore Western Australia. Royal Dutch Shell announced their investment in FLNG on 20 May 2011 and construction began in October 2012.\n\nIn April 2010 Shell's FLNG technology was selected as the Sunrise Joint Venture’s preferred option for developing the Greater Sunrise gas fields in the Timor Sea. This followed an extensive and rigorous concept-evaluation process during which the merits of the project were weighed up against alternative onshore solutions. The Woodside-operated JV is now seeking to engage regulators on the concept selection process. Following the decision by Shell to go ahead with its Prelude FLNG development, the Sunrise project would be the second deployment of Shell’s proprietary FLNG design.\nThe Shell project is scheduled to begin processing gas in 2016.\n\nIn February 2011, Petronas awarded a FEED contract for an FLNG unit to a consortium of Technip and Daewoo Shipbuilding & Marine Engineering. The facility will be located in Malaysia, although the specific gas field is unknown.\n\nPetronas first FLNG, the \"PFLNG SATU\" produced and delivered its first LNG cargo from the \"Kanowit\" gas field offshore Bintulu, Sarawak, Malaysia on 1 April 2017. The first cargo was fully loaded onto the LNG carrier Seri Camellia and headed for the Asia market.\n\nIn March 2011, Petronas awarded a FEED contract for another FLNG unit called \"PFLNG2\" to consortium of JGC Corporation. and Samsung Heavy Industry. This Facility will be placed next to PFLNG1 describe above.\n\nGDF Suez Bonaparte – a joint venture undertaken by the Australian oil and gas exploration company Santos (40%) and the French multi-international energy company GDF Suez (60%) – has awarded a pre-FEED contract for the Bonaparte FLNG project offshore Northern Australia. The final investment decision is expected in 2014, with startup planned for 2018. The first phase of the project calls for a floating LNG production facility with a capacity of 2 million mt/year.\n\nIn October 2016, Exmar NV performance tested a facility design by Black & Veatch using the proprietary PRICO(R) process. The facility has a single liquefaction train that can produce 72 million cubic feet a day of LNG.\n\nIn June 4th, 2018, Golar LNG announced that their FLNG Hilli Episeyo had got a customer acceptance after successfully being tested in 16 days commissioning. FLNG Hilli Episeyo will serve Parenco Cameroon SA in Cameroon's water. FLNG Hilli Episeyo uses PRICO(R) technology from Black & Veatch and was built in Keppel Shipyard in Singapore.\n\nIn 2020, Fortuna FLNG owned by a joint venture between Ophir Energy and Golar LNG is expected to come on stream producing around 2.2 mmtpa of gas in Equatorial Guinea. The LNG facility will become the first FLNG to operate in Africa.\n\nMoving LNG production to an offshore setting presents a demanding set of challenges. In terms of the design and construction of the FLNG facility, every element of a conventional LNG facility needs to fit into an area roughly one quarter the size, whilst maintaining appropriate levels of safety and giving increased flexibility to LNG production.\n\nOnce a facility is in operation, wave motion will present another major challenge. LNG containment systems need to be capable of withstanding the damage that can occur when the sea’s wave and current motions cause sloshing in the partly filled tanks. Product transfers also need to deal with the effects of winds, waves and currents in the open seas.\n\nSolutions to reduce the effect of motion and weather are addressed in the design, which must be capable of withstanding – and even reducing – the impact of waves. In this area, technological development has been mainly evolutionary rather than revolutionary, leveraging and adapting technologies that are currently applied to offshore oil production or onshore liquefaction. For example, traditional LNG loading arms have been adapted to enable LNG transfers in open water, and hose-based solutions for both side-by-side transfers in calmer seas and tandem transfers in rougher conditions are nearing fruition.\n\nAmong fossil fuels, natural gas is relatively clean burning. It is also abundant and affordable and may be able to meet world energy needs by realising the potential of otherwise unviable gas reserves (several of which can be found offshore North West Australia). FLNG technology also provides a number of environmental and economic advantages:\n\n\nThe FLNG facility will be moored directly above the natural gas field. It will route gas from the field to the facility via risers. When the gas reaches the facility, it will be processed to produce natural gas, LPG, and natural gas condensate. The processed feed gas will be treated to remove impurities, and liquefied through freezing, before being stored in the hull. Ocean-going carriers will offload the LNG, as well as the other liquid by-products, for delivery to markets worldwide. The conventional alternative to this would be to pump gas through pipelines to a shore-based facility for liquefaction, before transferring the gas for delivery.\n\nIn the case of Shell’s Prelude FLNG, engineers have managed to fit every component of an LNG plant into an area roughly one quarter the size of a conventional onshore plant. Even so, Shell's facility will be the largest floating offshore facility ever built: It will measure around 488m long and 74m wide, and when fully ballasted will weigh 600,000 tonnes (roughly six times as much as the USS Nimitz aircraft carrier).\n\nThe specifications make Shell's FLNG facility particularly well-suited for fields with high production rates for reserves starting at and beyond – less than a tenth the size of the Groningen gas field in the Netherlands.\n\nThe First FLNG, Shell 'Prelude' FLNG facility will be constructed at Samsung's Geoje Island shipyard in Korea. Some modules may be constructed elsewhere and then transferred to the shipyard for assembly. The facility can remain on station for more than 25 years, and its lifetime can be further extended through overhaul and refurbishment. The hull has a design life of 50 years.\n\nPetronas two FLNG facility will be constructed at different place, Samsung and Daewoo.\n\nA unique feature of Shell’s FLNG design is its ability to stay safely moored in harsh weather conditions, including category five cyclones. Potentially, this could result in more uptime for the facility.\n\nAdditionally, Shell designers have optimised safety on the facility by locating storage facilities and process equipment as far from crew accommodation as possible. As a result of this, the accommodation areas of visiting LNG carriers are also at maximum distance from critical safety equipment. Safety gaps have been allowed between modules of process equipment so that gas can disperse quickly in the event of a gas leak.\n"}
{"id": "570838", "url": "https://en.wikipedia.org/wiki?curid=570838", "title": "Gateway, Inc.", "text": "Gateway, Inc.\n\nGateway Inc., previously Gateway 2000, was an American computer hardware company based in South Dakota and later California, that developed, manufactured, supported, and marketed a wide range of personal computers, computer monitors, servers, and computer accessories. \n\nIt was acquired by Acer in October 2007.\n\nGateway was founded on September 5, 1985, on a farm outside Sioux City, Iowa, by Ted Waitt and Mike Hammond. Originally called Gateway 2000, it was one of the first widely successful direct-sales PC companies, utilizing a sales model copied from Dell, and playing up its Iowa roots with low-tech advertisements proclaiming \"Computers from Iowa?\" \n\nGateway 2000 was also an innovator in low-end computers with the first sub-$1,000 name-brand PC, the all-in-one Astro.\n\nGateway built brand recognition in part by shipping computers in spotted boxes patterned after Holstein cow markings. In 1989, Gateway moved its corporate offices and production facilities to North Sioux City, South Dakota. In line with the Holstein cow mascot, Gateway opened a chain of \"farm-styled\" retail stores called Gateway Country Stores, mostly in suburban areas across the United States. It dropped the \"2000\" from its name on October 31, 1998.\n\nAOL acquired Gateway.net, the online component of Gateway, Inc., in October 1999 for US$800 million.\n\nTo grow beyond its model of selling high-end PCs by phone, and to attract top management and engineers, Gateway relocated its base of operations to La Jolla, California, in May 1998. In an effort to cut operating costs, Gateway made another move, this time to Poway, California, in October 2001. After acquiring eMachines in 2004, Gateway again relocated its corporate headquarters, to Irvine, California.\n\nIn 2003, the Securities and Exchange Commission filed fraud charges against three former Gateway executives: CEO Jeff Weitzen, former chief financial officer John Todd, and former controller Robert Manza. The lawsuit alleged that the executives engaged in securities violations and misled investors about the health of the company. Weitzen was cleared of securities fraud in 2006, however, Todd and Manza were found liable for inflating revenue in a jury trial which concluded in March 2007.\n\nIn 2002, Gateway expanded into the consumer electronics world with products that included plasma screen TVs, digital cameras, DLP projectors, wireless Internet routers, and MP3 players. While the company enjoyed some success in gaining substantial market share from traditional leaders in the space, particularly with plasma TVs and digital cameras, the limited short-term profit potential of those product lines led then-CEO Wayne Inouye to pull the company out of that segment during 2004. Gateway still acts as a retailer selling third-party electronic goods online.\n\nGateway resourced customer support within North America, priding itself as \"100% North America-based support\". Gateway also moved build-to-order desktop, laptop, and server manufacturing back to the United States, with the opening of its Gateway Configuration Center in Nashville, Tennessee, in September 2006. It employed 385 people in that location. By April 2007, Gateway notebook computers were produced in China and its desktops had \"made in Mexico\" stickers.\n\nOn October 16, 2007, Acer completed its acquisition of Gateway.\n\nIn September 2002, Gateway entered the consumer electronics market with aggressively priced plasma TVs. At the time, Gateway's US$2,999 price for a 42\" plasma TV undercut name brand competitors by thousands of dollars per unit. In 2003, the company expanded the range of plasma TVs and added digital cameras, MP3 players, and other devices. By early 2004, in terms of volume, Gateway had moved into a leadership position in the plasma TV category in the United States. However, pressure to achieve profits after the acquisition of eMachines led the company to phase Gateway-branded consumer electronics out of their product line.\n\n\n"}
{"id": "44030009", "url": "https://en.wikipedia.org/wiki?curid=44030009", "title": "Generalized Wiener filter", "text": "Generalized Wiener filter\n\nThe Wiener filter as originally proposed by Norbert Wiener is a signal processing filter which uses knowledge of the statistical properties of both the signal and the noise to reconstruct an optimal estimate of the signal from a noisy one-dimensional time-ordered data stream. The generalized Wiener filter generalizes the same idea beyond the domain of one-dimensional time-ordered signal processing, with two-dimensional image processing being the most common application.\n\nConsider a data vector formula_1 which is the sum of independent signal and noise vectors formula_2 with zero mean and covariances formula_3 and formula_4. The generalized Wiener Filter is the linear operator formula_5 which minimizes the expected residual between the estimated signal and the true signal, formula_6. The formula_5 that minimizes this is formula_8, resulting in the Wiener estimator formula_9. In the case of Gaussian distributed signal and noise, this estimator is also the maximum a posteriori estimator.\n\nThe generalized Wiener filter approaches 1 for signal-dominated parts of the data, and S/N for noise-dominated parts.\n\nAn often-seen variant expresses the filter in terms of inverse covariances. This is mathematically equivalent, but avoids excessive loss of numerical precision in the presence of high-variance modes. In this formulation, the generalized Wiener filter becomes formula_10 using the identity formula_11.\n\nThe cosmic microwave background (CMB) is a homogeneous and isotropic random field, and its covariance is therefore diagonal in a spherical harmonics basis. Any given observation of the CMB will be noisy, with the noise typically having different statistical properties than the CMB. It could for example be uncorrelated in pixel space. The generalized Wiener filter exploits this difference in behavior to isolate as much as possible of the signal from the noise.\nThe Wiener-filtered estimate of the signal (the CMB in this case) formula_9 requires the inversion of the usually huge matrix formula_13. If S and N were diagonal in the same basis this would be trivial, but often, as here, that isn't the case. The solution must in these cases be found by solving the equivalent equation formula_14, for example via conjugate gradients iteration. In this case all the multiplications can be performed in the appropriate basis for each matrix, avoiding the need to store or invert more than their diagonal. The result can be seen in the figure.\n\n"}
{"id": "1053191", "url": "https://en.wikipedia.org/wiki?curid=1053191", "title": "H3 (pyrotechnics)", "text": "H3 (pyrotechnics)\n\nH3 is a pyrotechnic composition which is used mostly as a burst charge for small diameter shells. It is friction and shock sensitive, as are most compositions containing chlorates. For this reason, H3 should be mixed using the \"diaper method\" and not with a ball mill. The composition consists of:\n\n\nDue to the potassium chlorate, H3 should not be mixed with sulfur or compositions containing sulfur, as sulfur increases the sensitivity of the mixture.\n\n"}
{"id": "46360364", "url": "https://en.wikipedia.org/wiki?curid=46360364", "title": "H5P", "text": "H5P\n\nH5P is a free and open-source content collaboration framework based on JavaScript. H5P is an abbreviation for HTML5 Package, and aims to make it easy for everyone to create, share and reuse interactive HTML5 content. Interactive videos, interactive presentations, quizzes, interactive timelines and more have been developed and shared using H5P on H5P.org. H5P is being used by 17 000+ websites. In June 2017 the core team announced that H5P will be supported financially by the Mozilla Foundation within the MOSS program.\n\nThe framework consists of a web based content editor, a web site for sharing content types, plugins for existing CMS's and a file format for bundling together HTML5 resources.\n\nThe web based editor is by default able to add and replace multimedia files and textual content in all kinds of H5P content types and applications. In addition a content type may provide custom widgets for the editor enabling any kind of editing capabilities and experiences including wysiwyg editing of the entire content type.\n\nH5P.org is the community website where H5P libraries, applications and content types may be shared. H5P applications and content types work the same way in all H5P compatible websites.\n\nCurrently four platform integrations exist, one for Drupal, WordPress., Tiki, and one for Moodle. The platform integrations include the generic H5P code as well as interface implementations and platform specific code needed to integrate H5P with the platforms. H5P has been designed to have a minimum of platform specific code and a minimum of backend code. Most of the code is JavaScript. The aim is to make it easy to integrate H5P with new platforms.\n\nThe file format consists of a metadata file in JSON format, a number of library files providing features and design for the content and a content folder where textual content is stored in JSON format and multimedia is stored as files or links to files on external sites.\n\nH5P's primary support website is H5P.org. Here, H5P may be tried out; it hosts the online manual for H5P and a living repository for H5P information, documentation and forums.\n"}
{"id": "1686404", "url": "https://en.wikipedia.org/wiki?curid=1686404", "title": "Integrated nanoliter system", "text": "Integrated nanoliter system\n\nThe integrated nanoliter system is a measuring, separating, and mixing device that is able to measure fluids to the nanoliter, mix different fluids for a specific product, and separate a solution into simpler solutions.\n\nAll features of the integrated nanoliter system are specifically designed for controlling very small volume of liquid (referred as microfluidic solutions). The integrated nanoliter system's scalability depends on what type of processing method the system is based on (refer as technology platform) with each processing method having its advantages and disadvantages. Possible uses for the integrated nanoliter system are in controlling biological fluids (refer as synthetic biology) and accurately detecting changes in cells for genetic purposes (such as single-cell gene expression analysis) where the smaller scale directly influences the result and accuracy.\n\nThe integrated nanoliter system consists of microfabricated fluidic channels, heaters, temperature sensors, and fluorescence detectors. The microfabricated fluidic channels (basically very small pipes) act as the main transportation structures for any fluids as well as where reactions occur within the system. For the desired reactions to occur, the temperature needs to be adjusted. Therefore, heaters are attached to some microfabricated fluidic channels. To monitor and maintain the desired temperature, temperature sensors are crucial for successful and desired reactions. In order to accurately track the fluids before and after a reaction, fluorescence detectors are used for detecting the movements of the fluids within the system. For instance, when a specific fluid passes a certain point where it triggers or excites emission of light, the fluorescence detector is able to receive that emission and calculate the time it takes to reach that certain point.\n\nThere are three different technology platforms for the integrated nanoliter system's scalability. Therefore, the main processing method of the integrated nanoliter system varies from the type of technology platform it is using. The three technology platforms for scalability are electrokinetic manipulation, vesicle encapsulation, and mechanical valving.\n\nThe main processing method for controlling the fluid under this technology platform is the capillary electrophoresis, which is an electrokinetic phenomena. Capillary electrophoresis is a great method for controlling fluids because the charged particles of the fluid are being directed by the controllable electric field within the system. However, a disadvantage of the technique is that the method of controlling the fluid's particles heavily depends on the particles' original charges. Another disadvantage is that the possible fluid \"leaks\" within the system. These \"leaks\" occur through diffusion which are dependent on the size of the fluid's particles.\n\nThe main processing method for controlling the fluid under this technology platform is to confine the fluids of interest in carrier molecules, which are generally droplets of water, vesicles, or micelles. The carrier molecules (with the fluid within them) are controlled by individually directing each carrier molecules within the microfabricated fluidic channels. This method is great for solving the possible fluid \"leaks\", since confinement of the fluid in a carrier molecule does not depend on the size of the fluid's particles. However, this technique has a disadvantage on how complex the solution can be when using the system.\n\nThe main processing method for controlling the fluid under this technology platform is the use of small mechanical valves. Mechanical valving is similar to a complex plumbing system because the microfabricated fluidic channels act as the plumbing pipes while the various controllable valves direct the fluid. Mechanical valving is also considered to be the most robust solution to the disadvantages of the electrokinetic manipulation and vesicle encapsulation, since the mechanical valves operate completely independent from the fluid's physical and chemical properties. Because the physical properties that make up the microfabricated fluidic channels and mechanical valves are difficult to process due to the system's extremely small scale, this technique has a disadvantage of creating an integrated nanoliter system with mechanical valving to the nanoliter scale.\n\nA possible use of the integrated nanoliter system is in synthetic biology (controlling biological fluids). Since the integrated nanoliter system is generally made up of many controllable microfabricated fluidic networks, integrated nanoliter systems are an ideal environment for controlling biological fluids. A common process of synthetic biology that uses the integrated nanoliter system is processing complex reactions among biological fluids, which usually involves separating a biological solution into individual pure or simpler reagent solutions then mixing the individual solutions for the desired product. An advantage of using the integrated nanoliter system in synthetic biology includes the extremely small length of the microfluidic networks that result in fast diffusion rates. Another advantage is the fast mixing rates due to the combination of diffusion and advection (chaotic mixing). Compared to previous microfluidic systems, another advantage is the smaller necessary amount of reagent solutions for a single operation due to the integrated nanoliter system's microscopic scalability. Smaller necessary amounts of reagent solutions tend to lead to more operations that can be carried out with less delay from gathering or reproducing the necessary amounts of reagent solutions.\n\nAnother possible use of the integrated nanoliter system is in single-cell gene expression analysis. One benefit of using the integrated nanoliter system is its capability to detect the changes of a gene expression more accurately than the previous technique of microarray. The nanoliter system's microscopic scalability (nanoliter to picoliter scale) allows it to analyze the gene expression at the single-cell level (around 1 picoliter), while the microarray analyzes changes of the gene expression by averaging a large group of cells. Another convenient and important benefit is the integrated nanoliter system's capability of having all the necessary biological fluids in the system before operation by storing each biological fluid in a specific microfabricated fluidic network. The integrated nanoliter system is convenient because the biological fluids are all controlled by a computer compared to how previous systems required a manual loading of every biological fluid. The integrated nanoliter system is also important for the gene expression analysis because the analysis would not be undesirably influenced by contamination due to the \"closed\" system while in operation.\n"}
{"id": "40407137", "url": "https://en.wikipedia.org/wiki?curid=40407137", "title": "Lawrence H. Johnston", "text": "Lawrence H. Johnston\n\nLawrence Harding \"Larry\" Johnston (February 11, 1918 – December 4, 2011) was an American physicist, a young contributor to the Manhattan Project. He was the only man to witness all three atomic explosions in 1945: the Trinity nuclear test and the atomic bombing of Hiroshima and Nagasaki.\n\nDuring World War II, he worked at the MIT Radiation Laboratory where he invented ground-controlled approach radar. In 1944, he went to the Manhattan Project's Los Alamos Laboratory, where he co-invented the exploding-bridgewire detonator.\n\nAfter the war he completed his Ph.D. thesis in 1950, and became an associate professor at the University of Minnesota in Minneapolis. He later worked at the Stanford Linear Accelerator Center as head of the electronics department, and was a professor at the University of Idaho in Moscow, where he taught until his retirement.\n\nBorn in Shandong, China, on February 11, 1918, Johnston's parents were American Presbyterian missionaries. The family returned to the United States in 1923, and his father became a Presbyterian pastor in Santa Maria, California.\n\nAfter graduation from Hollywood High School in 1936, Johnston earned an associate degree at Los Angeles City College. He transferred to the University of California in Berkeley, where Luis Walter Alvarez was a graduate student. Johnston received his bachelor's degree in physics in 1940.\n\nJohnston intended to study for his doctorate under Alvarez, but instead followed him east to the MIT Radiation Laboratory near Boston in February 1941. Alvarez and Johnston worked together on ground-controlled approach radar. This allowed aircraft to be guided a safe landing in adverse weather conditions, based on radar images, and would later prove crucial during the Berlin Airlift. They were awarded US Patents 2,555,101 and 2,585,855 for it.\n\nWhile a graduate student at Berkeley, Johnston met Mildred (Millie) Hillis, a girl who shared his strong Christian faith. When Alvarez discovered how much Johnston missed Hillis, he arranged for Johnston to be flown back to Berkeley. Johnston and Hillis married, and returned to Boston together. She sometimes accompanied them on field trips to test the ground-controlled approach radar system. They had five children: Mary Virginia (Ginger), Margy, Dan, Lois, and Karen.\n\nIn 1944, Johnston followed Alvarez to the Manhattan Project's Los Alamos Laboratory, where Robert Oppenheimer, who was also from the University of California, was the director. Johnston became involved in the development of the Fat Man plutonium bomb. Because of the high level of spontaneous fission in reactor plutonium, it was decided to use a nearly critical sphere of the metal and compress it quickly into a much smaller and denser core using explosives, a technical challenge at the time.\n\nTo create the symmetrical implosion required to compress the plutonium to the required density, thirty-two explosive charges were simultaneously detonated around the spherical core. Using conventional explosive techniques with blasting caps, progress towards achieving simultaneity to within a small fraction of a microsecond was discouraging. Alvarez directed Johnston to use a large capacitor to deliver a high voltage charge directly to each explosive lens, replacing blasting caps with exploding-bridgewire detonators. This detonated all thirty-two charges within a few tenths of a microsecond. The invention was critical to the success of the implosion-type nuclear weapon. Johnston was awarded US Patent 3,040,660 for the exploding-bridgewire detonator.\n\nJohnston and Alvarez's next task for the Manhattan Project was to develop a set of calibrated microphone/transmitters to be parachuted from an aircraft to measure the strength of the blast wave from the atomic explosion, so as to allow the scientists to calculate the bomb's energy. He observed the Trinity nuclear test from a B-29 Superfortress that also carried fellow Project Alberta members Harold Agnew and Deak Parsons.\n\nFlying in the B-29 Superfortress \"The Great Artiste\" in formation with the \"Enola Gay\", Alvarez and Johnston measured the blast effect of the Little Boy bomb which was dropped on Hiroshima. A few days later, again flying in \"The Great Artiste\", Johnston used the same equipment to measure the strength of the Nagasaki explosion. He was the only person to witness the Trinity test and the bombings of both Hiroshima and Nagasaki.\n\nJohnston never regretted the part he had played in the bombings. Years later, Johnston recalled:\n\nAfter the war, Johnston returned to graduate school at Berkeley. Under Alvarez's supervision, he wrote his PhD thesis at the Lawrence Berkeley Laboratory on the \"Development of the Alvarez-type proton linear accelerator\". After he graduated in 1950, he became an associate professor at the University of Minnesota. There, he built a 68 MeV proton linear accelerator, which he used to perform proton-proton scattering experiments. In 1964, he joined the Physics Laboratory of The Aerospace Corporation, where he learned techniques for investigating far infrared radiation.\n\nIn 1964, Johnston moved to the Stanford Linear Accelerator Center as head of the Electronics Department. He worked there on the construction of a , 20 GeV electron linear accelerator. He became a professor of physics at the University of Idaho in 1967, and focused on nuclear physics, far infrared lasers, and molecular spectroscopy. Johnston retired in 1988 at age 70, and continued to reside in Moscow as professor emeritus until his death.\n\nIn retirement, Johnston made a number of trips to Israel to work on biblical archaeology projects. He was a strong supporter of Christian ministries, and believed in intelligent design. Johnston died of lung cancer at age 93 in his home in Moscow on December 4, 2011. Married for 69 years, he was survived by his wife Millie and their five children.\n\n\n"}
{"id": "5331853", "url": "https://en.wikipedia.org/wiki?curid=5331853", "title": "List of films about computers", "text": "List of films about computers\n\nThis is a list of films about computers, featuring fictional films in which activities involving computers play a central role in the development of the plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "22767015", "url": "https://en.wikipedia.org/wiki?curid=22767015", "title": "Maritime hydraulics in antiquity", "text": "Maritime hydraulics in antiquity\n\nWith the issue of supplying water to a very large population, the Romans developed hydraulic systems for multiple applications: public water supply, power using water mills, hydraulic mining, fish tanks, irrigation, fire fighting, and of course aqueduct (Stein 2004). Scientists such as Ctesibius and Archimedes produced the first inventions involving hydraulic power (Oleson 1984). \n\nThe most prevalent hydraulic pump used in maritime situations in ancient Rome was the bilge pump, which functioned to siphon collected water out of a ship’s hull (Oleson 1984). The bilge pump was an improvement on the first hydraulic pumps used in antiquity: force pumps. Invented around the early 3rd century BCE, the most primitive design of a force pump consisted of a piston pushing water out of a tube, constructed by soldering individual bronze elements (Stein 246). \n\nWritten accounts from Phil of Byzantium (late 3rd century BCE), Vitruvius (c. 25 BCE), and Hero of Alexandria (c. CE 50) confirm archaeological evidence for the original simple design, and both Hero and Vitruvius specify that the pump was typically made of bronze (Stein 2004). Somewhere along the way a new design arose that involved cutting apertures into an individual wood block and then rendering the block pressure-proof with tightly fitting plugs and plates (Stein 2004). \n\nAlthough cheaper and easier to manufacture, assemble, and repair, the wooden pumps manufactured from then on would most likely not have been durable enough for maritime use, which is why we usually find lead pipes associated with bilge pumps, bilge wells, and hydraulic devices of other function about ships. The amphora shipwreck discovered at Grado in Gorizia, Italy, which dates to around 200 CE, and contained what archaeologists hypothesized to be a ‘hydraulics’ system to change the water in live fish tanks, since other evidence indicates the ship’s involvement in the processed fish trade (Beltrame and Gaddi 2005). \n\nThis hypothesis has been disputed, since the wooden box protecting a lead pipe along the longitudinal axis of the hull found on the wreck suggest the existence of a bilge well (Oleson and Stein 2007). Previous ships involved in live fish tank transport, entitled navis vivariae, did not employ a hydraulic system, and for this reason the hypothesis proves especially questionable (Boetto 124). For example, one of the wrecks found in Claudius’s harbor at the modern Fiumicino Airport in Italy was a small fish craft dating somewhere around the 2nd century CE. Archaeologists discovered a fish-well in the middle of the ship that used the hole/plump system bored in the bottom of the boat to fill the tank (Boetto 2006). Other than the Grado wreck no other evidence exists of hydraulic systems being used in the fish industry.\n\nBeltrame, C. and Gaddi, D. 2005. “The Rigging and the “Hydraulic System” of the Roman wreck at Grado, Gorizzia, Italy,” The International Journal of Nautical Archaeology 34: 79-87. \n\nBeltrame, C. and Gaddi, D. “Preliminary Analysis of the Hull of the Roman Ship from Grado, Gorizia, Italy,” IJNA36: 138-147. \n\nBoetto, G. 2006. “Roman techniques for transport and conservation of fish. The case of the Fiumicino 5 wreck in the light of some ethnographic comparisons,” in L. Blue and F. Hocker (eds), Conducted by the Sea, X International Symposium on Boat and Ship Archaeology, Roskilde 2003. \n\nOleson, J. P. 1984. Greek and Roman Mechanical Water lifting Devices: the History of a Technology. Toronto: Toronto University Press. \n\nOleson, J. P. and Stein R. 2007. “Comment on a Recent Article Concerning the Hydraulic System of the Roman Wreck at Grado, Gorizia, Italy,” IJNA 35 (2): 415-417. \n\nStein, R. J. B. 2004. “Roman Wooden Force Pumps: A case study in innovation.” Journal of Roman Archaeology 17 221-50.\n"}
{"id": "14902968", "url": "https://en.wikipedia.org/wiki?curid=14902968", "title": "Mark Delany", "text": "Mark Delany\n\nMark Delany is an Australian computer programmer and consultant specializing in e-mail infrastructure and anti-spam techniques.\n\nHe is the chief architect and inventor of DomainKeys, an e-mail authentication system designed to verify the DNS domain of an e-mail sender and the message integrity. Mark is one of the authors of DomainKeys Identified Mail, a development of DomainKeys.\n\nHe was also lead architect for Yahoo! Mail.\n"}
{"id": "54955703", "url": "https://en.wikipedia.org/wiki?curid=54955703", "title": "Masergy Communications", "text": "Masergy Communications\n\nMasergy Communications (, ) is a software defined networking services company founded in 2000 and headquartered in Plano, Texas. The company provides Secure Hybrid Networking, Managed Security, and Cloud Communications.\n\nMasergy's customers include Huisman, Pattonair, PRGX, Inc., Eurostar, Dolby Laboratories, and the Hallmark Channel.\n\nMasergy was acquired by Berkshire Partners in 2016 for an undisclosed amount, and had been previously acquired by ABRY Partners LLC, a private equity investment firm, for an undisclosed price.\n\nEarly on, Masergy was backed by venture investors. In 2003, Centennial Ventures and Meritage Funds jointly invested a total of $31 million in Masergy. And in 2005, WestAM led a group that invested $30 million in Masergy.\n\nChris MacFarland joined Masergy as its chief operating officer in 2008, and was named its CEO in 2010. In August 2011, Masergy was acquired by ABRY Partners LLC, a private equity investment firm, for an undisclosed price.\nMasergy acquired Broadcore Communications for an undisclosed purchase price in July 2012. Broadcore offered communications services, including video calling and call recording, for businesses. \n\nIn April 2014, Masergy acquired Global DataGuard, broadening its portfolio to include managed security services for enterprise customers. Its Unified Enterprise Security solution includes advanced persistent threat management and adaptive network behavioral analysis.\n\nIn September 2018, James Parker was named the new CEO.\n\nMasergy delivers Secure Hybrid Networking, cybersecurity and cloud communications to global enterprises for clients in industries such as manufacturing, healthcare, entertainment, finance, broadcasting, and more. The company provides voice, data, and video network services across the globe, specializing in Ethernet-based virtual private networks (VPNs) and wide area networks (WANs), Masergy's cloud services include network and application management, global cloud communications, and hosted remote access. The company's managed services focus on disaster recovery and security. \n\nIn 2017, Masergy added a new option to its SD-WAN service called SD-WAN Go, which includes embedded firewalls, zero-touch provisioning from the Masergy portal, and WAN optimization.\n\nMay 2013: Masergy’s CEO, Chris MacFarland, was named a finalist in Ernst & Young’s Enterpreneur of the Year Southwest Area North.\n\nJune 2015: Masergy’s CEO, Chris MacFarland, was awarded Ernst & Young’s Entrepreneur of the Year 2015 for the technology/telecom sector in the U.S. southwest.\n\n2015: Masergy ranked #4180 on Inc. magazine’s Inc. 5000 list for 2015. The magazine cited Masergy’s 67 percent growth rate over three years. This also marked the fifth time Masergy was included on the Inc. 5000 list.\n\n2017: Masergy’s CEO, Chris MacFarland, was named as one of the top 10 highest rated CEOs on Glassdoor.\n"}
{"id": "18554090", "url": "https://en.wikipedia.org/wiki?curid=18554090", "title": "Monastery of Mary, Mother of Grace", "text": "Monastery of Mary, Mother of Grace\n\nThe Monastery of Mary, Mother of Grace, is a convent maintained by the Discalced Carmelites of the Roman Catholic Church. Founded in 1936 by the New Orleans Carmel, the monastery is located in Lafayette, Louisiana within the Diocese of Lafayette.\n\nThe monastery covers that include an organic garden, fruit trees, and a variety of plants and shrubs that contribute to the monastery's peaceful and meditative environment.\n\nNuns at Mary, Mother of Grace, pray several times each day, and attend Mass daily. They observe strict silence except at certain times, such as during Mass, when the nuns sing hymns. When they are not praying, the nuns share work duties, from gardening to cleaning to answering correspondence from around the globe. The nuns do not leave the monastery except for special reasons.\n"}
{"id": "28654390", "url": "https://en.wikipedia.org/wiki?curid=28654390", "title": "Multi-function structure", "text": "Multi-function structure\n\nMulti-function material is a composite material. The traditional approach to the development of structures is to address the loadcarrying function and other functional requirements separately. Recently, however, there has been increased interest in the development of load-bearing materials and structures which have integral non-load-bearing functions, guided by recent discoveries about how multifunctional biological systems work.\n\nWith conventional structural materials, it has been difficult to achieve simultaneous improvement in multiple structural functions, but the increasing use of composite materials has been driven in part by the potential for such improvements. The multi-functions can vary from mechanical to electrical and thermal functions. The most widely used composites have polymer matrix materials, which are typically poor conductors. Enhanced conductivity could be achieved with reinforcing the composite with carbon nanotubes for instance.\n\nAmong the many functions that can be attained are Electrical/thermal conductivity, Sensing and actuation, Energy harvesting/storage, Self-healing capability, Electromagnetic interference (EMI) shielding and recyclability and biodegradability. See also Functionally graded materials (FGM) which are composite materials where the composition or the microstructure are locally varied so that a certain variation of the local material properties is achieved. However, FGM can be designed for specific function and applications.\n\nMany applications such as Re-configurable aircraft wings, shape-changing aerodynamic panels for flow control, Variable geometry engine exhausts, turbine blade, wind turbine configuration at different wind speed, Microelectromechanical systems (micro-switches), mechanical memory cells, valves, micropumps,Flexible direction panel position in solar cells,Innovative architecture (adaptive shape panels for roofs and windows), Flexible and foldable electronic devices and Optics (shape changing mirrors for active focusing in adaptive optical systems)\n"}
{"id": "2851518", "url": "https://en.wikipedia.org/wiki?curid=2851518", "title": "N. Katherine Hayles", "text": "N. Katherine Hayles\n\nN. Katherine Hayles (born 16 December 1943) is a postmodern literary critic, most notable for her contribution to the fields of literature and science, electronic literature, and American literature. She is professor and Director of Graduate Studies in the Program in Literature at Duke University.\n\nHayles was born in Saint Louis, Missouri to Edward and Thelma Bruns. She received her B.S. in Chemistry from the Rochester Institute of Technology in 1966, and her M.S. in Chemistry from the California Institute of Technology in 1969. She worked as a research chemist in 1966 at Xerox Corporation and as a chemical research consultant Beckman Instrument Company from 1968-1970. Hayles then switched fields and received her M.A. in English Literature from Michigan State University in 1970, and her Ph.D. in English Literature from the University of Rochester in 1977. She is a social and literary critic.\n\nHer scholarship primarily focuses on the \"relations between science, literature, and technology.\" Hayles has taught at UCLA, University of Iowa, University of Missouri–Rolla, the California Institute of Technology, and Dartmouth College. She was the faculty director of the Electronic Literature Organization from 2001-2006.\n\nHayles understands \"human\" and \"posthuman\" as constructions that emerge from historically specific understandings of technology, culture and embodiment; \"human and \"posthuman\" views each produce unique models of subjectivity. Within this framework \"human\" is aligned with Enlightenment notions of liberal humanism, including its emphasis on the \"natural self\" and the freedom of the individual. Conversely, posthuman does away with the notion of a \"natural\" self and emerges when human intelligence is conceptualized as being co-produced with intelligent machines. According to Hayles the posthuman view privileges information over materiality, considers consciousness as an epiphenomenon and imagines the body as a prosthesis for the mind . Specifically Hayles suggests that in the posthuman view \"there are no essential differences or absolute demarcations between bodily existence and computer simulation...\" The posthuman thus emerges as a deconstruction of the liberal humanist notion of \"human.\" Hayles disregards the idea of a form of immortality created through the preservation of human knowledge with computers, instead opting for a specification within the definition of posthuman that one embraces the possibilities of information technology without the imagined concepts of infinite power and immortality, tropes often associated with technology and dissociated with traditional humanity. This idea of the posthuman also ties in with cybernetics in the creation of the feedback loop that allows humans to interact with technology through a blackbox, linking the human and the machine as one. Thus, Hayles links this to an overall cultural perception of virtuality and a priority on information rather than materiality.\n\nDespite drawing out the differences between \"human\" and \"posthuman\", Hayles is careful to note that both perspectives engage in the erasure of embodiment from subjectivity. In the liberal humanist view, cognition takes precedence over the body, which is narrated as an object to possess and master. Meanwhile, popular conceptions of the cybernetic posthuman imagine the body as merely a container for information and code. Noting the alignment between these two perspectives, Hayles uses \"How We Became Posthuman\" to investigate the social and cultural processes and practices that led to the conceptualization of information as separate from the material that instantiates it. Drawing on diverse examples, such as Turing's Imitation Game, Gibson's \"Neuromancer\" and cybernetic theory, Hayles traces the history of what she calls \"the cultural perception that information and materiality are conceptually distinct and that information is in some sense more essential, more important and more fundamental than materiality.\" By tracing the emergence of such thinking, and by looking at the manner in which literary and scientific texts came to imagine, for example, the possibility of downloading human consciousness into a computer, Hayles attempts to trouble the information/material separation and in her words, \"...put back into the picture the flesh that continues to be erased in contemporary discussions about cybernetic subjects.” In this regard, the posthuman subject under the condition of virtuality is an \"amalgam, a collection of heterogeneous components, a material-informational entity whose boundaries undergo continuous construction and reconstruction.\" Hayles differentiates \"embodiment\" from the concept of \"the body\" because \"in contrast to the body, embodiment is contextual, enmeshed within the specifics of place, time, physiology, and culture, which together compose enactment.\" Hayles specifically examines how various science fiction novels portray a shift in the conception of information, particularly in the dialectics of presence/absence toward pattern/randomness. She diagrams these shifts to show how ideas about abstraction and information actually have a \"local habitation\" and are \"embodied\" within the narratives. Although ideas about \"information\" taken out of context creates abstractions about the human \"body\", reading science fiction situates these same ideas in \"embodied\" narrative.\"\n\nWithin the field of Posthuman Studies, Hayles' \"How We Became Posthuman\" is considered \"the key text which brought posthumanism to broad international attention\". In the years since this book was published, it has been both praised and critiqued by scholars who have viewed her work through a variety of lenses; including those of cybernetic history, feminism, postmodernism, cultural and literary criticism, and conversations in the popular press about humans' changing relationships to technology.\n\nReactions to Hayles' writing style, general organization, and scope of the book have been mixed. The book is generally praised for displaying depth and scope in its combining of scientific ideas and literary criticism. Linda Brigham of Kansas State University claims that Hayles manages to lead the text \"across diverse, historically contentious terrain by means of a carefully crafted and deliberate organizational structure.\" Some scholars found her prose difficult to read or over-complicated. Andrew Pickering describes the book as \"hard going\" and lacking of \"straightforward presentation.\" Dennis Weiss of York College of Pennsylvania accuses Hayles of \"unnecessarily complicat[ing] her framework for thinking about the body\", for example by using terms such as \"body\" and \"embodiment\" ambiguously. Weiss however acknowledges as convincing her use of science fiction in order to reveal how \"the narrowly focused, abstract constellation of ideas\" of cybernetics circulate through a broader cultural context. Craig Keating of Langara College on the contrary argues that the obscurity of some texts questions their ability to function as the conduit for scientific ideas.\n\nSeveral scholars reviewing \"How We Became Posthuman\" highlighted the strengths and shortcomings of her book vis a vis its relationship to feminism. Amelia Jones of University of Southern California describes Hayles' work as reacting to the misogynistic discourse of the field of cybernetics. As Pickering wrote, Hayles' promotion of an \"embodied posthumanism\" challenges cybernetics' \"equation of human-ness with disembodied information\" for being \"another male trick to feminists tired of the devaluation of women's bodily labor.\" Stephanie Turner of Purdue University also described Hayles' work as an opportunity to challenge prevailing concepts of the human subject which assumed the body was white, male, and European, but suggested Hayles' dialectic method may have taken too many interpretive risks, leaving some questions open about \"which interventions promise the best directions to take.\"\n\nReviewers were mixed about Hayles' construction of the posthuman subject. Weiss describes Hayles' work as challenging the simplistic dichotomy of human and post-human subjects in order to \"rethink the relationship between human beings and intelligent machines,\" however suggests that in her attempt to set her vision of the posthuman apart from the \"realist, objectivist epistemology characteristic of first-wave cybernetics\", she too, falls back on universalist discourse, premised this time on how cognitive science is able to reveal the \"true nature of the self.\" Jones similarly described Hayles' work as reacting to cybernetics' disembodiment of the human subject by swinging too far towards an insistence on a \"physical reality\" of the body apart from discourse. Jones argued that reality is rather \"determined in and through the way we view, articulate, and understand the world\".\n\nIn terms of the strength of Hayles' arguments regarding the return of materiality to information, several scholars expressed doubt on the validity of the provided grounds, notably evolutionary psychology. Keating claims that while Hayles is following evolutionary psychological arguments in order to argue for the overcoming of the disembodiment of knowledge, she provides \"no good reason to support this proposition.\" Brigham describes Hayles' attempt to connect autopoietic circularity to \"an inadequacy in Maturana's attempt to account for evolutionary change\" as unjustified. Weiss suggests that she makes the mistake of \"adhering too closely to the realist, objectivist discourse of the sciences,\" the same mistake she criticizes Weiner and Maturana for committing.\n\n\n\n\n\n\n\n\n"}
{"id": "33846270", "url": "https://en.wikipedia.org/wiki?curid=33846270", "title": "Oxide jacking", "text": "Oxide jacking\n\nThe expansive force of rusting, which may be called oxide jacking or rust burst, is a phenomenon that can cause damage to structures made of stone, masonry, concrete or ceramics, and reinforced with metal components. A definition is \"the displacement of building elements due to the expansion of iron and steel products as the metal rusts and becomes iron oxide\". Corrosion of other metals such as aluminum can also cause oxide jacking.\n\nAccording to metallurgist Jack Harris, \"Oxidation is usually accompanied by a net expansion so that when it occurs in a confined space stresses are generated in the metal component itself or in any surrounding medium such as stone or cement. So much energy is released by oxidation that the stresses generated are of sufficient magnitude to deform or fracture all known materials.\"\n\nAs early as 1915, it was recognized that certain modern metal alloys are more susceptible to excessive oxidation when subjected to weathering than other metals. At that time, there was a trend to replace wrought iron fasteners with mild steel equivalents, which were less expensive. Unexpectedly, the mild steel fasteners failed in real world use much more quickly than anticipated, leading to a return to use of wrought iron in certain applications where length of service was important.\n\nIn a 1987 article in \"New Scientist\", Jack Harris reported that oxide jacking has caused significant damage to many historic structures in the United Kingdom, including St Paul's Cathedral, the British Museum and the Albert Memorial in London, Gloucester Cathedral, St. Margaret's Church in King's Lynn, Winchester Cathedral, and Blackburn Cathedral.\n\nHarris also wrote that oxide jacking also damaged the ancient Horses of Saint Mark on the exterior of St. Mark's Basilica in Venice. Expansive rusting of iron and steel bolts and reinforcements affected the structural integrity of the copper horse sculptures, which were relocated indoors and replaced with replicas. Poorly-designed early 20th-century renovations also led to oxide jacking damage to the Acropolis of Athens.\n\nIn the United States, rusting of iron pegs inserted into holes in the stone entrance stair in order to support handrails resulted in cracking of the steps at the Basilica of the Sacred Heart in Notre Dame, Indiana.\n\nOxide jacking damaged the terra cotta cornice on the Land Title Building in Philadelphia, designed in 1897 and expanded in 1902 by pioneer skyscraper architect Daniel Burnham. The Land Title complex, with its two interconnected towers, is on the National Register of Historic Places. By 1922, experts on architectural terra cotta were warning that the rusting of embedded iron fasteners could cause decorative building components to fail. This 1902 cornice is nearly high, projects from the facade of the building and is long. The cornice was stabilized, steel anchors subject to rusting were replaced with new stainless steel anchors, and the cornice was completely renovated. The project was completed in 1991.\n\nFlooding in 2007 damaged the modernist Farnsworth House in Plano, Illinois, designed in 1945 by Ludwig Mies van der Rohe, and now owned by the National Trust for Historic Preservation. Among the damage discovered by an architect inspecting the house in 2007 was oxide jacking at the corners of the house's steel framework. The house flooded again in 2008.\n\nStructures built of concrete and reinforced with metal rebar are also subject to damage by oxide jacking. Expansion of corroded rebar causes spalling of the concrete. Structures exposed to a marine environment, or where salt is used for de-icing purposes, are especially susceptible to this type of damage.\n\nResearch in the 1960s showed that 22% of concrete bridge decks in Pennsylvania showed signs of spalling due to oxide jacking within four years of construction. Oxide jacking caused widespread damage to concrete council houses built in the United Kingdom in the post World War II era.\n\nAccording to an expert in the field, the problem resulted in \"intensive worldwide research into the causes and repair of reinforcement corrosion, which in turn led to a vast output of research papers, conferences and publications on the subject.\"\n\nCountertop components fabricated out of granite and other natural stones are sometimes reinforced with metal rods inserted into grooves cut into the underside of the stone, and bonded in place with various resins. This procedure is called \"rodding\" by countertop fabricators. Most commonly, these rods will be placed near sink cutouts to prevent cracking of the brittle stone countertop during transportation and installation. Data published by the Marble Institute of America shows that this technique results in a 600% increase in the deflection strength of the component.\n\nHowever, if a metal rod subject to oxidation or other forms of corrosion is used, and moisture from a sink or faucet reaches the rod, oxide jacking can crack the countertop directly above the rod. Mild steel and some grades of aluminium rods are known to cause oxide jacking failures in granite countertops. Skilled stone repair professionals can disassemble the cracked stone, remove the metal rod, and reassemble the stone using various resins tinted to match the colors of the stone. This type of problem can be prevented by using reinforcing rods made of stainless steel or fiberglass in the rodding procedure.\n\n\n"}
{"id": "39265457", "url": "https://en.wikipedia.org/wiki?curid=39265457", "title": "Passive sign convention", "text": "Passive sign convention\n\nIn electrical engineering, the passive sign convention (PSC) is a sign convention or arbitrary standard rule adopted universally by the electrical engineering community for defining the sign of electric power in an electric circuit. The convention defines electric power flowing out of the circuit \"into\" an electrical component as positive, and power flowing into the circuit \"out\" of a component as negative. So a passive component which consumes power, such as an appliance or light bulb, will have \"positive\" power dissipation, while an active component, a source of power such as an electric generator or battery, will have \"negative\" power dissipation. This is the standard definition of power in electric circuits; it is used for example in computer circuit simulation programs such as SPICE.\n\nTo comply with the convention, the direction of the voltage and current variables used to calculate power and resistance in the component must have a certain relationship: the current variable must be defined so positive current enters the positive voltage terminal of the device. These directions may be different from the directions of the actual current flow and voltage.\n\nThe passive sign convention states that in components in which the conventional current variable i is defined as entering the device through the terminal which is positive as defined by the voltage variable v,\n\nthe power p and resistance r are given by \n\nIn components in which the current i is defined so positive current enters the device through the negative voltage terminal, power and resistance are given by\n\nWith these definitions, passive components (loads) will have p > 0 and r > 0, and active components (power sources) will have p < 0 and r < 0.\n\nIn electrical engineering, power represents the rate of electrical energy flowing into or out of a given component or control volume. Power is a signed quantity; negative power just represents power flowing in the opposite direction from positive power. From the standpoint of power flow, electrical components in a circuit can be divided into two types: \nSome components can be either a source or a load, depending on the voltage or current through them. For example, a rechargeable battery acts as a source when it is used to produce power, but as a load when it is being recharged.\n\nSince it can flow in either direction, there are two possible ways to define electric power; two possible \"reference directions\": either power flowing into an electrical component, or power flowing out of the component, can be defined as positive. Whichever is defined as positive, the other will be negative. The passive sign convention arbitrarily defines power flowing \"out\" of the circuit (\"into\" the component) as positive, so passive components have \"positive\" power flow.\n\nIn an AC (alternating current) circuit the current and voltage switch direction with each half-cycle of the current, but the definitions above still apply; at any given instant, in passive components the current flows from the positive terminal to the negative, while in active components it flows the other direction.\n\nThe power flow p and resistance r of an electrical component are related to the voltage v and current i variables by the defining equation for power and Ohm's law:\n\nLike power, voltage and current are signed quantities. The current flow in a wire has two possible directions, so when defining a current variable i the direction which represents positive current flow must be indicated, usually by an arrow on the circuit diagram. This is called the \"reference direction for current i. If the actual current is in the opposite direction, the variable i will have a negative value. Similarly in defining a variable v representing the voltage between two terminals, the terminal which represents the positive side must be specified, usually with a plus sign. This is called the \"reference direction for voltage v. If the terminal marked positive actually has a lower voltage than the other one, then the variable v will have a negative value.\n\nTo understand the passive sign convention, it is important to distinguish the reference directions of the variables, v and i, which can be assigned at will, from the direction of the actual \"voltage\" and \"current\", which is determined by the circuit. The idea of the PSC is that by assigning the reference direction of variables v and i in a component with the right relationship, the power flow in passive components calculated from Eq. (1) will come out positive, while the power flow in active components will come out negative. It is not necessary to know whether a component produces or consumes power when analyzing the circuit; reference directions can be assigned arbitrarily, directions to currents and polarities to voltages, then the PSC is used to calculate the power in components. If the power comes out positive, the component is a load, consuming electric power and converting it to some other kind of power. If the power comes out negative, the component is a source, converting some other form of power to electric power.\n\nThe above discussion shows that choosing the relative direction of the voltage and current variables in a component determines the direction of power flow that is considered positive. The reference directions of the individual variables are not important, only their relation to each other. There are two choices: \n\n\n\nIn practice it is not necessary to assign the voltage and current variables in a circuit to comply with the PSC. Components in which the variables have a \"backward\" relationship, in which the current variable enters the negative terminal, can still be made to comply with the PSC by changing the sign of the constitutive relations (1) and (2) used with them. A current entering the negative terminal is equivalent to a negative current entering the positive terminal, so in such a component\n\nOne advantage of defining all the variables in a circuit to comply with the PSC is that it makes it easy to express conservation of energy. Since electric energy cannot be created or destroyed, at any given instant every watt of power consumed by a load component must be produced by some source component in the circuit. Therefore the sum of all the power consumed by loads equals the sum of all the power produced by sources. Since with the PSC the power dissipation in sources is negative and power dissipation in loads is positive, the sum of all the power dissipation in all the components in a circuit is always zero\n\nSince the sign convention only deals with the directions of the \"variables\" and not with the direction of the actual \"current\", it also applies to alternating current (AC) circuits, in which the direction of the voltage and current periodically reverses. In AC circuits, even though the voltage and current reverse direction, a \"formal\" direction of current flow and voltage polarity are defined by considering the voltage and current direction in the first half of the cycle \"positive\". During the second half of the AC cycle, in a resistive AC circuit, both the voltage and current in the device reverse direction, so the sign of the voltage and current reverse. Since the power is the product of voltage and current, the two sign reversals cancel each other, and the sign of the power flow is unchanged.\n\nIn practice, the power output of power sources such as batteries and generators is not given in negative numbers, as required by the passive sign convention. No manufacturer sells a \"−5 kilowatt generator\". The standard practice in electric power circuits is to use positive values for the power and resistance of power sources, as well as loads. This avoids confusion over the meaning of \"negative power\", and particularly \"negative resistance\". In order to make the power for both sources and loads come out positive, instead of the PSC, separate sign conventions must be used for sources and loads. These are called the \"generator-load conventions\"\nUsing this convention, positive power flow in source components is power \"produced\", while positive power flow in load components is power \"consumed\". \nAs with the PSC, if the variables in a given component do not conform to the applicable convention, the component can still be made to conform by using negative signs in the constitutive equations (1) and (2)\n\nThis convention may seem preferable to the PSC, since the power \"P\" and resistance \"R\" always have positive values. However it cannot be used in electronics, because it is not possible to classify some electronic components unambiguously as \"sources\" or \"loads\". Some electronic components may act as sources of power with negative resistance in some portions of their operating range, and as absorbers of power with positive resistance in other portions, or even in different portions of the AC cycle. The power consumption or production of a component depends on its current-voltage characteristic curve. Whether the component acts as a source or load may depend on the current i or voltage v in it, which is not known until the circuit is analyzed. For example, if the voltage across a rechargeable battery's terminals is less than its open-circuit voltage, it will act as a source, while if the voltage is greater it will act as a load and recharge. So it is necessary for power and resistance variables to be able to take on both positive and negative values.\n"}
{"id": "1131331", "url": "https://en.wikipedia.org/wiki?curid=1131331", "title": "Position error", "text": "Position error\n\nPosition error is one of the errors affecting the systems in an aircraft for measuring airspeed and altitude. It is not practical or necessary for an aircraft to have an airspeed indicating system and an altitude indicating system that are exactly accurate. A small amount of error is tolerable. It is caused by the location of the static vent that supplies the altimeter.\n\nAll aircraft are equipped with a small hole in the surface of the aircraft called the static port. The air pressure in the vicinity of the static port is conveyed by a conduit to the altimeter and the airspeed indicator. This static port and the conduit constitute the aircraft's static system. The objective of the static system is to sense the pressure of the air at the altitude at which the aircraft is flying. In an ideal static system the air pressure fed to the altimeter and airspeed indicator is equal to the pressure of the air at the altitude at which the aircraft is flying.\n\nAs the air flows past an aircraft in flight, the streamlines are affected by the presence of the aircraft, and the speed of the air relative to the aircraft is different at different positions on the aircraft's outer surface. In consequence of Bernoulli's principle, the different speeds of the air result in different pressures at different positions on the aircraft's surface. The ideal position for a static port is a position where the local air pressure in flight is always equal to the pressure remote from the aircraft, however there is no position on an aircraft where this ideal situation exists for all angles of attack. When deciding on a position for a static port, aircraft designers attempt to find a position where the error between static pressure and free-stream pressure is a minimum across the operating range of angle of attack of the aircraft. The residual error at any given angle of attack is called the position error.\nPosition error affects the indicated airspeed and the indicated altitude. Aircraft manufacturers use the aircraft flight manual to publish details of the error in indicated airspeed and indicated altitude across the operating range of speeds. In many aircraft, the effect of position error on airspeed is shown as the difference between indicated airspeed and calibrated airspeed. In some low-speed aircraft, the position error is shown as the difference between indicated airspeed and equivalent airspeed.\n\nBernoulli's principle states that total pressure (or stagnation pressure) is constant along a streamline. There is no variation in stagnation pressure, regardless of the position on the streamline where it is measured. There is no position error associated with stagnation pressure.\n\nThe Pitot tube supplies pressure to the airspeed indicator. Pitot pressure is equal to stagnation pressure providing the Pitot tube is aligned with the local airflow, it is located outside the boundary layer, and outside the wash from the propeller. Pitot pressure can suffer alignment error but it is not vulnerable to position error.\n\nAircraft design standards specify a maximum amount of Pitot-static system error. The error in indicated altitude must not be excessive because it is important for pilots to know their altitude with reasonable accuracy for the purpose of traffic separation. US Federal Aviation Regulations, Part 23, §23.1325(e) includes the following requirement for the static pressure system:\n\nThe error in indicated airspeed must also not be excessive. Part 23, §23.1323(b) includes the following requirement for the airspeed indicating system:\n\nFor the purpose of complying with an aircraft design standard that specifies a maximum permissible error in the airspeed indicating system it is necessary to measure the position error in a representative aircraft. There are many different methods for measuring position error. Some of the more common methods are:\n\n\n\n"}
{"id": "24222", "url": "https://en.wikipedia.org/wiki?curid=24222", "title": "Public-key cryptography", "text": "Public-key cryptography\n\nPublic-key cryptography, or asymmetric cryptography, is any cryptographic system that uses pairs of keys: \"public keys\" which may be disseminated widely, and \"private keys\" which are known only to the owner. This accomplishes two functions: authentication, where the public key verifies that a holder of the paired private key sent the message, and encryption, where only the paired private key holder can decrypt the message encrypted with the public key.\n\nIn a public key encryption system, any person can encrypt a message using the receiver's public key. That encrypted message can only be decrypted with the receiver's private key. To be practical, the generation of a public and private key -pair must be computationally economical. The strength of a public key cryptography system relies on the computational effort (\"work factor\" in cryptography) required to find the private key from its paired public key. Effective security only requires keeping the private key private; the public key can be openly distributed without compromising security.\n\nPublic key cryptography systems often rely on cryptographic algorithms based on mathematical problems that currently admit no efficient solution, particularly those inherent in certain integer factorization, discrete logarithm, and elliptic curve relationships. Public key algorithms, unlike symmetric key algorithms, do \"not\" require a secure channel for the initial exchange of one or more secret keys between the parties.\n\nBecause of the computational complexity of asymmetric encryption, it is usually used only for small blocks of data, typically the transfer of a symmetric encryption key (e.g. a session key). This symmetric key is then used to encrypt the rest of the potentially long message sequence. The symmetric encryption/decryption is based on simpler algorithms and is much faster.\n\nIn a public key signature system, a person can combine a message with a private key to create a short \"digital signature\" on the message. Anyone with the corresponding public key can combine a message, a putative digital signature on it, and the known public key to verify whether the signature was valid, i.e. made by the owner of the corresponding private key. Changing the message, even replacing a single letter, will cause verification to fail. In a secure signature system, it is computationally infeasible for anyone who does not know the private key to deduce it from the public key or any number of signatures, or to find a valid signature on any message for which a signature has not hitherto been seen. Thus the authenticity of a message can be demonstrated by the signature, provided the owner of the private key keeps the private key secret.\n\nPublic key algorithms are fundamental security ingredients in cryptosystems, applications and protocols. They underpin various Internet standards, such as Transport Layer Security (TLS), S/MIME, PGP, and GPG. Some public key algorithms provide key distribution and secrecy (e.g., Diffie–Hellman key exchange), some provide digital signatures (e.g., Digital Signature Algorithm), and some provide both (e.g., RSA).\n\nPublic key cryptography finds application in, among others, the information technology security discipline, information security. Information security (IS) is concerned with all aspects of protecting electronic information assets against security threats. Public key cryptography is used as a method of assuring the confidentiality, authenticity and non-repudiability of electronic communications and data storage.\n\nTwo of the best-known uses of public key cryptography are:\n\n\nAn analogy to public key encryption is that of a locked mail box with a mail slot. The mail slot is exposed and accessible to the public – its location (the street address) is, in essence, the public key. Anyone knowing the street address can go to the door and drop a written message through the slot. However, only the person who possesses the key can open the mailbox and read the message.\n\nAn analogy for digital signatures is the sealing of an envelope with a personal wax seal. The message can be opened by anyone, but the presence of the unique seal authenticates the sender.\n\nA central problem with the use of public key cryptography is confidence/proof that a particular public key is authentic, in that it is correct and belongs to the person or entity claimed, and has not been tampered with or replaced by a malicious third party. The usual approach to this problem is to use a public key infrastructure (PKI), in which one or more third parties – known as certificate authorities – certify ownership of key pairs. PGP, in addition to being a certificate authority structure, has used a scheme generally called the \"web of trust\", which decentralizes such authentication of public keys by a central mechanism, and substitutes individual endorsements of the link between user and public key. To date, no fully satisfactory solution to the \"public key authentication problem\" has been found.\n\nDuring the early history of cryptography, two parties would rely upon a key that they would exchange by means of a secure, but non-cryptographic, method such as a face-to-face meeting or a trusted courier. This key, which both parties kept absolutely secret, could then be used to exchange encrypted messages. A number of significant practical difficulties arise with this approach to distributing keys.\n\nIn his 1874 book \"The Principles of Science\", William Stanley Jevons wrote:\nCan the reader say what two numbers multiplied together will produce the number 8616460799? I think it unlikely that anyone but myself will ever know.\nHere he described the relationship of one-way functions to cryptography, and went on to discuss specifically the factorization problem used to create a trapdoor function. In July 1996, mathematician Solomon W. Golomb said: \"Jevons anticipated a key feature of the RSA Algorithm for public key cryptography, although he certainly did not invent the concept of public key cryptography.\"\n\nIn 1970, James H. Ellis, a British cryptographer at the UK Government Communications Headquarters (GCHQ), conceived of the possibility of \"non-secret encryption\", (now called public key cryptography), but could see no way to implement it. In 1973, his colleague Clifford Cocks implemented what has become known as the RSA encryption algorithm, giving a practical method of \"non-secret encryption\", and in 1974, another GCHQ mathematician and cryptographer, Malcolm J. Williamson, developed what is now known as Diffie–Hellman key exchange. \nThe scheme was also passed to the USA's National Security Agency. With a military focus, and low computing power, the power of public key cryptography was unrealised in both organisations:\n\nI judged it most important for military use ... if you can share your key rapidly and electronically, you have a major advantage over your opponent. Only at the end of the evolution from Berners-Lee designing an open internet architecture for CERN, its adaptation and adoption for the Arpanet ... did public key cryptography realise its full potential.\n\n—Ralph Benjamin\n\nTheir discovery was not publicly acknowledged for 27 years, until the research was declassified by the British government in 1997.\n\nIn 1976, an asymmetric key cryptosystem was published by Whitfield Diffie and Martin Hellman who, influenced by Ralph Merkle's work on public key distribution, disclosed a method of public key agreement. This method of key exchange, which uses exponentiation in a finite field, came to be known as Diffie–Hellman key exchange. This was the first published practical method for establishing a shared secret-key over an authenticated (but not confidential) communications channel without using a prior shared secret. Merkle's \"public key-agreement technique\" became known as Merkle's Puzzles, and was invented in 1974 and published in 1978.\n\nIn 1977, a generalization of Cocks' scheme was independently invented by Ron Rivest, Adi Shamir and Leonard Adleman, all then at MIT. The latter authors published their work in 1978, and the algorithm came to be known as RSA, from their initials. RSA uses exponentiation modulo a product of two very large primes, to encrypt and decrypt, performing both public key encryption and public key digital signature. Its security is connected to the extreme difficulty of factoring large integers, a problem for which there is no known efficient general technique. In 1979, Michael O. Rabin published a related cryptosystem that is probably secure as long as the factorization of the public key remains difficult – it remains an assumption that RSA also enjoys this security.\n\nSince the 1970s, a large number and variety of encryption, digital signature, key agreement, and other techniques have been developed in the field of public key cryptography. The ElGamal cryptosystem, invented by Taher ElGamal relies on the similar and related high level of difficulty of the discrete logarithm problem, as does the closely related DSA, which was developed at the US National Security Agency (NSA) and published by NIST as a proposed standard.\n\nThe introduction of elliptic curve cryptography by Neal Koblitz and Victor Miller, independently and simultaneously in the mid-1980s, has yielded new public key algorithms based on the discrete logarithm problem. Although mathematically more complex, elliptic curves provide smaller key sizes and faster operations for approximately equivalent estimated security.\n\nPublic key cryptography is often used to secure electronic communication over an open networked environment such as the Internet, without relying on a hidden or covert channel, even for key exchange. Open networked environments are susceptible to a variety of communication security problems, such as man-in-the-middle attacks and spoofs. Communication security typically includes requirements that the communication must not be readable during transit (preserving confidentiality), the communication must not be modified during transit (preserving the integrity of the communication), the communication must originate from an identified party (sender authenticity), and the recipient must not be able to repudiate or deny receiving the communication. Combining public key cryptography with an Enveloped Public Key Encryption (EPKE) method, allows for the secure sending of a communication over an open networked environment. In other words, even if an adversary listens to an entire conversation including the key exchange, the adversary would not be able to interpret the conversation.\n\nThe distinguishing technique used in public key cryptography is the use of asymmetric key algorithms, where a key used by one party to perform encryption is not the same as the key used by another in decryption. Each user has a pair of cryptographic keys – a public encryption key and a private decryption key. For example, a key pair used for digital signatures consists of a private signing key and a public verification key. The public key may be widely distributed, while the private key is known only to its proprietor. The keys are related mathematically, but the parameters are chosen so that calculating the private key from the public key is unfeasible.\n\nIn contrast, symmetric key algorithms use a \"single\" secret key, which must be shared and kept private by both the sender (for encryption) and the receiver (for decryption). To use a symmetric encryption scheme, the sender and receiver must securely share a key in advance.\n\nBecause symmetric key algorithms are nearly always much less computationally intensive than asymmetric ones, it is common to exchange a key using a key-exchange algorithm, then transmit data using that key and a symmetric key algorithm. PGP, SSH, and the SSL/TLS family of schemes use this procedure, and are thus called \"hybrid cryptosystems\".\n\nSome encryption schemes can be proven secure on the basis of the presumed difficulty of a mathematical problem, such as factoring the product of two large primes or computing discrete logarithms. Note that \"secure\" here has a precise mathematical meaning, and there are multiple different (meaningful) definitions of what it means for an encryption scheme to be \"secure\". The \"right\" definition depends on the context in which the scheme will be deployed.\n\nThe most obvious application of a public key encryption system is confidentiality – a message that a sender encrypts using the recipient's public key can be decrypted only by the recipient's paired private key. This assumes, of course, that no flaw is discovered in the basic algorithm used.\n\nAnother application in public key cryptography is the digital signature. Digital signature schemes can be used for sender authentication and non-repudiation. The sender computes a digital signature for the message to be sent, then sends the signature (together with the message) to the intended receiver. Digital signature schemes have the property that signatures can be computed only with the knowledge of the correct private key. To verify that a message has been signed by a user and has not been modified, the receiver needs to know only the corresponding public key. In some cases (e.g., RSA), a single algorithm can be used to both encrypt and create digital signatures. In other cases (e.g., DSA), each algorithm can only be used for one specific purpose.\n\nTo achieve both authentication and confidentiality, the sender should include the recipient's name in the message, sign it using his private key, and then encrypt both the message and the signature using the recipient's public key.\n\nThese characteristics can be used to construct many other (sometimes surprising) cryptographic protocols and applications, such as digital cash, password-authenticated key agreement, multi-party key agreement, time-stamping services, non-repudiation protocols, etc.\n\nEnveloped Public Key Encryption (EPKE) is the method of applying public key cryptography and ensuring that an electronic communication is transmitted confidentially, has the contents of the communication protected against being modified (communication integrity) and cannot be denied from having been sent (non-repudiation). This is often the method used when securing communication on an open networked environment such by making use of the Transport Layer Security (TLS) or Secure Sockets Layer (SSL) protocols.\n\nEPKE consists of a two-stage process that includes both Public Key Encryption (PKE) and a digital signature. Both Public Key Encryption and digital signatures make up the foundation of Enveloped Public Key Encryption (these two processes are described in full in their own sections).\n\nFor EPKE to work effectively, it is required that:\n\nTo send a message using EPKE, the sender of the message first signs the message using their own private key, this ensures non-repudiation of the message. The sender then encrypts their digitally signed message using the receiver's public key thus applying a digital envelope to the message. This step ensures confidentiality during the transmission of the message. The receiver of the message then uses their private key to decrypt the message thus removing the digital envelope and then uses the sender's public key to decrypt the sender's digital signature. At this point, if the message has been unaltered during transmission, the message will be clear to the receiver.\n\nDue to the computationally complex nature of RSA-based asymmetric encryption algorithms, the time taken to encrypt large documents or files to be transmitted can be relatively long. To speed up the process of transmission, instead of applying the sender's digital signature to the large documents or files, the sender can rather hash the documents or files using a cryptographic hash function and then digitally sign the generated hash value, therefore enforcing non-repudiation. Hashing is a much faster computation to complete as opposed to using an RSA-based digital signature algorithm alone. The sender would then sign the newly generated hash value and encrypt the original documents or files with the receiver's public key. The transmission would then take place securely and with confidentiality and non-repudiation still intact. The receiver would then verify the signature and decrypt the encrypted documents or files with their private key.\n\nNote: The sender and receiver do not usually carry out the process mentioned above manually though, but rather rely on sophisticated software to automatically complete the EPKE process.\n\nThe goal of Public Key Encryption (PKE) is to ensure that the communication being sent is kept confidential during transit.\n\nTo send a message using PKE, the sender of the message uses the public key of the receiver to encrypt the contents of the message. The encrypted message is then transmitted electronically to the receiver and the receiver can then use their own matching private key to decrypt the message.\n\nThe encryption process of using the receiver's public key is useful for preserving the confidentiality of the message as only the receiver has the matching private key to decrypt the message. Therefore, the sender of the message cannot decrypt the message once it has been encrypted using the receiver's public key. However, PKE does not address the problem of non-repudiation, as the message could have been sent by anyone that has access to the receiver's public key.\n\nA digital signature is meant to prove a message came from a particular sender; neither can anyone impersonate the sender nor can the sender deny having sent the message. This is useful for example when making an electronic purchase of shares, allowing the receiver to prove who requested the purchase. Digital signatures do not provide confidentiality for the message being sent.\n\nThe message is signed using the sender's private signing key. The digitally signed message is then sent to the receiver, who can then use the sender's public key to verify the signature.\n\nIn order for Enveloped Public Key Encryption to be as secure as possible, there needs to be a \"gatekeeper\" of public and private keys, or else anyone could create key pairs and masquerade as the intended sender of a communication, proposing them as the keys of the intended sender. This digital key \"gatekeeper\" is known as a certification authority. A certification authority is a trusted third party that can issue public and private keys, thus certifying public keys. It also works as a depository to store key chain and enforce the trust factor.\n\nAn analogy that can be used to understand the advantages of an asymmetric system is to imagine two people, Alice and Bob, who are sending a secret message through the public mail. In this example, Alice wants to send a secret message to Bob, and expects a secret reply from Bob.\n\nWith a symmetric key system, Alice first puts the secret message in a box, and locks the box using a padlock to which she has a key. She then sends the box to Bob through regular mail. When Bob receives the box, he uses an identical copy of Alice's key (which he has somehow obtained previously, maybe by a face-to-face meeting) to open the box, and reads the message. Bob can then use the same padlock to send his secret reply.\n\nIn an asymmetric key system, Bob and Alice have separate padlocks. First, Alice asks Bob to send his open padlock to her through regular mail, keeping his key to himself. When Alice receives it, she uses it to lock a box containing her message, and sends the locked box to Bob. Bob can then unlock the box with his key and read the message from Alice. To reply, Bob must similarly get Alice's open padlock to lock the box before sending it back to her.\n\nThe critical advantage in an asymmetric key system is that Bob and Alice never need to send a copy of their keys to each other. This prevents a third party – perhaps, in this example, a corrupt postal worker who opens unlocked boxes – from copying a key while it is in transit, allowing the third party to spy on all future messages sent between Alice and Bob. In addition, if Bob were careless and allowed someone else to copy \"his\" key, Alice's messages to \"Bob\" would be compromised, but Alice's messages to \"other people\" would remain secret, since the other people would be providing different padlocks for Alice to use.\n\nAnother kind of asymmetric key system, called a three-pass protocol, requires neither party to even touch the other party's padlock (or key to get access); Bob and Alice have separate padlocks.\nFirst, Alice puts the secret message in a box, and locks the box using a padlock to which only she has a key. She then sends the box to Bob through regular mail. When Bob receives the box, he adds his own padlock to the box, and sends it back to Alice. When Alice receives the box with the two padlocks, she removes her padlock and sends it back to Bob. When Bob receives the box with only his padlock on it, Bob can then unlock the box with his key and read the message from Alice. Note that, in this scheme, the order of decryption is NOT the same as the order of encryption – this is only possible if commutative ciphers are used. A commutative cipher is one in which the order of encryption and decryption is interchangeable, just as the order of multiplication is interchangeable (i.e., codice_1). This method is secure for certain choices of commutative ciphers, but insecure for others (e.g., a simple codice_2). For example, let codice_3 and codice_3 be two encryption functions, and let \"codice_5\" be the message so that if Alice encrypts it using codice_3 and sends codice_7 to Bob. Bob then again encrypts the message as codice_8 and sends it to Alice. Now, Alice decrypts codice_8 using codice_3. Alice will now get codice_7, meaning when she sends this again to Bob, he will be able to decrypt the message using codice_3 and get \"codice_5\". Although none of the keys were ever exchanged, the message \"codice_5\" may well be a key (e.g., Alice's Public key). This three-pass protocol is typically used during key exchange.\n\nNot all asymmetric key algorithms operate in this way. In the most common, Alice and Bob each own \"two\" keys, one for encryption and one for decryption. In a secure asymmetric key encryption scheme, the private key should not be deducible from the public key. This makes possible public key encryption, since an encryption key can be published without compromising the security of messages encrypted with that key.\n\nIn other schemes, either key can be used to encrypt the message. When Bob encrypts a message with his private key, only his public key will successfully decrypt it, authenticating Bob's authorship of the message. In the alternative, when a message is encrypted with the public key, only the private key can decrypt it. In this arrangement, Alice and Bob can exchange secret messages with no prior secret agreement, each using the other's public key to encrypt, and each using their own private key to decrypt.\n\nAmong symmetric key encryption algorithms, only the one-time pad can be proven to be secure against any adversary – no matter how much computing power is available. However, there is no public key scheme with this property, since all public key schemes are susceptible to a \"brute-force key search attack\". Such attacks are impractical if the amount of computation needed to succeed – termed the \"work factor\" by Claude Shannon – is out of reach of all potential attackers. In many cases, the work factor can be increased by simply choosing a longer key. But other algorithms may have much lower work factors, making resistance to a brute-force attack irrelevant. Some special and specific algorithms have been developed to aid in attacking some public key encryption algorithms – both RSA and ElGamal encryption have known attacks that are much faster than the brute-force approach. These factors have changed dramatically in recent decades, both with the decreasing cost of computing power and with new mathematical discoveries.\n\nAside from the resistance to attack of a particular key pair, the security of the certification hierarchy must be considered when deploying public key systems. Some certificate authority – usually a purpose-built program running on a server computer – vouches for the identities assigned to specific private keys by producing a digital certificate. Public key digital certificates are typically valid for several years at a time, so the associated private keys must be held securely over that time. When a private key used for certificate creation higher in the PKI server hierarchy is compromised, or accidentally disclosed, then a \"man-in-the-middle attack\" is possible, making any subordinate certificate wholly insecure.\n\nMajor weaknesses have been found for several formerly promising asymmetric key algorithms. The 'knapsack packing' algorithm was found to be insecure after the development of a new attack. Recently, some attacks based on careful measurements of the exact amount of time it takes known hardware to encrypt plain text have been used to simplify the search for likely decryption keys (see \"side channel attack\"). Thus, mere use of asymmetric key algorithms does not ensure security. A great deal of active research is currently underway to both discover, and to protect against, new attack algorithms.\n\nAnother potential security vulnerability in using asymmetric keys is the possibility of a \"man-in-the-middle\" attack, in which the communication of public keys is intercepted by a third party (the \"man in the middle\") and then modified to provide different public keys instead. Encrypted messages and responses must also be intercepted, decrypted, and re-encrypted by the attacker using the correct public keys for different communication segments, in all instances, so as to avoid suspicion. This attack may seem to be difficult to implement in practice, but it is not impossible when using insecure media (e.g., public networks, such as the Internet or wireless forms of communications) – for example, a malicious staff member at Alice or Bob's Internet Service Provider (ISP) might find it quite easy to carry out. In the earlier postal analogy, Alice would have to have a way to make sure that the lock on the returned packet really belongs to Bob before she removes her lock and sends the packet back. Otherwise, the lock could have been put on the packet by a corrupt postal worker pretending to be Bob, so as to fool Alice.\n\nOne approach to prevent such attacks involves the use of a certificate authority, a trusted third party responsible for verifying the identity of a user of the system. This authority issues a tamper-resistant, non-spoofable digital certificate for the participants. Such certificates are signed data blocks stating that this public key belongs to that person, company, or other entity. This approach also has its weaknesses – for example, the certificate authority issuing the certificate must be trusted to have properly checked the identity of the key-holder, must ensure the correctness of the public key when it issues a certificate, must be secure from computer piracy, and must have made arrangements with all participants to check all their certificates before protected communications can begin. Web browsers, for instance, are supplied with a long list of \"self-signed identity certificates\" from PKI providers – these are used to check the \"bona fides\" of the certificate authority and then, in a second step, the certificates of potential communicators. An attacker who could subvert any single one of those certificate authorities into issuing a certificate for a bogus public key could then mount a \"man-in-the-middle\" attack as easily as if the certificate scheme were not used at all. In an alternate scenario rarely discussed, an attacker who penetrated an authority's servers and obtained its store of certificates and keys (public and private) would be able to spoof, masquerade, decrypt, and forge transactions without limit.\n\nDespite its theoretical and potential problems, this approach is widely used. Examples include SSL and its successor, TLS, which are commonly used to provide security for web browser transactions (for example, to securely send credit card details to an online store).\n\nThe public key algorithms known thus far are relatively computationally costly compared with most symmetric key algorithms of apparently equivalent security. The difference factor is the use of typically quite large keys. This has important implications for their practical use. Most are used in hybrid cryptosystems for reasons of efficiency – in such a cryptosystem, a shared secret key (\"session key\") is generated by one party, and this much briefer session key is then encrypted by each recipient's public key. Each recipient then uses his own private key to decrypt the session key. Once all parties have obtained the session key, they can use a much faster symmetric algorithm to encrypt and decrypt messages. In many of these schemes, the session key is unique to each message exchange, being pseudo-randomly chosen for each message.\n\nThe binding between a public key and its \"owner\" must be correct, or else the algorithm may function perfectly and yet be entirely insecure in practice. As with most cryptography applications, the protocols used to establish and verify this binding are critically important. Associating a public key with its owner is typically done by protocols implementing a public key infrastructure – these allow the validity of the association to be formally verified by reference to a trusted third party in the form of either a hierarchical certificate authority (e.g., X.509), a local trust model (e.g., SPKI), or a web of trust scheme, like that originally built into PGP and GPG, and still to some extent usable with them. Whatever the cryptographic assurance of the protocols themselves, the association between a public key and its owner is ultimately a matter of subjective judgment on the part of the trusted third party, since the key is a mathematical entity, while the owner – and the connection between owner and key – are not. For this reason, the formalism of a public key infrastructure must provide for explicit statements of the policy followed when making this judgment. For example, the complex and never fully implemented X.509 standard allows a certificate authority to identify its policy by means of an object identifier, which functions as an index into a catalog of registered policies. Policies may exist for many different purposes, ranging from anonymity to military classifications.\n\nA public key will be known to a large and, in practice, unknown set of users. All events requiring revocation or replacement of a public key can take a long time to take full effect with all who must be informed (i.e., all those users who possess that key). For this reason, systems that must react to events in real time (e.g., safety-critical systems or national security systems) should not use public key encryption without taking great care. There are four issues of interest:\n\nA malicious (or erroneous) revocation of some (or all) of the keys in the system is likely, or in the second case, certain, to cause a complete failure of the system. If public keys can be revoked individually, this is a possibility. However, there are design approaches that can reduce the practical chance of this occurring. For example, by means of certificates, we can create what is called a \"compound principal\" – one such principal could be \"Alice and Bob have Revoke Authority\". Now, only Alice and Bob (in concert) can revoke a key, and neither Alice nor Bob can revoke keys alone. However, revoking a key now requires both Alice \"and\" Bob to be available, and this creates a problem of reliability. In concrete terms, from a security point of view, there is now a \"single point of failure\" in the public key revocation system. A successful Denial of Service attack against either Alice or Bob (or both) will block a required revocation. In fact, any partition of authority between Alice and Bob will have this effect, regardless of how it comes about.\n\nBecause the principle allowing revocation authority for keys is very powerful, the mechanisms used to control it should involve both as many participants as possible (to guard against malicious attacks of this type), while at the same time as few as possible (to ensure that a key can be revoked without dangerous delay). Public key certificates that include an expiration date are unsatisfactory in that the expiration date may not correspond with a real-world revocation but at least such certificates need not all be tracked down system-wide, nor must all users be in constant contact with the system at all times.\n\nAfter a key has been revoked or when a new user is added to a system, a new key must be distributed in some predetermined manner. Assume that Carol's key has been revoked. Until a new key has been distributed, no one will be able to send her messages and messages from her cannot be signed without violating system protocols (i.e., without a valid public key, no one can encrypt messages to her).\n\nOne could leave the power to create, certify, and revoke keys in the hands of each user, as the original PGP design did, but this raises problems of user understanding and operation. For security reasons, this approach has considerable difficulties – if nothing else, some users could be forgetful, inattentive, or confused. On the one hand, a message revoking a public key certificate should be spread as fast as possible, while on the other hand, parts of the system might be rendered inoperable before a new key can be installed. The time window can be reduced to zero by always issuing the new key together with the certificate that revokes the old one, but this requires co-location of authority to both revoke keys and generate new keys.\n\nIt is most likely a system-wide failure if the (possibly combined) principal that issues new keys fails by issuing keys improperly. This is an instance of a \"common mutual exclusion\" – a design can make the reliability of a system high, but only at the cost of system availability (and \"vice versa\"). \n\nNotification of a key certificate revocation must be spread to all those who might potentially hold it, and as rapidly as possible.\n\nThere are but two means of spreading information (i.e., a key revocation) in a distributed system: either the information is \"pushed\" to users from a central point (or points), or else it is \"pulled\" from a central point (or points) by the end users.\n\nPushing the information is the simplest solution, in that a message is sent to all participants. However, there is no way of knowing whether all participants will actually \"receive\" the message. If the number of participants is large, and some of their physical or network distances are great, then the probability of complete success (which is, in ideal circumstances, required for system security) will be rather low. In a partly updated state, the system is particularly vulnerable to \"denial of service\" attacks as security has been breached, and a vulnerability window will continue to exist as long as some users have not \"gotten the word\". Put another way, pushing certificate revocation messages is neither easy to secure, nor very reliable.\n\nThe alternative to pushing is pulling. In the extreme, all certificates contain all the keys needed to verify that the public key of interest (i.e., the one belonging to the user to whom one wishes to send a message, or whose signature is to be checked) is still valid. In this case, at least some use of the system will be blocked if a user cannot reach the verification service (i.e., one of the systems that can establish the current validity of another user's key). Again, such a system design can be made as reliable as one wishes, at the cost of lowering security – the more servers to check for the possibility of a key revocation, the longer the window of vulnerability.\n\nAnother trade-off is to use a somewhat less reliable, but more secure, verification service, but to include an expiration date for each of the verification sources. How long this \"timeout\" should be is a decision that requires a trade-off between availability and security that will have to be decided in advance, at the time of system design.\n\nAssume that the principal authorized to revoke a key has decided that a certain key must be revoked. In most cases, this happens after the fact – for instance, it becomes known that at some time in the past an event occurred that endangered a private key. Let us denote the time at which it is decided that the compromise occurred as \"T\".\n\nSuch a compromise has two implications. First, messages encrypted with the matching public key (now or in the past) can no longer be assumed to be secret. One solution to avoid this problem is to use a protocol that has perfect forward secrecy. Second, signatures made with the \"no-longer-trusted-to-be-actually-private key\" after time \"T\" can no longer be assumed to be authentic without additional information (i.e., who, where, when, etc.) about the events leading up to the digital signature. These will not always be available, and so all such digital signatures will be less than credible. A solution to reduce the impact of leaking a private key of a signature scheme is to use timestamps.\n\nLoss of secrecy and/or authenticity, even for a single user, has system-wide security implications, and a strategy for recovery must thus be established. Such a strategy will determine who has authority to, and under what conditions one must, revoke a public key certificate. One must also decide how to spread the revocation, and ideally, how to deal with all messages signed with the key since time \"T\" (which will rarely be known precisely). Messages sent to that user (which require the proper – now compromised – private key to decrypt) must be considered compromised as well, no matter when they were sent.\n\nExamples of well-regarded asymmetric key techniques for varied purposes include:\n\nExamples of asymmetric key algorithms not widely adopted include:\n\nExamples of notable – yet insecure – asymmetric key algorithms include:\n\nExamples of protocols using asymmetric key algorithms include:\n\n"}
{"id": "37258637", "url": "https://en.wikipedia.org/wiki?curid=37258637", "title": "Salt pig", "text": "Salt pig\n\nA salt pig is a container used to hold salt, to make it easily accessible to pinch or spoon measure into dishes. They are available in many materials, but are generally ceramic, porcelain, earthenware or clay.\nThe earthenware construction of a salt pig can help keep the salt from clumping in humid kitchens.\nAccording to the blog \"Mundane Essays\", a blog in which writer Muness Alrubaiehis researched the origin of the term \"salt pig,\" the use of \"pig\" is found in Scots and northern English dialect meaning an earthenware vessel.\n"}
{"id": "2554507", "url": "https://en.wikipedia.org/wiki?curid=2554507", "title": "Scalding", "text": "Scalding\n\nScalding (from the Latin word \"calidus\", meaning hot) is a form of thermal burn resulted from heated fluids such as boiling water or steam. Most scalds are considered first or second degree burns, but third degree burns can result, especially with prolonged contact.\n\nMost scalds result from exposure to high-temperature water such as tap water in baths and showers or cooking water boiled for the preparation of foods or from the spilling of hot drinks, such as coffee.\n\nScalds can be more severe when steam impinges on the naked skin, because steam can reach higher temperatures than water and transfers latent heat by condensation. On the other hand when clothes are soaked with hot water the heat transfer is often of a longer duration since the body part can not be removed from the heat source as quickly. \n\nIt is recommended for the temperature of tap water not to exceed 38 — 45 °C to avoid discomfort and scalding. The technical implementation is complicated by the necessity to keep warm water at a temperature of 55 – 60 °C to inhibit the growth of legionella bacteria. \nWater hotter than 60 °C can induce scalding injuries within seconds, while it takes approximately 2 minutes to achieve an injury in 50 °C hot water. \n\nScalds are generally more common in children, especially from the accidental spilling of hot liquids.\n\nApplying first aid for scalds is the same as for burns. First, the site of the injury should be removed from the source of heat, to prevent further scalding. If the burn is at least second degree, remove any jewelry or clothing from the site, unless it is already stuck to the skin. Cool the scald for about 20 minutes with cool or lukewarm (not cold) water, such as water from a tap.\n\nWith second-degree burns, blisters will form, but should never be popped, as it only increases chances of infection. With third-degree burns, it is best to wrap the injury very loosely to keep it clean, and seek expert medical attention.\n\nIce should be avoided, as it can do further damage to area around the injury, as should butter, toothpaste, and specialized creams.\n\nThe carcasses of beef, poultry and pigs are commonly scalded after slaughter to facilitate the removal of feathers and hair. Methods including immersion in tanks of hot water or spraying with steam. The scalding may either be hard or soft in which the temperature or duration is varied. A hard scald of 58 °C (136.4 °F) for 2.5 minutes will remove the epidermis of poultry, and this is commonly used for carcasses that will be frozen so that their appearance is white and attractive.\n\nScalded milk is milk that has been heated to . At this temperature, bacteria are killed, enzymes in the milk are destroyed, and many of the proteins are denatured.\n\nIn cooking, milk is typically scalded to increase its temperature, or to change the consistency or other cooking interactions due to the denaturing of proteins.\n\nRecipes that call for scalded milk include café au lait, baked milk, and ryazhenka. Scalded milk is used in yogurt to make the proteins unfold, and to make sure that all organisms that could out-compete the yogurt culture's bacteria are killed.\n\nMilk is both scalded and also cooled in many recipes, such as for bread and other yeast doughs, as pasteurization does not kill all bacteria, and the wild yeasts that may also be present can alter the texture and flavor. In addition, scalding milk improves the rise due to inhibition of bread rise by certain undenatured milk proteins.\n\n"}
{"id": "12546407", "url": "https://en.wikipedia.org/wiki?curid=12546407", "title": "Second-generation biofuels", "text": "Second-generation biofuels\n\nSecond-generation biofuels, also known as advanced biofuels, are fuels that can be manufactured from various types of non-food biomass. Biomass in this context means plant materials and animal waste used especially as a source of fuel.\n\nFirst-generation biofuels are made from the sugars and vegetable oils found in food crops using standard processing technologies. Second-generation biofuels are made from different feedstocks and therefore may require different technology to extract useful energy from them. Second generation feedstocks include lignocellulosic biomass or woody crops, agricultural residues or waste, as well as dedicated non-food energy crops grown on marginal land unsuitable for crop production.\n\nThe term second-generation biofuels is used loosely to describe both the 'advanced' technology used to process feedstocks into biofuel, but also the use of non-food crops, biomass and wastes as feedstocks in 'standard' biofuels processing technologies if suitable. This causes some considerable confusion. Therefore it is important to distinguish between second-generation feedstocks and second-generation biofuel processing technologies.\n\nThe development of second-generation biofuels has seen a stimulus since the food vs. fuel dilemma regarding the risk of diverting farmland or crops for biofuels production to the detriment of food supply. The biofuel and food price debate involves wide-ranging views, and is a long-standing, controversial one in the literature.\n\nSecond-generation biofuel technologies have been developed to enable the use of non-food biofuel feedstocks because of concerns to food security caused by the use of food crops for the production of first-generation biofuels. The diversion of edible food biomass to the production of biofuels could theoretically result in competition with food and land uses for food crops.\n\nFirst-generation bioethanol is produced by fermenting plant-derived sugars to ethanol, using a similar process to that used in beer and wine-making (see Ethanol fermentation). This requires the use of food and fodder crops, such as sugar cane, corn, wheat, and sugar beet. The concern is that if these food crops are used for biofuel production that food prices could rise and shortages might be experienced in some countries. Corn, wheat, and sugar beet can also require high agricultural inputs in the form of fertilizers, which limit the greenhouse gas reductions that can be achieved. Biodiesel produced by transesterification from rapeseed oil, palm oil, or other plant oils is also considered a first-generation biofuel.\n\nThe goal of second-generation biofuel processes is to extend the amount of biofuel that can be produced sustainably by using biomass consisting of the residual non-food parts of current crops, such as stems, leaves and husks that are left behind once the food crop has been extracted, as well as other crops that are not used for food purposes (non-food crops), such as switchgrass, grass, jatropha, whole crop maize, miscanthus and cereals that bear little grain, and also industry waste such as woodchips, skins and pulp from fruit pressing, etc.\n\nThe problem that second-generation biofuel processes are addressing is to extract useful feedstocks from this woody or fibrous biomass, where the useful sugars are locked in by lignin, hemicellulose and cellulose. All plants contain lignin, hemicellulose and cellulose. These are complex carbohydrates (molecules based on sugar). Lignocellulosic ethanol is made by freeing the sugar molecules from cellulose using enzymes, steam heating, or other pre-treatments. These sugars can then be fermented to produce ethanol in the same way as first-generation bioethanol production. The by-product of this process is lignin. Lignin can be burned as a carbon neutral fuel to produce heat and power for the processing plant and possibly for surrounding homes and businesses. Thermochemical processes (liquefaction) in hydrothermal media can produce liquid oily products from a wide range of feedstock that has a potential to replace or augment fuels. However, these liquid products fall short of diesel or biodiesel standards. Upgrading liquefaction products through one or many physical or chemical processes may improve properties for use as fuel.\n\nThe following subsections describe the main second-generation routes currently under development.\n\nCarbon-based materials can be heated at high temperatures in the absence (pyrolysis) or presence of oxygen, air and/or steam (gasification).\n\nThese thermochemical processes yield a mixture of gases including hydrogen, carbon monoxide, carbon dioxide, methane and other hydrocarbons, and water. Pyrolysis also produces a solid char. The gas can be fermented or chemically synthesised into a range of fuels, including ethanol, synthetic diesel, synthetic gasoline or jet fuel.\n\nThere are also lower temperature processes in the region of 150–374 °C, that produce sugars by decomposing the biomass in water with or without additives.\n\nGasification technologies are well established for conventional feedstocks such as coal and crude oil. Second-generation gasification technologies include gasification of forest and agricultural residues, waste wood, energy crops and black liquor. Output is normally syngas for further synthesis to e.g. Fischer-Tropsch products including diesel fuel, biomethanol, BioDME (dimethyl ether), gasoline via catalytic conversion of dimethyl ether, or biomethane (synthetic natural gas). Syngas can also be used in heat production and for generation of mechanical and electrical power via gas motors or gas turbines.\n\nPyrolysis is a well established technique for decomposition of organic material at elevated temperatures in the absence of oxygen. In second-generation biofuels applications forest and agricultural residues, wood waste and energy crops can be used as feedstock to produce e.g. bio-oil for fuel oil applications. Bio-oil typically requires significant additional treatment to render it suitable as a refinery feedstock to replace crude oil.\n\nTorrefaction is a form of pyrolysis at temperatures typically ranging between 200–320 °C. Feedstocks and output are the same as for pyrolysis.\n\nHydrothermal liquefaction is a process similar to pyrolysis that can process wet materials. The process is typically at moderate temperatures up to 400 °C and higher than atmospheric pressures. The capability to handle a wide range of materials make hydrothermal liquefaction viable for producing fuel and chemical production feedstock.\n\nChemical and biological processes that are currently used in other applications are being adapted for second-generation biofuels. Biochemical processes typically employ pre-treatment to accelerate the hydrolysis process, which separates out the lignin, hemicellulose and cellulose. Once these ingredients are separated, the cellulose fractions can be fermented into alcohols.\n\nFeedstocks are energy crops, agricultural and forest residues, food industry and municipal biowaste and other biomass containing sugars. Products include alcohols (such as ethanol and butanol) and other hydrocarbons for transportation use.\n\nThe following second-generation biofuels are under development, although most or all of these biofuels are synthesized from intermediary products such as syngas using methods that are identical in processes involving conventional feedstocks, first-generation and second-generation biofuels. The distinguishing feature is the technology involved in producing the intermediary product, rather than the ultimate off-take.\n\nA process producing liquid fuels from gas (normally syngas) is called a gas-to-liquid (GtL) process. When biomass is the source of the gas production the process is also referred to as biomass-to-liquids (BTL).\n\n\nThe Fischer–Tropsch (FT) process is a gas-to-liquid (GtL) process. When biomass is the source of the gas production the process is also referred to as biomass-to-liquids (BTL).\nA disadvantage of this process is the high energy investment for the FT synthesis and consequently, the process is not yet economic.\n\n\n\n\nTo qualify as a second generation feedstock, a source must not be suitable for human consumption. Second-generation biofuel feedstocks include specifically grown inedible energy crops, cultivated inedible oils, agricultural and municipal wastes, waste oils, and algae. Nevertheless, cereal and sugar crops are also used as feedstocks to second-generation processing technologies. Land use, existing biomass industries and relevant conversion technologies must be considered when evaluating suitability of developing biomass as feedstock for energy.\n\nPlants are made from lignin, hemicellulose and cellulose; second-generation technology uses one, two or all of these components. Common lignocellulosic energy crops include wheat straw, \"Arundo donax\", \"Miscanthus\" spp., short rotation coppice poplar and willow. However, each offers different opportunities and no one crop can be considered 'best' or 'worst'.\n\nMunicipal Solid Waste comprises a very large range of materials, and total waste arisings are increasing. In the UK, recycling initiatives decrease the proportion of waste going straight for disposal, and the level of recycling is increasing each year. However, there remains significant opportunities to convert this waste to fuel via gasification or pyrolysis.\n\nGreen waste such as forest residues or garden or park waste may be used to produce biofuel via different routes. Examples include Biogas captured from biodegradable green waste, and gasification or hydrolysis to syngas for further processing to biofuels via catalytic processes.\n\nBlack liquor, the spent cooking liquor from the kraft process that contains concentrated lignin and hemicellulose, may be gasified with very high conversion efficiency and greenhouse gas reduction potential to produce syngas for further synthesis to e.g. biomethanol or BioDME.\n\nThe yield of crude tall oil from process is in the range of 30 – 50 kg / ton pulp.\n\nLignocellulosic biofuels reduces greenhouse gas emissions by 60–90% when compared with fossil petroleum (Börjesson.P. et al. 2013. Dagens och framtidens hållbara biodrivmedel), which is on par with the better of current biofuels of the first-generation, where typical best values currently is 60–80%. In 2010, average savings of biofuels used within EU was 60% (Hamelinck.C. et al. 2013 Renewable energy progress and biofuels sustainability, Report for the European Commission). In 2013, 70% of the biofuels used in Sweden reduced emissions with 66% or higher. (Energimyndigheten 2014. Hållbara biodrivmedel och flytande biobränslen 2013).\n\nAn operating lignocellulosic ethanol production plant is located in Canada, run by Iogen Corporation. The demonstration-scale plant produces around 700,000 litres of bioethanol each year. A commercial plant is under construction. Many further lignocellulosic ethanol plants have been proposed in North America and around the world.\n\nThe Swedish specialty cellulose mill Domsjö Fabriker in Örnsköldsvik, Sweden develops a biorefinery using Chemrec's black liquor gasification technology. When commissioned in 2015 the biorefinery will produce 140,000 tons of biomethanol or 100,000 tons of BioDME per year, replacing 2% of Sweden's imports of diesel fuel for transportation purposes. In May 2012 it was revealed that Domsjö pulled out of the project, effectively killing the effort.\n\nIn the UK, companies like INEOS Bio and British Airways are developing advanced biofuel refineries, which are due to be built by 2013 and 2014 respectively. Under favourable economic conditions and strong improvements in policy support, NNFCC projections suggest advanced biofuels could meet up to 4.3 per cent of the UK's transport fuel by 2020 and save 3.2 million tonnes of each year, equivalent to taking nearly a million cars off the road.\n\nHelsinki, Finland, 1 February 2012 – UPM is to invest in a biorefinery producing biofuels from crude tall oil in Lappeenranta, Finland. The industrial scale investment is the first of its kind globally. The biorefinery will produce annually approximately 100,000 tonnes of advanced second-generation biodiesel for transport. Construction of the biorefinery will begin in the summer of 2012 at UPM’s Kaukas mill site and be completed in 2014. UPM's total investment will amount to approximately EUR 150 million.\n\nCalgary, Alberta, 30 April 2012 – Iogen Energy Corporation has agreed to a new plan with its joint owners Royal Dutch Shell and Iogen Corporation to refocus its strategy and activities. Shell continues to explore multiple pathways to find a commercial solution for the production of advanced biofuels on an industrial scale, but the company will NOT pursue the project it has had under development to build a larger scale cellulosic ethanol facility in southern Manitoba.\n\nSo-called \"drop-in\" biofuels can be defined as \"liquid bio-hydrocarbons that are functionally equivalent to petroleum fuels and are fully compatible with existing petroleum infrastructure\".\n\nThere is considerable interest in developing advanced biofuels that can be readily integrated in the existing petroleum fuel infrastructure – i.e. \"dropped-in\" – particularly by sectors such as aviation, where there are no real alternatives to sustainably produced biofuels for low carbon emitting fuel sources. Drop-in biofuels by definition should be fully fungible and compatible with the large existing \"petroleum-based\" infrastructure.\n\nAccording to a July 2014 report published by the IEA Bioenergy Task 39, entitled \"The Potential and Challenges of Drop-in Biofuels\", there are several ways to produce drop-in biofuels that are functionally equivalent to petroleum-derived transportation fuel blendstocks. These are discussed within three major sections of the full report and include: \n\n\nA fourth category is also briefly described that includes \"hybrid\" thermochemical/biochemical technologies suchas fermentation of synthesis gas and catalytic reforming of sugars/carbohydrates.\n\nThe report concludes by stating:\n\nTremendous entrepreneurial activity to develop and commercialize drop-in biofuels from aquatic and terrestrial feedstocks has taken place over the past several years. However, despite these efforts, drop-in biofuels represent only a small percentage (around 2%) of global biofuel markets. ... Due to the increased processing and resource requirements (e.g., hydrogen and catalysts) needed to make drop-in biofuels as compared to conventional biofuels, large scale production of cost-competitive drop-in biofuels is not expected to occur in the near to midterm. Rather, dedicated policies to promote development and commercialization of these fuels will be needed before they become significant contributors to global biofuels production. Currently, no policies (e.g., tax breaks, subsidies etc.) differentiate new, more fungible and infrastructure ready drop-in type biofuels from less infrastructure compatible oxygenated biofuels. ... Thus, while tremendous technical progress has been made in developing and improving the various routes to drop-in fuels, supportive policies directed specifically towards the further development of drop-in biofuels are likely to be needed to ensure their future commercial success.\n\n\n"}
{"id": "2355047", "url": "https://en.wikipedia.org/wiki?curid=2355047", "title": "Shimadzu Corp.", "text": "Shimadzu Corp.\n\nThe company was established by in 1875. X-ray devices, the spectrum camera, the electron microscope, and the gas chromatograph were developed and commercialized in advance of other Japanese companies. The American arm of the company, Shimadzu Scientific Instruments, was founded in 1975.\n\nIn 2002, Koichi Tanaka, a longstanding employee, won the Nobel Prize in Chemistry for developing a method of mass spectrometric analysis of biological macromolecules.\n\nThe company also developed, in 2001, an ultra-high speed video camera, HyperVision HPV-1, which is capable of recording at 1,000,000 FPS, while in 2016 it released the HyperVision HPV-X2, a camera that achieves ultra-high-speed continuous recording at 10 million frames per second at Full Pixel Resolution. Other products developed by Shimadzu include head-mounted displays.\n\nShimadzu is the world's only producer of a \"Direct-Conversion\" Flat Panel Detector for Cardiac, Angiography and General Radiography examinations.\n\nThe company had revenue of ¥264.048 billion yen ($2.8 billion USD) in FY 2012, with 10,395 employees as of March 31, 2013.\n\nShimadzu Corporation in India is represented by KAV Marketing and Engineering in New Delhi, India.\n\n"}
{"id": "40546894", "url": "https://en.wikipedia.org/wiki?curid=40546894", "title": "Signal-to-interference-plus-noise ratio", "text": "Signal-to-interference-plus-noise ratio\n\nIn information theory and telecommunication engineering, the signal-to-interference-plus-noise ratio (SINR) (also known as the signal-to-noise-plus-interference ratio (SNIR)) is a quantity used to give theoretical upper bounds on channel capacity (or the rate of information transfer) in wireless communication systems such as networks. Analogous to the SNR used often in wired communications systems, the SINR is defined as the power of a certain signal of interest divided by the sum of the interference power (from all the other interfering signals) and the power of some background noise. If the power of noise term is zero, then the SINR reduces to the signal-to-interference ratio (SIR). Conversely, zero interference reduces the SINR to the signal-to-noise ratio (SNR), which is used less often when developing mathematical models of wireless networks such as cellular networks.\n\nThe complexity and randomness of certain types of wireless networks and signal propagation has motivated the use of stochastic geometry models in order to model the SINR, particularly for cellular or mobile phone networks.\n\nSINR is commonly used in wireless communication as a way to measure the quality of wireless connections. Typically, the energy of a signal fades with distance, which is referred to as a path loss in wireless networks. Conversely, in wired networks the existence of a wired path between the sender or transmitter and the receiver determines the correct reception of data. In a wireless network ones has to take other factors into account (e.g. the background noise, interfering strength of other simultaneous transmission). The concept of SINR attempts to create a representation of this aspect.\n\nThe definition of SINR is usually defined for a particular receiver (or user). In particular, for a receiver located at some point \"x\" in space (usually, on the plane), then its corresponding SINR given by\n\nwhere \"P\" is the power of the incoming signal of interest, \"I\" is the interference power of the other (interfering) signals in the network, and \"N\" is some noise term, which may be a constant or random. Like other ratios in electronic engineering and related fields, the SINR is often expressed in decibels or dB.\n\nTo develop a mathematical model for estimating the SINR, a suitable mathematical model is needed to represent the propagation of the incoming signal and the interfering signals. A common model approach is to assume the propagation model consists of a random component and non-random (or deterministic) component.\n\nThe deterministic component seeks to capture how a signal decays or attenuates as it travels a medium such as air, which is done by introducing a path-loss or attenuation function. A common choice for the path-loss function is a simple power-law. For example, if a signal travels from point \"x\" to point \"y\", then it decays by a factor given by the path-loss function\n\nwhere the path-loss exponent \" α>2\", and \"|x-y|\" denotes the distance between point \"y\" of the user and the signal source at point \"x\". Although this model suffers from a singularity (when \"x=y\"), its simple nature results in it often being used due to the relatively tractable models it gives. Exponential functions are sometimes used to model fast decaying signals.\n\nThe random component of the model entails representing multipath fading of the signal, which is caused by signals colliding with and reflecting off various obstacles such as buildings. This is incorporated into the model by introducing a random variable with some probability distribution. The probability distribution is chosen depending on the type of fading model and include Rayleigh, Rician, log-normal shadow (or shadowing), and Nakagami.\n\nThe propagation model leads to a model for the SINR. Consider a collection of 'n' base stations located at points \"x\" to \"x\" in the plane or 3D space. Then for a user located at, say \"x=0\", then the SINR for a signal coming from base station, say, \"x\", is given by\n\nwhere \"F\" are fading random variables of some distribution. Under the simple power-law path-loss model becomes\n\nIn wireless networks, the factors that contribute to the SINR are often random (or appear random) including the signal propagation and the positioning of network transmitters and receivers. Consequently, in recent years this has motivated research in developing tractable stochastic geometry models in order to estimate the SINR in wireless networks. The related field of continuum percolation theory has also been used to derive bounds on the SINR in wireless networks.\n\n"}
{"id": "48113", "url": "https://en.wikipedia.org/wiki?curid=48113", "title": "Society for Worldwide Interbank Financial Telecommunication", "text": "Society for Worldwide Interbank Financial Telecommunication\n\nThe Society for Worldwide Interbank Financial Telecommunication (SWIFT) provides a network that enables financial institutions worldwide to send and receive information about financial transactions in a secure, standardized and reliable environment. SWIFT also sells software and services to financial institutions, much of it for use on the SWIFTNet Network, and ISO 9362. Business Identifier Codes (BICs, previously Bank Identifier Codes) are popularly known as \"SWIFT codes\".\n\nThe majority of international interbank messages use the SWIFT network. , SWIFT linked more than 11,000 financial institutions in more than 200 countries and territories, who were exchanging an average of over 15 million messages per day (compared to an average of 2.4 million daily messages in 1995). SWIFT transports financial messages in a highly secure way but does not hold accounts for its members and does not perform any form of clearing or settlement.\n\nSWIFT does not facilitate funds transfer: rather, it sends payment orders, which must be settled by correspondent accounts that the institutions have with each other. Each financial institution, to exchange banking transactions, must have a banking relationship by either being a bank or affiliating itself with one (or more) so as to enjoy those particular business features.\n\nSWIFT is a cooperative society under Belgian law owned by its member financial institutions with offices around the world. SWIFT headquarters, designed by Ricardo Bofill Taller de Arquitectura are in La Hulpe, Belgium, near Brussels. The chairman of SWIFT is Yawar Shah, originally from Pakistan, and its CEO is Gottfried Leibbrandt, originally from the Netherlands. SWIFT hosts an annual conference, called Sibos, specifically aimed at the financial services industry.\n\nSWIFT was founded in Brussels in 1973 under the leadership of its inaugural CEO, Carl Reuterskiöld (1973–1989), and was supported by 239 banks in fifteen countries. It started to establish common standards for financial transactions and a shared data processing system and worldwide communications network designed by Logica and developed by The Burroughs Corporation. Fundamental operating procedures, rules for liability, etc., were established in 1975 and the first message was sent in 1977. SWIFT's first United States operating center was inaugurated by Governor John N. Dalton of Virginia in 1979.\n\nSWIFT has become the industry standard for syntax in financial messages. Messages formatted to SWIFT standards can be read by, and processed by, many well-known financial processing systems, whether or not the message traveled over the SWIFT network. SWIFT cooperates with international organizations for defining standards for message format and content. SWIFT is also \"Registration authority\" (RA) for the following ISO standards:\n\nIn RFC 3615 \"urn:swift:\" was defined as Uniform Resource Names (URNs) for SWIFT FIN.\n\nThe SWIFT secure messaging network is run from three data centers, one in the United States, one in the Netherlands and one in a secret location known only by a restricted number of employees for security reasons. These centers share information in near real-time. In case of a failure in one of the data centers, the other is able to handle the traffic of the complete network. SWIFT uses submarine communications cables to transmit its data.\n\nSWIFT opened a third data center in Switzerland, which started operating in 2009. Since then, data from European SWIFT members are no longer mirrored to the U.S. data center. The distributed architecture partitions messaging into two messaging zones: European and Trans-Atlantic. European zone messages are stored in the Netherlands and in a part of the Switzerland operating center; Trans-Atlantic zone messages are stored in the United States and in a part of the Switzerland operating center that is segregated from the European zone messages. Countries outside of Europe were by default allocated to the Trans-Atlantic zone but could choose to have their messages stored in the European zone.\n\nSWIFT moved to its current IP network infrastructure, known as SWIFTNet, from 2001 to 2005, providing a total replacement of the previous X.25 infrastructure. The process involved the development of new protocols that facilitate efficient messaging, using existing and new message standards. The adopted technology chosen to develop the protocols was XML, where it now provides a wrapper around all messages legacy or contemporary. The communication protocols can be broken down into:\n\nInterAct\n\nFileAct\n\nBrowse\nSWIFT provides a centralized store-and-forward mechanism, with some transaction management. For bank A to send a message to bank B with a copy or authorization with institution C, it formats the message according to standard and securely sends it to SWIFT. SWIFT guarantees its secure and reliable delivery to B after the appropriate action by C. SWIFT guarantees are based primarily on high redundancy of hardware, software, and people.\n\nDuring 2007 and 2008, the entire SWIFT Network migrated its infrastructure to a new protocol called SWIFTNet Phase 2. The main difference between Phase 2 and the former arrangement is that Phase 2 requires banks connecting to the network to use a Relationship Management Application (RMA) instead of the former bilateral key exchange (BKE) system. According to SWIFT's public information database on the subject, RMA software should eventually prove more secure and easier to keep up-to-date; however, converting to the RMA system meant that thousands of banks around the world had to update their international payments systems to comply with the new standards. RMA completely replaced BKE on 1 January 2009.\n\nSWIFT means several things in the financial world: \n\nUnder 3 above, SWIFT provides turn-key solutions for members, consisting of linkage clients to facilitate connectivity to the SWIFT network and CBTs or \"computer based terminals\" which members use to manage the delivery and receipt of their messages. Some of the more well-known interfaces and CBTs provided to their members are:\n\n\nThere are four key areas that SWIFT services fall under in the financial marketplace: Securities, Treasury & Derivatives, Trade Services and Payments & Cash Management.\n\nSecurities\n\nTreasury & Derivatives\n\nCash Management\n\nTrade Services\nSwift Ref, the global payment reference data utility, is SWIFT’s unique reference data service. Swift Ref sources data direct from data originators, including central banks, code issuers and banks making it easy for issuers and originators to maintain data regularly and thoroughly. SWIFTRef constantly validates and cross-checks data across the different data sets.\n\nSWIFT offers a secure person-to-person messaging service, SWIFTNet Mail, which went live on 16 May 2007. SWIFT clients can configure their existing email infrastructure to pass email messages through the highly secure and reliable SWIFTNet network instead of the open Internet. SWIFTNet Mail is intended for the secure transfer of sensitive business documents, such as invoices, contracts and signatories, and is designed to replace existing telex and courier services, as well as the transmission of security-sensitive data over the open Internet. Seven financial institutions, including HSBC, FirstRand Bank, Clearstream, DnB NOR, Nedbank, and Standard Bank of South Africa, as well as SWIFT piloted the service.\n\nA series of articles published on 23 June 2006 in \"The New York Times\", \"The Wall Street Journal\", and the \"Los Angeles Times\" revealed a program, named the Terrorist Finance Tracking Program, which the US Treasury Department, Central Intelligence Agency (CIA), and other United States governmental agencies initiated after the 11 September attacks to gain access to the SWIFT transaction database.\n\nAfter the publication of these articles, SWIFT quickly came under pressure for compromising the data privacy of its customers by allowing governments to gain access to sensitive personal information. In September 2006, the Belgian government declared that these SWIFT dealings with American governmental authorities were a breach of Belgian and European privacy laws.\n\nIn response, and to satisfy members' concerns about privacy, SWIFT began a process of improving its architecture by implementing a distributed architecture with a two-zone model for storing messages (see Operations centers).\n\nConcurrently, the European Union negotiated an agreement with the United States government to permit the transfer of intra-EU SWIFT transaction information to the United States under certain circumstances. Because of concerns about its potential contents, the European Parliament adopted a position statement in September 2009, demanding to see the full text of the agreement and asking that it be fully compliant with EU privacy legislation, with oversight mechanisms emplaced to ensure that all data requests were handled appropriately. An interim agreement was signed without European Parliamentary approval by the European Council on 30 November 2009, the day before the Lisbon Treaty—which would have prohibited such an agreement from being signed under the terms of the Codecision procedure—formally came into effect. While the interim agreement was scheduled to come into effect on 1 January 2010, the text of the agreement was classified as \"EU Restricted\" until translations could be provided in all EU languages and published on 25 January 2010.\n\nOn 11 February 2010, the European Parliament decided to reject the interim agreement between the EU and the USA by 378 to 196 votes. One week earlier, the parliament's civil liberties committee had already rejected the deal, citing legal reservations.\n\nIn March 2011, it was reported that two mechanisms of data protection had failed: EUROPOL released a report complaining that the USA's requests for information had been too vague (making it impossible to make judgments on validity) and that the guaranteed right for European citizens to know whether their information had been accessed by USA authorities had not been put into practice.\n\nIn January 2012, the advocacy group United Against Nuclear Iran (UANI) implemented a campaign calling on SWIFT to end all relations with Iran's banking system, including the Central Bank of Iran. UANI asserted that Iran's membership in SWIFT violated U.S. and EU financial sanctions against Iran as well as SWIFT's own corporate rules.\n\nConsequently, in February 2012, the U.S. Senate Banking Committee unanimously approved sanctions against SWIFT aimed at pressuring the Belgian financial telecommunications network to terminate its ties with blacklisted Iranian banks. Expelling Iranian banks from SWIFT would potentially deny Iran access to billions of dollars in revenue and spending using SWIFT but not from using IVTS. Mark Wallace, president of UANI, praised the Senate Banking Committee.\n\nInitially SWIFT denied it was acting illegally, but now says \"it is working with U.S. and European governments to address their concerns that its financial services are being used by Iran to avoid sanctions and conduct illicit business.\" Targeted banks would be—amongst others—Saderat Bank of Iran, Bank Mellat, Post Bank of Iran and Sepah Bank. On 17 March 2012, following agreement two days earlier between all 27 member states of the Council of the European Union and the Council's subsequent ruling, SWIFT disconnected all Iranian banks from its international network that had been identified as institutions in breach of current EU sanctions and warned that even more Iranian financial institutions could be disconnected from the network.\n\nIn February 2016, most Iranian banks reconnected to the network following lift of sanctions on Joint Comprehensive Plan of Action.\n\nOn 26 February 2012 the Danish newspaper \"Berlingske\" reported that US authorities have sufficient control over SWIFT to seize money being transferred between two European Union (EU) countries (Denmark and Germany), since they succeeded in seizing around US$26,000 that was being transferred from a Danish businessman to a German bank. The transaction was automatically routed through the US, possibly because of the USD currency used in the transaction, which is how the United States was able to seize the funds. The money was a payment for a batch of Cuban cigars previously imported to Germany by a German supplier. As justification for the seizure, the U.S. Treasury stated that the Danish businessman had violated the United States embargo against Cuba.\n\n\"Der Spiegel\" reported in September 2013 that the National Security Agency (NSA) widely monitors banking transactions via SWIFT, as well as credit card transactions. The NSA intercepted and retained data from the SWIFT network used by thousands of banks to securely send transaction information. SWIFT was named as a \"target\", according to documents leaked by Edward Snowden. The documents revealed that the NSA spied on SWIFT using a variety of methods, including reading \"SWIFT printer traffic from numerous banks\". In April 2017, a group known as the Shadow Brokers released files allegedly from the NSA which indicate that the agency monitored financial transactions made through SWIFT.\n\nAs mentioned above SWIFT had disconnected all Iranian banks from its international network as a sanction against Iran. However, as of 2016 Iranian banks which are no longer on international sanctions lists, were reconnected to SWIFT. Even though in theory this enables movement of money from and to these Iranian banks, foreign banks remain wary of doing business with the country. Due to primary sanctions, transactions of U.S. banks with Iran, or transactions in U.S. dollars with Iran, remain prohibited. \n\nSimilarly, in August 2014 the UK planned to press the EU to block Russian use of SWIFT as a sanction due to Russian military intervention in Ukraine. However, SWIFT refused to do so. In their official statement they said, \"SWIFT regrets the pressure, as well as the surrounding media speculation, both of which risk undermining the systemic character of the services that SWIFT provides its customers around the world\". SPFS, a Russia-based SWIFT equivalent, was created by the Central Bank of Russia as a backup measure.\n\nIn September 2018 the European Union foreign policy head, Federica Mogherini, proposed the development of a new \"special purpose financial vehicle\" intended to bypass the U.S. controlled Society for Worldwide Interbank Financial Telecommunication payments system - commonly known as SWIFT. The seven founding members of this new system are to be Iran, the European Commission, Germany, France, the U.K., Russia and China - but not the United States.\n\nThe United States, having withdrawn from the JCPOA - better known as \"the Iran Nuclear Deal\" - has decreed severe sanctions against any nation trading with Iran. The new payments system is designed to remove certain banking transactions with Iran from the purview of U.S. authorities, and as such, to escape U.S. sanctions against nations trading with Iran. The apparent goal is to encourage Iran to continue to adhere to the terms of the JCPOA, which forbids the testing, development and manufacture of nuclear weapons, even though no claims of breach have been made by any party of the JCPOA except the United States. \n\nIn time, it could be used more generally to evade other U.S. sanctions, such as those against the German-Russian pipeline project known as Nordstream 2. \n\nSWIFT has also rejected calls to boycott Israeli banks from its network.\n\nIn 2016 an $81 million theft from the Bangladesh central bank via its account at the New York Federal Reserve Bank was traced to hacker penetration of SWIFT's Alliance Access software, according to a New York \"Times\" report. It was not the first such attempt, the society acknowledged, and the security of the transfer system was undergoing new examination accordingly. Soon after the reports of the theft from the Bangladesh central bank, a second, apparently related, attack was reported to have occurred on a commercial bank in Vietnam.\n\nBoth attacks involved malware written to both issue unauthorized SWIFT messages and to conceal that the messages had been sent. After the malware sent the SWIFT messages that stole the funds, it deleted the database record of the transfers then took further steps to prevent confirmation messages from revealing the theft. In the Bangladeshi case, the confirmation messages would have appeared on a paper report; the malware altered the paper reports when they were sent to the printer. In the second case, the bank used a PDF report; the malware altered the PDF viewer to hide the transfers.\n\nIn May 2016, Banco del Austro (BDA) in Ecuador sued Wells Fargo after Wells Fargo honored $12 million in fund transfer requests that had been placed by thieves. In this case, the thieves sent SWIFT messages that resembled recently canceled transfer requests from BDA, with slightly altered amounts; the reports do not detail how the thieves gained access to send the SWIFT messages. BDA asserts that Wells Fargo should have detected the suspicious SWIFT messages, which were placed outside of normal BDA working hours and were of an unusual size. Wells Fargo claims that BDA is responsible for the loss, as the thieves gained access to the legitimate SWIFT credentials of a BDA employee and sent fully authenticated SWIFT messages.\n\nIn the first half of 2016, an anonymous Ukrainian bank, with the episode being investigated by ISACA, and others—even \"dozens\" that are not being made public—were variously reported to have been \"compromised\" through the SWIFT network and to have lost money.\n\n"}
{"id": "56332137", "url": "https://en.wikipedia.org/wiki?curid=56332137", "title": "Song Jian", "text": "Song Jian\n\nSong Jian (; born 29 December 1931) is a Chinese missile scientist and politician. He was deputy chief designer of China's submarine-launched ballistic missile and one of the country's leading scientists in the post-Cultural Revolution era. Following the Chinese government's announcement in 1979 to advocate for one child per family, he became a leading advocate for rapid implementation and broad coverage of China's one-child policy. He served in high-ranking political positions including Vice Minister of Aerospace Industry, Director of the State Science and Technology Commission (1985–1998), vice-premier-level State Councillor (1986–1998), President of the Chinese Academy of Engineering, and Vice Chairperson of the Chinese People's Political Consultative Conference.\n\nSong Jian was born on 29 December 1931 in Rongcheng, Shandong Province. In 1946, he enlisted in the Communist Party's Eighth Route Army during the Chinese Civil War at the age of 14.\n\nAfter the establishment of the People's Republic of China in 1949, he studied at Harbin Industrial University and Beijing Foreign Language Institute, before being sent to the Soviet Union in 1953 on the recommendation of Liu Shaoqi, Vice Chairman of China. Described as a \"brilliant\" student, he studied cybernetics and military science under the theorist A. A. Feldbaum. He earned an associate Ph.D. degree from Moscow State University and a Ph.D. from Bauman Moscow State Technical University. He published seven papers in Russian on control theory, which won praise from Soviet and American scientists.\n\nAfter the Sino-Soviet split in 1960, Song returned to China and was put in charge of control systems at the Fifth Academy (later known as the Seventh Ministry of Machine Building or Missile Ministry) of the Ministry of National Defense. He was one of China's top experts on missile guidance systems. Qian Xuesen, the \"father of China's space and missile defence programs\", highly praised Song's ability and declared that Song was China's foremost control theorist, surpassing Qian himself. Qian personally chose Song to co-author the revised edition of his \"Engineering Cybernetics\", regarded as a bible of Chinese military science.\n\nAt the beginning of the Cultural Revolution, Song's home was ransacked by the Red Guards before Premier Zhou Enlai included him in the list of top 50 scientists considered indispensable to national defence and afforded special protection. Song was sent to the Jiuquan Missile Base in the desert where he could focus on his studies and research, before returning to Beijing in 1969. His work on anti-ballistic missiles attracted Zhou's attention.\n\nIn the late 1970s, Song applied his expertise in cybernetics to the problem of population control and became a proponent of China's one-child policy. At the same time, he continued to work in the missile and aerospace programs and rapidly ascended the political hierarchy. He was appointed deputy chief designer of China's submarine-launched ballistic missile in February 1980 and Vice Minister of Aerospace Industry in 1982. In 1985 he became Director of the powerful State Science and Technology Commission, and the next year he additionally became a State Councillor, a vice-premier-level position. He held both positions until 1998, when he was appointed President of the Chinese Academy of Engineering and Vice Chairperson of the Chinese People's Political Consultative Conference (CPPCC).\n\nSong was an alternate members of the 12th Central Committee of the Communist Party of China, and a full member of the 13th, 14th, and 15th central committees.\n\nAfter the end of the Cultural Revolution, China's new leader Deng Xiaoping reduced military spending and urged scientists to focus their energy on solving the country's urgent economic problems, one of which was China's huge and fast growing, yet impoverished population. In 1978, as China made its initial announcement in support of one child families, Song attended the Seventh World Congress of the International Federation of Automatic Control in Helsinki, Finland, where he encountered the cybernetic-based population control theory associated with the Club of Rome. He saw the theory as a precise and scientific approach to the population control problem, which seemed superior to the Marxist perspectives that had long predominated in China.\n\nBased on assumptions of future trends, Song and his group performed calculations that determined the \"ideal\" population for China in the next 100 years was 650 to 700 million, about two-thirds of its then-population of 1 billion. In order to achieve this long-term environmentally sustainable population, he showed that the \"optimal\" trajectory was to reduce fertility rapidly to one child per couple by 1985 and maintain that level for 20 to 40 years, and then slowly raise it to the replacement level (2.1 children per woman).\n\nFollowing the Chinese central government's decision to advocate for one-child families in 1979, Song and his associates entered the picture, actively supporting and promoting the one child ideal through conference discussions in 1980 in Chengdu. They presented their work to members of the Chinese Academy of Sciences, and through the country's top scientists, came to the attention of, and won support of, China's top leaders for rapid implementation and broad coverage of one-child limits. Song's work was endorsed by Vice Premiers Chen Muhua and Wang Zhen, who recommended it to Chen Yun, the second most influential official after Deng Xiaoping. Shocked by Song's population projections, the highest of which projected China's population to reach 4 billion by 2080 if women continued to have three births per woman, China's leaders were convinced that rapid adoption of a near universal one-child policy was the country's only option. Although some leaders, including Zhao Ziyang and Hu Yaobang, expressed doubts about its feasibility, Song spoke forcefully at a top-secret high-level meeting convened in April 1980, winning many over to his recommendation of universal one-child limits. In September, the third session of the Fifth National People's Congress approved the policy.\n\nAlthough it is widely agreed that Song's population projections influenced the speed and scope of implementation of one-child limits, several leading scholars have refuted Greenhalgh's thesis that Song \"hijacked the population policymaking process\" and that he should be considered both the inventor and central architect of the one-child policy, a thesis that has often been regurgitated without much critical reflection. Liang Zhongtang, who participated in the critical policy discussions in Chengdu in 1980 and emerged as the foremost internal critic of one-child limits, confirms that Greenhalgh put too much emphasis on Song and his group.. Wang et al. agree, concluding that \"the idea of the one-child policy came from leaders within the Party, not from scientists who offered evidence to support it.\". Goodkind suggests that Song and his colleagues were \"more like expert witnesses for a government already determined to prosecute one-child restrictions, taking advantage of the opportunity to become players in a vast and expanding government bureaucracy\" Indeed, upon learning of Song's work in February 1980, correspondence from Wang Zhen, Chen Muhua, and other top officials suggests that they were already highly sympathetic to Song's position. \n\nIt is also important to note that the universal one-child limits advocated by Song lasted only five years. In the mid 1980s, China began to permit exemptions for rural parents whose first child was a daughter (an exemption allowed due to the unpopularity of the universal one-child rule), which, along with other exemptions, resulted in a \"1.5-child\" policy that lasted for nearly 30 years. Thus, the policy in place since the mid 1980s that has been commonly referred to as \"the one-child policy\" was actually a less restrictive policy, the very sort that China might have adopted in 1980 even without the population projections and cybernetic models of Song and his colleagues. \n\nAs the director of the State Science and Technology Commission, Song was in charge of China's science and technology policies. He oversaw the , which aims to popularize scientific knowledge and technology to benefit common people, the , which encourages scientists to commercialize their scientific discoveries, and the 863 Program, which aims to stimulate the development of high-tech research in China. He also launched the Xia–Shang–Zhou Chronology Project to determine a more accurate chronology of the earliest dynasties in Chinese history.\n\nSong Jian is an academician of both the Chinese Academy of Sciences and the Chinese Academy of Engineering. He is a foreign member of the US National Academy of Engineering, the Russian Academy of Sciences, and the Royal Swedish Academy of Engineering Sciences. He is also a member of Euro-Asian Academy of Sciences and the International Academy of Astronautics.\n\n"}
{"id": "32344400", "url": "https://en.wikipedia.org/wiki?curid=32344400", "title": "Specialist Engineering Contractors Group", "text": "Specialist Engineering Contractors Group\n\nThe Specialist Engineering Contractors' Group (SEC Group) is a United Kingdom construction industry organisation, currently representing six specialist engineering membership associations. Collectively, these associations comprise over 60,000 companies with a total workforce of over 300,000 individuals.\n\nThe SEC Group reflects the combined views of these organisations, notably in the Strategic Forum for Construction, where it is one of two bodies representing specialist contractors (the other is the National Specialist Contractors Council).\n\nIt is particularly active in the areas of achieving: more cost effective procurement; fair and prompt payment; and the early involvement of specialist contractors in construction projects, to ensure more effective delivery.\n\nThe Lift and Escalator Industry Association (LEIA) is an advisory body for the lift and escalator industry. It was formed in 1997 through the merger of two-long standing associations dating back to 1932. They represent around 95% of specialist contractors in its sector.\n\nThe LEIA operates Liftex, a trade show and exhibition for the lifts and escalator industry, every three years. The next LEIA exhibition will be Liftex 2019 on 15-16 May 2019. A different exhibition with a similar name is operated by the Lifting Equipment Engineers Association.\n\n"}
{"id": "863945", "url": "https://en.wikipedia.org/wiki?curid=863945", "title": "Temporal multithreading", "text": "Temporal multithreading\n\nTemporal multithreading is one of the two main forms of multithreading that can be implemented on computer processor hardware, the other being simultaneous multithreading. The distinguishing difference between the two forms is the maximum number of concurrent threads that can execute in any given pipeline stage in a given cycle. In temporal multithreading the number is one, while in simultaneous multithreading the number is greater than one. Some authors use the term super-threading synonymously.\n\nThere are many possible variations of temporal multithreading, but most can be classified into two sub-forms:\n\n\nThe main processor pipeline may contain multiple threads, with context switches effectively occurring between pipe stages (e.g., in the barrel processor). This form of multithreading can be more expensive than the coarse-grained forms because execution resources that span multiple pipe stages may have to deal with multiple threads. Also contributing to cost is the fact that this design cannot be optimized around the concept of a \"background\" thread — any of the concurrent threads implemented by the hardware might require its state to be read or written on any cycle.\n\nIn any of its forms, temporal multithreading is similar in many ways to simultaneous multithreading. As in the simultaneous process, the hardware must store a complete set of states per concurrent thread implemented. The hardware must also preserve the illusion that a given thread has the processor resources to itself. Fairness algorithms must be included in both types of multithreading situations to prevent one thread from dominating processor time and/or resources.\n\nTemporal multithreading has an advantage over simultaneous multithreading in that it causes lower processor heat output; however, it allows only one thread to be executed at a time.\n\n"}
{"id": "26692737", "url": "https://en.wikipedia.org/wiki?curid=26692737", "title": "Tests of Engineering Aptitude, Mathematics, and Science", "text": "Tests of Engineering Aptitude, Mathematics, and Science\n\nTests of Engineering Aptitude, Mathematics, and Science (TEAMS) is an annual competition originally organized by the Junior Engineering Technical Society (JETS). TEAMS is an annual theme-based competition for students in grades 9-12, aimed at giving them the opportunity to discover engineering and how they can make a difference in the world.\n\nThe TEAMS competition was created in 1975 at the University of Illinois for the state of Illinois. In 1978, JETS expanded TEAMS to become a national competition. In 1993, the TEAMS test changed format into a format very similar to the one used today. Since 2008, the TEAMS competitions have had a theme.\n\nThe 2010 theme for the TEAMS competition delved in the problems engineers face while providing access to clean water. It was named for Samuel Taylor Coleridge's famous quote, \"Water, water, everywhere, / Nor any drop to drink.\"\n\nThis competition is divided in two parts. The first part, lasting an hour and a half, has 80 multiple choice questions. Each group of ten questions is related to a specific problem relating to the overall theme. The second part consists of eight open-ended tasks that are aimed at encouraging teamwork to develop the best answer. This competition is taken part by each participating school in a regional competition; the scores at that date determine the standings in the regional, state, and national level. There are six school divisions, one home division, one group division and two levels (9th/10th grade level & 11th/12th grade level).\n\nEach team consists of eight high school students. A school may submit multiple teams. Thousands of teams participate in this competition each year.\n"}
{"id": "18468843", "url": "https://en.wikipedia.org/wiki?curid=18468843", "title": "Third-party billing", "text": "Third-party billing\n\nThird-party billing is a form of billing where an intermediary handles the invoicing and payment between a purchaser and a vendor.\n\nIn telecommunications, it can refer to an operator-assisted telephone call, usually in conjunction with the assistance of a live or automated telephone operator. \n\nIt can also refer to a service that allows consumers to make purchases through authorized merchants, service providers and telecommunications companies, and have charges placed directly on their local phone bill, as an alternate payment option. \n\nThird party billing serves nearly 12 million households in the United States, and handles hundreds of millions of authorized transactions for consumers and businesses each year. Several businesses, including Fortune 500 companies, choose to have their services included on their phone bills to reduce administrative needs and costs. \n\nIn transportation, third party billing is the transference of transportation charges to a party other than the shipper or consignee.\n\nCramming happens when third party billing occurs on phones without consent of the person being billed. \n"}
{"id": "22364466", "url": "https://en.wikipedia.org/wiki?curid=22364466", "title": "Timeline of United States discoveries", "text": "Timeline of United States discoveries\n\nTimeline of United States discoveries encompasses the breakthroughs of human thought and knowledge of new scientific findings, phenomena, places, things, and what was previously unknown to exist. From a historical stand point, the timeline below of United States discoveries dates from the 18th century to the 21st century, which have been achieved by discoverers who are either native-born or naturalized citizens of the United States.\n\nWith an emphasis of discoveries in the fields of astronomy, physics, chemistry, medicine, biology, geology, paleontology, and archaeology, United States citizens acclaimed in their professions have contributed much. For example, the \"Bone Wars,\" beginning in 1877 and ending in 1892, was an intense period of rivalry between two American paleontologists, Edward Drinker Cope and Othniel Charles Marsh, who initiated several expeditions throughout North America in the pursuit of discovering, identifying, and finding new species of dinosaur fossils. In total, their large efforts resulted in when 142 species of dinosaurs being discovered. With the founding of the National Aeronautics and Space Administration (NASA) in 1958, a vision and continued commitment by the United States of finding extraterrestrial and astronomical discoveries has helped the world to better understand our solar system and universe. As one example, in 2008, the \"Phoenix\" lander discovered the presence of frozen water on the planet Mars of which scientists such as Peter H. Smith of the University of Arizona Lunar and Planetary Laboratory (LPL) had suspected before the mission confirmed its existence.\n\n1747 Charge conservation\n\n1796 Johnston Atoll\n\n1798 Tabuaeran\n\n1798 Teraina\n\n1798 Palmyra Atoll\n\nPalmyra Atoll, a territory of the United States, a Pacific Remote Islands Marine National Monument, and a part of the wider United States Minor Outlying Islands, is a atoll located in the North Pacific Ocean almost due south of the Hawaiian Islands, roughly halfway between the U.S. state of Hawaii and the U.S. territory of American Samoa. The atoll consists of an extensive reef, two shallow lagoons, and some 50 sand and reef-rock islets and bars covered with lush, tropical vegetation. The islets of the atoll are all connected, except Sand Island and the two Home Islets in the west and Barren Island in the east. The largest island is Cooper Island in the north, followed by Kaula Island in the south. Cooper Island is privately owned by The Nature Conservancy and managed as a nature reserve. The rest of the atoll is managed by the United States Fish and Wildlife Service and is directly administered by the Office of Insular Affairs, an agency of the United States Department of Interior. Palmyra Atoll's history is long and colorful. It was first sighted on June 14, 1798, by Captain Edmund Fanning and officially discovered in 1802 by Captain Sawle of the American ship \"Palmyra\".\n\n1798 Kingman Reef\n\n1821 South Orkney Islands\n\n1822 Howland Island\n\n1825 Baker Island\n\n1831 Chloroform\n\n1858 Hadrosaurus foulki\n\nHadrosaurus was a dubious genus of a hadrosaurid dinosaur that lived near what is now the coast of New Jersey in the late Cretaceous, around 80 million years ago. It was likely bipedal for the purposes of running, but could use its forelegs to support itself while grazing. Like all hadrosaurids, Hadrosaurus was herbivorous. Its teeth suggest it ate twigs and leaves. In the summer of 1858 while vacationing in Haddonfield, New Jersey, William Parker Foulke discovered the world's first nearly-complete skeleton of any species of dinosaur, the Hadrosaurus (named by Joseph Leidy), an event that would rock the scientific world and forever change our view of natural history. To this day, Haddonfield, New Jersey is considered to be \"ground zero\" of dinosaur paleontology.\n\n1859 Midway Atoll\n\nMidway Atoll, better known as Midway Island or collectively as the Midway islands, is a territory of the United States and a part of the wider United States Minor Outlying Islands that is located in the North Pacific Ocean near the northwestern end of the Hawaiian Islands. As a 2.4-square-mile (6.2 km²) atoll, Midway Atoll is one-third of the way between Honolulu, Hawaii and Tokyo, Japan, approximately 140 nautical miles (259 kilometers) east of the International Date Line, about 2,800 nautical miles (5,200 kilometers) west of San Francisco, California, and 2,200 nautical miles (4,100 kilometers) east of Tokyo, Japan. Midway Atoll consists of a ring-shaped barrier reef and several sand islets. The two significant pieces of land, Sand Island and Eastern Island, provide habitat for millions of seabirds. Because of the importance of marine and biological environment, Midway Atoll is an insular area known as the Midway Atoll National Wildlife Refuge that is administered and managed by the United States Fish and Wildlife Service, an agency of the United States Department of Interior. Midway Atoll is perhaps best known as the site of the Battle of Midway, fought in World War II on June 4–6, 1942 and the decisive turning point of the Pacific War when the United States Navy defeated an attack by the Empire of Japan. First known as \"Middlebrooks Islands\", Midway Atoll was discovered by U.S. Captain N.C. Brooks aboard his ship, \"Gambia\", on July 8, 1859.\n\n1859 Petroleum jelly\n\nPetroleum jelly, petrolatum or soft paraffin is a semi-solid mixture of hydrocarbons originally promoted as a topical ointment for its healing properties. The raw material for petroleum jelly was discovered in 1859 by Robert Chesebrough, a chemist from New York. In 1870, this product was branded as Vaseline Petroleum Jelly.\n\n1873 Chemical potential\n\n1875 Red Delicious\n\nThe Red Delicious is a clone of apple cultigen, now comprising more than 50 cultivars. The Red Delicious apple was discovered in 1875 by Jesse Hiatt on his farm in Peru, Iowa. Believing that the seedling was nothing more than nuisance. After chopping down the tree three times, Hiatt decided to let the tree grow and eventually, it produced an unknown and new harvest of red apples. Hiatt would eventually sell the rights to this type of apple to the Stark Brothers Nurseries and Orchards who renamed it the Red Delicious.\n\n1877 Deimos\n\nDeimos is the smaller and outer of Mars' two moons. It was discovered by Asaph Hall in 1877.\n\n1877 Phobos\n\nPhobos is the larger and closer of Mars' two small moons. It was discovered by Asaph Hall in 1877.\n\n1888 Cliff Palace\n\nThe Cliff Palace is the largest cliff dwelling in North America. The structure built by the Ancient Pueblo Peoples is located in Mesa Verde National Park in their former homeland region. The cliff dwelling and park are in the southwestern corner of Colorado, in the Southwestern United States. The ancient ruins of Cliff Palace were co-discovered during a snowstorm in December 1888 by Richard Wetherill and Charlie Mason who were searching for stray cattle on Chapin Mesa.\n\n1889 Torosaurus\n\nTorosaurus was a herbivorous dinosaur that lived during the Late Cretaceous Period about 70 million years ago in what is now North America. Torosaurus had an enormous head that measured 8 feet (2.5 m) in length. Its skull is one of the largest know up to date, no other land animal has ever had a skull larger than Torosaurus. Torosaurus frill made up about one-half the total skull length. The first fossils of Torosaurus were discovered in 1889, in Wyoming by John Bell Hatcher. The American paleontologist Othniel Charles Marsh would later name the specimen \"Torosaurus latus\", in recognition of the bull-like size of its skull and its large eyebrow horns. Ever since, the specimen has been in display at the Peabody Museum in New Haven, Connecticut.\n\n1891 Thescelosaurus\n\nThescelosaurus was a bipedal dinosaur with a sturdy build, small wide hands, and a long pointed snout from the Late Cretaceous Period, approximately 66 million years ago. As a herbivore, Thescelosaurus was not a tall dinosaur and probably browsed the ground selectively to find food. Its leg structure and proportionally heavy build suggests that it was not a fast runner like other dinosaurs. The first fossils of Thescelosaurus were co-discovered in 1891 by John Bell Hatcher and William H. Utterback, in Wyoming. However, this discovery remained stored until Charles W. Gilmore named the dinosaur in 1913.\n\n1892 Amalthea\n\nAmalthea is the third moon of Jupiter in order of distance from the planet. It was discovered on September 9, 1892, by Edward Emerson Barnard.\n\n1899 Phoebe\n\n1902 Tyrannosaurus\n\nTyrannosaurus, a bipedal carnivore, is a genus of theropod dinosaur. The species Tyrannosaurus rex, commonly abbreviated to T. rex, is a fixture in popular culture. It lived throughout what is now western North America, with a much wider range than other tyrannosaurids. Fossils are found in a variety of rock formations dating to the last two million years of the Cretaceous Period, 67 to 66 million years ago. It was among the last non-avian dinosaurs to exist prior to the Cretaceous–Tertiary extinction event. In 1902, the first skeleton of Tyrannosaurus was discovered in Hell Creek, Montana by American paleontologist Barnum Brown. In 1908, Brown discovered a better preserved skeleton of Tyrannosaurus.\n\n1908 Seyfert galaxies\n\n1909 Burgess shale\n\nThe formation of Burgess shale — located in the Canadian Rockies of British Columbia — is one of the world's most celebrated fossil fields, and the best of its kind. It is famous for the exceptional preservation of the soft parts of its fossils. It is (Middle Cambrian) old, one of the earliest soft-parts fossil beds. The rock unit is a black shale, and crops out at a number of localities near the town of Field, British Columbia in the Yoho National Park. The Burgess Shale was discovered by American palaeontologist Charles Doolittle Walcott in 1909, towards the end of the season's fieldwork. He returned in 1910 with his sons, establishing a quarry on the flanks of Fossil Ridge. The significance of soft-bodied preservation, and the range of organisms he recognized as new to science, led him to return to the quarry almost every year until 1924. At this point, aged 74, he had amassed over 65,000 specimens. Describing the fossils was a vast task, pursued by Walcott until his death in 1927.\n\n1910 Propane\n\n1912 Golden Delicious\n\nGolden Delicious is a large, yellow skinned cultivar of apple and very sweet to the taste. The original Golden Delicious tree is thought to have been discovered by Anderson Mullins on a hill near Porter Creek in Clay County, West Virginia. The Stark Brothers Nursery soon purchased the tree which spawned a leading cultivar in the United States and abroad. The Golden Delicious is the state fruit of West Virginia.\n\n1912 Smoking-cancer link\n\n1914 Sinope\n\n1915 Zener diodes\n\n1916 Barnard's Star\n\n1916 Covalent bonding\n\n1916 Heparin\n\n1917 Vitamin A\n\n1923 Oviraptor\n\n1924 Uncle Sam Diamond\n\n1925 Cepheid variables\n\n1927 Electron diffraction\n\n1928 Jones Diamond\n\n1930 Pluto\n\nFollowing the discovery of the planet Neptune in 1846, there was considerable speculation that another planet might exist beyond its orbit. The search began in the mid-19th century but culminated at the start of the 20th century with a quest for Planet X. Percival Lowell proposed the Planet X hypothesis to explain apparent discrepancies in the orbits of the gas giants, particularly Uranus and Neptune, speculating that the gravity of a large unseen planet could have perturbed Uranus enough to account for the irregularities. The discovery of Pluto by Clyde Tombaugh in 1930 initially appeared to validate Lowell's hypothesis, and Pluto was considered the ninth planet until 2006.\n\n1931 Heavy hydrogen\n\n1931 Cosmic radio waves\n\n1932 Positrons\n\n1932 Homeostasis\n\n1933 Heavy water\n\n1933 Polyvinylidene chloride\n\n1936 Elliptical galaxies\n\n1936 Muons\n\n1936 Vitamin E\n\n1936 Sodium thiopental\n\n1937 Niacin\n\n1937 Electron capture\n\n1938 Fluropolymers\n\n1938 Animal echolocation\n\n1938 Carme\n\n1938 Lysithea\n\n1940 Plutonium\n\n1942 Cyanoacrylate\n\n1943 Streptomycin\n\n1944 Americium\n\n1944 Curium\n\n\n1946 Cloud seeding\n\n1948 Warfarin\n\n1948 Miranda\n\n1948 Serotonin\n\n1948 Tetracycline\n\n1949 Nereid\n\n1949 Berkelium\n\n1950 Californium\n\n1951 Barium stars\n\n1951 Ananke\n\n1952 Polio vaccine\n\n1952 Einsteinium\n\n1952 Rapid eye movement\n\n1953 DNA structure\n\nIn 1953, based on X-ray diffraction images and the information that the bases were paired, James D. Watson along with Francis Crick co-discovered what is now widely accepted as the first accurate double-helix model of DNA structure.\n\n1955 Mendelevium\n\n1955 Antiproton\n\n\"'1956 Porous silicon\"\n\n1956 Kaon\n\n1956 Antineutron\n\n1956 Neutrino\n\n1956 Nucleic acid hybridization\n\n1958 Van Allen radiation belt\n\n1959 Antiproton\n\n1960 Seafloor spreading\n\n1961 Eta meson\n\n1964 Xi baryon\n\n1964 Cosmic microwave background radiation\n\n1964 Quark\n\n1964 1930 Lucifer\n\n1964 Hepatitis B virus\n\n1965 Aspartame\n\n1965 Pulsating white dwarves\n\n1968 Up quark\n\n1968 Down quark\n\n1969 Mosher's acid\n\n1969 Interstellar formaldehyde\n\n1970 Reverse transcriptase\n\n1972 Opiate receptors\n\n1974 Australopithecus \"Lucy\"\n\nLucy is the common name of AL 288-1, several hundred pieces of bone representing about 40% of the skeleton of an individual Australopithecus afarensis. Lucy is reckoned to have lived 3.2 million years ago. This hominid was significant as the skeleton shows evidence of small skull capacity akin to that of apes and of bipedal upright walk akin to that of humans, providing further evidence that bipedalism preceded increase in brain size in human evolution. While working in collaboration with a joint French-British-American team, Lucy was discovered in Hadar, Ethiopia on November 24, 1974, when American paleoanthropologist Donald Johanson, coaxed away from his paperwork by graduate student Tom Gray for a spur-of-the-moment survey, caught the glint of a white fossilized bone out of the corner of his eye, and recognized it as hominid. Later described as the first known member of \"Australopithecus afarensis\". Dr. Johanson's girlfriend suggested she be named \"Lucy\" after the Beatles' song \"Lucy in the Sky with Diamonds\" which was played repeatedly during the night of the discovery.\n\n1974 J/ψ mesons\n\n1974 Charm quark\n\n1974 Binary pulsars\n\n1974 Leda\n\n1974 Seaborgium\n\n1975 1983 Bok\n\n1975 Themisto\n\n1975 Amarillo Starlight\n\n1976 D mesons\n\n1976 Hepatitis B virus vaccine\n\n1977 Tau lepton\n\n1977 Rings of Uranus\n\nThe planet Uranus has a system of rings intermediate in complexity between the more extensive set around Saturn and the simpler systems around Jupiter and Neptune. The rings of Uranus were discovered on March 10, 1977, by James L. Elliot, Edward W. Dunham, and Douglas J. Mink. More than 200 years ago, William Herschel also reported observing rings, but modern astronomers are skeptical that he could actually have noticed them, as they are very dark and faint.\n\n1977 Upsilon mesons\n\n1977 Bottom quark\n\n1978 Restriction endonucleases\n\n1978 Charon\n\n1979 Metis\n\n1979 Thebe\n\n1979 Rings of Jupiter\n\n1980 Oncogene\n\n1980 Pandora\n\n1980 Prometheus\n\n1980 Atlas\n\n1981 Larissa\n\n1983 Pneumococcal polysaccharide vaccine\n\n1984 \"Whydah\" wreckage\n\nFirst launched in 1715 from London, England, the \"Whydah\" was a three-masted ship of galley-style design measuring in length, rated at 300 tons burden, and could travel at speeds up to . Christened \"Whydah\" after the West African slave trading kingdom of Ouidah, the vessel was configured as a heavily armed trading and transport ship for use in the Atlantic slave trade, carrying goods from England to exchange for slaves in West Africa. It would then travel to the Caribbean to trade the slaves for precious metals, sugar, indigo, and medicinal ingredients, which would then be transported back to England. Captained by the English pirate \"Black Sam\" Bellamy, the \"Whydah\", on April 26, 1717, sailed into a violent storm dangerously close to Cape Cod and was eventually driven onto the shoals at Wellfleet, Massachusetts. At midnight she hit a sandbar in of water some from the coast of what is now Marconi Beach. Pummelled by -an-hour winds and 30 to waves, the main mast snapped, pulling the ship into some of water where she violently capsized, taking Bellamy, all but two of his 145 men, and over 4.5 tons of gold, silver and jewels with it. After years of exhaustive searching, it was in 1984 that world headlines were made when American archeological explorer Barry Clifford found the only solidly-identified pirate shipwreck ever discovered, the \"Whydah\". Two-hundred thousand artifacts and sunken treasures were discovered in the shipwreck as well.\n\n1985 Puck\n\n\n1985 RMS \"Titanic\" wreckage\n\nThe RMS \"Titanic\" was an \"Olympic\" class passenger liner owned by the White Star Line and was built at the Harland and Wolff shipyard in Belfast, in what is now Northern Ireland. At the time of her construction, she was the largest passenger steamship in the world. Shortly before midnight on April 14, 1912, four days into the ship's maiden voyage, \"Titanic\" struck an iceberg and sank two hours and forty minutes later, early on April 15, 1912. The sinking resulted in the deaths of 1,517 of the 2,223 people on board, making it one of the deadliest peacetime maritime disasters in history. After nearly 74 years of being lost at sea on the bottom of the ocean floor, a joint Franco-American expedition led by American oceanographer Dr. Robert D. Ballard, discovered the wreckage of the RMS \"Titanic\" two miles (3 km) beneath the waves of the North Atlantic on September 1, 1985. Ballard was then forced to wait a year for weather conditions favorable to a manned mission to view the wreck at close range. In 1986, Ballard and his two-man crew, in the ALVIN submersible, made the first two and-a-half hour descent to the ocean floor to view the wreck first-hand. Over the next few days, they descended again and again and, using the Jason Jr. remote camera, recorded the first scenes of the ruined interior of the luxury liner.\n\n1986 Portia\n\n1986 Juliet\n\n1986 Cressida\n\n1986 Rosalind\n\n1986 Belinda\n\n1986 Desdemona\n\n1986 Cordelia\n\n1986 Ophelia\n\n1986 Bianca\n\n1986 Tumor suppressor gene\n\n1989 Rings of Neptune\n\n1989 Proteus\n\n1989 Despina\n\n1989 Galatea\n\n1989 Thalassa\n\n1989 Naiad\n\n1989 \"Bismarck\" wreckage\n\n1990 Strawn-Wagner Diamond\n\nThe Strawn-Wagner Diamond is a rare 3.03 carat diamond that is certified by the American Gem Society (AGS) as the world's most perfect diamond in terms of its cut and the highest grade possible, the \"Triple Zero\". The Strawn-Wagner Diamond was discovered in 1990 at the Crater of Diamonds State Park by Shirley Strawn of Murfreesboro, Arkansas.\n\n1993 Comet Shoemaker-Levy 9\n\n\n1995 Top quark\n\n1995 Comet Hale-Bopp\n\n1998 USS \"Yorktown\" (CV-5) wreckage\n\n1998 Embryonic stem cell lines\n\n2001 Interstellar vinyl alcohol\n\n2003 Sedna\n\n2003 Psamathe\n\n2003 Mab\n\n2003 Perdita\n\n2003 Cupid\n\n2004 Orcus\n\n2005 Makemake\n\n2005 Eris\n\nEris, formal designation 136199 Eris, is the largest-known dwarf planet in the Solar System and the ninth-largest body known to orbit the Sun directly. It is approximately 2,500 kilometres in diameter and 27% more massive than the dwarf planet Pluto. Eris was discovered in 2005 at W. M. Keck Observatory by American astronomer Michael E. Brown.\n\n2005 Dysnomia\n\nDysnomia, officially (136199) Eris I Dysnomia, is the only known moon of the dwarf planet Eris. In conjunction of finding Eris, American astronomer Michael E. Brown discovered Eris' satellite, Dysnomia, at W. M. Keck Observatory in 2005.\n\n2005 Hydra\n\nHydra is the outer-most natural satellite of Pluto. It was discovered along with Nix in June 2005 by the Hubble Space Telescope's Pluto Companion Search Team, which is composed of Hal A. Weaver, Alan Stern, Max J. Mutchler, Andrew J. Steffl, Marc W. Buie, William J. Merline, John R. Spencer, Eliot F. Young, and Leslie A. Young.\n\n2005 Nix\n\nNix is a natural satellite of Pluto. It was discovered along with Hydra in June 2005 by the Hubble Space Telescope's Pluto Companion Search Team, composed of Hal A. Weaver, Alan Stern, Max J. Mutchler, Andrew J. Steffl, Marc W. Buie, William J. Merline, John R. Spencer, Eliot F. Young, and Leslie A. Young.\n\n2005 KV63 at the Valley of the Kings\n\n2007 Human genome and variation mapping\n\n2007 Di-positronium\n\n"}
{"id": "11382489", "url": "https://en.wikipedia.org/wiki?curid=11382489", "title": "Watchman's chair", "text": "Watchman's chair\n\nA watchman's chair is a design of unupholstered wood construction featuring a forward slanted seat, such that the watchman could not readily fall asleep without sliding downward and off the front of the chair. The design was developed in Western Europe, and was used from late medieval times well into the 19th century. Currently this antique furniture item is found primarily in the possession of collectors and museums.\n\nThere are a number of references to the watchman's chair in literature such as the allusion to its use in Collins's \"Jezebel\". Sir Toby was described to be sitting in a canopied watchman's chair in one of Shakespeare's plays.\n\nThis article is not about the \"watchman's chair\" deriving from the Congo, which has a traditional African design.\n\n\n"}
