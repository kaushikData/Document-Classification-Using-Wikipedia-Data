{"id": "36024012", "url": "https://en.wikipedia.org/wiki?curid=36024012", "title": "Acetrax", "text": "Acetrax\n\nAcetrax is a video on demand company which was founded in the UK and Switzerland and incorporated as a Swiss company in Zürich in 2006\nand which was bought by Sky.\nThe company provides over-the-top (OTT) content in Europe, for connected TVs. They claim to have their contents on TVs in 282 million households, in 48 countries.\nIt is understood that Sky's acquisition of Acetrax in May 2012 will provide the Sky customers who do not currently subscribe to pay TV - with films and music. The move seen as part of the satellite broadcaster's move into the internet TV market.\n\nThe gross assets of Acetrax stood at £2.3 million as of December 31, 2011, according to Sky.\n\nAcetrax ceased operations as of June 21, 2013.\n\nAs well as video on demand, Acetrax has also created an app which is embedded in phones such as Samsung, LG, Panasonic, Toshiba, Grundig.\n\n"}
{"id": "27753548", "url": "https://en.wikipedia.org/wiki?curid=27753548", "title": "Active antenna", "text": "Active antenna\n\nAn active antenna is an antenna that contains active electronic components such as transistors, as opposed to most antennas which only consist of passive components such as metal rods, capacitors and inductors. Active antenna designs allow antennas of limited size to have a wider frequency range (bandwidth) than passive antennas, and are primarily used in situations where a larger passive antenna is either impractical (inside a portable radio) or impossible (suburban residential area that disallows use of large outdoor low-frequency antennas).\n\nMost active antennas consist of a short conventional antenna, such as a small whip antenna, connected to an active component (usually a FET). The signal attenuation caused by the antenna-size-to-wavelength mismatch is compensated by the active circuit. The active circuit consists of an impedance translating stage and an optional amplification stage. This arrangement is especially useful for constructing compact low frequency antennas which, due to budgetary, spatial, or practical requirements (e.g., installation in vehicles), must be downsized. Low frequency signal wavelengths range from one to ten kilometers.\n\nPower for the active components may be supplied by batteries, a filtered power supply, or through the signal feeder itself (phantom power). Antennas containing active impedance translating and optionally amplifying stages are usually used only for receiving, since operation of such stages is unidirectional.\n\n\nUlrich L. Rohde: Active Antennas, RF Design, May/June 1981\n"}
{"id": "33559485", "url": "https://en.wikipedia.org/wiki?curid=33559485", "title": "Arlen F. Chase", "text": "Arlen F. Chase\n\nArlen F. Chase (born 1953) is a Mesoamerican archaeologist and is faculty member in the anthropology department at the University of Nevada, Las Vegas. Previously, he served as the Departmental Chair at the University of Central Florida, noted for his work on exploring traces of Mayan civilization using lidar.\n\nChase took his BA and PhD in Anthropology at the University of Pennsylvania, and then worked in the University of Pennsylvania's anthropology department. His fieldwork began with Mesoamerican excavations at Holmul and Orange Walk, Belize with the Santa Rita Corozal project, Belize, followed by excavations at Tikal, Copan, Grasshopper Pueblo and Quintana Roo.\n\nChase moved to the University of Central Florida in 1984. He helped to transform it into one of the top American universities in the study of the Southern Lowland Maya. He and his wife Dr. Diane Zaino Chase have together published many peer reviewed articles and books on the subjects of hieroglyphics, settlement patterns, urbanism and ceramic studies of the Maya.\n\nSince 1984, Chase has conducted many excavations in the Petén region of Belize. From 1985 he has been Co-director of the Maya urban complex of Caracol. Chase developed excavation techniques to explore the history of Caracol from the Preclassic to the Postclassic period of Maya occupation, to reveal the archaeology while preserving the built structures as part of Belize's cultural heritage.\n\nIn 2016, he joined the anthropology department at the University of Nevada, Las Vegas (UNLV).\n\nChase teamed with University of Central Florida biologist Dr John Weishampel to obtain a NASA Space Archaeology Program & UCF-UF Space Research Initiative grant to use lidar (light radar) sensors to detect changes in the rainforest canopy, reforestation patterns and modifications to archaeological sites. The lidar system was installed in an aircraft flown over Caracol. The system used laser beams that were projected to the ground, bounced back and were recorded. These recordings produced a detailed three dimension map of both the Forest canopy and the ruins of Caracol. The speed and precision of the system was far superior to traditional ground surveys. In a 2013 talk at the University of Minnesota, Arlen and Diane Chase talked about their most recent work at Caracol, including data generated by lidar.\n\n\n\n\n\n\n\n"}
{"id": "47704444", "url": "https://en.wikipedia.org/wiki?curid=47704444", "title": "Beartooth Radio", "text": "Beartooth Radio\n\nBeartooth is a highly specialized radio that communicates with similar devices completely independent of cell phone towers or Wi-Fi signals. It has a variety of features: voice, encrypted text, geolocation, and beaconing, along with SOS broadcast capability for any device in range. Its battery pack can double the current battery life of your phone. The company was co-founded by businessman Michael Monaghan and Kevin Ames, a former smokejumper and telecommunications expert who designed and implemented ad-hoc communication systems supporting backcountry firefighters.\n\nThe Beartooth smartphone accessory, launched at Disrupt SF 2014, allowed you to maintain connection with friends having like devices even when the cellular network fails or is overburdened. The Beartooth accessory combines a 3,000mAh backup battery and a 900Mhz digital radio that functions independent of cellular network or wifi signals. Pair the Beartooth with your smart phone, launch one of their apps, and you’re now connected by voice and text to anyone else with a Beartooth. Beartooth will use short-range radio frequencies to allow you to talk to another Beartooth user within range. Users can choose to call a specific Beartooth user or to put a call out to any devices in range, including traditional 2-way radio devices \n\nBeartooth has raised $2.9M in venture funding as of June, 2015.\n"}
{"id": "54388629", "url": "https://en.wikipedia.org/wiki?curid=54388629", "title": "BudgetBakers", "text": "BudgetBakers\n\nBudgetBakers is a fintech startup that develops tools for personal finance management and budgeting for individuals and small businesses. Founded by Prague-based developer Jan Muller, BudgetBakers' main offering is the personal finance app Wallet, which is available on Android, iOS and as a web app. Founded in 2010, BudgetBakers was funded by angel investors. It was then selected as to attend the StartupYard accelerator in Prague in 2015, where it also garnered its first external investment.\n\nWallet by BudgetBakers grew its user base to more than 1.7 million in the next year, spread its presence in countries like the USA, Canada, India, Indonesia, Malaysia and most of the European countries. In 2016, it raised seed funding to the tune of €290K.\n\nBudgetBakers started off as a side project of Jan Muller, who worked as a software developer with leading Czech companies Avast, Ceska Sporitelna and Seznam.cz. Having developed the during his free time, Muller gained confidence as angel investor Martin Jiřička came on board as the startup's first investor. In 2015, BudgetBakers was selected as one among 7 other growing startups to attend the startup accelerator program hosted by StartupYard.\n\nIn this three month-program, the startup was able to reshape its vision, plan out its strategies and concentrate on scaling up the product. \"By entering SY, we had to slow down development of the new version of BudgetBakers and start focusing more on management, finances and defining the strategy that will bring us to next level. At the beginning it was very unusual for me, because i was used to spending my whole day programming. I had to turn towards management, and that was a challenge for me. But now I have to say, it helped me to crystallize my vision in term of growth and success,\" said Muller, in an interview. \"I’ve come to realize that our priority has to be our customer experience and our market strategy, instead of just building an ever richer feature set. There’s no use in creating the world’s most amazing software, if nobody is ready to use that software to make a positive impact in their lives. So we’ve had to adjust our attitude towards development, and grow up as a company and a team. That’s been an amazing experience for us, and we’re really glad we decided to do it.\"\n\nIt was during this period that the present CEO Michal Kratochvil became an advisor to the startup. \"Since then, BudgetBakers has grown to over a million registered users, tripling its paying membership since last year, as it has repositioned itself to focus more broadly on personal finance, and rolled out features that allow customers to manage all of their personal finances in one secure cloud platform,\" says EU-Startups.com. In June 2016, BudgetBakers \"raised seed funding from startup investor Petr Zamecnik and former CEO of HomeCredit and GE Money, Ivan Svitek. Along with the funding round, BudgetBakers revealed that Michal Kratochvil, former Managing Director of Accenture in Central Europe, has joined as the startup’s new CEO.\"\n\nBudgetBakers has been a pioneer in the Czech fintech revolution. It was also selected as one of the 40 growing startups to attend Google's recent CEE All Stars event hosted at its entrepreneur space, Campus Warsaw. Wallet by BudgetBakers was also awarded Google Play's Editor's Choice Badge and was in the list of the 10 best lifestyle apps in the Czech Republic of 2017.\n\n"}
{"id": "4734524", "url": "https://en.wikipedia.org/wiki?curid=4734524", "title": "Compact excavator", "text": "Compact excavator\n\nA compact or mini excavator is a tracked or wheeled vehicle with an approximate operating weight from 0.7 to 8.5 tonnes. It generally includes a standard backfill blade and features independent boom swing.\n\nHydraulic Excavators are somewhat different from other construction equipment in that all movement and functions of the machine are accomplished through the transfer of hydraulic fluid. The compact excavator's work group and blade are activated by hydraulic fluid acting upon hydraulic cylinders. The excavator's slew (rotation) and travel functions are also activated by hydraulic fluid powering hydraulic motors.\n\nMost compact hydraulic excavators have three distinct assemblies: house, undercarriage and workgroup.\n\nThe house structure contains the operator's compartment, engine compartment, hydraulic pump and distribution components. The house structure is attached to the top of the undercarriage via a swing bearing. The house, along with the workgroup, is able to rotate or slew upon the undercarriage without limit due to a hydraulic distribution valve which supplies oil to the undercarriage components.\n\nSlew\nSlewing refers to rotating the excavator's house assembly. Unlike a conventional backhoe, the operator can slew the entire house and workgroup upon the undercarriage for spoil placement.\n\nThe undercarriage consists of rubber or steel tracks, drive sprockets, rollers, idlers and associated components/structures. The undercarriage supports the house structure and the workgroup.\n\nThe workgroup of a compact hydraulic excavator consists of the boom, dipper or arm, and attachment (e.g. auger, bucket or breaker). It is connected to the front of the excavator's house structure via a swing frame that allows the workgroup to be hydraulically pivoted left or right to achieve offset digging for trenching parallel with the tracks. Certain manufacturers including Terex and Caterpillar (CAT) offer an extendable boom option, much like the extendable boom of a backhoe loader.\n\nThe primary purpose of boom swing is for offset digging around obstacles or along foundations, walls or forms. A secondary use is cycling in areas too narrow for cab rotation. Independent boom swing is one of the major advantages of a compact excavator over other excavation equipment.\n\nThe backfill blade is used for grading, leveling, backfilling, trenching, and general dozer work. The blade can be used to increase dump height and digging depth depending on its position in relation to the excavator's workgroup, this makes it very versatile.\n\nIn recent years, hydraulic excavator capabilities have expanded far beyond excavation tasks. With the advent of hydraulic powered attachments such as a tiltrotator, breaker, a grapple or an auger, the excavator is frequently used in many applications other than excavation and with the tiltrotator attachment, actually serves as an effective tool carrier. Many excavators feature quick coupler (quick-attach) mounting systems for simplified attachment mounting, dramatically increasing the machine's utilization on the jobsite.\n\nThere are two distinct classes of compact excavators, conventional tail swing - units that have a rear counterweight that will extend beyond the tracks when the house rotates, and zero-tail swing - units with a house whose diameter stays within the width of the tracks through full rotation. Zero-tail swing units allow operators to focus on digging and not looking at where they are swinging and are intended for operation in limited spaces, like next to a wall.\n\n"}
{"id": "4632770", "url": "https://en.wikipedia.org/wiki?curid=4632770", "title": "Conservation Reserve Program", "text": "Conservation Reserve Program\n\nThe Conservation Reserve Program (CRP) is a cost-share and rental payment program of the United States Department of Agriculture (USDA). Under the program, the government pays farmers to take certain agriculturally used croplands out of production and convert them to vegetative cover, such as cultivated or native bunchgrasses and grasslands, wildlife and pollinators food and shelter plantings, windbreak and shade trees, filter and buffer strips, grassed waterways, and riparian buffers. The purpose of the program is to reduce land erosion, improve water quality and effect wildlife benefits.\n\nThe program originally began in the 1950s as the conservation branch of the Soil Bank Program which was enacted by the Agriculture Act of 1956. The theory behind this branch of the Soil Bank Program was to focus on lands that were at high risk of erosion, remove them from agricultural production, and establish native or alternative permanent vegetative cover in an effort to counteract actual or potential erosion. This was considered by proponents to be beneficial to sustainable agriculture generally, by lessening the effects of erosion. Originally, the program called for three-year contracts in which the government would pay for land improvements that increased soil, water, forestry, or wildlife quality if the farmer would agree not to harvest or graze contracted land.\n\nAlthough the roots of the program were established in the 1950s, advocates did not start pushing the program heavily until the 1980s, in response to more prevalent practices of the 1970s, whereby farmers increasingly began to cultivate \"fence row to fence row\", and remove native habitat and vegetative stands from the fields, which was perceived as having detrimental effects on soil, water, and habitat quality [2]. Many programs would be established in the 1980s to address these issues. \n\nThe CRP has gone through many changes. Whenever there is a new proposed Farm Bill, the CRP is a large focus due to a high level of public pressure and the program's perceived benefits.\n\nThe Farm Bill of 1985 was the first act that officially established the CRP as we know it today. Many changes were made in this Farm Bill as compared to the regulations of the program set forth by the Agricultural Act of 1954. One of these changes was changing the contract lengths from the previous three-year commitment to anywhere from 10 to 15 years [7]. The rationale was that this would allow the new vegetative cover and other management practices more time to become established and produce the desired benefits. Also during the time of this Farm Bill, the amount of land allowed to be enrolled in the CRP, which had to be specified as \"highly erodible,\" rose from in 1986 to in 1990 [7]. Additionally, this Farm Bill allowed the Secretary of Agriculture to provide up to 50% of the cost to landowners for installing conservation measures [7]. This Bill also prohibited any farming or grazing on land that became enrolled into CRP, effectively removing any CRP land from agricultural production [7].\n\nThe Farm Bill of 1990 included a major change to the CRP by expanding the list of eligible lands to include marginal pasture lands converted to wetlands or established as wildlife habitat prior to enactment of the 1990 Farm Bill, marginal pasturelands to be devoted to trees in or near riparian areas, lands that the Secretary may determine causes an environmental threat to water quality, croplands converted to grassed waterways or strips as part of a conservation plan, croplands subject to an easement of the useful life of newly created wildlife habitat, shelterbelts, or filter strips devoted to trees or shrubs, and lands that pose an off-farm environmental threat or pose a threat of degradation of production due to soil salinity [7]. This large increase in the types of eligible lands allowed for lands that weren’t really \"highly erodible,\" yet not in production either, to be designated as beneficial to the environment around in the area.\n\nAlong with that alteration, there were other smaller differences between this Bill and the one previous. The Farm Bill of 1990 allowed limited fall and winter grazing on land enrolled in the CRP, it allowed \"alley cropping\" between hardwood tree stands in return for paying less money to the landowner, the 50% cost share was expanded from only vegetative cover to include hardwood trees, shelterbelts, windbreaks, and wildlife corridors, and the Bill allowed for land already in the CRP that had been converted to vegetative cover to be converted to hardwood trees, windbreaks, shelterbelts, wildlife corridors, or wetlands under certain circumstances [7]. This was a fundamental change to the Farm Bill program because it shifted the CRP's focus from agricultural conservation and production to more of a natural resource or general environmental conservation program.\n\nThe 1996 Farm Bill did not make many changes to the CRP. The major change made was in relation to early termination of CRP contracts. This bill allowed the termination of contracts after five years, with the exception of filterstrips, waterways, strips adjacent to riparian areas; lands with an erodibility index of over 15; and other lands that the Secretary deems to be highly sensitive [7]. Also, cropping history became a part of the eligibility system used to determine if a piece of land should be enrolled into the CRP. Land was deemed eligible for CRP if it was cropped two of the last five years and meets at least one of the following requirements: had an erodibility index of eight or higher; was considered a cropped wetland; was associated with or surrounds non-cropped wetlands; was devoted to a highly beneficial environmental practice (for example, filter strips); was subject to scour erosion; was located in national or state CRP conservation priority areas; or was marginal pastureland in riparian areas [7]. Other changes came in the form of appropriation bills. The Appropriations Bill of 2001 provided an exception to the requirement that the owner plant vegetative cover when excessive rainfall or flooding made planting impossible and the Appropriations Bill of 2002 created a new pilot program for enrollment of wetland and buffer acreage in CRP [7].\n\nThe changes made to the program in the 2002 Farm Bill changed the cropping history requirements from two out of the previous five years, to four out of the previous six [7]. This was done to make sure the land being enrolled in the program was actually cropland as opposed to land where somebody cropped it for two years just so it could be enrolled. The other change was done to increase the eligibility of lands. Under this Farm Bill, land under expiring contracts is automatically eligible to be considered for re-enrollment; contracts expiring during 2002 could be extended by one year; and existing covers must be retained, if feasible, when expiring contracts are re-enrolled [7].\n\nThe 2008 Farm Bill stated that alfalfa grown in approved crop rotation practice is to be considered an agricultural commodity and can be used to fulfill the requirement that eligible land be cropped in four out of six previous years, which had been challenged in the previous years [1]. Also for the first time, the bill allowed for management and cost-share incentives for thinning to improve condition of resources on the land containing trees, windbreaks, shelterbelts, and wildlife corridors [1].\n\nThere are four main agencies involved in the program, the United States Department of Agriculture (USDA), Commodity Credit Corporation (CCC), the Farm Service Agency (FSA) and the Natural Resources Conservation Service (NRCS). The USDA is the governing body that works to form the CRP with every Farm Bill and they are the top of the chain of command when it comes to the program. The CCC is the actual corporation, which is controlled by the USDA, that landowners enter into contracts with when enrolling in the program. FSA is the administrative body that runs the program for the USDA so that they don’t have to oversee every aspect of the program. NRCS is the technical agency which supports CRP by implementing it on private lands.\n\nEven though the CRP may be beneficial to many aspects of the environment, not every landowner can or might want to become involved in the program.\n\nThe main limiting factor in the amount of land allowed in the program is money. Each Farm Bill establishes the amount of money that will be budgeted for rental payments and cost share subsidies. With a limited amount of money available, the NRCS can only allow the most qualified land to be given incentive payments. There are very specific procedures and steps that are taken to decide whether or not land will be admitted to the program.\n\nMoreover, CRP is considered to be undesirable by some land owners because it prevents or minimizes the use of their land for agricultural production and by decreasing farm income.\n\nContinuous Sign-Up Contracts: Environmentally desirable land devoted to certain conservation practices may be enrolled in CRP at any time under continuous sign-up [10]. Provided the land and the landowner meet the requirements, offers are automatically accepted and not subject to competitive bidding [10].\n\nTo be eligible for CRP continuous sign-up enrollment, a landowner must have owned or operated the land for at least 12 months prior to submitting the offer, unless: the new landowner acquired the land due to the previous landowner’s death, ownership change occurred due to foreclosure where the owner exercised a timely right or redemption in accordance with state law, or the circumstances of the acquisition present adequate assurance to FSA that the new owner did not acquire the land for the purpose of placing it in CRP [10]. Landowners must be U.S. citizens or resident aliens and may not exceed certain income thresholds in order to be eligible for payments.\n\nTo be eligible for placement in General CRP, land must be recognized as \"cropland\" (including field margins) that is planted or considered planted to an agricultural commodity 4 of the previous 6 crop years from 1996 to 2001, and which is physically and legally capable of being planted in a normal manner to an agricultural commodity. For Continuous CRP the land must be recognized as \"marginal pastureland\" that is bordered to a stream, creek, river, sink-hole, and/or duck nest. Also, the land must be suitable for certain conservation practices; which are riparian buffers, wildlife habitat buffers, wetland buffers, filter strips, wetland restoration, grass waterways, shelterbelts, living snow fences, contour grass strips, salt tolerant vegetation, and shallow water areas for wildlife [10].\n\nThere are three main types of payments within CRP. They are rental payments, cost-share assistance, and financial incentives.\n\nLandowners receive rental payments from the FSA in return for creating long-term land cover. These payments depend on two main factors: The productivity of the soils in the area and the average dry land crop rent or cash rent equivalent [10]. However, the annual rental rate may not exceed the FSA’s maximum payment amount [10].\n\nFSA provides cost-share assistance to participants who establish approved cover on eligible cropland. The cost-share assistance can be an amount not more than 50 percent of the participants' costs in establishing approved practices [10].\n\nAs part of or on top of rental rate agreements, the FSA also adds on an additional financial incentive of up to 20% of the soil-rental rate for field windbreaks, grass waterways, filter strips, and riparian buffers [10]. Also, additional percentages could be added to the amount for maintenance and upkeep of the CRP areas [10].\n\nCRP benefits many native plants, animals and ecosystems. Even something as simple as a filter strip between an agricultural field and a stream can have a profound effect on the stability and health of the ecosystems and processes that occur in and around the field, as the greater environment. \n\nWhen CRP first started, the primary goal was to curb soil erosion due to agricultural practices. The effects of CRP on erosion are clearly visible today. CRP protects soil productivity by establishing conservation covers on at-risk land to reduce sheet, rill, and wind erosion. As of December 1, 2015, CRP has prevented more than 9 billion tons of soil from eroding.\n\nOne of the most important benefits provided by CRP is the improvement of water quality due to the reduction of erosion and runoff reaching water bodies. The more common runoff materials from agricultural lands included chemical fertilizers, nitrogen, phosphorus, and sediments which all have the potential to alter the environmental health of water bodies. Filter strips and buffer strips along the edge of agriculture fields can intercept the runoff materials and keep them from leaving the field. The Food and Agricultural Policy Research Institute (FAPRI) estimated 278 million pounds less nitrogen and 59 million pounds less phosphorus left fields in 2007 due to CRP, 95 and 86 percent reductions, respectively.\n\nCRP also benefits wildlife, most notably, birds. The most visible increase in bird population has occurred for native birds such as the Prairie Pothole Duck, Redhead (duck), Sage grouse, grassland birds, and a non-native migratory species, the ring-necked pheasants:\n\n\n"}
{"id": "22994507", "url": "https://en.wikipedia.org/wiki?curid=22994507", "title": "Direct electric action", "text": "Direct electric action\n\nDirect electric action is one of various systems used in pipe organs to control the flow of air (wind) into the organ's pipes when the corresponding keys or pedals are depressed. In direct electric action, the valves beneath the pipes are opened directly by electro-magnet solenoids, while with electro-pneumatic action, the electro-magnet's action admits air into a pneumatic or small bellows which in turn operates the pipe's valve.\n\nOther types of actions used in pipe organs are tracker action, using mechanical linkage of rods and levers, and tubular-pneumatic action which utilizes a change of pressure within lead tubing which connects the key to the valve pneumatic.\n\n"}
{"id": "6904022", "url": "https://en.wikipedia.org/wiki?curid=6904022", "title": "Document modelling", "text": "Document modelling\n\nDocument modelling looks at the inherent structure in documents. It looks not at the structure in formatting which is the classic realm of word-processing tools, but at the structure in content. Because document content is typically viewed as the \"ad hoc\" result of a creative process, the art of document modelling is still in its infancy. Most document modelling comes in the form of document templates evidenced most often as word-processing documents, fillable PDF forms, and XML templates. The particular strength of XML in this context is its ability to model document components in a tree-like structure, and its separation of content and style.\n\nDocument modelling goes beyond mere form-filling and mail-merge to look at the structure of information in, for example, a legal document, a contract, an inspection report, or some form of analysis.\n\nDocument modelling therefore looks at the structures and patterns of the written work, and breaks it down into different options or branches. It then labels the branches and the results. Without effective document modelling, it is difficult to get full value from a document automation initiative, for example, using document assembly software. But by using a model that contains hundreds and thousands of branches, a user can create close to infinite structured variations almost to the point that such systems can rival the unstructured drafting of a specialist. In fact, the results of a sophisticated document model can surpass those of the specialist in terms of lack of error and consistency of prose.\n\n"}
{"id": "20149404", "url": "https://en.wikipedia.org/wiki?curid=20149404", "title": "Dormakaba", "text": "Dormakaba\n\ndormakaba Holding AG (trading as dormakaba) is a global security group based in Rümlang, Switzerland, and employs around 16,000 staff in over 60 countries. It formed as the result of a merger between Kaba Group and Dorma in September 2015 and is publicly traded on the SIX Swiss Exchange.\n\nThe company was founded in 1862 in Zurich by Franz Bauer as a locksmith shop and cash register factory. The firm was sold in 1915 to Leo Bodmer, who renamed the company to Bauer AG. In 1934, the inventor Fritz Schori created the first cylinder lock with reversible key. Bauer AG patented the invention and named the lock after the firm's founder Franz Bauer, known in German as 'Kassenbauer' (maker of cash registers) – in short 'Kaba'.\n\nOver the years, the company gradually expanded its portfolio and established itself throughout Europe. In 1995, Bauer Holding AG was renamed Kaba Holding AG and the company's shares were listed on the Zurich stock exchange.\n\nWith the takeover of Unican Security Systems (Montreal) in 2001, Kaba gained a foothold in North America and added the Silca, Ilco and Kaba Mas brands as part of the merger\n\nIn 2006, the company acquired the Wah Yuet Group in China, the US company Computerized Security Systems (CSS) with its Saflok and La Gard brands, and the Dutch company H. Cillekens Zn. B.V. In addition, various joint ventures with the Indian Minda Group were entered.\n\nAt Security 2008, Kaba was awarded the “Security Innovation Award” in gold for its newly developed identification technology RCID.\n\nIn 2011, Kaba sold the Business Segment Door Automation (Kaba Gilgen AG, formerly Gilgen AG) to the Japanese Group.\n\nIn April 2015, Kaba announced a merger with German-based Dorma Holding. The new company is styled dormakaba and will be operationally based in Rümlang, Switzerland where the headquarters of Kaba Group are located.\n\nIn June 2017, Dormakaba announced their new Quantum Pixel electronic door lock at HITEC 2017 which is the world's largest hospitality technology event.\n\nThe Kaba Group produces components and systems for securing and organizing access as well as for recording enterprise and personnel data. The company operates with four business segments - Access + Data Systems EMEA/AP, Access + Data Systems Americas, Industrial Locks and Key Systems - providing the following groups of products:\n\n"}
{"id": "41920098", "url": "https://en.wikipedia.org/wiki?curid=41920098", "title": "ENIAC Day", "text": "ENIAC Day\n\nENIAC Day or the World’s First Computer Day is celebrated on 15 February.\nOn February 10, 2011, the City of Philadelphia officially declared that February 15, 2011 - the 65th anniversary of the unveiling of the Electronic Numerical Integrator and Computer (ENIAC), the world's first general-purpose electronic computer, developed at the University of Pennsylvania's Moore School of Electrical Engineering - would that year and henceforth be known as ENIAC Day.\n"}
{"id": "47231848", "url": "https://en.wikipedia.org/wiki?curid=47231848", "title": "Ente Nazionale Italiano di Unificazione", "text": "Ente Nazionale Italiano di Unificazione\n\nEnte Nazionale Italiano di Unificazione (Italian National Unification, acronym UNI) is a private non-profit association that performs regulatory activities in Italy across industrial, commercial, and service sectors, with the exception of electrical engineering and electronic competence of CEI .\n\nThe UNI is recognized by the Italian State and by the European Union, and represents Italian legislative activity at the International Standards Organization (ISO) and European Committee for Standardization (CEN).\n\nPiero Torretta is the president of the organization.\nThe UNI was formed in 1921 with the initials \"UNIM\" in the face of demands for standardization of mechanical engineering at the time. At the 1928 General Confederation of Italian Industry (Confindustria) it was promoted to include all sectors of industry, becoming the current UNI.\n\nThe main tasks of UNI are:\n\n\nThe UNI also avails of federated entities for specific fields of competence. Between them in the field of standardization in the field of computer science is relevant UNINFO, which is the UNI, the areas of competence, at the ISO, the ISO / IEC JTC1 (ISO / IEC Joint Technical Committee) and the CEN.\n\nOther federated entity UNI is the CTI, Italian Committee thermo energy and environment, which deals with the legislative activity in the areas of heating technology and the production and utilization of thermal energy, both nationally and internationally, where he, on behalf of UNI, the work of numerous groups CEN and ISO detaining, for some of these, the relative answering techniques.\n\nThe most important federated entity UNI is today the Italian Gas Committee (CIG) that deals with regulation of the sector gas, nationally and internationally and cooperates with many Italian and European institutions in the same sector. Currently this committee is working on several initiatives with strong impact on national energy issues, such as the introduction of biomethane in transport networks and distribution of natural gas, the European project on the quality of gas, the project of smart meters for natural gas.\n\nThe published standards can significantly affect the safety of workers, the public, or the environment, that the government refer to them recalling them in documents legislative and attributing to them some level of cogency.\n\nThe designation of a UNI standard shows its origin (# denotes a number):\n\n"}
{"id": "5731761", "url": "https://en.wikipedia.org/wiki?curid=5731761", "title": "Flow blue", "text": "Flow blue\n\nFlow blue (occasionally 'flown blue') is a style of white earthenware, sometimes porcelain, that originated in the Regency era, sometime in the 1820s, among the Staffordshire potters of England. The name is derived from the blue glaze that blurred or \"flowed\" during the firing process.\nMost flow blue ware is a kind of transferware, where the decorative patterns were applied with a paper stencil to often white-glazed \"blanks\", or standard pottery shapes, though some wares were hand painted. The stencils burned away in the kiln. The blue glazes used in flow blue range from gray-blue to sometimes greenish blue, to an inky blue; however the most desirable and sought-after shade is a vivid cobalt blue. Mulberry is another form of flow blue, where the glaze is more purple in hue.\n\n"}
{"id": "43890317", "url": "https://en.wikipedia.org/wiki?curid=43890317", "title": "Fog computing", "text": "Fog computing\n\nFog computing or fog networking, also known as fogging, is an architecture that uses edge devices to carry out a substantial amount of computation, storage, communication locally and routed over the internet backbone. \n\nFog computing can be perceived both in large cloud systems and big data structures, making reference to the growing difficulties in accessing information objectively. This results in a lack of quality of the obtained content. The effects of fog computing on cloud computing and big data systems may vary. However, a common aspect is a limitation in accurate content distribution, an issue that has been tackled with the creation of metrics that attempt to improve accuracy.\n\nFog networking consists of a control plane and a data plane. For example, on the data plane, fog computing enables computing services to reside at the edge of the network as opposed to servers in a data-center. Compared to cloud computing, fog computing emphasizes proximity to end-users and client objectives, dense geographical distribution and local resource pooling, latency reduction and backbone bandwidth savings to achieve better quality of service (QoS) and edge analytics/stream mining, resulting in superior user-experience and redundancy in case of failure while it is also able to be used in Assisted Living scenarios.\n\nFog networking supports the Internet of Things (IoT) concept, in which most of the devices used by humans on a daily basis will be connected to each other. Examples include phones, wearable health monitoring devices, connected vehicle and augmented reality using devices such as the Google Glass.\n\nSPAWAR, a division of the US Navy, is prototyping and testing a scalable, secure Disruption Tolerant Mesh Network to protect strategic military assets, both stationary and mobile. Machine control applications, running on the mesh nodes, \"take over\", when internet connectivity is lost. Use cases include Internet of Things e.g. smart drone swarms.\n\nISO/IEC 20248 provides a method whereby the data of objects identified by edge computing using Automated Identification Data Carriers [AIDC], a barcode and/or RFID tag, can be read, interpreted, verified and made available into the \"Fog\" and on the \"Edge,\" even when the AIDC tag has moved on.\nIn 2012, the need to extend cloud computing with fog computing emerged, in order to cope with huge number of IoT devices and big data volumes for real-time low-latency applications.\n\nOn November 19, 2015, Cisco Systems, ARM Holdings, Dell, Intel, Microsoft, and Princeton University, founded the OpenFog Consortium, to promote interests and development in fog computing. Cisco Sr. Managing-Director Helder Antunes became the consortium's first chairman and Intel's Chief IoT Strategist Jeff Fedders became its first president.\n\nBoth cloud computing and fog computing provide storage, applications, and data to end-users. However, fog computing has a closer proximity to end-users and bigger geographical distribution.\n\nCloud Computing – the practice of using a network of remote servers hosted on the Internet to store, manage, and process data, rather than a local server or a personal computer. Cloud Computing can be a heavyweight and dense form of computing power.\n\nFog computing – a term created by Cisco that refers to extending cloud computing to the edge of an enterprise's network. Also known as Edge Computing or fogging, fog computing facilitates the operation of compute, storage, and networking services between end devices and cloud computing data centers. While edge computing is typically referred to the location where services are instantiated, fog computing implies distribution of the communication, computation, and storage resources and services on or close to devices and systems in the control of end-users. Fog computing is a medium weight and intermediate level of computing power. Rather than a substitute, fog computing often serves as a complement to cloud computing. \n\nMist computing – a lightweight and rudimentary form of computing power that resides directly within the network fabric at the extreme edge of the network fabric using microcomputers and microcontrollers to feed into Fog Computing nodes and potentially onward towards the Cloud Computing platforms.\n\nNational Institute of Standards and Technology in March, 2018 released a definition of the Fog computing adopting much of Cisco's commercial terminology as NIST Special Publication 500-325, \"Fog Computing Conceptual Model\" that defines Fog computing as an horizontal, physical or virtual resource paradigm that resides between smart end-devices and traditional cloud computing or data center. This paradigm supports vertically-isolated, latency-sensitive applications by providing ubiquitous, scalable, layered, federated, and distributed computing, storage, and network connectivity. Thus Fog Computing is most distinguished by distance from the Edge. Fog Computing is physically and functionally intermediate between Edge nodes and centralized Clouds. Much of the terminology is not defined including key architectural terms like \"smart\" and the distinction between Fog Computing from Edge Computing does not have generally agreed acceptance.\n"}
{"id": "431773", "url": "https://en.wikipedia.org/wiki?curid=431773", "title": "Funnel", "text": "Funnel\n\nA funnel is a tube or pipe that is wide at the top and narrow at the bottom, used for guiding liquid or powder into a small opening.\n\nFunnels are usually made of stainless steel, aluminium, glass, or plastic. The material used in its construction should be sturdy enough to withstand the weight of the substance being transferred, and it should not react with the substance. For this reason, stainless steel or glass are useful in transferring diesel, while plastic funnels are useful in the kitchen. Sometimes disposable paper funnels are used in cases where it would be difficult to adequately clean the funnel afterwards (for example, in adding motor oil to a car). Dropper funnels, also called dropping funnels or tap funnels, have a tap to allow the controlled release of a liquid. A flat funnel, made of polypropylene, utilises living hinges and flexible walls to fold flat.\n\nThe term \"funnel\" may refer to the chimney or smokestack on a steam locomotive and commonly refers to the same on a ship. The term \"funnel\" is also applied to other seemingly strange objects like a smoking pipe or a kitchen bin.\n\nThere are many different kinds of funnels that have been adapted for specialized applications in the laboratory. Filter funnels, thistle funnels (shaped like thistle flowers), and dropping funnels have stopcocks which allow the fluids to be added to a flask slowly. For solids, a powder funnel with a wide and short stem is more appropriate as it does not clog easily.\n\nWhen we use with filter paper, filter bunnels, Büchner and Hirsch funnels can be used to remove fine particles from a liquid in a process called filtration. For more demanding applications, the filter paper in the latter two may be replaced with a sintered glass frit. Separatory funnels are used in liquid-liquid extractions. The Tullgren funnel is used to collect arthropods from plant litter or similar material.\n\nGlass is the material of choice for laboratory applications due to its inertness compared with metals or plastics. However, plastic funnels made of nonreactive polyethylene are used for transferring aqueous solutions. Plastic is most often used for powder funnels that do not come into contact with solvent in normal use.\n\n\nThe inverted funnel is a symbol of madness. It appears in many Medieval depictions of the mad; for example, in Hieronymus Bosch's \"Ship of Fools\" and \"Allegory of Gluttony and Lust\". The Cebuano word for inverted funnel is \"\"; such devices are sometimes used as timers.\n\nIn popular culture, the Tin Woodman in L. Frank Baum's novel \"The Wonderful Wizard of Oz\" (and in most dramatizations of it) uses an inverted funnel for a hat, though that is never specifically mentioned in the story—it originated in W. W. Denslow's original illustrations for the book.\n\nIn the East Coast of the United States, \"beer funnel\" is another term for \"beer bong\". \"Funneling\" a beer involves pouring an entire beer into a funnel attached to a tube, in which a person then consumes the beer via the tube.\n\nIn the computing world, a funnel is frequently used as the icon for the filter functionality.\n\n\n<br>\n"}
{"id": "47894108", "url": "https://en.wikipedia.org/wiki?curid=47894108", "title": "Gas carbon", "text": "Gas carbon\n\nGas carbon is a carbon that is obtained when the destructive distillation of coal is done.\n"}
{"id": "36545274", "url": "https://en.wikipedia.org/wiki?curid=36545274", "title": "Geeks Without Bounds", "text": "Geeks Without Bounds\n\nGeeks Without Bounds (informally known as GWOB) is a humanitarian organization of technologists, first responders, policymakers, and volunteers that work towards improving access to communication and technology. With a focus on working with communities that have limited infrastructure due to violence, negligence, or catastrophe, they organize hackathons for humanitarian technology, and help prototype projects intended to turn into long-term initiatives through their Accelerator for Humanitarian Projects.\n\nGeeks Without Bounds was initially announced on August 19, 2010 at Gnomedex 10 in Seattle, Washington, and formally launched on October 10, 2010. by Johnny \"Diggz\" Higgins, and Willow Brugh as a fiscally sponsored program of The School Factory. In 2012, GWOB became a separate entity, fiscally sponsored by The School Factory.\n\nGeeks Without Bounds is a non-profit organization, and is primarily volunteer driven. Operational activities are coordinated through a six person board of directors which interact directly with sponsors and organizations around the world. They have organized hackathons, and networks of supportive hackerspaces in a variety of cities since 2010, and have partnered with the Random Hacks of Kindness project.\n\nGeeks Without Bounds is fiscally sponsored by Mentor House, an organization based in Tacoma, Washington. GWOB was originally founded as a project of School Factory and then spun out as its own organization under School Factory's fiscal sponsorship until that organization closed its doors in 2017.\n\nPartners include Random Hacks of Kindness, NetHope, Startup World, and SoftLayer.\n\nIndividual event sponsors and partners have included Hewlett-Packard, The Next Web, AT&T, and the International Space Apps Challenge.\n\nOrganizational sponsors, along with donations from individuals, and grants fund the operational activities of Geeks Without Bounds.\n\nIn order to maintain transparency in accounting, income and expenses are provided to the public and published online.\n\nMuch of their work has focused on providing humanitarian aid in areas recovering from natural disasters and similar crises, working with STAR-TIDES and Crisis Commons. They have an annual application cycle for projects that want to join the Accelerator for Humanitarian Initiatives.\n\nGeeks Without Bounds has organized various humanitarian hackathons.\n\nGWOB assisted with setup and maintenance of electrical generation and distribution, as well as local wireless mesh networking and connection to the Lakota-owned Internet point of presence.\n\nIn Tanzania, GWOB helped launch a system by which local communities were able to directly report problems in the water infrastructure to the responsible agencies and receive ETA and other responsive information.\n"}
{"id": "9328646", "url": "https://en.wikipedia.org/wiki?curid=9328646", "title": "Grab bar", "text": "Grab bar\n\nGrab bars are safety devices designed to enable a person to maintain balance, lessen fatigue while standing, hold some of their weight while maneuvering, or have something to grab onto in case of a slip or fall. A caregiver may use a grab bar to assist with transferring a patient from one place to another. A worker may use a grab bar to hold on to as he or she climbs, or in case of a fall.\n\nGrab bars must bear high loads and sudden impacts, and most jurisdictions have building regulations specifying what loads they must bear. They are generally mounted to masonry walls or to the studs of stud walls (which may need to be specially strengthened). They can be mounted through drywall into a strong wooden wall stud or other structural member, but not mounted only on the drywall, as it will not bear the users' weight.\n\nGrab bars are made of metal, plastic, fiberglass, and composites. For wet areas such as bathrooms, the material must be waterproof. Stainless steel, nylon-coated mild steel, epoxy-coated aluminum, ABS plastic, and even vinyl-coated metal and plastic.\n\nGrab bars increase accessibility and safety for people with a variety of disabilities or mobility difficulties. Although they are most commonly seen in public handicapped toilet stalls, grab bars are also used in private homes, assisted living facilities, hospitals, and nursing homes. Grab bars are most commonly installed next to a toilet or in a shower or bath enclosure.\n\nSome grab bars also have a light feature and double as a night light offering up a little more safety at night when using the bathroom.\n\nMany jurisdictions have regulations on grab bar placement and floorplans for public bathrooms (American ADA, British Doc M regs).\n\nGrab bars are often used in conjunction with other medical devices to increase safety. For example, a grab bar added to a shower is frequently used with a shower chair and hand held shower head. Grab bars installed by a doorway are usually added near a railing. In addition, grab bars can be placed on any wall where extra support is needed even if it is not the \"usual place\" they are used.\nGrab bars can be installed in different positions:\n\n\nThere are many considerations when deciding which grab bar to use and how best to install it. Properly securing a grab bar is important so that it doesn't pull out of the wall when pressure is applied to it. Each installation should be properly secured into wall blocking or studs to provide the best support. If no studs are available, specialized mollies can be used to spread out grab forces across a wider area of the wall.\n\nThe Americans with Disabilities Act of 1990 \"Accessibility Guidelines for Buildings and Facilities\" (ADAAG) defines requirements for installing grab bars in public bathing and toileting facilities. The guidelines are supported by substantial research regarding the best placement of grab bars.\n\nThe following is a subset of ADA grab bar guidelines:\n\n\nWhile the ADA guidelines provide specifics on the placement of grab bars in public locations, they do not require a specific style. The British Doc M regulations specify a minimum contrast between bars and background. Many public facilities opt for the cheapest grab bars, which usually have an institutional look. However, grab bars are actually available in many styles, finishes and colors. Manufacturers have begun to understand the need to blend in with home decor, offering grab bars that have style and pizazz. For the home, grab bars do not need to be ADA compliant, but those guidelines should be considered. In addition to straight grab bars, there are fold-out bars, those that clamp onto the side of the bathtub, L-shaped, U-shaped and corner grab bars. Grab bars are also made with built in LED lighting and can come in many different colours.\n\nGrab bars in industry and construction are found on equipment or above fixed ladders where footholds exist but other handholds are lacking. They may be positioned horizontally, vertically, or at an angle.\n\nWhen using grab bars as safety devices in order to prevent falls, your best choice would be a horizontal bar. Scientific research has found that gripping strength is far greater using a horizontal bar than a vertical bar in a fall situation. This makes horizontal grab bars the safest choice.\n\nOccupational Safety and Health Administration (OSHA) guidelines describe the requirements for grab bar clearance, diameter and spacing on fixed ladders. These regulations state that the clearance in the back of grab bars must be at least 4 inches, the diameter similar to the ladder rungs and, when horizontal, grab bars must be spaced by a continuation of the rung spacing. In 2008-2009 alone, the USDOL Bureau of Labor Statistics reported 241 casualties from ladder falls.\n\nSiderail extensions horizontal grab bars may be bolted or welded to fixed ladders. Grab bars may be mounted to the curb for access to rooftops and rooftop hatches.\n\n"}
{"id": "1451319", "url": "https://en.wikipedia.org/wiki?curid=1451319", "title": "Herb Sutter", "text": "Herb Sutter\n\nHerb Sutter is a prominent C++ expert. He is also a book author and was a columnist for Dr. Dobb's Journal. He joined Microsoft in 2002 as a platform evangelist for Visual C++ .NET, rising to lead software architect for C++/CLI. Sutter served as secretary and convener of the ISO C++ standards committee for over 10 years. In September 2008 he was replaced by P. J. Plauger. He then re-assumed the convener position, after Plauger resigned in October 2009. In recent years Sutter was lead designer for C++/CX and C++ AMP.\n\nSutter was born and raised in Oakville, Ontario, before studying computer science at Canada's University of Waterloo.\n\nFrom 1995 to 2001 he was chief technology officer at PeerDirect where he designed the PeerDirect database replication engine.\n\nFrom 1997 to 2003, Sutter regularly created C++ programming problems and posted them on the Usenet newsgroup comp.lang.c++.moderated, under the title \"Guru of the Week\". The problems generally addressed common misconceptions or poorly understood concepts in C++. Sutter later published expanded versions of many of the problems in his first two books, \"Exceptional C++\" and \"More Exceptional C++\". New articles, mostly related to C++11, were published since November 2011.\n\n\"The Free Lunch Is Over\" is an article from Herb Sutter published in 2005. It stated that microprocessor serial-processing speed is reaching a physical limit, which leads to two main consequences:\n\n\n"}
{"id": "6402491", "url": "https://en.wikipedia.org/wiki?curid=6402491", "title": "Hydrogen compressor", "text": "Hydrogen compressor\n\nA hydrogen compressor is a device that increases the pressure of hydrogen by reducing its volume resulting in compressed hydrogen or liquid hydrogen.\n\nHydrogen compressors are closely related to hydrogen pumps and gas compressors: both increase the pressure on a fluid and both can transport the fluid through a pipe. As gases are compressible, the compressor also reduces the volume of hydrogen gas, whereas the main result of a pump raising the pressure of a liquid is to allow the liquid hydrogen to be transported elsewhere.\n\nA proven method to compress Hydrogen is to apply reciprocating piston compressors. Widely used in refineries, they are the backbone of refining crude oil. Reciprocating piston compressors are commonly available as either oil-lubricated or non-lubricated; for high pressure (350 - 700 bar), non-lubricated compressors are preferred to avoid oil contamination of the Hydrogen. Expert know-how on piston sealing and packing rings can ensure that reciprocating compressors outperform the competing technologies in terms of MTBO (Mean Time Between Overhaul).\n\nAn ionic liquid piston compressor is a hydrogen compressor based on an ionic liquid piston instead of a metal piston as in a piston-metal diaphragm compressor.\n\nA multi-stage electrochemical hydrogen compressor incorporates a series of membrane-electrode-assemblies (MEAs), similar to those used in proton exchange membrane fuel cells; this type of compressor has no moving parts and is compact. The electrochemical compressor works similar to a fuel cell, a voltage is applied to the membrane and the resulting electric current pulls hydrogen through the membrane. With electrochemical compression of hydrogen, a pressure of 14500 psi (1000bar or 100MPa) is achieved. A patent is pending claiming an exergy efficiency of 70 to 80% for pressures up to 10,000 psi or 700 bars. A single stage electrochemical compression to 800 bar was reported in 2011. DOE has supported developments related to developing low cost electrochemical hydrogen compressors for heat pumps with Xergy Inc.</ref.>\n\nIn a hydride compressor, thermal and pressure properties of a hydride are used to absorb low-pressure hydrogen gas at ambient temperatures and then release high-pressure hydrogen gas at higher temperatures; the bed of hydride is heated with hot water or an electric coil.\n\nPiston-metal diaphragm compressors are stationary high-pressure compressors, four-staged water-cooled, 11–15 kW, 30–50 Nm3/h 40 MPa for dispensation of hydrogen. Since compression generates heat, the compressed gas is to be cooled between stages making the compression less adiabatic and more isothermal. The default assumption on diaphragm hydrogen compressors is an adiabatic efficiency of 70%. Used in hydrogen stations.\n\nThe guided rotor compressor (GRC) is a positive-displacement rotary compressor based upon an envoluted trochoid geometry which utilizes a parallel trochoid curve to define its basic compression volume. It has a typical 80 to 85% adiabatic efficiency.\n\nThe single-piston linear compressor uses dynamic counterbalancing, where an auxiliary movable mass is flexibly attached to a movable piston assembly and to the stationary compressor casing using auxiliary mechanical springs with zero vibration export at minimum electrical power and current consumed by the motor. It is used in cryogenics\n\n\n"}
{"id": "21321771", "url": "https://en.wikipedia.org/wiki?curid=21321771", "title": "ISO/IEC 4909", "text": "ISO/IEC 4909\n\nISO/IEC 4909:2006 establishes specifications for financial transaction cards using track 3 and is intended to permit interchange based on the use of magnetic stripe encoded information. It specifies the data content and physical location of read/write information on track 3 and is to be used in conjunction with the relevant parts of ISO/IEC 7811 and ISO/IEC 7812.\n\nISO/IEC 4909:2006 recognizes the need for formats of track 3 which can be used independently of, or in conjunction with, track 2 as defined in ISO/IEC 7813. This approach is intended to permit the greatest degree of flexibility within the financial community in facilitating international interchange.\n\nUsing track 3 in conjunction with track 2 is a mode of operation in both on-line and off-line interchange environments. This mode of operation requires that the original encoded data on track 2 be read; the data on track 3 be read; and, if update is required, all the data on track 3 be rewritten.\n\nIndependent use of track 3 is an alternative mode of operation permitting both on-line interchange and off-line interchange based on mutual agreement between interested parties. It requires reading only of the data on track 3 and, if update is required, the rewriting of all the data on track 3.\n\n"}
{"id": "5458387", "url": "https://en.wikipedia.org/wiki?curid=5458387", "title": "Illinois Technology and Research Corridor", "text": "Illinois Technology and Research Corridor\n\nThe Illinois Technology and Research Corridor is a region of commerce and industry located along Interstate 88 in the Chicago metropolitan area, primarily in DuPage, Kane, and DeKalb Counties. The corridor is home to the headquarters or regional centers for many Fortune 1000 companies (including many specializing in research, development, logistics, and technology), several office and industrial parks, colleges and universities, research and scientific institutions, medical centers, government centers, and abundant shopping, dining, lodging, and entertainment amenities. In addition to the I-90 Golden Corridor, the I-94 Lakeshore Corridor, and the I-55 Industrial Corridor, the Illinois Technology and Research Corridor is one of the principal economic centers in suburban Chicago.\n\nCities and villages located partially or wholly within the scope of the Illinois Technology and Research Corridor include:\n\nMajor companies with a strong presence in the Illinois Technology and Research Corridor include:\n\n\n\n\n\n\n"}
{"id": "10240863", "url": "https://en.wikipedia.org/wiki?curid=10240863", "title": "Improvement Support Systems", "text": "Improvement Support Systems\n\nImprovement Support Systems (ISS) are computerized or electronic specialized systems that help to improve processes, resources, products, businesses, and so on.\n\nExamples of ISS are systems that help to improve:\n"}
{"id": "271180", "url": "https://en.wikipedia.org/wiki?curid=271180", "title": "LBCAST", "text": "LBCAST\n\nLBCAST (lateral buried charge accumulator and sensing transistor array) is a type of photo sensor which the manufacturer claims is simpler and thus smaller and faster than CMOS sensors. It was developed over ten years by Nikon, in parallel with other manufacturer's development of CMOS, and resulted in shipping product in 2003.\n\nBoth CMOS and LBCAST technologies branched from researchers discussions of \"amplifying sensors\" as a way to develop an imaging sensor with lower power requirements than the already-existing CCD sensor technology, for use in portable devices such as DSLR cameras.\n\nFrom the Nikon Website:\n\nThe main differences between LBCAST and CMOS-based sensors appear to be those given below:\n\n\n\nAs at end 2006, the Nikon D2H and the D2Hs were the only publicly available cameras known to carry the sensor. Nikon has opted to use CCDs sourced from Sony in most of their low and mid range cameras and used a CMOS sensor in the flagship D2Xs/D2x. The LBCAST sensor in the D2Hs has remained at 4.1MP.\n\nWith the advent of the D3, D700 and D300 cameras in 2007 and 2008, all featuring CMOS sensor technology, it is unknown whether LBCAST plays a part in the design of the CMOS sensor of either, since Nikon's implementation of LBCAST is an adaptation of CMOS, and it is therefore technically correct to refer to the known instances of LBCAST as CMOS, Nikon has not been forthcoming to requests for specific information on the D3 sensor and Nikon have claimed in the past that LBCAST would be further developed.\n\nThe following weaknesses have been cited as affecting the Nikon D2H, although whether these issues have origins specific to LBCAST, and whether LBCAST necessitates these problems, is not known.\n\n"}
{"id": "33324707", "url": "https://en.wikipedia.org/wiki?curid=33324707", "title": "Laminar flame speed", "text": "Laminar flame speed\n\nLaminar flame speed is an intrinsic characteristic of premixed combustible mixtures. It is the speed at which an un-stretched laminar flame will propagate through a quiescent mixture of unburned reactants. Laminar flame speed is given the symbol \"s\". According to the thermal flame theory of Mallard and Le Chatelier, the un-stretched laminar flame speed is dependent on only three properties of a chemical mixture: the thermal diffusivity of the mixture, the reaction rate of the mixture and the temperature through the flame zone:\n\nformula_1\n\nformula_2 is thermal diffusivity,\n\nformula_3 is reaction rate,\n\nand the temperature subscript u is for unburned, b is for burned and i is for ignition temperature.\n\nLaminar flame speed is a property of the mixture (fuel structure, stoichiometry) and thermodynamic conditions upon mixture ignition (pressure, temperature). Turbulent flame speed is a function of the aforementioned parameters, but also heavily depends on the flow field. As flow velocity increases and turbulence is introduced, a flame will begin to wrinkle, then corrugate and eventually the flame front will be broken and transport properties will be enhanced by turbulent eddies in the flame zone. As a result, the flame front of a turbulent flame will propagate at a speed that is not only a function of the mixture's chemical and transport properties but also properties of the flow and turbulence.\n\n"}
{"id": "1811469", "url": "https://en.wikipedia.org/wiki?curid=1811469", "title": "LonTalk", "text": "LonTalk\n\nLonTalk is a protocol optimized for control. Originally developed by Echelon Corporation for networking devices over media such as twisted pair, powerlines, fiber optics, and RF. It is popular for the automation of various functions in industrial control, home automation, transportation, and buildings systems such as lighting and HVAC; see Intelligent building, the protocol has now been adopted as an open international control networking standard in the ISO/IEC 14908 family of standards. Published through ISO/IEC JTC 1/SC 6, this standard specifies a multi-purpose control network protocol stack optimized for smart grid, smart building, and smart city applications..\n\nLonTalk is part of the technology platform called LonWorks.\n\nThe protocol is defined by ISO/IEC 14908.1 and published by ISO/IEC JTC 1/SC 6. The LonTalk protocol has also been ratified by standards setting bodies in the following industries & regions:\n\n\nThe protocol is only available from the official distribution organizations of each regional standards body or in the form of microprocessors manufactured by companies that have ported the standard to their respective chip designs.\n\nAn April 2015 cryptanalysis paper claims to have found serious security flaws in the OMA Digest algorithm of the Open Smart Grid Protocol, which itself is built on the same EN 14908 foundations as LonTalk. The authors speculate that \"every other LonTalk-derived standard\" is similarly vulnerable to the key-recovery attacks described.\n\n\n"}
{"id": "39179456", "url": "https://en.wikipedia.org/wiki?curid=39179456", "title": "Magnetic spin vortex disc", "text": "Magnetic spin vortex disc\n\nMagnetic material synthesis and characterization technology continue to improve, allowing for the production of various shapes, sizes, and compositions of magnetic material to be studied and tuned for improved properties. One of the places which has seen great advancement is in the synthesis of magnetic materials at nanometer length scales. Pedro Alexandre Lino Silva made 'experimental proof of magnetic vortex'. Nanoparticle research has seen a great deal of interest in a number of fields as many phenomena can be explained by what is occurring on the nanoscale, which can be probed more effectively using nanometer sized materials. One unique type of materials which have seen a recent surge in research interest have been known as \"nanoflakes\" where they resemble flakes or discs of nanometer thickness and micrometer dimensions. Nanomaterials of this shape have seen use in a number of fields including energy storage, as [electrodes] of electrochemical cells, and in cancer therapy to kill cancer cells.\n\n"}
{"id": "11435382", "url": "https://en.wikipedia.org/wiki?curid=11435382", "title": "Mahape", "text": "Mahape\n\nMahape is a suburb of Navi Mumbai located near Ghansoli. The nearby railway stations are Ghansoli and Koparkhairane.\n"}
{"id": "32546325", "url": "https://en.wikipedia.org/wiki?curid=32546325", "title": "Makani Power", "text": "Makani Power\n\nMakani Power is an Alameda, California-based company that developed airborne wind turbines with the support of Google X and the U.S. Department of Energy office of ARPA-E. Makani is a leader in the development of airborne wind power extraction systems.\nMakani was founded in 2006 by Saul Griffith, Don Montague, and Corwin Hardham. It received funding as part of Google.org's Renewable Energy cheaper than Coal (RE<C) initiative. \"Makani\" is Hawaiian for \"wind.\" One of the founders, Corwin Hardham, died in 2012 at age 38. On May 23, 2013, Makani Power was acquired by Google and was folded into Google X.\n\nIn order to meet its goal of producing low-cost renewable energy, the Makani kite-energy system uses autonomous tethered wings which fly in a circular path and generate electricity via wind turbines mounted upon the main wing, a method already in public domain; expert Miles Loyd in 1980 stated that for large scale purposes flying the generators was expected to be disfavored because of the need to fly the mass of the generators; many of Makani Power competitors have generators kept on the ground, like KiteGen, Italy. The electricity is transmitted to the ground via an electrical cable within the kite's tether. Several patent applications have been made; some have been granted.\n\nIn December 2016, Makani operated for the first time a 600 kW prototype with 28 meter wing span.\n\n\n"}
{"id": "19080817", "url": "https://en.wikipedia.org/wiki?curid=19080817", "title": "Micromixer", "text": "Micromixer\n\nIn mechanics, a micromixer is a device based on mechanical microparts used to mix fluids. This device represents a key technology to fields such as chemical industry, pharmaceutical industry, analytical chemistry, biochemical analysis, and high-throughput synthesis, since it makes use of the miniaturization of the fluids associated in the mixing to reduce quantities involved in the chemical and/or biochemical processes.\n\nThere are two types of micromixers: passive and active.\n\nThe active ones use an external energy source, which can be electric or magnetic, to perform the mixing of the fluids. In the passive ones, there is no power source employed and basically, pressure guides the flow of fluids.\n\n"}
{"id": "32379819", "url": "https://en.wikipedia.org/wiki?curid=32379819", "title": "Mäurer &amp; Wirtz", "text": "Mäurer &amp; Wirtz\n\nMäurer & Wirtz is a German manufacturer of personal care products and perfumes. Since 1990 the company has been an independent subsidiary of Dalli Group.\n\nThe company is managed by the fifth generation of the Wirtz family, joint CEO Hermann Wirtz.\n\nThe headquarters and production site is in Stolberg near the city of Aachen with 400 employees. The company sells a range of consumer products using the sales divisions Cosmeurop Perfumes, Théany Cosmetics, NewYorker Cosmetics, s.Oliver Cosmetics and comma Cosmetics. Besides its own brands, such as Betty Barclay and 4711, the company produces licensed brands, such as s.Oliver and Otto Kern.\n\nThe main markets are Germany, Benelux, Austria, Switzerland and exports to 135 countries. The company's products are distributed and sold through perfumery and drugstore retailers, department stores, drug stores and selected supermarkets.\n\nThe history of the family business goes back to Michael Mäurer and his stepson Andreas August Wirtz, who established a soapworks in 1845 in Stolberg. Initially, only soft soaps, curd soaps and fine soaps were sold in a grocery store for the local market. Over time, the products were established in the Rhineland and later also in neighboring countries (mainly France and Benelux). In 1884 the production of washing powder began and then around the turn of the century the company's first trademarks were registered. Mäurer & Wirtz now produces detergents, soap products, perfumes and cosmetics. Since 1992, products under the \"Betty Barclay\" brand have also been produced under license. In 2007, the company took over the brands \"4711\", \"Tosca\", \"Sir Irisch Moos\" and \"Extase\" from Procter & Gamble, and in 2011 the \"Baldessarini\" fragrance line, and \"Windsor\" were acquired.\n\nMäurer & Wirtz divided its assortment in 2010 into three business units, which will cover different market and user segments: The segment \"Beauty\" includes own brands and licensed brands from the low-price sector, the pillar \"prestige\" is in charge for luxury fragrances and under the third segment \"4711\" the products from the Cologne \"Glockengasse\" were brought together:\n\n"}
{"id": "47611264", "url": "https://en.wikipedia.org/wiki?curid=47611264", "title": "Phase-locked loop ranges", "text": "Phase-locked loop ranges\n\nThe terms hold-in range, pull-in range (acquisition range), and lock-in range are widely used by engineers for the concepts of frequency deviation ranges within which phase-locked loop-based circuits can achieve lock under various additional conditions.\n\nIn the classic books on phase-locked loops, published in 1966, such concepts as hold-in, pull-in, lock-in, and other frequency ranges for which PLL can achieve lock, were introduced. They are widely used nowadays (see, e.g. contemporary engineering literature and other publications). Usually in engineering literature only non-strict definitions are given for these concepts. F. Gardner in 1979 in the 2nd edition of his well-known work, Phaselock Techniques, formulated the following problem[p. 70] (see also the 3rd edition[p. 187–188]): \"There is no natural way to define exactly any unique lock-in frequency\". The lack of rigorous explanations led to the paradox: \"despite its vague reality, lock-in range is a useful concept\"[p. 70]. Many years of using definitions based on the above concepts has led to the advice given in a handbook on synchronization and communications, namely to check the definitions carefully before using them. First rigorous mathematical definitions were given in.\n\n\nNote that in general formula_5, because formula_6 also depends on initial input of VCO.\n\nDefinition of locked state\n\nIn a locked state: 1) the phase error fluctuations are small, the frequency error is small; 2) PLL approaches the same locked state after small perturbations of the phases and filter state.\nDefinition of hold-in range.\n\nA largest interval of frequency deviations formula_7 for which a locked state exists is called a hold-in range, and formula_8 is called hold-in frequency.\nValue of frequency deviation belongs to the hold-in range if the loop re-achieves locked state after small perturbations of the filter's state, the phases and frequencies of VCO and the input signals. This effect is also called steady-state stability. In addition, for a frequency deviation within the hold-in range, after a small changes in input frequency loop re-achieves a new locked state (tracking process).\n\nAlso called acquisition range, capture range.\n\nAssume that the loop power supply is initially switched off and then at formula_9 the power is switched on, and assume that the initial frequency difference is sufficiently large. The loop may not lock within one beat note, but the VCO frequency will be slowly tuned toward the reference frequency (acquisition process). This effect is also called a transient stability. The pull-in range is used to name such frequency deviations that make the acquisition process possible (see, e.g. explanations in [, p. 40], [, p. 61]).\nDefinition of pull-in range.\n\nPull-in range is a largest interval of frequency deviations formula_10 such that PLL acquires lock for arbitrary initial phase, initial frequency, and filter state. Here formula_11 is called pull-in frequency.\nAssume that PLL is initially locked. Then the reference frequency formula_12 is suddenly changed in an abrupt manner(step change). Pull-in range guarantees that PLL will eventually synchronize, however this process may take a long time. Such long acquisition process is called cycle slipping.\nIf difference between initial and final phase deviation is larger than formula_13, we say that cycle slipping takes place.\nHere, sometimes, the limit of the difference or the maximum of the difference is considered\nDefinition of lock-in range.\n\nIf the loop is in a locked state, then after an abrupt change of formula_15 free within a lock-in range formula_16, the PLL acquires lock without cycle slipping. Here formula_17 is called lock-in frequency.\n"}
{"id": "27788626", "url": "https://en.wikipedia.org/wiki?curid=27788626", "title": "Platform evangelism", "text": "Platform evangelism\n\nPlatform evangelism (also called developer relations, developer and platform evangelism, developer advocacy, or API evangelism) is the application of technology evangelism to a multi-sided platform. It seeks to accelerate the growth of a platform's commercial ecosystem of complementary goods, created by independent (third-party) developers, as a means to the end of maximizing the platform's market share.\n\nA multi-sided platform creates value by bringing together two or more different groups who can create more value together than apart. Examples include buyers and sellers at an auction; readers and advertisers of a newspaper; and men and women at an online dating service. The platform vendor can profit by capturing a portion of the money that changes hands. Platform vendors can serve as \"de facto\" regulators of their markets.\n\nMany platforms have only two sides: one of consumers and the other of independent (third-party) developers. Independent developers produce and sell complementary goods, also called \"platform applications,\" directly to the platform's consumers. These applications rely on the platform's services to function. Generally speaking, consumers prefer a platform with more and higher-value applications, while developers prefer a platform with more and higher-paying consumers.\n\nRecent examples of two-sided platforms that successfully attracted both consumers and developers include Apple iPhone, Nintendo Wii, Adobe Flash, and Microsoft Windows.\n\nOther examples include household electricity (for appliances), the farm tractor's three-point hitch (for farming implements), camera lens mounts (for interchangeable lenses), the Picatinny rail (for gun-mounted accessories), and media players such as record players, CD players, and DVD players (for media content).\n\nPlatform evangelism establishes and nurtures a platform ecosystem, which requires five simultaneous activities: 1) sales, 2) enablement, 3) feedback, 4) intelligence, and 5) regulation.\n\nEcosystem sales is the attempt to convince third parties to develop complementary goods for the platform's commercial ecosystem. The characteristics of successful platform evangelists and salespersons are essentially identical, including deep product knowledge, empathy, humor, integrity, communication skills, positive attitude, infectious enthusiasm, a sincere desire to help others, etc. The primary difference is in \"hunger for money\" among salespersons. This is unlikely to be satisfied by technology evangelism, which is more likely to be a cost center than a profit center, and hence incapable of paying sales-like commissions.\n\nThis aspect of platform evangelism can be seen as vendor-sponsored change agency in the diffusion of an innovation (the platform). As such, platform evangelism is responsible for creating the resources that enable developers to progress swiftly through the innovation adoption process (\"developer enablement resources\").\n\nEach different phase of the platform adoption process requires different developer enablement resources. It is the responsibility of the platform evangelist to ensure that each developer enablement resource comes into existence in a timely manner.\n\nBecause developers will often choose an inferior platform if its rate of improvement suggests that it will soon become the superior platform, even the superior platform must improve rapidly. To facilitate this rapid improvement, platform evangelism organizes and champions the feedback of ecosystem developers within the platform vendor.\n\nEcosystem intelligence gathers information about the activities and intentions of other platform vendors from sources in and around the developer ecosystem.\n\nTo avoid the market failure of multi-sided platforms, platform evangelism will often engage in \"de facto\" regulation of the commercial ecosystem that surrounds its platform. Such regulation combines legal, technological, informational, and other instruments (along with price setting) to minimize the costs of externalities, complexity, uncertainty, information asymmetry, and coordination problems.\n\nOften, many competing two-sided platforms―each offering roughly the same benefits on the developer side―start diffusing through a market at approximately the same time. Each platform's vendor competes with the other vendors, via platform evangelism, to gain market share among the market's potential developers.\n\nIf it is expensive to re-develop an application to target a second (or third) platform, then developers will tend to adopt, first, the platform which they believe has the highest lifetime profit opportunity. This tends to become a self-fulfilling prophecy, in which the platform which is initially perceived to have the highest lifetime profit opportunity tends to accumulate the most applications, which then makes it more attractive to consumers, which makes it more attractive to developers, etc., in a virtuous cycle, until a critical mass (also known as a tipping point) is reached, such that it out-competes the other platforms…and eventually does indeed come to offer the highest lifetime profit potential, as prophesied. This makes it very important for a platform vendor to convince developers \"from the outset\", even before the platform becomes commercially available to consumers, that its platform will have the highest lifetime profit potential. This contributes to the high-tech industry's hype cycle.\n\nIf the market has strong network effects, then those platforms which gain the fewest early adopters may cease to exist, making them unavailable to later potential adopters. This is path dependence. If the number of reasonable platform choices in a market falls to 1, then that \"only reasonable choice\" becomes the market's \"de facto\" standard (also known as its Dominant Design) by definition.\n\nIf the cost of switching away from a \"de facto\" standard to a new alternative exceeds the benefit gained by the earliest market participants who make that switch, then the market will tend to become locked into the \"de facto\" standard, with its market share approaching 100%. Markets that are highly interconnected are more resistant to change than less interconnected markets. Lock-in tends to make a \"de facto\" standard impervious to incremental competition, such that only a disruptive innovation can displace it. Owning a locked-in \"de facto\" standard can be a license to print money for a very long time.\n\nNot all markets become locked into a \"de facto\" standard, and indeed, many factors act against such an outcome. However, owning the first platform to establish a critical mass of complementary goods, with high switch-out costs, can create first-mover advantages that can go a long way towards ensuring that one gets a viable share of the mature market.\n\nTherefore, with regard to a two-sided platform's developer side, it is platform evangelism's responsibility to:\n\nIn short, it is platform evangelism's responsibility to design, develop, maintain, and extend the size and health of the platform's ecosystem, such that potential developers will choose to participate \"first\", \"best\", or \"only\" in that ecosystem.\n\nUltimately, these are all means to the end of gaining the highest possible share of the developer side of the targeted market. Whether this is, in turn, a means to the end of maximizing share on the market's consumer side, or \"vice versa\", depends on the platform vendor's pricing strategy.\n\nThe economics of multi-sided markets have only recently been subject to academic scrutiny, so the theory underpinning platform evangelism's regulatory function is likely to be poorly understood by its practitioners—a situation that has contributed to many disastrous market failures.\n\nMore generally, evangelizing the developer side of a multi-sided platform can be seen as using social influence to accelerate the diffusion of an innovation, within a dynamic system, in the presence of network effects, for economic gain (to the platform vendor or to society as a whole). Therefore, a solid understanding of the theory and practice of social influence, diffusion of innovations, system dynamics, innovation dynamics, network effects, innovation economics, and the economics of multi-sided platforms is essential to the design and management of efficient and effective platform evangelism campaigns.\n\nMost of the theory and practice of platform evangelism, as influenced by these theoretical underpinnings, has yet to be comprehensively documented, and is therefore not available to novice evangelists or their managers. Because the development of expertise in any domain takes considerable time and practice, vendors of new platforms cannot reasonably be expected to have in-house expertise in designing and managing effective platform evangelism campaigns.\n\n"}
{"id": "55509384", "url": "https://en.wikipedia.org/wiki?curid=55509384", "title": "Post War Building Studies", "text": "Post War Building Studies\n\nThe Post-War Building Studies are a set of technical reports published by the British Ministry of Works starting in 1944. The Directorate of Post-War Building was established in 1941 under Sir James West. The Directorate was charged with coordinating solutions for construction of housing to replace homes that had been destroyed as well as homes that had been deferred due to war. The Directorate reported to the Minister of Works, initially Lord Reith then later Lord Portal. The publications were produced by various committees (such as the Burt Committee) composed of architects, engineers, and representatives from the building industry. The studies standardized non-traditional methods of building construction including the use of pre-fabricated elements and poured concrete. A new standard system for wiring homes for electricity was described in report no. 11. The reports had a significant impact on the design and construction of buildings in the UK after the war and continue to be cited as references, though their recommendations on fire safety were later found to be insufficient as apartment buildings became taller. However, the BS 1363 power socket (another product of the studies) has proved long-lasting and is still in use in British homes today. While not made part of mandatory building codes and regulations, the reports provided technical guidance and information on application of non-traditional building techniques and materials while overcoming material and labour shortages.\n\nBased on the experience following World War I, it was expected that housing construction demand would be very high after WWII ended, both due to pent-up demand that had not been fulfilled and also due to replacement or repair of housing that had been bombed during the war. Labour and material were expected to be in short supply. Interest in industrial methods, pre-fabrication and new materials was high during the period between the wars, and such publications as the \"Tudor Walters Report\" of 1918 gave details on new methods of construction and new materials, including recommendations to improve construction efficiency by better site organisation, increased accuracy of cost accounting, and keeping building trade workers regularly employed. The Directorate of Post-War Building and the Directorate of Building Materials were established by the Ministry of Works. These groups took on research into new methods and published the \"Post War Building Studies\" in 33 volumes between 1944 and 1946. Experimental work was carried out at the Building Research Station and reported in the series.\n\n\n"}
{"id": "53937260", "url": "https://en.wikipedia.org/wiki?curid=53937260", "title": "Power-to-heat", "text": "Power-to-heat\n\nPower-to-Heat (also PtH or P2H) is a rather new name for the conversion of electrical energy into heat. This can be done through conventional heating resistors, electrode boilers and heat pumps. \n\nThe purpose of PtH systems is to utilize excess electricity generated by renewable energy sources which would otherwise curtailed. Hence RES-E can displace fossil energy and reduce emissions in the heating sector.. In contrast to simple electric heating systems such as night storage heating which covers the complete heating requirements, Power-to-Heat systems are hybrid systems, which additional have traditional heating systems using chemical fuels like wood or natural gas. When there are excess energy the heat production can result from electric energy otherwise the traditional heating system will be used. In order to increase flexibility power-to-heat systems are often coupled with heat accumulators. The power supply occurs for the most part in the local and district heating networks. Power-to-heat systems are also able to supply buildings or industrial systems with heat.\n\n"}
{"id": "1276320", "url": "https://en.wikipedia.org/wiki?curid=1276320", "title": "Proper transfer function", "text": "Proper transfer function\n\nIn control theory, a proper transfer function is a transfer function in which the degree of the numerator does not exceed the degree of the denominator. \n\nA strictly proper transfer function is a transfer function where the degree of the numerator is less than the degree of the denominator.\nThe difference between the degree of the denominator (number of poles) and degree of the numerator (number of zeros) is the \"relative degree\" of the transfer function.\n\nThe following transfer function:\n\nis proper, because\n\nis biproper, because\n\nbut is not strictly proper, because\n\nThe following transfer function is not proper (or strictly proper)\nbecause\n\nThe following transfer function is strictly proper\nbecause\n\nA proper transfer function will never grow unbounded as the frequency approaches infinity:\n\nA strictly proper transfer function will approach zero as the frequency approaches infinity (which is true for all physical processes):\n\nAlso, the integral of the real part of a strictly proper transfer function is zero.\n\n"}
{"id": "58681816", "url": "https://en.wikipedia.org/wiki?curid=58681816", "title": "Reciprocity (optoelectronic)", "text": "Reciprocity (optoelectronic)\n\nOptoelectronic reciprocity relations relate properties of a diode under illumination to the photon emission of the same diode under applied voltage. The relations are useful for interpretation of luminescence based measurements of solar cells and modules and for the analysis of recombination losses in solar cells.\n\nSolar cells and light-emitting diodes are both semiconducting diodes that are operated in a different voltage and illumination regime and that serve different purposes. A solar cell is operated under illumination (usually by solar radiation) and is typically kept at the maximum power point where the product of current and voltage are maximized. A light emitting diode is operated at an applied forward bias (without external illumination). While a solar cell converts the energy contained in the electromagnetic waves of the incoming solar radiation into electric power (voltage x current) a light-emitting diode does the inverse, namely converting electrical power into electromagnetic radiation. A solar cell and a light emitting diode are typically made from different materials and optimized for different purposes; however, conceptually every solar cell could be operated as a light emitting diode and vice versa. Given that the operation principles have a high symmetry it is fair to assume that the key figures of merit that are used to characterize photovoltaic and luminescent operation of diodes are related to each other. These relations become particularly simple in a situation, where recombination rates scale linearly with minority carrier density and are explained below.\n\nThe photovoltaic quantum efficiency formula_1 is a spectral quantity that is generally measured as a function of photon energy (or wavelength). The same is true for the electroluminescence spectrum formula_2 of a light emitting diode under applied forward voltage formula_3. Under certain conditions specified below, these two properties measured on the same diode are connected via the equation\nwhere formula_5 is the black body spectrum emitted by a surface (the diode) into the hemisphere above the diode in units of photons per area, time and electron interval. In this case the black body spectrum is given by\nwhere formula_7 is the Boltzmann constant, formula_8 is Planck’s constant, formula_9 is the speed of light in vacuum, and formula_10 is the temperature of the diode.\nThis simple relation is useful for the analysis of solar cells using luminescence-based characterization methods. Luminescence used for characterization of solar cells is useful because of the ability to image the luminescence of solar cells and modules in short periods of times, while spatially resolved measurements of photovoltaic properties (such as photocurrent or photovoltage) would be very time-consuming and technically difficult.\n\nEquation (1) is valid for the practically relevant situation, where the neutral base region of a pn-junction makes up most of the volume of the diode. Typically, the thickness of a crystalline Si solar cell is ~ 200 µm while the thickness of the emitter and space charge region is only on the order of hundreds of nanometers, i.e. three orders of magnitude thinner. In the base of a pn-junction, recombination is typically linear with minority carrier concentration over a large range of injection conditions and charge carrier transport is by diffusion. In this situation, the Donolato theorem. is valid that states that the collection efficiency formula_11 is related to the normalized minority carrier concentration formula_12 via\nwhere formula_14 is a spatial coordinate and formula_15 defines the position of the edge of the space charge region (where the neutral zone and the space charge region connect). Thus, if formula_16, the collection efficiency is one. Further away from the edge of the space charge region, the collection efficiency will be smaller than one depending on the distance and the amount of recombination happening in the neutral zone. The same holds for the electron concentration in the dark under applied bias. Here, the electron concentration will also decrease from the edge of the space charge region towards the back contact. This decrease as well as the collection efficiency will be approximately exponential (with the diffusion length controlling the decay). \n\nThe Donolato theorem is based on the principle of detailed balance and connects the processes of charge carrier injection (relevant in the luminescent mode of operation) and charge carrier extraction (relevant in the photovoltaic mode of operation).\nIn addition, the detailed balance between absorption of photons and radiative recombination can be mathematically expressed using the van Roosbroeck-Shockley equation as\nHere, formula_18 is the absorption coefficient, formula_19 is the radiative recombination coefficient, formula_20 is the refractive index, formula_21 is the intrinsic charge carrier concentration. \nA derivation of equation (1) can be found in ref. \n\nThe reciprocity relation (eq. (1)) is only valid if absorption and emission is dominated by the neutral region of the pn-junction shown in the adjacent figure.\nThis is a good approximation for crystalline silicon solar cells and the method can also be used for Cu(In,Ga)Se based solar cells. However the equations has limitations when applied to solar cells where the space charge region is of comparable size to the total absorber volume. This is the case for instance for organic solar cells or amorphous Si solar cells.\nThe reciprocity relation is also invalid if the emission of the solar cell is not from delocalized conduction and valence band states as would be the case for most mono and polycrystalline semiconductors but from localized states (defect states). This limitation is relevant for microcrystalline and amorphous silicon solar cells.\n\nThe open-circuit voltage formula_22 of a solar cell is the voltage created by a certain amount of illumination if the contacts of the solar cell are not connected, i.e. in open circuit. The voltage that can build up in such as situation is directly connected to the density of electrons and holes in the device. These densities in turn depend on the rates of photogeneration (determined by the amount of illumination) and the rates of recombination. The rate of photogeneration is usually determined by the typically used illumination with white light with a power density of 100mW/cm (called one sun) and by the band gap of the solar cell and does not change much between different devices of the same type. The rate of recombination however might vary over orders of magnitude depending on the quality of the material and the interfaces. Thus, the open-circuit voltage depends quite drastically on the rates of recombination at a given concentration of charge carriers. The highest possible open-circuit voltage, the radiative open-circuit voltage formula_23, is obtained if all recombination is radiative and non-radiative recombination is negligible. This is the ideal situation, because radiative recombination cannot be avoided other than by avoiding light absorption (principle of detailed balance). However, since absorption is a key requirement for a solar cell and necessary to achieve a high concentration of electrons and holes as well, radiative recombination is a necessity (see van Roosbroeck-Shockley equation ). If non-radiative recombination is substantial and non negligible, the open-circuit voltage will be reduced depending on the ratio between the radiative and non-radiative recombination currents (where the recombination currents are the integral of the recombination rates over volume). This leads to a second reciprocity relation between the photovoltaic and the luminescent operation mode of a solar cell because the ratio of radiative to total (radiative and non-radiative) recombination currents is the external luminescence quantum efficiency formula_24 of a (light emitting) diode. Mathematically, this relation is expressed as,\nThus, any reduction in the external luminescence quantum efficiency by one order of magnitude would lead to a reduction in open-circuit voltage (relative to formula_23) by formula_27. Equation (2) is frequently used in the literature on solar cells. For instance for an improved understanding of the open-circuit voltage in organic solar cells and for comparing voltage losses between different photovoltaic technologies.\n"}
{"id": "42934713", "url": "https://en.wikipedia.org/wiki?curid=42934713", "title": "Reconfigurable antenna", "text": "Reconfigurable antenna\n\nA reconfigurable antenna is an antenna capable of modifying dynamically its frequency and radiation properties in a controlled and reversible manner. In order to provide a dynamical response, reconfigurable antennas integrate an inner mechanism (such as RF switches, varactors, mechanical actuators or tunable materials) that enable the intentional redistribution of the RF currents over the antenna surface and produce reversible modifications over its properties. Reconfigurable antennas differ from smart antennas because the reconfiguration mechanism lies inside the antenna rather than in an external beamforming network. The reconfiguration capability of reconfigurable antennas is used to maximize the antenna performance in a changing scenario or to satisfy changing operating requirements.\n\nReconfigurable antennas can be classified according to the antenna parameter that is dynamically adjusted, typically the frequency of operation, radiation pattern or polarization.\n\nFrequency reconfigurable antennas can adjust dynamically their frequency of operation. They are particularly useful in situations where several communications systems converge because the multiple antennas required can be replaced by a single reconfigurable antenna. Frequency reconfiguration is generally achieved by modifying physically or electrically the antenna dimensions using RF-switches, impedance loading or tunable materials.\n\nRadiation pattern reconfigurability is based on the intentional modification of the spherical distribution of radiation pattern. Beam steering is the most extended application and consists in steering the direction of maximum radiation to maximize the antenna gain in a link with mobile devices. Pattern reconfigurable antennas are usually designed using movable/rotatable structures or including switchable and reactively-loaded parasitic elements.. In last 10 years, metamaterial-based reconfigurable antennas have gained attention due their small form factor, wide beam steering range and wireless applications.\n\nPolarization reconfigurable antennas are capable of switching between different polarization modes. The capability of switching between horizontal, vertical and circular polarizations can be used to reduce polarization mismatch losses in portable devices. Polarization reconfigurability can be provided by changing the balance between the different modes of a multimode structure.\n\nCompound reconfiguration is the capability of simultaneously tuning several antenna parameters, for instance frequency and radiation pattern. The most common application of compound reconfiguration is the combination of frequency agility and beam-scanning to provide improved spectral efficiencies. Compound reconfigurability is achieved by combining in the same structure different single-parameter reconfiguration techniques or by reshaping dynamically a pixel surface.\n\n"}
{"id": "4502545", "url": "https://en.wikipedia.org/wiki?curid=4502545", "title": "Reducer", "text": "Reducer\n\nA reducer is the component in a pipeline that reduces the pipe size from a larger to a smaller bore (inner diameter).\n\nThe length of the reduction is usually equal to the average of the larger and smaller pipe diameters. There are two main types of reducer: concentric and eccentric reducers.\n\nA reducer can be used either as nozzle or as diffuser depending on the mach number of the flow.\n\nA reducer allows for a change in pipe size to meet hydraulic flow requirements of the system, or to adapt to existing piping of a different size. Reducers are usually concentric but eccentric reducers are used when required to maintain the same top-or bottom-of-pipe level.\n\nThese fittings are manufactured in inch and metric size.\n\n\n"}
{"id": "31973253", "url": "https://en.wikipedia.org/wiki?curid=31973253", "title": "SAGE III on ISS", "text": "SAGE III on ISS\n\nSAGE III on ISS is the fourth generation of a series of NASA Earth-observing instruments, known as the Stratospheric Aerosol and Gas Experiment. The first SAGE III instrument was launched on the Russian Meteor (satellite) spacecraft. The recently revised SAGE III will be mounted to the International Space Station where it will use the unique vantage point of ISS to make long-term measurements of ozone, aerosols, water vapor, and other gases in Earth's atmosphere.\n\nThe first SAGE instrument was launched February 18, 1979, to collect data on the various gases in the atmosphere, including ozone. The data collected on and the following instrument , which began taking measurements in , were critical to the discovery of the Earth's ozone hole and the creation of 1987 Montreal Protocol, which banned ozone-depleting substances, such as chlorofluorocarbon (CFC). \n\nSAGE III on ISS is a nearly exact replica of Meteor-3M, sent into orbit in 2001 on a Russian satellite. Meteor-3M went out of service in March 2006 when the satellite's power supply stopped working. The new instrument was built in anticipation of being attached to the space station in 2005. A change in ISS design, however, put those plans on hold. The instrument was stored in a Class 100 clean room in a sealed shipping container under a continuous gaseous nitrogen purge. The purge kept clean dry \"air\" inside the instrument. \n\nRecently, the opportunity arose for to be placed on ISS, and build on the long record of stratospheric gas data that its ancestors created. The week of February 14, 2011, scientists at NASA Langley Research Center pulled the instrument from storage to begin initial testing and calibrations in preparation prepping it for launch.\n\nThe 76-kilogram (168 lb) instrument is a grating spectrometer that measures ultraviolet and visible energy. It relies upon the flight-proven designs used in the Stratospheric Aerosol Measurement () and first and second SAGE instruments. The design incorporates Charge Coupled Device (CCD) array detectors and a 16 bit A/D converter. Combined, these devices allow for wavelength calibration, a self-consistent determination of the viewing geometry, lunar occultation measurements, and expanded wavelength coverage.\n\nThe SAGE III sensor assembly consists of pointing and imaging subsystems and a UV/visible spectrometer. The pointing and imaging systems are employed to acquire light from either the Sun or Moon by vertically scanning across the object. The spectrometer uses an 800 element CCD linear array to provide continuous spectral coverage between 290 and 1030 nm. Additional aerosol information is provided by a discrete photodiode at 1550 nm. This configuration enables to make multiple measurements of absorption features of target gaseous species and multi-wavelength measurements of broadband extinction by aerosols.\n\nThe SAGE III mission is an important part of NASA's Earth Observation System and is designed to fulfill the primary scientific objective of obtaining high quality, global measurements of key components of atmospheric composition and their long-term variability. The primary focus of on ISS will be to study aerosols, clouds, water vapor, pressure and temperature, nitrogen dioxide, nitrogen trioxide, and chlorine dioxide.\n\nAerosols play an essential role in the radiative and chemical processes that govern the Earth's climate. Since stratospheric aerosol loading has varied by a factor of 30 since 1979, long-term monitoring of tropospheric and stratospheric aerosols is crucial. aerosol measurements will provide important contributions in the area of aerosol research.\n\nClouds play a major role in determining the planet's solar and longwave energy balance and, thus, are important in governing the Earth's climate. will provide measurements of mid and high level clouds including thin or \"sub-visual\" clouds that are not detectable by nadir-viewing passive remote sensors. These observations are important because while low clouds primarily reflect incoming solar radiation back into space (acting to cool the planet), mid and high level clouds enhance the \"greenhouse\" effect by trapping infrared radiation (acting to warm the planet). Also, the presence of thin cloud near the tropopause may play a significant role in heterogeneous chemical processes that lead to ozone destruction in mid-latitudes.\n\nWater vapor is the dominant greenhouse gas and plays a crucial role in regulating the global climate system. An improved understanding of the global water vapor distribution can enhance our ability to understand water's role in climate processes. water vapor measurements will provide important contributions on the long-term effect of this green house gas.\n\nOzone research has remained at the forefront of atmospheric science for many years because stratospheric ozone shields the Earth's surface (and its inhabitants) from harmful ultraviolet radiation. Since recent declines in stratospheric ozone have been linked to human activity, accurate long-term measurements of ozone remain crucial. \n\nIt is important to monitor ozone levels in the lower stratosphere and upper troposphere since observed trends are the largest and most poorly understood at those altitudes. SAGE III's high vertical resolution and long-term stability make it uniquely well suited to make these measurements. will also be able to look at the relationship between aerosol, cloud, and chemical processes affecting ozone argue for simultaneous measurements of these atmospheric constituents (such as those made by ).\n\nSAGE III temperature measurements will provide a unique data set for monitoring and understanding atmospheric temperature changes. In particular, the long-term stability and self-calibration capabilities of may permit the detection of trends in stratospheric and mesospheric temperature that will be important diagnostics of climate change. temperature measurements in the upper stratosphere and mesosphere will be a new source of long-term temperature measurements in this region of the atmosphere to complement existing long-term measurements made by satellites (MLS, ACE, SABER) and ground based lidar systems. temperature measurements will also allow the monitoring of periodic temperature changes, such as those associated with the solar cycle and quasi-biennial oscillation, and the effects of radiative forcing by aerosols.\n\nNitrogen dioxide (NO), nitrogen trioxide (NO), and chlorine dioxide (OClO) play crucial roles in stratospheric chemistry and the catalytic cycles that destroy stratospheric ozone. NO measurements are important because the processes that occur in the Antarctic winter and spring and give rise to the ozone hole effectively convert NO to nitric acid (HNO). Thus NO is an important diagnostic of ozone hole chemistry. Since it is measured during both solar and lunar occultation events, observations of NO will improve our understanding of the strong diurnal (daily) cycles in stratospheric processes. In addition, will make virtually unique measurements of nitrogen trioxide (NO). Although it is short-lived in the presence of sunlight, NO plays an active role in the chemistry of other reactive nitrogen species such as NO and di-nitrogen pentoxide (NO) and, thus, indirectly in ozone chemistry. Since few other measurements of NO are available, measurements, which are made during lunar occultation (nighttime) events, will provide crucial validation for our current understanding of reactive nitrogen chemistry.\n\nSAGE III was launched on the SpaceX CRS-10 mission using a Falcon 9 with Dragon. traveled in the unpressurized trunk of Dragon, which launched on February 19, 2017. Upon arrival, NASA used Dextre to berth the instrument onto an ExPRESS Logistics Carrier platform on the ISS.\n\nNASA Langley Research Center, based out of Hampton, Virginia, is leading the mission. Ball Aerospace & Technologies Corp. built the SAGE III-ISS instrument in Boulder, Colorado, and the European Space Agency and Thales Alenia Space, headquartered in France, are providing a hexapod to keep the instrument pointing in the right direction as the ISS maneuvers in space.\n\n\n"}
{"id": "50960649", "url": "https://en.wikipedia.org/wiki?curid=50960649", "title": "Sansui AU-11000", "text": "Sansui AU-11000\n\nThe Sansui AU-11000 is a home audio integrated amplifier built by Japanese audio electronics manufacturer Sansui during the mid 1970s. The amplifier is known across the audiophile community for its high-output power and low total harmonic distortion (THD).\n\nThe AU-11000 has an output power of 110 watts-per-channel, and was one of the earliest home audio amplifiers to use multiple transistors per channel using a \"Push-Pull\" method to add more amplification power. The AU-11000 uses independent circuitry in the amplification process, in such a way that, each individual channel has its own circuits. This means everything from the power supply to the bias control board all have 2 identical and independent circuits per channel to separate the 2 stereo channels. This also minimizes the Total Harmonic Distortion (THD) within the amplifier down to 0.08%.\n\nThe AU-11000 has its own power protection built into the amplifier. When the power switch is turned to the \"ON\" position, the power light comes on as green for a second, and turns red for a few seconds until the power protection circuitry has confirmed it is safe to power-up. Once it has finished, the power light turns and remains green until the amplifier is turned off. The AU-11000 also has its own block diagram printed on the upper-front casing.\n\nSansui designed the AU-11000 with the input, output and speaker terminals on the sides of the unit. The rear of the unit has the power cord and outlets only.\n\nThe AU-11000 has features such as a logarithmic volume control, a 3-position level-set muting, a -20db mute switch, 3-position high & low filters, 3-Band Linear Bass/Midrange/Treble controls, 3 optional frequency settings for Bass & Treble controls, 3-position Tone Selection, A & B Speakers, Tape Input/Output control, 5-position mono-stereo selector switch, Tuner input, 2 Auxiliary inputs, 2 phono inputs, 2 rear 'always on' power outlets and 1 switched power outlet that is controlled by the units main power switch. There is an XLR input for Tape 2, and a spring-loaded ground connection plug.\n\nThere are a few variations of the Sansui AU-11000. Many units share some of the same internal components as the less-powerful AU-9900, such as the Bias Board [F-2580] and the Power Supply Board [F-2566], though the AU-11000 has much larger power capacitors. Some units have an 'A' at the end of the model number (AU-11000A), indicating it has 120 Watts-per-channel and a linear volume control.\n\nThere is a lot of discussion among the home audio community about how to properly set the DC Offset & Current Bias within the amplifier. This is not something for the average do-it-yourself'er or handyman, and should be performed by a qualified technician.\n\nAU-11000's with the F-2580 bias board can be adjusted using the service manual. AU-11000's with the F-2583 bias board have VR01 & VR03 switched within the circuit. VR02 & VR04 and also switched respectively. This means that the DC Offset & Current Bias trim locations are switched with one-another on the F-2583, compared to the F-2580.\n\nThe Service Manual depicts proper adjustments of the F-2580 only. The picture on the right shows the proper illustration for the F-2583.\n\nThe AU-11000 uses 8 TO-3 style power-output transistors, 4 per channel. 2 of the 4 are Sanken 2SC1116 [NPN] transistors, and the other 2 are Sanken 2SA747 [PNP] transistors. Two of the same transistors are paired together onto a single Power Transistor Circuit [F-2581], and are paired with an identical circuit using the 2 different Power Transistors. The 2 circuits function together to power the respective channel using a \"Push-Pull\" method to create the 110 Watts per Channel. The 2 circuits make up each power amplifier per channel. This makes a total of 8 power-output transistors within the amplifier.\n\nEach amplifier channel has a temperature sensor built-in as part of the power protection circuitry. If the amp is too hot, the power protection circuitry will not allow the amplifier to power on. The power protection circuit has (4) 2Amp fuses. There are (4) 7Amp fuses on the power supply board [F-2566]. Since the power supply board is actually 2 identical power supplies (1 per channel), there are 2 fuses per individual power supply.\n\nThere is a ground screw inside the front-bezel which links the ground to several of the AU-11000's circuit boards. This is often overlooked when checking the ground connections.\n"}
{"id": "5636358", "url": "https://en.wikipedia.org/wiki?curid=5636358", "title": "Sequencing batch reactor", "text": "Sequencing batch reactor\n\nSequencing batch reactors (SBR) or sequential batch reactors are a type of activated sludge process for the treatment of wastewater. SBR reactors treat wastewater such as sewage or output from anaerobic digesters or mechanical biological treatment facilities in batches. Oxygen is bubbled through the mixture of wastewater and activated sludge to reduce the organic matter (measured as biochemical oxygen demand (BOD) and chemical oxygen demand (COD)). The treated effluent may be suitable for discharge to surface waters or possibly for use on land.\n\nWhile there are several configurations of SBRs, the basic process is similar. The installation consists of one or more tanks that can be operated as plug flow or completely mixed reactors. The tanks have a “flow through” system, with raw wastewater (\"influent\") coming in at one end and treated water (\"effluent\") flowing out the other. In systems with multiple tanks, while one tank is in settle/decant mode the other is aerating and filling. In some systems, tanks contain a section known as the bio-selector, which consists of a series of walls or baffles which direct the flow either from side to side of the tank or under and over consecutive baffles. This helps to mix the incoming Influent and the \"returned activated sludge\" (RAS), beginning the biological digestion process before the liquor enters the main part of the tank.\n\nThere are five stages in the treatment process:\n\nThe inlet valve opens and the tank is being filled in, while mixing is provided by mechanical means (no air). This stage is also called the anoxic stage. Aeration of the mixed liquor is performed during the second stage by the use of fixed or floating mechanical pumps or by transferring air into fine bubble diffusers fixed to the floor of the tank. No aeration or mixing is provided in the third stage and the settling of suspended solids starts. During the fourth stage the outlet valve opens and the \"clean\" supernatant liquor exits the tank.\n\nAeration times vary according to the plant size and the composition/quantity of the incoming liquor, but are typically 60 to 90 minutes. The addition of oxygen to the liquor encourages the multiplication of aerobic bacteria and they consume the nutrients. This process encourages the conversion of nitrogen from its reduced ammonia form to oxidized nitrite and nitrate forms, a process known as nitrification.\n\nTo remove phosphorus compounds from the liquor, aluminium sulfate (alum) is often added during this period. It reacts to form non-soluble compounds, which settle into the sludge in the next stage.\n\nThe \"settling\" stage is usually the same length in time as the aeration. During this stage the sludge formed by the bacteria is allowed to settle to the bottom of the tank. The aerobic bacteria continue to multiply until the dissolved oxygen is all but used up. Conditions in the tank, especially near the bottom are now more suitable for the anaerobic bacteria to flourish. Many of these, and some of the bacteria which would prefer an oxygen environment, now start to use oxidized nitrogen instead of oxygen gas (as an alternate terminal electron acceptor) and convert the nitrogen to a gaseous state, as nitrogen oxides or, ideally, molecular nitrogen (dinitrogen, N) gas. This is known as denitrification.\n\nAnoxic SBR can be used for anaerobic processes, such as the removal of ammonia via Anammox, or the study of slow-growing microorganisms. In this case, the reactors are purged of oxygen by flushing with inert gas and there is no aeration.\n\nAs the bacteria multiply and die, the sludge within the tank increases over time and a waste activated sludge (WAS) pump removes some of the sludge during the settle stage to a digester for further treatment. The quantity or “age” of sludge within the tank is closely monitored, as this can have a marked effect on the treatment process.\n\nThe sludge is allowed to settle until clear water is on the top 20 to 30 percent of the tank contents.\n\nThe decanting stage most commonly involves the slow lowering of a scoop or “trough” into the basin. This has a piped connection to a lagoon where the final effluent is stored for disposal to a wetland, tree growing lot, ocean outfall, or to be further treated for use on parks, golf courses etc.\n\nIn some situations in which a traditional treatment plant cannot fulfill required treatment (due to higher loading rates, stringent treatment requirements, etc.) the owner might opt to convert their traditional system into a multi-SBR plant. Conversion to SBR will create a longer sludge age, minimizing sludge handling requirements downstream of the SBR.\n\nThe reverse can also be done where in SBR Systems would be converted into extended aeration (EA) systems. SBR treatment systems that could not cope up with a sudden constant increase of influent would easily be converted into EA plants. Extended aeration plants are more flexible in flow rate, eliminating restrictions presented by pumps located throughout the SBR systems. Clarifiers can be retrofitted in the equalization tanks of the SBR.\n\n"}
{"id": "3553733", "url": "https://en.wikipedia.org/wiki?curid=3553733", "title": "Shai Agassi", "text": "Shai Agassi\n\nShai Agassi (, born April 19, 1968) is an Israeli entrepreneur. He is the founder and former CEO of Better Place, which had developed a model and infrastructure for employing electric cars as an alternative to fossil fuel technology. The company went bankrupt in 2013, having spent over $850 million while deploying less than 1000 cars.\n\nPrior to founding Better Place, Agassi was President of the Products and Technology Group (PTG) at SAP AG until 2007. In 2003, at the age of 36, Agassi was named one of the top 20 'Global Influentials for 2003' by CNN-Time magazine. In 2009, Agassi was included in TIME magazine's 100 most influential people list.\nIn 2010, Foreign Policy magazine included Agassi on its annual list of the 100 most influential global thinkers.\n\nThroughout the 90s, Agassi started and successfully sold a number of technology startups, in the areas of enterprise software, internet technology, multimedia and small business administration. Agassi has a bachelor's degree in computer science and has been awarded a large number of patents in software, automotive and energy infrastructure.\n\nAfter graduating from Technion - Israel Institute of Technology, Agassi set out as a software entrepreneur. He founded TopTier Software (originally called Quicksoft Development) in Israel in 1992 and later moved the company's headquarters to California. Agassi served the company in various capacities including chairman, chief technology officer, and then CEO. He was directly involved in all critical phases of the company's development, including its strategic plan, technical direction and financing, management of two acquisitions, and negotiation of OEM agreements with companies such as SAP, Baan, and Microsoft. TopTier was a leading enterprise portal vendor when SAP acquired the company in April 2001 at a price of $400 million USD.\n\nIn addition to TopTier Software, Agassi co-founded several other companies with his father, Reuven Agassi, including Quicksoft Ltd., a leading multimedia software localization and distribution company in the Israeli market; TopManage, a developer of small business software that was also acquired by SAP in April 2002 (which became SAP Business One, the small business offering by SAP); and Quicksoft Media, a multimedia production company that ceased operations in 1995.\n\nMany of these acquisitions were done while Agassi was already working at SAP, leading to many SAP employees to question these as an obvious conflict of interests.\n\nHe wished to be the next CEO of SAP after Henning Kagermann vacated that space in 2007. However, Mr. Kagermann's contract as CEO was extended until 2009 by the supervisory board. This led Agassi to resign.\n\nAt SAP he was responsible for SAP's overall technology strategy and execution. In this leadership position, he oversaw the development of the integration and application platform SAP NetWeaver, SAP xApps packaged composite applications, SAP SRM, and SAP Business One. Before his appointment to the SAP Executive Board, Agassi was CEO of SAP Portals and later of the combined company SAP Markets and SAP Portals, which previously operated as a fully owned subsidiary of SAP AG. He was appointed to the SAP Executive Board in 2002. Together with the head of the Application Platform & Architecture (AP&A) group, Peter Zencke, Agassi co-led the Suite Architecture Team, which aligns the software architecture across all SAP solutions.\n\nIn January 2008, the Israeli government announced its support for a broad effort to promote the use of electric cars, embracing a joint venture between Better Place, Renault and its partner, Nissan Motor Company. Renault and Better Place were to work on the development of electric cars which could be powered by exchangeable batteries.\n\nAgassi initially raised $200MM for this project. Investors included VantagePoint Venture Partners, Israel Corporation, Israel Cleantech Ventures, Morgan Stanley, and private investors led by Michael Granoff of Maniv Energy Capital. In 2009 he raised an additional $135 million for Better Place Denmark, including an investment from DONG Energy, the leading utility in Denmark. Following the announcement in Israel, Better Place had launched its network in Denmark, Australia and in two US locations - Hawaii and Northern California. The company said it was in talks with more than 25 countries around the world, but only in Israel were battery swap stations built. In early 2010, Better Place raised its Series-B round at an amount of $350MM led by new investors from HSBC, Morgan Stanley and Lazard, as well as all previous investors. In November 2011, the company raised its third equity financing round of $200 million from a group of investors including GE, UBS bank and others.\n\nAgassi resigned his position as CEO of Better Place in October 2012. On 26 May 2013, Better Place filed for bankruptcy in the Israeli courts. Less than 1,400 cars were deployed in Israel, after spending about US$850 million in private capital. The bankruptcy receivers sold off the remaining assets in November 2013 for only $450,000.\n\n\n"}
{"id": "56256205", "url": "https://en.wikipedia.org/wiki?curid=56256205", "title": "Single-electron transistor", "text": "Single-electron transistor\n\nA single-electron transistor (SET) is a sensitive electronic device based on the Coulomb blockade effect. In this device the electron flows through a tunnel junction between source/drain to a quantum dot (conductive island). Moreover, the electrical potential of the island can be tuned by a third electrode, known as the gate, which is capacitively coupled to the island.\nFig. 1 shows the basic schematic of a SET device. The conductive island is sandwiched between two tunnel junctions, which are modeled by a capacitance (\"C\" and \"C\") and a resistor (\"R\" and \"R\") in parallel.\n\nThe increasing relevance of the Internet of things and the healthcare applications give more relevant impact to the electronic device power consumption. For this purpose, ultra-low-power consumption is one of the main research topics into the current electronics world. The amazing number of tiny computers used in the day-to-day world, e.g. mobile phones and home electronics, implies a significant power consumption level by the implemented devices. In this scenario, the single-electron transistor has appeared as a suitable candidate to achieve this low power range with a high level of device integration. The main technological difference between the well-established MOSFET device (metal-oxide-semiconductor field-effect transistor) and the SET lies in the device channel concept. Instead of having a conduction channel, as in case of MOSFET, which does not allow further reductions in its length, this channel is replaced by a small conducting \"island\" or quantum dot (QD). The device thus takes advantage of the Coulomb-blockade phenomenon in controlling the transfer of individual electrons to the QD. Source and drain regions are separated from the QD by tunnel junctions. The research on SET is mainly supported by \"orthodox theory\" based on three assumptions:\n\n\nThe main benefits of the SET use are a high device integration level and ultra-low power consumption. Moreover, the SET fabrication process is CMOS-compatible (complementary metal–oxide–semiconductor), which increases the possibilities for integrating them into complex circuits. At this point some drawbacks appear to be overcome such as low current level and the low temperature operation. The current level of the SET can be amplified by manufacturing together with a field-effect transistor (FET), by generating a hybrid SET-FET circuit (Fig. 2).\n\nAfterwards, the thermal fluctuations can suppress the Coulomb blockade; then, the electrostatic charging energy must be greater than \"kT\". This condition implies the maximum allowed island capacitance is inversely proportional to temperature. For these systems, to solve the drawback related to the SET operative only at cryogenic temperature it should be considered that an island capacitance below 1 aF is required to be room temperature operative. Note that the island capacitance is a function of their size. In this sense, to manufacture room temperature operative SETs the island size should be reduced towards 10 nm. Note that this level of device dimensions can jeopardize the SET manufacturability.\n\nIn this context, the relevance of the SET-based circuits have been recently highlighted through the granting of a project by the European Union, IONS4SET (#688072). The project looks for the manufacturing feasibility of SET-FET circuits operative at room temperature. The main goal of this project is to design SET manufacturability process-flow for large-scale operations seeking to extend the use of the hybrid SET-CMOS architectures. To assure room temperature operation, single dots of diameters below 5 nm have to be fabricated and located between source and drain with tunnel distances of a few nanometers. Up to now there is no reliable process-flow to manufacture a hybrid SET-FET circuit operative at room temperature. In this context, this EU project explores a more feasible way to manufacture the SET-FET circuit by using pillar dimensions of approximately 10 nm.\n"}
{"id": "1043812", "url": "https://en.wikipedia.org/wiki?curid=1043812", "title": "Snap freezing", "text": "Snap freezing\n\nSnap freezing (or cook-chill or blast freezing) is the process of rapid cooling of a substance for the purpose of preservation. It is widely used in the culinary and scientific industries.\n\nCooked meals can be preserved by rapid freezing after cooking is almost complete. The main target group for these products are those with little time for cooking such as schools, prisons, and hospitals.\n\nThe process involves the cooking of meals at a central factory, rapidly chilling them for storage until they are needed. Cook chill foods need to be packed in shallow trays to make the process more efficient. The food is cooled to a temperature under 3 degrees Celsius within 90 minutes of cooking and stored at a maintained temperature of 0 to 3 degrees Celsius. The meals can then be transported in refrigerated transport to where the food is to be reheated and consumed when needed. \nThe length of storage depends on the method used but is usually five days. For longer storage the food may be subjected to pasteurization after cooking.\n\nThese processes have the advantage that preparation and cooking of meals is not tied to the times when the food is to be served, enabling staff and equipment to be used more efficiently. A properly managed operation is capable of supplying high quality meals economically despite high initial equipment costs. There are potential problems; careful attention has to be paid to hygiene, as there are a number of points in the process where food pathogens can gain access. This requires careful attention to both the control of the process and to staff training.\n\nSnap-freeze is a term often used in scientific papers to describe a process by which a sample is very quickly lowered to temperatures below -70 °C. This is often accomplished by submerging a sample in liquid nitrogen. This prevents water from crystallising when it forms ice, and so better preserves the structure of the sample (e.g. RNA, protein, or live cells)\n\n"}
{"id": "17281970", "url": "https://en.wikipedia.org/wiki?curid=17281970", "title": "Trailing cone", "text": "Trailing cone\n\nTrailing cones (or trailing wires as they are often incorrectly called or trailing static cones), were first developed and tested in the 1950s and 1960s as a simple means of calibrating the static pressure (altitude reporting) error of an aircraft's pitot-static system. It does this by giving an accurate measurement of the ambient atmospheric pressure (static pressure) well clear of the aircraft's fuselage. The trailing cone system trails at least one fuselage length behind the aircraft (SpaceAge Control) via a high-strength pressure tube. Static pressure is measured forward of the cone by several static ports. The cone stabilizes and aligns the ports relative to the freestream airflow.\n\nThe FAA states in Advisory Circular AC 91-85A:\n\"Where precision flight calibrations are used to quantify or verify altimetry system performance they may be accomplished by any of the following methods. Flight calibrations should only be performed once appropriate ground checks have been completed. Uncertainties in application of the method must be assessed and taken into account in the data package.\n\n\n\n\n"}
{"id": "4308043", "url": "https://en.wikipedia.org/wiki?curid=4308043", "title": "Walking bus", "text": "Walking bus\n\nA walking bus is a form of student transport for schoolchildren who, chaperoned by two adults (a \"Driver\" leads and a \"conductor\" follows), walk to school along a set route, in much the same way a school bus would drive them to school. Like a traditional bus, walking buses have a fixed route with designated \"bus stops\" and \"pick up times\" in which they pick up children.\n\nThe concept of the walking bus was first invented in Japan Australian transport activist David Engwicht is often given credit for inventing the WSB system in the 1990s. It was introduced in the United Kingdom in 1998 by Hertfordshire County Council. It was first used by pupils of Wheatfields Junior School in St Albans, United Kingdom in 1998 \n\nWalking Buses have remained popular in the United Kingdom and have recently gained a level of popularity elsewhere in Europe, North America and New Zealand. Proponents of walking buses say that its aims are to:\n\n\nIn Auckland, New Zealand, as of November 2007, one hundred schools run 230 Walking School Buses with over 4,000 children and 1,500 adults participating.\n\nIn some countries, parents and/or children on walking buses are encouraged to wear brightly coloured jackets or waistcoats. This has led to criticism that the walking bus is too regimented, and fails to achieve its original purpose of improving children's independent mobility. David Engwicht, whose 1992 book \"Reclaiming our Cities and Towns\" is credited by some as the origin of the Walking School Bus concept, has since stated that \"The moment the Walking Bus turns into an official program, it creates some significant difficulties, particularly in litigious and risk-adverse cultures.\"\n\nThe walking school bus can help to reduce childhood obesity rates in the United States by increasing active transportation to school. The American Public Health Association cites that participation in active transportation to school has reduced by a third in the last 40 years. The reduction in active transportation has coincided with an increase in childhood obesity rates. The Centers for Disease Control and Prevention (CDC) reports the prevalence of obesity among children has doubled and among adolescents has quadrupled in the past 30 years. In 2012, 18% of children and 21% of adolescents were obese.\n\nAccording to the Safe Routes to School National Partnership, 1 mile of walking equates to 2/3rds of the recommendation of 60 minutes a day. In addition to the potential health benefits, the walking school bus can help children arrive to school safely, on time, and ready to learn. Walking to school counts towards the recommended 60 minutes of physical activity a day. Another benefit is the positive association between physical activity and improved academic performance. In a study done by the CDC, they found that with a minimum of 60 minutes a day there were increase in academic behavior, cognitive skills and attitudes.\n\nThe communities that support walking buses may see reduced congestion around school grounds. The walking bus is a non-polluting and sustainable transport alternative to cars and buses. In regards to the safety of the walking bus; the section about Safe Routes to School below goes into more detail about improvements being made to increase the safety for commuting pedestrians.\n\nThe built environment plays the biggest role in whether or not a community is walkable. The built environment consists of the human made surroundings that affect one’s life. There are numerous factors that make up the built environment including the availability and quality of sidewalks, crosswalks and parks, the amount of traffic and proximity to schools/parks/shops, etc. These factors determine the walkability of an area.\n\nWalkability has been shown to be closely tied to childhood obesity. One study found that “the chances of a child being obese or overweight were 20-60 percent higher among children in neighborhoods where it was not safe to walk around or where there were no sidewalks.” In addition, children in neighborhoods with sidewalks and safe places to cross the street are more likely to be physically active than children living in neighborhoods without those safe infrastructure elements.\n\nThe walking bus depends on a walkable and safe built environment. Safe Routes to School is US government program that funds improvements in the built environment near schools.\n\nChildren who walk or bicycle to school have higher daily levels of physical activity and better cardiovascular fitness than do children who do not actively commute to school. In a study of adolescents, 100% of the students who walked both to and from school met the recommended levels of 60 or more minutes of moderate to vigorous physical activity on weekdays.\n\nIn a pilot, randomized controlled trial (RCT) in Houston, Texas, children that were assigned to a Walking School Bus group increased their weekly rate of active commuting by 38.0% over a five-week period, while children assigned to a no intervention group decreased their active commuting rate by a small margin.\n\nIn 2005, the US Federal Transportation Reauthorization Bill—The Safe, Accountable, Flexible, Efficient Transportation Equity Act (SAFETEA-LU 109-59) was passed which provided nearly $1 billion funding to all states for Safe Routes to School (SRTS) programs. The program's purpose is to: “enable and encourage children, including those with disabilities, to walk and bicycle to school; to make walking and bicycling to school safe and more appealing; and to facilitate the planning, development and implementation of projects that will improve safety, and reduce traffic, fuel consumption, and air pollution in the vicinity of schools.” It is a 100% federally funded program which is administered by state departments of transportation. Money goes towards infrastructure-related programs in addition to non-infrastructure activities such as education and enforcement-year time period, restricting data to school-travel hours.\n\nThis law is central to the success of programs like the Walking Bus because it identifies the many components involved. Active transport to school and examples such as the walking school bus require collaboration from many different sectors (transportation, law enforcement, school policies, and local government to name a few). In addition, the ability to create an environment conducive to walkability is dependent on the built environment as well as numerous policies from these various sectors.\nAccording to the Institute of Medicine Report, Measuring Progress in Obesity Prevention, the environment people live in can have a profound effect on the amount of physical activity which they engage in. According to James Sallis in his workshop presentation, informal or formal policies issued by the government or private sector, can affect physical activity in four ways. First, zoning, building codes, public transportation and recreational facility policies affect ability to engage in physical activity. Second, policies affecting physical activity in schools—physical education classes, recess, and walkability to and from school. Policies providing incentives such as parking and commuting in other ways, and insurance subsidies promote walkability and physical activity for adults. Finally funding policies have a strong influence on physical activity. The 2005 federal law supporting safe Routes to School is meant to facilitate physical activity and increase walkability.\n\nTwo national programs that focus on physical activity environment policies are the Health and Human Services Healthy People 2020 and the National Physical Activity Plan, sponsored by numerous organizations including the YMCA, the American Heart Association and the American Cancer Association. The plan is a public-private partnership to create policies that promote physical activity for all individuals. Healthy People 2020 lists objectives to boost physical activity and also supports the recording of national data to track interventions. These are two examples of policies to promote physical activity. The Walking School Bus fits into their programs because it increases activity and translates a sedentary time of the day (transport in car or bus to school) to an active one.\n\nPedestrian injuries have found to be associated with the built environment elements, rather than with increased walking to school. Safe Routes to School (SRTS) provides federal funds for improvements of the built environment near schools with the goal of increasing walking and biking to school.\n\nThe effectiveness of SRTS has been demonstrated in multiple studies. One national study analyzed data across 18 states over 16 years. The study compared rates of pedestrian/bicyclist injury in school-aged children to adults, restricting data to school-travel hours. While adult injury rates were largely unchanged over the study period, implementation of SRTS was associated with around a 20% reduction in both in pedestrian/bicyclist injury and fatality risk in children. In Texas, traffic crash data over 5 years showed that pedestrian and bicyclist injuries decreased about 40% after implementation of SRTS. In New York city, SRTS decreased pedestrian injury by one-third in school-age children.\n\nThe benefits of SRTS may extend beyond children. While the national study showed adult pedestrian injury risk unchanged, both the Texas and New York studies have demonstrated reduced pedestrian injury risk in adults with SRTS interventions.\n\nSRTS programs are also effective in increasing walking and bicycling. In California, the effect of SRTS implementation varied widely but rates of observed walking and biking to school increased by 20 to 200%.\n\nBicycle train is the same concept but instead of walking the children and adult(s) move by bicycle.\n\nIn Japan, some traffic accidents have occurred that resulted in the death of multiple schoolkids who were walking to school in a group. In response to these accidents, schools have chosen to let their students walk to school via different paths, and cities have chosen to reduce speed limits, install road guards, and paint pedestrian zones to reduce the chance of accidents occurring again.\n\n\n"}
{"id": "25000433", "url": "https://en.wikipedia.org/wiki?curid=25000433", "title": "Waveguide filter", "text": "Waveguide filter\n\nA waveguide filter is an electronic filter that is constructed with waveguide technology. Waveguides are hollow metal tubes inside which an electromagnetic wave may be transmitted. Filters are devices used to allow signals at some frequencies to pass (the passband), while others are rejected (the stopband). Filters are a basic component of electronic engineering designs and have numerous applications. These include selection of signals and limitation of noise. Waveguide filters are most useful in the microwave band of frequencies, where they are a convenient size and have low loss. Examples of microwave filter use are found in satellite communications, telephone networks, and television broadcasting.\n\nWaveguide filters were developed during World War II to meet the needs of radar and electronic countermeasures, but afterwards soon found civilian applications such as use in microwave links. Much of post-war development was concerned with reducing the bulk and weight of these filters, first by using new analysis techniques that led to elimination of unnecessary components, then by innovations such as dual-mode cavities and novel materials such as ceramic resonators.\n\nA particular feature of waveguide filter design concerns the mode of transmission. Systems based on pairs of conducting wires and similar technologies have only one mode of transmission. In waveguide systems, any number of modes are possible. This can be both a disadvantage, as spurious modes frequently cause problems, and an advantage, as a dual-mode design can be much smaller than the equivalent waveguide single mode design. The chief advantages of waveguide filters over other technologies are their ability to handle high power and their low loss. The chief disadvantages are their bulk and cost when compared with technologies such as microstrip filters.\n\nThere is a wide array of different types of waveguide filters. Many of them consist of a chain of coupled resonators of some kind that can be modelled as a ladder network of LC circuits. One of the most common types consists of a number of coupled resonant cavities. Even within this type, there are many subtypes, mostly differentiated by the means of coupling. These coupling types include apertures, irises, and posts. Other waveguide filter types include dielectric resonator filters, insert filters, finline filters, corrugated-waveguide filters, and stub filters. A number of waveguide components have filter theory applied to their design, but their purpose is something other than to filter signals. Such devices include impedance matching components, directional couplers, and diplexers. These devices frequently take on the form of a filter, at least in part.\n\nThe common meaning of \"waveguide\", when the term is used unqualified, is the hollow metal kind, but other waveguide technologies are possible. The scope of this article is limited to the metal-tube type. The post-wall waveguide structure is something of a variant, but is related enough to include in this article—the wave is mostly surrounded by conducting material. It is possible to construct waveguides out of dielectric rods, the most well known example being optical fibres. This subject is outside the scope of the article with the exception that dielectric rod resonators are sometimes used \"inside\" hollow metal waveguides. Transmission line technologies such as conducting wires and microstrip can be thought of as waveguides, but are not commonly called such and are also outside the scope of this article.\n\nIn electronics, filters are used to allow signals of a certain band of frequencies to pass while blocking others. They are a basic building block of electronic systems and have a great many applications. Amongst the uses of waveguide filters are the construction of duplexers, diplexers, and multiplexers; selectivity and noise limitation in receivers; and harmonic distortion suppression in transmitters.\n\nWaveguides are metal conduits used to confine and direct radio signals. They are usually made of brass, but aluminium and copper are also used. Most commonly they are rectangular, but other cross-sections such as circular or elliptical are possible. A waveguide filter is a filter composed of waveguide components. It has much the same range of applications as other filter technologies in electronics and radio engineering but is very different mechanically and in principle of operation.\n\nThe technology used for constructing filters is chosen to a large extent by the frequency of operation that is expected, although there is a large amount of overlap. Low frequency applications such as audio electronics use filters composed of discrete capacitors and inductors. Somewhere in the very high frequency band, designers switch to using components made of pieces of transmission line. These kinds of designs are called distributed element filters. Filters made from discrete components are sometimes called lumped element filters to distinguish them. At still higher frequencies, the microwave bands, the design switches to waveguide filters, or sometimes a combination of waveguides and transmission lines.\n\nWaveguide filters have much more in common with transmission line filters than lumped element filters; they do not contain any discrete capacitors or inductors. However, the waveguide design may frequently be equivalent (or approximately so) to a lumped element design. Indeed, the design of waveguide filters frequently starts from a lumped element design and then converts the elements of that design into waveguide components.\n\nOne of the most important differences in the operation of waveguide filters compared to transmission line designs concerns the mode of transmission of the electromagnetic wave carrying the signal. In a transmission line, the wave is associated with electric currents on a pair of conductors. The conductors constrain the currents to be parallel to the line, and consequently both the magnetic and electric components of the electromagnetic field are perpendicular to the direction of travel of the wave. This transverse mode is designated TEM (transverse electromagnetic). On the other hand, there are infinitely many modes that any completely hollow waveguide can support, but the TEM mode is not one of them. Waveguide modes are designated either TE (transverse electric) or TM (transverse magnetic), followed by a pair of suffixes identifying the precise mode.\n\nThis multiplicity of modes can cause problems in waveguide filters when spurious modes are generated. Designs are usually based on a single mode and frequently incorporate features to suppress the unwanted modes. On the other hand, advantage can be had from choosing the right mode for the application, and even sometimes making use of more than one mode at once. Where only a single mode is in use, the waveguide can be modelled like a conducting transmission line and results from transmission line theory can be applied.\n\nAnother feature peculiar to waveguide filters is that there is a definite frequency, the cutoff frequency, below which no transmission can take place. This means that in theory low-pass filters cannot be made in waveguides. However, designers frequently take a lumped element low-pass filter design and convert it to a waveguide implementation. The filter is consequently low-pass by design and may be considered a low-pass filter for all practical purposes if the cutoff frequency is below any frequency of interest to the application. The waveguide cutoff frequency is a function of transmission mode, so at a given frequency, the waveguide may be usable in some modes but not others. Likewise, the guide wavelength (λ) and characteristic impedance (\"Z\") of the guide at a given frequency also depend on mode.\n\nThe mode with the lowest cutoff frequency of all the modes is called the dominant mode. Between cutoff and the next highest mode, this is the only mode it is possible to transmit, which is why it is described as dominant. Any spurious modes generated are rapidly attenuated along the length of the guide and soon disappear. Practical filter designs are frequently made to operate in the dominant mode.\n\nIn rectangular waveguide, the TE mode (shown in figure 2) is the dominant mode. There is a band of frequencies between the dominant mode cutoff and the next highest mode cutoff in which the waveguide can be operated without any possibility of generating spurious modes. The next highest cutoff modes are TE, at exactly twice the TE mode, and TE which is also twice TE if the waveguide used has the commonly used aspect ratio of 2:1. The lowest cutoff TM mode is TM (shown in figure 2) which is formula_1 times the dominant mode in 2:1 waveguide. Thus, there is an octave over which the dominant mode is free of spurious modes, although operating too close to cutoff is usually avoided because of phase distortion.\n\nIn circular waveguide, the dominant mode is TE and is shown in figure 2. The next highest mode is TM. The range over which the dominant mode is guaranteed to be spurious-mode free is less than that in rectangular waveguide; the ratio of highest to lowest frequency is approximately 1.3 in circular waveguide, compared to 2.0 in rectangular guide.\n\nEvanescent modes are modes below the cutoff frequency. They cannot propagate down the waveguide for any distance, dying away exponentially. However, they are important in the functioning of certain filter components such as irises and posts, described later, because energy is stored in the evanescent wave fields.\n\nLike transmission line filters, waveguide filters always have multiple passbands, replicas of the lumped element prototype. In most designs, only the lowest frequency passband is useful (or lowest two in the case of band-stop filters) and the rest are considered unwanted spurious artefacts. This is an intrinsic property of the technology and cannot be designed out, although design can have some control over the frequency position of the spurious bands. Consequently, in any given filter design, there is an upper frequency beyond which the filter will fail to carry out its function. For this reason, true low-pass and high-pass filters cannot exist in waveguide. At some high frequency there will be a spurious passband or stopband interrupting the intended function of the filter. But, similar to the situation with waveguide cutoff frequency, the filter can be designed so that the edge of the first spurious band is well above any frequency of interest.\n\nThe range of frequencies over which waveguide filters are useful is largely determined by the waveguide size needed. At lower frequencies the waveguide needs to be impractically large in order to keep the cutoff frequency below the operational frequency. On the other hand, filters whose operating frequencies are so high that the wavelengths are sub-millimetre cannot be manufactured with normal machine shop processes. At frequencies this high, fibre-optic technology starts to become an option.\n\nWaveguides are a low-loss medium. Losses in waveguides mostly come from ohmic dissipation caused by currents induced in the waveguide walls. Rectangular waveguide has lower loss than circular waveguide and is usually the preferred format, but the TE circular mode is very low loss and has applications in long distance communications. Losses can be reduced by polishing the internal surfaces of the waveguide walls. In some applications which require rigorous filtering, the walls are plated with a thin layer of gold or silver to improve surface conductivity. An example of such requirements is satellite applications which require low loss, high selectivity, and linear group delay from their filters.\n\nOne of the main advantages of waveguide filters over TEM mode technologies is the quality of their resonators. Resonator quality is characterised by a parameter called Q factor, or just \"Q\". The \"Q\" of waveguide resonators is in the thousands, orders of magnitude higher than TEM mode resonators. The resistance of conductors, especially in wound inductors, limits the \"Q\" of TEM resonators. This improved \"Q\" leads to better performing filters in waveguides, with greater stop band rejection. The limitation to \"Q\" in waveguides comes mostly from the ohmic losses in the walls described earlier, but silver plating the internal walls can more than double \"Q\".\n\nWaveguides have good power handling capability, which leads to filter applications in radar. Despite the performance advantages of waveguide filters, microstrip is often the preferred technology due to its low cost. This is especially true for consumer items and the lower microwave frequencies. Microstrip circuits can be manufactured by cheap printed circuit technology, and when integrated on the same printed board as other circuit blocks they incur little additional cost.\n\nThe idea of a waveguide for electromagnetic waves was first suggested by Lord Rayleigh in 1897. Rayleigh proposed that a coaxial transmission line could have the centre conductor removed, and waves would still propagate down the inside of the remaining cylindrical conductor despite there no longer being a complete electrical circuit of conductors. He described this in terms of the wave reflecting repeatedly off the internal wall of the outer conductor in a zig-zag fashion as it progressed down the waveguide. Rayleigh was also the first to realise that there was a critical wavelength, the cutoff wavelength, proportional to the cylinder diameter, above which wave propagation is not possible. However, interest in waveguides waned because lower frequencies were more suitable for long distance radio communication. Rayleigh's results were forgotten for a time and had to be rediscovered by others in the 1930s when interest in microwaves revived. Waveguides were first developed, in a circular form, by George Clark Southworth and J. F. Hargreaves in 1932.\n\nThe first analogue filter design which went beyond a simple single resonator was created by George Ashley Campbell in 1910 and marked the beginning of filter theory. Campbell's filter was a lumped-element design of capacitors and inductors suggested by his work with loading coils. Otto Zobel and others quickly developed this further. Development of distributed element filters began in the years before World War II. A major paper on the subject was published by Mason and Sykes in 1937; a patent filed by Mason in 1927 may contain the first published filter design using distributed elements.\nMason and Sykes' work was focused on the formats of coaxial cable and balanced pairs of wires, but other researchers later applied the principles to waveguides as well. Much development on waveguide filters was carried out during World War II driven by the filtering needs of radar and electronic countermeasures. A good deal of this was at the MIT Radiation Laboratory (Rad Lab), but other laboratories in the US and the UK were also involved such as the Telecommunications Research Establishment in the UK. Amongst the well-known scientists and engineers at Rad Lab were Julian Schwinger, Nathan Marcuvitz, Edward Mills Purcell, and Hans Bethe. Bethe was only at Rad Lab a short time but produced his aperture theory while there. Aperture theory is important for waveguide cavity filters, which were first developed at Rad Lab. Their work was published after the war in 1948 and includes an early description of dual-mode cavities by Fano and Lawson.\n\nTheoretical work following the war included the commensurate line theory of Paul Richards. Commensurate lines are networks in which all the elements are the same length (or in some cases multiples of the unit length), although they may differ in other dimensions to give different characteristic impedances. Richards' transformation allows any lumped element design to be taken \"as is\" and transformed directly into a distributed element design using a very simple transform equation. In 1955 K. Kuroda published the transformations known as Kuroda's identities. These made Richard's work more usable in unbalanced and waveguide formats by eliminating the problematic series connected elements, but it was some time before Kuroda's Japanese work became widely known in the English speaking world. Another theoretical development was the network synthesis filter approach of Wilhelm Cauer in which he used the Chebyshev approximation to determine element values. Cauer's work was largely developed during World War II (Cauer was killed towards the end of it), but could not be widely published until hostilities ended. While Cauer's work concerns lumped elements, it is of some importance to waveguide filters; the Chebyshev filter, a special case of Cauer's synthesis, is widely used as a prototype filter for waveguide designs.\n\nDesigns in the 1950s started with a lumped element prototype (a technique still in use today), arriving after various transformations at the desired filter in a waveguide form. At the time, this approach was yielding fractional bandwidths no more than about . In 1957, Leo Young at Stanford Research Institute published a method for designing filters which \"started\" with a distributed element prototype, the stepped impedance prototype. This filter was based on quarter-wave impedance transformers of various widths and was able to produce designs with bandwidths up to an octave (a fractional bandwidth of ). Young's paper specifically addresses directly coupled cavity resonators, but the procedure can equally be applied to other directly coupled resonator types.\n\nThe first published account of a cross-coupled filter is due to John R. Pierce at Bell Labs in a 1948 patent. A cross-coupled filter is one in which resonators that are not immediately adjacent are coupled. The additional degrees of freedom thus provided allow the designer to create filters with improved performance, or, alternatively, with fewer resonators. One version of Pierce's filter, shown in figure 3, uses circular waveguide cavity resonators to link between rectangular guide cavity resonators. This principle was not at first much used by waveguide filter designers, but it was used extensively by mechanical filter designers in the 1960s, particularly R. A. Johnson at Collins Radio Company.\n\nThe initial non-military application of waveguide filters was in the microwave links used by telecommunications companies to provide the backbone of their networks. These links were also used by other industries with large, fixed networks, notably television broadcasters. Such applications were part of large capital investment programs. They are now also used in satellite communications systems.\n\nThe need for frequency-independent delay in satellite applications led to more research into the waveguide incarnation of cross-coupled filters. Previously, satellite communications systems used a separate component for delay equalisation. The additional degrees of freedom obtained from cross-coupled filters held out the possibility of designing a flat delay into a filter without compromising other performance parameters. A component that simultaneously functioned as both filter and equaliser would save valuable weight and space. The needs of satellite communication also drove research into the more exotic resonator modes in the 1970s. Of particular prominence in this respect is the work of E. L. Griffin and F. A. Young, who investigated better modes for the band when this began to be used for satellites in the mid-1970s.\n\nAnother space-saving innovation was the dielectric resonator, which can be used in other filter formats as well as waveguide. The first use of these in a filter was by S. B. Cohn in 1965, using titanium dioxide as the dielectric material. Dielectric resonators used in the 1960s, however, had very poor temperature coefficients, typically 500 times worse than a mechanical resonator made of invar, which led to instability of filter parameters. Dielectric materials of the time with better temperature coefficients had too low a dielectric constant to be useful for space saving. This changed with the introduction of ceramic resonators with very low temperature coefficients in the 1970s. The first of these was from Massé and Pucel using barium tetratitanate at Raytheon in 1972. Further improvements were reported in 1979 by Bell Labs and Murata Manufacturing. Bell Labs' barium nonatitanate resonator had a dielectric constant of 40 and \"Q\" of 5000–10,000 at . Modern temperature-stable materials have a dielectric constant of about 90 at microwave frequencies, but research is continuing to find materials with both low loss and high permittivity; lower permittivity materials, such as zirconium stannate titanate (ZST) with a dielectric constant of 38, are still sometimes used for their low loss property.\n\nAn alternative approach to designing smaller waveguide filters was provided by the use of non-propagating evanescent modes. Jaynes and Edson proposed evanescent mode waveguide filters in the late 1950s. Methods for designing these filters were created by Craven and Young in 1966. Since then, evanescent mode waveguide filters have seen successful use where waveguide size or weight are important considerations.\n\nA relatively recent technology being used inside hollow-metal-waveguide filters is finline, a kind of planar dielectric waveguide. Finline was first described by Paul Meier in 1972.\n\nMultiplexers were first described by Fano and Lawson in 1948. Pierce was the first to describe multiplexers with contiguous passbands. Multiplexing using directional filters was invented by Seymour Cohn and Frank Coale in the 1950s. Multiplexers with compensating immittance resonators at each junction are largely the work of E. G. Cristal and G. L. Matthaei in the 1960s. This technique is still sometimes used, but the modern availability of computing power has led to the more common use of synthesis techniques which can directly produce matching filters without the need for these additional resonators. In 1965 R. J. Wenzel discovered that filters which were singly terminated, rather than the usual doubly terminated, were complementary—exactly what was needed for a diplexer. Wenzel was inspired by the lectures of circuit theorist Ernst Guillemin.\n\nMulti-channel, multi-octave multiplexers were investigated by Harold Schumacher at Microphase Corporation, and his results were published in 1976. The principle that multiplexer filters may be matched when joined together by modifying the first few elements, thus doing away with the compensating resonators, was discovered accidentally by E. J. Curly around 1968 when he mistuned a diplexer. A formal theory for this was provided by J. D. Rhodes in 1976 and generalised to multiplexers by Rhodes and Ralph Levy in 1979.\n\nFrom the 1980s, planar technologies, especially microstrip, have tended to replace other technologies used for constructing filters and multiplexers, especially in products aimed at the consumer market. The recent innovation of post-wall waveguide allows waveguide designs to be implemented on a flat substrate with low-cost manufacturing techniques similar to those used for microstrip.\n\nWaveguide filter designs frequently consist of two different components repeated a number of times. Typically, one component is a resonator or discontinuity with a lumped circuit equivalent of an inductor, capacitor, or LC resonant circuit. Often, the filter type will take its name from the style of this component. These components are spaced apart by a second component, a length of guide which acts as an impedance transformer. The impedance transformers have the effect of making alternate instances of the first component appear to be a different impedance. The net result is a lumped element equivalent circuit of a ladder network. Lumped element filters are commonly ladder topology, and such a circuit is a typical starting point for waveguide filter designs. Figure 4 shows such a ladder. Typically, waveguide components are resonators, and the equivalent circuit would be LC resonators instead of the capacitors and inductors shown, but circuits like figure 4 are still used as prototype filters with the use of a band-pass or band-stop transformation.\n\nFilter performance parameters, such as stopband rejection and rate of transition between passband and stopband, are improved by adding more components and thus increasing the length of the filter. Where the components are repeated identically, the filter is an image parameter filter design, and performance is enhanced simply by adding more identical elements. This approach is typically used in filter designs which use a large number of closely spaced elements such as the waffle-iron filter. For designs where the elements are more widely spaced, better results can be obtained using a network synthesis filter design, such as the common Chebyshev filter and Butterworth filters. In this approach the circuit elements do not all have the same value, and consequently the components are not all the same dimensions. Furthermore, if the design is enhanced by adding more components then all the element values must be calculated again from scratch. In general, there will be no common values between the two instances of the design. Chebyshev waveguide filters are used where the filtering requirements are rigorous, such as satellite applications.\n\nAn impedance transformer is a device which makes an impedance at its output port appear as a different impedance at its input port. In waveguide, this device is simply a short length of waveguide. Especially useful is the quarter-wave impedance transformer which has a length of λ/4. This device can turn capacitances into inductances and vice versa. It also has the useful property of turning shunt-connected elements into series-connected elements and vice versa. Series-connected elements are otherwise difficult to implement in waveguide.\n\nMany waveguide filter components work by introducing a sudden change, a discontinuity, to the transmission properties of the waveguide. Such discontinuities are equivalent to lumped impedance elements placed at that point. This arises in the following way: the discontinuity causes a partial reflection of the transmitted wave back down the guide in the opposite direction, the ratio of the two being known as the reflection coefficient. This is entirely analogous to a reflection on a transmission line where there is an established relationship between reflection coefficient and the impedance that caused the reflection. This impedance must be reactive, that is, it must be a capacitance or an inductance. It cannot be a resistance since no energy has been absorbed—it is all either transmitted onward or reflected. Examples of components with this function include irises, stubs, and posts, all described later in this article under the filter types in which they occur.\n\nAn impedance step is an example of a device introducing a discontinuity. It is achieved by a step change in the physical dimensions of the waveguide. This results in a step change in the characteristic impedance of the waveguide. The step can be in either the E-plane (change of height) or the H-plane (change of width) of the waveguide.\n\nA basic component of waveguide filters is the cavity resonator. This consists of a short length of waveguide blocked at both ends. Waves trapped inside the resonator are reflected back and forth between the two ends. A given geometry of cavity will resonate at a characteristic frequency. The resonance effect can be used to selectively pass certain frequencies. Their use in a filter structure requires that some of the wave is allowed to pass out of one cavity into another through a coupling structure. However, if the opening in the resonator is kept small then a valid design approach is to design the cavity as if it were completely closed and errors will be minimal. A number of different coupling mechanisms are used in different classes of filter.\n\nThe nomenclature for modes in a cavity introduces a third index, for example TE. The first two indices describe the wave travelling up and down the length of the cavity, that is, they are the transverse mode numbers as for modes in a waveguide. The third index describes the longitudinal mode caused by the interference pattern of the forward travelling and reflected waves. The third index is equal to the number of half wavelengths down the length of the guide. The most common modes used are the dominant modes: TE in rectangular waveguide, and TE in circular waveguide. TE circular mode is used where very low loss (hence high \"Q\") is required but cannot be used in a dual-mode filter because it is circularly symmetric. Better modes for rectangular waveguide in dual-mode filters are TE and TE. However, even better is the TE circular waveguide mode which can achieve a \"Q\" of 16,000 at .\n\nTuning screws are screws inserted into resonant cavities which can be adjusted externally to the waveguide. They provide fine tuning of the resonant frequency by inserting more, or less thread into the waveguide. Examples can be seen in the post filter of figure 1: each cavity has a tuning screw secured with jam nuts and thread-locking compound. For screws inserted only a small distance, the equivalent circuit is a shunt capacitor, increasing in value as the screw is inserted. However, when the screw has been inserted a distance λ/4 it resonates equivalent to a series LC circuit. Inserting it further causes the impedance to change from capacitive to inductive, that is, the arithmetic sign changes.\n\nAn iris is a thin metal plate across the waveguide with one or more holes in it. It is used to couple together two lengths of waveguide and is a means of introducing a discontinuity. Some of the possible geometries of irises are shown in figure 5. An iris which reduces the width of a rectangular waveguide has an equivalent circuit of a shunt inductance, whereas one which restricts the height is equivalent to a shunt capacitance. An iris which restricts both directions is equivalent to a parallel LC resonant circuit. A series LC circuit can be formed by spacing the conducting portion of the iris away from the walls of the waveguide. Narrowband filters frequently use irises with small holes. These are always inductive regardless of the shape of the hole or its position on the iris. Circular holes are simple to machine, but elongated holes, or holes in the shape of a cross, are advantageous in allowing the selection of a particular mode of coupling.\n\nIrises are a form of discontinuity and work by exciting evanescent higher modes. Vertical edges are parallel to the electric field (E field) and excite TE modes. The stored energy in TE modes is predominately in the magnetic field (H field), and consequently the lumped equivalent of this structure is an inductor. Horizontal edges are parallel to the H field and excite TM modes. In this case the stored energy is predominately in the E field and the lumped equivalent is a capacitor.\n\nIt is fairly simple to make irises that are mechanically adjustable. A thin plate of metal can be pushed in and out of a narrow slot in the side of the waveguide. The iris construction is sometimes chosen for this ability to make a variable component.\n\nAn iris-coupled filter consists of a cascade of impedance transformers in the form of waveguide resonant cavities coupled together by irises. In high power applications capacitive irises are avoided. The reduction in height of the waveguide (the direction of the E field) causes the electric field strength across the gap to increase and arcing (or dielectric breakdown if the waveguide is filled with an insulator) will occur at a lower power than it would otherwise.\n\nPosts are conducting bars, usually circular, fixed internally across the height of the waveguide and are another means of introducing a discontinuity. A thin post has an equivalent circuit of a shunt inductor. A row of posts can be viewed as a form of inductive iris.\n\nA post filter consists of several rows of posts across the width of the waveguide which separate the waveguide into resonant cavities as shown in figure 7. Differing numbers of posts can be used in each row to achieve varying values of inductance. An example can be seen in figure 1. The filter operates in the same way as the iris-coupled filter but differs in the method of construction.\n\nA post-wall waveguide, or substrate integrated waveguide, is a more recent format that seeks to combine the advantages of low radiation loss, high \"Q\", and high power handling of traditional hollow metal pipe waveguide with the small size and ease of manufacture of planar technologies (such as the widely used microstrip format). It consists of an insulated substrate pierced with two rows of conducting posts which stand in for the side walls of the waveguide. The top and bottom of the substrate are covered with conducting sheets making this a similar construction to the triplate format. The existing manufacturing techniques of printed circuit board or low temperature co-fired ceramic can be used to make post-wall waveguide circuits. This format naturally lends itself to waveguide post filter designs.\n\nA dual-mode filter is a kind of resonant cavity filter, but in this case each cavity is used to provide two resonators by employing two modes (two polarizations), so halving the volume of the filter for a given order. This improvement in size of the filter is a major advantage in aircraft avionics and space applications. High quality filters in these applications can require many cavities which occupy significant space.\n\nDielectric resonators are pieces of dielectric material inserted into the waveguide. They are usually cylindrical since these can be made without machining but other shapes have been used. They can be made with a hole through the centre which is used to secure them to the waveguide. There is no field at the centre when the TE circular mode is used so the hole has no adverse effect. The resonators can be mounted coaxial to the waveguide, but usually they are mounted transversally across the width as shown in figure 8. The latter arrangement allows the resonators to be tuned by inserting a screw through the wall of the waveguide into the centre hole of the resonator.\n\nWhen dielectric resonators are made from a high permittivity material, such as one of the barium titanates, they have an important space saving advantage compared to cavity resonators. However, they are much more prone to spurious modes. In high-power applications, metal layers may be built into the resonators to conduct heat away since dielectric materials tend to have low thermal conductivity.\n\nThe resonators can be coupled together with irises or impedance transformers. Alternatively, they can be placed in a stub-like side-housing and coupled through a small aperture.\n\nIn insert filters one or more metal sheets are placed longitudinally down the length of the waveguide as shown in figure 9. These sheets have holes punched in them to form resonators. The air dielectric gives these resonators a high \"Q\". Several parallel inserts may be used in the same length of waveguide. More compact resonators may be achieved with a thin sheet of dielectric material and printed metallisation instead of holes in metal sheets at the cost of a lower resonator \"Q\".\n\nFinline is a different kind of waveguide technology in which waves in a thin strip of dielectric are constrained by two strips of metallisation. There are a number of possible topological arrangements of the dielectric and metal strips. Finline is a variation of slot-waveguide but in the case of finline the whole structure is enclosed in a metal shield. This has the advantage that, like hollow metal waveguide, no power is lost by radiation. Finline filters can be made by printing a metallisation pattern on to a sheet of dielectric material and then inserting the sheet into the E-plane of a hollow metal waveguide much as is done with insert filters. The metal waveguide forms the shield for the finline waveguide. Resonators are formed by metallising a pattern on to the dielectric sheet. More complex patterns than the simple insert filter of figure 9 are easily achieved because the designer does not have to consider the effect on mechanical support of removing metal. This complexity does not add to the manufacturing costs since the number of processes needed does not change when more elements are added to the design. Finline designs are less sensitive to manufacturing tolerances than insert filters and have wide bandwidths.\n\nIt is possible to design filters that operate internally entirely in evanescent modes. This has space saving advantages because the filter waveguide, which often forms the housing of the filter, does not need to be large enough to support propagation of the dominant mode. Typically, an evanescent mode filter consists of a length of waveguide smaller than the waveguide feeding the input and output ports. In some designs this may be folded to achieve a more compact filter. Tuning screws are inserted at specific intervals along the waveguide producing equivalent lumped capacitances at those points. In more recent designs the screws are replaced with dielectric inserts. These capacitors resonate with the preceding length of evanescent mode waveguide which has the equivalent circuit of an inductor, thus producing a filtering action. Energy from many different evanescent modes is stored in the field around each of these capacitive discontinuities. However, the design is such that only the dominant mode reaches the output port; the other modes decay much more rapidly between the capacitors.\n\nCorrugated-waveguide filters, also called ridged-waveguide filters, consist of a number of ridges, or teeth, that periodically reduce the internal height of the waveguide as shown in figures 10 and 11. They are used in applications which simultaneously require a wide passband, good passband matching, and a wide stopband. They are essentially low-pass designs (above the usual limitation of the cutoff frequency), unlike most other forms which are usually band-pass. The distance between teeth is much smaller than the typical λ/4 distance between elements of other filter designs. Typically, they are designed by the image parameter method with all ridges identical, but other classes of filter such as Chebyshev can be achieved in exchange for complexity of manufacture. In the image design method the equivalent circuit of the ridges is modelled as a cascade of LC half sections. The filter operates in the dominant TE mode, but spurious modes can be a problem when they are present. In particular, there is little stopband attenuation of TE and TE modes.\n\nThe waffle-iron filter is a variant of the corrugated-waveguide filter. It has similar properties to that filter with the additional advantage that spurious TE and TE modes are suppressed. In the waffle-iron filter, channels are cut through the ridges longitudinally down the filter. This leaves a matrix of teeth protruding internally from the top and bottom surfaces of the waveguide. This pattern of teeth resembles a waffle iron, hence the name of the filter.\n\nA stub is a short length of waveguide connected to some point in the filter at one end and short-circuited at the other end. Open-circuited stubs are also theoretically possible, but an implementation in waveguide is not practical because electromagnetic energy would be launched out of the open end of the stub, resulting in high losses. Stubs are a kind of resonator, and the lumped element equivalent is an LC resonant circuit. However, over a narrow band, stubs can be viewed as an impedance transformer. The short-circuit is transformed into either an inductance or a capacitance depending on the stub length.\n\nA waveguide stub filter is made by placing one or more stubs along the length of a waveguide, usually λ/4 apart, as shown in figure 12. The ends of the stubs are blanked off to short-circuit them. When the short-circuited stubs are λ/4 long the filter will be a band-stop filter and the stubs will have a lumped-element approximate equivalent circuit of parallel resonant circuits connected in series with the line. When the stubs are λ/2 long, the filter will be a band-pass filter. In this case the lumped-element equivalent is series LC resonant circuits in series with the line.\n\nAbsorption filters dissipate the energy in unwanted frequencies internally as heat. This is in contrast to a conventional filter design where the unwanted frequencies are reflected back from the input port of the filter. Such filters are used where it is undesirable for power to be sent back towards the source. This is the case with high power transmitters where returning power can be high enough to damage the transmitter. An absorption filter may be used to remove transmitter spurious emissions such as harmonics or spurious sidebands. A design that has been in use for some time has slots cut in the walls of the feed waveguide at regular intervals. This design is known as a leaky-wave filter. Each slot is connected to a smaller gauge waveguide which is too small to support propagation of frequencies in the wanted band. Thus those frequencies are unaffected by the filter. Higher frequencies in the unwanted band, however, readily propagate along the side guides which are terminated with a matched load where the power is absorbed. These loads are usually a wedge shaped piece of microwave absorbent material. Another, more compact, design of absorption filter uses resonators with a lossy dielectric.\n\nThere are many applications of filters whose design objectives are something other than rejection or passing of certain frequencies. Frequently, a simple device that is intended to work over only a narrow band or just one spot frequency will not look much like a filter design. However, a broadband design for the same item requires many more elements and the design takes on the nature of a filter. Amongst the more common applications of this kind in waveguide are impedance matching networks, directional couplers, power dividers, power combiners, and diplexers. Other possible applications include multiplexers, demultiplexers, negative-resistance amplifiers, and time-delay networks.\n\nA simple method of impedance matching is stub matching with a single stub. However, a single stub will only produce a perfect match at one particular frequency. This technique is therefore only suitable for narrow band applications. To widen the bandwidth multiple stubs may be used, and the structure then takes on the form of a stub filter. The design proceeds as if it were a filter except that a different parameter is optimised. In a frequency filter typically the parameter optimised is stopband rejection, passband attenuation, steepness of transition, or some compromise between these. In a matching network the parameter optimised is the impedance match. The function of the device does not require a restriction of bandwidth, but the designer is nevertheless forced to choose a bandwidth because of the \"structure\" of the device.\n\nStubs are not the only format of filter than can be used. In principle, any filter structure could be applied to impedance matching, but some will result in more practical designs than others. A frequent format used for impedance matching in waveguide is the stepped impedance filter. An example can be seen in the duplexer pictured in figure 13.\n\nDirectional couplers, power splitters, and power combiners are all essentially the same type of device, at least when implemented with passive components. A directional coupler splits a small amount of power from the main line to a third port. A more strongly coupled, but otherwise identical, device may be called a power splitter. One that couples exactly half the power to the third port (a coupler) is the maximum coupling achievable without reversing the functions of the ports. Many designs of power splitter can be used in reverse, whereupon they become power combiners.\n\nA simple form of directional coupler is two parallel transmission lines coupled together over a λ/4 length. This design is limited because the electrical length of the coupler will only be λ/4 at one specific frequency. Coupling will be a maximum at this frequency and fall away on either side. Similar to the impedance matching case, this can be improved by using multiple elements, resulting in a filter-like structure. A waveguide analogue of this coupled lines approach is the Bethe-hole directional coupler in which two parallel waveguides are stacked on top of each other and a hole provided for coupling. To produce a wideband design, multiple holes are used along the guides as shown in figure 14 and a filter design applied. It is not only the coupled-line design that suffers from being narrow band, all simple designs of waveguide coupler depend on frequency in some way. For instance the rat-race coupler (which can be implemented directly in waveguide) works on a completely different principle but still relies on certain lengths being exact in terms of λ.\n\nA diplexer is a device used to combine two signals occupying different frequency bands into a single signal. This is usually to enable two signals to be transmitted simultaneously on the same communications channel, or to allow transmitting on one frequency while receiving on another. (This specific use of a diplexer is called a duplexer.) The same device can be used to separate the signals again at the far end of the channel. The need for filtering to separate the signals while receiving is fairly self-evident but it is also required even when combining two transmitted signals. Without filtering, some of the power from source A will be sent towards source B instead of the combined output. This will have the detrimental effects of losing a portion of the input power and loading source A with the output impedance of source B thus causing mismatch. These problems could be overcome with the use of a directional coupler, but as explained in the previous section, a wideband design requires a filter design for directional couplers as well.\n\nTwo widely spaced narrowband signals can be diplexed by joining together the outputs of two appropriate band-pass filters. Steps need to be taken to prevent the filters from coupling to each other when they are at resonance which would cause degradation of their performance. This can be achieved by appropriate spacing. For instance, if the filters are of the iris-coupled type then the iris nearest to the filter junction of filter A is placed λ/4 from the junction where λ is the guide wavelength in the passband of filter B. Likewise, the nearest iris of filter B is placed λ/4 from the junction. This works because when filter A is at resonance, filter B is in its stopband and only loosely coupled and vice versa. An alternative arrangement is to have each filter joined to a main waveguide at separate junctions. A decoupling resonator is placed λ/4 from the junction of each filter. This can be in the form of a short-circuited stub tuned to the resonant frequency of that filter. This arrangement can be extended to multiplexers with any number of bands.\n\nFor diplexers dealing with contiguous passbands proper account of the crossover characteristics of filters needs to be considered in the design. An especially common case of this is where the diplexer is used to split the entire spectrum into low and high bands. Here a low-pass and a high-pass filter are used instead of band-pass filters. The synthesis techniques used here can equally be applied to narrowband multiplexers and largely remove the need for decoupling resonators.\n\nA directional filter is a device that combines the functions of a directional coupler and a diplexer. As it is based on a directional coupler it is essentially a four-port device, but like directional couplers, port 4 is commonly permanently terminated internally. Power entering port 1 exits port 3 after being subject to some filtering function (usually band-pass). The remaining power exits port 2, and since no power is absorbed or reflected this will be the exact complement of the filtering function at port 2, in this case band-stop. In reverse, power entering ports 2 and 3 is combined at port 1, but now the power from the signals rejected by the filter is absorbed in the load at port 4. Figure 15 shows one possible waveguide implementation of a directional filter. Two rectangular waveguides operating in the dominant TE mode provide the four ports. These are joined together by a circular waveguide operating in the circular TE mode. The circular waveguide contains an iris coupled filter with as many irises as needed to produce the required filter response.\n\n </math>\n\n"}
{"id": "40595678", "url": "https://en.wikipedia.org/wiki?curid=40595678", "title": "Winaero", "text": "Winaero\n\nWinaero is a website hosting freeware tweaking tools for Microsoft Windows. It is operated by Russian developer, Sergey Tkachenko. The website sports 50+ freeware tools for modifying the behavior of Microsoft Windows. Notable amongst these are Skip Metro Suite which allowed skipping the Windows 8 Start screen, booting straight to the Windows desktop and customizing the Modern UI hot corners. Other notable tools include Ribbon Disabler, which allows disabling the Explorer Ribbon interface and Personalization Panel which replicates the full personalization restricted by low end editions of Windows. The latest addition is Winaero Tweaker which unifies most of the tools under a single tool to modify hidden Windows settings.\n\nThe website also regularly gives tips on tweaking Windows through its blog and offers free themes, visual styles and HD wallpapers to customize Windows.\n\nWinaero posts daily topics ranging from tweaks and tips for Windows, to troubleshooting guides, to free visual styles and theme packs.\n\nThe developer and founder of Winaero is Sergey Tkachenko.\n\nWinaero started in July 2011 as a simple English download page for the former Russian original project by Sergey Tkachenko. Winreview later ceased to operate and Winaero became the sole focus of the developer. From August 2012, Winaero started to publish English articles on its blog.\n\nWinaero's software utilities have been recognized by WinSuperSite, Lifehacker, PCWorld, Engadget and several other reputable news sites and blogs. A number of Winaero tools have been featured on these sites.\n\n"}
{"id": "863398", "url": "https://en.wikipedia.org/wiki?curid=863398", "title": "World Business Council for Sustainable Development", "text": "World Business Council for Sustainable Development\n\nThe World Business Council for Sustainable Development (WBCSD) is a CEO-led organization of over 200 international companies. The Council is also connected to 60 national and regional buisness councils and partner organizations. \n\nIts origins date back to the Rio de Janeiro Earth Summit of 1992, when Stephan Schmidheiny, a Swiss business entrepreneur, was appointed chief adviser for business and industry to the secretary general of the United Nations Conference on Environment and Development (UNCED). He created a forum called \"Business Council for Sustainable Development\", which went on to become \"Changing Course\", a book that coined the concept of eco-efficiency.\n\nThe WBCSD was created in 1995 in a merger of the Business Council for Sustainable Development and the World Industry Council for the Environment and is based at the Maison de la paix in Geneva, Switzerland, and offices in New York and New Delhi.\n\nThe Council works on a variety of issues related to sustainable development. It works to achieve the Sustainable Development Goals (SDGs) through the transformation of six economic systems. These are Circular Economy, Cities and Mobility, Climate and Energy, Food, Land and Water, People and Redefining Value. Each system transformation is set up as a WBCSD Program with a number of supplementary Projects. \n\nA 2003 World Bank/IFC commissioned study identified the WBCSD as one of the \"most influential forums\" for companies on corporate social responsibility (CSR) issues. A 2004 Globescan survey found the WBCSD as the second most effective SD research organization. The 2006 survey by the same company reports that 54% of all surveyed experts believe the WBCSD will play a \"major role\" in advancing sustainable development. Only the European Union received higher approval (69%).\n\nIn the 2007's Ethisphere Institute list of the 100 Most Influential People in Business Ethics, WBCSD President Bjoern Stigson has been ranked 9th, which made him the 2nd most influential NGO leader.\n\nMembership of the WBCSD is by invitation of the Executive Committee to companies committed to sustainable development. Among its over 180 members are well-known companies such as DuPont, 3M, Nestlé, BP, Danone and Royal Dutch Shell. WBCSD provides its members with the value, impact and voice to navigate unprecedented global changes and allows them to prosper from transforming their operations. By joining the WBCSD, member companies benefit from insights into the latest knowledge on sustainability, sustainability tools, peer collaboration across the value chain, participation in policy development and a chance to profile their sustainability work. \n\nMember companies pledge their support and contribution to the WBCSD by making available their knowledge and experience, and appropriate human resources. They are asked to publicly report on their environmental performance and to aspire to widen their reporting to cover all three pillars of sustainable development - economic, social and environmental.\n\nA key element is the personal commitment of the Chief Executive Officers (CEOs), acting as Council Members. They are influential advocates for the WBCSD’s policy positions and they co-chair the working groups. They also organize support for the WBCSD’s work program and ensure the adoption of sustainable management practices within their companies.\n\nForética claims to be global network of Spanish speaking businesses and professionals whose mission is to promote an ethical management and corporate social responsibility by the establishment of a National standard in Spain known as SGE-21, (sistema de gestación ética para siglo 21) which is also Annex 1 to ISO 26000 and supported by CSR Europe \nMembership is said to around 400 and includes companies of all sizes and sectors, as well as individual specialists, professionals and academics and it was incorporated into WBCSD on 2 September 2014 \n\nThe WBCSD is a member-led organization governed by a Council composed of the Council Members of its member companies. The Council elects the Executive Committee, including the Chairman and four Vice Chairmen. Past chairmen include:\n\n\nMost of WBCSD's member companies are headquartered in Europe (47%). 22% member companies are headquartered in Asia, 22% in North America and 5% in Latin America. The geographically least represented regions are Africa, Australasia and the Middle East with 1% each.\n\nAccording to Greenpeace the \"World Business Council for Sustainable Development \" is among the key players responsible for holding the world societies from tackling the climate change and energy management challenges for the past 20 years. The WBCSD Executive Committee was dominated by the largest nonrenewable energy and carbon-intensive companies in the world at least until 2011. According to Greenpeace the WBCSD executive committee has been a ‘Who’s Who’ of the world’s largest carbon-intensive companies.\n\nThe Sierra Club has collaborated with the World Business Council on a number of initiatives, as well as inviting its representatives to speak at Sierra Club events. The Environmental Defense Fund recommends the World Business Council's auditing methods to companies seeking to reduce greenhouse emissions, and the Natural Resources Defense Council has drawn upon WBCSD guidelines in drawing up their own guidelines for determining biofuels sustainability. The WBCSC's Vision 2020 report was highlighted by The Guardian as \"the largest concerted corporate sustainability action plan to date – include reversing the damage done to ecosystems, addressing rising greenhouse gas emissions and ensuring societies move to sustainable agriculture.\"\n\n"}
