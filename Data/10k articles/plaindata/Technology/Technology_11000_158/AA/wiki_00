{"id": "25680096", "url": "https://en.wikipedia.org/wiki?curid=25680096", "title": "4D BIM", "text": "4D BIM\n\n4D BIM, an acronym for 4D Building Information Modeling and a term widely used in the CAD industry, refers to the intelligent linking of individual 3D CAD components or assemblies with time- or schedule-related information. The use of the term 4D is intended to refer to the fourth dimension: time, i.e. 4D is 3D plus schedule (time). \n\nThe construction of the 4D models enables the various participants (from architects, designers, contractors to clients) of a construction project, to visualize the entire duration of a series of events and display the progress of construction activities through the lifetime of the project. This BIM-centric approach towards project management technique has a very high potential to improve the project management and delivery of construction project, of any size or complexity.\n\nIn 1998, Sir John Egan, in his report \"Rethinking Construction\", argued that certain principles and management techniques could successfully cross-over from other industries like manufacturing to serve the project delivery demands of the construction industry. The Egan Report cited \"Technology as a Tool\":\n\n4D BIM adds a new dimension (time) to 3D CAD or solid modelling, it enables a sequence of events to be depicted visually on a time line that has been populated by a 3D model (augmenting traditional Gantt charts and Critical Path (CPM) schedules often used in project management). Construction sequences can be reviewed as a series of problems using 4D BIM, enabling users to explore options, manage solutions and optimize results. It enables construction product development, collaborative and transparent project implementation, partnering with the supply chain and production of components, and is in keeping with Egan's vision: \"sustained improvement should then be delivered through use of techniques for eliminating waste and increasing value for the customer.\"\n\nAs an advanced construction management technique, it is increasingly used by project delivery teams working on larger projects. For example, it is used in the construction of projects including tall buildings, bridges, highways, tunnels, university campuses and hospital complexes, luxury residential, residential and infrastructure such as courthouses, levee systems, hydro-electric power generation stations, mining and industrial process facilities. 4D BIM has traditionally been used for higher end projects due to the associated costs, but technologies are now emerging that allow the process to be used by laymen or to drive processes such as manufacture.\n\n"}
{"id": "54114449", "url": "https://en.wikipedia.org/wiki?curid=54114449", "title": "AccuPoll", "text": "AccuPoll\n\nAccuPoll is an American company that engages in the design, development, and sale of electronic voting system. Their associated products and services are for use in federal,\n\nstate, local, and private elections in the United States.\n\nTheir touch screen voting system provides a machine-readable format for machine scanning of the ballot, and also a format designed for human readability of the ballot content without the mechanical assistance. \n\nThe company has strategic relationships with partners in systems integration and original equipment manufacturers: \n"}
{"id": "19788251", "url": "https://en.wikipedia.org/wiki?curid=19788251", "title": "Aeronautical Telecommunication Network", "text": "Aeronautical Telecommunication Network\n\nThe Aeronautical Telecommunication Network (ATN) is an internetwork architecture that allows ground/ground, air/ground, and avionic data subnetworks to interoperate by adopting common interface services and protocols based on the ISO OSI Reference Model.\n\nThe European part of ATN used for ground/ground communications is represented by PENS. \n\n"}
{"id": "9788661", "url": "https://en.wikipedia.org/wiki?curid=9788661", "title": "Air dryer", "text": "Air dryer\n\n\"See Also: Compressed air dryer\"\n\nA compressed air dryer is used for removing water vapor from compressed air. Compressed air dryers are commonly found in a wide range of industrial and commercial facilities.\n\nThe process of air compression concentrates atmospheric contaminants, including water vapor. This raises the dew point of the compressed air relative to free atmospheric air and leads to condensation within pipes and the compressed air cools downstream of the compressor.\n\nExcessive water in compressed air, in either the liquid or vapor phase, can cause a variety of operational problems for users of compressed air. These include freezing of outdoor air lines, corrosion in piping and equipment, malfunctioning of pneumatic process control instruments, fouling of processes and products, and more.\n\nThere are various types of compressed air dryers. Their performance characteristics are typically defined by the dew point.\n\nWater vapor is removed from compressed air to prevent condensation from occurring and to prevent moisture from interfering in sensitive industrial processes.\n\n\nRefrigeration dryers employ two heat exchangers, one for air-to-air and one for air-to-refrigeration. However, there is also a single TRISAB heat exchanger that combines both functions. The compressors used in this type of dryer are usually of the hermetic type and the most common gas used is R-134a and R-410a for smaller air dryers up to 100 cfm. Older and larger dryers still use R-22 and R-404a refrigerants. The goal of having two heat exchangers is that the cold outgoing air cools down the hot incoming air and reduces the size of compressor required. At the same time the increase in the temperature of outgoing air prevents re-condensation.\n\nSome manufacturers produce \"cycling dryers\". These store a cold mass that cools the air when the compressor is OFF. When the refrigeration compressor runs, the large mass takes much longer to cool, so the compressor runs longer, and stays OFF longer. These units operate at lower dew points, typically in the 35–40 °F range. When selected with the optional \"cold coalescing filter\", these units can deliver compressed air with lower dew points. Non-cycling dryers use a hot gas by pass valve to prevent the dryer from icing up.\n\nSome manufacturers produce \"cold coalescing filters\" that are positioned inside of the air dryer at the point of the lowest air temperature (the point at which maximum condensation has occurred).,\n\nSome manufacturers are marketing compressors with built-in refrigeration dryers, but these have had a mixed acceptance in the market.\n\nA deliquescent dryer typically consists of a pressure vessel filled with a hygroscopic medium that absorbs water vapor. The medium gradually dissolves—or deliquesces—to form a solution at the base of the pressure vessel. The liquid must be regularly drained from the vessel and new medium must be added. The medium is usually in tablet or briquette form.\n\nDeliquescent dryers have no moving parts and don't require electrical power for operation. Common applications therefore often involve remote, hazardous, or mobile work sites. Deliquescent dryers are used for removing water vapor from compressed air, natural gas, and waste gases such as landfill gas and digester gas.\n\nThe performance of a deliquescent dryer, as measured by outlet dew point, is highly dependent on the temperature of the air or gas being processed, with cooler temperatures resulting in better performance\n\nThe term \"desiccant dryer\" refers to a broad class of dryers. Other terms commonly used are regenerative dryer and twin tower dryer, and to a lesser extent adsorption dryer.\n\nThe compressed air is passed through a pressure vessel with two \"towers\" filled with a media such as activated alumina, silica gel, molecular sieve or other desiccant material. This desiccant material attracts the water from the compressed air via adsorbtion. As the water clings to the desiccant, the desiccant \"bed\" becomes saturated. The dryer is timed to switch towers based on a standard NEMA cycle, once this cycle completes some compressed air from the system is used to \"purge\" the saturated desiccant bed by simply blowing the water that has adhered to the desiccant off.\n\nThe duty of the desiccant is to bring the pressure dew point of the compressed air to a level in which the water will no longer condense, or to remove as much water from the compressed air as possible. A standard dew point that is expected by a regenerative dryer is −40 °C (−40 °F); this means that when the air leaves the dryer there is as much water in the air as if the air had been \"cooled\" to −40 °C (−40 °F). Required dew point is dependent on application and −70 °C is required in some applications. Many newer dryers come equipped with a dew dependent switching (DDS) which allows for the dryer to detect dew point and shorten or lengthen the drying cycle to fulfill the required dew point. Oftentimes this will save significant amounts of energy which is one of the largest factors when determining the proper compressed air system.\n\nThe regeneration of the desiccant vessel can be during three different methods:\n\n\"Membrane dryer\" refers to a dehumidification membrane that removes water vapor from compressed air.\n\nTypically, the compressed air is first filtered with a high-quality coalescing filter. This filter removes liquid water, oil and particulate from the compressed air. The water vapor–laden air then passes through the center bore of hollow fibers in the membrane bundle. At the same time, a small portion of the dry air product is redirected along the outside surface of the fibers to sweep out the water vapor which has permeated the membrane. The moisture-laden sweep gas is then vented to the atmosphere, and clean, dry air is supplied to the application. The membrane air dryers are designed to operate continuously, 24 hours per day, 7 days per week. Membrane air dryers are quiet, reliable and require no electricity to operate.\n\nSome dryers are non-porous, which means they only permeate water vapor. Non-porous membranes' drying power is only a function of flow rate, pressure. The sweep flow is strictly controlled by an orifice and is not a function of temperature.\n\nPorous membranes are modified nitrogen membranes and pass air as well, usually changing the composition of the compressed air by reducing the oxygen content. The only maintenance required is changing the prefilter cartridge twice a year. The performance of porous membranes are dependent on temperature as well as operating pressure and flow.\n\nMembrane air dryers depress the incoming dew point. Most dryers have a challenge air dew point and pressure specification. So if the inlet dew point is lower than the specified challenge air then the outlet dew point is even lower than specified. For example, a dryer could be rated at a −40 °F dew point with a challenge of +70 °F dew point and 100 psig. If the incoming air has an inlet dew point of only 32 °F, the outlet dew point will be somewhat less. Pressure also plays a role. If the pressure is higher than the rated specification then the outlet dew point will be lowered. This lowering of the outlet dew point is due to the longer residence time that the air has inside the membrane. Using the spec above, an operating pressure of 120 psig will yield a lower outlet dew point than specified. The extent of the improvement is dependent on the nature of the membrane and could vary among manufacturers.\n\nDew point suppression is not a feature of refrigerated dryers, as they chill the incoming air to a fixed temperature, usually 35 °F. So a lower dew point challenge will not yield a dew point lower than 35 °F.\n\nMembrane air dryers are used in pneumatic components, spray painting, laser plenum purge, air bearings, air spindles, medical equipment, air guns and pneumatic brakes for vehicles and trains.\n\n"}
{"id": "13837507", "url": "https://en.wikipedia.org/wiki?curid=13837507", "title": "Arab Agricultural Revolution", "text": "Arab Agricultural Revolution\n\nThe Arab Agricultural Revolution is the transformation in agriculture from the 8th to the 13th century in the Islamic region of the Old World. The agronomic literature of the time, with major books by Ibn Bassal and Abū l-Khayr al-Ishbīlī, demonstrates the extensive diffusion of useful plants to Medieval Spain (al-Andalus), and the growth in Islamic scientific knowledge of agriculture and horticulture. Medieval Arab historians and geographers described al-Andalus as a fertile and prosperous region with abundant water, full of fruit from trees such as the olive and pomegranate. Archaeological evidence demonstrates improvements in animal husbandry and in irrigation such as with the \"sakia\" water wheel. These changes made agriculture far more productive, supporting population growth, urbanisation, and increased stratification of society.\n\nThe revolution was first described by the historian Antonio Garcia Maceira in 1876. The name was coined by the historian Andrew Watson in an influential but at the time controversial 1974 paper. However, 40 years on, it has proven useful to historians and has been supported by findings in archaeology and archaeobotany.\n\nThe first Arabic book on agronomy to reach al-Andalus, in the 10th century, was Ibn Wahshiyya's \"al-Filahat al-nabatiyya\" (Nabatean Agriculture), from Iraq; it was followed by texts written in al-Andalus, such as the \"Mukhtasar kitab al-filaha\" (Abridged Book of Agriculture) by Al-Zahrawi (Abulcasis) from Cordoba, around 1000 AD.\n\nThe eleventh century agronomist Ibn Bassal of Toledo described 177 species in his \"Dīwān al-filāha\" (The Court of Agriculture). Ibn Bassal had travelled widely across the Islamic world, returning with a detailed knowledge of agronomy. His practical and systematic book both gives detailed descriptions of useful plants including leaf and root vegetables, herbs, spices and trees, and explains how to propagate and care for them.\n\nThe twelfth century agronomist Abū l-Khayr al-Ishbīlī of Seville described in minute detail in his \"Kitāb al-Filāha\" (Treatise on Agriculture) how olive trees should be grown, grafted, treated for disease, and harvested, and gave similar detail for crops such as cotton.\n\nMedieval Islamic agronomists including Ibn Bassal and Abū l-Khayr described agricultural and horticultural techniques including how to propagate the olive and the date palm, crop rotation of flax with wheat or barley, and companion planting of grape and olive. These books demonstrate the importance of agriculture both as a traditional practice and as a scholarly science. In al-Andalus, there is evidence that the almanacs and manuals of agronomy helped to catalyse change, causing scholars to seek out new kinds of vegetable and fruit, and to carry out experiments in botany; in turn, these helped to improve actual practice in the region's agriculture. During the 11th century Abbadid dynasty in Seville, the sultan took a personal interest in fruit production, discovering from a peasant the method he had used to grow some exceptionally large melons—pinching off all but ten of the buds, and using wooden props to hold the stems off the ground.\n\nArchaeological evidence from the measurement of bones (osteometry) demonstrates that sheep in southern Portugal increased in size during the Islamic period, while cattle increased when the area became Christian after its reconquest. The archaeologist Simon Davis assumes that the change in size signifies improvement by animal husbandry, while in his view the choice of sheep is readily explained by the Islamic liking for mutton.\n\nThe Islamic period in the Fayyum depression of Middle Egypt, like medieval Islamic Spain (al-Andalus), was characterised by extremely large-scale systems of irrigation, with both the supply, via gravity-fed canals, and the management of water under local tribal control. In the Islamic period in al-Andalus, whose rural parts were equally tribal, the irrigation canal network was much enlarged. Similarly, in the Fayyum, new villages were established in the period, and new water-dependent orchards and sugar plantations were developed.\n\nThe \"sakia\" or animal-powered irrigation wheel was likely introduced to Islamic Spain in early Umayyad times (in the 7th century). Improvements to it were described by Hispano-Arabic agronomists in the 11th and 12th centuries. From there, \"sakia\" irrigation was spread further around Spain and Morocco. A 13th century observer claimed there were \"5000\" waterwheels along the Guadalquivir in Islamic Spain; even allowing for medieval exaggeration, irrigation systems were certainly extensive in the region at that time. The supply of water was sufficient for cities as well as agriculture: the Roman aqueduct network into the city of Cordoba was repaired in the Umayyad period, and extended.\n\nMedieval Arab historians such as Ibn Bassam, Ibn Hayyan, and Ibn Hazm, and geographers such as al-Bakri, al-Idrisi, and Al-Zuhri, described Islamic Spain as a fortunate entity. Indeed, the tenth-century Jewish scribe Menahem Ben Saruq wrote to the Khazar king \"The name of our land in which we dwell ... in the language of the Arabs, the inhabitants of the land, al-Andalus ... the land is rich, abounding in rivers, springs, and aqueducts; a land of corn, oil, and wine, of fruits and all manner of delicacies; it has pleasure-gardens and orchards, fruitful trees of every kind, including ... [the white mulberry] upon which the silkworm feeds\". al-Maqqari, quoting the ninth-century Ahmad Ibn Muhammad Ibn Musa al-Razi, describes al-Andalus as a rich land \"with good, arable soil, fertile settlements, flowing copiously with plentiful rivers and fresh springs.\" Al-Andalus was associated with cultivated trees like olive and pomegranate. After the Christian reconquest, arable farming was frequently abandoned, the land reverting to pasture, though some farmers tried to adopt Islamic agronomy. Western historians have wondered if the Medieval Arab historians were reliable, given that they had a motive to emphasize the splendour of al-Andalus, but evidence from archaeology has broadly supported their claims.\n\nIn 1876, the historian Antonia Garcia Maceira argued that where the Romans and then the Goths who farmed in Spain made little effort to improve their crops or to import species from other regions, under \"the Arabs\", there was an agricultural \"revolution\" in Al-Andalus (Islamic Spain) caused \"by implementing the knowledge that they acquired through observation during their peregrinations, and the result was extensive agricultural settlement.\"\n\nIn 1974, the historian Andrew Watson published a paper proposing an extension of Garcia Maceira's hypothesis of agricultural revolution in Al-Andalus. Watson argued that the economy established by Arab and other Muslim traders across the Old World enabled the diffusion of many crops and farming techniques throughout the Islamic world, as well as the adaptation of crops and techniques from and to regions outside it. Crops from Africa, such as sorghum, from China, such as citrus fruits, and from India, such as mango, rice, cotton and sugar cane, were distributed throughout Islamic lands, which he believed had not previously grown these plants. He listed eighteen such crops. Watson suggested that these introductions, along with an increased mechanization of agriculture and irrigation, led to major changes in economy, population distribution, vegetation cover, agricultural production and income, population, urban growth, distribution of the labour force, industries linked to agriculture, cooking, diet and clothing in the Islamic world.\n\nIn 1997, the historian of science Howard R. Turner wrote that Islamic study of soil, climate, seasons and ecology \"promoted a remarkably advanced horticulture and agriculture. The resulting knowledge, transmitted to Europe after the eleventh century, helped to improve farming techniques, widen the variety of crops, and increase yields on the continent's farmlands. In addition, an enormous variety of crops was introduced to the West from or through Muslim lands\".\n\nThe historian of Islam Salah Zaimeche stated in 2002 that the \"accepted wisdom\" that agriculture was not improved until the last few centuries in Europe had been overturned by work by Watson, Thomas Glick and L. Bolens among others.\n\nIn 2006, James E. McClellan III and Harold Dorn stated in their book \"Science and Technology in World History\" that Islam had depended as much on its farmers as its soldiers, and that the farmers had helped to create a \"scientific civilisation\": \"in what amounted to an agricultural revolution they adapted new and more diversified food crops to the Mediterranean ecosystem: rice, sugar cane, cotton, melons, citrus fruits, and other products. With rebuilt and enlarged systems of irrigation, Islamic farming extended the growing season and increased productivity.\" They stated further that the importance of these efforts was indicated by the \"uninterrupted series\" of books on agriculture and irrigation; another indication was provided by the many books on particular animals of importance to Islamic farming and government, including horses and bees. They ascribed the population growth, urbanisation, social stratification, centralisation of politics and state-controlled scholarship to the improvement in agricultural productivity.\n\nBy 2008, the archaeozoologist Simon Davis could write without qualification that in the Iberian peninsula \"Agriculture flourished: the Moslems introduced new irrigation techniques and new plants like sugar cane, rice, cotton, spinach, pomegranates and citrus trees, to name just a few... Seville had become a Mecca for agronomists, and its hinterland, or \"Aljarafe\", their laboratory.\"\n\nWatson's work was met with some early scepticism, such as from the historian Jeremy Johns in 1984. Johns argued that Watson's selection of 18 plants was \"peculiar\", since the banana, coconut, mango and shaddock were unimportant in the Islamic region at the time, detracting from the discussion of the staple crops. Johns further noted that the evidence of diffusion of crops was imperfect, that Watson made \"too many minor slips and larger errors\" such as getting dates wrong or claiming that a 1439 document was Norman, and had failed to make best use of the evidence that was available, such as of the decline of classical agriculture, or even to mention the changing geomorphology. Johns however concluded that \"The hypothesis of an 'Abbasid agricultural revolution is challenging and may well prove useful\".\n\nThe historian Eliyahu Ashtor wrote in 1976 that agricultural production declined in the period immediately after the Arab conquest in areas of Mesopotamia and Egypt, on the limited basis of records of taxes collected on cultivated areas. In a 2012 paper focusing on the Sawād area of Iraq, Michele Campopiano concluded that Iraqi agricultural output declined in the 7th to 10th century; he attributed this decline to \"competition of the different ruling groups to gain access to land surplus\".\n\nIn 2009, the historian Michael Decker stated that widespread cultivation and consumption of four staples, namely durum wheat, Asiatic rice, sorghum and cotton were already commonplace under the Roman Empire and Sassanid Empire, centuries before the Islamic period. He suggested that their actual role in Islamic agriculture had been exaggerated, arguing that the agricultural practices of Muslim cultivators did not fundamentally differ from those of pre-Islamic times, but evolved from the hydraulic know-how and 'basket' of agricultural plants inherited from their Roman and Persian predecessors. In the case of cotton, which the Romans grew mainly in Egypt, the plant remained a minor crop in the classical Islamic period: the major fibre was flax, as in Roman times. Decker further asserted that the advanced state of ancient irrigation practices \"rebuts sizeable parts of the Watson thesis,\" since for example in Spain, archaeological work indicated that the Islamic irrigation system was developed from the existing Roman network, rather than replacing it. Decker agreed that \"Muslims made an important contribution to world farming through the westward diffusion of some crops\", but that the introduction of \"agronomic techniques and materials\" had been less widespread and less consistent than Watson had suggested. Furthermore, there is clear evidence that agricultural devices such as watermills and waterwheels, shadufs, norias, sakias, water screws and water pumps were widely known and applied in Greco-Roman agriculture long before the Muslim conquests.\n\nD. Fairchild Ruggles rejected the view that the medieval Arab historians had been wrong to claim that agriculture had been revolutionised, and that it had instead simply been restored to a state like that before the collapse of the Roman Empire. She argued that while the medieval Arab historians may not have had a reliable picture of agricultural knowledge before their time, they were telling the truth about a dramatic change to the landscape of Islamic Spain. A whole new \"system of crop rotation, fertilization, transplanting, grafting, and irrigation\" was swiftly and systematically put into place under a new legal framework of land ownership and tenancy. In her view, therefore, there was indeed an agricultural revolution in al-Andalus, but it consisted principally of new social institutions rather than of new agronomic techniques. Ruggles stated that this \"dramatic economic, scientific, and social transformation\" began in al-Andalus and had spread throughout the Islamic Mediterranean by the 10th century.\n\nLooking back over 40 years of scholarship since Watson's theory, the historian of land use Paolo Squatriti wrote in 2014 that the thesis had been widely used and cited by historians and archaeologists working in different fields. It \"proved to be applicable in scholarly debates about technological diffusion in pre-industrial societies, the 'decline' of Islamic civilization, the relations between elite and peasant cultural systems, Europe's historical Sonderweg in the second millennium CE, the origins of globalization, [and] the nature of Mediterraneity.\" Squatriti noted that Watson had originally trained in economics, and applied this interest to his historical studies. Squatriti described Watson's paper as concise and elegant, and popular for its usefulness in supporting the theses of many different historians. He observed that Watson's thesis did not depend on claims of new introductions of plants into any region, but of their \"diffusion and normalization\", i.e. of their becoming widely and generally used, even if they were known from Roman times. Calling Watson's \"philological\" approach \"old fashioned\", and given that Watson had worked \"virtually without archaeology\", Squatrini expressed surprise that recent research in archaeobotany had failed to \"decisively undermine\" Watson's thesis.\n\n\n"}
{"id": "2821381", "url": "https://en.wikipedia.org/wiki?curid=2821381", "title": "Beatrice Hicks", "text": "Beatrice Hicks\n\nBeatrice Alice Hicks (January 2, 1919 – October 21, 1979) was an American engineer, the first woman engineer to be hired by Western Electric, and both co-founder and first president of the Society of Women Engineers. Despite entering the field at a time where engineering was seen as an inappropriate career for a woman, Hicks held a variety of leadership positions and eventually became the owner of an engineering firm. During her time there, Hicks developed a gas density switch that would be used in the U.S. space program, including the Apollo moon landing missions.\n\nBeatrice Hicks was born in 1919 in Orange, New Jersey, to William Lux Hicks, a chemical engineer, and Florence Benedict. Hicks decided at an early age that she wished to be an engineer. While her parents neither supported nor opposed Hicks' desired career path, some of her teachers and classmates tried to discourage her from becoming an engineer, viewing it as a socially unacceptable role for a woman. She graduated from Orange High School in 1935 and received a bachelor's degree in chemical engineering from Newark College of Engineering (now New Jersey Institute of Technology) in 1939, one of only two women in her class. During college, Hicks worked in the treasury office of an Abercrombie & Fitch store as a telephone operator, and in the university's library. After receiving her undergraduate degree, Hicks stayed at Newark College of Engineering for three years as a research assistant, where she studied the history of Edward Weston's inventions and took additional classes at night.\n\nIn 1942 Hicks took a job at the Western Electric Company, designing and testing quartz crystal oscillators in Kearny, New Jersey. She was the first woman to be employed by Western Electric as an engineer, and she spent three years working there. Upon the death of her father, she joined the Bloomfield, New Jersey based Newark Controls Company, a metalworking firm that her father had founded. Hicks served as chief engineer and then as vice president in charge of engineering, before purchasing control of the company from her uncle in 1955. Hicks designed and patented a gas density switch later used in the U.S. space program, including the moon landing, and was a pioneer in the field of sensors that detected when devices were reaching structural limits. Hicks authored several technical papers on the gas density switch. While at Newark Controls Hicks pursued a master's degree in physics, which she received in 1949 from the Stevens Institute of Technology.\n\nIn 1950 Hicks and other women based on the East coast of the United States began meeting in an organization, the goal of which was to advance female engineers and increase female participation in engineering. The organization was incorporated as the Society of Women Engineers two years later. Hicks served as the president of the organization for two consecutive terms, from 1950 to 1952. In 1963 the Society of Women Engineers presented their highest honor, the Society of Women Engineers Achievement Award, to Hicks. Hicks toured the United States, championing the cause of female engineers through outreach and speaking engagements. She believed that while female engineers would initially be closely watched, they would also be quickly accepted.\n\nIn 1948 Hicks married fellow engineer Rodney Duane Chipp, who held two director level engineering positions before starting a consulting firm. In 1960 the couple were selected by the National Society of Professional Engineers for a month-long research and speaking tour of South America, which focused on international cooperation between American and South American engineers. When Chipp died in 1966, Hicks sold off Newark Controls Company and took over her late husband's consulting business. Hicks was also selected to serve on the Defense Advisory Committee for Women in Services between 1960 and 1963, was the director of the First International Conference of Women Engineers and Scientists, and represented the United States at four International Management Congresses.\n\nHicks died on October 21, 1979 in Princeton, New Jersey.\n\nBecause of her role in Newark Controls Company, \"Mademoiselle\" magazine named Hicks \"Woman of the Year in Business\" in 1952. In 1978 Hicks was invited to join the National Academy of Engineering, the highest professional honor in engineering, and became the sixth woman to join the organization. In 2002 Hicks was inducted into the National Women's Hall of Fame.\n\nHicks received honorary doctorates from Hobart and William Smith College, Rensselaer Polytechnic Institute, Stevens Institute of Technology, and Worcester Polytechnic Institute. She was the first female recipient of an honorary doctorate from Rensselaer Polytechnic Institute. She was a member of both the American Society of Mechanical Engineers and the Institute of Electrical and Electronics Engineers.\n\n"}
{"id": "13364357", "url": "https://en.wikipedia.org/wiki?curid=13364357", "title": "CETO", "text": "CETO\n\nCETO is a wave-energy technology that converts kinetic energy from ocean swell into electrical power and (in CETO 5) directly desalinates freshwater through reverse osmosis. The technology was developed and tested onshore and offshore in Fremantle, Western Australia. In early 2015 a CETO 5 production installation was commissioned and connected to the grid. all the electricity generated is being purchased to contribute towards the power requirements of HMAS Stirling naval base at Garden Island, Western Australia. Some of the energy will also be used directly to desalinate water.\nCETO is designed to be a simple and robust wave technology. CETO is claimed to be the only ocean-tested wave-energy technology globally that is both fully submerged and generates power and or desalinated water onshore. The CETO technology has been independently verified by Energies Nouvelles (EDF EN) and the French naval contractor DCNS.\n\nThe name is inspired by the Greek ocean goddess, Ceto. the system distinguishes itself from other wave-energy devices in being fully submerged. Submerged buoys are moved by the ocean swell, driving pumps that pressurize seawater delivered ashore by a subsea pipeline. Once onshore, the high-pressure seawater is used to drive hydro-electric turbines, generating electricity. The high-pressure seawater can also be used to supply a reverse osmosis desalination plant, producing freshwater. Some historic conventional seawater desalination plants are large emitters of greenhouse gases; this is due to the amount of energy required to drive the grid-connected pumps that deliver the high-pressure seawater to reverse osmosis membranes for the removal of the salt.\n\n CETO 6 is in development, and differs from CETO 5 in having a larger buoy, with the electrical generation onboard and the power being transferred to shore by an electrical cable. The buoy is designed for deeper water and further offshore than CETO 5.\n\nOn completion of Stage 1 of the Perth Wave Energy Project, Carnegie enlisted Frazer-Nash Consultancy Ltd to verify the CETO 3 unit's measured and modelled capacity. During the CETO 3 in-ocean trial, Frazer–Nash verified the peak measured capacity to be 78 kW and delivered a sustained pressure of 77 bar, above what is required for seawater reverse-osmosis desalination.\n\nStage 1, already been completed, involved the manufacture, deployment and testing of a single commercial-scale autonomous CETO unit off Garden Island. For this stage, the CETO unit was not connected to shore but was stand-alone and autonomous, providing telemetric data back to shore for confirmation and independent verification of the unit's performance.\n\nStage 2 involved the design, construction, deployment and operational performance evaluation of a grid-connected commercial-scale wave-energy demonstration project, also at Garden Island. The facility consisted of multiple submerged CETO units in an array, subsea pipeline(s) to shore, hydraulic conditioning equipment, and an onshore power generation facility.\n\nIn early 2015 a multi megawatt system was connected to the grid, with all the electricity being bought to power HMAS Stirling naval base. Two fully submerged buoys which are anchored to the seabed, transmit the energy from the ocean swell through hydraulic pressure onshore; to drive a generator for electricity, and also to produce fresh water. a third buoy is planned for installation.<ref name =\"abc.net.au/20150218\"></ref>\n\nThe Réunion Island project is a joint venture between Carnegie and EDF Energies Nouvelles. The project will initially consist of the deployment of a single, autonomous commercial scale unit (stage 1) which will be followed by a 2MW plant (stage 2) and a further expansion of the project to a nominal 15MW installed capacity (stage 3). stage 1 has been awarded $5M of French government funding.\n\nA cable between a buoy and the seabed anchored hydraulic pump snapped in a CETO 4 prototype installation in January 2014. The buoy was swept away during Cyclone Bejisa, which also led to a fatality and widespread damage on Réunion Island. The design was an earlier iteration than the Perth CETO 5 installation and lacked the quick release mechanism which was included in CETO 5.\n\nCarnegie has signed a formal funding and collaboration agreement with the Irish Government's Sustainable Energy Association (SEAI) for a €150,000 project to evaluate potential CETO wave sites in Ireland and develop a site-specific conceptual design. The project is 50% funded by the SEAI and 50% by Carnegie, and forms the first phase of detailed design for a potential 5 MW commercial demonstration project in Irish waters. The project was under way in 2011 and is being managed through Carnegie's Irish subsidiary, CETO Wave Energy Ireland Limited.\n\n\n\n\n"}
{"id": "57688582", "url": "https://en.wikipedia.org/wiki?curid=57688582", "title": "CMOS amplifiers", "text": "CMOS amplifiers\n\nCMOS amplifiers are ubiquitous analog circuits which are used in computers, audio systems, smart phones, cameras, telecommunication systems, biomedical circuits and many other systems, and their performance has great impact on the overall specifications of the systems. They take their name from the use of MOSFET (Metal Oxide Semiconductor Field Effect Transistor) as opposite to Bipolar Junction Transistors (BJT). MOSFETS are simpler to fabricate and therefore less expensive than BJT amplifiers, still providing a sufficiently high transconductance to allow the design of very high performance circuits. In high performance CMOS amplifier circuits, transistors are not only used to amplify the signal but are also used as active loads to achieve higher gain and output swing in comparison with resistive loads.\n\nCMOS technology had been primarily introduced to design digital circuits. In the last decades, in order to improve speed, power consumption, required area and other aspects of digital integrated circuits (ICs), the feature size of MOSFET transistors has shrunk (minimum channel length of transistors reduces in newer CMOS technologies). This phenomenon predicted by Gordon Moore in 1975, which is called Moore’s law, and states that in about each 2 years, the number of transistors doubles for the same silicon area of ICs. Progress in memory circuits design is an interesting example to see how process advancement have affected the required size and their performance in the last decades. In 1956, a 5MB Hard Disk Drive (HDD) weighted over a tone, while these days having 50000 times more capacity with a weight of several tens of grams is very common.\n\nWhile digital ICs have enormously benefited from the feature size shrinking, analog CMOS amplifiers have not gained corresponding advantages due to the intrinsic limitations imposed by an analog design, like the intrinsic gain reduction of short channel transistors which affects the overall gain of amplifier. Novel techniques to achieve higher gain also create new problems, like stability of amplifier for closed-loop applications. In the following, we will address both aspects and summarize few different methods to overcome these problems.\n\nMaximum gain of a single MOSFET transistor is called intrinsic gain, and is equal to:\n\nformula_1\n\nWhich gm is the transconductance, and ro is the output resistance of transistor. As a first order approximation, ro is directly proportional to the channel length of transistors. In a single-stage amplifier, one can increase channel length in order to get higher output resistance and gain as well. But, it will also increase parasitic capacitance of transistor, which will limit the bandwidth of amplifier. Channel length of transistors become smaller in modern CMOS technologies, and makes achieving high gain in single-stage amplifier very challenging. In order to achieve high gain, many techniques have been suggested in the literature. In the following sections, we will have a brief look to different topologies of amplifiers, and their features.\n\nTelescopic, Folded Cascode (FC), or Recycling FC (RFC) are the most common single stage amplifiers. All these structures use transistors as active loads in order to provide higher output resistance (= higher gain), and output swing. Telescopic amplifier, provides higher gain (due to higher output resistance) and higher bandwidth (due to smaller non-dominant pole at cascode node). In contrast, it has limited output swing and difficulty in implementation of unity-gain buffer. Although FC has lower gain and bandwidth, it can provide higher output swing which is important advantage in modern CMOS technologies, where supply voltage is reduced. Also, since the DC voltage of input and output nodes can be at the same level, it is more suitable for implementation of unity-gain buffer. FC is recently used to implement integrator in a Bio-Nano sensor application. Also, it can be used as a stage in multi-stage amplifiers. As an example, FC is used as the input stage of a two-stage amplifier in designing of a potentiostat circuit, which is to measure neuronal activities, or DNA sensing. Also, it can be used to realize transimpedance amplifier (TIA). TIA can be used in amperometric biosensors to measure current of cells or solutions in order to define the characteristics of a device under test \nIn the last decade, circuit designer have proposed different modified versions of FC circuit. RFC is one of the modified versions of FC amplifier, which provides higher gain, higher bandwidth, and also higher slew rate in comparison with FC (for the same power consumption). Recently, RFC amplifier has used in hybrid CMOS-graphene sensor array for subsecond measurement of dopamine. It is used as a low noise amplifier to implement integrator.\n\nIn many applications, amplifier drives capacitor as a load. In some applications, like switched capacitor circuits,the value of capacitive load changes in different cycles. Therefore, it will affect time constant of output node and the frequency response of amplifier. Stable behavior of amplifier for all possible capacitive loads is necessary, and designer must consider this issue during designing of circuit. Designer should ensure that phase margin (PM) of the circuit will be enough for the worst case. In order to have proper circuit behavior and time response, designers usually consider PM of 60 degrees. for higher values of PM, circuit will be more stable, but it will take longer time for output voltage to reach its final value. \nIn Telescopic, and FC amplifiers, dominant pole is located at the output nodes. Also, there is a non-dominant pole at cascode node. Since capacitive load connected to output nodes, its value will affect the location of the dominant pole. This figure shows how capacitive load affects the location of dominant pole (W1) and stability. Increasing of capacitive load, will move dominant pole toward the origin, and since unity gain frequency (Wunity) is Av (amplifier gain) times of W1, it will also move toward the origin. Therefore, PM will increase, which improves stability. So, if we ensure stability of a circuit for a minimum capacitive load, it will remain stable for larger load values. In order to achieve greater than 60 degrees PM, the non-dominant pole (W2) must be greater than 1.7 times of Wunity.\n\nIn some applications, like switched capacitor filters or integrators, and different types of analog-to-digital converters, having high gain (70-80 dB) is needed, and achieving the required gain sometimes is impossible with single-stage amplifiers. This is more serious in modern CMOS technologies, which transistors have smaller output resistance due to shorter channel length. In order to achieve high gain as well as high output swing, multi-stage amplifiers have been invented. To implement two-stage amplifier, one can use FC amplifier as the first stage and a common source amplifier as the second stage. Also, to implement four-stage amplifier, 3 common source amplifier can be cascaded with FC amplifier. It should be mentioned that to drive large capacitive loads or small resistive loads, the output stage should be class AB. For example, common source amplifier with class AB behavior can be used as the final stage in three-stage amplifier to not only improve drive capability, but also gain. Class AB amplifier can be used as a column driver in LCDs.\n\nUnlike single-stage amplifiers, multi-stage amplifiers usually have 3 or more poles and if they used in feedback networks, the closed loop system will probably unstable. In order to have stable behavior in multi-stage amplifiers, it is necessary to use compensation network. The main goal of compensation network is to modify transfer function of the system in such a way to achieve enough PM. So, by the use of compensation network, we should get frequency response similar to what we showed for single-stage amplifiers. \nIn single-stage amplifiers, capacitive load is connected to output node which dominant pole happens there, and increasing its value improves PM. So, it acts like a compensation capacitor (network). To compensate multi-stage amplifiers, compensation capacitor is usually used to move dominant pole to lower frequency in order to achieve enough PM. \n\nThe following figure shows the block diagram of a two-stage amplifier in fully differential and single ended modes. In a two-stage amplifier, input stage can be a Telescopic or FC amplifier. For the second stage, common source amplifier with active load is a common choice. Since output resistance of the first stage is much greater than the second stage, dominant pole is at the output of the first stage. Without compensation, the amplifier is unstable or at least it does not have enough PM. It is important to mention that the load capacitance will be connected to the output of the second stage, which non-dominant pole happens there. Therefore, unlike single-stage amplifiers, increasing of capacitive load, will move non-dominant pole to lower frequency and will deteriorate PM. Mesri et al. suggested two-stage amplifiers which behave like single-stage amplifiers, and amplifiers remains stable for larger values of capacitive loads.\nTo have proper behavior, we need to compensate two-stage or multi-stage amplifiers. The simplest way for compensation of two-stage amplifier, as shown in the left block diagram of the below figure, is to connect compensation capacitor at the output of the first stage, and move dominant pole to lower frequencies. But, realization of capacitor on silicon chip requires considerable area. The most common compensation method in two-stage amplifiers is Miller compensation (middle block diagram in the below figure. In this method, a compensation capacitor is placed between input and output node of the second stage. In this case, the compensation capacitor will appear 1+|Av2| times greater at the output of the first stage, and will push the dominant pole as well as unity gain frequency to lower frequencies. Moreover, because of pole splitting effect, it will also move the non-dominant pole to higher frequencies. Therefore, it is a good candidate to make the amplifier stable. The main advantage of Miller compensation method, is to reduce size of the required compensation capacitor by a factor of 1+|Av2|. The issue raised from Miller compensation capacitor is introducing right-half plane (RHP) zero, which reduces PM. Hopefully, different methods have suggested to solve this issue. As an example, to cancel the effect of RHP zero, nulling resistor can be used in series with compensation capacitor (right block diagram of the below figure). Based on the resistor value, we can push RHP zero to higher frequency (in order to cancel its effect on PM), or to move it LHP (in order to improve PM), or even remove the first non-dominant pole to improve Bandwidth and PM. This method of compensation is recently used in amplifier design for potentiostat circuit. Because of process variation, resistor value can change more than 10%, and therefore affects stability. Using current buffer or voltage buffer in series with compensation capacitor is another option to get better results.\n"}
{"id": "53144876", "url": "https://en.wikipedia.org/wiki?curid=53144876", "title": "Canon X-07", "text": "Canon X-07\n\nCanon X-07 is one of the first personal computers available in France manufactured by Canon.\nThis is a laptop (or rather hand-held) based around the NSC800 (compatible with Z80) shipping Microsoft BASIC.\n\nThe specifications included:\n"}
{"id": "4449282", "url": "https://en.wikipedia.org/wiki?curid=4449282", "title": "Chebyshev's Lambda Mechanism", "text": "Chebyshev's Lambda Mechanism\n\nThe Chebyshev's Lambda Mechanism is a four-bar mechanism that converts rotational motion to approximate straight-line motion with approximate constant velocity. The precise design trades off straightness, lack of acceleration, and the proportion of the driving rotation that is spent in the linear portion of the full curve.\n\nThe example to the right spends over half of the cycle in the near straight portion.\n\nThe Chebyshev's Lambda Mechanism is a cognate linkage of the Chebyshev linkage.\n\nThe linkage was first shown in Paris on the Exposition Universelle (1878) as \"The Plantigrade Machine\".\n\nThe Chebyshev's Lambda Mechanism looks like the Greek letter lambda, therefore the linkage is also known as Lambda Mechanism.\n\n\n"}
{"id": "28496644", "url": "https://en.wikipedia.org/wiki?curid=28496644", "title": "Clonk (fishing)", "text": "Clonk (fishing)\n\nA clonk is a fishing tool which has been used in Europe to fish for Wels catfish. It consists of a stick with three parts: handle, fork and heel. Originally it was made of wood but nowadays there are clonks made of plastic or metal too because they are easier to produce than wood.\n\nThe process of clonking may look an easy job at first, but it requires some practice. The air bubble produced when the clonk strikes the water is cut by the fork and this produces a unique sound similar to opening a wine bottle. This sound stimulates the defensive instinct in nearby catfish and they attack the \"intruder\", which in this case is the bait.\n"}
{"id": "5402608", "url": "https://en.wikipedia.org/wiki?curid=5402608", "title": "Cooling vest", "text": "Cooling vest\n\nA cooling vest is a piece of specially made clothing designed to lower or stabilize body temperature and make exposure to warm climates or environments more bearable. Cooling vests are used by many athletes, construction workers, and welders, as well as individuals suffering from multiple sclerosis, hypohidrotic ectodermal dysplasia, or various types of sports injuries.\n\nCooling vests range in weight from around two to eight pounds, depending on the model. While many sub-types do exist, cooling vests fall into one of 5 primary types:\n\n\nThe effects of cooling vests on athletes to improve their performance has been evaluated on several occasions; at the 2004 Summer Olympics several Americans and Australians were fitted with cooling vests supplied by Nike and Arctic Heat 2004,2008,2012,2016 Olympics, used prior to their events.\n\nCooling vests are also used by persons with multiple sclerosis. In multiple sclerosis, nerve fibers become demyelinated which leads to pain and discomfort when temperature is elevated. Nerve fibers may also be remyelinating or in the process of repairing themselves and still be sensitive to elevated temperatures. The cooling vest keeps the patient's temperature down, reducing the pain symptoms. In 2005, a 12-week study at the University of Buffalo was funded by the National Institute on Disability and Rehabilitation Research, a division of the U.S. Department of Education, to determine if people with multiple sclerosis could exercise longer with the help of a cooling vest.\n"}
{"id": "8620994", "url": "https://en.wikipedia.org/wiki?curid=8620994", "title": "Copper(I) acetylide", "text": "Copper(I) acetylide\n\nCopper(I) acetylide, or cuprous acetylide, is a chemical compound with the formula CuC. Although never characterized by X-ray crystallography, the material has been claimed at least since 1856. One form is claimed to be a monohydrate with formula .). It is a reddish solid, that easily explodes when dry.\n\nMaterials purported to be copper acetylide can be prepared by treating acetylene with a solution of copper(I) chloride and ammonia:\nThis reaction produces a reddish solid precipitate.\n\nWhen dry, copper acetylide is a heat and shock sensitive high explosive, more thermally sensitive than silver acetylide.\n\nCopper acetylide is thought to form inside pipes made of copper or an alloy with high copper content, which may result in violent explosion. This was found to be the cause of explosions in acetylene plants, and led to abandonment of copper as a construction material in such plants. Copper catalysts used in petrochemistry can also possess a degree of risk under certain conditions.\n\nCopper acetylide is the substrate of Glaser coupling for the formation of polyynes. In a typical reaction, a suspension of . in an amoniacal solution is treated with air. The copper is oxidized to and forms a blue soluble complex with the ammonia, leaving behind a black solid residue. The latter has been claimed to consist of carbyne, an elusive allotrope of carbon:\nThis interpretation has been disputed.\n\nFreshly prepared copper acetylide reacts with hydrochloric acid to form acetylene and copper(I) chloride. Samples that have been aged with exposure to air or to copper(II) ions liberate also higher polyynes H(−C≡C−)H, with \"n\" from 2 to 6, when decomposed by hydrochloric acid. A \"carbonaceous\" residue of this decomposition also has the spectral signature of (−C≡C−) chains. It has been conjectured that oxidation causes polymerization of the acetylide anions in the solid into carbyne-type anions .C(≡C−C≡)C or polycumulene-type anions C(=C=C=)C.\n\nThermal decomposition of copper acetylide in vacuum is not explosive and leaves copper as a fine powder at the bottom of the flask, while depositing a fluffy very fine carbon powder on the walls. On the basis of spectral data, this powder was claimed to be carbyne C(−C≡C−)C rather than graphite as expected.\n\nThough not practically useful as an explosive due to high sensitivity and reactivity towards water, it is interesting as a curiosity because it is one of the very few explosives that do not liberate any gaseous products upon detonation.\n\nThe formation of copper acetylide when a gas is passed through a solution of copper(I) chloride is used as a test for the presence of acetylene.\n\nReactions between Cu and alkynes occur only if a terminal hydrogen is present (as it is slightly acidic in nature). Thus, this reaction is used for identification of terminal alkynes.\n\n"}
{"id": "16764104", "url": "https://en.wikipedia.org/wiki?curid=16764104", "title": "Courreges ZOOOP", "text": "Courreges ZOOOP\n\nThe Zooop is a three-seat electric car produced by the Paris-based fashion house Maison de Courrèges.\n\nThe Courreges Zooop performance is 150 kW and weighing just 690 kilograms, also features a range of 450 kilometers. Remarkably, the car is not produced for a car manufacturer, but it is produced by the famous fashion design house Maison de Courrèges, which peculiarly did scarce promotion for its new model outside its native country.\n"}
{"id": "4332925", "url": "https://en.wikipedia.org/wiki?curid=4332925", "title": "Discharger", "text": "Discharger\n\nA discharger in electronics is a device or circuit that releases stored energy or electric charge from a battery, capacitor or other source.\n\nDischarger types include:\n\nSee also Bleeder resistor\n"}
{"id": "3034286", "url": "https://en.wikipedia.org/wiki?curid=3034286", "title": "Electronic lock", "text": "Electronic lock\n\nAn electronic lock (or electric lock) is a locking device which operates by means of electric current. Electric locks are sometimes stand-alone with an electronic control assembly mounted directly to the lock. Electric locks may be connected to an access control system, the advantages of which include: key control, where keys can be added and removed without re-keying the lock cylinder; fine access control, where time and place are factors; and transaction logging, where activity is recorded. Electronic locks can also be remotely monitored and controlled, both to lock and to unlock.\n\nElectric locks use magnets, solenoids, or motors to actuate the lock by either supplying or removing power. Operating the lock can be as simple as using a switch, for example an apartment intercom door release, or as complex as a biometric based access control system.\n\nThere are two basic types of locks: \"preventing mechanism\" or operation mechanism.\n\nThe most basic type of electronic lock is a magnetic lock (informally called a \"mag lock\"). A large electro-magnet is mounted on the door frame and a corresponding armature is mounted on the door. When the magnet is powered and the door is closed, the armature is held fast to the magnet. Mag locks are simple to install and are very attack-resistant. One drawback is that improperly installed or maintained mag locks can fall on people, and also that one must unlock the mag lock to both enter and to leave. This has caused fire marshals to impose strict rules on the use of mag locks and access control practice in general. Additionally, NFPA 101 (Standard for Life Safety and Security), as well as the ADA (Americans with Disability Act) require \"no prior knowledge\" and \"one simple movement\" to allow \"free egress\". This means that in an emergency, a person must be able to move to a door and immediately exit with one motion (requiring no push buttons, having another person unlock the door, reading a sign, or \"special knowledge\").\n\nOther problems include a lag time (delay), because the collapsing magnetic field holding the door shut does not release instantaneously. This lag time can cause a user to collide with the still-locked door. Finally, mag locks fail unlocked, in other words, if electrical power is removed they unlock. This could be a problem where security is a primary concern. Additionally, power outages could affect mag locks installed on fire listed doors, which are required to remain latched at all times except when personnel are passing through. Most mag lock designs would not meet current fire codes as the primary means of securing a fire listed door to a frame. Because of this, many commercial doors (this typically does not apply to private residences) are moving over to stand-alone locks, or electric locks installed under a Certified Personnel Program. \n\nThe first mechanical recodable card lock was invented in 1976 by Tor Sørnes, who had worked for VingCard since the 1950s. The first card lock order was shipped in 1979 to Westin Peachtree Plaza Hotel, Atlanta, US. This product triggered the evolution of electronic locks for the hospitality industry.\n\nElectric strikes (also called electric latch release) replace a standard strike mounted on the door frame and receive the latch and latch bolt. Electric strikes can be simplest to install when they are designed for one-for-one drop-in replacement of a standard strike, but some electric strike designs require that the door frame be heavily modified. Installation of a strike into a fire listed door (for open backed strikes on pairs of doors) or the frame must be done under listing agency authority, if any modifications to the frame are required (mostly for commercial doors and frames). In the US, since there is no current Certified Personnel Program to allow field installation of electric strikes into fire listed door openings, listing agency field evaluations would most likely require the door and frame to be de-listed and replaced. \n\nElectric strikes can allow mechanical free egress: a departing person operates the lockset in the door, not the electric strike in the door frame. Electric strikes can also be either \"fail unlocked\" (except in Fire Listed Doors, as they must remain latched when power is not present), or the more-secure \"fail locked\" design. Electric strikes are easier to attack than a mag lock. It is simple to lever the door open at the strike, as often there is an increased gap between the strike and the door latch. Latch guard plates are often used to cover this gap.\n\nElectric mortise and cylindrical locks are drop-in replacements for door-mounted mechanical locks. An additional hole must be drilled in the door for electric power wires. Also, a power transfer hinge is often used to get the power from the door frame to the door. Electric mortise and cylindrical locks allow mechanical free egress, and can be either fail unlocked or fail locked. In the US, UL rated doors must retain their rating: in new construction doors are cored and then rated. but in retrofits, the doors must be re-rated.\n\nElectrified exit hardware, sometimes called \"panic hardware\" or \"crash bars\", are used in fire exit applications. A person wishing to exit pushes against the bar to open the door, making it the easiest of mechanically-free exit methods. Electrified exit hardware can be either fail unlocked or fail locked. A drawback of electrified exit hardware is their complexity, which requires skill to install and maintenance to assure proper function. Only hardware labeled \"Fire Exit Hardware\" can be installed on fire listed doors and frames and must meet both panic exit listing standards and fire listing standards.\n\nMotor-operated locks are used throughout Europe. A European motor-operated lock has two modes, day mode where only the latch is electrically operated, and night mode where the more secure deadbolt is electrically operated.\n\nIn South Korea, most homes and apartments have installed electronic locks, which are currently replacing the lock systems in older homes. South Korea mainly uses a lock system by Gateman.\n\nA feature of electronic locks is that the locks can deactivated or opened by authentication, without the use of a traditional physical key: \n\nPerhaps the most common form of electronic lock uses a keypad to enter a numerical code or password for authentication. Such locks typically provide, and some feature an audible response to each press. Combination lengths are usually between 4 and 6 digits long.\n\nAnother means of authenticating users is to require them to scan or \"swipe\" a security token such as a smart card or similar, or to interact a token with the lock. For example, some locks can access stored credentials on a personal digital assistant (PDA) or smartphone, by using infrared, Bluetooth, or NFC data transfer methods.\n\nAs biometrics become more and more prominent as a recognized means of positive identification, their use in security systems increases. Some electronic locks take advantage of technologies such as fingerprint scanning, retinal scanning, iris scanning and voice print identification to authenticate users.\n\nRadio-frequency identification (RFID) is the use of an object (typically referred to as an \"RFID tag\") applied to or incorporated into a product, animal, or person for the purpose of identification and tracking using radio waves. Some tags can be read from several meters away and beyond the line of sight of the reader. This technology is also used in some modern electronic locks.\n\n"}
{"id": "15294873", "url": "https://en.wikipedia.org/wiki?curid=15294873", "title": "Electroosmotic pump", "text": "Electroosmotic pump\n\nAn electroosmotic pump (EOP), or EO pump, is used for generating flow or pressure by use of an electric field. One application of this is removing liquid flooding water from channels and gas diffusion layers and direct hydration of the proton exchange membrane in the membrane electrode assembly (MEA) of the proton exchange membrane fuel cells.\n\nElectroosmotic pumps are fabricated from silica nanospheres or hydrophilic porous glass, the pumping mechanism is generated by an external electric field applied on an electric double layer (EDL), generates high pressures (e.g., more than 340 atm (34 MPa) at 12 kV applied potentials) and high flow rates (e.g., 40 ml/min at 100 V in a pumping structure less than 1 cm³ in volume). EO pumps are compact, have no moving parts, and scale favorably with fuel cell design. The EO pump might drop the parasitic load of water management in fuel cells from 20% to 0.5% of the fuel cell power.\n\nHigh pressures or high flow rates are obtained by positioning several regular electroosmotic pumps in series or parallel respectively.\n\nPorous pumping is created by the use of sintered glass.\n\nPlanar shallow electroosmotic pumps are made of parallel shallow microchannels.\n\nElectroosmotic effects can also be induced without external fields in order to power micron-scale motion. Bimetallic gold/silver patches have been shown to generate local fluid pumping by this mechanism when hydrogen peroxide is added to the solution. A related motion can be induced by silver phosphate particles, which can be tailored to generate reversible firework behavior among other properties.\n\n\n"}
{"id": "46435454", "url": "https://en.wikipedia.org/wiki?curid=46435454", "title": "Glitch removal", "text": "Glitch removal\n\nGlitch removal is the elimination of glitchesunnecessary signal transitions without functionalityfrom electronic circuits. Power dissipation of a gate occurs in two ways: static power dissipation and dynamic power dissipation. Glitch power comes under dynamic dissipation in the circuit and is directly proportional to switching activity. Glitch power dissipation is 20%-70% of total power dissipation and hence glitching should be eliminated for low power design.\n\nSwitching activity occurs due to signal transitions which are of two types: functional transition and a glitch. Switching power dissipation is directly proportional to the switching activity (α), load capacitance (C), Supply voltage (V), and clock frequency (\"f\") as:\n\nSwitching activity means transition to different levels. Glitches are dependent on signal transitions and more glitches results in higher power dissipation. As per above equation switching power dissipation can be controlled by controlling switching activity (α), voltage scaling etc.\n\nAs discussed, more transition results in more glitches and hence more power dissipation. To minimize glitch occurrence, switching activity should be minimized. For example, Gray code could be used in counters instead of binary code, since every increment in Gray code only flips one bit. \n\nGate freezing minimizes power dissipation by eliminating glitching. It relies on the availability of modified standard library cells such as the so-called F-Gate. This method consists of transforming high glitch gates into modified devices which filter out the glitches when a control signal is applied. When the control signal is high, the F-Gate operates as normal but when the control signal is low, the gate output is disconnected from the ground. As a result it can never be discharged to logic 0 and glitches are prevented.\n\nHazards in digital circuits are unnecessary transitions due to varying path delays in the circuit. Balanced path delay techniques can be used for resolving differing path delays. To make path delays equal, buffer insertion is done on the faster paths. Balanced path delay will avoid glitches in the output.\n\nHazard filtering is another way to remove glitching. In hazard filtering gate propagation delays are adjusted. This results in balancing all path delays at the output.\n\nHazard filtering is preferred over path balancing as path balancing consumes more power due to the insertion of additional buffers.\n\nGate upsizing and gate downsizing techniques are used for path balancing. A gate is replaced by a logically equivalent but differently-sized cell so that delay of the gate is changed. Because increasing the gate size also increases power dissipation, gate-upsizing is only used when power saved by glitch removal is more than the power dissipation due to the increase in size. Gate sizing affects glitching transitions but does not affect the functional transition.\n\nThe delay of a gate is a function of its threshold voltage. Non-critical paths are selected and threshold voltage of the gates in these paths is increased. This results in balanced propagation delay along different paths converging at the receiving gate. Performance is maintained since it is determined by the time required by the critical path. A higher threshold voltage also reduces the leakage current of a path.\n\n\n\n"}
{"id": "42187404", "url": "https://en.wikipedia.org/wiki?curid=42187404", "title": "Gunnar Johannsen", "text": "Gunnar Johannsen\n\nGunnar Johannsen (born 1940) is a German cyberneticist, and Emeritus Professor of Systems Engineering and Human-Machine Systems at the University of Kassel, known for his contributions in the field of human-machine systems.\n\nBorn and raised in Hamburg, Johannsen received his Dipl.-Ing. in communication and information engineering from the Technical University of Berlin in 1967, where in 1971 he also received his Dr.-Ing. in flight guidance and manual control. He studied Sound Engineering at the Berlin University of the Arts for another three years, and received his habilitation in 1980.\n\nIn 1971 Johannsen started his academic career as division head at the Research Institute for Human Engineering (FGAN-FAT) in Wachtberg, south of Bonn. After his habilitation in 1980 he became privatdozent in the field of human-machine systems at the Department of Mechanical Engineering of the RWTH Aachen University. In 1982 he moved to the University of Kassel, where he was appointed Professor of Systems Engineering and Human-Machine Systems until his retirement in 2006. Over the years Johannsen was visiting Scholar at the University of Illinois at Urbana-Champaign in 1977−78, at the Kyoto Institute of Technology in 1995 and 2004; at the Vienna University of Technology in 1999; and at the University of British Columbia in 2004.\n\nJohannsen was fellow of the International Federation of Automatic Control, where in 1981 he founded the working Group on Man-Machine Systems, and served in different positions over the years. In 1995 he was awarded the Japanese-German Research Award by the Japan Society for the Promotion of Science. In 2001 he was elected Fellow of the IEEE. In 2005 the University of Valenciennes and Hainaut-Cambresis awarded him an Docteur Honoris Causa, and in 2007 the Benjamin Franklin Institute of Technology nominated him for the 2007 Bower Award and Prize for Achievement in Science on Human-Centered Computing.\n\nIn 1986, Jim Alty's HCI group at The Turing Institute won a major European Strategic Program on Research in Information Technology 1 contract to investigate the use of Knowledge Based systems in Process Control Interfaces called GRADIENT (Graphical Intelligent Dialogues, P600). with Gunnar Johannsen of Kassel University, Peter Elzer of the Clausthal University of Technology and Asea Brown Boveri to create intelligent interfaces for process control operators.\n\nThis work had a major impact on process control interface design. The initial pilot phase report (Alty, Elzer et al., 1985) was widely used and cited. Many research papers were produced A follow-on large ESPRIT research project was PROMISE (Process Operators Multimedia Intelligent Support Environment) working with DOW Benelux (Netherlands), Tecsiel (Italy) and Scottish Power (Scotland).\n\nJohannsen author and co-author of numerous publications in the field of human-machine systems and control theory. A selection:\n\nArticles, a selection:\n\n"}
{"id": "1227788", "url": "https://en.wikipedia.org/wiki?curid=1227788", "title": "Harold Pender Award", "text": "Harold Pender Award\n\nThe Harold Pender Award, initiated in 1972 and named after founding Dean Harold Pender, is given by the Faculty of the School of Engineering and Applied Science of the University of Pennsylvania to an outstanding member of the engineering profession who has achieved distinction by significant contributions to society. The Pender Award is the School of Engineering's highest honor.\n\n"}
{"id": "39519569", "url": "https://en.wikipedia.org/wiki?curid=39519569", "title": "Hu Zhengyan", "text": "Hu Zhengyan\n\nHu Zhengyan (; 15841674) was a Chinese artist, printmaker and publisher. He worked in calligraphy, traditional Chinese painting, and seal-carving, but was primarily a publisher, producing academic texts as well as records of his own work.\n\nHu lived in Nanjing during the transition from the Ming dynasty to the Qing dynasty. A Ming loyalist, he was offered a position at the rump court of the Hongguang Emperor, but declined the post, and never held anything more than minor political office. He did, however, design the Hongguang Emperor's personal seal, and his loyalty to the dynasty was such that he largely retired from society after the emperor's capture and death in 1645. He owned and operated an academic publishing house called the Ten Bamboo Studio, in which he practised various multi-colour printing and embossing techniques, and he employed several members of his family in this enterprise. Hu's work at the Ten Bamboo Studio pioneered new techniques in colour printmaking, leading to delicate gradations of colour which were not previously achievable in this art form.\n\nHu is best known for his manual of painting entitled \"The Ten Bamboo Studio Manual of Painting and Calligraphy\", an artist's primer which remained in print for around 200 years. His studio also published seal catalogues, academic and medical texts, books on poetry, and decorative writing papers. Many of these were edited and prefaced by Hu and his brothers.\n\nHu was born in Xiuning County, Anhui Province in 1584 or early 1585. Both his father and elder brother Zhengxin (, art name Wusuo, ) were physicians, and after he turned 30 he travelled with them while they practised medicine in the areas around Lu'an and Huoshan. It is commonly stated that Zhengyan himself was also a doctor, though the earliest sources attesting to this occur only in the second half of the 19th century.\n\nBy 1619, Hu had moved to Nanjing where he lived with his wife Wu. Their home on Jilongshan (, now also known as Beiji Ge), a hill located just within the northern city wall, served as a meeting-house for like-minded artists. Hu named it the Ten Bamboo Studio (\"Shizhuzhai\", ), after the ten bamboo plants that grew in front of the property. It functioned as the headquarters for his printing business, where he employed ten artisans including his two brothers Zhengxin and Zhengxing (, art name Zizhu, ) and his sons Qipu () and Qiyi (, courtesy name ).\n\nDuring Hu's lifetime, the Ming dynasty, which had ruled China for over 250 years, was overthrown and replaced by China's last imperial dynasty, the Qing. Following the fall of the capital Beijing in 1644, remnants of the imperial family and a few ministers set up a Ming loyalist regime in Nanjing with Zhu Yousong on the throne as the Hongguang Emperor. Hu, who was noted for his seal-carving and facility with seal script, created a seal for the new Emperor. The court offered him the position of Drafter for the Secretariat (\"zhongshu sheren\", ) as a reward, but he did not accept the role (although he did accord himself the title of \"zhongshu sheren\" in some of his subsequent personal seals).\n\nAccording to Wen Ruilin's \"Lost History of the South\" (\"Nanjiang Yishi\", ), prior to the Qing invasion of Nanjing Hu studied at the National University there, and whilst a student was employed by the Ministry of Rites to record official proclamations; he produced the \"Imperial Promotion of Minor Learning\" (\"Qin Ban Xiaoxue\", ) and the \"Record of Displayed Loyalty\" (\"Biaozhong Ji\", ) as part of this work. As a result, he was promoted to the Ministry of Personnel and gained admittance to the Hanlin Academy, but before he could take up this appointment, Beijing had fallen to the Manchu rebellion. Since contemporaneous biographies (Wen's work was not published until 1830) make no mention of these events, it has been suggested that they were fabricated after Hu's death.\n\nHu retired from public life and went into seclusion in 1646, after the end of the Ming dynasty. Xiao Yuncong and Lü Liuliang recorded visiting him during his later years, in 1667 and 1673 respectively. He died in poverty at the age of 90, sometime around late 1673 or early 1674.\n\nHu Zhengyan was a noted seal-carver, producing personal seals for numerous dignitaries. His style was rooted in the classical seal script of the Han dynasty, and he followed the Huizhou school of carving founded by his contemporary He Zhen. Hu's calligraphy, although balanced and with a clear compositional structure, is somewhat more angular and rigid than the classical models he followed. Huizhou seals attempt to impart an ancient, weathered impression, although unlike other Huizhou artists Hu did not make a regular practice of artificially aging his seals.\n\nHu's work was known outside his local area. Zhou Lianggong, a poet who lived in Nanjing around the same time as Hu and was a noted art connoisseur, stated in his \"Biography of Seal-Carvers\" (\"Yinren Zhuan\", ) that Hu \"creates miniature stone carvings with ancient seal inscriptions for travellers to fight over and treasure\", implying that his carvings were popular with visitors and travellers passing through Nanjing.\n\nIn 1644, Hu took it upon himself to create a new Imperial seal for the Hongguang Emperor, which he carved after a period of fasting and prayer. He presented his creation with an essay, the \"Great Exhortation of the Seal\" (\"Dabao Zhen\", ), in which he bemoaned the loss of the Chongzhen Emperor's seal and begged Heaven's favour in restoring it. Hu was concerned that his essay would be overlooked because he had not written it in the form of rhyming, equally-footed couplets (\"pianti\", ) used in the Imperial examinations, but his submission and the seal itself were nevertheless both accepted by the Southern Ming court.\n\nDespite his reputation as an artist and seal-carver, Hu was primarily a publisher. His publishing house, the Ten Bamboo Studio, produced reference works on calligraphy, poetry and art; medical textbooks; books on etymology and phonetics; and copies of as well as commentaries on the Confucian Classics. Unlike other publishers in the area, the Ten Bamboo Studio did not publish works of narrative fiction such as plays or novels. This bias towards academia was likely a consequence of the studio's location: the mountain on which Hu took up residence was just to the north of the Nanjing Guozijian (National Academy), which provided a captive market for academic texts. Between 1627 and 1644, the Ten Bamboo Studio produced over twenty printed books of this kind, aimed at a wealthy, literary audience. The studio's earliest publications were medical textbooks, the first of which, \"Tested Prescriptions for Myriad Illnesses\" (\"Wanbing Yanfang\", ) was published in 1631 and proved popular enough to be reissued ten years later. Hu's brother Zhengxin was a medical practitioner and appears to have been the author of these books.\n\nDuring the 1630s the Ten Bamboo Studio also produced political works extolling the rule of the Ming; these included the \"Imperial Ming Record of Loyalty\" (\"Huang Ming Biaozhong Ji\", ), a biography of loyal Ming officials, and the \"Edicts of the Imperial Ming\" (\"Huang Ming Zhaozhi\", ), a list of Imperial proclamations. After the fall of the Ming Dynasty, Hu renamed the studio the Hall Rooted in the Past (\"Digutang\", ) as a sign of his affiliation with the previous dynasty, although the Ten Bamboo imprint continued to be used. Despite Hu's withdrawal from society after 1646, the studio continued to publish well into the Qing dynasty, for the most part focussing on seal impression catalogues showcasing Hu's carving work.\n\nThe Ming dynasty had seen considerable advancement in the process of colour printing in China. At his studio, Hu Zhengyan experimented with various forms of woodblock printing, creating processes for producing multi-coloured prints and embossed printed designs. As a result, he was able to produce some of China's first printed publications in colour, using a block printing technique known as \"assorted block printing\" (\"douban yinshua\", ). This system made use of multiple blocks, each carved with a different part of the final image and each bearing a different colour. It was a lengthy, painstaking process, requiring thirty to fifty engraved printing blocks and up to seventy inkings and impressions to create a single image. Hu also employed a related form of multiple-block printing called \"set-block printing\" (\"taoban yinshua\", ), which had existed since the Yuan period some 200 years earlier but had only recently come into fashion again. He refined these block printing techniques by developing a process for wiping some of the ink off the blocks before printing; this enabled him to achieve gradation and modulation of shades which were not previously possible.\n\nIn some images, Hu employed a blind embossing technique (known as \"embossed designs\" (\"gonghua\", ) or \"embossed blocks\" (\"gongban\", ), using an uninked, imprinted block to stamp designs onto paper. He used this to create white relief effects for clouds and for highlights on water or plants. This was a relatively new process, having been invented by Hu's contemporary Wu Faxiang, who was also a Nanjing-based publisher. Wu had used this technique for the first time in his book \"Wisteria Studio Letter Paper\" (\"Luoxuan Biangu Jianpu\", ), published in 1626. Both Hu and Wu used embossing to create decorative writing papers, the sale of which provided a sideline income for the Ten Bamboo Studio.\n\nHu's most notable work is the \"Ten Bamboo Studio Manual of Painting and Calligraphy\" (\"Shizhuzhai Shuhuapu\", ), an anthology of around 320 prints by around thirty different artists (including Hu himself), published in 1633. It consists of eight sections, covering calligraphy, bamboo, flowers, rocks, birds and animals, plums, orchids and fruit. Some of these sections had been released previously as single volumes. As well as a collection of artworks, it was also intended as an artistic primer, with instructions on correct brush position and technique and several pictures designed for beginners to copy. Although these instructions only appear in the sections on orchids and bamboo, the book still remains the first example of a categorical and analytical approach to Chinese painting. In this book, Hu used his multiple-block printing methods to obtain gradations of colour in the images, rather than obvious outlines or overlaps. The manual is bound in the \"butterfly binding\" (\"hudie zhuang\", ) style, whereby whole-folio illustrations are folded so that each occupies a double-page spread. This binding style allows the reader to lay the book flat in order to look at a particular image. Cambridge University Library released a complete digital scan of the manual, including all writings and illustrations in August, 2015. Said Charles Aylmer, Head of the Cambridge University Chinese Department, \"The binding is so fragile, and the manual so delicate, that until it was digitized, we have never been able to let anyone look through it or study it – despite its undoubted importance to scholars.\"\n\nThis volume went on to influence colour printing across China, where it paved the way for the later but better-known \"Manual of the Mustard Seed Garden\" (\"Jieziyuan Huazhuan\" ), and also in Japan, where it was reprinted and foreshadowed the development in ukiyo-e of the colour woodblock printing process known as \"nishiki-e\" . The popularity of the \"Ten Bamboo Studio Manual\" was such that print runs continued to be produced all the way through to the late Qing dynasty.\n\nHu also produced the work \"Ten Bamboo Studio Letter Paper\" (\"Shizhuzhai Jianpu\", ), a collection of paper samples, which made use of the \"gonghua\" stamped embossing technique to make the illustrations stand out in relief. Whilst primarily a catalogue of decorative writing papers, it also contained paintings of rocks, people, ritual vessels and other subjects. The book was bound in the \"wrapped back\" (\"baobei zhuang\", ) style, in which the folio pages are folded, stacked, and sewn along the open edges. Originally published in 1644, it was reissued in four volumes between 1934 and 1941 by Zheng Zhenduo and Lu Xun, and revised and republished again in 1952.\n\nOther works produced by Hu's studio included a reprint of Zhou Boqi's manual of seal-script calligraphy, \"The Six Styles of Calligraphy, Correct and Erroneous\" (\"Liushu Zheng'e\", ) and the related \"Necessary Investigations into Calligraphy\" (\"Shufa Bi Ji\", ), which discussed common errors in the formation of characters. With his brother Zhengxin, Hu edited a new introductory edition of the Confucian classics, entitled \"The Standardised Text of the Four Books, Identified and Corrected\" (\"Sishu Dingben Bianzheng\", ) (1640), giving the correct formation and pronunciation of the text. A similar approach was taken with the \"Essentials of the Thousand Character Classic in Six Scripts\" (\"Qianwen Liushu Tongyao\", ) (1663), which Hu compiled with the aid of his calligraphy teacher, Li Deng. It was published after Li's death, partly in homage to him.\n\nThe three Hu brothers worked together to collate a student primer on poetry by their contemporary Ye Tingxiu, which was called simply the \"Discussion of Poetry\" (\"Shi Tan\", ) (1635). Other works on poetry from the studio included \"Helpful Principles to the Subtle Workings of Selected Tang Poems\" (\"Leixuan Tang Shi Zhudao Weiji\", ), which was a compilation of several works on poetry and included colophons by Hu Zhengyan himself.\n\nAmong the studio's more obscure publications was a text on Chinese dominoes entitled \"Paitong Fuyu\" (), written under a pseudonym but with a preface by Hu Zhengyan.\n\n"}
{"id": "28346083", "url": "https://en.wikipedia.org/wiki?curid=28346083", "title": "Juwi", "text": "Juwi\n\nThe Juwi Holding AG is a company which builds renewable power supply facilities.\nFounded by Fred Jung and Matthias Willenbacher in 1996, in Rhineland-Palatinate, Germany.\nThe today's headquarters is in Wörrstadt. The name \"juwi\" is an acronym based on the initials of its two founders.\n\nIn its early days Juwi was a two-man company. Today Juwi has over 900 employees and 900 million Euro revenue.\n\nSolar parks are built by Juwi, for example Waldpolenz Solar Park.\n\n"}
{"id": "16909515", "url": "https://en.wikipedia.org/wiki?curid=16909515", "title": "Korangi Industrial Area", "text": "Korangi Industrial Area\n\nKorangi Industrial Area (KIA) is located in [Korangi District, in Karachi, Sindh, Pakistan. It is one of the largest industrial areas of Pakistan. It houses approximately 4500 industries, commercial and trading units including textile, steel, pharmaceutical, automobile, chemical, engineering and flour mills. The Korangi Association of Trade & Industry (KATI) is representative trade body of this industrial estate. Mr. S.M. Muneer is the Patron-in-Chief of KATI. \n\n\n"}
{"id": "49247746", "url": "https://en.wikipedia.org/wiki?curid=49247746", "title": "L'Increvable", "text": "L'Increvable\n\nL'Increvable is the name given to the washing machine built to last 50 years and created by french designer Julien Phedyaeff. It is a device that comes as an assembly kit.\n\nAccording to Julien Phedyaeff, the idea dates back to 2013, during his master's thesis on 'extended research on object dismantling practises'. He made a report that detailed how spare parts, when available, were sold at exceedingly high prices. His invention is designed to be delivered in kit from 'a healthy, solid and improvable base'. In order for it to be lighter, the washing machine lacks a concrete ballast. It is replaced by a reservoir which fills with water during the first use. \nThe machine originally was a project presented for his diploma at the École Nationale Supérieure de Création Industrielle which he obtained in June 2014. The project received congratulations from the jury and obtained a label from 'l’Observeur du design 2015'.\n\n\n"}
{"id": "56495543", "url": "https://en.wikipedia.org/wiki?curid=56495543", "title": "Laser chemical vapor deposition", "text": "Laser chemical vapor deposition\n\nLaser chemical vapor deposition (LCVD) is a chemical process used to produce high purity, high performance films, fibers, and mechanical hardware (MEMS). The process is used in the semiconductor industry for spot coating, the MEMS industry for 3-D printing of hardware such as springs and heating elements, and the composites industry for boron and ceramic fibers. As with conventional CVD, one or more gas phase precursors are thermally decomposed, and the resulting chemical species 1) deposit on a surface, or 2) react, form the desired compound, and then deposit on a surface, or a combination of (1) and (2).\n"}
{"id": "1852610", "url": "https://en.wikipedia.org/wiki?curid=1852610", "title": "LightScribe", "text": "LightScribe\n\nLightScribe is an optical disc recording technology that was created by the Hewlett-Packard Company. It uses specially coated recordable CD and DVD media to produce laser-etched labels with text or graphics, as opposed to stick-on labels and printable discs. Although HP no longer is developing the technology it is still maintained and supported by a number of independent enthusiasts.\n\nThe LightScribe method uses the laser in a way similar to when plain data are written to the disc; a greyscale image of the label is etched onto the upper side of the disc. In the beginning, the discs were available only in a sepia color but today are available in many monochromatic colors.\n\nThe purpose of LightScribe is to allow users to create direct-to-disc labels (as opposed to stick-on labels), using their optical disc writer. Special discs and a compatible disc writer are required. Before or after burning data to the read-side of the disc, the user turns the disc over and inserts it with the label side down. The drive's laser then etches into the label side in such a way that an image is produced.\n\nLightScribe was conceived by Hewlett-Packard engineer Daryl Anderson, and the coating's chemistry was developed by Dr. Makarand Gore, and brought to market through the joint design efforts of HP's imaging and optical storage divisions in 2004.\n\nIt was the first direct to disc labeling technology that allowed users to laser etch images to the label side of a disc. DiscT@2 technology had been on the market since 2002, but DiscT@2 allows users to burn only to the unused portion of the data side of the disc. In 2005, LabelFlash became the main competitor for LightScribe.\n\nVarious brands manufacture the required media. Dual Layer DVD+Rs are currently the highest-capacity disc to support the technology. There has yet to be any development of LightScribe Blu-ray discs.\n\nCompanies such as HP, Samsung, LaCie and LiteOn have discontinued or are phasing out LightScribe drives with only LG continuing to manufacturing drives; until the manufacturing of LG LightScribe drives was discontinued.\n\n, LightScribe.com HP's official LightScribe website has been removed. This has been replaced with the following message:\n\n, the website returns a 404 error. Apparently, on HP Desktop PCs Customer Support, it still has an article of instructions and FAQ about LightScribe.\n\n, the website now redirects to Hewlett-Packard's homepage.\n\nThe surface of a LightScribe disc is coated with a reactive dye that changes color when it absorbs 780 nm infrared laser light. The etched label will show no noticeable fading under exposure to indoor lighting for at least two years. Optical media should always be stored in a protective sleeve or case that keeps the data content in the dark and safe from scratches. If stored this way, the label should last the life of the disc in real-world applications.\n\nLightScribe labels burn in concentric circles, moving outward from the center of the disc. Images with the largest diameters will take longest to burn.\n\nLightScribe is monochromatic, initially a grey etch on a gold-looking surface. From late 2006, LightScribe discs are also available in various surface colors, under the v1.2 specification. The etching is still in shades of grey.\n\nIt is not possible to replace a LightScribe label with a new design (as in erase the surface of the disc), but it is possible to add more content to a label that has already been burned.\n\nThe center of every LightScribe disc has a unique code that allows the drive to know the precise rotational position of the disc. This, in combination with the drive hardware, allows it to know the precise position from the center outwards, and the disc can be labeled while spinning at high speed using these references. It also serves a secondary purpose: the same disc can be labeled with the same image multiple times. Each successive labeling will darken the blacks and increase image contrast (see drawbacks). Successive burns are perfectly aligned.\n\nSpecial storage precautions are necessary to prevent LightScribe discs from fading. HP's LightScribe website warns users to \"keep discs away from extreme heat, humidity and direct sunlight\", \"store them in a cool, dark place\", \"use polypropylene disc sleeves rather than PVC sleeves\", and also notes that \"residual chemicals on your fingers could cause discoloration of the label image\". Such chemicals include common hand lotions and hair care products. Users not observing these precautions have reported LightScribe discs to become visibly faded within two months in the worst case. This drawback makes the technology unsuitable for applications involving continuous handling, and for such popular uses as car music compilation discs, which typically have unavoidable high light and temperature exposure. Since many disc players present internal temperatures significantly higher than room temperature, LightScribe discs should also not be left in disc players for long periods of time.\n\nLightScribe discs may form a visible white powder coating. This is due to crystallization of some of the label-side coating. It is not harmful and can easily be removed with a water-dampened cloth. Wiping the disc with a damp cloth does not harm the inscribed label. At this point, LightScribe support has not explained which conditions lead to this reaction, nor the precautions that can be taken to avoid it.\n\nMultiple LightScribes of the same image increases contrast, but the image quality decreases with successive burns. Noticeable contrast variations are seen in solid shades.\n\nA LightScribe optical drive was used by Maher El-Kady, a graduate of UCLA in 2012 to successfully turn a specially prepared graphite oxide layer coated onto a DVD into graphene. El-Kady and Richard Kaner, his lab professor, used an unmodified LightScribe drive. The disc was prepared by coating the disc with an aqueous solution of graphite oxide and allowing it to dry. Areas of the coating illuminated by the LightScribe laser were turned into graphene. Various shapes can be drawn, which allowed the scientist duo essentially to laser-print an ultracapacitor on graphene using consumer-grade technology.\n\n\n"}
{"id": "7565507", "url": "https://en.wikipedia.org/wiki?curid=7565507", "title": "List of Fujitsu products", "text": "List of Fujitsu products\n\nFujitsu, a multinational computer hardware and IT services company, provides services and consulting as well as a range of products including computing products, software, telecommunications, microelectronics, and more. Fujitsu also offers customized IT products that go beyond the off-the shelf products listed below.\n"}
{"id": "11572324", "url": "https://en.wikipedia.org/wiki?curid=11572324", "title": "List of Indian inventions and discoveries", "text": "List of Indian inventions and discoveries\n\nThis list of Indian inventions and discoveries details the inventions, scientific discoveries and contributions of premodern and modern India, including both the ancient, classical and post classical nations in the subcontinent historically referred to as India and the modern Indian state. It draws from the whole cultural and technological history of India, during which architecture, astronomy, cartography, metallurgy, logic, mathematics, metrology and mineralogy were among the branches of study pursued by . During recent times science and technology in the Republic of India has also focused on automobile engineering, information technology, communications as well as research into space and polar technology.\n\nFor the purposes of this list, inventions are regarded as technological firsts developed in India, and as such does not include foreign technologies which India acquired through contact. It also does not include technologies or discoveries developed elsewhere and later invented separately in India, nor inventions by Indian emigres in other places. Changes in minor concepts of design or style and artistic innovations do not appear on in the lists.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe fiber is also known as \"pashm\" or \"pashmina\" for its use in the handmade shawls of Kashmir, India. The woolen shawls made from wool in Kashmir region of India find written mention between the 3rd century BCE and the 11th century CE. However, the founder of the cashmere wool industry is traditionally held to be the 15th-century ruler of Kashmir, \"Zayn-ul-Abidin\", who employed weavers from Central Asia.\n\n\"It is India that gave us the ingenuous method of expressing all numbers by the means of ten symbols, each symbol receiving a value of position, as well as an absolute value; a profound and important idea which appears so simple to us now that we ignore its true merit, but its very simplicity, the great ease which it has lent to all computations, puts our arithmetic in the first rank of useful inventions, and we shall appreciate the grandeur of this achievement when we remember that it escaped the genius of Archimedes and Apollonius, two of the greatest minds produced by antiquity.\"\n\n\n\n\n\n\n\n\n\n A total of 558 weights were excavated from Mohenjodaro, Harappa, and Chanhu-daro, not including defective weights. They did not find statistically significant differences between weights that were excavated from five different layers, each about 1.5 m in thickness. This was evidence that strong control existed for at least a 500-year period. The 13.7-g weight seems to be one of the units used in the Indus valley. The notation was based on the binary and decimal systems. 83% of the weights which were excavated from the above three cities were cubic, and 68% were made of chert.\n\n\n"}
{"id": "27826340", "url": "https://en.wikipedia.org/wiki?curid=27826340", "title": "MT4 ECN Bridge", "text": "MT4 ECN Bridge\n\nMT4 ECN Bridge is a technology that allows a user to access interbank foreign exchange market through the MetaTrader 4 (MT4) electronic trading platform. MT4 was designed to allow trading between a broker and its clients and so did not provide for passing orders through to wholesale forex market via electronic communication networks (ECNs). In response a number of third party software companies developed Straight-through processing bridging software to allow the MT4 server to pass orders placed by clients directly to an ECN and feed trade confirmations back automatically.\n\nRetail foreign exchange brokers in general are divided into two categories:\n\nThe MetaTrader4 trading platform was designed for dealing desk brokers, but the popularity of the trading platform meant that retail forex traders as well as brokers wanted to provide a way to trade directly on the interbank foreign exchange market via MT4.\n\nA number of software companies attempted to develop bridging technology that allowed the broker to connect the MT4 server to an ECN. The first practical solution to bridging MT4 and ECN appeared in 2006. There are two main types of bridge technology which effects the way orders are processed: the STP and ECN models. The table below shows the main differences.\n\nECN means direct access to the market where you can trade with other traders. Your orders are actually displayed in the market and are seen by others, who in turn can introduce their own orders and if the prices match, a deal is complete.\n\n\n\n\n"}
{"id": "12784273", "url": "https://en.wikipedia.org/wiki?curid=12784273", "title": "Medi Script", "text": "Medi Script\n\nMedi Script was North America’s first instant credit system. Developed in 1965-67 by the West brothers in Vancouver, B.C., Canada, the system was designed to provide consumer credit at the retail store level, something that did not exist at that point in history.\n\nAt that time the Canadian chartered banks controlled all the legal credit granting ability with the exception of vehicle fuel companies like the Esso credit card which was not available to general merchants and shop keepers. The West brothers hoped to change this.\n\nMedi Script was designed to allow individual storeowners and merchants the ability to grant controlled, third party credit to their customers by subscribing for a fee to the Medi Script company who would then in turn provide the active credit for the customers in question.\n\nThe name Medi Script was used because of an obscure law at the time that allowed the medical establishment to grant credit.\n\nWhile salespeople were actively marketing this new idea the Royal Bank of Canada was looking at the Medi Script company with the stated intent of purchasing the company and the rights to the Medi Script concept. During the negotiation process in 1968 the Royal Bank launched their own version dubbed the Chargex Credit card. Subsequently, the deal with the West brothers collapsed and their business collapsed as their idea took on a new life without them.\n"}
{"id": "18906", "url": "https://en.wikipedia.org/wiki?curid=18906", "title": "Microfluidics", "text": "Microfluidics\n\nMicrofluidics deals with the behaviour, precise control and manipulation of fluids that are geometrically constrained to a small, typically sub-millimeter, scale at which capillary penetration governs mass transport. It is a multidisciplinary field at the intersection of engineering, physics, chemistry, biochemistry, nanotechnology, and biotechnology, with practical applications in the design of systems in which low volumes of fluids are processed to achieve multiplexing, automation, and high-throughput screening. Microfluidics emerged in the beginning of the 1980s and is used in the development of inkjet printheads, DNA chips, lab-on-a-chip technology, micro-propulsion, and micro-thermal technologies.\n\nTypically, micro means one of the following features:\n\nTypically in microfluidic systems fluids are transported, mixed, separated or otherwise processed. The various applications of such systems rely on passive fluid control using capillary forces. In some applications, external actuation means are additionally used for a directed transport of the media. Examples are rotary drives applying centrifugal forces for the fluid transport on the passive chips. Active microfluidics refers to the defined manipulation of the working fluid by active (micro) components such as micropumps or microvalves. Micropumps supply fluids in a continuous manner or are used for dosing. Microvalves determine the flow direction or the mode of movement of pumped liquids. Often processes which are normally carried out in a lab are miniaturised on a single chip in order to enhance efficiency and mobility as well as reducing sample and reagent volumes.\n\nThe behaviour of fluids at the microscale can differ from \"macrofluidic\" behaviour in that factors such as surface tension, energy dissipation, and fluidic resistance start to dominate the system. Microfluidics studies how these behaviours change, and how they can be worked around, or exploited for new uses.\n\nAt small scales (channel size of around 100 nanometers to 500 micrometers) some interesting and sometimes unintuitive properties appear. In particular, the Reynolds number (which compares the effect of the momentum of a fluid to the effect of viscosity) can become very low. A key consequence is co-flowing fluids do not necessarily mix in the traditional sense, as flow becomes laminar rather than turbulent; molecular transport between them must often be through diffusion.\n\nHigh specificity of chemical and physical properties (concentration, pH, temperature, shear force, etc.) can also be ensured resulting in more uniform reaction conditions and higher grade products in single and multi-step reactions.\n\nMicrofluidic structures include micropneumatic systems, i.e. microsystems for the handling of off-chip fluids (liquid pumps, gas valves, etc.), and microfluidic structures for the on-chip handling of nanoliter (nl) and picoliter (pl) volumes. To date, the most successful commercial application of microfluidics is the inkjet printhead. Additionally, advances in microfluidic manufacturing allow the devices to be produced in low-cost plastics and part quality may be verified automatically.\n\nAdvances in microfluidics technology are revolutionizing molecular biology procedures for enzymatic analysis (e.g., glucose and lactate assays), DNA analysis (e.g., polymerase chain reaction and high-throughput sequencing), and proteomics. The basic idea of microfluidic biochips is to integrate assay operations such as detection, as well as sample pre-treatment and sample preparation on one chip.\n\nAn emerging application area for biochips is clinical pathology, especially the immediate point-of-care diagnosis of diseases. In addition, microfluidics-based devices, capable of continuous sampling and real-time testing of air/water samples for biochemical toxins and other dangerous pathogens, can serve as an always-on \"bio-smoke alarm\" for early warning.\n\nMicrofluidic technology has led to the creation of powerful tools for biologists to control the complete cellular environment, leading to new questions and discoveries. Many diverse advantages of this technology for microbiology are listed below:\n\nSome of these areas are further elaborated in the sections below.\n\nIn open microfluidics, at least one boundary of the system is removed, exposing the fluid to air or another interface (i.e. liquid). Advantages of open microfluidics include accessibility to the flowing liquid for intervention, larger liquid-gas surface area, and minimized bubble formation. Another advantage of open microfluidics is the ability to integrate open systems with surface-tension driven fluid flow, which eliminates the need for external pumping methods such as peristaltic or syringe pumps. Open microfluidic devices are also easy and inexpensive to fabricate by milling, thermoforming, and hot embossing. In addition, open microfluidics eliminates the need to glue or bond a cover for devices which could be detrimental for capillary flows. Examples of open microfluidics include open-channel microfluidics, rail-based microfluidics, paper-based, and thread-based microfluidics. Disadvantages to open systems include susceptibility to evaporation, contamination, and limited flow rate.\n\nThese technologies are based on the manipulation of continuous liquid flow through microfabricated channels. Actuation of liquid flow is implemented either by external pressure sources, external mechanical pumps, integrated mechanical micropumps, or by combinations of capillary forces and electrokinetic mechanisms. Continuous-flow microfluidic operation is the mainstream approach because it is easy to implement and less sensitive to protein fouling problems. Continuous-flow devices are adequate for many well-defined and simple biochemical applications, and for certain tasks such as chemical separation, but they are less suitable for tasks requiring a high degree of flexibility or fluid manipulations. These closed-channel systems are inherently difficult to integrate and scale because the parameters that govern flow field vary along the flow path making the fluid flow at any one location dependent on the properties of the entire system. Permanently etched microstructures also lead to limited reconfigurability and poor fault tolerance capability. Computer-aided design automation approaches for continuous-flow microfluidics have been proposed in recent years to alleviate the design effort and to solve the scalability problems.\n\nProcess monitoring capabilities in continuous-flow systems can be achieved with highly sensitive microfluidic flow sensors based on MEMS technology which offers resolutions down to the nanoliter range.\n\nDroplet-based microfluidics is a subcategory of microfluidics in contrast with continuous microfluidics; droplet-based microfluidics manipulates discrete volumes of fluids in immiscible phases with low Reynolds number and laminar flow regimes. Interest in droplet-based microfluidics systems has been growing substantially in past decades. Microdroplets allow for handling miniature volumes (μl to fl) of fluids conveniently, provide better mixing, encapsulation, sorting, and sensing, and suit high throughput experiments. Exploiting the benefits of droplet-based microfluidics efficiently requires a deep understanding of droplet generation to perform various logical operations such as droplet motion, droplet sorting, droplet merging, and droplet breakup.\n\nAlternatives to the above closed-channel continuous-flow systems include novel open structures, where discrete, independently controllable droplets\nare manipulated on a substrate using electrowetting. Following the analogy of digital microelectronics, this approach is referred to as digital microfluidics. Le Pesant et al. pioneered the use of electrocapillary forces to move droplets on a digital track. The \"fluid transistor\" pioneered by Cytonix also played a role. The technology was subsequently commercialised by Duke University. By using discrete unit-volume droplets, a microfluidic function can be reduced to a set of repeated basic operations, i.e., moving one unit of fluid over one unit of distance. This \"digitisation\" method facilitates the use of a hierarchical and cell-based approach for microfluidic biochip design. Therefore, digital microfluidics offers a flexible and scalable system architecture as well as high fault-tolerance capability. Moreover, because each droplet can be controlled independently, these systems also have dynamic reconfigurability, whereby groups of unit cells in a microfluidic array can be reconfigured to change their functionality during the concurrent execution of a set of bioassays. Although droplets are manipulated in confined microfluidic channels, since the control on droplets is not independent, it should not be confused as \"digital microfluidics\". One common actuation method for digital microfluidics is electrowetting-on-dielectric (EWOD). Many lab-on-a-chip applications have been demonstrated within the digital microfluidics paradigm using electrowetting. However, recently other techniques for droplet manipulation have also been demonstrated using magnetic force, surface acoustic waves, optoelectrowetting, mechanical actuation, etc.\n\nPaper-based microfluidic devices fill a growing niche for portable, cheap, and user-friendly medical diagnostic systems. \nPaper based microfluidics rely on the phenomenon of capillary penetration in porous media. In order to tune fluid penetration in porous substrates such as paper, in two and three dimensions, the pore structure, wettability and geometry of the microfluidic devices can be controlled while the viscosity and evaporation rate of the liquid play a further significant role. Many such devices feature hydrophobic barriers on hydrophilic paper that passively transport aqueous solutions to outlets where biological reactions take place. Current applications include portable glucose detection and environmental testing, with hopes of reaching areas that lack advanced medical diagnostic tools.\n\nEarly biochips were based on the idea of a DNA microarray, e.g., the GeneChip DNAarray from Affymetrix, which is a piece of glass, plastic or silicon substrate, on which pieces of DNA (probes) are affixed in a microscopic array. Similar to a DNA microarray, a protein array is a miniature array where a multitude of different capture agents, most frequently monoclonal antibodies, are deposited on a chip surface; they are used to determine the presence and/or amount of proteins in biological samples, e.g., blood. A drawback of DNA and protein arrays is that they are neither reconfigurable nor scalable after manufacture. Digital microfluidics has been described as a means for carrying out Digital PCR.\n\nIn addition to microarrays, biochips have been designed for two-dimensional electrophoresis, transcriptome analysis, and PCR amplification. Other applications include various electrophoresis and liquid chromatography applications for proteins and DNA, cell separation, in particular, blood cell separation, protein analysis, cell manipulation and analysis including cell viability analysis and microorganism capturing.\n\nBy combining microfluidics with landscape ecology and nanofluidics, a nano/micro fabricated fluidic landscape can be constructed by building local patches of bacterial habitat and connecting them by dispersal corridors. The resulting landscapes can be used as physical implementations of an adaptive landscape, by generating a spatial mosaic of patches of opportunity distributed in space and time. The patchy nature of these fluidic landscapes allows for the study of adapting bacterial cells in a metapopulation system. The evolutionary ecology of these bacterial systems in these synthetic ecosystems allows for using biophysics to address questions in evolutionary biology.\n\nThe ability to create precise and carefully controlled chemoattractant gradients makes microfluidics the ideal tool to study motility, chemotaxis and the ability to evolve / develop resistance to antibiotics in small populations of microorganisms and in a short period of time. These microorganisms including bacteria and the broad range of organisms that form the marine microbial loop, responsible for regulating much of the oceans' biogeochemistry.\n\nMicrofluidics has also greatly aided the study of durotaxis by facilitating the creation of durotactic (stiffness) gradients.\n\nBy rectifying the motion of individual swimming bacteria, microfluidic structures can be used to extract mechanical motion from a population of motile bacterial cells. This way, bacteria-powered rotors can be built.\n\nThe merger of microfluidics and optics is typical known as optofluidics. Examples of optofluidic devices are tunable microlens arrays and optofluidic microscopes.\n\nMicrofluidic flow enables fast sample throughput, automated imaging of large sample populations, as well as 3D capabilities. or superresolution.\n\nAcoustic droplet ejection uses a pulse of ultrasound to move low volumes of fluids (typically nanoliters or picoliters) without any physical contact. This technology focuses acoustic energy into a fluid sample in order to eject droplets as small as a millionth of a millionth of a litre (picoliter = 10 litre). ADE technology is a very gentle process, and it can be used to transfer proteins, high molecular weight DNA and live cells without damage or loss of viability. This feature makes the technology suitable for a wide variety of applications including proteomics and cell-based assays.\n\nMicrofluidic fuel cells can use laminar flow to separate the fuel and its oxidant to control the interaction of the two fluids without a physical barrier as would be required in conventional fuel cells.\n\nIn order to understand the prospects for life to exist elsewhere in the universe, astrobiologists are interested in measuring the chemical composition of extraplanetary bodies. Because of their small size and wide-ranging functionality, microfluidic devices are uniquely suited for these remote sample analyses. From an extraterrestrial sample, the organic content can be assessed using microchip capillary electrophoresis and selective fluorescent dyes. These devices are capable of detecting amino acids, peptides, fatty acids, and simple aldehydes, ketones, and thiols. These analyses coupled together could allow powerful detection of the key components of life, and hopefully inform our search for functioning extraterrestrial life.\n\n\n\n\n"}
{"id": "21498286", "url": "https://en.wikipedia.org/wiki?curid=21498286", "title": "Modulus robot", "text": "Modulus robot\n\nThe household robot Modulus, described by the manufacturer as \"the friend of \"Homo sapiens\"\", was made by Sirius, a company Massimo Giuliana set up in 1982 for marketing home and personal computers, and which decided to start building its own domestic robot back in 1984. When the first \"Modulus\" prototype had been realized, the company asked Isao Hosoe, a Japanese designer who has been living and working in Milan for many years, to study its \"body-work\". Hosoe's work, however, went well beyond this, and was followed by a complete technological reprocessing of the robot. Data Process was responsible for the design and manufacture of the electronic and mechanical parts, while Sirius used the expertise of an American company, the RB Robot Corporation, for the software (its founder, Joseph H. Bosworth, is known by some as \"the father of personal robotics\").\n\nTwo million dollars were invested in developing this particular piece of equipment. Research carried out in the United States showed that there would be greater development in this sector. It was also estimated that the use of \"Modulus\" could provide an opportunity to bring back into operation many PCs that were bought during the boom, but which are not used seldom, if ever. A good slice of the \"Modulus\" market could consist of the owners of these personal computers, newly aware of the possibility of connecting them to a personal robot.\n\n\"Modulus\" was designed as a robot with possible domestic applications, but in reality it is open to any future development. Modularity - hence its name - is one of its principal characteristics, and it has been designed for adaptation to the widest possible range of applications. Comparing the robot with man, \"Modulus\" can be said to have an electronic \"circulatory system\" that permits the various extremities (arms, head, etc.) to communicate with the brain (CPU in a computer). The \"Modulus\" robots could have abilities such as a phonemes synthesizer, voice recognition, infrared communication, etc., making it suitable for performing many functions ranging from helping to teach children to assisting the handicapped or invalids.\n\nWhen studying the eventual appearance of \"Modulus\", Isao Hosoe began by looking at the robots of the past. These included \"Electro\", built by Westinghouse in 1939. Hosoe hit on the right appearance by studying human expressions and gestures, bearing in mind that a domestic robot needs to be appealing on account of its proximity to man. Its eyelids have to open, its pupils dilate or contract. It must be able to nod or shake its head, bend its torso, and raise, lower and rotate its arms. \"Modulus\", however, has no feet. Available in three configurations - \"Base\", \"Service & Security\" and \"Moddy\" - \"Modulus\" stands on a Base unit 35 cm in diameter and 15 cm high, two two-speed motors connected to rubber wheels, and two spherical stabilizers. It comes with a small infrared Instrument for connecting it to another remote control device or for interacting with the major home and personal computers.\n\nThis first unit can be added to for different functions. As it stands it can be used in hobbies as a home computer, self-propelled peripheral, and can be useful for people wanting to learn to program robots. The simplest attachments which can be connected to the Base unit are a vacuum cleaner and a plotter-mechanism that uses felt pens, etc., to produce drawings of considerable precision.\n\nThe second \"Modulus\" configuration, the Service Robot, is obtained by fitting the Techno-cake home-security and service unit on to the Base. One of the components allow the robot to signal the presence of smoke, gas, water, and intruders; at the first sign of danger it informs the computer or triggers a siren or preset vocal message. An arm with ample freedom of movement and considerable gripping power can be added to the Service Robot. Using its meteo-system, the robot will fetch its owners umbrella if it is going to rain; a simple whistle will bring it toddling over to you.\n\nModdy is the third and most advanced version of the robot. It is obtained by adding a torso, two arms, and a head to the other two units. The robot can carry a tray with its two arms. Its head and big eyes make it into a really human character, reliable, and a companion to play with. It is not surprising then if Isao Hosoe, who designed \"Modulus\" together with Ann Marinelli, Donato Greco, and Alessio Pozzoli, also received special advice from his two children, ten-year-old Takeo and fifteen-year-old Taro.\n\nThe first two units were previously available on the market. Base complete with vacuum cleaner and plotter-mechanism cost about a million lire, while the price of the Techno-Cake varied from two to five million lire, depending on the type and number of components in function (not all the components were available yet, on account of the time needed to develop the software). It would probably take up to a year before Moddy was ready, which would cost between eight and ten million lire, but unfortunately the \"Modulus\" project became too expensive, causing the bankruptcy of Sirius before Moddy could be completed. Some of the robots were later sold to companies who distributed them for the aid of handicapped while many of the Moddy robots were later sold by Arngren Electronics A/S during the early 1990s.\n\n"}
{"id": "16505204", "url": "https://en.wikipedia.org/wiki?curid=16505204", "title": "Namibia Food and Allied Workers Union", "text": "Namibia Food and Allied Workers Union\n\nThe Namibia Food and Allied Workers Union (Nafau) is a trade union in Namibia affiliated with the National Union of Namibian Workers (Nunw). Nafau's organizational strongholds include the fishing industries around Walvis Bay and Lüderitz.\n\nNafau was founded on 20 September 1986. Originally led by recently released political prisoner John Pandeni, the Union was the first NUNW-affiliated industrial union in Namibia.\n\n"}
{"id": "16999115", "url": "https://en.wikipedia.org/wiki?curid=16999115", "title": "Off-site construction", "text": "Off-site construction\n\nOffsite construction refers to the planning, design, fabrication, and assembly of building elements at a location other than their final installed location to support the rapid and efficient construction of a permanent structure. Such building elements may be prefabricated in a different location and transported to the site or prefabricated on the construction site and then transported to their final location. Offsite construction is characterized by an integrated planning and supply chain optimization strategy. Offsite manufacturing (OSM), offsite production (OSP) and offsite fabrication (OSF) are used when referring primarily to the factory work proper.\n\nOff-site construction (like on-site construction) can be used for a variety of purposes including residential, educational, health care and commercial applications. Buildings can range from a few modular units to several hundred. They can be arranged in architectural configurations and can be many stories in height.\nOff-site construction is similar to modular construction, but it is focused primarily on permanent construction; modular construction can be either permanent or relocatable. Also known as offsite construction, or OSC, and also incorporates many MMC - or Modern Methods of Construction\n\n\n"}
{"id": "24663914", "url": "https://en.wikipedia.org/wiki?curid=24663914", "title": "PICMG 2.5", "text": "PICMG 2.5\n\nPICMG 2.5 is a specification by PICMG that standardizes the utilization of CompactPCI user definable pins for the computer telephony functions of standard TDM bus, telephony rear IO, 48 VDC and ringing distribution in a 6U chassis environment.\n\n"}
{"id": "22115540", "url": "https://en.wikipedia.org/wiki?curid=22115540", "title": "Pocket video camera", "text": "Pocket video camera\n\nA pocket video camera is a tapeless camcorder that is small enough to be carried in one's pocket. \n\nA typical pocket video camera has an LCD screen of at least 1.5\", the ability to capture either only standard video or both standard video and high-definition video (H.264/MPEG-4 AVC), at least 128MB of internal memory and a Secure Digital (SD) card extension or at least 4GB internal memory with no SD card slot. Power may be supplied by a proprietary Ni-MH rechargeable battery or 2 standard AA batteries. Dimensions 2.5 x 4.5 x 0.9 in and a weight of 3.8 oz are typical, and a USB connector is usually built-in.\n\nPocket camcorders weigh only from 0.2 to 0.4 lbs and fit neatly into any pocket or purse for ultra portability. Examples include the Creative Vado HD and the Flip Video. The Sony Bloggie MHS-PM5 can shoot video in Full HD and has 4x digital zoom. The Sanyo Xacti PD1 can shoot video in Full HD 1080p at 30fps, with Stereo audio and has 3x optical zoom.\n\n \n"}
{"id": "3089556", "url": "https://en.wikipedia.org/wiki?curid=3089556", "title": "Project MERRIMAC", "text": "Project MERRIMAC\n\nProject MERRIMAC was a domestic espionage operation coordinated under the Office of Security of the CIA. It involved information gathering procedures via infiltration and surveillance on Washington-based anti-war groups that might pose potential threats to the CIA. However, the type of data gathered also included general information on the infrastructure of targeted communities. Project MERRIMAC and its twin program, Project RESISTANCE were both coordinated by the CIA Office of Security. In addition, the twin projects were branch operations that relayed civilian information to their parent program, Operation CHAOS. The Assassination Archives and Research Center believes that Project MERRIMAC began in February 1967.\n\n\n"}
{"id": "23365934", "url": "https://en.wikipedia.org/wiki?curid=23365934", "title": "Robert Vallée", "text": "Robert Vallée\n\nRobert Vallée (5 October 1922 in Poitiers, France – 1 January 2017, Paris, France) was a French cyberneticist and mathematician. He was Professor at the Paris 13 University (University of Paris-Nord) and president of the World Organization of Systems and Cybernetics (WOSC).\n\nAt the beginning of the 1950s, Vallée wrote his first publications on what he named \"opérateur d'observation\" (which means in English \"operator of observation\"). The latter, in the simplest case, allows a cybernetic system to observe the state of its environment and itself. Thereafter, on the basis of these results, a decisional operator will be able to indicate the action to be taken. The two stages of perception and decision are distinguished by \"intellectual convenience\", but it is interesting to gather them in a unique operator, known as \"pragmatic\". A decision is influenced by the observation of events, but also by past perceptions. That means that, in the observation made at a given moment, traces of past observations are also present. Eventually, these processes follow one another in a loop. Vallée defined the study of this situation with the term \"epistemo-praxeology\", underlining the existing link between knowledge (episteme), resulting from observation, and action (praxis). Regarding the observation problem, Vallée was also interested in information theory.\n\nVallée also nourished private interests in sociological problems as well as in history. The first led him to describe a cybernetic creature covering the whole surface of the globe with its communication net (1952), an idea which has also been proposed (under the name of \"cybionte\", 1975) by Joël de Rosnay. He also wrote articles devoted to historical aspects of cybernetics and systems, referring to René Descartes, Louis de Broglie, and Norbert Wiener.\n\nVallée was born on 5 October 1922, in Poitiers, (France), as the son of professors in history. In 1969 he married the editor and translator Nicole Georges-Lévy. With his wife, Robert Vallée contributed to a translation, in French, of Norbert Wiener's book, \"Cybernetics: Or Control and Communication in the Animal and the Machine\".\n\nTowards the end of the 1920s and during the 1930s, Vallée attended the College of Angoulême where, in 1940, he obtained a bachelor's degree in Latin-Greek, mathematics, and philosophy. Between 1944 and 1946, he was student at the École Polytechnique in Paris. During the summer of 1954, he took part in the \"Foreign Students Summer Project\" of the Massachusetts Institute of Technology (with Norbert Wiener and Armand Siegel). In 1961 he became a Doctor of Science in mathematics with a thesis on an extension of the general relativity of Kaluza-Klein, under the direction of André Lichnerowicz (University of Paris).\n\nDuring his career, Vallée occupied several positions. Between 1956 and 1958, he was Associate-Director of the Institute Blaise Pascal in Paris. From 1961 to 1971, he was university lecturer in mathematics at the École Polytechnique and at the University of Franche-Comté (1962–1971) where he subsequently became Professor. Between 1971 and 1987 he was Professor at the University of Paris-Nord where he was also dean of the Faculty of Economics from 1973 to 1975 and president of the Department of Economical mathematics from 1975 to 1987. In 1987, the University of Paris-Nord conferred on him the title of Professor Emeritus. Vallée also gave a doctoral course on dynamic systems at the University of Paris 1 Pantheon-Sorbonne between 1975 and 1987.\n\nVallée was active within several associations and organizations, in particular:\n\n\nHe also was a member of the International Society for the Systems Sciences, the American Society for Cybernetic,s the Tutmonda Asocio pri Kibernetiko, Informatiko kaj Sistemiko (TAKIS), and of the international league of scientists for the use of the French language.\n\nIn his career several titles were conferred on him:\n\nFrom 1987 to 1999, Vallée was chief editor of the \"Revue Internationale de Systémique\" (AFCET) as well as member of the editorial boards of \"Kybernetes\" (official review of the WOSC), \"Economies et Sociétés\" (ISMEA), \"International Journal for Biological Systems\", \"Cybernetics and Human Knowing\", \"Grundlagenstudien aus Kybernetik und Geisteswissenschaft\" (TAKIS), \"Robotica\", and \"Res-Systemica\" (electronic journal of the French Association of Science of Systems (AFSCET) and the European Union of Systemics).\n\n\n"}
{"id": "4058157", "url": "https://en.wikipedia.org/wiki?curid=4058157", "title": "Roughing filter", "text": "Roughing filter\n\nRoughing filters provide pretreatment for turbid water or simple, low maintenance treatment when high water quality is not needed.\n\n"}
{"id": "28300749", "url": "https://en.wikipedia.org/wiki?curid=28300749", "title": "Scented water", "text": "Scented water\n\nScented water, odoriferous water or sweet water, is a water with a sweet aromatic smell. It is made of flowers or herbs and is the precursor of the modern day perfume. Scented waters are also used in making other products such as pomanders and body care products.\n\nSome of the flowers and herbs used to make scented water are:\n\n\nIn Medieval times a scented water was used for hygiene by a person washing their hands with it before meals, since many meals were forkless. The wealthy of Medieval times had their linen clothes boiled in scented water. Varahamihira (6th century) was using scented water to clean his toothbrushes. Just prior to Marie Antoinette's execution in 1793 one of her servants was able to smuggle her some minimal need requests, one being some scented water for her teeth.\n\nHippocrates mentions scented water in his medical practice:\nBoccaccio in his Decameron mentions scented washing waters:\n\nIt was a custom in the Roman Empire for amphitheatres to be furnished with scented water jet fountain sprays for refreshing the spectators. The overhead awning (velarium) shielding the spectators for the Colosseum was saturated with scented water for dripping fragrant water on spectators' heads to keep them cool.\n\nHenry Percy, 6th Earl of Northumberland in 1511-12 had various scented waters made for him, which consisted of his usage of over a gallon each week for the year. In the 1571 estate inventory of John Brodocke, an apothecary in Southampton, lists 32 types of scented water that he had as some of the items in his shop. In 17th century Constantinople the Venetians and Genoese had apothecary shops that had merchants of scented water including rose water. Scented waters were still being used into the 19th century of the wealthy nobles in Egypt as a type of perfume. It was an Egyptian custom then to sprinkle guests with rose water or orange-flower water after dinner just before they were to go home.\n\nEven though it is said by scholars that the art of perfumery has it roots in antiquity, the city of Montpellier has self established itself as the birthplace of perfumery. In the 18th century they honored guests of the city with scented water and other 'gifts of fragrance'. There are even perfume manuals that describe Montpellier perfumes as being \"a la mode de Montpellier\" because of extra ingredients used that enhance their fragrances.\n\nScented rose water is and has been used from time to time for cooking and flavoring foods in various recipes. Rose water is used in Greek cuisine.\n\nIn Medieval times scented water was used for sensual pleasures.\n\nIn Medieval times scented water was used at baptisms.\n\nMary Hooper in her book called \"Petals in the Ashes\" about the Great Fire of 1666 explains that the first step in making \"rose water\" is to gather together 3 or 4 full roses. These roses should not have been treated in any way with toxic pesticide for obvious reasons. One is to then take a pint of water and put into a saucepan. Then heat the water with the roses in it gently, not allowing it to boil. You are to cook the rose petals until they become transparent. After that happens, allow the cooked rose water to cool. Then drain through a sieve forcing the water to go through the rose petals. The resulting \"rose water\" is to be refrigerated. The scented water then is used as a cooling agent on the body as well as the face.\n\nHooper explains that scented water to bathe in can be made of dried herbs. She explains the idea of making such a scented water is to put 8 tablespoons of dried herbs in a pan of 1 pint of water and simmer for 10 minutes. This water then is to be allowed to cool. Then it is to be strained to remove all the herb pieces. For the actual bath then a quarter of this scented water is to be put into your bath, highlighted with fresh flower petals on top of the bath water. She explains roses and lovage have a cleansing and deodorising effect. For a refreshing effect she recommends to use rosemary and hyssop. For a relaxing effect to use lime flowers and lavender. For a soothing effect use chamomile and lemon balm.\n\n\n\n\n"}
{"id": "26062108", "url": "https://en.wikipedia.org/wiki?curid=26062108", "title": "Sewer dosing unit", "text": "Sewer dosing unit\n\nA sewer dosing unit (SDU) is a plumbing device to allow effective sewage disposal with low liquid-flow rates. With a global emphasis on water saving, many new buildings and renovations are seeing the installation of water saving fixtures such as low flow shower heads and low flush toilets. With the decrease in wastewater flows problems are arising from waste solids not being carried completely to the main sewers, often causing blockages.\n\n\nIndependent testing has shown that the use of an SDU allows for the use of low flush toilets using as little as 0.8 L to push general waste solids more than 25 metres along a typical wastewater drainpipe. \nAn advantage of the SDU is that it can use more than just the toilet wastewater to keep drains clear of blockages.\n\n\n"}
{"id": "54133220", "url": "https://en.wikipedia.org/wiki?curid=54133220", "title": "Snapt Inc", "text": "Snapt Inc\n\nSnapt Inc. is a software company that provides load balancing, acceleration, security and caching for websites, applications and services.\n\nThe company’s primary product is the Snapt application delivery controller or ADC.\n\nSnapt Inc. was founded in 2012 in South Africa and has an office, and headquarters, in Atlanta, Georgia.\n\nCompetitors include: F5 Networks, KEMP Technologies, Cisco Systems, and Citrix Systems.\n\nSnapt Inc.’s product range helps optimize and offload content, as well as spread requests between various servers to ensure that websites, applications, and services load as quickly as possible.\n\nThe company achieved year-on-year growth of 400% in both 2014 and 2015. By 2016, Snapt Inc’s load balancing and web acceleration offering had been used by over 10,000 clients in 50 countries including: Intel, Target, Cisco Systems and MTV. \nIn mid-2016, the company raised $1 million in investment funding, from private equity and venture capital firm, Convergence Partners.\n\nSnapt Inc’s ADC solution includes a layer 7 load balancer, web accelerator for web acceleration and caching, as well as a web application firewall.\nThe company's offerings are virtual machine (VM) and cloud-based.\n"}
{"id": "764380", "url": "https://en.wikipedia.org/wiki?curid=764380", "title": "Sound reinforcement system", "text": "Sound reinforcement system\n\nA sound reinforcement system is the combination of microphones, signal processors, amplifiers, and loudspeakers in enclosures all controlled by a mixing console that makes live or pre-recorded sounds louder and may also distribute those sounds to a larger or more distant audience. In many situations, a sound reinforcement system is also used to enhance or alter the sound of the sources on the stage, typically by using electronic effects, such as reverb, as opposed to simply amplifying the sources unaltered.\n\nA sound reinforcement system for a rock concert in a stadium may be very complex, including hundreds of microphones, complex live sound mixing and signal processing systems, tens of thousands of watts of amplifier power, and multiple loudspeaker arrays, all overseen by a team of audio engineers and technicians. On the other hand, a sound reinforcement system can be as simple as a small public address (PA) system, consisting of, for example, a single microphone connected to a 100 watt amplified loudspeaker for a singer-guitarist playing in a small coffeehouse. In both cases, these systems \"reinforce\" sound to make it louder or distribute it to a wider audience.\n\nSome audio engineers and others in the professional audio industry disagree over whether these audio systems should be called sound reinforcement (SR) systems or PA systems. Distinguishing between the two terms by technology and capability is common, while others distinguish by intended use (e.g., SR systems are for live event support and PA systems are for reproduction of speech and recorded music in buildings and institutions). In some regions or markets, the distinction between the two terms is important, though the terms are considered interchangeable in many professional circles.\n\nA typical sound reinforcement system consists of; input transducers (e.g., microphones), which convert sound energy such as a person singing into an electric signal, signal processors which alter the signal characteristics (e.g., equalizers that adjust the bass and treble, compressors that reduce signal peaks, etc.), amplifiers, which produce a powerful version of the resulting signal that can drive a loudspeaker and output transducers (e.g., loudspeakers in speaker cabinets), which convert the signal back into sound energy (the sound heard by the audience and the performers). These primary parts involve varying numbers of individual components to achieve the desired goal of reinforcing and clarifying the sound to the audience, performers, or other individuals.\n\nSound reinforcement in a large format system typically involves a signal path that starts with the signal inputs, which may be instrument pickups (on an electric guitar or electric bass) or a microphone that a vocalist is singing into or a microphone placed in front of an instrument or guitar amplifier. These signal inputs are plugged into the input jacks of a thick multicore cable (often called a \"snake\"). The snake then delivers the signals of all of the inputs to one or more mixing consoles.\n\nIn a coffeehouse or small nightclub, the snake may be only routed to a single mixing console, which an audio engineer will use to adjust the sound and volume of the onstage vocals and instruments that the audience hears through the main speakers and adjust the volume of the monitor speakers that are aimed at the performers.\n\nMid- to large-size performing venues typically route the onstage signals to two mixing consoles: the front of house (FOH), and the stage monitor system, which is often a second mixer at the side of the stage. In these cases, at least two audio engineers are required; one to do the main mix for the audience at FOH and another to do the monitor mix for the performers on stage.\n\nOnce the signal arrives at an input on a mixing console, this signal can be adjusted in many ways by the sound engineer. A signal can be equalized (e.g., by adjusting the bass or treble of the sound), compressed (to avoid unwanted signal peaks), or panned (that is sent to the left or right speakers). The signal may also be routed into an external effects processor, such as a reverb effect, which outputs a \"wet\" (effected) version of the signal, which is typically mixed in varying amounts with the \"dry\" (effect-free) signal. Many electronic effects units are used in sound reinforcement systems, including digital delay and reverb. Some concerts use pitch correction effects (e.g., AutoTune), which electronically correct any out-of-tune singing.\n\nMixing consoles also have additional \"sends\", also referred to as \"auxes\" or \"aux sends\" (an abbreviation for \"auxiliary send\"), on each input channel so that a different mix can be created and sent elsewhere for another purpose. One usage for aux sends is to create a mix of the vocal and instrument signals for the monitor mix (this is what the onstage singers and musicians hear from their monitor speakers or in-ear monitors). Another use of an aux send is to select varying amounts of certain channels (via the aux send knobs on each channel), and then route these signals to an effects processor. A common example of the second use of aux sends is to send all of the vocal signals from a rock band through a reverb effect. While reverb is usually added to vocals in the main mix, it is not usually added to electric bass and other rhythm section instruments.\n\nThe processed input signals are then mixed to the master faders on the console. The next step in the signal path generally depends on the size of the system in place. In smaller systems, the main outputs are often sent to an additional equalizer, or directly to a power amplifier, with one or more loudspeakers (typically two, one on each side of the stage in smaller venues, or a large number in big venues) that are connected to that amplifier. In large-format systems, the signal is typically first routed through an equalizer then to a crossover. A crossover splits the signal into multiple frequency bands with each band being sent to separate amplifiers and speaker enclosures for low, middle, and high-frequency signals. Low-frequency signals are sent to amplifiers and then to subwoofers, and middle and high-frequency sounds are typically sent to amplifiers which power full-range speaker cabinets. Using a crossover to separate the sound into low, middle and high frequencies can lead to a \"cleaner\", clearer sound (see bi-amplification) than routing all of the frequencies through a single full-range speaker system. Nevertheless, many small venues still use a single full-range speaker system, as it is easier to set up and less expensive.\n\nMany types of input transducers can be found in a sound reinforcement system, with microphones being the most commonly used input device. Microphones can be classified according to their method of transduction, polar pattern or their functional application. Most microphones used in sound reinforcement are either dynamic or condenser microphones. One type of directional microphone, called \"cardioid\" mics, are widely used in live sound, because they reduce pickup from the side and rear, helping to avoid unwanted feedback from the stage monitor system.\n\nMicrophones used for sound reinforcement are positioned and mounted in many ways, including base-weighted upright stands, podium mounts, tie-clips, instrument mounts, and headset mounts. Microphones on stands are also placed in front of instrument amplifiers to pick up the sound. Headset mounted and tie-clip mounted microphones are often used with wireless transmission to allow performers or speakers to move freely. Early adopters of headset mounted microphones technology included country singer Garth Brooks, Kate Bush, and Madonna.\n\nOther types of input transducers include magnetic pickups used in electric guitars and electric basses, contact microphones used on stringed instruments, and pianos and phonograph pickups (cartridges) used in record players. Electronic instruments such as synthesizers can have their output signal routed directly to the mixing console. A DI unit may be necessary to adapt some of these sources to the inputs of the console.\n\nWireless systems are typically used for electric guitar, bass, handheld microphones and in-ear monitor systems. This lets performers move about the stage during the show or even go out into the audience without the worry of tripping over or disconnecting cables.\n\nMixing consoles are the heart of a sound reinforcement system. This is where the sound engineer can adjust the volume and tone of each input, whether it is a vocalist's microphone or the signal from an electric bass, and mix, equalize and add effects to these sound sources. Doing the mixing for a live show requires a mix of technical and artistic/creative skills. A sound engineer needs to have an expert knowledge of speaker and amplifier set-up and other technologies. As well, a sound engineer has to have a good \"ear\" for what the music should sound like in order to create a good mix and use effects units appropriately. Every genre and style of music has different approaches to live sound mixing. The mixing approach for a hard rock band is very different from the mixing approach for a bluegrass music show; a hard rock show will typically have powerful subwoofer cabinets to create a \"heavy\", loud onstage sound. A bluegrass show will typically have a more \"acoustic\" or \"natural\" sound, in which the PA system simply reinforces the volume of the more quiet instruments.\n\nMultiple consoles can be used for different applications in a single sound reinforcement system. The Front of House (FOH) mixing console must be located where the operator can see the action on stage and hear the output of the loudspeaker system. Some venues with permanently installed systems such as religious facilities and theaters place the mixing console within an enclosed booth, but this approach is more common for broadcast and recording applications. This is far less common in live sound reproduction, since the engineer performs best when he can hear what the audience hears.\n\nLarge music productions often use a separate stage monitor mixing console which is dedicated to creating mixes for the performers' on-stage or in-ear monitors. These consoles are typically placed at the side of the stage so that the operator can communicate with the performers on stage. In cases where performers have to play at a venue that does not have a monitor engineer near the stage, the monitor mixing is done by the FOH engineer from the FOH console, which is located amongst the audience or at the back of the hall. This arrangement can be problematic because the performers end up having to request changes to the monitor mixes with \"...hand signals and clever cryptic phrases\" which may be misunderstood. The engineer also cannot hear the changes that he is applying to the monitors on stage, often resulting in a reduction of the quality of the onstage monitor mix.\n\nIn the 2010s, small PA systems for venues such as bars and clubs are now available with features that were formerly only available on professional-level equipment, such as digital reverb effects, graphic equalizers, and, in some models, feedback prevention circuits which electronically sense and prevent feedback \"howls\" before they become a problem. Digital effects units may offer multiple pre-set and variable reverb, echo and related effects. Digital loudspeaker management systems offer sound engineers digital delay (to ensure speakers are in sync with each other), limiting, crossover functions, EQ filters, compression and other functions in a single rack-mountable unit. In previous decades, sound engineers typically had to transport a substantial number of rack-mounted analog effects unit devices to accomplish these tasks.\n\nEqualizers are electronic devices that allow audio engineers to control the tone and frequencies of the sound in a channel, group (e.g., all the mics on a drumkit) or an entire stage's mix. The bass and treble controls on a home stereo are a simple type of equalizer. Equalizers exist in pro sound reinforcement systems in three forms: shelving equalizers (typically for a whole range of bass and treble frequencies), graphic equalizers and parametric equalizers. Graphic equalizers have faders (vertical slide controls) which together resemble a frequency response curve plotted on a graph. The faders can be used to boost or cut specific frequency bands. \n\nUsing equalizers, frequencies which are too weak, such as a singer with modest projection in her/his lower register, can be boosted. Frequencies which are too loud, such as a \"boomy\" sounding bass drum, or an overly resonant dreadnought guitar can be cut. Sound reinforcement systems typically use graphic equalizers with one-third octave frequency centers. These are typically used to equalize output signals going to the main loudspeaker system or the monitor speakers on stage. Parametric equalizers are often built into each channel in mixing consoles, typically for the mid-range frequencies. They are also available as separate rackmount units which can be connected to a mixing board. Parametric equalizers typically use knobs and sometimes buttons. The audio engineer can select which frequency band to cut or boost, and then use additional knobs to cut or boost this frequency range. Parametric equalizers first became popular in the 1970s and have remained the program equalizer of choice for many engineers since then.\n\nA high-pass (low-cut) and/or low-pass (high-cut) filter may also be included on equalizers or audio consoles. High-pass (low-cut) and low-pass (high-cut) filters restrict a given channel's bandwidth extremes. Cutting very low frequency sound signals (termed infrasonic, or \"subsonic\", a misnomer) reduces the waste of amplifier power which does not produce audible sound and which moreover can be hard on the low-range speakers. A low-pass filter to cut ultrasonic energy is useful to prevent interference from radio frequencies, lighting control hum, or digital circuitry creeping into the power amplifiers. Such filters are often included with graphic and parametric equalizers to give the audio engineer full control of the frequency range. If their response is steep enough, high-pass filters and low-pass filters function as end-cut filters. A feedback suppressor is an automatically-adjusted band-reject or notch filter which includes a microprocessor to detect the onset of feedback \"howls\" and direct the filter to suppress the feedback by lowering the gain right at the offending frequency.\n\nCompressors are designed to help the audio engineer to manage the dynamic range (loudness) of an audio signal, or a group of audio signals. Prior to the invention of automatic compressors, audio engineers accomplished the same goal by \"riding the faders\", which mean listening carefully to the mix and lowering the faders of any singer or instrument which was getting too loud. A compressor accomplishes this by reducing the gain of a signal that is above a defined level (the threshold) by a defined amount (the ratio). Without this gain reduction, a signal that gets, say 10% louder as an input, will be 10% louder at the output. With the gain reduced, a signal that gets 10% louder at the input will be perhaps 3% louder at the output. Most compressors available are designed to allow the operator to select a ratio within a range typically between 1:1 and 20:1, with some allowing settings of up to ∞:1. A compressor with an infinite ratio is typically referred to as a \"limiter\". The speed that the compressor adjusts the gain of the signal (called the attack) is typically adjustable as is the final output of the device.\n\nCompressor applications vary widely from objective system design criterion to subjective applications determined by variances in program material and engineer preference. Some system design criteria specify limiters for component protection and gain structure control. Artistic signal manipulation is a subjective technique widely utilized by mix engineers to improve clarity or to creatively alter the signal in relation to the program material. An example of artistic compression is the typical heavy compression used on the various components of a modern rock drum kit. The drums are processed to be perceived as sounding more punchy and full.\nA noise gate sets a threshold where if it is quieter it will not let the signal pass and if it is louder it opens the gate. A noise gate's function is in a sense the opposite to that of a compressor. Noise gates are useful for microphones which will pick up noise which is not relevant to the program, such as the hum of a miked electric guitar amplifier or the rustling of papers on a minister's lectern. Noise gates are also used to process the microphones placed near the drums of a drum kit in many hard rock and metal bands. Without a noise gate, the microphone for a specific instrument such as the floor tom will also pick up signal from nearby drums or cymbals. With a noise gate, the threshold of sensitivity for each microphone on the drum kit can be set so that only the direct strike and subsequent decay of the drum will be heard, not the nearby sounds.\n\nReverberation and delay effects are widely used in sound reinforcement systems to enhance the sound of the mix and create a desired artistic effect. Reverb and delay add a sense of spaciousness to the sound, imitating the sound of a singing voice or instrument in a large, reverberant hall. Many mixing boards designed for live sound include on-board reverb effects. Modulation effects such as flanger, phaser, and chorus are also applied to some instruments. An exciter \"livens up\" the sound of audio signals by applying dynamic equalization, phase manipulation and harmonic synthesis of typically high frequency signals.\n\nThe appropriate type, variation, and level of effects is quite subjective and is often collectively determined by a production's audio engineer, artists, bandleader, music producer, or musical director. Reverb, for example, can give the effect of signal being present in anything from a small room to a massive hall, or even in a space that does not exist in the physical world. The use of reverb often goes unnoticed by the audience, as it often sounds more natural than if the signal was left \"dry\" (without effects). The use of effects in the reproduction of 2010-era pop music is often in an attempt to mimic the sound of the studio version of the artist's music in a live concert setting. For example, an audio engineer may use an Auto Tune effect to produce unusual vocal sound effects that a singer used on his/her recordings.\n\nA feedback suppressor detects unwanted audio feedback and suppresses it, typically by automatically inserting a notch filter into the signal path of the system, which prevents feedback \"howls\" from occurring. Audio feedback can create unwanted loud, screaming noises which are disruptive to the performance, and which can damage performers' and audience members' ears and speakers. Audio feedback from microphones occurs when a microphone \"hears\" the sound it is picking up through the monitor speakers or the main speakers. While microphone audio feedback is almost universally regarded as a negative phenomenon, in hard rock and heavy metal music, electric guitarists purposely create guitar feedback to create unique, sustained sounds with their guitar and guitar amplifier. This type of feedback is sought out by guitarists, so the sound engineer does not try to prevent it.\n\nA power amplifier is an electronic device which uses electrical power and circuitry to boost a low-voltage level signal (e.g., the signal from a vocalist's mic) and provides enough electrical power to drive a loudspeaker and produce sound. All speakers, including headphones, require power amplification. Most professional audio amplifiers also provide protection from clipped (overloaded) signals, as a power amplifier pushed into clipping can damage or destroy speakers. Amplifiers also typically provide protection against short circuits across the output, and excessive temperature (e.g., overheating). A limiter is often used to protect loudspeakers and amplifiers from power amp clipping. \n\nAudio engineers select amplifiers that provide enough headroom. \"Headroom\" refers to the amount by which the signal-handling capabilities of an audio system exceed a designated nominal level. Headroom can be thought of as a safety zone allowing transient audio peaks to exceed the nominal level without damaging the system or the audio signal, e.g., via clipping. Standards bodies differ in their recommendations for nominal level and headroom. When an audio engineer has selected an amplifier (or amplifiers) with enough headroom, this also helps to ensure that the signal will remain clean and undistorted.\n\nLike most sound reinforcement equipment products, professional amplifiers are typically designed to be mounted within standard 19-inch racks. Rack-mounted amps are typically housed in road cases, sturdy plastic protective boxes which prevent damage to the equipment during transportation. Active loudspeakers have internally mounted amplifiers that have been selected by the manufacturer to be a good amplifier for use with the given loudspeaker. Some active loudspeakers also have equalization, crossover and mixing circuitry built in.\n\nSince amplifiers can generate a significant amount of heat, thermal dissipation is an important factor for operators to consider when mounting amplifiers into equipment racks. Many power amplifiers feature internal fans to draw air across their heat sinks. The heat sinks can become clogged with dust, which can adversely affect the cooling capabilities of the amplifier.\n\nIn the 1970s and 1980s, most PAs employed heavy Class AB amplifiers. In the late 1990s, power amplifiers in PA applications became lighter, smaller, more powerful, and more efficient, with the increasing use of switching power supplies and Class D amplifiers, which offered significant weight- and space-savings as well as increased efficiency. Often installed in railroad stations, stadia, and airports, Class D amplifiers can run with minimal additional cooling and with higher rack densities, compared to older amplifiers.\n\nDigital loudspeaker management systems (DLMS) that combine digital crossover functions, compression, limiting, and other features in a single unit have become popular since their introduction. They are used to process the mix from the mixing console and route it to the various amplifiers. Systems may include several loudspeakers, each with its own output optimized for a specific range of frequencies (i.e. bass, midrange, and treble). Bi-amplifiication, tri-amplification, or quad-amplification of a sound reinforcement system with the aid of a DLMS results in a more efficient use of amplifier power by sending each amplifier only the frequencies appropriate for its respective loudspeaker. Most DLMS units that are designed for use by non-professionals have calibration and testing functions such as a pink noise generator coupled with a real-time analyzer to allow automated room equalization.\n\nA simple and inexpensive PA loudspeaker may have a single full-range loudspeaker driver, housed in a suitable enclosure. More elaborate, professional-caliber sound reinforcement loudspeakers may incorporate separate drivers to produce low, middle, and high frequency sounds. A crossover network routes the different frequencies to the appropriate drivers. In the 1960s, horn loaded theater loudspeakers and PA speakers were almost always \"columns\" of multiple drivers mounted in a vertical line within a tall enclosure. The 1970s to early 1980s was a period of innovation in loudspeaker design with many sound reinforcement companies designing their own speakers. The basic designs were based on commonly known designs and the speaker components were commercial speakers.\n\nThe areas of innovation were in cabinet design, durability, ease of packing and transport, and ease of setup. This period also saw the introduction of the hanging or \"flying\" of main loudspeakers at large concerts. During the 1980s the large speaker manufacturers started producing standard products using the innovations of the 1970s. These were mostly smaller two way systems with 12\", 15\" or double 15\" woofers and a high frequency driver attached to a high frequency horn. The 1980s also saw the start of loudspeaker companies focused on the sound reinforcement market. The 1990s saw the introduction of Line arrays, where long vertical arrays of loudspeakers with a smaller cabinet are used to increase efficiency and provide even dispersion and frequency response. This period also saw the introduction of inexpensive molded plastic speaker enclosures mounted on tripod stands. Many feature built-in power amplifiers which made them practical for non-professionals to set up and operate successfully. The sound quality available from these simple 'powered speakers' varies widely depending on the implementation.\n\nMany sound reinforcement loudspeaker systems incorporate protection circuitry, preventing damage from excessive power or operator error. Positive temperature coefficient resistors, specialized current-limiting light bulbs, and circuit-breakers were used alone or in combination to reduce driver failures. During the same period, the professional sound reinforcement industry made the Neutrik Speakon NL4 and NL8 connectors the standard input connectors, replacing 1/4\" jacks, XLR connectors, and Cannon multipin connectors which are all limited to a maximum of 15 amps of current. XLR connectors are still the standard input connector on active loudspeaker cabinets.\n\nThe three different types of transducers are subwoofers, compression drivers, and tweeters. They all feature the combination of a voicecoil, magnet, cone or diaphragm, and a frame or structure. Loudspeakers have a power rating (in watts) which indicates their maximum power capacity, to help users avoid overpowering them. Thanks to the efforts of the Audio Engineering Society (AES) and the loudspeaker industry group ALMA, power-handling specifications became more trustworthy, although adoption of the EIA-426-B standard is far from universal. Around the mid 1990s trapezoidal-shaped enclosures became popular as this shape allowed many of them to be easily arrayed together.\n\nA number of companies are now making lightweight, portable speaker systems for small venues that route the low-frequency parts of the music (electric bass, bass drum, etc.) to a powered subwoofer. Routing the low-frequency energy to a separate amplifier and subwoofer can substantially improve the bass-response of the system. Also, clarity may be enhanced, because low-frequency sounds take a great deal of power to amplify; with only a single amplifier for the entire sound spectrum, the power-hungry low-frequency sounds can take a disproportionate amount of the sound system's power.\n\nProfessional sound reinforcement speaker systems often include dedicated hardware for safely \"flying\" them above the stage area, to provide more even sound coverage and to maximize sight lines within performance venues.\n\nMonitor loudspeakers, also called \"foldback\" loudspeakers, are speaker cabinets which are used onstage to help performers to hear their singing or playing. As such, monitor speakers are pointed towards a performer or a section of the stage. They are generally sent a different mix of vocals or instruments than the mix that is sent to the main loudspeaker system. Monitor loudspeaker cabinets are often a wedge shape, directing their output upwards towards the performer when set on the floor of the stage. Two-way, dual driver designs with a speaker cone and a horn are common, as monitor loudspeakers need to be smaller to save space on the stage. These loudspeakers typically require less power and volume than the main loudspeaker system, as they only need to provide sound for a few people who are in relatively close proximity to the loudspeaker. Some manufacturers have designed loudspeakers for use either as a component of a small PA system or as a monitor loudspeaker. In the 2000s, a number of manufacturers produced powered monitor speakers, which contain an integrated amplifier.\n\nUsing monitor speakers instead of in ear monitors typically results in an increase of stage volume, which can lead to more feedback issues and progressive hearing damage for the performers in front of them. The clarity of the mix for the performer on stage is also typically not as clear as they hear more extraneous noise from around them. The use of monitor loudspeakers, active (with an integrated amplifier) or passive, requires more cabling and gear on stage, resulting in an even more cluttered stage. These factors, amongst others, have led to the increasing popularity of in-ear monitors.\n\nIn-ear monitors are headphones that have been designed for use as monitors by a live performer. They are either of a \"universal fit\" or \"custom fit\" design. The universal fit in ear monitors feature rubber or foam tips that can be inserted into virtually anybody's ear. Custom fit in ear monitors are created from an impression of the users ear that has been made by an audiologist. In-ear monitors are almost always used in conjunction with a wireless transmitting system, allowing the performer to freely move about the stage while maintaining their monitor mix.\n\nIn-ear monitors offer considerable isolation for the performer using them, meaning that the monitor engineer can craft a much more accurate and clear mix for the performer. With in-ear monitors, each performer can be sent their own customized mix; although this was also the case with monitor speakers, the in-ear monitors of one performer cannot be heard by the other musicians. A downside of this isolation is that the performer cannot hear the crowd or the comments other performers on stage that do not have microphones (e.g., if the bass player wishes to communicate to the drummer). This has been remedied by larger productions by setting up a pair of microphones on each side of the stage facing the audience that are mixed into the in-ear monitor sends.\n\nSince their introduction in the mid-1980s, in-ear monitors have grown to be the most popular monitoring choice for large touring acts. The reduction or elimination of loudspeakers other than instrument amplifiers on stage has allowed for cleaner and less problematic mixing situations for both the front of house and monitor engineers. Audio feedback is greatly reduced and there is less sound reflecting off the back wall of the stage out into the audience, which affects the clarity of the mix the front of house engineer is attempting to create.\n\nSound reinforcement systems are used in a broad range of different settings, each of which poses different challenges.\n\nAudio visual (AV) rental systems have to be able to withstand heavy use, and even abuse from renters. For this reason, rental companies tend to own speaker cabinets which are heavily braced and protected with steel corners, and electronic equipment such as power amplifiers or effects are often mounted into protective road cases. As well, rental companies tend to select gear which has electronic protection features, such as speaker-protection circuitry and amplifier limiters.\n\nAs well, rental systems for non-professionals need to be easy to use and set up, and they must be easy to repair and maintain for the renting company. From this perspective, speaker cabinets need to have easy-to-access horns, speakers, and crossover circuitry, so that repairs or replacements can be made. Some rental companies often rent powered amplifier-mixers, mixers with onboard effects, and powered subwoofers for use by non-professionals, which are easier to set up and use.\n\nMany touring acts and large venue corporate events will rent large sound reinforcement systems that typically include one or more audio engineers on staff with the renting company. In the case of rental systems for tours, there are typically several audio engineers and technicians from the rental company that tour with the band to set up and calibrate the equipment. The individual that mixes the band is often selected and provided by the band, as they have become familiar with the various aspects of the show and have worked with the act to establish a general idea of how they want the show to sound. The mixing engineer for an act sometimes also happens to be on staff with the rental company selected to provide the gear for the tour.\n\nSetting up sound reinforcement for live music clubs and dance events often poses unique challenges, because there is such a large variety of venues which are used as clubs, ranging from former warehouses or music theaters to small restaurants or basement pubs with concrete walls. Dance events may be held in huge warehouses, aircraft hangars or outdoor spaces. In some cases, clubs are housed in multi-story venues with balconies or in \"L\"-shaped rooms, which makes it hard to get a consistent sound for all audience members. The solution is to use fill-in speakers to obtain good coverage, using a delay to ensure that the audience does not hear the same sound at different times. \n\nThe number of subwoofer speaker cabinets and power amplifiers dedicated to low-frequency sounds used in a club depends on the type of club, the genres of music played there (live or via a DJ), and the size of the venue. A small coffeehouse where traditional folk, bluegrass or jazz groups are the main performers may have no subwoofers, and instead rely on the full-range main PA speakers to reproduce bass sounds. On the other hand, a club where hard rock or heavy metal music bands play or a nightclub where house music DJs play dance music may have multiple large 18\" subwoofers in big cabinets and powerful amplifiers dedicated for subwoofers, as these genres and music styles typically use powerful, deep bass sound.\n\nAnother challenge with designing sound systems for live music clubs is that the sound system may need to be used for both prerecorded music played by DJs and live music. If the sound system is optimized for prerecorded DJ music, then it will not provide the appropriate sound qualities (or mixing equipment and monitoring equipment) needed for live music, and vice versa. A club system designed for DJs needs a DJ mixer and space for record players. Clubs tend to focus on either live music or DJ shows. However, clubs which feature both types of shows may face challenges providing the desired equipment and set-up for both uses. In contrast, a live music club needs a mixing board designed for live sound, an onstage monitor system, and a multicore \"snake\" cable running from the stage to the mixer. Lastly, live music clubs can be a hostile environment for sound gear, in that the air may be hot, humid, and smoky; in some clubs, keeping racks of power amplifiers cool may be a challenge. Often an air conditioned room just for the amplifiers is utilised.\n\nDesigning systems in churches and similar religious facilities often poses a challenge, because the speakers may have to be unobtrusive to blend in with antique woodwork and stonework. In some cases, audio designers have designed custom-painted speaker cabinets so that the speakers will blend in with the church architecture. Some church facilities, such as sanctuaries or chapels are long rooms with low ceilings, which means that additional fill-in speakers are needed throughout the room to give good coverage. An additional challenge with church SR systems is that, once installed, they are often operated by amateur volunteers from the congregation, which means that they must be easy to operate and troubleshoot.\n\nSome mixing consoles designed for houses of worship have automatic mixers, which turn down unused channels to reduce noise, and automatic feedback elimination circuits which detect and notch out frequencies that are feeding back. These features may also be available in multi-function consoles used in convention facilities and multi-purpose venues.\n\nTouring sound systems have to be powerful and versatile enough to cover many different halls and venues, and they are available many different sizes and shapes. Touring systems range from mid-sized systems for bands playing nightclub and other mid-sized venues to large systems for groups playing stadiums, arenas and outdoor festivals. They also need to use \"field-replaceable\" components such as speakers, horns, and fuses, which are easily accessible for repairs during a tour. Tour sound systems are often designed with substantial redundancy features, so that in the event of equipment failure or amplifier overheating, the system will continue to function. Touring systems for bands performing for crowds of a few thousand people and up are typically set up and operated by a team of technicians and engineers who travel with the performers to every show.\n\nMainstream bands that are going to perform in mid- to large-sized venues during their tour schedule one to two weeks of technical rehearsal with the entire concert system and production staff, including audio engineers, at hand. This allows the audio and lighting engineers to become familiar with the show and establish presets on their digital equipment (e.g., digital mixers) for each part of the show, if needed. Many modern musical groups work with their Front of House and monitor mixing engineers during this time to establish what their general idea is of how the show and mix should sound, both for themselves on stage and for the audience. \n\nThis often involves programming different effects and signal processing for use on specific songs, to make the songs sound somewhat similar to the studio versions. To manage a show with a lot of effects changes, the mixing engineers for the show often choose to use a digital mixing console so that they can save and automatically recall these many settings in between each song. This time is also used by the system technicians to get familiar with the specific combination of gear that is going to be used on the tour and how it acoustically responds during the show. These technicians remain busy during the show, making sure the SR system is operating properly and that the system is tuned correctly, as the acoustic response of a room or venue will respond differently throughout the day depending on the temperature, humidity, and number of people in the room or space.\n\n\"Weekend band\" PA systems are a niche market for small, powerful touring SR gear. Weekend bands need systems that are small enough to fit into a minivan or a car trunk, and yet powerful enough to give adequate and even sound dispersion and vocal intelligibility in a noisy club or bar. As well, the systems need to be easy and quick to set up. Sound reinforcement companies have responded to this demand by offering equipment that fulfills multiple roles, such as powered mixers (a mixer with an integrated power amplifier and effects) and powered subwoofers (a subwoofer with an integrated power amplifier and crossover). These products minimize the number of wiring connections that bands have to make to set up the system and they take less time to set up. Some subwoofers have metal speaker mount holes built into the top, so that they can double as a base for the stand-mounted full-range PA speaker cabinets.\n\nSound for live theater, operatic theater, and other dramatic applications may pose problems similar to those of churches, in cases where a theater is an old heritage building where speakers and wiring may have to blend in with woodwork. The need for clear sight lines in some theaters may make the use of regular speaker cabinets unacceptable; instead, slim, low-profile speakers are often used instead.\n\nIn live theater and drama, performers move around onstage, which means that wireless microphones may have to be used. Wireless microphones need to be set up and maintained properly, to avoid interference and reception problems.\n\nSome of the higher budget theater shows and musicals are mixed in surround sound live, often with the show's sound operator triggering sound effects that are being mixed with music and dialogue by the show's mixing engineer. These systems are usually much more extensive to design, typically involving a separate sets of speakers for different zones in the theater.\n\nA subtle type of sound reinforcement called acoustic enhancement is used in some concert halls where classical music such as symphonies and opera is performed. Acoustic enhancement systems help give a more even sound in the hall and prevent \"dead spots\" in the audience seating area by \"...augment[ing] a hall's intrinsic acoustic characteristics.\" The systems use \"...an array of microphones connected to a computer [which is] connected to an array of loudspeakers.\" However, as concertgoers have become aware of the use of these systems, debates have arisen, because \"...purists maintain that the natural acoustic sound of [Classical] voices [or] instruments in a given hall should not be altered.\"\n\nKai Harada's article \"Opera's Dirty Little Secret\" states that opera houses have begun using electronic acoustic enhancement systems \"...to compensate for flaws in a venue's acoustical architecture.\" Despite the uproar that has arisen amongst operagoers, Harada points out that none of the opera houses using acoustic enhancement systems \"...use traditional, Broadway-style sound reinforcement, in which most if not all singers are equipped with radio microphones mixed to a series of unsightly loudspeakers scattered throughout the theatre.\" Instead, most opera houses use the sound reinforcement system for acoustic enhancement, and for subtle boosting of offstage voices, onstage dialogue, and sound effects (e.g., church bells in Tosca or thunder in Wagnerian operas).\n\nAcoustic enhancement systems include LARES (Lexicon Acoustic Reinforcement and Enhancement System) and SIAP, the System for Improved Acoustic Performance. These systems use microphones, computer processing \"with delay, phase, and frequency-response changes\", and then send the signal \"... to a large number of loudspeakers placed in extremities of the performance venue.\" Another acoustic enhancement system, VRAS (Variable Room Acoustics System) uses \"...different algorithms based on microphones placed around the room.\" The Deutsche Staatsoper in Berlin and the Hummingbird Centre in Toronto use a LARES system. The Ahmanson Theatre in Los Angeles, the Royal National Theatre in London, and the Vivian Beaumont Theater in New York City use the SIAP system.\n\nLecture halls and conference rooms pose the challenge of reproducing speech clearly to a large hall, which may have reflective, echo-producing surfaces. One issue with reproducing speech is that the microphone used to pick up the sound of an individual's voice may also pick up unwanted sounds, such as the rustling of papers on a podium. A more tightly directional microphone may help to reduce unwanted background noises. \n\nAnother challenge with doing live sound for individuals who are speaking at a conference is that, in comparison with professional singers, individuals who are invited to speak at a forum may not be familiar with how microphones work. Some individuals may accidentally point the microphone towards a speaker or monitor speaker, which may cause audio feedback \"howls\". In some cases, when an individual who is speaking does not speak enough directly into the microphone, the audio engineer may ask the individual to wear a lavaliere microphone, which can be clipped onto a lapel. \n\nIn some conferences, sound engineers have to provide microphones for a large number of people who are speaking, in the case of a panel conference or debate. In some cases, automatic mixers are used to control the levels of the microphones, and turn off the channels for microphones which are not being spoken into, to reduce unwanted background noise and reduce the likelihood of feedback.\n\nSystems for outdoor sports facilities and ice rinks often have to deal with substantial echo, which can make speech unintelligible. Sports and recreational sound systems often face environmental challenges as well, such as the need for weather-proof outdoor speakers in outdoor stadiums and humidity- and splash-resistant speakers in swimming pools. Another challenge with sports sound reinforcement setups is that in many arenas and stadiums, the spectators are on all four sides of the playing field. This requires 360 degree sound coverage. This is very different from the norm with music festivals and music halls, where the musicians are on stage and the audience is seated in front of the stage.\n\nLarge-scale sound reinforcement systems are designed, installed, and operated by audio engineers and audio technicians. During the design phase of a newly constructed venue, audio engineers work with architects and contractors, to ensure that the proposed design will accommodate the speakers and provide an appropriate space for sound technicians and the racks of audio equipment. Sound engineers will also provide advice on which audio components would best suit the space and its intended use, and on the correct placement and installation of these components. During the installation phase, sound engineers ensure that high-power electrical components are safely installed and connected and that ceiling or wall mounted speakers are properly mounted (or \"flown\") onto rigging. When the sound reinforcement components are installed, the sound engineers test and calibrate the system so that its sound production will be even across the frequency spectrum.\n\nA sound reinforcement system should be able to accurately reproduce a signal from its input, through any processing, to its output without any coloration or distortion. However, due to inconsistencies in venue sizes, shapes, building materials, and even crowd densities, this is not always possible without prior calibration of the system. This can be done in one of several ways.\nThe oldest method of system calibration involves a set of healthy ears, test program material (i.e. music or speech), a graphic equalizer, and last but certainly not least, a familiarity with the proper (or desired) frequency response. One must then listen to the program material through the system, take note of any noticeable frequency changes or resonances, and subtly correct them using the equalizer. Experienced engineers typically use a specific playlist of music that they have become very familiar with every time they calibrate a new system. This \"by ear\" process is still done by many engineers, even when analysis equipment is used, as a final check of how the system sounds with music or speech playing through the system.\n\nAnother method of manual calibration requires a pair of high-quality headphones patched into the input signal \"before\" any processing (such as the pre-fade-listen of the test program input channel of the mixing console, or the headphone output of the CD player or tape deck). One can then use this direct signal as a near-perfect reference with which to find any differences in frequency response. This method may not be perfect, but it can be very helpful with limited resources or time, such as using pre-show music to correct for the changes in response caused by the arrival of a crowd. Because this is still a very subjective method of calibration, and because the human ear is so dynamic in its own response, the program material used for testing should be as similar as possible to that for which the system is being used.\n\nSince the development of digital signal processing (DSP), there have been many pieces of equipment and computer software designed to shift the bulk of the work of system calibration from human auditory interpretation to software algorithms that run on microprocessors. One tool for calibrating a sound system using either DSP or Analog Signal Processing is a Real Time Analyzer (RTA). This tool is usually used by piping pink noise into the system and measuring the result with a special calibrated microphone connected to the RTA. Using this information, the system can be adjusted to help achieve the desired response. The displayed response from the RTA mic cannot be taken as a perfect representation of the room as the analysis will be different, sometimes drastically, when the mic is placed in different position in front of the system.\n\nMore recently, sound engineers have seen the introduction of dual \"fft\" (fast-fourier transform) based audio analysis software which allows an engineer to view not only frequency vs. amplitude (pitch vs. volume) information that an RTA provides, but also to see the same signals (sounds) in the time domain. This provides the engineer with much more meaningful data than an rta alone. Also, dual fft analysis allows one to compare the source signal with the output signal and view the difference. This is a very fast way to calibrate a system to sound as close as possible to the original source material. As with any such measurement tool, it must always be verified using actual human ears. Some DSP system processing devices have been designed for use by non-professionals that automatically make adjustments in the system EQ based upon what is being read from the RTA mic. These are practically never used by professionals, as they almost never calibrate the system as well as a professional audio engineer can manually.\n\nProfessional audio stores sell microphones, speaker enclosures, monitor speakers, mixing boards, rack-mounted effects units and related equipment designed for use by audio engineers and technicians. Professional audio stores are also called \"pro audio stores\", \"pro sound stores\", \"sound reinforcement\" companies, \"PA system companies\" or \"audio-visual companies\", with the latter name being used when a store supplies a significant amount of video equipment for events, such as video projectors and screens. Stores often use the word \"professional\" or \"pro\" in their name or the description of their store, to differentiate their stores from consumer electronics stores, which sell consumer-grade loudspeakers, home cinema equipment, and amplifiers, which are designed for private, in-home use.\n\n\n"}
{"id": "4733351", "url": "https://en.wikipedia.org/wiki?curid=4733351", "title": "Trading room", "text": "Trading room\n\nA trading room gathers traders operating on financial markets. The trading room is also often called the front office. The terms \"dealing room\" and \"trading floor\" are also used, the latter being inspired from that of an open outcry stock exchange. As open outcry is gradually replaced by electronic trading, the trading room gets the only living place that is emblematic of the financial market. It is also the likeliest place within the financial institution where the most recent technologies are implemented before being disseminated in its other businesses.\n\nTrading rooms are also known as \"trading labs\" or \"finance labs\" in universities and business schools. Trading rooms, have become a key medium in creating a \"wall street atmosphere\".\n\nBefore the sixties or seventies, the banks' capital market businesses were mostly split into many departments, sometimes scattered at several sites, as market segments: money market (domestic and currencies), foreign exchange, long-term financing, exchange, bond market. By gathering these teams to a single site, banks want to ease:\n\nTrading rooms first appeared among United States bulge bracket brokers, such as Morgan Stanley, from 1971, with the creation of NASDAQ, which requires an equity trading desk on their premises, and the growth of the secondary market of federal debt products, which requires a bond trading desk.\n\nThe spread of trading rooms in Europe, between 1982 and 1987, has been subsequently fostered by two reforms of the financial markets organization, that were carried out roughly simultaneously in the United Kingdom and France.\n\nIn the United Kingdom, the Big Bang on the London Stock Exchange, removed the distinction between stockbrokers and stockjobbers, and prompted US investment banks, hitherto deprived of access to the LSE, to set up a trading room in the City of London.\n\nIn France, the deregulation of capital markets, carried out by Pierre Bérégovoy, Economics and Finance Minister, between 1984 and 1986, led to the creation of money-market instruments, of an interest-rate futures market, MATIF, of an equity options market, MONEP, the streamlining of sovereign debt management, with multiple-auction bond issues and the creation of a primary dealer status. Every emerging market segment raised the need for new dedicated trader positions inside the trading room.\n\nA trading room serves two types of business:\nBrokers and investment banks set up their trading rooms first and large asset-management firms subsequently followed them.\n\nThe business type determines peculiarities in the organisation and the software environment inside the trading room.\n\nTrading rooms are made up of \"desks\", specialised by product or market segment (equities, short-term, long-term, options...), that share a large open space.\n\nAn investment bank's typical room makes a distinction between:\n\nSales make deals tailored to their corporate customers' needs, that is, their terms are often specific. Focusing on their customer relationship, they may deal on the whole range of asset types.\nMany large institutions have grouped their cash and derivative desks, while others, such as UBS or Deutsche Bank, for example, giving the priority to customer relationship, structure their trading room as per customer segment, around sales desks.\n\nSome large trading rooms hosts offshore traders, acting on behalf of another entity of the same institution, located in another time-zone. One room in Paris may have traders paid for by the New York City subsidiary, and whose working hours are consequently shifted. On the foreign exchange desk, because this market is live on a 24/24 basis, a rolling book organisation can be implemented, whereby, a London-based trader, for instance, will inherit, at start of day, the open positions handed over by the Singapore, Tokyo, or Bahrein room, and manages them till his own end-of-day, when they are handed over to another colleague based in New York City.\n\nSome institutions, notably those that invested in a \"rapid development\" (RAD) team, choose to blend profiles inside the trading room, where traders, financial engineers and front-office dedicated software developers sit side by side. The latter therefore report to a head of trading rather than to a head of IT.\n\nMore recently, a profile of compliance officer has also appeared; he or she makes sure the law, notably that relative to market use, and the code of conduct, are complied with.\n\nThe middle office and the back office are generally not located in the trading room.\n\nThe organisation is somewhat simpler with asset management firms:\n\nThe development of trading businesses, during the eighties and nineties, required ever larger trading rooms, specifically adapted to IT- and telephony cabling. Some institutions therefore moved their trading room from their downtown premises, from the City to Canary Wharf, from inner Paris to La Défense, and from Wall Street towards Times Square or New York City's residential suburbs in Connecticut; UBS Warburg, for example, built a trading room in Stamford, Connecticut in 1997, then enlarged it in 2002, to the world's largest one, with about floor space, allowing the installation of some working positions and monitors. \nThe \"Basalte\" building of Société Générale is the first ever building specifically dedicated to trading rooms; it is fit for double power sourcing, to allow trading continuity in case one of the production sources is cut off.\nJP Morgan is planning to construct a building, close to the World Trade Center site, where all six floors dedicated to trading rooms will be cantilevered, the available ground surface being only .\n\nTelephone and teleprinter have been the broker's first main tools. The teleprinter, or Teletype, got financial quotes and printed them out on a ticker tape. US equities were identified by a ticker symbol made of one to three letters, followed by the last price, the lowest and the highest, as well as the volume of the day. Broadcasting neared real time, quotes being rarely delayed by more than 15 minutes, but the broker looking for a given security's price had to read the tape...\n\nAs early as 1923, the Trans-Lux company installed the NYSE with a projection system of a transparent ticker tape onto a large screen. This system has been subsequently adopted by most NYSE-affiliated brokers till the 1960s.\n\nIn 1956 a solution called Teleregister, came to the market; this electro-mechanical board existed in two versions, of the top 50 or top 200 securities listed on the NYSE; but one had to be interested in those equities, and not in other ones...\n\nDuring the 1960s, the trader's workstation was remarkable for the overcrowding of telephones. The trader juggled with handsets to discuss with several brokers simultaneously. The electromechanical, then electronic, calculator enabled him or her to perform basic computations.\n\nIn the 1970s, if the emergence of the PABX gave way to some simplification of the telephony equipment, the development of alternative display solutions, however, lead to a multiplication of the number of video monitors on their desks, pieces of hardware that were specific and proprietary to their respective financial data provider.\nThe main actors of the financial data market were; Telerate, Reuters, Bloomberg with its Bloomberg Terminal, Knight Ridder notably with its Viewtron offering, Quotron and Bridge, more or less specialised on the money market, foreign exchange, securities market segments, respectively, for the first three of them.\n\nFrom the early 1980s trading rooms multiplied and took advantage of the spread of micro-computing. Spreadsheets emerged, the products on offer being split between the MS-DOS/Windows/PC world and the Unix world. For PC, there was Lotus 1-2-3, it was quickly superseded by Excel, for workstations and terminals. For UNIX, there was Applix and Wingz among others. Along video monitors, left space had to be found on desks to install a computer screen.\n\nQuite rapidly, Excel got very popular among traders, as much as a decision support tool as a means to manage their position, and proved to be a strong factor for the choice of a Windows NT platform at the expense of a Unix or VAX/VMS platform.\n\nThough software alternatives multiplied during this decade, the trading room was suffering from a lack of interoperability and integration. To begin with, there was scant automated transmission of trades from the front-office desktop tools, notably Excel, towards the enterprise application software that gradually got introduced in back-offices; traders recorded their deals by filling in a form printed in a different colour depending on the direction (buy/sell or loan/borrow), and a back-office clerk came and picked piles of tickets at regular intervals, so that these could be re-captured in another system.\n\nVideo display applications were not only wrapped up in cumbersome boxes, their retrieval-based display mode was no longer adapted to markets that had been gaining much liquidity and henceforth required decisions in a couple of seconds. Traders expected market data to reach them in real time, with no intervention required from them with the keyboard or the mouse, and seamlessly feed their decision support and position handling tools.\n\nThe digital revolution, which started in the late 1980s, was the catalyst that helped meet these expectations. It found expression, inside the dealing room, in the installation of a digital data display system, a kind of local network. Incoming flows converged from different data providers, and these syndicated data were distributed onto traders' desktops. One calls a \"feed-handler\" the server that acquires data from the integrator and transmits them to the local distribution system.\n\nReuters, with its TRIARCH 2000, Teknekron, with its TIB, Telerate with TTRS, Micrognosis with MIPS, soon shared this growing market. This infrastructure is a prerequisite to the further installation, on each desktop, of the software that acquires, displays and graphically analyses these data.\n\nThis type of software usually enables the trader to assemble the relevant information into composite pages, comprising a news panel, in text format, sliding in real time from bottom to top, a quotes panel, for instance spot rates against the US dollar, every quote update or « tick » showing up in reverse video during one or two seconds, a graphical analysis panel, with moving averages, MACD, candlesticks or other technical indicators, another panel that displays competitive quotes from different brokers, etc...\n\nTwo software package families were belonging to this new generation of tools, one dedicated to Windows-NT platforms, the other to Unix and VMS platforms.\n\nHowever Bloomberg and other, mostly domestic, providers, shunned this movement, preferring to stick to a service bureau model, where every desktop-based monitor just displays data that are stored and processed on the vendor's premises. The approach of these providers was to enrich their database and functionalities enough so that the issue of opening up their datafeed to any spreadsheet or third-party system gets pointless.\n\nThis decade also witnessed the irruption of television inside trading rooms. Press conferences held by central bank presidents are henceforth eagerly awaited events, where tone and gestures are decrypted. The trader has one eye on a TV set, the other on a computer screen, to watch how markets react to declarations, while having, very often, one customer over the phone. Reuters, Bloomberg, CNN, CNBC each propose their news channel specially dedicated to financial markets.\n\nThe development of the internet triggered the fall of the cost of information, including financial information. It hit a serious blow to integrators who, like Reuters, had invested a lot the years before to deliver data en masse and in real time to the markets, but henceforth recorded a wave of terminations of their data subscriptions as well as flagging sales of their data distribution and display software licences.\n\nMoreover, the cable operators' investors lead to a huge growth of information capacity transport worldwide. Institutions with several trading rooms in the world took advantage of this bandwidth to link their foreign sites to their headquarters in a hub and spoke model. The emergence of technologies like Citrix supported this evolution, since they enable remote users to connect to a virtual desktop from where they then access headquarters applications with a level of comfort similar to that of a local user. While an investment bank previously had to roll out a software in every trading room, it can now limit such an investment to a single site. The implementation cost of an overseas site gets reduced, mostly, to the telecoms budget.\n\nAnd since the IT architecture gets simplified and centralised, it can also be outsourced. Indeed, from the last few years, the main technology providers active on the trading rooms market have been developing hosting services.\n\nFrom the late 1980s, worksheets have been rapidly proliferating on traders' desktops while the head of the trading room still had to rely on consolidated positions that lacked both real time and accuracy. The diversity of valuation algorithms, the fragility of worksheets incurring the risk of loss of critical data, the mediocre response times delivered by PCs when running heavy calculations, the lack of visibility of the traders' goings-on, have all raised the need for shared information technology, or enterprise applications as the industry later called it.\n\nBut institutions have other requirements that depend on their business, whether it is trading or investment.\n\nWithin the investment bank, the trading division is keen to implement synergies between desks, such as:\nSuch processes require mutualisation of data.\n\nHence a number of package software come to the market, between 1990 and 1993 : Infinity, Summit, Kondor+, Finance Kit, Front Arena, Murex and Sophis Risque, are quickly marketed under the umbrella of risk-management, a term more flattering though somewhat less accurate than that of position-keeping.\n\nThough Infinity died, in 1996, with the dream of the toolkit that was expected to model any innovation a financial engineer could have designed, the other systems are still well and alive in trading rooms. Born during the same period, they share many technical features, such as a three-tier architecture, whose back-end runs on a Unix platform, a relational database on either Sybase or Oracle, and a graphical user interface written in English, since their clients are anywhere in the world. Deal capture of transactions by traders, position-keeping, measure of market risks (interest-rates and foreign exchange), calculation of Profit & Loss (P&L), per desk or trader, control of limits set per counterparty, are the main functionalities delivered by these systems.\n\nThese functions will be later entrenched by national regulations, that tend to insist on adequate IT: in France, they are defined in 1997 in an instruction from the “Commission Bancaire” relative to internal control.\n\nTelephone, used on over-the-counter (OTC) markets, is prone to misunderstandings. Should the two parties fail to clearly understand each other on the trade terms, it may be too late to amend the transaction once the received confirmation reveals an anomaly.\n\nThe first markets to discover electronic trading are the foreign-exchange markets. Reuters creates its Reuter Monitor Dealing Service in 1981. Contreparties meet each other by the means of the screen and agree on a transaction in videotex mode, where data are loosely structured. Its next generation product, an electronic trading platform called Dealing 2000, ported on Windows, is launched in 1989. Like EBS, which competes with it head-on from 1997, it mostly handles spot trades.\n\nSeveral products pop up in the world of electronic trading including Bloomberg Terminal, BrokerTec, TradeWeb and Reuters 3000 Xtra for securities and foreign exchange. While the Italian-born Telematico (MTS) finds its place, in the European trading rooms for trading of sovereign-debt.\n\nMore recently other specialised products have come to the market, such as Swapswire, to deal interest-rate swaps, or SecFinex and EquiLend, to place securities loans or borrowings (the borrower pays the subscription fee to the service).\n\nHowever, these systems also generally lack liquidity. Contrarily to an oft-repeated prediction, electronic trading did not kill traditional inter-dealer brokerage. Besides, traders prefer to mix both modes: screen for price discovery, and voice to arrange large transactions.\n\nFor organised markets products, processes are different: customer orders must be collected and centralised; some part of them can be diverted for internal matching, through so-called alternative trading systems (ATS); orders with a large size, or on equities with poor liquidity or listed on a foreign bourse, and orders from corporate customers, whose sales contact is located in the trading room, are preferably routed either towards brokers, or to multilateral trading facilities (MTF); the rest goes directly to the local stock exchange, where the institution is electronically connected to.\n\nOrders are subsequently executed, partially of fully, then allocated to the respective customer accounts. The increasing number of listed products and trading venues have made it necessary to manage this order book with an adequate software.\n\nStock exchanges and futures markets propose their own front-end system to capture and transmit orders, or possibly a programming interface, to allow member institutions to connect their order management system they developed in-house. But software publishers soon sell packages that take in charge the different communication protocols to these markets; The UK-based Fidessa has a strong presence among LSE members; Sungard Global Trading and the Swedish Orc Software are its biggest competitors.\n\nIn program trading, orders are generated by a software program instead of being placed by a trader taking a decision. More recently, it is rather called algorithmic trading. It applies only to organised markets, where transactions do not depend on a negotiation with a given counterparty.\n\nA typical usage of program trading is to generate buy or sell orders on a given stock as soon as its price reaches a given threshold, upwards or downwards. A wave of stop sell orders has been largely incriminated, during the 1987 financial crises, as the main cause of acceleration of the fall in prices. However, program trading has not stopped developing, since then, particularly with the boom of ETFs, mutual funds mimicking a stock-exchange index, and with the growth of structured asset management; an ETF replicating the FTSE 100 index, for instance, sends multiples of 100 buy orders, or of as many sell orders, every day, depending on whether the fund records a net incoming or outgoing subscription flow. Such a combination of orders is also called a basket. Moreover, whenever the weight of any constituent stock in the index changes, for example following an equity capital increase, by the issuer, new basket orders should be generated so that the new portfolio distribution still reflects that of the index. If a program can generate more rapidly than a single trader a huge quantity of orders, it also requires monitoring by a financial engineer, who adapts its program both to the evolution of the market and, now, to requirements of the banking regulator checking that it entails no market manipulation. Some trading rooms may now have as many financial engineers as traders.\n\nThe spread of program trading variants, many of which apply similar techniques, leads their designers to seek a competitive advantage by investing in hardware that adds computing capacity or by adapting their software code to multi-threading, so as to ensure their orders reach the central order book before their competitors'. The success of an algorithm therefore measures up to a couple of milliseconds.\nThis type of program trading, also called \"high-frequency trading\", conflicts however with the fairness principle between investors, and some regulators consider forbidding it\n\nWith order executions coming back, the mutual fund's manager as well the investment bank's trader must update their positions. However, the manager does not need to revalue his in real time: as opposed to the trader whose time horizon is the day, the portfolio manager has a medium to long term perspective. Still, the manager needs to check that whatever he sells is available on his custodial account; he also needs a benchmarking functionality, whereby he may track his portfolio performance with that of his benchmark; should it diverge by too much, he would need a mechanism to rebalance it by generating automatically a number of buys and sells so that the portfolio distribution gets back to the benchmark's.\n\nIn most countries the banking regulation requires a principle of independence between front-office and back-office: a deal made by the trading room must be validated by the back-office to be subsequently confirmed to the counterparty, to be settled, and accounted for. Both services must report to divisions that are independent from each at the highest possible level in the hierarchy.\n\nIn Germany, the regulation goes further, a \"four eyes' principle\" requiring that every negotiation carried by any trader should be seen by another trader before being submitted to the back-office.\n\nIn Continental Europe, institutions have been stressing, since the early 1990s, on Straight Through Processing (STP), that is, automation of trade transmission to the back-office. Their aim is to raise productivity of back-office staff, by replacing trade re-capture by a validation process. Publishers of risk-management or asset-management software meet this expectation either by adding back-office functionalities within their system, hitherto dedicated to the front-office, or by developing their connectivity, to ease integration of trades into a proper back-office-oriented package.\n\nAnglo-Saxon institutions, with fewer constraints in hiring additional staff in back-offices, have a less pressing need to automate and develop such interfaces only a few years later.\n\nOn securities markets, institutional reforms, aiming at reducing the settlement lag from a typical 3 business days, to one day or even zero day, can be a strong driver to automate data processes.\n\nAs long as front-office and back-offices run separately, traders most reluctant to capture their deals by themselves in the front-office system, which they naturally find more cumbersome than a spreadsheet, are tempted to discard themselves towards an assistant or a middle-office clerk. An STP policy is then an indirect means to compel traders to capture on their own. Moreover, IT-based trade-capture, in the shortest time from actual negotiation, is growingly seen, over the years, as a \"best practice\" or even a rule.\n\nBanking regulation tends to deprive traders from the power to revalue their positions with prices of their choosing. However, the back-office staff is not necessarily best prepared to criticize the prices proposed by traders for complex or hardly liquid instruments and that no independent source, such as Bloomberg, publicize.\n\nWhether as an actor or as a simple witness, the trading room is the place that experiences any failure serious enough to put the company's existence at stake.\n\nIn the case of Northern Rock, Bear Stearns or Lehman Brothers, all three wiped out by the subprime crisis, in 2008, if the trading room finally could not find counterparts on the money market to refinance itself, and therefore had to face a liquidity crisis, each of those defaults is due to the company's business model, not to a dysfunction of its trading room.\n\nOn the contrary, in the examples shown below, if the failure has always been precipitated by market adverse conditions, it also has an operational cause :\n\nThese operational causes, in the above columns, are due to organisational or IT flaws :\n\n\nTrading rooms are also used in the sports gambling sector. The term is often used to refer to the liabilities and odds setting departments of bookmakers where liabilities are managed and odds are adjusted. Examples include internet bookmakers based in the Caribbean and also legal bookmaking operations in the United Kingdom such as William Hill, Ladbrokes and Coral which operate trading rooms to manage their risk. The growth of betting exchanges such as Betfair has also led to the emergence of \"trading rooms\" designed for professional gamblers. (reference: Racing Post newspaper 19/7/07) The first such establishment was opened in Edinburgh in 2003 but later folded. Professional gamblers typically pay a daily \"seat\" fee of around £30 per day for the use of IT facilities and sports satellite feeds used for betting purposes. Today there are eight such trading rooms across the UK, with two based in London – one in Highgate and one in Canary Wharf.\n\n"}
{"id": "34350145", "url": "https://en.wikipedia.org/wiki?curid=34350145", "title": "Triggertrap", "text": "Triggertrap\n\nTriggertrap was a company that created several different hardware and software products centred on triggering SLR cameras.\n\nProducts included several Arduino-based camera triggers, along with Android and iOS apps which interface with cameras using a device that plugs into the headphone socket of the smartphone or tablet.\n\nTriggertrap ceased trading on the 31st of January 2017\n\nThe story of Triggertrap started in July 2011, when Haje Jan Kamps started a Kickstarter campaign aiming to raise support for a new type of camera trigger. The project asked for $25,000, but within a month nearly 900 supporters had pledged more than $77,000 in exchange for more than 950 Triggertrap v1 products - which is nearly three times more than what they wanted for the project.\n\nThe Triggertrap v1 is a programmable trigger based on Arduino open-source architecture, and the source-code for the product is downloadable from GitHub. It has a built-in ambient light sensor, laser sensor, and sound sensor. In addition, it has an auxiliary port, which enables Triggertrap v1 to trigger a camera based on anything that generates an electric signal.\n\nThe Triggertrap v1 is classed as a high-speed device, able to use the ambient light sensor to respond and fire the external flash such that it would correctly sync at shutter speeds down to 1/640th of a second- that’s a response time of less than 1.6 milliseconds.\n\nBy using the Laser sensor, users can use a laser pointer or laser module (both visible and infra-red lasers are supported), pointing it at the laser sensor of the device. The user can then choose to trigger \"on break\" (when the laser beam is broken), \"on make\" (when the laser beam hits the sensor), or both.\n\nThe device has a built-in sensor (a microphone with simple amplification circuitry) that can trigger the camera whenever Triggertrap v1 senses sound over a user-adjustable threshold.\n\nTriggertrap v1 also has an auxiliary port, to which the user can connect anything as long as the user can find a way to create an electric signal. This can be used to connect the Triggertrap v1 to a number of other devices. Among other things, the product's users have used the Aux sensor to take photos as part of theme-park rides and Halloween haunted house-type projects.\n\nThe device can control hundreds of cameras via infra-red or wired trigger system (depending upon cameras capability). \"Part of the excitement about the Triggertrap\", says its inventor, Haje Jan Kamps, \"is that I have no idea what people are going to use it for. It's an incredibly versatile piece of photography kit, and because it is so easy to hack, I'm expecting creativity to go off the charts.\"\n\nThe open-source nature of the Triggertrap v1 has attracted users to create a series of hacks and amendments to the original design, including 3D printed casings, suction mounts, tripod mounts, multi-camera adaptations, and others.\n\nIn addition to the Triggertrap v1, the Triggertrap company markets a Triggertrap Shield for Arduino. This is a feature-compatible version of the Triggertrap v1. After a user-configurable change in the config files, it runs on the same source code. The Shield for Arduino is cheaper, and aimed more at the hacking crowd.\n\nIn May 2012, Triggertrap introduced Triggertrap Mobile for iOS, followed by a version for Android in September 2012.\n\nTriggertrap Mobile utilises the sensors and processing power of a smartphone or tablet running IOS to trigger cameras based on sound, motion, vibration, or location, in addition to timelapse, bulb ramping, and other features. The Android app has more limited functionality, to some users dismay.\n\nThe apps connect to a camera by connecting a Triggertrap Mobile Dongle to the headphone socket of the smartphone or tablet. The app uses a coded audio signal, which the Mobile Dongle then translates into a signal the camera can use to trigger. The company currently sells nine different connection cables, covering more than 300 different camera models.\n\nIn November 2013, the \"Triggertrap Ada\" was successfully crowd-funded via Kickstarter, raising £290,386 though the original goal was £50,000. On 2 March 2015, Triggertrap announced that they had failed to bring the product into production and that the remaining 20% of the funds from the Kickstarter campaign was going to be returned to the original backers.\n\nTriggertrap and CEO Haje Jan Kamps received criticism from backers of the failed Triggertrap Ada project. Some backers have questioned the accuracy of the updates during the course of the campaign. Less than three months before the project was cancelled, Triggertrap announced that the shipping date would be May 2015, exactly 12 months after the original shipping estimate. Others have questioned the allocation of funds by Triggertrap and their interpretation of the Terms of Service that Triggertrap entered into when the project was created. Some have threatened legal action.\n\nThe Triggertrap products have been well received in the media, with an App of the Day accolade from Gizmodo, an Editor's Choice reward from Trusted Reviews, a 9/10 rating from ePhotozine, a 'Recommended' rating from Forbes, and many others.\n\nTriggertrap Products have clocked up a large number of positive reviews, including on Camera Rec, Trusted Reviews, ePhotozine, Imaging Resource, Forbes\n\nTriggertrap users maintain an active community pool on the website, Flickr. Contributors post a wide range of images taken with the assistance of a Triggertrap V1 or Trggertrap Mobile device.\n\nTheir latest Timelapse Pro application for iOS devices has been less well received, largely due to the perceived high cost and limited functionality. Triggertrap owners have voiced concerns about the app being IOS only and have questioned the use cases provided by Triggertrap that would involve the monopolization of a mobile device for several days. Others have argued that the new features should have been included in the existing app.\n\n"}
{"id": "26883627", "url": "https://en.wikipedia.org/wiki?curid=26883627", "title": "Via fence", "text": "Via fence\n\nA via fence, also called a picket fence, is a structure used in planar electronic circuit technologies to improve isolation between components which would otherwise be coupled by electromagnetic fields. It consists of a row of via holes which, if spaced close enough together, form a barrier to electromagnetic wave propagation of slab modes in the substrate. Additionally if radiation in the air above the board is also to be suppressed, then a strip pad with via fence allows a shielding can to be electrically attached to the top side, but electrically behave as if it continued through the PCB.\n\nModern electronics have components and sub-units at high densities to achieve small size. Typically, many functions are integrated on to the same board or die. If not properly shielded from each other, many problems can result including poor frequency response, noise performance, and distortion.\n\nVia fences are used to shield microstrip and stripline transmission lines, guard edges of printed circuit boards, shield functional circuit units from each other, and to form the walls of waveguides integrated into a planar format. Via fences are cheap and easy to implement, but use up board space and are not as effective as solid metal walls.\n\nPlanar technologies are used at microwave frequencies and make use of printed circuit tracks as transmission lines. As well as interconnections, these lines can be used to form components of functional units such as filters and couplers. Planar lines readily couple to each other when in close proximity, an effect called parasitic coupling. The coupling is due to fringing fields spreading from the edges of the line and intersecting adjacent lines or components. This is a desirable feature within the unit where it is made use of as part of the design. It is not desirable, however, that the fields couple to adjacent units. Modern electronic devices are usually required to be small. That, and the drive to keep down costs, leads to a high degree of integration and circuit units in less than desirable proximity. Via fences are one method that can be used to reduce parasitic coupling between such units.\n\nAmongst the many problems that can be caused by parasitic coupling are reducing bandwidth, degrading passband flatness, reducing amplifier output power, increasing reflections, worsening noise figure, causing amplifier instability, and providing undesirable feedback paths.\n\nIn stripline, via fences running parallel to the line on either side serve to tie together the groundplanes, so preventing the propagation of parallel-plate modes. A similar arrangement is used to suppress unwanted modes in metal-backed coplanar waveguide.\n\nA via fence consists of a row of via holes, that is, holes that pass through the substrate and are metallised on the inside to connect to pads on the top and bottom of the substrate. In a stripline format both the top and bottom of the dielectric sheet are covered with a metal ground plane so any via holes are automatically grounded at both ends. In other planar formats such as microstrip there is a ground plane only at the bottom of the substrate. In these formats it is the usual practice to connect the top pads of the via fence with a metal track (see figure 2). This still does not completely fence off the field as can be done in stripline. In stripline the field can only propagate between the ground planes, but in microstrip it is able to leak over the top of the via fence. Nevertheless, connecting the top pads improves isolation by . In some technologies it is more convenient to form the fence from conducting posts rather than vias.\n\nIsolation can be further improved by placing a metal wall on top of the via fence. These walls commonly form part of the device enclosure. The large holes in the via fences seen in figures 1 and 5 are screw holes for clamping these walls in place. The wall casting belonging to this circuit is shown in figure 3.\n\nThe design of the fence needs to consider the size and spacing of the vias. Ideally, vias should act as short circuits, but they are not ideal and a via equivalent circuit can be modelled as a shunt inductance. Sometimes, a more complex model is required such as the equivalent circuit shown in figure 4. \"L\" is due to the inductance of the pads and \"C\" is the capacitance between them. \"R\" and \"L\" are, respectively, the resistance and inductance of the via hole metallisation. Resonances must be considered, in particular the parallel resonance of \"C\" and \"L\" will allow electromagnetic waves to pass at the resonant frequency. This resonance needs to be placed outside the operating frequencies of the equipment concerned. Spacing of the fences needs to be small in comparison to a wavelength (λ) in the substrate dielectric so as to make the fence appear solid to impinging waves. If too large, waves will be able to pass through the gaps. A common rule of thumb is to make the spacing less than λ/20 at the maximum operating frequency.\n\nVia fences are used primarily at RF and microwave frequencies wherever planar formats are being applied. They are used in printed circuit technologies such as microstrip, ceramic technologies such as low temperature co-fired ceramic, monolithic microwave integrated circuits, and system-on-a-package technology. They are especially important in isolating circuit units operating at different frequencies.\n\nAlso called via stitching, via fences can be used around the edge of a printed circuit board, an example can be seen in figure 5. This may be done to prevent electromagnetic interference with other equipment, or even to block radiation re-entering from elsewhere on the same circuit.\n\nVia fences are also used in post-wall waveguide, also known as laminated waveguide (LWG). In LWG, two parallel via fences form the sidewalls of a waveguide. Between them, and the upper and lower groundplanes of the substrate, is an electromagnetically isolated space. There is no electrical conductor within this space, but electromagnetic waves can exist within the enclosed dielectric material of the substrate and their direction of propagation is guided by the LWG. This technology is typically used at millimetre band frequencies and consequently dimensions are quite small. Furthermore, good isolation requires that the vias are closely spaced. Typically, isolation is required between guides, that is per fence. A typical W band () fence specification meeting this requirement in LWG is vias spaced between centres. This can be challenging to manufacture, and a higher density of vias is sometimes achieved by constructing the fence from two staggered rows of vias.\n\nVia fences are cheap and convenient. When used on planar formats they require no additional processes to manufacture. On a printed circuit for instance, they are made in the same process that creates the track patterns. However, via fences are not able to approach the isolation achievable with unbroken metal walls.\n\nVia fences use up a lot of valuable substrate real estate and so will increase the overall size of the assembly. Via fences too close to the line being guarded can degrade the isolation otherwise achievable. In stripline, a rule of thumb is to place the fences at least four times the trace to groundplane distance away from the line being guarded.\n\n"}
{"id": "20368922", "url": "https://en.wikipedia.org/wiki?curid=20368922", "title": "Vivek Kundra", "text": "Vivek Kundra\n\nVivek Kundra (born October 9, 1974) is an American administrator who served as the first chief information officer of the United States from March, 2009 to August, 2011 under President Barack Obama. He is currently the Chief Operating Officer at Sprinklr and a visiting Fellow at Harvard University.\n\nHe previously served in D.C. Mayor Adrian Fenty's cabinet as the District's Chief Technology Officer and in Virginia Governor Tim Kaine's cabinet as Assistant Secretary of Commerce and Technology.\n\nKundra was born in New Delhi, India, on October 9, 1974. He moved to Tanzania with his family at the age of one, when his father joined a group of professors and teachers to provide education to local residents. Kundra learned Swahili as his first language, in addition to Hindi and English. His family moved to the Washington, D.C. metropolitan area when he was eleven.\n\nKundra attended college at the University of Maryland, College Park where he received a degree in Psychology. He earned a master's degree in Information Technology, from University of Maryland University College. Additionally, he is a graduate of the University of Virginia's Sorensen Institute for Political Leadership.\n\nKundra served as director of Infrastructure Technology for Arlington County, Virginia, starting September 11, 2001.\n\nGovernor Tim Kaine appointed Kundra in January 2006 to the post of Assistant Secretary of Commerce and Technology for Virginia, the first dual cabinet role in the state's history.\n\nMayor Adrian Fenty appointed him on March 27, 2007 to the cabinet post of Chief Technology Officer (CTO) for the District of Columbia. Kundra worked on developing programs to spur open source and crowdsourced applications using publicly accessible Web services from the District of Columbia. Building on the work of Suzanne Peck, who preceded him as DC's CTO and created the D.C. Data Catalog, he used that data as the source material for an initiative called Apps for Democracy. The contest yielded 47 web, iPhone and Facebook applications from residents in 30 days. Mayor Fenty stated that the program cost the District \"50 thousand dollars total and we estimate that we will save the district millions of dollars in program development costs\". This cost-benefit was claimed by the D.C. government as savings in internal operational and contractual costs. Taking a page from Kundra this initiative was mirrored by New York City's mayor Michael Bloomberg in launching a \"BigApps\" contest housed at NYC BigApps as well as New York City's DataMine. The city of San Francisco launched a data portal similar DC's in 2009.\n\nKundra won recognition for the project management system he implemented for the District government. The system imagined projects as publicly traded companies, project schedules as quarterly reports, and user satisfaction as stock prices. Buying or selling a stock corresponded to adding resources to a project or taking them away. The goal of management was to optimize the project portfolio for return on investment. The system effectively replaced subjective judgments about projects with objective, data driven analytics.\n\nKundra's efforts to use cloud-based web applications in the D.C. government have also been considered innovative. Following the D.C. example driven by Kundra, the city of Los Angeles is now taking steps to adopt the cloud computing model for its IT needs. A D.C. spokeswoman said that the District of Columbia paid $479,560 for the Enterprise Google Apps license, which is $3.5 million less than what it had planned to spend on an alternative plan. Since its deployment in July 2008 Google Apps is available to 38,000 D.C. city employees, but only 1,000–2,000 are actively using Google Docs. Only 200 employees are actively using Gmail. In late 2010, hoping to spur use of Gmail, the city ran a pilot program, selecting about 300 users and having them use the Google product for three months. Google participated closely in the project, but Gmail ultimately didn't pass the \"as good or better\" test with the users, who preferred Exchange/Outlook. In July 2011, the General Services Administration (GSA) became the first federal agency to migrate its email services for 17,000 employees and contractors to the cloud-based Google Apps for Government, saving $15.2 million over 5 years. As of July 2011, government agencies in 42 states are leveraging cloud-based messaging and collaboration services.\n\nKundra also moved the city's geographic information systems department to a middle school.\n\nBefore his appointment as CIO, Mr. Kundra served as technology adviser on President Barack Obama's transition team. Kundra was officially named by President Obama on March 5, 2009, to the post of Federal CIO.\n\nThe Federal Chief Information Officer is responsible for directing the policy and strategic planning of federal information technology investments as well as for oversight of federal technology spending. Until Kundra, the position had previously been more limited within the Office of Management and Budget where a federal chief information officer role had been created by the E-Government Act of 2002. The Federal CIO establishes and oversees enterprise architecture to ensure system interoperability and information sharing and maintains information security and privacy across the federal government. According to President Obama, as Chief Information Officer, Kundra \"will play a key role in making sure our government is running in the most secure, open, and efficient way possible\". To further President Obama's overall technology agenda, Vivek Kundra, Jeffrey Zients, the Chief Performance Officer, and Aneesh Chopra, the Chief Technology Officer, will work closely together. Kundra and Chopra previously worked in Governor Tim Kaine's administration.\n\nKundra made it a priority to focus on the following areas: \n\nOne of his first projects was the launch of Data.gov, a site for access to raw government data. Another project launched by Kundra in June 2009 was the Federal IT Dashboard, which gives an assessment (in terms of cost, schedule and CIO ranking) of many large government IT projects.\n\nKundra launched the Data.gov platform on May 21, 2009 with the goal of providing public access to raw datasets generated by the Executive Branch of the Federal Government to enable public participation and private sector innovation. Data.gov draws conceptual parallels from the DC Data Catalog launched by Kundra when he was CTO of Washington, D.C., where he published vast amounts of datasets for public use. Immediately after the Data.gov launch, the Apps for America contest by the Sunlight Foundation challenged the American people to develop innovative solutions using Data.gov. San Francisco, the City of New York, the State of California, the State of Utah, the State of Michigan, and the Commonwealth of Massachusetts have launched public access websites modeled after Data.gov. Internationally, over 46 countries have launched open data sites patterned after Data.Gov, some using the U.S. Data.gov software which was made open source and made available on GitHub. Additionally, states, cities and counties have launched sites, notablysome cities in Canada, and the UK are following suit.\n\nOn June 30, 2009, at the Personal Democracy Forum in New York, Vivek Kundra, unveiled the IT Dashboard that tracks over $76 billion in Federal IT spending. The IT Dashboard is part of USASpending.gov to track all government spending. The IT Dashboard is designed to provide CIOs of individual government agencies, the public and agency leaders unprecedented visibility into the operations and performance of Federal IT investments (spending), and the ability to provide and receive direct feedback to those directly accountable. In January 2010, Kundra followed up the work on the IT Dashboard with TechStat accountability sessions. These sessions are designed to turn around, halt or terminate at-risk and failing IT projects in the federal government. It allows agency CIOs, CFOs, and other key stakeholders to find solutions for IT projects that are over budget, behind schedule, or under-performing.\n\nKundra launched the federal government strategy and the cloud computing portal Apps.gov at NASA's Ames Research Center, Moffett Field in California, on September 15, 2009. Apps.gov is a new service provided by the GSA where federal agencies can subscribe to IT services. Kundra saw the cloud as an alternative to hardware investments, as means to reduce IT costs, and to shift focus of federal IT from infrastructure management to strategic projects. This initiative aims to use commercially derived technologies to promote software tools, vast data storage and data sharing, and to foster collaboration across all federal agencies. Howard Schmidt, White House cybersecurity coordinator, will work closely with the Federal CIO and CTO with respect to cloud initiatives and has the responsibility of orchestrating all cybersecurity activities across the government.\n\nOn December 9, 2010, Kundra published the \"25 Point Implementation Plan to Reform Federal Information Technology Management\", which included Cloud First as one of its top priorities for achieving IT efficiency. Cloud First required each agency to identify three cloud initiatives. He announced his decision to leave the federal government and join Harvard University within 7 months of this strategy, too short for any of cloud first initiatives to have demonstrated cost savings. After a short 5 months at Harvard he left to join Salesforce.com, a cloud SaaS and PaaS provider.\n\nThe first major cloud project during his tenure was GSA's migration of e-mail/Lotus Notes to the Gmail and Salesforce.com's platform. GSA awarded a contract for e-mail in December 2010 and a five-year contract to Salesforce in August 2011. A September 2012 Inspector General report found the savings and cost analysis not verifiable and recommended GSA update its cost analysis. GSA office of CIO was unable to provide documentation supporting its analysis regarding the initial projected savings for government staffing and contractor support. The audit found that the agency could neither verify those savings nor clearly determine if the cloud migration is meeting agency expectations despite initial claims that indicated 50% cost savings \n\nKundra published a 25-point implementation plan to reform how the federal government manages information technology. The execution plan follows his decision to reevaluate some of the government's most troubled IT projects. Of 38 projects reviewed, four have been canceled, 11 have been rescoped, and 12 have cut the time for delivery of functionality down by more than half, from two to three years down to an average of 8 months, achieving a total of $3 billion in lifecycle budget reductions, according to whitehouse.gov\n\nOn March 13, 2009, Kundra was placed on indefinite leave following an FBI raid on his former D.C. office and the arrest of two individuals in relation to a bribery investigation. Kundra returned to duties after five days with no finding of wrongdoing on his part.\n\nKundra left his post as chief information officer in August 2011 to accept an academic fellowship at Harvard University, conducting research at both the Berkman Center for Internet & Society and the Joan Shorenstein Center on the Press, Politics and Public Policy.\n\nIn January 2012 Kundra joined Salesforce.com as Executive Vice President of Emerging Markets. On May 16, 2018 Kundra left Salesforce and joined the private start-up Sprinklr as Chief Operating Officer.\n\nIn May 2011, Kundra was selected by EMC Corporation for their Data Hero Visionary Award for his pioneering work under the Obama Administration to reform how the Federal government manages and uses information technology. EMC states that, \"Kundra has led the nation to seek innovative solutions to lower the cost of government operations, while exploring ways to fundamentally change the way the public sector and the public interact\".\n\nIn March 2011, Kundra was selected by the World Economic Forum as a Young Global Leader for his professional accomplishments, commitment to society and potential to contribute to shaping the future of the world.\n\nKundra was awarded the 2010 National Cyber Security Leadership Award by the SANS Institute for uncovering more than $300 million each year in wasted federal spending on ineffective certification and accreditation reporting and demonstrating an alternative approach called \"continuous monitoring\" that provides more effective security for federal systems at lower costs.\n\nKundra was named Chief of the Year on December 21, 2009, by InformationWeek for driving unprecedented change in federal IT.\n\nKundra was named by InfoWorld among the top 25 CTO's in the country.\n\nHe was also selected as a 2008 MIT Sloan CIO Symposium Award Finalist on 'Balancing Innovation and Cost Leadership'. Both organizations cited the \"stock market\" approach to IT portfolio management that Kundra implemented for the District of Columbia. The system measured project performance and allocated IT investments similar to the way the public companies trade on the stock market.\n\nHarvard Kennedy School of Government's Ash Institute also awarded the Innovations in American Government Award (2009) to \"District of Columbia's Data Feeds: Democratization of Government Data\". The project spearheaded by Kundra, Mayor Fenty, and CPO David Gragan was cited for \"increase in civic participation, government accountability, and transparency in D.C. government practices\" through sites like the Digital Public Square and the DC Data Catalog.\n\nKundra was recognized as the 2008 Government Sector IT Executive of the Year by the Tech Council of Maryland. The organization cited Kundra's efforts to increase public access to government information and services through live data feeds and data sets. Kundra was also a recipient of the Federal 100 Award for significant contributions to the federal information technology community.\n\n\n"}
{"id": "1648875", "url": "https://en.wikipedia.org/wiki?curid=1648875", "title": "Wilkinson power divider", "text": "Wilkinson power divider\n\nIn the field of microwave engineering and circuit design, the Wilkinson Power Divider is a specific class of power divider circuit that can achieve isolation between the output ports while maintaining a matched condition on all ports. The Wilkinson design can also be used as a power combiner because it is made up of passive components and hence reciprocal. First published by Ernest J. Wilkinson in 1960, this circuit finds wide use in radio frequency communication systems utilizing multiple channels since the high degree of isolation between the output ports prevents crosstalk between the individual channels.\n\nIt uses quarter wave transformers, which can be easily fabricated as quarter wave lines on printed circuit boards. It is also possible to use other forms of transmission line (e.g. coaxial cable) or lumped circuit elements (inductors and capacitors).\n\nThe scattering parameters for the common case of a 2-way equal-split Wilkinson power divider at the design frequency is given by\n\nInspection of the \"S\" matrix reveals that the network is reciprocal (formula_2), that the terminals are matched (formula_3), that the output terminals are isolated (formula_4=0), and that equal power division is achieved (formula_5). The non-unitary matrix results from the fact that the network is lossy. An ideal Wilkinson divider would yield formula_6. \n\nNetwork theorem governs that a divider cannot satisfy all three conditions (being matched, reciprocal and loss-less) at the same time. Wilkinson divider satisfies the first two (matched and reciprocal), and cannot satisfy the last one (being loss-less). Hence, there is some loss occurring in the network.\n\nNo loss occurs when the signals at ports 2 and 3 are in phase and have equal magnitude. In case of noise input to ports 2 and 3, the noise level at port 1 does not increase, half of the noise power is dissipated in the resistor.\n\nBy cascading, the input power might be divided to any formula_7-number of outputs.\nUnequal/Asymmetric Division Through Wilkinson Divider\n\nIf the arms for port 2 and 3 are connected with un-equal impedances, then asymmetric division of power can be achieved. \nWhen characteristic impedance is formula_8, and one wants to split power as formula_9 and formula_10, and formula_9 ≠ formula_10, then the design can be created following the equations:\n\nA new constant formula_13 is defined for ease of expression, where formula_14\n\nThen the design guideline is:<br>\nformula_15<br>\nformula_16<br> <br>\nformula_17<br>\n\nIt is to be noted that it can be set formula_18 the above expression, and one would obtain the equal-splitting Wilkinson Divider.\n\n\n"}
{"id": "33186583", "url": "https://en.wikipedia.org/wiki?curid=33186583", "title": "Xeon Phi", "text": "Xeon Phi\n\nXeon Phi is a series of x86 manycore processors designed and made entirely by Intel. They are intended for use in supercomputers, servers, and high-end workstations. Its architecture allows use of standard programming languages and APIs such as OpenMP.\n\nSince it was originally based on an earlier GPU design by Intel, it shares application areas with GPUs. The main difference between Xeon Phi and a GPGPU like Nvidia Tesla is that Xeon Phi, with an x86-compatible core, can, with less modification, run software that was originally targeted at a standard x86 CPU.\n\nInitially in the form of PCIe-based add-on cards, a second generation product, codenamed \"Knights Landing\" was announced in June 2013. These second generation chips could be used as a standalone CPU, rather than just as an add-in card.\nIn June 2013, the Tianhe-2 supercomputer at the National Supercomputer Center in Guangzhou (NSCC-GZ) was announced as the world's fastest supercomputer (, it is ). It uses Intel Xeon Phi coprocessors and Ivy Bridge-EP Xeon processors to achieve 33.86 petaFLOPS.\n\nThe Larrabee microarchitecture (in development since 2006) introduced very wide (512-bit) SIMD units to a x86 architecture based processor design, extended to a cache-coherent multiprocessor system connected via a ring bus to memory; each core was capable of four-way multithreading. Due to the design being intended for GPU as well as general purpose computing, the Larrabee chips also included specialised hardware for texture sampling. The project to produce a retail GPU product directly from the Larrabee research project was terminated in May 2010.\n\nAnother contemporary Intel research project implementing x86 architecture on a many-multicore processor was the 'Single-chip Cloud Computer' (prototype introduced 2009), a design mimicking a cloud computing computer datacentre on a single chip with multiple independent cores: the prototype design included 48 cores per chip with hardware support for selective frequency and voltage control of cores to maximize energy efficiency, and incorporated a mesh network for inter-chip messaging. The design lacked cache-coherent cores and focused on principles that would allow the design to scale to many more cores.\n\nThe Teraflops Research Chip (prototype unveiled 2007) is an experimental 80-core chip with two floating point units per core, implementing a 96-bit VLIW architecture instead of the x86 architecture. The project investigated intercore communication methods, per-chip power management, and achieved 1.01 TFLOPS at 3.16 GHz consuming 62 W of power.\n\nIntel's Many Integrated Core (MIC) prototype board, named \"Knights Ferry\", incorporating a processor codenamed \"Aubrey Isle\" was announced May 31, 2010. The product was stated to be a derivative of the \"Larrabee\" project and other Intel research including the \"Single-chip Cloud Computer\".\n\nThe development product was offered as a PCIe card with 32 in-order cores at up to 1.2 GHz with four threads per core, 2 GB GDDR5 memory, and 8 MB coherent L2 cache (256 KB per core with 32 KB L1 cache), and a power requirement of ~300 W, built at a 45 nm process. In the \"Aubrey Isle\" core a 1,024-bit ring bus (512-bit bi-directional) connects processors to main memory. Single board performance has exceeded 750 GFLOPS. The prototype boards only support single precision floating point instructions.\n\nInitial developers included CERN, Korea Institute of Science and Technology Information (KISTI) and Leibniz Supercomputing Centre. Hardware vendors for prototype boards included IBM, SGI, HP, Dell and others.\n\nThe \"Knights Corner\" product line is made at a 22 nm process size, using Intel's Tri-gate technology with more than 50 cores per chip, and is Intel's first many-cores commercial product.\n\nIn June 2011, SGI announced a partnership with Intel to use the MIC architecture in its high performance computing products. In September 2011, it was announced that the Texas Advanced Computing Center (TACC) will use Knights Corner cards in their 10 petaFLOPS \"Stampede\" supercomputer, providing 8 petaFLOPS of compute power. According to \"Stampede: A Comprehensive Petascale Computing Environment\" the \"second generation Intel (Knights Landing) MICs will be added when they become available, increasing Stampede's aggregate peak performance to at least 15 PetaFLOPS.\"\n\nOn November 15, 2011, Intel showed an early silicon version of a Knights Corner processor.\n\nOn June 5, 2012, Intel released open source software and documentation regarding Knights Corner.\n\nOn June 18, 2012, Intel announced at the 2012 Hamburg International Supercomputing Conference that \"Xeon Phi\" will be the brand name used for all products based on their Many Integrated Core architecture. In June 2012, Cray announced it would be offering 22 nm 'Knight's Corner' chips (branded as 'Xeon Phi') as a co-processor in its 'Cascade' systems.\n\nIn June 2012, ScaleMP announced a virtualization update allowing Xeon Phi as a transparent processor extension, allowing legacy MMX/SSE code to run without code changes.\nAn important component of the Intel Xeon Phi coprocessor’s core is its vector processing unit (VPU).\nThe VPU features a novel 512-bit SIMD instruction set, officially known as Intel Initial Many Core Instructions (Intel IMCI). Thus, the VPU can execute 16 single-precision (SP) or 8 double-precision (DP) operations per cycle. The VPU also supports Fused Multiply-Add (FMA) instructions and hence can execute 32 SP or 16 DP floating point operations per cycle. It also provides support for integers.\nThe VPU also features an Extended Math Unit (EMU) that can execute operations such as reciprocal, square root, and logarithm, thereby allowing these operations to be executed in a vector fashion with high bandwidth. The EMU operates by calculating polynomial approximations of these functions.\n\nOn November 12, 2012, Intel announced two Xeon Phi coprocessor families using the 22 nm process size: the Xeon Phi 3100 and the Xeon Phi 5110P. The Xeon Phi 3100 will be capable of more than 1 teraFLOPS of double precision floating point instructions with 240 GB/sec memory bandwidth at 300 W. The Xeon Phi 5110P will be capable of 1.01 teraFLOPS of double precision floating point instructions with 320 GB/sec memory bandwidth at 225 W. The Xeon Phi 7120P will be capable of 1.2 teraFLOPS of double precision floating point instructions with 352 GB/sec memory bandwidth at 300 W.\n\nOn June 17, 2013, the Tianhe-2 supercomputer was announced by TOP500 as the world's fastest. Tianhe-2 used Intel Ivy Bridge Xeon and Xeon Phi processors to achieve 33.86 petaFLOPS. It was the fastest on the list for two and a half years, lastly in November 2015.\n\nThe cores of Knights Corner are based on a modified version of P54C design, used in the original Pentium. The basis of the Intel MIC architecture is to leverage x86 legacy by creating an x86-compatible multiprocessor architecture that can use existing parallelization software tools. Programming tools include OpenMP, OpenCL, Cilk/Cilk Plus and specialised versions of Intel's Fortran, C++ and math libraries.\n\nDesign elements inherited from the Larrabee project include x86 ISA, 4-way SMT per core, 512-bit SIMD units, 32 KB L1 instruction cache, 32 KB L1 data cache, coherent L2 cache (512 KB per core), and ultra-wide ring bus connecting processors and memory.\n\nThe Knights Corner instruction set documentation is available from Intel.\n\nCode name for the second generation MIC architecture product from Intel. Intel officially first revealed details of its second generation Intel Xeon Phi products on June 17, 2013. Intel said that the next generation of Intel MIC Architecture-based products will be available in two forms, as a coprocessor or a host processor (CPU), and be manufactured using Intel's 14 nm process technology. Knights Landing products will include integrated on-package memory for significantly higher memory bandwidth.\n\nKnights Landing contains up to 72 Airmont (Atom) cores with four threads per core, using LGA 3647 socket supporting up to 384 GB of \"far\" DDR4 2133 RAM and 8–16 GB of stacked \"near\" 3D MCDRAM, a version of the Hybrid Memory Cube. Each core has two 512-bit vector units and supports AVX-512 SIMD instructions, specifically the Intel AVX-512 Foundational Instructions (AVX-512F) with Intel AVX-512 Conflict Detection Instructions (AVX-512CD), Intel AVX-512 Exponential and Reciprocal Instructions (AVX-512ER), and Intel AVX-512 Prefetch Instructions (AVX-512PF).\n\nThe National Energy Research Scientific Computing Center announced that Phase 2 of its newest supercomputing system \"Cori\" would use Knights Landing Xeon Phi coprocessors.\n\nOn June 20, 2016, Intel launched the Intel Xeon Phi product family x200 based on the Knights Landing architecture, stressing its applicability to not just traditional simulation workloads, but also to machine learning. The model lineup announced at launch included only Xeon Phi of bootable form-factor, but two versions of it: standard processors and processors with integrated Intel Omni-Path architecture fabric. The latter is denoted by the suffix F in the model number. Integrated fabric is expected to provide better latency at a lower cost than discrete high-performance network cards.\n\nOn November 14, 2016, the 48th list of TOP500 contained 10 systems using Knights Landing platforms.\n\nThe PCIe based co-processor variant of Knight's Landing was never offered to the general market and was discontinued by August 2017. This included the 7220A, 7240P and 7220P coprocessor cards.\n\nIntel announced the product discontinuance during Summer 2018\n\nAll models can boost to their peak speeds, adding 200 MHz to their base frequency when running just one or two cores. When running from three to the maximum number of cores, the chips can only boost 100 MHz above the base frequency. All chips run high-AVX code at a frequency reduced by 200 MHz.\nKnights Hill was the codename for the third-generation MIC architecture, for which Intel announced the first details at SC14. It was to be manufactured in a 10 nm process.\n\nKnights Hill was expected to be used in the United States Department of Energy Aurora supercomputer, to be deployed at Argonne National Laboratory. However, Aurora was delayed in favor of using an \"advanced architecture\" with a focus on machine learning.\n\nIn 2017, Intel announced that Knights Hill had been canceled in favor of another architecture built from the ground up to enable Exascale computing in the future. This new architecture in now expected for 2020–2021. The terms \"Exascale computing\" and \"high performance computing\" (HPC) got meanwhile linked by intel towards their own term Omni-Path Architecture (OPA) in publications.\n\nKnights Mill is Intel's codename for a Xeon Phi product specialized in deep learning, initially released in December 2017. Nearly identical in specifications to Knights Landing, Knights Mill includes optimizations for better utilization of AVX-512 instructions and enables 4-way hyperthreading. Single-precision and variable-precision floating-point performance increased, at the expense of double-precision floating-point performance.\n\nAn empirical performance and programmability study has been performed by researchers, in which the authors claim that achieving high performance with Xeon Phi still needs help from programmers and that merely relying on compilers with traditional programming models is still far from reality. However, research in various domains, such as life sciences, deep learning and computer-aided engineering demonstrated that exploiting both the thread- and SIMD-parallelism of Xeon Phi achieves significant speed-ups.\n\n\n\n"}
{"id": "5764847", "url": "https://en.wikipedia.org/wiki?curid=5764847", "title": "Yodlee", "text": "Yodlee\n\nYodlee is an American software company that develops an account aggregation service that allows users to see their credit card, bank, investment, email, travel reward accounts, etc. on one screen. In addition, Yodlee Labs (formerly Yodlee MoneyCenter), a free web application that helps consumers with their finances online, provides features such as bill payment, expense tracking, and investment management (similar to personal finance services provided by Intuit's Quicken). Yodlee's Privacy Policy FAQ references Yodlee as a licensee of the TRUSTe Privacy Program.. Part of Yodlee's business model consists of selling its customer's financial transaction data to investors. \n\nIn August 2015, Envestnet acquired Yodlee.\n\nAs of 2013, Yodlee has over 45 million users, and over 150 financial institutions and portals (including 5 of the top 10 U.S. banks) offer services powered by Yodlee. Yodlee's aggregation engine powers several applications for partners, including websites like Money Dashboard, MoneyStrand, Thrive, and several large banks and financial institutions. In 2010, Yodlee partnered with Y Combinator, providing its financial services platforms to all Y Combinator-funded companies.\n\nYodlee was started since 1999 by Venkat Rangan (vice chancellor of Amrita University); Sam Inala, Ramakrishna \"Schwark\" Satyavolu, Srihari Sampath Kumar (all earlier at Microsoft); and Sukhinder Singh (earlier at Amazon.com and Junglee).\n\nYodlee started operations (and is headquartered) in Redwood Shores, California. It also has offices in London, UK, and Bangalore, India. In 2000, Yodlee merged with its main competitor in the data aggregation space, an Atlanta-based company called VerticalOne, which was owned at the time by SecurityFirst, an internet banking firm.\n\nBy 2010 Yodlee was said to have raised at least $116 million over its 10-year lifespan. In a press release in June 2008, it was announced that Bank of America led a $35 million financing and existing Yodlee investors, including its largest shareholder Warburg Pincus along with Accel Partners and Institutional Venture Partners participated in the financing round.\n\nOn October 3, 2014, Yodlee went public on NASDAQ, trading under the symbol YDLE. It raised $75 million at $12/share. On August 10, 2015 Yodlee sold itself to Envestnet for a reported $660 Million. As a result, it is no longer listed on the NASDAQ. In November 2017 Token partnered with Yodlee for payments and financial data aggregation.\n"}
