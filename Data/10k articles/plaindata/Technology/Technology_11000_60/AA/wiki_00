{"id": "49274252", "url": "https://en.wikipedia.org/wiki?curid=49274252", "title": "AC 20-115", "text": "AC 20-115\n\nThe Advisory Circular AC 20-115, Airborne Software Assurance, identifies the RTCA published standard DO-178 as defining a suitable means for demonstrating compliance for the use of software within aircraft systems. The present revision C of the circular identifies DO-178 Revision C as the active revision of that standard.\n\nThis AC calls attention to DO-178C as a means, but not the only means to secure FAA approval of software. Earlier revisions of the AC were brief, serving little more than to call attention to active DO-178 revisions. Revision C is considerably longer, giving guidance in how to transition from DO-178 revision B to C.\n\n"}
{"id": "59178840", "url": "https://en.wikipedia.org/wiki?curid=59178840", "title": "American Micro Devices", "text": "American Micro Devices\n\nAmerican Micro Devices was an American company based in Minneapolis Minnesota developing semiconductor devices. It was founded in 1964, but dissolved in 1991. It is not related to Advanced Micro Devices. \n"}
{"id": "55909465", "url": "https://en.wikipedia.org/wiki?curid=55909465", "title": "Andrea Goldsmith (engineer)", "text": "Andrea Goldsmith (engineer)\n\nAndrea Goldsmith is an American electrical engineer and the Stephen Harris Professor in the School of Engineering at Stanford University, as well as a faculty affiliate at the Stanford Neurosciences Institute. Her interests are in the design, analysis and fundamental performance limits of wireless systems and networks, and in the application of communication theory and signal processing to neuroscience. She also co-founded and served as chief technology officer of Plume WiFi and Quantenna Communications.\n\nGoldsmith was raised in the San Fernando Valley, California. Her father Werner Goldsmith was a professor of mechanical engineering at UC Berkeley, and her mother Adrienne Goldsmith was an animator for cartoon shows including \"The Rocky and Bullwinkle Show\". Goldsmith earned her bachelor's degree in engineering math from the University of California, Berkeley, in 1986, and her MS and PhD in electrical engineering from UC Berkeley in 1991 and 1994, respectively. In the years between obtaining her bachelor's and PhD, she spent four years as a systems engineer at a Silicon Valley defense communications startup.\n\nGoldsmith started her academic career at the California Institute of Technology and was there for four years. She joined Stanford in 1999, becoming an associate professor in 2002 and a full professor in 2007. At Stanford, she has served as chair of the faculty senate, and on the school's task force on women and leadership. In 2006, she took a leave of absence from Stanford and co-founded Quantenna Communications, a company that produces silicon chipsets designed for high-speed, wireless high-definition video home networking. She served as chief technology officer of the startup until returning to Stanford in 2008. She was also a founder and CTO of Plume WiFi, which was founded in 2014 and develops WiFi technology.\n\nAs an inventor and consultant, she has secured 28 patents. She has authored and co-authored several books, including \"Wireless Communication\", \"MIMO Wireless Communications\" and \"Principles of Cognitive Radio\". She has launched and led several multi-university research projects, including DARPA's ITMANET program, and she is a principal investigator in the National Science Foundation Center on the Science of Information.\n\nIn the IEEE, Goldsmith served on the board of governors for both the Information Theory and Communications societies. She has also been a distinguished lecturer for both societies, served as president of the IEEE Information Theory Society in 2009, founded and chaired the Student Committee of the IEEE Information Theory society, and chaired the Emerging Technology Committee of the IEEE Communications Society. She chairs the IEEE Committee on Diversity and Inclusion.\n\nShe won the 2017 Women in Communications Engineering Mentorship Award from the IEEE Communications Society for her efforts in encouraging women in the fields of technology and engineering. In 2017, she was elected to the Academy of Arts and Sciences, and also to the National Academy of Engineering.\n\n\n\nGoldsmith lives in Menlo Park, California, with her husband, scientist Arturo Salz, and their children.\n\n"}
{"id": "10958185", "url": "https://en.wikipedia.org/wiki?curid=10958185", "title": "Automobile dependency", "text": "Automobile dependency\n\nAutomobile dependency is the concept that some city layouts cause automobiles to be favored over alternate forms of transportation such as bicycles, public transit, and walking.\n\nWhen it comes to automobile use, there is a spiralling effect where traffic congestion produces the 'demand' for more and bigger roads and removal of 'impediments' to traffic flow, such as pedestrians, signalised crossings, traffic lights, cyclists, and various forms of street-based public transit such as streetcars (trams).\n\nThese measures make automobile use more pleasurable and advantageous at the expense of other modes of transport, so greater traffic volumes are induced. Additionally, the urban design of cities adjusts to the needs of automobiles in terms of movement and space. Buildings are replaced by parking lots. Open air shopping streets are replaced by enclosed shopping malls. Walk-in banks and fast-food stores are replaced by drive-in versions of themselves that are inconveniently located for pedestrians. Town centres with a mixture of commercial, retail and entertainment functions are replaced by single-function business parks, 'category-killer' retail boxes and 'multiplex' entertainment complexes, each surrounded by large tracts of parking.\n\nThese kinds of environments require automobiles to access them, thus inducing even more traffic onto the increased roadspace. This results in congestion, and the cycle above continues. Roads get ever bigger, consuming ever greater tracts of land previously used for housing, manufacturing and other socially and economically useful purposes. Public transit becomes less and less viable and socially stigmatised, eventually becoming a minority form of transportation. People's choices and freedoms to live functional lives without the use of the car are greatly reduced. Such cities are automobile dependent.\n\nAutomobile dependency is seen primarily as an issue of environmental sustainability due to the consumption of non-renewable resources and production of greenhouse gases responsible for global warming. It is also an issue of social and cultural sustainability. Like gated communities, the private automobile produces physical separation between people and reduces the opportunities for unstructured social encounter that is a significant aspect of social capital formation and maintenance in urban environments.\n\nAccording to the \"Handbook on estimation of external costs in the transport sector\" made by the Delft University and which is the main reference in European Union for assessing the externalities of cars, the main external costs of driving a car are:\n\nThere are a number of planning and design approaches to redressing automobile dependency, known variously as New Urbanism, Transit-oriented development, and Smart growth. Most of these approaches focus on the physical urban design, urban density and landuse zoning of cities. Dr. Paul Mees, a transport planning academic formerly at the University of Melbourne argues that investment in good public transit, centralised management by the public sector and appropriate policy priorities are more significant than issues of urban form and density.\n\nThere are, of course, many who argue against a number of the details within any of the complex arguments related to this topic, particularly relationships between urban density and transit viability, or the nature of viable alternatives to automobiles that provide the same degree of flexibility and speed. There is also research into the future of automobility itself in terms of shared usage, size reduction, roadspace management and more sustainable fuel sources.\n\nCar-sharing is one example of a solution to automobile dependency. Research has shown that in the United States, services like Zipcar, have reduced demand by about 500,000 cars. In the developing world, companies like eHi, Carrot, Zazcar and Zoom have replicated or modified Zipcar's business model to improve urban transportation to provide a broader audience with greater access to the benefits of a car and provide\"last-mile\" connectivity between public transportation and an individual's destination. Car sharing also reduces private vehicle ownership.\n\nWhether smart growth does or can reduce problems of automobile dependency associated with urban sprawl has been fiercely contested for several decades. The influential study in 1989 by Peter Newman and Jeff Kenworthy compared 32 cities across North America, Australia, Europe and Asia. The study has been criticised for its methodology, but the main finding, that denser cities, particularly in Asia, have lower car use than sprawling cities, particularly in North America, has been largely accepted, but the relationship is clearer at the extremes across continents than it is within countries where conditions are more similar.\n\nWithin cities studies from across many countries (mainly in the developed world) have shown that denser urban areas with greater mixture of land use and better public transport tend to have lower car use than less dense suburban and exurban residential areas. This usually holds true even after controlling for socio-economic factors such as differences in household composition and income.\n\nThis does not necessarily imply that suburban sprawl causes high car use, however. One confounding factor, which has been the subject of many studies, is residential self-selection: people who prefer to drive tend to move towards low-density suburbs, whereas people who prefer to walk, cycle or use transit tend to move towards higher density urban areas, better served by public transport. Some studies have found that, when self-selection is controlled for, the built environment has no significant effect on travel behaviour. More recent studies using more sophisticated methodologies have generally rejected these findings: density, land use and public transport accessibility can influence travel behaviour, although social and economic factors, particularly household income, usually exert a stronger influence.\n\nReviewing the evidence on urban intensification, smart growth and their effects on automobile use Melia et al. (2011) found support for the arguments of both supporters and opponents of smart growth. Planning policies that increase population densities in urban areas do tend to reduce car use, but the effect is a weak one, so doubling the population density of a particular area will not halve the frequency or distance of car use.\n\nThese findings led them to propose the paradox of intensification:\n\n\nAt the citywide level, it may be possible, through a range of positive measures to counteract the increases in traffic and congestion that would otherwise result from increasing population densities: Freiburg im Breisgau in Germany is one example of a city which has been more successful in reducing automobile dependency and constraining increased in traffic despite substantial increases in population density.\n\nThis study also reviewed evidence on the local effects of building at higher densities. At the level of the neighbourhood or individual development, positive measures (like improvements to public transport) will usually be insufficient to counteract the traffic effect of increasing population density. This leaves policy-makers with four choices: intensify and accept the local consequences, sprawl and accept the wider consequences, a compromise with some element of both, or intensify accompanied by more radical measures such as parking restrictions, closing roads to traffic and carfree zones.\n\n\n"}
{"id": "51364327", "url": "https://en.wikipedia.org/wiki?curid=51364327", "title": "BaDoinkVR", "text": "BaDoinkVR\n\nBaDoinkVR is a virtual reality porn production company founded in 2006. It is the AVN Awards 2018 VR Site of the Year award winner. The company is part of \"CM Productions (TeamCMP)\", and sister site to \"BaDoink VIP, VRCosplayX, 18VR, RealVR and BabeVR\". BaDoinkVR is headquartered in Rochester, New York with satellite offices in Barcelona, Spain and Silicon Valley. The company was the first to drive mass consumer trial of VR adult videos by seeding the market with 20,000 free virtual reality cardboard goggles.\n\nFounded in 2006, \"TeamCMP\" initially launched BaDoink as a premier online adult entertainment site with a proprietary technology platform enabling users to consume content on any device as well as wirelessly stream exclusive content from any device to any TV. In 2015, the company expanded its platform and content production to focus on virtual reality with the launch of BaDoinkVR.\n\nThe web site features 360 and 180-degree immersive videos, motion tracking and binaural audio. Videos on \"BaDoinkVR.com\" can be viewed with Oculus Go, Oculus Rift, Playstation VR, HTC Vive, Samsung Gear VR and Google Cardboard with an iOS or Android smartphone using BaDoink’s \"VR Player\", a virtual reality app that is compatible with the devices listed.\n\nAs of early 2016, BaDoinkVR is consistently listed as one of the top three leading companies in virtual reality porn.\n\nIn February 2016, BaDoinkVR announced a partnership with Amsterdam based Kiiroo, a manufacturer of teledildonics and haptic technology that allows partners to simulate sexual relations over a distance. The two companies will also be producing virtual reality videos focused on sex education.\n\nIn May 2015, BaDoink was notified that the brand and logo, disguised as the \"BaDoink Ultra App\", were being used to spread the Reveton/IcePol ransomware. They identified the site that was distributing the ransomware, and proceeded to remedy the situation, alerting the site's hosting company, and submitting a DMCA takedown request. The company was also in contact with the FBI Cyber Division and sent a cease and desist to the owners of the domain names as well.\n\n"}
{"id": "1956164", "url": "https://en.wikipedia.org/wiki?curid=1956164", "title": "Bidding stick", "text": "Bidding stick\n\nA bidding stick (sometimes also referred to as a budstikke, war arrow, or stembod) is a term for a wooden object, such as a club or baton, carried by a messenger and used by Northern Europeans, for example in Scotland and Scandinavia, to rally people for \"things\" (assemblies) and for defence or rebellion.\n\nIn Scotland, such a token (Scottish Gaelic: \"crann-tara\", translated as \"fiery cross\" or \"cross of shame\") was used to rally clan members to arms. The practice is described in the novels and poetry of Sir Walter Scott. A small burning cross or charred piece of wood would be carried from town to town. A widely known use was in 1745, during the Jacobite rising although it was used more recently in Canada, among Scottish settlers during the War of 1812, and among Clan Grant in 1820. In 1820, over 800 fighting men of Clan Grant were gathered, by the passing of the Fiery Cross, to come to the aid of their Clan Lord and his sister in the village of Elgin.\n\nThe name \"Crann Tara\" was used for a Scottish Gaelic current affairs programme on Grampian Television (ITV).\n\nWhen an enemy had arrived, bidding sticks (Old Swedish: \"buþkafle\" (sg.)) were sent in all directions. In Sweden, they consisted of clubs, or just wooden chunks; in Norway, there were repurposed arrows. Sometimes the bidding sticks had a string attached to one end and were charred on the other end; Olaus Magnus (1555) relates that those who did not bring the club to the next village would be hanged and their homesteads burnt down.\n\nWhen the people were assembled to a thing, the object was in the shape of an axe, or if the meeting concerned blasphemy, it was a cross.\n\nThe objects were signed with runes or other marks in order to indicate the reason for the assembly (e.g. election of king at the Stone of Mora), and who had sent them. During the Middle Ages, using \"buþkaflar\" was the official method of assembling people, and they were only allowed to be carved by certain officials, e.g. governors and sheriffs.\n\nThey were especially efficient, however, when they were used to levy people against royal oppression and high taxes. After the Dalecarlian rebellion of 1743, strong checks were placed on the use of bidding sticks.\n\nIn Sweden, the bidding stick was standardized during the village reorganizations in 1742, and it was at the village level that they were frequently used. During the 19th and 20th centuries, more specific messages were attached to the clubs or inserted into a hollow space. Still in the early 20th century, there was a paragraph in Swedish law that stated that the bidding stick would be sent between the villages if there was a forest fire. — Similar paragraphs were also present in the Finnish legislation concerning the correct use of \"arpakapula\", or \"budkavle\" in Finland’s Swedish, till the 20th century.\n\nThe concept of the bidding stick has been used as the name for several newspapers, including the Norwegian papers \"Budstikka\", \"Budstikken\", and \"Bremanger Budstikke\", and the Faroese paper \"Tingakrossur\".\n"}
{"id": "21115753", "url": "https://en.wikipedia.org/wiki?curid=21115753", "title": "Bread warmer", "text": "Bread warmer\n\nA bread warmer can describe a number of different devices used to keep bread from cooling too fast. Examples include baskets with cloths, ceramic disks, or cabinets placed over a heat source such as steam radiators.\n\n"}
{"id": "3727298", "url": "https://en.wikipedia.org/wiki?curid=3727298", "title": "Cetaphil", "text": "Cetaphil\n\nCetaphil or is a line of skin care products from Galderma Laboratories, including cleansers, bar soap, cream, lotion, and moisturizers. Cetaphil products are commonly sold at grocery stores and pharmacies throughout the United States, Canada and India. They are also available in pharmacies in \nAustralia, Hong Kong, South Korea, Indonesia, the Philippines, Singapore, some European, Latin American and Caribbean countries.\n\nIn Indonesia, Cetaphil entered the market in January 2013 partnering with Deitz Noots Indonesia as its Indonesian distributor.\n\nThe brand makes products for people with sensitive, dry and/or acne-prone skin as an alternative to harsher types of soap.\n\nCetaphil is highly used and known for its line of scent-free lotion and moisturizers. It is oil-free and can be applied to all types of skin because of its simplicity and non-harsh chemical build up.\n\nCetaphil cleanser ingredients: water, cetyl alcohol, propylene glycol, sodium lauryl sulfate, stearyl alcohol, methylparaben, propylparaben, butylparaben.\n\n"}
{"id": "401154", "url": "https://en.wikipedia.org/wiki?curid=401154", "title": "Checked baggage", "text": "Checked baggage\n\nChecked baggage is luggage delivered to an airline or train for transportation in the hold of an aircraft or baggage car of a passenger train. Checked baggage is inaccessible to the passenger during the flight or ride, as opposed to carry-on baggage.\n\nThis baggage is limited by airlines with regard to size, weight, and number, usually dependent upon the fare paid, or class of ticket. Baggage exceeding the limits is regarded as \"excess baggage\".\n\nEvery airline has its own policies with regard to baggage allowance. Often the policy is also dependent on where the flight goes to or comes from. Tickets executed by multiple airlines may have different rules. Usually the exact conditions of a specific booking are mentioned in the ticket information online. \n\nOn short-haul internal flights in the US, with some exceptions, checked baggage is no longer complimentary with most discounted economy tickets, and must be paid for in addition to the ticket price; a passenger generally has to hold a higher or full fare economy ticket, travel in a premium cabin, or hold elite status on an airline to be afforded complimentary checked baggage. For long-haul and transoceanic flights, checked baggage is included as standard.\n\nLow-cost carriers such as Ryanair in Europe and AirAsia in Asia charge for checked baggage, whilst for full-service airlines the cost is included in the ticket price.\n\nAccording to the rules of most air transportation authorities, such as the U.S. Federal Aviation Administration and European Union's Joint Aviation Authorities, should passengers flying internationally with checked baggage fail to arrive at the departure gate before the flight is closed, that person's baggage must be retrieved from the aircraft hold before the flight is permitted to take off. For Singapore, passengers with prohibited items are required to take it out from the bags retrieved at the check-in counter, failing which the baggage will be flagged with prohibited items and will be dumped away before boarding. In the United States, this does not apply to domestic flights since all bags are required to go through explosive detection machines (EDS) prior to loading. Making sure passengers board flights onto which they have checked baggage is called \"passenger-baggage reconciliation\" and is accomplished automatically through two commercially available systems. The security presumption of passenger-baggage reconciliation is that terrorists will not want to kill themselves, and will not board an aircraft if they have caused a bomb to be placed in its hold. This presumption does not hold true of suicide bombers.\n\nUnaccompanied suitcases led to the downing of four flights, when a bomb inside the suitcase exploded:\n\nSpare lithium-ion batteries, inclusive of battery packs and powerbanks are not allowed on checked-in luggage.\n\nExcess baggage is the amount of baggage that is in excess of the free allowance in size, number, or weight permitted for the journey. At the carrier's discretion, this may be carried at an extra charge, but no guarantee is made and it may have to be sent as freight instead. Some airlines impose excess baggage embargoes on certain (usually smaller) routes, indicating that they will accept no (or very little) excess baggage.\n"}
{"id": "2013553", "url": "https://en.wikipedia.org/wiki?curid=2013553", "title": "Chinois", "text": "Chinois\n\nA chinois (; ) is a conical sieve with an extremely fine mesh. It is used to strain custards, purees, soups, and sauces, producing a very smooth texture. It can also be used to dust food with a fine layer of powdered ingredient. \n\n\"Chinois\" is a loanword from the French adjective meaning Chinese.\n\nA related utensil is the tamis – a flat strainer through which food is pressed with a scraper or pestle.\n\nA similarly-shaped utensil is the China cap, a reference to the conical Asian hats common in China. It is a perforated metal conical strainer with much larger holes than a chinois. A China cap is used to remove seeds and other coarse matter from soft foods, but produces a coarser-textured product than the chinois.\n\nBoth the chinois and the China cap often are used with a cone-shaped pestle. With the pestle tip placed in the bottom of the strainer, it is rolled against the sides of the device to work liquids and soft food through it. In this way, the chinois functions much like a tamis, and the China cap functions similar to a food mill. A small ladle can also be used instead of a pestle, allowing scooping solids from the sides of the strainer as well as pressing liquid through the mesh.\n"}
{"id": "6654559", "url": "https://en.wikipedia.org/wiki?curid=6654559", "title": "Data Interchange Standards Association", "text": "Data Interchange Standards Association\n\nThe Data Interchange Standards Association (DISA) was the organization that supported various other organizations, for the most part, responsible for the development of cross-industry electronic business interchange standards.\n\nDISA served as the Secretariat for ASC X12 and their X12 EDI and XML standards development process. As of January 2016, DISA no longer exists.\n\nThe Accredited Standards Committee (ASC) X12 develops and maintains the most widely implemented EDI standards. These standards interface with a multitude of e-commerce technologies and serve as the premier tool for integrating e-commerce applications. Through the X12 Committee's standards and active participation in emerging and relevant technical initiatives (XML, ebXML), they foster cross-industry consensus and set the norm for more effective data exchange.\n\n\n"}
{"id": "195113", "url": "https://en.wikipedia.org/wiki?curid=195113", "title": "Digital divide", "text": "Digital divide\n\nA digital divide is an economic and social inequality with regard to access to, use of, or impact of information and communication technologies (ICT). The divide within countries (such as the digital divide in the United States) may refer to inequalities between individuals, households, businesses, or geographic areas, usually at different socioeconomic levels or other demographic categories. The divide between differing countries or regions of the world is referred to as the global digital divide, examining this technological gap between developing and developed countries on an international scale.\n\nThe term \"digital divide\" describes a gap in terms of access to and usage of information and communication technology. It was traditionally considered to be a question of having or not having access, but with a global mobile phone penetration of over 95%, it is becoming a relative inequality between those who have more and less bandwidth and more or fewer skills. Conceptualizations of the digital divide have been described as \"who, with which characteristics, connects how to what\":\nDifferent authors focus on different aspects, which leads to a large variety of definitions of the digital divide. \"For example, counting with only 3 different choices of subjects (individuals, organizations, or countries), each with 4 characteristics (age, wealth, geography, sector), distinguishing between 3 levels of digital adoption (access, actual usage and effective adoption), and 6 types of technologies (fixed phone, mobile... Internet...), already results in 3x4x3x6 = 216 different ways to define the digital divide. Each one of them seems equally reasonable and depends on the objective pursued by the analyst\".\nThe \"digital divide\" is also referred to by a variety of other terms which have similar meanings, though may have a slightly different emphasis: digital inclusion, digital participation, basic digital skills, media literacy and digital accessibility.\n\nThe National Digital Inclusion Alliance, a US-based nonprofit organization, has found the term \"digital divide\" to be problematic, since there are a multiplicity of divides. Instead, they chosen to use the term \"digital inclusion\", providing a definition:\nDigital Inclusion refers to the activities necessary to ensure that all individuals and communities, including the most disadvantaged, have access to and use of Information and Communication Technologies (ICTs). This includes 5 elements: 1) affordable, robust broadband internet service; 2) internet-enabled devices that meet the needs of the user; 3) access to digital literacy training; 4) quality technical support; and 5) applications and online content designed to enable and encourage self-sufficiency, participation and collaboration.\n\nThe infrastructure by which individuals, households, businesses, and communities connect to the Internet address the physical mediums that people use to connect to the Internet such as desktop computers, laptops, basic mobile phones or smartphones, iPods or other MP3 players, gaming consoles such as Xbox or PlayStation, electronic book readers, and tablets such as iPads.\n\nTraditionally the nature of the divide has been measured in terms of the existing numbers of subscriptions and digital devices. Given the increasing number of such devices, some have concluded that the digital divide among individuals has increasingly been closing as the result of a natural and almost automatic process. Others point to persistent lower levels of connectivity among women, racial and ethnic minorities, people with lower incomes, rural residents, and less educated people as evidence that addressing inequalities in access to and use of the medium will require much more than the passing of time. Recent studies have measured the digital divide not in terms of technological devices, but in terms of the existing bandwidth per individual (in kbit/s per capita). \n\nAs shown in the Figure on the side, the digital divide in kbit/s is not monotonically decreasing, but re-opens up with each new innovation. For example, \"the massive diffusion of narrow-band Internet and mobile phones during the late 1990s\" increased digital inequality, as well as \"the initial introduction of broadband DSL and cable modems during 2003–2004 increased levels of inequality\". This is because a new kind of connectivity is never introduced instantaneously and uniformly to society as a whole at once, but diffuses slowly through social networks. As shown by the Figure, during the mid-2000s, communication capacity was more unequally distributed than during the late 1980s, when only fixed-line phones existed. The most recent increase in digital equality stems from the massive diffusion of the latest digital innovations (i.e. fixed and mobile broadband infrastructures, e.g. 3G and fiber optics FTTH).\nMeasurement methodologies of the digital divide, and more specifically an Integrated Iterative Approach General Framework (Integrated Contextual Iterative Approach – ICI) and the digital divide modeling theory under measurement model DDG (Digital Divide Gap) are used to analyze the gap existing between developed and developing countries, and the gap among the 27 members-states of the European Union.\n\nInstead of tracking various kinds of digital divides among fixed and mobile phones, narrow- and broadband Internet, digital TV, etc., it has recently been suggested to simply measure the amount of kbit/s per actor. This approach has shown that the digital divide in kbit/s per capita is actually widening in relative terms: \"While the average inhabitant of the developed world counted with some 40 kbit/s more than the average member of the information society in developing countries in 2001, this gap grew to over 3 Mbit/s per capita in 2010.\" \n\nThe upper graph of the Figure on the side shows that the divide between developed and developing countries has been diminishing when measured in terms of subscriptions per capita. In 2001, fixed-line telecommunication penetration reached 70% of society in developed OECD countries and 10% of the developing world. This resulted in a ratio of 7 to 1 (divide in relative terms) or a difference of 60% (divide in measured in absolute terms). During the next decade, fixed-line penetration stayed almost constant in OECD countries (at 70%), while the rest of the world started a catch-up, closing the divide to a ratio of 3.5 to 1. The lower graph shows the divide not in terms of ICT devices, but in terms of kbit/s per inhabitant. While the average member of developed countries counted with 29 kbit/s more than a person in developing countries in 2001, this difference got multiplied by a factor of one thousand (to a difference of 2900 kbit/s). In relative terms, the fixed-line capacity divide was even worse during the introduction of broadband Internet at the middle of the first decade of the 2000s, when the OECD counted with 20 times more capacity per capita than the rest of the world. This shows the importance of measuring the divide in terms of kbit/s, and not merely to count devices. The International Telecommunications Union concludes that \"the bit becomes a unifying variable enabling comparisons and aggregations across different kinds of communication technologies\".\n\nHowever, research shows that the digital divide is more than just an access issue and cannot be alleviated merely by providing the necessary equipment. There are at least three factors at play: information accessibility, information utilization and information receptiveness. More than just accessibility, individuals need to know how to make use of the information and communication tools once they exist within a community. Information professionals have the ability to help bridge the gap by providing reference and information services to help individuals learn and utilize the technologies to which they do have access, regardless of the economic status of the individual seeking help.\n\nInternet connectivity can be utilized at a variety of locations such as homes, offices, schools, libraries, public spaces, Internet cafe and others. There are also varying levels of connectivity in rural, suburban, and urban areas.\n\nCommon Sense Media, a nonprofit group based in San Francisco, surveyed almost 1,400 parents and reported in 2011 that 47 percent of families with incomes more than $75,000 had downloaded apps for their children, while only 14 percent of families earning less than $30,000 had done so.\n\nThe gap in a digital divide may exist for a number of reasons. Obtaining access to ICTs and using them actively has been linked to a number of demographic and socio-economic characteristics: among them income, education, race, gender, geographic location (urban-rural), age, skills, awareness, political, cultural and psychological attitudes. Multiple regression analysis across countries has shown that income levels and educational attainment are identified as providing the most powerful explanatory variables for ICT access and usage. Evidence was found that Caucasians are much more likely than non-Caucasians to own a computer as well as have access to the Internet in their homes. As for geographic location, people living in urban centers have more access and show more usage of computer services than those in rural areas. Gender was previously thought to provide an explanation for the digital divide, many thinking ICT were male gendered, but controlled statistical analysis has shown that income, education and employment act as confounding variables and that women with the same level of income, education and employment actually embrace ICT more than men (see Women and ICT4D). However, each nation has its own set of causes or the digital divide. For example, the digital divide in Germany is unique because it is not largely due to difference in quality of infrastructure.\n\nOne telling fact is that \"as income rises so does Internet use ...\", strongly suggesting that the digital divide persists at least in part due to income disparities. Most commonly, a digital divide stems from poverty and the economic barriers that limit resources and prevent people from obtaining or otherwise using newer technologies.\n\nIn research, while each explanation is examined, others must be controlled in order to eliminate interaction effects or mediating variables, but these explanations are meant to stand as general trends, not direct causes. Each component can be looked at from different angles, which leads to a myriad of ways to look at (or define) the digital divide. For example, measurements for the intensity of usage, such as incidence and frequency, vary by study. Some report usage as access to Internet and ICTs while others report usage as having previously connected to the Internet. Some studies focus on specific technologies, others on a combination (such as Infostate, proposed by Orbicom-UNESCO, the Digital Opportunity Index, or ITU's ICT Development Index). Based on different answers to the questions of who, with which kinds of characteristics, connects how and why, to what there are hundreds of alternatives ways to define the digital divide. \"The new consensus recognizes that the key question is not how to connect people to a specific network through a specific device, but how to extend the expected gains from new ICTs\". In short, the desired impact and \"the end justifies the definition\" of the digital divide.\n\nDuring the mid-1990s the US Department of Commerce, National Telecommunications & Information Administration (NTIA) began publishing reports about the Internet and access to and usage of the resource. The first of three reports is entitled \"Falling Through the Net: A Survey of the ‘Have Nots’ in Rural and Urban America\" (1995), the second is \"Falling Through the Net II: New Data on the Digital Divide\" (1998), and the final report \"Falling Through the Net: Defining the Digital Divide\" (1999). The NTIA’s final report attempted to clearly define the term digital divide; \"the digital divide—the divide between those with access to new technologies and those without—is now one of America's leading economic and civil rights issues. This report will help clarify which Americans are falling further behind, so that we can take concrete steps to redress this gap.\" Since the introduction of the NTIA reports, much of the early, relevant literature began to reference the NTIA’s digital divide definition. The digital divide is commonly defined as being between the \"haves\" and \"have-nots.\"\n\nThe Facebook Divide, a concept derived from the \"digital divide\", is the phenomenon with regard to access to, use of, or impact of Facebook on individual society and among societies. It is suggested at the International Conference on Management Practices for the New Economy (ICMAPRANE-17) on February 10–11, 2017. Additional concepts of Facebook Native and Facebook Immigrants are suggested at the conference. The Facebook Divide, Facebook native, Facebook immigrants, and Facebook left-behind are concepts for social and business management research. Facebook Immigrants are utilizing Facebook for their accumulation of both bonding and bridging social capital. These Facebook Native, Facebook Immigrants, and Facebook left-behind induced the situation of Facebook inequality. In February 2018, the Facebook Divide Index was introduced at the ICMAPRANE conference in Noida, India, to illustrate the Facebook Divide phenomenon.\n\nOvercoming the divide \n\nAn individual must be able to connect in order to achieve enhancement of social and cultural capital as well as achieve mass economic gains in productivity. Therefore, access is a necessary (but not sufficient) condition for overcoming the digital divide. Access to ICT meets significant challenges that stem from income restrictions. The borderline between ICT as a necessity good and ICT as a luxury good is roughly around the \"magical number\" of US$10 per person per month, or US$120 per year, which means that people consider ICT expenditure of US$120 per year as a basic necessity. Since more than 40% of the world population lives on less than US$2 per day, and around 20% live on less than US$1 per day (or less than US$365 per year), these income segments would have to spend one third of their income on ICT (120/365 = 33%). The global average of ICT spending is at a mere 3% of income. Potential solutions include driving down the costs of ICT, which includes low cost technologies and shared access through Telecentres.\n\nFurthermore, even though individuals might be capable of accessing the Internet, many are thwarted by barriers to entry such as a lack of means to infrastructure or the inability to comprehend the information that the Internet provides. Lack of adequate infrastructure and lack of knowledge are two major obstacles that impede mass connectivity. These barriers limit individuals' capabilities in what they can do and what they can achieve in accessing technology. Some individuals have the ability to connect, but they do not have the knowledge to use what information ICTs and Internet technologies provide them. This leads to a focus on capabilities and skills, as well as awareness to move from mere access to effective usage of ICT.\n\nThe United Nations is aiming to raise awareness of the divide by way of the World Information Society Day which has taken place yearly since May 17, 2006. It also set up the Information and Communications Technology (ICT) Task Force in November 2001. Later UN initiatives in this area are the World Summit on the Information Society, which was set up in 2003, and the Internet Governance Forum, set up in 2006.\n\nIn the year 2000, the United Nations Volunteers (UNV) programme launched its Online Volunteering service, which uses ICT as a vehicle for and in support of volunteering. It constitutes an example of a volunteering initiative that effectively contributes to bridge the digital divide. ICT-enabled volunteering has a clear added value for development. If more people collaborate online with more development institutions and initiatives, this will imply an increase in person-hours dedicated to development cooperation at essentially no additional cost. This is the most visible effect of online volunteering for human development.\n\nSocial media websites serve as both manifestations of and means by which to combat the digital divide. The former describes phenomena such as the divided users demographics that make up sites such as Facebook and Myspace or Word Press and Tumblr. Each of these sites host thriving communities that engage with otherwise marginalized populations. An example of this is the large online community devoted to Afrofuturism, a discourse that critiques dominant structures of power by merging themes of science fiction and blackness. Social media brings together minds that may not otherwise meet, allowing for the free exchange of ideas and empowerment of marginalized discourses.\n\nAttempts to bridge the digital divide include a program developed in Durban, South Africa, where very low access to technology and a lack of documented cultural heritage has motivated the creation of an \"online indigenous digital library as part of public library services.\" This project has the potential to narrow the digital divide by not only giving the people of the Durban area access to this digital resource, but also by incorporating the community members into the process of creating it.\n\nTo address the divide The Gates Foundation began the Gates Library Initiative. The Gates Foundation focused on providing more than just access, they placed computers and provided training in libraries. In this manner if users began to struggle while using a computer, the user was in a setting where assistance and guidance was available. Further, the Gates Library Initiative was \"modeled on the old-fashioned life preserver: The support needs to be around you to keep you afloat.\"\n\nIn nations where poverty compounds effects of the digital divide, programs are emerging to counter those trends. Prior conditions in Kenya—lack of funding, language and technology illiteracy contributed to an overall lack of computer skills and educational advancement for those citizens. This slowly began to change when foreign investment began. In the early 2000s, The Carnegie Foundation funded a revitalization project through the Kenya National Library Service (KNLS). Those resources enabled public libraries to provide information and communication technologies (ICT) to their patrons. In 2012, public libraries in the Busia and Kiberia communities introduced technology resources to supplement curriculum for primary schools. By 2013, the program expanded into ten schools.\n\nCommunity Informatics (CI) provides a somewhat different approach to addressing the digital divide by focusing on issues of \"use\" rather than simply \"access\". CI is concerned with ensuring the opportunity not only for ICT access at the community level but also, according to Michael Gurstein, that the means for the \"effective use\" of ICTs for community betterment and empowerment are available. Gurstein has also extended the discussion of the digital divide to include issues around access to and the use of \"open data\" and coined the term \"data divide\" to refer to this issue area.\n\nOnce an individual is connected, Internet connectivity and ICTs can enhance his or her future social and cultural capital. Social capital is acquired through repeated interactions with other individuals or groups of individuals. Connecting to the Internet creates another set of means by which to achieve repeated interactions. ICTs and Internet connectivity enable repeated interactions through access to social networks, chat rooms, and gaming sites. Once an individual has access to connectivity, obtains infrastructure by which to connect, and can understand and use the information that ICTs and connectivity provide, that individual is capable of becoming a \"digital citizen\".\n\nIn the United States, research provided by Sungard Availability Services notes a direct correlation between a company's access to technological advancements and its overall success in bolstering the economy. The study, which includes over 2,000 IT executives and staff officers, indicates that 69 percent of employees feel they do not have access to sufficient technology in order to make their jobs easier, while 63 percent of them believe the lack of technological mechanisms hinders their ability to develop new work skills. Additional analysis provides more evidence to show how the digital divide also affects the economy in places all over the world. A BCG Report suggests that in countries like Sweden, Switzerland, and the U.K., the digital connection among communities is made easier, allowing for their populations to obtain a much larger share of the economies via digital business. In fact, in these places, populations hold shares approximately 2.5 percentage points higher. During a meeting with the United Nations a Bangladesh representative expressed his concern that poor and undeveloped countries would be left behind due to a lack of funds to bridge the digital gap.\n\nThe digital divide also impacts children's ability to learn and grow in low-income school districts. Without Internet access, students are unable to cultivate necessary tech skills in order to understand today's dynamic economy. Federal Communication Commission's Broadband Task Force created a report showing that about 70% of teachers give students homework that demand access to broadband. Even more, approximately 65% of young scholars use the Internet at home to complete assignments as well as connect with teachers and other students via discussion boards and shared files. A recent study indicates that practically 50% of students say that they are unable to finish their homework due to an inability to either connect to the Internet, or in some cases, find a computer. This has led to a new revelation: 42% of students say they received a lower grade because of this disadvantage. Finally, according to research conducted by the Center for American Progress, \"if the United States were able to close the educational achievement gaps between native-born white children and black and Hispanic children, the U.S. economy would be 5.8 percent—or nearly $2.3 trillion—larger in 2050\".\n\nFurthermore, according to the 2012 Pew Report \"Digital Differences\", a mere 62% of households who make less than $30,000 a year use the Internet, while 90% of those making between $50,000 and $75,000 had access. Studies also show that only 51% of Hispanics and 49% of African Americans have high-speed Internet at home. This is compared to the 66% of Caucasians that too have high-speed Internet in their households. Overall, 10% of all Americans don't have access to high-speed Internet, an equivalent of almost 34 million people. Supplemented reports from the Guardian demonstrate the global effects of limiting technological developments in poorer nations, rather than simply the effects in the United States. Their study shows that the rapid digital expansion excludes those who find themselves in the lower class. 60% of the world's population, almost 4 billion people, have no access to the Internet and are thus left worse off.\n\nSince gender, age, racial, income, and educational gaps in the digital divide have lessened compared to past levels, some researchers suggest that the digital divide is shifting from a gap in access and connectivity to ICTs to a knowledge divide. A knowledge divide concerning technology presents the possibility that the gap has moved beyond access and having the resources to connect to ICTs to interpreting and understanding information presented once connected.\n\nThe second-level digital divide, also referred to as the production gap, describes the gap that separates the consumers of content on the Internet from the producers of content. As the technological digital divide is decreasing between those with access to the Internet and those without, the meaning of the term digital divide is evolving. Previously, digital divide research has focused on accessibility to the Internet and Internet consumption. However, with more and more of the population with access to the Internet, researchers are examining how people use the Internet to create content and what impact socioeconomics are having on user behavior.\nNew applications have made it possible for anyone with a computer and an Internet connection to be a creator of content, yet the majority of user generated content available widely on the Internet, like public blogs, is created by a small portion of the Internet using population. Web 2.0 technologies like Facebook, YouTube, Twitter, and Blogs enable users to participate online and create content without having to understand how the technology actually works, leading to an ever-increasing digital divide between those who have the skills and understanding to interact more fully with the technology and those who are passive consumers of it. Many are only nominal content creators through the use of Web 2.0, posting photos and status updates on Facebook, but not truly interacting with the technology.\n\nSome of the reasons for this production gap include material factors like the type of Internet connection one has and the frequency of access to the Internet. The more frequently a person has access to the Internet and the faster the connection, the more opportunities they have to gain the technology skills and the more time they have to be creative.\n\nOther reasons include cultural factors often associated with class and socioeconomic status. Users of lower socioeconomic status are less likely to participate in content creation due to disadvantages in education and lack of the necessary free time for the work involved in blog or web site creation and maintenance. Additionally, there is evidence to support the existence of the second-level digital divide at the K-12 level based on how educators' use technology for instruction. Schools' economic factors have been found to explain variation in how teachers use technology to promote higher-order thinking skills.\n\nThe global digital divide describes global disparities, primarily between developed and developing countries, in regards to access to computing and information resources such as the Internet and the opportunities derived from such access. As with a smaller unit of analysis, this gap describes an inequality that exists, referencing a global scale.\n\nThe Internet is expanding very quickly, and not all countries—especially developing countries—are able to keep up with the constant changes. The term \"digital divide\" doesn't necessarily mean that someone doesn’t have technology; it could mean that there is simply a difference in technology. These differences can refer to, for example, high-quality computers, fast Internet, technical assistance, or telephone services. The difference between all of these is also considered a gap.\n\nIn fact, there is a large inequality worldwide in terms of the distribution of installed telecommunication bandwidth. In 2014 only 3 countries (China, US, Japan) host 50% of the globally installed bandwidth potential (see pie-chart Figure on the right). This concentration is not new, as historically only 10 countries have hosted 70–75% of the global telecommunication capacity (see Figure). The U.S. lost its global leadership in terms of installed bandwidth in 2011, being replaced by China, which hosts more than twice as much national bandwidth potential in 2014 (29% versus 13% of the global total).\n\nThe global digital divide is a special case of the digital divide, the focus is set on the fact that \"Internet has developed unevenly throughout the world\" causing some countries to fall behind in technology, education, labor, democracy, and tourism. The concept of the digital divide was originally popularized in regard to the disparity in Internet access between rural and urban areas of the United States of America; the \"global\" digital divide mirrors this disparity on an international scale.\n\nThe global digital divide also contributes to the inequality of access to goods and services available through technology. Computers and the Internet provide users with improved education, which can lead to higher wages; the people living in nations with limited access are therefore disadvantaged. This global divide is often characterized as falling along what is sometimes called the north-south divide of \"northern\" wealthier nations and \"southern\" poorer ones.\n\nSome people argue that basic necessities need to be considered before achieving digital inclusion, such as an ample food supply and quality health care. Minimizing the global digital divide requires considering and addressing the following types of access:\nInvolves \"the distribution of ICT devices per capita…and land lines per thousands\". Individuals need to obtain access to computers, landlines, and networks in order to access the Internet. This access barrier is also addressed in Article 21 of the Convention on the Rights of Persons with Disabilities by the United Nations. \nThe cost of ICT devices, traffic, applications, technician and educator training, software, maintenance and infrastructures require ongoing financial means.\nFinancial access and \"the levels of household income play a significant role in widening the gap\" \nEmpirical tests have identified that several socio-demographic characteristics foster or limit ICT access and usage. Among different countries, educational levels and income are the most powerful explanatory variables, with age being a third one. \n\nWhile a Global Gender Gap in access and usage of ICT's exist, empirical evidence show that this due to unfavorable conditions with respect to employment, education and income and not to technophobia or lower ability. On the contrary, in the contexts under study, women with the prerequsites for access and usage turn out to be more active users of digital tools than men.\nIn order to use computer technology, a certain level of information literacy is needed. Further challenges include information overload and the ability to find and use reliable information. \nComputers need to be accessible to individuals with different learning and physical abilities including complying with Section 508 of the Rehabilitation Act as amended by the Workforce Investment Act of 1998 in the United States. \nIn illustrating institutional access, Wilson states \"the numbers of users are greatly affected by whether access is offered only through individual homes or whether it is offered through schools, community centers, religious institutions, cybercafés, or post offices, especially in poor countries where computer access at work or home is highly limited\". \nGuillen & Suarez argue that \"democratic political regimes enable a faster growth of the Internet than authoritarian or totalitarian regimes\". The Internet is considered a form of e-democracy and attempting to control what citizens can or cannot view is in contradiction to this. Recently situations in Iran and China have denied people the ability to access certain websites and disseminate information. Iran has prohibited the use of high-speed Internet in the country and has removed many satellite dishes in order to prevent the influence of Western culture, such as music and television.\nMany experts claim that bridging the digital divide is not sufficient and that the images and language needed to be conveyed in a language and images that can be read across different cultural lines. A 2013 study conducted by Pew Research Center noted how participants taking the survey in Spanish were nearly twice as likely to not use the internet.\n\nIn the early 21st century, residents of developed countries enjoy many Internet services which are not yet widely available in developing countries, including:\n\n\nThere are four specific arguments why it is important to \"bridge the gap\":\n\n\nWhile these four arguments are meant to lead to a solution to the digital divide, there are a couple other components that need to be considered. The first one is rural living versus suburban living. Rural areas used to have very minimal access to the Internet, for example. However, nowadays, power lines and satellites are used to increase the availability in these areas. Another component to keep in mind is disabilities. Some people may have the highest quality technologies, but a disability they have may keep them from using these technologies to their fullest extent.\n\nUsing previous studies (Gamos, 2003; Nsengiyuma & Stork, 2005; Harwit, 2004 as cited in James), James asserts that in developing countries, \"internet use has taken place overwhelmingly among the upper-income, educated, and urban segments\" largely due to the high literacy rates of this sector of the population. As such, James suggests that part of the solution requires that developing countries first build up the literacy/language skills, computer literacy, and technical competence that low-income and rural populations need in order to make use of ICT.\n\nIt has also been suggested that there is a correlation between democrat regimes and the growth of the Internet. One hypothesis by Gullen is, \"The more democratic the polity, the greater the Internet use...Government can try to control the Internet by monopolizing control\" and Norris \"et al.\" also contends, \"If there is less government control of it, the Internet flourishes, and it is associated with greater democracy and civil liberties.\n\nFrom an economic perspective, Pick and Azari state that \"in developing nations…foreign direct investment (FDI), primary education, educational investment, access to education, and government prioritization of ICT as all important\". Specific solutions proposed by the study include: \"invest in stimulating, attracting, and growing creative technical and scientific workforce; increase the access to education and digital literacy; reduce the gender divide and empower women to participate in the ICT workforce; emphasize investing in intensive Research and Development for selected metropolitan areas and regions within nations\".\n\nThere are projects worldwide that have implemented, to various degrees, the solutions outlined above. Many such projects have taken the form of Information Communications Technology Centers (ICT centers). Rahnman explains that \"the main role of ICT intermediaries is defined as an organization providing effective support to local communities in the use and adaptation of technology. Most commonly an ICT intermediary will be a specialized organization from outside the community, such as a non-governmental organization, local government, or international donor. On the other hand, a social intermediary is defined as a local institution from within the community, such as a community-based organization.\n\nOther proposed solutions that the Internet promises for developing countries are the provision of efficient communications within and among developing countries, so that citizens worldwide can effectively help each other to solve their own problems. Grameen Banks and Kiva loans are two microcredit systems designed to help citizens worldwide to contribute online towards entrepreneurship in developing communities. Economic opportunities range from entrepreneurs who can afford the hardware and broadband access required to maintain Internet cafés to agribusinesses having control over the seeds they plant.\n\nAt the Massachusetts Institute of Technology, the IMARA organization (from Swahili word for \"power\") sponsors a variety of outreach programs which bridge the Global Digital Divide. Its aim is to find and implement long-term, sustainable solutions which will increase the availability of educational technology and resources to domestic and international communities. These projects are run under the aegis of the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and staffed by MIT volunteers who give training, install and donate computer setups in greater Boston, Massachusetts, Kenya, Indian reservations the American Southwest such as the Navajo Nation, the Middle East, and Fiji Islands. The CommuniTech project strives to empower underserved communities through sustainable technology and education. According to Dominik Hartmann of the MIT's Media Lab, interdisciplinary approaches are needed to bridge the global digital divide.\n\nBuilding on the premise that any effective solution must be decentralized, allowing the local communities in developing nations to generate their own content, one scholar has posited that social media—like Facebook, YouTube, and Twitter—may be useful tools in closing the divide. As Amir Hatem Ali suggests, \"the popularity and generative nature of social media empower individuals to combat some of the main obstacles to bridging the digital divide\". Facebook’s statistics reinforce this claim. According to Facebook, more than seventy-five percent of its users reside outside of the US. Moreover, more than seventy languages are presented on its website. The reasons for the high number of international users are due to many the qualities of Facebook and other social media. Amongst them, are its ability to offer a means of interacting with others, user-friendly features, and the fact that most sites are available at no cost. The problem with social media, however, is that it can be accessible, provided that there is physical access. Nevertheless, with its ability to encourage digital inclusion, social media can be used as a tool to bridge the global digital divide.\n\nSome cities in the world have started programs to bridge the digital divide for their residents, school children, students, parents and the elderly. One such program, founded in 1996, was sponsored by the city of Boston and called the Boston Digital Bridge Foundation. It especially concentrates on school children and their parents, helping to make both equally and similarly knowledgeable about computers, using application programs, and navigating the Internet.\n\nFree Basics is a partnership between social networking services company Facebook and six companies (Samsung, Ericsson, MediaTek, Opera Software, Nokia and Qualcomm) that plans to bring affordable access to selected Internet services to less developed countries by increasing efficiency, and facilitating the development of new business models around the provision of Internet access. In the whitepaper realised by Facebook's founder and CEO Mark Zuckerberg, connectivity is asserted as a \"human right\", and Internet.org is created to improve Internet access for people around the world.\n\n\"Free Basics provides people with access to useful services on their mobile phones in markets where internet access may be less affordable. The websites are available for free without data charges, and include content about news, employment, health, education and local information etc. By introducing people to the benefits of the internet through these websites, we hope to bring more people online and help improve their lives.\"\n\nHowever, Free Basics is also accused of violating net neutrality for limiting access to handpicked services. Despite a wide deployment in numerous countries, it has been met with heavy resistance notably in India where the Telecom Regulatory Authority of India eventually banned it in 2016.\n\nSeveral projects to bring internet to the entire world with a satellite constellation have been devised in the last decade, one of these being Starlink by Elon Musk's company SpaceX. Unlike Free Basics, it would provide people with a full internet access and would not be limited to a few selected services. In the same week Starlink was announced, serial-entrepreneur Richard Branson announced his own project OneWeb, a similar constellation with approximately 700 satellites that has already procured communication frequency licenses for their broadcast spectrum and could possibly be operational as early as in 2019.\n\nThe biggest hurdle of these projects is the astronomical financial and logistical costs of launching so many satellites. After the failure of previous satellite-to-consumer space ventures, satellite industry consultant Roger Rusch said \"It's highly unlikely that you can make a successful business out of this.\" Musk has publicly acknowledged this business reality, and indicated in mid-2015 that while endeavoring to develop this technically-complicated space-based communication system he wants to avoid overextending the company and stated that they are being measured in the pace of development.\n\nOne Laptop Per Child (OLPC) is another attempt to narrow the digital divide. This organization, founded in 2005, provides inexpensively produced \"XO\" laptops (dubbed the \"$100 laptop\", though actual production costs vary) to children residing in poor and isolated regions within developing countries. Each laptop belongs to an individual child and provides a gateway to digital learning and Internet access. The XO laptops are designed to withstand more abuse than higher-end machines, and they contain features in context to the unique conditions that remote villages present. Each laptop is constructed to use as little power as possible, have a sunlight-readable screen, and is capable of automatically networking with other XO laptops in order to access the Internet—as many as 500 machines can share a single point of access.\n\nSeveral of the 67 principles adopted at the World Summit on the Information Society convened by the United Nations in Geneva in 2003 directly address the digital divide:\n\n\n\n\n"}
{"id": "15459738", "url": "https://en.wikipedia.org/wiki?curid=15459738", "title": "Equipment manager", "text": "Equipment manager\n\nAn equipment manager is the person in charge of equipment used by a business or organization. Their duties include purchasing, maintenance, repair, inventory, transportation, storage, cleaning, and liquidation. They are responsible for providing the proper equipment for the job, either on-site, or off-site. In sports, an equipment manager is a person who is in charge of a sports team's equipment. In professional and collegiate sports, this is usually a full-time job and includes transportation, laundry, repairs and regular service (such as sharpening of skates for ice hockey).\n\nIn association football, the kit manager or kit man oversees the players' equipment.\n\nIn golf, the equipment manager oversees the fleet of equipment used on the golf course for turf management. This may include:\nAnd numerous other pieces of equipment a Golf Course or the Turf Care industry employs. \n\nThe term has also been used less frequently as a synonym with \"Fleet Manager\" (fleet management).\n\n\n"}
{"id": "20206544", "url": "https://en.wikipedia.org/wiki?curid=20206544", "title": "Falling Number", "text": "Falling Number\n\nThe Falling Number (FN), also referred to as the Hagberg Number, is the internationally standardized (ICC 107/1, ISO 3093-2004, AACC 56-81B) and most popular method for determining sprout damage. With the Falling Number test, so-called weather or sprout damaged wheat or rye, which is disastrous for bread making quality, could be detected at the grain silo intake within a few minutes. \n\nSprouting or pre-harvest germination is caused by damp or rainy weather conditions during the final stage of maturation of the crop. The germination causes an accelerated production of the starch degrading enzyme alpha-amylase. Severely sprouted grain kernels can contain several thousand times the amount of enzyme of sound unsprouted kernels. Because of this, very low levels of severely sprouted kernels mixed into sound wheat can cause the entire lot to exhibit significant amylase activity. Since its introduction in the early 1960s, the FN test has become a world standard in the grain and flour milling industries for measuring alpha-amylase activity in wheat, durum wheat, triticale, rye and barley, as well as milled products made from these grains.\n\nThe Falling Number method was developed at the end of the 1950s by Sven Hagberg and his co-worker Harald Perten, both at the Cereal Laboratory of the Swedish Institute for the Crafts and Industries.\n\nThe Falling Number method is uncomplicated, but requires an apparatus which follows the international standards. Such an apparatus consists of a water bath, a test tube, a stirring rod, and a stirring device. The test was performed manually when first employed, test instrumentation today is mostly automated.\n\nTo analyze a grain sample it first needs to be ground to a powder; a flour sample can be analyzed as is. The sample is put into the test tube; distilled water is added, and the tube is then shaken vigorously to achieve a homogeneous mix. The tube is then placed in the boiling water bath, and the operator begins to stir the sample. Simultaneously the starch begins to gelatinize and the slurry becomes more viscous. The mixing ensures the gelatinization is homogeneous in the slurry, crucial for consistent test results. An additional effect of the high temperature is that the alpha-amylase enzyme contained in the grain begins to break the starch down into glucose and maltose, thereby reducing the viscosity of the slurry. The amount of starch break-down is directly proportionate to the alpha-amylase activity, meaning that the higher the activity of the alpha-amylase, the lower the viscosity will be.\n\nAfter 60 seconds of mixing, the stirrer is dropped from the top of the test tube, and the operator measures the time it takes for the stirrer to reach the bottom. That time, measured in seconds, is the Falling Number. When the stirrer is dropped, its speed and thus the time it takes it to fall to the bottom, will be determined by the viscosity of the slurry. In other words, the more sprouted the grain was the higher the alpha-amylase activity will be. The higher the alpha-amylase activity the lower the viscosity of the slurry. The lower the viscosity of the slurry the faster the stirrer will fall to the bottom. That is why more sprouted grain results in a lower Falling Number as Falling Number is the time it takes the stirrer to fall to the bottom.\n"}
{"id": "18526787", "url": "https://en.wikipedia.org/wiki?curid=18526787", "title": "Fragment-based lead discovery", "text": "Fragment-based lead discovery\n\nFragment-based lead discovery (FBLD) also known as fragment-based drug discovery (FBDD) is a method used for finding lead compounds as part of the drug discovery process. Fragments are small organic molecules which are small in size and low in molecular weight. It is based on identifying small chemical fragments, which may bind only weakly to the biological target, and then growing them or combining them to produce a lead with a higher affinity. FBLD can be compared with high-throughput screening (HTS). In HTS, libraries with up to millions of compounds, with molecular weights of around 500 Da, are screened, and nanomolar binding affinities are sought. In contrast, in the early phase of FBLD, libraries with a few thousand compounds with molecular weights of around 200 Da may be screened, and millimolar affinities can be considered useful. FBLD is a technique being used in research for discovering novel potent inhibitors.\n\nIn analogy to the rule of five, it has been proposed that ideal fragments should follow the 'rule of three' (molecular weight < 300, ClogP < 3, the number of hydrogen bond donors and acceptors each should be < 3 and the number of rotatable bonds should be < 3). Since the fragments have relatively low affinity for their targets, they must have high water solubility so that they can be screened at higher concentrations.\n\nIn fragment-based drug discovery, the low binding affinities of the fragments pose significant challenges for screening. Many biophysical techniques have been applied to address this issue. In particular, ligand-observe nuclear magnetic resonance (NMR) methods such as water-ligand observed via gradient spectroscopy (waterLOGSY), saturation transfer difference spectroscopy (STD-NMR), F NMR spectroscopy and inter-ligand Overhauser effect (ILOE) spectroscopy, protein-observe NMR methods such as H-N heteronuclear single quantum coherence (HSQC) that utilises isotopically-labelled proteins, surface plasmon resonance (SPR) and isothermal titration calorimetry (ITC) are routinely-used for ligand screening and for the quantification of fragment binding affinity to the target protein.\n\nOnce a fragment (or a combination of fragments) have been identified, protein X-ray crystallography is used to obtain structural models of the protein-fragment(s) complexes. Such information can then be used to guide organic synthesis for high-affinity protein ligands and enzyme inhibitors.\n\nAdvantages of screening low molecular weight fragment based libraries over traditional higher molecular weight chemical libraries are several. These include:\n\n\n\n"}
{"id": "1185274", "url": "https://en.wikipedia.org/wiki?curid=1185274", "title": "Friend-to-friend", "text": "Friend-to-friend\n\nA friend-to-friend (or F2F) computer network is a type of peer-to-peer network in which users only make direct connections with people they know. Passwords or digital signatures can be used for authentication.\n\nUnlike other kinds of private P2P, users in a friend-to-friend network cannot find out who else is participating beyond their own circle of friends, so F2F networks can grow in size without compromising their users' anonymity. Retroshare, WASTE, GNUnet, Freenet and OneSwarm are examples of software that can be used to build F2F networks, though RetroShare is the only one of these configured for friend-to-friend operation by default.\n\nMany F2F networks support indirect anonymous or pseudonymous communication between users who do not know or trust one another. For example, a node in a friend-to-friend overlay can automatically forward a file (or a request for a file) anonymously between two friends, without telling either of them the other's name or IP address. These friends can in turn automatically forward the same file (or request) to their own friends, and so on.\n\nDan Bricklin coined the term \"friend-to-friend network\" in 2000.\n\n\n\n"}
{"id": "15639656", "url": "https://en.wikipedia.org/wiki?curid=15639656", "title": "Fuzzy transportation", "text": "Fuzzy transportation\n\nThe aim of fuzzy transportation is to find the least transportation cost of some commodities through a capacitated network when the supply and demand of nodes and the capacity and cost of edges are represented as fuzzy numbers. This problem is a new branch in combinatorial optimization and network flow problems. Combinatorial algorithms can be provided to solve fuzzy transportation problem to find the fuzzy optimal flow(s). Such methods are capable of handling the decision maker's risk taking. Some application of such standpoint were presented in industries. Liu and Kao pursued this attempt to find better solution for this problem Some of the applications have been presented in public transit network design, traffic assignment, Hazmat transportation, Network Design, and distribution systems.\nThe theory of this problem has been also presented in the literature..\n\nIt is interesting to check that which methods in traditional fuzzy optimization problem can be extended to combinatorial optimization problems e.g., transformation that they maintain the nice structure of problem. Then, valuable algorithms can be proposed for fuzzy combinatorial optimization to take the uncertainty of real problems into account.\n\nBy using fuzzy transportation, it is a reasonable attempt to find special solutions for hazardous material transportation because of the possibility of implementing the optimistic and pessimistic concepts into account. In the application of fuzzy optimization for transportation studies has been discussed together with some references to the recent researches.\n"}
{"id": "11347077", "url": "https://en.wikipedia.org/wiki?curid=11347077", "title": "Gateleg table", "text": "Gateleg table\n\nA gateleg table is a type of furniture first introduced in England in the 16th century. The table top has a fixed section and one or two hinged leaves, which, when not in use, fold down below the fixed section to hang vertically. \n\nAs such, gateleg tables are a subset of the type known as a dropleaf. The hinged section, or flap, was supported on pivoted legs joined at the top and bottom by stretchers constituting a gate. Large flaps had two supports, which had the advantage of providing freer leg space in the centre. The earliest gateleg tables of the 16th and 17th century were typically made of oak.\n\n"}
{"id": "25212749", "url": "https://en.wikipedia.org/wiki?curid=25212749", "title": "Global e-Schools and Communities Initiative", "text": "Global e-Schools and Communities Initiative\n\nThe Global e-Schools and Communities Initiative is an international not-for-profit organisation providing demand-driven assistance to developing countries seeking to harness the potential of Information and Communication Technologies (ICT) to improve their education systems.\n\nGeSCI was established in 2003, borne out of the work of the United Nations Information and Communication Technologies Task Force which identified education as an area in critical need of development, and one where ICT has the potential to make positive impacts. The UN ICT Task Force approved a proposal for a UN-affiliated organisation to provide demand-driven assistance to developing countries seeking to harness the potential of ICT to improve the quality of teaching and learning in primary and secondary education. GeSCI is governed from Dublin, with the support of Irish Aid, Swedish International Development Cooperation Agency (Sida), Swiss Agency for Development and Cooperation (SDC) and the Ministry for Foreign Affairs of Finland.\n\nEducation is considered one of the cornerstones of social economic development. Research has shown that education contributes to poverty reduction and increased economic growth, which in turn leads to an increase in the individual's standard of living; enables the individual to participate in wealth generating activities, leads to the creation of employment and the overall development of society. However, the traditional role of education to promote socio-economic development is being re-examined as greater emphasis is placed on access to education, quality and outcomes of the education system.\n\nThe education sector is seen as the natural source for the creation of technological literacy and the development of new technological skills as well as other skills that are needed in the new millennium, like problem solving skills, collaboration skills, critical reading and information retrieval, etc. For new technologies like ICT, the creation of these new skills has meant the introduction of ICT into educational institutions and the introduction of computer literacy or media literacy courses as well as new teaching and learning methods.\n\nThe relationship between ICT, education and development in a knowledge economy is increasingly being captured by developing country governments through their poverty reduction strategies \n\nThese efforts have been spurred on by the setting of internationally agreed development goals, such as the MDGs and the Education For All (EFA) goals. Recent monitoring efforts have revealed that several countries, mostly in sub-Saharan Africa (SSA) and the Arab states will find it difficult to approach Universal Primary Completion in the coming decade, while participation rates for secondary education are lowest in SSA (25%), South and West Asia (53%) and the Arab State (66%). This has led to initiatives such as the Fast Track Initiative (FTI). For countries to achieve the MDG and EFA goals, UNESCO notes that there will be a need to, not only allocate more resources to education, but to have these resources planned for and used more effectively. Many donors, including the World Bank, now also acknowledge that ICT can be leveraged to solve some of these challenges facing the education sector.\n"}
{"id": "9803278", "url": "https://en.wikipedia.org/wiki?curid=9803278", "title": "Goethals Medal", "text": "Goethals Medal\n\nThe Goethals Medal is a national award given annually by the Society of American Military Engineers (SAME) to a registered engineer who is also a member of SAME prior to nomination. The nominated engineer must have made \"eminent and notable contributions in engineering, design, or construction in the past five years.\"\n\nThe award is named in honor of General George Washington Goethals, a civil engineer and United States Army officer best known for his work as Chief engineer on the Panama Canal and his service in WWI as Acting Quartermaster General.\n\n\n"}
{"id": "6233768", "url": "https://en.wikipedia.org/wiki?curid=6233768", "title": "Grande Baroque", "text": "Grande Baroque\n\nGrande Baroque is a sterling silver tableware pattern.\n\nGrande Baroque was designed by William S. Warren in 1941, – in his words – “To reflect the very essence of merriment and adventure, of artistic progress.” \n\nWilliam S. Warren spent four years developing this pattern.\n\nThe Grande Baroque pattern was part four of a six-part pattern release, released from 1934 to 1950, by Wallace Silversmiths, called the “Third Dimension Beauty collection”, all of them designed by William S. Warren. These patterns are called “three Dimension” because the design of these patterns is apparent from the front, back, or profile.\n\nGrand Baroque also was one of the five patterns profiled in the book \"Wallace Beauty Moods in Silver\", written by William S. Warren in 1947, to discuss five of the six \"Three Dimension\" designs.\n\nThe design crowned by the acanthus leaf (acanthus leaf), includes classic symbols of the Renaissance period, five petaled flowers on the spoons, a narcissus on the forks, and a rose on the knives.\n\nThe Grande Baroque flatware is produced by Wallace Silversmiths.\n\nOriginally conceived as only a flatware pattern, hollow ware was soon added to the line. As the demand for this pattern increased, a Golden Grand Baroque was introduced. \n\nBesides the traditional place settings in lunch, dinner, and Continental size, there are now over a hundred flatware pieces in this pattern.\n"}
{"id": "8940580", "url": "https://en.wikipedia.org/wiki?curid=8940580", "title": "Hellburners", "text": "Hellburners\n\nHellburners (Dutch: \"hellebranders\") were specialised fireships used in the Siege of Antwerp (1584-1585) during the Eighty Years' War between the Dutch rebels and the Habsburgs. They were floating bombs, also called \"Antwerp Fire\", and did immense damage to the Spanish besiegers. Hellburners have been described as an early form of weapons of mass destruction.\n\nThe hellburners were constructed by the Italian engineer Federigo Giambelli, who had been hired and subsidised by Elizabeth I of England, unofficially supporting the rebels, to assist the city. In the winter of 1585, Antwerp was besieged by the army of Alexander Farnese, the commander of the Habsburg forces in the Spanish Netherlands, who had constructed a ship bridge over the River Scheldt near Kalloo between Antwerp and the sea, to starve the population by blockade; it had been completed on 25 February. To supply the city it was imperative to destroy the ship bridge.\n\nGiambelli first proposed to use three medium-sized merchantmen, the \"Oranje\", \"Post\" and \"Gulden Leeuw\", but this was refused, only two smaller vessels being made available: the \"Fortuyn\" (\"Fortune\") and \"Hoop\" (\"Hope\") of about seventy tons. The innovative part of the project consisted in the \"Hoop\" employing a fuse consisting of a combined clockwork and flintlock mechanism provided by an Antwerp watchmaker, Bory; the \"Fortuyn\" used a delayed fuse mechanism.\n\nTo ensure destruction, very large charges were used. To intensify and channel the explosion, an oblong \"fire chamber\" was constructed on each ship, 1 metre in diameter. The bay was fitted with a brick floor, 30 centimetres thick and 5 metres wide; the walls of the chamber were 1.5 metres thick; the roof consisted of old tombstones, stacked vertically and sealed with lead. The chambers with a length of 12 metres were each filled with a charge of about 7,000 pounds of high quality corned gunpowder. On top of the chambers a mixture of rocks and iron shards and other objects was placed, again covered in slabs; the spaces next to the chambers were likewise filled. The whole was covered with a conventional wooden deck.\n\nThe two fireships were successfully used in the night of 4–5 April 1585. Giambelli had prepared 32 normal fireships to be first launched in several waves to deceive the Spaniards. In fact the commander supervising the operation, Vice-Admiral Jacob Jacobsen, set all ships on their course in quick succession, from fort \"Boerenschans\", the hellburners last. The current and ebb tide carried the ships towards the bridge. The decks of the hellburners were piled with wood and small charges with slow fuses, which gave the impression that they were conventional fireships, causing the Spanish troops to try to extinguish the fire.\n\nThe \"Fortuyn\" ran ashore on the west river bank some distance from the bridge and its, probably only partial, explosion did little damage to the Spanish forces, but the \"Hoop\" drifted along the same bank between the river shore and a protective row of anchored ships forming a raft in front of the main bridge and touched the latter near the junction of the fixed wooden shore structure and the attached ships. When the time bomb aboard the \"Hoop\" exploded, about eight hundred troops were killed, the sconce \"Santa Maria\" was devastated, and the ship bridge was ripped apart over a distance of 60 metres; the blast was heard in a 80-kilometer radius. Farnese was wounded in the explosion. However, the damage to the bridge was quickly repaired, and a rebel relief fleet failed to exploit the opportunity to break through, because it was at first mistakenly thought the attempt at the bridge had been unsuccessful.\n\nLast of all came the two infernal ships, swaying unsteadily with the current; the pilots of course, as they neared the bridge, having noiselessly effected their escape in the skiffs. The slight fire upon the deck scarcely illuminated the dark phantom-like hulls. Both were carried by the current clear of the raft, which, by a great error of judgment, as it now appeared, on the part of the builders, had only been made to protect the floating portion of the bridge. The 'Fortune' came first, staggering inside the raft, and then lurching clumsily against the dyke, and grounding near Kalloo, without touching the bridge. There was a moment's pause of expectation. At last the slow match upon the deck burned out, and there was a faint and partial explosion, by which little or no damage was produced...\n\nThe troops of Parma, crowding on the palisade, and looking over the parapets, now began to greet the exhibition with peals of derisive laughter. It was but child's play, they thought, to threaten a Spanish army, and a general like Alexander Farnese, with such paltry fire-works as these. Nevertheless all eyes were anxiously fixed upon the remaining fire-ship, or \"hell-burner,\" the 'Hope,' which had now drifted very near the place of its destination. Tearing her way between the raft and the shore, she struck heavily against the bridge on the Kalloo side, close to the block-house at the commencement of the floating portion of the bridge. A thin wreath of smoke was seen curling over a slight and smouldering fire upon her deck...\n\nThe clockwork had been better adjusted than the slow match in the 'Fortune.' Scarcely had Alexander reached the entrance of Saint Mary's Fort, at the end of the bridge, when a horrible explosion was heard. The 'Hope' disappeared, together with the men who had boarded her, and the block-house, against which she had struck, with all its garrison, while a large portion of the bridge, with all the troops stationed upon it, had vanished into air. It was the work of a single instant. The Scheldt yawned to its lowest depth, and then cast its waters across the dykes, deep into the forts, and far over the land. The earth shook as with the throb of a volcano. A wild glare lighted up the scene for one moment, and was then succeeded by pitchy darkness. Houses were toppled down miles away, and not a living thing, even in remote places, could keep its feet. The air was filled with a rain of plough-shares, grave-stones, and marble balls, intermixed with the heads, limbs, and bodies, of what had been human beings. Slabs of granite, vomited by the flaming ship, were found afterwards at a league's distance, and buried deep in the earth. A thousand soldiers were destroyed in a second of time; many of them being torn to shreds, beyond even the semblance of humanity.\n\nRichebourg disappeared, and was not found until several days later, when his body was discovered; doubled around an iron chain, which hung from one of the bridge-boats in the centre of the river. The veteran Robles, Seigneur de Billy, a Portuguese officer of eminent service and high military rank, was also destroyed. Months afterwards, his body was discovered adhering to the timber-work of the bridge, upon the ultimate removal of that structure, and was only recognized by a peculiar gold chain which he habitually wore. Parma himself was thrown to the ground, stunned by a blow on the shoulder from a flying stake. The page, who was behind him, carrying his helmet, fell dead without a wound, killed by the concussion of the air.\n\nThe events in Antwerp gave the hellburners an immediate notoriety; the concept generated enormous interest with military experts all over Europe. The fireships sent against the Spanish Armada on 7 August 1588 in the night before the Battle of Gravelines were taken to be hellburners, because Giambelli was known to be employed by Elizabeth in England at that date, and eight regular warships, much larger than typical fireships of the time, had been sacrificed for the attack. They were actually nowhere near as deadly; the English at that moment even lacking the gunpowder to resupply their ships for regular use, but were successful in breaking the fleet's formation, their mistaken identity contributing to the panic. Giambelli was in fact working on constructing a mined ship beam from masts, costing £2000, to block the Thames against an invasion.\n\n"}
{"id": "19332936", "url": "https://en.wikipedia.org/wiki?curid=19332936", "title": "Heritage turkey", "text": "Heritage turkey\n\nA heritage turkey is one of a variety of strains of domestic turkey which retains historic characteristics that are no longer present in the majority of turkeys raised for consumption since the mid-20th century. Heritage turkeys can be differentiated from other domestic turkeys in that they are biologically capable of being raised in a manner that more closely matches the natural behavior and life cycle of wild turkeys. Heritage turkeys have a relatively long lifespan and a much slower growth rate than turkeys bred for industrial agriculture, and unlike industrially-bred turkeys, can reproduce without artificial insemination.\n\nMore than ten different turkey breeds are classified as heritage turkeys, including the Auburn, Buff, Black, Bourbon Red, Narragansett, Royal Palm, Slate, Standard Bronze, and Midget White. Some prominent chefs, farmers, and food critics have also contended that heritage turkey meat tastes better and is more healthy.\n\nDespite increasing interest in heritage turkeys, they are still a tiny minority, perhaps 25,000 raised annually compared to more than 200,000,000 industrial turkeys and 7,000,000 turkeys in the wild, and most heritage breeds are endangered in some respect.\n\nFor most of history, turkeys were primarily raised on small family farms for meat and as a form of pest control (turkeys are prodigious eaters of insects). But with the advent of factory farming of poultry, turkeys began to be selectively bred for increasingly larger size, focusing especially on the production of breast meat. Beginning in the 1920s and continuing in to the 1950s, broad-breasted fowl began to replace all other types of turkey in commercial production. The favorite breed at the time was the Broad Breasted Bronze, which was developed from the Standard Bronze. In the 1960s producers began to heavily favor turkeys that did not show the dark pin feathers in their carcass, and thus the Broad Breasted White grew to dominate the industry, a trend which continues to this day.\n\nTo meet perceived consumer demand and increase producers' profit margins, the goal in turkey farming became the production of the maximum amount of breast meat at the lowest possible cost. As a result of selection for this single trait, 70% of the weight of mass market turkeys is in their breast. Consequently, the birds are so heavy that they are completely incapable of reproducing without artificial insemination, and they reach such extreme weights so quickly their overall development fails to keep pace with their rapidly accruing muscle mass, resulting in severe immune system, cardiac, respiratory and leg problems. \n\nFor over 35 years, the overwhelming majority of the 280 million turkeys produced in North America each year have been the product of a few genetic strains of Broad Breasted White. The breeding stock for these birds are owned largely by three multinational corporations: Hybrid Turkeys of Ontario, Canada, British United Turkeys of America in Lewisburg, West Virginia, and Nicholas Turkey Breeding Farms in Sonoma, California.\n\nAlong with the adoption of the Broad Breasted White by industrial producers, other turkey varieties faded in numbers. Other than exhibition birds and those on a scant few small farms, other turkeys virtually disappeared. By the end of the 20th century, all but the Broad Breasted White were in danger of extinction. Around this time, conservation organizations began to recognize the plight of heritage turkeys; The Livestock Conservancy considered heritage turkeys to be the most critically endangered of all domestic animals circa 1997. A census conducted by the Conservancy found less than 1,500 total breeding birds (out of all heritage varieties) were left in the country. Some breeds, such as the Narragansett, had less than a dozen individuals left, and many considered most heritage turkeys to be beyond hope.\n\nThe Livestock Conservancy, Slow Food USA, the Society for the Preservation of Poultry Antiquities (SPAA), the Heritage Turkey Foundation, and a few hundred key poultry enthusiasts launched a major effort to restore breeding populations of heritage turkeys in the late 20th century. One man in particular, Frank Reese Jr., has been credited by sources such as ABC News and \"The New York Times\" as being instrumental in preserving heritage breeds, but small farmers all across the country were also important; strains of heritage turkey kept in genetic isolation for years by family farms preserved heritage breeds for the future. Primary motivations for the endeavor included a passion for historic breeds and maintaining genetic diversity among domestic animals which humans depend upon. Consumer and restaurant interest was also motivated by a support of local and sustainable foods.\n\nIn a 2003 census by the Livestock Conservancy, heritage turkey populations had increased by more than 200 percent. By 2006, the count of heritage turkeys in the U.S. was up to 8,800 breeding birds. Though all but the Bourbon Red and Royal Palm are still considered critically endangered, the birds have rebounded significantly.\n\nWhile the moniker of heritage turkey is not a government-regulated label like organic foods, it does have a precise definition. The most notable heritage turkeys today come from specific breeds, such as the Bourbon Red, but any fowl regardless of breed can be defined as a heritage turkey if it meets the criteria mentioned below. Only a few of these are recognized by the American Poultry Association through inclusion in the \"Standard of Perfection\".\n\nAlong with the surge in popularity of heritage turkeys, some farmers have (perhaps unknowingly) passed off birds which do not meet the basic definition of true heritage birds in an effort to cash in on the phenomenon. To be a true heritage turkey, birds must meet three specific criteria.\n\nThe first criterion is that heritage turkeys are able to mate naturally with no intervention from humans, and with expected fertility rates of 70-80%. Hens can lay fertile eggs, and brood their clutches to hatching. According to The Livestock Conservancy, birds must be the result of natural reproduction in order to truly be called heritage turkeys.\n\nExcept for a few flocks of toms kept for semen production, commercial turkeys generally never live past the point at which they reach market weight. Heritage turkeys are capable of the full normal lifespan of wild turkeys. Breeding hens are commonly productive for 5–7 years and breeding toms for 3–5 years. They are also more well-suited for outdoor and/or free range conditions in pastured poultry operations.\n\nAll heritage turkeys have a relatively slow to moderate rate of growth. Turkeys raised in industrial agriculture are slaughtered at 14 to 18 weeks of age, while heritage turkeys reach a marketable weight in about 28 weeks, giving the birds time to develop a strong skeletal structure and healthy organs prior to building muscle mass. This growth rate is identical to that of the commercial varieties of the first half of the 20th century.\n\nHeritage turkeys have been praised by chefs and food critics alike as being richer in flavor than industrial birds, though the lack of a large amount of breast meat means cooking times and methods may differ substantially from non-heritage birds. Heritage turkeys are closer in taste to wild turkeys, but are several pounds larger. Part of this stated increase in flavor is due to a difference in the maturity between industrial turkeys and heritage ones - if birds are slaughtered at less than four months old, they fail to ever accrue fat layers.\n\nDue to their rarity and the length of time involved in their growth, heritage turkeys are also far more expensive than their more common brethren. While turkeys from factory farms may be given away along with other purchases, heritage turkeys can cost upwards of $200 (USD), though prices have fallen in some areas as they become more common.\n\nIn addition to a difference in culinary characteristics, heritage turkeys are often considered to be a healthier food; as a result of the diet of pasture-raised turkeys, heritage meat contains far higher levels of omega-3 fatty acids, which help prevent heart disease.\n\n\n\n\n"}
{"id": "2613978", "url": "https://en.wikipedia.org/wiki?curid=2613978", "title": "IEEE Annals of the History of Computing", "text": "IEEE Annals of the History of Computing\n\nThe IEEE Annals of the History of Computing is a quarterly peer-reviewed academic journal published by the IEEE Computer Society. It covers the history of computing, computer science, and computer hardware. It was founded in 1979 by the AFIPS, in particular by Saul Rosen, who was an editor until his death in 1991.\n\nThe journal publishes scholarly articles, interviews and memoirs by computer pioneers, and news and events in the field. It was established in July 1979 as \"Annals of the History of Computing\", with Bernard Galler as editor-in-chief. The journal became an IEEE publication in 1992, as was retitled to \"IEEE Annals of the History of Computing\". The 2013 impact factor was 0.524. The current editor is Nathan Ensmenger at Indiana University School of Informatics.\n\n\n"}
{"id": "25529127", "url": "https://en.wikipedia.org/wiki?curid=25529127", "title": "ITT Interconnect Solutions", "text": "ITT Interconnect Solutions\n\nITT Interconnect Solutions is a globally diversified connector and connector assembly manufacturing company. Founded in 1915 as Cannon (company) by James H. Cannon, the company developed some of the first equipment for sound films in the early years of the movie industry, including a synchronous motor drive to remotely operate a motion picture projector together with a phonograph. The first \"Cannon plug\", the M-1 connector, was initially designed as a quick grounding connection for the electrical motor on a portable meat grinder and was adapted for movie sound equipment, enabling the new electrical camera to move freely about while “shooting” a scene. Cannon's M-1 connector was incorporated into the sound equipment used to make the first \"talking\" motion picture, \"The Jazz Singer\". Cannon continued to develop connectors for the entertainment industry, including the “P” Series audio connectors developed for Paramount Studios, as well as connectors used in the first radio microphones, the first black-and-white television cameras, and the first color television equipment.\n\nIn the early 1930s, Cannon was contracted by Douglas Aircraft Company to develop a circular connector for use on the DC-1 and on the subsequent DC-2 and DC-3 aircraft platforms. As the aerospace industry evolved, McDonnell Douglas Aircraft – as it became known – remained a key customer for Cannon throughout its existence. During the late 1930s with World War II on the horizon, Cannon began volume production of multi-contact electrical connectors, which were used by virtually every aircraft builder in the United States. Cannon's AN Series (for \"Army/Navy\") connector, developed for military aircraft applications, set the standard for modern military connector specifications.\nIn the 1950s, Cannon developed the D-subminiature connector for the military/aerospace market, and the company became an international supplier of connectors and interconnection systems with umbilical connectors designed to meet the specialized needs of guided missiles ranging from small rockets to multistage vehicles. To date, ITT Interconnect Solutions’ (ICS) connectors have been used on every free world space mission.\nITT Cannon also developed the XLR connector in the 1950s for military communications applications as well as sound equipment for professional recording studios. Over the years, the XL Series audio connector became a universal solution for both analog and digital connections, and in 2008 the connector was inducted into the TECnology Hall of Fame by the Mix Foundation for Excellence in Audio.\nDuring this time, the company also became a key connector supplier to the U.S space program with the development of several specialized connectors for a wide range of aerospace applications, including a special microminiature connector developed for use on space suits. NASA awarded Cannon the Medallion for Distinguished Service for its efforts in the first U.S. manned lunar landing. The United Space Alliance recently presented ITT Interconnect Solutions with a certificate of appreciation for supporting the shuttle and international space station programs with a variety of circular and fiber optic connectors.\n\nITT Cannon entered the medical electronics industry with the development of fiber optic microminiature connectors for diagnostic equipment as well as electronic prostheses to restore eyesight, hearing and other functions. These fiber optic connectors also opened up opportunities in the telecommunications market.\n\nIn the automotive and transportation markets, ITT Cannon developed heavy duty durable connectors capable of withstanding high temperatures and harsh environments, many of which are used in vehicles designed by BMW, Ferrari, General Motors and many others.\n\nIn 2001, ITT Cannon purchased BIW, a manufacturer of electrical power connectors, cables and harnessing. BIW designed and manufactured the first power feedthru connectors for electric submersible pumps used in artificial lifts for oil wells, and the company developed the first field attachable connectors for electric submersible pump power cables.\n\nIn 2003, ITT Cannon acquired VEAM, a manufacturer of specialized high reliabiilty DIN rail, multipin, bulkhead sensor and power connectors serving the transportation, military and nuclear markets. As part of ITT Interconnect Solutions, VEAM also offers complete junction box assemblies for mass transit applications.\n\nIn 2007, ITT Cannon changed its name to ITT Interconnect Solutions. Headquartered in Irvine, California, ITT Interconnect Solutions is a division of ITT Corporation.\n"}
{"id": "51665455", "url": "https://en.wikipedia.org/wiki?curid=51665455", "title": "Inside the Factory", "text": "Inside the Factory\n\nInside the Factory is a television series produced by Voltage TV for BBC. Each episode explores how a specific product is made inside a factory. The series is presented by Gregg Wallace and Cherry Healey, with historian Ruth Goodman providing a look at how products came to exist as we know them today.\n\n"}
{"id": "617484", "url": "https://en.wikipedia.org/wiki?curid=617484", "title": "Joi Ito", "text": "Joi Ito\n\nIto has received recognition for his role as an entrepreneur focused on Internet and technology companies and has founded, among other companies, PSINet Japan, Digital Garage and Infoseek Japan. Ito is the chairman of the board of PureTech Health. Ito is a strategic advisor to Sony Corporation, and a board member of The New York Times Company, the John S. and James L. Knight Foundation, the John D. and Catherine T. MacArthur Foundation, and General Partner of Neoteny Labs. Ito writes a monthly column in the Ideas section of Wired.\n\nIto was born in Kyoto, Japan. His family moved to Canada and then when Ito was about age 3 to a suburb of Detroit, Michigan, in the United States where his father became a research scientist and his mother a secretary for Energy Conversion Devices, Inc., now Ovonics. Company founder Stanford R. Ovshinsky was impressed with Ito, whom he thought of almost as his son. Ovshinsky helped Ito develop his interests in technology and social movements, and at age 13 gave him work with scientists, saying, \"He was not a child in the conventional sense.\"\n\nIto and his sister Mizuko Ito, who is called Mimi, spent summers in Japan with their grandmother who taught them traditional Japanese culture. At 14, he returned to Japan when his mother was promoted to president of Energy Conversion Devices Japan. He studied at the Nishimachi International School and for high school, the American School in Japan in Tokyo. Ito also learned \"street language, street smarts, and computers\". One of few Japanese using modems before deregulation of networking reached Japan in 1985, Ito had found The Source and the original MUD by his teens (and by 26 was working on his own MUD).\n\nIto returned to the United States to attend Tufts University as a computer science major, where he met, among others, Pierre Omidyar, later founder of eBay. Finding his course work too rigid and believing that learning computer science in school was \"stupid\", Ito dropped out of Tufts to briefly work for Ovonics. Ovshinsky encouraged him to return to school. He enrolled at the University of Chicago in physics but dropped out on discovering, in his opinion, the program at Chicago to be more oriented towards producing practical engineers than towards teaching an intuitive understanding of physics. In the Fall of 1985 he became the first student to register for a pioneering program of online courses offered by Connected Education, Inc., for undergraduate credit from The New School for Social Research.\n\nIto is one of Timothy Leary's godsons—a close non-traditional family-like relationship, an idea said to have been conceived by Leary for a few of his friends. Ito's sister is Mizuko Ito, a cultural anthropologist studying media technology use, and the musician Cornelius is his second cousin. Ito currently lives in Cambridge, Massachusetts, with his wife Mizuka Ito (née Kurogane). Joi and Mizuka had a daughter, Kio (輝生) on May 11, 2017.\n\nIto became a disk jockey working in nightclubs in Chicago such as The Limelight and The Smart Bar and to work with Metasystems Design Group to start a virtual community in Tokyo. Later, Ito ran a nightclub in Roppongi, Japan called XY Relax with help from Joe Shanahan of Metro Chicago/Smart Bar. He helped bring industrial music from Chicago (Wax Trax) and later the rave scene, managing a DJ team and visual artists, including importing Anarchic Adjustment to Japan.\nIto was the Chairman of Creative Commons from December 2006 until 2012. He is on the board of Digital Garage, Culture Convenience Club (CCC), Tucows, and EPIC, and is on the advisory boards of Creative Commons and WITNESS. He is the founder and CEO of the venture capital firm Neoteny Co., Ltd. In October 2004, he was named to the board of ICANN for a three-year term starting December 2004. In August 2005, he joined the board of the Mozilla Foundation and served until April 2016. He served on the board of the Open Source Initiative (OSI) from March 2005 until April 2007. He currently serves as a Board Emeritus for OSI. He was a founding board member of Expression College for Digital Arts as well as the Zero One Art and Technology Network. In 1999, he served as the Associate to Mr. Mount (the executive producer) on the film \"The Indian Runner\". Ito also served as a Board Member of Energy Conversion Devices from 1995 to 2000.\n\nIto is a venture capitalist and angel investor and was an early stage investor in Kickstarter, Twitter, Six Apart, Technorati, Flickr, SocialText, Dopplr, Last.fm, Rupture, Kongregate, Fotopedia, Diffbot, Formlabs, 3Dsolve and other Internet companies. A vocal advocate of emergent democracy and the sharing economy, Ito is a doctoral candidate in Business Administration focusing on the sharing economy at the Graduate School of International Corporate Strategy, Hitotsubashi University. He is the author of \"Emergent Democracy\". Ito is Senior Visiting Researcher of Keio Research Institute at SFC. In May 2011, it was announced that Ito's company, Digital Garage, will provide PR, marketing, product marketing research and market research for Linkedin Japan.\n\nIto is a PADI IDC Staff Instructor, an Emergency First Responder Instructor Trainer, and a Divers Alert Network (DAN) Instructor Trainer.\n\nIn recent years, Ito has become critical of what he sees as Japan's inward focus. He stated in a 2011 interview that he thinks Japan needs to look internationally if it is to continue to be \"relevant\".\n\nIto has written opinion editorials for the Asian \"Wall Street Journal\" and \"The New York Times\" and has published articles in numerous other magazines and newspapers. He has had regular columns in \"The Daily Yomiuri\", \"Mac World Japan\", \"Asahi Pasocom\", \"Asahi Doors\", and other media sources. His photographs have been used in \"The New York Times\" Online, \"BusinessWeek\", \"American Heritage\", Wired News, Forbes, and BBC News. He was on the early editorial mastheads of \"Wired\" and \"Mondo 2000\". He has authored and co-authored a number of books including \"Dialog – Ryu Murakami X Joichi Ito\" with Ryu Murakami, and \"Freesouls: Captured and Released\" with Christopher Adams, a book of Ito's photographs that includes essays by several prominent figures in the free culture movement. He has hosted televisions shows including \"The New Breed\" and \"SimTV\" shows on NHK.\n\nHe is currently the host of a TV show called \"Super-Presentation\" airing weekly in Japan on NHK.\n\nIto was listed by \"Time\" magazine as a member of the \"Cyber-Elite\" in 1997. He was also named one of the 50 \"Stars of Asia\" in the \"Entrepreneurs and Dealmakers\" category by \"BusinessWeek\" and commended by the Japanese Ministry of Posts and Telecommunications for supporting the advancement of IT in 2000. He was selected by the World Economic Forum in 2001 as one of the \"Global Leaders for Tomorrow\" and chosen by \"Newsweek\" as a member of the \"Leaders of The Pack (high technology industry)\" in 2005, and listed by \"Vanity Fair\" as a member of \"The Next Establishment\" in the October Issue, 2007 and 2011. Joi Ito was also named by \"BusinessWeek\" as one of the 25 Most Influential People on the Web in 2008. On July 22, 2011 he was awarded a Lifetime Achievement Award in recognition of his role as one of the world's leading advocates of Internet freedom from the University of Oxford Internet Institute. In 2011, with Ethan Zuckerman, he was named by \"Foreign Policy\" magazine to its list of top global thinkers, in which he stated the \"Best idea\" is \"Users controlling their own data\". Ito received the degree of Doctor of Literature, honoris causa, from The New School in 2013. On March 11, 2014, Ito was inducted into the SXSW Interactive Festival Hall of Fame. He was a TED speaker at the March 21, TED2014. In 2014, Ito was awarded the Golden Plate Award by the Academy of Achievement. On May 17, 2015 Ito received a Doctor of Humane Letters, honoris causa, from Tufts University. Ito was elected to the American Academy of Arts and Sciences in April 2017. On May 11, 2017 Ito was awarded the IRI Medal.\n\n\"The New York Times\" reported in April 2011 that Joi Ito was named to be the director of the MIT Media Lab. His appointment was called an \"unusual choice\" since Ito studied at two colleges, but did not finish his degrees. \"The choice is radical, but brilliant,\" said Larry Smarr, director of Calit2. Ito officially began his role on September 1, 2011. He was appointed Professor of the Practice of Media Arts and Sciences at MIT, effective July 1, 2016.\n\nNicholas Negroponte, Media Lab's co-founder and chairman emeritus, described the choice as bringing the media to \"Joi's world\". In an interview with Asian Scientist Magazine, Joi Ito discusses his vision for the MIT Media Lab, and how he likes the word “learning” better than the word “education”.\n\n\n\n"}
{"id": "26464437", "url": "https://en.wikipedia.org/wiki?curid=26464437", "title": "Laster Technologies", "text": "Laster Technologies\n\nLaster Technologies is a technology company based near Paris, France that develops augmented reality eyewear. The company was founded in January 2005 by a group of experts in optics and visual image processing at CNRS (Centre Nacional de la Recherche Scientifique). Laster Technologies' patented technology is advertised under the trademark EnhancedView.\n\nLaster Technologies products include eyewear devices coupled with software enabling man-machine interface functions such as gestural recognition, digital image correlation and tracking, and voice recognition. Potential applications for these products include defense and security, industrial maintenance, medical data monitoring and imaging, navigation, and sports.\n\nIn September 2006, the company won a research grant for the development of a prototype demonstrator for augmented reality in French Museums. It worked on this project with four partners: INRIA, Cité des Sciences et de l'Industrie, the Pierre-and-Marie-Curie University and Naska films. In July 2009, the company presented the augmented reality experience \"Observe the Earth in 3D\" at the inauguration of the permanent exhibition Explora, \"Objective : Earth\" at the Cité des Sciences et de l'Industrie. A projected video of Earth globe was placed at the center of a semi-circular table. Visitors are placed around the table and wear Laster glasses which allows them to see the Earth globe and overlay of virtual satellites around the Earth.\n\n"}
{"id": "15911260", "url": "https://en.wikipedia.org/wiki?curid=15911260", "title": "Lights For Learning", "text": "Lights For Learning\n\nLights For Learning is a British-based charity founded in 2000. They provide solar powered electric lighting to places of education with no access to artificial light, mainly in rural African schools. The organisation received registered charity status in 2004 and has now fitted schools in several African countries and the Philippines with their unique lighting system.\n\n"}
{"id": "32114271", "url": "https://en.wikipedia.org/wiki?curid=32114271", "title": "List of Lexmark products", "text": "List of Lexmark products\n\nThe following is a partial list of products currently sold under the Lexmark brand.\n\nsuffixes:\nd: duplex print \ne: eSF support (embedded apps)\nn: network (letter omitted when 'e' is present) \nh: hard disk drive (omitted in series where 100% of models include it)\nt: extra paper tray\nf: staple finisher\np: staple and hole punch finisher \nx: high-capacity paper tray\nm: mailbox\ns: offset stacker\n\nCurrent Line:\n\nMonochrome printers\n\nColor Printers\nMonochrome Multifunction Products\n\nColor Multifunction Products\n\nDot Matrix Printers\n\n\n"}
{"id": "25687934", "url": "https://en.wikipedia.org/wiki?curid=25687934", "title": "List of transport megaprojects", "text": "List of transport megaprojects\n\nThis is a list of transport megaprojects. Care should be taken in comparing the cost of projects from different times, even a few years apart due to inflation; comparing nominal costs without taking this into account can be highly misleading. \n\nThe size and scope of projects varies considerably; some projects are made up of a package of different infrastructure projects, while others may be single part of a larger network.\n\nIn order to limit the size and scope of this list, recent infrastructure that cost less than $1 billion is not listed; this is a common definition of a megaproject. However, historically significant projects may be listed. See also list of megaprojects.\n"}
{"id": "1866529", "url": "https://en.wikipedia.org/wiki?curid=1866529", "title": "Low-energy house", "text": "Low-energy house\n\nA low-energy house is any type of house that from design, technologies and building products uses less energy, from any source, than a traditional or average contemporary house. In the practice of sustainable design, sustainable architecture, low-energy building, energy-efficient landscaping low-energy houses often use active solar and passive solar building design techniques and components to reduce their energy expenditure.\n\nThe meaning of the term 'low-energy house' has changed over time, but in Europe it generally refers to a house that uses around half of the German or Swiss low-energy standards referred to below for space heating, typically in the range from 30 kWh/m²a to 20 kWh/m²a (9,500 Btu/ft²/yr to 6,300 Btu/ft²/yr). Below this the term 'Ultra-low-energy building' is often used.\n\nThe term can also refer to any dwelling whose energy use is below the standards demanded by current building codes. Because national standards vary considerably around the world, 'low-energy' developments in one country may not meet 'normal practice' in another.\n\nIn some countries the term relates to a specific building standard. In particular, these seek to limit the energy used for space heating, since in many climate zones it represents the largest energy use. Other energy use may also be regulated. The history of passive solar building design gives an international look at one form of low-energy building development and standards.\n\nIn Germany a low-energy house \"(Niedrigenergiehaus)\" has a limit equivalent to 7 litres of heating oil for each square metre of room \"for space heating\" annually (50 kWh/m²a or 15,850 Btu/ft²/yr). In Switzerland, the term is used in connection with the \"MINERGIE\" standard (42 kWh/m²a or 13,300 Btu/ft²/yr) or the Minergie-P (equivalent to the Passivhaus).\n\nIn comparison, the German \"Passivhaus\" ultra-low-energy standard, currently undergoing adoption in some other European countries, has a maximum space heating requirement of 15 kWh/m²a or 4,755 Btu/ft²/yr.\n\nA \"sub-10 passive house\" is under construction in Ireland that has an independently evaluated PHPP (Passive House) rating of 9.5 kW/m/year. Its form of construction also tackles the issue of embodied energy, which can significantly distort the lifecycle CO emissions associated with even low energy use houses.\n\nIn the United States, the ENERGY STAR program is the largest program defining low-energy homes and consumer products. Homes earning ENERGY STAR certification use at least 15% less energy than standard new homes built to the International Residential Code, although homes typically achieve 20%–30% savings.\n\nIn addition, the US Department of Energy launched a program in 2008 with the goal of spreading zero-energy housing over the US. Currently, participating builders commit to constructing new homes that achieve 30% savings on a home energy rating scale.\n\nIn Canada, builders may voluntarily use a range of standards, labels, and certification programs to demonstrate that they have achieved a very high level of energy performance in a given project. These include:\n\n\nIn British Columbia, all of the above programs align with the BC Energy Step Code, a provincial regulation that local governments may use, if they wish, to incentivize or require a level of energy efficiency in new construction that goes above and beyond the requirements of the base building code. The BC Energy Step Code is designed as a technical roadmap to help the province reach its target that all new buildings will attain a net zero energy ready level of performance by 2032.\n\nBeyond ultra-low-energy buildings are those that use, on average over the course of a year, no imported energy - zero-energy buildings – or even those that generate a surplus - energy-plus houses – both of which have been and are being successfully built.\n\nThis can be achieved by a mixture of energy conservation technologies and the use of renewable energy sources. However, in the absence of recognized standards, the mix between these – and consequently the energy-use profile and environmental impact of the building – can vary significantly.\n\nAt one end of the spectrum are buildings with an ultra-low space heating requirement that therefore require low levels of imported energy, even in winter, approaching the concept of an autonomous building.\n\nAt the opposite end of the spectrum are buildings where few attempts are made to reduce the space heating requirement and which therefore use high levels of imported energy in winter. While this can be balanced by high levels of renewable energy generation throughout the year, it imposes greater demands on the traditional national energy infrastructure during the peak winter season.\n\nLow-energy buildings typically use high levels of insulation, energy efficient windows, low levels of air infiltration and heat recovery ventilation to lower heating and cooling energy. They may also use passive solar building design techniques or active solar technologies. These homes may use hot water heat recycling technologies to recover heat from showers and dishwashers. Lighting and miscellaneous energy use is allieviated with fluorescent lighting and efficient appliances. Weatherization provides more information on increasing building energy efficiency.\n\nPassive Houses are required to achieve a whole building air change rate of no more than 0.6 ac/hr \n<ref group=footnote name=\"ac/hr\">ac/hr = air changes per hour; \"air change\" refers to the house volume</ref> under forced pressurisation and depressurisation testing at 50Pa minimum. On site blower door testing by certified testers is used to prove compliance.\n\nA significant feature of ultra-low-energy buildings is the increasing importance of heat loss through linear thermal bridging within the construction. Failure to eliminate thermal pathways from warm to cold surfaces (\"bridges\") creates the conditions for interstitial condensation forming deep within the construction and lead to potentially serious issues of mould growth and rot. With near zero filtration losses through the fabric of the dwelling, air movement cannot be relied upon to dry out the construction and a comprehensive condensation risk analysis of every abutment detail is recommended.\n\n\n\nPassive solar building design and energy-efficient landscaping support the low-energy house in conservation and can integrate them into a neighborhood and environment. Following passive solar building techniques, where possible buildings are compact in shape to reduce their surface area, with principal windows oriented towards the equator - south in the northern hemisphere and north in the southern hemisphere - to maximize passive solar gain. However, the use of solar gain, especially in temperate climate regions, is secondary to minimizing the overall house energy requirements. On the other hand in hot climates temperatures excess heat can create uncomfortable indoor conditions. Passive alternatives to air conditioning systems such as temperature-dependent venting have been shown to be effective in regions with cooling needs. Other techniques to combat excessive solar heat gains include Brise soleils, trees, attached pergolas with vines, vertical gardens, green roofs among others.\n\nLow-energy houses can be constructed from dense or lightweight materials, but some internal thermal mass is normally incorporated to reduce summer peak temperatures, maintain stable winter temperatures, and prevent possible overheating in spring or autumn before the higher sun angle \"shades\" mid-day wall exposure and window penetration. Exterior wall color, when the surface allows choice, for reflection or absorption insolation qualities depends on the predominant year-round ambient outdoor temperature. The use of deciduous trees and wall trellised or self attaching vines can assist in climates not at the temperature extremes.\n\nTo minimize the total primary energy consumption, the many passive and active daylighting techniques are the first daytime solution to employ. For low light level days, non-daylighted spaces, and nighttime; the use of creative-sustainable lighting design using low-energy sources such as 'standard voltage' compact fluorescent lamps and solid-state lighting with Light-emitting diode-LED lamps, organic light-emitting diodes, and PLED - polymer light-emitting diodes; and 'low voltage' electrical filament-Incandescent light bulbs, and compact Metal halide, Xenon and Halogen lamps, can be used.\n\nSolar powered exterior circulation, security, and landscape lighting - with photovoltaic cells on each fixture or connecting to a central Solar panel system, are available for gardens and outdoor needs. Low voltage systems can be used for more controlled or independent illumination, while still using less electricity than conventional fixtures and lamps. Timers, motion detection and natural light operation sensors reduce energy consumption, and light pollution even further for a Low-energy house setting.\n\nAppliance consumer products meeting independent energy efficiency testing and receiving Ecolabel certification marks for reduced electrical-'natural-gas' consumption and product manufacturing carbon emission labels are preferred for use in Low-energy houses. The ecolabel certification marks of Energy Star and EKOenergy are examples.\n\n\"Buildings\"\n\n\"Air and temperature\"\n\n\"Solar\"\n\n\"Sustainable\"\n\n\"Energy Rating standards\"\n\n\n\n"}
{"id": "6267677", "url": "https://en.wikipedia.org/wiki?curid=6267677", "title": "Mae West Lips Sofa", "text": "Mae West Lips Sofa\n\nThe Mae West Lips Sofa is a surrealist sculpture in the form of a sofa by Salvador Dalí. The light red, 86.5 x 183 x 81.5 cm (34 x 72 x 32 in) sized seating furniture made of wood and satin was shaped in 1937 after the lips of actress Mae West, whom Dalí apparently found fascinating. Dalí never intended for the sofa to serve a functional use. He also claimed that he partly based the design of the sofa on a pile of rocks near Cadaqués and Portlligat, where he stayed for many years with his wife, Gala Éluard Dalí.\n\nEdward James, a rich British patron of the Surrealists in the 1930s, originally commissioned this piece from Dalí. It is now part of the art collections at the Museum Boijmans Van Beuningen in Rotterdam. Another version is on display at the Dalí Theatre and Museum in Figueres, Catalonia, Spain. A version is owned by the Victoria and Albert Museum, having exhibited another example at the 2007 exhibition \"Surreal Things: Surrealism and Design\".\nIn 1972, Italian designer group Studio 65 produced an updated version of Dalí's \"Mae West Lips Sofa\", called the \"Marilyn Bocca Sofa\" in honor of actress and sex symbol Marilyn Monroe. Unlike Dalí's original work, the \"Marilyn Bocca Sofa\" has a more practical use in the home, and has become an icon of classic modern design.\n\n"}
{"id": "55658347", "url": "https://en.wikipedia.org/wiki?curid=55658347", "title": "Mansour Gholami", "text": "Mansour Gholami\n\nMansour Gholami (, born 1953 in Hamedan) is an Iranian professor, politician and the current Minister of Science, a position he held since 29 October 2017. He was Chancellor of Bu-Ali Sina University in two terms, first from 1997 until 2004 and the second from 2014 until 2017.\n"}
{"id": "42303484", "url": "https://en.wikipedia.org/wiki?curid=42303484", "title": "NewsPlus", "text": "NewsPlus\n\nNewsPlus is a news aggregator company based in Singapore and Hyderabad. It aggregates content from news sites, social media and blogs. The service can be accessed via web browser, or via mobile apps for iOS, Android.\n\nNewsPlus is localized into 10 languages.\n\nNewsPlus brings news for India, USA, Philippines, Indonesia, Thailand, Malaysia, Pakistan, South Africa, and the local news for all major cities and towns.\n\nNewsPlus app was originally launched under name of Veooz in August 2015 Since then product evolved significantly, with support for additional languages, and addition features like, magazines, direct sharing, social features.\n"}
{"id": "4288288", "url": "https://en.wikipedia.org/wiki?curid=4288288", "title": "OpenDocument Format Alliance", "text": "OpenDocument Format Alliance\n\nThe Open Document Format Alliance (ODF Alliance) is a Washington, D.C.-based lobbying organization established by IBM, Sun Microsystems and SIIA to \"promote and advance the use of OpenDocument Format (ODF) as the primary document format for governments\" Although originally focused on promotion of ODF \"via legislation or by executive policy decision\", the ODF Alliance also did extensive PR and lobbying in opposition to the Microsoft-backed Office Open XML standard.\n\nFounded by IBM, Sun and 33 other companies and organizations on March 3, 2006, the Alliance now boasts more than 500 members. However, funding sources are not itemized in the organization's 2007 annual report, so membership financial participation is unclear.\n\nThe ODF Alliance does not give a physical location on their website, but from documents filed with the State of New York, the organization appears to be headquartered on the 6th floor of 1090 Vermont Avenue NW in Washington, DC 20005, sharing offices with the Software and Information Industry Association (SIIA), one of the original backers of the ODF Alliance.\n\nThe executive director is Marino Marcich, who served for four years in the Bush administration's State Department prior to joining the ODF Alliance. While at the Department of State, he was part of former Secretary of State Colin Powell's inner circle. Prior to joining the Bush administration, \"Marcich also served as the National Association of Manufacturers (NAM) assistant vice president for international economic affairs, leading key lobby campaigns for World Trade Organization accession and trade-promotion authority.\"\n\nPublic Relations for the ODF Alliance are handled by Rational PR, headed by Kathyrn Brownlee. Although Rational PR does not list the ODF Alliance as a client as of April 2008, SIIA is listed.\n\nThe ODF Alliance website provides no information on funding or management structure. At this time, the website lists 6 \"national chapters\" outside of the United States; Brazil, Europe, India, Malaysia, Poland and Portugal, but it is unclear if those chapter organizations have any input into the policy decisions of the organization.\n\nODF Preferences\nThe ODF Alliance has also been very active in lobbying U.S. State and International governments on ODF issues to promote legislative mandates for the use of the ODF document format. The ODF Alliance has provided public testimony in the following fora:\n\n\nAnti-OOXML\nDuring late 2007 though 2008, the ODF Alliance was focused on lobbying ISO members in order to prevent OOXML from being accepted by ISO.\n\n\nLast know Newsletter\nThe ODF Alliance website's last snapshot saved by Archive.org dates back from March 24, 2013. Even then, the latest published news was the \"ODFA Newsletter\" from October 2010, implying the Alliance ceased to operate long before the website was shut down. During 2014 the domain name odfalliance.org went from displaying error pages to briefly displaying ODF-related news, and to finally housing legal information about injury lawyers. In 2015 odfalliance.org housed cigar news. No news could be found about exactly when or why the ODFA was dissolved.\n\n"}
{"id": "12147575", "url": "https://en.wikipedia.org/wiki?curid=12147575", "title": "Persian well", "text": "Persian well\n\nA Persian well is a type of water well found in the Middle East, often used in conjunction with a qanat. These wells feature an ox-driven pump where the ox walks in circles around a central drive shaft which turns a wheel that raises water via a chain of buckets from the qanat or a well. In some portions of some qanats the water flows fast enough that a subterranean waterwheel may harness enough power to raise the buckets of water to the surface level. \n\n"}
{"id": "23086", "url": "https://en.wikipedia.org/wiki?curid=23086", "title": "Poker equipment", "text": "Poker equipment\n\nThe following is a list of equipment used for a game of poker:\n\n"}
{"id": "26985551", "url": "https://en.wikipedia.org/wiki?curid=26985551", "title": "Roman lead pipe inscription", "text": "Roman lead pipe inscription\n\nA Roman lead pipe inscription is a Latin inscription on a Roman water pipe made of lead which provides brief information on its manufacturer and owner, often the reigning emperor himself as the supreme authority. The identification marks were created by full text stamps.\n\nLead, a product of the ancient silver smelting process, was produced in the Roman Empire with an estimated peak production of 80,000 metric tons per year – a truly industrial scale. The metal was used along with other materials in the vast water supply network of the Romans for the manufacture of water pipes, particularly for urban plumbing. \n\nThe method of manufacturing the lead pipes is recorded by Vitruvius and Frontinus. The lead was poured into sheets of a uniform 3m length, which were bent to form a cylinder and soldered at the seam. The lead pipes could range in size from approximately up to diameter depending on the required rate of flow.\n\nSince the 19th century, the hypothesis has occasionally been put forward that the Roman inscriptions were created by movable type printing. A recent investigation by the typesetter and linguist Herbert Brekle, however, concludes that all material evidence points to the use of common text stamps. Brekle describes the manufacturing method as follows:\n\nBrekle lists the following reasons for the employment of stamps and against that of movable type: for printing on lead sheets the way the Romans created them, it would be much more practical to use single stamp blocks than sets of individual letters, since the latter would be unstable and would have required a clamp or some similar mechanism to maintain the necessary cohesion. Neither impressions of such clamps nor of the fine lines between the individual letters typical for the use of movable type are discernible in the inscriptions. By contrast, the outer rim of one examined stamp block left a raised rectangular edge running around the inscription text, thus providing positive evidence for the use of such a printing device. \n\nIn addition, evidence of the poor positioning of movable type, such as individual letters tilting to the right or left or deviating from the baseline – something which could have been expected to occur at least in a few extant specimens – is notably absent. In those inscriptions where the letters are not properly aligned, the entire text is blurred, which clearly points to the use of full text stamps. Finally, it needs to be considered that archaeological excavations have never unearthed ancient sets of movable type, whereas moulds with reversed inscription texts for stamp printing have indeed been recovered.\n\n\n"}
{"id": "15668833", "url": "https://en.wikipedia.org/wiki?curid=15668833", "title": "Season cracking", "text": "Season cracking\n\nSeason cracking is a form of stress-corrosion cracking of brass cartridge cases originally reported from British forces in India. During the monsoon season, military activity was temporarily reduced, and ammunition was stored in stables until the dry weather returned. Many brass cartridges were subsequently found to be cracked, especially where the case was crimped to the bullet. It was not until 1921 that the phenomenon was explained by Moor, Beckinsale and Mallinson: ammonia from horse urine, combined with the residual stress in the cold-drawn metal of the cartridges, was responsible for the cracking.\n\nSeason cracking is characterised by deep brittle cracks which penetrate into affected components. If the cracks reach a critical size, the component can suddenly fracture, sometimes with disastrous results. However, if the concentration of ammonia is very high, then attack is much more severe, and attack over all exposed surfaces occurs. The problem was solved by annealing the brass cases after forming so as to relieve the residual stresses.\nThe attack takes the form of a reaction between ammonia and copper to form the cuprammonium ion, formula [Cu(NH)], a chemical complex which is water-soluble, and hence washed from the growing cracks. The problem of cracking can therefore also occur in copper and any other copper alloy, such as bronze. The tendency of copper to react with ammonia was exploited in making rayon, and the deep blue colour of the aqueous solution of copper(II) oxide in ammonia is known as Schweizer's reagent.\n\nAlthough the problem was first found in brass, any alloy containing copper will be susceptible to the problem. It includes copper itself (as used in pipe for example), bronzes and other alloys with a significant copper content.\n\n\n"}
{"id": "997715", "url": "https://en.wikipedia.org/wiki?curid=997715", "title": "Spin isomers of hydrogen", "text": "Spin isomers of hydrogen\n\nMolecular hydrogen occurs in two isomeric forms, one with its two proton nuclear spins aligned parallel (orthohydrogen), the other with its two proton spins aligned antiparallel (parahydrogen). These two forms are often referred to as spin isomers.\n\nParahydrogen is in a lower energy state than is orthohydrogen. At room temperature and thermal equilibrium, thermal excitation causes hydrogen to consist of approximately 75% orthohydrogen and 25% parahydrogen. When hydrogen is liquified at low temperature, there is a slow spontaneous transition to a predominantly para ratio, with the released energy having implications for storage. Essentially pure parahydrogen form can be obtained at very low temperatures, but it is not possible to obtain a sample containing more than 75% orthohydrogen by heating.\n\nA mixture or 50:50 mixture of ortho- and parahydrogen can be synthesised in the laboratory by passing it over an iron(III) oxide catalyst at liquid nitrogen temperature (77 K) or by storing hydrogen at 77 K for 2-3 hours in the presence of activated charcoal. In the absence of a catalyst, gas phase parahydrogen takes days to relax to normal hydrogen at room temperature while it takes hours to do so in organic solvents.\n\nEach hydrogen molecule (H) consists of two hydrogen atoms linked by a covalent bond. If we neglect the small proportion of deuterium and tritium which may be present, each hydrogen atom consists of one proton and one electron. Each proton has an associated magnetic moment, which is associated with the proton's spin of ½. In the H molecule, the spins of the two hydrogen nuclei (protons) couple to form a triplet state known as orthohydrogen, and a singlet state known as parahydrogen.\n\nThe triplet orthohydrogen state has total nuclear spin I = 1 so that the component along a defined axis can have the three values M = 1, 0, or −1. The corresponding nuclear spin wavefunctions are formula_1 and formula_2. This uses standard bra–ket notation; the symbol ↑ represents the spin-up wavefunction and the symbol ↓ the spin-down wavefunction, so ↑↓ means that the first nucleus is up and the second down. Each orthohydrogen energy level then has a (nuclear) spin degeneracy of three, meaning that it corresponds to three states of the same energy (in the absence of a magnetic field). The singlet parahydrogen state has nuclear spin quantum numbers I = 0 and M = 0, with wavefunction formula_3. Since there is only one possibility, each parahydrogen level has a spin degeneracy of one and is said to be non-degenerate.\n\nThe para form is more stable than the ortho form by 1.06 kJ/mol. The ratio between the ortho and para forms is about 3:1 at standard temperature and pressure in favoring the ortho as a result of thermal energy. However, if chemical equilibrium between the two forms is established, the para form dominates at low temperatures (approx. 99.8% at 20 K). The heat of vaporization is only 0.904 kJ/mol. As a result, ortho liquid hydrogen equilibrating to the para form releases enough energy to cause significant loss by boiling.\n\nSince protons have spin 1/2, they are fermions and the permutational antisymmetry of the total H wavefunction imposes restrictions on the possible rotational states the two forms of H can adopt. Orthohydrogen, with symmetric nuclear spin functions, can only have rotational wavefunctions that are antisymmetric with respect to permutation of the two protons, corresponding to odd values of the rotational quantum number J; conversely, parahydrogen with an antisymmetric nuclear spin function, can only have rotational wavefunctions that are symmetric with respect to permutation of the two protons, corresponding to even J. Applying the rigid rotor approximation, the energies and degeneracies of the rotational states are given by:\n\nformula_4.\n\nThe rotational partition function is conventionally written as:\n\nformula_5.\n\nHowever, as long as these two spin isomers are not in equilibrium, it is more useful to write separate partition functions for each:\n\nformula_6\n\nThe factor of 3 in the partition function for orthohydrogen accounts for the spin degeneracy associated with the +1 spin state; when equilibrium between the spin isomers is possible, then a general partition function incorporating this degeneracy difference can be written as:\n\nformula_7\n\nThe molar rotational energies and heat capacities are derived for any of these cases from:\n\nformula_8\n\nPlots shown here are molar rotational energies and heat capacities for ortho- and parahydrogen, and the \"normal\" ortho/para (3:1) and equilibrium mixtures:\n\nBecause of the antisymmetry-imposed restriction on possible rotational states, orthohydrogen has residual rotational energy at low temperature wherein nearly all the molecules are in the J = 1 state (molecules in the symmetric spin-triplet state cannot fall into the lowest, symmetric rotational state) and possesses nuclear-spin entropy due to the triplet state's threefold degeneracy. The residual energy is significant because the rotational energy levels are relatively widely spaced in H; the gap between the first two levels when expressed in temperature units is twice the characteristic rotational temperature for H:\n\nformula_9.\n\nThis is the T = 0 intercept seen in the molar energy of orthohydrogen. Since \"normal\" room-temperature hydrogen is a 3:1 ortho:para mixture, its molar residual rotational energy at low temperature is (3/4) x 2Rθ = 1091 J/mol, which is somewhat larger than the enthalpy of vaporization of normal hydrogen, 904 J/mol at the boiling point, T = 20.369 K. Notably, the boiling points of parahydrogen and normal (3:1) hydrogen are nearly equal; for parahydrogen ∆H = 898 J/mol at T = 20.277 K, and it follows that nearly all the residual rotational energy of orthohydrogen is retained in the liquid state.\n\nHowever, orthohydrogen is thermodynamically unstable at low temperatures and spontaneously converts into parahydrogen. This process lacks any natural de-excitation radiation mode, so it is slow in the absence of a catalyst which can facilitate interconversion of the singlet and triplet spin states. At room temperature, hydrogen contains 75% orthohydrogen, a proportion which the liquefaction process preserves if carried out in the absence of a catalyst like ferric oxide, activated carbon, platinized asbestos, rare earth metals, uranium compounds,\nchromic oxide, or some nickel compounds to accelerate the conversion of the liquid hydrogen into parahydrogen. Alternatively, additional refrigeration equipment can be used to slowly absorb the heat that the orthohydrogen fraction will (more slowly) release as it spontaneously converts into parahydrogen. If orthohydrogen is not removed from rapidly liquified hydrogen, without a catalyst, the heat released during its decay can boil off as much as 50% of the original liquid.\n\nThe unusual heat capacity of hydrogen was discovered in 1912 by Arnold Eucken. The two forms of molecular hydrogen were first proposed by Werner Heisenberg and Friedrich Hund in 1927. Taking into account this theoretical framework, pure parahydrogen was first synthesized by Paul Harteck and Karl Friedrich Bonhoeffer in 1929. When Heisenberg was awarded the 1932 Nobel prize in physics for the creation of quantum mechanics, this discovery of the \"allotropic forms of hydrogen\" was singled out as its most noteworthy application. Modern isolation of pure parahydrogen has since been achieved using rapid in-vacuum deposition of millimeters thick solid parahydrogen (p–H) samples which are notable for their excellent optical qualities.\n\nWhen an excess of parahydrogen is used during hydrogenation reactions (instead of the normal mixture of orthohydrogen to parahydrogen of 3:1), the resultant product exhibits hyperpolarized signals in proton NMR spectra, an effect termed PHIP (Parahydrogen Induced Polarisation) or, equivalently, PASADENA (Parahydrogen And Synthesis Allow Dramatically Enhanced Nuclear Alignment) (named for first recognition of the effect by Bowers and Weitekamp of Caltech), a phenomenon that has been used to study the mechanism of hydrogenation reactions.\n\nSignal amplification by reversible exchange (SABRE) is a technique to hyperpolarize samples without chemically modifying them. Compared to orthohydrogen or organic molecules, a much greater fraction of the hydrogen nuclei in parahydrogen align with an applied magnetic field. In SABRE, a metal center reversibly binds to both the test molecule and a parahydrogen molecule facilitating the target molecule to pick up the polarization of the parahydrogen. This technique can be improved and utilized for a wide range of organic molecules by using an intermediate \"relay\" molecule like ammonia. The ammonia efficiently binds to the metal center and picks up the polarization from the parahydrogen. The ammonia then transfers it other molecules that don't bind as well to the metal catalyst. This enhanced NMR signal allows the rapid analysis of very small amounts of material.\n\nOther molecules and functional groups containing two hydrogen atoms, such as water and methylene, also have ortho- and para- forms (e.g. orthowater and parawater), but this is of little significance for their thermal properties. Their ortho-para ratios differ from that of dihydrogen.\n\nMolecular oxygen () also exists in three lower-energy triplet states and one singlet state, as ground-state paramagnetic triplet oxygen and energized highly reactive diamagnetic singlet oxygen. These states arise from the spins of their unpaired electrons, not their protons or nuclei.\n\n"}
{"id": "2066422", "url": "https://en.wikipedia.org/wiki?curid=2066422", "title": "Steam donkey", "text": "Steam donkey\n\nSteam donkey, or donkey engine, is the common nickname for a steam-powered winch, or logging engine, widely used in past logging operations, though not limited to logging. They were also found in the mining, maritime, and nearly any other industry that needed a powered winch.\n\nSteam donkeys acquired their name from their origin in sailing ships, where the \"donkey\" engine was typically a small secondary engine used to load and unload cargo and raise the larger sails with small crews, or to power pumps. They were classified by their cylinder type – simplex (single-acting cylinder) or duplex (a compound engine); by their connection to the winches (or \"drums\") – triple-drum, double-drum, etc.; and by their different uses: high-lead yarder, ground-lead yarder, loader, snubber, incline hoist, etc. A good deal of the cable-logging terminology derived from 19th-century merchant sailing, as much of the early technology originated from that industry.\n\nA logging engine comprised at least one powered winch around which was wound hemp rope or (later) steel cable. They were usually fitted with a boiler and usually equipped with skids, or sleds made from logs, to aid them during transit from one \"setting\" to the next. The larger steam donkeys often had a \"donkey house\" (a makeshift shelter for the crew) built either on the skids or as a separate structure. Usually, a water tank, and sometimes a fuel oil tank, was mounted on the back of the sled. In rare cases, steam donkeys were also mounted on wheels. Later steam donkeys were built with multiple horizontally mounted drums/spools, on which were wound heavy steel cable instead of the original rope.\n\nThis describes the use of a steam donkey for logging operations. In the simplest setup, a \"line horse\" would carry the cable out to a log in the woods. The cable would be attached, and, on signal, the steam donkey's operator (engineer) would open the regulator, allowing the steam donkey to drag, or \"skid\", the log towards it. The log was taken either to a mill or to a \"landing\" where the log would be transferred for onward shipment by rail, road or river (either loaded onto boats or floated directly in the water). Later, a \"haulback\" drum was added, where a smaller cable could be routed around the \"setting\" and connected to the end of the heavier \"mainline\" to replace the line horse.\n\nIf a donkey was to be moved, one of its cables was attached to a tree, stump or other strong anchor, and the machine would drag itself overland to the next yarding location.\n\nIn Canada, and in particular Ontario, the donkey engine was often mounted on a barge that could float and thus winch itself over both land and water. Log booms would be winched across water with the engine and it would often be reconfigured with a saw to mill the timber.\n\nJohn Dolbeer, a founding partner of the Dolbeer and Carson Lumber Company in Eureka, California, invented the logging engine in that city in August 1881. The patent (number: 256553) was issued April 18, 1882. On Dolbeer's first model, a 150-foot, 4½ inch manila rope was wrapped several times around a gypsy head (vertically mounted spool) and attached at the other end to a log.\n\nThe invention of the internal-combustion engine led to the development of the diesel-powered tractor crawler, which eventually put an end to the steam donkey. Though some have been preserved in museums, very few are in operating order. A great number still sit abandoned in the forests.\n\nSteam donkeys were also found to be useful for powering other machines such as pile drivers, slide-back loaders (also known as \"slide-jammers\", cranes which were used to load logs onto railroad cars and which moved along the flat-bed rail cars that were to be loaded), and cherry pickers (a sled-mounted crane used for loading, onto railroad cars, logs that a grading crew had cut down). Sailing fishing vessels on the North sea operated vertical steam-driven capstans called ´the donkey´ to haul fishing lines and -nets, some of which are still operating today, even though converted to compressed air.\n\nAn auxiliary engine on a sailing craft (which \"does\" propel the vessel) is still sometimes informally known as \"the donk\".\n\nA functioning steam donkey is on display and occasionally operated at Fort Humboldt State Historic Park in Eureka, California. A non-functioning steam donkey accompanied by a plaque explaining the history of the machine is on permanent display at Grizzly River Run, an attraction at Disney California Adventure Park. Another collection of steam donkeys is located at the Point Defiance Park, Camp 6 Logging Museum located in Tacoma, Washington. The collection includes various steam donkeys, including one of the last very large ones built, and others at various stages of restoration.\n\nAnother steam donkey is on display along an interpretive trail at the Sierra Nevada Logging Museum in Calaveras County, California, an indoor open-air museum about the Sierra Nevada logging industry and history.\n\nAnother steam donkey is preserved at Legoland Billund on their Wild West gold-mine-themed railroad.\n\nOn August 1, 2009, a Steam Donkey was officially unveiled at McLean Mill National Historic Site in Port Alberni, British Columbia. It is now the only commercially operating steam donkey in North America. On that occasion, due to extreme fire risk, demonstrations of the donkey were not performed, but the logs hauled by previous test runs of the donkey (and had been loaded onto a truck) were dumped into the McLean Mill millpond, representing the first steam-powered commercial logging operation in North America in decades. This machine continued to operate after R. B. Mclean shuttered the steam-powered McLean Mill site in 1965. It ran until 1972 and was abandoned on site. It was restored by the Alberni Valley Industrial Heritage Society in 1986 for Expo 86 and, more recently, was re-certified for commercial use at McLean Mill. Agreements have been made with forestland owner Island Timberlands (owned by Brookfield Asset Management) to log, mill, and sell trees and lumber from the surroundings of McLean Mill.\n\nA wide-face steam donkey (called that because the width of the drum is greater in proportion to that in later machines) has been operational at the Tillamook County Pioneer Museum in Tillamook, Oregon, since the early 1980s. Manufactured by the Puget Sound Iron & Steel Works in the early 1900s, this donkey was abandoned in the woods when the Reiger family finished logging their land in about 1952. The steam donkey was rescued and restored from 1979 to 1981. It was donated to the Pioneer Museum by the Ned Rieger family and has been on display on the Museum grounds.\n\nA vertical steam capstan called 'donkey' for hauling fishing lines and nets is preserved on the museum fishing vessel 'Balder' in the historic harbour of Vlaardingen (near Rotterdam), the Netherlands.\n\n\n"}
{"id": "16320058", "url": "https://en.wikipedia.org/wiki?curid=16320058", "title": "Tantus", "text": "Tantus\n\nTantus is a company that produces silicone sex toys, including vibrators, dildos, butt-plugs and strap-on harness kits. They are based in Sparks, Nevada, and are the largest producer of silicone sex toys in the United States. All of their product are made from high grade medical silicone which is considered to have safety and environmental benefits, and is the main concept of their marketing. One of their best known products is the Feeldoe dildo, which is a harness-less strap-on dildo. At ANME 2000 Tantus was the first company to introduce silicone toys to mainstream adult store buyers.\n\nKris Victor and Metis Black co-founded Tantus in 1998. Metis Black became sole owner in 2008, at which time they broadened their offerings to include some of the first Silicone products for men, including the first silicone cockrings and slings.\n\nMetis Black has gone on to write several articles on sexual health and plastics, the chemistry of lubricants and has been featured on articles about green sex toys.\n\nBlack is also on the boards of several non-profits: The Center for Sex and Culture, where she is currently treasurer, and Woodhull Sexual Freedom Alliance. \n"}
{"id": "1605798", "url": "https://en.wikipedia.org/wiki?curid=1605798", "title": "Technological change", "text": "Technological change\n\nTechnological change (TC), technological development, technological achievement, or technological progress is the overall process of invention, innovation and diffusion of technology or processes. In essence, technological change covers the invention of technologies (including processes) and their commercialization or release as open source via research and development (producing emerging technologies), the continual improvement of technologies (in which they often become less expensive), and the diffusion of technologies throughout industry or society (which sometimes involves disruption and convergence). In short, technological change is based on both better and more technology.\n\nIn its earlier days, technological change was illustrated with the 'Linear Model of Innovation', which has now been largely discarded to be replaced with a model of technological change that involves innovation at all stages of research, development, diffusion, and use. When speaking about \"modeling technological change,\" this often means the process of innovation. This process of continuous improvement is often modeled as a curve depicting decreasing costs over time (for instance fuel cell which have become cheaper every year). TC is also often modelled using a learning curve, ex.: Ct=C0 * Xt^-b\n\nTechnological change itself is often included in other models (e.g. climate change models) and was often taken as an exogenous factor. These days TC is more often included as an endogenous factor. This means that it is taken as something you can influence. Today, there are sectors that maintain policy can influence the speed and direction of technological change. For instance, proponents of the Induced Technological Change hypothesis state that policy makers can steer the direction of technological advances by influencing relative factor prices and this can be demonstrated in the way climate policies impact the use of fossil fuel energy, specifically how it becomes relatively more expensive. Until now, the empirical evidence about the existence of policy induced innovation effects is still lacking and this may be attributed to a variety of reasons outside the sparsity of models (e.g. long-term policy uncertainty and exogenous drivers of (directed) innovation). A related concept is the notion of Directed Technical Change with more emphasis on price induced directional rather than policy induced scale effects.\n\nThe creation of something new, or a \"breakthrough\" technology. This is often included in the process of product development and relies on research. This can be demonstrated in the invention of the spreadsheet software. Newly invented technologies are conventionally patented.\n\nDiffusion pertains to the spread of a technology through a society or industry. The diffusion of a technology theory generally follows an S-shaped curve as early versions of technology are rather unsuccessful, followed by a period of successful innovation with high levels of adoption, and finally a dropping off in adoption as a technology reaches its maximum potential in a market. In the case of a personal computer, it has made way beyond homes and into business settings, such as office workstations and server machines to host websites.\n\nFor mathematical treatment of diffusion see: Logistic function\n\nFor examples of diffusion of technologies see: Diffusion of innovations#International Institute for Applied Systems Analysis (IIASA)\n\nFor assorted diffusion curves such as appliances, household electrification and communications see: Diffusion of innovations#Diffusion data\n\nUnderpinning the idea of technological change as a social process is general agreement on the importance of social context and communication. According to this model, technological change is seen as a social process involving producers and adopters and others (such as government) who are profoundly affected by cultural setting, political institutions and marketing strategies.\n\nIn free market economies, the maximization of profits is a powerful driver of technological change. Generally, only those technologies that promise to maximize profits for the owners of incoming producing capital are developed and reach the market. Any technological product that fails to meet this criterion - even though they may satisfy very important societal needs - are eliminated. Therefore, technological change is a social process strongly biased in favor of the financial interests of capital. There are currently no well established democratic processes, such as voting on the social or environmental desirability of a new technology prior to development and marketing, that would allow average citizens to direct the course of technological change.\n\nEmphasis has been on four key elements of the technological change process: (1) an innovative technology (2) communicated through certain channels (3) to members of a social system (4) who adopt it over a period of time. These elements are derived from Everett M. Rogers Diffusion of innovations theory using a communications-type approach.\n\nRogers proposed that there are five main attributes of innovative technologies which influence acceptance. He called these criteria ACCTO, which stands for Advantage, Compatibility, Complexity, Trialability, and Observability. \"Relative advantage\" may be economic or non-economic, and is the degree to which an innovation is seen as superior to prior innovations fulfilling the same needs. It is positively related to acceptance (e.g. the higher the relative advantage, the higher the adoption level, and vice versa). \"Compatibility\" is the degree to which an innovation appears consistent with existing values, past experiences, habits and needs to the potential adopter; a low level of compatibility will slow acceptance. \"Complexity\" is the degree to which an innovation appears difficult to understand and use; the more complex an innovation, the slower its acceptance. \"Trialability\" is the perceived degree to which an innovation may be tried on a limited basis, and is positively related to acceptance. Trialability can accelerate acceptance because small-scale testing reduces risk. \"Observability\" is the perceived degree to which results of innovating are visible to others and is positively related to acceptance.\n\nCommunication channels are the means by which a source conveys a message to a receiver. Information may be exchanged through two fundamentally different, yet complementary, channels of communication. Awareness is more often obtained through the \"mass media\", while uncertainty reduction that leads to acceptance mostly results from \"face-to-face communication\".\n\nThe social system provides a medium through which and boundaries within which, innovation is adopted. The structure of the social system affects technological change in several ways. Social norms, opinion leaders, change agents, government and the consequences of innovations are all involved. Also involved are cultural setting, nature of political institutions, laws, policies and administrative structures.\n\nTime enters into the acceptance process in many ways. The time dimension relates to the innovativeness of an individual or other adopter, which is the relative earlyness or lateness with which an innovation is adopted.\n\nIn economics, technological change is a change in the set of feasible production possibilities.\n\nA technological innovation is Hicks neutral, following John Hicks (1932), if a change in technology does not change the ratio of capital's marginal product to labour's marginal product for a given capital-to-labour ratio. A technological innovation is Harrod neutral (following Roy Harrod) if the technology is labour-augmenting (i.e. helps labor); it is Solow neutral if the technology is capital-augmenting (i.e. helps capital).\n\n"}
{"id": "10635829", "url": "https://en.wikipedia.org/wiki?curid=10635829", "title": "Teeny Ted from Turnip Town", "text": "Teeny Ted from Turnip Town\n\nTeeny Ted from Turnip Town (2007), published by Robert Chaplin, is certified by \"Guinness World Records\" as the world's smallest reproduction of a printed book. The book was produced in the Nano Imaging Laboratory at Simon Fraser University in Vancouver, British Columbia, Canada, with the assistance of SFU scientists Li Yang and Karen Kavanagh.\n\nThe book's size is 0.07 mm x 0.10 mm. The letters are carved into 30 microtablets on a polished piece of single crystalline silicon, using a focused-gallium-ion beam with a minimum diameter of 7 nanometers (this was compared to the head of a pin at 2 mm across). The book has its own ISBN, .\n\nThe story was written by Malcolm Douglas Chaplin and is \"a fable about Teeny Ted’s victory in the turnip contest at the annual county fair.\"\n\nThe book has been published in a limited edition of 100 copies by the laboratory and requires a scanning electron microscope to read the text.\n\nIn December 2012, a Library Edition of the book was published with a full title of Teeny Ted from Turnip Town & the Tale of Scale: A Scientific Book of Word Puzzles and an ISBN number . On the title page it is referred to as the \"Large Print Edition of the World's Smallest Book\". The book was published using funds from a successful Kickstarter campaign with contributors' names shown on the dust jacket.\n\n\n"}
{"id": "5983758", "url": "https://en.wikipedia.org/wiki?curid=5983758", "title": "The Ancient Engineers", "text": "The Ancient Engineers\n\nThe Ancient Engineers is a 1963 science book by L. Sprague de Camp, one of his most popular works. It was first published by Doubleday and has been reprinted numerous times by other publishers. Translations into German and Polish have also appeared. Portions of the work had previously appeared as articles in the magazines \"Fate\", \"Isis\" and \"Science Digest\".\n\nThe work is an examination of engineering through the ages from 3000 BC to 1519 AD, from the monumental works of the Egyptians through the speculative inventions of Leonardo da Vinci. The technological legacies of Mesopotamia, Egypt, Greece, Rome, China, the medieval Arabs and Europeans, and Renaissance Europe, are all covered in separate sections, focusing particularly on architectural, military and civil engineering.\n\nThe following review is often quoted in reference to this book: \"Mr. de Camp has the trick of being able to show technology engaging in feats as full of derring-do as those of Hannibal's army. History as it should be told.\" —Isaac Asimov, \"The New York Times Book Review\", 1963\n\n\n"}
{"id": "23157899", "url": "https://en.wikipedia.org/wiki?curid=23157899", "title": "Thermal dissolution", "text": "Thermal dissolution\n\nThermal dissolution is a hydrogen-donor solvent refining process. It may be used for the shale oil extraction and coal liquefaction. Other liquids extraction processes from solid fuels are pyrolysis and hydrogenation.\n"}
{"id": "13338605", "url": "https://en.wikipedia.org/wiki?curid=13338605", "title": "Thomas W. Wälde", "text": "Thomas W. Wälde\n\nThomas W. Wälde (9 January 1949 – 11 October 2008), former United Nations (UN) Inter-regional Adviser on Petroleum and Mineral Legislation, was Professor & Jean-Monnet Chair at the Centre for Energy, Petroleum and Mineral Law and Policy (CEPMLP), Dundee.\n\nThomas Wälde died on 11 October 2008 in the south of France.\n\nThomas Wälde grew up in Heidelberg (Germany) and went to school at the Kurfuerst-Friedrich-Gymnasium. He was from a South-West German family; his great uncle, Reinhold Maier, was the first Ministerpraesident of Baden-Wuerttemberg; another uncle, Heinz Maier-Leibnitz, a well known German professor of nuclear physics, director of German and French nuclear physics research laboratories and President of the German National Science Foundation (DFG).\n\nThomas Wälde lived and worked from his home/offices outside St Andrews, Scotland, Heidelberg and Bormes-Les-Mimosas. His second wife, Professor Charlotte Wälde, has served as co-director of the AHRC Centre on Intellectual Property Law at Edinburgh University. His son, from his first marriage, is reading law in Vienna and his daughter attends St Leonard's School in St Andrews.\n\nHe studied law, in the traditional German way, at the Universities of Heidelberg, Lausanne-Geneva, Berlin and Frankfurt, with his law degree (Referendar) and doctorate (Juristische Folgenorientierung - a study on decision theory as an interpretative tool for international economic law) - in Frankfurt. \n\nHe did his professional legal training in Frankfurt (including as an intern at the UN Centre on Transnational Corporations in New York) and obtained his \"Assessor\" grade there. He also worked as Associate Officer and resident consultant with the UN/CTC in New York and UNIDO in Vienna and was fellow at the Institute for International Economic Law in Frankfurt (founded by Heinrich Kronstein who was also professor at Georgetown Law School in the US and founder of the Washington-based International Law Institute). \n\nWälde was at Harvard Law School (1972–74) as LL.M. and subsequent visiting scholar. His Harvard LL.M. dissertation - on comparative company law - was published in 1974. Detlev Vagts was his academic mentor and teacher at Harvard Law School; he also worked as research assistant for the late Professor and ICJ Judge Richard Baxter. In 1978, he obtained the now prestigious price by the German Research Foundation (DFG) for a publication on transnational investment agreements (published in Rabelz Zeitschrift) which was, a decade later, named after Heinz Maier-Leibnitz, at one time the President of DFG.\n\nICJ president Roslyn Higgins had served as an academic mentor for Thomas Wälde since he moved from the United Nations in New York to the University of Dundee, Scotland in 1991.\n\nWälde started in 1980 as UN interregional adviser on mineral law - with the remit to provide rapid ad-hoc advisory services to developing country governments throughout the world. He later became responsible for energy/petroleum and international investment policy as well. At the UN, he advised over 60 governments on legislative reform and contract negotiations with international investors mainly. He was also, from 1981–1983, UN investigator on occupation practices in Palestinian territories and responsible for the Secretary General's reports on \"Permanent Sovereignty over Natural resources\" and the Permanent Sovereignty in Occupied Palestinian territories reports. Wälde set up numerous investment advisory projects - combining legal, financial and technical expertise - to support investment project negotiations; organised training seminars and international UN conferences in the field of mining and oil and gas. He initiated the UN project for environmental guidelines in mining and was chair of the drafting group that produced the first version of the \"Berlin Guidelines\" in 1990.\n\nAt Dundee, Wälde, as Professor and Executive Director, developed the Centre for Petroleum and Mineral Law into the world's largest graduate school in its field - with four students in 1991 growing to well over 140 LL.M., MBA, MSC, MBA and PHD students in 2002/2003 (when he gave up the directorship). Student numbers went up, from 1991 to 2002, by a factor of about 40 and fees were raised by a factor of 4. The centre obtained as a recognition for its spectacular growth the prestigious Queen's Award for Enterprise in 2004.\n\nFollowing his directorship of CEPMLP/Dundee, Wälde developed academic and professional expertise in international dispute resolution, both mediation and arbitration for large, complex, cross-border transnational disputes, primarily (but not exclusively) in the field of oil, gas, energy, infrastructure and mining (but also gaming and private equity) based on contract and investment treaties. He set up OGEMID, the mainly international electronic discussion and intelligence forum which is by now a \"must\" for anybody seriously engaged in international investment disputes, but also in complex commercial disputes in the energy and resources field. He acted as co-arbitrator in the NAFTA Chapter XI arbitration Thunderbird v Mexico; as co-arbitrator in the BIT-based arbitration of K+ v Czech Republic; and, in 2008, as co-arbitrator in a CAFTA dispute. He has also been appointed to international disputes in the field of mining and energy (electricity). He frequently acts as expert witness and (expert) co-counsel in international arbitrations relating to oil, gas, energy, mining and infrastructure, including Glamis v US, Duke v Peru, Nykomb v Latvia plus commercial, BIT, ECT and NAFTA-based arbitratons under UNCITRAL, ICSID, NAFTA and CAFTA procedural rules. He has also mediated commercial disputes between international oil companies and the SwePol dispute concerning an electricity interconnector between Poland and Sweden.\n\nHe was a frequent expert, but also counsel, mediator and arbitrator in international energy and investment disputes (International Centre for Settlement of Investment Disputes (ICSID), the North American Free Trade Agreement (NAFTA), Energy Charter Treaty, Bilateral Investment Treaty (BIT) and commercial contract disputes). Special member of AIPN, member of several int'l arbitral institutions, Rechtsanwalt (Frankfurt) & barrister (Lincoln's Inn - Essex Court Chambers, London). Adviser to the international institutions in the oil and gas field (the Organization of Petroleum Exporting Countries, International Energy Agency (IEA), UN, APEC, European Union, World Bank). Identified as leading international energy lawyer in a Euromoney survey, leading international lawyer in a Cambridge-sponsored Who's Who in International law and one of three international arbitrators resident in Scotland. Formerly (up to 1990) Interregional Adviser on Mineral and Petroleum law and International Investment Policy, United Nations, New York; staff and consultant for UN Centre on Transnational Corporations and United Nations Industrial Development Organization (1976–1980); Reporter for International Law Association Foreign Investment Law Committee (damages and tax-related investment disputes). Frequent speaker and author on international investment law, natural resources, mineral, energy and oil and gas law, including renegotiation, taxation, indirect expropriation, de-commissioning (abandonment) of offshore operations; state enterprise privatisation, investment treaties, environmental regulation; arbitration; Energy Charter Treaty.\n\nThomas Wälde was a prolific writer and speaker, and spoke at conferences around the world. He served as visiting professor at Panthéon-Assas University and American University. He is fellow of the investment programme at the British Institute of International & Comparative Law, at Columbia University's Law School and other EU-law focused institutions. He obtained a Jean-Monnet Chair in an EU-wide competition from the EU Commission in 1995 - on EU Energy and Economic Law. He was a \"Special Member\" of the Association of Int'l Petroleum Negotiators (AIPN), panel of energy/resources arbitrators of the Permanent court of Int'l Arbitration; Member of the Institut pour l'Arbitrage International; member of the IBA, LCIA, DIS; ICDR, ILA; ASIL, ITA (Academic Council). He was named in several professional guides as a leading international energy lawyers and one of three international arbitrators in Scotland. He was formerly the Chair, Energy, Petroleum and Mineral Law and Policy Trust; Director of the 2004 Hague Academy for Int'l Law Research Seminar on Int'l Investment Law. He was on the IUCN Energy Working Group and the World Energy Council's Task force on Energy Investment & Trade.\n\nHe could write and speak in English, German, French and Spanish, with some knowledge of Italian, Russian and Arabic. He worked in all corners of the world. He developed and led negotiation assistance inter alia in several investment projects related to coal (Colombia), gold (Mali), Guyana (uranium), Dominican Republic (nickel, oil), Cayman Islands (oil) – all led to completed transaction.\n\n\n\n"}
{"id": "19925726", "url": "https://en.wikipedia.org/wiki?curid=19925726", "title": "Truviso", "text": "Truviso\n\nTruviso (pronounced \"true-VEE-so\") is a continuous analytics, venture-backed, startup headquartered in Foster City, California developing and supporting its solution leveraging PostgreSQL, to deliver a proprietary analytics solutions for net-centric customers. Truviso was acquired by Cisco Systems, Inc. on May 4, 2012.\n\nTruviso was founded in 2006 by UC Berkeley professor Michael J. Franklin and his Ph.D. student Sailesh Krishnamurthy, advancing on the research of Berkeley's Telegraph project.\n\nTruviso's TruCQ product leverages and extends the open source PostgreSQL database to enable analysis of streaming data, including queries that combine those streams with other streaming data or with historical/staged data. One public example of Truviso's customers using continuous analytics is the dynamic tag cloud visualization of blog indexer Technorati.\n\nTruviso is one of the pioneers in the continuous analytics space which seeks to alter how business intelligence is done—rather than accumulating data first and then running queries on the data set stored in a relational database or a data warehouse, Truviso has always-on queries which process streaming data as it arrives, continuously. For many queries this approach yields results hundreds or thousands of times faster and more efficiently.\n\nTruviso has received funding from ONSET Ventures, Diamondhead Ventures, and the UPS Strategic Enterprise Fund.\n\nTruviso was acquired by Cisco on May 4, 2012\n\nTruviso's analytics approach is to have always-on queries analyzing streaming data. This strategy for handling continuously flowing data is different from traditional business intelligence approaches of first accumulating data and then running batch queries for reporting and analysis.\n\nTruviso has developed a continuous analytics solution to solve the challenges of high-volume, always-on data analysis. Truviso's solution is based on a scalable PostgreSQL platform capable of concurrent query execution, utilizing standard SQL against live streams of data. Truviso's approach enables analysis of heterogeneous data regardless of whether the data is flowing, staged, or some combination of the two.\n\n\nOn May 4, 2010, Truviso announced that the company developed a specific application for web analytics called Visitor Insight & Analytics.\n\n\n"}
{"id": "31506584", "url": "https://en.wikipedia.org/wiki?curid=31506584", "title": "Whitehurst &amp; Son sundial", "text": "Whitehurst &amp; Son sundial\n\nThe Whitehurst & Son sundial was produced in Derby in 1812 by the nephew of John Whitehurst. It is a fine example of a precision sundial telling local apparent time with a scale to convert this to local mean time, and is accurate to the nearest minute. The sundial is now housed in the Derby Museum and Art Gallery. \n\nThe Whitehurst family was known in Derby as eminent mechanics. John Whitehurst (1713–1788) was born in Congleton, but came to Derby where he entered business as a watch and clock maker. He moved to London when appointed to the post of Inspector of Weights. His nephew continued the business under the name of Whitehurst & Son. The family business was known for their turret clocks.\n\nSundial construction is based on understanding the geometry of the solar system, and particularly how the sun will cast a shadow onto a flat surface, in this case a horizontal surface. Each day over a yearly cycle the shadow will be different from the day before, and the shadow is specific to the location of the dial, particularly its latitude. The dial is designed to tell local apparent time so the longitude is not significant. By this we mean that noon will be at the point when the sun is highest in sky and due south, standard time would be when the sun was due south at another point such as the Royal Observatory at Greenwich. Derby is at 1° 28′ 46.2″ West of Greenwich, so the sun is approximately 5 mins and 52.05 s later in reaching noon. The other point to consider when telling the time is that as the earth moves around the sun in a slight ellipse, day length varies slightly giving a cumulative difference from the average, of up to 16 minutes in November and February, this, with another correction, is known as the equation of time and was really irrelevant until people started comparing sundials with mechanical clocks, which must either ignore the consequences or be re balanced each day to make them correspond with the natural cycle. They measure average time or local mean time. Railways timetabling demanded a fixed noon and fixed day leading to the adoption of Greenwich Mean Time. The local Midland Railway had adopted Greenwich Mean Time by January 1848.\n\nThis particular bronze sundial is marked \"Whitehurst and Son/Derby/1812\" and is thought to have been made for George Benson Strutt (who was the younger brother of the cotton spinner William Strutt), for his home Bridge Hill House, in Belper. This has the precise location of 53° 1′ 49.08″ north, and 1° 29′ 26.88″ West of Greenwich, which is slightly different from those of Derby, at 52° 55′ 00″ north, a difference of 6′ 49″. The Longitude is almost identical, and gives only a 2-second time difference.\n\nThis dial is stoutly made, with a thick gnomon, one edge of gnomon is the style which throws the shadow before noon, and the other edge is the style which throws the shadow after noon. The dial plate is not made up of one complete circle, but two semicircles separated by the thickness of the style. In this design of sundial, the angle of the style to the dial plate is exactly the same as latitude, which is 53° 1′ 49″ the latitude of the Bridge Hill House. The dial plate will be perfectly horizontal, slight adjustments for latitude can be made were the dial moved by shimmying up the dial plate by the change in degree, from the horizontal. As the dial is now 0° 6′ 49″, or about 1/10th of a degree south- the nose of the gnomon needs to be raised by that amount.\n\nIn the \"horizontal sundial\" (also called a \"garden sundial\"), the plane that receives the shadow is aligned horizontally, rather than being perpendicular to the style as in the equatorial dial. Hence, the line of shadow does not rotate uniformly on the dial face; rather, the hour lines are spaced according to a calculation.\nThe dial plate is precision engraved, the hour lines having been calculated using the formula:\n\nwhere λ is the sundial's geographical latitude (and the angle the style makes with horizontal), θ is the angle between a given hour-line and the noon hour-line (which always points towards true North) on the plane, and \"t\" is the number of hours before or after noon.\n\nFor each of the hours 1 to 6, the formula is calculated. For instance, for Belper, England, at 3 hours after noon, we substitute the numbers 53.03 (Belper's latitude) and 3 into the formula\n\nThis generates these results\nThe hours before noon are exactly the same, the dial is symmetrical, the other lines are mirror images of those above- 2 way symmetry.\nIn the same manner, the half hours and the minute lines would be calculated. As noon lies exactly on the north-south line, and not 5' and 54\" to one side we can tell that this dial is telling Belper time and not Greenwich time.\n\nSignificantly, on the plate we can see a pair of scales that help the observer make the equation of time correction. One scale gives the date in months and days while alongside of it another engraved with the minutes on that day that the watch would be running faster or slower. Here it is labeled \"Watch Slower, Watch Faster. The 15th April is one day when no conversion needs to be made.\" This dial can be used both to read solar time shown by sundials and also the mean time that is favoured by clocks, with the practical purpose that observers can use the dial to calibrate their pocket watches, which in 1812 would not always run true. By 1820 watch manufacture had improved:the Lever escapement had become universally adopted and frequent calibration was no longer needed.\n\nAnother Whitehurst sundial dated around 1800 sold for £1850 in 2005 in Derby.\n\n\n\n"}
