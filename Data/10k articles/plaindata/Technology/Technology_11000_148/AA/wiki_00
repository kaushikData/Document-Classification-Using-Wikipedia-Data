{"id": "5235850", "url": "https://en.wikipedia.org/wiki?curid=5235850", "title": "AV receiver", "text": "AV receiver\n\nAn audio/video receiver (AVR) is a consumer electronics component used in a home theater. Its purpose is to receive audio and video signals from a number of sources, and to process them to drive loudspeakers and displays such as a television, monitor or video projector. Inputs may come from a satellite receiver, radio, DVD players, Blu-ray Disc players, VCRs or video game consoles. The AVR source selection and settings such as volume, are typically set by a remote controller.\n\nThe term receiver basically refers to an amplifier that has a built-in radio tuner. With A/V receivers, the basic functionality is to receive an audio signal, amplify the audio signal, and allow pass-through of the corresponding video signal to a display device such as a projector or a television.\n\nAs home entertainment options expanded, so did the role of the receiver. The ability to handle a variety of digital audio signals was added. More amplifiers were added for surround-sound playback. Video switching was added to simplify changing from one device to another. Within the last few years, video processing has been added to many receivers.\n\nThe term audio/video receiver (AVR) or Home Theater Receiver is used to distinguish the multi-channel audio/video receiver (home theater receiver) from the simpler stereo receiver, though the primary function of both is amplification.\n\nAV receivers may also be known as \"digital audio-video receivers\" or \"digital media renderers\".\n\nThe AV receiver is classified as an audio frequency electronic amplifier. But with the rapid addition of several features, AV receivers now generally have significant additional functionality.\n\nStereo receivers have two channels of amplification, while AV receivers may have more than two. The standard for AV receivers is five channels of amplification. These are usually referred to as 5.1 receivers. This provides for a left, right, center, left surround and right surround speaker to be powered by the receiver. 7.1 receivers are becoming more common and provide for two additional surround channels, left rear surround and right rear surround. The \".1\" refers to the LFE (low-frequency effects/bass) channel, the signal of which is usually sent to an amplified subwoofer unit. 5.1 and 7.1 receivers don't usually provide amplification for this channel. Instead, they provide a line level output.\n\nAudio amplifier power, normally specified in Watts, is not always as significant as it may seem from the specification. Due to the logarithmic nature of human hearing, audio power or sound pressure level (SPL), must be increased by ten times to sound twice as loud. This is why SPL is measured on a logarithmic scale in decibels (dBs). An increase of 10dBs results in a perceived doubling of loudness. Another complication with human hearing is that as the SPL decreases the perceived volume of the low and high frequencies decreases faster than the central frequencies around 2 kHz.\n\nThere are different standards for rating amplifier power depending on country, manufacturer, and model. Other factors also come into play: distortion, headroom, speaker efficiency. Thus, it is possible for an amplifier with a specified lower power to sound louder than an amplifier with a specified higher power.\n\nBecause of these factors, it is not easy to compare the perceived loudness of amplifiers solely from their specified power in Watts.\n\nAV receivers usually provide one or more decoders for sources with more than two channels of audio information. This is most common with movie soundtracks, which use one of a variety of different types of encoding formats.\n\nThe first common soundtrack format was Dolby Pro Logic, a surround sound processing technology. This format contains a center channel and a surround channel mixed into the left and right channels using a process called matrixing, providing a total of four channels. Receivers with Dolby Pro Logic decoders can separate out the center and surround channels from the left and right channels.\n\nWith the introduction of the DVD, the Dolby Digital format became a standard. Dolby Digital ready receivers included inputs and amplifiers for the additional channels. Most current AV receivers provide a Dolby Digital decoder and at least one digital S/PDIF input which can be connected to a source which provides a Dolby Digital output.\n\nA somewhat less common surround sound decoder called DTS is standard on current AV receivers.\n\nWhen Dolby Labs and DTS introduced technologies to add a rear center surround channel, these technologies found their way into AV Receivers. Receivers with six amplifiers (known as 6.1 receivers) will typically have both Dolby and DTS's technologies. These are Dolby Digital EX and DTS ES.\n\nDolby introduced Dolby Pro Logic II to allow stereo sources to play back as if they were encoded in surround sound. DTS introduced a similar technology, . These decoders have become common on most current receivers.\n\nAs the number of playback channels were increased on receivers, other decoders have been added to some receivers. For example, Dolby Labs created Dolby Pro Logic IIx to take advantage of receivers with more than five channels of playback.\n\nWith the introduction of high definition players (e.g. Blu-ray Disc and HD DVD), yet more decoders have been added to some receivers. Lossless Dolby TrueHD and DTS-HD Master Audio decoders are available on many receivers.\n\nWhile Dolby Digital has been the standard for television and video games, Dolby Digital Plus has been adopted for online streaming services such as Netflix, and adds a much higher bit rate and can support more channels.\n\nDolby Atmos and DTS:X adds in ceiling height channels.\n\nMost receivers offer specialized Digital Signal Processors (DSP) made for handling various presets and audio effects. Some may offer simple equalizers and balance adjustments to complex DSP audio field simulations such as \"Hall\", \"Arena\", \"Opera\", etc. that simulate or attempt to replicate as if the audio were being played in the places through use of surround sound and echo effects.\n\nThere are a variety of possible connections on an AV receiver. Standard connectors include:\n\n\nAnalog audio connections usually use RCA plugs in stereo pairs. Inputs and outputs are both common. Outputs are provided mainly for cassette tape decks.\n\nAnalog audio connections using XLR (Balanced) connectors are uncommon, and are usually found on more expensive receivers.\n\nSome receivers are also equipped with phono inputs, for connecting a turntable with a magnetic cartridge, although many receivers are lacking this input. In the last years, since vinyl players are gaining popularity, there are some manufacturers of A/V receivers which are offering a phono input on some models. Some receivers also have a selector for either MM/MC cartridge type. This feature is mainly offered to allow people owning a turntable to connect it and listen to their vinyl collection. Most receivers will only play in stereo, either front or main speakers, but some receivers may play on all speakers deppending on the manufacturer and model.\n\nDigital connections allow for the transmission of PCM, Dolby Digital or DTS audio. Common devices include CD players, DVD players, or satellite receivers.\n\nComposite video connections use a single RCA plug on each end. Composite video is standard on all AV receivers allowing for the switching of video devices such as VHS players, cable boxes, and game consoles. DVD players may be connected via composite video connectors although a higher bandwidth connection is recommended.\n\nS-Video connections offer better quality than composite video. It uses a DIN jack.\n\nSCART connections generally offer the best quality video at standard-definition, due to the use of pure RGB signalling (although composite and S-Video may alternatively be offered over a SCART connector). SCART provides video and audio in one connection.\n\nComponent video has become the best connection for analog video as higher definitions such as 720p have become common. The YPbPr signalling provides a good compromise between resolution and colour definition.\n\nHDMI is becoming common on AV receivers. It provides for the transmission of both audio and video. HDMI is a relatively new technology and there are reported issues with devices not properly working with each other (referred to as hand-shake issues between devices), especially cable/satellite boxes connected to a display through an AV receiver. Different levels of support are provided by receivers with HDMI connections. Some will only switch video and not provide for audio processing. Some will not handle multi-channel LPCM. Multi-channel LPCM is a common way for Blu-ray Disc and HD DVD players to transmit the best possible audio.\n\nSome AV Receivers can convert from one video format to another. This is commonly called upconversion or transcoding. A smaller number of receivers provide for de-interlacing of video signals. For example, a receiver with upconversion, deinterlacing and upscaling can take an interlaced composite signal at 480i (480 lines per frame sent as a field of 240 even numbered lines 0,2,4,8...478 followed by a field of 240 odd numbered lines 1,3,5...479) and convert it to component video while also deinterlacing and upscaling it to a higher resolution such as 720p (720 lines per frame with all lines in normal sequence 0,1,2...719).\n\nAV receivers though primarily used for amplification may or may not have an inbuilt AM/FM radio tuner among other features such as LAN connectivity for various Internet applications and some with multi-room audio solutions.\n\nEven though some AVRs may have an AM/FM tuner it is not a primary or mandatory function as an AVR still remains an amplifier.\n\nSome models have HD Radio tuners.\n\nSome models have Internet radio and PC streaming access capabilities with an Ethernet port.\n\nSome notable brands of AV receivers include:\n\n\n"}
{"id": "4075450", "url": "https://en.wikipedia.org/wiki?curid=4075450", "title": "Actor model middle history", "text": "Actor model middle history\n\nIn computer science, the Actor model, first published in 1973 , is a mathematical model of concurrent computation. This article reports on the middle history of the Actor model in which major themes were initial implementations, initial applications, and development of the first proof theory and denotational model. It is the follow on article to Actor model early history which reports on the early history of the Actor model which concerned the basic development of the concepts. The article Actor model later history reports on developments after the ones reported in this article.\n\nCarl Hewitt [1974] published the principle of Actor induction which is:\nIn his doctoral dissertation, Aki Yonezawa developed further techniques for proving properties of Actor systems including those that make use of migration. Russ Atkinson and Carl Hewitt developed techniques for proving properties of Serializers that are guardians of shared resources. Gerry Barber's doctoral dissertation concerned reasoning about change in knowledgeable office systems.\n\nGarbage collection (the automatic reclamation of unused storage) was an important theme in the development of the Actor model.\n\nIn his doctoral dissertation, Peter Bishop developed an algorithm for garbage collection in distributed systems. Each system kept lists of links of pointers to and from other systems. Cyclic structures were collected by incrementally migrating Actors (objects) onto other systems which had their addresses until a cyclic structure was entirely contained in a single system where the garbage collector could recover the storage.\n\nHenry Baker developed an algorithm for real-time garbage collection in his doctoral dissertation. The fundamental idea was to interleave collection activity with construction activity so that there would not have to be long pauses while collection takes place. See incremental garbage collection.\n\nHenry Lieberman and Carl Hewitt [1983] developed a real time garbage collection based on the lifetimes of Actors (Objects). The fundamental idea was to allocate Actors (objects) in generations so that only the latest generations would have to be examined during a garbage collection. See generational garbage collection.\n\nHenry Lieberman, Dan Theriault, \"et al.\" developed Act1, an Actor programming language. Subsequently for his masters thesis, Dan Theriault developed Act2. These early proof of concept languages were rather inefficient and not suitable for applications. In his doctoral dissertation, Ken Kahn developed Ani, which he used to develop several animations. Bill Kornfeld developed the Ether programming language for the Scientific Community Metaphor in his doctoral dissertation. William Athas and Nanette Boden [1988] developed Cantor which is an Actor programming language for scientific computing. Jean-Pierre Briot [1988, 1999] developed means to extend Smalltalk 80 for Actor computations. Christine Tomlinson, Mark Scheevel, Greg Lavender, Greg Meredith, \"et al.\" [1995] at MCC developed an Actor programming language for InfoSleuth agents in Rosette.\n\nCarl Hewitt, Beppe Attardi, and Henry Lieberman [1979] developed proposals for delegation in message passing. This gave rise to the so-called inheritance anomaly controversy in object-oriented concurrent programming languages [Satoshi Matsuoka and Aki Yonezawa 1993, Giuseppe Milicia and Vladimiro Sassone 2004].\n\nIn his doctoral dissertation, Will Clinger developed the first denotational model of Actor systems. See denotational semantics of the Actor model.\n\n"}
{"id": "593", "url": "https://en.wikipedia.org/wiki?curid=593", "title": "Animation", "text": "Animation\n\nAnimation is a method in which pictures are manipulated to appear as moving images. In traditional animation, images are drawn or painted by hand on transparent celluloid sheets to be photographed and exhibited on film. Today most animations are made with computer-generated imagery (CGI). Computer animation can be very detailed 3D animation, while 2D computer animation can be used for stylistic reasons, low bandwidth or faster real-time renderings. Other common animation methods apply a stop motion technique to two and three-dimensional objects like paper cutouts, puppets or clay figures.\n\nCommonly the effect of animation is achieved by a rapid succession of sequential images that minimally differ from each other. The illusion—as in motion pictures in general—is thought to rely on the phi phenomenon and beta movement, but the exact causes are still uncertain. \nAnalog mechanical animation media that rely on the rapid display of sequential images include the phénakisticope, zoetrope, flip book, praxinoscope and film. Television and video are popular electronic animation media that originally were analog and now operate digitally. For display on the computer, techniques like animated GIF and Flash animation were developed.\n\nApart from short films, feature films, animated gifs and other media dedicated to the display moving images, animation is also heavily used for video games, motion graphics and special effects.\n\nThe physical movement of image parts through simple mechanics in for instance the moving images in magic lantern shows can also be considered animation. Mechanical animation of actual robotic devices is known as animatronics.\n\nAnimators are artists who specialize in creating animation.\n\nThe word \"animation\" stems from the Latin \"animationem\" (nominative \"animatio\"), noun of action from past participle stem of \"animare\", meaning \"the action of imparting life\". The primary meaning of the English word is \"liveliness\" and has been in use much longer than the meaning of \"moving image medium\".\n\nThe history of animation started long before the development of cinematography. Humans have probably attempted to depict motion as far back as the paleolithic period. Shadow play and the magic lantern offered popular shows with moving images as the result of manipulation by hand and/or some minor mechanics.\n\nA 5,200-year old pottery bowl discovered in Shahr-e Sukhteh, Iran, has five sequential images painted around it that seem to show phases of a goat leaping up to nip at a tree. In 1833, the phenakistiscope introduced the stroboscopic principle of modern animation, which would also provide the basis for the zoetrope (1866), the flip book (1868), the praxinoscope (1877) and cinematography.\nCharles-Émile Reynaud further developed his projection praxinoscope into the Théâtre Optique with transparent hand-painted colorful pictures in a long perforated strip wound between two spools, patented in December 1888. From 28 October 1892 to March 1900 Reynaud gave over 12,800 shows to a total of over 500.000 visitors at the Musée Grévin in Paris. His \"Pantomimes Lumineuses\" series of animated films each contained 300 to 700 frames that were manipulated back and forth to last 10 to 15 minutes per film. Piano music, song and some dialogue were performed live, while some sound effects were synchronized with an electromagnet.\n\nWhen film became a common medium some manufacturers of optical toys adapted small magic lanterns into toy film projectors for short loops of film. By 1902, they were producing many chromolithography film loops, usually by tracing live-action film footage (much like the later rotoscoping technique).\n\nSome early filmmakers, including J. Stuart Blackton, Arthur Melbourne-Cooper, Segundo de Chomón and Edwin S. Porter experimented with stop-motion animation, possibly since around 1899. Blackton's \"The Haunted Hotel\" (1907) was the first huge success that baffled audiences with objects apparently moving by themselves and inspired other filmmakers to try the technique for themselves.\n\nJ. Stuart Blackton also experimented with animation drawn on blackboards and some cutout animation in \"Humorous Phases of Funny Faces\" (1906).\nIn 1908, Émile Cohl's \"Fantasmagorie\" was released with a white-on-black chalkline look created with negative prints from black ink drawings on white paper. The film largely consists of a stick figure moving about and encountering all kinds of morphing objects, including a wine bottle that transforms into a flower.\n\nInspired by Émile Cohl's stop-motion film \"Les allumettes animées [Animated Matches]\" (1908), Ladislas Starevich started making his influential puppet animations in 1910.\n\nWinsor McCay's \"Little Nemo\" (1911) showcased very detailed drawings. His \"Gertie the Dinosaur\" (1914) was an also an early example of character development in drawn animation.\n\nDuring the 1910s, the production of animated short films typically referred to as \"cartoons\", became an industry of its own and cartoon shorts were produced for showing in movie theaters. The most successful producer at the time was John Randolph Bray, who, along with animator Earl Hurd, patented the cel animation process that dominated the animation industry for the rest of the decade.\n\n\"El Apóstol\" (Spanish: \"The Apostle\") was a 1917 Argentine animated film utilizing cutout animation, and the world's first animated feature film. Unfortunately, a fire that destroyed producer Federico Valle's film studio incinerated the only known copy of \"El Apóstol\", and it is now considered a lost film.\n\nThe earliest extant feature-length animated film is The Adventures of Prince Achmed (1926) made by director Lotte Reiniger and her collaborators Carl Koch and Berthold Bartosch.\n\nIn 1932, the first short animated film created entirely with Technicolor (using red/green/blue photographic filters and three strips of film) was Walt Disney's \"Flowers and Trees\", directed by Burt Gillett. But, the first feature film that was done with this technique, apart from the movie The Vanities Fair (1935), by Rouben Mamoulian, was \"Snow White and the Seven Dwarfs\", also by Walt Disney.\n\nIn 1958, Hanna-Barbera released \"The Huckleberry Hound Show\", the first half hour television program to feature only in animation. Terrytoons released \"Tom Terrific\" that same year. Television significantly decreased public attention to the animated shorts being shown in theaters.\n\nComputer animation has become popular since \"Toy Story\" (1995), the first feature-length animated film completely made using this technique.\n\nIn 2008, the animation market was worth US$68.4 billion. Animation as an art and industry continues to thrive as of the mid-2010s because well-made animated projects can find audiences across borders and in all four quadrants. Animated feature-length films returned the highest gross margins (around 52%) of all film genres in the 2004–2013 timeframe.\n\nTraditional animation (also called cel animation or hand-drawn animation) was the process used for most animated films of the 20th century. The individual frames of a traditionally animated film are photographs of drawings, first drawn on paper. To create the illusion of movement, each drawing differs slightly from the one before it. The animators' drawings are traced or photocopied onto transparent acetate sheets called cels, which are filled in with paints in assigned colors or tones on the side opposite the line drawings. The completed character cels are photographed one-by-one against a painted background by a rostrum camera onto motion picture film.\n\nThe traditional cel animation process became obsolete by the beginning of the 21st century. Today, animators' drawings and the backgrounds are either scanned into or drawn directly into a computer system. Various software programs are used to color the drawings and simulate camera movement and effects. The final animated piece is output to one of several delivery media, including traditional 35 mm film and newer media with digital video. The \"look\" of traditional cel animation is still preserved, and the character animators' work has remained essentially the same over the past 70 years. Some animation producers have used the term \"tradigital\" (a play on the words \"traditional\" and \"digital\") to describe cel animation that uses significant computer technology.\n\nExamples of traditionally animated feature films include \"Pinocchio\" (United States, 1940), \"Animal Farm\" (United Kingdom, 1954), \"Lucky and Zorba\" (Italy, 1998), and \"The Illusionist\" (British-French, 2010). Traditionally animated films produced with the aid of computer technology include \"The Lion King\" (US, 1994), \"The Prince of Egypt\" (US, 1998), \"Akira\" (Japan, 1988), \"Spirited Away\" (Japan, 2001), \"The Triplets of Belleville\" (France, 2003), and \"The Secret of Kells\" (Irish-French-Belgian, 2009).\n\nFull animation refers to the process of producing high-quality traditionally animated films that regularly use detailed drawings and plausible movement, having a smooth animation. Fully animated films can be made in a variety of styles, from more realistically animated works like those produced by the Walt Disney studio (\"The Little Mermaid\", \"Beauty and the Beast\", \"Aladdin\", \"The Lion King\") to the more 'cartoon' styles of the Warner Bros. animation studio. Many of the Disney animated features are examples of full animation, as are non-Disney works, \"The Secret of NIMH\" (US, 1982), \"The Iron Giant\" (US, 1999), and \"Nocturna\" (Spain, 2007). Fully animated films are animated at 24 frames per second, with a combination of animation on ones and twos, meaning that drawings can be held for one frame out of 24 or two frames out of 24.\n\nLimited animation involves the use of less detailed or more stylized drawings and methods of movement usually a choppy or \"skippy\" movement animation. Limited animation uses fewer drawings per second, thereby limiting the fluidity of the animation. This is a more economic technique. Pioneered by the artists at the American studio United Productions of America, limited animation can be used as a method of stylized artistic expression, as in \"Gerald McBoing-Boing\" (US, 1951), \"Yellow Submarine\" (UK, 1968), and certain anime produced in Japan. Its primary use, however, has been in producing cost-effective animated content for media for television (the work of Hanna-Barbera, Filmation, and other TV animation studios) and later the Internet (web cartoons).\n\nRotoscoping is a technique patented by Max Fleischer in 1917 where animators trace live-action movement, frame by frame. The source film can be directly copied from actors' outlines into animated drawings, as in \"The Lord of the Rings\" (US, 1978), or used in a stylized and expressive manner, as in \"Waking Life\" (US, 2001) and \"A Scanner Darkly\" (US, 2006). Some other examples are \"Fire and Ice\" (US, 1983), \"Heavy Metal\" (1981), and \"Aku no Hana\" (2013).\n\nLive-action/animation is a technique combining hand-drawn characters into live action shots or live action actors into animated shots. One of the earlier uses was in Koko the Clown when Koko was drawn over live action footage. Other examples include \"Allegro Non Troppo\" (Italy, 1976), \"Who Framed Roger Rabbit\" (US, 1988), \"Space Jam\" (US, 1996) and \"Osmosis Jones\" (US, 2001).\n\nStop-motion animation is used to describe animation created by physically manipulating real-world objects and photographing them one frame of film at a time to create the illusion of movement. There are many different types of stop-motion animation, usually named after the medium used to create the animation. Computer software is widely available to create this type of animation; traditional stop motion animation is usually less expensive but more time-consuming to produce than current computer animation.\n\n\n\nComputer animation encompasses a variety of techniques, the unifying factor being that the animation is created digitally on a computer. 2D animation techniques tend to focus on image manipulation while 3D techniques usually build virtual worlds in which characters and objects move and interact. 3D animation can create images that seem real to the viewer.\n\n2D animation figures are created or edited on the computer using 2D bitmap graphics and 2D vector graphics. This includes automated computerized versions of traditional animation techniques, interpolated morphing, onion skinning and interpolated rotoscoping.\n\n2D animation has many applications, including analog computer animation, Flash animation, and PowerPoint animation. Cinemagraphs are still photographs in the form of an animated GIF file of which part is animated.\n\nFinal line advection animation is a technique used in 2D animation, to give artists and animators more influence and control over the final product as everything is done within the same department. Speaking about using this approach in \"Paperman\", John Kahrs said that \"Our animators can change things, actually erase away the CG underlayer if they want, and change the profile of the arm.\"\n\n3D animation is digitally modeled and manipulated by an animator. The animator usually starts by creating a 3D polygon mesh to manipulate. A mesh typically includes many vertices that are connected by edges and faces, which give the visual appearance of form to a 3D object or 3D environment. Sometimes, the mesh is given an internal digital skeletal structure called an armature that can be used to control the mesh by weighting the vertices. This process is called rigging and can be used in conjunction with keyframes to create movement.\n\nOther techniques can be applied, mathematical functions (e.g., gravity, particle simulations), simulated fur or hair, and effects, fire and water simulations. These techniques fall under the category of 3D dynamics.\n\n\n\n\nAn animator is an artist who creates a visual sequence (or audio-visual if added sound) of multiple sequential images that generate the illusion of movement, that is, an animation. Animations are currently in many areas of technology and video, such as cinema, television, video games or the internet. Generally, these works require the collaboration of several animators. The methods to create these images depend on the animator and style that one wants to achieve (with images generated by computer, manually ...).\n\nAnimators can be divided into animators of characters (artists who are specialized in the movements, dialogue and acting of the characters) and animators of special effects (for example vehicles, machinery or natural phenomena such as water, snow, rain).\n\nThe creation of non-trivial animation works (i.e., longer than a few seconds) has developed as a form of filmmaking, with certain unique aspects. Traits common to both live-action and animated feature-length films are labor-intensity and high production costs.\n\nThe most important difference is that once a film is in the production phase, the marginal cost of one more shot is higher for animated films than live-action films. It is relatively easy for a director to ask for one more take during principal photography of a live-action film, but every take on an animated film must be manually rendered by animators (although the task of rendering slightly different takes has been made less tedious by modern computer animation). It is pointless for a studio to pay the salaries of dozens of animators to spend weeks creating a visually dazzling five-minute scene if that scene fails to effectively advance the plot of the film. Thus, animation studios starting with Disney began the practice in the 1930s of maintaining story departments where storyboard artists develop every single scene through storyboards, then handing the film over to the animators only after the production team is satisfied that all the scenes make sense as a whole. While live-action films are now also storyboarded, they enjoy more latitude to depart from storyboards (i.e., real-time improvisation).\n\nAnother problem unique to animation is the requirement to maintain a film's consistency from start to finish, even as films have grown longer and teams have grown larger. Animators, like all artists, necessarily have individual styles, but must subordinate their individuality in a consistent way to whatever style is employed on a particular film. Since the early 1980s, teams of about 500 to 600 people, of whom 50 to 70 are animators, typically have created feature-length animated films. It is relatively easy for two or three artists to match their styles; synchronizing those of dozens of artists is more difficult.\n\nThis problem is usually solved by having a separate group of visual development artists develop an overall look and palette for each film before animation begins. Character designers on the visual development team draw model sheets to show how each character should look like with different facial expressions, posed in different positions, and viewed from different angles. On traditionally animated projects, maquettes were often sculpted to further help the animators see how characters would look from different angles.\n\nUnlike live-action films, animated films were traditionally developed beyond the synopsis stage through the storyboard format; the storyboard artists would then receive credit for writing the film. In the early 1960s, animation studios began hiring professional screenwriters to write screenplays (while also continuing to use story departments) and screenplays had become commonplace for animated films by the late 1980s.\n\nCriticism of animation has been common in media and cinema since its inception. With its popularity, a large amount of criticism has arisen, especially animated feature-length films. Many concerns of cultural representation, psychological effects on children have been brought up around the animation industry, which has remained rather politically unchanged and stagnant since its inception into mainstream culture.\n\nAs with any other form of media, animation too has instituted awards for excellence in the field. The original awards for animation were presented by the Academy of Motion Picture Arts and Sciences for animated shorts from the year 1932, during the 5th Academy Awards function. The first winner of the Academy Award was the short \"Flowers and Trees\", a production by Walt Disney Productions. The Academy Award for a feature-length animated motion picture was only instituted for the year 2001, and awarded during the 74th Academy Awards in 2002. It was won by the film \"Shrek\", produced by DreamWorks and Pacific Data Images. Disney/Pixar have produced the most films either to win or be nominated for the award. The list of both awards can be obtained here:\n\nSeveral other countries have instituted an award for best animated feature film as part of their national film awards: Africa Movie Academy Award for Best Animation (since 2008), BAFTA Award for Best Animated Film (since 2006), César Award for Best Animated Film (since 2011), Golden Rooster Award for Best Animation (since 1981), Goya Award for Best Animated Film (since 1989), Japan Academy Prize for Animation of the Year (since 2007), National Film Award for Best Animated Film (since 2006). Also since 2007, the Asia Pacific Screen Award for Best Animated Feature Film has been awarded at the Asia Pacific Screen Awards. Since 2009, the European Film Awards have awarded the European Film Award for Best Animated Film.\nThe Annie Award is another award presented for excellence in the field of animation. Unlike the Academy Awards, the Annie Awards are only received for achievements in the field of animation and not for any other field of technical and artistic endeavor. They were re-organized in 1992 to create a new field for Best Animated feature. The 1990s winners were dominated by Walt Disney, however, newer studios, led by Pixar & DreamWorks, have now begun to consistently vie for this award. The list of awardees is as follows: \n\n\n\n"}
{"id": "5343488", "url": "https://en.wikipedia.org/wiki?curid=5343488", "title": "Bicycle and motorcycle dynamics", "text": "Bicycle and motorcycle dynamics\n\nBicycle and motorcycle dynamics is the science of the motion of bicycles and motorcycles and their components, due to the forces acting on them. Dynamics falls under a branch of physics known as classical mechanics. Bike motions of interest include balancing, steering, braking, accelerating, suspension activation, and vibration. The study of these motions began in the late 19th century and continues today.\n\nBicycles and motorcycles are both single-track vehicles and so their motions have many fundamental attributes in common and are fundamentally different from and more difficult to study than other wheeled vehicles such as dicycles, tricycles, and quadracycles. As with unicycles, bikes lack lateral stability when stationary, and under most circumstances can only remain upright when moving forward. Experimentation and mathematical analysis have shown that a bike stays upright when it is steered to keep its center of mass over its wheels. This steering is usually supplied by a rider, or in certain circumstances, by the bike itself. Several factors, including geometry, mass distribution, and gyroscopic effect all contribute in varying degrees to this self-stability, but long-standing hypotheses and claims that any single effect, such as gyroscopic or trail, is solely responsible for the stabilizing force have been discredited.\n\nWhile remaining upright may be the primary goal of beginning riders, a bike must lean in order to maintain balance in a turn: the higher the speed or smaller the turn radius, the more lean is required. This balances the roll torque about the wheel contact patches generated by centrifugal force due to the turn with that of the gravitational force. This lean is usually produced by a momentary steering in the opposite direction, called countersteering. Countersteering skill is usually acquired by motor learning and executed via procedural memory rather than by conscious thought. Unlike other wheeled vehicles, the primary control input on bikes is steering torque, not position.\n\nAlthough longitudinally stable when stationary, bikes often have a high enough center of mass and a short enough wheelbase to lift a wheel off the ground under sufficient acceleration or deceleration. When braking, depending on the location of the combined center of mass of the bike and rider with respect to the point where the front wheel contacts the ground, bikes can either skid the front wheel or flip the bike and rider over the front wheel. A similar situation is possible while accelerating, but with respect to the rear wheel.\n\nThe history of the study of bike dynamics is nearly as old as the bicycle itself. It includes contributions from famous scientists such as Rankine, Appell, and Whipple. In the early 19th century Karl von Drais, credited with inventing the two-wheeled vehicle variously called the laufmaschine, velocipede, draisine, and dandy horse, showed that a rider could balance his device by steering the front wheel. In 1869, Rankine published an article in \"The Engineer\" repeating von Drais's assertion that balance is maintained by steering in the direction of a lean.\n\nIn 1897, the French Academy of Sciences made understanding bicycle dynamics the goal of its Prix Fourneyron competition. Thus, by the end of the 19th century, , Emmanuel Carvallo, and Francis Whipple had showed with rigid-body dynamics that some safety bicycles could actually balance themselves if moving at the right speed. Bourlet won the Prix Fourneyron, and Whipple won the Cambridge University Smith Prize. It is not clear to whom should go the credit for tilting the steering axis from the vertical which helps make this possible.\n\nIn 1970, David E. H. Jones published an article in \"Physics Today\" showing that gyroscopic effects are not necessary to balance a bicycle. Since 1971, when he identified and named the wobble, weave and capsize modes, Robin Sharp has written regularly about the behavior of motorcycles and bicycles. While at Imperial College, London, he worked with David Limebeer and Simos Evangelou.\n\nIn the early 1970s, Cornell Aeronautical Laboratory (CAL, later Calspan Corporation in Buffalo, NY USA) was sponsored by the Schwinn Bicycle Company and others to study and simulate bicycle and motorcycle dynamics. Portions of this work have now been released to the public and scans of over 30 detailed reports have been posted at this TU Delft Bicycle Dynamics site.\n\nSince the 1990s, Cossalter, et al., have been researching motorcycle dynamics at the University of Padova. Their research, both experimental and numerical, has covered weave, wobble, chatter, simulators, vehicle modelling, tire modelling, handling, and minimum lap time maneuvering.\n\nIn 2007, Meijaard, et al., published the canonical linearized equations of motion, in the \"Proceedings of the Royal Society A\", along with verification by two different methods. These equations assumed the tires to roll without slip, that is to say, to go where they point, and the rider to be rigidly attached to the rear frame of the bicycle.\n\nIn 2011, Kooijman, et al., published an article in \"Science\" showing that neither gyroscopic effects nor so-called caster effects due to trail are necessary for a bike to balance itself. They designed a two-mass-skate bicycle that the equations of motion predict is self-stable even with negative trail, the front wheel contacts the ground in front of the steering axis, and with counter-rotating wheels to cancel any gyroscopic effects. Then they constructed a physical model to validate that prediction. This may require some of the details provided below about steering geometry or stability to be re-evaluated. Bicycle dynamics was named 26 of \"Discover\"s 100 top stories of 2011.\n\nIn 2013, Eddy Merckx Cycles was awarded over €150,000 with Ghent University to examine bicycle stability.\n\nIf the bike and rider are considered to be a single system, the forces that act on that system and its components can be roughly divided into two groups: internal and external. The external forces are due to gravity, inertia, contact with the ground, and contact with the atmosphere. The internal forces are caused by the rider and by interaction between components.\n\nAs with all masses, gravity pulls the rider and all the bike components toward the earth. At each tire contact patch there are ground reaction forces with both horizontal and vertical components. The vertical components mostly counteract the force of gravity, but also vary with braking and accelerating. For details, see the section on \"longitudinal stability\" below. The horizontal components, due to friction between the wheels and the ground, including rolling resistance, are in response to propulsive forces, braking forces, and turning forces. Aerodynamic forces due to the atmosphere are mostly in the form of drag, but can also be from crosswinds. At normal bicycling speeds on level ground, aerodynamic drag is the largest force resisting forward motion. At faster speed, aerodynamic drag becomes overwhelmingly the largest force resisting forward motion.\n\nTurning forces are generated during maneuvers for balancing in addition to just changing direction of travel. These may be interpreted as centrifugal forces in the accelerating reference frame of the bike and rider; or simply as inertia in a stationary, inertial reference frame and not forces at all. \"Gyroscopic\" forces acting on rotating parts such as wheels, engine, transmission, etc., are also due to the inertia of those rotating parts. They are discussed further in the section on gyroscopic effects below.\n\nInternal forces, those between components of the bike and rider system, are mostly caused by the rider or by friction. In addition to pedaling, the rider can apply torques between the steering mechanism (front fork, handlebars, front wheel, etc.) and rear frame, and between the rider and the rear frame. Friction exists between any parts that move against each other: in the drive train, between the steering mechanism and the rear frame, etc. In addition to brakes, which create friction between rotating wheels and non-rotating frame parts, many bikes have front and rear suspensions. Some motorcycles and bicycles have a steering damper to dissipate undesirable kinetic energy, and some bicycles have a spring connecting the front fork to the frame to provide a progressive torque that tends to steer the bicycle straight ahead. On bikes with rear suspensions, feedback between the drive train and the suspension is an issue designers attempt to handle with various linkage configurations and dampers.\n\nMotions of a bike can be roughly grouped into those out of the central plane of symmetry: lateral; and those in the central plane of symmetry: longitudinal or vertical. Lateral motions include balancing, leaning, steering, and turning. Motions in the central plane of symmetry include rolling forward, of course, but also stoppies, wheelies, brake diving, and most suspension activation. Motions in these two groups are linearly decoupled, that is they do not interact with each other to the first order. An uncontrolled bike is laterally unstable when stationary and can be laterally self-stable when moving under the right conditions or when controlled by a rider. Conversely, a bike is longitudinally stable when stationary and can be longitudinally unstable when undergoing sufficient acceleration or deceleration.\n\nOf the two, lateral dynamics has proven to be the more complicated, requiring three-dimensional, multibody dynamic analysis with at least two generalized coordinates to analyze. At a minimum, two coupled, second-order differential equations are required to capture the principal motions. Exact solutions are not possible, and numerical methods must be used instead. Competing theories of how bikes balance can still be found in print and online. On the other hand, as shown in later sections, much longitudinal dynamic analysis can be accomplished simply with planar kinetics and just one coordinate.\n\nWhen discussing bike balance, it is necessary to distinguish carefully between \"stability\", \"self-stability\", and \"controllability\". Recent research suggests that \"rider-controlled stability of bicycles is indeed related to their self-stability.\"\n\nA bike remains upright when it is steered so that the ground reaction forces exactly balance all the other internal and external forces it experiences, such as gravitational if leaning, inertial or centrifugal if in a turn, gyroscopic if being steered, and aerodynamic if in a crosswind.\nSteering may be supplied by a rider or, under certain circumstances, by the bike itself. This self-stability is generated by a combination of several effects that depend on the geometry, mass distribution, and forward speed of the bike. Tires, suspension, steering damping, and frame flex can also influence it, especially in motorcycles.\n\nEven when staying relatively motionless, a rider can balance a bike by the same principle. While performing a track stand, the rider can keep the line between the two contact patches under the combined center of mass by steering the front wheel to one side or the other and then moving forward and backward slightly to move the front contact patch from side to side as necessary. Forward motion can be generated simply by pedaling. Backwards motion can be generated the same way on a fixed-gear bicycle. Otherwise, the rider can take advantage of an opportune slope of the pavement or lurch the upper body backwards while the brakes are momentarily engaged.\n\nIf the steering of a bike is locked, it becomes virtually impossible to balance while riding. On the other hand, if the gyroscopic effect of rotating bike wheels is cancelled by adding counter-rotating wheels, it is still easy to balance while riding. One other way that a bike can be balanced, with or without locked steering, is by applying appropriate torques between the bike and rider similar to the way a gymnast can swing up from hanging straight down on uneven parallel bars, a person can start swinging on a swing from rest by pumping their legs, or a double inverted pendulum can be controlled with an actuator only at the elbow.\n\nThe rider applies torque to the handlebars in order to turn the front wheel and so to control lean and maintain balance. At high speeds, small steering angles quickly move the ground contact points laterally; at low speeds, larger steering angles are required to achieve the same results in the same amount of time. Because of this, it is usually easier to maintain balance at high speeds. As self-stability typically occurs at speeds above a certain threshold, going faster increases the chances that a bike is contributing to its own stability.\n\nThe farther forward (closer to front wheel) the center of mass of the combined bike and rider, the less the front wheel has to move laterally in order to maintain balance. Conversely, the farther back (closer to the rear wheel) the center of mass is located, the more front wheel lateral movement or bike forward motion is required to regain balance. This can be noticeable on long-wheelbase recumbents, choppers, and wheelie bikes. It can also be a challenge for touring bikes that carry a heavy load of gear over or even behind the rear wheel. Mass over the rear wheel can be more easily controlled if it is lower than mass over the front wheel.\n\nA bike is also an example of an inverted pendulum. Just as a broomstick is more easily balanced in the hand than a pencil, a tall bike (with a high center of mass) can be easier to balance when ridden than a low one because the tall bike's lean rate (rate at which its angle of lean increases as it begins to fall over) will be slower. However, a rider can have the opposite impression of a bike when it is stationary. A top-heavy bike can require more effort to keep upright, when stopped in traffic for example, than a bike which is just as tall but with a lower center of mass. This is an example of a vertical second-class lever. A small force at the end of the lever, the seat or handlebars at the top of the bike, more easily moves a large mass if the mass is closer to the fulcrum, where the tires touch the ground. This is why touring cyclists are advised to carry loads low on a bike, and panniers hang down on either side of front and rear racks.\n\nA factor that influences how easy or difficult a bike will be to ride is trail, the distance that the front wheel ground contact point trails behind the steering axis ground contact point. The steering axis is the axis about which the entire steering mechanism (fork, handlebars, front wheel, etc.) pivots. In traditional bike designs, with a steering axis tilted back from the vertical, positive trail tends to steer the front wheel into the direction of a lean, independent of forward speed. This can be simulated by pushing a stationary bike to one side. The front wheel will usually also steer to that side. In a lean, gravity provides this force. The dynamics of a moving bike are more complicated, however, and other factors can contribute to or detract from this effect.\n\nTrail is a function of head angle, fork offset or rake, and wheel size. Their relationship can be described by this formula:\nwhere formula_2 is wheel radius, formula_3 is the head angle measured clock-wise from the horizontal and formula_4 is the fork offset or rake. Trail can be increased by increasing the wheel size, decreasing the head angle, or decreasing the fork rake.\n\nThe more trail a traditional bike has, the more stable it feels, although too much trail can make a bike feel difficult to steer. Bikes with negative trail (where the contact patch is in front of where the steering axis intersects the ground), while still rideable, are reported to feel very unstable. Normally, road racing bicycles have more trail than touring bikes but less than mountain bikes. Mountain bikes are designed with reduced head angles than road bikes to improve stability for descents, and therefore have greater trail. Touring bikes are built with small trail to allow the rider to control a bike weighed down with baggage. As a consequence, an unloaded touring bike can feel unstable. In bicycles, fork rake, often a curve in the fork blades forward of the steering axis, is used to diminish trail. Bikes with negative trail exist, such as the Python Lowracer, and are rideable, and an experimental bike with negative trail has been shown to be self-stable.\n\nIn motorcycles, rake refers to the head angle instead, and offset created by the triple tree is used to diminish trail.\n\nA small survey by Whitt and Wilson found:\n\nHowever, these ranges are not hard and fast. For example, LeMond Racing Cycles offers \nboth with forks that have 45 mm of offset or rake and the same size wheels:\n\nThe amount of trail a particular bike has may vary with time for several reasons. On bikes with front suspension, especially telescopic forks, compressing the front suspension, due to heavy braking for example, can steepen the steering axis angle and reduce trail. Trail also varies with lean angle, and steering angle, usually decreasing from a maximum when the bike is straight upright and steered straight ahead. Trail can decrease to zero with sufficiently large lean and steer angles, which can alter how stable a bike feels. Finally, even the profile of the front tire can influence how trail varies as the bike is leaned and steered.\n\nA measurement similar to trail, called either mechanical trail, normal trail, or true trail, is the \"perpendicular\" distance from the steering axis to the centroid of the front wheel contact patch.\n\nA factor that influences the directional stability of a bike is wheelbase, the horizontal distance between the ground contact points of the front and rear wheels. For a given displacement of the front wheel, due to some disturbance, the angle of the resulting path from the original is inversely proportional to wheelbase. Also, the radius of curvature for a given steer angle and lean angle is proportional to the wheelbase. Finally, the wheelbase increases when the bike is leaned and steered. In the extreme, when the lean angle is 90°, and the bike is steered in the direction of that lean, the wheelbase is increased by the radius of the front and rear wheels.\n\nAnother factor that can also contribute to the self-stability of traditional bike designs is the distribution of mass in the steering mechanism, which includes the front wheel, the fork, and the handlebar. If the center of mass for the steering mechanism is in front of the steering axis, then the pull of gravity will also cause the front wheel to steer in the direction of a lean. This can be seen by leaning a stationary bike to one side. The front wheel will usually also steer to that side independent of any interaction with the ground. Additional parameters, such as the fore-to-aft position of the center of mass and the elevation of the center of mass also contribute to the dynamic behavior of a bike.\n\nThe role of the gyroscopic effect in most bike designs is to help steer the front wheel into the direction of a lean. This phenomenon is called \"precession\" and the rate at which an object precesses is inversely proportional to its rate of spin. The slower a front wheel spins, the faster it will precess when the bike leans, and vice versa.\nThe rear wheel is prevented from precessing as the front wheel does by friction of the tires on the ground, and so continues to lean as though it were not spinning at all. Hence gyroscopic forces do not provide any resistance to tipping.\n\nAt low forward speeds, the precession of the front wheel is too quick, contributing to an uncontrolled bike’s tendency to oversteer, start to lean the other way and eventually oscillate and fall over. At high forward speeds, the precession is usually too slow, contributing to an uncontrolled bike’s tendency to understeer and eventually fall over without ever having reached the upright position. This instability is very slow, on the order of seconds, and is easy for most riders to counteract. Thus a fast bike may feel stable even though it is actually not self-stable and would fall over if it were uncontrolled.\n\nAnother contribution of gyroscopic effects is a roll moment generated by the front wheel during countersteering. For example, steering left causes a moment to the right. The moment is small compared to the moment generated by the out-tracking front wheel, but begins as soon as the rider applies torque to the handlebars and so can be helpful in motorcycle racing. For more detail, see the section countersteering, below, and the countersteering article.\n\nBetween the two unstable regimes mentioned in the previous section, and influenced by all the factors described above that contribute to balance (trail, mass distribution, gyroscopic effects, etc.), there may be a range of forward speeds for a given bike design at which these effects steer an uncontrolled bike upright. It has been proven that neither gyroscopic effects nor positive trail are sufficient by themselves or necessary for self-stability, although they certainly can enhance hands-free control.\n\nHowever, even without self-stability a bike may be ridden by steering it to keep it over its wheels. Note that the effects mentioned above that would combine to produce self-stability may be overwhelmed by additional factors such as headset friction and stiff control cables. This video shows a riderless bicycle exhibiting self-stability.\n\nLongitudinal acceleration has been shown to have a large and complex effect on lateral dynamics. In one study, positive acceleration eliminates self stability, and negative acceleration (deceleration) changes the speeds of self stability.\n\nIn order for a bike to turn, that is, change its direction of forward travel, the front wheel must aim approximately in the desired direction, as with any front-wheel steered vehicle. Friction between the wheels and the ground then generates the centripetal acceleration necessary to alter the course from straight ahead as a combination of cornering force and camber thrust. The radius of the turn of an upright (not leaning) bike can be roughly approximated, for small steering angles, by:\nwhere formula_6 is the approximate radius, formula_7 is the wheelbase, formula_8 is the steer angle, and formula_9 is the caster angle of the steering axis.\n\nHowever, unlike other wheeled vehicles, bikes must also lean during a turn to balance the relevant forces: gravitational, inertial, frictional, and ground support. The angle of lean, \"θ\", can easily be calculated using the laws of circular motion:\n\nwhere \"v\" is the forward speed, \"r\" is the radius of the turn and \"g\" is the acceleration of gravity. This is in the idealized case. A slight increase in the lean angle may be required on motorcycles to compensate for the width of modern tires at the same forward speed and turn radius.\n\nIt can also be seen however that this simple 2-dimensional model, essentially an inverted pendulum on a turntable, predicts that the steady-state turn is unstable. If the bike is displaced slightly downwards from its equilibrium lean angle, the torque of gravity increases, that of centrifugal force decreases and the displacement gets amplified. A more-sophisticated model that allows a wheel to steer, adjust the path, and counter the torque of gravity, is necessary to capture the self-stability observed in real bikes.\n\nFor example, a bike in a 10 m (33 ft) radius steady-state turn at 10 m/s (36 km/h, 22 mph) must be at an angle of 45.6°. A rider can lean with respect to the bike in order to keep either the torso or the bike more or less upright if desired. The angle that matters is the one between the horizontal plane and the plane defined by the tire contacts and the location of the center of mass of bike and rider.\n\nThis lean of the bike decreases the actual radius of the turn proportionally to the cosine of the lean angle. The resulting radius can be roughly approximated (within 2% of exact value) by:\n\nwhere \"r\" is the approximate radius, \"w\" is the wheelbase, \"θ\" is the lean angle, \"δ\" is the steer angle, and \"φ\" is the caster angle of the steering axis. As a bike leans, the tires' contact patches move farther to the side causing wear. The portions at either edge of a motorcycle tire that remain unworn by leaning into turns is sometimes referred to as .\n\nThe finite width of the tires alters the actual lean angle of the rear frame from the ideal lean angle described above. The actual lean angle between the frame and the vertical must increase with tire width and decrease with center of mass height. Bikes with fat tires and low center of mass must lean more than bikes with skinnier tires or higher centers of mass to negotiate the same turn at the same speed.\n\nThe increase in lean angle due to a tire thickness of 2\"t\" can be calculated as\n\nwhere \"φ\" is the ideal lean angle, and \"h\" is the height of the center of mass. For example, a motorcycle with a 12 inch wide rear tire will have \"t\" = 6 inches. If the combined bike and rider center of mass is at a height of 26 inches, then a 25° lean must be increased by 7.28°: a nearly 30% increase. If the tires are only 6 inches wide, then the lean angle increase is only 3.16°, just under half.\n\nIt has been shown that the couple created by gravity and the ground reaction forces is necessary for a bicycle to turn at all. On a custom built bicycle with spring-loaded outriggers that exactly cancel this couple, so that the bicycle and rider may assume any lean angle when traveling in a straight line, riders find it impossible to make a turn. As soon as the wheels deviate from a straight path, the bicycle and rider begin to lean in the opposite direction, and the only way to right them is to steer back onto the straight path.\n\nIn order to initiate a turn and the necessary lean in the direction of that turn, a bike must momentarily steer in the opposite direction. This is often referred to as countersteering. With the front wheel now at a finite angle to the direction of motion, a lateral force is developed at the contact patch of the tire. This force creates a torque around the longitudinal (roll) axis of the bike, and this torque causes the bike to lean away from the initially steered direction and toward the direction of the desired turn. Where there is no external influence, such as an opportune side wind to create the force necessary to lean the bike, countersteering is necessary to initiate a rapid turn.\n\nWhile the initial steer torque and steer angle are both opposite the desired turn direction, this may not be the case to maintain a steady-state turn. The sustained steer angle is usually in the same direction as the turn, but may remain opposite to the direction of the turn, especially at high speeds. The sustained steer torque required to maintain that steer angle is usually opposite the turn direction. The actual magnitude and orientation of both the sustained steer angle and sustained steer torque of a particular bike in a particular turn depend on forward speed, bike geometry, tire properties, and combined bike and rider mass distribution. Once in a turn, the radius can only be changed with an appropriate change in lean angle, and this can be accomplished by additional countersteering out of the turn to increase lean and decrease radius, then into the turn to decrease lean and increase radius. To exit the turn, the bike must again countersteer, momentarily steering more into the turn in order to decrease the radius, thus increasing inertial forces, and thereby decreasing the angle of lean.\n\nOnce a turn is established, the torque that must be applied to the steering mechanism in order to maintain a constant radius at a constant forward speed depends on the forward speed and the geometry and mass distribution of the bike. At speeds below the capsize speed, described below in the section on \"Eigenvalues\" and also called the \"inversion\" speed, the self-stability of the bike will cause it to tend to steer into the turn, righting itself and exiting the turn, unless a torque is applied in the opposite direction of the turn. At speeds above the capsize speed, the capsize instability will cause it to tend to steer out of the turn, increasing the lean, unless a torque is applied in the direction of the turn. At the capsize speed no input steering torque is necessary to maintain the steady-state turn.\n\nSeveral effects influence the steering angle, the angle at which the front assembly is rotated about the steering axis, necessary to maintain a steady-state turn. Some of these are unique to single-track vehicles, while others are also experienced by automobiles. Some of these may be mentioned elsewhere in this article, and they are repeated here, though not necessarily in order of importance, so that they may be found in one place.\n\nFirst, the actual kinematic steering angle, the angle projected onto the road plane to which the front assembly is rotated is a function of the steering angle and the steering axis angle:\nwhere formula_14 is the kinematic steering angle, formula_8 is the steering angle, and formula_9 is the caster angle of the steering axis.\n\nSecond, the lean of the bike decreases the actual radius of the turn proportionally to the cosine of the lean angle. The resulting radius can be roughly approximated (within 2% of exact value) by:\nwhere formula_6 is the approximate radius, formula_7 is the wheelbase, formula_20 is the lean angle, formula_8 is the steering angle, and formula_9 is the caster angle of the steering axis.\n\nThird, because the front and rear tires can have different slip angles due to weight distribution, tire properties, etc., bikes can experience understeer or oversteer. When understeering, the steering angle must be greater, and when oversteering, the steering angle must be less than it would be if the slip angles were equal to maintain a given turn radius. Some authors even use the term \"counter-steering\" to refer to the need on some bikes under some conditions to steer in the opposite direction of the turn (negative steering angle) to maintain control in response to significant rear wheel slippage.\n\nFourth, camber thrust contributes to the centripetal force necessary to cause the bike to deviate from a straight path, along with cornering force due to the slip angle, and can be the largest contributor. Camber thrust contributes to the ability of bikes to negotiate a turn with the same radius as automobiles but with a smaller steering angle. When a bike is steered and leaned in the same direction, the camber angle of the front tire is greater than that of the rear and so can generate more camber thrust, all else being equal.\n\nWhile countersteering is usually initiated by applying torque directly to the handlebars, on lighter vehicles such as bicycles, it can also be accomplished by shifting the rider’s weight. If the rider leans to the right relative to the bike, the bike leans to the left to conserve angular momentum, and the combined center of mass remains nearly in the same vertical plane. This leftward lean of the bike, called counter lean by some authors, will cause it to steer to the left and initiate a right-hand turn as if the rider had countersteered to the left by applying a torque directly to the handlebars. This technique may be complicated by additional factors such as headset friction and stiff control cables.\n\nThe combined center of mass does move slightly to the left when the rider leans to the right relative to the bike, and the bike leans to the left in response. The action, in space, would have the tires move right, but this is prevented by friction between the tires and the ground, and thus pushes the combined center of mass left. This is a small effect, however, as evidenced by the difficulty most people have in balancing a bike by this method alone.\n\nAs mentioned above in the section on balance, one effect of turning the front wheel is a roll moment caused by gyroscopic precession. The magnitude of this moment is proportional to the moment of inertia of the front wheel, its spin rate (forward motion), the rate that the rider turns the front wheel by applying a torque to the handlebars, and the cosine of the angle between the steering axis and the vertical.\n\nFor a sample motorcycle moving at 22 m/s (50 mph) that has a front wheel with a moment of inertia of 0.6 kg·m, turning the front wheel one degree in half a second generates a roll moment of 3.5 N·m. In comparison, the lateral force on the front tire as it tracks out from under the motorcycle reaches a maximum of 50 N. This, acting on the 0.6 m (2 ft) height of the center of mass, generates a roll moment of 30 N·m.\n\nWhile the moment from gyroscopic forces is only 12% of this, it can play a significant part because it begins to act as soon as the rider applies the torque, instead of building up more slowly as the wheel out-tracks. This can be especially helpful in motorcycle racing.\n\nBecause of theoretical benefits, such as a tighter turning radius at low speed, attempts have been made to construct motorcycles with two-wheel steering. One working prototype by Ian Drysdale in Australia is reported to \"work very well.\" Issues in the design include whether to provide active control of the rear wheel or let it swing freely. In the case of active control, the control algorithm needs to decide between steering with or in the opposite direction of the front wheel, when, and how much. One implementation of two-wheel steering, the Sideways bike, lets the rider control the steering of both wheels directly. Another, the Swing Bike, had the second steering axis in front of the seat so that it could also be controlled by the handlebars.\n\nMilton W. Raymond built a long low two-wheel steering bicycle, called \"X-2\", with various steering mechanisms to control the two wheels independently. Steering motions included \"balance\", in which both wheels move together to steer the tire contacts under the center of mass; and \"true circle\", in which the wheels steer equally in opposite directions and thus steering the bicycle without substantially changing the lateral position of the tire contacts relative to the center of mass. X-2 was also able to go \"crabwise\" with the wheels parallel but out of line with the frame, for instance with the front wheel near the roadway center line and rear wheel near the curb. \"Balance\" steering allowed easy balancing despite long wheelbase and low center of mass, but no self-balancing (\"no hands\") configuration was discovered. True circle, as expected, was essentially impossible to balance, as steering does not correct for misalignment of the tire patch and center of mass. Crabwise cycling at angles tested up to about 45° did not show a tendency to fall over, even under braking. X-2 is mentioned in passing in Whitt and Wilson's \"Bicycling Science\" 2nd edition.\n\nBecause of the theoretical benefits, especially a simplified front-wheel drive mechanism, attempts have been made to construct a rideable rear-wheel steering bike. The Bendix Company built a rear-wheel steering bicycle, and the U.S. Department of Transportation commissioned the construction of a rear-wheel steering motorcycle: both proved to be unrideable. Rainbow Trainers, Inc. in Alton, Illinois, offered US$5,000 to the first person \"who can successfully ride the rear-steered bicycle, Rear Steered Bicycle I\". One documented example of someone successfully riding a rear-wheel steering bicycle is that of L. H. Laiterman at Massachusetts Institute of Technology, on a specially designed recumbent bike. The difficulty is that turning left, accomplished by turning the rear wheel to the right, initially moves the center of mass to the right, and vice versa. This complicates the task of compensating for leans induced by the environment. Examination of the eigenvalues for bicycles with common geometries and mass distributions shows that when moving in reverse, so as to have rear-wheel steering, they are inherently unstable. This does not mean they are unridable, but that the effort to control them is higher. Other, purpose-built designs have been published, however, that do not suffer this problem.\n\nBetween the extremes of bicycles with classical front-wheel steering and those with strictly rear-wheel steering is a class of bikes with a pivot point somewhere between the two, referred to as center-steering, and similar to articulated steering. An early implementation of the concept was the Phantom bicycle in the early 1870s promoted as a safer alternative to the penny-farthing. This design allows for simple front-wheel drive and current implementations appear to be quite stable, even rideable no-hands, as many photographs illustrate.\nThese designs, such as the Python Lowracer, a recumbent, usually have very lax head angles (40° to 65°) and positive or even negative trail. The builder of a bike with negative trail states that steering the bike from straight ahead forces the seat (and thus the rider) to rise slightly and this offsets the destabilizing effect of the negative trail.\n\nBicycles have been constructed, for investigation and demonstration purposes, with the steering reversed so that turning the handlebars to the left causes the front wheel to turn to the right, and vica versa. It is possible to ride such a bicycle, but it has been found that riders experienced with normal bicycles find it very difficult to learn, if they can manage it at all.\n\nTiller effect is the expression used to describe how handlebars that extend far behind the steering axis (head tube) act like a tiller on a boat, in that one moves the bars to the right in order to turn the front wheel to the left, and vice versa. This situation is commonly found on cruiser bicycles, some recumbents, and some motorcycles. It can be troublesome when it limits the ability to steer because of interference or the limits of arm reach.\n\nTires have a large influence over bike handling, especially on motorcycles, but also on bicycles. Tires influence bike dynamics in two distinct ways: finite crown radius and force generation. Increase the crown radius of the front tire has been shown to decrease the size or eliminate self stability. Increasing the crown radius of the rear tire has the opposite effect, but to a lesser degree.\n\nTires generate the lateral forces necessary for steering and balance through a combination of cornering force and camber thrust. Tire inflation pressures have also been found to be important variables in the behavior of a motorcycle at high speeds. Because the front and rear tires can have different slip angles due to weight distribution, tire properties, etc., bikes can experience understeer or oversteer. Of the two, understeer, in which the front wheel slides more than the rear wheel, is more dangerous since front wheel steering is critical for maintaining balance.\nAlso, because real tires have a finite contact patch with the road surface that can generate a scrub torque, and when in a turn, can experience some side slipping as they roll, they can generate torques about an axis normal to the plane of the contact patch.\nOne torque generated by a tire, called the self aligning torque, is caused by asymmetries in the side-slip along the length of the contact patch. The resultant force of this side-slip occurs behind the geometric center of the contact patch, a distance described as the pneumatic trail, and so creates a torque on the tire. Since the direction of the side-slip is towards the outside of the turn, the force on the tire is towards the center of the turn. Therefore, this torque tends to turn the front wheel in the direction of the side-slip, away from the direction of the turn, and therefore tends to \"increase\" the radius of the turn.\n\nAnother torque is produced by the finite width of the contact patch and the lean of the tire in a turn. The portion of the contact patch towards the outside of the turn is actually moving rearward, with respect to the wheel's hub, faster than the rest of the contact patch, because of its greater radius from the hub. By the same reasoning, the inner portion is moving rearward more slowly. So the outer and inner portions of the contact patch slip on the pavement in opposite directions, generating a torque that tends to turn the front wheel in the direction of the turn, and therefore tends to \"decrease\" the turn radius.\n\nThe combination of these two opposite torques creates a resulting yaw torque on the front wheel, and its direction is a function of the side-slip angle of the tire, the angle between the actual path of the tire and the direction it is pointing, and the camber angle of the tire (the angle that the tire leans from the vertical). The result of this torque is often the suppression of the inversion speed predicted by rigid wheel models described above in the section on steady-state turning.\n\nA highsider, highside, or high side is a type of bike motion which is caused by a rear wheel gaining traction when it is not facing in the direction of travel, usually after slipping sideways in a curve. This can occur under heavy braking, acceleration, a varying road surface, or suspension activation, especially due to interaction with the drive train. It can take the form of a single slip-then-flip or a series of violent oscillations.\n\nBike maneuverability and handling is difficult to quantify for several reasons. The geometry of a bike, especially the steering axis angle makes kinematic analysis complicated. Under many conditions, bikes are inherently unstable and must always be under rider control. Finally, the rider's skill has a large influence on the bike's performance in any maneuver. Bike designs tend to consist of a trade-off between maneuverability and stability.\n\nThe primary control input that the rider can make is to apply a torque directly to the steering mechanism via the handlebars. Because of the bike's own dynamics, due to steering geometry and gyroscopic effects, direct position control over steering angle has been found to be problematic.\n\nA secondary control input that the rider can make is to lean the upper torso relative to the bike. As mentioned above, the effectiveness of rider lean varies inversely with the mass of the bike. On heavy bikes, such as motorcycles, rider lean mostly alters the ground clearance requirements in a turn, improves the view of the road, and improves the bike system dynamics in a very low-frequency passive manner. In motorcycle racing, leaning the torso, moving the body, and projecting a knee to the inside of the turn relative to the bike can also \"cause an aerodynamic yawing moment that facilitates entering and rounding the turn.\"\n\nThe need to keep a bike upright to avoid injury to the rider and damage to the vehicle even limits the type of maneuverability testing that is commonly performed. For example, while automobile enthusiast publications often perform and quote skidpad results, motorcycle publications do not. The need to \"set up\" for a turn, lean the bike to the appropriate angle, means that the rider must see further ahead than is necessary for a typical car at the same speed, and this need increases more than in proportion to the speed.\n\nSeveral schemes have been devised to rate the handling of bikes, particularly motorcycles.\n\n\nAlthough its equations of motion can be linearized, a bike is a nonlinear system. The variable(s) to be solved for cannot be written as a linear sum of independent components, i.e. its behavior is not expressible as a sum of the behaviors of its descriptors. Generally, nonlinear systems are difficult to solve and are much less understandable than linear systems. In the idealized case, in which friction and any flexing is ignored, a bike is a conservative system. Damping, however, can still be demonstrated: under the right circumstances, side-to-side oscillations will decrease with time. Energy added with a sideways jolt to a bike running straight and upright (demonstrating self-stability) is converted into increased forward speed, not lost, as the oscillations die out.\n\nA bike is a nonholonomic system because its outcome is path-dependent. In order to know its exact configuration, especially location, it is necessary to know not only the configuration of its parts, but also their histories: how they have moved over time. This complicates mathematical analysis. Finally, in the language of control theory, a bike exhibits non-minimum phase behavior. It turns in the direction opposite of how it is initially steered, as described above in the section on countersteering\n\nThe number of degrees of freedom of a bike depends on the particular model being used. The simplest model that captures the key dynamic features, called the \"Whipple model\" after Francis Whipple who first developed the equations for it, has four rigid bodies with knife edge wheels rolling without slip on a flat smooth surface, and has 7 degrees of freedom (configuration variables required to completely describe the location and orientation of all 4 bodies):\nAdding complexity to the model, such as rider movement, suspension movement, tire compliance, or frame flex, adds degrees of freedom. While the rear frame does pitch with leaning and steering, the pitch angle is completely constrained by the requirement for both wheels to remain on the ground, and so can be calculated geometrically from the other seven variables. If the location of the bike and the rotation of the wheels are ignored, the first five degrees of freedom can also be ignored, and the bike can be described by just two variables: lean angle and steer angle.\n\nThe equations of motion of an idealized bike, consisting of\n\ncan be represented by a single fourth-order linearized ordinary differential equation or two coupled second-order differential equations, the lean equation\nand the steer equation\nwhere \nThese can be represented in matrix form as \nwhere\n\nIn this idealized and linearized model, there are many geometric parameters (wheelbase, head angle, mass of each body, wheel radius, etc.), but only four significant variables: lean angle, lean rate, steer angle, and steer rate. These equations have been verified by comparison with multiple numeric models derived completely independently.\n\nThe equations show that the bicycle is like an inverted pendulum with the lateral position of its support controlled by terms representing roll acceleration, roll velocity and roll displacement to steering torque feedback. The roll acceleration term is normally of the wrong sign for self-stabilization and can be expected to be important mainly in respect of wobble oscillations. The roll velocity feedback is of the correct sign, is gyroscopic in nature, being proportional to speed, and is dominated by the front wheel contribution. The roll displacement term is the most important one and is mainly controlled by trail, steering rake and the offset of the front frame mass center from the steering axis. All the terms involve complex combinations of bicycle design parameters and sometimes the speed. The limitations of the benchmark bicycle are considered and extensions to the treatments of tires, frames and riders \n, and their implications, are included. Optimal rider controls for stabilization and path-following control are also discussed.\n\nIt is possible to calculate eigenvalues, one for each of the four state variables (lean angle, lean rate, steer angle, and steer rate), from the linearized equations in order to analyze the normal modes and self-stability of a particular bike design. In the plot to the right, eigenvalues of one particular bicycle are calculated for forward speeds of 0–10 m/s (22 mph). When the real parts of all eigenvalues (shown in dark blue) are negative, the bike is self-stable. When the imaginary parts of any eigenvalues (shown in cyan) are non-zero, the bike exhibits oscillation. The eigenvalues are point symmetric about the origin and so any bike design with a self-stable region in forward speeds will not be self-stable going backwards at the same speed.\n\nThere are three forward speeds that can be identified in the plot to the right at which the motion of the bike changes qualitatively:\nBetween these last two speeds, if they both exist, is a range of forward speeds at which the particular bike design is self-stable. In the case of the bike whose eigenvalues are shown here, the self-stable range is 5.3–8.0 m/s (12–18 mph). The fourth eigenvalue, which is usually stable (very negative), represents the castoring behavior of the front wheel, as it tends to turn towards the direction in which the bike is traveling. Note that this idealized model does not exhibit the \"wobble or shimmy\" and \"rear wobble\" instabilities described above. They are seen in models that incorporate tire interaction with the ground or other degrees of freedom.\n\nExperimentation with real bikes has so far confirmed the weave mode predicted by the eigenvalues. It was found that tire slip and frame flex are \"not important for the lateral dynamics of the bicycle in the speed range up to 6\" m/s. The idealized bike model used to calculate the eigenvalues shown here does not incorporate any of the torques that real tires can generate, and so tire interaction with the pavement cannot prevent the capsize mode from becoming unstable at high speeds, as Wilson and Cossalter suggest happens in the real world.\n\nBikes, as complex mechanisms, have a variety of modes: fundamental ways that they can move. These modes can be stable or unstable, depending on the bike parameters and its forward speed. In this context, \"stable\" means that an uncontrolled bike will continue rolling forward without falling over as long as forward speed is maintained. Conversely, \"unstable\" means that an uncontrolled bike will eventually fall over, even if forward speed is maintained. The modes can be differentiated by the speed at which they switch stability and the relative phases of leaning and steering as the bike experiences that mode. Any bike motion consists of a combination of various amounts of the possible modes, and there are three main modes that a bike can experience: capsize, weave, and wobble. A lesser known mode is rear wobble, and it is usually stable.\n\n\"Capsize\" is the word used to describe a bike falling over without oscillation. During capsize, an uncontrolled front wheel usually steers in the direction of lean, but never enough to stop the increasing lean, until a very high lean angle is reached, at which point the steering may turn in the opposite direction. A capsize can happen very slowly if the bike is moving forward rapidly. Because the capsize instability is so slow, on the order of seconds, it is easy for the rider to control, and is actually used by the rider to initiate the lean necessary for a turn.\n\nFor most bikes, depending on geometry and mass distribution, capsize is stable at low speeds, and becomes less stable as speed increases until it is no longer stable. However, on many bikes, tire interaction with the pavement is sufficient to prevent capsize from becoming unstable at high speeds.\n\n\"Weave\" is the word used to describe a slow (0–4 Hz) oscillation between leaning left and steering right, and vice versa. The entire bike is affected with significant changes in steering angle, lean angle (roll), and heading angle (yaw). The steering is 180° out of phase with the heading and 90° out of phase with the leaning. This AVI movie shows weave.\n\nFor most bikes, depending on geometry and mass distribution, weave is unstable at low speeds, and becomes less pronounced as speed increases until it is no longer unstable. While the amplitude may decrease, the frequency actually increases with speed.\n\n\"Wobble\", \"shimmy\", \"tank-slapper\", \"speed wobble\", and \"death wobble\" are all words and phrases used to describe a rapid (4–10 Hz) oscillation of primarily just the front end (front wheel, fork, and handlebars). Also involved is the yawing of the rear frame which may contribute to the wobble when too flexible. This instability occurs mostly at high speed and is similar to that experienced by shopping cart wheels, airplane landing gear, and automobile front wheels. While wobble or shimmy can be easily remedied by adjusting speed, position, or grip on the handlebar, it can be fatal if left uncontrolled.\n\nWobble or shimmy begins when some otherwise minor irregularity, such as fork asymmetry, accelerates the wheel to one side. The restoring force is applied in phase with the progress of the irregularity, and the wheel turns to the other side where the process is repeated. If there is insufficient damping in the steering the oscillation will increase until system failure occurs. The oscillation frequency can be changed by changing the forward speed, making the bike stiffer or lighter, or increasing the stiffness of the steering, of which the rider is a main component.\n\nThe term \"rear wobble\" is used to describe a mode of oscillation in which lean angle (roll) and heading angle (yaw) are almost in phase and both 180° out of phase with steer angle. The rate of this oscillation is moderate with a maximum of about 6.5 Hz. Rear wobble is heavily damped and falls off quickly as bike speed increases.\n\nThe effect that the design parameters of a bike have on these modes can be investigated by examining the eigenvalues of the linearized equations of motion. For more details on the equations of motion and eigenvalues, see the section on the equations of motion above. Some general conclusions that have been drawn are described here.\n\nThe lateral and torsional stiffness of the rear frame and the wheel spindle affects wobble-mode damping substantially. Long wheelbase and trail and a flat steering-head angle have been found to increase weave-mode damping. Lateral distortion can be countered by locating the front fork torsional axis as low as possible.\n\nCornering weave tendencies are amplified by degraded damping of the rear suspension. Cornering, camber stiffnesses and relaxation length of the rear tire make the largest contribution to weave damping. The same parameters of the front tire have a lesser effect. Rear loading also amplifies cornering weave tendencies. Rear load assemblies with appropriate stiffness and damping, however, were successful in damping out weave and wobble oscillations.\n\nOne study has shown theoretically that, while a bike leaned in a turn, road undulations can excite the weave mode at high speed or the wobble mode at low speed if either of their frequencies match the vehicle speed and other parameters. Excitation of the wobble mode can be mitigated by an effective steering damper and excitation of the weave mode is worse for light riders than for heavy riders.\n\nRiding on a treadmill is theoretically identical to riding on stationary pavement, and physical testing has confirmed this. Treadmills have been developed specifically for indoor bicycle training. Riding on rollers is still under investigation.\n\nAlthough bicycles and motorcycles can appear to be simple mechanisms with only four major moving parts (frame, fork, and two wheels), these parts are arranged in a way that makes them complicated to analyze. While it is an observable fact that bikes can be ridden even when the gyroscopic effects of their wheels are canceled out, the hypothesis that the gyroscopic effects of the wheels are what keep a bike upright is common in print and online.\n\nExamples in print:\n\nBikes may experience a variety of longitudinal forces and motions. On most bikes, when the front wheel is turned to one side or the other, the entire rear frame pitches forward slightly, depending on the steering axis angle and the amount of trail. On bikes with suspensions, either front, rear, or both, trim is used to describe the geometric configuration of the bike, especially in response to forces of braking, accelerating, turning, drive train, and aerodynamic drag.\n\nThe load borne by the two wheels varies not only with center of mass location, which in turn varies with the amount and location of passengers and luggage, but also with acceleration and deceleration. This phenomenon is known as load transfer or weight transfer, depending on the author, and provides challenges and opportunities to both riders and designers. For example, motorcycle racers can use it to increase the friction available to the front tire when cornering, and attempts to reduce front suspension compression during heavy braking has spawned several motorcycle fork designs.\n\nThe net aerodynamic drag forces may be considered to act at a single point, called the center of pressure. At high speeds, this will create a net moment about the rear driving wheel and result in a net transfer of load from the front wheel to the rear wheel. Also, depending on the shape of the bike and the shape of any fairing that might be installed, aerodynamic lift may be present that either increases or further reduces the load on the front wheel.\n\nThough longitudinally stable when stationary, a bike may become longitudinally unstable under sufficient acceleration or deceleration, and Euler's second law can be used to analyze the ground reaction forces generated. For example, the normal (vertical) ground reaction forces at the wheels for a bike with a wheelbase formula_40 and a center of mass at height formula_41 and at a distance formula_42 in front of the rear wheel hub, and for simplicity, with both wheels locked, can be expressed as:\nThe frictional (horizontal) forces are simply\nwhere formula_47 is the coefficient of friction, formula_48 is the total mass of the bike and rider, and formula_34 is the acceleration of gravity. Therefore, if\nwhich occurs if the center of mass is anywhere above or in front of a line extending back from the front wheel contact patch and inclined at the angle\nabove the horizontal, then the normal force of the rear wheel will be zero (at which point the equation no longer applies) and the bike will begin to flip or loop forward over the front wheel.\n\nOn the other hand, if the center of mass height is behind or below the line, such as on most tandem bicycles or long-wheel-base recumbent bicycles, as well as cars, it is less likely that the front wheel can generate enough braking force to flip the bike. This means they can decelerate up to nearly the limit of adhesion of the tires to the road, which could reach 0.8 g if the coefficient of friction is 0.8, which is 40% more than an upright bicycle under even the best conditions. \"Bicycling Science\" author David Gordon Wilson points out that this puts upright bicyclists at particular risk of causing a rear-end collision if they tailgate cars.\n\nSimilarly, powerful motorcycles can generate enough torque at the rear wheel to lift the front wheel off the ground in a maneuver called a wheelie. A line similar to the one described above to analyze braking performance can be drawn from the rear wheel contact patch to predict if a wheelie is possible given the available friction, the center of mass location, and sufficient power. This can also happen on bicycles, although there is much less power available, if the center of mass is back or up far enough or the rider lurches back when applying power to the pedals.\n\nOf course, the angle of the terrain can influence all of the calculations above. All else remaining equal, the risk of pitching over the front end is reduced when riding up hill and increased when riding down hill. The possibility of performing a wheelie increases when riding up hill, and is a major factor in motorcycle hillclimbing competitions.\n\nWhen braking, the rider in motion is seeking to change the speed of the combined mass \"m\" of rider plus bike. This is a negative acceleration \"a\" in the line of travel. \"F\"=\"ma\", the acceleration \"a\" causes an inertial forward force \"F\" on mass \"m\".\nThe braking \"a\" is from an initial speed \"u\" to a final speed \"v\", over a length of time \"t\". The equation \"u\" - \"v\" = \"at\" implies that the greater the acceleration the shorter the time needed to change speed. The stopping distance \"s\" is also shortest when acceleration \"a\" is at the highest possible value compatible with road conditions: the equation \"s\" = \"ut\" + 1/2 \"at\" makes \"s\" low when \"a\" is high and \"t\" is low.\n\nHow much braking force to apply to each wheel depends both on ground conditions and on the balance of weight on the wheels at each instant in time. The total braking force cannot exceed the gravity force on the rider and bike times the coefficient of friction \"µ\" of the tire on the ground. \"mgµ\" >= \"Ff\" + \"Fr\". A skid occurs if the ratio of either \"Ff\" over \"Nf\" or \"Fr\" over \"Nr\" is greater than \"µ\", with a rear wheel skid having less of a negative impact on lateral stability.\n\nWhen braking, the inertial force \"ma\" in the line of travel, not being co-linear with \"f\", tends to rotate \"m\" about \"f\". This tendency to rotate, an overturning moment, is resisted by a moment from \"mg\". \n\nTaking moments about the front wheel contact point at an instance in time:\nOther factors:\n\nValues for \"µ\" vary greatly depending on a number of factors:\n\nMost of the braking force of standard upright bikes comes from the front wheel. As the analysis above shows, if the brakes themselves are strong enough, the rear wheel is easy to skid, while the front wheel often can generate enough stopping force to flip the rider and bike over the front wheel. This is called a \"stoppie\" if the rear wheel is lifted but the bike does not flip, or an \"endo\" (abbreviated form of \"end-over-end\") if the bike flips. On long or low bikes, however, such as cruiser motorcycles and recumbent bicycles, the front tire will skid instead, possibly causing a loss of balance. Assuming no loss of balance, it is possible to calculate optimum braking performance depending on the bike's geometry, the location of center of gravity of bike and rider, and the maximum coefficient of friction.\n\nIn the case of a front suspension, especially telescoping fork tubes, the increase in downward force on the front wheel during braking may cause the suspension to compress and the front end to lower. This is known as \"brake diving\". A riding technique that takes advantage of how braking increases the downward force on the front wheel is known as \"trail braking\".\n\nThe limiting factors on the maximum deceleration in front wheel braking are:\n\nFor an upright bicycle on dry asphalt with excellent brakes, pitching will probably be the limiting factor. The combined center of mass of a typical upright bicycle and rider will be about back from the front wheel contact patch and above, allowing a maximum deceleration of 0.5 \"g\" (5 m/s or 16 ft/s). If the rider modulates the brakes properly, however, pitching can be avoided. If the rider moves his weight back and down, even larger decelerations are possible.\n\nFront brakes on many inexpensive bikes are not strong enough so, on the road, they are the limiting factor. Cheap cantilever brakes, especially with \"power modulators\", and Raleigh-style side-pull brakes severely restrict the stopping force. In wet conditions they are even less effective. Front wheel slides are more common off-road. Mud, water, and loose stones reduce the friction between the tire and trail, although knobby tires can mitigate this effect by grabbing the surface irregularities. Front wheel slides are also common on corners, whether on road or off. Centripetal acceleration adds to the forces on the tire-ground contact, and when the friction force is exceeded the wheel slides.\n\nThe rear brake of an upright bicycle can only produce about 0.25 \"g\" (~2.5 m/s) deceleration at best, because of the decrease in normal force at the rear wheel as described above. All such bikes with only rear braking are subject to this limitation: for example, bikes with only a coaster brake, and fixed-gear bikes with no other braking mechanism. There are, however, situations that may warrant rear wheel braking\n\n\nExpert opinion varies from \"use both levers equally at first\"\nto \"the fastest that you can stop any bike of normal wheelbase is to apply the front brake so hard that the rear wheel is just about to lift off the ground,\" depending on road conditions, rider skill level, and desired fraction of maximum possible deceleration.\n\nBikes may have only front, only rear, full suspension or no suspension that operate primarily in the central plane of symmetry; though with some consideration given to lateral compliance. The goals of a bike suspension are to reduce vibration experienced by the rider, maintain wheel contact with the ground, reduce the loss of momentum when riding over an object, reduce impact forces caused by jumps or drops and maintain vehicle trim. The primary suspension parameters are stiffness, damping, sprung and unsprung mass, and tire characteristics. Besides irregularities in the terrain, brake, acceleration, and drive-train forces can also activate the suspension as described above. Examples include bob and pedal feedback on bicycles, the shaft effect on motorcycles, and \nsquat and brake dive on both.\n\nThe study of vibrations in bikes includes its causes, such as engine balance, wheel balance, ground surface, and aerodynamics; its transmission and absorption; and its effects on the bike, the rider, and safety. An important factor in any vibration analysis is a comparison of the natural frequencies of the system with the possible driving frequencies of the vibration sources. A close match means mechanical resonance that can result in large amplitudes. A challenge in vibration damping is to create compliance in certain directions (vertically) without sacrificing frame rigidity needed for power transmission and handling (torsionally). Another issue with vibration for the bike is the possibility of failure due to material fatigue Effects of vibration on riders include discomfort, loss of efficiency, Hand-Arm Vibration Syndrome, a secondary form Raynaud's disease, and whole body vibration. Vibrating instruments may be inaccurate or difficult to read.\n\nThe primary cause of vibrations in a properly functioning bicycle is the surface over which it rolls. In addition to pneumatic tires and traditional bicycle suspensions, a variety of techniques have been developed to damp vibrations before they reach the rider. These include materials, such as carbon fiber, either in the whole frame or just key components such as the front fork, seatpost, or handlebars; tube shapes, such as curved seat stays;, gel handlebar grips and saddles and special inserts, such as Zertz by Specialized, and Buzzkills by Bontrager.\n\nIn addition to the road surface, vibrations in a motorcycle can be caused by the engine and wheels, if unbalanced. Manufacturers employ a variety of technologies to reduce or damp these vibrations, such as engine balance shafts, rubber engine mounts, and tire weights. The problems that vibration causes have also spawned an industry of after-market parts and systems designed to reduce it. Add-ons include handlebar weights, isolated foot pegs, and engine counterweights. At high speeds, motorcycles and their riders may also experience aerodynamic flutter or buffeting. This can be abated by changing the air flow over key parts, such as the windshield.\n\nA variety of experiments have been performed in order to verify or disprove various hypotheses about bike dynamics.\n\n\n\nVideos:\n\nResearch centers:\n\nConferences:\n"}
{"id": "881901", "url": "https://en.wikipedia.org/wiki?curid=881901", "title": "Ceiling fan", "text": "Ceiling fan\n\nA ceiling fan is a mechanical fan, usually electrically powered, suspended from the ceiling of a room, that uses hub-mounted rotating blades to circulate air.Ceiling fans typically rotate more slowly than other types of circulating fans, such as electric desk fans. They cool people effectively by introducing slow movement into the otherwise still, hot air of a room. Fans never actually cool air, unlike air-conditioning equipment, they in fact heat the air due to the waste heat from the motor and friction, but use significantly less power (cooling air is thermodynamically expensive). Conversely, a ceiling fan can also be used to reduce the stratification of warm air in a room by forcing it down to affect both occupants' sensations and thermostat readings, thereby improving climate control energy efficiency.\n\nPunkah-type ceiling fans date back to 500 BC, and are native to India. Unlike modern rotary fans, these punkah fans move air by moving to and fro, and were operated manually by cord.\n\nThe first rotary ceiling fans appeared in the early 1860s and 1870s in the United States. At that time, they were not powered by any form of electric motor. Instead, a stream of running water was used, in conjunction with a turbine, to drive a system of belts which would turn the blades of two-blade fan units. These systems could accommodate several fan units, and so became popular in stores, restaurants, and offices. Some of these systems survive today, and can be seen in parts of the southern United States where they originally proved useful.\n\nThe electrically powered ceiling fan was invented in 1882 by Philip Diehl. He had engineered the electric motor used in the first electrically powered Singer sewing machines, and in 1882 he adapted that motor for use in a ceiling-mounted fan. Each fan had its own self-contained motor unit, with no need for belt drive.\n\nAlmost immediately he faced fierce competition due to the commercial success of the ceiling fan. He continued to make improvements to his invention and created a light kit fitted to the ceiling fan to combine both functions in one unit. By World War I most ceiling fans were made with four blades instead of the original two, which made fans quieter and allowed them to circulate more air. The early turn-of-the-century companies who successfully commercialized the sale of ceiling fans in the United States were the Hunter Brothers division of Robbins & Myers, Westinghouse Corporation and Emerson Electric.\n\nBy the 1920s, ceiling fans were commonplace in the United States, and had started to take hold internationally. From the Great Depression of the 1930s, until the introduction of electric air conditioning in the 1950s, ceiling fans slowly faded out of vogue in the U.S., almost falling into total disuse in the U.S. by the 1960s; those that remained were considered items of nostalgia.\n\nMeanwhile, electric ceiling fans became very popular in other countries, particularly those with hot climates, such as India and the Middle East, where a lack of infrastructure and/or financial resources made energy-hungry and complex freon-based air conditioning equipment impractical. In 1973, Texas entrepreneur H. W. (Hub) Markwardt began importing highly efficient ceiling fans to the United States that were manufactured in India by Crompton Greaves, Ltd. Crompton Greaves had been manufacturing ceiling fans since 1937 through a joint venture formed by Greaves Cotton of India and Crompton Parkinson of England, and had perfected the world's most energy efficient ceiling fans thanks to its patented 20 pole induction motor with a highly efficient heat-dissipating cast aluminum rotor. These Indian manufactured ceiling fans caught on slowly at first, but Markwardt's Encon Industries branded ceiling fans (which stood for ENergy CONservation) eventually found great success during the energy crisis of the late 1970s and early 1980s, since they consumed far less energy (under 70 watts of electricity) than the antiquated shaded pole motors used in most other American made fans. The fans became very effective energy saving appliances for residential and commercial use by supplementing expensive air conditioning with a cooling wind-chill effect. Fans used for comfort create a wind chill by increasing the heat transfer coefficient, but do not lower temperatures directly.\nDue to this renewed commercial success using ceiling fans effectively as an energy conservation application, many American manufacturers also started to produce, or significantly increase production of, ceiling fans. In addition to the imported Encon ceiling fans, the Casablanca Fan Company was founded in 1974. Other American manufacturers of the time included the Hunter Fan Co. (which was then a division of Robbins & Myers, Inc), FASCO (F. A. Smith Co.), and Emerson Electric; which was often branded as Sears-Roebuck.\n\nThrough the 1980s and 1990s, ceiling fans remained popular in the United States. Many small American importers, most of them rather short-lived, started importing ceiling fans. Throughout the 1980s, the balance of sales between American-made ceiling fans and those imported from manufacturers in India, Taiwan, Hong Kong and eventually China changed dramatically with imported fans taking the lion's share of the market by the late 1980s. Even the most basic U.S-made fans sold for $200 to $500, while the most expensive imported fans rarely exceeded $150.\n\nSince 2000, important inroads have been made by companies such as Monte Carlo, Minka Aire, Quorum, Craftmade, Litex and Fanimation - offering higher price ceiling fans with more decorative value. In 2001, Washington Post writer Patricia Dane Rogers wrote, “Like so many other mundane household objects, these old standbys are going high-style and high-tech.”\n\nUnlike air conditioners, fans only move air—they do not directly change its temperature. Therefore, ceiling fans that have a mechanism for reversing the direction in which the blades push air (most commonly an electrical switch on the unit's switch housing, motor housing, or lower canopy) can help in both heating and cooling.\n\nSome ceiling fans, mostly Hunter ones made in or before 1984, are mechanically reversible (have adjustable blade pitch) instead of an electrically reversible motor. In this case, the blade should be pitched to the right (or left if the motor spins clockwise) for downdraft, and left (or right if the motor spins clockwise) for updraft. Hunter Hotel Original is one example. In very rare case, such as late 1984 Hunter Original, fans are both mechanically reversible and electrically reversible, in which case it can blow air up, or down, in either direction. Some ceiling fans can only blow air in one direction and are not reversible in any way, more often downdraft only, but rarely updraft only. It's the case on most antique fans, and most industrial fans.\n\nFor cooling, the fan's direction of rotation should be set so that air is blown downward (Usually counter-clockwise from beneath), unless in rare case in which more breeze would be felt when blowing upward, such as when it's installed in hallway where blades would be so close to the walls. The blades should lead with the upturned side as they spin. The breeze created by a ceiling fan speeds the evaporation of perspiration on human skin, which makes the body's natural cooling mechanism much more efficient. Since the fan works directly on the body, rather than by changing the temperature of the air, during the summer it is a waste of electricity to leave a ceiling fan on when no one is in a room unless there's air conditioning, open windows, or anything that can heat up the room (such as oven) and fan is just to move air around.\n\nFor heating, ceiling fans should usually be set to turn the opposite direction (usually clockwise; the blades should spin with the downward turned side leading) and on a low speed (or the lowest speed the fan is able to circulate the air down to the floor). Air naturally stratifies—that is, warmer air rises to the ceiling while cooler air sinks. Unfortunately, this means it is colder on or near the floor where human beings spend most of their time. A ceiling fan, with its direction of rotation set so that air is drawn upward, pulls up the colder air below, forcing the warmer air nearer the ceiling to move down to take its place, without blowing a stream of air directly at the occupants of the room. This action works to even out the temperature in the room, making it cooler nearer the ceiling, but warmer nearer the floor. Thus the thermostat in the area can be set a few degrees lower to save energy, while maintaining the same level of comfort. It is important to run the fan at a low speed (or a lowest speed the fan is able to circulate the air down to the floor) to minimize the wind chill effect described above. However if the ceiling is high enough, or the lowest speed downdraft would not create wind chill effect, it can be left on downdraft year around.\n\nAn additional use of ceiling fans is coupling them with an air conditioning unit. Through-the-wall/through-the-window air conditioning units typically found in rented properties in North America usually have both the tasks of cooling the air inside the room and circulating it. Provided the ceiling fan is properly sized for the room in which it is operating, its efficiency of moving air far exceeds that of an air conditioning unit, therefore, for peak efficiency, the air conditioner should be set to a low fan setting and the ceiling fan should be used to circulate the air.\n\nThe key components of a ceiling fan are the following:\n\nOther components, which vary by model and style, can include:\n\n\n\n\nThe way in which a fan is operated depends on its manufacturer, style, and the era in which it was made. Operating methods include:\n\n\nMany styles of ceiling fans have been developed over the years in response to several different factors such as growing energy-consumption consciousness and changes in decorating styles. The advent and evolution of electronic technology has also played a major role in ceiling fan development. Following is a list of major ceiling fan styles and their defining characteristics:\n\n\n\n\n\nA typical ceiling fan weighs between 15 and 50 pounds when fully assembled. While many junction boxes can support that weight while the fan is hanging still, a fan \"in operation\" exerts many additional stresses—notably torsion—on the object from which it is hung; this can cause an improper junction box to fail. For this reason, in the United States the National Electric Code (document NFPA 70, Article 314) states that ceiling fans must be supported by an electrical junction box listed for that use. It is a common mistake for homeowners to replace a light fixture with a ceiling fan without upgrading to a proper junction box.\n\nAnother concern with installing a ceiling fan relates to the height of the blades relative to the floor. Building codes throughout the United States prohibit residential ceiling fans from being mounted with the blades closer than seven feet from the floor; this sometimes proves, however, to not be high enough. If a ceiling fan is turned on and a person fully extends his or her arms into the air, as sometimes happens during normal tasks such as stretching or changing bedsheets, it is possible for the blades to strike their hands, potentially causing injury. Also, if one is carrying a long and awkward object, one end may inadvertently enter the path of rotation of a ceiling fan's blades, which can cause damage to the fan. Building codes throughout the United States also prohibit industrial ceiling fans from being mounted with the blades closer than 10 feet from the floor for these reasons.\n\nIn 2004, \"MythBusters\" tested the idea that a ceiling fan is capable of decapitation if an individual was to stick his or her neck into a running fan. Two versions of the myth were tested, with the first being the \"jumping kid\", involving a kid jumping up and down on a bed, jumping too high and entering the fan from below and the second being the \"lover's leap\", involving a husband dressed in a costume, leaping towards his wife in bed and entering the fan side-on. Kari Byron, Tory Belleci and Scottie Chapman took the lead on the investigation, though original MythBusters Jamie Hyneman and Adam Savage also assisted.\n\nFirst, Kari and Scottie purchased a regular household fan and also an industrial fan, which has metal blades as opposed to wood and a more powerful motor. They and Tory then fashioned their human analogs - ballistic gel busts of Adam with actual human craniums, pig spines to approximate human spines, and latex arteries filled with fake blood - and then constructed rigs for both scenarios.\n\nThey busted the myth in both scenarios with both household and industrial fans, as tests proved that residential ceiling fans are, apparently by design, largely incapable of causing more than minor injury, having low-torque motors that stop quickly when blocked and blades composed of light materials that tend to break easily if impacted at speed (the household fan test of the \"lover's leap\" scenario actually broke the fan blades.) They did find that industrial fans, with their steel blades and higher speeds, proved capable of causing injury and laceration - building codes require industrial fans to be mounted with blades 10 feet above the floor, and the industrial fan test of the \"lover's leap\" scenario produced a lethal injury where the fan sliced through the jugular and into the vertebrae - but still lost energy rapidly once blocked and were unable to decapitate the test dummy. As a finale, Scottie, Tory and Kari created an even more dangerous fan with a lawn mower engine as the fan motor and razor sharp blades made from sheet metal in an attempt to duplicate the result, and even it was unable to achieve decapitation, but it caused lethal and horrifying injuries that compelled Adam to put it into the \"\"MythBusters\" Hall of Fame.\"\n\nWobbling is usually caused by the weight of fan blades being out of balance with each other. This can happen due to a variety of factors, including blades being warped, blade irons being bent, blades or blade irons not being screwed on straight, or blades being different weights or shapes or sizes. (Minute differences matter.) Also, if all the blades do not exert an equal force on the air (because they have different angles, for instance), the vertical reaction forces can cause wobbling. Wobble can also be caused by a motor flaw, but that very rarely occurs. Wobbling is not affected by the way in which the fan is mounted or the mounting surface.\n\nContrary to popular misconception, wobbling alone will not cause a ceiling fan to fall. Ceiling fans are secured by clevis pins locked with either split pins or R-clips, so wobbling won't have an effect on the fan's security, unless of course, the pins/clips were not secured. To date, there are no reports of a fan wobbling itself off the ceiling and falling. However, a severe wobble can cause light fixture shades or covers to gradually loosen over time and potentially fall, posing a risk of injury to anyone under the fan, and also from any resulting broken glass. It is also worth mentioning that when the MythBusters were designing a fan with the goal of chopping off someone's head, Scottie used an edge finder to find the exact center of their blades with the aim of eliminating potentially very dangerous wobbling of their steel blades. It is important that, when installing the fan, the installer closely follows the manufacturer's instructions with regard to using proper mounting screws. It is also important that all screws (especially the set screws which hold twist-on downrods in place) be tight, and any ceiling fan light fixtures are properly assembled with their shades and covers securely attached.\n"}
{"id": "1391016", "url": "https://en.wikipedia.org/wiki?curid=1391016", "title": "Cellulosic ethanol", "text": "Cellulosic ethanol\n\nCellulosic ethanol is ethanol (ethyl alcohol) produced from cellulose (the stringy fiber of a plant) rather than from the plant's seeds or fruit. It is a biofuel produced from grasses, wood, algae, or other plants. The fibrous parts of the plants are mostly inedible to animals, including humans, except for ruminants (grazing, cud-chewing animals such as cows or sheep).\n\nConsiderable interest in cellulosic ethanol exists due to its important economical potential. Growth of cellulose by plants is a mechanism that captures and stores solar energy chemically in nontoxic ways with resultant supplies that are easy to transport and store. Additionally, transport may be unneeded anyway, because grasses or trees can grow almost anywhere temperate. This is why commercially practical cellulosic ethanol is widely viewed as a next level of development for the biofuel industry that could reduce demand for oil and gas drilling and even nuclear power in ways that grain-based ethanol fuel alone cannot. Potential exists for the many benefits of carbonaceous liquid fuels and petrochemicals (which today's standard of living depends on) but in a carbon cycle–balanced and renewable way (recycling surface and atmosphere carbon instead of pumping underground carbon up into it and thus adding to it). Commercially practical cellulosic alcohol could also avoid one of the problems with today's conventional (grain-based) biofuels, which is that they set up competition for grain with food purposes, potentially driving up the price of food. To date, what stands in the way of these goals is that production of cellulosic alcohol is not yet sufficiently practical on a commercial scale.\n\nCellulosic ethanol is a type of biofuel produced from lignocellulose, a structural material that comprises much of the mass of plants. Lignocellulose is composed mainly of cellulose, hemicellulose and lignin. Corn stover, \"Panicum virgatum\" (switchgrass), \"Miscanthus\" grass species, wood chips and the byproducts of lawn and tree maintenance are some of the more popular cellulosic materials for ethanol production. Production of ethanol from lignocellulose has the advantage of abundant and diverse raw material compared to sources such as corn and cane sugars, but requires a greater amount of processing to make the sugar monomers available to the microorganisms typically used to produce ethanol by fermentation.\n\nSwitchgrass and \"Miscanthus\" are the major biomass materials being studied today, due to their high productivity per acre. Cellulose, however, is contained in nearly every natural, free-growing plant, tree, and bush, in meadows, forests, and fields all over the world without agricultural effort or cost needed to make it grow.\n\nOne of the benefits of cellulosic ethanol is it reduces greenhouse gas emissions (GHG) by 85% over reformulated gasoline. By contrast, starch ethanol (e.g., from corn), which most frequently uses natural gas to provide energy for the process, may not reduce GHG emissions at all depending on how the starch-based feedstock is produced. According to the National Academy of Sciences in 2011, there is no commercially viable bio-refinery in existence to convert lignocellulosic biomass to fuel. Absence of production of cellulosic ethanol in the quantities required by the regulation was the basis of a United States Court of Appeals for the District of Columbia decision announced January 25, 2013, voiding a requirement imposed on car and truck fuel producers in the United States by the Environmental Protection Agency requiring addition of cellulosic biofuels to their products. These issues, along with many other difficult production challenges, led George Washington University policy researchers to state that \"in the short term, [cellulosic] ethanol cannot meet the energy security and environmental goals of a gasoline alternative.\"\n\nThe French chemist, Henri Braconnot, was the first to discover that cellulose could be hydrolyzed into sugars by treatment with sulfuric acid in 1819. The hydrolyzed sugar could then be processed to form ethanol through fermentation. The first commercialized ethanol production began in Germany in 1898, where acid was used to hydrolyze cellulose. In the United States, the Standard Alcohol Company opened the first cellulosic ethanol production plant in South Carolina in 1910. Later, a second plant was opened in Louisiana. However, both plants were closed after World War I due to economic reasons.\n\nThe first attempt at commercializing a process for ethanol from wood was done in Germany in 1898. It involved the use of dilute acid to hydrolyze the cellulose to glucose, and was able to produce 7.6 liters of ethanol per 100 kg of wood waste ( per ton). The Germans soon developed an industrial process optimized for yields of around per ton of biomass. This process soon found its way to the US, culminating in two commercial plants operating in the southeast during World War I. These plants used what was called \"the American Process\" — a one-stage dilute sulfuric acid hydrolysis. Though the yields were half that of the original German process ( of ethanol per ton versus 50), the throughput of the American process was much higher. A drop in lumber production forced the plants to close shortly after the end of World War I. In the meantime, a small but steady amount of research on dilute acid hydrolysis continued at the USFS's Forest Products Laboratory. During World War II, the US again turned to cellulosic ethanol, this time for conversion to butadiene to produce synthetic rubber. The Vulcan Copper and Supply Company was contracted to construct and operate a plant to convert sawdust into ethanol. The plant was based on modifications to the original German Scholler process as developed by the Forest Products Laboratory. This plant achieved an ethanol yield of per dry ton, but was still not profitable and was closed after the war.\n\nWith the rapid development of enzyme technologies in the last two decades, the acid hydrolysis process has gradually been replaced by enzymatic hydrolysis. Chemical pretreatment of the feedstock is required to prehydrolyze (separate) hemicellulose, so it can be more effectively converted into sugars. The dilute acid pretreatment is developed based on the early work on acid hydrolysis of wood at the USFS's Forest Products Laboratory. Recently, the Forest Products Laboratory together with the University of Wisconsin–Madison developed a sulfite pretreatment to overcome the recalcitrance of lignocellulose for robust enzymatic hydrolysis of wood cellulose.\n\nUS President George W. Bush, in his State of the Union address delivered January 31, 2006, proposed to expand the use of cellulosic ethanol. In his State of the Union Address on January 23, 2007, President Bush announced a proposed mandate for of ethanol by 2017. It is widely recognized that the maximum production of ethanol from corn starch is per year, implying a proposed mandate for production of some more per year of cellulosic ethanol by 2017. Bush's proposed plan includes $2 billion funding (from 2007 to 2017?) for cellulosic ethanol plants, with an additional $1.6 billion (from 2007 to 2017?) announced by the USDA on January 27, 2007.\n\nIn March 2007, the US government awarded $385 million in grants aimed at jump-starting ethanol production from nontraditional sources like wood chips, switchgrass, and citrus peels. Half of the six projects chosen will use thermochemical methods and half will use cellulosic ethanol methods.\n\nThe American company Range Fuels announced in July 2007 that it was awarded a construction permit from the state of Georgia to build the first commercial-scale -per-year cellulosic ethanol plant in the US. Construction began in November, 2007. The Range Fuels plant was built in Soperton, GA, but was shut down in January 2011, without ever having produced any ethanol. It had received a $76 million grant from the US Department of Energy, plus $6 million from the State of Georgia, plus an $80 million loan guaranteed by the U.S. Biorefinery Assistance Program. The United States (US) and Brazil have been the two leading producers of fuel ethanol since the 1970s.\n\nThe two ways of producing ethanol from cellulose are:\n\nAs is normal for pure ethanol production, these methods include distillation.\n\nThe stages to produce ethanol using a biological approach are:\n\nIn 2010, a genetically engineered yeast strain was developed to produce its own cellulose-digesting enzymes. Assuming this technology can be scaled to industrial levels, it would eliminate one or more steps of cellulolysis, reducing both the time required and costs of production.\n\nAlthough lignocellulose is the most abundant plant material resource, its usability is curtailed by its rigid structure. As a result, an effective pretreatment is needed to liberate the cellulose from the lignin seal and its crystalline structure so as to render it accessible for a subsequent hydrolysis step. By far, most pretreatments are done through physical or chemical means. To achieve higher efficiency, both physical and chemical pretreatments are required. Physical pretreatment is often called size reduction to reduce biomass physical size. Chemical pretreatment is to remove chemical barriers so the enzymes can have access to cellulose for microbial reactions.\n\nTo date, the available pretreatment techniques include acid hydrolysis, steam explosion, ammonia fiber expansion, organosolv, sulfite pretreatment, AVAP® (SO2-ethanol-water) fractionation, alkaline wet oxidation and ozone pretreatment. Besides effective cellulose liberation, an ideal pretreatment has to minimize the formation of degradation products because of their inhibitory effects on subsequent hydrolysis and fermentation processes. The presence of inhibitors will not only further complicate the ethanol production but also increase the cost of production due to entailed detoxification steps. Even though pretreatment by acid hydrolysis is probably the oldest and most studied pretreatment technique, it produces several potent inhibitors including furfural and hydroxymethyl furfural (HMF) which are by far regarded as the most toxic inhibitors present in lignocellulosic hydrolysate. Ammonia Fiber Expansion (AFEX) is a promising pretreatment with no inhibitory effect in resulting hydrolysate.\n\nMost pretreatment processes are not effective when applied to feedstocks with high lignin content, such as forest biomass. Organosolv, SPORL ('sulfite pretreatment to overcome recalcitrance of lignocellulose') and SO2-ethanol-water (AVAP®) processes are the three processes that can achieve over 90% cellulose conversion for forest biomass, especially those of softwood species. SPORL is the most energy efficient (sugar production per unit energy consumption in pretreatment) and robust process for pretreatment of forest biomass with very low production of fermentation inhibitors. Organosolv pulping is particularly effective for hardwoods and offers easy recovery of a hydrophobic lignin product by dilution and precipitation. AVAP® process effectively fractionates all types of lignocellulosics into clean highly digestible cellulose, undegraded hemicellulose sugars, reactive lignin and lignosulfonates, and is characterized by efficient recovery of chemicals.\n\nThere are two major cellulose hydrolysis (cellulolysis) processes: a chemical reaction using acids, or an enzymatic reaction use cellulases.\n\nThe cellulose molecules are composed of long chains of sugar molecules. In the hydrolysis of cellulose (that is, cellulolysis), these chains are broken down to free the sugar before it is fermented for alcohol production.\n\nIn the traditional methods developed in the 19th century and at the beginning of the 20th century, hydrolysis is performed by attacking the cellulose with an acid. Dilute acid may be used under high heat and high pressure, or more concentrated acid can be used at lower temperatures and atmospheric pressure. A decrystalized cellulosic mixture of acid and sugars reacts in the presence of water to complete individual sugar molecules (hydrolysis). The product from this hydrolysis is then neutralized and yeast fermentation is used to produce ethanol. As mentioned, a significant obstacle to the dilute acid process is that the hydrolysis is so harsh that toxic degradation products are produced that can interfere with fermentation. BlueFire Renewables uses concentrated acid because it does not produce nearly as many fermentation inhibitors, but must be separated from the sugar stream for recycle [simulated moving bed (SMB) chromatographic separation, for example] to be commercially attractive.\n\nAgricultural Research Service scientists found they can access and ferment almost all of the remaining sugars in wheat straw. The sugars are located in the plant’s cell walls, which are notoriously difficult to break down. To access these sugars, scientists pretreated the wheat straw with alkaline peroxide, and then used specialized enzymes to break down the cell walls. This method produced of ethanol per ton of wheat straw.\n\nCellulose chains can be broken into glucose molecules by cellulase enzymes.\n\nThis reaction occurs at body temperature in the stomachs of ruminants such as cattle and sheep, where the enzymes are produced by microbes. This process uses several enzymes at various stages of this conversion. Using a similar enzymatic system, lignocellulosic materials can be enzymatically hydrolyzed at a relatively mild condition (50 °C and pH 5), thus enabling effective cellulose breakdown without the formation of byproducts that would otherwise inhibit enzyme activity. All major pretreatment methods, including dilute acid, require an enzymatic hydrolysis step to achieve high sugar yield for ethanol fermentation.\nCurrently, most pretreatment studies have been laboratory-based, but companies are exploring means to transition from the laboratory to pilot, or production scale.\n\nVarious enzyme companies have also contributed significant technological breakthroughs in cellulosic ethanol through the mass production of enzymes for hydrolysis at competitive prices.\n\nThe fungus \"Trichoderma reesei\" is used by Iogen Corporation to secrete \"specially engineered enzymes\" for an enzymatic hydrolysis process. Their raw material (wood or straw) has to be pre-treated to make it amenable to hydrolysis.\n\nAnother Canadian company, SunOpta, uses steam explosion pretreatment, providing its technology to Verenium (formerly Celunol Corporation)'s facility in Jennings, Louisiana, Abengoa's facility in Salamanca, Spain, and a China Resources Alcohol Corporation in Zhaodong. The CRAC production facility uses corn stover as raw material.\n\nGenencor and Novozymes have received United States Department of Energy funding for research into reducing the cost of cellulases, key enzymes in the production of cellulosic ethanol by enzymatic hydrolysis. A recent breakthrough in this regard was the discovery and inclusion of \"lytic polysaccharide monooxygenases\". These enzymes are capable of boosting significantly the action of other cellulases by oxidatively attacking a polysaccharide substrate.\n\nOther enzyme companies, such as Dyadic International, are developing genetically engineered fungi which would produce large volumes of cellulase, xylanase and hemicellulase enzymes, which can be used to convert agricultural residues such as corn stover, distiller grains, wheat straw and sugarcane bagasse and energy crops such as switchgrass into fermentable sugars which may be used to produce cellulosic ethanol.\n\nIn 2010, BP Biofuels bought out the cellulosic ethanol venture share of Verenium, which had itself been formed by the merger of Diversa and Celunol, and with which it jointly owned and operated a per year demonstration plant in Jennings, LA, and the laboratory facilities and staff in San Diego, CA. BP Biofuels continues to operate these facilities, and has begun first phases to construct commercial facilities. Ethanol produced in the Jennings facility was shipped to London and blended with gasoline to provide fuel for the Olympics.\n\nKL Energy Corporation, formerly KL Process Design Group, began commercial operation of a per year cellulosic ethanol facility in Upton, WY in the last quarter of 2007. The Western Biomass Energy facility is currently achieving yields of per dry ton. It is the first operating commercial cellulosic ethanol facility in the nation. The KL Energy process uses a thermomechanical breakdown and enzymatic conversion process. The primary feedstock is soft wood, but lab tests have already proven the KL Energy process on wine pomace, sugarcane bagasse, municipal solid waste, and switchgrass.\n\nTraditionally, baker’s yeast (\"Saccharomyces cerevisiae\"), has long been used in the brewery industry to produce ethanol from hexoses (six-carbon sugars). Due to the complex nature of the carbohydrates present in lignocellulosic biomass, a significant amount of xylose and arabinose (five-carbon sugars derived from the hemicellulose portion of the lignocellulose) is also present in the hydrolysate. For example, in the hydrolysate of corn stover, approximately 30% of the total fermentable sugars is xylose. As a result, the ability of the fermenting microorganisms to use the whole range of sugars available from the hydrolysate is vital to increase the economic competitiveness of cellulosic ethanol and potentially biobased proteins.\n\nIn recent years, metabolic engineering for microorganisms used in fuel ethanol production has shown significant progress. Besides \"Saccharomyces cerevisiae\", microorganisms such as \"Zymomonas mobilis\" and \"Escherichia coli\" have been targeted through metabolic engineering for cellulosic ethanol production.\n\nRecently, engineered yeasts have been described efficiently fermenting xylose, and arabinose, and even both together. Yeast cells are especially attractive for cellulosic ethanol processes because they have been used in biotechnology for hundreds of years, are tolerant to high ethanol and inhibitor concentrations and can grow at low pH values to reduce bacterial contamination.\n\nSome species of bacteria have been found capable of direct conversion of a cellulose substrate into ethanol. One example is \"Clostridium thermocellum\", which uses a complex cellulosome to break down cellulose and synthesize ethanol. However, \"C. thermocellum\" also produces other products during cellulose metabolism, including acetate and lactate, in addition to ethanol, lowering the efficiency of the process. Some research efforts are directed to optimizing ethanol production by genetically engineering bacteria that focus on the ethanol-producing pathway.\n\nThe gasification process does not rely on chemical decomposition of the cellulose chain (cellulolysis). Instead of breaking the cellulose into sugar molecules, the carbon in the raw material is converted into synthesis gas, using what amounts to partial combustion. The carbon monoxide, carbon dioxide and hydrogen may then be fed into a special kind of fermenter. Instead of sugar fermentation with yeast, this process uses \"Clostridium ljungdahlii\" bacteria. This microorganism will ingest carbon monoxide, carbon dioxide and hydrogen and produce ethanol and water. The process can thus be broken into three steps:\n\nA recent study has found another \"Clostridium\" bacterium that seems to be twice as efficient in making ethanol from carbon monoxide as the one mentioned above.\n\nAlternatively, the synthesis gas from gasification may be fed to a catalytic reactor where it is used to produce ethanol and other higher alcohols through a thermochemical process. This process can also generate other types of liquid fuels, an alternative concept successfully demonstrated by the Montreal-based company Enerkem at their facility in Westbury, Quebec.\n\nStudies are intensively conducted to develop economic methods to convert both cellulose and hemicellulose to ethanol. Fermentation of glucose, the main product of cellulose hydrolyzate, to ethanol is an already established and efficient technique. However, conversion of xylose, the pentose sugar of hemicellulose hydrolyzate, is a limiting factor, especially in the presence of glucose. Moreover, it cannot be disregarded as hemicellulose will increase the efficiency and cost-effectiveness of cellulosic ethanol production.\n\nSakamoto (2012) et al. show the potential of genetic engineering microbes to express hemicellulase enzymes. The researchers created a recombinant Saccharomyces cerevisiae strain that was able to:\n\n\nThe strain was able to convert rice straw hydrolyzate to ethanol, which contains hemicellulosic components. Moreover, it was able to produce 2.5x more ethanol than the control strain, showing the highly effective process of cell surface-engineering to produce ethanol.\n\nThe shift to a renewable fuel resource has been a target for many years now. However, most of its production is with the use of corn ethanol. In the year 2000, only 6.2 billion liters were produced in the United States, but this number has expanded over 800% to 50 billion litres in just a decade (2010). Government pressures to shift to renewable fuel resources have been apparent since the U.S. Environmental Protection Agency implemented the 2007 Renewable Fuel Standard (RFS), which required that a certain percentage of renewable fuel be included in fuel products. The shift to cellulosic ethanol production from corn ethanol has been strongly promoted by the US government. Even with these policies in place and the government's attempts to create a market for cellulose ethanol, there was no commercial production of this fuel in 2010 and 2011. The Energy Independence and Security Act originally set goals of 100 million, 250 million, and 500 million gallons for the years 2010, 2011, and 2012 respectively. However, as of 2012 it was projected that the production of cellulosic ethanol would be approximately 10.5 million gallons--far from its target. In 2007 alone, the US government provided 1 billion US dollars for cellulosic ethanol projects, while China invested 500 million US dollars into cellulosic ethanol research.\n\nDue to the lack of existing commercialized plant data, it is difficult to determine the exact method of production that will be most commonly employed. Model systems try to compare different technologies' costs, but these models cannot be applied to commercial-plant costs. Currently, there are many pilot and demonstration facilities open that exhibit cellulosic production on a smaller scale. These main facilities are summarized in the table below.\n\nStart-up costs for pilot scale lignocellulosic ethanol plants are high. On 28 February 2007, the U.S. Department of Energy announced $385 million in grant funding to six cellulosic ethanol plants. This grant funding accounts for 40% of the investment costs. The remaining 60% comes from the promoters of those facilities. Hence, a total of $1 billion will be invested for approximately capacity. This translates into $7/annual gallon production capacity in capital investment costs for pilot plants; future capital costs are expected to be lower. Corn-to-ethanol plants cost roughly $1–3/annual gallon capacity, though the cost of the corn itself is considerably greater than for switchgrass or waste biomass.\n\nAs of 2007, ethanol is produced mostly from sugars or starches, obtained from fruits and grains. In contrast, cellulosic ethanol is obtained from cellulose, the main component of wood, straw, and much of the structure of plants. Since cellulose cannot be digested by humans, the production of cellulose does not compete with the production of food, other than conversion of land from food production to cellulose production (which has recently started to become an issue, due to rising wheat prices.) The price per ton of the raw material is thus much cheaper than that of grains or fruits. Moreover, since cellulose is the main component of plants, the whole plant can be harvested. This results in much better yields—up to , instead of 4-5 short tons/acre (9–11 t/ha) for the best crops of grain.\n\nThe raw material is plentiful. An estimated 323 million tons of cellulose-containing raw materials which could be used to create ethanol are thrown away each year in US alone. This includes 36.8 million dry tons of urban wood wastes, 90.5 million dry tons of primary mill residues, 45 million dry tons of forest residues, and 150.7 million dry tons of corn stover and wheat straw.\nTransforming them into ethanol using efficient and cost-effective hemi(cellulase) enzymes or other processes might provide as much as 30% of the current fuel consumption in the United States. Moreover, even land marginal for agriculture could be planted with cellulose-producing crops, such as switchgrass, resulting in enough production to substitute for all the current oil imports into the United States.\n\nPaper, cardboard, and packaging comprise a substantial part of the solid waste sent to landfills in the United States each day, 41.26% of all organic municipal solid waste (MSW) according to California Integrated Waste Management Board's city profiles. These city profiles account for accumulation of daily per landfill where an average population density of 2,413 per square mile persists. All these, except gypsum board, contain cellulose, which is transformable into cellulosic ethanol. This may have additional environmental benefits because decomposition of these products produces methane, a potent greenhouse gas.\n\nReduction of the disposal of solid waste through cellulosic ethanol conversion would reduce solid waste disposal costs by local and state governments. It is estimated that each person in the US throws away of trash each day, of which 37% contains waste paper, which is largely cellulose. That computes to 244 thousand tons per day of discarded waste paper that contains cellulose. The raw material to produce cellulosic ethanol is not only free, it has a negative cost—i.e., ethanol producers can get paid to take it away.\n\nIn June 2006, a U.S. Senate hearing was told the current cost of producing cellulosic ethanol is US $2.25 per US gallon (US $0.59/litre), primarily due to the current poor conversion efficiency. At that price, it would cost about $120 to substitute a barrel of oil (), taking into account the lower energy content of ethanol. However, the Department of Energy is optimistic and has requested a doubling of research funding. The same Senate hearing was told the research target was to reduce the cost of production to US $1.07 per US gallon (US $0.28/litre) by 2012. \"The production of cellulosic ethanol represents not only a step toward true energy diversity for the country, but a very cost-effective alternative to fossil fuels. It is advanced weaponry in the war on oil,\" said Vinod Khosla, managing partner of Khosla Ventures, who recently told a Reuters Global Biofuels Summit that he could see cellulosic fuel prices sinking to $1 per gallon within ten years.\n\nIn September 2010, a report by Bloomberg analyzed the European biomass infrastructure and future refinery development. Estimated prices for a litre of ethanol in August 2010 are EUR 0.51 for 1g and 0.71 for 2g. The report suggested Europe should copy the current US subsidies of up to $50 per dry tonne.\n\nRecently on October 25, 2012, BP, one of the leaders in fuel products, announced the cancellation of their proposed $350 million commercial-scale plant. It was estimated that the plant would be producing 36 million gallons a year at its location in Highlands County of Florida. BP has still provided 500 million US dollars for biofuel research at the Energy Biosciences Institute. General Motors (GM) has also invested into cellulosic companies more specifically Mascoma and Coskata. There are many other companies in construction or heading towards it. Abengoa is building a 25 million-gallon per year plant in \\ technology platform based on the fungus Myceliophthora thermophila to convert lignocellulose into fermentable sugars. Poet is also in midst of producing a 200 million dollar, 25-million-gallon per year in Emmetsburg, Iowa. Mascoma now partnered with Valero has declared their intention to build a 20 million gallon per year in Kinross, Michigan. China Alcohol Resource Corporation has developed a 6.4 million liter cellulosic ethanol plant under continuous operation.\n\nAlso, since 2013, the Brazilian company GranBio is working to become a producer of biofuels and biochemicals. The family-held company is commissioning an 82 million liters per year (22 MMgy) cellulosic ethanol plant (2G ethanol) in the state of Alagoas, Brazil, which will be the first industrial facility of the group. GranBio's second generation ethanol facility is integrated to a first generation ethanol plant operated by Grupo Carlos Lyra, uses process technology from Beta Renewables, enzymes from Novozymes and yeast from DSM. Breaking ground in January 2013, the plant is in final commissioning. According to GranBio Annual Financial Records, the total investment was 208 million US Dollars.\n\nCellulases and hemicellulases used in the production of cellulosic ethanol are more expensive compared to their first generation counterparts. Enzymes required for maize grain ethanol production cost 2.64-5.28 US dollars per cubic meter of ethanol produced. Enzymes for cellulosic ethanol production are projected to cost 79.25 US dollars, meaning they are 20-40 times more expensive. The cost differences are attributed to quantity required. The cellulase family of enzymes have a one to two order smaller magnitude of efficiency. Therefore, it requires 40 to 100 times more of the enzyme to be present in its production. For each ton of biomass it requires 15-25 kilograms of enzyme. More recent estimates are lower, suggesting 1 kg of enzyme per dry tonne of biomass feedstock. There is also relatively high capital costs associated with the long incubation times for the vessel that perform enzymatic hydrolysis. Altogether, enzymes comprise a significant portion of 20-40% for cellulosic ethanol production. A recent paper estimates the range at 13-36% of cash costs, with a key factor being how the cellulase enzyme is produced. For cellulase produced offsite, enzyme production amounts to 36% of cash cost. For enzyme produced onsite in a separate plant, the fraction is 29%; for integrated enzyme production, the faction is 13%. One of the key benefits of integrated production is that biomass instead of glucose is the enzyme growth medium. Biomass costs less, and it makes the resulting cellulosic ethanol a 100% second-generation biofuel, i.e., it uses no ‘food for fuel’.\n\nIn general there are two types of feedstocks: forest (woody) Biomass and agricultural biomass. In the US, about 1.4 billion dry tons of biomass can be sustainably produced annually. About 370 million tons or 30% are forest biomass. Forest biomass has higher cellulose and lignin content and lower hemicellulose and ash content than agricultural biomass. Because of the difficulties and low ethanol yield in fermenting pretreatment hydrolysate, especially those with very high 5 carbon hemicellulose sugars such as xylose, forest biomass has significant advantages over agricultural biomass. Forest biomass also has high density which significantly reduces transportation cost. It can be harvested year around which eliminates long term storage. The close to zero ash content of forest biomass significantly reduces dead load in transportation and processing. To meet the needs for biodiversity, forest biomass will be an important biomass feedstock supply mix in the future biobased economy. However, forest biomass is much more recalcitrant than agricultural biomass. Recently, the USDA Forest Products Laboratory together with the University of Wisconsin–Madison developed efficient technologies that can overcome the strong recalcitrance of forest (woody) biomass including those of softwood species that have low xylan content. Short-rotation intensive culture or tree farming can offer an almost unlimited opportunity for forest biomass production.\n\nWoodchips from slashes and tree tops and saw dust from saw mills, and waste paper pulp are common forest biomass feedstocks for cellulosic ethanol production.\n\nThe following are a few examples of agricultural biomass:\n\nSwitchgrass (\"Panicum virgatum\") is a native tallgrass prairie grass. Known for its hardiness and rapid growth, this perennial grows during the warm months to heights of 2–6 feet. Switchgrass can be grown in most parts of the United States, including swamplands, plains, streams, and along the shores & \"interstate highways\". It is \"self-seeding\" (no tractor for sowing, only for mowing), resistant to many diseases and pests, & can produce high yields with low applications of fertilizer and other chemicals. It is also tolerant to poor soils, flooding, & drought; improves soil quality and prevents erosion due its type of root system.\n\nSwitchgrass is an approved cover crop for land protected under the federal Conservation Reserve Program (CRP). CRP is a government program that pays producers a fee for not growing crops on land on which crops recently grew. This program reduces soil erosion, enhances water quality, and increases wildlife habitat. CRP land serves as a habitat for upland game, such as pheasants and ducks, and a number of insects. Switchgrass for biofuel production has been considered for use on Conservation Reserve Program (CRP) land, which could increase ecological sustainability and lower the cost of the CRP program. However, CRP rules would have to be modified to allow this economic use of the CRP land.\n\nMiscanthus × giganteus is another viable feedstock for cellulosic ethanol production. This species of grass is native to Asia and is the sterile triploid hybrid of \"Miscanthus sinensis\" and \"Miscanthus sacchariflorus\". It can grow up to tall with little water or fertilizer input. Miscanthus is similar to switchgrass with respect to cold and drought tolerance and water use efficiency. Miscanthus is commercially grown in the European Union as a combustible energy source.\n\nCorn cobs and corn stover are the most popular agricultural biomass.\n\nIt has been suggested that Kudzu may become a valuable source of biomass.\n\nThe environmental impact from the production of fuels is an important factor in determining its feasibility as an alternative to fossil fuels. Over the long run, small differences in production cost, environmental ramifications, and energy output may have large effects. It has been found that cellulosic ethanol can produce a positive net energy output. The reduction in green house gas (GHG) emissions from corn ethanol and cellulosic ethanol compared with fossil fuels is drastic. Corn ethanol may reduce overall GHG emissions by about 13%, while that figure is around 88% or greater for cellulosic ethanol. As well, cellulosic ethanol can reduce carbon dioxide emissions to nearly zero.\n\nA major concern for the viability of current alternative fuels is the cropland needed to produce the required materials. For example, the production of corn for corn ethanol fuel competes with cropland that may be used for food growth and other feedstocks. The difference between this and cellulosic ethanol production is that cellulosic material is widely available and is derived from a large resource of things. Some crops used for cellulosic ethanol production include switchgrass, corn stover, and hybrid poplar. These crops are fast-growing and can be grown on many types of land which makes them more versatile. Cellulosic ethanol can also be made from wood residues (chips and sawdust), municipal solid waste such as trash or garbage, paper and sewage sludge, cereal straws and grasses. It is particularly the non-edible portions of plant material which are used to make cellulosic ethanol, which also minimizes the potential cost of using food products in production.\n\nThe effectiveness of growing crops for the purpose of biomass can vary tremendously depending on the geographical location of the plot. For example, factors such as precipitation and sunlight exposure may greatly effect the energy input required to maintain the crops, and therefore effect the overall energy output. A study done over five years showed that growing and managing switchgrass exclusively as a biomass energy crop can produce 500% or more renewable energy than is consumed during production. The levels of GHG emissions and carbon dioxide were also drastically decreased from using cellulosic ethanol compared with traditional gasoline.\n\nIn 2008, there was only a small amount of switchgrass dedicated for ethanol production. In order for it to be grown on a large-scale production it must compete with existing uses of agricultural land, mainly for the production of crop commodities. Of the United States' 2.26 billion acres (9.1 million km) of unsubmerged land, 33% are forestland, 26% pastureland and grassland, and 20% crop land. A study done by the U.S. Departments of Energy and Agriculture in 2005 determined whether there were enough available land resources to sustain production of over 1 billion dry tons of biomass annually to replace 30% or more of the nation’s current use of liquid transportation fuels. The study found that there could be 1.3 billion dry tons of biomass available for ethanol use, by making little changes in agricultural and forestry practices and meeting the demands for forestry products, food, and fiber. A recent study done by the University of Tennessee reported that as many as 100 million acres (400,000 km, or 154,000 sq mi) of cropland and pasture will need to be allocated to switchgrass production in order to offset petroleum use by 25 percent.\nCurrently, corn is easier and less expensive to process into ethanol in comparison to cellulosic ethanol. The Department of Energy estimates that it costs about $2.20 per gallon to produce cellulosic ethanol, which is twice as much as ethanol from corn. Enzymes that destroy plant cell wall tissue cost 30 to 50 cents per gallon of ethanol compared to 3 cents per gallon for corn. The Department of Energy hopes to reduce production cost to $1.07 per gallon by 2012 to be effective. However, cellulosic biomass is cheaper to produce than corn, because it requires fewer inputs, such as energy, fertilizer, herbicide, and is accompanied by less soil erosion and improved soil fertility. Additionally, nonfermentable and unconverted solids left after making ethanol can be burned to provide the fuel needed to operate the conversion plant and produce electricity. Energy used to run corn-based ethanol plants is derived from coal and natural gas. The Institute for Local Self-Reliance estimates the cost of cellulosic ethanol from the first generation of commercial plants will be in the $1.90–$2.25 per gallon range, excluding incentives. This compares to the current cost of $1.20–$1.50 per gallon for ethanol from corn and the current retail price of over $4.00 per gallon for regular gasoline (which is subsidized and taxed).\n\nOne of the major reasons for increasing the use of biofuels is to reduce greenhouse gas emissions. In comparison to gasoline, ethanol burns cleaner, thus putting less carbon dioxide and overall pollution in the air. Additionally, only low levels of smog are produced from combustion. According to the U.S. Department of Energy, ethanol from cellulose reduces greenhouse gas emission by 86 percent when compared to gasoline and to corn-based ethanol, which decreases emissions by 52 percent. Carbon dioxide gas emissions are shown to be 85% lower than those from gasoline. Cellulosic ethanol contributes little to the greenhouse effect and has a five times better net energy balance than corn-based ethanol. When used as a fuel, cellulosic ethanol releases less sulfur, carbon monoxide, particulates, and greenhouse gases. Cellulosic ethanol should earn producers carbon reduction credits, higher than those given to producers who grow corn for ethanol, which is about 3 to 20 cents per gallon.\n\nIt takes 0.76 J of energy from fossil fuels to produce 1 J worth of ethanol from corn.\nThis total includes the use of fossil fuels used for fertilizer, tractor fuel, ethanol plant operation, etc. Research has shown that fossil fuel can produce over five times the volume of ethanol from prairie grasses, according to Terry Riley, President of Policy at the Theodore Roosevelt Conservation Partnership. The United States Department of Energy concludes that corn-based ethanol provides 26 percent more energy than it requires for production, while cellulosic ethanol provides 80 percent more energy. Cellulosic ethanol yields 80 percent more energy than is required to grow and convert it. The process of turning corn into ethanol requires about 1700 times (by volume) as much water as ethanol produced. Additionally, it leaves 12 times its volume in waste. Grain ethanol uses only the edible portion of the plant. \nCellulose is not used for food and can be grown in all parts of the world. The entire plant can be used when producing cellulosic ethanol. Switchgrass yields twice as much ethanol per acre than corn. Therefore, less land is needed for production and thus less habitat fragmentation. Biomass materials require fewer inputs, such as fertilizer, herbicides, and other chemicals that can pose risks to wildlife. Their extensive roots improve soil quality, reduce erosion, and increase nutrient capture. Herbaceous energy crops reduce soil erosion by greater than 90%, when compared to conventional commodity crop production. This can translate into improved water quality for rural communities. Additionally, herbaceous energy crops add organic material to depleted soils and can increase soil carbon, which can have a direct effect on climate change, as soil carbon can absorb carbon dioxide in the air. As compared to commodity crop production, biomass reduces surface runoff and nitrogen transport. Switchgrass provides an environment for diverse wildlife habitation, mainly insects and ground birds. Conservation Reserve Program (CRP) land is composed of perennial grasses, which are used for cellulosic ethanol, and may be available for use.\n\nFor years American farmers have practiced row cropping, with crops such as sorghum and corn. Because of this, much is known about the effect of these practices on wildlife. The most significant effect of increased corn ethanol would be the additional land that would have to be converted to agricultural use and the increased erosion and fertilizer use that goes along with agricultural production. Increasing our ethanol production through the use of corn could produce negative effects on wildlife, the magnitude of which will depend on the scale of production and whether the land used for this increased production was formerly idle, in a natural state, or planted with other row crops.\nAnother consideration is whether to plant a switchgrass monoculture or use a variety of grasses and other vegetation. While a mixture of vegetation types likely would provide better wildlife habitat, the technology has not yet developed to allow the processing of a mixture of different grass species or vegetation types into bioethanol. Of course, cellulosic ethanol production is still in its infancy, and the possibility of using diverse vegetation stands instead of monocultures deserves further exploration as research continues.\n\nA study by Nobel Prize winner Paul Crutzen found ethanol produced from corn had a \"net climate warming\" effect when compared to oil when the full life cycle assessment properly considers the nitrous oxide (N20) emissions that occur during corn ethanol production. Crutzen found that crops with less nitrogen demand, such as grasses and woody coppice species, have\nmore favourable climate impacts.\n\nCellulosic ethanol commercialization is the process of building an industry out of methods of turning cellulose-containing organic matter into fuel. Companies such as Iogen, POET, and Abengoa are building refineries that can process biomass and turn it into ethanol, while companies such as DuPont, Diversa, Novozymes, and Dyadic are producing enzymes which could enable a cellulosic ethanol future. The shift from food crop feedstocks to waste residues and native grasses offers significant opportunities for a range of players, from farmers to biotechnology firms, and from project developers to investors.\n\nThe cellulosic ethanol industry developed some new commercial-scale plants in 2008. In the United States, plants totaling 12 million liters (3.17 million gal) per year were operational, and an additional 80 million liters (21.1 million gal.) per year of capacity - in 26 new plants - was under construction. In Canada, capacity of 6 million liters per year was operational. In Europe, several plants were operational in Germany, Spain, and Sweden, and capacity of 10 million liters per year was under construction.\n\nItaly-based Mossi & Ghisolfi Group broke ground for its 13 MMgy cellulosic ethanol facility in northwestern Italy on April 12, 2011. The project will be the largest cellulosic ethanol project in the world, 10 times larger than any of the currently operating demonstration-scale facilities.\n\n"}
{"id": "696280", "url": "https://en.wikipedia.org/wiki?curid=696280", "title": "Commercialization", "text": "Commercialization\n\nCommercialization or commercialisation is the process of introducing a new product or production method into commerce—making it available on the market. The term often connotes especially entry into the mass market (as opposed to entry into earlier niche markets), but it also includes a move from the laboratory into (even limited) commerce. Many technologies begin in a research and development laboratory or in an inventor's workshop and may not be practical for commercial use in their infancy (as prototypes). The \"development\" segment of the \"research and development\" spectrum requires time and money as systems are engineered with a view to making the product or method a paying commercial proposition. The product launch of a new product is the final stage of new product development - at this point advertising, sales promotion, and other marketing efforts encourage commercial adoption of the product or method. Beyond commercialization (in which technologies enter the business world) can lie consumerization (in which they become consumer goods, as for example when computers went from the laboratory to the enterprise and then to the home, pocket, or body).\n\nCommercialization is often confused with sales, marketing, or business development. The commercialization process has three key aspects:\n\nProposed commercialization of a product can raise the following questions:\n\n\n\n\n"}
{"id": "25818098", "url": "https://en.wikipedia.org/wiki?curid=25818098", "title": "Corrugated box design", "text": "Corrugated box design\n\nCorrugated box design is the process of matching design factors for corrugated fiberboard boxes with the functional physical, processing and end-use requirements. Packaging engineers work to meet the performance requirements of a box while controlling total costs throughout the system.\n\nIn addition to the structural design discussed in this article, printed bar codes, labels, and graphic design are also vital.\n\nCorrugated boxes are used frequently as shipping containers. Boxes need to contain the product from manufacturing through distribution to sale and sometimes end-use. Boxes provide some measure of product protection by themselves but often require inner components such as cushioning, bracing and blocking to help protect fragile contents. The shipping hazards depend largely upon the particular logistics system being employed. For example, boxes unitized into a unit load on a pallet do not encounter individual handling while boxes sorted and shipped through part of their distribution cycle as mixed loads or express carriers can receive severe shocks, kicks, etc...\n\nOrdinary shipping containers require printing and labels to identify the contents, provide legal and regulatory information, and bar codes for routing. Boxes that are used for marketing, merchandising, and point-of-sale often have high graphics to help communicate the contents. Some boxes are designed for display of contents on the shelf. Others are designed to help dispense the contents. Popular for their strength, durability, lightness, recyclability, and cost-effectiveness, corrugated boxes are used for the shipping of a variety of items. Due to the quality and safety of packaging items in corrugated boxes, they are used widely in the food industry. The boxes handle the pressure that comes with stacking, making them ideal for easy transporting.\n\nMore than 95% of all products in the United States are shipped in corrugated boxes. Corrugated paperboard accounts for more than half of all the paper recycled in the US.\n\nOne of the important functions of a corrugated box is to provide crush resistance (product protection) and adequate strength for stacking in warehouses.\n\nA box can be designed by optimizing the grade of corrugated board, box design, flute direction, and inner supports. Support from the product also provides \"load sharing\" and can be an important factor. Box closures sometimes can have effects on box stacking strength.\n\nIf long-term storage of corrugated boxes in high humidity is expected, extra strength and moisture resistance is called for.\n\nThe method of loading boxes on pallets strongly affects stacking. Vertical columns provide the best box performance while interlocking patterns of boxes significantly reduce performance. The interaction of the boxes and pallets is also important.\n\nBox compression testing is a means of evaluating boxes, stacks of boxes, and unit loads under controlled conditions. Field conditions of stacking and dynamic compression do not have the same degree of control. Compression strength can be estimated based on container construction, size, and use parameters: actual package testing is often conducted to verify these estimates.\n\nMany packaging engineers find it beneficial to periodically audit warehouses and visit customer's package receiving operations. When field performance is observed or documented to have problems, a new cycle of design and testing may be justified.\nfor a corrugated box to withstand the deformation while stacking or storage (stack load = 1/3 compression strength).\nwhere stack load=gross weight of box *stack height.\n\nFiber Box Association has a method for estimating compression strength which includes the following factors:\n\nPackaging engineers design corrugated boxes to meet the particular needs of the product being shipped, the hazards of the shipping environment, (shock, vibration, compression, moisture, etc.), and the needs of retailers and consumers\n\nEngineers and designers start with the needs of the particular project: cost constraints, machinery capabilities, product characteristics, logistics needs, applicable regulations, consumer needs, etc. Often designs are made with Computer Aided Design programs connected to automated sample making tables. Several design and construction options might be considered.\n\nSamples are often submitted to performance testing based on ASTM or other standard test protocols such as the International Safe Transit Association. Structural design is matched with graphic design. For consumer based designs, marketing personnel sometimes use Focus groups or more quantitative means of assessing acceptance. Test markets are employed for major programs.\n\nThe process starts by making corrugated board on a corrugating line, a long series of linked machines which may be in size of a football field. A finished piece of singlewall corrugated board is a single corrugated layer sandwiched between two liners.\n\nSkilled workers prepare job tickets for each stack of box blanks and route the blanks to fabrication machines. Printing dies and patterns are prepared on large, flexible, rubber or tin sheets. They are loaded onto rollers and the box blanks are fed through it, where each is trimmed, printed, cut, scored, folded, and glued to form a box. Finished boxes are then stacked and sent to a banding machine to be wrapped and shipped.\n\nThe most common box style is the Regular Slotted Container (RSC). All flaps are the same length from score to edge. Typically the major flaps meet in the middle and the minor flaps do not, unless the width is equal to the length. Box styles in Europe are typically specified by a 4-digit code provided by the European Federation of Corrugated Board Manufacturers (FEFCO): A regular slotted container is coded 0201.\n\nThe manufacturer's joint is most often joined with adhesive but may also be taped or stitched. The box is shipped flat (knocked down) to the packager who sets up the box, fills it, and closes it for shipment. Box closure may be by tape, adhesive, staples, strapping, etc.\n\nMany other styles of corrugated boxes and structures are available:\n\nThe size of a box can be measured for either internal (for product fit) or external (for handling machinery or palletizing) dimensions. Boxes are usually specified and ordered by the internal dimensions.\n\nRetailers often ask for merchandise to be delivered to them in shipping containers which allow the easy stocking of full case loads. The goal is to put the case directly onto shelves and stocking locations without individually handling the unit packs or primary packages. Retailers often require products to come in shelf -ready packaging to reduce stocking costs and save labor expenses.\nSeveral specialized box designs are available.\n\nMany items are shipped individually (in part or entirely) by express carrier, mail, or other mixed logistics systems. The demands of multiple manual handlings, automated sortation, and uncontrolled stacking in trucks or air containers put severe stress on boxes, box closures, and the contents. Boxes designed for unit load handling and storage may not be suited to mixed logistics systems.\n\nLess than truckload shipping puts more stress on corrugated shipping containers than shipment by uniform pallet loads in trucks or intermodal containers. Boxes sometimes need to be heavier construction to match the needs of the distribution system.\n\nMany items being supplied to governments are handled very well: boxes are unitized, shipped on covered trucks or intermodal containers, and storage is in warehouses. Normal “domestic boxes” and commercial packaging are acceptable.\n\nMilitary materiel, field supplies, and humanitarian aid often encounter severe handling and uncontrolled storage. Special box specifications for government shipments are often applicable. Weather-resistant fiberboards, box construction, box closure, and unitizing are needed.\nMany international shipments are handled very well: boxes are unitized, shipped on covered trucks or intermodal containers, and storage is in warehouses. Normal “domestic boxes” are commonly used.\n\nBreak bulk cargo needs to be water resistant and heavy duty. Even shipments initially sent via intermodal freight transport may have subsequent logistics operations that are extreme and uncontrolled. The specific conditions in the destination port and the country of use need to be considered for design of export boxes.\n\nShipment of dangerous goods or hazardous materials are highly regulated. Based on the UN Recommendations on the Transport of Dangerous Goods model regulations, each country has coordinated design and performance requirements for shipment. For example, in the US, the Department of Transportation has jurisdiction and published requirements in Title 49 of the Code of Federal Regulations. Corrugated boxes are described in 4G requirements. Performance (severe drop test, etc.) needs to be certified for the box and contents.\n\nSome carriers have additional requirements.\n\nThe means of closing a box is an important aspect of design. It affects the types of equipment available to production lines, the measured laboratory performance, the field performance, and the ability of end-users to easily and safely open the box.\n\nBox closures include:\n\n\n"}
{"id": "33434229", "url": "https://en.wikipedia.org/wiki?curid=33434229", "title": "Creep-testing machine", "text": "Creep-testing machine\n\nA creep-testing machine measures the creep (the tendency of a material after being subjected to high levels of stress, e.g. high temperatures, to change its form in relation to time) of an object. It is a device that measures the alteration of a material after it has been put through different forms of stress. Creep machines are important to see how much strain (load) an object can handle under pressure, so engineers and researchers are able to determine what materials to use. The device generates a creep time-dependent curve by calculating the steady rate of creep in reference to the time it takes for the material to change. Creep machines are primarily used by engineers to determine the stability of a material and its behaviour when it is put through ordinary stresses.\n\nThe first creep testing machines were created in 1948 in Britain to test materials for aircraft to see how they would stand in high altitudes, temperature and pressure. The machines were first developed to further calculate and understand the steady rate of creep in materials. Creep is the tendency of a material to change form over time after facing high temperature and stress. Creep increases with temperature and it is more common when a material is exposed to high temperatures for a long time or at the melting point of the material. Creep machines are used to understand the creep of materials and determine which type can do the job better, which is important when making and designing materials for everyday uses. They most commonly test the creep of alloys and plastics for the understanding of their properties and advantages of one material's use over another.\n\nResearchers look to test objects with a creep machine to understand the process of metallurgy and the physical mechanical properties of a metal, test the development of alloys, receive data from the loads that are derived and to find out whether a sample or material is within the boundary of what they are testing. The basic design of a creep machine is the furnace, loading device and support structure.\n\nThe main type of creep testing machine that is most commonly used is a constant load creep testing machine.The constant load creep machine consists of a loading platform, foundation, fixture devices and furnace. The fixture devices are the grips and pull rods.\n\n\nCreep machines are most commonly used in experiments to determine how efficient and stable a material is. The machine is used by students and companies to create a creep curve on how much pressure and stress a material can handle. The machine is able to calculate the stress rate, time and pressure.\n\nCreep testing has three different applications in the industry:\n\nCreep is dependent on time so the curve that the machine generates is a time vs. strain graph. The slope of a creep curve is the creep rate dε/dt The trend of the curve is an upward slope. The graphs are important to learn the trends of the alloys or materials used and by the production of the creep-time graph, it is easier to determine the better material for a specific application.\n\nThere are three stages of creep: \nBy examining the three stages above, scientists are able to determine the temperature and interval in which an object will be disturbed once exposed to the load. Some materials have a very small secondary creep state and may go straight from the primary creep to the tertiary creep state. This is dependent on the properties of the material that is being tested. This is important to note because going straight to the tertiary state causes the material to break faster from its form.\n\nA linear graph denotes that the material under stress is gradually deforming and this would be harder to track at what level of stress an object can handle. This would also mean that the material would not have distinct stages,which would make object's breaking point would be less predictable. This is a disadvantage to scientists and engineers when trying to determine the level of creep the object can handle.\n"}
{"id": "1910136", "url": "https://en.wikipedia.org/wiki?curid=1910136", "title": "Cyclotol", "text": "Cyclotol\n\nCyclotol is an explosive consisting of castable mixtures of RDX and TNT.<br>\nIt is related to the more common Composition B, which is roughly 60% RDX and 40% TNT; various compositions of Cyclotol contain from 65% to 80% RDX.\n\nTypical ranges are from 60/40 to 80/20 RDX/TNT, with the most common being 70/30, while the military mostly uses 77/23 optimized in warheads.\n\nCyclotol is not commonly used, but was reportedly the main explosive used in at least some models of US Nuclear weapon. Sublette lists Cyclotol as the explosive in the US B28 nuclear bomb and possibly related weapons that used the common Python primary - W34, W28, W40, and W49.\nIt was used in the B53 nuclear bomb and associated W53 warhead.\n\nIt was also used in the Mecca Masjid bombing which killed 16 people.\n"}
{"id": "2072855", "url": "https://en.wikipedia.org/wiki?curid=2072855", "title": "Decanter", "text": "Decanter\n\nA decanter is a vessel that is used to hold the decantation of a liquid (such as wine) which may contain sediment. Decanters, which have a varied shape and design, have been traditionally made from glass or crystal. Their volume is usually equivalent to one standard bottle of wine (0.75 litre). \n\nA carafe, which is also traditionally used for serving alcoholic beverages, is similar in design to a decanter but is not supplied with a stopper.\n\nThroughout the history of wine, decanters have played a significant role in the serving of wine. The vessels would be filled with wine from amphoras and brought to the table where they could be more easily handled by a single servant. The Ancient Romans pioneered the use of glass as a material. After the fall of the Western Roman Empire, glass production became scarce causing the majority of decanters to be made of bronze, silver, gold, or earthenware. The Venetians reintroduced glass decanters during the Renaissance period and pioneered the style of a long slender neck that opens to a wide body, increasing the exposed surface area of the wine, allowing it to react with air. In the 1730s, British glass makers introduced the stopper to limit exposure to air. Since then, there has been little change to the basic design of the decanter. \n\nAlthough conceived for wine, other alcoholic beverages, such as cognac or single malt Scotch whisky, are often stored and served in stoppered decanters. Certain cognacs and malt whiskies are sold in decanters such as the 50-year-old single malt Dalmore or the Bowmore Distillery 22 Year Old.\n\nLiquid from another vessel is poured into the decanter in order to separate a small volume of liquid, containing the sediment, from a larger volume of \"clear\" liquid, which is free of such. In the process, the sediment is left in the original vessel, and the clear liquid is transferred to the decanter. This is analogous to racking, but performed just before serving.\n\nDecanters have been used for serving wines that are laden with sediments in the original bottle. These sediments could be the result of a very old wine or one that was not filtered or clarified during the winemaking process. In most modern winemaking, the need to decant for this purpose has been significantly reduced, because many wines no longer produce a significant amount of sediment as they age.\n\nAnother reason for decanting wine is to aerate it, or allow it to \"breathe\". The decanter is meant to mimic the effects of swirling the wine glass to stimulate the oxidation processes which triggers the release of more aromatic compounds. In addition it is thought to benefit the wine by smoothing some of the harsher aspects of the wine (like tannins or potential wine faults like mercaptans). Many wine writers, such as author Karen MacNeil in the book \"The Wine Bible\", advocate decanting for the purposes of aeration, especially with very tannic wines like Barolo, Bordeaux, Cabernet Sauvignon, Port, and Rhône wines while noting that decanting could be harmful for more delicate wines like Chianti and Pinot noir.\n\nHowever, the effectiveness of decanting is a topic of debate, with some wine experts such as oenologist Émile Peynaud claiming that the prolonged exposure to oxygen actually diffuses and dissipates more aroma compounds than it stimulates, in contrast to the effects of the smaller scale exposure and immediate release that swirling the wine in a drinker's glass has. In addition it has been reported that the process of decanting over a period of a few hours does not have the effect of softening tannins. The softening of tannins occurs during the winemaking and oak aging when tannins go through a process of polymerization that can last days or weeks; decanting merely alters the perception of sulfites and other chemical compounds in the wine through oxidation, which can give some drinkers the sense of softer tannins in the wine. In line with the view that decanting can dissipate aromas, wine expert Kerin O'Keefe prefers to let the wine evolve slowly and naturally in the bottle, by uncorking it a few hours ahead, a practice suggested by wine producers such as Bartolo Mascarello and Franco Biondi Santi. \n\nOther wine experts, such as writer Jancis Robinson, tout the aesthetic value of using a decanter, especially one with an elegant design and made with clear glass, and believe that for all but the most fragile of wines that there is not much significant damage to the wine by decanting it. A decanter can also be used to present wine anonymously.\n"}
{"id": "12695899", "url": "https://en.wikipedia.org/wiki?curid=12695899", "title": "Digital duplicator", "text": "Digital duplicator\n\nA digital duplicator, also known as a printer-duplicator, is a printing technology designed for high-volume print jobs (20 copies or more). Digital duplicators can provide a reliable and cost efficient alternative to toner-based copiers or offset printing equipment.\n\nThe digital duplicator begins by digitally scanning the original and then transferring it to a master template through a thermal imaging process. Then the master is automatically wrapped around a print cylinder, where the ink is drawn through the perforations in the master creating the print.\n\nDigital duplicators are known for their high speed in comparison to other printing methods. They are able to produce anywhere from 45 to 180 prints per minute, while maintaining a per page cost that can be as low as 1/3 of a cent. They are also considered very reliable because they do not use heat or copier components, such as toner.\n"}
{"id": "43830039", "url": "https://en.wikipedia.org/wiki?curid=43830039", "title": "Double-T armature", "text": "Double-T armature\n\nDouble-T armature - Shape of armature used in some motors and generators. Two 'T' shapes extending from axis.\n\nIts use in early Siemens dynamos caused a pulsing variable DC output which was improved upon by the Gramme dynamo.\n\nAnother improvement is the three-pole armature ('triple-T' armature) as in this image :\n\n"}
{"id": "967372", "url": "https://en.wikipedia.org/wiki?curid=967372", "title": "Drinking straw", "text": "Drinking straw\n\nA drinking straw or drinking tube is a small pipe that allows its user to more conveniently consume a beverage. A thin tube of paper, plastic (such as polypropylene and polystyrene), or other material is used by placing one end in the mouth and the other in the beverage. A combination of muscular action of the tongue and cheeks reduces air pressure in the mouth and above the liquid in the straw, whereupon atmospheric pressure forces the beverage through the straw. Drinking straws can be straight or have an angle-adjustable bellows segment.\n\nPlastic straws account for a significant amount of unrecycled plastic waste, and contribute to plastic pollution in the ocean. As a result, numerous campaigns in the 2010s have led to companies considering a switch to paper straws, even though 90% of ocean plastic comes from 10 rivers in Asia and Africa.\n\nThe first known straws were made by the Sumerians, and were used for drinking beer, probably to avoid the solid byproducts of fermentation that sink to the bottom. The oldest drinking straw in existence, found in a Sumerian tomb dated 3,000 BCE, was a gold tube inlaid with the precious blue stone lapis lazuli. Argentines and their neighbors have, for several hundred years, used (for drinking mate tea) a similar metallic device called a bombilla, that acts as both a straw and a sieve.\nIn the 1800s, the rye grass straw came into fashion because it was cheap and soft, but it had an unfortunate tendency to turn to mush in liquid.\n\nMarvin C. Stone patented the modern drinking straw, made of paper, in 1888, to address the shortcomings of the rye grass straw. He came upon the idea while drinking a mint julep on a hot day in Washington, D.C.; the taste of the rye was mixing with the drink and giving it a grassy taste, which he found unsatisfactory. He wound paper around a pencil to make a thin tube, slid out the pencil from one end, and applied glue between the strips. He later refined it by building a machine that would coat the outside of the paper with wax to hold it together, so the glue wouldn't dissolve in bourbon.\n\nEarly paper straws had a narrow bore similar to that of the grass stems then in common use. It was common to use two of them, to reduce the effort needed to take each sip. (The cocktail straw, which is sometimes used in pairs, may be derived from such early straws.) Modern plastic straws are made with a larger bore, and only one is needed for ease of drinking.\n\n\n\nPlastic drinking straw production contributes to petroleum consumption, and the used straws become part of global plastic pollution when discarded, most after a single use. One anti-straw advocacy group has estimated that about 500 million straws are used daily in the United States alone – an average 1.6 straws per capita per day. This statistic has been criticized as inaccurate, because it is a guess made by Milo Cress, who was 9 years old at the time, after some phone conversations with straw manufacturers. This figure has been widely cited by major news organizations. In 2017 the market research firm Fredonia Group estimated the number to be 390 million. By weight, drinking straws account for approximately 0.022% of the plastic waste that gets dumped in bodies of water each year.\n\nStraws are typically made from polypropylene, mixed with colorants and plasticizers, and do not biodegrade in the environment. Since the material is strong it can however be reused or recycled into other products. Waste straws in Uganda are collected from beer and soft drink depots, cleaned, and woven into mats for picnics and prayers or joined to form bags.\n\nEnvironmentally more friendly alternatives to plastic straws, some even reusable, exist, although not always readily available:\n\n\nEnvironmental groups have encouraged consumers to object to \"forced\" inclusion of plastic straws with food service. Campaigns advocate providing a non-plastic straw to consumers who request one, especially as some people have disabilities that prevent sipping from the rim. Pro-environment critics say that plastic straw bans are insufficient to address the issue of plastic waste, as mostly symbolic. People with disabilities worry that banning plastic straws will make less accessible and many argue that alternatives to plastic straws (metal, paper, glass etc.) aren't good enough alternatives to plastics as they can be choking hazards, have allergy risks and be inflexible.\n\nThe movement follows the discovery of plastic particles in oceanic garbage patches and larger plastic waste-reduction efforts that focused on banning plastic bags in some jurisdictions. It has been speeded by viral videos, including one featuring a sea turtle bleeding as a plastic straw is removed from its nostril.\n\nAfter the British proposition, fellow Commonwealth nation Canada was considering banning the straws too. An unofficial online survey showed that over 70% of voters agreed with a plastic straw ban.\n\nStarting in 2019, a ban of plastic straws will go into effect in the City of Vancouver, due to a vote in May 2018 that included banning other single-use items.\n\nIn May 2018, the European Union proposed a ban on single-use plastics including straws, cotton buds, cutlery, balloon sticks and drink stirrers.\n\nOn April 19, 2018, ahead of Earth Day, a proposal to phase out single-use plastics was announced during the meeting of the Commonwealth Heads of Government. This will include plastic drinking straws, which cannot be recycled and contribute to ocean deterioration, damaging ecosystems and wildlife. It is estimated that as of 2018, about 23 million straws are used and discarded daily in the UK.\n\nA few months before, Queen Elizabeth II banned the plastic straws and other one-use plastic items from her palaces.\n\nIn 2015, Williamstown, Massachusetts banned straws that are not recyclable or compostable as part of its Article 42 polystyrene regulations.\n\nOn November 7, 2017, the city of Santa Cruz, California implemented a ban on all non-recyclable to-go containers, straws, and lids but allowed for 6 months for all businesses to come into compliance before enforcement would occur. On January 1, 2018, the city of Alameda, California citing the Santa Cruz effort, implemented an immediate ban on all straws, except if requested by a customer, and gave business until July 1, 2018 when it would be required that all straws to be of compostable paper and that all other to-go containers be recyclable.\n\nIn the first half of 2018, three towns in Massachusetts banned petrochemical plastic straws directly in the case of Provincetown, and as part of broader sustainable food packaging laws in Andover and Brookline. The city of Seattle implemented a ban on non-compostable disposable straws on July 1, 2018.\n\nA drinking straw ban has been proposed in New York City since May 2018. Local regulations have also been passed in Malibu, California; Davis, California; San Luis Obispo, California; Miami Beach, Florida; and Fort Myers, Florida.\n\nA statewide California law restricting the providing of single-use plastic straws will go into effect on January 1, 2019. Under the law, restaurants will only be allowed to provide single-use plastic straws upon request. The law will apply to sit-down restaurants but exempts fast-food restaurants, delis, coffee shops, and restaurants that do takeout only. The law will not apply to to-go cups and takeaway drinks. A restaurant will receive warnings for its first two violations, then a $25 per day fine for each subsequent violation, up to a maximum of $300 in a year. In a statement released upon his signing the legislation into law, Governor Jerry Brown said \"It is a very small step to make a customer who wants a plastic straw ask for it. And it might make them pause and think again about an alternative. But one thing is clear, we must find ways to reduce and eventually eliminate single-use plastic products.\"\n\nAfter consideration of a ban in the UK, in 2018, after a two month trial of paper straws at a number of outlets in the UK, McDonald's announced they would be switching to paper straws for all locations in the United Kingdom and Ireland. and testing the switch in U.S. locations in June 2018.\n\nA month after the Vancouver ban passed (but before it took effect) Canada's second-largest fast food chain, A&W announced they would have plastic straws fully phased out by January 2019 in all of their locations.\n\nVarious independent restaurants have also stopped using plastic straws.\n\nStarbucks announced conversion by 2020 to no-straw lids for all cold drinks except for frappucinos, which will be served with straws made from paper or other sustainable materials. However, the new lids will actually have more plastic than the old lid-straw combination.\n\nHyatt Hotels announced straws would be provided by request only, starting September 1, 2018. Royal Caribbean plans to offer only paper straws on request by 2019, and IKEA said it would eliminate all single-use plastic items by 2020. Other conversions include Waitrose, London City Airport, and Burger King UK stores starting September 2018. A few other cruise lines, air lines, beverage companies, and hotels, have also made partial or complete reductions, but most companies in those industries have not, as of May 2018.\n\nNicholson Baker's novel, \"The Mezzanine\" (1988), includes a detailed discussion of various types of drinking straws experienced by the narrator and their relative merits.\n\n"}
{"id": "43291417", "url": "https://en.wikipedia.org/wiki?curid=43291417", "title": "Dry low emission", "text": "Dry low emission\n\nDry low emission (abbreviation DLE) is a technology that reduces NOx emissions that exhausts out of gas fired turbines.\n\nThe amount of NOx produced depends on the combustion temperature.\nWhen the combustion takes place at a lower temperature the NOx emissions are reduced.\nGas turbines with DLE technology were developed to achieve lower emissions without using water or steam to reduce combustion temperature (Wet Low Emission (WLE) technology).\nWLE technology demands cleaning of large amounts of water, is heavy, takes more space and can be difficult to install offshore.\nA DLE combustor uses the principle of lean premixed combustion, and is similar to the SAC combustor with some exceptions.\nA DLE combustor takes up more space than a SAC turbine and if the turbine is changed it can not be connected directly to existing equipment without considerable changes in the positioning of the equipment.\nThe SAC turbine has one single concentric ring where the DLE turbine has two or three rings with premixers depending on gas turbine type.\nDLE technology demands an advanced control system with a large number of burners.\nDLE results in lower NOx emissions because the process is run with less fuel and air, the temperature is lower and combustion takes place at a lower temperature.\n\nIncreased focus on environmental issues led to increased research on new and better gas turbines with water and steam cooling methods in the middle of the 1970s.\nThe best technology could in 1980 reduce NOx emissions to 42 ppm and this was later reduced to 25 ppm.\nIn the 1990s ammonium and catalysators was tested and late in the 1980s the turbine producers started to develop \"Dry Low Emission-technology\" (DLE)\nto be able to get around the technology that demanded water or steam injection.\nDuring the next ten years the DLE technology was developed and installed in many places leading to a reduction of NOx emissions less than 25 ppm.\nIt is difficult to achieve less than 9 ppm NOx emissions with DLE turbines.\nTo achieve a reduction from 25 ppm to 9 ppm more than 6 percent air must pass through the premixer.\nNewer generations of DLE burners have an extra injection leading to better control.\nAdditional systems like \"selective catalytic reduction\" (SCR) are necessary to achieve emissions lower than 2.5 ppm.\nTechnologies using water or steam (Wet Low Emission (WLE) can achieve approximately the same level of NOx emissions (25–42 ppm) when they lower the combustion temperature.\n\nThis is not a complete list, but examples of where and when DLE technology has been implemented or are planned to be implemented.\n\nDLE turbines were introduced offshore in Norway in 1998.\nAll gas turbines installed offshore in Norway after year 2000 which uses only gas as fuel are DLE-turbines.\nThe Kårstø facilities were planned with DLE technology.\nDLE technology is used by Statoil ASA, Hammerfest LNG.\nThe Norwegian Storting has decided that the Gina Krogh platform shall not be equipped with DLE technology due to the fact that all off Utsirahøyden will have electrical power supplies from the mainland.\n\n30 April 2012 a gas turbine generator with DLE technology was opened in Waidhaus in Bavaria in Germany.\n\nIn Alberta in Canada it was in 2003 planned to use DLE technology in the power supplies.\n\nDLE is planned to be used in Australia.\n\nDLE is planned to be use in Egypt from 2006 GE Gas turbines frame 9 and Now 2018 applied with siemens Gas turbine 8000H .\n\nThe Malampaya field in the Republic of the Philippines was awarded in August 1998, construction of the topside was begun in June 1999 and completed on 28 March. 2001.\nThe topside was equipped with the worlds first RB211 with DLE technology.\n"}
{"id": "23418953", "url": "https://en.wikipedia.org/wiki?curid=23418953", "title": "Du Noüy ring method", "text": "Du Noüy ring method\n\nThe du Noüy ring method is a technique for measuring the surface tension of a liquid. The method involves slowly lifting a ring, often made of platinum, from the surface of a liquid. The force, formula_1, required to raise the ring from the liquid's surface is measured and related to the liquid's surface tension, formula_2:\n\nformula_3\n\nwhere formula_4 is the radius of the inner ring of the liquid film pulled and formula_5 is the radius of the outer ring of the liquid film. formula_6 is the weight of the ring minus the buoyant force due to the part of the ring below the liquid surface.\n\nWhen the ring's thickness is much smaller than its diameter, this equation can be simplified to:\n\nformula_7\n\nwhere R is the average of the inner and outer radius of the ring.\n\nThis technique was proposed by the French physicist Pierre Lecomte du Noüy (1883–1947) in a paper published in 1925.\n\nThe measurement is performed with a force tensiometer, which typically uses an electrobalance to measure the excess force caused by the liquid being pulled up and automatically calculates and displays the surface tension corresponding to the force. Earlier, torsion wire balances were commonly used. The maximum force is used for the calculations, and empirically determined correction factors are required to remove the effect caused by the finite diameter of the ring:\n\nformula_8\n\nwith f being the correction factor.\n\nThe most common correction factors include those by Zuidema and Waters (for liquids with low interfacial tension), Huh and Mason (which covers a wider range than Zuidema-Waters), and Harkins and Jordan (more precise than Huh-Mason while still covering the most widely used liquids).\n\n\n"}
{"id": "1249856", "url": "https://en.wikipedia.org/wiki?curid=1249856", "title": "Dual piping", "text": "Dual piping\n\nDual piping is a system of plumbing installations used to supply both potable and reclaimed water to a home or business. Under this system, two completely separate water piping systems are used to deliver water to the user. This system prevents mixing of the two water supplies, which is undesirable, since reclaimed water is usually not intended for human consumption.\n\nIn the United States, reclaimed water is distributed in lavender (light purple) pipes, to alert users that the pipes contain non-potable water. Hong Kong has used a dual piping system for toilet flushing with sea water since the 1950s.\n\nAccording to the El Dorado Irrigation District in California, the average dual-piped home used approximately of potable water in 2006. The average single family residence with traditional piping using potable water for irrigation as well as for domestic uses used between , higher elevation, and , lower elevation.\n\n"}
{"id": "3775165", "url": "https://en.wikipedia.org/wiki?curid=3775165", "title": "Eidoloscope", "text": "Eidoloscope\n\nThe Eidoloscope was an early motion picture system created by Eugene Augustin Lauste, Woodville Latham and his two sons through their business, the Lambda Company, in New York City in 1894 and 1895. The Eidoloscope was demonstrated for members of the press on April 21, 1895 and opened to the paying public on Broadway on May 20.\n\nOriginally called the Pantoptikon (also spelled 'Panoptikon'), it is perhaps the first widescreen film format, with an aspect ratio of 1.85. It had a film gauge of 51 mm and an aperture of 37 mm by 20 mm. It was instrumental in the history of film in that it created what became known as the \"Latham loop\", which are two loops of film, one on each side of the intermittent movement, which act as a buffer between continuously moving sprockets and the jerky motion of the intermittent movement. This relieved strain on the filmstrip and so enabled the shooting and projection of much longer motion pictures than had previously been possible.\n\nWoodville Latham, one of the creators of the eidoloscope, was originally a chemistry professor. Woodville's sons were in the business of showing boxing matches and would frequently hear complaints from patrons about how someone should make a machine that projects film on a screen. That way, more people could view the film at the same time, unlike the kinetoscope. It was a much more efficient method of exhibition that would reduce start-up costs, since each parlor would need only one machine instead of six. W.K.L Dickson, an employee of Edison's at the same time, joined the Latham's and their project to help raise finances and the knowledge of how to move forward in the business. The Eidoloscope was engineered mainly by Lauste, who also assisted with the design of the Latham loop. (Later, Dickson would credit Lauste with the loop's invention).\n\nThe Latham's named their company after the Greek letter for \"L\", lambda. The features that the Lathams produced were of poor quality and lacked the tilts and pans that other features were beginning to utilize. Unfortunately, the company did not last long since disputes over the copyrights from The Eidoloscope Company shareholders brought the Lathams' demise in 1896.\n\n\n\"Early Motion-Picture Companies.\" The Emergence of Cinema: The American Cinema to 1907. Charles Musser. Ed. Charles Harpole. Vol. 1. New York: Charles Scribner's Sons, 1990. [133]-157. History of the American Cinema 1.\n\n\"Projecting Motion Pictures: Invention and Innovation.\" The Emergence of Cinema: The American Cinema to 1907. Charles Musser. Ed. Charles Harpole. Vol. 1. New York: Charles Scribner's Sons, 1990. [91]-105. History of the American Cinema 1.\n"}
{"id": "17351381", "url": "https://en.wikipedia.org/wiki?curid=17351381", "title": "Elizabethan and Jacobean furniture", "text": "Elizabethan and Jacobean furniture\n\nElizabethan furniture is the form which the Renaissance took in England in furniture and general ornament, and in furniture it is as distinctive a form as its French and Italian counterparts.\n\nFor many years Gothic architecture had been moving toward the low lines of the Tudor style, somewhat impelled by the widespread effects of the Italian trecento. Yet the physical and mental insularity of England made absolute change a very slow process, and it was not entirely achieved during the reign of Elizabeth I. Thus instead of the exquisite lightness of the pointed and ogee arches, an arch from the time of Henry VIII barely lifts itself above the level of a straight lintel, under square spandrels.\n\nThe effects of the Renaissance spread slowly to England, although the Artists of the Tudor court included many immigrants from more advanced milieus. Pietro Torrigiano, Holbein and others were in touch with the latest movements on the Continent.\n\nLong after that Shakespeare finds occasion to speak of\n\nKing Hal himself having had a taste for novelty and splendor that leaned kindly to foreign fashions, and the pageantry of the era of James I, that \"wisest fool in Europe,\" not having wrought immediate effect with the quips and conceits through which eventually the Elizabethan degenerated into the Jacobean.\n\nAnd if the movement was tardy even then, it was still slower in the previous Tudor era — that three-quarters of a century just preceding the precise Elizabethan. In spite of a few articles of Renaissance furniture procured abroad for the royal family or some of the high nobility, a barbarous mixture of the old and new yet prevailed in England at the period when France enjoyed the accomplished Henry II style, and when Italy reveled in the perfect fantasies of the Italian cinquecento.\n\nThe term Elizabethan has been used distinctively in relation to the Renaissance, rather than exactly in relation to the English styles; for it really began some years before Elizabeth was born and extended over some years after she died, only then receiving its full development. It is not quite possible to fix the exact limits of the different variations of any main style, one shade overlapping and blending with another. Thus there are chairs with the exceedingly high and narrow backs and small square seats which are called Elizabethan, but which were in use with much the same ornament for an indefinite previous period, and there are palaces and country seats built in Elizabeth's last days, but decorated with the additional characteristics more particularly belonging to the Jacobean. In the Louvre and old armory the upper portion is pierced in all the Gothic foliations of the Flamboyant, while the lower portion is decorated with panels carved in all the richest caprices of the cinquecento.\n\nAttempts at classicism are everywhere in the Elizabethan. Once in a while in a mantelpiece the attempt is almost a success, and the result an exceedingly stately and beautiful object with channeled columns, architrave, and frieze. But poorly executed work has a few pillars and pilasters with misunderstood details, a strap often clasped and buckled about them, some clumsy scrolls and rosettes, with masks and busts of the ancients, scattered ill-drawn human figures, and here and there huge terms, heads rising from flat vases, or pedestals narrowing at the base.\nGrecian columns of singular disproportion form the main structure of bedsteads, tables, and cabinets. These columns are noted for their clumsy thickness, and in one of the first misapprehensions of the classic that mark the style, they rise from huge spherical clusters of foliage, usually the acanthus. At about half their length, these columns are frequently broken by another huge spherical cluster; on this sometimes half the foliage growing downward, half growing upward, and divided in the middle by a careful strap and buckle; occasionally the upper half of this globe is absent. The lower part of the columns is often covered with arabesques, and the upper half merely fluted, or else covered with a fine imbricate carving. In some of the tables, instead of columns, a sort of caryatid — female half-figure, neither exactly sphinx nor monster, dressed out in straps and ending in rude scrolls — formed the support at each of the four corners.\n\nThe tables thus upheld were mighty constructions, sometimes they can be pulled apart in an extension, but oftener bound by firm crossbars and almost immovable through their weight. In the cabinets the lower part was usually a closed cupboard, paneled and ornamented, with terms between the different divisions, the figure issuing from the vase being now a head only, and now two-thirds of the whole; the top projected, and was upheld by the big columns; and all the surfaces were enriched with sculptures after the approved fashion.\n\nOf the bedsteads with heavy canopies and cornices, the Great Bed of Ware follows the styles, although it is a caricature in size. Sir Toby Belch speaks of this piece of furniture when he advises Sir Andrew Aguecheek: \"And as many lies as will lie in thy sheet of paper, although the sheet were big enough for the Bed of Ware in England, set 'em down; go about it.\" Still, it is to be remembered that its twelve foot square size was not at all unusual, and was matched by other beds on the Continent.\n\nAlthough its curious translation of classic shapes is significant, the strap and buckle predominate over everything else.\n\nStrapwork, together with shieldwork, was very prominent in the Henry II style. It was a method of ornament particularly applicable to jewelry and work in gold. Cellini used it entirely. \"I therefore made four small figures of boys,\" says he, \"with four little grotesques, which completed the ring; and I added to it a few fruits and \"ligatures\" in enamel, so that the jewel and the ring appeared admirably suited to each other.\" Both in the French and the Italian work the method was mingled with better classic detail, and with finer natural imitation, but hardly in the Saracenic itself was the tracery so prominent as in the Elizabethan. If the type was meager, its play of line was infinite: curve led to curve, intricacy to intricacy, and over all ornamented surfaces, the scrolls that supported other forms — panels or scutcheons or masks — the figures, the faceted jewel forms, opened into successions and sequences of interlacing and escaping straps and ribbons, and transformed into the representation of all the gay buckling and harnessing of chivalry.\n\nThese ribbons and straps and buckles were always flat in surface, however curved in shape and situation, and they rose from their background at right angles as actual straps would if laid on flatly, not using contrasts of light and shade, but seeking only the effect of line chasing line. When the use of the cartouche became more general, one form of light and shade came to the assistance of this sort of ornament, for the supports of the shield were frequently pierced with countless openings, crescent-shaped, lozenged, circular, rectangular, apparently in a mere haphazard openwork, but revealed in an overall view as repeating the straps and ribbons again with the contours of their perforation. While this pierced shieldwork, with its innumerable flat and curved planes, came afterward to assume more importance in the Jacobean, there was nothing of the Elizabethan that was not ornamented with the strapwork in some form or other.\n\nThe vast screens between the sides of rooms or walls themselves were filled with flourishes of this carven tracery, as seen in Crewe Hall. Even of the ceilings conformed to the carved style. There are few grander effects in interior decoration than the intersecting curves and angles of a lofty old Elizabethan ceiling. Of course, in the use of the strap and shield, heraldry and its escutcheons and crests entered largely into the ornament of the Elizabethan. The ensigns armorial, set in all shapes and surrounded by all the curious mantling to be devised, appeared everywhere in conjunction with the family motto and with the intertwined initials of husband and wife, over gateways, over doorways, on dead-wall, over the fireplace; and stairways were decorated with carved monsters sitting on the baluster-tops and holding before them the family arms, frequently looking as if they had just escaped from one of the quarterings. Even such a room sometimes had stylistic mixtures such as wainscots which were set in the little square panels or in the parchment panels of the preceding reigns, or in the round-arched panels peculiar to the Elizabethan itself — miniature and open representations of which are to be seen on the back of the chair made from the wood of Sir Francis Drake's ship. \n\nNevertheless, in the Elizabethan the Gothic is never quite forgotten. Its vertical lines are always breaking through the horizontal of the invading classic; its reverend monsters look with special unkindness on the fantasticism of the new monsters that Cellini described as the promiscuous breed of animals and flowers; its ornaments insist upon their right before the Grecian; in architecture its gables still rise, although with a skyline gnawed out by the scrolls as worms gnaw out the sides of a leaf; and in furniture its cove surmounts the tops of those cabinets whose fronts are the facades of temples. The steadfast English mind clung to the old order of things, and relinquished with reluctance the last relics of a style that had been for centuries a part of its life. If it must have the egg and dart, it would keep the Tudor flower too. Thus all the Renaissance that came into England, after the bloody Wars of the Roses made it possible to think of art and luxury, paid toll to the Gothic on the way, and the result was a singular miscellany, for its Gothic had now forgotten, and its Renaissance had never known why it had existed. It is rather the talent with which the medley of material was handled, the broad masses, yet curious elaboration, and the scale of magnificence, that give the style its charm rather than anything in its original and bastard composition.\n\nThe Renaissance of the Elizabethan came into England by way of the Low Countries. The importation of furniture into England from Flanders and Holland was so significant that a hundred years earlier a law was enacted forbidding the practice — nevertheless carved woodwork was one of the important articles of commerce with the Low Countries, and the country homes of England of this period were filled with articles of Dutch and Flemish workmanship.\n\nHistorical influences include:\nWhether from any of these causes or from purely commercial ones, what became part of the Elizabethan furniture style was the top-heavy and overloaded Dutch cabinet and the table with big columnar legs capable of upholding mighty serving dishes, and both covered with Flemish ornament. Many forgeries in the style were made in Holland long afterward due to their high value.\n\nIt is this importation and custom that accounts for something of the character of the Elizabethan articles; for the Flemings, although fond of magnificence, and accustomed to all the splendor of the Burgundian court, never became absolute masters of the fully developed Italian style. Nor was the Fleming so thoroughly the master of his materials that his execution quite answered his ideas. Both German and Spanish workmanship came much nearer to the complete spirit of the Renaissance, the latter leaving little to be desired. The Flemish is, however, generally held to be the most dramatic carving of the North. Although the French handled the human figure lightly and fancifully their drawing was apt to be incorrect, such as in giving too much weight and size to the head. Yet after some years the Flemish work became less dignified and desirable. It was lumbered with turned work sawed in half and glued on, with panels overlaying and intersecting each other at odd angles, and with cumbrous pendants under the corners, all of which work was injurious, and much of which was ugly. In the later period of the Elizabethan, the Italians themselves may have supplied artists and workmen for the furniture, but they must have worked hampered by the tastes and prejudices existing around them. A certain rudeness of carving prevails throughout the earlier part of the style, and is considered to give breadth of effect. The old carvers hid none of the means by which they gained their ends, and left even the tool marks in full sight.\n\nIn that portion of the Elizabethan which is often considered as the Jacobean, although it was but the completer development of the former, the globular excrescences of the columns elongated themselves into equally vast and far uglier acorn-shaped supports. A good deal of inlaid work was then used, and the carving did its best to reach and render the ideas of the cinquecento. It is, indeed, styled the cinquecento period of English art, every surface being rough with arabesques of griffins, vases, rosettas, dolphins, scrolls, foliages, Cupids, and mermaids with double tails curling round them on either side. Meantime the cartouche and its straps — \"ligatures\" they were called in Italy, \"cuirs\" in France and Flanders, were still often used. Scallop shells received a particular share of favor, having been recently brought home from foreign seas, and was immediately seized by the designers in need of other shapes. The Flemings made seats that enclosed the sitter in the valves of this scallop, carved just rudely enough to excuse their eccentricity. Settees were made at this time whose backs consisted of several just such immense scallops as those of these Holland House Gilt Chamber chairs; and the same idea of decoration peeps out in fan-like frills at every spare corner of the Neo-Jacobean revival of the style. These shell forms of furniture might befit an oceanside home, but they must have been singularly out of place on dry land and among the huge and heavy articles that surrounded them in the Jacobean mansions.\nThere was something, on the whole, in the early Elizabethan replete with dignity, a massy magnificence that agreed with that of the era and the monarch, that went well, too, with the mighty farthingales and ruffs of the ladies, the trunk-hose and puffed and banded doublets of the gallants, while the people who used it — Shakespeare, Walter Raleigh, Ben Jonson, Francis Bacon — still have a peculiar interest. Well as it suited doughy old Queen Bess herself, the forms which it took under her successor, with their assumption of foreign conceits and their display of profuse gilding, accorded no less characteristically with the arrogant, pedantic, and petty James. All of this furniture, however, is exceedingly attractive, and there are few who would not rejoice over any article of it which is not too unwieldy for modern quarters. A typical sideboard and dresser offer a medley of design, with not too well drawn fawns and satyrs, fruits and flowers, Cupids, birds, scrolls, shields and straps, cornucopias, mermaids, monsters and foliages. They belong to the beginning of the later period. It was no light matter to clear the floor for the dance of the Capulets when the servant cried \"Away with the joint-stools, remove the court-cupboard, look to the plate!\".\n\nBy the close of the Jacobean era, the style held its own with slight variation and innovation, for some reigns. The execution of the carving was coarse and careless during the time of the first Stuarts, but afterward rose to be classed with the finest known; inlaid work, also, was more freely used and attained much excellence. There was increasing prevalent luxury in every thing. Fine pottery, for instance, became more frequent; for although glass had been made in London under Elizabeth's patronage, \"porselyn\" was rare, and even earthenware was not then very general, gold and silver plate making the vessels of the rich, and pewter mugs and platters and wooden trenchers being still those of the poor, while mention is made of \"five dishes of earth painted, such as are brought from Venice,\" which were presented to the queen as something unusual; and it was thought a gift not unworthy of royalty when Lord Burleigh offered her a \"porringer of white porselyn garnished with gold.\" The first use of the famous Dutch tiles is thought to belong to the reign of Charles I.\nMirrors, which were very rare in Elizabeth's time, became more common in that of the Charleses, the Duke of Buckingham, during the reign of the second Charles, bringing a colony of Venetian glassmakers to Lambeth. One Elizabethan mirror is some three and a half by four and a half feet in size — five feet was the largest made till the latter part of the eighteenth century — the frame is carved in oak and partially gilt, and the glass is set flatly. In one mirror of the time of Charles II the glass is beveled, and in the glasses of the Merry Monarch's predecessor the frames were so made as to throw the glass forward and give it projection. Quicksilvered glass itself, unset, became a novelty, so that sometimes whole rooms, and even the ceilings, were lined with it. The mirrors made by the duke's colony were of superior excellence; they had an inch-wide bevel all along their outer extremity, whether they were rectangular or curved. \"This,\" says Mr. Pollen, \"gives preciousness and prismatic light to the whole glass. It is of great difficulty in execution, the plate being held by the workman over his head, and the edge cut by grinding. The feats of skill in this kind, in the form of interrupted curves and short lines and angles, are rarely accomplished by modern workmen, and the angle of the bevel itself is generally too acute, whereby the prismatic light produced by this portion of the mirror is in violent and too showy contrast to the remainder.\"\n\nWall hangings had been long in use — the leather, the damask, velvent, and arras or tapestry. The Flemish tapestries, from the time of their first manufacture, were in great favor. Elizabeth had a set wrought signalizing the dispersion and destruction of the Spanish Armada. So fine had they become that they were often preferred to other decoration, and in the Stuart time were stretched across the noble old carved panelwork itself. \"Here I saw the new fabric of French tapestry,\" wrote Evelyn, in the last years of Charles II, concerning the Gobelins tapestry, established under the royal patronage in France: \"for design, tenderness of work, and incomparable imitation of the best paintings, beyond any thing I had ever beheld. Some pieces had Versailles, St. Germains, and other palaces of the French king, with huntings, figures, and landscapes, exotic fowls, and all to the life rarely done.\" Yet works in tapestry had been, long before this, under royal protection in England also, the Raphael cartoons having been purchased by Charles I for the use of the establishment at Mortlake, which, however, did not outlast that sovereign more than half a century; and the employment of draperies had become so profuse that they now largely took the place of the heavy paneled wooden tops which had so long encumbered the bedsteads.\n\"This article is text adapted from an article in Harper's Magazine from 1877-78\"\n\n"}
{"id": "55796829", "url": "https://en.wikipedia.org/wiki?curid=55796829", "title": "Exfoliation corrosion (metallurgy)", "text": "Exfoliation corrosion (metallurgy)\n\nIn metallurgy, exfoliation corrosion (also called lamellar corrosion) is a severe type of intergranular corrosion that raises surface grains from metal by forming corrosion products at grain boundaries under the surface. It is frequently found on extruded sections where grain thickness is not as thick as the rolled grain. It can affect aircraft structures , marine vessels , heaters and other objects.\n"}
{"id": "1443370", "url": "https://en.wikipedia.org/wiki?curid=1443370", "title": "Ferrocement", "text": "Ferrocement\n\nFerrocement or ferro-cement (also called thin-shell concrete or ferro-concrete) is a system of reinforced mortar or plaster (lime or cement, sand and water) applied over layer of metal mesh, woven expanded-metal or metal-fibers and closely spaced thin steel rods such as rebar. The metal commonly used is iron or some type of steel. It is used to construct relatively thin, hard, strong surfaces and structures in many shapes such as hulls for boats, shell roofs, and water tanks. Ferrocement originated in the 1840s in France and is the origin of reinforced concrete. It has a wide range of other uses including sculpture and prefabricated building components. The term \"ferrocement\" has been applied by extension to other composite materials, including some containing no cement and no ferrous material.\n\n\"Cement\" and \"concrete\" are used interchangeably but there are technical distinctions and the meaning of \"cement\" has changed since the mid-nineteenth century when \"ferrocement\" originated. Ferro- means iron although metal commonly used in ferro-cement is the iron alloy steel. Cement in the nineteenth century and earlier meant \"mortar\" or broken stone or tile mixed with lime and water to form a strong mortar. Today cement usually means Portland cement, Mortar is a paste of a binder (usually Portland cement), sand and water; and concrete is a fluid mixture of Portland cement, sand, water and crushed stone aggregate which is poured into formwork (shuttering). \"Ferro-concrete\" is the original name of reinforced concrete (armored concrete) known at least since the 1890s and in 1903 it was well described in London's Society of Engineer's \"Journal\" but is now widely confused with ferrocement.\n\nThe inventors of ferrocement are Frenchmen Joseph Monier who dubbed it \"ciment armé\" (armored cement) and Joseph-Louis Lambot who constructed a batteau with the system in 1848. Lambot exhibited the vessel at the Exposition Universelle in 1855 and his name for the material \"ferciment\" stuck. Lambot patented his batteau in 1855 but the patent was granted in Belgium and only applied to that country. At the time of Monier's first patent, July 1867, he planned to use his material to create urns, planters, and cisterns. These implements were traditionally made from ceramics, but large-scale, kiln-fired projects were expensive and prone to failure. In 1875, Monier expanded his patents to include bridges and designed his first steel-and-concrete bridge. The outer layer was sculpted to mimic rustic logs and timbers, thereby also ushering Faux Bois (wood grain) concrete. In the first half of the twentieth century Italian Pier Luigi Nervi was noted for his use of ferro-cement, in Italian called \"ferro-cemento\".\n\n\"ferrocement\" being referred to as ferro-concrete or reinforced concrete to better describe the end product instead of its components.\n\nFerro concrete has relatively good strength and resistance to impact. When used in house construction in developing countries, it can provide better resistance to fire, earthquake, and corrosion than traditional materials, such as wood, adobe and stone masonry. It has been popular in developed countries for yacht building because the technique can be learned relatively quickly, allowing people to cut costs by supplying their own labor. In the 1930s through 1950's, it became popular in the United States as a construction and sculpting method for novelty architecture, examples of which created \"dinosaurs in the desert\".\n\nThe desired shape may be built from a multi-layered construction of mesh, supported by an armature, or grid, built with rebar and tied with wire. For optimum performance, steel should be rust-treated, (galvanized) or stainless steel. (In early practice, in the desert, or for exterior scenery construction, \"sound building practice\" was not considered, or perhaps unknown as it grew in some cases, from a folk craft tradition of masons collaborating with blacksmiths.) Over this finished framework, an appropriate mixture (grout or mortar) of Portland cement, sand and water and/or admixtures is applied to penetrate the mesh. During hardening, the assembly may be kept moist, to ensure that the concrete is able to set and harden slowly and to avoid developing cracks that can weaken the system. Steps should be taken to avoid trapped air in the internal structure during the wet stage of construction as this can also create cracks that will form as it dries. Trapped air will leave voids that allow water to collect and degrade (rust) the steel. Modern practice often includes spraying the mixture at pressure (a technique called shotcrete) or some other method of driving out trapped air.\n\nOlder structures that have failed offer clues to better practices. In addition to eliminating air where it contacts steel, modern concrete additives may include acrylic liquid \"admixtures\" to slow moisture absorption and increase shock resistance to the hardened product or to alter curing rates. These technologies, borrowed from the commercial tile installation trade, have greatly aided in the restoration of these structures. Chopped glass or poly fiber can be added to reduce crack development in the outer skin. (Chopped fiber could inhibit good penetration of the grout to steel mesh constructions. This should be taken into consideration and mitigated, or limited to use on outer subsequent layers. Chopped fibers may also alter or limit some wet sculpting techniques.)\n\nThe economic advantage of ferro concrete structures is that they are stronger and more durable than some traditional building methods. Depending on the quality of construction and the climate of its location, houses may pay for themselves with almost zero maintenance and lower insurance requirements. Water tanks could pay for themselves by not needing periodic replacement, if properly constructed of reinforced concrete.\n\nFerro concrete structures can be built quickly, which can have economic advantages. In inclement weather conditions, the ability to quickly erect and enclose the building allows workers to shelter within and continue interior finishing.\n\nIn India, ferro concrete is used often because the constructions made from it are more resistant to earthquakes. Earthquake resistance is dependent on good construction technique and additional reinforcement of the concrete.\n\nIn the 1970s, designers adapted their yacht designs to the then very popular backyard building scheme of building a boat using ferrocement. Its big attraction was that for minimum outlay and costs, a reasonable application of skill, an amateur could construct a smooth, strong and substantial yacht hull. A ferro-cement hull can prove to be of similar or lower weight than a fiber reinforced plastic (fiberglass), aluminum, or steel hull. New methods of laminating layers of cement and steel mesh in a mold may bring new life to ferro-cement boat-building. A thorough examination of reinforced concrete and current practice would benefit the boat builder. An example of a well known ferro-cement boat is \"Hardiesse\", the Falmouth sail-training ship.\n\nThere are basically three types of methods of ferrocement. They are following\n\n\nThe advantages of a well built ferro concrete construction are the low weight, maintenance costs and long lifetime in comparison with purely steel constructions. However, meticulous building precision is considered crucial here. Especially with respect to the cementitious composition and the way in which it is applied in and on the framework, and how or if the framework has been treated to resist corrosion.\n\nWhen a ferro concrete sheet is mechanically overloaded, it will tend to fold instead of break or crumble like stone or pottery. As a container, it may fail and leak but possibly hold together. Much depends on techniques used in the construction.\n\nThe disadvantage of ferro concrete construction is the labor-intensive nature of it, which makes it expensive for industrial application in the western world. In addition, threats to degradation (rust) of the steel components is a possibility if air voids are left in the original construction, due to too dry a mixture of the concrete being applied, or not forcing the air out of the structure while it is in its wet stage of construction, through vibration, pressurized spraying techniques, or other means. These air voids can turn to pools of water as the cured material absorbs moisture. If the voids occur where there is untreated steel, the steel will rust and expand, causing the system to fail.\n\nIn modern practice, the advent of liquid acrylic additives and other advances to the grout mixture, create slower moisture absorption over the older formulas, and also increase bonding strength to mitigate these failures. Restoration steps should include treatment to the steel to arrest rust, using practices for treating old steel common in auto body repair.\n\n\n"}
{"id": "1636646", "url": "https://en.wikipedia.org/wiki?curid=1636646", "title": "Food technology", "text": "Food technology\n\nFood technology is a branch of food science that deals with the production processes that make foods.\n\nEarly scientific research into food technology concentrated on food preservation. Nicolas Appert’s development in 1810 of the canning process was a decisive event. The process wasn’t called canning then and Appert did not really know the principle on which his process worked, but canning has had a major impact on food preservation techniques.\n\nLouis Pasteur's research on the spoilage of wine and his description of how to avoid spoilage in 1864 was an early attempt to apply scientific knowledge to food handling. Besides research into wine spoilage, Pasteur researched the production of alcohol, vinegar, wines and beer, and the souring of milk. He developed pasteurization—the process of heating milk and milk products to destroy food spoilage and disease-producing organisms. In his research into food technology, Pasteur became the pioneer into bacteriology and of modern preventive medicine.\n\nDevelopments in food technology have contributed greatly to the food supply and have changed our world. Some of these developments are: \n\nIn the past, consumer attitude towards food technologies was not common talk and was not important in food development. Nowadays the food chain is long and complicated, foods and food technologies are diverse; consequently the consumers are uncertain about the food quality and safety and find it difficult to orient themselves to the subject. \nThat is why consumer acceptance of food technologies is an important question. However, in these days acceptance of food products very often depends on potential benefits and risks associated with the food. This also includes the technology the food is processed with. Attributes like \"uncertain\", \"unknown\" or \"unfamiliar\" are associated with consumers’ risk perception and consumer very likely will reject products linked to these attributes. Especially innovative food processing technologies are connected to these characteristics and are perceived as risky by consumers\nAcceptance of the different food technologies is very different. Whereas pasteurisation is well recognised, high pressure treatment or even microwaves are perceived as risky very often. In studies done within Hightech Europe project, it was found that traditional technologies were well accepted in contrast to innovative technologies.\n\nConsumers form their attitude towards innovative food technologies by three main factors mechanisms. First, knowledge or beliefs about risks and benefits which are correlated with the technology. Second, attitudes are based on their own experience and third, based on higher order values and beliefs.\nA number of scholars consider the risk-benefit trade-off as one of the main determinants of consumer acceptance, although some researchers place more emphasis on the role of benefit perception (rather than risk) in consumer acceptance.\n\nRogers (2010) defines five major criteria that explain differences in the acceptability or adoption of new technology by consumers: complexity, compatibility, relative advantage, trialability and observability.\n\nAcceptance of innovative technologies can be improved by providing non-emotional and concise information about these new technological processes methods. According to a study made by HighTech project also written information seems to have higher impact than audio-visual information on the consumer in case of sensory acceptance of products processed with innovative food technologies.\n\n\n"}
{"id": "12800909", "url": "https://en.wikipedia.org/wiki?curid=12800909", "title": "Gross merchandise volume", "text": "Gross merchandise volume\n\nGross Merchandise Volume (alternatively Gross Merchandise Value or GMV) is a term used in online retailing to indicate a total sales dollar value for merchandise sold through a particular marketplace over a certain time frame. Site revenue comes from fees and is different from the dollar value of items sold.\n\nGMV for e-commerce retail companies means sale price charged to the customer multiplied by the number of items sold. For example, if a company sells 10 books at $100, the GMV is $1,000. This is also considered as \"gross revenue\". In this case, the business model is based on a retail model, where the company basically purchases the items, maintains inventory (if need be) and finally, sells or delivers the items to customers. It does not tell the net sales as GMV does not include discounts, costs involved and returns of products.\n"}
{"id": "51408572", "url": "https://en.wikipedia.org/wiki?curid=51408572", "title": "HeiQ Materials AG", "text": "HeiQ Materials AG\n\nHeiQ Materials AG (German pronunciation: [ˈhaɪkju]) is a Swiss specialty chemistry company, headquartered in Zurich, Switzerland. It was founded in 2005 as a spin-off of Swiss Federal Institute of Technology Zurich (ETH).\n\nHeiQ produces and sells textile finishing and other auxiliaries. But its core business activity is to conduct co-joint research and development projects with consumer textile products brands or textile producers for textile finishing to achieve effects that are currently not in market or not optimized to certain products.\n\nFor the Deepwater Horizon oil spill that began in April 2010 in the Gulf of Mexico, HeiQ, TWE Group and Beyond Surface Technologies jointly developed an oil-absorbing, water-repelling, nonwoven fabric, in the name Oilguard for oil relief efforts. The product was intended for beach protection against oil spills and was applied to the shoreline. This allows for the fabric to absorb crude oil while repelling the seawater. This innovation was rewarded the Swiss Technology Award (2010) and the Swiss Award (2013). The water-repelling property of Oilguard is achieved with a textile finishing that creates the Lotus Effect on the surface of the non-woven fabric. The non-fluorinated finishing makes the fabric only water repellent but not oil repellent, therefore the fabric absorbs oil crudes but not the seawater.\n\nThe company also has three subsidiaries in the United States, Hong Kong and Australia, known as HeiQ ChemTex, HeiQ Ltd. and HeiQ Australia Pty. Ltd. respectively.\n"}
{"id": "169548", "url": "https://en.wikipedia.org/wiki?curid=169548", "title": "Ingenuity", "text": "Ingenuity\n\nIngenuity is the quality of being clever, original, and inventive, often in the process of applying ideas to solve problems or meet challenges.\n\nIngenuity (Ingenium) is the root Latin word for engineering. For example, the process of figuring out how to cross a mountain stream using a fallen log, building an airplane model from a sheet of paper, or starting a new company in a foreign culture all involve the exercising of ingenuity. Human ingenuity has led to various technological developments through applied science, and can also be seen in the development of new social organizations, institutions, and relationships. Ingenuity involves the most complex human thought processes, bringing together our thinking and acting both individually and collectively to take advantage of opportunities and/or overcome problems.\n\nOne example of how ingenuity is used conceptually can be found in the analysis of Thomas Homer-Dixon, building on that of Paul Romer, to refer to what is usually called \"instructional capital\". In this case, Homer-Dixon used the phrase 'ingenuity gap' denotes the space between a challenge and its solution. His particular contribution is to explore the social dimensions of ingenuity. Typically we think of ingenuity being used to build faster computers or more advanced medical treatments.\n\nHomer-Dixon argues that as the complexity of the world increases, our ability to solve the problems we face is becoming critical. Human ingenuity is also included in many school systems, with most teachers encouraging students to be educated in human ingenuity.\n\nThese challenges require more than improvements arising from physics, chemistry and biology, as one will need to consider the highly complex interactions of individuals, institutions, cultures, and networks involving all of the human family around the globe. Organizing ourselves differently, communicating and making decisions in new ways, are examples of social ingenuity. If one's ability to generate adequate solutions to these problems is inadequate, the ingenuity gap will lead to a wide range of social problems. The full exploration of these ideas in meeting social challenges is featured in \"The Ingenuity Gap\", one of Thomas Homer-Dixon's earliest books.\n\nIn another of Homer-Dixon's books, The Up Side of Down, he argues that increasingly expensive oil, driven by scarcity, will lead to great social instability. Walking across an empty room requires very little ingenuity. If the room is full of snakes, hungry bears, and land mines, the ingenuity requirement will have gone up considerably.\n\n is often inherent in creative individuals, and thus is considered hard to separate from individual capital.\n\nIt is not clear though if Dixon or Romer considered it impossible to do so, or if they were simply not familiar with the prior analysis of \"applied ideas\", \"intellectual capital\", \"talent\", or \"innovation\" where instructional and individual contributions have been carefully separated, by economic theorists.\n\n"}
{"id": "1247612", "url": "https://en.wikipedia.org/wiki?curid=1247612", "title": "Intelligent Machines Research Corporation", "text": "Intelligent Machines Research Corporation\n\nIntelligent Machines Research Corporation (IMR) was founded by David H. Shepard and William Lawless, Jr. in 1952 to commercialize the work Shepard had done with the help of Harvey Cook in building \"Gismo\", a machine later called the \"Analyzing Reader\".\n\nIBM obtained a license on IMR's patents in 1953 and in 1955 contracted with IMR to build a developmental system which was able to read constrained hand printed numeric characters if reasonably well formed. However, IBM did not market this system. In 1959 IBM did market a system of its own, classifying it as an Optical Character Recognition (OCR) system, and the term OCR from then on has been standard in the industry for this technology.\n\nIMR went on to deliver the world's first several commercially used systems, including one used by Readers Digest in its book subscription department. Readers Digest donated this system many years later to the Smithsonian, where it was once put on display. The second system was sold to the Standard Oil Company of California, as arranged by the Farrington Manufacturing Company, a leading company in the credit card business at that time, with many systems to read oil company credit cards to follow, one of which was also on display at the Smithsonian later on.\n\nIn 1959 Farrington acquired IMR, and the numeric font designed by Shepard, called Farrington 7B, has been standard for most of the well known credit cards since that time. Shepard later left Farrington and founded Cognitronics Corporation in 1962.\n\nBoth Shepard and Lawless had been NSA employees at one time. Lawless later held key positions in IBM.\n"}
{"id": "24813120", "url": "https://en.wikipedia.org/wiki?curid=24813120", "title": "Linear compressor", "text": "Linear compressor\n\nA linear compressor is a gas compressor where the piston moves along a linear track to minimize friction and reduce energy loss during conversion of motion. This technology has been successfully used in cryogenic applications which must be oilless.\n\nA number of patents for linear compressors powered by free-piston engines were issued in the 20th century, including:\n\nThe first market introduction of a linear compressor to compress refrigerant in a refrigerator was in 2001.\n\nThe single piston linear compressor uses dynamic counterbalancing, where an auxiliary movable mass is flexibly attached to a movable piston assembly and to the stationary compressor casing using auxiliary mechanical springs with zero vibration export at minimum electrical power and current consumed by the motor. It is used in cryogenics.\n\nLinear compressors are used in LG and Kenmore refrigerators. Compressors of this type have less noise, and are more energy efficient than conventional refrigerator compressors.\n\n"}
{"id": "35593767", "url": "https://en.wikipedia.org/wiki?curid=35593767", "title": "Local Government ICT Network", "text": "Local Government ICT Network\n\nThe Local Government ICT Network, also known as LG ICT Network, is the knowledge sharing portal for information and communication technologies (ICT) in local government in South Africa. The Network is hosted by the South African Local Government Association (SALGA) and supported by the \"Deutsche Gesellschaft für Internationale Zusammenarbeit\" (German Agency for International Cooperation). It has been publicly available since May 2011 as a dedicated online community for ICT practitioners in public service delivery and has members from almost all 278 municipalities of South Africa.\n\nIts objectives include the sharing of interesting documents, the identification and promotion of best practices, the provision of regular updates about important events, jobs and news. The LG ICT Network is intended to serve as a space for free discussion and exchange.\n\nIn August 2011 a 2-day live Event ConnectIT was held in Birchwood Convention Centre, Johannesburg with over 400 participants and participation from the private and public sector.\nIn 2012 the Network presented its work among other occasion at the Public Participation Conference of the Gauteng Legislature and at the Tech Demo Africa 2012.\n\n"}
{"id": "20097187", "url": "https://en.wikipedia.org/wiki?curid=20097187", "title": "M-Module", "text": "M-Module\n\nM-Modules are a mezzanine standard mainly used in industrial computers. Being mezzanines, they are always plugged on a carrier PCB that supports this format. The modules communicate with their carrier over a dedicated bus, and can have all kinds of special functions.\n\nM-Modules are standardized as ANSI/VITA 12-1996 and are especially suited for adding any kind of real-world I/O to a system in a flexible way. They are modular I/O extensions for all types of industrial computers, from embedded systems up to high-end workstations. The M-Module Interface - a fast asynchronous parallel interface - offers sophisticated functions like 32-bit data bus, burst transfers up to 100 MB/s, DMA and trigger capabilities. M-Modules also offer direct front-panel connection rather than requiring a separate adapter panel with ribbon-cable connections. This provides a clean path for sensitive signals without loss of data or signal quality - using, for example, shielded D-Sub connectors and coaxial cables.\n\nThe mezzanine approach to placing multiple functions in a single card slot has been around for a long time both in proprietary and open standard forms. Valid arguments can be put forth for both of these approaches. The M-Module is one open standard that is gaining increasing popularity for applications in the fields of analog and digital I/O, instrumentation, robotics, motion functions and fieldbuses. This standard was originally developed in Germany by MEN Mikro Elektronik for VMEbus applications and was soon expanded to support the CompactPCI bus as well. It has been embraced as ANSI/VITA 12-1996. \n\nIn addition to the single wide form shown, M-Modules can be developed in double, triple and quadruple wide configurations. Because of the standard's genesis in the VME world it is sized such that 4 fit in a 6U module and 2 in a 3U module. Conveniently, because of the way other backplane standards have evolved, 4 units easily fit the front panel space in VXI and 6U cPCI/PXI while 2 will fit in the front panel space of 3U cPCI/PXI and up to 8 will fit in a 1U LXI rack mount carrier.\n\nAt the present time a number of instruments are available in the M-Module form factor in the following categories:\n\n\nA significant advantage to the M-Module is that it has a relatively straight forward set of electrical and mechanical specifications. This enables an engineer to design a function that might be required without having to become an expert on VXI, PXI or LXI as carriers are available to allow the design to be ported to the backplane or bus of the test system in use.\n\nAs with any mezzanine card, a means must be provided through which the card may be adapted to a backplane or higher level interface. Such a device is generally referred to as a carrier. These come in two types: non-intelligent and intelligent. The functions performed by the former include the simpler functions such as mounting and providing power as well as the more complex such as providing translation between bus types, protocols, routing of triggers and interrupts and making each mezzanine appear as a separate instrument to the host backplane. Intelligent carriers will generally perform all of the functions of the non-intelligent plus perform pre or post processing of data, allow the combination of multiple instruments into composite instruments that then may be controlled at a higher level, and perform translation of commands from older instruments so as to facilitate replacement of Legacy instruments.\n\nOf equal or greater importance in the support of the mezzanine is the software. The majority of the M-Module instrument types referenced above come with VXI/PXI Plug-and-Play or IVI drivers. However, a number of the more control oriented M-Modules are supported only with C drivers. Actions are underway that are described below which allow application of the Plug-and-Play drivers across multiple platforms.\n\nPerhaps the greatest advantage of an M-Module mezzanine instrument is the ability of both the vendor and the user to become \"Platform Agnostic\". From the vendor's perspective, it is only necessary to develop one instrument, say a Pulse Generator, and with the use of carriers he can produce the same product into VXI, PXI, VME, LXI and other applications. This greatly reduces development costs when compared to the development of pulse generators for multiple buses.\n\n"}
{"id": "17591639", "url": "https://en.wikipedia.org/wiki?curid=17591639", "title": "Olidata", "text": "Olidata\n\nOlidata is an Italian computer system manufacturer. The company was founded in Cesena, Italy in 1982 by Carlo Rossi and Adolfo Savini as a limited liability company (LLC). Olidata specializes in software development. The company's accounting software and administrative software divisions were eventually sold to Olivetti. \n\nOlidata is one of the largest manufacturers of computer hardware in Italy.\n\nThe company also manufactures LCD televisions. In April 2008, Olidata announced the production of its JumPc, a modified version of Intel's Classmate PC.\n\nIn 2009, Acer acquired 29.9% of Olidata.\n\n\n"}
{"id": "3089261", "url": "https://en.wikipedia.org/wiki?curid=3089261", "title": "Operation CHAOS", "text": "Operation CHAOS\n\nOperation CHAOS or Operation MHCHAOS was the code name (CIA cryptonym) of a United States Central Intelligence Agency domestic espionage project targeting the American people from 1967 to 1974, established by President Johnson and expanded under President Nixon, whose mission was to uncover possible foreign influence on domestic race, anti-war and other protest movements. The operation was launched under Director of Central Intelligence (DCI) Richard Helms by chief of counter-intelligence James Jesus Angleton, and headed by Richard Ober. The \"MH\" designation is to signify the program had a worldwide area of operations.\n\nThe CIA began domestic recruiting operations in 1959 in the process of finding Cuban exiles who could be used in the campaign against communist Cuba and President Fidel Castro. As these operations expanded, the CIA formed a Domestic Operations Division in 1964. In 1965, US President Lyndon Johnson requested that the CIA begin its own investigation into domestic dissent—independent of the FBI's ongoing COINTELPRO.\n\nThe CIA developed numerous operations targeting American dissents in the US. Many of these programs operated under the CIA's Office of Security, including:\n\nWhen President Nixon came to office in 1969, existing domestic surveillance activities were consolidated into Operation CHAOS. Operation CHAOS first used CIA stations abroad to report on antiwar activities of United States citizens traveling abroad, employing methods such as physical surveillance and electronic eavesdropping, utilizing \"liaison services\" in maintaining such surveillance. The operations were later expanded to include 60 officers. In 1969, following the expansion, the operation began developing its own network of informants for the purposes of infiltrating various foreign antiwar groups located in foreign countries that might have ties to domestic groups. Eventually, CIA officers expanded the program to include other leftist or counter-cultural groups with no discernible connection to Vietnam, such as groups operating within the . The domestic spying of Operation CHAOS also targeted the Israeli embassy, and domestic Jewish groups such as the B'nai B'rith. In order to gather intelligence on the embassy and B'nai B'rith, the CIA purchased a garbage collection company to collect documents that were to be destroyed.\n\nTargets of Operation CHAOS within the antiwar movement included:\n\nAt its finality, Operation CHAOS contained files on 7,200 Americans, and a computer index totaling 300,000 civilians and approximately 1,000 groups. \n\nThe aim of the programs was to compile reports on \"illegal and subversive\" contacts between United States civilian protesters and \"foreign elements\" which \"might range from casual contacts based merely on mutual interest to closely controlled channels for party directives.\" \n\nDCI Richard Helms informed President Johnson on November 15, 1967, that the CIA had uncovered \"no evidence of any contact between the most prominent peace movement leaders and foreign embassies in the U.S. or abroad.\" Helms repeated this assessment in 1969. In total, 6 reports were compiled for the White House and 34 for cabinet level officials.\n\nThe secret program was exposed by investigative journalist Seymour Hersh in a 1974 article in the \"New York Times\" entitled \"Huge CIA Operation Reported in US Against Antiwar Forces, Other Dissidents in Nixon Years\". Amid the uproar of the Watergate break-in involving two former CIA officers, Operation CHAOS had been closed in 1973. Further details were revealed in 1975 during Representative Bella Abzug's House Subcommittee on Government Information and individual Rights. The government, in response to the revelations, felt pressured enough to launch the Commission on CIA Activities Within the United States (The Rockefeller Commission), led by then Vice President Nelson Rockefeller, to investigate the depth of the surveillance. Richard Cheney, then Deputy White House Chief of Staff, is noted as having stated the Rockefeller Commission was to avoid \"... congressional efforts to further encroach on the executive branch.\"\n\nFollowing the revelations by the Rockefeller Commission, then-DCI George H. W. Bush admitted that \"the operation in practice resulted in some improper accumulation of material on legitimate domestic activities.\"\n\n\n"}
{"id": "1136493", "url": "https://en.wikipedia.org/wiki?curid=1136493", "title": "Pipe cleaner", "text": "Pipe cleaner\n\nA pipe cleaner or chenille stem is a type of brush originally intended for removing moisture and residue from smoking pipes. Besides cleaning pipes, they can be used for any application that calls for cleaning out small bores or tight places. Special pipe cleaners are manufactured specifically for cleaning out medical apparatus and for engineering applications. They are popular for winding around bottle necks to catch drips, bundling things together, colour-coding, and applying paints, oils, solvents, greases, and similar substances. They can also be used like a twist tie.\n\nSmoking pipe cleaners normally use some absorbent material, usually cotton or sometimes viscose. Bristles of stiffer material, normally monofilament nylon or polypropylene are sometimes added to better scrub out what is being cleaned. Microfilament polyester is used in some technical pipe cleaners because polyester wicks liquid away rather than absorbing it as cotton does. Some smoking pipe cleaners are made conical or tapered so that one end is thick and one end thin. The thin end is for cleaning the small bore of the pipe stem and then the thick end for the bowl or the wider part of the stem. When used for cleaning purposes, pipe cleaners are normally discarded after one or two uses.\n\nPipe cleaners are commonly used in arts and crafts projects. \"Craft\" pipe cleaners are usually made with polyester or nylon pile and are often longer and thicker than the \"cleaning\" type, and available in many different colors. Craft pipe cleaners are not very useful for cleaning purposes, because the polyester does not absorb liquids, and the thicker versions may not even fit down the stem of a normal pipe or into the usual hard-to-access area of applications that call for cleaning small bores or tight places.\nIn Japan, crafting with pipe cleaners is known as Mogol art. Its name derived from the Portuguese word Mughal for a style of weaving. Workshops in malls and schools in Japan have been led by Atushi Kitanaka on an effort to support the pipe cleaner industry. Ikuyo Fujita（藤田育代 \"Fujita Ikuyo\"）is a Japanese artist who works primarily in needle felt painting and mogol (pipe cleaner) art. Use of pipe cleaners as an art format where animals are made by twisting pipe cleaners together. They can also be used to create whiskers for an animal mask or nose.\n\nA pipe cleaner is made of two lengths of wire, called the core, twisted together trapping short lengths of fibre between them, called the pile. Pipe cleaners are usually made two at a time, as the inner wires of each pipe cleaner have the yarn wrapped around them, making a coil, the outer wires trap the wraps of yarn, which are then cut, making the tufts. Chenille yarn is made in much the same way, which is why craft pipe cleaners are often called \"chenille stems\". The word chenille comes from French meaning caterpillar. Some pipe cleaner machines are actually converted chenille machines. Some machines produce very long pipe cleaners which are wound onto spools. The spools may be sold as-is or cut to length depending on the intended use. Other machines cut the pipe cleaners to length as they come off the machines. Smoking pipe cleaners are usually 15 – 17 cm (6 – 7 inches) long. Craft ones are often 30 cm (12 inches) and can be up to 50 cm (20 inches).The diameter comes in 4 mm; 6 mm; and 15 mm sizes Jumbo pipe cleaners have a 30 mm diameter with lengths of 18 inches and 6.5 feet.\n"}
{"id": "2564847", "url": "https://en.wikipedia.org/wiki?curid=2564847", "title": "ReserveAmerica", "text": "ReserveAmerica\n\nReserveAmerica, owned by Aspira is a product that provides online campsite reservation and license processing for federal, state, provincial, private and local government park, campground, and conservation agencies in North America. It claims to be the \"#1 Access Point for Outdoor Recreation,\" processing more than 3.5 million camping reservations per year, with more than 150,000 campsites available.\n\nReserveAmerica was founded in 1984 as a software development company specializing in reservations for the local recreation industry. In 1992, the company developed a reservation system for state and federal park systems, which they claim processed the industry's first online reservation in September, 1997. ReserveAmerica has provided reservations services for the National Park Service since 1997. In 2001, they were acquired by InterActiveCorp (IAC), which owns Ask.com, Citysearch and Match.com. \n\nIn June, 2005, the United States Forest Service awarded the company a $97 million contract to provide a single source federal recreation information and reservation service. As of September 2014, the reservation service offers centralized information for more than 100,000 campgrounds, cabins, parks and tours of national sites, historic homes and caves through a single Web-based portal. Besides the Forest Service, participating agencies include the National Park Service, the Bureau of Land Management, the Bureau of Reclamation and the U.S. Army Corps of Engineers. ReserveAmerica also has some state parks contracts.\n\nIn 2009, IAC sold ReserveAmerica to ACTIVE Network. In 2017, the Communities and Sports divisions of ACTIVE Network were acquired by Global Payments and ACTIVE Network's Outdoors division, including ReserveAmerica, became the independent company, Aspira. Headquartered in Dallas, TX with nine offices offices spanning the United States, Canada and Asia, Aspira's suite of reservation and licensing technology and service solutions is centered around connecting its customers with outdoor adventure seekers from around the world. \n\n"}
{"id": "17322678", "url": "https://en.wikipedia.org/wiki?curid=17322678", "title": "SWAP (instrument)", "text": "SWAP (instrument)\n\nThe \"Sun Watcher using Active Pixel System Detector and Image Processing\" (SWAP) telescope is a compact EUV imager on board the PROBA2 mission that will observe the Sun in extreme ultraviolet (EUV). \nSWAP will provide images of the solar corona at a temperature of roughly 1 million degrees. This instrument was built upon the heritage of the Extreme ultraviolet Imaging Telescope (EIT) which monitors the solar corona since 1996.\n\nSWAP will continue the systematic CME (coronal mass ejection) watch program at an improved image cadence (typically 1 image every minute). With this higher cadence, SWAP will monitor events in the low solar corona that might be relevant for space weather. These events include EIT waves (global waves propagating across the solar disc from the CME eruption site), EUV dimming regions (transient coronal holes from where the CME has lifted off) and filament instabilities (a specific type of flickering during the rise of a filament). SWAP will also take advantage of offpointings provided by the agility featured of PROBA2 platform to follows coronal mass ejections.\n\nSWAP was built at the and will be operated from the PROBA-2 Science Center at the Royal Observatory of Belgium.\n\nSWAP has been used to study coronal brightspot dynamics.\n\n\n"}
{"id": "553947", "url": "https://en.wikipedia.org/wiki?curid=553947", "title": "S band", "text": "S band\n\nThe S band is a designation by the Institute of Electrical and Electronics Engineers (IEEE) for a part of the microwave band of the electromagnetic spectrum covering frequencies from 2 to 4 gigahertz (GHz). Thus it crosses the conventional boundary between the UHF and SHF bands at 3.0 GHz. The S band is used by airport surveillance radar for air traffic control, weather radar, surface ship radar, and some communications satellites, especially those used by NASA to communicate with the Space Shuttle and the International Space Station. The 10 cm radar short-band ranges roughly from 1.55 to 5.2 GHz. The S band also contains the 2.4–2.483 GHz ISM band, widely used for low power unlicensed microwave devices such as cordless phones, wireless headphones (Bluetooth), wireless networking (WiFi), garage door openers, keyless vehicle locks, baby monitors as well as for medical diathermy machines and microwave ovens (typically at 2.495GHz).\n\nIn the U.S., the FCC approved satellite-based Digital Audio Radio Service (DARS) broadcasting in the S band from 2.31 to 2.36 GHz, currently used by Sirius XM Radio. More recently, it has approved for portions of the S band between 2.0 and 2.2 GHz the creation of Mobile Satellite Service (MSS) networks in connection with Ancillary Terrestrial Components (ATC). There are presently a number of companies attempting to deploy such networks, including ICO Satellite Management and TerreStar.\n\nThe 2.6 GHz range is used for China Multimedia Mobile Broadcasting, a satellite radio and mobile TV standard which, as with proprietary systems in the U.S., is incompatible with the open standards used in the rest of the world.\n\nIn May 2009, Inmarsat and Solaris Mobile (a joint venture between Eutelsat and SES, now EchoStar Mobile) were awarded each a 2×15 MHz portion of the S band by the European Commission. The two companies are allowed two years to start providing pan-European MSS services for 18 years. Allocated frequencies are 1.98 to 2.01 GHz for Earth to space communications, and from 2.17 to 2.2 GHz for space to Earth communications.\nEutelsat W2A satellite launched in April, 2009 and located at 10° East is currently the unique satellite in Europe operating on S band frequencies.\n\nIn some countries, S band is used for Direct-to-Home satellite television (unlike similar services in most countries, which use K band). The frequency typically allocated for this service is 2.5 to 2.7 GHz (LOF 1.570 GHz).\n\nIndoStar-1 was the world's first commercial communications satellite to use S-band frequencies for broadcast (pioneered by van der Heyden), which efficiently penetrate the atmosphere and provide high-quality transmissions to small-diameter 80 cm antennas in regions that experience heavy rainfall such as Indonesia. Similar performance is not economically feasible with comparable Ku- or C-band DTH satellite systems since more power is required in these bands to penetrate the moist atmosphere.\n\nWireless network equipment compatible with IEEE 802.11b and 802.11g standards use the 2.4 GHz section of the S band. Some digital cordless telephones operate in this band too. Microwave ovens operate at 2495 or 2450 MHz. IEEE 802.16a and 802.16e standards use a part of the frequency range of S band; under WiMAX standards most vendors are now manufacturing equipment in the range of 3.5 GHz. The exact frequency range allocated for this type of use varies between countries.\n\nIn North America, is an ISM band used for unlicensed spectrum devices such as cordless phones, wireless headphones, and video senders, among other consumer electronics uses, including Bluetooth which operates between 2.402 GHz and 2.480 GHz.\n\nAmateur radio and amateur satellite operators have two S-band allocations, 13 cm (2.4 GHz) and 9 cm (3.4 GHz). Amateur television repeaters also operate in these bands.\n\nAirport surveillance radars typically operate in the 2700–2900 MHz range.\n\nParticle accelerators may be powered by S-band RF sources. The frequencies are then standardized at 2.998 GHz (Europe) or 2.856 GHz (US).\n\nThe National NEXRAD Radar network operates with S-band frequencies. Before implementation of this system, C-band frequencies were commonly used for weather surveillance.\n\nIn the United States, the 3.55 to 3.7 GHz band is becoming shared spectrum under rules adopted by the Federal Communications Commission in April 2015 as a result of the National Broadband Plan (United States). The biggest user of CBRS (Citizens Broadband Radio service) spectrum is the United States Navy. Cable companies are planning to use the band for wireless broadband in rural areas, with Charter Communications beginning tests of the service in January 2018.\n\nUsed as a transmit intermediate frequency in satellite communications as a replacement for L-band where a single/shared coaxial connection is used between the modem/IDU and antenna/ODU for both the transmit and receive signals. This is to prevent interference between the transmit and receive signals which would otherwise not occur on a dual coaxial setup where the transmit and receive signals are separate and both can use the whole L-band frequency range. In a single coaxial connection using S-Band to \"frequency shift\" the transmit signal away from L-band, a multiplier such as 10, is usually applied to form the SHF frequency. For example, the modem would transmit at 2.815GHz IF (S-Band) to the ODU and then the ODU up-converts this signal to 28.15GHz SHF (Ka-Band) towards the satellite.\n\nS band is also used in optical communications to refer to the wavelength range 1460 nm to 1530 nm.\n\n\n"}
{"id": "25666960", "url": "https://en.wikipedia.org/wiki?curid=25666960", "title": "Sally Saw", "text": "Sally Saw\n\nA sally saw is a portable, mechanical, motorized saw. It was made obsolete by the invention of the chainsaw.\n\nIt was used for limbing (removing branches from the stem of a felled tree), and bucking (cutting a felled and delimbed tree into logs).\n\nA sally saw consists of several parts:\n\nThe sally saw was created by the Cummings Machine Works in the first half of the 20th century.\n"}
{"id": "29485", "url": "https://en.wikipedia.org/wiki?curid=29485", "title": "Skyscraper", "text": "Skyscraper\n\nA skyscraper is a continuously habitable high-rise building that has over 40 floors and is taller than approximately . Historically, the term first referred to buildings with 10 to 20 floors in the 1880s. The definition shifted with advancing construction technology during the 20th Century. Skyscrapers may host commercial offices or residential space, or both. For buildings above a height of , the term \"supertall\" can be used, while skyscrapers reaching beyond are classified as \"megatall\".\n\nOne common feature of skyscrapers is having a steel framework that supports curtain walls. These curtain walls either bear on the framework below or are suspended from the framework above, rather than resting on load-bearing walls of conventional construction. Some early skyscrapers have a steel frame that enables the construction of load-bearing walls taller than of those made of reinforced concrete.\n\nModern skyscrapers' walls are not load-bearing, and most skyscrapers are characterized by large surface areas of windows made possible by steel frames and curtain walls. However, skyscrapers can have curtain walls that mimic conventional walls with a small surface area of windows. Modern skyscrapers often have a tubular structure, and are designed to act like a hollow cylinder to resist wind, seismic, and other lateral loads. To appear more slender, allow less wind exposure, and transmit more daylight to the ground, many skyscrapers have a design with setbacks, which are sometimes also structurally required.\n\nThe term \"skyscraper\" was first applied to buildings of steel framed construction of at least 10 stories in the late 19th century, a result of public amazement at the tall buildings being built in major American cities like Chicago, New York City, Philadelphia, Detroit, and St. Louis. The first steel-frame skyscraper was the Home Insurance Building (originally 10 stories with a height of ) in Chicago, Illinois in 1885. Some point to Philadelphia's 10-story Jayne Building (1849–50) as a proto-skyscraper, or to New York's seven-floor Equitable Life Building (New York City), built in 1870, for its innovative use of a kind of skeletal frame, but such designation depends largely on what factors are chosen. Even the scholars making the argument find it to be purely academic.\n\nThe structural definition of the word \"skyscraper\" was refined later by architectural historians, based on engineering developments of the 1880s that had enabled construction of tall multi-story buildings. This definition was based on the steel skeleton—as opposed to constructions of load-bearing masonry, which passed their practical limit in 1891 with Chicago's Monadnock Building.\n\nThe Council on Tall Buildings and Urban Habitat defines skyscrapers as those buildings which reach or exceed in height. Others in the United States and Europe also draw the lower limit of a skyscraper at . \n\nThe Emporis Standards Committee defines a high-rise building as \"a multi-story structure between 35–100 meters tall, or a building of unknown height from 12–39 floors\" and a skyscraper as \"a multi-story building whose architectural height is at least .\" Some structural engineers define a highrise as any vertical construction for which wind is a more significant load factor than earthquake or weight. Note that this criterion fits not only high-rises but some other tall structures, such as towers. \n\nThe word \"skyscraper\" often carries a connotation of pride and achievement. The skyscraper, in name and social function, is a modern expression of the age-old symbol of the world center or \"axis mundi\": a pillar that connects earth to heaven and the four compass directions to one another.\n\nThe tallest building in ancient times was the Great Pyramid of Giza in ancient Egypt, built in the 26th century BC. It was not surpassed in height for thousands of years, the Lincoln Cathedral having exceeded it in 1311–1549, before its central spire collapsed. The latter in turn was not surpassed until the Washington Monument in 1884. However, being uninhabited, none of these structures actually comply with the modern definition of a skyscraper.\n\nHigh-rise apartments flourished in classical antiquity. Ancient Roman insulae in imperial cities reached 10 and more stories. Beginning with Augustus (r. 30 BC-14 AD), several emperors attempted to establish limits of 20–25 m for multi-story buildings, but met with only limited success. Lower floors were typically occupied by shops or wealthy families, the upper rented to the lower classes. Surviving Oxyrhynchus Papyri indicate that seven-story buildings existed in provincial towns such as in 3rd century AD Hermopolis in Roman Egypt.\n\nThe skylines of many important medieval cities had large numbers of high-rise urban towers, built by the wealthy for defense and status. The residential Towers of 12th century Bologna numbered between 80 and 100 at a time, the tallest of which is the high Asinelli Tower. A Florentine law of 1251 decreed that all urban buildings be immediately reduced to less than 26 m. Even medium-sized towns of the era are known to have proliferations of towers, such as the 72 up to 51 m height in San Gimignano.\n\nThe medieval Egyptian city of Fustat housed many high-rise residential buildings, which Al-Muqaddasi in the 10th century described as resembling minarets. Nasir Khusraw in the early 11th century described some of them rising up to 14 stories, with roof gardens on the top floor complete with ox-drawn water wheels for irrigating them. Cairo in the 16th century had high-rise apartment buildings where the two lower floors were for commercial and storage purposes and the multiple stories above them were rented out to tenants. An early example of a city consisting entirely of high-rise housing is the 16th-century city of Shibam in Yemen. Shibam was made up of over 500 tower houses, each one rising 5 to 11 stories high, with each floor being an apartment occupied by a single family. The city was built in this way in order to protect it from Bedouin attacks. Shibam still has the tallest mudbrick buildings in the world, with many of them over high.\n\nAn early modern example of high-rise housing was in 17th-century Edinburgh, Scotland, where a defensive city wall defined the boundaries of the city. Due to the restricted land area available for development, the houses increased in height instead. Buildings of 11 stories were common, and there are records of buildings as high as 14 stories. Many of the stone-built structures can still be seen today in the old town of Edinburgh. The oldest iron framed building in the world, although only partially iron framed, is The Flaxmill (also locally known as the \"Maltings\"), in Shrewsbury, England. Built in 1797, it is seen as the \"grandfather of skyscrapers\", since its fireproof combination of cast iron columns and cast iron beams developed into the modern steel frame that made modern skyscrapers possible. In 2013 funding was confirmed to convert the derelict building into offices.\n\nIn 1857 Elisha Otis introduced the safety elevator, allowing convenient and safe passenger movement to upper floors. Another crucial development was the use of a steel frame instead of stone or brick, otherwise the walls on the lower floors on a tall building would be too thick to be practical. An early development in this area was Oriel Chambers in Liverpool, England. It was only five floors high. Further developments led to the world's first skyscraper, the ten-story Home Insurance Building in Chicago, built in 1884–1885. While its height is not considered very impressive today, it was at that time. The building of tall buildings in the 1880s gave the skyscraper its first architectural movement the Chicago School, which developed what has been called the Commercial Style.\n\nThe architect, Major William Le Baron Jenney, created a load-bearing structural frame. In this building, a steel frame supported the entire weight of the walls, instead of load-bearing walls carrying the weight of the building. This development led to the \"Chicago skeleton\" form of construction. In addition to the steel frame, the Home Insurance Building also utilized fireproofing, elevators, and electrical wiring, key elements in most skyscrapers today.\n\nBurnham and Root's Rand McNally Building in Chicago, 1889, was the first all-steel framed skyscraper, while Louis Sullivan's Wainwright Building in St. Louis, Missouri, 1891, was the first steel-framed building with soaring vertical bands to emphasize the height of the building and is therefore considered to be the first early skyscraper.\n\nMost early skyscrapers emerged in the land-strapped areas of Chicago and New York City toward the end of the 19th century. A land boom in Melbourne, Australia between 1888 and 1891 spurred the creation of a significant number of early skyscrapers, though none of these were steel reinforced and few remain today. Height limits and fire restrictions were later introduced. London builders soon found building heights limited due to a complaint from Queen Victoria, rules that continued to exist with few exceptions.\n\nConcerns about aesthetics and fire safety had likewise hampered the development of skyscrapers across continental Europe for the first half of the twentieth century. Some notable exceptions are the tall 1898 Witte Huis \"(White House)\" in Rotterdam; the Royal Liver Building in Liverpool, completed in 1911 and high; the tall 1924 Marx House in Düsseldorf, Germany; the Kungstornen \"(Kings' Towers)\" in Stockholm, Sweden, which were built 1924–25, the Edificio Telefónica in Madrid, Spain, built in 1929; the Boerentoren in Antwerp, Belgium, built in 1932; the Prudential Building in Warsaw, Poland, built in 1934; and the Torre Piacentini in Genoa, Italy, built in 1940.\n\nAfter an early competition between Chicago and New York City for the world's tallest building, New York took the lead by 1895 with the completion of the tall American Surety Building, leaving New York with the title of the world's tallest building for many years.\n\nModern skyscrapers are built with steel or reinforced concrete frameworks and curtain walls of glass or polished stone. They use mechanical equipment such as water pumps and elevators.\n\nFrom the 1930s onwards, skyscrapers began to appear around the world—such as in Latin America (such as São Paulo, Rio de Janeiro, Buenos Aires, Santiago, Lima, Caracas, Bogotá, Panama City, Mexico City, Monterrey) and in Asia (Tokyo, Shanghai, Hong Kong, Manila, Jakarta, Singapore, Mumbai, Seoul, Kuala Lumpur, Taipei, Bangkok).\n\nImmediately after World War II, the Soviet Union planned eight massive skyscrapers, seven of which were built by 1953, dubbed the \"Seven Sisters of Moscow\". The Building of Moscow State University was the tallest building in Europe in 1953–1990. Other skyscrapers in the style of Socialist Classicism were erected in East Germany (Frankfurter Tor), Poland (PKiN), Ukraine (Hotel Ukrayina), Latvia (Academy of Sciences) and other countries. The western countries of Europe also began to permit taller skyscrapers than before WW2, such as Madrid during the 1950s (Gran Vía). Finally, skyscrapers also began to be constructed in cities of Africa, the Middle East and Oceania (mainly Australia) from the late 1950s on.\n\nSkyscraper projects after World War II typically rejected the classical designs of the early skyscrapers, instead embracing the uniform international style; many older skyscrapers were redesigned to suit contemporary tastes or even demolished—such as New York's Singer Building, once the world's tallest skyscraper.\n\nGerman architect Ludwig Mies van der Rohe became one of the world's most renowned architects in the second half of the 20th century. He conceived of the glass façade skyscraper and, along with Norwegian Fred Severud, he designed the Seagram Building in 1958, a skyscraper that is often regarded as the pinnacle of the modernist high-rise architecture.\n\nAfter the Great Depression skyscrapers construction suffered a hiatus for over thirty years due to economic problems. A revival occurred with structural innovations that transformed the industry, making it possible for people to live and work in \"cities in the sky\".\n\nIn the early 1960s structural engineer Fazlur Rahman Khan, considered the \"father of tubular designs\" for high-rises, discovered that the dominating rigid steel frame structure was not the only system apt for tall buildings, marking a new era of skyscraper construction in terms of multiple structural systems. His central innovation in skyscraper design and construction was the concept of the \"tube\" structural system, including the \"framed tube\", \"trussed tube\", and \"bundled tube\". His \"tube concept\", using all the exterior wall perimeter structure of a building to simulate a thin-walled tube, revolutionized tall building design. These systems allow greater economic efficiency, and also allow skyscrapers to take on various shapes, no longer needing to be rectangular and box-shaped. The first building to employ the tube structure was the Chestnut De-Witt apartment building, this building is considered to be a major development in modern architecture. These new designs opened an economic door for contractors, engineers, architects, and investors, providing vast amounts of real estate space on minimal plots of land. Over the next fifteen years, many towers were built by Fazlur Rahman Khan and the \"Second Chicago School\", including the hundred story John Hancock Center and the massive Willis Tower. Other pioneers of this field include Hal Iyengar and William LeMessurier.\n\nMany buildings designed in the 70s lacked a particular style and recalled ornamentation from earlier buildings designed before the 50s. These design plans ignored the environment and loaded structures with decorative elements and extravagant finishes. This approach to design was opposed by Fazlur Khan and he considered the designs to be whimsical rather than rational. Moreover, he considered the work to be a waste of precious natural resources. Khan's work promoted structures integrated with architecture and the least use of material resulting in the least carbon emission impact on the environment. The next era of skyscrapers will focus on the environment including performance of structures, types of material, construction practices, absolute minimal use of materials/natural resources, emboided energy within the structures, and more importantly, a holistically integrated building systems approach.\n\nModern building practices regarding supertall structures have led to the study of \"vanity height\". Vanity height, according to the CTBUH, is the distance between the highest floor and its architectural top (excluding antennae, flagpole or other functional extensions). Vanity height first appeared in New York City skyscrapers as early as the 1920s and 1930s but supertall buildings have relied on such uninhabitable extensions for on average 30% of their height, raising potential definitional and sustainability issues. The current era of skyscrapers focuses on sustainability, its built and natural environments, including the performance of structures, types of materials, construction practices, absolute minimal use of materials and natural resources, energy within the structure, and a holistically integrated building systems approach. LEED is a current green building standard.\n\nArchitecturally, with the movements of Postmodernism, New Urbanism and New Classical Architecture, that established since the 1980s, a more classical approach came back to global skyscraper design, that remains popular today. Examples are the Wells Fargo Center, NBC Tower, Parkview Square, 30 Park Place, the Messeturm, the iconic Petronas Towers and Jin Mao Tower.\n\nOther contemporary styles and movements in skyscraper design include organic, sustainable, neo-futurist, structuralist, high-tech, deconstructivist, blob, digital, streamline, novelty, critical regionalist, vernacular, Neo Art Deco and neo-historist, also known as revivalist.\n\n3 September is the global commemorative day for skyscrapers, called \"Skyscraper Day\".\n\nNew York City developers competed among themselves, with successively taller buildings claiming the title of \"world's tallest\" in the 1920s and early 1930s, culminating with the completion of the Chrysler Building in 1930 and the Empire State Building in 1931, the world's tallest building for forty years. The first completed tall World Trade Center tower became the world's tallest building in 1972. However, it was overtaken by the Sears Tower (now Willis Tower) in Chicago within two years. The tall Sears Tower stood as the world's tallest building for 24 years, from 1974 until 1998, until it was edged out by Petronas Twin Towers in Kuala Lumpur, which held the title for six years.\n\nThe design and construction of skyscrapers involves creating safe, habitable spaces in very tall buildings. The buildings must support their weight, resist wind and earthquakes, and protect occupants from fire. Yet they must also be conveniently accessible, even on the upper floors, and provide utilities and a comfortable climate for the occupants. The problems posed in skyscraper design are considered among the most complex encountered given the balances required between economics, engineering, and construction management.\n\nOne common feature of skyscrapers is a steel framework from which curtain walls are suspended, rather than load-bearing walls of conventional construction. Most skyscrapers have a steel frame that enables them to be built taller than typical load-bearing walls of reinforced concrete. Skyscrapers usually have a particularly small surface area of what are conventionally thought of as walls. Because the walls are not load-bearing most skyscrapers are characterized by surface areas of windows made possible by the concept of steel frame and curtain wall. However, skyscrapers can also have curtain walls that mimick conventional walls and have a small surface area of windows.\n\nThe concept of a skyscraper is a product of the industrialized age, made possible by cheap fossil fuel derived energy and industrially refined raw materials such as steel and concrete. The construction of skyscrapers was enabled by steel frame construction that surpassed brick and mortar construction starting at the end of the 19th century and finally surpassing it in the 20th century together with reinforced concrete construction as the price of steel decreased and labour costs increased.\n\nThe steel frames become inefficient and uneconomic for supertall buildings as usable floor space is reduced for progressively larger supporting columns. Since about 1960, tubular designs have been used for high rises. This reduces the usage of material (more efficient in economic terms – Willis Tower uses a third less steel than the Empire State Building) yet allows greater height. It allows fewer interior columns, and so creates more usable floor space. It further enables buildings to take on various shapes.\n\nElevators are characteristic to skyscrapers. In 1852 Elisha Otis introduced the safety elevator, allowing convenient and safe passenger movement to upper floors. Another crucial development was the use of a steel frame instead of stone or brick, otherwise the walls on the lower floors on a tall building would be too thick to be practical. Today major manufacturers of elevators include Otis, ThyssenKrupp, Schindler, and KONE.\n\nAdvances in construction techniques have allowed skyscrapers to narrow in width, while increasing in height. Some of these new techniques include mass dampers to reduce vibrations and swaying, and gaps to allow air to pass through, reducing wind shear.\n\nGood structural design is important in most building design, but particularly for skyscrapers since even a small chance of catastrophic failure is unacceptable given the high price. This presents a paradox to civil engineers: the only way to assure a lack of failure is to test for all modes of failure, in both the laboratory and the real world. But the only way to know of all modes of failure is to learn from previous failures. Thus, no engineer can be absolutely sure that a given structure will resist all loadings that could cause failure, but can only have large enough margins of safety such that a failure is acceptably unlikely. When buildings do fail, engineers question whether the failure was due to some lack of foresight or due to some unknowable factor.\n\nThe load a skyscraper experiences is largely from the force of the building material itself. In most building designs, the weight of the structure is much larger than the weight of the material that it will support beyond its own weight. In technical terms, the dead load, the load of the structure, is larger than the live load, the weight of things in the structure (people, furniture, vehicles, etc.). As such, the amount of structural material required within the lower levels of a skyscraper will be much larger than the material required within higher levels. This is not always visually apparent. The Empire State Building's setbacks are actually a result of the building code at the time (1916 Zoning Resolution), and were not structurally required. On the other hand, John Hancock Center's shape is uniquely the result of how it supports loads. Vertical supports can come in several types, among which the most common for skyscrapers can be categorized as steel frames, concrete cores, tube within tube design, and shear walls.\n\nThe wind loading on a skyscraper is also considerable. In fact, the lateral wind load imposed on super-tall structures is generally the governing factor in the structural design. Wind pressure increases with height, so for very tall buildings, the loads associated with wind are larger than dead or live loads.\n\nOther vertical and horizontal loading factors come from varied, unpredictable sources, such as earthquakes.\n\nBy 1895, steel had replaced cast iron as skyscrapers' structural material. Its malleability allowed it to be formed into a variety of shapes, and it could be riveted, ensuring strong connections. The simplicity of a steel frame eliminated the inefficient part of a shear wall, the central portion, and consolidated support members in a much stronger fashion by allowing both horizontal and vertical supports throughout. Among steel's drawbacks is that as more material must be supported as height increases, the distance between supporting members must decrease, which in turn increases the amount of material that must be supported. This becomes inefficient and uneconomic for buildings above 40 stories tall as usable floor spaces are reduced for supporting column and due to more usage of steel.\n\nA new structural system of framed tubes was developed in 1963. Fazlur Khan and J. Rankine defined the framed tube structure as \"a three dimensional space structure composed of three, four, or possibly more frames, braced frames, or shear walls, joined at or near their edges to form a vertical tube-like structural system capable of resisting lateral forces in any direction by cantilevering from the foundation.\" Closely spaced interconnected exterior columns form the tube. Horizontal loads (primarily wind) are supported by the structure as a whole. Framed tubes allow fewer interior columns, and so create more usable floor space, and about half the exterior surface is available for windows. Where larger openings like garage doors are required, the tube frame must be interrupted, with transfer girders used to maintain structural integrity. Tube structures cut down costs, at the same time allowing buildings to reach greater heights. Concrete tube-frame construction was first used in the DeWitt-Chestnut Apartment Building, completed in Chicago in 1963, and soon after in the John Hancock Center and World Trade Center.\n\nThe tubular systems are fundamental to tall building design. Most buildings over 40-stories constructed since the 1960s now use a tube design derived from Khan's structural engineering principles, examples including the construction of the World Trade Center, Aon Center, Petronas Towers, Jin Mao Building, and most other supertall skyscrapers since the 1960s. The strong influence of tube structure design is also evident in the construction of the current tallest skyscraper, the Burj Khalifa.\n\nKhan pioneered several other variations of the tube structure design. One of these was the concept of X-bracing, or the \"trussed tube\", first employed for the John Hancock Center. This concept reduced the lateral load on the building by transferring the load into the exterior columns. This allows for a reduced need for interior columns thus creating more floor space. This concept can be seen in the John Hancock Center, designed in 1965 and completed in 1969. One of the most famous buildings of the structural expressionist style, the skyscraper's distinctive X-bracing exterior is actually a hint that the structure's skin is indeed part of its 'tubular system'. This idea is one of the architectural techniques the building used to climb to record heights (the tubular system is essentially the spine that helps the building stand upright during wind and earthquake loads). This X-bracing allows for both higher performance from tall structures and the ability to open up the inside floorplan (and usable floor space) if the architect desires.\n\nThe John Hancock Center was far more efficient than earlier steel-frame structures. Where the Empire State Building (1931), required about 206 kilograms of steel per square metre and Chase Manhattan Bank Building (1961) required 275, the John Hancock Center required only 145. The trussed tube concept was applied to many later skyscrapers, including the Onterie Center, Citigroup Center and Bank of China Tower.\n\nAn important variation on the tube frame is the \"bundled tube\", which uses several interconnected tube frames. The Willis Tower in Chicago used this design, employing nine tubes of varying height to achieve its distinct appearance. The bundled tube structure meant that \"buildings no longer need be boxlike in appearance: they could become sculpture.\"\n\nThe invention of the elevator was a precondition for the invention of skyscrapers, given that most people would not (or could not) climb more than a few flights of stairs at a time. The elevators in a skyscraper are not simply a necessary utility, like running water and electricity, but are in fact closely related to the design of the whole structure: a taller building requires more elevators to service the additional floors, but the elevator shafts consume valuable floor space. If the service core, which contains the elevator shafts, becomes too big, it can reduce the profitability of the building. Architects must therefore balance the value gained by adding height against the value lost to the expanding service core.\n\nMany tall buildings use elevators in a non-standard configuration to reduce their footprint. Buildings such as the former World Trade Center Towers and Chicago's John Hancock Center use sky lobbies, where express elevators take passengers to upper floors which serve as the base for local elevators. This allows architects and engineers to place elevator shafts on top of each other, saving space. Sky lobbies and express elevators take up a significant amount of space, however, and add to the amount of time spent commuting between floors.\n\nOther buildings, such as the Petronas Towers, use double-deck elevators, allowing more people to fit in a single elevator, and reaching two floors at every stop. It is possible to use even more than two levels on an elevator, although this has never been done. The main problem with double-deck elevators is that they cause everyone in the elevator to stop when only people on one level need to get off at a given floor.\n\nBuildings with sky lobbies include the World Trade Center, Petronas Twin Towers and Taipei 101. The 44th-floor sky lobby of the John Hancock Center also featured the first high-rise indoor swimming pool, which remains the highest in America.\n\nSkyscrapers are usually situated in city centers where the price of land is high. Constructing a skyscraper becomes justified if the price of land is so high that it makes economic sense to build upwards as to minimize the cost of the land per the total floor area of a building. Thus the construction of skyscrapers is dictated by economics and results in skyscrapers in a certain part of a large city unless a building code restricts the height of buildings.\n\nSkyscrapers are rarely seen in small cities and they are characteristic of large cities, because of the critical importance of high land prices for the construction of skyscrapers. Usually only office, commercial and hotel users can afford the rents in the city center and thus most tenants of skyscrapers are of these classes. Some skyscrapers have been built in areas where the bedrock is near surface, because this makes constructing the foundation cheaper, for example this is the case in Midtown Manhattan and Lower Manhattan, in New York City, but not in-between these two parts of the city.\n\nToday, skyscrapers are an increasingly common sight where land is expensive, as in the centers of big cities, because they provide such a high ratio of rentable floor space per unit area of land.\n\nformula_1\n\nOne problem with skyscrapers is car parking. In the largest cities most people commute via public transport, but for smaller cities a lot of parking spaces are needed. Multi-storey car parks are impractical to build very tall, so a lot of land area is needed.\n\nThere may be a correlation between skyscraper construction and great income inequality but this has not been conclusively proven.\n\nThe amount of steel, concrete, and glass needed to construct a single skyscraper is large, and these materials represent a great deal of embodied energy. Skyscrapers are thus energy intensive buildings, but skyscrapers have a long lifespan, for example the Empire State Building in New York City, United States completed in 1931 and is still in active use.\n\nSkyscrapers have considerable mass, which means that they must be built on a sturdier foundation than would be required for shorter, lighter buildings. Building materials must also be lifted to the top of a skyscraper during construction, requiring more energy than would be necessary at lower heights. Furthermore, a skyscraper consumes a lot of electricity because potable and non-potable water has to be pumped to the highest occupied floors, skyscrapers are usually designed to be mechanically ventilated, elevators are generally used instead of stairs, and natural lighting cannot be utilized in rooms far from the windows and the windowless spaces such as elevators, bathrooms and stairwells.\n\nSkyscrapers can be artificially lit and the energy requirements can be covered by renewable energy or other electricity generation with low greenhouse gas emissions. Heating and cooling of skyscrapers can be efficient, because of centralized HVAC systems, heat radiation blocking windows and small surface area of the building. There is Leadership in Energy and Environmental Design (LEED) certification for skyscrapers. For example, the Empire State Building received a gold Leadership in Energy and Environmental Design rating in September 2011 and the Empire State Building is the tallest LEED certified building in the United States, proving that skyscrapers can be environmentally friendly. Also the 30 St Mary Axe in London, the United Kingdom is an environmentally friendly skyscraper.\n\nIn the lower levels of a skyscraper a larger percentage of the building cross section must be devoted to the building structure and services than is required for lower buildings:\n\nIn low-rise structures, the support rooms (chillers, transformers, boilers, pumps and air handling units) can be put in basements or roof space—areas which have low rental value. There is, however, a limit to how far this plant can be located from the area it serves. The farther away it is the larger the risers for ducts and pipes from this plant to the floors they serve and the more floor area these risers take. In practice this means that in highrise buildings this plant is located on 'plant levels' at intervals up the building.\n\nAt the beginning of the 20th century, New York City was a center for the Beaux-Arts architectural movement, attracting the talents of such great architects as Stanford White and Carrere and Hastings. As better construction and engineering technology became available as the century progressed, New York City and Chicago became the focal point of the competition for the tallest building in the world. Each city's striking skyline has been composed of numerous and varied skyscrapers, many of which are icons of 20th-century architecture:\n\nMomentum in setting records passed from the United States to other nations with the opening of the Petronas Twin Towers in Kuala Lumpur, Malaysia, in 1998. The record for the world's tallest building has remained in Asia since the opening of Taipei 101 in Taipei, Taiwan, in 2004. A number of architectural records, including those of the world's tallest building and tallest free-standing structure, moved to the Middle East with the opening of the Burj Khalifa in Dubai, United Arab Emirates.\n\nThis geographical transition is accompanied by a change in approach to skyscraper design. For much of the twentieth century large buildings took the form of simple geometrical shapes. This reflected the \"international style\" or modernist philosophy shaped by Bauhaus architects early in the century. The last of these, the Willis Tower and World Trade Center towers in New York, erected in the 1970s, reflect the philosophy. Tastes shifted in the decade which followed, and new skyscrapers began to exhibit postmodernist influences. This approach to design avails itself of historical elements, often adapted and re-interpreted, in creating technologically modern structures. The Petronas Twin Towers recall Asian pagoda architecture and Islamic geometric principles. Taipei 101 likewise reflects the pagoda tradition as it incorporates ancient motifs such as the ruyi symbol. The Burj Khalifa draws inspiration from traditional Islamic art. Architects in recent years have sought to create structures that would not appear equally at home if set in any part of the world, but that reflect the culture thriving in the spot where they stand.\n\nThe following list measures height of the roof. The more common gauge is the \"highest architectural detail\"; such ranking would have included Petronas Towers, built in 1996.\n\nMany skyscrapers were never built due to financial problems, politics and culture. The Chicago Spire was to be the tallest building in the Western Hemisphere, but it was on hold due to the global financial crisis of 2008. One year later, the project was cancelled.\n\nProposals for such structures have been put forward, including the Burj Mubarak Al Kabir in Kuwait and Azerbaijan Tower in Baku. Kilometer-plus structures present architectural challenges that may eventually place them in a new architectural category. The first building under construction and planned to be over one kilometre tall is the Jeddah Tower.\n\nSeveral wooden skyscraper designs have been designed and built. A 14-story housing project in Bergen, Norway known as 'Treet' or 'The Tree' became the world's tallest wooden apartment block when it was completed in late 2015. The Tree's record was eclipsed by Brock Commons, an 18-story wooden dormitory at the University of British Columbia in Canada, when it was completed in September 2016. \n\nA 40-story residential building 'Trätoppen' has been proposed by architect Anders Berensson to be built in Stockholm, Sweden. Trätoppen would be the tallest building in Stockholm, though there are no immediate plans to begin construction. The tallest currently-planned wooden skyscraper is the 70-story W350 Project in Tokyo, to be built by the Japanese wood products company Sumitomo Forestry Co. to celebrate its 350th anniversary in 2041. An 80-story wooden skyscraper, the River Beech Tower, has been proposed by a team including architects Perkins + Will and the University of Cambridge. The River Beech Tower, on the banks of the Chicago River in Chicago, Illinois, would be 348 feet shorter than the W350 Project despite having 10 more stories. \n\nWooden skyscrapers are estimated to be around a quarter of the weight of an equivalent reinforced-concrete structure as well as reducing the building carbon footprint by 60–75%. Buildings have been designed using cross-laminated timber (CLT) which gives a higher rigidity and strength to wooden structures. CLT panels are prefabricated and can therefore speed up building time.\n\n\n\n"}
{"id": "21525554", "url": "https://en.wikipedia.org/wiki?curid=21525554", "title": "Small plates", "text": "Small plates\n\nSmall plates is a manner of dining that became popular in US food service after 2000. \"Small plates\" may either refer to small dishes resembling appetizers which are ordered à la carte and often shared (such as tapas), or to the small courses served as part of a more formal meal.\n\nSome types of small plates which have influenced the modern US concept are:\n\n"}
{"id": "506530", "url": "https://en.wikipedia.org/wiki?curid=506530", "title": "SmartMedia", "text": "SmartMedia\n\nSmartMedia is a flash memory card standard owned by Toshiba, with capacities ranging from 2 MB to 128 MB. SmartMedia memory cards are no longer manufactured.\n\nThe SmartMedia format was launched in the summer of 1995 to compete with the MiniCard, CompactFlash, and PC Card formats. Although memory cards are nowadays associated with digital cameras, digital audio players, PDAs, and similar devices, SmartMedia was pitched as a successor to the computer floppy disk. Indeed, the format was originally named Solid State Floppy Disk Card (SSFDC). The SSFDC forum, a consortium aiming to promote SSFDC as an industry standard, was founded in April 1996, consisting of 37 initial members.\n\nA SmartMedia card consists of a single NAND flash chip embedded in a thin plastic card, although some higher capacity cards contain multiple, linked chips. It was one of the smallest and thinnest of the early memory cards, only 0.76mm thick, and managed to maintain a favorable cost ratio as compared to the others. SmartMedia cards lack a built-in controller chip, which kept the cost down. This feature later caused problems, since some older devices would require firmware updates to handle larger capacity cards. The lack of built-in controller also made it impossible for the card to perform automatic wear levelling, a process which prevents premature wearout of a sector by mapping the writes to various other sectors in the card.\n\nSmartMedia cards can be used in a standard 3.5\" floppy drive by means of a FlashPath adapter. This is possibly the only way of obtaining flash memory functionality with very old hardware, and it remains one of SmartMedia's most distinctive features. This method was not without its own disadvantages, as it required special drivers offering only very basic file read/write capability (or read-only on Macintosh systems) and was limited to floppy disk transfer speeds. However, this was not so troublesome in the earlier days of the format when card sizes were limited (generally 8~16MB) and USB interfaces were both uncommon and low-speed, with digital cameras connecting via \"high speed\" serial links that themselves needed drivers and special transfer programs. The fifteen minutes taken to read a nearly-full 16MB card - directly to hard disk - via Flashpath using the slowest (128kbit/s) PC floppy controller was still simpler and slightly faster than the quickest reliable (115.2kbit/s) serial link, without the need for connection, synching and thumbnail previewing, and only beaten by expensive parallel-port based external card readers that could do the same job in two minutes or less (1000kbit/s-plus, comparable to USB 1.0) when connected to a compatible high-speed ECP or EPP port (and ~5 minutes using a basic PPT in failsafe mode).\n\nTypically, SmartMedia cards were used as storage for portable devices, in a form that could easily be removed for access by a PC. For example, pictures taken with a digital camera would be stored as image files on a SmartMedia card. A user could copy the images to a computer with a SmartMedia reader. A reader was typically a small box connected via USB or some other serial connection. Modern computers, both laptops and desktops, will occasionally have SmartMedia slots built in. While availability of dedicated SmartMedia readers has dropped off, readers that read multiple card types (such as 4-in-1, 10-in-1) continue to include the format, but even these have decreased in quantity, with many dropping SmartMedia in favour of MicroSD and/or Memory Stick Micro.\n\nSmartMedia was popular in digital cameras, and reached its peak in about 2001 when it garnered nearly half of the digital camera market. It was backed especially by Fujifilm and Olympus, though the format started to exhibit problems as camera resolutions increased. Cards larger than 128 MB were not available, and the compact digital cameras were reaching a size where even SmartMedia cards were too big to be convenient. Eventually Toshiba switched to smaller, higher-capacity Secure Digital cards, and SmartMedia ceased to have major support after Olympus and Fujifilm both switched to xD. It did not find as much support in PDAs, MP3 Players, or Pagers as some other formats, especially in North America and Europe, though there was still significant use.\n\nSmartMedia cards larger than 128 MB were never released, although there were rumors of a 256 MB card being planned. Technical specifications for the memory size were released, and the 256 MB cards were even advertised in some places. Some older devices cannot support cards larger than 16 or sometimes 32 MB without a firmware update, if at all. \nSmartMedia cards came in two formats, 5 V and the more modern 3.3 V (sometimes marked 3 V), named for their main supply voltages. The packaging was nearly identical, except for the reversed placement of the notched corner. Many older SmartMedia devices only support 5V SmartMedia cards, whereas many newer devices only support 3.3V cards. In order to protect 3.3V cards from being damaged in 5V-only devices, the card reader should have some mechanical provision (such as detecting the type of notch) to disallow insertion of an unsupported type of card. Some low-cost, 5V-only card readers do not operate this way, and inserting a 3.3V card into such a 5V-only reader will result in permanent damage to the card. Dual-voltage card readers are highly recommended.\n\nThere is an oversized xD-to-SmartMedia adapter that allows xD cards to use a SmartMedia port, but it does not fit entirely inside a SmartMedia slot. There is a limit on the capacity of the xD card when used in such adapters (sometimes 128 MB or 256 MB), and the device is subject to the restrictions of the SmartMedia reader as well.\n\nSmartMedia memory cards are no longer manufactured as of around 2006. There have been no new devices designed for SmartMedia for quite a long time now. Smartmedia cards are still frequently available on eBay mostly in used condition with new cards coming up from time to time.\n\nMany SmartMedia cards include a little-known copy protection feature known as \"ID\". This is why many cards are marked with \"ID\" beside the capacity. This gave every card a unique identification number for use with copy protection systems. One of the few implementations of this primitive DRM system was by the Korean company Game Park, which used it to protect commercial titles for the GP32 handheld gaming system. Samsung's 1999 Yepp Hip-Hop MP3 player also used the feature in order to implement Secure Digital Music Initiative DRM.\nSmartMedia cards frequently become corrupted and unusable when the cards are read / written in a card reading device. Affected SmartMedia cards will be unusable and the camera or device will be unable to format, read or write to the card. Data loss and a change in the capacity that the device displays are also signs of a low level format corruption or a corrupted CIS (Card Information System). The majority of these card format errors can be repaired and the data or photos can be recovered. A low level format with the proper software and equipment can return the SmartMedia memory card to its proper working order. With some research on the internet one can find several sources of information and companies that provide a SmartMedia card repair service. Considering the dwindling supply of SmartMedia memory format, these corrupted cards and the devices they support are needlessly being discarded. The majority of corrupted or unusable SmartMedia cards can be repaired.\nA recent discovery is that Flashpath adaptors can sometimes corrupt cards when the battery goes low, which is easily repaired by using software such as Winhex or simply cloning a card of identical capacity to the failed card.\nNormally the data on them is still recoverable so it is advised to clone it to an image first so bitwise errors can be corrected.\n\n\nA SmartMedia card, and the FlashPath adapter, is used as a plot device in the film \"Colombiana\" (2011), during the opening scenes set in the mid-1990s. A card is swallowed by the 9 year old orphaned victim to hide it, then regurgitated.\n\n\n"}
{"id": "48074821", "url": "https://en.wikipedia.org/wiki?curid=48074821", "title": "Spectral regrowth", "text": "Spectral regrowth\n\nSpectral regrowth is the intermodulation products generated in the presence of a digital transmitter added to an analog communication system.\n"}
{"id": "39849030", "url": "https://en.wikipedia.org/wiki?curid=39849030", "title": "Sure Chill Technology", "text": "Sure Chill Technology\n\nSure Chill is a cooling technology which is currently being used in medical refrigerators, but has wider potential in the future for domestic refrigerators and beverage coolers. According to BBC, the refrigerator 'can stay at 4C for more than 10 days without power, and is used mainly in Africa' to store vaccines and other medical supplies. It can be powered by electricity or solar and uses the physics of water to store energy, thus not relying on batteries. http://www.thetimes.co.uk/tto/business/industries/technology/article4086699.ece\n\nIn 2013 the technology that uses an ice-mass to maintain refrigerator temperature without power won a $100,000 research award from the Bill and Melinda Gates Foundation. In 2014, the company was awarded a further $1.4 Million from the Gates Foundation to develop a life-saving vaccine cooler as part of the foundation’s goal to eliminate preventable diseases worldwide. \n\nIan Tansley is the inventor of the technology in Tywyn, Wales with extensive experience in cooling technologies. Tansley spent many years working with vaccine refrigerators in some of the world’s most inhospitable places. A constant frustration for him was their dependence on re-chargeable batteries which were costly, unreliable and hard to replace in remote areas.\n\nIn 2013, both Sir Richard Feachem and Sir John Houghton were announced as Advisory Board members to The Sure Chill Company. Sir Richard stated “It is my view that Sure Chill technology will make a huge contribution to childhood\nvaccination programs worldwide, especially in poorer communities.” Whilst Sir John stated “ Sure Chill Technology has the potential to improve living conditions and healthcare of populations in some of the poorest countries, and to lessen the impact of global warming on mankind. There can be no greater purpose.”\n"}
{"id": "24028211", "url": "https://en.wikipedia.org/wiki?curid=24028211", "title": "The Combustion Institute", "text": "The Combustion Institute\n\nThe Combustion Institute is an educational non-profit, international, scientific and engineering society whose purpose is to promote research in combustion science. The institute was established in 1954, and its headquarters are in Pittsburgh, Pennsylvania, United States. The current president of the combustion institute is James F. Driscoll.\n\nThe support of this important field of study spanning many scientific and engineering disciplines is done through the discussion of research findings at regional, national and the biennial international symposia, and through the publication of the \"Proceedings of the Combustion Institute\" and the Institute’s journals, \"Combustion and Flame\" and the affiliated journals Progress in Energy and Combustion Science, Combustion Science and Technology and Combustion Theory and Modelling .\n\nThe institute serves as the parent organization for thirty three national sections organized in many countries (the US being divided into three sections) as of 2012:\n\nIn honor of fiftieth anniversary of Combustion Institute, the leading combustion scientists John D. Buckmaster, Paul Clavin, Amable Liñán, Moshe Matalon, Norbert Peters, Gregory Sivashinsky and Forman A. Williams wrote a paper in the \"Proceedings of the Combustion Institute\".\n\nThe international symposium on combustion is organised by the Combustion Institute biennially. The first symposium on combustion was held in 1928 in the United States and the first international symposium on combustion was held on 1948, even though the combustion institute itself was found on 1954. Thirty six symposiums has been held so far and the 37th symposium is to be held on 2018.\n\nDuring each International Symposium, The Combustion Institute awards the following:\n\n"}
{"id": "8992114", "url": "https://en.wikipedia.org/wiki?curid=8992114", "title": "Tiltrotator", "text": "Tiltrotator\n\nA tiltrotator (known under a number of trade names) is an hydraulic attachment/tool used on most excavators, and backhoes between 1,5-40 tons in the Nordic countries (Sweden, Finland, and Norway). A tiltrotator is mounted on the excavator such that the excavator bucket can be rotated through 360 degrees and one tilts +/- 45 degrees, in order to increase the flexibility and precision of the excavator. A tiltrotator can best be described as a wrist between the arm of the excavator and bucket, (or whatever other tool is fitted to the tiltrotator's quick coupler). With its integral quick coupler and rotary swivel, the tiltrotator can also use extra hydraulic functions to power and manipulate other worktools below it such as a breaker, grapple or an auger, which can be attached to the quickcoupler on the tiltrotator, for simplified attachment mounting, dramatically increasing the excavator's utilization on the jobsite. control systems which allows the operator to operate the machine fully through the joysticks. Recognized control systems are SVAB, ICS by Rototilt, Engcon Microprop DC2 and Steelwrist XControl G2. The tiltrotator is nowadays controlled through machines two or four electro-proportionally controlled hydraulic lines.\n\nThe Tiltrotator (also known as a Rotortilt) was introduced to the market in Sweden in the early 1980s by the Norgrens under the family owned and operated company named Noreco, and has become the standard in Scandinavia. The concept has recently gained popularity in other countries such as the Netherlands, Germany, UK, Japan, Canada and United States.\nThere are three major manufacturers of the Tiltrotator concept today (although similar products are sold by various competitors); Rototilt Group AB, (founded in 1973 by Allan Jonsson) who purchased Noreco in 1992, hired the Norgren brothers, and registered the brandname Rototilt, Engcon, (founded in 1993 by also ex-Noreco employee, Stig Engstrom) who started manufacturing their own product using only \"tiltrotator\" to avoid trademark infringements, and Steelwrist, (founded in 2005 by a team of industrialists), who started manufacturing after acquiring a new design originating from a tiltrotator called Dento which was released in 1989.\n\nOther companies manufacturing their version of tiltrotator on a smaller scale include SMP, HKS, Kinshofer, Marttiini Metals, Catsu and Giant China under various product names as tiltrotator, swingrotator, swingotilt, mRoto, Nox, Rotiltor etc., although it's still commonplace on the market as a genericized trademark to call all versions a Tiltrotator.\n\n"}
{"id": "31125036", "url": "https://en.wikipedia.org/wiki?curid=31125036", "title": "Veil (cosmetics)", "text": "Veil (cosmetics)\n\nA cosmetic veil is a powder applied after the other cosmetics to fixate the make up, reduce oiliness, give a matte finish and lustre.\n\nBesides minerals, it may use rice flour to soak up excess oiliness.\n\n"}
{"id": "23442715", "url": "https://en.wikipedia.org/wiki?curid=23442715", "title": "Videocassette recorder", "text": "Videocassette recorder\n\nA videocassette recorder, VCR, or video recorder is an electromechanical device that records analog audio and analog video from broadcast television or other source on a removable, magnetic tape videocassette, and can play back the recording. Use of a VCR to record a television program to play back at a more convenient time is commonly referred to as \"timeshifting\". VCRs can also play back prerecorded tapes. In the 1980s and 1990s, prerecorded videotapes were widely available for purchase and rental, and blank tapes were sold to make recordings. \n\nMost domestic VCRs are equipped with a television broadcast receiver (tuner) for TV reception, and a programmable clock (timer) for unattended recording of a television channel from a start time to an end time specified by the user. These features began as simple mechanical counter-based single-event timers, but were later replaced by more flexible multiple-event digital clock timers. In later models the multiple timer events could be programmed through a menu interface displayed on the playback TV screen (\"on-screen display\" or OSD). This feature allowed several programs to be recorded at different times without further user intervention, and became a major selling point.\n\nThe history of the videocassette recorder follows the history of videotape recording in general. In 1953, Dr. Norikazu Sawazaki developed a prototype helical scan video tape recorder.\n\nAmpex introduced the Quadruplex videotape professional broadcast standard format with its Ampex VRX-1000 in 1956. It became the world's first commercially successful videotape recorder using two-inch (5.1 cm) wide tape. Due to its high price of , the Ampex VRX-1000 could be afforded only by the television networks and the largest individual stations.\n\nIn 1959, Toshiba introduced a \"new\" method of recording known as helical scan, releasing the first commercial helical scan video tape recorder that year. It was first implemented in reel-to-reel videotape recorders (VTRs), and later used with cassette tapes.\n\nIn 1963 Philips introduced their EL3400 1-inch helical scan recorder, aimed at the business and domestic user, and Sony marketed the 2\" PV-100, their first reel-to-reel VTR, intended for business, medical, airline, and educational use.\n\nThe Telcan, produced by the UK Nottingham Electronic Valve Company in 1963, was the first home video recorder. It could be bought as a unit or in kit form for £60, equivalent to approximately £1,100 (over US$1,600) in 2014 currency. However, there were several drawbacks: it was expensive, was not easy to assemble, and could only record 20 minutes at a time. It recorded in black-and-white, the only format available in the UK at the time.\n\nThe half-inch tape Sony model CV-2000, first marketed in 1965, was their first VTR intended for home use. Ampex and RCA followed in 1965 with their own reel-to-reel monochrome VTRs priced under US$1,000 for the home consumer market.\n\nThe EIAJ format was a standard half-inch format used by various manufacturers. EIAJ-1 was an open-reel format. EIAJ-2 used a cartridge that contained a supply reel; the take-up reel was part of the recorder, and the tape had to be fully rewound before removing the cartridge, a slow procedure.\n\nThe development of the videocassette followed the replacement by cassette of other open reel systems in consumer items: the Stereo-Pak four-track audio cartridge in 1962, the compact audio cassette and Instamatic film cartridge in 1963, the 8-track cartridge in 1965, and the Super 8 home movie cartridge in 1966.\n\nIn 1972, videocassettes of movies became available for home use.\n\nSony demonstrated a videocassette prototype in October 1969, then set it aside to work out an industry standard by March 1970 with seven fellow manufacturers. The result, the Sony U-matic system, introduced in Tokyo in September 1971, was the world's first commercial videocassette format. Its cartridges, resembling larger versions of the later VHS cassettes, used 3/4-inch (1.9 cm) tape and had a maximum playing time of 60 minutes, later extended to 80 minutes. Sony also introduced two machines (the VP-1100 videocassette player and the VO-1700, also called the VO-1600 video-cassette recorder) to use the new tapes. U-matic, with its ease of use, quickly made other consumer videotape systems obsolete in Japan and North America, where U-matic VCRs were widely used by television newsrooms (Sony BVU-150 and Trinitron DXC 1810 video camera) schools and businesses. But the high cost - in 1971 for a combination TV/VCR, equivalent to over $8000 in 2014 dollars – kept it out of most homes.\n\nIn 1970, Philips developed a home video cassette format specially made for a TV station in 1970 and available on the consumer market in 1972. Philips named this format \"Video Cassette Recording\" (although it is also referred to as \"N1500\", after the first recorder's model number).\n\nThe format was also supported by Grundig and Loewe. It used square cassettes and half-inch (1.3 cm) tape, mounted on coaxial reels, giving a recording time of one hour. The first model, available in the United Kingdom in 1972, was equipped with a simple timer that used rotary dials. At nearly £600, it was expensive and the format was relatively unsuccessful in the home market. This was followed by a digital timer version in 1975, the N1502. In 1977 a new, incompatible, long-play version (\"VCR-LP\") or N1700, which could use the same blank tapes, sold quite well to schools and colleges.\n\nThe Avco Cartrivision system, a combination television set and VCR from Cartridge Television Inc. that sold for US$1,350, was the first videocassette recorder to have pre-recorded tapes of popular movies available for rent. Like the Philips VCR format, the square Cartrivision cassette had the two reels of half-inch tape mounted on top of each other, but it could record up to 114 minutes, using an early form of video format that recorded every other video field and played it back three times.\n\nCassettes of major movies such as \"The Bridge on the River Kwai\" and \"Guess Who's Coming to Dinner\" were ordered via catalog at a retailer, delivered by parcel mail, and then returned to the retailer after viewing. Other cassettes on sports, travel, art, and how-to topics were available for purchase. An optional monochrome camera could be bought to make home videos. Cartrivision was first sold in June 1972, mainly through Sears, Macy's, and Montgomery Ward department stores in the United States. The system was abandoned thirteen months later after poor sales.\n\nVCR started gaining mass market traction in 1975. Six major firms were involved in the development of the VCR: RCA, JVC, AMPEX, Matsushita Electric / Panasonic, Sony, and Toshiba. Of these, the big winners in the growth of this industry were Japanese companies Matsushita Electric / Panasonic, JVC, and Sony, which developed more technically advanced machines with more accurate electronic timers and greater tape duration. The VCR started to become a mass market consumer product; by 1979 there were three competing technical standards using mutually incompatible tape cassettes.\n\nThe industry boomed in the 1980s as more and more customers bought VCRs. By 1982, 10% of households in the United Kingdom owned a VCR. The figure reached 30% in 1985 and by the end of the decade well over half of British homes owned a VCR.\n\nEarlier machines used mechanically-operated control switches; these were replaced by microswitches, solenoids and motor-driven mechanisms.\n\nThe two major standards were Sony's Betamax (also known as Betacord or just Beta), and JVC's VHS (Video Home System), which competed for sales in what became known as the format war.\n\nBetamax was first to market in November 1975, and was argued by many to be technically more sophisticated in recording quality, although many users did not perceive a visual difference. The first machines required an external timer, and could only record one hour, or two hours at lower quality (LP). The timer was later incorporated within the machine as a standard feature.\n\nThe rival VHS format, introduced in Japan by JVC in September 1976, introduced in the United States in July 1977 by RCA, had a longer two-hour recording time with a T-120 tape, or four hours in lower-quality \"long play\" mode (RCA SelectaVision models, introduced in September 1977).\n\nIn 1978 the majority of consumers in the U.K. chose to rent rather than purchase this new expensive home entertainment technology. JVC introduced the HR3300 model with E-30, E-60, (E-120 VHS-1 1976), E-180 minute cassette tapes with up to three hours=(VHS-2 1977)) recording time, a thinner 4 hour length (E240 tape VHS-3 1981) soon followed (E-400 VHS-5 last edition 1999).\n\nThe rental market was a contributing factor for acceptance of the VHS, for a variety of reasons. In those pre-digital days TV broadcasters could not offer the wide choice of a rental store, and tapes could be played as often as desired. Material was available on tape with violent or sexual scenes not available on broadcasts. Home video cameras allowed tapes to be recorded and played back.\n\nTwo hours and 4 hours recording times were considered enough for recording movies and sports. Although Sony later introduced L-500 (2 hour) and L-750 (3 hour) Betamax tapes in addition to the L-250 (1 hour) tape, the consumer market had swiftly moved toward the VHS system as a preferred choice. During the 1980s dual-speed (long play) models of both Beta and VHS recorders were introduced, allowing much longer recording times. The recording length on World Wide Standard on consumer video recorders (VHS) Was 8hrs with PAL colour encoding and 5hs-46mins with NTSC color encoding. The total recording length on The World Wide Standard On Professional Broadcasting (Betamax) was 3hrs 35mins on PAL colour configuration, and 5hrs on NTSC color configuration.\n\nLonger tapes became available a few years later, extending to 10hrs and then 12hrs LP on PAL (Europe) and 7hrs 50mins and then further extended with VHS-5 (Final edition of the Video Home System) in 1999 TO 8hrs 16mins on NTSC (North America, Japan). Betamax tape length was extended only using a DigiBetacam-40 or HDCAM cassette to 4hrs 20mins on PAL and 6hrs 30mins on NTSC models.\nWith the introduction of DVD recorders, combined VHS and DVD recorders were produced, allowing both types of media to be played, and transfer of tape material to DVD.\n\nA third format, Video 2000, or V2000 (also marketed as \"Video Compact Cassette\") was developed and introduced by Philips in 1978, sold only in Europe. Grundig developed and marketed their own models based on the V2000 format. Most V2000 models featured piezoelectric head positioning to dynamically adjust tape tracking. V2000 cassettes had two sides, and like the audio cassette could be flipped over halfway through their recording time, which gave them up to twice the recording length of VHS tapes. User switchable record-protect levers were used instead of the breakable lugs on VHS and Beta cassettes.\n\nThe half-inch tape used contained two parallel quarter-inch tracks, one for each side. It had a recording time of 4 hours per side, later extended to 8 hours per side on a few models. Machines had a 'real time' tape position counter with the information retained on the tape, so when tapes were loaded the position was known; this feature was only implemented on VHS recorders much later. V2000 became available in early 1979, later than its two rivals. The V2000 system did not sell well, and was discontinued in 1985.\n\nSome less successful consumer videocassette formats include:\n\nIn the early 1980s US film companies fought to suppress the VCR in the consumer market, citing concerns about copyright violations. In Congressional hearings, Motion Picture Association of America head Jack Valenti decried the \"savagery and the ravages of this machine\" and likened its effect on the film industry and the American public to the Boston strangler:\n\nIn the case \"Sony Corp. of America v. Universal City Studios, Inc.\", the Supreme Court of the United States ruled that the device was allowable for private use. Subsequently the film companies found that making and selling video recordings of their productions had become a major income source.\n\nThe video cassette recorder is sensitive to changes in temperature and humidity. If the machine (or tape) was moved from a cold to a hotter environment there could be condensation of moisture on the internal parts, such as the rotating video head drum. Some later models were equipped with a moisture detector which would prevent operation in this case, but it could not detect moisture on the surface of a tape.\n\nMagnetic tapes could be mechanically damaged when ejected from the machine due to moisture or other problems. Rubber drive belts and rollers hardened with age, causing malfunctions. Faults such as these damaged the tape irreversibly, a particular problem with irreplaceable tapes filmed by users. VHS tapes recorded in LP or EP/SLP mode were more sensitive to minor head misalignment over time or from machine to machine. Tapes recorded on a machine made before 1995 tend to not play well on newer machines due to slight changes in helical scanning head design.\n\nThe videocassette recorder remained in home use throughout the 1980s and 1990s, despite the advent of competing technologies such as LaserDisc (LD) and Video CD (VCD). While Laserdisc offered higher quality video and audio, the discs are heavy (weighing about one pound each), cumbersome, much more prone to damage if dropped or mishandled, and furthermore only home LD players, not recorders, were available. The VCD format found a niche with Asian film imports, but did not sell widely. Many Hollywood studios did not release feature films on VCD in North America because the VCD format had no means of preventing perfect copies being made on CD-R discs, which were already popular when the format was introduced. In an attempt to lower costs, manufacturers began dropping nonessential features from their VCR models. The built-in display was dropped in favor of on screen display for setup, programming, and status, and many buttons were eliminated from the VCR's front panel, their functions accessible only from the VCR's remote control.\n\nFrom about 2000 DVD became the first universally successful optical medium for playback of pre-recorded video, as it gradually overtook VHS to become the most popular consumer format. DVD recorders and other digital video recorders dropped rapidly in price, making the VCR obsolete. DVD rentals in the United States first exceeded those of VHS in June 2003. \n\nThe declining market combined with a US Federal Communications Commission mandate effective March 1, 2007, that all new TV tuners in the US be ATSC tuners encouraged major electronics makers, except Funai, JVC, and Panasonic, to end production of standalone units for the US market. Most new standalone VCRs in the US since then can only record from external baseband sources (usually composite video), including CECBs which (by NTIA mandate) all have composite outputs, as well as those ATSC tuners (including TVs) and cable boxes that come with composite outputs. However, JVC did ship one model of D-VHS deck with a built-in ATSC tuner, the HM-DT100U, but it remains extremely rare, and therefore expensive. In July 2016, Funai Electric, the last remaining manufacturer of VHS recorders, announced it would cease production of VHS recorders by the end of the month.\n\nAs a result of winning the format war over HD DVD, the new high definition optical disc format Blu-ray Disc was expected to replace the DVD format. However, with many homes still having a large supply of VHS tapes and with all Blu-ray players designed to play regular DVDs and CDs by default, some manufacturers began to make VCR/Blu-ray combo players. \n\nAlthough consumers have passed over videocassettes for home video playback in favor of DVDs since the early 2000s, VCRs still retained a significant share in home video recording during that decade. While the adoption of DVD players has been strong, DVD recorders for home theater use have been slow to pick up (although DVD recorder-writer drives became de facto standard equipment in personal computers in the mid-2000s).\n\nAlthough technologically superior to VHS, there were several main drawbacks with recordable DVDs that slowed their adoption. When standalone DVD recorders first appeared on the Japanese consumer market in 1999, these early units were very expensive, costing between $2500 and $4000 (USD). (As of early 2007, DVD recorders from notable brands are selling for US$200 or €150 and less, with even lower \"street prices\".) Different DVD recordable formats also caused confusion, as early units supported only DVD-RAM and DVD-R discs, but the more recent units can record to all major formats DVD-R, DVD-RW, DVD+R, DVD+RW, DVD-R DL and DVD+R DL. Some of these DVD formats are not rewritable, whereas videocassettes could be recorded over repeatedly (notwithstanding physical wear). Another important drawback of DVD recording is that one single-layer DVD is limited to around 120 minutes of recording if the quality is not to be significantly reduced, while VHS tapes are readily available up to 210 minutes (standard play) in NTSC areas and even 300 minutes in PAL areas. Dual layer DVDs, which increase the high quality recording mode to almost four hours, are increasingly available, but the cost of this medium was still relatively high compared to standard single-layer discs. Another factor was the increasing use of digital video file formats and online video sharing, skipping physical media entirely. For these reasons, DVD recorders never took hold of the video recording market like VCRs had.\n\nDigital video recorders have since come to dominate the market for home recording of television shows.\n\nOne of the problems faced with the use of video recorders was the exchange of recordings between PAL, SECAM and NTSC countries. Multi Standard video recorders and TV sets gradually overcame these incompatibility problems.\n\nThe U-matic machines were always made with Stereo, and both Betamax and VHS recorded the audio tracks using a fixed linear recording head. However the relatively slow tape speed of Beta and VHS was inadequate for good quality audio, and significantly limited the sound quality. Betamax's release of Beta Hi-Fi in the early 1980s was quickly followed by JVCs release of HiFi audio on VHS (model HR-D725U). Both systems delivered flat full-range frequency response (20 Hz to 20 kHz), excellent 70 dB signal-to-noise ratio (in consumer space, second only to the compact disc), dynamic range of 90 dB, and professional audio-grade channel separation (more than 70 dB). In VHS the incoming HiFi audio is frequency modulated (\"Audio FM\" or \"AFM\"), modulating the two stereo channels (L, R) on two different frequency-modulated carriers and recorded using the same high-bandwidth helical scanning technique used for the video signal. To avoid crosstalk and interference from the primary video carrier, VHS used \"depth multiplexing\", in which the modulated audio carrier pair was placed in the hitherto-unused frequency range between the luminance and the color carrier (below 1.6 MHz), and recorded first. Subsequently, the video head erases and re-records the video signal (combined luminance and color signal) over the same tape surface, but the video signal's higher center frequency results in a shallower magnetization of the tape, allowing both the video and residual AFM audio signal to coexist on tape. (PAL versions of Beta Hi-Fi use the same technique). During playback, VHS Hi-Fi recovers the depth-recorded AFM signal by subtracting the audio head's signal (which contains the AFM signal contaminated by a weak image of the video signal) from the video head's signal (which contains only the video signal), then demodulates the left and right audio channels from their respective frequency carriers. The end result of the complex process is audio of outstanding fidelity, which was uniformly solid across all tape-speeds (EP, LP or SP.)\n\nSuch devices were often described as \"HiFi audio\", \"Audio FM\" / \"AFM\" (FM standing for \"Frequency Modulation\"), and sometimes informally as \"Nicam\" VCRs (due to their use in recording the Nicam broadcast audio signal). They remained compatible with non-HiFi VCR players since the standard audio track was also recorded, and were at times used as an alternative to audio cassette tapes due to their exceptional bandwidth, frequency range, and extremely flat frequency response.\n\nThe 8 mm format always used the video portion of the tape for sound, with an FM carrier between the band space of the chrominance and luminance on the tape. 8 mm could be upgraded to Stereo, by adding an extra FM signal for Stereo difference.\n\nThe professional Betacam SP format of videocassette also used AFM on the higher-end \"BVW\"-series of Betacam SP deck models from Sony (such as the BVW-75) to offer 2 extra tracks of audio alongside the 2 standard Dolby C-encoded linear audio tracks for the format, for a total of 4 audio tracks. However, the 2 AFM tracks were accessible only on those decks equipped with AFM audio (like the BVW-75).\n\nDue to the path followed by the video and Hi-Fi audio heads being striped and discontinuous—unlike that of the linear audio track—head-switching is required to provide a continuous audio signal. While the video signal can easily hide the head-switching point in the invisible vertical retrace section of the signal, so that the exact switching point is not very important, the same is obviously not possible with a continuous audio signal that has no inaudible sections. Hi-Fi audio is thus dependent on a much more exact alignment of the head switching point than is required for non-HiFi VHS machines. Misalignments may lead to imperfect joining of the signal, resulting in low-pitched buzzing. The problem is known as \"head chatter\", and tends to increase as the audio heads wear down.\n\nThe sound quality of Hi-Fi VHS stereo is comparable to the quality of CD audio, particularly when recordings were made on high-end or professional VHS machines that have a manual audio recording level control. This high quality compared to other consumer audio recording formats such as compact cassette attracted the attention of amateur and hobbyist recording artists. Home recording enthusiasts occasionally recorded high quality stereo mixdowns and master recordings from multitrack audio tape onto consumer-level Hi-Fi VCRs. However, because the VHS Hi-Fi recording process is intertwined with the VCR's video-recording function, advanced editing functions such as audio-only or video-only dubbing are impossible. A short-lived alternative to the hifi feature for recording mixdowns of hobbyist audio-only projects was a PCM adaptor so that high-bandwidth digital video could use a grid of black-and-white dots on an analog video carrier to give pro-grade digital sounds though DAT tapes made this obsolete.\n\nIntroduced in 1983, Macrovision is a system that reduces the quality of recordings made from commercial video tapes, DVDs and pay-per-view broadcasts by adding random peaks of luminance to the video signal during vertical blanking. These confuse the automatic level adjustment of the recording VCR which causes the brightness of the picture to constantly change, rendering the recording unwatchable.\n\nWhen creating a copy-protected videocassette, the Macrovision-distorted signal is stored on the tape itself by special recording equipment. By contrast, on DVDs there is just a marker asking the player to produce such a distortion during playback. All standard DVD players include this protection and obey the marker, though unofficially many models can be modified or adjusted to disable it.\n\nAlso, the Macrovision protection system may fail to work on older VCR's made before 1986 and some high end decks built afterwards, usually due to the lack of an AGC system. Newer VHS and S-VHS machines (and DVD recorders) are susceptible to this signal; generally, machines of other tape formats are unaffected, such as all 3 Betamax variants. VCRs designated for \"professional\" usage typically have an adjustable AGC system, a specific \"Macrovision removing\" circuit, or Time Base Corrector (TBC) and can thus copy protected tapes with or without preserving the protection. Such VCRs are usually overpriced and sold exclusively to certified professionals (linear editing using the 9-Pin Protocol, TV stations etc.) via controlled distribution channels in order to prevent their being used by the general public (however, said professional VCRs can be purchased reasonably by consumers on the second-hand/used market, depending on the VCR's condition). Nowadays, most DVDs still have copyright protection, but certain DVDs do not have it, usually pornography and bootlegs. However, some DVDs, such as certain DVD sets, do not have the protection against VHS copying, possibly due to the VHS format no longer used as a major retail medium for home video.\n\nThe flying erase head is a feature that may be found in some high end home VCRs as well as some broadcast grade VCRs to cleanly edit the video.\n\nNormally, the tape is passed longitudinally through two fixed erase heads, one located just before the tape moves to the video head drum and the other right next to the audio/control head stack. Upon recording, the erase heads erase any old recording contained on the tape to prevent anything already recorded on it from interfering with what is being recorded.\n\nHowever, when trying to edit footage deck to deck, portions of the old recording's video may be between the erase head and video recording heads. This results in a faint rainbow-like noise at and briefly after the point of the cut as the old video recording missed by the fixed erase head is never completely erased as the new recording is printed.\n\nThe flying erase head is so-called because an erase head is mounted on the video head drum and rotates around in the same manner as the video heads. In the record mode, the erase head is active and erases the video precisely down to the recorded video fields. The flying erase head runs over the tape and the video heads record the signal virtually instantly after the flying erase head has passed.\n\nSince the erase head erases the old signal right before the video heads write onto the tape, there is no remnant of the old signal to cause visible distortion at and after the moment a cut is made, resulting in a clean edit. In addition, the ability of flying erase heads to erase old video off the tape right before recording new video on it allows the ability to perform insert editing, where new footage can be placed within an existing recording with clean cuts at the beginning and end of the edit.\n\nIn addition to the standard home VCR, a number of variants have been produced over the years. These include combined \"all-in-one\" devices such as the TV/VCR combo (a TV and VCR in one unit) and DVD/VCR units and even TV/VCR/DVD all-in-one units.\n\nDual-deck VCRs (marketed as \"double-decker\") have also been sold, albeit with less success.\n\nMost camcorders produced in the 20th century also feature an integrated VCR. Generally, they include neither a timer nor a TV tuner. Most of these use smaller format videocassettes, such as 8 mm, VHS-C, or MiniDV, although some early models supported full-size VHS and Betamax. In the 21st century, digital recording became the norm while videocassette tapes dwindled away gradually; tapeless camcorders use other storage media such as DVDs, or internal flash memory, hard drive, and SD card.\n\n\n\n"}
{"id": "30612927", "url": "https://en.wikipedia.org/wiki?curid=30612927", "title": "Vuzix", "text": "Vuzix\n\nVuzix is an American multinational technology firm headquartered in Rochester, New York. Founded in 1997 by Paul Travers, Vuzix is a supplier of wearable display technology, virtual reality and augmented reality. Vuzix manufactures and sells computer display devices and software. Vuzix personal display devices are used for mobile and immersive augmented reality applications, such as 3D gaming, manufacturing training, and military tactical equipment. On January 5, 2015, Intel acquired 30% of Vuzix's stock for $24.8 million.\n\nPaul Travers founded Vuzix in 1997 in Rochester, NY, under the name Interactive Imaging Systems, purchasing the assets of the virtual reality company Forte Technologies. The company started with products for the military and U.S. Defense, but then developed consumer virtual reality systems and was the first company to deliver consumer virtual reality. The Company has offices in New York, Japan, and the UK and is the current market leader for video eyewear. Forte was a pioneer during the mid-1990s developing immersive head mounted displays for Virtual Reality and video gaming applications.\n\nIn 1997, the Company was hired as a subcontractor to Raytheon, designing display electronics for a digital night vision weapon sight.\n\nIn 2000, the VFX1 Headgear was featured in a commemorative US Postal Service stamp collection celebrating the 1990s. The VFX1 was replaced by a higher resolution system dubbed the VFX3D in mid 2000.\n\nIn 2001, Vuzix launched its first consumer electronics product, the iCOM personal internet browser.\n\nIn 2005, Vuzix provided a custom high resolution handheld display system that created the 3D imagery for Hitachi's pavilion at the 2005 World’s Fair in Aichi, Japan. The Hitachi Pavilion allowed users to interact with computer generated models and dioramas of endangered species in a Mixed Reality ride.\n\nIn 2005, the company changed its name to Icuiti. Later that year, Icuiti launched its first product designed specifically for consumers, the V920 Video Eyewear.\n\nAlso in 2005, Icuiti was awarded its first military R&D contract to develop a high resolution monocular display device for viewing tactical maps and video. This development would lead to the Tac-Eye product line, which began rate production in 2009 and is currently used in many major military programs including the Battlefield Air Operations kit.\n\nA re-branding in 2007 changed the Company's name from Icuiti to Vuzix.\n\nIn 2010, Vuzix introduced the first production model see-through augmented reality glasses the STAR 1200. It was released in August 2011 for $4999.\n\nVuzix is currently under contract with DARPA to design and build a next generation heads up display for military ground personnel. The DARPA PCAS program is large R&D program with the goal of allowing the joint tactical air controller the ability to rapidly engage multiple, moving and simultaneous targets within his area of responsibility.\n\nIn January 2013, at CES, Vuzix Corporation demonstrated working models of its new M100 Smart Glasses, the official press event of CES. Rated as one of the five top gadgets expected at CES by CNBC and given the CES Innovations award, the Vuzix Smart Glasses M100, expected to ship in second half of 2013, will enable wearers to access data and content from an iOS or Android smart phone from the cloud and ‘hands free’. The m100 was officially released in December 2013 for $1000: double the original announced price.\n\nIn January 2015, Intel invested $25 million in the company, gaining 30% ownership of the company.\n\nIn August 2017, Vuzix and BlackBerry became partners to deliver smart glasses for the enterprise.\n\n\n\n"}
{"id": "363639", "url": "https://en.wikipedia.org/wiki?curid=363639", "title": "Webbing", "text": "Webbing\n\nWebbing is a strong fabric woven as a flat strip or tube of varying width and fibres, often used in place of rope. It is a versatile component used in climbing, slacklining, furniture manufacturing, automobile safety, auto racing, towing, parachuting, military apparel, load securing, and many other fields.\n\nOriginally made of cotton or flax, most modern webbing is made of synthetic fibers such as nylon, polypropylene or polyester. Webbing is also made from exceptionally high-strength material, such as Dyneema, and Kevlar. Webbing is both light and strong, with breaking strengths readily available in excess of 10,000 lb (44.4 kN)\n\nThere are two basic constructions of webbing. Flat webbing is a solid weave, with seatbelts and most backpack straps being common examples. Tubular webbing consists of a flattened tube, and is commonly used in climbing and industrial applications.\n\nIn rock climbing, nylon webbing is used in slings, runners, harnesses, anchor extensions and quickdraws. Webbing is used in many ways in hiking and camping gear including backpacks, straps, load adjusters and tent adjusters. There are two types of webbing: tubular and flat. The most popular webbing is one inch, but it is available in two and three inch widths.\n\nNarrower webbing is frequently looped through chockstones, which are jammed into cracks as safety anchors. In other cases, webbing is looped over rock outcroppings. Webbing is less likely to inch its way off the rock than tubular rope. Note that webbing construction is either utterly flat or flat-tubular; the latter tends to handle better but knots are more likely to jam.\n\nThe most popular knots in webbing are the water knot and the grapevine knot. The latter is stronger, but uses more webbing for the knot. It is customary to leave a few centimetres extending from the knot, and in many cases climbers tape the ends down onto the main loops.\n\nWebbing is also less expensive than rope of similar size, particularly kernmantle rope, which requires elaborate and expensive manufacturing. Unlike climbing rope, which is generally sold with recognizable brand names, webbing manufacture is typically generic. Climbing shops sell it off of a spool on a per yard or per foot basis.\n\nWebbing is cut with a hot wire as is nylon rope, which prevents fraying and unravelling. However, when webbing does fray and unravel, the result is less disastrous than with rope, providing a modest advantage. Webbing suffers the drawback of less elasticity than perlon rope, and it may be more difficult to handle with gloves or mittens on.\n\nSlacklines often use flat or tubular 1-inch (2.5 cm) webbing, or flat 2-inch (5 cm) webbing. Other widths are used, but are less common.\n\nWhite water rafting boats use tubular webbing for bow lines, stern lines, \"chicken lines\" (around the exterior perimeter of the boat), equipment tie down, or floor lacing for self-bailing rafts. Rafters call tubular webbing \"hoopie\" or \"hoopi\". Rafters also use camstraps with flat webbing for equipment tie down.\n\nLife preservers are also crafted using nylon or cotton webbing that conforms to federal standards and guidelines.\n\nSeat belts are an obvious example of webbings used in auto safety but there are myriad other uses. Nylon and polyester webbing are used a great deal in auto racing safety for a large variety of items. Racing harnesses restraining the driver have used nylon webbing for years, but since the death of Dale Earnhardt polyester webbing is becoming more popular due to its increased strength, and lower rate of elongation under load. The nylon commercial type 9 webbing generally used in racing harnesses stretches approximately 20 to 30 percent of its initial length at 2500 lb (11.1 kN) while polyester only stretches 5 to 15 percent. Window nets to prevent objects from entering the driver compartment are constructed of polypropylene webbing, as are helmet nets used to reduce side loads to the head in Sprint cars. The HANS device uses webbing tethers to attach the helmet to the collar, and the Hutchens device is made almost entirely of webbing.\n\nWebbing is used in couches and chairs as a base for the seating areas that is both strong and flexible. Webbing used as a support is often rubberised to improve resilience and add elasticity. Many types of outdoor furniture use little more than thin light webbing for the seating areas. Webbing is also used to reinforce joints and areas that tend to flex.\n\nMilitary webbing, otherwise known as Mil-Spec webbing, is typically made of strips of woven narrow fabrics of high tensile strength, such as Nylon, Kevlar, and Nomex. When these materials are used for parachute and ballooning applications, they must also conform to PIA (Parachute Industry Association) standards.\n\nMil-Spec webbing is used to make military belts, packs, pouches, and other forms of webbing equipment. The British Army adopted cotton webbing to replace leather after the Second Boer War although leather belts are still worn in more formal dress. The term is still used for a soldier's combat equipment, although cotton webbing has since been replaced with stronger materials. The webbing system used by the British Army today is known as Personal Load Carrying Equipment. Americans use All-purpose Lightweight Individual Carrying Equipment and MOLLE.\n\nGenerally, a soldier is also provided a rucksack to carry survival items for anywhere between 24 hours and a week. Webbing is designed so that if the pack is lost or abandoned, the soldier can survive on emergency rations, water and clothing, carried in it for up to 24 hours, or longer if the supplies are rationed.\n\nTypical contents of military webbing equipment include cooking equipment, 24 hours worth of rations, water, ammunition, first aid or survival supplies, cold weather/rain gear, anti-gas/CBRN gear and sheltering equipment (such as a tent quarter/shelter half, poles, rope, etc.). Items are generally stored in an ordered fashion in a combination of ammo and utility pouches. The ammo pouches are reserved for ammunition in the form of magazines, however it is common for soldiers to store their weapon cleaning kit in the same pouch.\n\nIn the first utility pouch, soldiers generally store their mess tins, pellet stoves, a lighter or waterproof matches, and enough rations to last 24 hours. In the second utility pouch is the army issue canteen and cup. The canteen can hold one liter of water. The mug has two folding metal handles which give the user a cool place to hold it while drinking a warm beverage. The third utility pouch contains a minor first aid kit, for minor cuts and scrapes, but not the first field dressing. This pouch may also contain various other items such as binoculars, a red light torch, a utility knife, or a compass. Other pouches can be attached for more storage capabilities. Examples include the Bowman radio pouch for the PRC 349 or the PRR pouch for the personal role radio. These pouches are more expensive due to their limited manufacture.\n\nMost military webbing systems incorporate a degree of modular construction with a yoke (shoulder harness), a belt and pouches specific to different loads, for example ammunition magazines may have dividers, special waterproofing, and/or tabs to help lift the magazines out. Different combinations of pouches can be used to suit the mission. In some better models the pouches are sewn directly onto a hip pad which prevents bouncing of the pouches and makes the webbing more comfortable.\n\nIt is unusual for western armies to fight while wearing a pack, and so prior to anticipated contact with the enemy the pack is usually stowed away from the forward edge of the battle area and webbing is used instead. Webbing belts are also used frequently by modern cadet and scout groups, as well as police and security forces.\n\nTie downs, tie straps, cargo straps, E-track straps, cargo hoist straps, tow ropes, winch straps, cargo nets, and dozens of other items are used by thousands of shipping and trucking companies every day. The transportation industry is perhaps the largest user of high strength webbing in the world.\n\nBelts, suspenders/braces, sandals and handbags/purses are woven from various forms of webbing. Corset-style back braces and other medical braces often incorporate straps made from webbing.\n\nDog collars, leashes, and dog harnesses frequently utilize webbing to make collars and leashes. While nylon and polyester are most common, polypropylene can also be used. These pet products are often sewn together with decorative ribbon or cotton fabric.\n\nWebbing is often outfitted with various forms of tie down hardware to extend its range of abilities (and create tie down straps). This hardware can take the form of:\n\n\nThere is also hardware associated with the various end fittings to attach them to a surface, such as footman's loops, brackets and E-track fittings.\n\n\n"}
{"id": "2189529", "url": "https://en.wikipedia.org/wiki?curid=2189529", "title": "Western Latin character sets (computing)", "text": "Western Latin character sets (computing)\n\nSeveral binary representations of character sets for common Western European languages are compared in this article. These encodings were designed for representation of Italian, Spanish, Portuguese, French, German, Dutch, English, Danish, Swedish, Norwegian, and Icelandic, which use the Latin alphabet, a few additional letters and ones with precomposed diacritics, some punctuation, and various symbols (including some Greek letters). Although they're called \"Western European\" many of these languages are spoken all over the world. Also, these character sets happen to support many other languages such as Malay, Swahili, and Classical Latin. \n\n\"This material is technically obsolete, having been functionally replaced by Unicode. However it continues to have historical interest.\"\n\nThe ISO-8859 series of 8-bit character sets encodes all Latin character sets used in Europe, albeit that the same code points have multiple uses that caused some difficulty. The arrival of Unicode, with a unique code point for every glyph, resolved these issues.\n\n\nThe earlier seven-bit U.S. ASCII encoding has characters sufficient to properly represent only English, Latin, and Swahili. It is missing some letters and letter-diacritic combinations used in other Latin-alphabet languages. However, since there was no other choice on most U.S.-supplied computer platforms, ASCII was unavoidable in most of the non-English-speaking world (seven-bit encoding was necessitated by the limitations of early computing networks). There was the ISO 646 group of encodings which replaced some of the symbols in ASCII with local characters, but space was very limited, and some of the symbols replaced were quite common in things like programming languages. \n\nAlthough seven-bit communication was the norm, most computers internally used eight-bit bytes, and they mostly put some form of characters in the 128 higher byte positions. In the early days most of these were system specific, but gradually a few standards were settled on.\n\nIn recent years, as storage and memory costs fall, the issues associated with multiple meanings of a given eight-bit code (there are seven ISO-Latin code sets alone) have ceased to be justified. All major operating systems have moved to Unicode as their main internal representation. However Windows does not support Unicode using their 8-bit character interfaces (by supporting UTF-8 in standard interfaces such as fopen), so many applications continue to be restricted to these legacy character sets.\n\nThe euro and its euro sign introduced significant pressure to support the euro sign (€), and most 8-bit character sets had to be adapted in some way.\n\nAll of these issues have been resolved as operating systems have been upgraded to support Unicode as standard, which encodes the euro sign at U+20AC (decimal 8364).\n\nCode points to U+007F are not shown in this table currently, as they are directly mapped in all character sets listed here. The ASCII coding standard defines the original specification for the mapping of the first 0-127 characters.\n\nThe table is arranged by Unicode code point. Character sets are referred to here by their IANA names in upper case.\n\n"}
{"id": "1352555", "url": "https://en.wikipedia.org/wiki?curid=1352555", "title": "Wheel and axle", "text": "Wheel and axle\n\nThe wheel and axle are one of six simple machines identified by Renaissance scientists drawing from Greek texts on technology. The wheel and axle consists of a wheel attached to a smaller axle so that these two parts rotate together in which a force is transferred from one to the other. A hinge or bearing supports the axle, allowing rotation. It can amplify force; a small force applied to the periphery of the large wheel can move a larger load attached to the axle.\n\nThe wheel and axle can be viewed as a version of the lever, with a drive force applied tangentially to the perimeter of the wheel and a load force applied to the axle, respectively, that are balanced around the hinge which is the fulcrum. The mechanical advantage of the wheel and axle is the ratio of the distances from the fulcrum to the applied loads, or what is the same thing the ratio of the diameter of the wheel and axle. A major application is in wheeled vehicles, in which the wheel and axle are used to reduce friction of the moving vehicle with the ground. Other examples of devices which use the wheel and axle are capstans, belt drives and gears.\n\nThe Halaf culture of 6500–5100 BCE has been credited with the earliest depiction of a wheeled vehicle, but this is doubtful as there is no evidence of Halafians using either wheeled vehicles or even pottery wheels.\n\nOne of the first applications of the wheel to appear was the potter's wheel, used by prehistoric cultures to fabricate clay pots. The earliest type, known as \"tournettes\" or \"slow wheels\", were known in the Middle East by the 5th millennium BCE (one of the earliest examples was discovered at Tepe Pardis, Iran, and dated to 5200–4700 BCE). These were made of stone or clay and secured to the ground with a peg in the center, but required effort to turn. True (freely-spinning) wheels were apparently in use in Mesopotamia by 3500 BCE and possibly as early as 4000 BCE, and the oldest surviving example, which was found in Ur (modern day Iraq), dates to approximately 3100 BCE.\n\nEvidence of wheeled vehicles appears in the second half of the 4th millennium BCE, near-simultaneously in Mesopotamia (Sumerian civilization), the Northern Caucasus (Maykop culture) and Central Europe (Cucuteni-Trypillian culture).\n\nAn early well-dated depiction of a wheeled vehicle (a wagon—four wheels, two axles) is on the Bronocice pot, a ca. 3635–3370 BCE ceramic vase, excavated in a Funnelbeaker culture settlement in southern Poland.\n\nThe oldest known example of a wooden wheel and its axle was found in 2002 at the Ljubljana Marshes some 20 km south of Ljubljana, the capital of Slovenia. According to radiocarbon dating, it is between 5,100 and 5,350 years old. The wheel was made of ash and oak and had a radius of 70 cm and the axle was 120 cm long and made of oak.\n\nIn Roman Egypt, Hero of Alexandria identified the wheel and axle as one of the simple machines used to lift weights. This is thought to have been in the form of the windlass which consists of a crank or pulley connected to a cylindrical barrel that provides mechanical advantage to wind up a rope and lift a load such as a bucket from the well.\n\nThe simple machine called a \"wheel and axle\" refers to the assembly formed by two disks, or cylinders, of different diameters mounted so they rotate together around the same axis.The thin rod which needs to be turned is called the axle and the wider object fixed to the axle, on which we apply force is called the wheel. A tangential force applied to the periphery of the large disk can exert a larger force on a load attached to the axle, achieving mechanical advantage. When used as the wheel of a wheeled vehicle the smaller cylinder is the axle of the wheel, but when used in a windlass, winch, and other similar applications (see medieval mining lift to right) the smaller cylinder may be separate from the axle mounted in the bearings. It cannot be used separately.\n\nAssuming the wheel and axle does not dissipate or store energy, that is it has no friction or elasticity, the power input by the force applied to the wheel must equal the power output at the axle. As the wheel and axle system rotates around its bearings, points on the circumference, or edge, of the wheel move faster than points on the circumference, or edge, of the axle. Therefore, a force applied to the edge of the wheel must be less than the force applied to the edge of the axle, because power is the product of force and velocity.\n\nLet \"a\" and \"b\" be the distances from the center of the bearing to the edges of the wheel \"A\" and the axle \"B.\" If the input force \"F\" is applied to the edge of the wheel \"A\" and the force \"F\" at the edge of the axle \"B\" is the output, then the ratio of the velocities of points \"A\" and \"B\" is given by \"a/b\", so the ratio of the output force to the input force, or mechanical advantage, is given by\n\nThe mechanical advantage of a simple machine like the wheel and axle is computed as the ratio of the resistance to the effort. The larger the ratio the greater the multiplication of force (torque) created or distance achieved. By varying the radii of the axle and/or wheel, any amount of mechanical advantage may be gained. In this manner, the size of the wheel may be increased to an inconvenient extent. In this case a system or combination of wheels (often toothed, that is, gears) are used. As a wheel and axle is a type of lever, a system of wheels and axles is like a compound lever.\n\nThe mechanical advantage of a wheel and axle with no friction is called the ideal mechanical advantage (IMA). It is calculated with the following formula:\n\nAll actual wheels have friction, which dissipates some of the power as heat. The actual mechanical advantage (AMA) of a wheel and axle is calculated with the following formula:\n\nwhere \n\nBasic Machines and How They Work, United States. Bureau of Naval Personnel, Courier Dover Publications 1965, pp. 3–1 and following preview online\n"}
