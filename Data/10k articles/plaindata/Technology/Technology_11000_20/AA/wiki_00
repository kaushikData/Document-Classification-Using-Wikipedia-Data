{"id": "4001353", "url": "https://en.wikipedia.org/wiki?curid=4001353", "title": "Adventure Activities Licensing Authority", "text": "Adventure Activities Licensing Authority\n\nThe Adventure Activities Licensing Authority (AALA) is the licensing authority for outdoor activity centres for young people in Great Britain. Since 2007 it has been part of the Health and Safety Executive (HSE), the government body charged with overseeing health and safety in all workplaces. AALA inspect and issue licences to providers. These licences give an assurance that, so far as is reasonably practicable, participants and employees can be 'safe'.\n\nOn October 15, 2010, Lord Young of Graffham, recommended that the AALA be abolished and the existing statutory licensing regime be replaced by a code of practice.\n\nThe AALA was created following the Lyme Bay canoeing tragedy in March, 1993 which involved a commercial organisation assuming responsibility for children's safety. A group of eight pupils and their teacher were accompanied by two instructors from an outdoor centre on the south coast of England. As a result of a series of errors, four of the teenagers drowned. The subsequent trial resulted in the prosecution of the parent company and the centre manager. The government initially resisted changing legislation until David Jamieson, the Member of Parliament for Plymouth Devonport, who represented the parents of the children who died, introduced a Private Member's Bill which in January 1995 became the Activity Centres (Young Persons’ Safety) Act 1995. In January 1995 an independent licensing authority, the AALA, was created to bring the act into reality.\n\nThe activities within the scope of the licensing scheme are:\n\n\nDespite many providers falling outside the legal remit of AALA, the standards are widely regarded as applying to any organisation providing outdoor activities and would probably, according to Marcus Baillie (the head of inspection services) be used as “the standard” in any court case. It is not clear if provision has declined across the country as a consequence of the Act, but risk analysis and management systems appear to have increased. It is fortunate that the head of inspection services and the inspectors all have extensive experience in the field of outdoor education and are typically well respected within the outdoor community. This adds credibility to AALA and, one hopes, to the field of outdoor education in general. It is difficult to predict the future of AALA and the consequences of this on the field. One possibility is the expansion of the remit to increase the range of activities covered. Another possibility is to widen the remit to apply to all providers regardless of the age groups with which they work. Further options might be considered for voluntary organisations. This currently looks very unlikely, primarily due to cost. There is also discussion about a non-statutory scheme for those providers who are outside the remit of the licensing scheme, but again cost of implementation is likely to prove prohibitive. There is, not surprisingly, some debate regarding whether AALA has achieved its original aims following the kayaking tragedy (Loynes, 1996). One study, which focused on sea kayaking, suggests that AALA has had almost entirely positive effects on both quality and quantity of provision in sea kayaking (Woolven, 2004). Empirical data on other activities are not yet available and it appears that no one is currently conducting research in this area.\n\nAlthough the AALA regulations appear to be working well and are typically well received they may have added force to the culture of over-protection placing, for example, an increased emphasis on safety and administration of risk assessments as opposed to a deeper discussion on educational practices.\n"}
{"id": "10518524", "url": "https://en.wikipedia.org/wiki?curid=10518524", "title": "Arlesey Bomb", "text": "Arlesey Bomb\n\nThe Arlesey Bomb is an angling weight developed by Richard Walker at the lake in Arlesey. Walker fished for perch in the lake, and very large perch could be caught in the deepest water. The Arlesey Bomb was developed to allow him to cast the long distances required. It is tear-shaped, with a loop at the top to attach the line. Its shape makes it aerodynamic to cast, but unlikely to snag on the river or lake bottom. The incorporation of a swivel also prevented the line getting twisted.\n"}
{"id": "35539399", "url": "https://en.wikipedia.org/wiki?curid=35539399", "title": "Blackmagic Design", "text": "Blackmagic Design\n\nBlackmagic Design is an Australian digital cinema company and manufacturer based in Port Melbourne, Victoria, Australia. It designs and manufactures broadcast and cinema hardware, most notably digital movie cameras, and also develops video editing software.\n\nThe company was founded in 2001 by Grant Petty and produced their first product in 2002, a capture card for macOS called DeckLink that was the first to offer uncompressed 10-bit video. The company later released new versions of the card and added color correction capabilities, support for Microsoft Windows, and full support for Adobe Premiere Pro and Microsoft DirectShow. In 2005 the company released several products, including the Multibridge family of PCIe bi-directional converters and the FrameLink family of DPX-based software. In 2006 the company released Blackmagic On-Air television production software. In 2009 the company acquired Da Vinci Systems, a company that had won Emmy Awards for film colouring and restoration equipment. In 2010 the company acquired Echolab, a manufacturer of vision mixers. In 2011, the company acquired Teranex, a developer of video processing products.\n\nAt the 2012 NAB Show Blackmagic announced their first Cinema Camera, soon after the company had acquired Cintel. In 2015 the company acquired eyeon Software, the original authors of Blackmagic Fusion, previously named eyeon Fusion. In 2016 the company acquired Fairlight.\n\nBlackmagic's first and main products have been broadcast video hardware, including live production switchers, real-time compositing processors, Cintel scanners, signal converters, and video monitors. The company then began producing similar products for the filmmaking industry, including cinema cameras and video monitors with integrated recording options.\n\n\n\nDaVinci Resolve was merged with Fusion from version 15.\n\n\n"}
{"id": "22254751", "url": "https://en.wikipedia.org/wiki?curid=22254751", "title": "Business Intelligence Competency Center", "text": "Business Intelligence Competency Center\n\nA Business Intelligence Competency Center (BICC) is a cross-functional organizational team that has defined tasks, roles, responsibilities and processes for supporting and promoting the effective use of Business Intelligence (BI) across an organization.\n\nGartner started advocating that companies need a BICC to develop and focus resources to be successful using business intelligence in 2001. Since then, the BICC concept has been further refined through practical implementations in organizations that have implemented BI and analytical software.\n\nIn practice, the term \"BICC\" is not well integrated into the nomenclature of business or public sector organizations and there are a large degree of variances in the organizational design for BICCs. Nevertheless, the popularity of the BICC concept has caused the creation of units that focus on ensuring the use of the information for decision-making from BI software and increasing the return on investment (ROI) of BI.\n\nA BICC coordinates the activities and resources to ensure that a fact-based approach to decision making is systematically implemented throughout an organization. It has responsibility for the governance structure for BI and analytical programs, projects, practices, software, and architecture. It is responsible for building the plans, priorities, infrastructure, and competencies that the organization needs to take forward-looking strategic decisions by using the BI and analytical software capabilities.\n\nA BICC’s influence transcends that of a typical business unit, playing a crucial central role in the organizational change and strategic process. Accordingly, the BICC’s purpose is to empower the entire organization to coordinate BI from all units. Through centralization, it \"…ensures that information and best practices are communicated and shared through the entire organization so that everyone can benefit from successes and lessons learned.\"\n\nThe BICC also plays an important organizational role facilitating interaction among the various cultures and units within the organization. Knowledge transfer, enhancement of analytic skills, coaching and training are central to the mandate of the BICC. A BICC should be pivotal in ensuring a high degree of information consumption and a ROI for BI.\n\nBusiness Intelligence Competency Centers in U.S. Healthcare\n\nNext to the U.S. government, the American healthcare industry generates the second largest amount of information every year. However, despite having complex information management needs, a KLAS report revealed that one-third of healthcare organizations do not have the appropriate business intelligence tools.\n\nSince finance and energy industries have successfully implemented business intelligence competency centers (BICCs) and have produced financial returns on their investment and accelerated decision-making speed, the healthcare industry is initiating use of BICCs. Creating a business intelligence competency center in healthcare involves prioritizing information needs, creating data governance structures, identifying data stewards to provide data quality assurance, establishing ongoing education programs, and defining predictive modeling, analytics, data warehouse, and cloud storage tools.\n\nSkills Needed\n\nInformation technology specialists in SQL design, relational databases, programming, reporting software, and analytics can provide the necessary technical information management skills. Data stewards, such as data analysts and scientists, understand the creation, capture, storage, and access processes needed to ensure high quality data.\n\nIn recent years knowledge-oriented shared service centres with primary focus on offering analytics and data mining as an internal service across the organization have emerged. These centres are often referred to as Analytics Competency Center (ACC), Analytics Center of Excellence, Analytics Service Center, Big Data CoC or Big Data Lab. It is predicated, that by the end of 2017 already a quarter of all large firms will have such a dedicated unit for data and analytics . In contrast to classic BICC these centres do not place emphasis on reporting, historical analysis and dashboards. While BICC usually create enterprise-wide data marts and warehouses to establish a foundation for trusted information, an ACC follows a more strategic objective. ACCs follow the strategic objective to transform the company towards a data driven company, build analytics expertise, formulate a data strategy, identify use cases for data mining, establish a manage a platform and drive the general adoption of analytics across the Organization. Usually, historically grown BICCs are transformed into an ACC, but also new formations of ACC can be found in practise.\n\n"}
{"id": "31677000", "url": "https://en.wikipedia.org/wiki?curid=31677000", "title": "California Green Chemistry Initiative", "text": "California Green Chemistry Initiative\n\nThe California Green Chemistry Initiative (CGCI) is a six-part initiative to reduce public and environmental exposure to toxins through improved knowledge and regulation of chemicals; two parts became statute in 2008. The other four parts were not passed, but are still on the agenda of the California Department of Toxic Substances Control green ribbon science panel discussions. The two parts of the California Green Chemistry Initiative that were passed are known as AB 1879 (Chapter 559, Statutes of 2008): Hazardous Materials and Toxic Substances Evaluation and Regulation and SB 509 (Chapter 560, Statutes of 2008): Toxic Information Clearinghouse. Implementation of CGCI has been delayed indefinitely beyond the January 1, 2011.\n\nGreen chemistry is the design of chemical products and processes that reduce or eliminate the use and generation of hazardous substances. Green chemistry is based upon twelve principles, identified in “Green Chemistry: Theory and Practice” and adopted by the US Environmental Protection Agency (EPA). It is an innovative technology which encourages the design of safer chemicals and products and minimizes the impact of wastes through increased energy efficiency, the design of chemical products that degrade after use and the use of renewable resources (instead of non-renewable fossil fuel such as petroleum, gas and coal). The Office of Pollution Prevention and Toxics (OPPT), created under the United States Pollution Prevention Act of 1990, promotes the use of chemistry for pollution prevention through voluntary, non-regulatory ' partnerships with academia, industry, other government agencies, and non-governmental organizations. The United States Environmental Protection Agency (EPA) promotes green chemistry as overseen by the OPPT. The California Green Chemistry Initiative moves beyond voluntary partnerships and voluntary information disclosure to require industry reporting and public disclosure.\n\nThe United States Environmental Protection Agency’s most important law to regulate the production, use and disposal of chemicals is the Toxic Substances Control Act of 1976 (TSCA). Over the years, TSCA has fallen behind the industry it is supposed to regulate and is an inadequate tool for providing the protection against today’s chemical risks. Green chemistry represents a major paradigm shift in industrial manufacturing as it is a proactive “cradle-to-cradle” approach that focuses environmental protection at the design stage of production processes.\n\nIn 2008, California governor Arnold Schwarzenegger signed two joined bills, AB 1879 and SB 507, which created California’s Green Chemistry Initiative (CGCI). AB 1879 increases regulatory authority over chemicals in consumer products. The law established an advisory panel of scientists, known as the green ribbon science panel, to guide research in chemical policy, create regulations for assessing alternatives, and set up an internet database of research on toxins. SB 509 was designed to ensure that information regarding the hazard traits, toxicological and environmental endpoints, and other vital data is available to the public, to businesses, and to regulators in a Toxics Information Clearinghouse. This legislation marks the biggest leap forward in California chemicals policy in nearly two decades and is intended to improve the health and safety of all Californians by providing the Department of Toxic Substances Control (DTSC) with the authority to control toxic substances in consumer products.\n\nThe bills were scheduled to go into regulatory affect January 1, 2011 with the adoption of the Green Chemistry Initiative. California has postponed the initiative, indefinitely, due to concerns raised by stakeholders and more specifically, controversial last minute changes in the final draft. The final or third draft contains substantial revisions, including scaled back manufacturer and retailer compliance requirements that were not well received by the environmental community. Assemblyman Mike Feur and several authors of AB 1879, assert that last minute changes by the California DTSC have drastically weakened the Green Chemistry Initiative and limited its scope. They are most concerned with the change to require the state to prove that a chemical is harmful before being regulated, mirroring what is currently required at the Federal level by TSCA. The original draft advocated a precautionary principle, or “cradle-to-cradle” approach. Environmentalists fear that CGCI will not remove chemicals off the shelves, but instead will create “paralysis by analysis” as companies litigate against the DTSC over unfavorable decisions.\n\nSociety historically managed its industrial and municipal wastes by disposal or incineration. Chemical regulation occurs only after a product is identified as hazardous. This problem-specific approach has led to the release of thousands of potentially harmful chemicals in our environment. Chemical regulation is a continuous game of catch up, in which banned chemicals are replaced with new chemicals that may be just as or more toxic. Many environmental laws are still based on the industrial production model of cradle-to-grave. The term “cradle-to-grave” is used to describe and assess the life-cycle of products, from raw material extraction through materials processing, manufacture, distribution, use and disposal. This traditional approach to chemicals management has serious environmental drawbacks because it does not consider what happens to a product after it is disposed of. The Resource Conservation and Recovery Act (RCRA) of 1976, exemplifies a cradle-to-grave management approach of hazardous waste. RCRA has been largely ineffective because its emphasis is on dealing with waste after it has been created; meanwhile emphasis on waste reduction is minimal. Waste does not disappear, it is simply transported elsewhere. Costly and burdensome hazardous waste disposal in the US has encouraged the exportation of hazardous waste to poor counties and developing nations willing to accept the waste for a fee.\n\nThe Green Chemistry initiative instead employs a cradle-to-cradle approach, representing a major paradigm shift in environmental policy and provides a proactive solution to toxic waste. The Earth’s capacity to accept toxic waste is practically nonexistent. The disposal of hazardous wastes is not the root problem but rather, the root symptom. The critical issue is the creation of toxic wastes. Requiring manufacturers to consider chemical exposure during manufacturing, throughout product use and after disposal, encourages the production of safer products.\n\nBy the time we find a product on a market shelf, 90% of the resources used to create that product was regarded as waste. This accounts for about 136 pounds of resources a week consumed by the average American and 2,000 pounds of waste support that consumption. As the population grows and the economy expands more and more products will be created, consumed, and disposed. Many negative externalities are related to the environmental consequences of production and use, including air pollution, anthropogenic climate change and water pollution. Under the current cycle of production, toxic chemical byproducts will continue to be produced and unleashed on our environment. It is important to carefully consider how toxic wastes are created in order to forgo the possibility of a world that is unsuitable for human life.\n\nOne of the biggest failures in market transactions is the imbalance of information that is provided to consumer via producer. “Information asymmetry” is an economic concept that is used to explain this failure: it deals with the study of decisions in transactions where one party has more or better information than the other. Due to a lack of information transparency, the public may lack vital information about the health and safety of products found on supermarket shelves. This lack of information may have led to a reversed purchasing decision. Yet without such labeling, consumers must make assumptions based on things like price or expertise. For example, one apple juice brand may be assumed healthier because it cost more and because the brand is advertised as “healthy” and “recommended by mothers”. Further, it may be assumed that the product is safe for consumption if it is sitting on a grocery store shelf and probably would not be approved by the government if it contained harmful chemicals. Assumptions such as these could inform a typical purchasing decision, despite their inaccuracy. Perhaps given more information, the same brand of apple juice would be less desirable if information on unhealthy preservatives, additives or pesticide residues was easily obtained. To make market transactions more efficient, the government could force more accurate labeling about products, laws could require companies to be more transparent, and the government could require that advertising be less persuasive and more informative. The Green Chemistry Initiative of California would address transparency issues by creating a public chemical inventory and requiring more stringent regulation of chemicals that may be toxic. The CGCI Draft Report suggests a green labeling system to identify consumer products with ingredients harmful to human health and the environment.\n\nThe United States is the world leader in chemicals manufacturing. As a multibillion-dollar industry, the chemical industry has a leading role in the US economy and because of this, a high level of influence in federal decision-making. Central to the modern world economy, it converts raw materials (oil, natural gas, air, water, metals, and minerals) into more than 70,000 different products. The chemical industry—producers of chemicals, household cleansers, plastics, rubber, paints and explosives, keeps a watchful eye on issues including environmental and health policy, taxes and trade. The industry is often the target of environmental groups, which charge that chemicals and chemical waste are polluting the air and water supply. And like most industries with pollution problems, chemical manufacturers oppose meddlesome government regulations that make it more difficult and expensive for them to do business. So do most Republicans, which is why this industry gives nearly three-fourths of its campaign contributions to the GOP. In addition to campaign contributions to elected officials and candidates, companies, labor unions, and other organizations spend billions of dollars each year to lobby Congress and federal agencies. Some special interests retain lobbying firms, many of them located along Washington's legendary K Street; others have lobbyists working in-house.\n\nAccording to website \"Opensecrets,\" the total number of clients lobbying for the chemical industry in 2010 was 143, which is the highest number in history. The first group on this list, American Chemistry Council spent $8,130,000 lobbying last year and Crop America, which comes second, spent $2,291,859 lobbying last year, FMC Corporation spent $1,230,000 and Koch Industries spent $8,070,000. The Chemical Industry wants limited testing of chemicals, more lengthy and costly studies of chemicals already proven to be dangerous, and an assumption that we are only exposed to one chemical at a time, and from one source at a time.\n\nAccording to \"Safer Chemicals, Healthy Families,\" a broad coalition of groups, including major environmental organizations like the Natural Resources Defense Council and the Environmental Defense Fund, health organizations like the Learning Disabilities Association, Breast Cancer Fund, and the Autism Society of America, health professionals and providers like the American Nurses Association, Planned Parenthood Federation of America, and the Mt. Sinai Children's Environmental Health Center, and concerned parents groups like MomsRising: there is growing national momentum and pressure to change the Toxic Substances Control Act (TSCA), our federal system for overseeing chemical safety, which has not been updated in thirty-five years. Polling data indicates overwhelming support for chemical regulation nationwide. According to polling data conducted by the Mellman Group, 84% say that \"tightening controls\" on chemical regulation is important, with 50% of those calling it \"very important.” Public Health Advocates want public disclosure of safety information for all chemicals in use, prompt action to phase out or reduce the most dangerous chemicals, deciding safety based on real world exposure to all sources of toxic chemicals.\n\nIn 2008, California Governor Arnold Schwarzenegger signed two state bills authorizing the state to identify toxic chemicals in industry and consumer products and analyze alternatives. AB 1879, written by Assemblyman Mike Feur, a Los Angeles Democrat, requires the state Department of Toxic Substances Control to assess chemicals and prioritize the most toxic for possible restrictions or bans. The environmental policy council, made up of heads of all state environmental protection agency boards and departments will oversee the program. SB 509, by Senator Joe Simitian, a Palo Alto Democrat, creates an online toxics information clearinghouse with information about the hazards of thousands of chemicals used in California. These bills are intended to put an end to chemical-by-chemical bans and remove harmful products at the design stage. The regulations are expected to motivate manufacturers of consumer products containing chemicals of concern to seek safer alternatives.\n\nSupporters of the bill include the California Association of Professional Scientists, the Chemical Industry Council of California, DuPont, BIOCOM, Grocery Manufacturers Association, the Breast Cancer Fund, Catholic Healthcare West, in addition to a broad array of environmental groups such as the Coalition for Clean Air, the Environmental Defense Fund, the Natural Resources Defense Council. The American Electronics Association (AEA) and Ford spoke in opposition to the bill, each requesting an exemption from its provisions. Also opposing were environmental justice advocates who indicated the bill did not go far enough. Meanwhile, large trade associations such as Consumer Specialty Products Association, Western States Petroleum Association, American Chemistry Council, CA Manufacturers and Technology Association, and CA Chamber of Commerce officially withdrew opposition to the measures.\n\nDue to outdated and inefficient or otherwise voluntary chemical regulation at the Federal level, the State of California has decided to take regulation into its own hands and develop stricter, environmentally-informed methodologies for dealing with the production of toxic wastes. California's economy is the largest of any state in the US, and is the eighth largest economy in the world. This position gives California an advantage when it comes to environmental standards: the impact of chemical regulation statewide can have a broader impact nationwide if manufacturers desire to stay competitive in California’s market. The Green Chemistry Initiative forces statewide industries to comply with greener standards of production, which may spark innovation on a wider basis.\n\nThe Green Chemistry initiative aims to regulate the creation and use of materials hazardous to human health and the environment by encouraging innovative design and manufacturing, and ultimately safer consumer product alternatives. To develop the regulatory framework, DTSC held a number of stakeholder and public workshops and invited direct public participation in the drafting of regulations on a wiki website. DTSC reportedly received over 57,000 comments and over 800 regulatory suggestions. Regulatory suggestions included industry assessments of risk and safety, alternative chemicals and life-cycle assessments and mandatory industry reporting, full public disclosure of substances contained in products, a green labelling program that would inform consumers of the potential health and environmental impacts of the chemicals contained in products and a mandated surcharge on chemicals and products to support a fund to address environmental problems. In December 2008, DTSC announced six policy recommendations for the Green Chemistry Initiative. In brief, those recommendations are: \n\nTwo of the six recommendations from this report were adopted: AB 1879 requires the DTSC to implement regulations to identify and prioritize chemicals of concern, evaluate alternatives, and specify regulatory responses where chemicals are found in products. SB 509 requires an online, public toxics information clearinghouse that includes science-based information on the toxicity and hazard traits of chemicals used in daily life. Essentially the recommended policy methods include authority tools that would regulate the approval on new chemicals in a more cautious manner as well as mandate the decimation of information, as provided by manufacturers to the public; innovation would be encouraged under this paradigm to replace harmful chemicals with greener alternatives and the California government would fund programs to help industries produce greener chemicals. Secondly, capacity or learning tools would be provided to the public in the form of the online database, giving the tools so that they have better ability to make market decisions that reflect their interests.\n\nEnvironmentalists say the amended regulations won't remove toxic products from the shelves and will create \"paralysis by analysis,\" as industries can litigate against DTSC over unfavorable department decisions. Activists say California was poised to lead the way on toxics regulation but now is faced with potentially one of the weakest chemical-regulatory mechanisms in the nation. According to CHANGE (Californians for a Healthy & Green Economy), the revised regulation is a betrayal of the Green Chemistry promise and ignores two years of public input, while caving to backroom industry lobbying. Furthermore, it is a betrayal to public interest groups, businesses, and residents of California and legislators who supported the intent of this bill, to protect Californians and spur a healthy, innovative green economy. Environmentalists say the toxics department gutted the initiative at the behest of the chemical industry, and then put out the changes for public comment during a 15-day period just before Thanksgiving. This was a violation of the law requiring a 45-day public comment period when a substantial reworking of state regulations is proposed. The new Director of California's Department of Toxic Substance Control, Debbie Raphael, announced that mid-October 2011 is the new target date for new draft regulations to implement California's Green Chemistry Law and new draft guidelines were issued October 31, 2011. The public comment period for the latest version of the draft regulations ends December 30, 2011.\n\nImplementation of CGCI has been delayed indefinitely beyond the January 1, 2011 deadline due to issues that arose after public review of the third draft. The third draft, which was made public December 2010, contains substantial revisions, including scaled back manufacturer and retailer compliance requirements that were not well received by the environmental community. DTSCs newest draft has made the following changes: \n"}
{"id": "3561034", "url": "https://en.wikipedia.org/wiki?curid=3561034", "title": "Color magazine (lighting)", "text": "Color magazine (lighting)\n\nThe term boomerang is also used to describe a color magazine.\n\nColor magazines are now becoming rarer with the widespread availability of programmable-colour LED lighting.\n"}
{"id": "19342460", "url": "https://en.wikipedia.org/wiki?curid=19342460", "title": "Cryo-adsorption", "text": "Cryo-adsorption\n\nCryo-adsorption is a method used for hydrogen storage where gaseous hydrogen at cryogenic temperatures (150—60 K) is physically adsorbed on porous material, mostly activated carbon. The achievable storage density is between liquid-hydrogen (LH) storage systems and compressed-hydrogen (CGH) storage systems.\n\n\n"}
{"id": "19349845", "url": "https://en.wikipedia.org/wiki?curid=19349845", "title": "Cryopreservation", "text": "Cryopreservation\n\nCryo-preservation or cryo-conservation is a process where organelles, cells, tissues, extracellular matrix, organs or any other biological constructs susceptible to damage caused by unregulated chemical kinetics are preserved by cooling to very low temperatures (typically −80 °C using solid carbon dioxide or −196 °C using liquid nitrogen). At low enough temperatures, any enzymatic or chemical activity which might cause damage to the biological material in question is effectively stopped. Cryopreservation methods seek to reach low temperatures without causing additional damage caused by the formation of ice crystals during freezing. Traditional cryopreservation has relied on coating the material to be frozen with a class of molecules termed cryoprotectants. New methods are constantly being investigated due to the inherent toxicity of many cryoprotectants. By default it should be considered that cryopreservation alters or compromises the structure and function of cells unless it is proven otherwise for a particular cell population. Cryoconservation of animal genetic resources is the process in which animal genetic material is collected and stored with the intention of conservation of the breed.\n\nWater-bears (\"Tardigrada\"), microscopic multicellular organisms, can survive freezing by replacing most of their internal water with the sugar trehalose, preventing it from crystallization that otherwise damages cell membranes. Mixtures of solutes can achieve similar effects. Some solutes, including salts, have the disadvantage that they may be toxic at intense concentrations. In addition to the water-bear, wood frogs can tolerate the freezing of their blood and other tissues. Urea is accumulated in tissues in preparation for overwintering, and liver glycogen is converted in large quantities to glucose in response to internal ice formation. Both urea and glucose act as \"cryoprotectants\" to limit the amount of ice that forms and to reduce osmotic shrinkage of cells. Frogs can survive many freeze/thaw events during winter if no more than about 65% of the total body water freezes. Research exploring the phenomenon of \"freezing frogs\" has been performed primarily by the Canadian researcher, Dr. Kenneth B. Storey.\n\nFreeze tolerance, in which organisms survive the winter by freezing solid and ceasing life functions, is known in a few vertebrates: five species of frogs (\"Rana sylvatica\", \"Pseudacris triseriata\", \"Hyla crucifer\", \"Hyla versicolor\", \"Hyla chrysoscelis\"), one of salamanders (\"Hynobius keyserlingi\"), one of snakes (\"Thamnophis sirtalis\") and three of turtles (\"Chrysemys picta\", \"Terrapene carolina\", \"Terrapene ornata\"). Snapping turtles \"Chelydra serpentina\" and wall lizards \"Podarcis muralis\" also survive nominal freezing but it has not been established to be adaptive for overwintering. In the case of \"Rana sylvatica\" one cryopreservant is ordinary glucose, which increases in concentration by approximately 19 mmol/l when the frogs are cooled slowly.\n\nOne of the most important early theoreticians of cryopreservation was James Lovelock. He suggested that damage to red blood cells during freezing was due to osmotic stress. During the early 1950s, Lovelock had also suggested that increasing salt concentrations in a cell as it dehydrates to lose water to the external ice might cause damage to the cell. In the mid-1950s, he experimented with the cryopreservation of rodents, determining that hamsters could be frozen with 60% of the water in the brain crystallized into ice with no adverse effects. Other organs were shown to be susceptible to damage.\n\nCryopreservation was applied to humans beginning in 1954 with three pregnancies resulting from the insemination of previously frozen sperm. Fowl sperm was cryopreserved in 1957 by a team of scientists in the UK directed by Christopher Polge. However, the rapid immersion of the samples in liquid nitrogen did not, for certain samples – such as some types of embryos, bone marrow and stem cells – produce the necessary viability to make them usable after thawing. Increased understanding of the mechanism of freezing injury to cells emphasised the importance of controlled or slow cooling to obtain maximum survival on thawing of the living cells. A controlled-rate cooling process, allowing biological samples to equilibrate to optimal physical parameters osmotically in a cryoprotectant (a form of anti-freeze) before cooling in a predetermined, controlled way proved necessary. The ability of cryoprotectants, in the early cases glycerol, to protect cells from freezing injury was discovered accidentally. Freezing injury has two aspects: direct damage from the ice crystals and secondary damage caused by the increase in concentration of solutes as progressively more ice is formed. During 1963, Peter Mazur, at Oak Ridge National Laboratory in the U.S., demonstrated that lethal intracellular freezing could be avoided if cooling was slow enough to permit sufficient water to leave the cell during progressive freezing of the extracellular fluid. That rate differs between cells of differing size and water permeability: a typical cooling rate around 1 °C/minute is appropriate for many mammalian cells after treatment with cryoprotectants such as glycerol or dimethyl sulphoxide, but the rate is not a universal optimum.\n\nStorage at very low temperatures is presumed to provide an indefinite longevity to cells, although the actual effective life is rather difficult to prove. Researchers experimenting with dried seeds found that there was noticeable variability of deterioration when samples were kept at different temperatures – even ultra-cold temperatures. Temperatures less than the glass transition point (Tg) of polyol's water solutions, around , seem to be accepted as the range where biological activity very substantially slows, and , the boiling point of liquid nitrogen, is the preferred temperature for storing important specimens. While refrigerators, freezers and extra-cold freezers are used for many items, generally the ultra-cold of liquid nitrogen is required for successful preservation of the more complex biological structures to virtually stop all biological activity.\n\nPhenomena which can cause damage to cells during cryopreservation mainly occur during the freezing stage, and include: solution effects, extracellular ice formation, dehydration and intracellular ice formation. Many of these effects can be reduced by cryoprotectants.\nOnce the preserved material has become frozen, it is relatively safe from further damage. However, estimates based on the accumulation of radiation-induced DNA damage during cryonic storage have suggested a maximum storage period of 1000 years.\n\n\n\n\n\nThe main techniques to prevent cryopreservation damages are a well established combination of \"controlled rate and slow freezing\" and a newer flash-freezing process known as \"vitrification\".\n\n\"Controlled-rate and slow freezing\", also known as \"slow programmable freezing (SPF)\", is a set of well established techniques developed during the early 1970s which enabled the first human embryo frozen birth Zoe Leyland during 1984. Since then, machines that freeze biological samples using programmable sequences, or controlled rates, have been used all over the world for human, animal and cell biology – \"freezing down\" a sample to better preserve it for eventual thawing, before it is frozen, or cryopreserved, in liquid nitrogen. Such machines are used for freezing oocytes, skin, blood products, embryo, sperm, stem cells and general tissue preservation in hospitals, veterinary practices and research laboratories around the world. As an example, the number of live births from frozen embryos 'slow frozen' is estimated at some 300,000 to 400,000 or 20% of the estimated 3 million in vitro fertilisation (IVF) births.\n\nLethal intracellular freezing can be avoided if cooling is slow enough to permit sufficient water to leave the cell during progressive freezing of the extracellular fluid. To minimize the growth of extracellular ice crystal growth and recrystallization, biomaterials such as alginates, polyvinyl alcohol or chitosan can be used to impede ice crystal growth along with traditional small molecule cryoprotectants. That rate differs between cells of differing size and water permeability: a typical cooling rate of about 1 °C/minute is appropriate for many mammalian cells after treatment with cryoprotectants such as glycerol or dimethyl sulfoxide, but the rate is not a universal optimum. The 1 °C / minute rate can be achieved by using devices such as a rate-controlled freezer or a benchtop portable freezing container.\n\nSeveral independent studies have provided evidence that frozen embryos stored using slow-freezing techniques may in some ways be 'better' than fresh in IVF. The studies indicate that using frozen embryos and eggs rather than fresh embryos and eggs reduced the risk of stillbirth and premature delivery though the exact reasons are still being explored.\n\nResearchers Greg Fahy and William F. Rall helped to introduce vitrification to reproductive cryopreservation in the mid-1980s. As of 2000, researchers claim vitrification provides the benefits of cryopreservation without damage due to ice crystal formation. The situation became more complex with the development of tissue engineering as both cells and biomaterials need to remain ice-free to preserve high cell viability and functions, integrity of constructs and structure of biomaterials. Vitrification of tissue engineered constructs was first reported by Lilia Kuleshova, who also was the first scientist to achieve vitrification of woman’s eggs (oocytes), which resulted in live birth in 1999. For clinical cryopreservation, vitrification usually requires the addition of cryoprotectants prior to cooling. The cryoprotectants act like antifreeze: they decrease the freezing temperature. They also increase the viscosity. Instead of crystallizing, the syrupy solution becomes an amorphous ice—it \"vitrifies\". Rather than a phase change from liquid to solid by crystallization, the amorphous state is like a \"solid liquid\", and the transformation is over a small temperature range described as the \"glass transition\" temperature.\n\nVitrification of water is promoted by rapid cooling, and can be achieved without cryoprotectants by an extremely rapid decrease of temperature (megakelvins per second). The rate that is required to attain glassy state in pure water was considered to be impossible until 2005.\n\nTwo conditions usually required to allow vitrification are an increase of the viscosity and a decrease of the freezing temperature. Many solutes do both, but larger molecules generally have a larger effect, particularly on viscosity. Rapid cooling also promotes vitrification.\n\nFor established methods of cryopreservation, the solute must penetrate the cell membrane in order to achieve increased viscosity and decrease freezing temperature inside the cell. Sugars do not readily permeate through the membrane. Those solutes that do, such as dimethyl sulfoxide, a common cryoprotectant, are often toxic in intense concentration. One of the difficult compromises of vitrifying cryopreservation concerns limiting the damage produced by the cryoprotectant itself due to cryoprotectant toxicity. Mixtures of cryoprotectants and the use of ice blockers have enabled the Twenty-First Century Medicine company to vitrify a rabbit kidney to −135 °C with their proprietary vitrification mixture. Upon rewarming, the kidney was transplanted successfully into a rabbit, with complete functionality and viability, able to sustain the rabbit indefinitely as the sole functioning kidney.\n\nGenerally, cryopreservation is easier for thin samples and small clumps of individual cells, because these can be cooled more quickly and so require lesser doses of toxic cryoprotectants. Therefore, cryopreservation of human livers and hearts for storage and transplant is still impractical.\n\nNevertheless, suitable combinations of cryoprotectants and regimes of cooling and rinsing during warming often allow the successful cryopreservation of biological materials, particularly cell suspensions or thin tissue samples. Examples include:\n\nAdditionally, efforts are underway to preserve humans cryogenically, known as cryonics. For such efforts either the brain within the head or the entire body may experience the above process. Cryonics is in a different category from the aforementioned examples, however: while countless cryopreserved cells, vaccines, tissue and other biologial samples have been thawed and used successfully, this has not yet been the case at all for cryopreserved brains or bodies. At issue are the criteria for defining \"success\". Proponents of cryonics claim that cryopreservation using present technology, particularly vitrification of the brain, may be sufficient to preserve people in an \"information theoretic\" sense so that they could be revived and made whole by hypothetical vastly advanced future technology. Not only is there no guarantee of its success, many people argue that human cryopreservation is unethical. According to certain views of the mind body problem, some philosophers believe that the mind, which contains thoughts, memories, and personality, is separate from the brain. When someone dies, their mind leaves the body. If a cryopreserved patient gets successfully resuscitated, no one knows if they would be the same person that they once were or if they would be an empty shell of the memory of who they once were. Right now scientists are trying to see if transplanting cryopreserved human organs for transplantation is viable, if so this would be a major step forward for the possibility of reviving a cryopreserved human.\n\nCryopreservation for embryos is used for embryo storage, e.g., when in vitro fertilization (IVF) has resulted in more embryos than is currently needed.\n\nPregnancies have been reported from embryos stored for 16 years. Many studies have evaluated the children born from frozen embryos, or “frosties”. The result has been uniformly positive with no increase in birth defects or development abnormalities. A study of more than 11,000 cryopreserved human embryos showed no significant effect of storage time on post-thaw survival for IVF or oocyte donation cycles, or for embryos frozen at the pronuclear or cleavage stages. Additionally, the duration of storage did not have any significant effect on clinical pregnancy, miscarriage, implantation, or live birth rate, whether from IVF or oocyte donation cycles. Rather, oocyte age, survival proportion, and number of transferred embryos are predictors of pregnancy outcome.\n\nCryopreservation of ovarian tissue is of interest to women who want to preserve their reproductive function beyond the natural limit, or whose reproductive potential is threatened by cancer therapy, for example in hematologic malignancies or breast cancer. The procedure is to take a part of the ovary and perform slow freezing before storing it in liquid nitrogen whilst therapy is undertaken. Tissue can then be thawed and implanted near the fallopian, either orthotopic (on the natural location) or heterotopic (on the abdominal wall), where it starts to produce new eggs, allowing normal conception to occur. The ovarian tissue may also be transplanted into mice that are immunocompromised (SCID mice) to avoid graft rejection, and tissue can be harvested later when mature follicles have developed.\n\nHuman oocyte cryopreservation is a new technology in which a woman’s eggs (oocytes) are extracted, frozen and stored. Later, when she is ready to become pregnant, the eggs can be thawed, fertilized, and transferred to the uterus as embryos.\nSince 1999, when the birth of the first baby from an embryo derived from vitrified-warmed woman’s eggs was reported by Kuleshova and co-workers in the journal of Human Reproduction, this concept has been recognized and widespread. This break-through in achieving vitrification of woman’s oocytes made an important advance in our knowledge and practice of the IVF process, as clinical pregnancy rate is four times higher after oocyte vitrification than after slow freezing. Oocyte vitrification is vital for preservation fertility in young oncology patients and for individuals undergoing IVF who object, either for religious or ethical reasons, to the practice of freezing embryos.\n\nSemen can be used successfully almost indefinitely after cryopreservation. The longest reported successful storage is 22 years. It can be used for sperm donation where the recipient wants the treatment in a different time or place, or as a means of preserving fertility for men undergoing vasectomy or treatments that may compromise their fertility, such as chemotherapy, radiation therapy or surgery.\n\nCryopreservation of immature testicular tissue is a developing method to avail reproduction to young boys who need to have gonadotoxic therapy. Animal data are promising, since healthy offsprings have been obtained after transplantation of frozen testicular cell suspensions or tissue pieces. However, none of the fertility restoration options from frozen tissue, i.e. cell suspension transplantation, tissue grafting and in vitro maturation (IVM) has proved efficient and safe in humans as yet.\n\nCryopreservation of whole moss plants, especially Physcomitrella patens, has been developed by Ralf Reski and coworkers and is performed at the International Moss Stock Center. This biobank collects, preserves, and distributes moss mutants and moss ecotypes.\n\nMSCs, when transfused immediately within a few hours post-thawing, may show reduced function or show decreased efficacy in treating diseases as compared to those MSCs which are in log phase of cell growth (fresh). As a result, cryopreserved MSCs should be brought back into log phase of cell growth in \"in vitro\" culture before these are administered for clinical trials or experimental therapies. Re-culturing of MSCs will help in recovering from the shock the cells get during freezing and thawing. Various clinical trials on MSCs have failed which used cryopreserved products immediately post-thaw as compared to those clinical trials which used fresh MSCs.\n\nBacteria and fungi can be kept short-term (months to about a year, depending) refrigerated, however, cell division and metabolism is not completely arrested and thus is not an optimal option for long-term storage (years) or to preserve cultures genetically or phenotypically, as cell divisions can lead to mutations or sub-culturing can cause phenotypic changes. A preferred option, species-dependent, is cryopreservation. Nematode worms are the only multicellular eukaryotes that have been shown to survive cryopreservation. \n\nFungi, notably zygomycetes, ascomycetes and higher basidiomycetes, regardless of sporulation, are able to be stored in liquid nitrogen or deep-frozen. Crypreservation is a hallmark method for fungi that do not sporulate (otherwise other preservation methods for spores can be used at lower costs and ease), sporulate but have delicate spores (large or freeze-dry sensitive), are pathogenic (dangerous to keep metabolically active fungus) or are to be used for genetic stocks (ideally to have identical composition as the original deposit). As with many other organisms, cryoprotectants like DMSO or glycerol (e.g. filamentous fungi 10% glycerol or yeast 20% glycerol) are used. Differences between choosing cryoprotectants are species (or class) dependent, but generally for fungi penetrating cryoprotectants like DMSO, glycerol or polyethylene glycol are most effective (other non-penetrating ones include sugars mannitol, sorbitol, dextran, etc.). Freeze-thaw repetition is not recommended as it can decrease viability. Back-up deep-freezers or liquid nitrogen storage sites are recommended. Multiple protocols for freezing are summarized below (each uses screw-cap polypropylene cryotubes):\n\nMany common culturable laboratory strains are deep-frozen to preserve genetically and phenotypically stable, long-term stocks. Sub-culturing and prolonged refrigerated samples may lead to loss of plasmid(s) or mutations. Common final glycerol percentages are 15, 20 and 25. From a fresh culture plate, one single colony of interest is chosen and liquid culture is made. From the liquid culture, the medium is directly mixed with equal amount of glycerol; the colony should be checked for any defects like mutations. All antibiotics should be washed from the culture before long-term storage. Methods vary, but mixing can be done gently by inversion or rapidly by vortex and cooling can vary by either placing the cryotube directly at −50 to −95 °C, shock-freezing in liquid nitrogen or gradually cooling and then storing at −80 °C or cooler (liquid nitrogen or liquid nitrogen vapor). Recovery of bacteria can also vary, namely if beads are stored within the tube then the few beads can be used to plate or the frozen stock can be scraped with a loop and then plated, however, since only little stock is needed the entire tube should never be completely thawed and repeated freeze-thaw should be avoided. 100% recovery is not feasible regardless of methodology.\n\nThe microscopic soil-dwelling nematode roundworms Panagrolaimus detritophagus and Plectus parvus are the only eukaryotic organisms that have been proven to be viable after long-term cryopreservation to date. In this case, the preservation was natural rather than artificial, due to permafrost.\n\n\n\n\n"}
{"id": "53256363", "url": "https://en.wikipedia.org/wiki?curid=53256363", "title": "Digital Review", "text": "Digital Review\n\nDigital Review is a digital media website founded by Salih Sarıkaya in 2014.\n\nDigital Review was created by Salih Sarıkaya, in January 2014. At Consumer Electronics Show, \"CES 2017\" the website was noted as being one of the top tech influencer brands.\n\nAs of October 2017, it has over 50,000 Twitter followers.\n\n"}
{"id": "8078", "url": "https://en.wikipedia.org/wiki?curid=8078", "title": "Dynamite", "text": "Dynamite\n\nDynamite is an explosive made of nitroglycerin, sorbents (such as powdered shells or clay) and stabilizers. It was invented by the Swedish chemist and engineer Alfred Nobel in Geesthacht, and patented in 1867. It rapidly gained wide-scale use as a more powerful alternative to black powder. \n\nToday dynamite is mainly used in the mining, quarrying, construction, and demolition industries. Dynamite is still the product of choice for trenching applications, and as a cost-effective alternative to cast boosters. Dynamite is occasionally used as an initiator or booster for AN and ANFO explosive charges.\n\nDynamite was invented by Swedish chemist Alfred Nobel in the 1860s and was the first safely manageable explosive stronger than black powder, which had been invented in China in the 9th century. Black powder is now popularly known as gunpowder, because while it is effective as a propellant, it is less suitable for shattering rock or fortifications. \n\nAlfred Nobel's father, Immanuel Nobel, was an industrialist, engineer, and inventor. He built bridges and buildings in Stockholm and founded Sweden's first rubber factory. His construction work inspired him to research new methods of blasting rock that were more effective than black powder. After some bad business deals in Sweden, in 1838 Immanuel moved his family to Saint Petersburg, where Alfred and his brothers were educated privately under Swedish and Russian tutors. At age 17, Alfred was sent abroad for two years; in the United States he met Swedish engineer John Ericsson and in France studied under famed chemist Théophile-Jules Pelouze and his pupil Ascanio Sobrero who had first synthesized nitroglycerin in 1847. It was in France that Nobel first encountered nitroglycerin, which Pelouze cautioned against using as a commercial explosive because of its high volatility.(flame ingnition ease) \n\nIn 1857, Nobel filed the first of several hundred patents, mostly concerning air pressure, gas and fluid gauges, but remained fascinated with nitroglycerin's potential as an explosive. Nobel, along with his father and brother Emil, experimented with various combinations of nitroglycerin and black powder. Nobel came up with a solution of how to safely detonate nitroglycerin by inventing the detonator, or blasting cap, that allowed a controlled explosion set off from a distance using a fuse. In the summer of 1863, Nobel performed his first successful detonation of pure nitroglycerin, using a blasting cap made of a copper percussion cap and mercury fulminate. In 1864, Alfred Nobel filed patents for both the blasting cap and his method of synthesizing nitroglycerin, which is composed of sulfuric acid, nitric acid and glycerin. On 3 September 1864, while experimenting with nitroglycerin, Emil and several others were killed in an explosion at the factory at Immanuel Nobel's estate at Heleneborg. After this, Alfred founded the company Nitroglycerin Aktiebolaget AB in Vinterviken to continue work in a more isolated area and the following year moved to Germany, where he founded another company, Dynamit Nobel. \n\nDespite the invention of the blasting cap, the volatility of nitroglycerin rendered it useless as a commercial explosive. To solve this problem, Nobel sought to combine it with another substance that would make it safe for transport and handling but yet would not reduce its effectiveness as an explosive. He tried combinations of cement, coal, and sawdust, but was unsuccessful. Finally, he tried diatomaceous earth, fossilized algae, that he brought from the Elbe River near his factory in Hamburg, which successfully stabilized the nitroglycerin into a portable explosive. \n\nNobel obtained patents for his inventions in England on 7 May 1867 and in Sweden on 19 October 1867. After its introduction, dynamite rapidly gained wide-scale use as a safe alternative to black powder and nitroglycerin. Nobel tightly controlled the patents, and unlicensed duplicating companies were quickly shut down. However, a few American businessmen got around the patent by using a slightly different formula.\n\nNobel originally sold dynamite as \"Nobel's Blasting Powder\" but decided to change the name to dynamite, from the Ancient Greek word \"dýnamis\" (), meaning \"power\".\n\nNitroglycerin by itself is a very strong explosive, but is extremely shock-sensitive (that is, physical shock can cause it to explode), and degrades over time to even more unstable forms, which makes it highly dangerous to transport or use. Dynamite combines nitroglycerin with absorbents and stabilizers, rendering it safe to use while retaining its powerful explosive properties.\n\nNobel's original composition of dynamite consisted of three parts \"explosive oil\" (as nitroglycerin was called), one part diatomaceous earth as the absorbent, and a small admixture of sodium carbonate antacid as the stabilizer. Ethylene glycol dinitrate was later added to the nitroglycerin to lower its freezing point and keep it from freezing into a slush at low temperatures, which made it unstable, or from sweating out when it thawed. Diatomaceous earth is not usually used today as an absorbent medium and it has been replaced by cheaper media such as sawdust, wood pulp, flour, or starch. Other stabilizers, such as calcium carbonate or zinc oxide, can be used in the place of sodium carbonate. Sodium nitrate is added to the medium as an oxidizer to improve the dynamite's brisance.\n\nDynamite is usually sold in the form of cardboard cylinders about long and about in diameter, with a weight of about . A stick of dynamite thus produced contains roughly 1 MJ (megajoule) of energy. Other sizes also exist, rated by either portion (Quarter-Stick or Half-Stick) or by weight.\n\nDynamite is usually rated by \"weight strength\" (the amount of nitroglycerin it contains), usually from 20% to 60%. For example, \"40% dynamite\" is composed of 40% nitroglycerin and 60% \"dope\" (the absorbent storage medium mixed with the stabilizer and any additives).\n\nThe maximum shelf life of nitroglycerin-based dynamite is recommended as one year from the date of manufacture under good storage conditions.\n\nOver time, regardless of the sorbent used, sticks of dynamite will \"weep\" or \"sweat\" nitroglycerin, which can then pool in the bottom of the box or storage area. For that reason, explosive manuals recommend the repeated turning over of boxes of dynamite in storage. Crystals will form on the outside of the sticks causing them to be even more shock, friction, and temperature sensitive. This creates a very dangerous situation. While the risk of an explosion without the use of a blasting cap is minimal for fresh dynamite, old dynamite is dangerous. Modern packaging helps eliminate this by placing the dynamite into sealed plastic bags, and using wax coated cardboard.\n\nDynamite is moderately sensitive to shock. Shock resistance tests are usually carried out with a drop-hammer: about 100 mg of explosive is placed on an anvil, upon which a weight of between 0.5 and 10 kg is dropped from different heights until detonation is achieved. With a hammer of 2 kg, mercury fulminate detonates with a drop distance of 1 to 2 cm, nitroglycerin with 4 to 5 cm, dynamite with 15 to 30 cm, and ammoniacal explosives with 40 to 50 cm.\n\nFor several decades beginning in the 1940s, the largest producer of dynamite in the world was the Union of South Africa. There the De Beers company established a factory in 1902 at Somerset West. The explosives factory was later operated by AECI (African Explosives and Chemical Industries). The demand for the product came mainly from the country's vast gold mines, centered on the Witwatersrand. The factory at Somerset West was in operation in 1903 and by 1907 it was already producing 340,000 cases, each, annually. A rival factory at Modderfontein was producing another 200,000 cases per year.\n\nThere were two large explosions at the Somerset West plant during the 1960s. Some workers died, but the loss of life was limited by the modular design of the factory and its earth works, and the planting of trees that directed the blasts upward. There were several other explosions at the Modderfontein factory. After 1985, pressure from trade unions forced AECI to phase out the production of dynamite. The factory then went on to produce ammonium nitrate emulsion-based explosives that are safer to manufacture and handle.\n\nDynamite was first manufactured in the U.S. by the Giant Powder Company of San Francisco, California whose founder had obtained the exclusive rights from Nobel in 1867. Giant was eventually acquired by the E. I. du Pont de Nemours Company who subsequently produced it under the Giant name until that company was dissolved by Du Pont in 1905.\nThereafter, Du Pont produced dynamite under its own name until 1911-12 when its explosives monopoly was broken up by the U.S. Circuit Court in the \"Powder Case\". \nTwo new companies that were formed upon the breakup, the Hercules Powder Company and the Atlas Powder Company took up the manufacture of dynamite (in different formulations) thereafter.\n\nCurrently only Dyno Nobel manufactures dynamite in the US. The only facility producing it is located in Carthage, Missouri, but the material is purchased from Dyno Nobel by other manufacturers, who put their label on the dynamite and boxes.\n\nOther explosives are often referred to or confused with dynamite:\n\nTNT is most commonly assumed to be the same as (or confused for) Dynamite, largely due to the ubiquity of both explosives during the 20th century and the civilian practice of preparing Dynamite charges in 8x1\" \"sticks\" wrapped in red waxed paper and shaped to fit the cylindrical boreholes drilled in the rock face. This incorrect connection between TNT and Dynamite was enhanced by Bugs Bunny cartoons where animators started labelling \"any\" kind of cartoon bomb (ranging from sticks of Dynamite to kegs of black powder) as \"TNT\" because the acronym was shorter, more memorable and didn't require literacy to recognize \"TNT\" meant \"bomb\" (similar to the use of XXX markings on whiskey bottles and barrels in cartoons). This eventually led to the general perception that TNT and Dynamite were one and the same.\n\nIn actuality, aside from both being high explosives, TNT and Dynamite have very little in common: TNT is a 2nd generation castable explosive adopted by the military \"The German armed forces adopted it as a filling for artillery shells in 1902\" forty years after Dynamite, which is a 1st generation phlegmatized explosive primarily intended for civilian earthmoving. TNT has never been popular or widespread in civilian earthmoving, as it is considerably more expensive and less powerful by weight than Dynamite, as well as being slower to mix and pack into cylindrical boreholes; for its part, Dynamite has never been popular in warfare because it degenerates quickly under severe conditions and can be detonated by either fire or a wayward bullet. TNT's primary asset is its remarkable insensitivity and stability: a full generation better than Dynamite, it is waterproof and incapable of detonating without the extreme shock and heat provided by a blasting cap (or a sympathetic detonation); this conveniently also allows it to be melted at 178 °F, poured into high explosive shells and allowed to re-solidify with no extra danger or change in the TNT's characteristics. As such, more than 90% of the TNT produced in America was always for the military market, with most filling shells, hand grenades and aerial bombs and the remainder being packaged in brown \"bricks\" (not red cylinders) for use as demolition charges by combat engineers.\n\nIn the United States, in 1885, the chemist Russell S. Penniman invented \"ammonium dynamite\", a form of explosive that used ammonium nitrate as a substitute for the more costly nitroglycerin. Ammonium nitrate has only 85% of the chemical energy of nitroglycerin.\n\nIt is rated by either \"weight strength\" (the amount of ammonium nitrate in the medium) or \"cartridge strength\" (the potential explosive strength generated by an amount of explosive of a certain density and grain size used in comparison to the explosive strength generated by an equivalent density and grain size of a standard explosive). For example, high-explosive \"65% Extra Dynamite\" has a weight strength of 65% ammonium nitrate and 35% \"dope\" (the absorbent medium mixed with the stabilizers and additives). Its \"cartridge strength\" would be its weight in pounds times its strength in relation to an equal amount of ANFO (the civilian baseline standard) or TNT (the military baseline standard). For example, 65% ammonium dynamite with a 20% cartridge strength would mean the stick was equal to an equivalent weight strength of 20% ANFO.\n\n\"Military dynamite\" is a dynamite substitute, formulated without nitroglycerin. It contains 75% RDX, 15% TNT, 5% SAE 10 motor oil, and 5% cornstarch, but is much safer to store and handle for long periods than Nobel's dynamite. Military dynamite substitutes much more stable chemicals for nitroglycerin.\n\nVarious countries around the world have enacted explosives laws and require licenses to manufacture, distribute, store, use, and possess explosives or ingredients.\n\n\n\n"}
{"id": "291726", "url": "https://en.wikipedia.org/wiki?curid=291726", "title": "Emergency tourniquet", "text": "Emergency tourniquet\n\nEmergency tourniquets are cuff-like devices designed to stop severe traumatic bleeding before or during transport to a care facility. They are wrapped around the limb, proximal to the site of trauma, and tightened until all blood vessels underneath are occluded. The design and construction of emergency tourniquets allows quick application by first aid responders or the injured persons themselves. Correct use of tourniquet devices have been shown to save lives under austere conditions with comparatively low risk of injury. In field trials, prompt application of emergency tourniquets before the patient goes into shock are associated with higher survival rates than any other scenario where tourniquets were used later or not at all.\n\nExisting guidelines call for the use of improvised \"rope-and-stick\" tourniquets as a last resort to stop severe bleeding. However, purpose-made tourniquet devices that are well designed can provide greatly increased safety and efficacy. Variability in performance has been shown to exist between various designs and application methods.\n\nMechanisms that confer sufficient mechanical advantage are essential for applying adequate pressure to stop bleeding, particularly on the lower extremities. Pressures that occlude venous but not arterial flow can exacerbate hemorrhage and cause damage to healthy tissue.\n\nResults from laboratory and field testing suggest that windlass and pneumatic mechanisms are effective where other systems fail due to excessive pain, slipping, inadequate force, or mechanical failure.\n\nPressure underneath a tourniquet cuff is not evenly distributed, with the highest pressures localized around the cuff centerline and decreasing to zero near the cuff edges. A high rate of change of pressure across the cuff width, or a high cuff pressure gradient, is a leading cause of nerve and muscle injury from tourniquet use. Tourniquets with wider straps or cuffs, especially those with pneumatic actuation in contrast to mechanical force, distribute pressure more evenly and produce lower pressure gradients. They are therefore more likely to stop bleeding and less likely to cause damage to underlying tissue, in addition to being significantly less painful than tourniquets with narrow straps and bands. Overpressure protection in certain emergency tourniquets also help to prevent excessive force from damaging the limb.\n\nPossible risks of complications—morbidity—related to emergency tourniquet use include \nEmergency care services implementing routine tourniquet use, especially in the civilian setting, should exercise caution and ensure that training is adequate for optimal results. However, given proper precautions, the occurrence of complications due to tourniquet use is quite rare. Designed tourniquet devices are routinely tightened over healthy limbs during training with no ill effects, and recent evidence from combat hospitals in Iraq suggests that morbidity rates are low when users adhere to standard best practices. Since no better alternatives exist for users to self-apply with only basic training, the benefit of tourniquet use far outweighs the risks.\n\nSafe tourniquet practice involves:\nLatest field trials suggest that wider straps are more effective and less painful than tourniquets with thinner straps. The concept of limb occlusion pressure is also gaining prominence over the misconception that greater applied force results in greater effectiveness. In addition, studies of failed cases indicate that the correct devices should be coupled with training that facilitates realistic expectations and correct user actions.\n\nDespite the success of widespread tourniquet deployment to limit combat casualties, many preventable deaths from hemorrhage occur where conventional tourniquet use is inappropriate. The need exists for controlling junctional bleeding, especially in the pelvic area. In 2012, the Combat Ready Clamp (CRoC) was selected by the U.S. Army Institute of Surgical Research (USAISR) for that purpose. Another emerging need is more refined training regimes and doctrine based on scientific evidence, which can ensure that future tourniquet practice and policies are in line with the most current body of knowledge.\n\nJunctional hemorrhage, bleeding from the areas at the junction of the trunk and its appendages, is a difficult problem in trauma. These areas are not amenable to extremity tourniquets because the area that is bleeding will not allow placement that can create circumferential pressure around the extremity and above the wound. Junctional arterial injuries can rapidly lead to death by exsanguination prior to care in the hospital so out-of-hospital control of junctional bleeding can be lifesaving.\n\nFor the study interval between October 2001 and June 2011, 4,596 battlefield fatalities were reviewed and analyzed. The stratification of mortality demonstrated that 87.3% of all injury mortality occurred pre-Military Treatment Facility (MTF). Of the pre-MTF deaths, 75.7% (n = 3,040) were classified as non-survivable, and 24.3% (n = 976) were deemed potentially survivable (PS). In this study the injury/physiologic focus of PS mortal wounding was largely associated with hemorrhage (90.9%) and the primary site of lethal hemorrhage was truncal (67.3%), followed by junctional (19.2%), and then peripheral-extremity (13.5%) hemorrhage.\n\nFrom the statistics cited above we can deduce that there were 976 PS patients, with 91% of those involving hemorrhage. This results in a total of 888 PS patients that died as a result of hemorrhage. From these hemorrhagic cases we find that 19.2% involved junctional hemorrhage resulting in a potentially survivable subgroup of 171 patients from this study. Therefore, if a device that can control junctional hemorrhage had been available, 171 persons with life-threatening hemorrhage from junctional regions could have been treated near the point of injury most probably resulting in reduced mortality in this subgroup. And if that junctional device were multi-site capable so it could also be used to control extremity hemorrhage if needed then perhaps more in the peripheral-extremity sub-group could have survived. Additionally it is unclear to what extent that the authors of this study categorized inter-pelvic bleeding, a common component of both amputations from blast and from blunt trauma like motor vehicle accidents, as truncal hemorrhage. However it is very clear that if a device were capable of controlling not only junctional hemorrhage but was also clearly indicated and effective for controlling inter-pelvic hemorrhage, it could potentially change which injuries are viewed as “potentially survivable” in this study and thereby further expand survivability.\n\n"}
{"id": "26983783", "url": "https://en.wikipedia.org/wiki?curid=26983783", "title": "Empower Up", "text": "Empower Up\n\nEmpower Up, formerly known as CREAM (Computer Reuse Education And Marketing), a 501(c)(3), was a computer recycling center in SW Washington. Started in mid-2002, Empower Up took old electronics in for donation, and recycled/distributed them.\n\nEmpower Up's history can be traced back to a collection event started by the City of Vancouver and Clark County in June 2001. The success of this collection prompted the city to start another collection in January 2002, to collect only reusable computers and monitors for the community. Due to the success of the second event, comments by sponsors, and the general public showed an interest in these collections.\n\nIn mid-2002, after both an event for re-usage and recycling of computers, departments of the City of Vancouver created the program Computer REuse and Marketing, or CREAM. This new organization would have a permanent location, permanent collection sites, and mobile collection units to travel around the area. CREAM would also make sure of the proper disposal of materials in electronics not suitable for redistribution. By 2008, CREAM had distributed 250 refurbish computers to needy families (with the help of Salvation Army Family Services), and recycled 3.5 million pounds of electronic waste.\n\nNear the beginning of 2009, due to new laws in Washington, it was recommended that the organization become an independent non-profit organization. With the help of Habitat for Humanity, and Clark County ReStore, the state created a 3-year plan to make CREAM an independent nonprofit. In February 2009, they established their first permanent building in Vancouver.\n\nIn April 2010, CREAM's name was changed to Empower Up, with the motto \"Technology Reuse In Action\", and changed to their current logo.\n\nIn October 2013 Empower Up moved to 3206 N E 52nd Street Vancouver Washington 98663 on St Johns Road. They expanded the Reuse store and added an eBay store. \n\nOn 27 August 2016 Empower Up was shut down permanently, citing \"economic pressure\".\n\nEmpower Up has recently received attention from many local and state news sources. They have been featured in places such as KATU local news, \"The Vancouver Voice\", and \"The Columbian\".\n\n\n"}
{"id": "5947555", "url": "https://en.wikipedia.org/wiki?curid=5947555", "title": "Eye liner", "text": "Eye liner\n\nEye liner or eyeliner is a cosmetic used to define the eyes. It is applied around the contours of the eye(s) to create a variety of aesthetic effects.\n\nEye liner was first used in Ancient Egypt and Mesopotamia as a dark black line around the eyes. As early as 10,000 BC, Egyptians and Mesopotamians wore various cosmetics including eye liner not only for aesthetics but to protect the skin from the desert sun. Research has also speculated that eye liner was worn to protect the wearer from the evil eye. The characteristic of having heavily lined eyes has been frequently depicted in ancient Egyptian art. They produced eye liner with a variety of materials, including copper ore and antimony. Ancient Egyptian kohl contained galena, which was imported from nearby regions in the Land of Punt, Coptos and Western Asia.\n\nIn the 1920s, Tutankhamun's tomb was discovered, introducing the use of eye liner to the Western world. The 1920s were an era commonly associated with many changes in women's fashion, and women felt freer to apply make-up more liberally.\n\nIn the 1960s, liquid eye liner was used to create thick black and white lines around the eyes in the make-up fashion associated with designers like Mary Quant. The '60s and '70s also saw new fashion trends which made use of eyeliner, eyeshadow and mascara in new ways. As goth and punk fashion developed, they employed eyeliner for a dark and dramatic effect.\n\nIn the late twentieth and early twenty-first century, heavy eye liner use has been associated with Goth fashion and Punk fashion. Eye liner of varying degrees of thickness has also become associated with the emo subculture and various alternative lifestyles. Guyliner is also a special style the emo subculture tend to use after being popularized from \"Pete Wentz\", bassist of the pop-punk band \"Fall Out Boy\". \n\nEye liner is commonly used in a daily make-up routine to define the eye or create the look of a wider or smaller eye. Eye liner can be used as a tool to create various looks as well as highlighting different features of the eyes. Eye liner can be placed in various parts of the eye to create different looks with a winged eye liner or tight lined at the waterline. Eye liner can be drawn above upper lashes or below lower lashes or both, even on the water lines of your eyes. Its primary purpose is to make the lashes look lush, but it also draws attention to the eye and can enhance or even change the eye's shape. Eye liner is available in a wide range of hues, from the common black, brown and grey to more adventurous shades such as bright primary colors, pastels, frosty silvers and golds, white and even glitter-flecked colors.\n\nEye liner can also be used for showing depression in photographs, such as the .\n\nAlthough many users of eye liner use it on a day-to-day basis for a simple definer of the eyes, there are others who use it for more than just an easy eye enlarger. There are many styles of eye liner. Most of them usually revolve around the winged eye liner, which is defining around your top eyelid shape and a line, about halfway toward the end of the eyebrow. It is usually connected to the defining around the top. However today there are some drastically toned eye liner. Nowadays it is also used on the lower lid of the eyes in different colors. \n\nTight lining is the use of eye liner tight against the waterline under the lashes of the upper lid, and above the lashes of the lower lid. Due to the proximity to the membranes, and the surface of the eye itself, waterproof eye liner is preferred. Tight lining is a technique which makes the eyelashes appear to start farther back on the eyelid, thus making them look longer. Gel eye liner and a small angled brush may be used to create this look.\n\nDepending on its texture, eye liner can be softly smudged or clearly defined. There are five main types of eye liner available on the market: each produces a different effect.\n\n\nTraditional wax-based eye liners are made from about 20 components. About 50% by weight are waxes (e.g., Japan wax, fats, or related soft materials that easily glide on to the skin. Stearyl heptanoate is found in most cosmetic eyeliner. Typical pigments include black iron oxides, as well as smaller amounts of titanium dioxide and Prussian blue.\n\n"}
{"id": "52987247", "url": "https://en.wikipedia.org/wiki?curid=52987247", "title": "Fasten (company)", "text": "Fasten (company)\n\nFasten Inc. is an American transportation network company based in Boston, Massachusetts. Founded in 2015 by CEO Kirill Evdakov, CMO Roman Levitskiy and COO Vlad Christoff, Fasten was first launched in September 2015 in Boston. In its initial round of funding, Fasten raised $9.2 million, contributed wholly by chairman Evgeny Lvov. Fasten now operates in two cities—Austin and Boston—with plans to expand to more U.S. cities in the future. Using the company's mobile app, customers can send requests for rides, which are then dispatched to the nearest available drivers. Fasten has emphasized that for every trip completed by a driver, the company only takes $0.99 of the fare, a lower commission compared to competing transportation businesses such as Uber and Lyft, which both take a percentage of the fare. In its first year, Fasten made a revenue of $500,000.\n\nIn its first year in Boston, Fasten mostly targeted college students in the Boston and Cambridge area. The company offered promotions such as $5 for any ride under 20 minutes, $3 off your first 100 rides, and free rides between midnight and 3am, with Fasten paying the difference between promotional prices and actual prices to drivers. To attract drivers, Fasten offered to pay drivers for not only the time the customer was in the car, but also for the distance and time covered when the driver was travelling to pick up their passenger. Evdakov stated that in its first year, Fasten's driver base in Boston \"grew 300% quarter by quarter.\"\n\nIn order to use Fasten, both riders and drivers need a GPS enabled smartphone with the app installed. Currently available in the Google Play store for Android devices, and from the App Store for devices powered by Apple's iOS. From this app, riders can enter a start (usually their current location) and end destination, and can request either a four- or six-seat car. Before requesting, riders will see an estimated wait time, an estimated time until arrival at destination, and an estimated cost. Instead of surge pricing in the app, Fasten offers potential riders the option to \"boost\" their ride, where riders can offer to pay drivers more in order to secure a ride during times of high demand. Upon requesting a ride, nearby drivers are notified of the pending request, and can choose whether or not to accept it. Drivers also see whether the rider has boosted the ride or not, and the multiplier of the booster.\n\nOnce a driver has accepted the request, the rider is able to view the driver's profile, including photo, the make and model of their car, license plate, and rating. Fasten rates drivers on a percentage scale, with riders giving drivers either thumbs up or down after each ride. Riders also see an estimated time until the driver arrives, with the driver's real-time position being shown on the map.\n\nCustomers have the option of contacting their driver through either phone or text. These communications are conducted through Fasten's servers, with Fasten providing both rider and driver with a temporary phone number to ensure anonymity. Fasten has emphasized that they require drivers to pull over before responding to phone calls or texts.\n\nAnother feature of the app is real-time pricing. While on the ride, riders can see the real-time fare they will be charged in the app as they travel along their route. This is similar to taxi cab pricing, and differs from Uber and Lyft, which offer either upfront pricing or calculate the fare to be paid after the trip has finished.\n\nFasten takes a fixed $0.99 commission for every trip completed by a driver, unlike competitors Uber and Lyft, which both take around 20-30% of the fare riders pay. According to its website, Fasten drivers can also elect to pay a fixed $20 daily fee or $80 weekly fee, pocketing in whole all fares made during this period.\n\nDrivers use their own cars, and must pay for their own fuel, car maintenance, and car insurance. During the ride, Fasten also offers additional and increasing levels of insurance coverage while a driver is looking for riders, while a driver is in transit to pick up a passenger, and while a passenger is on the ride, respectively.\n\nDrivers must undergo rigorous background checks before being allowed to work for Fasten. The company reviews an applicants county and federal courthouse records, state criminal databases, national sex offender registry, social security history, and motor vehicle records. Evdakov also stated that Fasten requires drivers' cars to be manufactured later than 2005 for safety reasons.\n\nInitial reception to Fasten in Boston was mixed. Riders were quick to laud Fasten's cheaper prices compared to similar services such as UberX and Lyft, especially due to the number of promotions that Fasten was running. Cars on Fasten's network also seemed to be of slightly higher quality. However, critics also pointed out the scarcity of drivers, with pickup times exceeding those of competing services. Fasten has tried to combat this and attract more drivers by offering amenities such as a driver's lounge in Boston. However, as Scott Krisner of the \"Boston Globe\" points out, Fasten has entered the market late and so must overcome \"huge brand awareness and loyal users who don’t think twice before hitting their app icons\" on both the rider and driver end.\n\nAlthough he did not mention Trump directly, on January 31, 2017, in response to President Trump's executive order suspending the U.S. Refugee Admissions Program and entry of foreign nationals from seven countries, CEO Evdakov released a statement stating: \"We should never let our national origin, religion, or political views divide us, nor prevent us from seeing the real human being in each other.\" He also affirmed that Fasten would \"match all tips riders give their drivers and donate 100% of the proceeds to nonprofit organizations that stand up for the rights of all people.\" This statement was released both as a blog on Fasten's website as well as contained in an email send to all riders and drivers. All proceeds will be donated to the American Civil Liberties Union.\n"}
{"id": "33193592", "url": "https://en.wikipedia.org/wiki?curid=33193592", "title": "Feature toggle", "text": "Feature toggle\n\nA feature toggle (also feature switch, feature flag, feature flipper, conditional feature, etc.) is a technique in software development that attempts to provide an alternative to maintaining multiple source-code branches (known as feature branches), such that a feature can be tested even before it is completed and ready for release. Feature toggle is used to hide, enable or disable the feature during run time. For example, during the development process, a developer can enable the feature for testing and disable it for other users.\n\nContinuous release and continuous deployment provide developers with rapid feedback about their coding. This requires the integration of their code changes as early as possible. Feature branches introduce a bypass to this process. Feature toggles are an important technique used for the implementation of continuous delivery.\n\nThe technique allows developers to release a version of a product that has unfinished features. These unfinished features are hidden (toggled) so they do not appear in the user interface. This allows many small incremental versions of software to be delivered without the cost of constant branching and merging. Feature toggles may allow shorter software integration cycles. A team working on a project can use feature toggle to speed up the process of development, that can include the incomplete code as well.\n\nFeature toggles are essentially variables that are used inside conditional statements. Therefore, the blocks inside these conditional statements can be toggled 'on or off' depending on the value of the feature toggles. A block of code which has been toggled 'off' is similar to it being commented out. This allows developers to control the flow of their software and bypass features that are not ready for deployment.\n\nThe main usage of feature toggles is to avoid conflict that can arise when merging changes in software at the last moment before release. Although this can lead to toggle debt. Toggle debt arises due to the dead code present in software after a feature has been toggled on permanently and produces overhead. This portion of the code has to be removed carefully as to not disturb other parts of the code.\n\nThere are two main types of feature toggle. One is a release toggle, which the developer determines to either keep or remove before a product release depending on its working. The other is a business toggle, which is kept because it satisfies a different usage compared to that of the older code.\n\nFeature toggles can be used in the following scenarios:\n\nFeature toggles can be stored as:\n\nFeature groups consist of feature toggles that work together. This allows the developer to easily manage a set of related toggles.\n\nAnother benefit of feature flags is canary launches. A canary release (or canary launch or canary deployment) allow developers to have features incrementally tested by a small set of users. If a feature's performance is not satisfactory, then it can be rolled back without any adverse effects.\n\nWhile the pattern can be implemented very simply in most programming languages (e.g. Java, Angular JS, PHP, JavaScript, etc. ), there are libraries available to further simplify usage.\n\nMartin Fowler states that a feature toggle \"should be your last choice when you're dealing with putting features into production\". Instead, it is best to break the feature into smaller parts that each can be implemented and safely introduced into the released product without causing other problems.\n\nFeature-toggling is used by many large websites including Flickr, Disqus, Etsy, reddit, Gmail and Netflix, as well as software such as Google Chrome Canary.\n\nThere are many open source feature toggling and feature flagging solutions for different programming languages and platforms.\n\n"}
{"id": "50438425", "url": "https://en.wikipedia.org/wiki?curid=50438425", "title": "GF Biochemicals", "text": "GF Biochemicals\n\nGF Biochemicals is a biochemical company founded in 2008. It was co-founded by and named after Mathieu Flamini and Pasquale Granata. It is the first company in the world able to mass-produce levulinic acid. The company worked with the University of Pisa for seven years on its production. In 2016 GF Biochemicals acquired the American company Segetis. The company has a plant in Caserta that employs around 80 people. In 2015, the company won the John Sime Award for Most Innovative New Technology. The company has offices in Milan and the Netherlands.\n"}
{"id": "43537953", "url": "https://en.wikipedia.org/wiki?curid=43537953", "title": "Gionee", "text": "Gionee\n\nGionee (stylized as GIONEE; ) is a Chinese smartphone manufacturer based in Shenzhen, Guangdong. Founded in 2002, it is one of China’s largest mobile phone manufacturers. According to Gartner, its market share in China was 4.7% in 2012, and it has expanded into other markets, including India, Taiwan, Bangladesh, Nigeria, Vietnam, Myanmar, Nepal, Thailand, the Philippines and Algeria.\n\nIn August 2016, Gionee India introduced their plans to build a manufacturing plant in India. At the same time, Gionee introduced its first Made in India smartphone with the introduction of the F103 model. The model was manufactured in a Foxconn plant in Sricity, Andhra Pradesh. Gionee India was sold to Karbonn Mobiles in September 2018 with license of its brand.\n\nBLU Products, a cell phone manufacturer based in Miami, Florida, sells many rebadged Gionee phones under their brand name in the United States and other North American markets, and engineers phones together with Gionee China and Gionee India.\n\nIn 2014, it was shown by the German media that Gionee was delivering smartphones and tablets with pre-installed malware.\n\n"}
{"id": "1689630", "url": "https://en.wikipedia.org/wiki?curid=1689630", "title": "Hydrogen analyzer", "text": "Hydrogen analyzer\n\nA hydrogen analyzer is a device used to measure the hydrogen concentration in steels and alloys. It also has industrial applications for corrosion monitoring.\n\n"}
{"id": "13733968", "url": "https://en.wikipedia.org/wiki?curid=13733968", "title": "Inprocomm", "text": "Inprocomm\n\nInprocomm, Inc. (formerly Integrated Programmable Communications, Inc.) was a wireless semiconductor design firm, based in Taiwan. The company originally focused on producing IEEE 802.11b, g and a/g chips before beginning to branch out to other portable devices. It was acquired by MediaTek, Inc. in early 2005.\n\nInprocomm produced seven wireless chip designs in total, along with accompanying reference designs for PCI, mini-PCI and CardBus. IPN2120 and IPN2220 are the most common; in particular, D-Link used an IPN2220 chip in their DI-624M router, and Linksys used the IPN2120 in some of their PCI cards (such as the WMP11 version 4) and mini-PCI cards. Linksys mini-PCI cards with IPN2220 can be found in some of the 2004 laptop models, such as the TravelMate 2300 and Aspire 1520 series from Acer, and the Packard Bell EasyNote A5560. Inprocomm chips have also been used in Buffalo and Toshiba products.\n\n"}
{"id": "27457119", "url": "https://en.wikipedia.org/wiki?curid=27457119", "title": "International Organization for Biological Control", "text": "International Organization for Biological Control\n\nThe International Organization for Biological and Integrated Control, usually referred to as IOBC, is a professional organization affiliated with the International Union of Biological Sciences (IUBS) and aims to be an effective advocate for biological control, integrated pest management (IPM) and integrated production (IP).\n\nThe IOBC serves as a resource for international organizations, for example: the European Commission on sustainable use of pesticides and the status of IPM in Europe, the EC Regulation of Biological Control Agents (REBECA) with regard to invertebrate biological control agents, the Consultative Group on International Agricultural Research on IPM (CGIAR), the European Plant Protection Organization (EPPO) on biological control agents and the Food and Agriculture Organization (FAO) with respect to the Convention on Biological Diversity (CBD).\n\nThe complete history of the IOBC was published in 1988. Briefly, in 1948 the idea of an international organization on biological control was conceived. By 1950 the IUBS decided to support the establishment of a \"Commission Internationale de Lutte Biologique\" (CILB) as part of the IUBS Division of Animal Biology and a committee was established to further this concept. In 1955 the statutes of the new organization were ratified by the IUBS and the first plenary session of the CILB took place at Antibes, France. In 1965, CILB changed its name from “Commission” to “Organization” thus becoming the “International Organization of Biological Control of Noxious Animals and Plants”. In 1969, under the auspices of the IUBS, an agreement was reached among organizations to merge IOBC and the “International advisory committee for biological Control” (active in English-speaking countries) into a single international organization under the name IOBC. The scientific journal \"Entomophaga\" (now superseded by \"BioControl\") is the official journal of the new organization. In 1971 Global IOBC was established and the former IOBC became IOBC/WPRS, one of the regional sections.\n\nThere are now six regional sections world-wide:\n\nThe IOBC promotes the development of biological control and its application in integrated pest management and international cooperation to these ends.\n\nThe IOBC collects, evaluates and disseminates information about biological control and promotes national and international action concerning research, training of personnel, coordination of large-scale application and public awareness of the economic and social importance of biological control.\n\nThe IOBC arranges conferences, meetings and symposia, and takes other actions to implement its general objectives.\n\nIn addition to serving as an umbrella organization for the six regional sections, the global organization publishes proceedings of meetings, a newsletter and books and has 10 working groups. These groups meet to discuss specific topics, usually agricultural pests, that often impact on biological control globally.\n\nSince using biological control agents is not like spraying a pesticide which gives immediate results, in order for growers to fully accept and use either biological control or an integrated pest management system, it is imperative that the quality of the control agents be uniformly good. For this purpose, a set of standards have been developed and updated, for assessing the quality control of commercially produced biological control agents. These guidelines have also been accepted and are used by the scientific community (see a selection of references).\n\nIn 2008 IOBC Global established a Commission on Biological Control and Access and Benefit Sharing. Under the Convention on Biological Diversity, countries have sovereign rights over their genetic resources. Agreements governing the access to these resources and the sharing of the benefits arising from their use needs to be established between involved parties. These agreements were created for a number of reasons, among which was the disproportionate monetary advantage developed countries were realizing in the pharmaceutical market. However, since the agreements are all encompassing there have been unforeseen consequences. These agreements also apply to species collected for potential use in biological control. Recent applications of CBD principles have already made it difficult or impossible to collect and export natural enemies for biological control research in several countries.\n\nThe West Palaearctic Regional Section is the most active of the regional sections with 20 working groups (that focus on crops, agricultural pests, and other topics) and five commissions which usually meet in different locations in member countries. Additionally it produces the \"IOBC/WPRS Bulletin\" which is recognized by the United States Department of Agriculture as one of the top research journals on organic production and organic food, newsletters and books.\n\nThe Pesticides and Beneficial Organisms working group is made up of scientists from many countries. They established standards, which are periodically updated, for testing the side effects of pesticides on a large range of natural enemies and ranking those effects. The purpose of establishing these standards was to provide a means by which pesticides could be ranked for their effect on beneficial organisms and pesticide testing could be compared from all regions of the world. With the results obtained from these standardized tests, selective pesticides for beneficial arthropods can be identified in order to enhance biological control in plant protection and reduce the impact of pesticides on non-target organisms. These standards which have been adopted by the scientific community in countries world-wide (see a brief selection of recent publications).\n\nOne of the commissions is on Integrated Production (IP), a concept of sustainable agriculture based on the use of natural resources and regulating mechanisms to replace potentially polluting inputs. The agronomic preventative measures and biological/physical/ chemical methods are carefully selected and balanced taking into account the protection of health of both farmers and consumers and of the environment. As such a Commission in Integrated Production and Integrated Pest Management was established and crop specific IP guidelines established for pome fruits, stone fruits, arable crops in Europe, grapes, soft fruits, olives, citrus and field grown vegetables.\n\nThe Organization’s official languages are English and French, although other languages may be spoken in some regional meetings.\n\n"}
{"id": "2008418", "url": "https://en.wikipedia.org/wiki?curid=2008418", "title": "Isochronous media access controller", "text": "Isochronous media access controller\n\nIsochronous media access controller (I-MAC) is a media access control whereby data must be transferred isochronously—in other words, the data must be transmitted at a steady rate, without interruption.\n"}
{"id": "410858", "url": "https://en.wikipedia.org/wiki?curid=410858", "title": "List of fire-retardant materials", "text": "List of fire-retardant materials\n\nFire-retardant materials should not be confused with fire-resistant materials. A fire resistant material is one that is designed to resist burning and withstand heat, however, fire-retardant materials are designed to burn slowly. An example of a fire-resistant material is one which is used in bunker gear worn by firefighters to protect them from the flames of a burning building. In the United Kingdom, after two significant construction fires which resulted in a combined loss of £1500 million, \"The Joint Code of Practice\" was introduced by the national fire safety organisation, {FPA}, to prevent fires on buildings undergoing construction work. The Joint Code of Practice provides advice on how to prevent fires such as the use of flame-retardant temporary protection materials such as some high quality floor protectors which are designed to burn slowly and prevent the spread of fires.\n\n\n\n\n"}
{"id": "21017", "url": "https://en.wikipedia.org/wiki?curid=21017", "title": "Microcontroller", "text": "Microcontroller\n\nA microcontroller (MCU for \"microcontroller unit\", or UC for \"μ-controller\") is a small computer on a single integrated circuit. In modern terminology, it is similar to, but less sophisticated than, a system on a chip (SoC); an SoC may include a microcontroller as one of its components. A microcontroller contains one or more CPUs (processor cores) along with memory and programmable input/output peripherals. Program memory in the form of ferroelectric RAM, NOR flash or OTP ROM is also often included on chip, as well as a small amount of RAM. Microcontrollers are designed for embedded applications, in contrast to the microprocessors used in personal computers or other general purpose applications consisting of various discrete chips.\n\nMicrocontrollers are used in automatically controlled products and devices, such as automobile engine control systems, implantable medical devices, remote controls, office machines, appliances, power tools, toys and other embedded systems. By reducing the size and cost compared to a design that uses a separate microprocessor, memory, and input/output devices, microcontrollers make it economical to digitally control even more devices and processes. Mixed signal microcontrollers are common, integrating analog components needed to control non-digital electronic systems. In the context of the internet of things, microcontrollers are an economical and popular means of data collection, sensing and actuating the physical world as edge devices.\n\nSome microcontrollers may use four-bit words and operate at frequencies as low as for low power consumption (single-digit milliwatts or microwatts). They generally have the ability to retain functionality while waiting for an event such as a button press or other interrupt; power consumption while sleeping (CPU clock and most peripherals off) may be just nanowatts, making many of them well suited for long lasting battery applications. Other microcontrollers may serve performance-critical roles, where they may need to act more like a digital signal processor (DSP), with higher clock speeds and power consumption.\n\nThe first microprocessor was the 4-bit Intel 4004 released in 1971, with the Intel 8008 and other more capable microprocessors becoming available over the next several years. However, both processors required external chips to implement a working system, raising total system cost, and making it impossible to economically computerize appliances.\n\nOne book credits TI engineers Gary Boone and Michael Cochran with the successful creation of the first microcontroller in 1971. The result of their work was the TMS 1000, which became commercially available in 1974. It combined read-only memory, read/write memory, processor and clock on one chip and was targeted at embedded systems.\n\nPartly in response to the existence of the single-chip TMS 1000, Intel developed a computer system on a chip optimized for control applications, the Intel 8048, with commercial parts first shipping in 1977. It combined RAM and ROM on the same chip. This chip would find its way into over one billion PC keyboards, and other numerous applications. At that time Intel's President, Luke J. Valenter, stated that the microcontroller was one of the most successful in the company's history, and expanded the division's budget over 25%.\n\nMost microcontrollers at this time had concurrent variants. One had EPROM program memory, with a transparent quartz window in the lid of the package to allow it to be erased by exposure to ultraviolet light, often used for prototyping. The other was either a mask programmed ROM from the manufacturer for large series, or a PROM variant which was only programmable once; sometimes this was signified with the designation OTP, standing for \"one-time programmable\". The PROM was of identical type of memory as the EPROM, but because there was no way to expose it to ultraviolet light, it could not be erased. The erasable versions required ceramic packages with quartz windows, making them significantly more expensive than the OTP versions, which could be made in lower-cost opaque plastic packages. For the erasable variants, quartz was required, instead of less expensive glass, for its transparency to ultraviolet—glass is largely opaque to UV—but the main cost differentiator was the ceramic package itself.\n\nIn 1993, the introduction of EEPROM memory allowed microcontrollers (beginning with the Microchip PIC16C84) to be electrically erased quickly without an expensive package as required for EPROM, allowing both rapid prototyping, and in-system programming. (EEPROM technology had been available prior to this time, but the earlier EEPROM was more expensive and less durable, making it unsuitable for low-cost mass-produced microcontrollers.) The same year, Atmel introduced the first microcontroller using Flash memory, a special type of EEPROM. Other companies rapidly followed suit, with both memory types.\n\nNowadays microcontrollers are cheap and readily available for hobbyists, with large online communities around certain processors.\n\nOn 21 June 2018, the \"world's smallest computer\" was announced by the University of Michigan. The device is a \"0.04mm3 16nW wireless and batteryless sensor system with integrated Cortex-M0+ processor and optical communication for cellular temperature measurement.\" It \"measures just 0.3 mm to a side—dwarfed by a grain of rice. [...] In addition to the RAM and photovoltaics, the new computing devices have processors and wireless transmitters and receivers. Because they are too small to have conventional radio antennae, they receive and transmit data with visible light. A base station provides light for power and programming, and it receives the data.\" The device is 1/10th the size of IBM's previously claimed world-record-sized computer from months back in March 2018, which is \"smaller than a grain of salt\", has a million transistors, costs less than $0.10 to manufacture, and, combined with blockchain technology, is intended for logistics and “crypto-anchors”—”digital fingerprints” applications.\n\nIn 2002, about 55% of all CPUs sold in the world were 8-bit microcontrollers and microprocessors.\n\nOver two billion 8-bit microcontrollers were sold in 1997, and according to Semico, over four billion 8-bit microcontrollers were sold in 2006. More recently, Semico has claimed the MCU market grew 36.5% in 2010 and 12% in 2011.\n\nA typical home in a developed country is likely to have only four general-purpose microprocessors but around three dozen microcontrollers. A typical mid-range automobile has about 30 microcontrollers. They can also be found in many electrical devices such as washing machines, microwave ovens, and telephones.\n\nCost to manufacture can be under $0.10 per unit.\n\nCost has plummeted over time, with the cheapest 8-bit microcontrollers being available for under in quantity (thousands) in 2009, and some 32-bit microcontrollers around US$1 for similar quantities.\n\nIn 2012, following a global crisis—a worst ever annual sales decline and recovery and average sales price year-over-year plunging 17%—the biggest reduction since the 1980s—the average price for a microcontroller was US$0.88 ($0.69 for 4-/8-bit, $0.59 for 16-bit, $1.76 for 32-bit).\nIn 2012, worldwide sales of 8-bit microcontrollers were around $4 billion, while 4-bit microcontrollers also saw significant sales.\nIn 2015, 8-bit microcontrollers could be bought for $0.311 (1,000 units), 16-bit for $0.385 (1,000 units), and 32-bit for $0.378 (1,000 units, but at $0.35 for 5,000).\n\nIn 2018, 8-bit microcontrollers can be bought for $0.294 (100 units), 16-bit for $0.393 (1,000 units, but at $0.563 for 100 or $0.349 for full reel of 2,000), and 32-bit for $0.503 (1,000 units, but at $0.466 for 5,000). A lower-priced 32-bit microcontroller, in units of one, can be had for $0.891.\n\nIn 2018, the low-priced microcontrollers above from 2015 are all more expensive (with inflation calculated between 2018 and 2015 prices for those specific units) at: the 8-bit microcontroller can be bought for $0.319 (1,000 units) or 2.6% higher, the 16-bit one for $0.464 (1,000 units) or 21% higher, and the 32-bit one for $0.503 (1,000 units, but at $0.466 for 5,000) or 33% higher.\n\nA microcontroller can be considered a self-contained system with a processor, memory and peripherals and can be used as an embedded system. The majority of microcontrollers in use today are embedded in other machinery, such as automobiles, telephones, appliances, and peripherals for computer systems.\n\nWhile some embedded systems are very sophisticated, many have minimal requirements for memory and program length, with no operating system, and low software complexity. Typical input and output devices include switches, relays, solenoids, LED's, small or custom liquid-crystal displays, radio frequency devices, and sensors for data such as temperature, humidity, light level etc. Embedded systems usually have no keyboard, screen, disks, printers, or other recognizable I/O devices of a personal computer, and may lack human interaction devices of any kind.\n\nMicrocontrollers must provide real-time (predictable, though not necessarily fast) response to events in the embedded system they are controlling. When certain events occur, an interrupt system can signal the processor to suspend processing the current instruction sequence and to begin an interrupt service routine (ISR, or \"interrupt handler\") which will perform any processing required based on the source of the interrupt, before returning to the original instruction sequence. Possible interrupt sources are device dependent, and often include events such as an internal timer overflow, completing an analog to digital conversion, a logic level change on an input such as from a button being pressed, and data received on a communication link. Where power consumption is important as in battery devices, interrupts may also wake a microcontroller from a low-power sleep state where the processor is halted until required to do something by a peripheral event.\n\nTypically micro-controller programs must fit in the available on-chip memory, since it would be costly to provide a system with external, expandable memory. Compilers and assemblers are used to convert both high-level and assembly language codes into a compact machine code for storage in the micro-controller's memory. Depending on the device, the program memory may be permanent, read-only memory that can only be programmed at the factory, or it may be field-alterable flash or erasable read-only memory.\n\nManufacturers have often produced special versions of their micro-controllers in order to help the hardware and software development of the target system. Originally these included EPROM versions that have a \"window\" on the top of the device through which program memory can be erased by ultraviolet light, ready for reprogramming after a programming (\"burn\") and test cycle. Since 1998, EPROM versions are rare and have been replaced by EEPROM and flash, which are easier to use (can be erased electronically) and cheaper to manufacture.\n\nOther versions may be available where the ROM is accessed as an external device rather than as internal memory, however these are becoming rare due to the widespread availability of cheap microcontroller programmers.\n\nThe use of field-programmable devices on a micro controller may allow field update of the firmware or permit late factory revisions to products that have been assembled but not yet shipped. Programmable memory also reduces the lead time required for deployment of a new product.\n\nWhere hundreds of thousands of identical devices are required, using parts programmed at the time of manufacture can be economical. These \"mask programmed\" parts have the program laid down in the same way as the logic of the chip, at the same time.\n\nA customized micro-controller incorporates a block of digital logic that can be personalized for additional processing capability, peripherals and interfaces that are adapted to the requirements of the application. One example is the AT91CAP from Atmel.\n\nMicrocontrollers usually contain from several to dozens of general purpose input/output pins (GPIO). GPIO pins are software configurable to either an input or an output state. When GPIO pins are configured to an input state, they are often used to read sensors or external signals. Configured to the output state, GPIO pins can drive external devices such as LEDs or motors, often indirectly, through external power electronics.\n\nMany embedded systems need to read sensors that produce analog signals. This is the purpose of the analog-to-digital converter (ADC). Since processors are built to interpret and process digital data, i.e. 1s and 0s, they are not able to do anything with the analog signals that may be sent to it by a device. So the analog to digital converter is used to convert the incoming data into a form that the processor can recognize. A less common feature on some microcontrollers is a digital-to-analog converter (DAC) that allows the processor to output analog signals or voltage levels.\n\nIn addition to the converters, many embedded microprocessors include a variety of timers as well. One of the most common types of timers is the programmable interval timer (PIT). A PIT may either count down from some value to zero, or up to the capacity of the count register, overflowing to zero. Once it reaches zero, it sends an interrupt to the processor indicating that it has finished counting. This is useful for devices such as thermostats, which periodically test the temperature around them to see if they need to turn the air conditioner on, the heater on, etc.\n\nA dedicated pulse-width modulation (PWM) block makes it possible for the CPU to control power converters, resistive loads, motors, etc., without using lots of CPU resources in tight timer loops.\n\nA universal asynchronous receiver/transmitter (UART) block makes it possible to receive and transmit data over a serial line with very little load on the CPU. Dedicated on-chip hardware also often includes capabilities to communicate with other devices (chips) in digital formats such as Inter-Integrated Circuit (I²C), Serial Peripheral Interface (SPI), Universal Serial Bus (USB), and Ethernet.\n\nMicro-controllers may not implement an external address or data bus as they integrate RAM and non-volatile memory on the same chip as the CPU. Using fewer pins, the chip can be placed in a much smaller, cheaper package.\n\nIntegrating the memory and other peripherals on a single chip and testing them as a unit increases the cost of that chip, but often results in decreased net cost of the embedded system as a whole. Even if the cost of a CPU that has integrated peripherals is slightly more than the cost of a CPU and external peripherals, having fewer chips typically allows a smaller and cheaper circuit board, and reduces the labor required to assemble and test the circuit board, in addition to tending to decrease the defect rate for the finished assembly.\n\nA micro-controller is a single integrated circuit, commonly with the following features:\n\nThis integration drastically reduces the number of chips and the amount of wiring and circuit board space that would be needed to produce equivalent systems using separate chips. Furthermore, on low pin count devices in particular, each pin may interface to several internal peripherals, with the pin function selected by software. This allows a part to be used in a wider variety of applications than if pins had dedicated functions.\n\nMicro-controllers have proved to be highly popular in embedded systems since their introduction in the 1970s.\n\nSome microcontrollers use a Harvard architecture: separate memory buses for instructions and data, allowing accesses to take place concurrently. Where a Harvard architecture is used, instruction words for the processor may be a different bit size than the length of internal memory and registers; for example: 12-bit instructions used with 8-bit data registers.\n\nThe decision of which peripheral to integrate is often difficult. The microcontroller vendors often trade operating frequencies and system design flexibility against time-to-market requirements from their customers and overall lower system cost. Manufacturers have to balance the need to minimize the chip size against additional functionality.\n\nMicrocontroller architectures vary widely. Some designs include general-purpose microprocessor cores, with one or more ROM, RAM, or I/O functions integrated onto the package. Other designs are purpose built for control applications. A micro-controller instruction set usually has many instructions intended for bit manipulation (bit-wise operations) to make control programs more compact. For example, a general purpose processor might require several instructions to test a bit in a register and branch if the bit is set, where a micro-controller could have a single instruction to provide that commonly required function.\n\nMicrocontrollers traditionally do not have a math coprocessor, so floating point arithmetic is performed by software. However, some recent designs do include an FPU and DSP optimized features. An example would be Microchip's PIC32 MIPS based line.\n\nMicrocontrollers were originally programmed only in assembly language, but various high-level programming languages, such as C, Python and JavaScript, are now also in common use to target microcontrollers and embedded systems. Compilers for general purpose languages will typically have some restrictions as well as enhancements to better support the unique characteristics of microcontrollers. Some microcontrollers have environments to aid developing certain types of applications. Microcontroller vendors often make tools freely available to make it easier to adopt their hardware.\n\nMicrocontrollers with specialty hardware may require their own non-standard dialects of C, such as SDCC for the 8051, which prevent using standard tools (such as code libraries or static analysis tools) even for code unrelated to hardware features. Interpreters may also contain nonstandard features, such as MicroPython, although a fork, CircuitPython, has looked to move hardware dependencies to libraries and have the language adhere to a more CPython standard.\n\nInterpreter firmware is also available for some microcontrollers. For example, BASIC on the early microcontrollers Intel 8052; BASIC and FORTH on the Zilog Z8 as well as some modern devices. Typically these interpreters support interactive programming.\n\nSimulators are available for some microcontrollers. These allow a developer to analyze what the behavior of the microcontroller and their program should be if they were using the actual part. A simulator will show the internal processor state and also that of the outputs, as well as allowing input signals to be generated. While on the one hand most simulators will be limited from being unable to simulate much other hardware in a system, they can exercise conditions that may otherwise be hard to reproduce at will in the physical implementation, and can be the quickest way to debug and analyze problems.\n\nRecent microcontrollers are often integrated with on-chip debug circuitry that when accessed by an in-circuit emulator (ICE) via JTAG, allow debugging of the firmware with a debugger. A real-time ICE may allow viewing and/or manipulating of internal states while running. A tracing ICE can record executed program and MCU states before/after a trigger point.\n\n, there are several dozen microcontroller architectures and vendors including:\n\n\nMany others exist, some of which are used in very narrow range of applications or are more like applications processors than microcontrollers. The microcontroller market is extremely fragmented, with numerous vendors, technologies, and markets. Note that many vendors sell or have sold multiple architectures.\n\nIn contrast to general-purpose computers, microcontrollers used in embedded systems often seek to optimize interrupt latency over instruction throughput. Issues include both reducing the latency, and making it be more predictable (to support real-time control).\n\nWhen an electronic device causes an interrupt, during the context switch the intermediate results (registers) have to be saved before the software responsible for handling the interrupt can run. They must also be restored after that interrupt handler is finished. If there are more processor registers, this saving and restoring process takes more time, increasing the latency. Ways to reduce such context/restore latency include having relatively few registers in their central processing units (undesirable because it slows down most non-interrupt processing substantially), or at least having the hardware not save them all (this fails if the software then needs to compensate by saving the rest \"manually\"). Another technique involves spending silicon gates on \"shadow registers\": One or more duplicate registers used only by the interrupt software, perhaps supporting a dedicated stack.\n\nOther factors affecting interrupt latency include:\n\n\nLower end microcontrollers tend to support fewer interrupt latency controls than higher end ones.\n\nSince the emergence of microcontrollers, many different memory technologies have been used. Almost all microcontrollers have at least two different kinds of memory, a non-volatile memory for storing firmware and a read-write memory for temporary data.\n\nFrom the earliest microcontrollers to today, six-transistor SRAM is almost always used as the read/write working memory, with a few more transistors per bit used in the register file. FRAM or MRAM could potentially replace it as it is denser which would make it more cost effective.\n\nIn addition to the SRAM, some microcontrollers also have internal EEPROM for data storage; and even ones that do not have any (or not enough) are often connected to external serial EEPROM chip (such as the BASIC Stamp) or external serial flash memory chip.\n\nA few recent microcontrollers beginning in 2003 have \"self-programmable\" flash memory.\n\nThe earliest microcontrollers used mask ROM to store firmware. Later microcontrollers (such as the early versions of the Freescale 68HC11 and early PIC microcontrollers) had EPROM memory, which used a translucent window to allow erasure via UV light, while production versions had no such window, being OTP (one-time-programmable). Firmware updates were equivalent to replacing the microcontroller itself, thus many products were not upgradeable.\n\nMotorola MC68HC805 was the first microcontroller to use EEPROM to store the firmware. EEPROM microcontrollers became more popular in 1993 when Microchip introduced PIC16C84 and Atmel introduced the an 8051-core microcontroller that was first one to use NOR Flash memory to store the firmware. Today's microcontrollers almost exclusively use flash memory, with a few models using FRAM, and some ultra-low-cost parts still use OTP or Mask-ROM.\n\n"}
{"id": "13913552", "url": "https://en.wikipedia.org/wiki?curid=13913552", "title": "Mobile phone signal", "text": "Mobile phone signal\n\nA mobile phone signal (also known as reception and service) is the signal strength (measured in dBm) received by a mobile phone from a cellular network (on the downlink). Depending on various factors, such as proximity to a tower, any obstructions such as buildings or trees, etc. this signal strength will vary. Most mobile devices use a set of bars of increasing height to display the approximate strength of this received signal to the mobile phone user. Traditionally five bars are used. (see five by five)\n\nGenerally, a strong mobile phone signal is more likely in an urban area, though these areas can also have some \"dead zones\", where no reception can be obtained. Cellular signals are designed to be resistant to multipath reception, which is most likely to be caused by the blocking of a direct signal path by large buildings, such as high-rise towers. By contrast, many rural or sparsely inhabited areas lack any signal or have very weak fringe reception; many mobile phone providers are attempting to set up towers in those areas most likely to be occupied by users, such as along major highways. Even some national parks and other popular tourist destinations away from urban areas now have cell phone reception, though location of radio towers within these areas is normally prohibited or strictly regulated, and is often difficult to arrange.\n\nIn areas where signal reception would normally be strong, other factors can have an effect on reception or may cause complete failure (see RF interference). From inside a building with thick walls or of mostly metal construction (or with dense rebar in concrete), signal attenuation may prevent a mobile phone from being used. Underground areas, such as tunnels and subway stations, will lack reception unless they are wired for cell signals. There may also be gaps where the service contours of the individual base stations (Cell towers) of the mobile provider (and/or its roaming partners) do not completely overlap.\n\nIn addition, the weather may affect the strength of a signal, due to the changes in radio propagation caused by clouds (particularly tall and dense thunderclouds which cause signal reflection), precipitation, and temperature inversions. This phenomenon, which is also common in other VHF radio bands including FM broadcasting, may also cause other anomalies, such as a person in San Diego \"roaming\" on a Mexican tower from just over the border in Tijuana, or someone in Detroit \"roaming\" on a Canadian tower located within sight across the Detroit River in Windsor, Ontario. These events may cause the user to be billed for \"international\" usage despite being in their own country, though mobile phone companies can program their billing systems to re-rate these as domestic usage when it occurs on a foreign cell site that is known to frequently cause such issues for their customers.\n\nThe volume of network traffic can also cause calls to be blocked or dropped due to a disaster or other mass call event which overloads the number of available radio channels in an area, or the number of telephone circuits connecting to and from the general public switched telephone network.\n\nAreas where mobile phones cannot transmit to a nearby mobile site, base station, or repeater are known as dead zones. In these areas, the mobile phone is said to be in a state of outage. Dead zones are usually areas where mobile phone service is not available because the signal between the handset and mobile site antennas is blocked or severely reduced, usually by hilly terrain, dense foliage, or physical distance.\n\nA number of factors can create dead zones, which may exist even in locations in which a wireless carrier offers coverage, due to limitations in cellular network architecture (the locations of antennas), limited network density, interference with other mobile sites, and topography. Since cell phones rely on radio waves, which travel though the air and are easily attenuated (particularly at higher frequencies), mobile phones may be unreliable at times. Like other radio transmissions, mobile phone calls can be interrupted by large buildings, terrain, trees, or other objects between the phone and the nearest base. Cellular network providers work continually to improve and upgrade their networks in order to minimize dropped calls, access failures, and dead zones (which they call \"coverage holes\" or \"no-service areas\"). For mobile virtual network operators, the network quality depends entirely on the host network for the particular handset in question. Some MVNOs use more than one host, which may even have different technologies (for example, different Tracfone handsets uses either CDMA and 1xRTT on Verizon Wireless, or GSM and UMTS on AT&T Mobility).\n\nDead zones can be filled-in with microcells, while picocells can handle even smaller areas without causing interference to the larger network. Personal microcells, such as those for a home, are called femtocells, and generally have the range of a cordless phone, but may not be usable for an MVNO phone. A similar system can be set up to perform inmate call capture, which prevents cellphones smuggled into a prison from being used. These still complete calls to or from pre-authorized users such as prison staff, while not violating laws against jamming. These systems must be carefully designed so as to avoid capturing calls from outside the prison, which would in effect create a dead zone for any passersby outside.\n\nIn the event of a disaster causing temporary dead zones, a cell on wheels may be brought in until the local telecom infrastructure can be restored. These portable units are also used where large gatherings are expected, in order to handle the extra load.\n\nA dropped call is a common term used and expressed by wireless mobile phone call subscribers when a call is abruptly cut-off (disconnected) during midconversation. This happens less often today than it would have in the early 1990s. The termination occurs unexpected and is influenced by a number of different reasons such as \"Dead Zones.\" In technical circles, it is called an abnormal release.\n\nOne reason for a call to be \"dropped\" is if the mobile phone subscriber travels outside the coverage area—the cellular network radio tower(s). After a telephone connection between two subscribers has been completed, it must remain within range of that subscribers network provider or that connection will lost (dropped). Not all cellular telephone radio towers are owned by the same telephone company (though this is not true to all locations) be maintained across a different company's network (as calls cannot be re-routed over the traditional phone network while in progress), also resulting in the termination of the call once a signal cannot be maintained between the phone and the original network.\n\nAnother common reason is when a phone is taken into an area where wireless communication is unavailable, interrupted, interfered with, or jammed. From the network's perspective, this is the same as the mobile moving out of the coverage area.\n\nOccasionally, calls are dropped upon handoff between cells within the same provider's network. This may be due to an imbalance of traffic between the two cell sites' areas of coverage. If the new cell site is at capacity, it cannot accept the additional traffic of the call trying to \"hand in.\" It may also be due to the network configuration not being set up properly, such that one cell site is not \"aware\" of the cell to which the phone is trying to handoff. If the phone cannot find an alternative cell to which to move that can take over the call, the call is lost.\n\nCo-channel and adjacent-channel interference can also be responsible for dropped calls in a wireless network. Neighbouring cells with the same frequencies interfere with each other, deteriorating the quality of service and producing dropped calls. Transmission problems are also a common cause of dropped calls. Another problem may be a faulty transceiver inside the base station.\n\nCalls can also be dropped if a mobile phone at the other end of the call loses battery power and stops transmitting abruptly.\n\nSunspots and solar flares are rarely blamed for causing interference leading to dropped calls, as it would take a major geomagnetic storm to cause such a disruption (except for satellite phones).\n\nExperiencing too many dropped calls is one of the most common customer complaints received by wireless service providers. They have attempted to address the complaint in various ways, including expansion of their home network coverage, increased cell capacity, and offering refunds for individual dropped calls.\n\nVarious signal booster systems are manufactured to reduce problems due to dropped calls and dead zones. Many options, such as wireless units and antennas, are intended to aid in strengthening weak signals.\n\nArbitrary Strength Unit (ASU) is an integer value proportional to the received signal strength measured by the mobile phone.\n\nIt is possible to calculate the real signal strength measured in dBm (and thereby power in Watts) by a formula. However, there are different formulas for 2G, 3G and 4G networks.\n\nIn GSM networks, ASU maps to RSSI (received signal strength indicator, see TS 27.007 sub clause 8.5).\nIn UMTS networks, ASU maps to RSCP level (received signal code power, see TS 27.007 sub clause 8.69 and TS 25.133 sub clause 9.1.1.3).\nIn LTE_(telecommunication) networks, ASU maps to RSRP (reference signal received power, see TS 36.133, sub-clause 9.1.4). The valid range of ASU is from 0 to 97. For the range 1 to 96, ASU maps to \nThe value of 0 maps to RSRP below -140 dBm and the value of 97 maps to RSRP above -44 dBm.\n\nOn Android devices however, the original GSM formula may prevail for UMTS. Tools like Network Signal Info can directly show the signal strength (in dBm), as well as the underlying ASU.\n\nASU shouldn't be confused with \"Active Set Update\". The Active Set Update is a signalling message used in handover procedures of UMTS and CDMA mobile telephony standards. On Android phones, the acronym ASU has nothing to do with Active Set Update. It has not been declared precisely by Google developers.\n\n"}
{"id": "45590719", "url": "https://en.wikipedia.org/wiki?curid=45590719", "title": "Mud cake (oil and gas)", "text": "Mud cake (oil and gas)\n\nMud cake (also mudcake) is the layer of particulates from drill mud coating (caking) the inside of a borehole after the suspension medium has seeped through a porous geological formation. Similar to filter cake.\n\nMud cake provides a physical barrier to prevent further penetration and loss of drilling fluid, as well a later loss of produced fluids, into a permeable formation.\n"}
{"id": "51661782", "url": "https://en.wikipedia.org/wiki?curid=51661782", "title": "Non-wires alternatives", "text": "Non-wires alternatives\n\nNon-wires alternatives (NWAs) are electric utility system investments and operating practices that can defer or replace the need for specific transmission and/or distribution projects, at lower total resource cost, by reliably reducing transmission congestion or distribution system constraints at times of maximum demand in specific grid areas. Transmission-related NWAs are also known as non-transmission alternatives (NTAs). They can be identified through least-cost planning and action, one geographic area at a time, for managing electricity supply and demand using all means available and necessary, including demand response, distributed generation (DG), energy efficiency, electricity and thermal storage, load management, and rate design.\n\n"}
{"id": "7391442", "url": "https://en.wikipedia.org/wiki?curid=7391442", "title": "North Carolina Research Campus", "text": "North Carolina Research Campus\n\nThe North Carolina Research Campus (NCRC) is a public-private research center in Kannapolis, North Carolina, United States. The Campus was envisioned by David H. Murdock, owner of Dole Food Company and Castle and Cooke, Inc., as a center for improving human health through research into nutrition and agriculture. The campus was formed and operates as a partnership with the State of North Carolina and the University of North Carolina system.\n\nThe scientists at the NC Research Campus are known for research that explores the health benefits of plant phytochemicals, nutrients such as choline and the impact of nutrition and exercise on human performance. Research also involves the study of nutrients and plant-based compounds to prevent chronic diseases such as cancer and diabetes.\n\nThe North Carolina Research Campus is located in Kannapolis, NC, near Charlotte. As a scientific community of eight universities, the David H. Murdock Research Institute, global companies and entrepreneurs, the shared mission is to improve human health through nutrition. Research and development focuses on safer, more nutritious crops, healthier foods and precision nutrition.\n\n"}
{"id": "24136423", "url": "https://en.wikipedia.org/wiki?curid=24136423", "title": "Nuclear renaissance", "text": "Nuclear renaissance\n\nSince about 2001 the term nuclear renaissance has been used to refer to a possible nuclear power industry revival, driven by rising fossil fuel prices and new concerns about meeting greenhouse gas emission limits.\n\nIn the 2009 \"World Energy Outlook\", the International Energy Agency stated that:\n\nA nuclear renaissance is possible but cannot occur overnight. Nuclear projects face significant hurdles, including extended construction periods and related risks, long licensing processes and manpower shortages, plus long‐standing issues related to waste disposal, proliferation and local opposition. The financing of new nuclear power plants, especially in liberalized markets, has always been difficult and the financial crisis seems almost certain to have made it even more so. The huge capital requirements, combined with risks of cost overruns and regulatory uncertainties, make investors and lenders very cautious, even when demand growth is robust.\nThe World Nuclear Association reported that nuclear electricity generation in 2012 was at its lowest level since 1999.\n\nIn 2015:\n\nMarch 2017 saw a setback for nuclear renaissance when producer of the AP1000 reactor Westinghouse Electric Company filed for Chapter 11 bankruptcy protection. Four months later the bankruptcy together with delays and cost overruns caused cancellation of the two AP1000 reactors under construction at the Virgil C. Summer Nuclear Generating Station.\n\nIn 2009 annual generation of nuclear power has been on a slight downward trend since 2007, decreasing 1.8% in 2009 to 2558 TWh with nuclear power meeting 13–14% of the world's electricity demand. A major factor in the decrease has been the prolonged repair of seven large reactors at the Kashiwazaki-Kariwa Nuclear Power Plant in Japan following the Niigata-Chuetsu-Oki earthquake.\n\nIn March 2011 the nuclear accidents at Japan's Fukushima I Nuclear Power Plant and shutdowns at other nuclear facilities raised questions among some commentators over the future of the renaissance. Platts has reported that \"the crisis at Japan's Fukushima nuclear plants has prompted leading energy-consuming countries to review the safety of their existing reactors and cast doubt on the speed and scale of planned expansions around the world\". In 2011 Siemens exited the nuclear power sector following the Fukushima disaster and subsequent changes to German energy policy, and supported the German government's planned energy transition to renewable energy technologies. China, Germany, Switzerland, Israel, Malaysia, Thailand, United Kingdom, Italy and the Philippines have reviewed their nuclear power programs. Indonesia and Vietnam still plan to build nuclear power plants. Countries such as Australia, Austria, Denmark, Greece, Ireland, Latvia, Liechtenstein, Luxembourg, Portugal, Israel, Malaysia, New Zealand, and Norway remain opposed to nuclear power. Following the Fukushima I nuclear accidents, the International Energy Agency halved its estimate of additional nuclear generating capacity built by 2035.\n\nThe World Nuclear Association has reported that “nuclear power generation suffered its biggest ever one-year fall through 2012 as the bulk of the Japanese fleet remained offline for a full calendar year”. Data from the International Atomic Energy Agency showed that nuclear power plants globally produced 2346 TWh of electricity in 2012 – seven per cent less than in 2011. The figures illustrate the effects of a full year of 48 Japanese power reactors producing no power during the year. The permanent closure of eight reactor units in Germany was also a factor. Problems at Crystal River, Fort Calhoun and the two San Onofre units in the USA meant they produced no power for the full year, while in Belgium Doel 3 and Tihange 2 were out of action for six months. Compared to 2010, the nuclear industry produced 11% less electricity in 2012.\n\nAs of July 2013, \"a total of 437 nuclear reactors were operating in 30 countries, seven fewer than the historical maximum of 444 in 2002. Since 2002, utilities have started up 28 units and disconnected 36 including six units at the Fukushima Daiichi nuclear power plant in Japan. The 2010 world reactor fleet had a total nominal capacity of about 370 gigawatts (or thousand megawatts). Despite seven fewer units operating in 2013 than in 2002, the capacity is still about 7 gigawatts higher\". The numbers of new operative reactors, final shutdowns and new initiated constructions according to International Atomic Energy Agency (IAEA) in 2010 are as follows:\n\nA total of 72 reactors were under construction at the beginning of 2014, the highest number in 25 years. Several of the under construction reactors are carry over from earlier eras; some are partially completed reactors on which work has resumed (e.g., in Argentina); some are small and experimental (e.g., Russian floating reactors); and some have been on the IAEA’s “under construction” list for years (e.g., in India and Russia). Reactor projects in Eastern Europe are essentially replacing old Soviet reactors shut down due to safety concerns. Most of the 2010 activity ― 30 reactors ― is taking place in four countries: China, India, Russia and South Korea. Turkey, the United Arab Emirates and Iran are the only countries that are currently building their first power reactors, Iran's construction began decades ago.\nVarious barriers to a nuclear renaissance have been suggested. These include: unfavourable economics compared to other sources of energy, slowness in addressing climate change, industrial bottlenecks and personnel shortages in the nuclear sector, and the contentious issue of what to do with nuclear waste or spent nuclear fuel. There are also concerns about more nuclear accidents, security, and nuclear weapons proliferation.\n\nNew reactors under construction in Finland and France, which were meant to lead a nuclear renaissance, have been delayed and are running over-budget. China has 22 new reactors under construction, and there are also a considerable number of new reactors being built in South Korea, India, and Russia. At the same time, at least 100 older and smaller reactors will \"most probably be closed over the next 10–15 years\". So the expanding nuclear programs in Asia are balanced by retirements of aging plants and nuclear reactor phase-outs.\n\nA study by UBS, reported on April 12, 2011, predicts that around 30 nuclear plants may be closed worldwide, with those located in seismic zones or close to national boundaries being the most likely to shut. The analysts believe that 'even pro-nuclear countries such as France will be forced to close at least two reactors to demonstrate political action and restore the public acceptability of nuclear power', noting that the events at Fukushima 'cast doubt on the idea that even an advanced economy can master nuclear safety'. In September 2011, German engineering giant Siemens announced it will withdraw entirely from the nuclear industry, as a response to the Fukushima nuclear disaster in Japan.\n\nThe 2011 World Energy Outlook report by the International Energy Agency stated that having \"second thoughts on nuclear would have far-reaching consequences\" and that a substantial shift away from nuclear power would boost demand for fossil fuels, putting additional upward pressure on the price of energy, raising additional concerns about energy security, and making it more difficult and expensive to combat climate change. The reports suggests that the consequences would be most severe for nations with limited local energy resources and which have been planning to rely heavily on nuclear power for future energy security, and that it would make it substantially more challenging for developing economies to satisfy their rapidly increasing demand for electricity.\n\nJohn Rowe, chair of Exelon (the largest nuclear power producer in the US), has said that the nuclear renaissance is \"dead\". He says that solar, wind and cheap natural gas have significantly reduced the prospects of coal and nuclear power plants around the world. Amory Lovins says that the sharp and steady cost reductions in solar power has been a \"stunning market success\".\n\nIn 2013 the analysts at the investment research firm Morningstar, Inc. concluded that nuclear power was not a viable source of new power in the West. On nuclear renaissance they wrote:\n\nThe economies of scale experienced in France during its initial build-out and the related strength of supply chain and labor pool were imagined by the dreamers who have coined the term ‘nuclear renaissance’ for the rest of the world. But outside of China and possibly South Korea this concept seems a fantasy, as should become clearer examining even theoretical projections for new nuclear build today.\n\nNuclear power plants are large construction projects with very high up-front costs. The cost of capital is also steep due to the risk of construction delays and obstructing legal action. The large capital cost of nuclear power has been a key barrier to the construction of new reactors around the world, and the economics have recently worsened, as a result of the global financial crisis. As the OECD’s Nuclear Energy Agency points out, \"investors tend to favor less capital intensive and more flexible technologies\". This has led to a large increase in the use of natural gas for base-load power production, often using more sophisticated combined cycle plants.\n\nMajor nuclear reactor accidents include Three Mile Island accident (1979), Chernobyl disaster (1986), and Fukushima (2011). A report in \"Lancet\" says that the effects of these accidents on individuals and societies are diverse and enduring. Relatively few immediate deaths have occurred, but nuclear-related fatalities are mostly in the hazardous uranium mining industry, which supplies fuel to nuclear reactors. There are also physical health problems directly attributable to radiation exposure, as well as psychological and social effects. The Fukushima accident forced more than 80,000 residents to evacuate from neighborhoods around the crippled nuclear plant. Evacuation and long-term displacement create severe health-care problems for the most vulnerable people, such as hospital inpatients and elderly people.\n\nCharles Perrow, in his book \"Normal accidents\" says that multiple and unexpected failures are built into complex and tightly-coupled systems, such as nuclear power plants. Such accidents often involve operator error and are unavoidable and cannot be designed around. Since the terrorist attacks of September 11, 2001, there has been heightened concern that nuclear power plants may be targeted by terrorists or criminals, and that nuclear materials may be purloined for use in nuclear or radiological weapons.\n\nNevertheless, newer reactor designs intended to provide increased safety have been developed over time. The next nuclear plants to be built will likely be Generation III or III+ designs, and a few are being built in Japan. However, safety risks may be the greatest when nuclear systems are the newest, and operators have less experience with them. Nuclear engineer David Lochbaum explained that almost all serious nuclear accidents occurred with what was at the time the most recent technology. He argues that \"the problem with new reactors and accidents is twofold: scenarios arise that are impossible to plan for in simulations; and humans make mistakes\".\n\nA nuclear power controversy has surrounded the deployment and use of nuclear fission reactors to generate electricity from nuclear fuel for civilian purposes. The controversy peaked during the 1970s and 1980s, when it \"reached an intensity unprecedented in the history of technology controversies\", in some countries.\n\nIn 2008 there were reports of a revival of the anti-nuclear movement in Germany and protests in France during 2004 and 2007. Also in 2008 in the United States, there were protests about, and criticism of, several new nuclear reactor proposals and later some objections to license renewals for existing nuclear plants.\n\nIn 2005, the International Atomic Energy Agency presented the results of a series of public opinion surveys in the \"Global Public Opinion on Nuclear Issues\" report. Majorities of respondents in 14 of the 18 countries surveyed believe that the risk of terrorist acts involving radioactive materials at nuclear facilities is high, because of insufficient protection. While majorities of citizens generally support the continued use of existing nuclear power reactors, most people do not favour the building of new nuclear plants, and 25% of respondents feel that all nuclear power plants should be closed down. Stressing the climate change benefits of nuclear energy positively influences 10% of people to be more supportive of expanding the role of nuclear power in the world, but there is still a general reluctance to support the building of more nuclear power plants. After the Fukushima Disaster, Civil Society Institute (CSI) found out that 58 percent of the respondents indicated less support of using nuclear power in the United States. Two-thirds of the respondents said they would protest the construction of a nuclear reactor within 50 miles of their homes.\n\nThere was little support across the world for building new nuclear reactors, a 2011 poll for the BBC indicates. The global research agency GlobeScan, commissioned by BBC News, polled 23,231 people in 23 countries from July to September 2011, several months after the Fukushima nuclear disaster. In countries with existing nuclear programmes, people are significantly more opposed than they were in 2005, with only the UK and US bucking the trend. Most believe that boosting energy efficiency and renewable energy can meet their needs.\n\nAs of March 2010, ten African nations had begun exploring plans to build nuclear reactors.\n\nSouth Africa (which has two nuclear power reactors), however, removed government funding for its planned new PBMRs in 2010.\n\nBetween 2007 and 2009, 13 companies applied to the Nuclear Regulatory Commission for construction and operating licenses to build 30 new nuclear power reactors in the United States. \nHowever, the case for widespread nuclear plant construction was eroded due to abundant natural gas supplies, slow electricity demand growth in a weak US economy, lack of financing, and uncertainty following the Fukushima nuclear disaster. Many license applications for proposed new reactors were suspended or cancelled. Only a few new reactors will enter service by 2020. These will not be the cheapest energy options available, but they are an attractive investment for utilities because the government mandates that taxpayers pay for construction in advance. In 2013, four aging, uncompetitive, reactors were permanently closed: San Onofre 2 and 3 in California, Crystal River 3 in Florida, and Kewaunee in Wisconsin. Vermont Yankee, in Vernon, is scheduled to close in 2014, following many protests. New York State is seeking to close Indian Point Energy Center, in Buchanan, 30 miles from New York City.\n\nNeither climate change abatement, nor the Obama Administration’s endorsement of nuclear power with $18.5 billion in loan guarantees, have been able to propel nuclear power in the US past existing obstacles. The Fukushima nuclear disaster hasn’t helped either.\n\nAs of 2014, the U.S. nuclear industry began a new lobbying effort, hiring three former senators — Evan Bayh, a Democrat; Judd Gregg, a Republican; and Spencer Abraham, a Republican — as well as William M. Daley, a former staffer to President Obama. The initiative is called Nuclear Matters, and it has begun a newspaper advertising campaign.\n\nLocations of new US reactors and their scheduled operating dates are:\n\nOn 29 March 2017, parent company Toshiba placed Westinghouse Electric Company in Chapter 11 bankruptcy because of US$9 billion of losses from its nuclear reactor construction projects. The projects responsible for this loss are mostly the construction of four AP1000 reactors at Vogtle in Georgia and V. C. Summer in South Carolina. The U.S. government had given $8.3 billion of loan guarantees on the financing of the four nuclear reactors being built in the U.S., and it is expected a way forward to completing the plant can be agreed. Peter A. Bradford, former U.S. Nuclear Regulatory Commission member, commented \"They placed a big bet on this hallucination of a nuclear renaissance\".\n\nAs of 2008, the greatest growth in nuclear generation was expected to be in China, Japan, South Korea and India.\n\nAs of early 2013 China had 17 nuclear reactors operating and 32 under construction, with more planned. \"China is rapidly becoming self-sufficient in reactor design and construction, as well as other aspects of the fuel cycle.\" However, according to a government research unit, China must not build \"too many nuclear power reactors too quickly\", in order to avoid a shortfall of fuel, equipment and qualified plant workers.\n\nFollowing the Fukushima disaster, many are questioning the mass roll-out of new plants in India, including the World Bank, the Indian Environment Minister, Jairam Ramesh, and the former head of the country's nuclear regulatory body, A. Gopalakrishnan. The massive Jaitapur Nuclear Power Project is the focus of concern – \"931 hectares of farmland will be needed to build the reactors, land that is now home to 10,000 people, their mango orchards, cashew trees and rice fields\". Fishermen in the region say their livelihoods will be wiped out.\n\nSouth Korea is exploring nuclear projects with a number of nations.\n\nAustralia is a major producer of uranium, which it exports as uranium oxide to nuclear power generating nations. Australia has a single research reactor at Lucas Heights, but does not generate electricity via nuclear power. As of 2015, the majority of the nation's uranium mines are in South Australia, where a Nuclear Fuel Cycle Royal Commission is investigating the opportunities and costs of expanding the state's role in the nuclear fuel cycle. As of January 2016, new nuclear industrial development (other than the mining of uranium) is prohibited by various acts of federal and state legislation. The Federal government will consider the findings of the South Australian Royal Commission after it releases its findings in 2016.\n\nWith several CANDU reactors facing closure selected plants will be completely refurbished between 2016 and 2026, extending their operation beyond 2050.\n\nOn 18 October 2010 the British government announced eight locations it considered suitable for future nuclear power stations. This has resulted in public opposition and protests at some of the sites. In March 2012, two of the big six power companies announced they would be pulling out of developing new nuclear power plants. The decision by RWE npower and E.ON follows uncertainty over nuclear energy following the Fukushima nuclear disaster last year. The companies will not proceed with their Horizon project, which was to develop nuclear reactors at Wylfa in North Wales and at Oldbury-on-Severn in Gloucestershire. Their decision follows a similar announcement by Scottish and Southern Electricity last year. Analysts said the decision meant the future of UK nuclear power could now be in doubt.\n\nThe 2011 Japanese Fukushima nuclear disaster has led some European energy officials to \"think twice about nuclear expansion\". Switzerland has abandoned plans to replace its old nuclear reactors and will take the last one offline in 2034. Anti-nuclear opposition intensified in Germany. In the following months the government decided to shut down eight reactors immediately (August 6, 2011) and to have the other nine off the grid by the end of 2022. Renewable energy in Germany is believed to be able to compensate for much of the loss. In September 2011 Siemens, which had been responsible for constructing all 17 of Germany's existing nuclear power plants, announced that it would exit the nuclear sector following the Fukushima disaster and the subsequent changes to German energy policy. Chief executive Peter Loescher has supported the German government's planned energy transition to renewable energy technologies, calling it a \"project of the century\" and saying Berlin's target of reaching 35% renewable energy sources by 2020 was feasible.\n\nOn October 21, 2013, EDF Energy announced that an agreement had been reached regarding new nuclear plants to be built on the site of Hinkley Point C. EDF Group and the UK Government agreed on the key commercial terms of the investment contract. The final investment decision is still conditional on completion of the remaining key steps, including the agreement of the EU Commission.\n\nIn February 2014, Amory Lovins commented that:\n\nBritain's plan for a fleet of new nuclear power stations is … unbelievable ... It is economically daft. The guaranteed price [being offered to French state company EDF] is over seven times the unsubsidised price of new wind in the US, four or five times the unsubsidised price of new solar power in the US. Nuclear prices only go up. Renewable energy prices come down. There is absolutely no business case for nuclear. The British policy has nothing to do with economic or any other rational base for decision making.\nIn December 2009 South Korea won a contract for four nuclear power plants to be built in the United Arab Emirates, for operation in 2017 to 2020.\n\nOn March 17, 2011, Israeli Prime Minister Benjamin Netanyahu stated that Israel was now unlikely to pursue civil nuclear energy.\n\nIn April 2010 Russia announced new plans to start building 10 new nuclear reactors in the next year.\n\nIn June 2009, Mark Cooper from the Vermont Law School said: \"The highly touted renaissance of nuclear power is based on fiction, not fact... There are numerous options available to meet the need for electricity in a carbon-constrained environment that are superior to building nuclear reactors\".\nIn September 2009, Luc Oursel, chief executive of Areva Nuclear Plants (the core nuclear reactor manufacturing division of Areva) stated: \"We are convinced about the nuclear renaissance\". Areva has been hiring up to 1,000 people a month, \"to prepare for a surge in orders from around the world\". However, in June 2010, Standard & Poor's downgraded Areva’s debt rating to BBB+ due to weakened profitability.\n\nIn 2010, Trevor Findlay from the \"Centre for International Governance Innovation\" stated that \"despite some powerful drivers and clear advantages, a revival of nuclear energy faces too many barriers compared to other means of generating electricity for it to capture a growing market share to 2030\".\n\nIn January 2010, the International Solar Energy Society stated that \"... it appears that the pace of nuclear plant retirements will exceed the development of the few new plants now being contemplated, so that nuclear power may soon start on a downward trend. It will remain to be seen if it has any place in an affordable future world energy policy\".\n\nIn March 2010, Steve Kidd from the World Nuclear Association said: \"Proof of whether the mooted nuclear renaissance is merely 'industry hype' as some commentators suggest or reality will come over the next decade\". In 2013 Kidd characterised the situation as a nuclear slowdown, requiring the industry to focus on better economics and improving public acceptance.\n\nIn August 2010, physicist Michael Dittmar stated that: \"Nuclear fission's contribution to total electric energy has decreased from about 18 per cent a decade ago to about 14 per cent in 2008. On a worldwide scale, nuclear energy is thus only a small component of the global energy mix and its share, contrary to widespread belief, is not on the rise\".\n\nIn March 2011, Alexander Glaser said: \"It will take time to grasp the full impact of the unimaginable human tragedy unfolding after the earthquake and tsunami in Japan, but it is already clear that the proposition of a global nuclear renaissance ended on that day\".\n\nIn 2011, Benjamin K. Sovacool said: \"The nuclear waste issue, although often ignored in industry press releases and sponsored reports, is the proverbial elephant in the room stopping a nuclear renaissance\".\n\n\n"}
{"id": "56842965", "url": "https://en.wikipedia.org/wiki?curid=56842965", "title": "Operational history of the Sukhoi Su-25", "text": "Operational history of the Sukhoi Su-25\n\nIn total, the Sukhoi Su-25 amassed a total of 60,000 sorties throughout its service in Afghanistan until the Soviet withdrawal in February 1989. The first combat Su-25 unit to be formed was the 200th Independent Attack Squadron (OShAE) based out of Sital-Chai in Soviet Azerbaijan. The first aircraft from the unit arrived in Afghanistan in the summer of 1981, with the full unit deploying in June to Shindand airfield in the western part of the country. The deployment was called Operation \"Exam\" and involved a team of engineers and technicians who would oversee the aircraft and evaluate its performance. The first Su-25 combat missions began on 25 July with the 200th OShAE flying counterinsurgency sorties and close air support in support of the 5th Motorized Rifle Division. By 1982, the Su-25's area of responsibility expanded and the aircraft began flying missions sometimes as far as Kabul with pilots flying on average 4–5, sometimes as many as 8, sorties a day. The aircraft typically flew with external fuel tanks to increase its range but was barred from flying in poor weather conditions or nighttime due to faulty navigational equipment. By the time the squadron left Afghanistan in October 1982, its Su-25s had logged more than 2,000 sorties with no losses, however some had sustained damage due to ground fire. The deployment allowed engineers to make several modifications to the aircraft's weapons systems, replacing them with ones more suited for mountain operations and installing increased countermeasures and defensive systems.\n\nThe introduction of the Su-25 in Afghanistan led to a vastly expanded scope of operations by Soviet ground commanders who could now insert their troops deeper into guerrilla territory with better air cover. The Mujahideen grew to fear and respect the Su-25 as the war grew on, adapting their tactics around the assumption of its use. Su-25s were used as development platforms for Soviet precision guided munitions, being some of the first Soviet aircraft to use such munitions in combat, typically against cave entrances or weapons storage sites. In April 1986, the Su-25 used a precision-guided weapon in combat for the first time, firing a Kh-25 and Kh-29 missile against enemy targets. The introduction of precision bombs and missiles allowed the Su-25 to begin flying missions at higher-altitude, reducing the risk posed by anti-aircraft artillery and shoulder launched surface-to-air missiles.\n\nDue to the fact Su-25s were often tasked with flying low-altitude, subsonic, close air support missions in support of ground forces in combat, they made frequent targets for Mujahideen anti-aircraft fire and surface-to-air missiles. Some 23 aircraft were shot down over the course of the war, with an additional dozen or so were lost in non-combat related incidents, while another dozen were written off due to extensive damage, and a further nine were destroyed by bombardments on the ground in Kabul and Kandahar. In all, Su-25s represented a quarter of Soviet Air Force (VVS) fixed-wing losses in the campaign.\n\nOn 16 January 1984, a Su-25 piloted by a Soviet lieutenant colonel was struck by a SA-7 missile and downed near Urgun, Afghanistan, making it the first shootdown of a Su-25 in the war.\n\nIn August 1988, a Su-25 piloted by Colonel Alexander Rutskoy was hit by anti-aircraft fire and crashed. Rutskoy ejected but landed on Pakistani soil where he was captured and later repatriated to the Soviets. Some sources state he was shot down by Pakistani warplanes. The aircraft's wreckage was reportedly salvaged by Pakistan and handed over to the United States. In October of that year, a Su-25 sustained a powerful hit at high-altitude north of Gardez, heavily damaging it but remaining flyable. The strike was reportedly the result of a missile fired by a Pakistani fighter jet.\n\nIn January 1989, a missile hit a Su-25 near Kabul killing the pilot in the last combat loss of the aircraft in Afghanistan.\n\nThe Iraqi Air Force was the first non-Eastern Bloc country to receive the Su-25, with Iraq receiving 69 'K' single-seat variants and four 'UBK' two-seater variants between 1986 and 1987, enough to equip two ground-attack regiments. The Su-25 saw extensive use in the latter stages of the Iran–Iraq War, its first action outside of Afghanistan, in combat against Iranian forces. One Su-25K purportedly survived a direct hit from an Iranian MIM-23 Hawk surface-to-air missile battery, managing to return to base and land. In total, the Su-25 took part in 900 combat sorties in the conflict and many of their pilots were personally decorated by Saddam Hussein. Two Su-25Ks were lost in combat in the war, of which one was lost to Iranian air defenses.\n\nThe Iraqi Air Force's Su-25 aircraft played little part in the Persian Gulf War as did most Iraqi aircraft. At least one Su-25 squadron saw action during the August 1990 Invasion of Kuwait. During the war, seven of the IAF's Su-25 aircraft were flown to neighboring Iran to shield them from air strikes. Two Su-25 jets were shot down by U.S. F-15C interceptors on 6 February 1991 using AIM-9 Sidewinder air-to-air missiles. However, most of Iraq's Su-25 fleet was destroyed on the ground in precision strikes on hardened aircraft shelters during the war and they were not used for ground attack against coalition forces.\n\nAngola received its first batch of Su-25 jets from the Soviet Union in 1988. During the Angolan Civil War, the Angolan Air Force used the Su-25 against UNITA forces advancing on Luanda but to little tactical effect as they often flew bombing missions from high altitudes, reducing the aircraft's effectiveness, an issue exacerbated by a withdrawal of Soviet support in 1991 which caused serviceability issues. The Su-25 was also flown by mercenaries from the South African private military company Executive Outcomes based out of Saurimo Airport against UNITA forces beginning in the early 1990s.\n\nDuring the War in Abkhazia, the Su-25 was used by the Georgian Air Force, the Abkhazian Air Force, and the Russian Air Force in a series of combat actions in the breakaway province. The Georgian Air Force flew as many as 215 combat missions in the Su-25 during the war. Russian Su-25s flying on the side of Abkhazian separatists performed numerous raids on Georgian positions near the besieged port city of Sukhumi, flying out of an airfield in Gudauta. On at least one occasion, a Russian Su-25 bombed a residential district of Sukhumi, killing a civilian and destroying numerous homes and other buildings. During the conflict, the Russian government asserted that Su-25s bombing Georgian positions were not Russian but, in fact, Georgian warplanes with painted Russian insignia, a claim that drew much doubt since many of these Su-25 aircraft were flying uninhibited through Russian airspace. On 6 January 1993, a Georgian Su-25 crashed in Abkhazia province with the pilot ejecting to safety. On 19 March 1993, a Russian Air Force Su-25 piloted by Major Vazlav Shipko was shot down by Georgian air defenses near Sukhumi, however some sources state he was flying a Su-27. On 4 July 1993, a Georgian Su-25 was struck by an Abkhazian surface-to-air missile, forcing the aircraft into the sea off Sukhumi and killing the pilot.\n\nThe Ethiopian Air Force operated the Su-25 during the Eritrean–Ethiopian War, after receiving a delivery of a pair of Su-25Ts and two Su-25UBKs from the Russian Air Force in March 2000. Combat operations were flown from Mek'ele and Debre Zeyit. On 15 May 2000, a Su-25 piloted by Flight Lieutenant Wondu Ghenda was shot down by an Eritrean Air Force MiG-29, killing the pilot.\n\nDuring the Nagorno-Karabakh War, both Armenia and Azerbaijan used Su-25 aircraft inherited from the Soviet Union. However, Armenia's use of its modest five jets during the war was minimal while the Azeri Air Force utilized it heavily in strafing and bombing missions against Armenian targets. At least three Su-25s operated by Azerbaijan were downed during the conflict. One was shot down in 1992 by a missile while conducting a ground attack mission over Armenian territory. In 1994, two were lost in separate incidents. One in February where the aircraft was struck by ground fire causing the pilot to eject and become captured. Another Su-25 was lost the following April with the pilot managing to eject. Human Rights Watch alleges in a 1994 report that Azerbaijan used its Su-25 aircraft in an indiscriminate manner against civilian population centers in Nagorno-Karabakh and Armenia.\n\nSu-25s saw heavy use by the Russian Air Force again during the First and Second Chechen War. During the First Chechen War, some 140 Su-25s flew 5,300 strike sorties, notably playing an instrumental role in the destruction and the capture of the heavily defended Grozny Presidential Palace, dropping BETAB-500 anti-concrete bombs on it. However, Russian Su-25s rarely operated at night or in poor weather due to faulty equipment and insufficient training, and aircraft rarely used precision munitions, utilizing unguided bombs and rockets for the most part instead. This lack of precision munitions and poor training led to several friendly fire incidents where Russian aircraft hit buildings occupied by Russian troops. However, Russian air losses during the campaign were low due to a lack of capable air defenses. Four Su-25s were lost in combat.\n\nDuring the initial push into Chechnya, Russian Su-25 and Su-24 strike aircraft destroyed the majority of the dilapidated Chechen air fleet on the ground in a matter of hours. In 1995, facilitated by clear weather, air cover provided by Su-25s allowed Russian troops to push across the Argun River and into the towns of Gudermes and Shali, setting the stage for a decisive victory over separatist forces in the region.\n\nChechen separatist leader Dzhokhar Dudayev was killed on 21 April 1996, after a Su-25 fired two laser-guided missiles at his position, after an A-50 aircraft pinpointed his satellite phone signal and relayed the location to the Su-25. A picture purportedly from the warhead as it approached Dudayev later surfaced in \"Argumenty i Fakty\", a Russian newspaper.\n\nIn 1999, Russian Su-25s once again saw heavy use in Chechnya with the onset of the Second Chechen War in 1999. During the war, the Su-25 was again primarily a close air support platform, providing support for ground troops at low altitude in conjunction with Mi-24 Hind and Ka-50 helicopters while Su-24s flew higher altitude precision strikes. Su-25s would often fly at altitudes between 16,000 and 20,000 feet before visually acquiring and attacking their targets at lower altitudes, typically utilizing unguided rockets and bombs. For more accurate strikes, Su-25s used Kh-25ML and Kh-29 laser-guided missiles. In a newer tactic not utilized during the First Chechen War, Su-25s engaged in \"free hunt\" operations, flying ahead of larger Russian formations in visual search of targets of opportunity. On occasion, these flights would be accompanied by reconnaissance Su-24s to increase the probability of locating mobile targets.\n\nA new all-weather with night capability variant, the Su-25T, was introduced into combat during this conflict with notable successes with six of the aircraft flying a total of 20 sorties throughout the fighting. The 'T' model had improved sighting and was modified to include the new B-13 rocket pod.\n\nSix Su-25 aircraft were lost during the Second Chechen War.\n\nIn June 2001, the government of Macedonia procured four Su-25 aircraft (three single seat, one two-seater) from Ukraine which promptly entered service in the Macedonian Air Force. In the first aerial combat action of the Macedonian military, they were used to combat ethnic Albanian insurgents in Macedonia. Mostly flown from an air force base near Petrovec, these aircraft were piloted by Ukrainian mercenaries as Macedonia did not have any pilots trained in flying the aircraft. They saw heavy use in ground attack missions against insurgents. During the Battle of Raduša in August 2001, the Su-25s supported beleaguered security forces in the town alongside Mi-24 helicopters.\n\nFour Su-25 aircraft (two one seaters, and two two-seaters) were procured from Belarus by the Ivory Coast in 2004. After the outbreak of the First Ivorian Civil War, the government of Laurent Gbagbo used his air force to bomb rebel forces. On 6 November 2004, a pair of Su-25 aircraft based out of Abidjan and piloted by mixed crews of Ivorians and Belorussians bombed a French encampment near the city of Bouaké, killing nine French Licorne peacekeepers and an American aid worker, and destroying several vehicles. The attack triggered the French to destroy the Ivorian air force, firing MILAN missiles at the two Su-25s that had attacked the French troops shortly after they landed, damaging them, and seizing the other two and disabling them.\n\nThe Sudanese Air Force has obtained fifteen Su-25 aircraft from Belarus since 2008. Despite Sudanese government assurances that the aircraft would not be used in the Darfur conflict, United Nations inspectors discovered four Su-25 bombers at El Fasher Airport in the region and numerous reports have surfaced of the aircraft taking part in indiscriminate targeting of villages.\n\nChad obtained six Su-25 jets from Ukraine to enhance its close air support capability. In May 2009, the Su-25s operated by the Chadian Air Force bombed rebel columns in southeastern Chad, near Abéché.\n\nThe Su-25 was once again pressed into service in the skies of Georgia during the Russo-Georgian War in 2008, seeing extensive use by both the Russian and Georgian air forces. At the beginning of the war, Georgia had a fleet of twelve Su-25 aircraft (two of them trainers) at its disposal. The aircraft saw particularly heavy use in fighting around Tskhinvali, the South Ossetian capital. Out of six Russian aircraft lost in the war, three were Su-25s. All were most likely lost to friendly fire.\n\nThe first Russian air loss of the campaign was a Su-25 piloted by Lieutenant Colonel Oleg Terebunsky of the 368th Attack Aviation Regiment, shot down over the territory of South Ossetia near the Zarsk pass, between Dzhava and Tskhinvali. He was hit by a MANPAD missile fired by South Ossetian militia around 6.00 p.m. on 8 August, in a case of friendly fire. Earlier in the day, a flight of four Georgian Su-25 planes had attacked a Russian army convoy in the same area. This was one of the few missions conducted by Georgia's Su-25s during the brief conflict as Tblisi reasoned that its aircraft would soon become easy targets for Russian interceptors. The aircraft returned to their bases and were hidden under camouflage netting to prevent them from being located.\n\nAt 10:30 a.m. on 9 August, a Su-25 piloted by Colonel Sergey Kobylash, commander of the 368th Attack Aviation Regiment, was shot down during an attack on a Georgian military formation south of Tskhinvali, on the Gori-Tskhinvali road. After making his initial approach, Kolybash's aircraft was struck by a missile that impacted his left engine, rendering it inoperable. Not long after, as Kobylash was returning to base at an altitude of 1,000 meters, a MANPAD missile struck his right engine, leaving the plane without thrust. Kobylash ejected north of Tskhinvali in a South Ossetian village of the Georgian enclave of in the Great Liakh gorge, where he was recovered by a Russian combat search and rescue team. Shortly after Kobylash was rescued, South Ossetian militants made the claim they had downed a Georgian Su-25 however this possibility was rejected as Georgia's air force had ceased air operations the day before, more than likely making the shootdown of the Su-25 another incident of friendly fire.\n\nAt 1:00 p.m. on 9 August, another Su-25 from the 368th Attack Aviation Regiment, piloted by Major Vladimir Edamenko, was struck by fire from a Russian ZSU-23-4 Shilka anti-aircraft gun near Dzhava killing the pilot. Two hours later, a Su-25 mistakenly attacked a Russian military convoy near Liakhva, destroying a fuel tanker and injuring several troops. The Russian soldiers returned fire with a MANPAD damaging the Su-25.\n\nRussian Su-25s also hit numerous targets deep inside Georgian territory including the Marneuli air base, however none of Georgia's Su-25 aircraft were lost. In Tskhinvali, Russian Su-25s attacked Georgian infantrymen as they rested in the outskirts of the city. The strikes killed 20 Georgian troops and injured countless more, causing the entire battalion to abandon their equipment and flee sparking rumors about the battalion's annihilation which played a hand in the Georgian withdrawal from the city. The Tbilisi Aircraft Manufacturing, a factory that produces Su-25s, was also bombed damaging a runway but otherwise causing no casualties.\n\nOn the night of 8 August, a Georgian Su-25 was shot down by a Russian Su-27. The pilot managed to eject but was reportedly executed by fighters on the ground.\n\nIn September 2015, the U.S. Department of Defense released satellite photos of an airfield in Latakia, a Syrian government-controlled city, showing twelve Su-25 planes on the tarmac. Airstrikes by Russian forces in support of the Assad government began later that same month. The aircraft have been used predominantly to strike opposition vehicles, fighting positions, artillery pieces, and other non-strategic targets in the country.\n\nIn January 2015, Syrian Air Force MiG-29s and Russian Su-25s took part in a joint-strike mission against militant targets in what was the first time Syrian and Russian warplanes flew a combat mission together.\n\nIn 2016, Alexander Galkin, the Russia’s Southern Military District commander, said Su-25s had flown more than 1,600 sorties and dropped around 6,000 bombs since the beginning of Russia's intervention in September 2015. In September 2017, the Russian Ministry of Defence released a video showing two Su-25s helping to rescue Russian troops encircled by Tahrir al-Sham militants. Two months later, a pair of Su-25s flying near Mayadin, in eastern Syria, was suddenly intercepted by a U.S. F-22 jet which abruptly entered the flight path of the two Russian warplanes and deployed flares and chaff in an attempt to cause the Su-25s to change their heading. The United States claimed the incident was not a provocation but a response to the Russian jets crossing the Euphrates River deconfliction line and not responding to radio calls.\n\nOn 3 February 2018, a Russian Su-25 piloted by Major Roman Filipov flying over Idlib Governorate was hit by a surface-to-air missile, downing the aircraft and forcing the pilot to eject. Filipov committed suicide to avoid capture on the ground.\n\nThe Ukrainian Air Force has made use of its fleet of Su-25 warplanes in the War in Donbass, using them for strike missions against pro-Russian separatists, and at times Russian government forces. During the First Battle of Donetsk Airport, Su-25s were witnessed partaking in ground attack missions, at times firing unguided rockets at separatist positions. During fighting at Kramatorsk Airport, Su-25s were used to deliver supplies to besieged Ukrainian troops, flying at a low altitude and parachuting provisions. A rocket and strafing attack on a regional administration building in Luhansk was purportedly carried out by a Ukrainian Su-25 aircraft. During the Battle of Debaltseve in 2015, Russian Air Force Su-25s participated in the route of Ukrainian troops and the subsequent capture of the city. After the shootdown of Malaysia Airlines Flight 17 in 2014, the Russian military cited the appearance of a Ukrainian Su-25 on its radar in the vicinity of the airliner as evidence that the Ukrainian military were the culprits behind the shootdown.\n\nFour Ukrainian Su-25 aircraft have been lost in combat during the conflict. On 16 July 2014, a Ukrainian Su-25 was struck by a missile and downed over eastern Ukraine. The pilot was able to eject and survived. The Ukrainian government placed the blame for the shootdown on a Russian Su-27 fighter jet.\n\nOn 23 July 2014, a pair of Su-25 jets flying near separatist-held Savur-Mohyla were shot down while flying at an altitude of 17,000 feet (5,200 meters). Both pilots were able to eject. The Ukrainian military said the aircraft had been downed by a long-range missile system, possibly fired from Russia.\n\nOn 29 August 2014, a Su-25 flying near Luhansk was shot down by a surface-to-air missile. The pilot ejected and was recovered by Ukrainian troops.\n\nIn late June 2014, twelve Su-25 planes arrived in Iraq aboard cargo planes after being purchased second-hand from Russia, entering combat service shortly later in the war against the Islamic State, despite having few munitions in its stockpile compatible with the type. The delivery was accompanied by technicians in order to train the Iraqis in the plane's operation and maintenance. The purchase was made after several delays complicated Baghdad's purchase of U.S. F-16 fighters. Seven more Su-25s from Iran arrived in Iraq before the end of June as well. These aircraft were formerly Iraqi Air Force jets that had fled to Iran during the 1990–1991 Gulf War. Experts noted that some of the aircraft has visible signs of wear and corrosion. In 2015, Iraqi pilots received training in Belarus on flying the Su-25. By 2016, Iraq had 21 Su-25s in its possession.\n\nIraqi Air Force Su-25s were used for close air support during the Battle of Ramadi and other battles in western Anbar Province. On 12 August 2014, a Su-25 piloted by Iraqi Colonel Jalil Al Awadi crashed after takeoff at Muthenna Air Base, severely damaging the plane and killing the pilot. Another Su-25 was withdrawn from service after suffering damage when it was hit by a surface-to-air missile during a mission over Kirkuk.\n\n"}
{"id": "52247264", "url": "https://en.wikipedia.org/wiki?curid=52247264", "title": "Philips Hue", "text": "Philips Hue\n\nPhilips Hue is a line of color changing LED lamps and white bulbs which can be controlled wirelessly. It was introduced in October 2012 and was updated in 2015 and 2016. The lamps are currently created and manufactured by Signify N.V., formerly the Philips Lighting division of Royal Philips N.V..\nThe Hue system was released in October 2012 as an Apple Store exclusive. The Hue system was marketed as the first iOS controlled lighting appliance. It uses the Zigbee lighting protocol to communicate, and can be controlled via smartphone apps over cellular network, Ethernet or Wi-Fi via a Zigbee–Ethernet bridge wired to a router. The initial system had bulbs capable of producing up to 600 lumens while the newer systems have bulbs that shine up to 800 lumens. The newer system also features HomeKit compatibility. In July 2018, an outdoor version of the Philips Hue suite was introduced, and in October 2018 a suite of entertainment-focused, free-standing light fittings.\n\nSecurity issues have been noted in the product including an issue in which the bulbs were able to be remotely controlled over the Internet by attackers with inexpensive equipment. In another proof-of-concept, researchers were able to remotely control light bulbs using a drone flying outside a window.\n\nIn an article in Forbes, Sath Porges called Phillips Hue the \"best product of 2012\". PC Magazine reviewed the white variation and named it as an editors' choice and said it was bright and affordable and had lots of features.\n"}
{"id": "26196692", "url": "https://en.wikipedia.org/wiki?curid=26196692", "title": "Remote therapy", "text": "Remote therapy\n\nRemote therapy, sometimes called telemental health applications or Internet-based psychotherapy, is a form of psychotherapy or related psychological practice in which a trained psychotherapist meets with a client or patient via telephone, cellular phone, the internet or other electronic media in place of or in addition to conventional face-to-face psychotherapy.\n\nInitially, it was primarily intended a substitution for conventional, face-to-face therapy in which a client or patient is required to visit a psychotherapists office. Increasingly, however, academics are studying the use of electronic media in treatment to explore the possibility of providing novel and potentially more effective therapies.\n\nAfter reviewing thirteen relevant studies, the authors of a meta-analytic review of psychotherapy mediated by remote communications technology concluded that:\n\nRemote therapy has the potential to overcome some of the barriers to conventional psychological therapy services. Telephone-based interventions are a particularly popular research focus and as a means of therapeutic communication may confer specific advantages in terms of their widespread availability and ease of operation. However, the available evidence is limited in quantity and quality. More rigorous trials are required to confirm these preliminary estimates of effectiveness.\n\nDespite the absence of complete study, remote therapy has enjoyed growing popularity as a replacement for traditional therapy and innovative practice made possible by electronic medium. Examples include:\n\n"}
{"id": "24822862", "url": "https://en.wikipedia.org/wiki?curid=24822862", "title": "Robotino", "text": "Robotino\n\nRobotino is a mobile robot system made by \"Festo Didactic\", and used for educational, training and research purposes.\n\nRobotino is based on an omnidirectional drive assembly, which enables the system to roam freely. The robot is controlled by an industry-standard PC system, which is powerful enough to plan routes for fully autonomous driving. Via a WLAN-Link, Robotino can send all sensor readings to an external PC. In the other direction, control commands can be issued by the external PC. This way, control programs can run on the external PC or on Robotino directly. Mixed mode or shared control are also possible.\nFor users with little prior robotics knowledge, Robotino can be readily programmed in its “native” programming environment RobotinoView II. More experienced programmers may find it useful that the robot can also be programmed in C, C++, Java, .NET, Matlab, Simulink, Labview and Microsoft Robotics Developer Studio.\n\nThe omnidirectional drive consists of three Mecanum wheels, all of which are individually controllable. These wheels are arranged at angles of 120°. Robotino has a bumper sensor around its circumference, infrared distance sensors, a color camera with VGA resolution, optical wheel encoders, power measurement for the entire system and the various motors, as well as a battery voltage monitor. Moreover, as optional additional sensors, Robotino can be equipped with a precise laser scanner, a gyroscope, and an indoor positioning system (created by Evolution Robotics). For signal input and output Robotino has several interfaces:\n\nPower is supplied by two 12V/5Ah lead-acid batteries or optionally by two 12V/9Ah NiMH batteries.\n\n"}
{"id": "14553", "url": "https://en.wikipedia.org/wiki?curid=14553", "title": "Secondary sector of the economy", "text": "Secondary sector of the economy\n\nThe secondary sector of the economy includes industries that produce a finished, usable product or are involved in construction.\n\nThis sector generally takes the output of the primary sector and manufactures finished goods or where they are suitable for use by other businesses, for export, or sale to domestic consumers. This sector is often divided into light industry and heavy industry. Many of these industries consume large quantities of energy and require factories and machinery to convert raw materials into goods and products. They also produce waste materials and waste heat that may cause environmental problems or cause pollution. The secondary sector supports both the primary and tertiary sector.\n\nSome economists contrast wealth-producing sectors in an economy such as manufacturing with the service sector which tends to be wealth-consuming.[1] Examples of service may include retail, insurance, and government. These economists contend that an economy begins to decline as its wealth-producing sector shrinks.[2] Manufacturing is an important activity to promote economic growth and development. Nations that export manufactured products tend to generate higher marginal GDP growth which supports higher incomes and marginal tax revenue needed to fund the quality of life initiatives such as health care and infrastructure in the economy. The field is an important source for engineering job opportunities. Among developed countries, it is an important source of well-paying jobs for the middle class to facilitate greater social mobility for successive generations on the economy. \n"}
{"id": "2501933", "url": "https://en.wikipedia.org/wiki?curid=2501933", "title": "Security lighting", "text": "Security lighting\n\nIn the field of physical security, security lighting is lighting that intended to deter or detect intrusions or other criminal activity on a piece of real property. It can also be used to increase a feeling of safety. Lighting is integral to crime prevention through environmental design.\n\nSecurity lighting can be counter-productive. Turning off lights halved the number of thefts and burglary in Övertorneå Sweden. \n\nA test in West Sussex UK showed that adding all-night lighting in some areas made people there feel safer, although crime rates increased 55% in those areas compared to control areas and to the county as a whole.\n\nBright, unshielded floodlights often prevent people from noticing criminal activity, and help criminals see what they are doing.\n\nWhile adequate lighting around a physical structure is deployed to reduce the risk of an intrusion, it is critical that the lighting be designed carefully as poorly arranged lighting can create glare which actually obstructs vision.\nStudies have shown that many criminals are aware of this effect and actively exploit it.\nThe optimal design will also depend on whether the area will be watched directly by humans or by closed-circuit television, and on the location of the observers or cameras.\n\nSecurity lighting may be subject to vandalism, possibly to reduce its effectiveness for a subsequent intrusion attempt. Thus security lights should either be mounted very high, or else protected by wire mesh or tough polycarbonate shields. Other lamps may be completely recessed from view and access, with the light directed out through a light pipe or reflected from a polished aluminium or stainless steel mirror. For similar reasons high security installations may provide a stand-by power supply for their security lighting.\n\nSome typical considerations include:\n\n\nSecurity lighting can be used in residential, commercial, industrial, institutional, and military settings. Some examples of security lighting include floodlights and low pressure sodium vapour lights. Most lights intended to be left on all night are high-intensity discharge lamps as these have good energy efficiency, thus reducing the cost of running a lamp for such long periods.\n\nA disadvantage of low pressure sodium lamps is that the colour is pure yellow, so the illuminated scene is seen without any colour differentiation. Consequently, high pressure sodium vapour lamps (which are still yellowish, but closer to golden white) are also used, at the cost of greater running expenses and increased light pollution. High pressure sodium lamps also take slightly longer to restrike after a power interruption.\n\nOther lights may be activated by sensors such as passive infrared sensors (PIRs), turning on only when a person (or other mammal) approaches. PIR activated lamps will usually be incandescent bulbs so that they can activate instantly; energy saving is less important since they will not be on all the time. PIR sensor activation can increase both the deterrent effect (since the intruder knows that he has been detected) and the detection effect (since a person will be attracted to the sudden increase in light). Some PIR units can be set up to sound a chime as well as turn on the light. Most modern units have a photocell so that they only turn on when it is dark.\n\nAn important limitation to the usefulness of security lighting is the simple fact that it is only useful at night. This is particularly significant for home owners because, contrary to a widespread myth, most household burglaries occur during the day, when the occupants are away at work or shopping.\n\nAs with any lighting, security lighting can reduce night vision, making it harder to see into areas that are unlit or are in shadow.\n\n"}
{"id": "35321005", "url": "https://en.wikipedia.org/wiki?curid=35321005", "title": "Simon House, Mount Eliza", "text": "Simon House, Mount Eliza\n\nSimon House is at 33 Daveys Bay Road, Mount Eliza in Melbourne, Australia. The house was constructed in the 1960s during the period of architectural experimentation on materiality and structure in houses. Simon House was the first project received on the day when Guilford Bell and Neil Clerehan opened their office in South Yarra. The Simons were Clerehan’s partners but the house was a collaboration of partners. Therefore, the house has a mixture of both Clerehan and Bell’s typical style of architecture.\n\nThe house is a single-storey house, planned in a rectangular shape. The house consists of five bedrooms, two toilets, two garages, one kitchen, dining room, living room, pool and terrace. The enclosed pool, natural finishes, glazed wall and an extensive view over the bay recalled contemporary Californian house of architects like Craig Ellwood. \n\nThe central core program of the house was a pool and terrace, positioned right in the centre of the house. With the pool and terrace in the centre with programs surrounds \nthat engaged with it, allows family members to gather together. It was said that was one of Clerehan’s conception of the house where how he deal with the provision of a frame for family life.\nThe pool and living room aligned on axis, forming the central part of a symmetrical square plan. Then the programs around the pool were given a corresponding position in the house’s spatial hierarchy. Because of this distinct insistence of having an equivalent position of programs, the programs are mirrored and consist of dual garages and entrances. It was said that was one of Bell’s typical style of architecture.\n\nBell has been influenced by the architecture and gardens of Japan. As a countryman, he feels, when in the city, a hunger for the freedom of sky and vegetation. He thinks that a building must stand in spectacular relationship to its environment. With the pool and terrace positioned in the centre of the house, the engagement between the indoor and outdoor spaces then became ambiguous. It creates a sense of feeling when one is inside, it will feels like outdoors and when one is outside it will feels like indoors. One of his design principal was that he thinks of views or views from the building first, instead of the form of the building. Therefore, glass panels are used across the living and dining areas to capture the view of the bay.\n\nUnfortunately, one of the garage have been renovated into a main entrance, also as a living room. Another garage is built right opposite the existing garage. In this approach, it loses the spatial qualities of the house but in turn allows a convenient use to the owner in the house’s circulation.\nThe materiality that uses in the house were unadorned, they are fibrous plaster ceiling left in natural, unpainted stated, flat steel decking roof, timber walls, and reinforced concrete floor slab under concrete block.\n\nNeil Clerehan and Guilford Bell has received numerous honours for his architectural works while they were practicing architecture. The Simon House was the most prominent work of Bell Clerehan partnership.\nThe home beautiful considered the pool, terrace to be one of Australia’s outstanding outdoor areas. The Simon house won the Victorian Architecture Medal in 1963-1964.\n"}
{"id": "47443594", "url": "https://en.wikipedia.org/wiki?curid=47443594", "title": "Singularity Rising", "text": "Singularity Rising\n\nSingularity Rising: Surviving and Thriving in a Smarter, Richer, and More Dangerous World is a book by James D. Miller that covers a broad spectrum of topics associated with the technological singularity, including cognitive enhancement and AI.\n"}
{"id": "58490571", "url": "https://en.wikipedia.org/wiki?curid=58490571", "title": "Slicer (3D printing)", "text": "Slicer (3D printing)\n\nThe slicer, also called slicing software, is a computer software used in the majority of 3D printing processes for the conversion of a 3D object model to specific instructions for the printer. In particular, the conversion from a model in STL format to printer commands in g-code format in fused filament fabrication and other similar processes \n\nThe slicer first divides the object as a stack of flat layers, followed by describe these layers as linear movements of the 3D printer extruder, fixation laser or equivalent. All these movements, together with some specific printer commands like the ones to control the extruder temperature or bed temperature, are finally written in the g-code file, that can after be transferred to the printer. \n\nNear all slicers have some additional features, like:\n\nIt exists a wide collection of slicer applications, some of them free and open-source. Some of the most used ones are :\n\n- Ultimaker Cura \n\n- Simplify3D\n\n- Slic3r\n\n- KISSlicer\n"}
{"id": "26178479", "url": "https://en.wikipedia.org/wiki?curid=26178479", "title": "South Carolina Hydrogen &amp; Fuel Cell Alliance", "text": "South Carolina Hydrogen &amp; Fuel Cell Alliance\n\nThe South Carolina Hydrogen & Fuel Cell Alliance (SCHFCA) is a Public-private collaborative with a mission of advancing the commercialization of hydrogen fuel cell technologies in the state of South Carolina. Government entities, in particular, the Department of Energy has funded SCHFCA with $188,788 for a hydrogen education program for state and local officials. State taxpayers have already chipped in $12.3 million for hydrogen fuel cell development, while other non-state entities like federal and private sources have invested nearly $115 million into the development of the technology.\n\nSCHFCA was founded in 2006 by six core institutions that were devoted to Hydrogen & Fuel Cell initiatives and development. \n\n\n\n"}
{"id": "40979903", "url": "https://en.wikipedia.org/wiki?curid=40979903", "title": "Sparx (video game)", "text": "Sparx (video game)\n\nSPARX (Smart, Positive, Active, Realistic, X-factor thoughts) is a free online computer program for New Zealand residents only, intended to help young persons with mild to moderate depression, stress or anxiety. Through the game, this e-therapy will teach them how to resolve their issues on their own, according to a talking psychotherapeutic approach called cognitive behavioural therapy.\nBefore taking part in this game, a personality test is required to determine if SPARX will be suited and helpful for the future user.\n\nBased in a 3D fantasy world, the game leads players through seven realms (each lasting between 30 and 40 minutes). In the beginning of SPARX, the user meets the Guide who explains what SPARX is and how it could help. Then the user customizes an avatar and starts to journey within the seven provinces in order to complete different quests. In the first level, gamers challenge GNATS (Gloomy Negative Automatic Thoughts). These GNATS fly towards the avatar and say negative things like, for example: \"you're a loser\". Further in the game, the user meets different characters, solves puzzles and completes mini games. As soon as a quest is completed, the Guide explains how to use new skills in order to feel better, solve problems and enjoy real life. Players complete one or two levels in the game each week, during three to seven weeks.\n\nUp to a quarter of young people will have experienced a depressive disorder by the age of 19. This is a major cause of disability. Youth depression can affect every aspect of a teen's life and lead to the use of drugs, an abuse of alcohol and, in certain extreme cases, to suicide. \"Sparx\" is designed to help adolescents reduce their depression symptoms.\nBetween 2009 and 2010, the Sparx team conducted an evaluation (a randomised controlled trial) of SPARX with 187 young people in order to see if it was effective in treating the symptoms of depression. They compared SPARX to standard care provided to young people with mild to moderate depression (e.g. a face-to-face therapy with a counsellor or a clinical psychologist).\n170 adolescents were assessed after intervention and 168 were assessed at the three-month follow-up point. Per protocol analyses showed that SPARX was not inferior to the most commonly used treatment.\n\nIn 2012, a group of researchers led by Dr Theresa Fleming, who is working in the department of Child and Youth Health and Psychological Medicine, decided to test the game. They recruited 187 teens with mild to moderate depression and assigned them to one of two groups: the first group played the video game and the other group received typical treatment from trained counselors at schools and youth clinics. More than 60 percent of the participants were girls with an average age of 16.\nIn both groups, levels of anxiety and depression were reduced by about one-third of them. The video game, however, helped more kids between 12 and 19 years old to recover from their depression. About 43.7 percent achieved remission in the SPARX group, compared to the 26.4 percent in usual care.\n\nBehind the SPARX project is a team of researchers and clinicians from The University of Auckland.\nPr Sally Merry, Dr Karolina Stasiak, Dr Theresa Fleming, Dr Matt Shepherd and Dr Mathijs Lucassen created it.\nAssociate Professor Sally Merry is a child and adolescent Psychiatrist, Head of Department of Psychological Medicine and Director of The Werry Centre for Child and Adolescent Mental Health.\nDr Stasiak also coordinated the main study of SPARX.\nDrs Fleming, Shepherd and Lucassen carried out doctoral studies of SPARX.\n\nIn 2011, SPARX won the World Summit Award, supervised by the United Nations, in the category of e-Health and Environment which honors excellence in multimedia and e-Content creation. \nSPARX was also rewarded by the 2013 International Digital Award from Netexplo, hosted by UNESCO, for being the first out of ten most innovative and promising digital initiative of the year.\n\nAfter a such success, in 2012 Dr Lucassen decided to develop another version of SPARX entitled Rainbow SPARX to help adolescents attracted to the same sex, or both sexes, or unsure of their sexuality. According to a small study carried out by Mathijs Lucassen, himself, more than 80% of the participants said that the game helped them to deal with their sexuality and would recommend it to other people.\n\n"}
{"id": "2435710", "url": "https://en.wikipedia.org/wiki?curid=2435710", "title": "Stirling Energy Systems", "text": "Stirling Energy Systems\n\nStirling Energy Systems is a Scottsdale, Arizona-based company which developed equipment for utility-scale renewable energy power plants and distributed electrical generating systems using parabolic dish and stirling engine technology, touted as the highest efficiency solar technology.\n\nIn April 2008, Ireland-based NTR purchased a majority stake in Stirling Energy Systems for $100M. As of 8/3/2011 NTR reported they were seeking 3rd party investment in Stirling Energy Systems.]\n\nOn 29 September 2011 Stirling Energy Systems filed for Chapter 7 bankruptcy, due to falling PV prices caused by subsidized Chinese Photo Voltaic \n\nIn April 2012 the Maricopa Solar plant in Phoenix, Arizona was bought by British company United Sun Systems in a joint venture with a Chinese/American corporation.\n\nAccording to their website, Stirling Energy Systems (SES) is a systems integration and project management company that is developing equipment for utility-scale renewable energy power plants and distributed electric generating systems (\"gensets\"). SES is teamed with Kockums Submarine Systems, NASA-Glenn Research Center, the U.S. Department of Energy (DOE), and The Boeing Company for solar power plants. SES claims it is positioned to become a premier worldwide renewable energy technology company to meet the global demand for renewable electric generating technologies through the commercialization of its own stirling cycle engine technology for solar power generation applications.\n\nOn 29 September 2011, Stirling Energy Systems filed for Chapter 7 bankruptcy as the Stirling dish technology could not compete against the falling costs of solar photovoltaics, according to media reports. The falling photovoltaic prices were caused by Chinese subsidies.\n\nIn April 2012 the Maricopa Solar plant in Phoenix, Arizona was bought by a European formation based in London called United Sun Systems\n\nAt the beginning of 2011 Stirling Energy's development arm, Tessera Solar, sold off its two large projects, the 709 MW Imperial Valley Solar Project and the 850 MW Calico Solar Energy Project to AES Solar and K.Road, respectively.\n\n\n"}
{"id": "43879696", "url": "https://en.wikipedia.org/wiki?curid=43879696", "title": "Tang Prize", "text": "Tang Prize\n\nThe Tang Prize () is a set of biennial international awards bestowed in four fields: Sustainable Development, Biopharmaceutical Science, Sinology, and Rule of Law. Nomination and selection are conducted by an independent selection committee, which is formed in partial cooperation with the Academia Sinica, Taiwan's top research institution.\n\nIn the advent of industrialization and globalization, humanity has greatly enjoyed the convenience brought about by science and technology. Yet, humanity also faces a multitude of critical environmental, socio-cultural, and ethical issues on an unparalleled scale, such as climate change, inequality, and moral degradation. Against this backdrop, a Taiwanese entrepreneur Samuel Yin established the Tang Prize Foundation in December 2012.\n\nWith the aim to recognize and support contributors for their revolutionary efforts in the research fields critical to the 21st century, the Tang Prize is global in reach. Laureates are selected on the basis of the originality of their work along with their contributions to society, irrespective of gender, religion, ethnicity, or nationality.\n\nThe award categories of the Tang Prize include Sustainable Development, Biopharmaceutical Science, Sinology, and Rule of Law.\n\nThe Prize in Sustainable Development recognizes those who have made extraordinary contributions to the sustainable development of human societies, especially through groundbreaking innovations in science and technology.\n\nThe Prize in Biopharmaceutical Science recognizes original biopharmaceutical or biomedical research that has led to significant advances towards preventing, diagnosing and/or treating major human diseases to improve human health.\n\nThe Prize in Sinology recognizes the study of Sinology in its broadest sense, awarding research on China and its related fields, such as Chinese thought, history, philology, linguistics, archaeology, philosophy, religion, traditional canons, literature, and art (excluding literary and art works). Honoring innovations in the field of Sinology, the Prize showcases Chinese culture and its contributions to the development of human civilization.\n\nThe Prize in Rule of Law recognizes individual(s) or institution(s) who have made significant contributions to the rule of law, reflected not only in the achievement of the candidate(s) in terms of the advancement of legal theory or practice, but also in the realization of the rule of law in contemporary societies through the influences or inspiration of the work of the candidate(s).\n\nEach laureate receives a Tang Prize medal and diploma. In addition, NT$40 million (US$1.3 million) cash prize is awarded in each category, as well as a research grant of NT$10 million (US$0.33 million), for a total of NT$50 million (US$1.63 million). Should two, or up to three, candidates receive an award in the same category, the cash prize and research grant are shared.\nNomination and selection for the first and second Tang Prize cycles (2013-2014 and 2015-2016, respectively) were conducted by the Academica Sinica on commission of the Tang Prize Foundation; beginning with the third prize cycle (2017-2018), nomination and selection are now conducted by an independent selection committee which is formed in partial cooperation with the Academia Sinica.\n\nThe Tang Prize Selection Committee is composed of four separate committees, one per prize category. The committees invite respected scholars and institutions from around the world, including many Nobel laureates, to submit nominees, ensuring those nominated have attained a sufficient level of achievement.\n\nEvents during the award year:\n"}
{"id": "50232146", "url": "https://en.wikipedia.org/wiki?curid=50232146", "title": "Thermal cleaning", "text": "Thermal cleaning\n\nThermal cleaning is a combined process involving pyrolysis and oxidation. As an industrial application, thermal cleaning is used to remove organic substances such as polymers, plastics and coatings from parts, products or production components like extruder screws, spinnerets and static mixers. Thermal cleaning is the most common cleaning method in industrial environment. A variety of different methods have been developed so far for a wide range of applications.\n\nHeat is supplied for pyrolysis and air is supplied for oxidation. Depending on the procedure, pyrolysis and oxidation can be applied consecutively or simultaneously. During thermal cleaning, organic material is converted into volatile organic compounds, hydrocarbons and carbonized gas. Inorganic elements remain. Typical process temperatures range between 750 °F to 1000 °F (400 °C to 540 °C).\n\nSeveral types of industrial thermal cleaning systems are available:\n\nFluidized bed systems use sand or aluminium oxide (alumina) as heating medium. They apply pyrolysis and oxidation simultaneously. These systems clean fast, from 30 minutes process time up to two hours. The medium does not melt or boil, nor emit any vapors or odors. Thermal shock can be a problem with some parts. Pollution control devices may be needed to protect the environment.\n\nVacuum ovens use pyrolysis in a vacuum. This method is very safe because uncontrolled combustion inside the cleaning chamber is avoided. The cleaning process in this relatively new approach takes 8 to 30 hours. Vacuum pyrolysis is the only method that applies pyrolysis and oxidation consecutively. In two-chamber versions, molten plastic drains into an unheated chamber to capture the bulk of the polymer to reduce the fumes. Vacuum ovens are also electrically powered.\n\nBurn-off ovens, also known as heat-cleaning ovens, are gas-fired and used for removing organics from heavy and large metal parts. The process time is moderate, approximately 4 hours. Fires can occur from the fumes created during cleaning. The design is simple and inexpensive. Different types are available. Modern types contain an additional afterburner that operates at a minimum of 1,500°F (816°C) and consumes any smoke created by the process.\n\nMolten salt baths belong to the oldest thermal cleaning systems. Cleaning with molten salt is fast: 15 to 30 minutes process time. The process has the risk of dangerous splatters due to chemical reactivity, or other potential hazards, like explosions or toxic hydrogen cyanide gas. Another disadvantage is that parts can be warped or altered in design tolerances. Molten salt baths can be environmentally unfriendly. Due to their disadvantages, they are rarely used today.\n\n"}
{"id": "6109368", "url": "https://en.wikipedia.org/wiki?curid=6109368", "title": "Thermal treatment", "text": "Thermal treatment\n\nThermal treatment is any waste treatment technology that involves high temperatures in the processing of the waste feedstock. Commonly this involves the combustion of waste materials.\n\nSystems that are generally considered to be thermal treatment include:\n\n\n\n"}
{"id": "34958128", "url": "https://en.wikipedia.org/wiki?curid=34958128", "title": "Tritucap", "text": "Tritucap\n\nThe Tritucap (from , chipping of the underwood) is a secondary forest wood chipper used for land preparation in tropical agricultural systems. Its scope is to replace slash and burn practices in shifting cultivation settings, thus preventing soil erosion and displacement of nutrients.\n\nThe Tritucap was developed by the Institute of Agricultural Engineering of the University of Göttingen within the project \"SHIFT Capoeira\", funded by the German Federal Ministry of Education and Research.\n"}
{"id": "43385718", "url": "https://en.wikipedia.org/wiki?curid=43385718", "title": "Vi-Jon Laboratories", "text": "Vi-Jon Laboratories\n\nVi-Jon Laboratories is a health and beauty care company that produces both Private Label and brand name products. Headquartered in the St. Louis suburb of Overland, they operate five manufacturing and distribution centers in Missouri and Tennessee. The company's products are supplied nationwide to retailers such as Kroger, Target, and Walgreens. Vi-Jon was founded as Peroxide Specialty Company in 1908 by John B. Brunner. Vi-Jon has been owned by investment firm Berkshire Partners since 2006. The company markets a line of hand sanitizers in the United States under the brand Germ-X.\n\nIn 2006 Procter & Gamble filed a lawsuit against Vi-Jon alleging that Vi-Jon copied the look of Crest Pro-Health mouth wash; the parties settled a few months later, with Vi-Jon agreeing to withdraw its product from the market, to not use bottle designs that are confusingly similar to those of P&G, to not make \"compare to Crest Pro-Health\" or gingivitis efficacy claims without specific testing to support these claims, and to pay P&G.\n\nIn August 2014 Berkshire announced its intention to sell Vi-Jon; as of March 2016 the company had not been sold.\n\n"}
{"id": "24445462", "url": "https://en.wikipedia.org/wiki?curid=24445462", "title": "Waterladder pump", "text": "Waterladder pump\n\nA waterladder pump, water ladder, dragon spine, dragon wheel or rahad is a low lift pump which is composed of sprockets that move a chain with paddles over a trough. Water is pumped as the paddles push the water up the trough.\n\nThe water ladder, as many low lift pumps, is commonly used for irrigation purposes and for drainage of lands. It is currently still used by farmers in south-east Asia.\n\nThe water ladder was built as an alternative to the paddle wheel to get around the problem that to lift water to a greater height, a bigger wheel is needed. Despite the emergence of new pumps that operate on other principles, the water ladder remains an important tool as some of its other benefits are that they can be built and repaired easily at a very low cost. This is possible as all the components can be built from local resources, such as wood; which can be obtained and shaped into the desired form easily.\n\nAs mentioned before, the pump only allows the lifting of water over a small height. This makes it unsuitable to water drainage or irrigation over larger height differences or many other pumping applications besides drainage and irrigation.\n\n"}
{"id": "41863", "url": "https://en.wikipedia.org/wiki?curid=41863", "title": "Waveguide", "text": "Waveguide\n\nA waveguide is a structure that guides waves, such as electromagnetic waves or sound, with minimal loss of energy by restricting expansion to one dimension or two. There is a similar effect in water waves constrained within a canal, or guns that have barrels which restrict hot gas expansion to maximize energy transfer to their bullets. Without the physical constraint of a waveguide, wave amplitudes decrease according to the inverse square law as they expand into three dimensional space.\n\nThere are different types of waveguides for each type of wave. The original and most common meaning is a hollow conductive metal pipe used to carry high frequency radio waves, particularly microwaves.\n\nThe geometry of a waveguide reflects its function. Slab waveguides confine energy in one dimension, fiber or channel waveguides in two dimensions. The frequency of the transmitted wave also dictates the shape of a waveguide: an optical fiber guiding high-frequency light will not guide microwaves of a much lower frequency. \n\nSome naturally occurring structures can also act as waveguides. The SOFAR channel layer in the ocean can guide the sound of whale song across enormous distances.\n\nWaves propagate in all directions in open space as spherical waves. The power of the wave falls with the distance \"R\" from the source as the square of the distance (inverse square law). A waveguide confines the wave to propagate in one dimension, so that, under ideal conditions, the wave loses no power while propagating.\nDue to total reflection at the walls, waves are confined to the interior of a waveguide.\n\nThe first structure for guiding waves was proposed by J. J. Thomson in 1893, and was first experimentally tested by Oliver Lodge in 1894. The first mathematical analysis of electromagnetic waves in a metal cylinder was performed by Lord Rayleigh in 1897.\nFor sound waves, Lord Rayleigh published a full mathematical analysis of propagation modes in his seminal work, “The Theory of Sound”. Jagadish Chandra Bose researched millimetre wavelengths using waveguides, and in 1897 described to the Royal Institution in London his research carried out in Kolkata.\n\nThe study of dielectric waveguides (such as optical fibers, see below) began as early as the 1920s, by several people, most famous of which are Rayleigh, Sommerfeld and Debye. \nOptical fiber began to receive special attention in the 1960s due to its importance to the communications industry.\n\nThe development of radio communication initially occurred at the lower frequencies because these could be more easily propagated over large distances. The long wavelengths made these frequencies unsuitable for use in hollow metal waveguides because of the impractically large diameter tubes required. Consequently, research into hollow metal waveguides stalled and the work of Lord Rayleigh was forgotten for a time and had to be rediscovered by others. Practical investigations resumed in the 1930s by George C. Southworth at Bell Labs and Wilmer L. Barrow at MIT. Southworth at first took the theory from papers on waves in dielectric rods because the work of Lord Rayleigh was unknown to him. This misled him somewhat; some of his experiments failed because he was not aware of the phenomenon of waveguide cutoff frequency already found in Lord Rayleigh's work. Serious theoretical work was taken up by John R. Carson and Sallie P. Mead. This work led to the discovery that for the TE mode in circular waveguide losses go down with frequency and at one time this was a serious contender for the format for long distance telecommunications.\n\nThe importance of radar in World War II gave a great impetus to waveguide research, at least on the Allied side. The magnetron developed in 1940 by John Randall and Harry Boot at the University of Birmingham in the United Kingdom provided a good power source and made microwave radars feasible. The most important centre of research was at the Radiation Laboratory (Rad Lab) at MIT but many others took part in the US, and in the UK such as the Telecommunications Research Establishment. The head of the Fundamental Development Group at Rad Lab was Edward Mills Purcell. His researchers included Julian Schwinger, Nathan Marcuvitz, Carol Gray Montgomery, and Robert H. Dicke. Much of the Rad Lab work concentrated on finding lumped element models of waveguide structures so that components in waveguide could be analysed with standard circuit theory. Hans Bethe was also briefly at Rad Lab, but while there he produced his small aperture theory which proved important for waveguide cavity filters, first developed at Rad Lab. The German side, on the other hand, largely ignored the potential of waveguides in radar until very late in the war. So much so that when radar parts from a downed British plane were sent to Siemens & Halske for analysis, even though they were recognised as microwave components, their purpose could not be identified.\n\nGerman academics were even allowed to continue publicly publishing their research in this field because it was not felt to be important.\n\nImmediately after World War II waveguide was the technology of choice in the microwave field. However, it has some problems; it is bulky, expensive to produce, and the cutoff frequency effect makes it difficult to produce wideband devices. Ridged waveguide can increase bandwidth beyond an octave, but a better solution is to use a technology working in TEM mode (that is, non-waveguide) such as coaxial conductors since TEM does not have a cutoff frequency. A shielded rectangular conductor can also be used and this has certain manufacturing advantages over coax and can be seen as the forerunner of the planar technologies (stripline and microstrip). However, planar technologies really started to take off when printed circuits were introduced. These methods are significantly cheaper than waveguide and have largely taken its place in most bands. However, waveguide is still favoured in the higher microwave bands from around Ku band upwards.\n\nThe uses of waveguides for transmitting signals were known even before the term was coined. The phenomenon of sound waves guided through a taut wire have been known for a long time, as well as sound through a hollow pipe such as a cave or medical stethoscope. Other uses of waveguides are in transmitting power between the components of a system such as radio, radar or optical devices. Waveguides are the fundamental principle of guided wave testing (GWT), one of the many methods of non-destructive evaluation.\n\nSpecific examples:\n\nA propagation mode in a waveguide is one solution of the wave equations, or, in other words, the form of the wave. Due to the constraints of the boundary conditions, there are only limited frequencies and forms for the wave function which can propagate in the waveguide. The lowest frequency in which a certain mode can propagate is the cutoff frequency of that mode. The mode with the lowest cutoff frequency is the fundamental mode of the waveguide, and its cutoff frequency is the waveguide cutoff frequency.\n\nPropagation modes are computed by solving the Helmholtz equation alongside a set of boundary conditions depending on the geometrical shape and materials bounding the region. The usual assumption for infinitely long uniform waveguides allows us to assume a propagating form for the wave, i.e. stating that every field component has a known dependency on the propagation direction (i.e. formula_1). More specifically, the common approach is to first replace all unknown time-varying unknown fields formula_2 (assuming for simplicity to describe the fields in cartesian components) with their complex phasors representation formula_3, sufficient to fully describe any infinitely long single-tone signal at frequency formula_4, (angular frequency formula_5), and rewrite the Helmholtz equation and boundary conditions accordingly. Then, every unknown field is forced to have a form like formula_6, where the formula_7 term represents the propagation constant (still unknown) along the direction along which the waveguide extends to infinity. The Helmholtz equation can be rewritten to accommodate such form and the resulting equality needs to be solved for formula_7 and formula_9, yielding in the end an eigenvalue equation for formula_7 and a corresponding eigenfunction formula_11for each solution of the former.\n\nThe propagation constant formula_7 of the guided wave is complex, in general. For a lossless case, the propagation constant might be found to take on either real or imaginary values, depending on the chosen solution of the eigenvalue equation and on the angular frequency formula_13. When formula_7 is purely real, the mode is said to be \"below cutoff\", since the amplitude of the field phasors tends to exponentially decrease with propagation; an imaginary formula_7, instead, represents modes said to be \"in propagation\" or \"above cutoff\", as the complex amplitude of the phasors does not change with formula_1.\n\nIn circuit theory, the impedance is a generalization of electrical resistivity in the case of alternating current, and is measured in ohms (formula_17).\nA waveguide in circuit theory is described by a transmission line having a length and characteristic impedance. In other words, the impedance indicates the ratio of voltage to current of the circuit component (in this case a waveguide) during propagation of the wave. This description of the waveguide was originally intended for alternating current, but is also suitable for electromagnetic and sound waves, once the wave and material properties (such as pressure, density, dielectric constant) are properly converted into electrical terms (current and impedance for example).\n\nImpedance matching is important when components of an electric circuit are connected (waveguide to antenna for example): The impedance ratio determines how much of the wave is transmitted forward and how much is reflected. In connecting a waveguide to an antenna a complete transmission is usually required, so an effort is made to match their impedances.\n\nThe reflection coefficient can be calculated using: formula_18, where formula_19 is the reflection coefficient (0 denotes full transmission, 1 full reflection, and 0.5 is a reflection of half the incoming voltage), formula_20 and formula_21 are the impedance of the first component (from which the wave enters) and the second component, respectively.\n\nAn impedance mismatch creates a reflected wave, which added to the incoming waves creates a standing wave. An impedance mismatch can be also quantified with the standing wave ratio (SWR or VSWR for voltage), which is connected to the impedance ratio and reflection coefficient by: formula_22, where formula_23 are the minimum and maximum values of the voltage absolute value, and the VSWR is the voltage standing wave ratio, which value of 1 denotes full transmission, without reflection and thus no standing wave, while very large values mean high reflection and standing wave pattern.\n\nWaveguides can be constructed to carry waves over a wide portion of the electromagnetic spectrum, but are especially useful in the microwave and optical frequency ranges. Depending on the frequency, they can be constructed from either conductive or dielectric materials. Waveguides are used for transferring both power and communication signals.\n\nWaveguides used at optical frequencies are typically dielectric waveguides, structures in which a dielectric material with high permittivity, and thus high index of refraction, is surrounded by a material with lower permittivity. The structure guides optical waves by total internal reflection. An example of an optical waveguide is optical fiber.\n\nOther types of optical waveguide are also used, including photonic-crystal fiber, which guides waves by any of several distinct mechanisms. Guides in the form of a hollow tube with a highly reflective inner surface have also been used as light pipes for illumination applications. The inner surfaces may be polished metal, or may be covered with a multilayer film that guides light by Bragg reflection (this is a special case of a photonic-crystal fiber). One can also use small prisms around the pipe which reflect light via total internal reflection —such confinement is necessarily imperfect, however, since total internal reflection can never truly guide light within a \"lower\"-index core (in the prism case, some light leaks out at the prism corners).\n\nAn \"acoustic waveguide\" is a physical structure for guiding sound waves. A duct for sound propagation also behaves like a transmission line. The duct contains some medium, such as air, that supports sound propagation.\n\nWaveguides are interesting objects of study from a strictly mathematical perspective. A waveguide (or tube) is defined as type of boundary condition on the wave equation such that the wave function must be equal to zero on the boundary and that the allowed region is finite in all dimensions but one (an infinitely long cylinder is an example.) A large number of interesting results can be proven from these general conditions. It turns out that any tube with a bulge (where the width of the tube increases) admits at least one bound state. This can be shown using the variational principles. An interesting result by Jeffrey Goldstone and Robert Jaffe is that any tube of constant width with a twist, admits a bound state.\n\nSound synthesis uses digital delay lines as computational elements to simulate wave propagation in tubes of wind instruments and the vibrating strings of string instruments.\n\n\n\n"}
{"id": "33939450", "url": "https://en.wikipedia.org/wiki?curid=33939450", "title": "WinRumors", "text": "WinRumors\n\nWinRumors was a website which was updated daily with news, rumors and reports about Microsoft and its products. The site was established in October 2010 by the site author, Tom Warren.\n\nIn 2011, WinRumors was nominated as \"Best new blog of the year\" by ComputerWeekly.\n\nJan 12, 2012 was the last post on WinRumors since his move to The Verge (website).\n\n"}
