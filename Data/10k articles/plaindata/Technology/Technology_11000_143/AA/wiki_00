{"id": "36235294", "url": "https://en.wikipedia.org/wiki?curid=36235294", "title": "3G adoption", "text": "3G adoption\n\n3G mobile telephony was relatively slow to be adopted 3Gglobally. In some instances, 3G networks do not use the same radio frequencies as 2G so mobile operators must build entirely new networks and license entirely new frequencies, especially so to achieve high data transmission rates. Other delays were due to the expenses of upgrading transmission hardware, especially for UMTS, whose deployment required the replacement of most broadcast towers. Due to these issues and difficulties with deployment, many carriers were not able to or delayed acquisition of these updated capabilities.\n\nIn December 2007, 190 3G networks were operating in 40 countries and 154 HSDPA networks were operating in 71 countries, according to the Global Mobile Suppliers Association (GSA). In Asia, Europe, Canada and the USA, telecommunication companies use W-CDMA technology with the support of around 100 terminal designs to operate 3G mobile networks.\n\nRoll-out of 3G networks was delayed in some countries by the enormous costs of additional spectrum licensing fees. (See Telecoms crash.) The license fees in some European countries were particularly high, bolstered by government auctions of a limited number of licenses and sealed bid auctions, and initial excitement over 3G's potential.\n\nThe 3G standard is perhaps well known because of a massive expansion of the mobile communications market post-2G and advances of the consumer mophone. An especially notable development during this time is the smartphone (for example, the iPhone, and the Android family), combining the abilities of a PDA with a mobile phone, leading to widespread demand for mobile internet connectivity. 3G has also introduced the term \"mobile broadband\" because its speed and capability make it a viable alternative for internet browsing, and USB Modems connecting to 3G networks are becoming increasingly common.\n\nThe first African use of 3G technology was a 3G video call made in Johannesburg on the Vodacom network in November 2004. The first commercial launch was by Emtel-ltd in Mauritius in 2004. In late March 2006, a 3G service was provided by the new company Wana in Morocco. In May 2007, Safaricom launched 3G services in Kenya while later that year Vodacom Tanzania also started providing services. In February 2012 Bharti Airtel Launched a 3.75G Network in selected cities in Kenya with a countrywide rollout planned for later in the year. In Egypt, Mobinil launched the service in 2008 and in Somaliland, Telesom started first 3G services on 3 July 2011, to both prepaid and postpaid subscription customers.\nTelecommunication networks in Nigeria like Globacom, Etisalat, Airtel and MTN provide 3G services to their numerous customers.\n\nAsia is also using 3G services very well. A lot of companies like Dialog Axiata PLC Sri Lanka (First to serve 3G Service in South Asia in 2006), BSNL, WorldCall, PTCL, Mobilink, Zong, Ufone, Telenor PK, Maxis, Vodafone, Airtel, Idea Cellular, Aircel, Tata DoCoMo and Reliance have released their 3G services.\n\nSri Lanka's All Mobile Networks(Dialog, Mobitel, Etisalat, Hutch, Airtel,) And CDMA Network Providers (Lankabell, Dialog,Suntel,SLT) Launched 3G Services.\n\nDialog, Mobitel launched 4G LTE Services In Sri Lanka.\nNot Only (Dialog CDMA, Lankabell CDMA Have 4G LTE Services.\nSri Lanka Telecom Have 4G LTE , FTTX Services..\n\nOn March 19, 2012, Etisalat Afghanistan, the fastest growing telecommunications company in the country and part of Etisalat Group, announced the launch of 3G services in Afghanistan. between 2013-2014 all telecommunications company ( Afghan Wireless, Etisalat, Roshan, MTN and Salaam Network) provided 3G, 3.5G and 3.75G services and they are planning for 4G services in 2016-2017.\n\nNepal was one of the first countries in southern Asia to launch 3G services. Nepal's first 3G company was NTC(Nepal Telecom Corporation) and the second was Ncell. Ncell also covered Mount Everest with 3G. NTC provides high speed video calling with other 3G services, as well as post-paid and pre-paid 3G SIM cards.\n\n3G and 4G was simultaneously launched in Pakistan on April 23, 2014 through a SMRA Auction. Three out of Five Companies got a 3G licence i.e. Ufone, Mobilink and Telenor while China Mobile's Zong got 3G as well as a 4G licence. Whereas fifth company, Warid Pakistan did not participate in the auction procedure, But they launched 4G LTE services on their existing 2G 1800 MHz spectrum due to Technology neutral terms and became world's first Telecom Company to transform directly from 2G to 4G. With that Pakistan joined the 3G and 4G world.\nIn the non-mobile sector, Pakistan's biggest telecommunication company PTCL launched its 3G network, EVO, in mid-2008 and has since then established itself in this sector. It provides 3G services in 105 cities across Pakistan. Omantel's WorldCall also provides 3G services in 50 cities Pakistan-wide. They provide mobile broadband service via dongles and modems. On 14 August 2010, Pakistan became the first country in the world to experience EVDO's RevB 3G technology that offers maximum speeds of 9.3 Mbit/s. At present the services of EVO Nitro (brand name) are available in Islamabad, Rawalpindi, Lahore and Karachi. The RevA network, with speeds if up to 3.1 Mbit/s is available in over 100 cities of the country.\n\nState-run mobile operator Teletalk Bangladesh limited and other GSM operators GrameenPhone, Banglalink, Robi and Airtel already started hi-speed 3G+ and 3.5G services using UMTS with HSDPA facilities. Grameenphone has a plan to launch 4G LTE services first time in Bangladesh using TD-LTE technology. Currently Grameenphone owned 10 MHz spectrum at 3G auction by BTRC.Robi and Airtel recently merged, newly merged company has a plan to introduce 4G operation soon. Two other data operators, Qubee and Banglalion, currently offer 4G Wimax services in Bangladesh. CityCell now switched off their operation by government order. 4G LTE services have already begun in Bangladesh through all mobile operators except Teletalk, the state run mobile operator. Bangladesh has a plan to introduce super speed 5G service soon. A test run will be conducted in the country in mid July 2018.\n\nChina announced in May 2008, that the telecoms sector was re-organized and three 3G networks would be allocated so that the largest mobile operator, China Mobile, would retain its GSM customer base. China Unicom would retain its GSM customer base but relinquish its CDMA2000 customer base, and launch 3G on the globally leading W-CDMA (UMTS) standard. The CDMA2000 customers of China Unicom would go to China Telecom, which would then launch 3G on the CDMA2000 1x EV-DO standard. This meant that China would have all three main cellular technology 3G standards in commercial use. Finally in January 2009, Ministry of industry and Information Technology of China awarded licenses of all three standards: TD-SCDMA to China Mobile, W-CDMA to China Unicom and CDMA2000 to China Telecom. The launch of 3G occurred on 1 October 2009, to coincide with the 60th Anniversary of the Founding of the People's Republic of China. By August 2011, China Telecom's 3G subscriber has exceeded 23 million\n\n11 December 2008, India entered the 3G arena with the launch of 3G enabled Mobile and Data services by Government owned Mahanagar Telephone Nigam Ltd MTNL in Delhi and later in Mumbai. MTNL becomes the first 3G Mobile service provider in India. After MTNL, another state operator Bharat Sanchar Nigam Ltd. (BSNL) launched 3G services on 22 Feb 2009 in Chennai and Kolkata and later launched 3G as Nationwide. The auction of 3G wireless spectrum was announced in April 2010 and 3G Spectrum allocated to all private operators on 1 September 2010.\n\nNorth Korea has had a 3G network since 2008, which is called Koryolink, a joint venture between Egyptian company Orascom Telecom Holding and the state-owned Korea Post and Telecommunications Corporation (KPTC) is North Korea's only 3G Mobile operator, and one of only two mobile companies in the country. According to Orascom quoted in BusinessWeek, the company had 125,661 subscribers in May 2010. The Egyptian company owns 75 percent of Koryolink, and is known to invest in infrastructure for mobile technology in developing nations. It covers Pyongyang, and five additional cities and eight highways and railways. Its only competitor, SunNet, uses GSM technology and suffers from poor call quality and disconnections. Phone numbers on the network are prefixed with +850 (0)192.\n\n3G services were made available in the Philippines on December 2008.\n\n3G services were made available in Singapore on October 2007. Widespread adoption of 3G began in January 2009, with the upgrading of phones to iPhone 3G and Android.\n\nIn Europe, mass market commercial 3G services were introduced starting in March 2003 by O2 in the UK and Italy. The European Union Council suggested that the 3G operators should cover 80% of the European national populations by the end of 2005.\n\nIn Canada, Bell Mobility, SaskTel and Telus launched a 3G EVDO network in 2005. Rogers Wireless was the first to implement UMTS technology, with HSDPA services in eastern Canada in late 2006. Realizing they would miss out on roaming revenue from the 2010 Winter Olympics, Bell and Telus formed a joint venture and rolled out a shared HSDPA network using Nokia Siemens technology. After the AWS spectrum in 2008, new entrants to the Canadian wireless markets including but not limited to Mobilicity, Wind Mobile and Vidéotron have deployed their own UMTS networks in Canada using the AWS spectrum.\n\nIn Iran Rightel won the bid for the third Operator license. Rightel is the first 3G operator in Iran. Rightel has commercially launched in the last months of 2011.\n\nIn Jordan, Orange is the first mobile 3G operator.\n\nMobitel Iraq is the first mobile 3G operator in Iraq. It was launched commercially on February 2007.\n\nMTN Syria is the first mobile 3G operator in Syria. It was launched commercially on May 2010.\n\nIn Lebanon Ministry of Telecoms launched a test period on September 20, 2011, where 4,000 smart-phone users were selected to enjoy 3G for one month and provide feedback. Currently, the test period is over, MTC Touch and Alfa began rolling out the new 3G services.\n\nSaudi Arabia has got 4G as well as 3G/HSPA With Zain KSA, Saudi Telecom, and Mobily KSA.\n\nTurkcell, Avea and Vodafone launched their 3G networks commercially on 30 July 2009 at the same time. Turkcell and Vodafone launched their 3G service on all provincial centres. Avea launched it on 16 provincial centres. It was after Turkey's monopoly mobile operator Turkcell accepted number portability, mobile operators attended frequency band auction and frequencies for 3G usage distributed around mobile operators. Turkcell got A band, Vodafone B and Avea C. Currently Turkcell and Vodafone have 3G networks on most of crowded cities and towns. Turkey has 3.9G networks now.\n\nIn late 2005, Vodafone NZ launched their 3G network, followed by Spark NZ's XT network in 2008, and newcomer 2degrees using a combination of Vodafone's 3G towers and their own in 2009. 2degrees has since built more towers, and is now self-sufficient in the major cities (Auckland, Hamilton, Wellington, Christchurch and Dunedin) but relies on a roaming agreement with Vodafone to cover the rest of the country. This gives it essentially the same footprint as Vodafone.\n"}
{"id": "2795314", "url": "https://en.wikipedia.org/wiki?curid=2795314", "title": "Airborne wind turbine", "text": "Airborne wind turbine\n\nAn airborne wind turbine is a design concept for a wind turbine with a rotor supported in the air without a tower, thus benefiting from more mechanical and aerodynamic options, the higher velocity and persistence of wind at high altitudes, while avoiding the expense of tower construction, or the need for slip rings or yaw mechanism. An electrical generator may be on the ground or airborne. Challenges include safely suspending and maintaining turbines hundreds of meters off the ground in high winds and storms, transferring the harvested and/or generated power back to earth, and interference with aviation.\n\nAirborne wind turbines may operate in low or high altitudes; they are part of a wider class of Airborne Wind Energy Systems (AWES) addressed by high-altitude wind power and crosswind kite power. When the generator is on the ground, then the tethered aircraft need not carry the generator mass or have a conductive tether. When the generator is aloft, then a conductive tether would be used to transmit energy to the ground or used aloft or beamed to receivers using microwave or laser. Kites and helicopters come down when there is insufficient wind; kytoons and blimps may resolve the matter with other disadvantages. Also, bad weather such as lightning or thunderstorms, could temporarily suspend use of the machines, probably requiring them to be brought back down to the ground and covered. Some schemes require a long power cable and, if the turbine is high enough, a prohibited airspace zone. As of July 2015, no commercial airborne wind turbines are in regular operation.\n\nAn aerodynamic airborne wind power system relies on the wind for support.\nMiles L. Loyd proposed and analyzed an efficient AWES in his work \"Crosswind Kite Power\" in 1980. Power output of AWES with crosswind wing motion is proportional to a square of a lift/drag ratio of the wing. Such AWES is based on the same aerodynamic principles as a conventional wind turbine (AWES), but it is more efficient because the air speed is constant along the wing span and the aerodynamic forces are resisted by tension of a tether, rather than by bending of a tower.\n\nBryan Roberts, a professor of engineering at the University of Technology, in Sydney, Australia, has proposed a helicopter-like craft which flies to altitude and stays there, held aloft by wings that generate lift from the wind, and held in place by a cable to a ground anchor. According to its designers, while some of the energy in the wind would be 'lost' on lift, the constant and potent winds would allow it to generate constant electricity. Since the winds usually blow horizontally, the turbines would be at an angle from the horizontal, catching winds while still generating lift. Deployment could be done by feeding electricity to the turbines, which would turn them into electric motors, lifting the structure into the sky.\n\nThe Dutch ex-astronaut and physicist Wubbo Ockels, working with the Delft University of Technology in the Netherlands, has designed and demonstrated an airborne wind turbine he called a \"Laddermill\". It consists of an endless loop of kites. The kites lift up one end of the endless loop (the \"ladder\"), and the released energy is used to drive an electric generator.\n\nA September 2009 paper from Carbon Tracking Ltd., Ireland has shown the capacity factor of a kite using ground-based generation to be 52.2%, which is better than terrestrial wind-farm capacity factors of 30%.\n\nA team from Worcester Polytechnic Institute in the United States has developed a smaller-scale with an estimated output of about 1 kW. It uses a kiteboarding kite to induce a rocking motion in a pivoting beam.\n\nThe KiteGen uses a prototype vertical-axis wind turbine (actually a semi-rigid cross wind kite). It is an innovative plan (still in the construction phase) that consists of one wind farm with a vertical spin axis, and employs kites to exploit high-altitude winds. The Kite Wind Generator (KWG) or KiteGen is claimed to eliminate all the static and dynamic problems that prevent the increase of the power (in terms of dimensions) obtainable from the traditional horizontal-axis wind turbine generators. Generating equipment would remain on the ground, and only the airfoils are supported by the wind. Such a wind power plant would be capable of producing the energy equivalent to a nuclear plant, while using an area of few square kilometres, without occupying it exclusively. (The majority of this area can still be used for agriculture, or navigation in the case of an offshore installation.)\n\nThe Rotokite is developed from Gianni Vergnano's idea. It uses aerodynamic profiles similar to kites that have been rotated on their own axis, emulating the performance of a propeller. The use of the rotation principle simplifies the problem of checking the flight of the kites and eliminates the difficulties due to the lengths of cables, enabling the production of wind energy at low cost. The Heli Wind Power is a project of Gianni Vergnano that uses a tethered kite.\n\nThe HAWE System is developed from Tiago Pardal's idea. The System that consists in a Pumping Cycle similar to kite systems. In Generation Phase the pulling force increase 5-10 times due to Magnus Effect of a spinning cylinder(aerial platform), like a kite the pulling force produced by the aerial platform will unwind the cable and generate electricity in the ground.In the Recovery Phase it rewinds the cable with no Magnus Effect in the aerial platform.\n\nIn August 2011 the German company SkySails, producer of kites for ship propulsion, announced a kite-based wind power system for on- and offshore applications that is supposed to be \"30% cheaper than current offshore solutions\".\n\nIn June 2012, the German company NTS GmbH had successfully tested X-Wind technology (spoken: Cross-Wind) on linear rail system in Freidland, Germany. \"NTS Energie- und Transportsysteme GmbH\" was found in 2006 by Uwe Ahrens. X-Wind technology combines two well-known technologies - automatically steered kites and generators on a closed loop rail system. Closed loop prototype is under construction at Mecklenburg-Vorpommern, Germany. This technology allows to harness increasingly stable and constant wind currents at altitudes between 200 and 500m. Technical report readings and measurements show that NTS X-Wind Systems double to triple the efficiency of conventional wind energy systems according to energy production.\n\nIn May 2013, the Californian company Makani Power, developer of some crosswind hybrid kite systems with onboard generator doubling as motor, has been acquired by Google.\n\nIn May 2013, an airborne wind energy system with a ground-based generator using fast motion transfer was suggested by L. Goldstein.\n\nIn 2015, a sails on rope wind and ocean current energy system was invented by a Taiwanese Johnson Hsu.\n\nOn 15 December 2015 Windswept and Interesting Ltd demonstrated a \"Daisy\" kite ring stack airborne wind turbine. The Daisy kite stack demonstrated on 15 December 2015 is the only airborne wind energy system to have won the someawe.org 100*3 AWE challenge. The Daisy system uses tensioned torsion transfer of kite motion to turn a ground based generator.\n\nKitemill flies an airplane tethered to a winch powering a 5 kW generator on ground.\n\nSince 2014, Kitewinder, a French company located near Bordeaux is working on the first commercial airborne wind turbine named Kiwee One. Kiwee one is a back pack airborne wind turbine for nomadic uses with a nominal power of 100 Watts. Kiwee one launch procedure is manual but the product is equipped with an automatic retrieval mechanism for low wind / high wind conditions. \n\nAn aerostat-type wind power system relies at least in part on buoyancy to support the wind-collecting elements. Aerostats vary in their designs and resulting lift-to-drag ratio; the kiting effect of higher lift-over-drag shapes for the aerostat can effectively keep an airborne turbine aloft; a variety of such kiting balloons were made famous in the kytoon by Domina Jalbert.\n\nBalloons can be incorporated to keep systems up without wind, but balloons leak slowly and have to be resupplied with lifting gas, possibly patched as well. Very large, sun-heated balloons may solve the helium or hydrogen leakage problems.\n\nAn Ontario based company called Magenn is developing a turbine called the Magenn Air Rotor System (MARS). A future -wide MARS system would use a horizontal rotor in a helium suspended apparatus which is tethered to a transformer on the ground. Magenn claims that their technology provides high torque, low starting speeds, and superior overall efficiency thanks to its ability to deploy higher in comparison to non-aerial solutions. The first prototypes were built by TCOM in April 2008. No production units have been delivered.\nBoston-based Altaeros Energies uses a helium-filled balloon shroud to lift a wind turbine into the air, transferring the resultant power down to a base station through the same cables used to control the shroud. A 35-foot prototype using a standard Skystream 2.5kW 3.7m wind turbine was flown and tested in 2012. In fall 2013, Altaeros was at work on its first commercial-scale demonstration in Alaska.\n\n\n"}
{"id": "42451997", "url": "https://en.wikipedia.org/wiki?curid=42451997", "title": "American historic carpentry", "text": "American historic carpentry\n\nAmerican historic carpentry is the historic methods with which wooden buildings were built in what is now the United States since European settlement. A number of methods were used to form the wooden walls and the types of \"structural carpentry\" are often defined by the wall, floor, and roof construction such as log, timber framed, balloon framed, or stacked plank. Some types of historic houses are called plank houses but \"plank house\" has several meanings which are discussed below. Roofs were almost always framed with wood, sometimes with timber roof trusses. Stone and brick buildings also have some wood framing for floors, interior walls and roofs.\n\nHistorically building methods were passed down from a master carpenter to an apprentice verbally, through demonstration, and through work experience. Designs, engineering details, floor plans, methods were time tested and communicated through rules of thumb rather than scientific study and documents. Each region of the world has variations on traditions, tools and materials. The carpenters who found themselves in the New World based their work on their traditions but adapted to new materials, climate, and mix of cultures. Immigrants to America were from all parts of the world so the history of American carpentry is very diverse and complex, but it is only four or five centuries old, a fraction of the history of many other regions.\n\nNotable examples of structural carpentry which were not used in America include cruck framing.\n\nCarpentry is one of the traditional trades but is not always clearly distinguished from the work of the joiner and cabinetmaker, in general a carpenter historically did the heavier, rougher work of framing a building including installing the sheathing and sub-flooring and installing pre-made doors and windows. Joiners did the finer work of installing trim and paneling. \nPlank and board are not consistently defined in history. Sometimes these terms are used synonymously. \"Board\" means a piece of lumber (timber) to thick and more than wide. \"Plank\" generally means a piece of lumber (timber) rectangular in shape and thicker than a board.\n\nTimber framing, historically called a \"braced frame\", was the most common method of building wooden buildings in America from the 17th-century European settlements until the early 20th century when timber framing was replaced by balloon framing and then platform framing in houses and what was called plank or \"joist\" framing in barns. The framing in barns is usually visible, but in houses is usually covered with the siding material on the outside and plaster or drywall on the inside. Variations of timber framing are described based on their nature at the foundation, sill plate, wall, wall plate, and roof.\n\nPosts which were dug into the ground are called earthfast or post in ground construction. This technique eliminated the need for bracing. Some buildings were framed with the posts landing on a foundation with \"interrupted sills\". Most buildings were framed with the posts landing on a heavy timber sill, the sills (rarely) laid on the ground, supported by stones or, late in the 19th century, concrete.\n\nThe structural carpentry of the walls are of several types and are discussed in detail below. French settlers called placing studs or posts on a sill spaced slightly apart poteaux-sur-sol which is similar to the English close studding. These are examples of \"half timbering\" where the framing is \"infilled\" with another material such as a mud mixture, stones, or bricks. Half timbering in America is found in limited areas, mostly of German settlement, including Old Salem, North Carolina, parts of Missouri, Louisiana, and Pennsylvania. Much more common was to build a framed building and add brick nogging between the framing which may not be considered half timbering. Half timbering is an architectural element in Tudor and Tudor Revival architecture.\n\nOne of the earliest descriptions of how to build timber framed buildings in America was in a publication titled \"Information and Direction to Such Persons as are Inclined to America, more Especially Those Related to the Province of Pennsylvania\" attributed to William Penn in 1684. Described is an earthfast, hewn frame \"filled in\" (half-timbered) with riven clapboards for the siding, roofing and loft flooring. The author called this a \"first house\" distinguishing that it is suitable until such time a better house can be built and then this building can be used as an outbuilding:\n\nEarthfast construction is still used for buildings and structures such as in pole building framing and stilt houses.\n\nLog building is the second most common type of carpentry in American history. In some regions and periods it was more common than timber framing. There are many different styles of log carpentry: (1) where the logs are made into squared beams and fitted tightly. This style is typical of defensive structures called a blockhouse. The walls needed to be thick and strong and not have gaps in-between; (2) Round logs are left spaced apart, often with the gaps filled with a material called chinking; (3) Planked log buildings have the wall timbers shaped into rectangular thus called planks and \"plank houses\".\n\nThe C. A. Nothnagle Log House, located in Gibbstown, New Jersey, was constructed in 1638 and is believed to be the oldest surviving log house in what is today the United States. The house was built by colonial settlers in what was then the Swedish colony of New Sweden.\n\nBalloon framing originated in the American Mid-west near Chicago in the 1830s. It is a rare type of American historic carpentry which was exported from America. Balloon framing is very important in history as the beginning of the transition away from the centuries-long method of timber framing to the common types of wood framing now in use.\n\nCorner post construction is known by many names listed below, but particularly as pièce sur pièce and it blurs the line between timber framing and log building. This type of carpentry has a frame with horizontal beams or logs tenoned into slots or mortises in the posts. Pièce sur pièce en coulisse: Literally piece on piece in a groove is a widespread type of carpentry which blurs the lines between log, plankwall and framing techniques, thus is classified as any of the above.\n\"The support of horizontal timbers by corner posts is an old form of construction in Europe. It was apparently carried across much of the continent from Silesia by the Lausitz urnfield culture in the late Bronze Age.\" Examples also persist in southern Sweden, in the Alps, Hungry, Poland, Denmark, and Canada. Usually the origin of corner post construction is credited to the immigrants of the far-Eastern French in Canada and Alpine-Alemannic Germans or Swiss in the U. S. This technique is best known in German as standerbohlenbau or bohlenstanderbau.\nThere are many names for corner post construction in many languages:\nFrench: Pièce sur pièce poteaux et pièce coulissante (piece on piece sliding in a groove) Pièce sur pièce en coulisse, poteaux et piece coulissante, pieces sur pieces, \n\nAn example of corner post construction is the Cray House in Stevensville, Maryland.\n\nPlank-frame house construction has a \"timber frame\" with the walls made of \"vertical planks\" attached to the frame. These houses may simply be called \"plank houses\". Some building historians prefer the term plank-on-frame. Plank-frame houses are known from the 17th century with concentrations in the Massachusetts Bay Colony and Colony of Rhode Island and Providence Plantations. The carpentry consists of a timber frame with vertical planks extending from sill to plate. Sometimes there are studs at the doors but mostly the vertical planks replace the studs. Both wood shingle or clapboard exterior siding and interior lath and plaster attach directly to the planks. Some examples of plank frame houses are the oldest house in New Hampshire, the Richard Jackson House, Thomas and Esther Smith House in Massachusetts, and the \n\nA palisade is a series of vertical pales (stakes) driven or set into the ground to form a fence or barrier. Palisade construction is a palisade or the similar use of timbers set on a sill; an example in England being the original portion of the ancient Greensted Church and the early type of stave church known as a palisade church. It was common for Native Americans and Europeans to build a palisade as part of a fort or to protect a village. Palisade construction is alluded to as a method of building of early dwellings. The nature of planting one end of a timber in the ground is called earthfast or post in ground construction which was a common way to build worldwide. A benefit of earthfast construction is the ground holds the posts from swaying which eliminates the need for bracing and anchors the structure to the ground. The French settlers called this carpentry \"en pieux\" or \"poteaux en terre\" and \"log on end\". This type of carpentry may not considered \"framing\". The French method of poteaux en terre was different than palisade construction in that the timbers were hewn two sides and spaced slightly apart with the gaps filled with a material called bousillage\n\nPalisade construction is similar in concept to vertical plank wall construction.\n\nVertical plank wall buildings are sometimes also called \"plank houses\". In Australia houses with vertical plank walls are called slab huts and the technique is similar to the American counterpart except in America these buildings may be two stories.\n\nSome plank-wall houses or creole cottages in the New Orleans area are called \"bargeboard\" or \"flatboat board\" houses because the vertical planks used to build the walls were reused planks from barges (flatboats) floated down the Mississippi River loaded with cargo and then broken up and the lumber sold. (Note the possibility of confusion with the different carpentry element called a bargeboard).\n\nAnother carpentry method which is sometimes called \"plank wall\", \"board wall\", \"plank-on-plank\", \"horizontal plank frame\" is the stacking of horizontal planks or boards to form a wall of solid lumber. Sometimes the planks were staggered or spaced apart to form keys for a coat of plaster. This method was recommended by Orson Squire Fowler for octagon houses in his book \"The Octagon House: A Home for All\" in 1848. Fowler mentions he had seen this wall type being built in central New York state while traveling in 1842.\n\n[[File:Box and Strip House, NHRC, Lubbock, TX IMG 1615.JPG|thumb|left|In Texas and some other areas box houses are known as \"box and strip\" houses, the strips are the battens.]]\n[[File:Row of shacks, 1906 earthquake in San Francisco.jpg|thumb|left|Over 5,000 relief cottages after the 1906 San Francisco earthquake were built using single-wall construction]]\nBox houses (boxed house, box frame, box and strip, piano box, single-wall, board and batten, and many other names) have minimal framing in the corners and widely spaced in the exterior walls, but like the vertical plank wall houses, the vertical boards are structural. The origins of boxed construction is unknown. The term \"box-frame\" was used in a reconstruction manual in 1868 after the American Civil War.\n\n\"Box house\" may also be a nickname for \"Classic Box\" or \"[[American Foursquare]]\" architectural styles in North America, and is also not to be confused with a general type of timber framing called a box frame.\n\nA variation on boxed construction is used on the [[Wesleyan Grove]] cottage: cottages around Oak Bluffs (Cottage City), Martha’s Vineyard, Massachusetts are built of vertical, tongue and groove planks without battens usually in a gothic style. This method was “inspired by the tent frame construction” of the original \"board tents\" used for Methodist [[Camp Meeting]]s beginning in 1835.\n\nAn A-frame building has framing with little or no walls, the rafters join at the ridge forming an A shape. This is the simplest type of framing but has historically been used for inexpensive cottages and farm shelters until the [[A-frame house]] was popularized in the 1950s as a style of vacation home in the United States.\n\nInside-out framing has the sheathing boards or planks on the inside of the framing. This type of structure was used for structures intended to contain bulk materials like ore, grain or coal.\n\n[[File:Ladviks gård 2013 11.jpg|thumb|A type of trussed plank frame barn in Sweden is representative of some types in America, the lack of heavy timbers in the framing give it the name \"plank frame barn\".]]\n\"Plank-framed barns\" are different than a plank-framed house. Plank framed barns developed in the American Mid-West, such as the patente in 1876 (#185,690) by William Morris and Joseph Slanser of La Rue, Ohio shows (several other patents followed). Sometimes they were also called a \"joist frame\", \"rib frame\" and \"trussed frame\" barns. Built of a “Construction in which none of the material used is larger than 2 inches thick.” rather than solid timbers. The reduction in availability of timber for barn building and experience with scantling framing resulted in the development of this lightweight barn framing using planks (“joists”) rather than timbers. Some stated advantages: cheaper, faster, no interior posts needed, use any length lumber, less skill, less lumber (either purchased or self-produced), “stronger”, lighter, all lumber can be purchased from a lumber yard, less labor, heavy timber getting scarce. Also, they were often similar to the Jennings barn design of 1879 (patent #218,031) with no tie beam so there were no beams to interfere with a hay fork (horse fork) on a track system (hay carrier) for pitching hay which became popular c. 1877. The gambrel roof shape lends itself to plank truss construction and became the most popular roof type. Plank frame barns were available by mail-order by 1910 from Chicago. Syn joist-frame, Shawver plank-frame and Wing plank-frame. “In large construction, such as barn framing, there are two general systems, the braced, pin-joint frame, made of heavy timbers, and the plank frame, made up of two inch planking, either in the form of the ‘plank truss’ or the ‘balloon frame.’” (Architectural Drawing and Design of Farm Structures, 1915)\n\n[[File:Nash4.JPG|thumb|A variation of a plank framed truss with metal plate connectors on a [[pole barn]]]]\nPlank framed truss was the name for roof trusses made with planks rather than [[timber roof trusses]]. In the 20th century it was typical for carpenters to make their own trusses by nailing planks together with wood plates at the joints. Today similar trusses are manufactured to engineering standards and use [[truss connector plate]]s.\n\nIn timber framing a \"single floor\" is a floor framed with single set of joists. A \"double floor\" is generally used for longer spans and joists, called bridging beams or joists, are supported by other beams called binding beams: the two layers of timbers providing the name double floor. In a double floor there may be two sets of joists, one for the floor above and one for the ceiling below.\n\nPlank and beam construction or framing is a type of framing with no joists but widely spaced beams spanned by heavy planks. This method developed in the early 19th century for industrial mill floors but may also be found in timber framed roofs. Also known as Slow burning construction, mill construction, and heavy timber construction originated in industrial [[Sawmill|mills]] in the 19th and early 20th centuries. The joists are eliminated by the use of heavy planks saving time and strength of the timbers because the joists notches were eliminated. The beams are spaced to apart and the planks are or more thick possibly with another layer of on the top as the finished flooring could span these distances. The planks may be laid flat and [[tongue and groove]]d or splined together or laid on edge called a \"laminated floor\". The name \"slow burning construction\" was coined in 1870 by [[Factory Mutual]] insurance company because large, smooth timbers with [[chamfer]]ed edges ignite slower and last longer in a fire allowing fire suppression crews more time to extinguish a fire. These beams are designed to be self-releasing in case of fire, that is if they burn through and collapse the connection with the masonry wall and joint at the post should allow the beam to fall away without pulling the wall or post down. A common way to join a beam and a masonry wall in this regard is a [[fire cut]], an angled cut on the end of the beam.\n\n[[File:Tunnel vision (Hartland Bridge).jpg|thumb|Howe trusses of the [[Hartland Bridge]] which crosses the [[Saint John River (Bay of Fundy)|Saint John River]] between Maine and New Brunswick, Canada)]]\n\nOther wooden structures do not necessarily have names for types of carpentry, but deserve mention. Carpenters were needed to build a variety of durable or temporary wooden structures such as a [[falsework]] and many other [[non-building structure]]s.\n\nSquare, saw, hammer, and a rule are the essentials for any carpenter old or new.\n\n\n[[Category:Building]]\n[[Category:Carpentry]]\n[[Category:Structural system]]\n[[Category:Vernacular architecture]]"}
{"id": "1417253", "url": "https://en.wikipedia.org/wiki?curid=1417253", "title": "CeCILL", "text": "CeCILL\n\nCeCILL (from CEA CNRS INRIA Logiciel Libre) is a free software license adapted to both international and French legal matters, in the spirit of and retaining compatibility with the GNU General Public License (GPL).\n\nIt was jointly developed by a number of French agencies: the \"Commissariat à l'Énergie Atomique\" (Atomic Energy Commission), the \"Centre national de la recherche scientifique\" (National Centre for Scientific Research) and the \"Institut national de recherche en informatique et en automatique\" (National Institute for Research in Computer Science and Control). It was announced on 5 July 2004 in a joint press communication of the CEA, CNRS and INRIA.\n\nIt has gained support of the main French Linux User Group and the Minister of Public Function, and was considered for adoption at the European level before the European Union Public Licence was created.\n\nThe CeCILL grants users the right to copy, modify, and distribute the licensed software freely. It defines the rights as passing from the copyright holder to a \"Licensor\", which may be the copyright holder or a further distributor, to the user or \"Licensee\". Like the GPL, it requires that modifications to the software be distributed under the CeCILL, but it makes no claim to work that executes in \"separate address spaces\", which may be licensed under terms of the licensee's choice. It does not grant a patent license (as some other common open-source licenses do), but rather includes a promise by the licensor not to enforce any patents it owns. In Article 9.4, the licensor agrees to provide \"technical and legal assistance\" if litigation regarding the software is brought against the licensee, though the extent of the assistance \"shall be decided on a case-by-case basis...pursuant to a memorandum of understanding\".\n\nThe disclaimers of warranty and liability are written in a manner different from other common open-source licenses in order to comply with French law. The CeCILL does not preclude the licensor from offering a warranty or technical support for its software, but requires that such services be negotiated in a separate agreement.\n\nThe license is compatible with the GPL through an explicit relicensing clause.\n\nArticle 13's explicit reference to French law and a French court does not limit users, who can still choose a jurisdiction of their choice by mutual agreement to solve any litigation they may experience. The explicit reference to a French court will be used only if mutual agreement is not possible; this immediately solves the problem of competence of laws (something that the GPL does not solve cleanly, except when all parties in a litigation are in the USA).\n\nVersion 2 was developed after consultations with the French-speaking Linux and Free Software Users' Association, the \"Association pour la Promotion et la Recherche en Informatique Libre\", and the Free Software Foundation; it was released on 21 May 2005. According to the CeCILL FAQ there are no major differences in spirit, though there are in terms.\n\nThe most notable difference in CeCILL v2 is the fact that the English text was approved not as a draft translation (as in CeCILL v1) but as an authentic text, in addition to the equally authentic French version. This makes the CeCILL license much easier to enforce internationally, as the cost of producing an authentic translation in any international court will be lower with the help of a second authentic reference text. The second difference is that the reference to the GNU General Public License, with which CeCILL v2 is now fully compatible, is explicitly defined precisely using its exact title and the exact name of the Free Software Foundation, to avoid all possible variations of the terms of the GPL v2. Some additional definitions were added to more precisely define the terms with less ambiguity. With these changes, the CeCILL is now fully enforceable according to WIPO rules, and according to French law in courts, without the legal problems remaining in GPL version 2 outside the United States.\n\nVersion 2.1 was released in June 2013. It allows relicensing to the GNU Affero General Public License and the European Union Public License as well as the GPL, and clarifies the language that requires licensees to give access to the source code (which had previously caused rejection of version 2.0 by the Open Source Initiative).\n\nNote that CeCILL v1 already allowed replacing a CeCILL v1 license by CeCILL v2, so all software previously licensed with CeCILL v1 in 2004 can be licensed with CeCILL v2, with legal terms enforceable as authentic not only in French but in English too.\n\nThe fact that it is protected by reputed public research centers (in France the INRIA, a founding member of the international W3 consortium, and the CEA working on atomic energy) which use them to publish their own open-source and free software, and by critical governmental organizations (which are also working in domains like military and defense systems) also gives much more security than using the GPL alone, as the license is supported officially by a government which is a full member of WIPO, and by an enforceable law. This also means that all international treaties related to the protection of intellectual rights do apply to CeCILL-licensed products, and so they are enforceable by law in all countries that signed any of the international treaties protected by WIPO. However, this also leaves open the possibility that the French government will make a future version of the CeCILL unfree and restricted.\n\nThe CeCILL license is approved as a \"Free Software\" license by the FSF with which the CeCILL project founders have worked. Since version 2.1, CeCILL is also approved by the Open Source Initiative as an \"Open Source\" license.\n\nThe CeCILL project also adds two other licenses:\n\nThese two licenses are also defined to make BSD-like and FSF's LGPL licenses enforceable internationally under WIPO rules.\n\n\nAlthough the three CeCILL licenses were developed and used for strategic French research systems (in the domain of defense, space launching systems, medical research, meteorology/climatology, and various domains of fundamental or applied physics), they are made to be usable also by the general public or any other commercial or non-profit organization, including from other governments, simply because these software component need and use (or are integrated with) component software or systems which were initially released with an open-source or free license, and they are operated by organizations that also have a commercial status.\n\nWithout these licenses, such systems could not have been built and used, and protected legally against various international patent claims. Due to the huge cost of these French strategic systems, a very strong licensing scheme was absolutely necessary to help protecting these investments against illegitimate claims by other commercial third parties, and one of the first needs was to make the well-known open-source and free licenses fully compatible and protected under the French law and the many international treaties ratified by France.\n\n"}
{"id": "5840643", "url": "https://en.wikipedia.org/wiki?curid=5840643", "title": "Commercial intelligence", "text": "Commercial intelligence\n\nCommercial Intelligence is the highest and most comprehensive form of legal, ethical open-source intelligence as practiced by diverse international and localized businesses.\nBusiness intelligence is a mis-nomer for data mining and enterprise dashboards that present useful patterns or distillations of internal information to the executive.\n\nCommercial Intelligence (CI) is the process of defining, gathering, analyzing and distributing accurate and relevant intelligence regarding the products, customers, competitors, business environment and the organization itself. This methodical program affects the organization’s tactics, decisions and operations.\n\nA 1998 study by the Futures Group, a Glastonbury, Connecticut-based consulting firm indicates that 82% of companies with annual revenues exceeding $10 billion and 60% of those with annual revenues exceeding $1 billion now have an organized intelligence system. While most larger companies have specific departments devoted to Commercial Intelligence, mid-sized companies tend to hire Commercial Intelligence firms, and smaller business owners are more likely to do it themselves.\n\nCommercial Intelligence can be obtained and analysed in various ways that include:\n\nPrimary sources of CI include:\n\nSecondary sources of CI include:\n\nIntelligence from both classifications can be found within the scope of open source intelligence (OSINT).\n\nEfficient and successful CI is a constant cycle that consists of 5 steps:\n\nMisrepresentation in CI is a form of cyber engineering. It is the act of falsely identifying yourself and bluffing people into giving you information they probably would not have had your true identity been provided. In fact, misrepresentation is the most common issue that subdivides numerous CI practitioners in many unclear ethical issues. The most common types of misrepresentation include:\nAll 3 misrepresentation issues share a common theme, which is: “What is the intent of the CI practitioner?”. Judging by a person’s intent, we can conclude whether or not the situation is ethical or non-ethical.\n\nClient conflict means any situation in which a practitioner is faced with a conflict of interest between a former and current client. The practitioner must attempt to determine how to act in the best interests of several clients in the same or substantially related matter. This type of situation usually arises with CI practitioners who are consultants. However, a clear solution is obvious; the practitioners agree that they should never service competing clients at the same time in order to keep classified information secure. In addition, another conflict may arise in the CI consultant and client relationship. The client usually hires the consultant to gather as much information about their competitors. However, there is an ethical issue concerning the length in which a consultant would go in order to retrieve the required information; how much is too much? When the consultant ignores the cost at which they are seeking this information, they tend to stumble upon various ethical issues, and it is in fact these ethical issues which the clients don’t want to be associated with that push them to hiring a CI consultant.\n\nIn 2000, Oracle Corporation hired a detective agency to investigate two research groups that supported Microsoft during the antitrust trial. After attempting to buy garbage, the agency discovered that those two research groups were falsely identifying themselves as independent advocacy groups when they were in reality funded by Microsoft. Oracle and the hired detective agency may have acted in questionable matters to obtain that information, but they believe that the outcome justifies their actions. Larry Ellison, Oracle’s chairman, says: “I feel very good about what we did. […] Maybe our investigation organization may have done things unsavoury, but it's not illegal. We got the truth out.” \n\nIn May 2004, Air Canada filed a $5 million corporate-espionage lawsuit against WestJet after discovering that their rival had hacked into Air Canada’s internal employee-only website over 243,000 times in a period of 10 months. WestJet used the confidential information it found to rearrange its load schedules as well as adjust routes and it significantly reduced service from the Hamilton airport and increased service in Toronto. WestJet later issued a rare apology to its rival and Robert Milton, the chief executive of parent ACE Aviation Holdings Inc, admitting that its actions of online snooping \"were both unethical and unacceptable\" and ended paying AirCanada $15.5-million in legal fees as well as donating $10-million to children's charities in the names of both airlines. Moreover, Hill, one of the founders of WestJet later resigned from his position as vice-president of strategic planning in July 2004.\n\nIn 2005, HP filed a lawsuit against its former VP, Karl Kamp, for $100 million, claiming Kamp had betrayed the company and appropriated its trade secrets as well as its money to start up his own flat-panel-TV company, as he was working on the TV project for HP. In January 2007. Kamb filed a countersuit. Not only did he deny stealing trade secrets but also claimed that HP was aware of what he had done with its money. He claimed that HP had asked him to gather classified information on Dell, whose entry into printers had threatened HP's most profitable line of business. In 2002, the CI unit hired Katsumi Iizuka, former president of Dell Japan until 1995, sold HP information on Dell’s plans to enter the printer business. The information they gathered revolved around printer models, specifications, terms and prices, many months before their launch. Further more, HP was accused of pretexting, which is the act of lying about one's identity to obtain privileged data and information, in order to obtain Kamb’s private phone records.\n\nThe Society of Competitive Intelligence Professionals is one of the only global membership organizations in the rapidly growing field of competitive intelligence and business strategy. SCIP is a global not-for-profit association whose 7,000 members conduct competitor research and analysis for large and small companies, and help manage planning competitive strategy. Established in 1986 in Washington DC, it is currently headquartered in Alexandria, Virginia. It focuses on enhancing the success of its members through education, leadership, support and networking. Since 1986, SCIP has developed drastically and globally, and has, today, chapters all around the world as well as alliance partnerships with numerous independent affiliate organis ations. On July 8, 2010 the SCIP Board of Directors voted to officially change the 25-year-old non-profit organisation's name from \"Society of Competitive Intelligence Professionals\" to \"Strategic and Competitive Intelligence Professionals\".\n\nSCIP Chapters are the best way for members to build relationships within the competitive intelligence discipline in their area. Its event are open for anyone to attend, become involved, and enjoy the benefits of programming and networking. The SCIP offers many opportunities to its members including exceptional educational and networking experiences with leading industry experts, as well as opportunities to increase knowledge by gaining access to rare and distinctive practices, tools and publications. In addition, the SCIP offers Collegiate Chapters for students who show interest in the world of commercial and competitive intelligence. Participant students in the Collegiate Chapter are offered training and education programs that represent unique opportunities for professional advancement. They are also benefitting exceptionally in terms of leadership skills, career development and indispensable CI know-how. Also, the SCIP awards mentoring programs and scholarships to students who deem prominent, promising and capable business students.\n\nEconomic espionage represents a failure of Commercial Intelligence, which uses open sources and other forms of ethical inquiry. Importantly, SCIP mandates that all this be done within a strictly ethical framework, which is why it has agreed upon a code of ethics;\n\n"}
{"id": "5624367", "url": "https://en.wikipedia.org/wiki?curid=5624367", "title": "Crop Trust", "text": "Crop Trust\n\nThe Crop Trust, officially known as the Global Crop Diversity Trust, is an international nonprofit organization which works to preserve crop diversity in order to protect global food security. It was established through a partnership between the United Nations Food and Agriculture Organization and CGIAR acting through Bioversity International.\n\nIn 2006, the Crop Trust entered into an agreement with the governing body of the International Treaty on Plant Genetic Resources for Food and Agriculture. The agreement recognises the Crop Trust as an \"essential element\" of the treaty's funding strategy in regards to the \"ex situ\" conservation and availability of plant genetics resources for food and agriculture. It also confirms the autonomy of the Crop Trust as a scientific organization in raising and disbursing funds.\n\nThe Crop Trust is assembling an endowment fund, the income from which will be used to support the conservation of distinct and important crop diversity, in perpetuity, through existing institutions. Crop diversity is the biological foundation of agriculture, and is the raw material plant breeders and farmers use to adapt crop varieties to pests and diseases. In the future, this crop diversity will play a central role in helping agriculture adjust to climate change and adapt to water and energy constraints.\n\nThe Crop Trust has its offices in Bonn, Germany, after relocating there from Rome, Italy. The executive board is chaired by Tim Fischer (Australia). The Crop Trust Trust also has a Donors' Council, chaired by Jaap Satter (Netherlands). As of May 2016 the organization had raised approximately US$300 million for its endowment. Main donors include: Australia, Canada, Germany, Ireland, Norway, Sweden, Switzerland, United Kingdom, United States, the Bill and Melinda Gates Foundation, and the Grains Research and Development Corporation (Australia). A number of developing countries have also provided support, including Egypt, Ethiopia and India. Further contributions have been received from private corporations, foundations, industry associations, and from private individuals. \n\nÅslaug Haga, a Norwegian politician who was chairperson of the Centre Party from 2003 to 2008, took over as executive director from Cary Fowler in 2013.\n\nSince its establishment, the Crop Trust has funded work in over 80 countries, and made its first grant for long-term conservation of a collection in late 2006. By 2011, the Crop Trust had established in-perpetuity support (i.e. grants funded through the Crop Trust's endowment) for collections of 15 crops: rice, cassava, wheat, barley, faba bean, pearl millet, maize, forages, banana, aroids, grass pea, sorghum, yam and lentil. \n\nIn 2007, the Crop Trust began a global initiative to rescue threatened, high-priority collections of crop diversity in developing countries and to support information systems to improve their conservation and availability. These efforts included providing support to developing countries and international agricultural research centers to deposit shipments of seed samples in the Svalbard Global Seed Vault for safety duplication purposes. \n\nIn 2010, the Crop Trust launched a global 10-year program to find, gather, catalog and save the wild relatives of 22 major food crops. These wild species contain untapped diversity to help address future challenges to agriculture.\n\nThe Crop Trust joined the Government of Norway and the Nordic Gene Bank in the 2008 establishment of the Svalbard Global Seed Vault, a \"fail-safe\" facility located at Svalbard, Norway. This facility provides a safety back-up for existing genebank collections, which are vulnerable to war, civil strife, natural disasters and even to equipment failure and mismanagement. The Vault has also been touted as providing a means for restoring agriculture in the event of a global catastrophe. It is designed to hold the seeds of some 4.5 million samples of different varieties of agricultural crops.\n\n"}
{"id": "43732182", "url": "https://en.wikipedia.org/wiki?curid=43732182", "title": "Cybertill", "text": "Cybertill\n\nCybertill is a cloud-based retail software provider, which incorporates point of sale (commonly referred to as EPoS), ecommerce, stock control, CRM, Merchandising and Warehousing modules. Cybertill’s headquarters are in Knowsley, near Liverpool, in the UK. The company is privately owned and funded by Merseyside Investment Fund (MSIF). and employs over 100 people.\n\nCybertill was founded by current CEO Ian Tomlinson in 2001. Its software offering consisted of EPoS, ecommerce and mail order systems delivered over an internet connection (referred to as cloud-based software). In late 2001 Cybertill secured funding from Merseyside Investment Fund (MSIF). Cybertill claims to be the world’s first cloud based EPoS and ecommerce system.\n\nCybertill’s first customer was a bike shop in York in 2001. By 2014, according to the company’s website, there are over 650 retailers using its system.\n\nThe company says it is currently migrating users to its latest iteration of its software entitled Retail Store.\n\nIn 2014 Cybertill announced it was developing new supply chain modules. These are merchandising and warehousing.\n\nCybertill is a cloud based retail system which incorporates EPoS, ecommerce and mail order applications. These modules allow retailers to carry out the following tasks such as stock control, ordering, managing promotions and loyalty programmes.\n\nCybertill also supplies a charity retail system, this includes an additional module for gift aid software. This enables charity retailers to claim gift aid on donated goods.\n\nAs well as having its own proprietary ecommerce platform, Cybertill claims that it can integrate with a variety of third party ecommerce platforms such as Magento. Several Magento companies such as Juno and Limesharp promote the fact that they work with Cybertill customers.\n\nCybertill has been featured by national newspapers such as The Guardian and The Financial Times as well as retail trade publications such as Retail Week, Drapers, and Retail Technology, and online news sites including professionaljeweller.com, retailgazette.co.uk, and theintegratedretailer.com\n\nIn recent times Cybertill has been included in TechMarketView’s British Battlers Tech Media Invest Top 100 and London Stock Exchange Group’s 1000 Companies to Inspire Britain in 2013\n"}
{"id": "2496333", "url": "https://en.wikipedia.org/wiki?curid=2496333", "title": "Decorticator", "text": "Decorticator\n\nA decorticator (from Latin: \"cortex\", bark) is a machine for stripping the skin, bark, or rind off nuts, wood, plant stalks, grain, etc., in preparation for further processing.\n\nIn 1861, a farmer named Bernagozzi from Bologna manufactured a machine called a\"scavezzatrice\", a decorticator for hemp. A working hemp decorticator from 1890, manufactured in Germany, is preserved in a museum in Bologna. \n\nMisconceptions about early versions of the device include the suggestion that the first working hemp decorticator was invented in the United States in 1935. In 1916, there were already five different kinds of \"machine brakes\" for hemp in use in the United States, and still others in Europe.\n\nIn Italy, the\"scavezzatrice\" faded in the 1950s because of monopolisation from fossil fuel, paper interests, synthetic materials and from other less profitable crops.\n\nMany types of decorticators have been developed since 1890.\n\nIn 1919, George Schlichten received a U.S. patent on his improvements of the decorticator for treating fiber bearing plants. Schlichten failed to find investors for production of his decorticator and died in 1923, a broken man.\n\nEven though many suppliers claim the \"full-automatic\" type for their decorticating units, the actual operation identifies the \"semi-automatic\" features by several stops during operation.\n\nThere are companies who produce and sell decorticators for different crops.\n"}
{"id": "8708823", "url": "https://en.wikipedia.org/wiki?curid=8708823", "title": "Depssi", "text": "Depssi\n\nThe Depssi card (Depth of field, sunrise, sunset indicator) is a handy guide/reference card that has two functions; it enables the outdoor photographer to quickly select the hyperfocal distance for different lenses (or zoom settings) using various apertures. On the reverse side of the card there are the angles/positions of sunrise and sunset for every month.\n\nThe card is made in a durable plastic suitable for prolonged use. The size of a credit card, it can be tucked into a wallet/purse or secured to your camera bag until needed.\n"}
{"id": "511091", "url": "https://en.wikipedia.org/wiki?curid=511091", "title": "Dipstick", "text": "Dipstick\n\nA dipstick is one of several measurement devices.\n\nSome dipsticks are dipped into a liquid to perform a chemical test or to provide a measure of quantity of the liquid.\n\nSince the late 20th century, a flatness/levelness measuring device trademarked \"Dipstick\" has been used to produce concrete and pavement surface profiles and to help establish profile measurement standards in the concrete floor and paving industries.\n\nA testing dipstick is usually made of paper or cardboard and is impregnated with reagents that indicate some feature of the liquid by changing color. In medicine, dipsticks can be used to test for a variety of liquids for the presence of a given substance, known as an analyte. For example, urine dipsticks are used to test urine samples for haemoglobin, nitrite (produced by bacteria in a urinary tract infection), protein, nitrocellulose, glucose and occasionally urobilinogen or ketones.\n\nThey are usually brightly coloured, and extremely rough to touch.\n\nDipsticks can also be used to measure the quantity of liquid in an otherwise inaccessible space, by inserting and removing the stick and then checking the extent of it covered by the liquid. The most familiar example is the oil level dipstick found on most internal combustion engines.\n\nOther kinds of dipsticks are used to measure everything from fuel levels to the amount of beer left in an ale cask (Firkin).\n\n\"Dipstick\" is the trade name of a profiling device manufactured by Face Construction Technologies of Norfolk, Virginia USA. The instrument is used in 65 countries on six continents to measure the flatness and levelness of concrete floor slabs and pavements.\n\nThe Dipstick measures concrete floor slab flatness/levelness in terms of Face Floor Profile Numbers (\"F-Numbers\"), a profile measurement system adopted in 1990 by the American Concrete Institute. F-Number measurement procedures were established by ASTM Standard E1155. The instrument also measures TR-34 Free Movement (FM); TR-34 Defined Movement (DM); Gap under Sliding Unleveled Straightedge; Gap under Rolling Straightedge; and DIN 18202.\n\nThe U.S. Federal Highway Administration (FHWA) and the World Bank (with its International Roughness Index... or \"IRI\") have established measurement procedures using Dipstick profiler data.\n\nThe American Association of State Highway and Transportation Officials (AASHTO) has established its Standard R 41 (most recently published as R 41-05 (2010)) to, \"... manually collect precision profile data utilizing the Face Technologies Dipstick. The instrument measures profiles (relative elevation differences) at a rate and accuracy greater than traditional rod and level surveys. Procedures for measuring both longitudinal and transverse profiles are described.\"\n\nThe Dipstick, with a reported accuracy of .01 mm ( 0.0004 inches), measures \"true\" profiles and is the most widely used and accepted Class 1 profiler for the purposes of calibrating other profilers.\n\nDipstick was used to obtain data that were used as ground truth in FHWA evaluations of the repeatability of IRI values as measured by other profilers and in Long-Term Pavement Performance (LTTP) studies conducted by several states.\n\nThe instrument was similarly used to produce reference measurements by the World Road Association (PIARC) in its 1998 \"International Experiment to Harmonise Longitudinal and Transverse Profile Measurement and Reporting Procedures.\" The PIARC experiment was conducted in the USA, Japan, Holland and Germany and included IRI values from airport runways and super highways to rough unpaved roads.\n\n"}
{"id": "55761788", "url": "https://en.wikipedia.org/wiki?curid=55761788", "title": "Elizabeth Beecroft", "text": "Elizabeth Beecroft\n\nElizabeth \"Betty\" Beecroft née Skirrow (1748–1812) was farmer's wife who bought the Kirkstall forge in 1779 and then managed it, making and selling iron and ironware. Later, she focussed upon the farm's produce such as selling butter.\n"}
{"id": "43730085", "url": "https://en.wikipedia.org/wiki?curid=43730085", "title": "European Association for Technical Communication", "text": "European Association for Technical Communication\n\nThe European Association for Technical Communication (tekom Europe e.V.) is the largest professional association for technical communication worldwide. The association connects more than 8,500 professionals like technical communicators, technical writers, and others from related fields. The working language is English.\n\ntekom Europe was founded in November 2013 in Wiesbaden, Germany, by representatives of tekom Deutschland, of the former tekom country groups and of the Italian association COM&TEC. and registered as a lobby organization in the European Transparency Register.\n\nThe main task of the association is to organize creators of user information in their countries and represent their interests on European level. tekom Europe supports important EU policies such as improving the training of young people, the employability and mobility of workers as well as the competitiveness of the European economy in general. One of the European initiatives which tekom Europe is involved in is ESCO (European Skills/Competences, qualifications and Occupations).\n\nThe association currently consists of members from nine countries: \n\nCandidate Countries (where a country organization will be founded soon): \n\ntekom Europe’s mission is…<br>\n… promote and further develop technical communication in Europe\n\n… set European standards for the quality of technical communication\n\n… increase the importance given to technical communication throughout Europe, both in commerce and among the general public\n\n… strengthen and harmonize the occupational profile of the technical writer\n\n… promote cooperation among universities and educational institutions at a European level\n\n… participate in developing international standards for user manuals and operating instructions\n\n\nChairperson: Isabelle Fleury\n\nDeputy Chairperson: Tiziana Sicila\n\nSecretary: Holger Thater\n\nTreasurer: Martin Rieder\n"}
{"id": "961961", "url": "https://en.wikipedia.org/wiki?curid=961961", "title": "Gene gun", "text": "Gene gun\n\nA gene gun or a biolistic particle delivery system, originally designed for plant transformation, is a device for delivering exogenous DNA (transgenes) to cells. The payload is an elemental particle of a heavy metal coated with DNA (typically plasmid DNA). This technique is often simply referred to as biolistics.\n\nThis device is able to transform almost any type of cell, including plants, and is not limited to transformation of the nucleus; it can also transform organelles, including plastids.\n\nThe gene gun was originally a Crosman air pistol modified to fire dense tungsten particles. It was invented by John C Sanford, Ed Wolf and Nelson Allen at Cornell University, and Ted Klein of DuPont, between 1983 and 1986. The original target was onions (chosen for their large cell size) and it was used to deliver particles coated with a marker gene. Genetic transformation was then proven when the onion tissue expressed the gene.\n\nThe earliest custom manufactured gene guns (fabricated by Nelson Allen) used a 22 caliber nail gun cartridge to propel an extruded polyethylene cylinder (bullet) down a 22 cal. Douglas barrel. A droplet of the tungsten powder and genetic material was placed on the bullet and shot down the barrel at a lexan \"stopping\" disk with a Petri dish below. The bullet welded to the disk and the genetic material blasted into the sample in the dish with a doughnut effect (devastation in the middle, a ring of good transformation and little around the edge). The gun was connected to a vacuum pump and was under vacuum while firing. The early design was put into limited production by a Rumsey-Loomis (a local machine shop then at Mecklenburg Rd in Ithaca, NY, USA). Later the design was refined by removing the \"surge tank\" and changing to nonexplosive propellants. DuPont added a plastic extrusion to the exterior to visually improve the machine for mass production to the scientific community. Biorad contracted with Dupont to manufacture and distribute the device. Improvements include the use of helium propellant and a multi-disk-collision delivery mechanism. Other heavy metals such as gold and silver are also used. Gold may be favored because it has better uniformity than tungsten and tungsten can be toxic to cells, but its use may be limited due to availability and cost.\n\nA construct is a piece of DNA inserted into the target's genome, including parts that are intended to be removed later. All biolistic transformations require a construct to proceed and while there is great variation among biolistic constructs, they can be broadly sorted into two categories: those which are designed to transform eukaryotic nuclei, and those designed to transform prokaryotic-type genomes such as mitochondria, plasmids or plastids.\n\nThose meant to transform prokaryotic genomes generally have the gene or genes of interest, at least one promoter and terminator sequence, and a reporter gene; which is a gene used to enable detection or removal of those cells which didn't integrate the construct into their DNA. These genes may each have their own promoter and terminator, or be grouped to produce multiple gene products from one transcript, in which case binding sites for translational machinery should be placed between each to ensure maximum translational efficiency. In any case the entire construct is flanked by regions called border sequences which are similar in sequence to locations within the genome, this allows the construct to target itself to a specific point in the existing genome.\n\nConstructs meant for integration into a eukaryotic nucleus follow a similar pattern except that: the construct contains no border sequences because the sequence rearrangement that prokaryotic constructs rely on rarely occurs in eukaryotes; and each gene contained within the construct must be expressed by its own copy of a promoter and terminator sequence.\n\nThough the above designs are generally followed, there are exceptions. For example, the construct might include a Cre-Lox system to selectively remove inserted genes; or a prokaryotic construct may insert itself downstream of a promoter, allowing the inserted genes to be governed by a promoter already in place and eliminating the need for one to be included in the construct.\n\nGene guns are so far mostly used with plant cells. However, there is much potential use in humans and other animals as well.\n\nThe target of a gene gun is often a callus of undifferentiated plant cells or a group of immature embryos growing on gel medium in a Petri dish. After the DNA-coated gold particles have been delivered to the cells, the DNA is used as a template for transcription (transient expression) and sometimes it integrates into a plant chromosome ('stable' transformation)\n\nIf the delivered DNA construct contains a selectable marker, then stably transformed cells can be selected and cultured using tissue culture methods. For example, if the delivered DNA construct contains a gene that confers resistance to an antibiotic or herbicide, then stably transformed cells may be selected by including that antibiotic or herbicide in the tissue culture media.\n\nTransformed cells can be treated with a series of plant hormones, such as auxins and gibberellins, and each may divide and differentiate into the organized, specialized, tissue cells of an entire plant. This capability of total re-generation is called totipotency. The new plant that originated from a successfully transformed cell may have new traits that are heritable. The use of the gene gun may be contrasted with the use of \"Agrobacterium tumefaciens\" and its Ti plasmid to insert DNA into plant cells. See transformation for different methods of transformation in different species.\n\nGene guns have also been used to deliver DNA vaccines.\n\nThe delivery of plasmids into rat neurons through the use of a gene gun, specifically DRG neurons, is also used as a pharmacological precursor in studying the effects of neurodegenerative diseases such as Alzheimer's disease.\n\nThe gene gun has become a common tool for labeling subsets of cells in cultured tissue. In addition to being able to transfect cells with DNA plasmids coding for fluorescent proteins, the gene gun can be adapted to deliver a wide variety of vital dyes to cells.\n\nGene gun bombardment has also been used to transform \"Caenorhabditis elegans\", as an alternative to microinjection.\n\nBiolistics has proven to be a versatile method of genetic modification and it is generally preferred to engineer transformation-resistant crops, such as cereals. Notably, \"Bt\" maize is a product of biolistics. Plastid transformation has also seen great success with particle bombardment when compared to other current techniques, such as \"Agrobacterium\" mediated transformation, which have difficulty targeting the vector to and stably expressing in the chloroplast. In addition, there are no reports of a chloroplast silencing a transgene inserted with a gene gun. Additionally, with only one firing of a gene gun, a skilled technician can generate two transformed organisms. This technology has even allowed for modification of specific tissues \"in situ\", although this is likely to damage large numbers of cells and transform only some, rather than all, cells of the tissue.\n\nBiolistics introduces DNA randomly into the target cells. Thus the DNA may be transformed into whatever genomes are present in the cell, be they nuclear, mitochondrial, plasmid or any others, in any combination, though proper construct design may mitigate this. Another issue is that the gene inserted may be overexpressed when the construct is inserted multiple times in either the same or different locations of the genome. This is due to the ability of the constructs to give and take genetic material from other constructs, causing some to carry no transgene and others to carry multiple copies; the number of copies inserted depends on both how many copies of the transgene an inserted construct has, and how many were inserted. Also, because eukaryotic constructs rely on illegitimate recombination, a process by which the transgene is integrated into the genome without similar genetic sequences, and not homologous recombination, which inserts at similar sequences, they cannot be targeted to specific locations within the genome, unless the transgene is co-delivered with genome editing reagents.\n\n"}
{"id": "335004", "url": "https://en.wikipedia.org/wiki?curid=335004", "title": "Graphing calculator", "text": "Graphing calculator\n\nA graphing calculator (also graphics / graphic display calculator) is a handheld computer that is capable of plotting graphs, solving simultaneous equations, and performing other tasks with variables. Most popular graphing calculators are also programmable, allowing the user to create customized programs, typically for scientific/engineering and education applications. Because they have large displays in comparison to standard 4-operation handheld calculators, graphing calculators also typically display several lines of text and calculations at the same time.\n\nAn early \"graphical calculator\" was designed and patented by electrical engineer, Edith Clarke, in 1925. The calculator was used to solve problems with electrical power line transmission.\n\nCasio produced the first commercially available graphing calculator, the fx-7000G, in 1985. Casio's innovations include an icon menu for easy access to functions (1994, FX-7700GE and later), graphing in several colors (1995, CFX-9800G), textbook-like \"Natural Display\" input and output (2004, FX-82ES/300ES & FX-9860G), expandable memory (2005, FX-9860SD), backlit screen (2006, FX-9860G Slim), full-color, high resolution backlit screen (2010, FX-CG10/CG20 PRIZM).\n\nSharp produced its first graphing calculator, the EL-5200, in 1986. Since then Sharp's innovations include models with a touchscreen (EL9600 series), Equation Editor (textbook-like input) (EL-9300 ( 1992 ) and later), and reversible keyboard to ease learning (one side has basic functions, the other side has additional functions) (March 2005, EL-9900).\n\nHewlett Packard followed in the form of the HP-28C. This was followed by the HP-28S (1988), HP-48SX (1990), HP-48S (1991), and many other models. Models like the HP 50g (2006) or the HP Prime (2013) feature a computer algebra system (CAS) capable of manipulating symbolic expressions and analytic solving. An unusual and powerful CAS \"calculator\" is the now obsolete year 2001 Casio Cassiopeia A-10 and A-11 (flip top) stylus-operated PDAs, which ran the Maple V symbolic engine. The HP-28 and -48 ranges were primarily meant for the professional science/engineering markets; the HP-38/39/40 were sold in the high school/college educational market; while the HP-49 series cater to both educational and professional customers of all levels. The HP series of graphing calculators is best known for its Reverse Polish notation (RPN) / Reverse Polish Lisp (RPL) interface, although the HP-49G introduced a standard expression entry interface as well.\n\nTexas Instruments has produced graphing calculators since 1990, the oldest of which was the TI-81. Some of the newer calculators are similar, with the addition of more memory, faster processors, and USB connection such as the TI-82, TI-83 series, and TI-84 series. Other models, designed to be appropriate for students 10–14 years of age, are the TI-80 and TI-73. Other TI graphing calculators have been designed to be appropriate for calculus, namely the TI-85, TI-86, TI-89 series, and TI-92 series (TI-92, TI-92 Plus, and Voyage 200). TI offers a CAS on the TI-89, TI-Nspire CAS and TI-92 series of calculators. TI calculators are targeted specifically to the educational market, but are also widely available to the general public.\n\nSome graphing calculators have a computer algebra system (CAS), which means that they are capable of producing symbolic results. These calculators can manipulate algebraic expressions, performing operations such as factor, expand, and simplify. In addition, they can give answers in exact form without numerical approximations. Calculators that have a computer algebra system are called symbolic or CAS calculators. Examples of symbolic calculators include the HP 50g, the HP Prime, the TI-89, the TI-Nspire CAS and TI-Nspire CX CAS and the Casio ClassPad series.\n\nMany graphing calculators can be attached to devices like electronic thermometers, pH gauges, weather instruments, decibel and light meters, accelerometers, and other sensors and therefore function as data loggers, as well as WiFi or other communication modules for monitoring, polling and interaction with the teacher. Student laboratory exercises with data from such devices enhances learning of math, especially statistics and mechanics.\n\nSince graphing calculators are usually readily user-programmable, such calculators are also widely used for gaming purposes, with a sizable body of user-created game software on most popular platforms. Some of the most notable and extensive community-driven graphing calculator archives are ticalc.org and cemetech.net. The ability to create games and other utilities within most graphing calculators has spurred the creation of numerous calculator hobbyist sites, where more advanced programs are created using a calculator's assembly language. It is simple to download games to a graphing calculator, as nearly all calculator program archives are free and open source. Even though handheld gaming devices fall in a similar price range, graphing calculators offer superior math programming capability for math based games. However, for developers and advanced users like researchers, analysts and gamers, 3rd party software development involving firmware mods, whether for powerful gaming or exploiting capabilities beyond the published data sheet and programming language, is a contentious issue with manufacturers and education authorities as it might incite unfair calculator use during standardized high school and college tests where these devices are targeted. Nowadays graduate (Masters) students and researchers have turned to advanced Computer Aided Math software for learning as well as experimenting.\n\n\nMost graphing calculators, as well as some non-graphing scientific and programmer's calculators can be programmed to automate complex and frequently used series of calculations and those inaccessible from the keyboard.\n\nThe actual programming can often be done on a computer then later uploaded to the calculators. The most common tools for this include the PC link cable and software for the given calculator, configurable text editors or hex editors, and specialized programming tools such as the below-mentioned implementation of various languages on the computer side.\n\nEarlier calculators stored programs on magnetic cards and the like; increased memory capacity has made storage on the calculator the most common implementation. Some of the newer machines can also use memory cards.\n\nMany graphing and scientific calculators will tokenize the program text, replacing textual programming elements with short numerical tokens. For example, take this line of TI-BASIC code:\ncodice_1\nIn a conventional programming language, this line of code would be nine characters long (eight not including a newline character). For a system as slow as a graphing calculator, this is too inefficient for an interpreted language. To increase program speed and coding efficiency, the above line of code would be only three characters. \"Disp_\" as a single character, \"[A]\" as a single character, and a newline character. This normally means that single byte chars will query the standard ASCII chart while two byte chars (the Disp_ for example) will build a graphical string of single byte characters but retain the two byte character in the program memory. Many graphical calculators work much like computers and use versions of 7-bit, 8-bit or 9-bit ASCII-derived character sets or even UTF-8 and Unicode. Many of them have a tool similar to the character map on Windows.\n\nThey also have BASIC like functions such as chr$, chr, char, asc, and so on, which sometimes may be more Pascal or C like. One example may be use of \"ord\", as in Pascal, instead of the \"asc\" of many Basic variants, to return the code of a character, i.e. the position of the character in the collating sequence of the machine.\n\nA cable and/or IrDA transceiver connecting the calculator to a computer make the process easier and expands other possibilities such as on-board spreadsheet, database, graphics, and word processing programs. The second option is being able to code the programs on board the calculator itself. This option is facilitated by the inclusion of full-screen text editors and other programming tools in the default feature set of the calculator or as optional items. Some calculators have QWERTY keyboards and others can be attached to an external keyboard which can be close to the size of a regular 102-key computer keyboard. Programming is a major use for the software and cables used to connect calculators to computers.\n\nThe most common programming languages used for calculators are similar to keystroke-macro languages and variants of BASIC. The latter can have a large feature set—approaching that of BASIC as found in computers—including character and string manipulation, advanced conditional and branching statements, sound, graphics, and more including, of course, the huge spectrum of mathematical, string, bit-manipulation, number base, I/O, and graphics functions built into the machine.\n\nLanguages for programming calculators fall into all of the main groups, i.e. machine code, low-level, mid-level, high-level languages for systems and application programming, scripting, macro, and glue languages, procedural, functional, imperative &. object-oriented programming can be achieved in some cases.\n\nMost calculators capable to being connected to a computer can be programmed in assembly language and machine code, although on some calculators this is only possible through using exploits. The most common assembly and machine languages are for TMS9900, SH-3, Zilog Z80, and various Motorola chips (e.g. a modified 68000) which serve as the main processors of the machines although many (not all) are modified to some extent from their use elsewhere. Some manufacturers do not document and even mildly discourage the assembly language programming of their machines because they must be programmed in this way by putting together the program on the PC and then forcing it into the calculator by various improvised methods.\n\nOther on-board programming languages include purpose-made languages, variants of Eiffel, Forth, and Lisp, and Command Script facilities which are similar in function to batch/shell programming and other glue languages on computers but generally not as full featured. Ports of other languages like BBC BASIC and development of on-board interpreters for Fortran, REXX, AWK, Perl, Unix shells (e.g., bash, zsh), other shells (DOS/Windows 9x, OS/2, and Windows NT family shells as well as the related 4DOS, 4NT and 4OS2 as well as DCL), COBOL, C, Python, Tcl, Pascal, Delphi, ALGOL, and other languages are at various levels of development.\n\nSome calculators, especially those with other PDA-like functions have actual operating systems including the TI proprietary OS for its more recent machines, DOS, Windows CE, and rarely Windows NT 4.0 Embedded et seq, and Linux. Experiments with the TI-89, TI-92, TI-92 Plus and Voyage 200 machines show the possibility of installing some variants of other systems such as a chopped-down variant of CP/M-68K, an operating system which has been used for portable devices in the past.\n\nTools which allow for programming the calculators in C/C++ and possibly Fortran and assembly language are used on the computer side, such as HPGCC, TIGCC and others. Flash memory is another means of conveyance of information to and from the calculator.\n\nThe on-board BASIC variants in TI graphing calculators and the languages available on HP-48 type calculators can be used for rapid prototyping by developers, professors, and students, often when a computer is not close at hand.\n\nMost graphing calculators have on-board spreadsheets which usually integrate with Microsoft Excel on the computer side. At this time, spreadsheets with macro and other automation facilities on the calculator side are not on the market. In some cases, the list, matrix, and data grid facilities can be combined with the native programming language of the calculator to have the effect of a macro and scripting enabled spreadsheet.\n\n\n"}
{"id": "11551222", "url": "https://en.wikipedia.org/wiki?curid=11551222", "title": "Heat pump and refrigeration cycle", "text": "Heat pump and refrigeration cycle\n\nThermodynamic heat pump cycles or refrigeration cycles are the conceptual and mathematical models for heat pumps and refrigerators. A heat pump is a machine or device that moves heat from one location (the \"source\") at a lower temperature to another location (the \"sink\" or \"heat sink\") at a higher temperature using mechanical work or a high-temperature heat source. Thus a heat pump may be thought of as a \"heater\" if the objective is to warm the heat sink (as when warming the inside of a home on a cold day), or a \"refrigerator\" if the objective is to cool the heat source (as in the normal operation of a freezer). In either case, the operating principles are identical. Heat is moved from a cold place to a warm place.\n\nAccording to the second law of thermodynamics heat cannot spontaneously flow from a colder location to a hotter area; work is required to achieve this. An air conditioner requires work to cool a living space, moving heat from the cooler interior (the heat source) to the warmer outdoors (the heat sink). Similarly, a refrigerator moves heat from inside the cold icebox (the heat source) to the warmer room-temperature air of the kitchen (the heat sink). The operating principle of the refrigeration cycle was described mathematically by Sadi Carnot in 1824 as a heat engine. A heat pump can be thought of as a heat engine which is operating in reverse.\n\nHeat pump and refrigeration cycles can be classified as \"vapor compression\", \"vapor absorption\", \"gas cycle\", or \"Stirling cycle\" types.\n\nThe vapor-compression cycle is used in most household refrigerators as well as in many large commercial and industrial refrigeration systems. Figure 1 provides a schematic diagram of the components of a typical vapor-compression refrigeration system. The thermodynamics of the cycle can be analysed on a diagram as shown in Figure 2. In this cycle, a circulating working fluid commonly called refrigerant such as Freon enters the compressor as a vapor. The vapor is compressed at constant entropy and exits the compressor superheated. The superheated vapor travels through the condenser which first cools and removes the superheat and then condenses the vapor into a liquid by removing additional heat at constant pressure and temperature. The liquid refrigerant goes through the expansion valve (also called a throttle valve) where its pressure abruptly decreases, causing flash evaporation and auto-refrigeration of, typically, less than half of the liquid. That results in a mixture of liquid and vapor at a lower temperature and pressure. The cold liquid-vapor mixture then travels through the evaporator coil or tubes and is completely vaporized by cooling the warm air (from the space being refrigerated) being blown by a fan across the evaporator coil or tubes. The resulting refrigerant vapor returns to the compressor inlet to complete the thermodynamic cycle.\n\nThe above discussion is based on the ideal vapor-compression refrigeration cycle, and does not take into account real-world effects like frictional pressure drop in the system, slight thermodynamic irreversibility during the compression of the refrigerant vapor, or non-ideal gas behavior (if any).\n\nIn the early years of the twentieth century, the vapor absorption cycle using water-ammonia systems was popular and widely used but, after the development of the vapor compression cycle, it lost much of its importance because of its low coefficient of performance (about one fifth of that of the vapor compression cycle). Nowadays, the vapor absorption cycle is used only where heat is more readily available than electricity, such as waste heat provided by solar collectors, or off-the-grid refrigeration in recreational vehicles.\n\nThe absorption cycle is similar to the compression cycle, except for the method of raising the pressure of the refrigerant vapor. In the absorption system, the compressor is replaced by an absorber which dissolves the refrigerant in a suitable liquid, a liquid pump which raises the pressure and a generator which, on heat addition, drives off the refrigerant vapor from the high-pressure liquid. Some work is required by the liquid pump but, for a given quantity of refrigerant, it is much smaller than needed by the compressor in the vapor compression cycle. In an absorption refrigerator, a suitable combination of refrigerant and absorbent is used. The most common combinations are ammonia (refrigerant) and water (absorbent), and water (refrigerant) and lithium bromide (absorbent).\n\nWhen the working fluid is a gas that is compressed and expanded but does not change phase, the refrigeration cycle is called a \"gas cycle\". Air is most often this working fluid. As there is no condensation and evaporation intended in a gas cycle, components corresponding to the condenser and evaporator in a vapor compression cycle are the hot and cold gas-to-gas heat exchangers.\n\nFor given extreme temperatures, a gas cycle may be less efficient than a vapor compression cycle because the gas cycle works on the reverse Brayton cycle instead of the reverse Rankine cycle. As such, the working fluid never receives or rejects heat at constant temperature. In the gas cycle, the refrigeration effect is equal to the product of the specific heat of the gas and the rise in temperature of the gas in the low temperature side. Therefore, for the same cooling load, gas refrigeration cycle machines require a larger mass flow rate, which in turn increases their size.\n\nBecause of their lower efficiency and larger bulk, \"air cycle\" coolers are not often applied in terrestrial refrigeration. The air cycle machine is very common, however, on gas turbine-powered jet airliners since compressed air is readily available from the engines' compressor sections. These jet aircraft's cooling and ventilation units also serve the purpose of heating and pressurizing the aircraft cabin.\n\nThe Stirling cycle heat engine can be driven in reverse, using a mechanical energy input to drive heat transfer in a reversed direction (i.e. a heat pump, or refrigerator). There are several design configurations for such devices that can be built. Several such setups require rotary or sliding seals, which can introduce difficult tradeoffs between frictional losses and refrigerant leakage.\n\nSince the Carnot cycle is a reversible cycle, the four processes that comprise it, two isothermal and two isentropic, can all be reversed as well. When this happens, it is called a reversed Carnot cycle. A refrigerator or heat pump that acts on the reversed Carnot cycle is called a Carnot refrigerator and Carnot heat pump respectively. In the first stage of this cycle (process 1-2), the refrigerant absorbs heat isothermally from a low-temperature source, T, in the amount Q. Next, the refrigerant is isentropically compressed (process 2-3) and the temperature rises to the high-temperature source, T. Then at this high temperature, the refrigerant rejects heat isothermally in the amount Q (process 3-4). Also during this stage, the refrigerant changes from a saturated vapor to a saturated liquid in the condenser. Lastly, the refrigerant expands isentropically where the temperature drops back to the low-temperature source, T (process 4-1).\n\nThe efficiency of a refrigerator or heat pump is given by a parameter called the coefficient of performance (COP).\n\nThe COP of a refrigerator is given by the following equation: \n\nThe COP of a heat pump is given by the following equation:\n\nBoth the COP of a refrigerator and a heat pump can be greater than one. Combining these two equations results in:\n\nThis implies that COP will be greater than one because COP will be a positive quantity. In a worst-case scenario, the heat pump will supply as much energy as it consumes, making it act as a resistance heater. However, in reality, as in home heating, some of Q is lost to the outside air through piping, insulation, etc., thus making the COP drop below unity when the outside air temperature is too low. Therefore, the system used to heat houses uses fuel.\n\nFor an ideal refrigeration cycle:\n\nFor an ideal heat pump cycle:\n\nFor Carnot refrigerators and heat pumps, COP is expressed in terms of temperatures:\n\n\n"}
{"id": "2206225", "url": "https://en.wikipedia.org/wiki?curid=2206225", "title": "High chair", "text": "High chair\n\nA high chair is a piece of furniture used for feeding older babies and younger toddlers. The seat is raised a fair distance from the ground, so that a person of adult height may spoon-feed the child comfortably from a standing position (hence the name). It often has a wide base to increase stability. There is a tray which is attached to the arms of the high chair, which allows the adult to place the food on it for either the child to pick up and eat or for the food to be spoon-fed to them. High chairs typically have seat belts to strap the child in.\n\nA booster chair is meant to be used with a regular chair to boost the height of a child sufficiently. Some boosters are a simple monolithic piece of plastic. Others are more complex and are designed to fold up and include a detachable tray.\n\nRarely, a chair can be suspended from the edge of the table avoiding the need for an adult chair or a high chair.\n\nThe EU standards EN 14988-1:2006 + A1 and EN 14988-2: 2006 + A1 on high chairs will be published by the member states during autumn 2012. Meanwhile, they are available at the online library of the European Committee for Standardization (CEN).\n"}
{"id": "26492080", "url": "https://en.wikipedia.org/wiki?curid=26492080", "title": "Horn furniture", "text": "Horn furniture\n\nHorn furniture is a name given to furniture which is manufactured completely by shed antlers or pieces of furniture such as e.g. cabinets which are appliqued with antler elements such as carved horn roses or with antler pieces from tusks, fallow deer, stag and deer. Trophies of chase have already been used during the late Middle Age for furnishing and in modern times furniture makers use the horns and antlers of animals such as cattle (usually longhorned ones), antelope, moose and elk.\n\nAntlers were already used in the late 15th century as a source material for clothes hooks, storage racks and chandeliers, the so-called \"lusterweibchen\". Furniture made by antlers have been invented in the 19th century and can therefore be assigned to the Biedermeier period. They were exclusively made for the European nobility to decorate their hunting castles and various manors. Historically mentioned horn furniture was developed in the year 1825 for a hunting castle of the count William of Nassau near Wiesbaden, Germany, A photograph of 1890 is the single existing evidence as the castles was destroyed in World War II and the interior furniture is regarded as lost. Further known collections of historic horn furniture are credited with the hunting room in the country estate of the Brandhof of the Archduke Johann of Austria or the antler collection of count Arco in Munich, Germany. Many other examples of horn furniture of that time can only be found with drawings and pictures, e.g. Joseph Danhauser (1780–1829) an Austrian furniture manufacturer left hundreds of drawing and another well-known aquarelle is the dressing room of Ferdinand Baron von Hildprandt.\n\nThe ambitious European middle-class people embodied a lifestyle trend of the cultivation of home décor during the second half of the 19th century. With the increasing popularity of the World Exhibitions furniture styling trends became more and more accessible. The London World Exhibition in 1851 presented antler furniture as one of its great novelties. The Hamburg sculptor and ivory carver H.F.C.Rampendahl exhibited horn furniture pieces such as bureaus, chairs and sofa. Many other artists followed and the manufacturing of horn furniture finalized in a serial production via catalogue sales.\n\nPopular horn furniture was related to the following well-known designers and or company names in Germany and Austria such as Gustav Lorenz, Heinrich Keitel, Kurt Schicker, Rudolf Brix, Vitus Madel & Sohn. In the United States the most famous designer was Friedrich Wenzel. Their portfolio covered the complete market of furnishings and additionally a lot of houseware.\n\nThe horn furniture trend suddenly ended at the end of the 1920s and the production was ceased completely. During the post-war period in Europe only individual orders could be recognized by antique furnishing experts.\n\nThe lodge style and cabin décor trend in modern households today is facing a reinvigorated demand for horn furniture. Home décor magazines show modern reproductions of seating horn furniture, lamps, candle stick and other small furniture.\n\n"}
{"id": "11234940", "url": "https://en.wikipedia.org/wiki?curid=11234940", "title": "ISO/TC 37", "text": "ISO/TC 37\n\nISO/TC 37 is a technical committee within the International Organization for Standardization (ISO) that prepares standards and other documents concerning methodology and principles for terminology and language resources.\n\nTitle: Terminology and other language and content resources\n\nScope: Standardization of principles, methods and applications relating to terminology and other language and content resources in the contexts of multilingual communication and cultural diversity\n\nISO/TC 37 is a so-called \"horizontal committee\", providing guidelines for all other technical committees that develop standards on how to manage their terminological problems. However, the standards developed by ISO/TC 37 are not restricted to ISO. Collaboration with industry is sought to ensure that the requirements and needs from all possible users of standards concerning terminology, language and structured content are duly and timely addressed.\n\nInvolvement in standards development is open to all stakeholders and requests can be made to the TC through any liaison or member organization (see the list of current members and liaisons of ISO/TC 37:)\n\nISO/TC 37 standards are therefore fundamental and should form the basis for many localization, translation, and other industry applications.\n\nInternational Standards are developed by experts from industry, academia and business who are delegates of their national standards institution or another organization in liaison. Involvement, therefore, is principally open to all stakeholders. They are based on consensus among those national standards institutes who collaborate in the respective committee by way of membership.\n\nISO/TC 37 develops International Standards concerning:\n\nISO/TC 37 looks upon a long history of terminology unification activities. In the past, terminology experts - even more so experts of terminology theory and methodology - had to struggle for wide recognition. Today their expertise is sought in many application areas, especially in various fields of standardization. The emerging multilingual information society and knowledge society will depend on reliable digital content. Terminology is indispensable here. This is because terminology plays a crucial role wherever and whenever specialized information and knowledge is being prepared (e.g. in research and development), used (e.g. in specialized texts), recorded and processed (e.g. in data banks), passed on (via training and teaching), implemented (e.g. in technology and knowledge transfer), or translated and interpreted. In the age of globalization the need for methodology standards concerning multilingual digital content is increasing - ISO/TC 37 has developed over the years the expertise for methodology standards for science and technology related content in textual form.\n\nThe beginnings of terminology standardization are closely linked to the standardization efforts of IEC (International Electrotechnical Commission, founded in 1906) and ISO (International Organization for Standardization, founded in 1946).\n\nA terminology standard according to ISO/IEC Guide 2 (1996) is defined as \"standard that is concerned with terms, usually accompanied by their definitions, and sometimes by explanatory notes, illustrations, examples, etc.\"\n\nISO 1087-1:2000 defines terminology as \"\"set of designations belonging to one special language\" and designations as \"representation of a concept by a sign which denotes it\".\" Here, concept representation goes beyond terms (being only linguistic signs), which is also supported by the state-of-the-art of terminology science, according to which terminology has three major functions:\n\n\nThe above indicates that terminological data (comprising various kinds of knowledge representation) possibly have a much more fundamental role in domain-related information and knowledge than commonly understood.\n\nToday, terminology standardization can be subdivided into two distinct activities:\n\nThe two are mutually interdependent, since the standardization of terminologies would not result in high-quality terminological data, if certain common principles, rules and methods are not observed. On the other hand, these standardized terminological principles, rules and methods must reflect the state-of-the-art of theory and methodology development in those domains, in which terminological data have to be standardized in connection with the formulation of subject standards.\n\nTerminology gained a special position in the field of standardization at large, which is defined as \"activity of establishing, with regard to actual or potential problems, provisions for common and repeated use, aimed at the achievement of the optimum degree of order in a given context\" (ISO/IEC 1996). Every technical committee or sub-committee or working group has to standardize subject matters, define and standardize its respective terminology. There is a consensus that terminology standardization precedes subject standardization (or \"subject standardization requires terminology standardization\").\n\nISO/TC 37 was put into operation in 1952 in order \"to find out and formulate general principles of terminology and terminological lexicography\" (as terminography was called at that time).\n\nThe history of terminology standardization proper - if one excludes earlier attempts in the field of metrology - started in the International Electrotechnical Commission (IEC), which was founded in London in 1906 following a recommendation passed at the International Electrical Congress, held in St. Louis, United States, on 15 September 1904, to the extent that: \"\"...steps should be taken to secure the co-operation of the technical societies of the world, by the appointment of a representative Commission to consider the question of the standardization of the nomenclature and ratings of electrical apparatus and machinery\".\" From the very beginning, IEC considered it its foremost task to standardize the terminology of electrotechnology for the sake of the quality of its subject standards, and soon embarked upon the International Electrotechnical Vocabulary (IEV), whose first edition, based on many individual terminology standards, was published in 1938. The IEV is still being continued today, covering 77 chapters as parts of the International Standard series IEC 60050. The IEV Online Database can be accessed on Electropedia \n\nThe predecessor to the International Organization for Standardization (ISO), the International Federation of Standardizing Associations (ISA, founded in 1926), made a similar experience. But it went a step further and - triggered by the publication of Eugen Wüster's book \"Internationale Sprachnormung in der Technik\" [International standardization of technical language] (Wüster 1931) - established in 1936 the Technical Committee ISA/TC 37 \"Terminology\" for the sake of formulating general principles and rules for terminology standardization.\n\nISA/TC 37 conceived a scheme of four classes of recommendations for terminology standardization mentioned below, but the Second World War interrupted its pioneering work. Nominally, ISO/TC 37 was established from the very beginning of ISO in 1946, but it was decided to re-activate it only in 1951 and the Committee started operation in 1952. Since then until 2009 the secretariat of ISO/TC 37 has been held by the International Information Centre for Terminology (Infoterm), on behalf of the Austrian Standards Institute Austria. Infoterm, an international non-governmental organization based in Austria, continues to collaborate as a twinning secretariat. After this the administration went to CNIS (China).\n\nTo prepare standards specifying principles and methods for the preparation and management of language resources within the framework of standardization and related activities. Its technical work results in International Standards (and Technical Reports) covering terminological principles and methods as well as various aspects of computer-assisted terminography. ISO/TC 37 is not responsible for the co-ordination of the terminology standardizing activities of other ISO/TCs.\n\n\nISO 639 Codes for the representation of names of languages, with the following parts:\n\n\"Note: Current status is not mentioned here - see ISO Website for most recent status. Many of these are in development.\":\n\n"}
{"id": "37362418", "url": "https://en.wikipedia.org/wiki?curid=37362418", "title": "InMusic Brands", "text": "InMusic Brands\n\ninMusic is the parent company for a family of brands of varying audio products used in the DJ, music production, live sound, musical instrument, pro audio, and consumer electronics industries.\n\nNumark Industries is a manufacturer of audio equipment for disk jockeys, founded in 1971, and based in Cumberland, Rhode Island, United States. Numark was formerly owned by Jack O'Donnell. Numark acquired Alesis in 2001. In 2012, O' Donnell acquired the M-Audio and AIR Music Technology brands from Avid Technology for $17 million. From now on, all brands will be managed under the holding company inMusic Brands. Numark makes DJ mixers, turntables, MIDI controllers, DJ CD players,\n\n\n"}
{"id": "45630591", "url": "https://en.wikipedia.org/wiki?curid=45630591", "title": "International Radio Corporation", "text": "International Radio Corporation\n\nThe International Radio Corporation (IRC) was an American radio receiver manufacturing company based in Ann Arbor, Michigan. It was established in 1931 by Charles Albert Verschoor with financial backing from Ann Arbor mayor William E. Brown, Jr., and a group of local business leaders. IRC manufactured numerous different radios, many bearing the Kadette name, including the first mass-produced AC/DC radio, the first pocket radio, and the first clock radio. Due to the seasonal nature of radio sales, the company attempted to diversify its offerings with a product that would sell well during the summer, eventually settling on a camera that would become the Argus. In 1939, IRC sold its radio-manufacturing business to its former General Sales Manager, W. Keene Jackson, although his new Kadette Radio Corporation only survived for a year before it went defunct. After World War II, International Industries and its International Research division became wholly owned subsidiaries of Argus, Inc., after which point the International name ceased to exist.\n\nThe International Radio Corporation was founded in 1931 in Ann Arbor, Michigan, the creation of Charles Albert Verschoor, who had begun making radios in the 1920s. Described as a \"colorful old-time promoter\" in a January 1945 \"Fortune\" magazine article and as a \"go-getting inventor\" by Mary Hunt, Verschoor had previous experience in automobile manufacturing as well. The company was initially financed with $10,000 raised by Ann Arbor mayor William E. Brown, Jr., and a group of local business leaders who desired to create a new local company with substantial potential for growth and job creation during the Great Depression. It was based out of a former furniture factory located at 405 Fourth Street on Ann Arbor's west side.\n\nIRC debuted its first radio, the International Duo, on August 7, 1931; it was named for its ability to receive both local longwave and European shortwave radio signals. It measured by by , at a time when most table radios measured in length without their separate speaker.\n\nShortly thereafter, IRC introduced the Kadette, the first mass-produced AC/DC radio; it was a four-tube, radio small enough to be easily portable that featured an innovative plastic cabinet. This cabinet material, called Bakelite, was fairly cheap to produce and helped IRC to turn a substantial profit on its radio sales. Manufactured by the Chicago Molded Products Company, the Kadette's plastic cabinet was the first to be used on a radio, although its Gothic styling gave it a fairly traditional appearance. The radio also boasted an innovative new circuit design, while its ability to operate on either alternating (AC) or direct current (DC) allowed it to operate without a power transformer, resulting in it being cheaper, smaller, and lighter than its competitors; it also allowed the Kadette to be plugged into typical household wall sockets. Furthermore, IRC released a kit that instructed customers how to modify their Kadettes for battery-powered mobile applications, such as in railroad cars and automobiles; in the words of Robert E. Mayer, this kit \"effectively started the car radio market\".\n\nThe popularity of the Kadette led to \"almost immediate profitability\" for IRC, and by 1933 it was the only company in Ann Arbor that was still able to pay dividends to its shareholders. During the early 1930s, Ann Arbor was less adversely affected by the Great Depression than Detroit or most other Michigan communities, although lost orders and inability to pay dividends were common occurrences for Ann Arbor-based companies.\n\nFollowing after the Kadette were a variety of other models, many of which were innovative in their own right: the Kadette Jr., the world's first pocket radio; the Kadette Jewel, the original Kadette's successor that was available in five different color combinations; the Kadette Classic, built with three different types of plastic; and the Kadette Clockette, which resembled a small mantel clock and was available in four different wooden case styles. IRC also introduced a number of related accessories, including the Tunemaster, a portable radio remote control, and the Kadette Autime, the first mass-produced clock radio.\n\nIn 1937, as its sales had climbed to $2,700,000, IRC introduced a 10-tube Kadette radio for $19.95, a price comparable with many four- and five-tube sets when its 10-tube competitors cost $100 or more. With three ballast tubes, these 10-tube radios were met with largely negative reviews; in the words of Alan Voorhees, they were \"$20 sets with extra ballast tubes thrown in\". They were also reminiscent of 10-tube radios that Verschoor had built between 1925 and 1930 under the \"Arborphone\" name, which had only five functioning tubes alongside five superfluous ones intended simply to impress prospective customers.\n\nFurthermore, when radio dealers sold IRC's 10-tube Kadettes, they achieved profit margins of 15% at most, far less than what they could earn selling premium models made by competitors. After the company began requiring its dealers to stock its slower selling units in order to also have access to its 10-tube Kadettes, some dealers resorted to giving unauthorized discounts to move the less attractive models, resulting in their total profit margins on the whole Kadette line falling to as low as 5% in some cases. As their profit margins fell, many dealers dropped Kadettes from their catalogs altogether; while IRC made efforts to reverse this trend, in many cases irreparable damage had already been done.\n\nWhile IRC's radio business was initially successful, it was generally seasonal in nature; due to better reception in winter as well as general patterns of behavior before the widespread adoption of air conditioning, sales of radios were much higher during the winter months than during the summer. This prompted Verschoor to explore possibilities for expanding the company's product line in order to reduce the slack periods caused by the seasonal variation in its radio sales. Looking for a product that could be produced relatively cheaply and that would also sell well during the summer months, he decided upon an inexpensive Leica-inspired camera that would ultimately become the Argus, which launched to nearly instant success in 1936. That same year, when IRC had 150 employees, it sold its Kadette AC/DC patents to RCA.\n\nIn 1938, Verschoor departed from IRC after being pressured to leave. By the early 1940s the company was being run by a \"modern management team\".\n\nIn 1939, International Industries sold its radio-manufacturing business to the company's former General Sales Manager, W. Keene Jackson. After renaming it the Kadette Radio Corporation, Jackson expressed his desire to expand its product line by adding television sets, vowing that the new company would \"employ every technical resource to bring the price of efficient television reception to the point where every American home can enjoy this new art as quickly as possible.\" However, Jackson's company suffered from the same problems that IRC had, and just a year after its establishment it was already out of business.\n\nWhile its radio business had faltered, International Industries had found success in the camera and optical equipment fields with its Argus line; by 1942, Argus, Inc.'s sales had climbed to $4,800,000, and during World War II the company employed 1,200 people. After the war, International Industries and its International Research division became wholly owned subsidiaries of Argus, Inc., and shortly thereafter the International name ceased to exist.\n"}
{"id": "21051961", "url": "https://en.wikipedia.org/wiki?curid=21051961", "title": "John Resig", "text": "John Resig\n\nJohn Resig is an American software engineer and entrepreneur, best known as the creator and lead developer of the jQuery JavaScript library.\n\nResig graduated with an undergraduate degree in Computer Science from Rochester Institute of Technology in 2005. During this time he worked with Ankur Teredesai on data mining instant messaging networks and Jon Schull on exploring new ways of encouraging real-time online collaboration.\n\nHe currently works as an application developer at Khan Academy. Previously, he was a JavaScript tool developer for the Mozilla Corporation. For his work on jQuery he was inducted into the Rochester Institute of Technology's Innovation Hall of Fame on April 30, 2010.\n\nResig has started or contributed to many JavaScript libraries, including:\n\nResig is a frequent guest speaker at companies like Google and Yahoo! and has presented at many conferences related to web technology, including SXSW, Webstock, MIX, and Tech4Africa.\n\nResig is the author of a widely read blog, and is the author of the book \"Pro JavaScript Techniques\", published by Apress in 2006, \"Secrets of the JavaScript Ninja\" with Bear Bibeault, published by Manning Publications in December 2012 and several other papers.\n\n"}
{"id": "14832515", "url": "https://en.wikipedia.org/wiki?curid=14832515", "title": "Joint Interface Control Officer", "text": "Joint Interface Control Officer\n\nThe Joint Interface Control Officer (JICO) is the senior multi-tactical data link interface control officer in support of joint task force operations. The JICO is responsible for effecting planning and management of the joint tactical data link network within a theater of operations.\n"}
{"id": "34659850", "url": "https://en.wikipedia.org/wiki?curid=34659850", "title": "List of soft contact lens materials", "text": "List of soft contact lens materials\n\nSoft contact lenses are one of several types on the U.S. Market approved by the U.S. Food and Drug Administration for \"corrective vision eyewear\" as prescribed by optometrists and ophthamologists. The American Optometric Association published a contact lens comparison chart called \"Advantages and Disadvantages of Various Types of Contact Lenses\" on the differences between them. These include: \n\nThe U.S. Food and Drug Administration (FDA) states that:\nThe U.S. Food and Drug Administration (FDA) classifies soft contact lenses into four groups for the U.S. Market. They are also subcategorized into 1st generation, 2nd generation, and 3rd generation lens materials. These 'water-loving' soft contact lens materials are categorized as\n\"Conventional Hydrophilic Material Groups (\"-filcon\"):\n\nNote: Being ionic in pH = 6.0 - 8.0\". This chart was published in the FDA Executive Summary Prepared for the May 13, 2014 Meeting of the Ophthalmic Devices Panel of the Medical Devices Advisory Committee.\nThe FDA has been considering updating soft contact lens group types and related guidance literature.\n\nThe materials that are classified in the 5 FDA groups include the ones listed in the next 5 sections:\n\nThe first contact lenses were made of a polymer called polymethylmethacrylate (PMMA) and became available in the 1960s. Lenses made of PMMA are called hard lenses. Soft contact lenses made of polyacrylamide were introduced in 1971.\n\nBelow is a list of most contact lens materials on the market, their water percentage, their oxygen permeability rating, and manufacturer brands. Note that the higher the oxygen permeability rating, the more oxygen gets to the eye.\n\nThere are three generations of silicone hydrogel contact lens materials:\n"}
{"id": "9523201", "url": "https://en.wikipedia.org/wiki?curid=9523201", "title": "List of telecommunications regulatory bodies", "text": "List of telecommunications regulatory bodies\n\nThis article is a list of the legal regulatory bodies that govern telecommunications systems in different countries. This list was prepared from sources referenced. The websites related to these sources have been variously updated. A check was carried out in January 2014 on the implementation of these effective online sites.\nThis list contains bodies ensuring effective regulatory role in a territory which is not necessarily a state, but is listed as \"territory\" or \"economy\" in the statistics of international institutions, in particular the International Telecommunication Union (ITU).\n\n\n"}
{"id": "5346005", "url": "https://en.wikipedia.org/wiki?curid=5346005", "title": "Little Cleo", "text": "Little Cleo\n\nThe Little Cleo is a small spoon lure made by the Acme Tackle Company which comes in nine sizes from ⁄ oz to 1 ⁄ oz, and in many different color combinations. Its wiggling action creates the illusion of an injured baitfish acting erratically, and often instigates reactionary strikes in predatory fish. The Little Cleo, which has been widely available for over a quarter of a century, is one of the most popular lures in use today and according to \"Field & Stream\" is one of the 50 greatest lures of all time.\n\n"}
{"id": "39606792", "url": "https://en.wikipedia.org/wiki?curid=39606792", "title": "Little Miss Geek", "text": "Little Miss Geek\n\nLittle Miss Geek is a campaign that aims to inspire young women to consider careers in the technology and video-games industries. Little Miss Geek is the non-for-profit subsidiary of Lady Geek, a campaigning agency which aims to make technology more accessible and appealing to women.\n\nThe campaign was launched on October 3, 2012 at the Apple Store on Regent Street by Belinda Parmar and her team at Lady Geek.\n\nThe Little Miss Geek campaign has received coverage in BBC, WIRED, Metro, The Guardian, The Independent and Computer Weekly.\n\nFor 2013 Ada Lovelace Day, Little Miss Geek created a campaign to put the 'Her In Hero'. Saying that 'Brilliant successful women in technology exist, but they are not celebrated in the same way their male counterparts are', the campaign urged schools and MPs to celebrate great female technologists, scientists and inventors to inspire girls with brilliant role models.\n\nThe campaign took place at over 15 schools across the UK, reaching out to over 10,000 students and gaining the support of over 40 MPs including Ms Jo Swinson MP, Hon Ed Vaizey MP, and Rt Hon Theresa May MP. The campaign received media coverage across Metro and the Guardian.\n\nAn event was held on the morning of Ada Lovelace Day at Highgate Wood School, featuring a talk from Siobhan Reddy of Media Molecule on her experiences with the games industry and how she got into it.\n\nOn 8 March 2013 (International Women's Day) Little Miss Geek ran the ‘Little Miss Geek ICT School Takeover’ at two schools in London: Queen Elizabeth's School for Girls and St Saviour's and St Olave's Church of England School. MP Simon Hughes attended the session and Boris Johnson who commented on the day. The events made use of Raspberry Pi computers in order to give girls actual software development experience, and address the gender imbalance in the technology industry.\n\nAccording to Parmar, the Takeover events were intended to address an image problem: \"girls think that people who work in technology are pizza-loving nerds who can't get girlfriends. The reality is technology is one of the most creative industries out there\". \"[Girls are] dreaming of using the iPad mini and the latest smart-phone, but they’re not dreaming of creating it,”\n\nParmar is a critic of British technology education. She states: \"The education system should be the place where we convert childhood experiences with technology into an understanding about computing, where we lay the groundwork for a child to push on into adulthood with not only an interest in tech, but also the skills to start competing in the industry. As it is, however, we are failing our youngsters.”.\n\nOn 25 April 2013 Little Miss Geek held a Wearable Tech Event in celebration of International Girls in ICT Day at St Saviour's and St Olave's Church of England School. Justin Tomlinson MP spoke to the girls about why the British Economy needs more women working in technology. The event featured contributions from fashion designer Francesca Rosella, wearable-technology specialists Cute Circuit, Microsoft and speakers from the British Fashion Council and was designed to shatter the \"myth that technology is a boys-club\".\n\nBritish Vogue described the event as a \"Collision of Fashion and Technology\", however Metro questioned whether fashion can be used to introduce girls to technology.\n\nWired Magazine, August 2013 (Print).\n\nThe Independent: iStyle.\n\nMarie Claire April 2013.\n\nGrazie April 2013.\n\nVogue April 2013.\n\nComputer Weekly April 2013.\n\nThe Independent March 2013.\n\nSky News March 2013.\n\nNew York Times March 2013.\n\nForbes March 2013.\n\nBBC News March 2013.\n\nYahoo News March 2013.\n"}
{"id": "22902883", "url": "https://en.wikipedia.org/wiki?curid=22902883", "title": "Live, virtual, and constructive", "text": "Live, virtual, and constructive\n\nLive, Virtual, and Constructive (LVC) Simulation is a broadly used taxonomy for classifying Models and Simulation (M&S). However, categorizing a simulation as a live, virtual, or constructive environment is problematic since there is no clear division between these categories. The degree of human participation in a simulation is infinitely variable, as is the degree of equipment realism. The categorization of simulations also lacks a category for simulated people working real equipment.\n\nThe LVC categories as defined by the United States Department of Defense in the Modeling and Simulation Glossary as follows:\n\n\nOther associated terms are as follows:\nLVC Enterprise - The overall enterprise of resources in which LVC activities take place.\nLVC Integration - The process of linking LVC simulations through a suitable technology or protocol to exploit simulation interoperability within a federated simulation environment such as the high level architecture (simulation) or Eurosim.\nLVC Integrating Architecture (LVC-IA) - The aggregate representation of the foundational elements of a LVC Enterprise, including hardware, software, networks, databases and user interfaces, policies, agreements, certifications/accreditations and business rules. LVC-IA is intrinsically an Enterprise Architecture, given the system-of-systems environment it must support.\n\nLVC-IA bridges M&S technology to the people who need and use the information gained through simulation. To accomplish this a LVC-IA provides the following:\n\nOther definitions used in LVC discussions (Webster's dictionary)\n\n\nCurrent and emerging technology to enable true LVC technology for Combat Air Forces (CAF) training require standardized definitions of CAF LVC events to be debated and developed. The dictionary terms used above provide a solid foundation of understanding of the fundamental structure of the LVC topic as applied universally to DoD activities. The terms and use cases described below are a guidepost for doctrine that uses these terms to eliminate any misunderstanding. The following paragraph uses these terms to layout the global view, and will be explained in detail throughout the rest of the document. In short:\n\nCentral to a functionally accurate understanding of the paragraph above is a working knowledge of the Environment definitions, provided below for clarity:\n\n\n\n\nThe Environments (L, V, & C) by themselves are generally well understood and apply universally to a diverse range of disciplines such as the medical field, law enforcement or operational military applications. Using the medical field as an example, the Live Environment can be a doctor performing CPR on a human patient in a critical real world situation. In this same context, the Virtual Environment would include a doctor practicing CPR on a training mannequin, and the Constructive Environment is the software within the training mannequin that drives its behavior. In a second example, consider fighter pilot training or operational testing. The Live environment is the pilot flying the combat aircraft. The Virtual environment would include that same pilot flying a simulator. The constructive environment includes the networks, computer generated forces, and weapons servers, etc. that enable the Live and Virtual environments to be connected and interact. Although there are clearly secondary and tertiary training benefits, it is important to understand combining one or more environments for the purpose of making Live real world performance better is the sole reason the LVC concept was created. \nHowever, when referring to specific activities or programs designed to integrate the environments across the enterprise, the use and application of terms differ widely across the DoD. Therefore, the words that describe specifically how future training or operational testing will be accomplished require standardization as well. This is best described by backing away from technical terminology and thinking about how human beings actually prepare for their specific combat responsibilities. In practice, human beings prepare for their roles in one of three Constructs: Live (with actual combat tools), in a Simulator of some kind, or in other Ancillary ways (tests, academics, computer-based training, etc.). Actions within each of the Constructs are further broken down into Components that specify differing ways to get the job done or achieve training objectives. The three Constructs are described below:\n\nLive is one of three constructs representing humans operating their respective disciplines’ operational system. Operational system examples could consist of a tank, a naval vessel, an aircraft or eventually even a deployed surgical hospital. Three components of the Live Construct follow\n\n\nA second construct representing humans operating simulator devices in lieu of Live operational systems. The Simulator Construct (the combination of Virtual and Constructive (VC)) is made up of three components that consist of \n\nIs the third construct other than Live or Simulator whereby training is accomplished via many components (not all-inclusive)\n\nUtilizing the definitions above, the following table provides a graphical representation of how the terms relate in the context of CAF Training or Operational Test:\n\nUsing the figure above as a guide, it is clear LVC activity is the use of the Virtual and Constructive environments to enhance scenario complexity for the Live environment – and nothing more. An LVC system must have a bi-directional, adaptable, ad-hoc and secure communication system between the Live environment and the VC environment. Most importantly, LVC used as a verb is an integrated interaction of the three environments with the Live environment always present. For example, a Simulator Construct VC event should be called something other than LVC (such as Distributed Mission Operations (DMO)). In the absence of the Live environment LVC and LC do not exist, making the use of the LVC term wholly inappropriate as a descriptor.\n\nAs the LVC Enterprise pertains to a training program, LVC lines of effort are rightly defined as “a collaboration of OSD, HAF, MAJCOM, Joint and Coalition efforts toward a technologically sound and fiscally responsible path for training to enable combat readiness.” The “lines of effort,” in this case, would not include Simulator Construct programs and development but would be limited to the Construct that includes the LVC Enterprise. The other common term, “Doing LVC” would then imply “readiness training conducted utilizing an integration of Virtual and Constructive assets for augmenting Live operational system scenarios and mission objective outcomes.” Likewise, LVC-Operational Training (in a CAF fighter training context) or “LVC-OT” are the tools and effort required to integrate Live, Virtual and Constructive mission systems, when needed, to tailor robust and cost-efficient methods of Operational Training and/or Test.\n\nTo ensure clarity of discussions and eliminate misunderstanding, when speaking in the LVC context, only the terms in this document should be used to describe the environments, constructs, and components. Words like “synthetic” and “digi” should be replaced with “Constructive” or “Virtual” instead. Additionally, Embedded Training (ET) systems, defined as a localized or self-contained Live/Constructive system (like on the F-22 or F-35) should not be confused with or referred to as LVC systems.\n\nPrior to 1990, the field of M&S was marked by fragmentation and limited coordination between activities across key communities. In recognition of these deficiencies, Congress directed the Department of Defense (DoD) to “... establish an Office of the Secretary of Defense (OSD) level joint program office for simulation to coordinate simulation policy, to establish interoperability standards and protocols, to promote simulation within the military departments, and to establish guidelines and objectives for coordination [sic] of simulation, wargaming, and training.” (ref Senate Authorization Committee Report, FY91, DoD Appropriations Bill, SR101-521, pp. 154–155, October 11, 1990) Consistent with this direction, the Defense Modeling and Simulation Office (DMSO) was created, and shortly afterwards many DoD Components designated organizations and/or points of contact to facilitate coordination of M&S activities within and across their communities. \nFor over a decade, the ultimate goal of the DoD in M&S is to create a LVC-IA to assemble models and simulations quickly, which create an operationally valid LVC environment to train, develop doctrine and tactics, formulate operational plans and assess warfighting situations. A common use of these LVC environments will promote closer interaction between operations and acquisition communities. These M&S environments will be constructed from composeable components interoperating through an integrated architecture. A robust M&S capability enables the DOD to meet operational and support objectives effectively across the diverse activities of the military services, combatant commands and agencies.\n\nThe number of available architectures have increased over time. M&S trends indicate that once a community of use develops around an architecture, that architecture is likely to be used regardless of new architectural developments. M&S trends also indicate that few, if any, architectures will be retired as new ones come online. When a new architecture is created to replace one or more of the existing set, the likely outcome is one more architecture will be added to the available set. As the number of mixed-architecture events increase over time, the inter-architecture communication problem increases as well.\n\nM&S has made significant progress in enabling users to link critical resources through distributed architectures.\n\nIn the mid 1980s, SIMNET became the first successful implementation of a large-scale, real-time, man-in-the-loop simulator networking for team training and mission rehearsal in military operations. The earliest successes that came through the SIMNET program was the demonstration that geographically dispersed simulation systems could support distributed training by interacting with each other across network connections.\n\nThe Aggregate Level Simulation Protocol (ALSP) extended the benefits of distributed simulation to the force-level training community so that different aggregate-level simulations could cooperate to provide theater-level experiences for battle-staff training. The ALSP has supported an evolving “confederation of models” since 1992, consisting of a collection of infrastructure software and protocols for both inter-model communication through a common interface and time advance using a conservative Chandy-Misra-based algorithm.\n\nAt about the same time, the SIMNET protocol evolved and matured into the Distributed Interactive Simulation (DIS) Standard. DIS allowed an increased number of simulation types to interact in distributed events, but was primarily focused on the platform-level training community. DIS provided an open network protocol standard for linking real-time platform-level wargaming simulations.\n\nIn the mid 1990s, the Defense Modeling and Simulation Office (DMSO) sponsored the High Level Architecture (HLA) initiative. Designed to support and supplant both DIS and ALSP, investigation efforts were started to prototype an infrastructure capable of supporting these two disparate applications. The intent was to combine the best features of DIS and ALSP into a single architecture that could also support uses in the analysis and acquisition communities while continuing to support training applications.\n\nThe DoD test community started development of alternate architectures based on their perception that HLA yielded unacceptable performance and included reliability limitations. The real-time test range community started development of the Test and Training Enabling Architecture (TENA) to provide low-latency, high-performance service in the hard-real-time application of integrating live assets in the test-range setting. TENA, through its common infrastructure, including the TENA Middleware and other complementary architecture components, such as the TENA Repository, Logical Range Archive, and other TENA utilities and tools, provides the architecture and software implementation and capabilities necessary to quickly and economically enable interoperability\namong range systems, facilities, and simulations.\n\nSimilarly, the U.S. Army started the development of the Common Training Instrumentation Architecture (CTIA) to link a large number of live assets requiring a relatively narrowly bounded set of data for purposes of providing After Action Reviews (AARs) on Army training ranges in the support of large-scale exercises.\n\nOther efforts that make the LVC architecture space more complex include universal interoperability software packages such as OSAMS or CONDOR developed and distributed by commercial vendors.\n\nAs of 2010 all of the DoD architectures remain in service with the exception of SIMNET. Of the remaining architectures: CTIA, DIS, HLA, ALSP and TENA, some are in early and growing use (e.g., CTIA, TENA) while others have seen a user-base reduction (e.g., ALSP). Each of the architectures is providing an acceptable level of capability within the areas where they have been adopted. However, DIS, HLA, TENA, and CTIA-based federations are not inherently interoperable with each other. when simulations rely on different architectures, additional steps must be taken to ensure effective communication between all applications. These additional steps, typically involving interposing gateways or bridges between the various architectures, may introduce increased risk, complexity, cost, level of effort, and preparation time. Additional problems extend beyond the implementation of individual simulation events. As a single example, the ability to reuse supporting models, personnel (expertise), and applications across the different protocols is limited. The limited inherent interoperability between the different protocols introduces a significant and unnecessary barrier to the integration of live, virtual, and constructive simulations.\n\nThe current status of LVC interoperability is fragile and subject to several reoccurring problems that must be resolved (often anew) whenever live, virtual or constructive simulation systems are to be components in a mixed-architecture simulation event. Some of the attendant problems stem from simulation system capability limitations and other system-to-system incompatibilities. Other types of problems arise from the general failure to provide a framework which achieves a more complete semantic-level interoperability between disparate systems. Interoperability, Integration and Composeablity have been identified as the most technical challenging aspects of a LVC-IA since at least 1996. The \"Study on the Effectiveness of Modeling and Simulation in the Weapon System Acquisition Process\" identified cultural and managerial challenges as well. By definition a LVC-IA is a socialtechnical system, a technical system that interacts directly with people. The following table identifies the 1996 challenges associated with the technical, cultural and managerial aspects. In addition, the challenges or gaps found in a 2009 study are also included. The table shows there is little difference between the challenges of 1996 and the challenges of 2009. \nA virtual or constructive model usually focuses on the fidelity or accuracy of the element being represented. A live simulation, by definition represents the highest fidelity, since it is reality. But a simulation quickly becomes more difficult when it is created from various live, virtual and constructive elements, or sets of simulations with various network protocols, where each simulation consists of a set of live, virtual and constructive elements. The LVC simulations are socialtechical systems due to the interaction between people and technology in the simulation. The users represent stakeholders from across the acquisition, analysis, testing, training, planning and experimentation communities. M&S occurs across the entire Joint Capabilities Integration Development System (JCID) lifecycle. See the \"M&S in the JCID Process\" figure. A LVC-IA is also considered an Ultra Large Scale (ULS) system due to the use by a wide variety of stakeholders with conflicting needs and the continuously evolving construction from heterogeneous parts. By definition, people are not just users but elements of a LVC simulation.\n\nDuring the development of various LVC-IA environments, attempts to understand the foundational elements of integration, composability and interoperability emerged. As of 2010, our understanding of these three elements are still evolving, just as software development continues to evolve. Consider software architecture; as a concept it was first identified in the research work of Edsger Dijkstra in 1968 and David Parnas in the early 1970s. The area of software architecture was only recently adopted in 2007 by ISO as ISO/IEC 42010:2007. Integration is routinely described using the methods of architectural and software patterns. The functional elements of integration can be understood due to universality of integration patterns, e.g. Mediation (intra-communication) and Federation (inter-communication); process, data synchronization and concurrency patterns.\n\nA LVC-IA is dependent on the Interoperability and Composability attributes, not just the technical aspects, but the social or cultural aspects as well. There are sociotechnical challenges, as well as ULS system challenges associated with these features. An example of a cultural aspect is the problem of composition validity. In an ULS the ability to control all interfaces to ensure a valid composition is extremely difficult. The VV&A paradigms are challenged to identify a level of acceptable validity.\n\nThe study of interoperability concerns methodologies to interoperate different systems distributed over a network system. Andreas Tolk introduced the Levels of Conceptual Interoperability Model (LCIM) which identified seven levels of interoperability among participating systems as a method to describe technical interoperability and the complexity of interoperations. Zeigler's Architecture for M&S extends on the three basic levels of interoperability; pragmatic, semantic, and syntactic. The pragmatic level focuses on the receiver’s interpretation of messages in the context of application relative to the sender’s intent. The semantic level concerns definitions and attributes of terms and how they are combined to provide shared meaning to messages. The syntactic level focuses on a structure of messages and adherence to the rules governing that structure. The linguistic interoperability concept supports simultaneous testing environment at multiple levels. The LCIM associate the lower layers with the problems of simulation interoperation while the upper layers relate to the problems of reuse and composition of models. They conclude “simulation systems are based on models and their assumptions and constraints. If two simulation systems are combined, these assumptions and constraints must be aligned accordingly to ensure meaningful results”. This suggests that levels of interoperability that have been identified in the area of M&S can serve as guidelines to discussion of information exchange in general. The Zeigler Architecture provides an architecture description language or conceptual model in which to discuss M&S. The LCIM provides a conceptual model as a means to discuss integration, interoperability and composability. The three linguistic elements relates the LCIM to the Ziegler conceptual model. Architectural and structural complexity an area of research in systems theory to measure the cohesion and coupling and is based on the metrics commonly used in software development projects. Zeigler, Kim, and Praehofer presents a theory of modeling and simulation which provides a conceptual framework and an associated computational approach to methodological problems in M&S. The framework provides a set of entities and relations among the entities that, in effect, present a ontology of the M&S domain.\n\nPetty and Weisel formulated the current working definition: \"Composability is the capability to select and assemble simulation components in various combinations into simulation systems to satisfy specific user requirements.\" Both a technical and user interaction is required indicative of a sociotechnical system is involved. The ability for a user to access data or access models is an important factor when considering composability metrics. If the user does not have visibility into a repository of models, the aggregation of models becomes problematic.\n\nIn \"Improving the Composability of Department of Defense Models and Simulation\", the factors associated with the ability to provide composability are as follows:\n\nTolk introduced an alternative view on Composability, focusing more on the need for conceptual alignment:\n\nIn other words: Propertied concepts, if they are modeled in more than one participating system, have to represent the same truth. It is not allowed for composable systems to gain different answer to the same question in both systems. The requirement for consistent representation of truth supersedes the requirement for meaningful use of received information known from interoperability.\n\nPage et al. suggest defining integratability contending with the physical/technical realms of connections between systems, which include hardware and firmware, protocols, networks, etc., interoperability contending with the software and implementation details of interoperations; this includes exchange of data elements via interfaces, the use of middleware, mapping to common information exchange models, etc., and composability contending with the alignment of issues on the modeling level. As captured, among others, by Tolk, successful interoperation of solutions of LVC components requires \"integratability of infrastructures, interoperability of systems, and composability of models\". LVC Architectures must holistically address all three aspects in well aligned systemic approaches.\n\nTo produce the greatest impact from its investments, the DoD needs to manage its M&S programs utilizing an enterprise-type approach. This includes both identifying gaps in M&S capabilities that are common across the enterprise and providing seed moneys to fund projects that have widely applicable payoffs, and conducting M&S investment across the Department in ways that are systematic and transparent. In particular, “Management processes for models, simulations, and data that … Facilitate the cost effective and efficient development of M&S systems and capabilities….” such as are cited in the vision statement require comprehensive Departmental M&S best-practice investment strategies and processes. M&S investment management requires metrics, both for quantifying the extent of potential investments and for identifying and understanding the full range of benefits resulting from these investments. There is at this time no consistent guidance for such practice.\nThe development & use costs associated with LVC can be summarized as follows:\n\n\nIn contrast, the fidelity of M&S is highest in Live, lower in Virtual, and lowest in Constructive. As such, DoD policy is a mixed use of LVC through the Military Acquisition life cycle, also known as the LVC Continuum. In the LVC Continuum figure to the right, the JCIDS process is related to the relative use of LVC through the Military Acquisition life cycle.\n\n"}
{"id": "12114637", "url": "https://en.wikipedia.org/wiki?curid=12114637", "title": "Maclaren", "text": "Maclaren\n\nMaclaren is a manufacturer of baby buggies, strollers and carriers based in England.\n\nStrollers based around Owen Maclaren's original design are sold in over 50 countries under the Maclaren brand. These include the Maclaren Volo, Globetrotter, Triumph, Quest, Techno XT, and Techno XLR.\n\nIn September 2000, the company went into receivership with large debts and was subsequently acquired by a family based in Monaco and Switzerland. The factory in Long Buckby closed in October 2000, and production was moved to Shenzhen, China.\n\nOn 29 December 2011, the U.S. unit of Maclaren filed for Chapter 7 Liquidation, but the company is considered one of the biggest players in this industry.\n\nIn November 2009, Maclaren voluntarily recalled its entire line of buggies sold in the U.S. and produced from 1999-2009, citing 12 reported fingertip amputations in its hinges. The company will provide free hinge covers for all consumers and advises against using the buggies until the hinge covers are installed.\n"}
{"id": "1437928", "url": "https://en.wikipedia.org/wiki?curid=1437928", "title": "Maul Camera Rocket", "text": "Maul Camera Rocket\n\nThe Maul Camera Rocket was a rocket for aerial photography developed by Alfred Maul's company from 1903 to 1912. The Maul Camera Rocket was demonstrated in 1912 to the Austrian Army and tested as a means for reconnaissance in the Turkish-Bulgarian War in 1912/1913. It was not used afterwards, because aircraft were much more effective.\n\nThe Maul Camera Rocket had a maximum flight altitude of 1 kilometre (3,300 ft), a launch mass of 42 kg (93 pounds), a diameter of 0.32 metre (12½ inches), a length of 6 metres (19 ft 8 in) and a fin span of 0.35 metres (1 ft 2 in).\n\n"}
{"id": "27509071", "url": "https://en.wikipedia.org/wiki?curid=27509071", "title": "Metadata Working Group", "text": "Metadata Working Group\n\nThe Metadata Working Group was formed in 2006 by Adobe Systems, Apple, Canon, Microsoft and Nokia. Sony joined later in 2008.\n\nThe focus of the group is to advance the interoperability of metadata stored in digital media. Its specification, Guidelines for Handling Image Metadata, defined the interoperability among Exif, IIM (old IPTC), and XMP with consumer digital images. The following properties were selected for interoperability:\n\nTest files for verification were added in 2008 and area available for download.\n"}
{"id": "58907894", "url": "https://en.wikipedia.org/wiki?curid=58907894", "title": "Micree Zhan", "text": "Micree Zhan\n\nMicree Zhan is a Chinese billionaire businessman, and the co-founder and co-CEO (with Jihan Wu) of bitcoin mining company Bitmain.\n\nZhan earned a bachelor's degree in electrical engineering from Shandong University in 2001, and a master's in engineering from the Chinese Academy of Sciences' Institute of Microelectronics in 2004.\n\nZhan's first job after university was at Tsinghua University as a research and development engineer in the Research Institute of Information Technology. In 2006, In 2006, he joined Unitend Technologies, an in 2010, founded DivaIP Technologies, developing TV set-top boxes.\n\nZhan owns 36.58% of Bitmain, and Wu 20.5%, and in August 2018 his estimated net worth was US$4 billion.\n"}
{"id": "35696706", "url": "https://en.wikipedia.org/wiki?curid=35696706", "title": "National Iranian Drilling Company", "text": "National Iranian Drilling Company\n\nNational Iranian Drilling Company (NIDC) is an Iranian company. The company was founded in 1979 and is based in Tehran, Iran. NIDC operates as a subsidiary of National Iranian Oil Company. NIDC engages in the exploration, development, and delineation drilling of oil and gas wells. It owns and operates drilling rigs and equipment for providing drilling services. \n\nThe company’s services include well logging, cementing and acidizing, drill stem test, well testing, training and development, and general services. It also provides engineering, programming, and industrial cleaning services to steam boilers, compressors suction and lub oil systems, water gackets, chillers, and heat exchangers, as well as industrial cleaning services to the oil and gas refineries. NIDC is in charge of all offshore and onshore drilling activities. NIDC provides more than 90 percent of drilling services needed by the oil companies inside the country. In 2011, NIDC, drilled or completed 192 oil and gas wells, drilled 454 thousand meters of wells and provided more than 8 thousand expert or technical services to customers. \n\n\n"}
{"id": "45443161", "url": "https://en.wikipedia.org/wiki?curid=45443161", "title": "Nintendo 3DS family", "text": "Nintendo 3DS family\n\nThe Nintendo 3DS family is a family of handheld game consoles developed and sold by Nintendo since 2011. It succeeded the Nintendo DS family.\n\nThroughout its lifetime, Sony's PlayStation Vita has been the main market competitor to the Nintendo 3DS family. There have been six models in the 3DS family: the original Nintendo 3DS and its XL variant, the Nintendo 2DS, and the New Nintendo 3DS and its XL variant, as well as the New Nintendo 2DS XL. Similar to the Nintendo DS family, which has been highly successful, the Nintendo 3DS family has also been successful, with nearly 69 million units shipped as of late September 2017.\n\nThe Nintendo 3DS (abbreviated to 3DS) is a portable game console produced by Nintendo. It is capable of projecting stereoscopic 3D effects without the use of 3D glasses or additional accessories. Nintendo announced the device in March 2010 and officially unveiled it at E3 2010 on June 15, 2010. The console succeeds the Nintendo DS, featuring backward compatibility with older Nintendo DS games and DSi only games, and competes with the Sony PlayStation Vita handheld console.\nThe handheld offers new features such as the StreetPass and SpotPass tag modes, powered by Nintendo Network; augmented reality, using its 3D cameras; and Virtual Console, which allows owners to download and play games originally released on older video game systems. It is also pre-loaded with various applications including: an online distribution store called Nintendo eShop; a social networking service called Miiverse; an Internet Browser; the Netflix, Hulu Plus and YouTube streaming video services; Nintendo Video; a messaging application called Swapnote (known as \"Nintendo Letter Box\" in Europe and Australia); and Mii Maker.\nThe Nintendo 3DS was first released in Japan on February 26, 2011, and worldwide beginning in March 2011. Less than six months later on July 28, 2011, Nintendo announced a significant price reduction from US$249 to US$169 amid disappointing launch sales. The company offered ten free Nintendo Entertainment System games and ten free Game Boy Advance games from the Nintendo eShop to consumers who bought the system at the original launch price. This strategy was considered a major success, and the console has gone on to become one of Nintendo's most successfully sold handheld consoles in the first two years of its release. As of September 30, 2017, all Nintendo 3DS models and 2DS models combined have sold 68.98 million units.\n\nThe Nintendo 3DS XL was unveiled on June 21, 2012, as a variation of the standard 3DS with and displays, and a larger battery. It was released July 28, 2012 in Japan and Europe, August 19, 2012 in North America, and August 23, 2012 in Australia and New Zealand.\n\nThe Nintendo 2DS was unveiled on August 28, 2013, for release on October 12, 2013 in North America, Europe, Australia and New Zealand. It was later released in Japan on February 27, 2016. The 2DS was designed as an entry-level model targeting children; it features a flat, non-folding form factor, and does not include the 3DS's signature autostereoscopic display (the 2DS uses a single display panel with an overlay mimicking the 3DS screen dimensions).\n\nThe New Nintendo 3DS is a hardware revision of the original 3DS models; they feature a faster processor, a refreshed hardware design, additional shoulder buttons and a pointing stick, and integrated Amiibo support. The standard-sized New 3DS also features a larger display than the original 3DS model. They were released in Japan in October 2014, in Australia and New Zealand in November 2014, and at retail in Europe and North America in February 2015 with only the XL model available in the North American market at launch. The non-XL model was later discontinued worldwide in 2017 with the XL model being discontinued in Europe at the end of 2017.\n\nOn April 27, 2017, Nintendo unveiled the New Nintendo 2DS XL. It is a variation of the New 3DS XL with a further streamlined design, and no autostereoscopic 3D display. It was first made available in Australia and New Zealand on June 15, 2017. \n\nThe Circle Pad Pro can attach to the Nintendo 3DS, and adds a second circle pad and ZR/ZL digital triggers. A model for the Nintendo 3DS XL, the Circle Pad Pro XL, is also available.\n\nThis accessory came bundled exclusively with every retail copy of \"\". The stand made the game, and other games with similar controls such as \"Liberation Maiden\", easier to play for various users, as it helped free the tension of suspending the console with one hand since the other hand would be using the stylus on the touch screen for longer periods than usual.\n\nAn NFC platform reader for Nintendo 3DS, Nintendo 3DS XL, and Nintendo 2DS was released on September 25, 2015 alongside \"\". This peripheral allows Amiibo and other NFC-based items to be supported on the aforementioned consoles. The New Nintendo 3DS comes with an NFC reader already built-in.\n\n\n"}
{"id": "30808282", "url": "https://en.wikipedia.org/wiki?curid=30808282", "title": "NorthStar Alarm", "text": "NorthStar Alarm\n\nNorthStar Alarm, also known as NorthStar Home or NorthStar Alarm Services, LLC, is a Utah-based home security provider that was founded in 2000. NorthStar is an authorized Honeywell Security Products Dealer, headquartered in Orem, Utah, and services homeowners and residents in 20 states throughout the U.S.\n\nNorthStar Home was formed in 2000, originally to take advantage of unique marketing opportunities created with the deregulation of public utilities brought about by the Energy Policy Act of 1992. Its original focus was in the telecommunication and energy markets. Because of the strong sales background of the company and its management, other service companies expressed interest in working with NorthStar to increase their customer base.\n\nThe company began operating sales offices in Ohio. Since then, the company has established a customer base in Alabama, Arkansas, Arizona, California, Colorado, Idaho, Indiana, Michigan, North Carolina, Nevada, New Mexico, Oklahoma, Oregon, Pennsylvania, South Carolina, Tennessee, Texas, Utah and Washington. NorthStar partners with UL listed Rapid Response Monitoring to provide advanced digital and two-way voice monitoring available.\n\nIn January 2015, it was announced that NorthStar would acquire 8000 accounts from Vision Security, effectively expanding the company's reach into Maryland, Virginia, Pennsylvania and Minnesota.\n\nNorthStar is a member of the National Fire Protection Association or NFPA, the Electronic Security Association or ESA, and is an authorized Honeywell Security Products Dealer.\n\nNorthStar became a member of the Utah Valley Chamber of Commerce in 2011, also known as the Provo-Orem Chamber of Commerce.\n\nIn 2010, NorthStar was ranked No. 55 on the Security Distributing and Marketing (SDM) 100 Report and in 2011 they were ranked No. 47. According to the 2012 SDM 100 Report, Northstar Alarm moved up to No. 40 on the Top 100 report for U.S. security firms, based on recurring monthly revenue. NorthStar continued to climb the ranks on the SDM Top 100 List and is now No.24 in the nation for security and No. 14 in the residential segment (2016).\n\nNorthStar entered into a $40 million financial partnership with Goldman Sachs and The Beekman Group in 2013.\n\nIn 2013, NorthStar was named as one of the \"Top Companies to Work for in Utah Valley.\"\n\nNorthStar, a Utah-based company noted as the 18th largest residential security and automation company in nation by SDM Magazine, was named a runner up for the “Best Companies to Work For” award by Utah Business Magazine. This is NorthStar’s first year in the running for the award.\n\nIn 2014, the MountainWest Capital Network unveiled its 2014 Utah 100 Awards Program recipients, including NorthStar Alarm at number 68, the company’s first appearance on the list. The Utah 100 is in its 20th year of operation and continues to recognize the fastest growing companies in Utah, based on successful revenue growth in the past year.\n\nNorthStar Alarm and its equity investors, The Beekman Group and The Goldman Sachs Group, Inc., are excited to announce the acquisition of certain assets from Vision Security, a residential alarm company listed at No. 47 overall on the SDM Top 100 List in 2014. This acquisition of assets expands NorthStar‘s geographical reach and provides additional resources for accelerated growth over the next five years. NorthStar's organic growth, along with the addition of Vision assets, has more than doubled the company's size since its equity recapitalization just 20 months ago.\n\nIn April 2016, NorthStar was recognized as the fastest-growing security firm in the nation by Security Dealer & Integrator Magazine.\n\nIn May 2016, NorthStar's corporate office moved into the new Orem University Place where they occupy approximately 30,000 square feet.\n\nIn December 2010, NorthStar worked with the non-profit organization Soles4Souls, Inc. to host a community shoe drive. Corporate employees have also participated in community 5k runs, triathlons and marathons to support local and national causes.\n\nIn May 2011, NorthStar employees participated in the Community Action 5K in Provo, UT to help raise money for the local food bank.\n\nIn 2011, the company also made charitable contributions to the non-profit organization, Miracle Flights for Kids, providing flights for three children and their parents to receive the urgent medical attention needed throughout the country. One of the families sent the company a special video thanking the owner of NorthStar for his generous donation.\n\nFor NorthStar's annual holiday service project in 2011, the company's employees provided gifts for children of local families through the United Way of Utah County's Sub for Santa program.\n\nOn April 10, 2012, NorthStar joined the international cause for the annual TOMS \"One Day without Shoes\" campaign to help raise awareness for children who grow up without shoes every day and are prone to disease and infections. NorthStar employees went barefoot for the day and donated TOMS gift cards to rally customer and community support for the cause via Facebook and Twitter. On April 16, 2013, NorthStar once again participated in the annual TOMS “One Day without Shoes” campaign to remember the cause from the previous year.\n\nOn May 18, 2013, NorthStar participated in the annual Run to Walk 5k for the Now I Can Foundation; a not-for-profit that assists children with physical disabilities to achieve their greatest physical potential.\n\n"}
{"id": "36864657", "url": "https://en.wikipedia.org/wiki?curid=36864657", "title": "Oil-based mud", "text": "Oil-based mud\n\nOil-based mud is a drilling fluid used in drilling engineering. It is composed of oil as the continuous phase and water as the dispersed phase in conjunction with emulsifiers, wetting agents and gellants. The oil base can be diesel, kerosene, fuel oil, selected crude oil or mineral oil.\n\nThe requirements are a gravity of 36–37 API, a flash point of , fire point of and an aniline point of .\n\nEmulsifiers are important to oil-based mud due to the likelihood of contamination. The water phase of oil-based mud can be freshwater, or a solution of sodium or calcium chloride. The external phase is oil and does not allow the water to contact the formation. The shales don't become water wet.\n\nPoor stability of the emulsion results in the two layers separating into two distinct layers. \nThe advantages are: \n\nOil-based muds are expensive, but are worth the cost when drilling through: \n\nThe disadvantages of using oil-based mud, especially in wildcat wells are:\n\nThis mud type can be used as a completion and workover fluid, a spotting fluid to relieve a stuck pipe and as packer or casing fluid. They are very good for \"Gumbo\" shales. The mud weight can be controlled from 7–22 lbs/gal. It is sensitive to temperature but does not dehydrate as in the case of water based mud as mentioned before. It has no limit on the drilled solids concentration. The water phase should be maintained above a pH of 7. Stability of the emulsion depends on the alkaline value.\n\n\n"}
{"id": "32271197", "url": "https://en.wikipedia.org/wiki?curid=32271197", "title": "Open sustainability innovation", "text": "Open sustainability innovation\n\nOpen sustainability innovation is the use of open innovation in the development of sustainable products, services and initiatives. This is an approach to marketing for companies may prove to be advantageous as it is not point of sale based, but rather offers consumers information they have previously never been exposed to. Creating a basis for more long term conversational relationships. As a result of this conversational relationship between companies and consumers ideas about the importance of sustainability and how people relate to this through consumption can arise. By offering an open communication way of marketing to consumers, companies may ultimately gain a competitive advantage based on trust and disclosure. Thus not only will open sustainability innovations promote the use of sustainable products and services, it will actually create a snowballing effect to other companies who will have to adopt new sustainability practices in order to remain on the market.\n\nThe key to open innovation is the integration of the consumers’ ideas into the process of innovation. This means that the creation of the innovation should come largely from the consumer’s input and ideas. Or the marketing strategy shows the consumer any externalities that the product may incur to the environment or compare it to other products on the market that are less sustainable. In any case, it is important to remember that these initiatives are largely conversational. The company must be willing to give much more information to the consumer than was ever really accepted in conventional marketing and the consumer must play a part by bringing in his own ideas and feelings. This entails that the issue of trust is not uni-lateral, the consumer gains trust from the company because of the interaction, but the company must first trust the consumers in order to spend the money, time and knowledge on their open strategy.\n\nAs the world is becoming a more globalized society and more and more people are adopting the western consumption patterns, there has been a coinciding concern for the impact on our environment incurred by the production, distribution, use and disposal of consumer goods. As a result of this awareness, there are more and more consumers who take sustainability as an important factor in their consumer intent and behavior. Thus it has become beneficial for companies to also not only adopt sustainable practice but also to involve these concerned consumers in that practice. The first companies to adopt sustainable open innovation in their marketing strategies acknowledged that only providing more sustainable products and services, but leaving out a communication about those new practices would not be enough to encourage the rest of the market to be more sustainable. By opening their new practices to the consumer, they put themselves on a platform where others would have to do the same or face failure in the market. \"There were some early developments in key industries during the 1980s and 1990s when the major players in the electronics industry collaborated with the goal of eliminating CFCs as a solvent in electronic assembly processes; and the quest for low emission vehicle technologies created some unprecedented levels of information sharing between the 'Big Three' car manufacturers. However, such collaborations were a long way from truly 'opening up'.\" This example represents the position that any kind of open sustainability innovation incurs on the market. Rather than seeking out a new product or new advertising scheme, companies look to each other as leaders in the industry, and customers, in turn have the opportunity to understand what they are really consuming.\n\nSustainability innovations play a crucial role in the process of developing successful sustainability marketing strategies. Passively heated houses, solar cells, organic food, fair trade products, hybrid cars and car sharing are just some vivid examples of sustainability innovations. Sustainability innovation is an outstanding way for acquisition of both competitive advantage and differentiation. If the aim of sustainability marketing is to transform the society to a more sustainable one, innovations should not be restricted just to technological innovations. At a first step, in many markets sustainability innovations presuppose so called soft innovations in social practices, finance and business relationships. The innovation is not necessarily a new product or a product advancement, but also entails brand new ways to communicate products, new ways to operate focus groups or using internet tools to have idea generating systems in order to learn from the consumers.\nIn the last years, sustainability performance of products and services has experienced continuous improvements. The above-mentioned examples of sustainable products became produced at lower cost, in a more resource efficient way and satisfied the increasing consumer concerns about sustainability issues. For example, especially in the last decade e.g. fuel efficiency improvements in cars, increased use of recycled materials in packaging and growth in fairly traded commodities and organic foods has been witnessed by consumers, non-government organizations, policy makers and companies alike. Nevertheless, some critics exist about these kinds of product improvement, emphasizing that they are only capable of achieving certain level of eco improvement and also in many cases the potential gains already exist or are known to the manufacturers. \n\nIn most companies there is a power struggle between the Quality Management departments and the environmental management departments because though the environmental people want to make changes to products in order for them to be more economically efficient, the Quality people do not want to sacrifice the efficiency and product qualities that are already enjoyed by the consumer. These quality management systems, however have a very big challenge moving into a more sustainable minded consumption world, because they have based their innovations on gradual continuous improvement, and they align their development with the status quo of the industry. This may prove to be a barrier because the changes needed to reach more sustainable products and services is quite radical, hence this struggle as stated before. This kind of deficiency can be sorted out by complementing environmental management systems by other kinds of systems and approaches that favour new discoveries of alternative approaches and nonstop searching for more radical, \"step change\" solutions to sustainability challenges.\n\nStep change solutions contain \"as a series of waves or types of innovation that can deliver increasing levels of eco-efficiency until a sustainable global economy can be established\". Each wave is I shown as an S-curve in this model and these S-curves are similar to the conventional product life cycles depicted in conventional models. It shows that over time the eco-efficiency or environmental benefits of products actually decreases over time. Thus moving along in time, the new curves and their very radical new approaches over ride the ones of the older curves. The model follows four steps of sustainable design innovation and change. Following are the four steps: \n\nWhereas classical marketing is characterized by a uni-directional, sender – receiver relationship, in present times, marketing has changed. Open innovation re-conceptualizes marketing into a bi-directional relationship, based on processes of reciprocal learning. Companies open up their Research and Development departments in order to let new stimuli of information and knowledge flow in, as well as to allow insights to flow out.\n\nIn order to be able to stand the huge competition prevailing in such markets, Porter and van der Linde identified the general importance of green innovations for a company’s competitiveness. \nIn 2010, the Greendex survey confirmed Porter’s and van der Linde’s emphasis on green innovations by identifying an increased environmentally oriented consumption behavior and an articulated desire by consumers to see less talking and more action to tackle environmental problems, from businesses and governments. Greendex is a quantitative consumer study of 17,000 consumers in 17 countries, in companionship of National Geographic and GlobeScan. Probands are interviewed regarding their energy use and conservation, transportation choices, food sources, the relative use of green products versus traditional products, attitudes towards the environment and sustainability, and knowledge of environmental issues. This study aims at a quantitative measurement of consumer behavior and at promoting sustainable consumption. \nWith regard to a company’s marketing strategy on sustainable products and services, McDonagh elaborated four principles: \"Ecological trust\", \"ecological access\", \"ecological disclosure\", and \"Ecological dialogue\".\n\nWith open innovations and sustainable mindsets only recently becoming more popular within big companies, open sustainability innovations are not yet that often observed.\nOpen Sustainability Innovation can be seen applied by companies in their Sustainability Marketing campaigns. In contrary to traditional marketing laws where point of sale, advertising and the product are the main drivers, Sustainability Marketing requires firms to \"build and maintain sustainable relationships with customers, the social environment and the natural environment.\"\n\nIt involves these four characteristics\n\n\nAn example of a company following the trend to share knowledge regarding sustainability innovations is Facebook. The social networking service started the Open Compute Project to share methods with the entire industry about their new energy-efficient data centre in Oregon, and therefore provides public access to all specifications and mechanical drawings of the facility and servers. According to Facebook, the building is 38% more efficient to build and run than similar data centers.\n"}
{"id": "1977068", "url": "https://en.wikipedia.org/wiki?curid=1977068", "title": "Personal Load Carrying Equipment", "text": "Personal Load Carrying Equipment\n\nPersonal Load Carrying Equipment (PLCE) is one of several current tactical webbing systems of the British Armed Forces. Dependant upon the year of design, and the decade of introduction, the webbing system was designated, and is commonly referred to, as either the \"85 Pattern\", the \"90 Pattern\" or the \"95 Pattern\" webbing.\n\nThe basic configuration consists of a belt, a shoulder harness and a number of individual pouches. Associated with the PLCE webbing system is a series of other similar load carrying equipment, individual items and rucksacks that are produced of the same materials and feature high interoperability.\n\nThe purpose of the PLCE webbing system is to retain the means by which a British soldier may operate for 48 hours or conduct a mission-specific task. Items and components may include a variety of munitions and weapon ancillaries, a three-fold entrenching tool, a bayonet, food and water (including a means to heat water and prepare food), Chemical, Biological, Radiological and Nuclear \"(CBRN)\" protective clothing and communications equipment. Soldiers will also often carry other personal items such as waterproof clothing and spare socks.\n\nThe PLCE webbing system had replaced the Type 58 Pattern webbing, which was Olive drab (OD/OG) in colour and made of canvas material. This system, after having been introduced to the forces in 1960, and considered long outdated by 1980, was still being part of the standard issue equipment of the British Armed Forces during the Falklands War in 1982.\n\nTo overcome the common issues associated with canvas materials, such as shrinking and accelerated decomposition in damp climate conditions, arctic climate conditions, or constant exposure to wet terrain, military load bearing systems, outside of the range of use by the British Armed Forces, had been revised to incorporate or be produced of newer and tougher materials.\n\nDuring the Vietnam War, the United States Armed Forces had introduced and serviced the M-1967 Modernized Load-Carrying Equipment (MLCE) in 1968, and later the All-purpose Lightweight Individual Carrying Equipment (ALICE)]] in 1974. Both systems had incorporated non-decomposing synthetic fabrics and were produced of highly durable nylon material.\n\nFollowing this influential lead, and reconsidering the progression and renewal of military load bearing systems around the world, Britain developed the Type 72 Pattern webbing, which mainly consisted of two ammunition pouches, to be worn on the front, and a field pack, to be worn on the back, produced of Polyurethane-coated nylon and Butyl rubber. This system was never generally issued, but was a Troop Trials Equipment.\n\nThe National People's Army (\"Nationale Volksarmee\") of the German Democratic Republic designed and introduced the \"Uniformtrageversuch\" 85-90 (UTV 85-90), resembling, with the exception of the clothing system, a modified duplicate of the British Type 58 Pattern webbing, inheriting very similar features in appearance and function. The shoulder harness was of identical design, the belt had featured the use of identical buckles, and Type 58 Pattern C-hooks had been incorporated in all components. Complementary items, such as pouches, had been changed in design and size, to meet Warsaw Pact requirements. The webbing system and components were produced of a more robust Nylon material, and featured the Strichtarn camouflage pattern. Whilst this late improvement was observed by the British Armed Forces, and being found to be an affront, especially against the British Army of the Rhine (BAOR), no further considerations have been made, as the testing of prototypic PLCE equipment was already underway.\n\nDevelopments resumed with the progression of the firearms development and introduction of the SA80 family of British small arms. Trials of experimental PLCE webbing and Combat Body Armour (CBA)]] were conducted with selected units in 1984 and 1985. Being very similar to the first standard issue PLCE webbing system, it used snaps of proprietary design for closure on all pouches.\n\nSeven years after the Falklands Conflict, the first standard issue PLCE webbing was introduced in 1989, it was designated the Type \"90 Pattern\" webbing and was Olive (OD/OG) in colour.\n\nThe original components used Type 58 Pattern C-hooks for the belt attachment, and angled D-rings for the shoulder harness attachment on the ammunition pouches. There were separate left and right pouches. The first utility pouches in production, had additional belt attachments for high mounting, similar to the ammunition pouches of the 1937 Pattern Web Equipment or Type 44 Pattern webbing. Later produced PLCE webbing of the Type 90 Pattern incorporates ambidextrous yoke fittings and the standard PLCE webbing belt attachments (see below).\n\nThe PLCE webbing system was also adopted by the Danish Defence Forces (\"Forsvaret\") in the M84 camouflage pattern and by the Defence Forces of Ireland (\"Óglaigh na hÉireann\") in Olive drab. The Permanent and Reserve Defence Forces now employ the Integrated Protection and Load Carrying System (IPLCS).\n\nMany other countries still issue, or have issued similar load bearing systems. By way of example, in its year of introduction to the forces, the United States Armed Forces have adopted the Individual Integrated Fighting System (IIFS).\n\nThe newest variant of the PLCE webbing system, of the Type 90 Pattern, has been in production since 1992 and features the Disruptive Pattern Material (DPM) camouflage pattern as being an integral feature of the now partially obsolete Combat Soldier 95 (\"Soldier 95\") clothing system. The official designation remained unchanged.\n\nWith the introduction of the Combat Soldier 95 (CS95 or \"Soldier 95\") clothing system in 1995, the common misconception arose that a complete revision of the Type 90 Pattern PLCE webbing system was taking place.\n\nThe clothing system underwent final development and entered troop trials in between the years 1992 and 1995. Garments were specifically manufactured for these trials, yet the PLCE webbing system issued, officially remained of the Type 90 Pattern.\n\nDue to confusion, or for the sake of convenience, individual components of the webbing system, were now, as of 1995, unofficially designated, described, and specified as the Type 95 Pattern webbing, as having been widely understood to be part of the Combat Soldier 95 clothing system. No such official change of designation had taken place.\n\nThe only array of PLCE components that could be considered of the unofficial Type 95 Pattern, are those components produced during or after the year of 1995.\n\nThe PLCE webbing system is produced from double-layered 1000 Denier internally rubberised Cordura Nylon, a long lasting and hard wearing fabric. Olive webbing of the same material (\"1000 D\") is being incorporated, along with a variety of hard wearing plastic fasteners (ITW Nexus), Hook and loop fasteners (Velcro) and anti-magnetic press stud fasteners (\"Pull the Dot\").\n\nThe pouches are opened and closed with Spanish Tab fasteners, they can be closed in two different ways, quick release or secure. Small sections of Velcro, sewn on the inside of the lids of the pouches, and the top front section of the pouches, allow for easy and effortless fastening. Added silencer strips allow to cover them when not needed.\n\nA standard ammunition pouch as issued (\"Pouch, Ammunition, Universal, DPM, IRR.\") has two pockets; single pocket versions (\"Pouch, Ammunition (Other Arms), DPM, IRR.\") are available for those individuals not required to carry as much ammunition. Pouches designated to hold ammunition initially contained a dividing strip to hold two magazines in separate compartments and eliminate rattle. Some soldiers, especially infantry soldiers, often removed these dividing strips to make it easier to insert and remove magazines. They also found that three magazines would then fit comfortably and without excessive noise, giving a total capacity of twelve magazines per person in a standard fighting configuration. Later issue ammunition pouches were manufactured without dividers, as eight magazines was not thought to be sufficient for sustained firefights with the enemy. Without dividers, each pouch can now alternatively contain one grenade.\n\nInfrared Reduction (or Resistant) (IRR) coating is applied to and incorporated in all fabric and webbing of the PLCE webbing system, which reduces its heat signature to that of natural foliage, when viewed through infrared night vision systems.\n\nThe Infantry Trials and Development Unit (ITDU) based in Warminster, conducted trials with the PLCE webbing system. It had decided for the system to be fit for purpose, and divided the system into three orders of dress:\n\nThe Assault Order consists of the very essentials needed to conduct a military task in the theatre of war. Ammunition, the water bottle, the entrenching tool, the bayonet, the helmet, and CBRN (Chemical, Biological, Radiological, and Nuclear) protective clothing (stowed in one of the detachable side pouches of the rucksack) is to be carried on operations and patrols of only short duration.\nThe Combat Order is the Assault Order in addition to the means of stowage for rations and personal equipment, that enable the British soldier to live and fight for a period of 24 hours. The second side pouch of the rucksack is now carried. In practice, the patrol packs are used by many units and individual soldiers instead of the side pouches, as they are to be found larger in size and more convenient.\nThe Marching Order is the Combat Order in addition to the carrying of the rucksack (Bergen) and is the fighting load required for operations of up to the duration of two weeks, without means of resupply, except for ammunition, rations and water. The complete Bergen (with side pouches attached) is being carried.\n\nThe initial basis of the PLCE webbing system is the belt; it features two D-rings at the back to attach to the shoulder harness, and many rows of narrow vertical slots sewn into the fabric. Two or more front pouches (ammunition or utility) attach to the belt; these have belt loops and feature T-Bar plastic tab attachments to provide more stability and security when connected to the belt. Every major component of the PLCE webbing system features two T-Bar plastic tab attachments.\n\nThe six-point shoulder harness attaches to the two D-rings on the belt and the two A-rings on each front pouch chosen. This benefits the distribution of weight and allows for a more comfortable carry, than provided with the use of a four-point shoulder harness. Pouches that are to be worn on the back (field pack, utility, water bottle, respirator, wire cutter, entrenching tool) attach to the belt using the same loop and tab system.\n\nThe detachable side pouches of a Bergen (\"Pouch, Side, Rucksack, DPM, IRR.\") can be attached to the dedicated shoulder harness (\"Yoke, Pouch Side, Rucksack, DPM, IRR.\") to construct a day-sack.\n\nIn the year 1997, the Defence Clothing & Textile Agency (\"DCTA\") had decided, that Type \"90 Pattern (Infantry)\" equipment was to be scaled and issued in the following capacity.\n\nThe standard issue accoutrement today, dependent upon the branch of service, was changed to the capacity of two front pouches (\"Pouch, Ammunition, Universal, DPM, IRR.\") instead of one. To this, privately purchased water bottle or utility pouches and hip pads are often added. The entrenching tool pouch (\"Carrier, Entrenching Tool Case, DPM, IRR.\") is sometimes used as an alternative water bottle pouch.\n\nMulti Terrain Pattern (MTP) PLCE is replacing all Disruptive Pattern Material (DPM) PLCE as the core issued webbing equipment for the British Armed Forces, this program started in 2015.\n\nThe following example of individual components, are all current issue items of the British Armed Forces, however many Commercial variants are available.\n\nA new standard issue piece of equipment (\"Waistcoat, Mans, General Purpose Ops.\") that was designed for carrying essential items on a more secure platform than the original PLCE webbing system.\n\nThis consists of a typical waistcoat design, fastened with three ITW Nexus clips. Two triple ammunition pouches are situated on the left hand side of the coat, along with a utility pouch, small utility pouch and a zippered pocket with an internal holster. The right side is similar but with three large utility pouches, along with a small utility pouch and again a zippered pocket with notepad holder. All pouches open and close with ITW Nexus clips as well as having storm seals.\n\nThe vest is adjusted through four ladderlock fasteners and webbing, the shoulders are adjustable with Velcro material. The concept is to gain a secure load carrying system that fits over body armour comfortably. There are various types of this vest depending upon the year of manufacture. The originals are as described above but newer models have loops on the left side for the bayonet frog, clips for a large hydration pouch, name patches on the left side, and a small utility pouch. Most recently, the Spanish tab fasteners are being incorporated again, instead of the ITW Nexus clips.\n\nThe standard issue assault vest, depending on the operational requirements, is available in either the Disruptive Pattern Material (\"DPM\") camouflage pattern, or the Desert DPM (\"DDPM\") camouflage pattern. Commercial variants are available in multiple colours, such as Black, Green, or the American Desert Camouflage Uniform (\"DCU\") camouflage pattern.\n\nThe main criticism of the PLCE webbing system amongst members of the British Armed Forces, is that the belt is prone to slipping. Some soldiers opt to change the plastic buckle for a Roll-Pin type, whereby the belt is threaded and tightened each time it is put on.\n\nMost other nations are developing, or have developed more modular load bearing systems, such as the Modular Lightweight Load-carrying Equipment (\"MOLLE\") which is being widely employed by most branches of the United States Armed Forces. Following this major improvement, the Pouch Attachment Ladder System (\"PALS\") had been incorporated into the Osprey Body Armour (\"OBA\") platform, which is currently being issued to British troops on operations world-wide.\n\nFurther international developments of influence and interest include the Infanterist der Zukunft (\"IdZ\") platform, the Fantassin à Équipements et Liaisons Intégrés (\"FÉLIN\") platform, the Norwegian Modular Arctic Network Soldier (\"NORMANS\") platform, and the Future Infantry Soldier As a System (\"F-INSAS\") platform, which was planned to be fielded by the Indian Armed Forces in the year 2020. No such sophisticated plans of a British Future Soldier programme had been announced by the Ministry of Defence (\"MoD\").\n\nIn the year 2015, the Personal Protective Equipment and Load Carriage System (\"VIRTUS\") platform, this is now on issue to high readiness units.\n\nDue to the introduction and constant improvement of protective equipment, such as the Osprey Body Armour (\"OBA\"), the PLCE webbing system is incompatible so Osprey Mk 4 has a MOLLE belt and under armour Yoke to allow Osprey pouches to be used as belt equipment.\n\nAfter the first introduction in 1989, PLCE in its current MTP form is still Britain's core issue webbing equipment which is compatible with ECBA armour and Mk 6 and 7 helmet, with Osprey issued to non-deployed infantry, leaving Virtus issued to deployed Infantry Commando and Parachute Units\n\n\n"}
{"id": "1481498", "url": "https://en.wikipedia.org/wiki?curid=1481498", "title": "Personal flotation device", "text": "Personal flotation device\n\nA personal flotation device (abbreviated as PFD; also referred to as a life jacket, life preserver, life belt, Mae West, life vest, life saver, cork jacket, buoyancy aid or flotation suit) is a piece of equipment designed to assist a wearer to keep afloat in water. The wearer may be either conscious or unconscious.\n\nPFDs are available in different sizes to accommodate variations in body weight. Designs differ depending on wearing convenience and level of protection.\n\nThe most ancient examples of primitive life jackets can be traced back to inflated bladders or animal skins or hollow, sealed gourds, for support when crossing deep streams and rivers. Purpose-designed buoyant safety devices consisting of simple blocks of wood or cork were used by Norwegian seamen.\n\nIn 1804 a cork life jacket was available for sale. \"The Sporting Magazine\" October, 1804, Vol.XXV, No.147, Page 129. MALLISON's SEAMAN'S FRIEND.\n\nPersonal flotation devices were not part of the equipment issued to naval sailors until the early 19th century, for example at the Napoleonic Battle of Trafalgar, although seamen who were press-ganged into naval service might have used such devices to jump ship and swim to freedom.\n\nIt was not until lifesaving services were formed that the personal safety of boat crews heading out in pulling boats in generally horrific sea conditions was addressed. The modern life jacket is generally credited to one Captain Ward, a Royal National Lifeboat Institution inspector in the United Kingdom, who created a cork vest in 1854 to be worn by lifeboat crews for both weather protection and buoyancy.\n\nIn 1900, French electrical engineer, Gustave Trouvé, patented a battery-powered wearable lifejacket. It incorporated small, rubber-insulated maritime electric batteries not only to inflate the jacket, but also to power a light to transmit and receive SOS messages and to launch a distress flare.\nThe rigid cork material eventually came to be supplanted by pouches containing watertight cells filled with kapok, a vegetable material. These soft cells were much more flexible and comfortable to wear compared with devices using hard cork pieces. Kapok buoyancy was used in many navies fighting in World War II. Foam eventually supplanted kapok for 'inherently buoyant' (vs. inflated and therefore not inherently buoyant) flotation.\n\nThe University of Victoria pioneered research and development of the UVic Thermo Float PFD, which provides superior protection from immersion hypothermia by incorporating a neoprene rubber \"diaper\" that seals the user's upper thigh and groin region from contact with otherwise cold, flushing and debilitating water.\n\nDuring World War II, research to improve the design of life jackets was also conducted in the UK by Edgar Pask OBE, the first Professor of Anaesthesia at Newcastle University. His research involved self-administered anaesthesia as a means of simulating unconsciousness in freezing sea-water. Pask's work earned him the OBE and the description of \"The bravest man in the RAF never to have flown an aeroplane\".\n\nThe Mae West was a common nickname for the first inflatable life preserver, which was invented in 1928 by Peter Markus (1885–1974) (US Patent 1694714), with his subsequent improvements in 1930 and 1931. The nickname originated because someone wearing the inflated life preserver often appeared to be as large-breasted as the actress Mae West. It was popular during the Second World War with U.S. Army Air Forces and Royal Air Force servicemen, who were issued inflatable Mae Wests as part of their flight gear. Air crew members whose lives were saved by use of the Mae West (and other personal flotation devices) were eligible for membership in the Goldfish Club.\n\nDevices designed and approved by authorities for use by civilians (recreational boaters, sailors, canoeists, kayakers) differ from those designed for use by passengers and crew of aircraft (helicopters, airplanes) and of commercial vessels (tugboats, passenger ferries, cargo ships). Devices used by government and military (e.g. water police, coast guard, navy, marines) generally have features not found on civilian or commercial models, for example compatibility with other items worn, like a survival vest, bulletproof vest/body armor, equipment harness, rappelling harness, or parachute harness, and the use of ballistic nylon cloth to protect pressurized canisters used for inflating the vest from injuring the wearer if struck by a round from a firearm. The ballistic cloth keeps the fragments from the canister from becoming shrapnel injurious to the user.\n\nLife jackets or life vests are mandatory on airplanes flying over water bodies, in which case they consist of a pair of air cells (bladders) that can be inflated by triggering the release of carbon dioxide gas from a canister—one for each cell. Or the cells can be inflated \"orally\" that is by blowing into a flexible tube with a one-way valve to seal the air in the cell. Life jackets must also be supplied on commercial seafaring vessels, be accessible to all crew and passengers, and be donned in an emergency.\n\nFlotation devices are also found in near water-edges and at swimming pools. They may take the form of a simple vest, a jacket, a full-body suit (one piece coverall), or their variations suited for particular purposes. They are most commonly made of a tough synthetic fiber material encapsulating a source of buoyancy, such as foam or a chamber of air, and are often brightly colored yellow or orange to maximize visibility for rescuers. Some devices consist of a combination of both buoyancy foam and an air chamber. Retroreflective \"SOLAS\" tape is often sewn to the fabric used to construct life jackets and PFDs to facilitate a person being spotted in darkness when a search light is shone towards the wearer.\nIn the US, federal regulations require all persons under the age of 13 to wear a life jacket (PFD) when in a watercraft under 12 meters long. State regulations may raise or lower this number and must be followed when in that state's jurisdiction.\n\nThe simplest and least buoyant type of PFD comes in the form of nylon-lined foam vests, used predominantly in water sports such as kayaking, canoeing and dinghy sailing.\n\nBuoyancy aids are designed to allow freedom of movement while providing a user with the necessary buoyancy. They are also designed for minimal maintenance and as they are only constructed from foam and can be mass-produced inexpensively, making them one of the most common forms of PFDs.\n\nSome buoyancy aids also come designed especially for children and youth. These vests may include one or two understraps to be worn between the legs of the wearer and also a headrest flap. The understraps are designed to keep the vest from riding up when worn in the water and restrict the wearer from slipping out of the life vest. These straps are adjustable and are included on many different life vests designed to be worn by everyone from infants to adults.\nThe headrest flap is designed to help support the head and keep it out of the water. A grab handle is attached to the headrest to be used if needed to rescue or lift someone out of the water.\n\nBuoyancy aids are rated by the amount of buoyancy they provide in Newtons - the minimum rating to be considered suitable as an adult life-jacket for offshore use is .\n\nLife jackets for outfitting large commercial transport ventures in potentially dangerous waters, such as coastal cruises, offshore passages, and overwater air flights, consisting of either a single air chamber or a pair of (twin or double) sealed air chambers constructed of coated nylon (sometimes with a protective outer encasing of heavier, tougher material such as vinyl), joined together. For use aboard ships they may be constructed of foam. Twin air chambers provide for redundancy in the event of one of the air chambers leaking or failing to \"fire\", for example if the thin air cell fabric is sliced open by sharp metal fragments during emergency evacuation and egress. Most life jackets for leisure use are of the single air chamber type.\n\nAircraft devices for crew and passengers are always inflatable since it may be necessary to swim down and away from a ditched or submerged aircraft and inflated or foam filled devices would significantly impede a person from swimming downward in order to escape a vehicle cabin. Upon surfacing, the person then inflates the device, orally or by triggering the gas canister release mechanism. Most commercial passenger life jackets are fitted with a plastic whistle for attracting attention. It has a light which is activated when in contact with water.\n\nQuality life jackets always provide more buoyancy than offered by the buoyancy aids alone. The positioning of the buoyancy on the wearer's torso is such that a righting moment (rotational force) results that will eventually turn most persons who are floating face down in the water (for example, because they are unconscious) into a face up orientation with their bodies inclined backward, unlike more simply designed common foam buoyancy vests.\n\nToday these air chamber vests are commonly referred to as 'inflatable life jackets or vests' and are available not only for commercial applications but also for those engaged in recreational boating, fishing, sailing, kayaking and canoeing. They are available in a variety of styles and are generally more comfortable and less bulky than traditional foam vests.\n\nThe air chambers are always located over the breast, across the shoulders and encircle the back of the head. They may be inflated by either self-contained carbon dioxide cartridges activated by pulling a cord, or blow tubes with a one-way valve for inflation by exhalation. Some inflatable life jackets also react with salt or fresh water, which causes them to self-inflate. The latest generation of self-triggering inflation devices responds to water pressure when submerged and incorporates an actuator known as a 'hydrostatic release'. All automatic life-jackets can be fired manually if required. Regardless of whether manually or automatically triggered, a pin punctures the cartridge/canister and the CO gas escapes into the sealed air chamber.\n\nHowever, there is a chance that these water pressure activated inflation devices do not inflate the life jacket if a person is wearing waterproof clothing and falls into the water face-down. In these cases the buoyancy of the clothing holds a person on the water surface, which prevents the hydrostatic release. As a result, a person can drown although wearing a fully functional life jacket. In addition there are some circumstances in which the use of self-triggering devices can result in the wearer becoming trapped underwater. For example, the coxswain of a bowloader rowing shell risks being unable to escape should the craft capsize.\n\nTo be on the safe side, a pill-activated inflation device is preferred. A small pill that dissolves on water contact is the safest option, as it also works in shallow waters where a hydrostatic activator fails. This type of jacket is called an 'automatic'. As it is more sensitive to the presence of water, early models could also be activated by very heavy rain or spray. For this reason, spare re-arming kits should be carried on board for each life jacket. However, with modern cup/bobbin mechanisms this problem rarely arises and mechanisms such as the Halkey Roberts Pro firing system have all but eliminated accidental firing.\n\nDrifting in open seas and international waters, as encountered on long sea voyages and by military forces, requires prolonged survival in water. Suitable life jackets are often attached to a vest with pockets and attachment points for distress signaling and survival aids, for example, a handheld two-way radio (walkie-talkie), emergency beacon (406 MHz frequency), signal mirror, sea marker dye, smoke or light signal flares, strobe light, first-aid supplies, concentrated nutritional items, water purification supplies, shark repellent, knife, and pistol.\n\nAccessories such as leg straps can be utilized to keep the inflated chambers in position for floating in a stable attitude, and splash or face shields constructed of clear see-through vinyl covers the head and face to prevent water from waves from inundating the face and entering the airway through the nose or mouth.\n\nSome formats of PFDs are designed for long term immersion in cold water in that they provide insulation as well as buoyancy. While a wetsuit of neoprene rubber or a diver's drysuit provides a degree of flotation, in most maritime countries they are not formally considered by regulatory agencies as approved lifesaving devices or as PFDs.\n\nIt is possible for an incapacitated person in the water to float face-down while wearing only a wet suit or a dry suit since they are not designed to serve as lifesaving devices in the normal understanding of that term.\n\nThe Mark 10 Submarine Escape Immersion Equipment (SEIE) suit is intended to allow submariners to escape from much deeper depths than currently possible with the Steinke hood. Some United States Navy submarines already have the system, with an ambitious installation and training schedule in place for the remainder of the fleet.\n\nBecause it is a full-body suit, the Mark 10 provides thermal protection once the wearer reaches the surface, and the Royal Navy has successfully tested it at depths.\n\nScuba divers commonly wear a buoyancy compensator, which has an inflatable gas chamber. The amount of gas can be increased or decreased to enable the diver to ascend, descend or maintain neutral buoyancy at a given water depth and to provide positive buoyancy in an emergency to bring him to the surface or keep him at the surface.\n\nSpecialized life jackets include shorter-profile vests commonly used for kayaking (especially playboating), and high-buoyant types for river outfitters and other whitewater professionals. PFDs which include harnesses for tethered rescue work ('live-bait rescue') and pockets or daisy-chains (a series of loops created by sewing flat nylon webbing at regular intervals for the attachment of rescue gear) are made for swiftwater rescue technicians.\n\nPersonal flotation devices have been developed for dogs and other pets.\n\nWhile the USCG does not certify personal flotation devices for animals, many manufacturers produce life jackets for dogs and cats. Dogs and cats have been known to die from drowning, either because they do not know how to swim, or because they tire out from overexposure or old age, or have a medical complication such as a seizure, or become unconscious.\n\nMost life jackets on the market are designed with foam that wraps around the animal's torso and neck. They provide a basic amount of buoyancy for a dog, but may not provide enough support for the head. They are not ideal for use with heavy dogs. However, they often incorporate a grab handle, which may help to hoist the dog back into the boat.\n\nAlthough most pet life jackets are passive devices, there is at least one automatically inflated life jacket available for pets (made by Critter’s Inflatable, LLC). An automatic flotation device is generally more expensive than a foam life jacket, but, like automatic PFDs designed for humans, they are less bulky to wear when not inflated, and when inflated may provide more buoyancy than foam devices. Automatic pet flotation devices are popular in the bulldog community, and also for water therapy where extra support may be needed under the head.\n\n"}
{"id": "29129698", "url": "https://en.wikipedia.org/wiki?curid=29129698", "title": "Plant floor communication", "text": "Plant floor communication\n\nPlant floor communications refers to the control and data communications typically found in automation environments, on a manufacturing plant floor or process plant. The difference between manufacturing and process is typically the types of control involved, discrete control or continuous control (aka process control). Many plants offer a hybrid of both discrete and continuous control. The underlying commonality between them all is that the automation systems are often an integration of multi-vendor products to form one system. Each vendor product typically offers communication capability for programming, maintaining and collecting data from their products. A properly orchestrated plant floor environment will likely include a variety of communications, some for machine to machine (M2M) communications – to facilitate efficient primary control over the process and some for Machine to Enterprise (M2E) communications – to facilitate connectivity with Business Systems that provide overall reporting, scheduling and inventory management functions.\n\nAutomation controllers typically offer communication modules to enable them to support a variety of industrial protocols, to facilitate machine to machine communications. These modules are often special designed for the protocol. A new class of module, the universal gateway, is becoming more prevalent as it offers the ability for an automation controller to communicate over one or more protocols simultaneously, and can be reconfigured for additional protocols without a module change.\n\nFew automation controllers offer direct connectivity to business systems such as MES and ERP systems. Overall integration of automation controllers to business systems are typically configured by system integrators, able to bring their unique knowledge on process, equipment and vendor solutions.\n\nIntegration is typically managed through one of three mechanisms:\n\nDirect Integration – Business systems include connectivity (communications to plant floor equipment) as part of their product offering. This requires the business system developers to offer specific support for the variety of plant floor equipment that they want to interface with. Business system vendors must be expert in their own products, and connectivity to other vendor products, often those offered by competitors.\n\nRelational Database (RDB) Integration – Business systems connect to plant floor data sources through a Relational Database Staging Table. Plant floor systems will deposit the necessary information into a Relational Data Base. The business system will remove and use the information from the RDB Table. The benefit of RDB Staging is that business system vendors do not need to get involved in the complexities of plant floor equipment integration. Connectivity becomes the responsibility of the system integrator.\n\nEATM (Enterprise Transaction Modules) – These devices have the ability to communicate directly with plant floor equipment and will transact data with the business system in methods best supported by the business system. Again, this can be through a staging table, Web Services, or through system specific business system APIs. The benefit of an EATM is that it offers a complete, off the shelf solution, minimizing long term costs and customization.\n\nCustom Integrated Solutions – Many system integrators designs offer custom crafted solutions, created on a per instance basis to meet site and system requirements. There are a wide variety of communications drivers available for plant floor equipment and there are separate products that have the ability to log data to relational database tables. Standards exist within the industry to support interoperability between software products, the most widely known being OPC, managed by the OPC Foundation. Custom Integrated Solutions typically run on workstation or server class computers. These systems tend to have the highest level of initial integration cost, and can have a higher long term cost in terms on maintenance and reliability. Long term costs can be minimized through careful system testing and thorough documentation.\n\n"}
{"id": "4646627", "url": "https://en.wikipedia.org/wiki?curid=4646627", "title": "Porting Authorisation Code", "text": "Porting Authorisation Code\n\nPorting Authorization Code (PAC) is a unique identifier (normally 9 characters long and in the format \"ABC123456\") used by some mobile network operators to facilitate mobile number portability (MNP). This allows users to retain their mobile telephone number when switching operators.\n\nTelecommunications service is regulated in the UK by Ofcom. On 25 July 2003, Ofcom introduced the \"General Conditions of Entitlement\" which apply to all communications networks and service providers in the UK. Several amendments to this original document have been issued since this time\n\nCondition 18 requires all providers to provide number portability but only to subscribers of publicly available telephone services who request it. Number portability must be provided as soon as practicable and on reasonable terms to subscribers and bilateral porting arrangements between providers must accord with agreed processes.\n\n\nSome mobile phone companies can charge a fee to move the customer's number. This is usually no more than £25. The provider must issue a PAC within 2 hours of the port-out request, if such request was made over the phone for fewer than 25 numbers on a single account. Customer debt is not a valid reason for a service provider to refuse issuing of a PAC. Service providers may not treat PAC requests as requests to terminate service. Pay-as-you-go customers will lose any unused credit when switching service providers.\n\nFrom 1st July 2019, customers will be able to request a PAC by text message, rather than having to call their existing network.\n\nIn India, the code is known as a 'Unique Porting Code (UPC)'. The rules for number portability are prescribed by the Telecom Regulatory Authority of India.\n\n\n"}
{"id": "991053", "url": "https://en.wikipedia.org/wiki?curid=991053", "title": "Programmer (hardware)", "text": "Programmer (hardware)\n\nA programmer (hardware), device programmer, chip programmer, device burner, or PROM writer is a piece of electronic equipment that arrange written software to configure programmable non-volatile integrated circuits, called programmable devices. The target devices include; PROM, EPROM, EEPROM, Flash memory, eMMC, MRAM, FeRAM, NVRAM, PLD, PLA, PAL, GAL, CPLD, FPGA, and MCU. These are terminologies in the field of computer hardware.\n\nProgrammer hardware has two variants. One is configuring the target device itself with a socket on the programmer. Another is configuring the device on a printed circuit board.\n\nIn the former case, the target device is inserted into a socket (usually ZIF) on top of the programmer. If the device is not a standard \"DIP packaging\", a plug-in adapter board, which converts the footprint with another socket, is used.\n\nIn the latter case, device programmer is directly connected to the printed circuit board by a connector, usually with a cable. This way is called \"on-board programming\", \"in-circuit programming\", or \"in-system programming\".\n\nAfterwards the data is transferred from the programmer into the device by applying signals through the connecting pins. Some devices have a serial interface\nfor receiving the programming data (including JTAG interface).\nOther devices require the data on parallel pins, followed by a programming pulse with a higher voltage for programming the data into the device.\n\nUsually device programmers are connected to a personal computer through a parallel port,\nUSB port,\nor LAN interface.\nA software program on the computer then transfers the data to the programmer,\n\nselects the device and interface type, and starts the programming process to read/ write/ erase/ blank the data inside the device.\n\nThere are four general types of device programmers:\n\nRegarding old PROM programmers, as the many programmable devices have different voltage requirements, every pin driver must be able to apply different voltages in a range of 025 Volts.\nBut according to the progress of memory device technology, recent flash memory programmers do not need high voltages.\n\nIn the early days of computing, booting mechanism was a mechanical devices usually consisted of switches and LEDs. It means the \"programmer\" was not an equipment but a human, who entered machine codes one by one, by setting the switches in a series of \"on\" and \"off\" positions. These positions of switches corresponded to the machine codes, similar to today's assembly language.\nNowadays, EEPROMs are used for bootstrapping mechanism as BIOS, and no need to operate mechanical switches for programming.\n\nFor each vendor's web site, refer to \"External links\" section.\n\n\n\n"}
{"id": "22882709", "url": "https://en.wikipedia.org/wiki?curid=22882709", "title": "Scattering rate", "text": "Scattering rate\n\nA formula may be derived mathematically for the rate of scattering when a beam of electrons passes through a material.\nDefine the unperturbed Hamiltonian by formula_1, the time dependent perturbing Hamiltonian by formula_2 and total Hamiltonian by formula_3.\n\nThe eigenstates of the unperturbed Hamiltonian are assumed to be\n\nIn the interaction picture, the state ket is defined by\n\nBy a Schrödinger equation, we see \nwhich is a Schrödinger-like equation with the total formula_3 replaced by formula_9.\n\nSolving the differential equation, we can find the coefficient of n-state.\n\nwhere, the zeroth-order term and first-order term are\n\nThe probability of finding formula_13 is found by evaluating formula_14.\n\nIn case of constant perturbation,formula_15 is calculated by \n\nUsing the equation which is\n\nThe transition rate of an electron from the initial state formula_19 to final state formula_20 is given by\n\nwhere formula_22 and formula_23 are the energies of the initial and final states including the perturbation state and ensures the formula_24-function indicate energy conservation.\n\nThe scattering rate w(k) is determined by summing all the possible finite states k' of electron scattering from an initial state k to a final state k', and is defined by\n\nThe integral form is \n\n"}
{"id": "3752644", "url": "https://en.wikipedia.org/wiki?curid=3752644", "title": "Selective availability anti-spoofing module", "text": "Selective availability anti-spoofing module\n\nA Selective Availability Anti-spoofing Module (SAASM) used by military Global Positioning System receivers to allow decryption of precision GPS observations, while the accuracy of civilian GPS receivers may be reduced by the United States military through Selective Availability (SA) and anti-spoofing (AS). However, on May 1, 2000 it was announced that SA was being discontinued, along with a United States Presidential Directive that no future GPS programs will include it. Before the advent of L2C, AS was meant to prevent access to dual-frequency observations to civilian users.\n\nSAASM allows satellite authentication, over-the-air rekeying, and contingency recovery. Those features are not available with the similar, but older, PPS-SM (Precise Positioning Service Security Module) system. PPS-SM systems require periodic updates with a classified \"Red Key\" that may only be transmitted by secure means (such as physically taking the receiver to a secure facility for rekeying or having a trusted courier deliver a paper tape with a new key to the receiver, after which that paper tape must be securely destroyed). SAASM systems can be updated with an encrypted \"Black Key\" that may be transmitted over unclassified channels. All military receivers newly deployed after the end of September 2006 must use SAASM.\n\nSAASM does not provide any additional anti-jam capability, however the higher data (chipping) rate of P(Y) code can provide a higher processing gain which will provide better tracking performance in a jamming environment. Future GPS upgrades, such as M-Code, will provide additional improvements to anti-jam capabilities.\n\nSAASM hardware is covered with an anti-tampering coating, to deter analysis of their internal operation.\n\nDeployment of the next generation military signal for GPS, called M-code, commenced with the launch of IIR-M and IIF satellites, beginning in 2005. A complete constellation of 18 satellites with M-code capability is planned for 2016.\n\n\n"}
{"id": "954216", "url": "https://en.wikipedia.org/wiki?curid=954216", "title": "Skeleton (computer programming)", "text": "Skeleton (computer programming)\n\nSkeleton programming is a style of computer programming based on simple high-level program structures and so called dummy code. Program skeletons resemble pseudocode, but allow parsing, compilation and testing of the code. Dummy code is inserted in a program skeleton to simulate processing and avoid compilation error messages. It may involve empty function declarations, or functions that return a correct result only for a simple test case where the expected response of the code is known.\n\nSkeleton programming facilitates a top-down design approach, where a partially functional system with complete high-level structures is designed and coded, and this system is then progressively expanded to fulfill the requirements of the project. Program skeletons are also sometimes used for high-level descriptions of algorithms. A program skeleton may also be utilized as a template that reflects syntax and structures commonly used in a wide class of problems.\n\nSkeleton programs are utilized in the template method design pattern used in object-oriented programming. In object-oriented programming, dummy code corresponds to an abstract method, a method stub or a mock object. In the Java remote method invocation (Java RMI) nomenclature, a stub communicates on the client-side with a skeleton on the server-side.\n\nA class skeleton is an outline of a class that is used in software engineering. It contains a description of the class's roles, and describes the purposes of the variables and methods, but does not implement them. The class is later implemented from the skeleton.\n\n"}
{"id": "19283801", "url": "https://en.wikipedia.org/wiki?curid=19283801", "title": "The Truth About Chernobyl", "text": "The Truth About Chernobyl\n\nThe Truth About Chernobyl is a 1991 book by Grigori Medvedev. Medvedev served as deputy chief engineer at the No. 1 reactor unit of the Chernobyl Nuclear Power Plant in the 1970s. At the time of the Chernobyl disaster in 1986, Medvedev was deputy director of the main industrial department in the Soviet Ministry of Energy dealing with the construction of nuclear power stations. Since Medvedev knew the Chernobyl plant well, he was sent back as a special investigator immediately after the 1986 catastrophe.\n\nIn his book, Medvedev provides extensive first-hand testimony, based on many interviews, describing minute by minute precisely what was and was not done both before and after the explosion. It has been described as a tragic tale of pervasive, institutionalized, bureaucratic incompetence leading up to the accident; and heroic, heartbreaking sacrifice among those who had to deal with the emergency afterwards.\n\nThe book is the single prime source for much of the actions of the operators, managers, firemen and others who are the actors in the Chernobyl disaster. It is, for example, the prime source for the article Individual involvement in the Chernobyl disaster. The book is written not in a documentary style but in a very personal style, often speaking in the first person. While it includes extensive direct quotes from some of those who survived the disaster, it does not include references beyond a bare seven footnotes.\n\nIn 1991 it was awarded the Los Angeles Times Book Prize, Science and technology.\n\n"}
{"id": "5278372", "url": "https://en.wikipedia.org/wiki?curid=5278372", "title": "Thermodynamicist", "text": "Thermodynamicist\n\nIn thermodynamics, a thermodynamicist is someone who studies thermodynamic processes and phenomena, i.e. the physics that deal with mechanical action and relations of heat.\n\nAmong the well-known number of , include Sadi Carnot, Rudolf Clausius, Willard Gibbs, Hermann von Helmholtz, and Max Planck.\n\nAlthough most consider the French physicist Nicolas Sadi Carnot to be the first true thermodynamicist, the term \"thermodynamics\" itself wasn’t coined until 1849 by Lord Kelvin in his publication \"An Account of Carnot's Theory of the Motive Power of Heat\".\n\nThe first thermodynamic textbook was written in 1859 by William Rankine, a civil and mechanical engineering professor at the University of Glasgow.\n\n"}
{"id": "7137701", "url": "https://en.wikipedia.org/wiki?curid=7137701", "title": "Thomas Tulis", "text": "Thomas Tulis\n\nThomas Tulis (born 1961) is an American photographer and painter living and working in Atlanta, Georgia. He was born in Chattanooga, Tennessee.\n\nTulis lives a very simple life.\n\nAfter a couple of years of college, Tulis joined the United States Army. After the army he put all of his time and efforts into his art. In 1985 he was able to open his first studio and that same year was asked to do his first exhibition.\n\n\n\n\n"}
{"id": "477037", "url": "https://en.wikipedia.org/wiki?curid=477037", "title": "Weather vane", "text": "Weather vane\n\nA weather vane, wind vane, or weathercock is an instrument for showing the direction of the wind. It is typically used as an architectural ornament to the highest point of a building. The word \"vane\" comes from the Old English word \"fana\" meaning \"flag\". \n\nAlthough partly functional, weather vanes are generally decorative, often featuring the traditional cockerel design with letters indicating the points of the compass. Other common motifs include ships, arrows and horses. Not all weather vanes have pointers. When the wind is sufficiently strong, the head of the arrow or cockerel (or equivalent depending on the chosen design) will indicate the direction from which the wind is blowing.\n\nThe weather vane was independently invented in ancient China and Greece around the same time during the 2nd century BCE. The earliest written reference to a weather vane appears in the \"Huainanzi\", and a weather vane was fitted on top of the Tower of the Winds in Athens.\n\nThe oldest textual reference to a weather vane comes from the Chinese \"Huainanzi\" dating from around 139 BCE, which describes a \"wind-observing fan\" (\"hou feng shan\", ). The Tower of the Winds on the ancient Greek \"agora\" in Athens once bore on its roof a wind vane in the form of a bronze Triton holding a rod in his outstretched hand, rotating as the wind changed direction. Below this was a frieze adorned with the eight Greek wind deities. The eight-metre-high structure also featured sundials, and a water clock inside. It dates from around 50 BCE.\n\nMilitary documents from the Three Kingdoms period of China (220–280) refer to the weather vane as \"five ounces\" (\"wu liang\", ), named after the weight of its materials. By the 3rd century, Chinese weather vanes were shaped like birds and took the name of \"wind-indicating bird\" (\"xiang feng wu\", ). The \"Sanfu huangtu\" (), a 3rd century book written by Miao Changyan about the palaces at Chang'an, describes a bird-shaped wind vane situated on a tower roof, which was possibly also an anemometer:\nThe Han 'Ling Tai' (Observatory Platform) was eight \"li\" north-west of Chang'an. It was called 'Ling Tai' because it was originally intended for observations of the Yin and the Yang and the changes occurring in the celestial bodies, but in the Han it began to be called Qing Tai. Guo Yuansheng, in his \"Shu Zheng Ji\" (Records of Military Expeditions), says that south of the palaces there was a Ling Tai, fifteen \"ren\" (120 feet) high,\nupon the top of which was the armillary sphere made by Zhang Heng. Also there was a wind-indicating bronze bird (\"xiang feng tong wu\"), which was moved by the wind; and it was said that the bird moved when a 1000-\"li\" wind was\nblowing. There was also a bronze gnomon 8 feet high, with a 13 feet long and 1 foot 2 inches broad. According to an inscription, this was set up in the 4th year of the Taichu reign-period (101 BCE). \nThe oldest surviving weather vane with the shape of a rooster is the \"Gallo di Ramperto\", made in 820 CE and now preserved in the Museo di Santa Giulia in Brescia, Lombardy.\n\nPope Leo IV had a cock placed on the Old St. Peter's Basilica or old Constantinian basilica.\n\nPope Gregory I said that the cock (rooster) \"was the most suitable emblem of Christianity\", being \"the emblem of St Peter\", a reference to in which Jesus predicts that Peter will deny him three times before the rooster crows.\n\nAs a result of this, the cock gradually began to be used as a weather vane on church steeples, and in the 9th century Pope Nicholas I ordered the figure to be placed on every church steeple.\n\nThe Bayeux Tapestry of the 1070s depicts a man installing a cock on Westminster Abbey.\n\nOne alternative theory about the origin of weathercocks on church steeples is that it was an emblem of the vigilance of the clergy calling the people to prayer.\n\nAnother theory says that the cock was not a Christian symbol but an emblem of the sun derived from the Goths.\n\nA few churches used weather vanes in the shape of the emblems of their patron saints. The City of London has two surviving examples. The weather vane of St Peter upon Cornhill is not in the shape of a rooster, but a key; while St Lawrence Jewry's weather vane is in the form of a gridiron.\n\nEarly weather vanes had very ornamental pointers, but modern wind vanes are usually simple arrows that dispense with the directionals because the instrument is connected to a remote reading station. An early example of this was installed in the Royal Navy's Admiralty building in London – the vane on the roof was mechanically linked to a large dial in the boardroom so senior officers were always aware of the wind direction when they met.\n\nModern \"aerovanes\" combine the directional vane with an anemometer (a device for measuring wind speed). Co-locating both instruments allows them to use the same axis (a vertical rod) and provides a coordinated readout.\n\nAccording to the Guinness World Records, the world's largest weather vane is a Tío Pepe sherry advertisement located in Jerez, Spain. The city of Montague, Michigan also claims to have the largest standard-design weather vane, being a ship and arrow which measures 48 feet tall, with an arrow 26 feet long.\n\nA challenger for the title of world's largest weather vane is located in Whitehorse, Yukon. The weather vane is a retired Douglas DC-3 CF-CPY atop a swiveling support. Located at the Yukon Transportation Museum beside Whitehorse International Airport, the weather vane is used by pilots to determine wind direction, used as a landmark by tourists and enjoyed by locals. The weather vane only requires a 5 knot wind to rotate.\n\nA challenger for the worlds tallest weather vane is located in Westlock, Alberta. The classic weather vane that reaches to 50 feet is topped by a 1942 Case Model D Tractor. This landmark is located at the Canadian Tractor Museum. \n\nThe term \"weathervane\" is also a slang word for a politician who has frequent changes of opinion. The National Assembly of Quebec has banned use of this slang term as a slur after its use by members of the legislature.\n\n\n"}
