{"id": "32122623", "url": "https://en.wikipedia.org/wiki?curid=32122623", "title": "Additive manufacturing file format", "text": "Additive manufacturing file format\n\nAdditive manufacturing file format (AMF) is an open standard for describing objects for additive manufacturing processes such as 3D printing. The official ISO/ASTM 52915:2016 standard is an XML-based format designed to allow any computer-aided design software to describe the shape and composition of any 3D object to be fabricated on any 3D printer. Unlike its predecessor STL format, AMF has native support for color, materials, lattices, and constellations.\n\nAn AMF can represent one object, or multiple objects arranged in a constellation. Each object is described as a set of non-overlapping volumes. Each volume is described by a triangular mesh that references a set of points (vertices). These vertices can be shared among volumes belonging to the same object. An AMF file can also specify the material and the color of each volume, as well as the color of each triangle in the mesh. The AMF file is compressed using the zip compression format, but the \".amf\" file extension is retained. A minimal AMF reader implementation must be able to decompress an AMF file and import at least geometry information (ignoring curvature).\n\nThe AMF file begins with the XML declaration line specifying the XML version and encoding. The remainder of the file is enclosed between an opening element and a closing element. The unit system can also be specified (millimeter, inch, feet, meter or micrometer). In absence of a units specification, millimeters are assumed.\n\nWithin the AMF brackets, there are five top level elements. Only a single object element is required for a fully functional AMF file.\n\n\nThe format uses a Face-vertex polygon mesh layout. Each top-level element specifies a unique id. The element can also optionally specify a material. The entire mesh geometry is contained in a single mesh element. The mesh is defined using one element and one or more elements. The required element lists all vertices that are used in this object. Each vertex is implicitly assigned a number in the order in which it was declared, starting at zero. The required child element gives the position of the point in 3D space using the , and elements.\nAfter the vertex information, at least one element must be included. Each volume encapsulates a closed volume of the object, Multiple volumes can be specified in a single object. Volumes may share vertices at interfaces but may not have any overlapping volume.\nWithin each volume, the child element is used to define triangles that tessellate the surface of the volume. Each element will list three vertices from the set of indices of the previously defined vertices given in the element. The indices of the three vertices of the triangles are specified using the , and elements. The order of the vertices must be according to the right-hand rule, such that vertices are listed in counter-clockwise order as viewed from the outside. Each triangle is implicitly assigned a number in the order in which it was declared, starting at zero.\n\nColors are introduced using the element by specifying the red, green, blue and alpha (transparency) channels in the sRGB color space as numbers in the range of 0 to 1. The element can be inserted at the material, object, volume, vertex, or triangle levels, and takes priority in reverse order (triangle color is highest priority). The transparency channel specifies to what degree the color from the lower level is blended in. By default, all values are set to zero.\n\nA color can also be specified by referring to a formula that can use a variety of coordinate-dependent functions.\n\nTexture maps allow assigning color or material to a surface or a volume, borrowing from the idea of Texture mapping in graphics. The element is first used to associate a texture-id with particular texture data. The data can be represented as either a 2D or a 3D array, depending on whether the color or material need to be mapped to a surface or a volume. The data is represented as a string of bytes in Base64 encoding, one byte per pixel specifying the grayscale level in the 0-255 range.\n\nOnce the texture-id is assigned, the texture data can be referenced in a color formula, such as in the example below.\n\nUsually, however, the coordinated will not be used directly as shown above, but transformed first to bring them from object coordinates to texture coordinates. For example, tex(1,f1(x,y,z),f2(x,y,z),f3(x,y,z)) where f1(), f2(), f3() are some functions, typically linear.\n\nMaterials are introduced using the <material> element. Each material is assigned a unique id. \nGeometric volumes are associated with materials by specifying a material-id within the <volume> element.\n\nNew materials can be defined as compositions of other materials. The element is used to specify the proportions of the composition, as a constant or as a formula dependent of the x, y, and z coordinates. A constant mixing proportion will lead to a homogenous material. A coordinate-dependent composition can lead to a graded material. More complex coordinate-dependent proportions can lead to nonlinear material gradients as well as periodic and non-periodic substructure. The proportion formula can also refer to a texture map using the tex(textureid,x,y,z) function. Reference to material-id \"0\" (void) is reserved and may be used to specify porous structures. Reference to the rand(x,y,z) function can be used to specify pseudo-random materials. The rand(x,y,z) function returns a random number between 0 and 1 that is persistent for that coordinate.\n\nMultiple objects can be arranged together using the element. A constellation can specify the position and orientation of objects to increase packing efficiency and to describe large arrays of identical objects. The element specifies the displacement and rotation an existing object needs to undergo to arrive into its position in the constellation. The displacement and rotation are always defined relatively to the original position and orientation in which the object was defined. A constellation can refer to another constellation as long as cyclic references are avoided.\n\nIf multiple top-level constellations are specified, or if multplie objects without constellations are specified, each of them will be imported with no relative position data. The importing program can then freely determine the relative positioning.\n\nThe element can optionally be used to specify additional information about the objects, geometries and materials being defined. For example, this information can specify a name, textual description, authorship, copyright information and special instructions. The element can be included at the top level to specify attributes of the entire file, or within objects, volumes and materials to specify attributes local to that entity.\n\nIn order to improve geometric fidelity, the format allows curving the triangle patches. By default, all triangles are assumed to be flat and all triangle edges are assumed to be straight lines connecting their two vertices. However, curved triangles and curved edges can optionally be specified in order to reduce the number of mesh elements required to describe a curved surface. The curvature information has been shown to reduce the error of a spherical surface by a factor of 1000 as compared to a surface described by the same number of planar triangles. Curvature should not create a deviation from the plane of the flat triangle that exceeds 50% of the largest dimension of the triangle.\n\nTo specify curvature, a vertex can optionally contain a child element to specify desired surface normal at the location of the vertex. The normal should be unit length and pointing outwards. If this normal is specified, all triangle edges meeting at that vertex are curved so that they are perpendicular to that normal and in the plane defined by the normal and the original straight edge. When the curvature of a surface at a vertex is undefined (for example at a cusp, corner or edge), an element can be used to specify the curvature of a single non-linear edge joining two vertices. The curvature is specified using the tangent direction vectors at the beginning and end of that edge. The element will take precedence in case of a conflict with the curvature implied by a element.\n\nWhen curvature is specified, the triangle is decomposed recursively into four sub-triangles. The recursion must be executed five levels deep, so that the original curved triangle is ultimately replaced by 1024 flat triangles. These 1024 triangles are generated \"on the fly\" and stored temporarily only while layers intersecting that triangle are being processed for manufacturing.\n\nIn both the and elements, coordinate-dependent formulas can be used instead of constants. These formulas can use various standard algebraic and mathematical operators and expressions.\n\nAn AMF can be stored either as plain text or as compressed text. If compressed, the compression is in ZIP archive format. A compressed AMF file is typically about half the size of an equivalent compressed binary STL file. The compression can be done manually using compression software such as WinZip, 7-Zip, or automatically by the exporting software during write. Both the compressed and uncompressed files have the AMF extension and it is the responsibility of the parsing program to determine whether or not the file is compressed, and if so to perform decompression during import.\n\nWhen the ASTM Design subcommittee began developing the AMF specifications, a survey of stakeholders revealed that the key priority for the new standard was the requirement for a non-proprietary format. Units and buildability issues were a concern lingering from problems with the STL format. Other key requirements were the ability to specify geometry with high fidelity and small file sizes, multiple materials, color, and microstructures. In order to be successful across the field of additive manufacturing, this file format was designed to address the following concerns\n\n\nSince the mid-1980s, the STL file format has been the \"de facto\" industry standard for transferring information between design programs and additive manufacturing equipment. The STL format only contained information about a surface mesh, and had no provisions for representing color, texture, material, substructure, and other properties of the fabricated target object. As additive manufacturing technology evolved from producing primarily single-material, homogenous shapes to producing multi-material geometries in full color with functionally graded materials and microstructures, there was a growing need for a standard interchange file format that could support these features. A second factor that ushered the development of the standard was the improving resolution of additive manufacturing technologies. As the fidelity of printing processes approached micron scale resolution, the number of triangles required to describe smooth curved surfaces resulted in unacceptably large file sizes.\n\nDuring the 1990s and 2000s, a number of proprietary file formats have been in use by various companies to support specific features of their manufacturing equipment, but the lack of an industry-wide agreement prevented widespread adoption of any single format. In January 2009, a new ASTM Committee F42 on Additive Manufacturing Technologies was established, and a design subcommittee was formed to develop a new standard. A survey was conducted in late 2009 leading to over a year of deliberations on the new standard. The resulting first revision of the AMF standard became official on May 2, 2011\n\nDuring the July 2013 meetings of ASTM’s F42 and ISO’s TC261 in Nottingham (UK), the Joint Plan for Additive Manufacturing Standards Development was approved. Since then, the AMF standard is managed jointly by ISO and ASTM.\n\nBelow is a simple AMF file describing a pyramid made of two materials, adapted from the AMF tutorial (548 bytes compressed). To create this AMF file, copy and paste the text below text into a text editor or an xml editor, and save the file as \"pyramid.amf\". Then compress the file with ZIP, and rename the file extension from \".zip\" to \".zip.amf\".\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<amf unit=\"inch\" version=\"1.1\">\n</amf>\n\n"}
{"id": "470926", "url": "https://en.wikipedia.org/wiki?curid=470926", "title": "Advanced Spaceborne Thermal Emission and Reflection Radiometer", "text": "Advanced Spaceborne Thermal Emission and Reflection Radiometer\n\nThe Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER) is a Japanese sensor which is one of five remote sensory devices on board the Terra satellite launched into Earth orbit by NASA in 1999. The instrument has been collecting data since February 2000.\n\nASTER provides high-resolution images of the planet Earth in 14 different bands of the electromagnetic spectrum, ranging from visible to thermal infrared light. The resolution of images ranges between 15 and 90 meters. ASTER data are used to create detailed maps of surface temperature of land, emissivity, reflectance, and elevation.\n\nIn April 2008, the SWIR detectors of ASTER began malfunctioning and were publicly declared non-operational by NASA in January 2009. All SWIR data collected after 1 April 2008 has been marked as unusable.\n\nThe ASTER Global Digital Elevation Model (GDEM) is available at no charge to users worldwide via electronic download.\n\nAs of 2 April 2016, the entire catalogue of ASTER image data became publicly available online at no cost. It can be downloaded with a free registered account from either NASA's Earth Data Search delivery system or from the USGS Earth Explorer delivery system.\n\nOn 29 June 2009, the Global Digital Elevation Model (GDEM) was released to the public.\nA joint operation between NASA and Japan's Ministry of Economy, Trade and Industry (METI), the Global Digital Elevation Model is the most complete mapping of the earth ever made, covering 99% of its surface.\nThe previous most comprehensive map, NASA's Shuttle Radar Topography Mission, covered approximately 80% of the Earth's surface, with a global resolution of 90 meters, and a resolution of 30 meters over the USA.\nThe GDEM covers the planet from 83 degrees North to 83 degrees South (surpassing SRTM's coverage of 56 °S to 60 °N), becoming the first earth mapping system that provides comprehensive coverage of the polar regions. It was created by compiling 1.3 million VNIR images taken by ASTER using single-pass stereoscopic correlation techniques, with terrain elevation measurements taken globally at 30-meter (98 ft) intervals.\n\nDespite the high nominal resolution, however, some reviewers have commented that the true resolution is considerably lower, and not as good as that of SRTM data, and serious artifacts are present.\n\nSome of these limitations have been confirmed by METI and NASA, who point out that the current version of the GDEM product is \"research grade\".\n\nDuring October 2011, version 2 of Global Digital Elevation Model was publicly released. This is considered an improvement upon version 1. These improvements include increased horizontal and vertical accuracy, better horizontal resolution, reduced presence of artifacts, and more realistic values over water bodies. However, one reviewer still regards the Aster version 2 dataset, although showing 'a considerable improvement in the effective level of detail', to still be regarded as 'experimental or research grade' due to presence of artefacts.\nA 2014 study showed that over rugged mountainous terrain the ASTER version 2 data set can be a more accurate representation of the ground than the SRTM elevation model.\n\n\n"}
{"id": "7629541", "url": "https://en.wikipedia.org/wiki?curid=7629541", "title": "Belfast Operative Bakers' Union", "text": "Belfast Operative Bakers' Union\n\nThe Belfast Operative Bakers' Union was a trade union representing bakery workers in Northern Ireland.\n\nThe union described itself as having been founded in 1817 as the Belfast Bakers' Society. By 1892, it was known as the Belfast Operative Bakers' Friendly and Allied Trades' Society, with 400 members.\n\nThe union became part of the Irish National Federal Union of Bakers in 1900, but it left again in 1905, with a slightly increased membership, which peaked at 693 in 1909.\n\nThe union merged into the Transport and General Workers' Union in 1930.\n\n"}
{"id": "51332769", "url": "https://en.wikipedia.org/wiki?curid=51332769", "title": "Builder's photo", "text": "Builder's photo\n\nA builder's photo, also called an official photo, is a specific type of photograph that is typically made by rail transport rolling stock manufacturers to show a vehicle that has been newly built or rebuilt. The builder's photo is meant to show an overview of the basic exterior form of a unit of rolling stock. Photographs made by railfans that show similar features to builder's photos are sometimes informally referred to as roster shots. Builder's photos were also made by some automobile manufacturers to show a representative sample of new models they produced.\n\nPrints of builder's photos were also often made for executives of the manufacturers and railroad companies to hang in their offices. Builder's photos were also reproduced as post cards as well as reprinted in advertisements to promote the railroad companies or manufacturers depicted therein. In the United Kingdom, steam locomotives were often temporarily painted in photographic grey color schemes so they would photograph well in black and white images. Some details in darker-colored areas of the subject were also sometimes painted in a high-contrast bright color to ensure that they would be visible in the photograph. Historians and preservationists use builder's photos as official references to show the equipment as-built.\n\nBuilder's photos are commonly shot from an angle that shows one end, often the designated front end, and a full side of the car or locomotive. The rolling stock is normally positioned on a section of track with no other rolling stock coupled to it for the photograph. Sometimes the photograph was further processed to reduce the contrast of or even entirely remove the background to further highlight the rolling stock that was photographed.\n"}
{"id": "5152268", "url": "https://en.wikipedia.org/wiki?curid=5152268", "title": "Business waste", "text": "Business waste\n\nBusiness (or commercial and industrial) waste – cover the commercial waste and industrial waste types . Generally, businesses are expected to make their own arrangements for the collection, treatment and disposal of their wastes. Waste from smaller shops and trading estates where local authority waste collection agreements are in place will generally be treated as municipal waste.\n\n"}
{"id": "21844934", "url": "https://en.wikipedia.org/wiki?curid=21844934", "title": "CMS Computers", "text": "CMS Computers\n\nCMS Computers is a manufacturer of desktop, laptop and tablet computers based in Warrington, UK. It primarily trades under the brand Zoostorm. The company was founded in 1993, and acquired by VIP Group in 2011.\n\nIn a 2009 press release, CMS Computers cited Microsoft figures placing the company as the fifth largest manufacturer of desktop computers, laptops and servers in the UK, and the third largest manufacturer of Intel-based systems.\n\n"}
{"id": "24264072", "url": "https://en.wikipedia.org/wiki?curid=24264072", "title": "Carpuject", "text": "Carpuject\n\nThe carpuject is a syringe device for the administration of injectable fluid medication. It was patented by the Sterling Drug Company, which became the Sterling Winthrop, after World War II. It is designed with a luer-lock device to accept a sterile hypodermic needle or to be linked directly to intravenous tubing line. The product can deliver an intravenous or intramuscular injection by means of a holder which attaches to the barrel and plunger to the barrel plug. Medication is prefilled into the syringe barrel. When the plug at the end of the barrel is advanced to the head of the barrel it discharges and releases the contents through the needle or into the lumen of the tubing.\n\nThe carpuject competed with the tubex injection system developed by Wyeth. It has been redesigned several times to comply with sterility and infection controls standards.\n\nIn 1974, Sterling opened a manufacturing plant in McPherson, Kansas. In 1988 Kodak purchased Winthrop Labs and in 1994 sold the injectable drug division and all intellectual property rights to Sanofi, a French pharmaceutical company, now Sanofi Aventis. In 1997 Sanofi sold the injectable carpuject line of business to Abbott Laboratories of Abbott Park, IL for US$200 million. They added generic injectable drugs to the injectable line. In about 2004 Abbott separated its hospital supply line into a separate hospital supply company, Hospira from its drug division. The split placed all of Abbott's hospital products in a separate division. In 2015, Hospira, including the carpuject device, was purchased by Pfizer.\n"}
{"id": "14467636", "url": "https://en.wikipedia.org/wiki?curid=14467636", "title": "Celco", "text": "Celco\n\nCelco is a CRT film recorder manufacturing company. The company has developed xCRT advanced imaging technology that every Celco's recorders are using.\n\nTwo brands of Celco's film recorder, Fury and Firestorm 2X, are only two film recorders that supported to record 70 mm IMAX film format.\n\nObsolete stuff\n\nArrilaser – Arri's laser film recorder using laser light source.\n\n\n"}
{"id": "2700875", "url": "https://en.wikipedia.org/wiki?curid=2700875", "title": "Cisco Express Forwarding", "text": "Cisco Express Forwarding\n\nCisco Express Forwarding (CEF) is an advanced layer 3 switching technology used mainly in large core networks or the Internet to enhance the overall network performance. Although CEF is a Cisco proprietary protocol other vendors of multi-layer switches or high-capacity routers offer a similar functionality where layer-3 switching or routing is done in hardware (in an ASIC) instead of by software and the (central) CPU.\n\nCEF is mainly used to increase packet switching speed by reducing the overhead and delays introduced by other routing techniques. CEF consists of two key components: The Forwarding Information Base (FIB) and adjacencies.\n\nThe FIB is similar to the routing table generated by multiple routing protocols, maintaining only the next-hop address for a particular IP-route.\n\nThe adjacency table maintains layer 2 or switching information linked to a particular FIB entry, avoiding the need for an Address Resolution Protocol (ARP) request for each table lookup. There are several types of adjacencies. Some are listed below:\n\n\nIn order to take full advantage of CEF, it is recommended to use distributed CEF (dCEF), where there is a FIB table on each of the line cards. This avoids the need for querying the main processor or routing table in order to get the next-hop information. Instead, fast switching will be performed on the line card itself.\n\nCEF currently supports Ethernet, Frame Relay, ATM, PPP, FDDI, tunnels, and Cisco HDLC.\n\n"}
{"id": "216082", "url": "https://en.wikipedia.org/wiki?curid=216082", "title": "Climbing harness", "text": "Climbing harness\n\nA climbing harness is an item of climbing equipment for rock-climbing, abseiling, or other activities requiring the use of ropes to provide access or safety such as industrial rope access, working at heights, etc. A harness secures a person to a rope or an anchor point.\n\nIn its simplest form, a harness can be made from a length of rope or a nylon webbing tied round the waist. However this is extremely uncomfortable unless the wearer is very light. It can also ride up to the abdomen or even the diaphragm under load and cause serious injury. Looping the rope between the legs will prevent this, though care should be taken to avoid sensitive areas. More sophisticated harnesses exist in many patterns, designed to give greater comfort and security, and more options for carrying equipment.\n\nWhile harnesses can be improvised, it is more common to use commercially produced harnesses, which often include padding and amenities such as gear loops. Most commercial climbing harnesses meet the guidelines and manufacturing standards of organizations such as the Union Internationale des Associations d'Alpinisme (UiAA) or European Committee for Standardization.\n\nHarnesses should be attached to dynamic (stretchy) rope, except possibly when abseiling where the rope is always taut. Falling onto a system consisting entirely of static components e.g. slings over even a very short distance e.g. a metre or two is enough to deliver large forces to the body and possibly cause the equipment to fail altogether. For via ferrata, the harness is attached to lines via a shock absorber that can absorb some of the impact in the event of a fall.\n\nAttachment of the rope to the harness is done using a knot called a figure-eight follow through, and often backed up with a stopper knot. Other knots are possible for the purpose, primarily a variation of the double bowline, but the figure-eight is usually preferred. Although it is harder to untie after a fall, it is inherently more secure, easier to tie, and easier to verify that it has been tied correctly. There are many variations of the bowline knot, and some will untie themselves when repeatedly stressed and unstressed, as is common in climbing.\n\nIn addition to the weight bearing parts of the harness, there are parts of the harness that are not designed to be part of the safety system. These include the gear loops, used for carrying equipment e.g. protection devices, carabiners, etc., and the elastic cords which pass behind the buttocks for the purpose of keeping the leg loops from slipping down while not under load. Any attempt to tie the safety system into these components could lead to failure and an unprotected fall. It is acceptable to attach the rope to a leg loop via a Prusik knot and carabiner, but this should only be done with the brake end of the rope in an abseil system in order to keep the Prusik clear of the belay plate. This end of the rope experiences considerably lower forces than the \"live\" end, due to the action of the belay device.\n\nThe invention of the climbing harness has been attributed to Jeanne Immink, a Dutch climber in the late nineteenth century. Some of the first climbing harnesses were devised in the U.K. in the early 1960s by Alan Waterhouse, Paul Seddon and Tony Howard who went on to form the Troll climbing equipment manufacturers. A harness designed by British climber Don Whillans was made by Troll for the 1970 Annapurna South Face Expedition. It went into mass production shortly afterwards and soon became popular worldwide.\n\nThe sit or seat harness was invented in the 1960s by Yosemite climbers. The first innovation was the Swami Belt, which was multiple loops of webbing around the waist. Then quickly came the Swami Seat, a sit harness tied from webbing, including leg loops and an integrated waist loop. Once the seat/sit harness came to be, suppliers of climbing gear started making them with stitching replacing the knots. The Swami Seat was revealed to the climbing world thru an article in Summit Magazine in the mid-60s. Sewn harnesses came later.\n\nA sit harness consists of a waist belt and two leg loops which are normally connected in the front of the hips through a permanent webbing loop called a \"belay loop\". Belay loops are extremely strong, but nonetheless still a single point of failure that caused at least one notorious death. For rock climbing, the rope typically goes through the two \"tie-in loops\" that are above and below the \"belay loop\". The figure-eight knot is mostly used for rock climbing. These are the most commonly used harnesses for recreational activities such as abseiling and rock climbing, as they afford a wide range of movement while still maintaining a high level of safety. Ensuring the harness fits correctly is key to avoiding pain in the upper thigh area caused by the leg loops being too tight around the upper legs and groin area, while at the same time ensuring that a climber flipped over in a fall does not slip out. The waist belt should be tightened snugly. Leg loops should not be particularly tight, but for obvious reasons neither should they have enough slack to allow extraneous parts of a male climber to get caught inside them.\n\nA chest harness is worn around the shoulders, usually with a sit harness so as to provide an additional attachment point. This attachment point allows for better balance in some situations such as when carrying a heavy pack (as the centre of mass is below the connection to the rope) and when the person in the harness may be unable to maintain an upright position (due to injury or other influences).\n\nA full-body harness is the combination of a sit harness and a chest harness which are permanently or semi-permanently connected to each other. This kind of harness normally offers a wide range of attachment points. It is most commonly used in industrial/rescue situations, and also commonly used by small children instead of a sit harness which is easier to slip out of.\n\nIn a study conducted, researchers came to a conclusion that there was no statistically significant evidence revealing a pattern between harness type and severity of climbing accidents. Direct rock contact in rock climbing was the main reason for injury, not the type of climbing harness used.\n\nMost harnesses are generally made from webbing. This webbing is often nylon webbing as polyester webbing doesn't hold triglides and d-rings as well, and it's easier to untie nylon webbing. Specifically, Nylon 66 is the most common type of nylon used for this webbing. The webbing is often also tubular webbing, instead of flat webbing. Within a single harness, there are many different weaves of nylon based on the intended function of the specific part of the harness. These weaves sometimes include polyester. The buckles are typically made of anodized aluminum. For the leg loops and waist belt, companies use various methods to make the harness comfortable. The most common method is cushioning the harness with foam and mesh. However, for more lightweight harnesses, some companies use wider waist belts and unidirectional fibers going along the waist belt to distribute the weight evenly and minimize pressure points. Harness designers use increasingly advanced materials such as Ultra High Molecular Weight Polyethylene (UHMWPE), aramid fibers (Kevlar, Vectran, etc.), and sailcloth (e.g. the Black Diamond Vision harness) in order to make harnesses lighter and more comfortable. Different harnesses use different materials for these fibers across the leg loops and waist belt.\n\n\n"}
{"id": "29576408", "url": "https://en.wikipedia.org/wiki?curid=29576408", "title": "Construction Management Association of America", "text": "Construction Management Association of America\n\nThe Construction Management Association of America (CMAA) is a non-profit and non-governmental, professional association serving the construction management industry. The Association was formed in 1982. Current membership is more than 14,000, including individual CM/PM practitioners, corporate members, and construction owners in both public and private sectors, along with academic and associate members. CMAA has 29 regional chapters.\n\nCMAA published the most recent revision of its \"Construction Management Standards of Practice\" in 2015. The SOP has been published and updated regularly by CMAA since the 1980s. It outlines standards for professional CM services in the areas of:\n\nAccording to the Bureau of Labor statistics, there is a growing movement toward certification of construction managers. CMAA established a voluntary certification program for construction managers, known as the Certified Construction Manager (CCM) program.The Construction Manager Certification Institute (CMCI)was established by CMAA to oversee the organization's certification program. In 2006, the CCM program was accredited by the American National Standards Institute under the International Organization for Standardization's ISO 17024, which recognizes certification programs for conformity assessment or a \"demonstration that specified requirements relating to a product, process, system, person or body are fulfilled.\"\n\nThe CCM certification requires individuals to possess a requisite amount of experience and/or education. The eligibility requirements are:\n\n1. Forty-eight months' experience as a CM in the qualifying areas as defined by the Qualifications Matrix and 2. One of the following:\n\n\nIn addition, two reference letters from a client or owner are required; they can be from any two projects that a candidate is documenting as part of their 48-month requirement. Finally, the candidate must pass the certification exam.\n\nMost applicants are certified within 4 to 7 months of submitting their applications. The length of time it takes to become certified depends upon how quickly a candidate can submit a complete application including project documentation, how quickly the references respond, and how quickly a candidate can take and pass the CCM exam.\n\nIn 2013, the Construction Industry Institute at the University of Texas adopted the CCM as \"a value-adding credential\" following a joint effort to compare and harmonize CII's Construction Best Practices with the CMAA SOP, and to assure that the CCM examination measured and recognized mastery of the Best Practices.\n\nIn 2013, CMAA became a member society of ABET, formerly the Accreditation Board for Engineering Technologies, the leading organization accrediting undergraduate and graduate education programs in engineering and related fields. ABET has introduced a new accreditation for undergraduate programs in Construction Management, with program-specific criteria adopted in 2015.\n\nCMAA does host an advocacy program to represent the construction management industry before the United States Congress, federal agencies, state and local governments, and industry stakeholders. CMAA is active in several industry coalitions advocating investment in infrastructure and other industry interests.\n\nA designation CMAA awards to certain members who have proven to be industry leaders and \"who have made significant contributions to their organizations, the industry and their profession,\" according to CMAA. The College of Fellows meet consistently to discuss CM standards and issues facing the profession. The College also produces industry topic white papers and all members are particularly active in the association, often having served as past national presidents or in other leadership roles.\n\nThe association produces several publications, including the \"Construction Management Standards of Practice\" which defines the standards and practices of the construction management profession. Other CMAA publications include a bi-monthly newsletter, the \"CM Advisor\".\n\nCMAA hosts two industry events every year for the construction management profession.\n\nCMAA's signature event is its National Conference & Trade Show held every fall.The national conference offers multiple seminars which count toward CCM certification or renewal, as well as Continuing Education Units for a variety of purposes. The conference also includes an awards ceremony known as the Project Achievement Awards. CMAA designates a panel of judges to evaluate and select entries of projects with the most significant contributions to the construction management industry.\n\nCMAA's Capital Projects Symposium is held every spring. The Symposium is designed for as a high level exploration of trends and issues affecting the delivery of capital projects and programs.\n\nCMAA has also conducted regular surveys of owner attitudes, preferences, needs and expectations across the industry, along with research into CM/PM fees, adoption of technology, workforce concerns and other topics.\n\n"}
{"id": "2223177", "url": "https://en.wikipedia.org/wiki?curid=2223177", "title": "Criminal Minds", "text": "Criminal Minds\n\nCriminal Minds is an American police procedural crime drama television series created and produced by Jeff Davis. It premiered on September 22, 2005 and has run for fourteen seasons on CBS. It tells the story of a group of behavioral profilers who work for the FBI as members of its Behavioral Analysis Unit (BAU). The team focuses on profiling criminals, called the \"unsub\" or \"unknown subject\", as well as victimology, in investigation of crimes. The show follows the team as they work various cases and tackle their personal struggles.\n\nThe show has an ensemble cast, with Jason Gideon (Mandy Patinkin), Aaron Hotchner (Thomas Gibson), Elle Greenaway (Lola Glaudini), Derek Morgan (Shemar Moore), Spencer Reid (Matthew Gray Gubler), Jennifer Jareau (A. J. Cook), and Penelope Garcia (Kirsten Vangsness) as the original cast. Throughout the show's later seasons, the characters Emily Prentiss (Paget Brewster), David Rossi (Joe Mantegna), Ashley Seaver (Rachel Nichols), Alex Blake (Jeanne Tripplehorn), Kate Callahan (Jennifer Love Hewitt), Tara Lewis (Aisha Tyler), Luke Alvez (Adam Rodriguez), Stephen Walker (Damon Gupton), and Matt Simmons (Daniel Henney) were introduced.\n\n\"Criminal Minds\" has gained critical acclaim for its characterization, pacing, atmosphere, acting, directing, and writing. It has also become a ratings hit for CBS, regularly featuring as one of the network's most-watched cable shows throughout its decade-long run. The show's success has spawned a media franchise, with several spinoffs, including a South Korean adaptation and a video game. Following the conclusion of the thirteenth season, CBS renewed the show for a fourteenth season, which premiered on October 3, 2018.\n\nWhen the series premiered in September 2005, it featured FBI agents Jason Gideon, Aaron Hotchner, Elle Greenaway, Derek Morgan, Spencer Reid, Jennifer \"JJ\" Jareau, and Penelope Garcia. For season 1, Garcia was not a main cast member but rather had a recurring role, although she appeared in most episodes. In 2006, at the start of season 2, Lola Glaudini announced her departure from the show, as she wanted to return home to New York City. Paget Brewster replaced her in the role of Emily Prentiss.\n\nAt the start of season 3, Mandy Patinkin announced his departure from the show because he was deeply disturbed by the content of the series. He left letters of apology for his fellow cast members, explaining his reasons and wishing them luck. Joe Mantegna replaced him as David Rossi, a best-selling author and FBI agent who comes out of retirement. During season three, A. J. Cook became pregnant with her first child. Her pregnancy was written into the show. Cook's son, Mekhai Andersen, has been written into a recurring role as Jennifer's son Henry. Cook's void during maternity leave was filled by Meta Golding, who played Jordan Todd, an FBI agent who works with the agency's Counter Terrorism Unit. In season 6, JJ is forced to accept a promotion at The Pentagon, causing her to leave the BAU.\n\nLater that season, Emily is seemingly killed off. Although she survives, she does not appear for the rest of the season. Cook and Brewster were both replaced by Rachel Nichols as Ashley Seaver, an FBI cadet. CBS's decision to release Cook and Brewster from their contracts resulted in numerous fans writing angry letters to the studio and signing protest petitions. CBS rehired Cook and Brewster as Jennifer Jareau and Emily Prentiss, respectively; Nichols was released. In February 2012 Brewster announced her departure from the show after the seventh season. She was replaced in the eighth season by Jeanne Tripplehorn, who played Alex Blake, a linguistics expert. Later in season nine, Paget Brewster made a special guest appearance, reprising her role as Emily Prentiss in the 200th episode.\n\nAfter two seasons, Tripplehorn was released from the show. Former \"Ghost Whisperer\" star Jennifer Love Hewitt joined the cast as Kate Callahan, a former undercover FBI agent who joins the BAU. During season 10, Jason Gideon was killed off-screen. Executive producer and showrunner Erica Messer said CBS and ABC Studios were fine with the decision because it was clear that Patinkin would not come back again, but the show would feature him in a flashback if he were ever to return in the future. Following the conclusion of season 10, Hewitt and Cook announced that they will both be on hiatus from the show due to their pregnancies. Hewitt did not return for season 11 or any of the following ones, while Cook returned after the first seven episodes of season 11. Aisha Tyler, who plays Dr. Tara Lewis, joined the show at the start of season 11 in a recurring role, though she appeared in most episodes.\n\nLater that season, Shemar Moore, who plays Derek Morgan, left the show after 11 seasons. He had thought to leave in the previous season when his contract ended but was persuaded to stay to give his character a proper sendoff. Messer said the initial thought was for Moore to do six episodes, but when that didn't feel like enough, they settled on Moore doing the first 18 episodes of that season, and he departed in March 2016. He is replaced in the twelfth season by former \"\" star Adam Rodriguez, who plays Luke Alvez, a Fugitive Task Force Agent. A week after Moore left, Paget Brewster made her second special guest appearance. In season 12, Brewster once again became a series regular.\n\nOn August 10, 2016, it was announced that Aisha Tyler would be promoted to series regular for the 12th season. The next day, it was reported that Thomas Gibson, who portrays Aaron Hotchner, had been suspended from and written off the show for at least one, most likely two, episodes in the 12th season owing to an on-set altercation with one of the producers. The day after that, Gibson was fired from the program due to this incident. On September 30, 2016, it was announced that Gibson's character would be replaced by Damon Gupton, who will play Special Agent Stephen Walker, a seasoned profiler from the Behavioral Analysis Program (the counterintelligence division of the FBI) who will bring his spy-hunting skills to the BAU. On June 11, 2017, it was announced that Gupton had been fired from the show after one season. CBS said his departure was \"part of a creative change on the show.\"\n\nOn June 20, 2017, CBS announced that Daniel Henney, who was a series regular on \"\" as Matt Simmons, would join the main show as a series regular for the 13th season.\n\n\n\nThe first season of \"Criminal Minds\" received mixed reviews from critics. It has a Metacritic score of 42 based on 21 reviews, indicating \"mixed or average reviews\".\n\nDorothy Rabinowitz said, in her review for \"The Wall Street Journal\", \"From the evidence of the first few episodes, \"Criminal Minds\" may be a hit, and deservedly\" and gave particular praise to Gubler and Patinkin's performance. Ned Martel in \"The New York Times\" was less positive, saying, \"The problem with \"Criminal Minds\" is its many confusing maladies, applied to too many characters.\" The reviewer felt that \"as a result, the cast seems like a spilled trunk of broken toys, with which the audience—and perhaps the creators—may quickly become bored.\" \"The Chicago Tribune\" reviewer, Sid Smith, felt that the show \"may well be worth a look\", though he too criticized the \"confusing plots and characters.\" Writing in \"PopMatters\", Marco Lanzagorta criticized the show after its premiere, saying it \"confuses critical thinking with supernatural abilities\" and that its characters conform to stereotypes. In the \"Los Angeles Times\", Mary McNamara gave a similar review, and praised Patinkin and Gubler's performances.\n\nIn 2016, a study by \"The New York Times\" of the 50 TV shows with the most Facebook Likes found that \"like several of the other police procedurals\", \"Criminal Minds\" \"is more popular in rural areas, particularly in the southeastern half of the country. It hits peak popularity in Alabama and rural Tennessee and is least popular in Santa Barbara, Calif.\"\n\nThe series is in syndication on the A&E Network and Ion Television, as well as on We TV and Sundance TV. Early seasons of the show have also begun airing on Rewind Networks's HITS TV channel in Southeast Asia, Hong Kong, and Taiwan.\n\n\"Criminal Minds\" is typically ranks in the top-ten in Digital video recorder (DVR) playback, drawing in a range of 2 to 3 million viewers, according to Nielsen prime DVR lift data. For the week of October 10, 2010, the show peaked at sixth in DVR playback, gaining 2.40 million viewers, while also ranking seventh in demo playback (1.0 demo).\n\n\"Criminal Minds\" has produced two spin-offs: \"\" and \"\", as well as a video game.\n\nThe Season 5 episode, \"The Fight\", introduced a second BAU team and launched a new series called \"Criminal Minds: Suspect Behavior\". The spin-off series debuted February 16, 2011, on CBS but was canceled after a short 13-episode season owing to low ratings. On September 6, 2011, CBS DVD released The Complete Series on a four-disc set. It was packaged as \"The DVD Edition\".\n\nThe cast features Forest Whitaker as the lead role of Sam Cooper; including Janeane Garofalo, Michael Kelly, Beau Garrett, Matt Ryan, Richard Schiff, and Kirsten Vangsness, who reprises her role as Penelope Garcia from the original series.\n\nA proposed new series in the Criminal Minds franchise to be named \"Criminal Minds: Beyond Borders\" was announced in January 2015. Former \"\" star Gary Sinise (who is also a producer on the show) and Anna Gunn were cast in the lead roles of and Lily Lambert, though the latter departed after the backdoor pilot. Tyler James Williams has been cast as Russ \"Monty\" Montgomery and Daniel Henney as Matt Simmons, with Alana de la Garza as Clara Seger and Annie Funke as Mae Jarvis further being cast as series regulars.\n\nThe series follows the FBI agents of the International Response Team (IRT) helping American citizens who find themselves in trouble abroad. CBS aired a backdoor pilot on April 8, 2015 in the \"Criminal Minds\" slot, with a crossover episode titled \"Beyond Borders\". The second spin-off series debuted March 16, 2016, on CBS. On May 16, 2016, CBS renewed the series for a second season. On May 14, 2017, CBS canceled the series after two seasons due to low ratings.\n\nIn 2017, tvN launched their own Korean version of \"Criminal Minds\". The episodes are based on the original American version after its third season. On the cast is Lee Joon-gi as Kim Hyun-joon (Derek Morgan), Moon Chae Won as Ha Sun-woo (Emily Prentiss), Son Hyun-joo as Kang Ki-hyung (Aaron Hotchner), Yoo Sun as Nana Hwang (Penelope Garcia), Lee Sun-bin as Yoo Min-young (Jennifer Jareau), and Go Yoon as Lee Han (Spencer Reid).\nThe episodes are 1 hour long.\n\nCBS announced in October 2009 that Legacy Interactive would develop a video game based on the show. The game would require players to examine crime scenes for clues to help solve murder mysteries. The interactive puzzle game was released in 2012, but the show's cast was not involved with the project so it did not feature any of their voices.\n\nThe second game, developed by FTX Games was released on October 29, 2018 on Android & iOS devices.\n\n"}
{"id": "3213527", "url": "https://en.wikipedia.org/wiki?curid=3213527", "title": "Cutaway (industrial)", "text": "Cutaway (industrial)\n\nA cutaway, in the industrial sense, refers to the display of a manufactured product, (an engine, a pump, a regulator, etc. . .) where a portion of the exterior housing has been removed to reveal the internal components, (pistons, bearings, seals, etc. . .) and their relationship to the functionality of the product.\n\nCutaways are typically used in product training, trade show environments, museum displays and for many additional applications. Cutaways are produced using a variety of methods by the manufacturer, by a cutaway service company, as mentioned above, or by an experienced machine shop. \n\nWhile 3D modeling and CAD (Computer Aided Drafting) programs continue to improve and bring us more features and benefits, the cutaway will continue to show the product as it appears in the real world, using actual parts and components to show relationships and functionality.\n"}
{"id": "3451068", "url": "https://en.wikipedia.org/wiki?curid=3451068", "title": "DO-212", "text": "DO-212\n\nDO-212 is a performance standard published by RTCA, Incorporated. It contains Minimum Operational Performance Standards (MOPS) for aircraft equipment required for the Automatic Dependent Surveillance (ADS) function (ADSF). The supporting hardware can be a stand-alone ADS unit (ADSU) or alternatively, the ADS function may be installed within other on-board equipment.\n\n\n"}
{"id": "9924990", "url": "https://en.wikipedia.org/wiki?curid=9924990", "title": "Digital textbook", "text": "Digital textbook\n\nA digital textbook is a digital book or e-book intended to serve as the text for a class. Digital textbooks may also be known as e-textbooks or e-texts. Digital textbooks are a major component of technology-based education reform. They may serve as the texts for a traditional face-to-face class, an online course or degree, or massive open online courses (MOOCs).\n\nThere are many potential advantages to digital textbooks. They may offer lower costs, make it easier to monitor student progress, and are easier and cheaper to update when needed. Open source e-textbooks may offer the opportunity to create free, modifiable textbooks for basic subjects, or give individual teachers the opportunity to create e-texts for their own classrooms. They may offer better access to quality texts in the developing world. For this reason, many schools and colleges around the world have made the implementation of digital textbooks a central component of education policy. For example, in South Korea, reading materials in all public schools will be digitized by 2015. In the United States, the Federal Communications Commission aims for every student to be able to access e-texts by 2017.\n\nHowever, the transition to e-textbooks is costly, complex and controversial. Students express a strong preference for printed materials in many surveys and across cultures. Many interconnected factors, from device access, to digital literacy, to teaching methods affect the implementation of digital textbooks in the classroom. Issues of overall value, book quality, privacy, and intellectual property have yet to be resolved.\n\nBecause digital textbooks must be accessed through an electronic device, such as a laptop or e-reader, schools and colleges must determine how to provide access to all students. Many school districts are now offering \"one-to-one\" technology programs, in which a tablet or laptop is issued to each student. This ensures that all of the devices meet the same requirements (such as memory or software) and that all the devices can be networked, monitored and upgraded together. However, the one-to-one model also imposes significant costs on school districts, and brings up issues of privacy and personal use.\n\nAn alternative to one-to-one is to ask students to use their own electronic devices in class. This is called Bring Your Own Device (BYOD) or, sometimes, Bring Your Own Technology (BYOT). BYOD allows students to choose their preferred device for studying. Compared to one-to-one, it decreases the technology and maintenance costs for institutions. But not all students' devices may be compatible with the digital textbooks require for a class, and the devices may not be able to network with each other. A BYOD approach may also count out students who cannot afford a computer, e-reader or smartphone.\n\nA major selling point of digital textbooks is that they offer the opportunity for students to access multimedia content, such as embedded videos, interactive presentations and hyperlinks. Tests and other assessments can be included in the textbook, classmates can work together, and student progress can be tracked. Touchscreen technology offers students the chance to participate in projects, research or experiments. This may offer a different or better learning experience than printed textbooks. Digitization also promises to offer improved access to textbooks for student with disabilities. For example, high-contrast displays, or text-to-speech programs can help visually impaired students use the same textbooks their classmates use. The creation of interactive and customizable content is an important part of digital textbook development. Interactive digital content is costly to produce, however, and research on learning outcomes is still in the preliminary stages.\n\nThe concepts of open access and open source support the idea of open textbooks, digital textbooks that are free (gratis) and easy to distribute, modify and update (libre). Schools, teachers or professors may design their own open textbooks by gathering open access scholarly articles or other open access resources into one text or one curriculum. Open textbooks offer affordable access, especially to basic and common information, and pose a challenge to traditional models of textbook publishing. Modifiable or community edited textbooks may also be difficult to establish as credible or scholarly sources.\n\nOther models for digital textbook publishing are more traditional. Textbook publishers may offer digital textbooks or digital curriculums that are standardized across classrooms, easier to update, and compliant with national standards, teaching methods or goals. This approach also offers pitfalls. License or renewal fees for digital textbooks may impose unexpected costs for institutions. For example, in 2013, the LA Unified School District announced that it would face an additional $60 million to license the curriculum for its one-to-one iPad program.\n\nThough many governments and school districts are making large investments in digital textbooks, adoption is slow. According to data from Bowker Market Research, in the spring semester of 2013, only 3% of college students used a digital textbook as their primary course material. In multiple studies, strong majorities of college students, teens, and children continue to express a preference for printed books. Furthermore, there is conflicting information about how digital textbooks affect learning, cognition and retention. However, students are growing more exposed to digital textbooks, and new research suggests that student performance is about the same whether students work from digital or printed texts.\n\n"}
{"id": "11921210", "url": "https://en.wikipedia.org/wiki?curid=11921210", "title": "EEWH", "text": "EEWH\n\nEEWH is the green building certification system in Taiwan. EEWH comprises nine indicators that fall into four categories - ecology, energy saving, waste reduction and health - hence the name EEWH. The system was launched in 1999.\n\nSince 2003, the indicators have been foliage; water soil content (infiltration and retention); energy savings (for the building envelope, lighting and HVAC); emissions reduction; construction waste reduction; water conservation; garbage and sewage improvements; biodiversity; and indoor environmental quality.\n\nEEWH has five levels: certified, bronze, silver, gold, and diamond. As of May 2008, one building (Beitou Public Library) had been rated diamond level, and one gold.\n\nEEWH is not equivalent to LEED (Leadership in Energy and Environmental Design Green Building Rating System) in the United States, CASBEE (Comprehensive Assessment System for Building Environmental Efficiency) in Japan, and HQE in France.\n\n"}
{"id": "50232013", "url": "https://en.wikipedia.org/wiki?curid=50232013", "title": "ELearning Africa", "text": "ELearning Africa\n\neLearning Africa is a three-day annual international conference on ICT-enhanced education, training and skills development in Africa which is organised by ICWE GmbH. Each year the event is hosted and co-organised by a different African government. It has been opened on previous occasions by Presidents, Vice Presidents and Prime Ministers of several African countries including Hage Geingob, Abdoulaye Wade, George Kunda, Edward Ssekandi, Pascal Koupaki, Mohamed Gharib Bilal and Debretsion Gebremichael. This pan-African conference focuses on the use of ICT to support education, training, skills and knowledge sharing across all sectors of Africa, enhancing sustainable development goals across the continent and enabling participants to develop multinational and cross-industry contacts and partnerships, as well as to build up their expertise and abilities. \n\nThe conference series was inaugurated at the United Nations Conference Centre in Addis Ababa, Ethiopia in 2006 and has since visited Kenya, Ghana, Senegal, Zambia, Tanzania, Benin, Namibia, Uganda, Ethiopia, Egypt and Mauritius. Some of the Keynote speakers at the 2016 conference included Ismail Serageldin, Thierry Zomahoun, Günter Nooke, Toyosi Akerele-Ogunsiji and Toby Shapshak. eLearning Africa 2018 will be held from 26 - 28 September at the Kigali Convention Centre located in Kigali, Rwanda.\n\nDelegates are decision-makers from the education, business and government sectors. eLearning Africa brings together experts and practitioners, as well as suppliers of education and training solutions.\n\nOver 12 consecutive years, eLearning Africa has hosted 16,228 participants from more than 100 countries, with over 85% coming from the African continent in 12 different locations (Ethiopia, Kenya, Ghana, Senegal, Zambia, Tanzania, Benin, Namibia, Uganda, Ethiopia for the 10 anniversary, Egypt, Mauritius and now Rwanda). More than 3,300 speakers have addressed the conference about every aspect of technology enhanced education and skills development. \n\nWith a population of one billion people, 41 percent of which are under the age of 15, the African continent is the hub for development in using ICT to expand access to learning and skills acquisition, and to improve the quality of learning and teaching. ICT supported learning has a widespread impact across education, training and professional development, with governments and ministries throughout the continent investing resources in support of IT development in schools and skills training. \neLearning Africa has previously partnered with organisations such as the African Union, ECOWAS, GIZ, UNECA, UNESCO-UNEVOC and the African Development Bank and the conference has been sponsored on different occasions by companies such as Microsoft, Google, Intel and Nokia.\n\nThe three-day conference is held in English and French. It includes three Plenary sessions and a Plenary debate, presenting the experiences, research, best practices, thinking and expertise that make up the complex picture of ICT for education, training and skills development in Africa today. On the second and third days of the conference, more than 200 speakers in around sixty-four smaller breakout sessions give presentations exploring further issues in Africa such as access to learning and vocational training, equality and quality in education, skills and employability, health, literacy and governance. \nThe sessions take a variety of different forms including Applied Practice, Discovery Demo, Learning Cafés and Knowledge Exchange sessions. In addition to the main programme, a number of special events take place alongside the conference, such as hackathons, product launches, sponsored workshops and best practice showcases. An exhibition is held throughout the conference in which exhibitors showcase their products and services and network with the conference delegates. \n\nAfrican Ministers and high-level ministerial representatives attend the Ministerial Round Table (MRT) discussions which take place parallel to the conference before the Opening Plenary on the first day. The MRT discussions are a closed meeting on ICT for development, education and training and are held under a different theme each year.\n\nThe eLearning Africa Report provides an annual overview of the state of eLearning in Africa and considers the impact technology is having on education and development throughout the continent. The report includes surveys and country guides as well as features, news and opinion pieces from a variety of authors. The report is free to download and is published every year around the time of the eLearning Africa conference. \n\nIn addition to this, a regular e-newsletter is delivered to a database of more than 40,000 international readers, sharing the latest news, perspectives and trends in ICT for development and learning in Africa.\n\n"}
{"id": "948168", "url": "https://en.wikipedia.org/wiki?curid=948168", "title": "Emily Warren Roebling", "text": "Emily Warren Roebling\n\nEmily Warren Roebling (September 23, 1843 – February 28, 1903) is known for her contribution to the completion of the Brooklyn Bridge after her husband Washington Roebling developed caisson disease (a.k.a. decompression disease). Her husband was a civil engineer and the chief engineer during the construction of the Brooklyn Bridge.\n\nEmily was born to Sylvanus and Phebe Warren at Cold Spring, New York, on September 23, 1843. She was the second youngest of twelve children. Emily’s interest in pursuing education was supported by her older brother Gouverneur K. Warren. The two siblings always held a close relationship. She attended school at the Georgetown Visitation Academy in Washington DC.\n\nIn 1864, during the American Civil War, Emily visited her brother, who was commanding the Fifth Army Corps (a.k.a. V Corps), at his headquarters. At a solider's ball that she attended during the visit, she became acquainted with Washington Roebling, the son of Brooklyn Bridge designer John A. Roebling, who was a civil engineer serving on Gouverneur Warren's staff. Emily and Washington married in a dual wedding ceremony (alongside another Warren sibling) in Cold Spring on January 18, 1865.\n\nAs John Roebling was starting his preliminary work on the Brooklyn Bridge, the newlyweds went to Europe to study the use of caissons for the bridge. In November 1867, Emily gave birth to the couple's only child, John A. Roebling II, while living in Germany.\n\nOn their return from their European studies, Washington's father died of tetanus following an accident at the bridge site, and Washington took charge of the Brooklyn Bridge's construction as chief engineer. As he immersed himself in the project, Washington developed decompression sickness, which was known at the time as \"caisson disease\". It affected him so badly that he became bed-ridden. Emily stepped in as the \"first woman field engineer\" and saw out the completion of the Brooklyn Bridge.\n\nAs the only person to visit her husband during his sickness, Emily was to relay information from Washington to his assistants and report the progress of work on the bridge. She developed an extensive knowledge of strength of materials, stress analysis, cable construction, and calculating catenary curves through Washington's teachings. Emily's knowledge was complemented by her prior interest in and study of the bridge's construction upon her husband's appointment to chief engineer. For the decade after Washington took to his sick bed, Emily's dedication to the completion of the Brooklyn Bridge was unyielding. She took over much of the chief engineer duties, including day-to-day supervision and project management. Emily and her husband jointly planned the bridge's continued construction. She dealt with politicians, competing engineers, and all those associated with the work on the bridge to the point where people believed she was behind the bridge's design.\n\nIn 1882, Washington's title of chief engineer was in jeopardy because of his sickness. In order to allow him to retain his title, Emily went to gatherings of engineers and politicians to defend her husband. To the Roeblings' relief, the politicians responded well to Emily's speeches, and Washington was permitted to remain chief engineer of the Brooklyn Bridge.\n\nThe Brooklyn Bridge was completed in 1883. In advance of the official opening, carrying a rooster as a sign of victory, Emily Roebling was the first to cross the bridge by carriage. At the opening ceremony, Emily was honored in a speech by Abram Stevens Hewitt, who said that the bridge was\nUpon completion of her work on the Brooklyn Bridge, Emily invested her time in several women's causes including Committee on Statistics of the New Jersey Board of Lady Managers for the World's Columbian Exposition, Committee of Sorosis, Daughters of the American Revolution, George Washington Memorial Association, and Evelyn College. This occurred when the Roebling family moved to Trenton, New Jersey. Emily also participated in social organizations such as the Relief Society during the Spanish–American War. She traveled widely—in 1896 she was presented to Queen Victoria, and she was in Russia for the coronation of Tsar Nicholas II. She also continued her education and received a law certificate from New York University.\n\nRoebling is also known for an influential essay she authored, \"A Wife's Disabilities,\" which won wide acclaim and awards. In the essay, she argued for greater women's rights and railed against discriminatory practices targeted at women. Until her death on February 28, 1903, she spent her remaining time with her family and kept socially and mentally active.\n\nToday the Brooklyn Bridge is marked with a plaque dedicated to the memory of Emily, her husband Washington Roebling, and her father-in-law John A. Roebling.\n\nIn 2018 \"The New York Times\" published a belated obituary for Emily.\n\n\n\n"}
{"id": "21977757", "url": "https://en.wikipedia.org/wiki?curid=21977757", "title": "Flying syringe", "text": "Flying syringe\n\nFlying syringe is a phrase that is used to refer to proposed, but not yet created, genetically modified mosquitoes that inject vaccines into people when they bite them.\n\nIn 2008 the Gates Foundation awarded $100,000 to Hiroyuki Matsuoka of Jichi Medical University in Japan to do research on them, with a condition that any discoveries that were funded by the grant must be made available at affordable prices in the developing world. If Matsuoka proves that his idea has merit, he will be eligible for an additional $1 million of funding. \"The Washington Post\" referred to flying syringes as a \"bold idea\".\n"}
{"id": "35933173", "url": "https://en.wikipedia.org/wiki?curid=35933173", "title": "Foba", "text": "Foba\n\nFOBA AG is a Swiss company that makes photographic studio equipment.\n\nIn 1939, just after World War II started in Europe, imports of photography products such as lights and dry presses from Germany was reduced to a trickle. Walter Friedrich, a precisions engineer, together with his wife, began to manufacture commodities for professional photographers in their rented home's small basement. He called his startup Fotogeräte- und Elektroapparate-Bau (\"Manufacturing of photographic and electrical equipment\"), or \"FOBA\". To save on shipping costs and to keep up the personal contact to his customers, Friedrich at first delivered products personally by bicycle.\n\nThe company also supplied cooking plates with integrated heating coils. This was a first in Switzerland.\n"}
{"id": "33513029", "url": "https://en.wikipedia.org/wiki?curid=33513029", "title": "Ghacks", "text": "Ghacks\n\nGhacks technology news is a blog created by Martin Brinkmann in October 2005. Its primary focus are web browser and Windows tips, software, guides and reviews.\n\nAn average of five posts are published each day of the week with topics ranging from Windows and Linux operating system news to web browser tips (focusing on Firefox, Internet Explorer, Google Chrome and Opera), online services like Gmail or Hotmail (Outlook.com) to general tech news and tips.\n\nPopular posts include login related troubleshooting guides like Gmail or Facebook.\n\nGhacks Technology News has been recognized by Lifehacker, Gizmodo, Donationcoder.com and several other reputable news sites and blogs for its coverage of freeware and open source software.\n\nThe website receives more than 400,000 visitors per day as of January 2017. The site has an Alexa traffic rank of 12,042 worldwide and 6,379 in the United States.\n\nTechnorati listed Ghacks as a top 100 blog in both the general technology and info technology category. As of October 2011 it occupies position 24 in the Info technology category and 68 in the overall technology category on Technorati.\n\nThe editor in chief and founder is Martin Brinkmann.\nAll authors that contribute articles for the site are listed in the footer area on the website.\n\nGhacks was created in 2005 as a development blog for a software called Google Hacks. Trademark issues made the founder of the site pick ghacks as the domain name. The software was soon thereafter discontinued and Ghacks turned from a development type blog to a software and online news oriented blog.\n\nThe site has held a Christmas Giveaway in the past three years which is an annual event where commercial software is being given away to readers of the site. This has since then been picked up by other tech blogs who are now also holding giveaways in the Christmas period as well.\n\nGhacks Technology News has received several recommendations in the past. This includes recommendations by DL.TV, Diggnation and KNLS Radio's \"Eye On The Web\".\n\nArticles are often republished on popular sites such as Lifehacker, The Blog Herald or Gizmodo.\n\n"}
{"id": "50148878", "url": "https://en.wikipedia.org/wiki?curid=50148878", "title": "Grace Banker", "text": "Grace Banker\n\nGrace D. Banker (October 25, 1892 – September 17, 1960) was a telephone operator who served during World War I (1917–1918) as chief operator of mobile for the American Expeditionary Forces (AEF) in the U.S. Army Signal Corps. She led thirty-three women telephone operators known popularly as Hello Girls. They were assigned in New York to travel to France to operate telephone switch boards at the war front in Paris, and at Chaumont, Haute-Marne. They also operated the telephone switch boards at First Army headquarters at Ligny-en-Barrois, about to the south of Saint-Mihiel, and later during the Meuse-Argonne Offensive. After her return to civilian life, Banker and her team members were treated as citizen volunteers and initially not given recognition as members of the military. In 1919, Banker was honoured with the Distinguished Service Medal for her services with the First Army headquarters during the St. Mihiel and Meuse-Argonne Offensives, with a commendation.\n\nBanker was born at Passaic, New Jersey on 25 October 1892. After graduating from Barnard College she joined American Telephone and Telegraph Company (AT&T) where she worked as a switchboards instructor. During World War I Banker was chosen to head a team of thirty-three telephone operators of Telephone Unit No. 1, for telephone operations, assigned to war duty in France. This was the first group of women who were given the popular name Hello Girls.\n\nBanker sailed with her team members from New Jersey on 6 March 1918, to take up the assignment as chief operator for First Army headquarters in Paris. After arriving with her team in England, the group set sail by ferry across the English Channel. However, bad weather, in the form of thick fog, prevented the ferry from reaching French shores, and it had to be anchored a few miles away to wait for the fog to lift. This location made the vessel an easy target for German bombing (at that time, one vessel out of four had suffered bombing), and the team members remained at full readiness to evacuate the vessel at short notice. The women's group stayed on deck in the open for forty-eight continuous hours. This situation did not dishearten Banker or her team members, and, as Banker later said: \"What good sports girls were in that First Unit! They took everything in their stride. They were the pioneers.\" \n\nOn arrival in Paris, Banker and her team were posted to the headquarters of the Advance Section in Chaumont sur Haute Marne, which was then the headquarters of General John J. Pershing. Five months later, Baker was asked to move to the war front, to the First Army headquarters at Ligny-en-Barrois, south of Saint-Mihiel. On 25 August 1918, she moved to the war front with only five operators helping her. For this operation at Saint-Mihiel, Banker had to make a choice of the best operators for the job, she selected: Suzanne Prevot, Esther Fresnel, Helen Hill, Berthe Hunt, and Marie Lange. Equipped with gas masks and helmets, the women operated from trenches where the danger was real; despite this, those not chosen to go felt left out.\n\nDuring offensive operations at Saint-Mihiel, though artillery bombing was in force, Banker and her team of operators manned the switchboards. When the First Army headquarters moved to Bar-le-Duc in September, Banker and her operators had to work in a place which was damaged extensively. They operated even under heavy bombing by German planes, but no team members were injured. They worked under severe weather conditions without heating, and their barracks leaked, and were later gutted, making conditions even harsher.\nFollowing the Armistice of 11 November 1918, fighting ceased. Banker and her team were then ordered to return to Paris. In Paris, Banker was deputed initially to work at the temporary residence of President Woodrow Wilson. As she did not find this job exciting compared to the work at the war front, she accepted an offer to move to the Army of Occupation at Coblenz, Germany; while there she was awarded the Distinguished Service Medal. \n\nAfter working for twenty months at the war front, in September 1919 Banker and the rest of her team returned home. General Edgar Russel, chief signal officer of the AEF, extolled their service as \"indispensable\". Reminiscing about her wartime experience as chief operator, Banker humorously noted that \"an afternoon in the switchboard office sometimes sounded like a scene from \"Alice in Wonderland\", where only the initiated can make sense of the proceedings\". She also noted the confidentiality aspect of her assignment when she was tested by an intelligence officer about her ability to keep a secret, which was about her posting out of the unit. About her work at the war front she said that \"the secrecy surrounding their operations gave it an aura of romance and set it apart from the civilian work\". After returning from the war front with her team, she reflected: \"We missed the First Army with its code of loyalty and hard work. We were back in the petty squabbles of civilian life where even chief operators had 'tantrums' and where the wives of civilians attached to the Peace Conference spilled all over Paris in Army cars.\"\n\nAfter the war, when they returned to civilian life, Banker and her team members were treated as citizen volunteers and not recognized as members of the military. They were not given a \"formal discharge or even a certificate of service\". Banker died on 17 September 1960, in Scarsdale, New York. In 1977 Congress enacted legislation that gave due recognition to Banker and her team, and treated them as \"veterans\".\n\nOn 26 May 1919 with Government order no. 70, Banker was honoured with the Distinguished Service Medal for her services with the First Army headquarters during the St. Mihiel and Meuse-Argonne Offensives, with the commendation which read: \"For exceptional ability... [and] untiring devotion to her exacting duties under trying conditions...to assure the success of the telephone service during the operations of the First Army against the Saint Michel salient and the exertions to the north of Verdun.\"\n\nBibliography\n"}
{"id": "56056501", "url": "https://en.wikipedia.org/wiki?curid=56056501", "title": "Groupe F", "text": "Groupe F\n\nGroupe F is a production company specialised in the design and performance of live shows and pyrotechnic events. It operates on five continents.\n\nFounded in 1990 in Bessèges (Gard, F) by François Montel, Alain Burkhalter and Didier Mandin, Groupe F takes off internationally in 1992 with the arrival of Éric Noel, Nicolas Mousques, Caroline and Christophe Berthonneau, designing the pyrotechnic effects of the Barcelona Summer Olympics closing ceremony. In 1993, Groupe F goes on a world tour with its show \"Oiseaux de Feu\" (Birds of Fire), followed by \"Un peu plus de Lumière\" and also performs the closing fireworks of the 1998 FIFA World Cup.\n\nOn 31 December 1999, Groupe F stages the pyrotechnic show on the Eiffel Tower celebrating the transition to the year 2000. Building on the worldwide success of the event, the group recruits a multidisciplinary team for the artistic and technical implementation of its major projects. Jonas Bidaut, Cédric Moreau, Eric Travers and Jeff Yelnik have supported the international development of Groupe F ever since.\n\nIn 2000, Groupe F starts exploring new scenographic territories and creates tools for monumental shows, combining light, video mapping, music, fire and human performers. The Palace of Versailles, the Pont du Gard and the Eiffel Tower host these new creations on several occasions.\n\nIn November 2017, the Ministry of Culture entrust the company with the opening show of the Louvre Abu Dhabi.\n\n\n\n\n"}
{"id": "35488855", "url": "https://en.wikipedia.org/wiki?curid=35488855", "title": "Head-carrying", "text": "Head-carrying\n\nCarrying on the head is a common practice in many parts of the world, as an alternative to carrying a burden on the back, shoulders and so on. People have carried burdens balanced on top of the head since ancient times, usually to do daily work, but sometimes in religious ceremonies or as a feat of skill, such as in certain dances.\n\nThe practice of carrying a burden on top of the head has existed since ancient times. Evidence of this is in the Book of Proverbs, which makes a reference to it in verses 25:21-22, \"If your enemy is hungry, give him food to eat; if he is thirsty, give him water to drink. In doing this, you will heap burning coals on his head, and the Lord will reward you\", which is using the metaphor of an ancient Egyptian ritual of repentance which involved carrying a basin of burning coals on top of the head. The verse in the Book of Proverbs is estimated to originate from the period of the third century BCE. According to an account by the ancient Egyptian writer Cha-em-wese, a thief returned a book stolen from a grave carrying such a pan of hot coals on top of his head, to show \"his consciousness and attitudes of shame, remorse, repentance, and ultimately correction\".\n\nCarrying on the head is common in many parts of the developing world, as only a simple length of cloth shaped into a ring or ball is needed to carry loads approaching the person's own weight. The practice is efficient, in a place or at a time when there are no vehicles available for carrying burdens.\n\nAfrican-American women, such as Alaiya, continued the practice during the 19th century, which they learned from their elders who had been enslaved from Africa. One observer during the American Civil War noted seeing the impressive sense of balance and dexterity that the practice gave women in South Carolina: \"I have seen a woman, with a brimming water-pail balanced on her head, or perhaps a cup, saucer, and spoon, stop suddenly, turn round, stoop to pick up a missile, rise again, fling it, light a pipe, and go through many revolutions with either hand or both, without spilling a drop\". Until the turn of the 20th century, African-American women in the Southern states continued carrying baskets and bundles of folded clothes on top of their heads, when they found work as washerwomen, doing laundry for white employers. This practice ended when the automobile became common in affluent communities, and employers began delivering the clothing to the homes of the washwomen, rather than the washwomen walking to the employers' homes.\n\nToday, women and men may be seen carrying burdens on top of their heads where there is no less expensive, or more efficient, way of transporting workloads. In India, women carry baskets of bricks to workmen on construction sites. It is also used by the lowest caste to carry away human waste that they scoop out of pit latrines, the practice of manual scavenging.\n\nIn East Africa, Luo women may carry loads of up to 70% of their own body weight balanced on top of their heads. Women of the Kikuyu tribe carry similar heavy loads, but using a leather strap wrapped around their forehead and the load to secure it while it is carried. (see tumpline) This results in a permanent groove in the forehead of the women. However, there is no evidence of other harmful effects on the health of women who carry heavy loads on top of their heads. Researchers speculate that training from a young age may explain this. Up to 20% of the person's body weight can be carried with no extra exertion of energy. Other researchers have shown that African and European women carrying 70% of their body weight in controlled studies used more oxygen while head-carrying, in contrast to carrying a load on their backs. The research did not support the notion that head-loading is less exerting than carrying on the back, \"although there is some evidence of energy saving mechanisms for back-loading at low speed/load combinations\".\n\nWomen in particular may have practical reasons for carrying on the head, as for many African women it is \"well-suited to the rough, rural terrain and the particular objects they carry—like buckets of water and bundles of firewood\", then abandoning the practice when they migrate to urban areas where their daily routines, and socially accepted practices, are different. In Ghana, affluent residents of the southern cities employ young women who migrate from the poorer northern region to work as \"head porters\", called \"kayayei\", for $2 a day.\n\nThere are several traditional dances of West African cultures that include balancing an object on the head as a skillful feat. Ritual dancing among worshippers of the thunder deity, Shango, sometimes balance a container of fire on their heads while dancing. The Egbado Yoruba have dances that include balancing \"delicate terracotta figures\" on the head while the arms and torso are moving. This tradition continued among Africans taken to America during the Atlantic slave trade. African-Americans in the 19th century had a popular type of dance competition called \"set the floor\" (\"set de flo'\"), during which individual dancers would take turns dancing. Competing dancers would try to perform complicated steps given to them by a caller (usually a fiddler), without stepping outside the bounds of a circle drawn on the ground. To add to the challenge, some dancers would compete while balancing a glass full of water on top of their heads, trying not to spill the water while they danced.\n\nDuring the Victorian era, when finishing schools for young women were at their peak and manners and comportment were more rigid, young women were sometimes instructed to improve their posture by balancing books or a teacup and saucer on their heads while walking and getting up or down from a chair. They were told to model themselves after \"the Egyptian water-carrier, with the jug of water poised so prettily on her head, and her figure so straight and beautiful\".\n\n"}
{"id": "315963", "url": "https://en.wikipedia.org/wiki?curid=315963", "title": "Heading indicator", "text": "Heading indicator\n\nThe heading indicator (also called an HI) is a flight instrument used in an aircraft to inform the pilot of the aircraft's heading. It is sometimes referred to by its older names, the directional gyro or DG, and also (UK usage) direction indicator or DI.\n\nThe primary means of establishing the heading in most small aircraft is the magnetic compass, which, however, suffers from several types of errors, including that created by the \"dip\" or downward slope of the Earth's magnetic field. Dip error causes the magnetic compass to read incorrectly whenever the aircraft is in a bank, or during acceleration or deceleration, making it difficult to use in any flight condition other than unaccelerated, perfectly straight and level. To remedy this, the pilot will typically manoeuvre the airplane with reference to the heading indicator, as the gyroscopic heading indicator is unaffected by dip and acceleration errors. The pilot will periodically reset the heading indicator to the heading shown on the magnetic compass.\n\nThe heading indicator works using a gyroscope, tied by an erection mechanism to the aircraft yawing plane, i. e. the plane defined by the longitudinal and the transverse axis of the aircraft. As such, any configuration of the aircraft yawing plane that does not match the local Earth horizontal results in an indication error. The heading indicator is arranged such that the gyro axis is used to drive the display, which consists of a circular compass card calibrated in degrees. The gyroscope is spun either electrically, or using filtered air flow from a suction pump (sometimes a pressure pump in high altitude aircraft) driven from the aircraft's engine. Because the Earth rotates (ω, 15° per hour, apparent drift), and because of small accumulated errors caused by imperfect balancing of the gyro, the heading indicator will drift over time (real drift), and must be reset using a magnetic compass periodically. The apparent drift is predicted by ω sin Latitude and will thus be greatest over the poles. To counter for the effect of Earth rate drift a latitude nut can be set (on the ground only) which induces a (hopefully equal and opposite) real wander in the gyroscope. Otherwise it would be necessary to manually realign the direction indicator once each ten to fifteen minutes during routine in-flight checks. Failure to do this is a common source of navigation errors among new pilots. Another sort of apparent drift exists in the form of transport wander, caused by the aircraft movement and the convergence of the meridian lines towards the poles. It equals the course change along a great circle (orthodrome) flight path. \nSome more expensive heading indicators are \"slaved\" to a magnetic sensor, called a \"flux gate\". The flux gate continuously senses the Earth's magnetic field, and a servo mechanism constantly corrects the heading indicator. These \"slaved gyros\" reduce pilot workload by eliminating the need for manual realignment every ten to fifteen minutes.\n\nThe prediction of drift in degrees per hour, is as follows:\n\nAlthough it is possible to predict the drift, there will be minor variations from this basic model, accounted for by gimbal error (operating the aircraft away from the local horizontal), among others. A common source of error here is the improper setting of the latitude nut (to the opposite hemisphere for example). The table however allows one to gauge whether an indicator is behaving as expected, and as such, is compared with the realignment corrections made with reference to the magnetic compass. Transport wander is an undesirable consequence of apparent drift.\n\n"}
{"id": "3177655", "url": "https://en.wikipedia.org/wiki?curid=3177655", "title": "Hydrogen fuel enhancement", "text": "Hydrogen fuel enhancement\n\nHydrogen fuel enhancement is the process of using a mixture of hydrogen and conventional hydrocarbon fuel in an internal combustion engine, typically in a car or truck, in an attempt to improve fuel economy, power output, emissions, or a combination thereof. Methods include hydrogen produced through an electrolysis, storing hydrogen on the vehicle as a second fuel, or reforming conventional fuel into hydrogen with a catalyst.\n\nThere has been a great deal of research into fuel mixtures, such as gasoline and nitrous oxide injection. Mixtures of hydrogen and hydrocarbons are no exception. These sources say that contamination from exhaust gases has been reduced in all cases, and they suggest that a small efficiency increase is sometimes possible.\n\nMany of these sources also suggest that modifications to the engine's air-fuel ratio, ignition timing, emissions control systems, electronic control systems and possibly other design elements, might be required in order to obtain any significant results. A modified vehicle in this way may not pass mandatory anti-smog controls. Due to the inherent complexity of these subsystems, a necessity of modern engine design and emissions standards, such claims made by proponents of hydrogen fuel enhancement are difficult to substantiate and always disputed.\n\nTo date, hydrogen fuel enhancement products have not been specifically addressed by the United States Environmental Protection Agency, as no research devices or commercial products have reports available as per the \"Motor Vehicle Aftermarket Retrofit Device Evaluation Program.\" They do, however, point out that installation of such devices often involves illegally tampering with an automobile's emissions control system, which could result in significant fines. \nEnvironment Canada does have a research paper on the subject. In tests done in their laboratory in 2004 they found no improvement in engine efficiency or fuel economy.\n\nHydrogen fuel enhancement from electrolysis (utilizing automotive alternators) has been promoted for use with gasoline-powered and diesel trucks, although electrolysis-based designs have repeatedly failed efficiency tests and contradict widely accepted laws of thermodynamics (i.e. conservation of energy). Proponents, who sell the units (often called \"HHO devices\"), claim that the dynamics are often misconstrued, and due to the chemical properties of the resulting mixture, it is possible to gain efficiency increases in a manner that does not violate any scientific laws. Many tests by consumer watch groups have shown negative results. This technique may seem appealing to some at first because it is easy to overlook energy losses in the system as a whole. Those unfamiliar with electrodynamics may not realize that the electrolytic cell drains current from a car's electrical system causing an increase in mechanical resistance in the alternator that will always result in a net power reduction.\n\nArvin Meritor, a Tier 1 supplier of automotive technology, at one time, was experimenting with a plasma reformer technology which would use hydrogen produced from the fuel to enhance engine combustion efficiency and reduce emissions of NO. This reference states that a 20% to 30% increase in engine thermal efficiency is possible. However, this requires that the engine should be modified to operate in the ultra-lean region of the plot of compression ratio vs. air/fuel equivalence ratio (lambda), along with other modifications. This technology would not work well as a retrofit to unmodified engine technology. This research was conducted in conjunction with the Sloan Automotive Laboratory at MIT. Eventually, the division conducting this research was sold off to an equity investment firm.\n\n"}
{"id": "25766740", "url": "https://en.wikipedia.org/wiki?curid=25766740", "title": "Kia Uvo", "text": "Kia Uvo\n\nUVO eServices is a subscription-free OEM infotainment and telematics service offered by Kia Motors America on select vehicles for the United States market. The system allows users to make hands-free calls on their smartphone, stream music, navigate to a POI, and perform vehicle diagnostics with the use of voice commands.\n\nThe integrated in-vehicle communications and entertainment system is developed by Kia Motors and other third-party developers.\n\nFirst launched in the 2011 model year, Kia’s UVO entertainment system included the HD radio, CD player, and a built-in digital jukebox. The system also interfaces with Bluetooth-enabled phones and utilizes touchscreen and voice command technology.\n\nThe UVO system is driven by vehicle communications with a smartphone application. Data transfers from the vehicle to the smartphone application utilize the customer’s mobile carrier resulting in a subscription-free telematics service. Users of the system are able to interact with their vehicle to retrieve Diagnostic Trouble Codes. Users can also load a Point of interest to the vehicle's head unit. Other technologies like GPS and IVR are also integrated into the system.\n\nUsers are able to access the vehicle data on the smartphone application and through a customer web portal. In the 2015 model year, select vehicles launched with an Android-based operating platform to allow flexibility to integrate 3rd-party applications. Certain vehicles have USB (for model years 2014 and later) and Wi-Fi connectivity (for model years 2015 and later).\n\nThe below table provides a short description of UVO eServices features. Feature availability depends on model and model year.\n\nBeginning with the 2015 Optima the UVO eServices with 8\" Nav system allows users to download applications from the UVO Download Center for in-vehicle use via the head unit. Currently, Yelp is available in the UVO Download Center.\n\nBeginning with 2015 Optima the UVO eServices with 8\" Nav system allows users to connect the vehicle head unit to a WiFi hotspot. Once connected to WiFi, most vehicles can access Google Local Search via a button on the steering wheel to search for POIs via Voice Recognition. 2015 UVO eServices with 8\" Nav system also supports Siri Eyes Free which allows users to access Siri via a button on the steering wheel for supported Apple devices.\n\nOwners of UVO equipped vehicles can register for an account on www.myuvo.com to view information related to their vehicle. Vehicle information available includes Vehicle Diagnostics, Trip Info, MyCarZone, and MyPOIs. In addition to viewing vehicle information, users can also view the maintenance schedule for their vehicle and schedule appointments with a dealer. Also, users can earn awards by actively using the website.\n\nThe UVO smartphone app is available in the iTunes and Google Play stores. The app interfaces with the vehicle through a USB or Bluetooth connection. The UVO eServices with 8\" Nav head unit requires a USB connection to transfer telematics data, while others use Bluetooth for data transfer. All head units can connect with Bluetooth for phone and audio functions.\n\nIn 2014, the app was re-skinned and new features were added. On compatible 2015 vehicles, the UVO app will display Trip Info and My Car Zone.\n\nThe UVO app serves as a web interface for vehicles that do not have embedded web capabilities. Data is transferred from the UVO head unit to the app via Bluetooth or USB, which is then transferred to the web and viewable from both the app and the web.\n\nThe original UVO was released on 2011 vehicles. Features include voice commands, Bluetooth, jukebox, and a rear-view camera. UVO was replaced by UVO eServices on most models by the 2014 model year. Although the standard version was discontinued, it is still used in 2015 LX, EX Sedonas.\n\nUVO eServices was the first UVO to interface with MyUVO.com and the UVO mobile app. It was introduced on 2013 model year Optima, Optima Hybrid, and Sportage. In addition to having all of the features on the original UVO, UVO eServices has Parking Minder, Enhanced Roadside Assist, and Vehicle Diagnostics. On 2015 model year vehicles, Trip Info and My Car Zone features are included.\n\nUVO eServices with 7\" Nav was introduced alongside UVO eServices on some 2013 models. In addition to all of the features of UVO and UVO eServices, the 7\" Nav version has Navigation and My POIs. On 2015 model year vehicles, Trip Info and My Car Zone are included.\n\nUVO eServices with 8\" Nav was introduced on the 2014 Soul. It has all of the features of UVO eServices with 7\" Nav, the larger screen accepts swipe gestures, and features are organized in the way that apps are organized on a tablet.\n\nOn September 22, 2014, an update for the 2014 Soul UVO with Nav system was released which unlocks the eServices including My Car Zone, Vehicle Diagnostics, and 911 Connect. It also includes updated maps and improved navigation features, as well as Siri Eyes-Free.\n\nUVO Premium was designed specifically for the 2015 K900. It shares most of its features with UVO eServices with 7\" Nav. It comes with a completely redesigned user interface and the touch screen from other UVO versions is replaced with a controller wheel on the center console.\n\nThe following vehicles are UVO capable, available as either an optional or standard feature. The date next to each vehicle indicates in which model year UVO was first available on that specific vehicle. On some models, UVO is not available on all trim levels.\n\nNorth America:\n\n\n"}
{"id": "46536830", "url": "https://en.wikipedia.org/wiki?curid=46536830", "title": "Lexoo", "text": "Lexoo\n\nLexoo is a UK-based legal technology company launched in June 2014 with headquarters in London, United Kingdom. Lexoo provides a lawyer-matching online marketplace, enabling businesses to find a lawyer by providing multiple quotes from specialised solicitors.\n\nLexoo was founded in June 2014 by Daniel van Binsbergen, a former lawyer at De Brauw Blackstone Westbroek, and Chris O'Sullivan, a developer, having raised $400,000 in seed funding from Forward Partners and Jonathan McKay, the Chairman of JustGiving. In November 2015, Lexoo raised a further $1.3 million in funding from a number of investors, including the London Co-Investment Fund, Duncan Jennings (founder of Vouchercodes.co.uk), Tim Jackson of Lean Investments, Robin Grant (founder of We Are Social) and Forward Partners. In October of 2018, Lexoo raised $4.4 million in Series A round led by Earlybird.\nTechCrunch identified that, compared to other markets, the fees charged by legal practitioners are traditionally non transparent, and considered Lexoo “a classic example of how markets can be made more efficient and transparent by moving them online”.\n\nForbes described Lexoo as “the democratisation of legal services”, noting that “the participating solicitors know they are quoting in a competitive environment, so they will offer their best price up front, without businesses having to ask for it. All the ingredients of a classic disruptive start-up.”\n\nIn June 2015, the Financial Times selected Lexoo as its \"Innovation to Watch\" and Startups.co.uk listed Lexoo among the top 100 startups in the UK in 2015.\n\nLexoo has also been featured by PE Hub, Legal Futures, Talk Business Magazine, the Guardian, and Tech City News.\n"}
{"id": "1179268", "url": "https://en.wikipedia.org/wiki?curid=1179268", "title": "Lionel Rees", "text": "Lionel Rees\n\nGroup Captain Lionel Wilmot Brabazon Rees, (31 July 1884 – 28 September 1955) was a Welsh aviator, flying ace, and a recipient of the Victoria Cross, the highest award for gallantry in the face of the enemy that can be awarded to British and Commonwealth forces. He was credited with eight confirmed aerial victories, comprising one enemy aircraft captured, one destroyed, one \"forced to land\" and five \"driven down\". Rees and his gunner, Flight Sergeant James McKinley Hargreaves, were the only two airmen to become aces flying the earliest purpose-built British fighter airplane, the Vickers Gunbus.\n\nRees also had a keen interest in archaeology. While flying from Cairo to Baghdad in the 1920s, he took some of the earliest archaeological aerial photographs of sites in eastern Transjordan (now Jordan), and published several articles in \"Antiquity\" and the journal of the Palestine Exploration Fund. He is considered a father of the archaeological studies of this remote area, and a pioneer of aerial archaeology. He was also an accomplished sailor.\n\nRees was born at 5 Castle Street, Caernarfon, on 31 July 1884, the son of Charles Herbert Rees, a solicitor and honorary colonel in the Royal Welch Fusiliers, and his wife Leonara. Rees attended Eastbourne College before entering the Royal Military Academy at Woolwich in 1902. He was commissioned in on 23 December 1903 into the Royal Garrison Artillery and was posted to Gibraltar. Promoted to lieutenant in 1906 he moved to Sierra Leone in 1908 and in May 1913 was seconded to the Southern Nigeria Regiment.\n\nIn 1912, Rees learned to fly at his own expense, receiving his Aviator's Certificate (no. 392) in January 1913. By 1913–14, Rees was attached to the West African Frontier Force when he was seconded to the Royal Flying Corps in August 1914, initially as an instructor at Upavon, he was promoted to captain in October 1914. In early 1915 he took command of the newly formed No. 11 Squadron at Netheravon and in July they moved to France. He first saw action flying the Vickers Gunbus with No. 11 Squadron in the mid-1915, earning a reputation as an aggressive pilot and an above average marksman.\n\nRees was awarded the Military Cross for his actions in 1915, gazetted as follows:\n\nBy this time he had claimed one aircraft captured, one destroyed, one \"forced to land\" and five \"driven down\".\n\nRees returned to England at the end of 1915 where he took command of the Central Flying School Flight at Upavon. In June 1916 he took No. 32 Squadron to France.\n\nRees was 31 years old and a temporary major in No. 32 Squadron when the following deed took place for which he was awarded the VC.\n\nIn the first hours of the Somme Offensive, Rees was on patrol, taking off in Airco DH.2 No. 6015 at 0555 hours. His attempt to join a formation of \"British\" machines brought an attack from one of the Germans. He shot up the attacker, hitting its fuselage between the two aircrew. As it dived away, Rees attacked a Roland. Long range fire from three other Germans did not discourage Rees from closing on it; it emitted a hazy cloud of smoke from its engine from the 30 rounds Rees fired into it and it fled. Rees then single-handedly went after five more Germans. A bullet in the thigh paralysed his leg, forcing him to temporarily break off his assault. As the shock of the wound wore off, he was able to pursue the German formation leader, which was leaving after dropping its bomb. He fired his Lewis machine gun empty. In frustration, he drew his pistol but dropped it into his DH.2's nacelle. Meanwhile, the German two-seater pulled away above him. The German formation was shattered and scattered.\n\nRees gave up the futile chase, and returned to base. Once landed, he calmly asked for steps so he could deplane. Once seated on the aerodrome grass, he had a tender fetched to take him to hospital. The valour of his actions earned him the Victoria Cross. Its citation reads:\n\nHe convalesced for a while due to his injuries from the 1 July action, and went on a War Office mission to the United States, becoming a temporary lieutenant colonel in May 1917. For the remainder of hostilities Rees commanded a School of Aerial Fighting based at RAF Turnberry.\n\nOn 2 November 1918 Rees was awarded the Air Force Cross in recognition of valuable flying services. In 1919, Rees was appointed an Officer of the Order of the British Empire. On 1 August 1919 he resigned his commission with the Royal Garrison Artillery and took a permanent commission in the newly formed Royal Air Force as a lieutenant-colonel. In 1920 Rees was presented with a sword and the freedom of Caernarfon.\nIn June 1920, he took command of the flying wing at RAF College Cranwell, becoming the assistant commandant in March 1923. Promoted to group captain after the RAF moved away from the Army rank structure, Rees became deputy director in the Air Ministry directorate of training and in January 1925 he became an additional air aide-de-camp to the King.\n\nRees was posted to RAF Amman in May 1926 and given command of the RAF Transjordan and Palestine in October 1926. He had a keen interest in archaeology, and while flying on the Cairo to Baghdad route during this period, he took some of the earliest archaeological aerial photographs of sites in eastern Transjordan (now Jordan), and published several articles in \"Antiquity\" and the journal of the Palestine Exploration Fund. He is considered a father of the archaeological studies of this remote area, and a pioneer of aerial archaeology.\n\nRees returned to England in October 1926 to take command of the RAF Depot at Uxbridge before he retired \nfrom the RAF in 1931 with the rank of group captain. In 1933, he sailed single-handedly across the Atlantic from Wales to Nassau in the Bahamas in a ketch. For this achievement he was awarded the prestigious Blue Water Medal by the Cruising Club of America in 1934.\n\nWhen the Second World War broke out, Rees returned to the United Kingdom from the Bahamas and once again joined the RAF. He relinquished his rank of group captain in January 1941 at his own request and was granted the rank of wing commander. He served in Africa. On 21 November 1942 Rees reverted to the rank of group captain on the retired list.\n\nRees returned home to the Bahamas and on 12 August 1947, aged 62, he married Sylvia Williams, a young local woman. They had three children. Rees died at the Princess Margaret Hospital in Nassau on 28 September 1955 from leukaemia. He was buried at Nassau war cemetery. Described as a real gentleman, during his time in Transjordan and possibly during other parts of his career, Rees was known to give his pay to service charities.\n\nRees wrote under the name 'L. W. B. Rees'.\n\n\n\n"}
{"id": "25220785", "url": "https://en.wikipedia.org/wiki?curid=25220785", "title": "Ministry of Mines and Energy (Brazil)", "text": "Ministry of Mines and Energy (Brazil)\n\nThe Ministry of Mines and Energy (MME) is a Brazilian government ministry established in 1960. It fosters investments in mining and energy-related activities, funds research and sets out government policies. Previously, mines and energy were the responsibility of the Ministry of Agriculture. The current Minister of Mines and Energy is Wellington Moreira Franco.\n\n"}
{"id": "12241852", "url": "https://en.wikipedia.org/wiki?curid=12241852", "title": "Modular addition", "text": "Modular addition\n\nModular additions are usually side and second-story additions to homes that are pre-fabricated at the facilities. General characteristics of a modular home apply. For a second-story modular addition the existing house should have a sound structure as modular rooms are 30%+ heavier than the same stick-built. Modular additions are built in the facility, brought to the site and “dropped” by crane on the new location. It is necessary to have a general contractor to supervise the project as there is always some work to tie new rooms into an existing house.\n\nEven though it is assumed that modular additions are less expensive than traditional stick built this is rarely true when compared by square footage. The buyer would generally get more wood and better insulation plus benefits of controlled environment but total cost of the contract would be about the same. \n\n"}
{"id": "12935028", "url": "https://en.wikipedia.org/wiki?curid=12935028", "title": "Mononitrotoluene", "text": "Mononitrotoluene\n\nMononitrotoluene, or methylnitrobenzene or nitrotoluene (MNT or NT), is a group of three organic compounds, a nitro derivative of toluene (or alternatively a methyl derivative of nitrobenzene). Its chemical formula is CH(CH)(NO).\n\nMononitrotoluene comes in three isomers, differing by the relative position of the methyl and nitro groups:\n\nTypical use of nitrotoluene is in production of pigments, antioxidants, agricultural chemicals, and photographic chemicals.\n\n\"Ortho\"-mononitrotoluene and \"para\"-mononitrotoluene can be also used as detection taggants for explosive detection.\n\n\n"}
{"id": "35959428", "url": "https://en.wikipedia.org/wiki?curid=35959428", "title": "Nad's", "text": "Nad's\n\nNad's is an Australian trademark of internationally sold waxing hair removal products, although chemical depilatory, skin-care and other products are also sold under the name.\n\nNad's was developed and founded in 1992 by Australian businesswoman Sue Ismie. The original product is a tub of natural, green-coloured gel consisting of honey, molasses, sugar and lemon juice. Inspiration for the product came from her daughter's desire to remove the hair from her arms. A \"concept\" used by her mother and grandmother was \"improved\" by Ismiel herself, in order to reduce the pain of the waxing process on her daughter. The product took twelve months to develop. Ismiel notified her colleagues at her employment of the result, in which \"people wanted me [Ismiel] to bring it in for them to take unwanted hair from their eyebrows and upper lips -- and everybody loved it\".\n\nThe process of developing the gel was completed in a large pot, later to be undertaken in Ismiel's garage which she made into a factory. She received her own stall at a market in Flemington, New South Wales, New South Wales, and soon got stalls in local shopping centres throughout Australia. The product's commercial success came from an investment in a television advertising campaign, in which a call centre was developed to take the orders of the product.\n\nNad's has six laser hair removal clinics across Sydney. On 6 July 2015, You by Sia has acquired all 6 Nad's Laser Clinics.\n\nIn 1997, Ismiel received a cheque from Woolworths for $1 million. Five years following its release, British newspaper \"The Guardian\" revealed that Nad's was the best-selling personal care product in Australia with a turnover of $7 million. The \"Sydney Morning Herald\" named it as Australia's best-known hair remover.\n"}
{"id": "5464478", "url": "https://en.wikipedia.org/wiki?curid=5464478", "title": "National Space Agency (Malaysia)", "text": "National Space Agency (Malaysia)\n\nThe National Space Agency (), abbreviated ANGKASA, is the national space agency of Malaysia. It was established in 2002 and its charter includes aims to upgrade, stimulate and foster the country's space programme through integrated and coordinated efforts by developing and applying space technologies.\n\nThe Angkasawan program was an initiative by the Malaysian government to send a Malaysian to the International Space Station on board Soyuz TMA-11. The program was named after the Malay word for astronaut, Angkasawan. It resulted in Sheikh Muszaphar Shukor becoming the first Malaysian in space on 10 October 2007.\n\nThe program was officially announced by the former Prime Minister of Malaysia, Mahathir bin Mohamad, as a joint programme with the Russian Federation. It was a project under the government-to-government offset agreement through the purchase of Sukhoi SU-30MKM fighter jets for the Royal Malaysian Air Force.\n\nUnder this agreement the Russian Federation bore the cost of training two Malaysians for space travel and for sending one to the International Space Station (ISS) in October 2007. It resulted in Sheikh Muszaphar Shukor becoming the first Malaysian in space on 10 October 2007.\n\nThe evidence suggests that the main objectives of the program are to uplift the national image and to instill in the younger generation greater interest in mathematics and science. At the launch, the Malaysian Science, Technology and Innovation Minister Datuk Seri Dr Jamaluddin Jarjis said: \"It is not merely a project to send a Malaysian into space. After 50 years of independence, we need a new shift and a new advantage to be more successful as a nation. \"We want to awe and inspire, and spur Malaysians to attain greater success by embracing science and technology.\" \n\nLater, Dr Jamaluddin Jarjis was more specific as to the objective of the program when he said that it \"was to create awareness among Malaysians the importance of science, technology and the space industry, which could help develop the economy further.\"\n\nSheikh Muszaphar Shukor himself said that \"I am not seeking fame or looking forward to be welcomed like a celebrity, but my quest is to inspire Malaysians, especially school children to like learning the subject of science and the space industry.\"\n\nIn April 2006, ANGKASA sponsored a conference of scientists and religious authorities, addressing the issue of how the circumstances of space travel would affect the obligations faced by Muslim astronauts (for instance, how can one face the qibla while orbiting the Earth).\n\nThe Malaysian National Space Agency, in co-operation with other Malaysian agencies, developed the \"Guidelines for Performing Islamic Rites at the International Space Station\" with the help of 150 scientists and clerics. The handbook details how Muslim astronauts may observe Islamic rites while in orbit. Daily prayer times are aligned with those at the point of departure and if Mecca cannot be located, the astronauts may pray toward the Earth itself. Female Muslim astronauts are required to cover everything but the palms of their hands and their faces.\n\nMalaysia uses both satellites purchased abroad and developed in country by Astronautic Technology Sdn Bhd. All of Malaysia's satellites are launched abroad.\n\nTiung SAT is the first Malaysian microsatellite. The satellite was developed through the technology transfer and training programme between Astronautic Technology Sdn Bhd (ATSB) Malaysia and Surrey Satellite Technology of the United Kingdom. TiungSAT-1 was launched aboard the Dnepr rocket from the Baikonur Cosmodrome in Kazakhstan on 26 September 2000. The satellite was developed for experiments in Earth imaging, observation of meteorology, detection of cosmic rays, data storage and communications.\n\nMEASAT (Malaysia East Asia Satellite) is the name of a family of communications satellites owned and operated by MEASAT Satellite Systems Sdn. Bhd. (MYX: 3875, formerly Binariang Satellite Systems Sdn. Bhd.), a Malaysian communications satellite operator.\n\nAs of 2006, the MEASAT satellite network consisted of three geostationary satellites designed and built by Boeing Satellite Systems. MEASAT-1 and MEASAT-2 were launched in 1996 and MEASAT-3 in 2006.\n\nThe MEASAT-1 and 2 satellites were designed to provide 12 years of both direct-to-user television service in Malaysia and general communications services in the region from Malaysia to the Philippines and from Beijing to Indonesia. With the launch of MEASAT-3, the coverage has extended to an area encompassing 70% of the world's population.\n\nRazakSAT is a high-resolution Malaysian Low Earth Orbit (LEO) satellite that was launched on board a Falcon 1 on 14 July 2009. It was placed into a unique near equatorial orbit that presents many imaging opportunities for the equatorial region.\nRazakSAT is an Earth-observing satellite developed in Malaysia and launched in 2009. Its primary instrument is a medium-aperture camera. Images from the camera are relayed to an Image Receiving and Processing ground station in Malaysia. It failed barely more than 1 year after launch, according to an audit report from October 2011.\n\nIn Summer 2001 MNSA started reviewing plans for a small indigenous space launcher that would be developed in collaboration with Japan and would allow Malaysia to join the Asian space powers.\n\nMNSA has few satellite-linked ground stations and main Malaysia Space Centre including acting multi-laboratorial Mission Operation Centre, Optical Calibration Laboratory and currently building multi-facilitated Assembly, Integration and Test Centre.\n\nFor performing of starts of future indigenous space launchers and foreign operators of space launch services (usual launch pads or air-space systems like Russian air launch etc.), Malaysia is planning to found energy-advantageous near-Equatorial spaceport in Tawau in Sabah in Malaysian part of Borneo island. \n"}
{"id": "410942", "url": "https://en.wikipedia.org/wiki?curid=410942", "title": "Organic movement", "text": "Organic movement\n\nThe organic movement broadly refers to the organizations and individuals involved worldwide in the promotion of organic farming and other organic products. It started around the first half of the 20th century, when modern large-scale agricultural practices began to appear.\n\nThe term “organic” can be broadly described as food that is not genetically engineered and grown without the assistance of toxic chemicals, including many synthetic pesticides, the use of arsenic, and fertilization using biosolids, which are often found to contain flame retardants and drugs among other things, even though they are processed. The term \"organic farming\" was coined by Lord Northbourne in 1940. The beginnings of the organic movement can be traced back to the beginning of the 1800s. In 1840 Justus Von Liebig developed a theory of mineral plant nutrition. Liebig believed that manure could be directly substituted by certain mineral salts. Many years later in 1910, preceding the First World War, chemists Fritz Haber and Carl Bosch developed an ammonia synthesis process, making use of nitrogen from the atmosphere. This form of ammonia had already been used to manufacture explosives, so after the war, it was implemented into the fertilization of agriculture.\n\nThe organic movement began in the early 1900s in response to the shift towards synthetic nitrogen fertilizers and pesticides in the early days of industrial agriculture. A relatively small group of farmers came together in various associations: Demeter International of Germany, which encouraged biodynamic farming and began the first certification program, the Australian Organic Farming and Gardening Society, the Soil Association of the United Kingdom, and Rodale Press in the United States, along with others. In 1972 these organizations joined to form the International Federation of Organic Agriculture Movements (IFOAM). In recent years, environmental awareness has driven demand and conversion to organic farming. Some governments, including the European Union, have begun to support organic farming through agricultural subsidy reform. Organic production and marketing have grown at a fast pace.\n\nToday, organic foods stores have captured a significant share of the grocery shopping market, specifically, Whole Foods Market, Wild Oats, Trader joe's and others.\n\n\nSpecifications for what may be classified as organic food may vary by location. According to the United States Department of Agriculture (USDA), organic food is produced by farmers who emphasize the use of renewable resources and the conservation of soil and water to enhance the quality of the environment. Organic poultry and dairy products come from animals that are given no antibiotics or growth hormones. Organic food is produced without the use of synthetic pesticides or fertilizers, bioengineering, and ionizing radiation. Before a product can be labeled \"organic\", a Government-approved certifier inspects the farm where the food is grown to make sure the farmer is following all the rules necessary to meet USDA organic standards. Companies that handle or process organic food before it reaches supermarkets and restaurants must be certified as well.\n\nThe recent interest in the organic industry has sparked the interest of many businesses from small local distributors to large companies that distribute many products nationally. The organic market is now a 14 billion dollar a year industry, that continues to grow especially from large corporations such as Wal-Mart that are now offering organic choices to their customers. Other companies that offer organic options include General Mills and Kraft. Some large companies have bought smaller already established organic companies such as Earth’s Best, Rice Dream soy milk, Garden of Eatin', Celestial Seasonings and Health Valley. When larger companies buy smaller companies it is called stealth ownership.\n\nOrganic cosmetics are products that are made with organic ingredients that were produced without the use of synthetic pesticides, herbicides, fungicides, and fertilizers.\n\nThe FDA does not have a definition of “Organic” in terms of organic cosmetics. FDA regulates cosmetics under the authority of the Federal Food, Drug, and Cosmetic Act (FD&C Act) and the Fair Packaging and Labeling Act (FPLA).\n\nThe USDA (the U.S. Department of Agriculture) requirements for the use of the term “organic” are separate from the laws and regulations that FDA applies for cosmetics. For more information on \"organic\" labeling for cosmetics, see the NOP publication, \"Cosmetics, Body Care Products, and Personal Care Products.\" Cosmetic products labeled with “organic” must follow both USDA regulations and FDA regulations of organic claims for labeling and safety requirements for cosmetics.\n\nThe Agricultural Marketing Service of USDA supervises the National Organic Program (NOP). The NOP regulations have the definition of “organic” and provide certification for agricultural ingredients if they have been produced under conditions that would meet the definition. Moreover, the regulations also include labeling standards based on the percentage of organic ingredients in every product.\n\nThe COSMetic Organic and Natural Standard (COSMOS) sets certification requirements for organic and natural cosmetics products in Europe.\n\nOrganic farming is a form of agriculture that relies on techniques such as crop rotation, green manure, compost, and biological pest control.\nThere have been multiple criticisms regarding organic food and organic marketing practices. Scientists at the University of Washington did a test of the urine of children who are on organic food diets and children who are on conventional food diets. The result was children on organic food diets ‘ urine had a median level of pesticide byproducts only one-sixth of children on conventional food diets. However, at the same time French, British and Swedish government food agencies have all concluded that there was no scientific proof that organic food is safer or has more nutrition than conventional foods.\n\nA 2014 study by a non-profit academic think tank alleged consumers are \"routinely deceived\" by intentional and endemic misleading health claims in organic marketing. Organic products typically cost 10% to 40% more than similar conventionally produced products. According to the UK's Food Standards Agency, \"Consumers may choose to buy organic fruit, vegetables and meat because they believe them to be more nutritious than other food. However, the balance of current scientific evidence does not support this view.\" A 12-month systematic review commissioned by the FSA in 2009 and conducted at the London School of Hygiene & Tropical Medicine based on 50 years' worth of collected evidence concluded that \"there is no good evidence that consumption of organic food is beneficial to health in relation to nutrient content.\" Although the source of the organic movement was small family farms, large corporations have started distributing more organic products and certain categories of organic foods, such as milk, have been reported by Michael Pollan to be highly concentrated and predominantly sourced to mega-farms.\n\n\n\n"}
{"id": "4198294", "url": "https://en.wikipedia.org/wiki?curid=4198294", "title": "Otto Schott", "text": "Otto Schott\n\nFriedrich Otto Schott (17 December 1851 – 27 August 1935) was a German chemist, glass technologist, and the inventor of borosilicate glass. He was the son of a window glass maker, Simon Schott. From 1870 to 1873 Schott studied chemical technology at the technical college in Aachen and at the universities of Würzburg and Leipzig. He attained a doctorate in glass chemistry at Friedrich Schiller University of Jena for his thesis “Contributions to the Theory and Practice of Glass Fabrication”.\n\nIn 1879, Schott developed a new lithium-based glass that possessed novel optical properties. Schott shared this discovery with Dr Ernst Abbe, which was the catalyst for a long professional relationship between the two. \n\nIn 1884, in association with Dr Ernst Abbe and Carl Zeiss, Otto founded \"Glastechnische Laboratorium Schott & Genossen\" (Schott & Associates Glass Technology Laboratory) in Jena. It was here, during the period 1887 through to 1893, that Schott developed borosilicate glass. Borosilicate glass is distinguished for its high tolerance to heat and a substantial resistance to thermal shock (sudden temperature changes) and resistance to degradation when exposed to chemicals.\n\nIn 1926, Otto Schott retired from active work at Schott & Gen.\n\n"}
{"id": "19042654", "url": "https://en.wikipedia.org/wiki?curid=19042654", "title": "Photofermentation", "text": "Photofermentation\n\nPhotofermentation is the fermentative conversion of organic substrate to biohydrogen manifested by a diverse group of photosynthetic bacteria by a series of biochemical reactions involving three steps similar to anaerobic conversion. Photofermentation differs from dark fermentation because it only proceeds in the presence of light.\n\nFor example, photo-fermentation with \"Rhodobacter sphaeroides\" SH2C (or many other purple non-sulfur bacteria) can be employed to convert small molecular fatty acids into hydrogen and other products.\n\nPhototropic bacteria produce hydrogen gas via photofermentation, where the hydrogen is sourced from organic compounds.\n\n<chem>C6H12O6 + 6H2O ->[{hv}] 6CO2 + 12H2</chem>\n\nPhotolytic producers are similar to phototrophs, but source hydrogen from water molecules that are broken down as the organism interacts with light. Photolytic producers consist of algae and certain photosynthetic bacteria.\n\n<chem>12H2O ->[{hv}] 12H2 + 6O2</chem>(algae)\n\n<chem>CO + H2O ->[{hv}] H2 + CO2</chem>(photolytic bacteria)\n\nPhotofermentation via purple nonsulfur producing bacteria has been explored as a method for the production of biofuel. The natural fermentation product of these bacteria, hydrogen gas, can be harnessed as a natural gas energy source. Photofermentation via algae instead of bacteria is used for bioethanol production, among other liquid fuel alternatives.\nThe bacteria and their energy source are held in a bioreactor chamber that is impermeable to air and oxygen free. The proper temperature for the bacterial species is maintained in the bioreactor. The bacteria are sustained with a carbohydrate diet consisting of simple saccharide molecules. The carbohydrates are typically sourced from agricultural or forestry waste.\n\nIn addition to wild type forms of \"Rhodopseudomonas palustris, s\"cientists have used genetically modified forms to produce hydrogen as well. Other explorations include expanding the bioreactor system to hold a combination of bacteria, algae or cyanobacteria. Ethanol production is performed by the algae \"Chlamydomonas reinhardtii\", among other species, in cycling light and dark environments. The cycling of light and dark environments has also been explored with bacteria for hydrogen production, increasing hydrogen yield.\n\nThe bacteria are typically fed with broken down agricultural waste or undesired crops, such as water lettuce or sugar beet molasses. The high abundance of such waste ensures the stable food source for the bacteria and productively uses human-produced waste. In comparison with dark fermentation, photofermentation produces more hydrogen per reaction and avoids the acidic end products of dark fermentation.\n\nThe primary limitations of photofermentation as a sustainable energy source stem from the precise requirements of maintaining the bacteria in the bioreactor. Researchers have found it difficult to maintain a constant temperature for the bacteria within the bioreactor. Furthermore, the growth media for the bacteria must be rotated and refreshed without introducing air to the bioreactor system, complicating the already expensive bioreactor set up.\n\n\n"}
{"id": "3674206", "url": "https://en.wikipedia.org/wiki?curid=3674206", "title": "Planetary surface construction", "text": "Planetary surface construction\n\nPlanetary-surface construction is the construction of artificial habitats and other structures on planetary surfaces. Planetary surface construction can be divided into three phases or classes, coinciding with a phased schedule for habitation (Kennedy 2002, Smith 1993):\n\n• Class I: Pre-integrated hard shell modules ready to use immediately upon delivery.\n\n• Class II: Prefabricated kit-of-parts that is surface assembled after delivery.\n\n• Class III: in-situ resource utilization (ISRU) derived structure with integrated Earth components.\n\nClass I structures are prepared and tested on Earth, and are designed to be fully self-contained habitats that can be delivered to the surface of other planets. In an initial mission to put human explorers on Mars, a Class I habitat would provide the bare minimum habitable facilities when continued support from Earth is not possible.\n\nThe Class II structures call for a pre-manufactured kit-of-parts system that has flexible capacity for demountability and reuse. Class II structures can be used to expand the facilities established by the initial Class I habitat, and can allow for the assembly of additional structures either before the crew arrives, or after their occupancy of the pre-integrated habitat.\n\nThe purpose of Class III structures is to allow for the construction of additional facilities that would support a larger population, and to develop the capacity for the local production of building materials and structures without the need for resupply from Earth.\n\nTo facilitate the development of technology required to implement the three phases, Cohen and Kennedy (1997) stress the need to explore robust robotic system concepts that can be used to assist in the construction process, or perform the tasks autonomously. Among other things, they suggest a roadmap that stresses the need for adapting structural components for robotic assembly, and determining appropriate levels of modularity, assembly, and component packaging. The roadmap also sets the development of experimental construction systems in parallel with components as an important milestone.\n\n\n\n"}
{"id": "43501255", "url": "https://en.wikipedia.org/wiki?curid=43501255", "title": "Pong Research", "text": "Pong Research\n\nPong Research Corporation was a cell phone accessories company that invented the Pong Case, a cell phone case with an embedded antenna that reduces exposure to cell phone radiation. \n\nPong Research was incorporated in 2011 and in 2014 relocated from Leesburg, Virginia to Encinitas, California. The company was financially backed by Catterton Partners, the leading consumer-focused private equity fund with over $4.5 billion under management.\nIn 2014, the company changed its name to Antenna79, Inc. In 2016 Antenna79 merged with Invisible Gadget Guard, Inc. (a cell phone accessories company specializing in screen protection) to become Penumbra Brands, LLC.\n\nThe Pong Case antenna technology was created by PhD scientists educated at Harvard, MIT, UCLA and the University of Manchester. Using their knowledge of manipulating radiofrequency (RF) electromagnetic fields, Pong invented a coupling and re-radiating antenna that redistributes cell phone near-field radiation. Pong’s first product was for the iPhone 3G.\n\nThe cases were made of a hard polycarbonate called Lexan and contain a micro-thin, gold-plated coupled antenna system (CAS) that redirects wireless energy away from the user, thus reducing exposure to mobile device radiation well below the FCC Specific Absorption Rate (SAR) limit—without compromising the device’s ability to communicate.\n\nThe cases use a passive antenna coupling technology works via a micro-thin gold-plated antenna embedded inside the Pong Case that redistributes the RF signal emitted from a mobile device away from the user’s head and body without compromising the device’s ability to communicate. Pong has 8 patents granted and 20 filings pending.\n\nIn 2017 the Pong Case was renamed \"alara\", which stands for \"as low as reasonably achievable\", in reference to the technology significantly reducing RF exposure without compromising the device’s ability to communicate. \n\nIn October 2009, Wired Magazine tested the product at Cetecom, an FCC-certified lab in Milpitas, California that tests cell phone radiation levels for handset manufacturers. Using a Specific Anthropomorphic Mannequin head filled with a brain tissue simulating fluid. After a phone call simulation with and without the Pong case, it was found that the Specific Absorption Rate (SAR) was reduced by 64.7% to 0.42 watts per kilogram.\n"}
{"id": "23674090", "url": "https://en.wikipedia.org/wiki?curid=23674090", "title": "RDF-Powerstation", "text": "RDF-Powerstation\n\nThe RDF-PowerStation is a peripheral thermal recovery plant, which is based on renewable energy (especially on Refuse derived fuel (RDF)).\n\nThe RDF-Powerstation is directly integrated in a building like a business centre and is used as a cogeneration or trigeneration plant. Due to that the incineration unit and all additional components, which are necessary for the operation of the plant and the utilization of the energy, are located in the consumers’ area. So the energy is produced and used at the same place. The technical realization of this concept is based on the configuration of the process diagram.\n\nBefore the crude materials (e.g. municipal or commercial waste) are introduced into the incineration unit, these resources are mechanically treated at an external station. The treatment intensity is significantly linked to the quality and composition of the primary material. So the treatment process can be very different – from an ordinary crushing and a rough sortation to a multistep milling process with various product streams and a finishing briquetting. The final product of this step – the Refuse derived fuel – is used in the “RDF- Powerstation”.\n\nThe combustion unit is equipped with a water-cooled reciprocating grate, a post- combustion chamber, where the selective non-catalytic reduction of the nitrogen oxides is included and a steam boiler with the heat exchangers. The main part of the heat energy, which is captured in the live steam, is used for the electricity production. Another portion is applied for a heating grid or an absorption refrigerator.\n\nFollowing the steam boiler the flue gas enters the 4-stage dry scrubber. There the pollutants of the flue gas are extracted by different additives, which separated at the baghouse filters. Additionally a catalyzer is included to enable an extra stage for the removal of the nitrogen oxides (SCR- process). At the end of flue gas purification process an algae reactor is mounted, where algae is converting carbon dioxide to biomass.\n\n"}
{"id": "29043290", "url": "https://en.wikipedia.org/wiki?curid=29043290", "title": "Repowering", "text": "Repowering\n\nRepowering is the process of replacing older power stations with newer ones that either have a greater nameplate capacity or more efficiency which results in a net increase of power generated. Repowering can happen in several different ways. It can be as small as switching out and replacing a boiler, to as large as replacing the entire system to create a more powerful system entirely. There are many upsides to repowering. The simple act of refurbishing the old with the new is in itself beneficial alongside the cost reduction for keeping the plant running. With less costs and a higher energy output, the process is excessively beneficial.\n\nCountries like Germany and Denmark that have a large number of wind turbines installed relative to their total land size have resorted to repowering older turbines in order to increase wind power capacity and generation. The power as well as use of wind farms has grown since the 1990s. \n\nWith new innovations to improve existing Wind power technology, the Repowering process is to upgrade these old wind turbines. By Repowering these old wind turbine with new upgrades this will greatly improve the efficiency of energy these new Wind turbine can generate. The new technology will greatly increase the amount of energy these Wind turbine can output. In California, the wind turbines are aging it would be effective to Repower the wind turbines but there seems to be a lack of economic incentive across legislature to put money forth for such processes. The inefficient turbines In California were implemented in the 1980s; they run at 1320 megawatts.\n\nThe potential benefits that run along with repowering these California turbines are as follows: \n\nAlthough many barriers continue to hinder rapid wind‐project repowering, a primary barrier is simply that many existing, aging wind facilities are more profitable, in the near term, in continued operations than they might be if they pursue repowering with new wind turbines. Therefore, it can be difficult to repower wind turbine facilities based on early numbers.\n\nIn 2005, a large majority of United States wind turbines were of the size range between 51 kW to 100 kW. In 2002 there was first put in a Megawatt-class turbine (KEMA, Inc.). By 2007, California was only able to repower 20% of the potential power that being 365 MW of the 1,640MW maximum.\n\nWith new environmental regulation in the United States, coal-fired power plants are becoming obsolete. As many as three-fourths of coal-fired power plants are being shut down. Short-term options include retiring the plant or quick conversion to direct firing of the boiler with natural gas. Repowering these old coal burning power plants into gas burning boilers. It's estimated that as much as 30 gigawatts (GW) of existing U.S. power generation capacity could be lost through plant closings due to new U.S. Environmental Protection Agency (EPA) regulations. There could be a saving of 20 percent of the capital cost instead of building brand new power plants founded by EPRI studied.\n\nThe configuration of these plants involves replacing the old coal boiler with gas-fired turbine (GT). The gas-fired outputs exhaust heat to a heat recovery steam generator (HRSG). From the output of the heat recovery steam generator it is run into a steam turbine which increases electricity production and the overall efficiency of the plant. \nThe gas-fired turbine (GT) and the heat recovery steam generator (HRSG) technology has been in utilize in many repowering projects over the past 20 years in the United States alone. With increasing environmental regulations of the United States Government and the lower fuel prices made the usage of GT/HRSG an option in utilizing to renew many old coal heating power plants. This modern gas turbines operate with higher efficiencies and adding a heat recovery steam generator (HRSG) raises overall plant efficiency to 40 percent to 50 percent (HHV) above the range of most coal-fired plants, reducing fuel consumption and lowering plant emissions. \nSiemens Corporation are also using this technology by combining the gas turbine (GT) in conjunction with the heat recovery steam generator (HRSG) with the steam turbine (ST) and the combined cycle power plants to produce the most efficient power generation facilities. Existing direct-fired plants can utilize this advanced cycle concept by adding a GT and a HRSG. This so-called repowering scheme makes the existing power generation facility equally efficient as modern combined cycle power plant.\n\nSiemens Corporation developed two ways in powering these old coal plants. The first one is called a Full Powering and the second is called Parallel Powering. Full Powering is only used with old plants because the boilers has reached the life of its usage. Full powering replaces the original boiler and gas-turbine (GT) and heat recovery steam generator are added (HRSG). While compared to the full repowering concept, this repowering scheme achieves slightly lower efficiency. Due to the two independent steam sources for the steam turbine, this concept provides a higher fuel flexibility and also greater flexibility in respect to load variations.\n\nAn example of a repowering project is of Fluor updating the Seward plant.The plant was a 521-MW coal-fired power plant. The plant burns waste coal. The project was to take three existing pulverized coal-fired boilers out and install two new Clean Coal Technology CFB boilers with major changes such as installing two Alstom CFB boilers along with an Alstom steam turbine generator. This plant is now the largest waste coal generator in the world with a capacity of 521-MW of capacity. It runs on 11,000 tons of waste coal per day.\n\n"}
{"id": "990677", "url": "https://en.wikipedia.org/wiki?curid=990677", "title": "SAS (software)", "text": "SAS (software)\n\nSAS (previously \"Statistical Analysis System\") is a software suite developed by SAS Institute for advanced analytics, multivariate analyses, business intelligence, data management, and predictive analytics.\n\nSAS was developed at North Carolina State University from 1966 until 1976, when SAS Institute was incorporated. SAS was further developed in the 1980s and 1990s with the addition of new statistical procedures, additional components and the introduction of JMP. A point-and-click interface was added in version 9 in 2004. A social media analytics product was added in 2010.\n\nSAS is a software suite that can mine, alter, manage and retrieve data from a variety of sources and perform statistical analysis on it. SAS provides a graphical point-and-click user interface for non-technical users and more advanced options through the SAS language.\n\nSAS programs have DATA steps, which retrieve and manipulate data, and PROC steps, which analyze the data. Each step consists of a series of statements.\n\nThe DATA step has executable statements that result in the software taking an action, and declarative statements that provide instructions to read a data set or alter the data's appearance. The DATA step has two phases: compilation and execution. In the compilation phase, declarative statements are processed and syntax errors are identified. Afterwards, the execution phase processes each executable statement sequentially. Data sets are organized into tables with rows called \"observations\" and columns called \"variables\". Additionally, each piece of data has a descriptor and a value.\n\nThe PROC step consists of PROC statements that call upon named procedures. Procedures perform analysis and reporting on data sets to produce statistics, analyses, and graphics. There are more than 300 procedures and each one contains a substantial body of programming and statistical work. PROC statements can also display results, sort data or perform other operations.\n\nSAS macros are pieces of code or variables that are coded once and referenced to perform repetitive tasks.\n\nSAS data can be published in HTML, PDF, Excel and other formats using the Output Delivery System, which was first introduced in 2007. The SAS Enterprise Guide is SAS's point-and-click interface. It generates code to manipulate data or perform analysis automatically and does not require SAS programming experience to use.\n\nThe SAS software suite has more than 200 components Some of the SAS components include:\n\nThe development of SAS began in 1966 after North Carolina State University re-hired Anthony Barr to program his analysis of variance and regression software so that it would run on IBM System/360 computers. The project was funded by the National Institute of Health and was originally intended to analyze agricultural data to improve crop yields. Barr was joined by student James Goodnight, who developed the software's statistical routines, and the two became project leaders. In 1968, Barr and Goodnight integrated new multiple regression and analysis of variance routines. In 1972, after issuing the first release of SAS, the project lost its funding. According to Goodnight, this was because NIH only wanted to fund projects with medical applications. Goodnight continued teaching at the university for a salary of $1 and access to mainframe computers for use with the project, until it was funded by the University Statisticians of the Southern Experiment Stations the following year. John Sall joined the project in 1973 and contributed to the software's econometrics, time series, and matrix algebra. Another early participant, Caroll G. Perkins, contributed to SAS' early programming. Jolayne W. Service and Jane T. Helwig created SAS' first documentation.\n\nThe first versions of SAS were named after the year in which they were released. In 1971, SAS 71 was published as a limited release. It was used only on IBM mainframes and had the main elements of SAS programming, such as the DATA step and the most common procedures in the PROC step. The following year a full version was released as SAS 72, which introduced the MERGE statement and added features for handling missing data or combining data sets. In 1976, Barr, Goodnight, Sall, and Helwig removed the project from North Carolina State and incorporated it into SAS Institute, Inc.\n\nSAS was re-designed in SAS 76 with an open architecture that allowed for compilers and procedures. The INPUT and INFILE statements were improved so they could read most data formats used by IBM mainframes. Generating reports was also added through the PUT and FILE statements. The ability to analyze general linear models was also added as was the FORMAT procedure, which allowed developers to customize the appearance of data. In 1979, SAS 79 added support for the CMS operating system and introduced the DATASETS procedure. Three years later, SAS 82 introduced an early macro language and the APPEND procedure.\n\nSAS version 4 had limited features, but made SAS more accessible. Version 5 introduced a complete macro language, array subscripts, and a full-screen interactive user interface called Display Manager. In 1985, SAS was rewritten in the C programming language. This allowed for the SAS' Multivendor Architecture that allows the software to run on UNIX, MS-DOS, and Windows. It was previously written in PL/I, Fortran, and assembly language.\n\nIn the 1980s and 1990s, SAS released a number of components to complement Base SAS. SAS/GRAPH, which produces graphics, was released in 1980, as well as the SAS/ETS component, which supports econometric and time series analysis. A component intended for pharmaceutical users, SAS/PH-Clinical, was released in the 1990s. The Food and Drug Administration standardized on SAS/PH-Clinical for new drug applications in 2002. Vertical products like SAS Financial Management and SAS Human Capital Management (then called CFO Vision and HR Vision respectively) were also introduced. JMP was developed by SAS co-founder John Sall and a team of developers to take advantage of the graphical user interface introduced in the 1984 Apple Macintosh and shipped for the first time in 1989. Updated versions of JMP were released continuously after 2002 with the most recent release being from 2016.\n\nSAS version 6 was used throughout the 1990s and was available on a wider range of operating systems, including Macintosh, OS/2, Silicon Graphics, and Primos. SAS introduced new features through dot-releases. From 6.06 to 6.09, a user interface based on the windows paradigm was introduced and support for SQL was added. Version 7 introduced the Output Delivery System (ODS) and an improved text editor. ODS was improved upon in successive releases. For example, more output options were added in version 8. The number of operating systems that were supported was reduced to UNIX, Windows and z/OS, and Linux was added. SAS version 8 and SAS Enterprise Miner were released in 1999.\n\nIn 2002, the Text Miner software was introduced. Text Miner analyzes text data like emails for patterns in Business Intelligence applications. In 2004, SAS Version 9.0 was released, which was dubbed \"Project Mercury\" and was designed to make SAS accessible to a broader range of business users. Version 9.0 added custom user interfaces based on the user's role and established the point-and-click user interface of SAS Enterprise Guide as the software's primary graphical user interface (GUI). The Customer Relationship Management (CRM) features were improved in 2004 with SAS Interaction Management. In 2008 SAS announced Project Unity, designed to integrate data quality, data integration and master data management.\n\nSAS sued World Programming, the developers of a competing implementation, World Programming System, alleging that they had infringed SAS's copyright in part by implementing the same functionality. This case was referred from the United Kingdom's High Court of Justice to the European Court of Justice on 11 August 2010. In May 2012, the European Court of Justice ruled in favor of World Programming, finding that \"the functionality of a computer program and the programming language cannot be protected by copyright.\"\n\nA free version was introduced for students in 2010. SAS Social Media Analytics, a tool for social media monitoring, engagement and sentiment analysis, was also released that year. SAS Rapid Predictive Modeler (RPM), which creates basic analytical models using Microsoft Excel, was introduced that same year. JMP 9 in 2010 added a new interface for using the R programming language from JMP and an add-in for Excel. The following year, a High Performance Computing appliance was made available in a partnership with Teradata and EMC Greenplum. In 2011, the company released Enterprise Miner 7.1. The company introduced 27 data management products from October 2013 to October 2014 and updates to 160 others. At the 2015 SAS Global Forum, it announced several new products that were specialized for different industries, as well as new training software.\n\nSAS had many releases since 1972. Since release 9.3, SAS/STAT has its own release numbering.\nAs of 2011 SAS's largest set of products is its line for customer intelligence. Numerous SAS modules for web, social media and marketing analytics may be used to profile customers and prospects, predict their behaviors and manage and optimize communications. SAS also provides the SAS Fraud Framework. The framework's primary functionality is to monitor transactions across different applications, networks and partners and use analytics to identify anomalies that are indicative of fraud. SAS Enterprise GRC (Governance, Risk and Compliance) provides risk modeling, scenario analysis and other functions in order to manage and visualize risk, compliance and corporate policies. There is also a SAS Enterprise Risk Management product-set designed primarily for banks and financial services organizations.\n\nSAS' products for monitoring and managing the operations of IT systems are collectively referred to as SAS IT Management Solutions. SAS collects data from various IT assets on performance and utilization, then creates reports and analyses. SAS' Performance Management products consolidate and provide graphical displays for key performance indicators (KPIs) at the employee, department and organizational level. The SAS Supply Chain Intelligence product suite is offered for supply chain needs, such as forecasting product demand, managing distribution and inventory and optimizing pricing. There is also a \"SAS for Sustainability Management\" set of software to forecast environmental, social and economic effects and identify causal relationships between operations and an impact on the environment or ecosystem.\n\nSAS has product sets for specific industries, such as government, retail, telecommunications and aerospace and for marketing optimization or high-performance computing.\n\nIn a 2005 article for the \"Journal of Marriage and Family\" comparing statistical packages from SAS and its competitors Stata and SPSS, Alan C. Acock wrote that SAS programs provide \"extraordinary range of data analysis and data management tasks,\" but were difficult to use and learn. SPSS and Stata, meanwhile, were both easier to learn (with better documentation) but had less capable analytic abilities, though these could be expanded with paid (in SPSS) or free (in Stata) add-ons. Acock concluded that SAS was best for power users, while occasional users would benefit most from SPSS and Stata. A comparison by the University of California, Los Angeles, gave similar results.\n\nCompetitors such as Revolution Analytics and Alpine Data Labs advertise their products as considerably cheaper than SAS'. In a 2011 comparison, Doug Henschen of \"InformationWeek\" found that start-up fees for the three are similar, though he admitted that the starting fees were not necessarily the best basis for comparison. SAS' business model is not weighted as heavily on initial fees for its programs, instead focusing on revenue from annual subscription fees.\n\nAccording to IDC, SAS is the largest market-share holder in \"advanced analytics\" with 35.4 percent of the market as of 2013. It is the fifth largest market-share holder for business intelligence (BI) software with a 6.9% share and the largest independent vendor. It competes in the BI market against conglomerates, such as SAP BusinessObjects, IBM Cognos, SPSS Modeler, Oracle Hyperion, and Microsoft BI. SAS has been named in the Gartner Leader's Quadrant for Data Integration Tool and for Business Intelligence and Analytical Platforms.\nA study published in 2011 in \"BMC Health Services Research\" found that SAS was used in 42.6 percent of data analyses in health service research, based on a sample of 1,139 articles drawn from three journals.\n\n\n"}
{"id": "50318", "url": "https://en.wikipedia.org/wiki?curid=50318", "title": "Symmetric multiprocessing", "text": "Symmetric multiprocessing\n\nSymmetric multiprocessing (SMP) involves a multiprocessor computer hardware and software architecture where two or more identical processors are connected to a single, shared main memory, have full access to all input and output devices, and are controlled by a single operating system instance that treats all processors equally, reserving none for special purposes. Most multiprocessor systems today use an SMP architecture. In the case of multi-core processors, the SMP architecture applies to the cores, treating them as separate processors.\n\nProfessor John D. Kubiatowicz considers traditionally SMP systems to contain processors without caches. Culler and Pal-Singh in their 1998 book \"Parallel Computer Architecture: A Hardware/Software Approach\" mention: \"The term SMP is widely used but causes a bit of confusion. [...] The more precise description of what is intended by SMP is a shared memory multiprocessor where the cost of accessing a memory location is the same for all processors; that is, it has uniform access costs when the access actually is to memory. If the location is cached, the access will be faster. but cache access times and memory access times are the same on all processors.\"\n\nSMP systems are \"tightly coupled multiprocessor systems\" with a pool of homogeneous processors running independently of each other. Each processor, executing different programs and working on different sets of data, has the capability of sharing common resources (memory, I/O device, interrupt system and so on) that are connected using a system bus or a crossbar.\n\nSMP systems have centralized shared memory called \"main memory\" (MM) operating under a single operating system with two or more homogeneous processors. Usually each processor has an associated private high-speed memory known as cache memory (or cache) to speed up the main memory data access and to reduce the system bus traffic.\n\nProcessors may be interconnected using buses, crossbar switches or on-chip mesh networks. The bottleneck in the scalability of SMP using buses or crossbar switches is the bandwidth and power consumption of the interconnect among the various processors, the memory, and the disk arrays. Mesh architectures avoid these bottlenecks, and provide nearly linear scalability to much higher processor counts at the sacrifice of programmability:\n\nSerious programming challenges remain with this kind of architecture because it requires two distinct modes of programming; one for the CPUs themselves and one for the interconnect between the CPUs. A single programming language would have to be able to not only partition the workload, but also comprehend the memory locality, which is severe in a mesh-based architecture.\n\nSMP systems allow any processor to work on any task no matter where the data for that task is located in memory, provided that each task in the system is not in execution on two or more processors at the same time. With proper operating system support, SMP systems can easily move tasks between processors to balance the workload efficiently.\n\nThe earliest production system with multiple identical processors was the Burroughs B5000, which was functional around 1961. However at run-time this was asymmetric, with one processor restricted to application programs while the other processor mainly handled the operating system and hardware interrupts. The Burroughs D825 first implemented SMP in 1962.\n\nIBM offered dual-processor computer systems based on its System/360 model 65 and the closely related model 67 and 67-2. The operating systems that ran on these machines were OS/360 M65MP and TSS/360. Other software developed at universities, notably the Michigan Terminal System (MTS), used both CPUs. Both processors could access data channels and initiate I/O. In OS/360 M65MP, peripherals could generally be attached to either processor since the operating system kernel ran on both processors (though with a \"big lock\" around the I/O handler). The MTS supervisor (UMMPS) has the ability to run on both CPUs of the IBM System/360 model 67-2. Supervisor locks were small and used to protect individual common data structures that might be accessed simultaneously from either CPU.\n\nOther mainframes that supported SMP included the UNIVAC 1108 II, released in 1965, which supported up to three CPUs, and the GE-635 and GE-645, although GECOS on multiprocessor GE-635 systems ran in a master-slave asymmetric fashion, unlike Multics on multiprocessor GE-645 systems, which ran in a symmetric fashion.\n\nStarting with its version 7.0 (1972), Digital Equipment Corporation's operating system TOPS-10 implemented the SMP feature, the earliest system running SMP was the DECSystem 1077 dual KI10 processor system. Later KL10 system could aggregate up to 8 CPUs in a SMP manner. In contrast, DECs first multi-processor VAX system, the VAX-11/782, was asymmetric, but later VAX multiprocessor systems were SMP.\n\nEarly commercial Unix SMP implementations included the Sequent Computer Systems Balance 8000 (released in 1984) and Balance 21000 (released in 1986). Both models were based on 10 MHz National Semiconductor NS32032 processors, each with a small write-through cache connected to a common memory to form a shared memory system. Another early commercial Unix SMP implementation was the NUMA based Honeywell Information Systems Italy XPS-100 designed by Dan Gielan of VAST Corporation in 1985. Its design supported up to 14 processors, but due to electrical limitations, the largest marketed version was a dual processor system. The operating system was derived and ported by VAST Corporation from AT&T 3B20 Unix SysVr3 code used internally within AT&T.\n\nEarlier non-commercial multiprocessing UNIX ports existed, including a port named MUNIX created at the Naval Postgraduate School by 1975.\n\nTime-sharing and server systems can often use SMP without changes to applications, as they may have multiple processes running in parallel, and a system with more than one process running can run different processes on different processors.\n\nOn personal computers, SMP is less useful for applications that have not been modified. If the system rarely runs more than one process at a time, SMP is useful only for applications that have been modified for multithreaded (multitasked) processing. Custom-programmed software can be written or modified to use multiple threads, so that it can make use of multiple processors.\n\nMultithreaded programs can also be used in time-sharing and server systems that support multithreading, allowing them to make more use of multiple processors.\n\nIn SMP, all of the processors are tightly coupled inside the same box with a bus or switch. Some of the components that are shared are global memory, disks, and I/O devices. Only one copy of an OS runs on all the processors, and the OS must be designed to take advantage of this architecture. Some of the basic advantages involves cost-effective ways to increase throughput. To solve different problems and tasks, SMP applies multiple processors to that one problem, known as parallel programming.\n\nHowever, there are a few limits on the scalability of SMP due to cache coherence and shared objects.\n\nUniprocessor and SMP systems require different programming methods to achieve maximum performance. Programs running on SMP systems may experience an increase in performance even when they have been written for uniprocessor systems. This is because hardware interrupts usually suspends program execution while the kernel that handles them can execute on an idle processor instead. The effect in most applications (e.g. games) is not so much a performance increase as the appearance that the program is running much more smoothly. Some applications, particularly building software and some distributed computing projects, run faster by a factor of (nearly) the number of additional processors. (Compilers by themselves are single threaded, but, when building a software project with multiple compilation units, if each compilation unit is handled independently, this creates an embarrassingly parallel situation across the entire multi-compilation-unit project, allowing near linear scaling of compilation time. Distributed computing projects are inherently parallel by design.)\n\nSystems programmers must build support for SMP into the operating system, otherwise, the additional processors remain idle and the system functions as a uniprocessor system.\n\nSMP systems can also lead to more complexity regarding instruction sets. A homogeneous processor system typically requires extra registers for \"special instructions\" such as SIMD (MMX, SSE, etc.), while a heterogeneous system can implement different types of hardware for different instructions/uses.\n\nWhen more than one program executes at the same time, an SMP system has considerably better performance than a uni-processor, because different programs can run on different CPUs simultaneously. Similarly, Asymmetric multiprocessing (AMP) usually allows only one processor to run a program or task at a time. For example, AMP can be used in assigning specific tasks to CPU based to priority and importance of task completion. AMP was created well before SMP in terms of handling multiple CPUs, which explains the lack of performance based on the example provided.\n\nIn cases where an SMP environment processes many jobs, administrators often experience a loss of hardware efficiency. Software programs have been developed to schedule jobs and other functions of the computer so that the processor utilization reaches its maximum potential. Good software packages can achieve this maximum potential by scheduling each CPU separately, as well as being able to integrate multiple SMP machines and clusters.\n\nAccess to RAM is serialized; this and cache coherency issues causes performance to lag slightly behind the number of additional processors in the system.\n\nSMP uses a single shared system bus that represents one of the earliest styles of multiprocessor machine architectures, typically used for building smaller computers with up to 8 processors.\n\nLarger computer systems might use newer architectures such as NUMA (Non-Uniform Memory Access), which dedicates different memory banks to different processors. In a NUMA architecture, processors may access local memory quickly and remote memory more slowly. This can dramatically improve memory throughput as long as the data are localized to specific processes (and thus processors). On the downside, NUMA makes the cost of moving data from one processor to another, as in workload balancing, more expensive. The benefits of NUMA are limited to particular workloads, notably on servers where the data are often associated strongly with certain tasks or users.\n\nFinally, there is computer clustered multiprocessing (such as Beowulf), in which not all memory is available to all processors. Clustering techniques are used fairly extensively to build very large supercomputers.\n\nVariable Symmetric Multiprocessing (vSMP) is a specific mobile use case technology initiated by NVIDIA. This technology includes an extra fifth core in a quad-core device, called the Companion core, built specifically for executing tasks at a lower frequency during mobile active standby mode, video playback, and music playback.\n\nProject Kal-El (Tegra 3), patented by NVIDIA, was the first SoC (System on Chip) to implement this new vSMP technology. This technology not only reduces mobile power consumption during active standby state, but also maximizes quad core performance during active usage for intensive mobile applications. Overall this technology addresses the need for increase in battery life performance during active and standby usage by reducing the power consumption in mobile processors.\n\nUnlike current SMP architectures, the vSMP Companion core is OS transparent meaning that the operating system and the running applications are totally unaware of this extra core but are still able to take advantage of it. Some of the advantages of the vSMP architecture includes cache coherency, OS efficiency, and power optimization. The advantages for this architecture are explained below:\n\n\nThese advantages lead the vSMP architecture to considerably benefit over other architectures using asynchronous clocking technologies.\n\n\n"}
{"id": "5230467", "url": "https://en.wikipedia.org/wiki?curid=5230467", "title": "Systems for Nuclear Auxiliary Power", "text": "Systems for Nuclear Auxiliary Power\n\nThe Systems Nuclear Auxiliary POWER (SNAP) program was a program of experimental radioisotope thermoelectric generators (RTGs) and space nuclear reactors flown during the 1960s by NASA.\n\nSNAP-1 was a test platform, never deployed. Used cerium-144 in a Rankine cycle, with mercury as the heat transfer fluid. Operated successfully for 2500 hours.\n\nSNAP-3 was first RTG used in a space mission (1961). Launched aboard U.S. Navy Transit 4A and 4B navigation satellites. The output of this RTG was a mere \"2.5-watts.\"\n\nSNAP-7 was designed for marine applications such as lighthouses and buoys; at least six units were deployed in the mid-1960s, with names SNAP-7A through SNAP-7F. SNAP-7D produced thirty watts of electric power using (about four kilograms) of strontium-90 as SrTiO. These were very large units, weighing between .\n\nAfter SNAP-3 on Transit 4A/B, SNAP-9A units served aboard many of the Transit satellite series. In April 1964 a SNAP-9A failed to achieve orbit and disintegrated, dispersing roughly of plutonium-238 over all continents. Most plutonium fell in the southern hemisphere. Estimated 6300GBq or 2100 man-Sv of radiation was released and led to NASA's development of solar photovoltaic energy technology.\n\nSNAP-11, an experimental RTG intended to power the Surveyor probes during the lunar night. They were to be powered by curium-242 (900 watts thermal) and produce 25 watts of electricity for 130 days. Designed with hot junction and cold junction. They had a liquid NaK thermal control system and a movable shutter to dump excess heat. They were not used on the Surveyor missions.\n\"In general, the SNAP 11 fuel block is a cylindrical multi-material unit which occupies the internal volume of the generator. TZM (molybdenum alloy) fuel capsule, fueled with curium-242 (CmO in an iridium matrix) is located in the center of the fuel block. capsule is surrounded by a platinum sphere, approximately 2 - 1 / 4 inches in diameter, which provides shielding and acts as an energy absorber for impact considerations. This assembly is enclosed in graphite and beryllium sub-assemblies to provide the proper thermal distribution and ablative protection.\"\nSNAP-19(B) was developed for the Nimbus-B satellite. \"The SNAP 19 generators are fuelled with plutonium 238 and employ lead telluride thermoelectric couples for energy conversion. Each of the [2] electrically paralleled generators produces approximately 30 watts at beginning of life. Each generator ... weighs less than 35 pounds and is ... 6-1/2 inches in diameter by 10-3/4 inches high. [extended by] six fins.\" Nimbus 3 used a SNAP-19B (with the recovered fuel from the Nimbus-B1 attempt).\n\nSNAP-19s powered Pioneer 10 and Pioneer 11 missions. They used P and N doped 'TAGS' (Ag—Ge—Sb—Te) thermoelectric elements.\n\nModified SNAP-19s were used for the Viking 1 and Viking 2 landers.\n\nSNAP-19c was used for the Indian CIA operation to track the Chinese missile launches through a Telemetry array at Nanda Devi in Uttarakhand.\n\nSNAP-21 and SNAP-23 were designed for underwater use and used strontium-90 as the radioactive source, encapsulated as either strontium oxide or strontium titanate. They produced about ten watts.\n\nFive SNAP-27 units provided electric power for the Apollo Lunar Surface Experiment Packages (ALSEP) left on the Moon by Apollo 12, 14, 15, 16, and 17. The fuel capsule, containing of plutonium-238 in oxide form (44,500 Ci or 1.65 PBq), was carried to the Moon in a separate fuel cask attached to the side of the Lunar Module. The fuel cask provided thermal insulation and added structural support to the fuel capsule. On the Moon, the Lunar Module pilot removed the fuel capsule from the cask and inserted it in the RTG.\n\nThese stations transmitted information about moonquakes and meteor impacts, lunar magnetic and gravitational fields, the Moon's internal temperature, and the Moon's atmosphere for several years after the missions. After ten years, a SNAP-27 still produced more than 90% of its initial output of 70 watts.\n\nThe fuel cask from the SNAP-27 unit carried by the Apollo 13 mission currently lies in of water at the bottom of the Tonga Trench in the Pacific Ocean. This mission failed to land on the moon, and the lunar module carrying its generator burnt up during re-entry into the Earth's atmosphere, with the trajectory arranged so that the cask would land in the trench. The cask survived re-entry, as it was designed to do, and no release of plutonium has been detected. The corrosion resistant materials of the capsule are expected to contain it for 10 half-lives (870 years).\n\nA series of compact nuclear reactors primarily developed for the U.S. government by the Atomics International division of North American Aviation.\n\nThe SNAP Experimental Reactor (SER) was the first reactor to be built by the specifications established for space satellite applications. The SER used uranium zirconium hydride as the fuel and eutectic sodium-potassium alloy (NaK) as the coolant and operated at approximately 50 kW thermal. The system did not have a power conversion but used a secondary heat air blast system to dissipate the heat to the atmosphere. The SER used a similar reactor reflector moderator device as the SNAP-10A but with only one reflector. Criticality was achieved in September 1959 with final shutdown completed in December 1961. The project was considered a success. It gave continued confidence in the development of the SNAP Program and it also led to in depth research and component development.\n\nThe SNAP-2 Developmental Reactor was the second SNAP reactor built. This device used Uranium-zirconium hydride fuel and had a design reactor power of 55 kW. It was the first model to use a flight control assembly and was tested from April 1961 to December 1962. The basic concept was that nuclear power would be a long term source of energy for manned space capsules. However, the crew capsule had to be shielded from deadly radiation streaming from the nuclear reactor. Surrounding the reactor with a radiation shield was out of the question. It would be far too heavy to launch with the rockets available at that time. To protect the \"crew\" and \"payload\", the SNAP-2 system used a \"shadow shield\". The shield was a truncated cone containing lithium hydride. The reactor was at the small end and the crew capsule/payload was in the shadow of the large end. \n\nStudies were performed on the reactor, individual components and the support system. Atomics International, a division of North American Aviation did the development and testing work. The SNAP-2 Shield Development unit was responsible for developing the radiation shield. Creating the shield meant melting lithium hydride and casting it into the form required. The form was a big truncated cone. Molten lithium hydride had to be poured into the casting mold a little at a time otherwise it would crack as it cooled and solidified. Cracks in the shield material would be fatal to any space crew or payload depending on it because it would allow radiation to stream through to the crew/payload compartment. As the material cooled, it would form kind of a hollowed vortex in the middle. The development engineers had to create ways to fill the vortex while maintaining the shield’s integrity. And, in doing all this they had to keep in mind that they were working with a material that could be explosively unstable in a moist oxygen rich environment. Analysis also revealed that under thermal and radiation gradients, the lithium hydride could disassociate and hydrogen ions could migrate through the shield. This would produce variations of shielding efficacy and could subject the payloads to intense radiation. Efforts were made to mitigate these effects.\n\nThe SNAP 2DR used a similar reactor reflector moderator device as the SNAP-10A but with two movable and internal fixed reflectors. The system was designed so that the reactor could be integrated with a mercury Rankine cycle to generate 3.5 kW of electricity.\n\nThe SNAP-8 reactors were designed, constructed and operated by Atomics International under contract with the National Aeronautics and Space Administration. Two SNAP-8 reactors were produced: The SNAP 8 Experimental Reactor and the SNAP 8 Developmental Reactor. Both SNAP 8 reactors used the same highly enriched uranium zirconium hydride fuel as the SNAP 2 and SNAP 10A reactors. The SNAP 8 design included primary and secondary NaK loops to transfer heat to the mercury rankine power conversion system. The electrical generating system for the SNAP 8 reactors was supplied by Aerojet General.\n\nThe SNAP 8 Experimental Reactor was a 600 kW reactor that was tested from 1963 to 1965.\nThe SNAP 8 Developmental Reactor had a reactor core measuring , contained a total of of fuel, had a power rating of 1 MW. The reactor was tested in 1969 at the Santa Susana Field Laboratory.\n\nThe SNAP-10A was a space-qualified nuclear reactor power system. It was built as a research project for the Air Force, to demonstrate the capability to generate higher power than RTGs. The reactor employed two moveable beryllium reflectors for control, and generated 35 kW at beginning of life. The system generated electricity by circulating NaK around lead tellurium thermocouples. To mitigate launch hazards, the reactor was never started until it reached a safe orbit. It was launched into earth orbit in April, 1965, and used to power an Agena-D research satellite, built by Lockheed/Martin. The system produced 500W of electrical power during an abbreviated 43-day flight test. The reactor was prematurely shut down by a faulty command receiver. It is predicted to remain in orbit for 4,000 years. \n\n"}
{"id": "653630", "url": "https://en.wikipedia.org/wiki?curid=653630", "title": "Tab stop", "text": "Tab stop\n\nA tab stop on a typewriter is a location where the carriage movement is halted by an adjustable end stop. Tab stops are set manually, and pressing the tab key causes the carriage to go to the next tab stop. In text editors on a computer, the same concept is implemented simplistically with automatic, fixed tab stops.\n\nModern word processors generalize this concept by offering tab stops that have an alignment attribute and cause the text to be automatically aligned at left, at right or center of the tab stop itself. Such tab stops are paragraph-specific properties and can be moved to a different location in any moment, or even removed.\n\nA tab stop is a horizontal position which is set for placing and aligning text on a page. There are at least five kinds of tab stops in general usage in word processing or in MS Word.\n\n\nElastic tab stops were invented by Nick Gravgaard in 2006 as an alternative way to handle tab stops in digital text files with a primary focus on editing source code. This means users need just one tab character between columns rather than inserting the exact number of spaces or fixed tabs on each line to make text line up. Unlike fixed tab stops, they automatically keep columns aligned, making them useful for viewing or editing tab-delimited text. When text is edited, tab stops on adjacent lines above and below the \"cell\" that is being changed are automatically moved to fit the widest cell of text in that column.\n\nElastic tab stops have been implemented in places such as Visual Studio as an extension, and in Go's tabwriter package used by the codice_1 command.\n\n"}
{"id": "19677025", "url": "https://en.wikipedia.org/wiki?curid=19677025", "title": "TechTown (Detroit)", "text": "TechTown (Detroit)\n\nTechTown is an urban research and technology business park located just north of the Edsel Ford Freeway (I-94) in the New Center area of Detroit. The organization defines itself as \"a community of entrepreneurs, investors, mentors, service providers and corporate partners creating an internationally recognized entrepreneurial village in the city of Detroit.\" TechTown is part of the Wayne State University system. It was founded in 2000 by Wayne State University, Henry Ford Health System, and General Motors.\n\nThe 12-block, park is located on the border of Detroit's Midtown and New Center. The Ford Piquette Avenue Plant, which churned out the first Model T, is a few blocks east of the research park.\n\n"}
{"id": "1845709", "url": "https://en.wikipedia.org/wiki?curid=1845709", "title": "Unified English Braille", "text": "Unified English Braille\n\nUnified English Braille Code (UEBC, formerly UBC, now usually simply UEB) is an English language Braille code standard, developed to permit representing the wide variety of literary and technical material in use in the English-speaking world today, in uniform fashion.\n\nStandard 6-dot braille only provides 63 distinct characters (not including the space character), and thus, over the years a number of distinct rule-sets have been developed to represent literary text, mathematics, scientific material, computer software, the @ symbol used in email addresses, and other varieties of written material. Different countries also used differing encodings at various times: during the 1800s American Braille competed with English Braille, in the War of the Dots. As a result of the expanding need to represent technical symbolism, and divergence during the past 100 years across countries, braille users who desired to read or write a large range of material have needed to learn different sets of rules, depending on what kind of material they were reading at a given time. Rules for a particular type of material were often not compatible from one system to the next (the rule-sets for literary/mathematical/computerized encoding-areas were sometimes conflicting—and of course differing approaches to encoding mathematics were not compatible with each other), so the reader would need to be notified as the text in a book moved from computer braille code for programming to Nemeth Code for mathematics to standard literary braille. Moreover, the braille rule-set used for math and computer science topics, and even to an extent braille for literary purposes, differed among various English-speaking countries.\n\nUnified English Braille is intended to develop one set of rules, the same everywhere in the world, which could be applied across various types of English-language material. The notable exception to this unification is Music Braille, which UEB specifically does not encompass, because it is already well-standardized internationally. Unified English Braille is designed to be readily understood by people familiar with the literary braille (used in standard prose writing), while also including support for specialized math and science symbols, computer-related symbols (the @ sign as well as more specialised programming-language syntax), foreign alphabets, and visual effects (bullets, bold type, accent marks, and so on).\n\nAccording to the original 1991 specification for UEB, the goals were:\n\nSome goals were specially and explicitly called out as key objectives, not all of which are mentioned above:\n\nGoals which were specifically not part of the UEB upgrade process were the ability to handle languages outside the Roman alphabet (cf. the various national variants of ASCII in the ISO 8859 series versus the modern pan-universal Unicode standard, which governs how writing systems are encoded for computerized use).\n\nWork on UEB formally began in 1991, and preliminary draft standard was published in March 1995 (as UBC), then upgraded several times thereafter. Unified English Braille (UEB) was originally known as Unified Braille Code (UBC), with the English-specific nature being implied, but later the word \"English\" was formally incorporated into its name—Unified English Braille Code (UEBC)—and still more recently it has come to be called Unified English Braille (UEB). On April 2, 2004, the International Council on English Braille (ICEB) gave the go-ahead for the unification of various English braille codes. This decision was reached following 13 years of analysis, research, and debate. ICEB said that Unified English Braille was sufficiently complete for recognition as an international standard for English braille, which the seven ICEB member-countries could consider for adoption as their national code. South Africa adopted the UEB almost immediately (in May 2004). During the following year, the standard was adopted by Nigeria (February 5, 2005), Australia (May 14, 2005), and New Zealand (November 2005). On April 24, 2010, the Canadian Braille Authority (CBA) voted to adopt UEB, making Canada the fifth nation to adopt UEB officially. On October 21, 2011, the UK Association for Accessible Formats voted to adopt UEB as the preferred code in the UK. On November 2, 2012 the Braille Authority of North America (BANA) became the sixth of the seven member-countries of the ICEB to officially adopt the UEB.\n\nThe major criticism against UEB is that it fails to handle mathematics or computer science as compactly as codes designed to be optimal for those disciplines. Besides requiring more space to represent and more time to read and write, the verbosity of UEB can make learning mathematics more difficult. Nemeth Braille, officially used in the United States since 1952, and as of 2002 the de facto standard for teaching and doing mathematics in braille in the USA, was specifically invented to correct the cumbersomeness of doing mathematics in braille. However, although the Nemeth encoding standard was officially adopted by the JUTC of the USA and the UK in the 1950s, in practice only the USA switched their mathematical braille to the Nemeth system, whereas the UK continued to use the traditional Henry Martyn Taylor coding (not to be confused with Hudson Taylor who was involved with the use of Moon Type for the blind in China during the 1800s) for their braille mathematics. Programmers in the United States who write their programming codefiles in braille—as opposed to in ASCII text with use of a screenreader for example—tend to use Nemeth-syntax numerals, whereas programmers in the UK use yet another system (not Taylor-numerals and not literary-numerals). The key difference of Nemeth Braille compared to Taylor (and UEB which uses an upgraded version of the Taylor encoding for math) is that Nemeth uses \"down-shifted\" numerals from the fifth decade of the Braille alphabet (overwriting various punctuation characters), whereas UEB/Taylor uses the traditional 1800s approach with \"up-shifted\" numerals from the first decade of the (English) Braille alphabet (overwriting the first ten letters, namely ABCDEFGHIJ). Traditional 1800s Braille, and also UEB, require insertion of numeral-prefixes when speaking of numerals, which makes representing some mathematical equations 42% more verbose. As an alternative to UEB, there were proposals in 2001 and 2009, and most recently these were the subject of various technical workshops during 2012. Although UEB adopts some features of Nemeth, the final version of UEB mandates up-shifted numerals, which are the heart of the controversy. According to BANA, which adopted UEB in 2012, the official braille codes for the USA will be UEB and Nemeth Braille (as well as Music Braille for vocals and instrumentals plus IPA Braille for phonetic linguistics), despite the use of contradictory representation of numerals and arithmetical symbols in the UEB and Nemeth encodings. Thus, although UEB has officially been adopted in most English-speaking ICEB member-countries, in the USA (and possibly the UK where UEB is only the \"preferred\" system) the new encoding is not to be the sole encoding.\n\nAnother proposed braille-notation for encoding math is GS8/GS6, which was specifically invented in the early 1990s as an attempt to get rid of the \"up-shifted\" numerals used in UEB—see Gardner–Salinas Braille. GS6 implements \"extra-dot\" numerals from the fourth decade of the English Braille alphabet (overwriting various two-letter ligatures). GS8 expands the braille-cell from 2×3 dots to 2×4 dots, quadrupling the available codepoints from the traditional 64 up to 256, but in GS8 the numerals are still represented in the same way as in GS6 (albeit with a couple unused dot-positions at the bottom).\n\nAttempts to give the numerals their own distinct position in Braille are not new: the original 1829 specification by Louis Braille gave the numerals their own distinct symbols, with the modern digraph-based literary-braille approach mentioned as an optional fallback. However, after trying the system out in the classroom, the dashes used in the numerals—as well as several other rows of special characters—were found to be too difficult to distinguish from dot-pairs, and thus the typical digraph-based numerals became the official standard in 1837.\n\nAs of 2013, with the majority of English-speaking ICEB member-countries having officially adopted UEB, there remain barriers to implementation and deployment. Besides ICEB member-nations, there are also many other countries with blind citizens that teach and use English: India, Hong Kong/China, Pakistan, the Philippines, and so on. Many of these countries use non-UEB math notation, for English-speaking countries specifically, versions of the Nemeth Code were widespread by 1990 (in the United States, Western Samoa, Canada including Quebec, New Zealand, Israel, Greece, India, Pakistan, Sri Lanka, Thailand, Malaysia, Indonesia, Cambodia, Vietnam, and Lebanon) in contrast to the similar-to-UEB-but-not-identical Taylor notation in 1990 (used by the UK, Ireland, Australia, Nigeria, Hong Kong, Jordan, Kenya, Sierra Leone, Singapore, and Zimbabwe). Some countries in the Middle East used Nemeth and Taylor math-notations as of 1990, i.e. Iran and Saudi Arabia. As of 2013, it is unclear whether the English-using blind populations of various ICEB and non-ICEB nations will move to adopt the UEB, and if so, at what rate. Beyond official adoption rates in schools and by individuals, there are other difficulties. The vast majority of existing Braille materials, both printed and electronic, are in non-UEB encodings. Furthermore, other technologies that compete with braille are now ever-more-widely affordable (screen readers for electronic-text-to-speech, plus physical-pages-to-electronic-text software combined with high-resolution digital cameras and high-speed document scanners, and the increasing ubiquity of tablets/smartphones/PDAs/PCs). The percentage of blind children who are literate in braille is already declining—and even those who know some system tend not to know UEB, since that system is still very new. Still, as of 2012 many of the original goals for UEB have already been fully or partially accomplished:\n\n\n"}
{"id": "23867229", "url": "https://en.wikipedia.org/wiki?curid=23867229", "title": "Union of Beverage and Related Industry Workers", "text": "Union of Beverage and Related Industry Workers\n\nSindicato de Trabajadores de la Industria de la Bebida y Similares (Spanish for \"Union of Beverage and Related Industry Workers\") is a trade union in Honduras founded in 1959. It is affiliated to FUTH, a trade union congress, and internationally to the IUF. STIBYS is working together with other left-wing organizations in the CNRP (National Coordination of Civil Resistance) and has been active in the resistance against the 2009 Honduras coup d'état.\n\nThe current general secretary of STIBYS is Carlos Humberto Reyes, who is standing in the November 2009 elections as an independent presidential candidate.\n"}
{"id": "3460556", "url": "https://en.wikipedia.org/wiki?curid=3460556", "title": "Wireless Industrial Networking Alliance", "text": "Wireless Industrial Networking Alliance\n\nThe Wireless Industrial Networking Alliance (WINA) is a coalition of industrial end-user companies, technology suppliers, industry organizations, software developers, system integrators, and others interested in the advancement of wireless solutions for industry.\n\nWINA's primary task is to develop standards organizations to assure their relevance to industrial requirements.\n\nAdditionally, WINA offer webinars to help educate the industry on the developing wireless industry.\n\n"}
{"id": "39443872", "url": "https://en.wikipedia.org/wiki?curid=39443872", "title": "Young Achievers Award", "text": "Young Achievers Award\n\nThe Young Achievers Awards is a national competition held annually in Uganda which selects and promotes the best practice and excellence in youth creativity. By demonstrating a young person’s potential to create an outstanding nation built on entrepreneurship, YAA aims at recognizing Uganda’s leaders of tomorrow today. It provides an opportunity for people from all walks of life to work together to address national developmental challenges in ICT, the Arts, Health, Education, Gender and access to better lives.\n\nFounded in 2009 by two young entreprenuers, Awel Uwihanganye and Ivan Serwano Kyambadde, the Award was created as a platform to showcase Africa’s best talent as an encouragement in their quest to create economic opportunity, attain financial independence and improve their leadership abilities to ultimately play meaningful roles in managing the affairs of society.\n\n\n"}
