{"id": "52032623", "url": "https://en.wikipedia.org/wiki?curid=52032623", "title": "Agile tooling", "text": "Agile tooling\n\nAgile tooling is the design and fabrication of manufacturing related-tools such as dies, molds, patterns, jigs and fixtures in a configuration that aims to maximise the tools' performance, minimise manufacturing time and cost, and avoid delay in prototyping. A fully functional agile tooling laboratory consists of CNC milling, turning and routing equipment. It can also include additive manufacturing platforms (such as fused filament fabrication, selective laser sintering, Stereolithography, and direct metal laser sintering), hydroforming, vacuum forming, die casting, stamping, injection molding and welding equipment.\n\nAgile tooling is similar to rapid tooling, which uses additive manufacturing to make tools or tooling quickly, either directly by making parts that serve as the actual tools or tooling components, such as mold inserts; or indirectly by producing patterns that are in turn used in a secondary process to produce the actual tools. Another similar technique is prototype tooling, where molds, dies and other devices are used to produce prototypes. Rapid manufacturing, and specifically rapid tooling technologies, are earlier in their development than rapid prototyping (RP) technologies, and are often extensions of RP.\n\nThe aim of all toolmaking is to catch design errors early in the design process; improve product design better products, reduce product cost, and reduce time to market.\n\nHundreds of universities and research centers around the globe are investing in additive manufacturing equipment in order to be positioned to make prototypes and tactile representations of real parts. Few have fully committed the concept of using additive manufacturing (AM) to create manufacturing tools (fixturing, clamps, molds, dies, patterns, negatives, etc.). AM experts seem to agree that tooling is a large, namely untapped market. Deloitte University Press estimated that in 2012 alone, the AM Tooling market $1.2 Billion. At that point in the development cycle of AM Tooling, much of the work was performed under the guise of “let’s try it and see what happens”.\n\nAdditive manufacturing, starting with today's infancy period, requires manufacturing firms to be flexible, ever-improving users of all available technologies to remain competitive. Advocates of additive manufacturing also predict that this arc of technological development will counter globalization, as end users will do much of their own manufacturing rather than engage in trade to buy products from other people and corporations. The real integration of the newer additive technologies into commercial production, however, is more a matter of complementing traditional subtractive methods rather than displacing them entirely.\n\nAutomotive – approaching niche vehicle markets (making less than 100, 000 vehicles), rather than high production volume\n\nAircraft – the U.S. aircraft industry operates in an environment where production volumes are relatively low and resulting product costs are relatively high. Agile tooling can be applied in the early design stage of the development cycle to minimize the high cost of redesign.\n\nMedical – cast tooling would benefit a great deal from agile tooling. However, the cost for the tooling may still be significantly greater than the cost of a casting piece, with high lead times. Since only several dozen or several hundred metal parts are needed, the challenge for mass production is still prevalent. A balance between these four areas – quantity, design, material, and speed are key to designing and producing a fully functional product.\n\n"}
{"id": "1800115", "url": "https://en.wikipedia.org/wiki?curid=1800115", "title": "Air horn", "text": "Air horn\n\nAn air horn is a pneumatic device designed to create an extremely loud moan for signaling purposes. It usually consists of a source which produces compressed air, which passes into a horn through a reed or diaphragm. The stream of air causes the reed or diaphragm to vibrate, creating sound waves, then the horn amplifies the sound making it louder. Air horns are widely employed as vehicle horns, installed on large semi-trailer trucks, fire trucks, trains, and some ambulances as a warning device, and on ships as a signaling device.\n\nAn air horn consists of a flaring metal or plastic horn or trumpet (called the \"bell\") attached to a small air chamber containing a metal reed or diaphragm in the throat of the horn. Compressed air flows from an inlet line through a narrow opening past the reed or diaphragm, causing it to vibrate, which creates sound waves. The flaring horn serves as an acoustic \"transformer\" to improve the transfer of sound energy from the diaphragm to the open air, making the sound louder. In most horns it also determines the pitch of the sound. When vibrated by the diaphragm, the column of air in the horn vibrates in standing waves. The length of the horn determines the wavelength of the sound waves generated, and thus the fundamental frequency (pitch) of the note produced by the horn. The longer the horn, the lower the pitch. \n\nLarger air horns used on ships and foghorns function similarly to a whistle; instead of a diaphragm the air escapes from a closed cylindrical resonator chamber through a precisely shaped slit directed against a knife edge (fipple). The air blowing past the knife edge oscillates, creating sound waves. The oscillations excite standing waves in the resonator chamber, so the length of the chamber determines the pitch of the note produced.\n\nIn trucks, the air horn is powered with compressed air from the vehicle's air brake system. A cord mounted on the ceiling of the operator's cab is pulled to open the valve, supplying varying amounts of air to the horn. Thus, an outstretched hand reaching upward and pumping is a signal to the driver of an air horn equipped vehicle, requesting a toot. In modern trucks the horn is actuated by a button on the steering wheel (just like a normal car horn). Some trucks have both electric and air horn, selectable by a switch on the dashboard. This is to prevent the use of the powerful air horn in populated areas.\n\nMany fire trucks, ambulances, and other large emergency vehicles operate air horns as a means of warning vehicles to clear the right-of-way.\n\nThere are also electronic horns for emergency vehicles, which produce a similar easily recognizable sound. These are typically integrated into the same system as the vehicle's electronic siren, and sound through the same speakers. In the last several decades, electronic sound systems with more widely varying frequencies have been chosen as common supplemental warning systems.\n\nOriginally, diesel locomotives were equipped with truck horns. After an accident in which a driver mistook a train for a truck, the need for a unique-sounding train horn became clear. Consequently, North American trains now have at least two horns with different tones forming the airhorn, that sound simultaneously, creating a harmonic interval or chord. Each individual horn is called a \"chime\". Three and five-chime configurations are the most common, but two chime horns also exist. \n\nFifteen to twenty seconds before entering a level crossing, federal law requires locomotives to sound their horns in a standard warning sequence. This succession consists of two long, one short, and one long horn sounding repeated as necessary until the locomotive clears the crossing. Exceptions to federal law occur in locations with established quiet zone ordinances that prohibit sounding locomotive horns.\n\nIn recent years, it has become a fad for car and truck enthusiasts to install large air horns on their vehicles. Some jurisdictions do not allow an airhorn to be attached, whether or not it can be activated.\n\nPortable air horns are also readily available packaged with a can of compressed air as the air source. These are often sounded by fans at sporting events such as American football, basketball, hockey and association football, and at other events such as graduations.\n\nSmall versions are sometimes used as bicycle horns, since it yields a louder warning sound than traditional bicycle bells or bulb reed horns.\n\nAnother use is as a non-lethal weapon for self-defense, mainly as an auditory distraction to get away from an attacker. For outdoor activities like hiking, hunting, cross-country skiing, canoeing, fishing, an air horn can be handy to frighten away unwanted or aggressive wildlife, signalling for help and to announce one's location.\n\nAt close range, they can also be used as freeze sprays (when the container is held upside-down).\n\nAdditionally, air horns (especially those that contain fluorocarbons) have the potential to be abused as a substitute for recreational drugs since many such refrigerants can be inhaled for a quick and dangerous intoxication.\n\nThe air horn is used for signalling in various sports:\n\nNHL arenas normally employ two horns — a high pitch horn (or a siren, as with the Boston Bruins and Montreal Canadiens' home rinks) that announces the end of the period (which is also used to indicate the start of instant replay reviews), and a much louder, lower pitch horn that is sounded when the home team scores and/or wins a game. Some MLB stadiums use horns for similar purposes, such as when a member of the home team hits a home run. In many places, air horns are used for signaling the end of the period or quarter on scoreboard systems. The entire idea of a goal horn in NHL ice hockey is said to have begun in 1974, when Bill Wirtz, then-owner of the Chicago Blackhawks, liked the sound of the horn on his yacht so much, that he had another example of it mounted inside of the \"Madhouse on Madison\" to be sounded whenever the Blackhawks scored a goal in their home rink. \n\nIn mixed martial arts, an air horn is commonly used to signal the end of a round as opposed to the bell used in boxing and professional wrestling.\n\nSamples of various air horns have been used by NHL teams when the home team scores a goal, often accompanied by music.\n\nThe air horn is also a popular sample in reggae music. Jamaican dancehall music was the first musical genre to use the effect, and has been using the airhorn sample for over 26 years, in live shows as well as on mixtape recordings, and in Puerto Rican reggaeton, a reggae hybrid genre since the late '80s and '90s. The sound effect has recently been used in hip hop music as well.\n\nThe \"Air Horn Orchestra\" gathered weekly every Wednesday from April 13, 2016 until the November 2, 2016 at the North Carolina Governor's Mansion to protest HB2 and Governor Pat McCrory by making a racket. The 30th and final performance is expected to set the Guinness World Record for the number of simultaneously sounding air horns, having had 342 participants. This was the last performance owing to the gubernatorial election six days later.\n\n"}
{"id": "51112949", "url": "https://en.wikipedia.org/wiki?curid=51112949", "title": "Ampleon", "text": "Ampleon\n\nAmpleon is a global semiconductor manufacturer spun off from the NXP Semiconductors in May 2015 headquartered in Nijmegen, Netherlands following the acquisition of the NXP Semiconductors RF Power business by the JIC Capital Management for US$1.8 billion. The company employs 1250 people worldwide and focuses on mobile broadband, multimarket, and RF energy electronic products.\n"}
{"id": "11353783", "url": "https://en.wikipedia.org/wiki?curid=11353783", "title": "Atmosphere-Space Interactions Monitor", "text": "Atmosphere-Space Interactions Monitor\n\nAtmosphere-Space Interactions Monitor (ASIM) is a project led by the European Space Agency that will place cameras and X-ray/γ-ray detectors on the International Space Station, where it will observe the upper atmosphere to study sprites, jets and elves and terrestrial gamma-ray flashes in connection with thunderstorms. It is hoped that measurements of these phenomena from space will contribute to the understanding of Earth's upper atmosphere.\n\nThe ASIM components, originally planned to be completed in 2014, were launched on 2 April 2018 and mounted on the \"Columbus\" External Payload Facility on 13 April 2018. Danish tech company Terma A/S is running the technical part of the project for ESA and DTU Space (\"National Space Institute\") from the Technical University of Denmark provides the scientific leadership of the project. Mission operations will be performed by the Belgian User Support and Operations Centre (B.USOC) in Uccle, Belgium.\n\nThe ASIM payload has a mass of and consists of sub-systems CEPA and DHPU, and two scientific instruments called MXGS and MMIA:\n\n\n"}
{"id": "31958408", "url": "https://en.wikipedia.org/wiki?curid=31958408", "title": "BuiltWith", "text": "BuiltWith\n\nBuiltWith is an Internet services company based in Manly, Australia, which launched in August, 2007. The goal of BuiltWith is to help web developers, researchers, and designers track what technologies are being used by other websites, which may help them to decide what technologies to implement themselves.\n\nBuiltWith Trends is a comprehensive resource of what technologies the web has been using since 2010, across the Internet's top one million most visited sites by traffic as ranked by Quantcast. Its data is often quoted in Internet media reports related to internet technology trends.\n\nIn 2010, John Resig opened the JQuery 2010 Keynote with statistics provided by BuiltWith Trends.\n\n\n"}
{"id": "45436992", "url": "https://en.wikipedia.org/wiki?curid=45436992", "title": "CMF design", "text": "CMF design\n\nColor, Materials, Finish (CMF) is an area of industrial design that focuses on the chromatic, tactile and decorative identity of products and environments.\n\nCMF design uses metadesign logic, the simultaneous planning of the identity of entire ranges of products for a given brand. This makes it possible, for example, to adopt a single color matrix, instead of using a series of separate and different color cards for each line of products, as previously done. A contribution to the development of this approach to design was the impetus provided by the proliferation in the 1980s of complete ranges of new systemic products.\n\nBrand products are often thought up by different designers who through the use of ad-hoc CMF design manuals can work together to ensure a unique but coordinated identity for the products. This working process is advantageous in terms of the choice of color base for systemic products that are either of heterogeneous origin or are considered OEM products. The latter, even if characterized by different forms, can be connoted with the base colors or materials that are representative of the brand due to CMF design. Since CMF design manuals and the color matrix have a prescriptive role, the designers who create them are rarely involved in the applicative distribution either of colors, materials or finishes of individual products.\n\n"}
{"id": "9263732", "url": "https://en.wikipedia.org/wiki?curid=9263732", "title": "Commercial Standard Digital Bus", "text": "Commercial Standard Digital Bus\n\nThe Commercial Standard Digital Bus (CSDB) is a unidirectional asynchronous bus, formerly known as the Collins Standard Digital Bus. The maximum speed is 50 kbit/s.\n\nIt is used on most buses in NSW.\n\nMost civilian aircraft use one of 3 serial buses: the Commercial Standard Digital Bus (CSDB),\nARINC 429, or AS-15531.\n\nThe Commercial Standard Digital Bus is a two-wire asynchronous broadcast data transmission bus. Data is transmitted over an interconnecting cable by devices that comply with Electronic Industries Association (EIA) RS-422A.\nThe physical layer is EIA-422.\n\nMessages on the CSDB consist of one address byte followed by any number of data bytes.\n"}
{"id": "16907810", "url": "https://en.wikipedia.org/wiki?curid=16907810", "title": "Confederación Española de Organizaciones Empresariales", "text": "Confederación Española de Organizaciones Empresariales\n\nConfederación Española de Organizaciones Empresariales (\"Spanish Confederation of Employers' Organizations\"), or CEOE, is a Spanish institution founded in June 1977 that represents the Spanish business community. It includes state-owned and private companies in all sectors. It's a member of BusinessEurope.\n\n\n"}
{"id": "2818533", "url": "https://en.wikipedia.org/wiki?curid=2818533", "title": "CyberBunker", "text": "CyberBunker\n\nCyberBunker is an Internet service provider that, according to its website, hosts \"services to any Web site 'except child pornography and anything related to terrorism'\". It served as a host for The Pirate Bay and as one of the many WikiLeaks mirrors. CyberBunker has also been accused of being a host for spammers, botnet command-and-control servers, malware and online scams. The company has also been involved in Border Gateway Protocol hijacks of IP addresses used by Spamhaus and the United States Department of Defense. The Spamhaus hijack was part of an exceptionally large distributed denial of service attack launched against them in March 2013. Because of the size of this attack it received considerable mainstream media attention.\n\nThe company is named for the place it was once housed in, a former Cold War bunker. The bunker was built in 1955 just outside the small town of Kloetinge in the south of the Netherlands. It was intended as a war time Provincial Military Command Center () of the Dutch military that could withstand a nuclear attack. It was discarded by the Dutch military in 1994. As of 2016 the physical location of CyberBunker is a widely known \"secret\".\n\nCyberBunker has a long history of run-ins with the law. In 2002, a fire broke out in the bunker from which they operated. After the fire was put out, it was discovered that besides internet hosting services, an MDMA laboratory was in operation. Three of the four men charged with the operation of the lab were convicted to three-year prison sentences; the fourth was acquitted due to a lack of evidence.\n\nIn October 2009 BitTorrent tracker The Pirate Bay, which had been subjected to legal action by various anti-piracy groups including Dutch copyright organisation BREIN, moved away from Sweden to CyberBunker. In 2010 the Hamburg district court ruled that CyberBunker, operating in Germany as \"CB3Rob Ltd & Co KG\", was no longer allowed to host The Pirate Bay, being subject to a €250,000 fine or up to 2 years imprisonment for each infringement.\n\nIn October 2011, Spamhaus identified CyberBunker as providing hosting for spammers and contacted their upstream provider, A2B, asking that service be cancelled. A2B initially refused, blocking only a single IP address linked to spamming. Spamhaus responded by blacklisting all of A2B address space. A2B capitulated, dropping CyberBunker, but then filed complaints with the Dutch police against Spamhaus for extortion.\n\nIn March 2013, Spamhaus added CyberBunker to its blacklist. Shortly afterwards a distributed denial of service (DDoS) attack of previously unreported scale (peaking at 300 Gbit/s; an average large-scale attack is often around 50 Gbit/s, while the largest known previously publicly reported attack was 100 Gbit/s) was launched against Spamhaus email and web servers using a Domain Name System (DNS) amplification attack; the attack had lasted for over a week. Steve Linford, chief executive for Spamhaus, said that they had withstood the attack. Other companies, such as Google, had made their resources available to help absorb the traffic. The attack was being investigated by five different national cyber-police-forces around the world. Spamhaus alleged that Cyberbunker, in cooperation with \"criminal gangs\" from Eastern Europe and Russia, was behind the attack; Cyberbunker did not respond to the BBC's request for comment on the allegation.\n\nCloudflare, an Internet security firm located in San Francisco, California assisting Spamhaus in combating the DoS attack was also targeted. On 28 March 2013, CyberBunker's website went offline for a short period of time, possibly becoming victim of a DDoS attack themselves.\n\nOn 29 March 2013, the unrelated secure data storage company BunkerInfra issued a press release stating they have been the owners of the former military bunker since 2010 and that any claims made by CyberBunker regarding their continued usage of the complex are false and that they have not been operating from the bunker since the fire in 2002. \"Businessweek\" reported them as stating that the bunker was \"full of junk\" when they acquired it, and quoted Guido Blaauw, their general manager, as stating that the CyberBunker publicity material was \"all Photoshop\".\n\nOn 25 April 2013 Sven Olaf Kamphuis, a vocal spokesman for CyberBunker, was arrested at the request of Dutch authorities near Barcelona by Spanish Police after collaboration through Eurojust. An anonymous press release uploaded on Pastebin the following day demanding the release of Kamphuis threatened with more large-scale attacks should he remain in custody. The Spanish authorities reported that Kamphuis operated from a well-equipped bunker and used a van as a mobile computing office. No further information on this bunker was provided. In September 2013 it was revealed that a second arrest had been made in April in relation to the Spamhaus attack, the suspect being a 16-year-old from London.\n\n"}
{"id": "38183293", "url": "https://en.wikipedia.org/wiki?curid=38183293", "title": "DaVinci (software)", "text": "DaVinci (software)\n\nDaVinci is a development tool used to create HTML5 mobile applications and media content. It includes a jQuery framework, which is a JavaScript library, and can be used by developers and designers, to create web applications used on mobile devices, that have a user experience similar to native applications. Business applications, games, and rich media content, such as HTML5 multi-media magazines, advertisements, and animation, may be produced with the tool. DaVinci is based on standard web technology, including HTML5, CSS3, and JavaScript.\n\nDaVinci is composed of DaVinci Studio and DaVinci Animator, which handle application programming and UI design, respectively. The tool has a WYSIWYG (What You See Is What You Get) authoring environment in which users may drag and drop components to build applications and design web content.\n\nOpen source libraries, such as KnockOut, jsView/jsRender, Impress.js, and turn.js are included in the tool. Other open source frameworks may also be integrated.\n\nThe Model View Controller (MVC) and Data Binding in JavaScript may be handled through DaVinci’s Data-Set Editor. Here, view components and model data may be visually bound, which allows users to create web applications with server integrated UI components without coding.\n\nDaVinci also includes a N-Screen editor, which automatically applies designs and functions to the screen size of various devices, such as smartphones, tablet PCs, and smartTV.\n\nDaVinci has worked closely with the jQuery Foundation in presenting the very first jQuery conference in an Asian district on November 12, 2012 in Seoul, South Korea. DaVinci had been used as a tool to demonstrate application development techniques at the conference.\n\n\n"}
{"id": "9811442", "url": "https://en.wikipedia.org/wiki?curid=9811442", "title": "Deep borehole disposal", "text": "Deep borehole disposal\n\nDeep borehole disposal is the concept of disposing high-level radioactive waste from nuclear reactors in extremely deep boreholes instead of in more traditional deep geological repositories that are excavated like mines. Deep borehole disposal seeks to place the waste as much as beneath the surface of the Earth and relies primarily on the thickness of the natural geological barrier to safely isolate the waste from the biosphere for a very long period of time so that it should not pose a threat to man and the environment. The concept was originally developed in the 1970s, but recently a proposal for a first experimental borehole has been proposed by a consortium headed by Sandia National Laboratories. \n\nThe waste would be put into the lower mile of such a hole, within crystalline rock to isolate it from the environment. The upper two miles of the borehole would be filled with protective layers including asphalt, bentonite, concrete and crushed rock that are expected to protect the environment during geologic time, and the hole would be lined with steel casing.\n\nA pair of proposed test boreholes in the United States were cancelled due to public opposition and lack of funding in 2016 and 2017.\n\nBeginning in 2016, the U.S. Department of Energy funded an experimental borehole extending over deep, in Rugby, North Dakota. The plans for this five-year project in Rugby did not involve nuclear waste, and instead would have tested other aspects of the borehole concept. However, following protests in North Dakota, a site was proposed in Spink County, South Dakota. After protests in South Dakota prevented the project from moving forward, the Department of Energy scrapped the project. Due to public opposition to the first experimental borehole, in late 2016 the Department of Energy announced a second project which would have involved four sites; two in New Mexico, one in Texas and one in South Dakota. The early stages of the project required gaining public support before the Department of Energy would have selected a final site for an experimental borehole. On 23 May 2017, the Department of Energy announced that funding priorities had changed and that the deep borehole project was de-funded.\n\nIn the diagram the solution domain is used for the purpose of computer modelling of heat flow around the borehole.\n\nThe concept involves drilling a borehole about down into the Earth's crust. High level waste, like spent nuclear fuel, would be sealed in strong steel containers and lowered down the borehole, filling the bottom one or two kilometers of the hole. Current technology limits the diameter of the borehole to less than 50 centimeters. This means that some waste currently stored in large containers would need to be repackaged in smaller containers. The rest of the borehole is then sealed with appropriate materials, including clay, cement, crushed rock backfill, and asphalt, to ensure a low-permeability barrier between the waste and the land surface. In some concepts, waste may be surrounded by cementitious grout or a highly compacted bentonite buffer matrix to provide improved containment and to reduce the effect of rock movement on the canisters' integrity. A high-temperature scenario involves very young hot waste in the containers which releases enough heat to create a melt zone around the borehole. As the waste decays and cools, the melt zone resolidifies, forming a solid granite sarcophagus around the containers, entombing the waste forever. Under both scenarios, chemically reducing conditions adjacent to the borehole will reduce the transport of most radionuclides.\n\nThe deep borehole concept can be applied to any amount of waste. For countries that do not rely on nuclear power plants, their entire inventory of high-level nuclear waste could perhaps be disposed of in a single borehole. Current estimates suggest that spent fuel generated from a single large nuclear power plant operating for multiple decades could be disposed of in fewer than ten boreholes. It is estimated that only 800 boreholes would be sufficient to store the entire existing nuclear waste stockpile of the USA. Borehole disposal programs could be terminated at any time with little loss of investment because each borehole is independent. The modular nature of borehole disposal would lend itself to regional, or on-site, disposal of nuclear waste. Another attraction of the deep borehole option is that holes might be drilled and waste emplaced using modifications of existing oil and gas drilling technologies.\n\nFinally, the environmental impact is small. The waste handling facility at the wellhead, plus a temporary security buffer zone, would require about one square kilometer. When the borehole is filled and sealed, the land can be returned to a pristine condition.\n\nEvery state in the U.S. has deep rocks suitable for its own borehole repository. The required crystalline basement rocks are located far below sedimentary rock that is not very dense, far below drinking water aquifers, and far below oil and gas deposits. A single borehole would not be big enough to hold all of the nuclear waste produced by a country like the United States, and therefore a number of them might eventually exist within a single country.\n\nScientists at the University of Sheffield in England say that deep boreholes for nuclear waste disposal can be built much more quickly than a traditional deep geological repository that is excavated like an underground mine for waste disposal. The mined repository approach has been pursued unsuccessfully for many years but the University of Sheffield engineers say that a borehole could be drilled, filled and sealed in no more than five years, in contrast to the decades required for a mined repository.\n\n"}
{"id": "30873555", "url": "https://en.wikipedia.org/wiki?curid=30873555", "title": "EbXML", "text": "EbXML\n\nElectronic Business using eXtensible Markup Language, commonly known as e-business XML, or ebXML (pronounced ee-bee-ex-em-el, [i'bi,eks,em'el]) as it is typically referred to, is a family of XML based standards sponsored by OASIS and UN/CEFACT whose mission is to provide an open, XML-based infrastructure that enables the global use of electronic business information in an interoperable, secure, and consistent manner by all trading partners.\n\nThe ebXML architecture is a unique set of concepts; part theoretical and part implemented in the existing ebXML standards work.\n\nThe ebXML work stemmed from earlier work on ooEDI (object oriented EDI), UML / UMM, XML markup technologies and the X12 EDI \"Future Vision\" work sponsored by ANSI X12 EDI.\n\nThe melding of these components began in the original ebXML work and the theoretical discussion continues today. Other work relates, such as the Object Management Group work and the OASIS BCM (Business-Centric Methodology) standard (2006).\n\nWhile the ebXML standards adopted by ISO and OASIS seek to provide formal XML-enabled mechanisms that can be implemented directly, the ebXML architecture is on concepts and methodologies that can be more broadly applied to allow practitioners to better implement e-business solutions.\n\nA particular instance is the Core Components Technical Specification (CCTS) work that continues within UN/CEFACT, whereas its cousin - UBL - Universal Business Language - specification is used within OASIS that implements specific XML transactions by applying the principles of CCTS to typical supply chain transactions such as invoice, purchase order, ship notice and so on.\n\nebXML was started in 1999 as a joint initiative between the United Nations Centre for Trade facilitation and Electronic Business (UN/CEFACT) and Organization for the Advancement of Structured Information Standards (OASIS). A joint coordinating committee composed of representatives from each of the two organizations led the effort. Quarterly meetings of the working groups were held between November 1999 and May 2001. At the final plenary a Memorandum of Understanding was signed by the two organizations, splitting up responsibility for the various specifications but continuing oversight by the joint coordinating committee.\n\nThe original project envisioned five layers of data specification, including XML standards for:\n\n\nAll work was completed based on a normative requirements document and the ebXML Technical Architecture Specification.\n\nAfter completion of the 6 specifications by the two organizations, 5 parts of the work were submitted to ISO TC 154 for approval. The International Organization for Standardization (ISO) has approved the following five ebXML specifications as the ISO 15000 standard, under the general title, Electronic business eXtensible markup language:\n\n\nOASIS technical committees and UN/CEFACT retain the responsibility for maintaining and advancing the above specifications.\n\nCollaborative Partner Profile Agreements are XML based documents specifying a trading agreement between trading partners. \nEach trading partner will have their own Collaboration Protocol Profile (CPP) document that describes their abilities in an XML format. For instance, this can include the messaging protocols they support, or the security capabilities they support. \nA CPA (Collaboration Protocol Agreement) document is the intersection of two CPP documents, and describes the formal relationship between two parties. The following information will typically be contained in a CPA document:\n\nThe Message Service Specification (ebMS) describes a communication-neutral mechanism Message Service Handlers (MSH) must implement in order to exchange business documents. ebMS3.0 is the current version of the specification.\nebMS3.0 is built as an extension on top of the SOAP with Attachments specification. The SOAP message contains the meta-data required to exchange the business document in a secure and reliable manner, while the business payload is attached to the SOAP message. Multiple business payloads may be attached to a single message, and the format of the payloads is beyond the scope of the ebXML specifications.\nThe information trading partners place in ebMS messages is largely dictated by the CPA agreement that defines the relationship between them. The following information is typically contained within ebMS messages:\n\nebMS is communication protocol neutral, although the most common underlying protocols are HTTP and SMTP.\n\n\n"}
{"id": "2234210", "url": "https://en.wikipedia.org/wiki?curid=2234210", "title": "Electrospray", "text": "Electrospray\n\nThe name electrospray is used for an apparatus that employs electricity to disperse a liquid or for the fine aerosol resulting from this process. High voltage is applied to a liquid supplied through an emitter (usually a glass or metallic capillary). Ideally the liquid reaching the emitter tip forms a Taylor cone, which emits a liquid jet through its apex. Varicose waves on the surface of the jet lead to the formation of small and highly charged liquid droplets, which are radially dispersed due to Coulomb repulsion.\n\nIn the late 16th century William Gilbert set out to describe the behaviour of magnetic and electrostatic phenomena. He observed that, in the presence of a charged piece of amber, a drop of water deformed into a cone. This effect is clearly related to electrosprays, even though Gilbert did not record any observation related to liquid dispersion under the effect of the electric field.\n\nIn 1750 the French clergyman and physicist Jean-Antoine (Abbé) Nollet noted water flowing from a vessel would aerosolize if the vessel was electrified and placed near electrical ground. He also noted that similarly \"a person, electrified by connection to a high-voltage generator, would not bleed normally if he were to cut himself; blood would spray from the wound\".\n\nIn 1882, Lord Rayleigh theoretically estimated the maximum amount of charge a liquid droplet could carry; this is now known as the \"Rayleigh limit\". His prediction that a droplet reaching this limit would throw out fine jets of liquid was confirmed experimentally more than 100 years later.\n\nIn 1914, John Zeleny published work on the behaviour of fluid droplets at the end of glass capillaries. This report presents experimental evidence for several electrospray operating regimes (dripping, burst, pulsating, and cone-jet). A few years later, Zeleny captured the first time-lapse images of the dynamic liquid meniscus. \nBetween 1964 and 1969 Sir Geoffrey Ingram Taylor produced the theoretical underpinning of electrospraying. Taylor modeled the shape of the cone formed by the fluid droplet under the effect of an electric field; this characteristic droplet shape is now known as the Taylor cone. He further worked with J. R. Melcher to develop the \"leaky dielectric model\" for conducting fluids.\n\nTo simplify the discussion, the following paragraphs will address the case of a positive electrospray with the high voltage applied to a metallic emitter. A classical electrospray setup is considered, with the emitter situated at a distance formula_1 from a grounded counter-electrode. The liquid being sprayed is characterized by its viscosity formula_2, surface tension formula_3, conductivity formula_4, and relative permittivity formula_5.\n\nUnder the effect of surface tension, the liquid meniscus assumes a semi-spherical shape at the tip of the emitter. Application of the positive voltage formula_6 will induce the electric field:\nwhere formula_8 is the liquid radius of curvature. This field leads to liquid polarization: the negative/positive charge carriers migrate toward/away from the electrode where the voltage is applied. At voltages below a certain threshold, the liquid quickly reaches a new equilibrium geometry with a smaller radius of curvature.\n\nVoltages above the threshold draw the liquid into a cone. Sir Geoffrey Ingram Taylor described the theoretical shape of this cone based on the assumptions that (1) the surface of the cone is an equipotential surface and (2) the cone exists in a steady state equilibrium. To meet both of these criteria the electric field must have azimuthal symmetry and have formula_9 dependence to balance the surface tension and produce the cone. The solution to this problem is:\n\nwhere formula_11 (equipotential surface) exists at a value of formula_12 (regardless of R) producing an equipotential cone. The magic angle necessary for formula_11 for all R is a zero of the Legendre polynomial of order 1/2, formula_14. There is only one zero between 0 and formula_15 at 130.7099°, which is the complement of the Taylor's now famous 49.3° angle.\n\nThe apex of the conical meniscus cannot become infinitelly small. A singularity develops when the hydrodynamic relaxation time formula_16 becomes larger than the charge relaxation time formula_17. The undefined symbols stand for characteristic length formula_18 and vacuum permittivity formula_19. Due to intrinsic varicose instability, the charged liquid jet ejected through the cone apex breaks into small charged droplets, which are radially dispersed by the space-charge.\n\nThe charged liquid is ejected through the cone apex and captured on the counter electrode as charged droplets or positive ions. To balance the charge loss, the excess negative charge is neutralized electrochemically at the emitter. Imbalances between the amount of charge generated electrochemically and the amount of charge lost at the cone apex can lead to several electrospray operating regimes. For cone-jet electrosprays, the potential at the metal/liquid interface self-regulates to generate the same amount of charge as that lost through the cone apex.\n\nElectrospray became widely used as ionization source for mass spectrometry after the Fenn group successfully demonstrated its use as ion source for the analysis of large biomolecules.\n\nA liquid metal ion source (LMIS) uses electrospray in conjunction with liquid metal to form ions. Ions are produced by field evaporation at tip of the Taylor cone. Ions from a LMIS are used in ion implantation and in focused ion beam instruments.\n\nSimilarly to the standard electrospray, the application of high voltage to a polymer solution can result in the formation of a cone-jet geometry. If the jet turns into very fine fibers instead of breaking into small droplets, the process is known as electrospinning .\n\nElectrospray techniques are used as low thrust electric propulsion rocket engines to control satellites, since the fine-controllable particle ejection allows precise and effective thrust.\n\nElectrospray may be used in nanotechnology, for example to deposit single particles on surfaces. This is done by spraying colloids on average containing only one particle per droplet. The solvent evaporates, leaving an aerosol stream of single particles of the desired type. The ionizing property of the process is not crucial for the application but may be used in electrostatic precipitation of the particles.\n\nInstead of depositing nanoparticles, nanoparticles and nano structures can also fabricated in situ by depositing metal ions to desired locations. Electrochemical reduction of ions to atoms and in situ assembly was believed to be the mechanism of nano structure formation.\n\nElectrospray has garnered attention in the field of drug delivery, and it has been used to fabricate drug carriers including polymer microparticles used in immunotherapy as well as lipoplexes used for nucleic acid delivery. The sub-micrometer-sized drug particles created by electrospray possess increased dissolution rates, thus increased bioavailability due to the increased surface area. The side-effects of drugs can thus be reduced, as smaller dosage is enough for the same effect.\n\nElectrospray is used in some air purifiers. Particulate suspended in air can be charged by the electrospray aerosol electrospray, manipulated by an electric field, and collected on a grounded electrode. This approach minimizes the production of ozone which is common to other types of air purifiers.\n"}
{"id": "23097121", "url": "https://en.wikipedia.org/wiki?curid=23097121", "title": "Enhancement or quenching of QD, Q-wire and QW radiations", "text": "Enhancement or quenching of QD, Q-wire and QW radiations\n\nIn the field of solid-state physics, enhancement or quenching of the radiation of QDs, Q-wires, and QW are methods used to reduce the radiative emission of quantum dots, wires and wells. Many methods have been developed to enhance or quench the radiation by adjusting the size, changing the structure, and adding other materials to the quantum structures. By doing these, the radiation patterns are regulated, which are expected to have the potential to lead to a new class of light sources. In this page, recent research on zinc oxide (ZnO) nanostructures is introduced and the principles of the enhancement and quenching in the structures are discussed.\n\nThe optical properties of ZnO quantum dots can be controlled by changing of the size. As the size of the ZnO nanocolloids increases, the absorbance increases, but optical band gap of the nanocolloids decreases. The third-order optical susceptibility increases with increasing particle size.\n\nRadiation fields are changed by the methods to fabricate the quantum wires. Micro photoluminescence spectra of an individual suspended as-grown ZnO nanowire and an individual ZnO nanowire processed by sonication/dispersion procedure. The thickness of the nanowire tends to change the radiation fields. The thickness of the nanowire is also related to the peak wavelength. For the nanowire with regular shape shown, only slight difference in UV to visible emission ratios was observed for the two parts with different diameters. Correspondingly, UV emission peaks have almost the same position at 375.2 nm. On the other hands, for the nanowire with irregular shape and rougher surface, with the decrease of the diameter, dramatically increased green emission and decreased UV emission can be observed, which was accompanied by the red shift of the UV emission peak energy.\n\nRadiation fields of ZnO quantum wells can be adjusted by coupling through localized surface plasmons. By sputtering Ag islands onto ZnO films, their band gap emission coming through the Ag island films was enhanced by threefolds, while the defect emission was quenched. The enhancement is mainly dependent on the Ag island size. photoluminescence spectra of seven samples which have different Ag island sizes are represented; #4 sample has the largest size of island, but # 1 sample has the smallest size of the island. It is revealed that sputtering time related to the island size have an effect on the enhancement of 380 nm band and 530 nm band. The PL enhancement or quenching may be due to the coupling of the light emission with the localized surface plasmon resonance of the Ag islands. When localized surface Plasmon resonance scattering dominates over the absorption process, the localized surface Plasmon energy can be recovered to free space emission, leading to the enhancement of light emission. Otherwise, light emission will be attenuated due to non-radiative dissipation of localized surface plasmon absorption.\n"}
{"id": "14330751", "url": "https://en.wikipedia.org/wiki?curid=14330751", "title": "FISEC", "text": "FISEC\n\nThe Food Industry Students European Council (FISEC) was the student association of the European Federation of Food Science and Technology, for students studying food science, food technology and related courses at a European university.\n\nFISEC was a non-profit, apolitical and independent association, which tries to help students to meet and learn from one another through international events.\n\n1. Help students to meet and in that way to become more internationally minded.\n\n2. Create opportunities for the students to meet and learn from one another through our organized events.\n\n3. To expand educational and job possibilities for all our members and to achieve personal development.\n\n4. Continuous development of communication and cooperation between students, universities and companies.\n\nThe Executive Committee consists of a President, Vice-President, Treasurer and Secretary.\n\nFISEC is holding two annual conferences, the General Assembly and the Food Convention.\n\nThese conferences provide an excellent opportunity to meet our European counterparts and to learn more about their industries, usually by visiting food companies and talking to their representatives. At the General Assembly we also organize the necessary official parts of an association, like electing a new president.\n\n1989, May : The First Food Convention takes place at ENSIA, Massy, France.\n\n1989, November: Starting point for the work between students of the ENSIA and the ESB, Porto, Portugal.\n\n1990, May 5: As a result of that cooperation FISEC is founded at the ESB in Porto by the following six universities: University of Reading, UK; University of Milan, Italy; Wageningen University, Netherlands; ESB Porto, Portugal; ENSIA, Massy and ENSBANA, Dijon, France.\n\n1991 : The Technical University of Berlin joined FISEC during this year . The last General Assembly was held in Nancy, France and the Food Convention was in Brussels, Belgium. A decision was taken for the next Food Convention to be in Berlin, Germany.\n\nThrough the years sometimes the separate FISEC teams have lost traces but it never stopped functioning and the students from the various universities across Europe have found their ways to gather.\n\n1998 : FISEC’s official meeting gathers many new members and a few Universities that are very active members till nowadays join the association : Croatia (Zagreb and Osijek), Portugal (Porto), Belgium (Brussels), Spain (Zaragoza), England (Reading), Hungary (Szeged), France (Nantes), Germany (Munich), Sweden(Kalmar) and Netherlands (Wageningen) are among the participants in the event.\n\n1999: The General Assembly was held in March 1999 and the Food Convention was organized by the team of FISEC Zagreb, Croatia in October during the same year.\n\n2000: The record is only for the Food Convention that was held in Brussels, Belgium where teams from Austria ( University of Vienna) and Bulgaria (Plovdiv) joined the members of FISEC.\n\n2001: The next two events gather even more interest.The General Assembly was organized by the team of FISEC-Berlin ( Germany) and the Food Convention was in Thessaloniki (Greece) . In 2001 Universities from Romania, Slovakia and Portugal(Algarve and Faro) are among FISEC’s new members.\n\n2002: The official website of FISEC was created by Stefan Topfl - member of the Technical University of Berlin, Germany. The General Assembly was held in Plovdiv and the Food Convention in Faro, Portugal.\n\n2003: The General Assembly has been organized in April 2003 by the FISEC team Nantes (France) and the Food Convention in the same year was voted to be in Brussels, Belgium.\nDuring this year the first steps in FISEC’s collaboration with other student’s non-governmental organizations have been done by its current president Mauro Portela.\n\n2004: General Assembly was held in Zagreb, Croatia\n\n2005: General Assembly was held in Szeged, Hungary and Food Convention was held in Istanbul Turkey on Istanbul Technical University\n\n2006: General Assembly was held in Plovdiv, Bulgaria and Food Convention was held in Faro, Portugal\n\n2007: General Assembly was held in Split, Croatia. Today's structure and members are accepted on this GA.\n"}
{"id": "9121395", "url": "https://en.wikipedia.org/wiki?curid=9121395", "title": "Filling factor", "text": "Filling factor\n\nFilling factor, formula_1, is a quantity measuring the efficiency of absorption of pump in the core of a double-clad fiber.\n\nThe efficiency of absorption of pumping energy in the fiber is an important parameter of a double-clad fiber laser. In many cases this efficiency can be approximated with\nwhere\n\nThe filling factor may depend on the initial distribution of the pump light, the shape of the cladding, and the position of the core within it.\n\nThe large (close to unity) filling factor is important in double-clad amplifiers; it allows them to reduce the requirements for the brightness of the pump and to reduce the length of the fiber laser. Such a reduction is especially important for the power scaling of various nonlinear processes, and contributions of stimulated scattering to the degradation of signal. Use of the filling factor for the estimate of the efficiency of absorption of the pump in fiber lasers allows quick estimates without performing complicated numerical simulations.\n\ndouble-clad fiber\nErbium Doped Fibre Amplifier (EDFA)\n"}
{"id": "1303216", "url": "https://en.wikipedia.org/wiki?curid=1303216", "title": "GeoServer", "text": "GeoServer\n\nIn computing, GeoServer is an open-source server written in Java that allows users to share, process and edit geospatial data. Designed for interoperability, it publishes data from any major spatial data source using open standards. GeoServer has evolved to become an easy method of connecting existing information to virtual globes such as Google Earth and NASA World Wind as well as to web-based maps such as OpenLayers, Google Maps and Bing Maps. GeoServer functions as the reference implementation of the Open Geospatial Consortium Web Feature Service standard, and also implements the Web Map Service, Web Coverage Service and Web Processing Service specifications.\n\nGeoServer aims to operate as a node within a free and open Spatial Data Infrastructure. Just as the Apache HTTP Server has offered a free and open web server to publish HTML, GeoServer aims to do the same for geospatial data.\n\nGeoServer reads a variety of data formats, including:\n\n\nThrough standard protocols it produces KML, GML, Shapefile, GeoRSS, PDF, GeoJSON, JPEG, GIF, SVG, PNG and more. In addition, one can edit data via the WFS transactional profile (WFS-T). GeoServer includes an integrated OpenLayers client for previewing data layers.\n\nGeoServer additionally supports efficient publishing of geospatial data to Google Earth through the use of network links, using KML. Advanced features for Google Earth output include templates for customized pop-ups, time and height visualizations, and \"super-overlays\".\n\nGeoServer relies on GeoTools, a GIS library.\n\n\nGeoServer uses the Spring Framework, providing a request dispatch architecture for modules implementing OGC services. The web administration application uses wicket, allowing extensions to contribute additional configuration screens. The application provides a REST API implemented using the spring-mvc-framework.\n\nGeoServer is a web application, supporting any common servlet container (a standalone distribution is available with the Jetty (web server) as an embedded server). GeoWebCache, a Java-based caching component similar to TileCache, is bundled with GeoServer, but available separately. Similarly, GeoServer packages GeoTools as a Java library, but it is also available separately.\n\nGeoServer is a longstanding application and has undergone several architectural changes. GeoServer 1.0 was built around the STRUTS framework, with the migration to Spring and Wicket taking place for GeoServer 2.0. Early versions of the REST API used restlet before migration to spring-mvc-framework.\n\n\n"}
{"id": "1748220", "url": "https://en.wikipedia.org/wiki?curid=1748220", "title": "Gravity-based structure", "text": "Gravity-based structure\n\nA gravity-based structure (GBS) is a support structure held in place by gravity. A common application for a GBS is an offshore oil platform. These structures are often constructed in fjords since their protected area and sufficient depth are very desirable for construction. A GBS intended for use as an offshore oil platform is constructed of steel reinforced concrete, often with tanks or cells which can be used to control the buoyancy of the finished GBS. When completed, a GBS is towed to its intended location and sunk. Prior to deployment, a study of the seabed have to be done in order to ensure it can withstand the vertical load exerted on it by that structure.\n\nGravity-based structures are also used for offshore wind power plants. By the end of 2010, 14 of the world's offshore wind farms were supported by gravity-based structures. The GBS are suited for water depths greater than 20 m. The deepest registered offshore wind farm with gravity-based structures is Thornton Bank 1, Belgium, with a depth up to 27.5 m. As offshore wind power plants are growing in size and moving towards deeper waters, the GBS is considered competitive in comparison with other support structures.\n"}
{"id": "8742571", "url": "https://en.wikipedia.org/wiki?curid=8742571", "title": "Heterostructure-emitter bipolar transistor", "text": "Heterostructure-emitter bipolar transistor\n\nThe Heterojunction-emitter bipolar transistor (HEBT), is a somewhat unusual arrangement with respect to emitter blocking of minority carriers. This is accomplished by using heterostructure confinement in the emitter, introducing an energy barrier to minority-carrier charge flow from the base. This is important as loss of minority carriers from the base to the emitter degrades analog performance. The main difference of the HEBT from the Heterojunction bipolar transistor (HBT) is that the emitter–base interface is the same as in a bipolar junction transistor (BJT) with the blocking energy gap being moved back into the emitter bulk region.\n\nThe main advantage of HEBT architecture, compared to the HBT is a simplified fabrication process for the emitter–base junction. In particular the HEBT does not require as tight parametric control during epitaxial growth, that equivalent abrupt or graded emitter structures might. This is very important as it is evident from scanning ion mass spectrometry data that out-diffusion base dopant into the emitter junction is difficult to control, as the base is, in general, very highly doped in order to enhance performance.\n\nThe HEBT is well positioned as a potential candidate for key roles in high-frequency optoelectronic markets, similar to the Heterojunction bipolar transistor. Also of importance for optoelectronic hybrids is that HEBT can be constructed in any semiconductor system that permits the use of band-gap–altering alloys in the emitter.\n\n"}
{"id": "2945880", "url": "https://en.wikipedia.org/wiki?curid=2945880", "title": "Hobble (device)", "text": "Hobble (device)\n\nA hobble (also, and perhaps earlier, hopple) or spancel is a device which prevents or limits the locomotion of a animal, by tethering one or more legs. Although hobbles are most commonly used on horses, they are sometimes used also on other animals. On dogs, they are used especially during force-fetch training to limit the movement of a dog's front paws when training it to stay still. They are made from leather, rope, or synthetic materials such as nylon or neoprene. There are various designs for breeding, casting, and mounting horses.\n\n\"Western\"-style horse hobbles are tied around the pasterns or cannon bones of the horse's front legs. They comprise three basic types:\n\nThe above patterns are unsuitable for training as they can tighten around a leg and cause injury.\n\nWestern hobbles are normally used to secure a horse when no tie device, tree, or other object is available for that purpose, e.g., when if traveling across open lands a rider has to dismount for various reasons. Hobbles also allow a horse to graze and move short and slow distances, yet prevent the horse from running off too far. This is handy at night if the rider has to get some sleep; using a hobble ensures that in the morning he can find his horse not too far away.\n\nHobble training a horse is a form of \"sacking out\" and desensitizing a horse to accept restraints on its legs. This helps a horse accept pressure on its legs in case it ever becomes entangled in barbed wire or fencing. A hobble trained horse is less likely to pull, struggle, and cut its legs in a panic, since it has been taught to give to pressure in its legs.\n\n\n\nHobbles date at least as far back as Ancient Egypt. Two Egyptian hieroglyphs are believed to depict hobbles.\n\n\n"}
{"id": "39114222", "url": "https://en.wikipedia.org/wiki?curid=39114222", "title": "Ice chips", "text": "Ice chips\n\nIce chips are small pieces of ice, usually smaller than ice cubes. They are often recommended before surgery or an invasive medical procedure. They may help to prevent oral mucositis or mouth sores associated with high-dose chemotherapy.\n\n"}
{"id": "26476123", "url": "https://en.wikipedia.org/wiki?curid=26476123", "title": "Iodine pit", "text": "Iodine pit\n\nThe iodine pit, also called the iodine hole or xenon pit, is a temporary disabling of a nuclear reactor due to buildup of short-lived nuclear poisons in the reactor core. The main isotope responsible is Xe, mainly produced by natural decay of I. I is a weak neutron absorber, while Xe is the strongest known neutron absorber. When Xe builds up in the fuel rods of a reactor, it significantly lowers their reactivity, by absorbing a significant amount of the neutrons which provide the nuclear reaction.\n\nThe presence of I and Xe in the reactor is one of the main reasons for its power fluctuations in reaction to change of control rod positions.\n\nThe buildup of short-lived fission products acting as nuclear poisons is called reactor poisoning, or xenon poisoning. Buildup of stable or long-lived neutron poisons is called reactor slagging.\n\nOne of the common fission products is Te, which undergoes beta decay with half-life of 19 seconds to I. I itself is a weak neutron absorber. It builds up in the reactor in the rate proportional to the rate of fission, which is proportional to the reactor thermal power. I undergoes beta decay with half-life of 6.57 hours to Xe. The yield of Xe for uranium fission is 6.3%; about 95% of Xe originates from decay of I.\n\nXe has a huge cross section for thermal neutrons, 2.6×10 barns, so it acts as a neutron absorber or \"poison\" that can slow or stop the chain reaction after a period of operation. This was discovered in the earliest nuclear reactors built by the Manhattan Project for plutonium production. As a result, the designers made provisions in the design to increase the reactor's reactivity (the number of neutrons per fission that go on to fission other atoms of nuclear fuel).\nXe reactor poisoning played a major role in the Chernobyl disaster.\n\nXe is the most powerful known neutron absorber. Its buildup in the fuel rods significantly lowers reactivity of the reactor core. By a neutron capture, Xe is transformed (\"burned\") to Xe, which is stable and does not significantly absorb neutrons. The burn rate is proportional to the neutron flux, which is proportional to the reactor power; a reactor running on twice the power will have twice the xenon burn rate.\n\nXe beta-decays with half-life of 9.2 hours to Cs; a poisoned core will spontaneously recover after several half-lives. For some reactors, the Xe concentration will be equal to its equilibrium concentration at full power. After about 3 days of shutdown, the core can be assumed to be free of Xe, without it introducing errors into the reactivity calculations.\n\nThe increase in the Xe concentration during lowering of the reactor power can lower the reactivity enough to effectively shut down the reactor. As there are not enough neutrons to offset their absorption by Xe, nor to burn the built-up xenon, the reactor has to be kept in shutdown state for 1–2 days until enough of the Xe decays. The inability of the reactor to be restarted in such state is called xenon precluded start up or dropping into an iodine pit; the duration of this situation is known as xenon dead time, poison outage, or iodine pit depth. Due to the risk of such situations, in the early Soviet nuclear industry, many servicing operations were performed on running reactors, as downtimes longer than an hour led to xenon buildup that could keep the reactor offline for significant time, lower the production of valuable weapon Pu, and cause an investigation by a committee and punishment of the operators.\n\nThe interdependence of Xe buildup and the neutron flux can lead to periodic power fluctuations. In large reactors, with little neutron flux coupling between their regions, flux nonuniformities can lead to formation of xenon oscillations, periodic local variations of reactor power moving through the core with a period of about 15 hours. A local variation of neutron flux causes increased burnup of Xe and production of I, depletion of Xe increases the reactivity in the core region. The local power density can change by factor of three or more, while the average power of the reactor stays more or less unchanged. Strong negative temperature coefficient of reactivity causes damping of these oscillations, and is a desired reactor design feature.\n\nThe reactivity of the reactor after the shutdown first decreases, then increases again, having a shape of a pit; this gave the \"iodine pit\" its name. The degree of poisoning, and the depth of the pit and the corresponding duration of the outage, depends on the neutron flux before the shutdown. Iodine pit behavior is not observed in reactors with neutron flux density below 5×10 neutrons ms, as the Xe is primarily removed by decay instead of neutron capture. As the core reactivity reserve is usually limited to 10% of Dk/k, thermal power reactors tend to use neutron flux at most about 5×10 neutrons ms to avoid restart problems after shutdown.\n\nThe concentration changes of Xe in the reactor core after its shutdown is determined by the short-term power history of the reactor (which determines the initial concentrations of I and Xe), and then by the half-life differences of the isotopes governing the rates of its production and removal; if the activity of I is higher than activity of Xe, the concentration of Xe will rise, and vice versa.\n\nDuring reactor operation at a given power level, a secular equilibrium is established within 40–50 hours, when the production rate of iodine-135, its decay to xenon-135, and its burning to xenon-136 and decay to caesium-135 are keeping the xenon-135 amount in the reactor constant at a given power level.\n\nThe equilibrium concentration of I is proportional to the neutron flux φ. The equilibrium concentration of Xe however depends very little on neutron flux for φ>10 neutrons ms.\n\nIncrease of the reactor power, and the increase of neutron flux, causes a rise in production of I and consumption of Xe. At first, the concentration of xenon decreases, then slowly increases again to a new equilibrium level as now excess I decays. During typical power increases from 50 to 100%, the Xe concentration falls for about 3 hours.\n\nDecrease of the reactor power lowers production of new I, but also lowers the burn rate of Xe. For a while Xe builds up, governed by the amount of available I, then its concentration decreases again to an equilibrium for the given reactor power level. The peak concentration of Xe occurs after about 11.1 hours after power decrease, and the equilibrium is reached after about 50 hours. A total shutdown of the reactor is an extreme case of power decrease.\n\nIf sufficient reactivity control authority is available, the reactor \"can\" be restarted, but a xenon burn-out transient must be carefully managed. As the control rods are extracted and criticality is reached, neutron flux increases many orders of magnitude and the Xe begins to absorb neutrons and be transmuted to Xe. The reactor \"burns off\" the nuclear poison. As this happens, the reactivity increases and the control rods must be gradually re-inserted or reactor power will increase. The time constant for this burn-off transient depends on the reactor design, power level history of the reactor for the past several days (therefore the Xe and I concentrations present), and the new power setting. For a typical step up from 50% power to 100% power, Xe concentration falls for about 3 hours.\n\nReactors with large physical dimensions, e.g. the RBMK type, can develop significant nonuniformities of xenon concentration through the core. Control of such nonhomogeneously poisoned cores, especially at low power, is a challenging problem. The Chernobyl disaster resulted from an attempt to recover the reactor from a nonuniformly poisoned state.\n\nThe iodine pit effect has to be taken in account for reactor designs. High values of power density, leading to high production rates of fission products and therefore higher iodine concentrations, require higher amount and enrichment of the nuclear fuel used to compensate. Without this reactivity reserve, a reactor shutdown would preclude its restart for several tens of hours until I/Xe sufficiently decays, especially shortly before replacement of spent fuel (with high burnup and accumulated nuclear poisons) with fresh one.\n\nFluid fuel reactors cannot develop xenon inhomogeneity because the fuel is free to mix. Also, the Molten Salt Reactor Experiment demonstrated that spraying the liquid fuel as droplets through a gas space during recirculation can allow xenon and krypton to leave the fuel salts. However, removing Xe from neutron exposure also means that the reactor will produce more of the long-lived fission product Cs.\n\n"}
{"id": "426992", "url": "https://en.wikipedia.org/wiki?curid=426992", "title": "Joe 90", "text": "Joe 90\n\nJoe 90 is a 1960s British science-fiction television series that follows the adventures of a nine-year-old boy, Joe McClaine, who starts a double life as a schoolchild-turned-superspy after his scientist father invents a device capable of duplicating expert knowledge and experience and transferring it to a human brain. Equipped with the skills of the foremost academic and military minds, Joe is recruited by the World Intelligence Network (WIN) and, as its \"Most Special Agent\", pursues the objective of world peace and saving human life. Created by Gerry and Sylvia Anderson and filmed by Century 21 Productions, the 30-episode series followed \"Thunderbirds\" and \"Captain Scarlet and the Mysterons\".\n\nFirst broadcast in the UK between September 1968 and April 1969 on the ATV network, \"Joe 90\" was the sixth and final of the Andersons' productions to be made exclusively using the form of marionette puppetry termed \"Supermarionation\". Their final puppet series, \"The Secret Service\", used this process only in combination with extensive live-action filming. As in the case of its antecedent, \"Captain Scarlet\", the puppets of \"Joe 90\" are of natural proportions as opposed to the more caricatured design of the characters of \"Thunderbirds\".\n\nAlthough not as successful as Century 21's previous efforts, since its inception, \"Joe 90\" has been praised, among other aspects, for the level of characterisation of its smaller puppet cast and the quality of its model sets and special effects. Critics have interpreted \"Joe 90\"<nowiki>'</nowiki>s spy-fi theme and the choice of a child character as the protagonist as either a \"kids play Bond\" concept or an enshrinement of children's powers of imagination. Points of criticism range from the violence depicted in a number of episodes to the absence of female characters, which is interpreted either as the inevitable result of the series' composition as a \"boy's own adventure\" or as being tantamount to sexism.\n\nAs for its earlier productions, Century 21 launched a number of merchandising campaigns based on \"Joe 90\", which included toy cars and comic strips featuring the continuing adventures of Joe McClaine. The series was syndicated in the United States in 1969, re-broadcast in the UK during the 1990s and released on DVD in most regions in the 2000s. The idea of a live-action film adaptation of \"Joe 90\" has been considered more than once since the 1960s, but without further development.\n\n\"Joe 90\" is set in the near future. The timeframe is most commonly stated to be 2012 and 2013; various other sources point to an undetermined year in the early 21st century, while the official scriptwriters' guide states that the year is 1998. Based on visual evidence, the events of \"The Unorthodox Shepherd\" occur in 2013.\n\nNine-year-old British schoolboy Joe McClaine is the adopted son of Professor Ian \"Mac\" McClaine, a computer expert. Outwardly, the McClaines are an unexceptional father and son; they live in an antiquated Elizabethan-style cottage overlooking Culver Bay in Dorset, and are waited on by their housekeeper, Mrs Harris. Yet, residing in a secret underground laboratory is Mac's latest invention, the \"BIG RAT\" (Brain Impulse Galvanoscope Record And Transfer), a machine capable of recording knowledge and experience from leading experts in various fields and transferring it to another human brain. At the heart of the design is the \"Rat Trap\": a spherical, rotating cage in which a subject is seated during the transfer of \"brain patterns\".\n\nSam Loover, a family friend and an agent of World Intelligence Network (WIN), persuades Mac to dedicate the services of Joe and the BIG RAT to the organisation: Joe will become a WIN operative with a difference, the unlimited possibilities offered by the BIG RAT serving as an invaluable tool for completing missions. After requisite knowledge and experience has been transferred, and provided that Joe is wearing customised glasses containing hidden electrodes (a portable storage device for brain impulses), he is able to carry out missions requiring proficiency in – among other disciplines – flying fighter aircraft, spaceflight, performing advanced neurosurgery and piano.\n\nGiven that a boy would never be suspected of espionage, Joe's innocence is as useful an asset as the BIG RAT, and he comes to be regarded as WIN's \"Most Special Agent\". Reporting to Shane Weston, the commander-in-chief of WIN's London Headquarters, Joe is also equipped with a special briefcase, which externally appears to be nothing more than a school case but which secretly contains an adapted handgun and transceiver. There is some inconsistency as to why Joe is assigned the codename \"90\". Contemporary series publicity stated that, in the pilot episode, Joe enlists in WIN as its 90th London-based agent. However, in the episode \"Project 90\", reference is made to the BIG RAT being documented in WIN's \"File Number 90\", from which Joe's designation is explicitly stated to originate. The series ends with a clip show episode, \"The Birthday\", in which a selection of Joe's missions are presented as flashbacks at a surprise party on the day that the character turns ten.\n\nLike antecedent series, plot elements of \"Joe 90\" include hi-tech gadgetry, rescue operations, secret organisations and criminal or terrorist threats to world security. An example of the advanced technology demonstrated is Professor McClaine's \"Jet Air Car\": a multiple-configuration land-, sea- and air-based vehicle built prior to the events of the series. The in-joke of \"WIN\", the abbreviated form of \"World Intelligence Network\", is similar to that of \"WASP\", the acronym for the World Aquanaut Security Patrol that appears in \"Stingray\".\n\nIn the fictional universe of \"Joe 90\", the Cold War—significant at the time of the series' TV debut, owing to the Soviet invasion of Czechoslovakia in August 1968—has ended, and a world government has been established. WIN is the successor organisation to MI6, the CIA and the KGB, all of which have been merged in the formation of the global network. Although the pilot sees Joe hijacking a Russian jet fighter to expose the secrets of its construction to the West, the storyline is ultimately revealed to be a speculative fiction posited by Weston as an example of the espionage that Joe would have to perform if he were to join WIN. The plot twist, in particular the revelation that Russia and the West are allies in the future, is praised by academic Nicholas J. Cull for its \"progressiveness of spirit\", and for exemplifying Anderson's \"[taking] an end to the Cold War as a given in his work\". Anderson was motivated by what he perceived as a \"duty to the rising generation to avoid perpetuating Cold War stereotypes\", once stating that he \"tried very hard not to put [his] ten cents into creating World War Three\".\n\nDespite the existence of a global government and intelligence organisation, the nations of Earth are still politically divided into Western and Eastern blocs; here, Cull argues, \"Joe 90\" is similar to other Anderson series in that it \"unashamedly capitalized on the Cold War cult of the secret agent whose skills defend the home from enemies unknown\". The recurring antagonist of WIN and Joe is the non-aligned \"Eastern Alliance\", which dominates Asia and appears in the episodes \"Attack of the Tiger\" and \"Mission X-41\". Meanwhile, villains in \"International Concerto\", \"Business Holiday\", \"Arctic Adventure\" and \"The Professional\" speak with Slavic accents. \"Arctic Adventure\" and \"Attack of the Tiger\" combine the threat from the East with the hazards of nuclear technology: in the former, Joe must recover a stray atomic warhead from the ocean floor while avoiding enemy submarines, while in the latter, he is tasked with destroying an Eastern nuclear device that is about to be launched into Earth orbit. By contrast, an episode that presents the benign aspects of such technology is \"Big Fish\", in which Joe labours to remove a defective nuclear submarine from the territorial waters of a Latin American police state.\n\nFollowing \"Captain Scarlet and the Mysterons\", \"Joe 90\" was purposely conceived and developed to be a different kind of Supermarionation series, placing the narrative emphasis less on action, advanced technology and visual effects and more on characterisation and plotlines subscribing more to the spy thriller genre than science fiction. Co-creator Gerry Anderson explained, \"The show majored on its characters, which I thought were all very good. The puppets had become so lifelike, I now strongly believed that they could carry the action without the usual massive assistance from futuristic hardware.\" Explaining his inspiration for the series, Anderson remembered his pre-Supermarionation days when he served as an assistant editor for such films as \"The Wicked Lady\", handling recording tape on a daily basis. While pondering on the blanking and re-use of such tape, Anderson made a connection to the human brain's electrical activities, explaining, \"I read somewhere that the human brain is controlled by electrical impulses and how thoughts are stored electronically. I started toying with the story potential of a process that would allow the recording of brain patterns and transferring them to another brain. I was really likening it to magnetic recording, where material could be stored or transferred to another tape.\" When it came to naming the lead character and, from that, the name of the new series, Anderson recalled that on one of his earlier productions, \"Fireball XL5\", the surname \"Ninety\" had been an early proposal for Colonel Steve Zodiac, and selected it for the new schoolboy protagonist.\n\nCommissioned by ITC financier Lew Grade in the autumn of 1967, with pre-production completed in October while the final episodes of \"Captain Scarlet\" were still being filmed, principal photography for \"Joe 90\" ran from 13 November 1967 to mid-August 1968 using two puppet stages at the Century 21 Studios on the Slough Trading Estate in Berkshire. The average shooting period for each episode was two weeks, as had been the case with the previous series. The script for the pilot (titled only in production documentation as \"The Most Special Agent\") was written by Anderson and his wife, Sylvia, as was the custom for every new puppet series that the couple developed in the 1960s. Before the concept of WIN was devised, Joe was to have become the \"Most Special Agent\" of the CIA. Most of the other episodes were written by Tony Barwick, with Shane Rimmer contributing six scripts. Rimmer was hired to write for \"Joe 90\" while co-writing a book with Barwick, who initially offered him a two-script contract (they were filmed as the episodes \"Splashdown\" and \"Big Fish\"). Since he was occupied by post-production on the second \"Thunderbirds\" feature film, \"Thunderbird 6\", and the development of his live-action film, \"Doppelgänger\", Gerry Anderson was unable to fulfil the producer role as he had done for \"Captain Scarlet\", and instead passed the responsibility to Reg Hill and David Lane. Lane recalls that, in his role as producer, he was \"responsible for looking at the scripts, the effects, the puppets, the whole thing really\". He found support in Anderson's long-serving collaborator Desmond Saunders, who directed the pilot and served as production controller for the rest of the series. Other directors for \"Joe 90\" included Leo Eaton, Alan Perry and Ken Turner, all of whom had contributed to \"Captain Scarlet\", and Peter Anderson, who was promoted from his earlier position as assistant director to replace the outgoing Brian Burgess and Robert Lynn.\n\nA Christmas-themed episode, \"The Unorthodox Shepherd\", featured location filming to an extent unprecedented for a puppet-based Anderson series. \"The Secret Service\", the Andersons' next production after \"Joe 90\", developed the hybridity further with the incorporation of extensive footage of live actors in long shot, intercut with scale puppet sequences.\n\nKeith Wilson and Grenville Nott superseded Bob Bell as heads of the art department and built the interior of Culver Bay Cottage from a design by Mike Trim. Anderson remembered his satisfaction with the cottage set: \"The interior, with its beams and lovely soft furnishings, was really beautiful.\" The construction of the BIG RAT model, meanwhile, was entrusted to the newly formed incorporated company Century 21 Props (or Electronics), which was responsible for the various gadgets that appeared in the series and was based in Bourne End in Buckinghamshire.\n\nAlthough mostly occupied with \"Thunderbird 6\" and \"Doppelgänger\", Derek Meddings briefly reprised his role as head of special effects to construct Professor McClaine's Jet Air Car. The design concept was a disappointment to Anderson, who commented: \"The car looked like no other piece of hardware we had had previously but I was wary of canning it as I feared I might be becoming stereotyped. Maybe the whole thing was becoming a bit narrow; all the ideas were becoming similar.\" Stephen La Rivière, writer of \"Filmed in Supermarionation: A History of the Future\", views the Jet Air Car as an update of \"Supercar\", the vehicle that appeared in Anderson's 1961 series of the same name, but agrees that while the Jet Air Car is the \"star vehicle\" of \"Joe 90\", it is visually unappealing in comparison to the \"beautiful, sleek design of its predecessor.\"\n\nThe Supermarionation puppets featured in \"Joe 90\" are of the more accurately proportioned kind introduced for \"Captain Scarlet\", and which would also be used for the Andersons' final puppet series, \"The Secret Service\". Simultaneously, the drive for enhanced realism across all major design aspects which started with the preceding series continued for \"Joe 90\". Main character puppets from \"Captain Scarlet\" were re-used for \"Joe 90\" with the exceptions of the Captain Scarlet and Captain Blue marionettes. Few new puppets were constructed, the only notable exceptions being Professor McClaine (sculpted by Mary Turner and modelled on 'bouncing bomb' designer Barnes Wallis), Joe (sculpted by Tim Cooksey), and Mrs Harris.\n\nThe Joe puppet was the first child marionette to be made as part of the new generation of Supermarionation puppets introduced for \"Captain Scarlet\", for which the sculpting team were careful to achieve realistic proportions for the body of a nine-year-old boy. The puppets of Sam Loover and Shane Weston had each made several appearances in the previous series, but for their regular role in the new series a variety of alternative heads were created from the \"expressionless\" templates – including \"smilers\", \"frowners\" and \"blinkers\" – and the Shane Weston puppet was re-wigged. Many of the recycled \"revamp puppets\", used to depict supporting characters for \"Captain Scarlet\", were also duplicated with darker skin colours to portray characters from a range of ethnicities. Further to these requirements, the use of two shooting soundstages necessitated the duplication of all the \"expressionless\" main character puppets to avoid conflicts over resources between the two filming units. As in the previous series, \"under control\" puppets, manipulated by levers from below as opposed to wires from a gantry above, feature in \"Joe 90\".\n\nThe opening and ending theme and incidental music of \"Joe 90\" composed by Barry Gray, who was responsible for music on other Anderson series. Episodes of \"Joe 90\" start with either a cold open (the first Anderson series to do so) or the main title sequence, which sees Joe sitting in the BIG RAT's \"Rat Trap\" and receiving transferred knowledge from the machine. The sequence is accompanied by Gray's opening theme, which is dominated by the notes of guitarist Vic Flick, known for performing lead guitar in the recording of the \"James Bond Theme\" for \"Dr. No\" (1962). In Anderson's biography, \"What Made Thunderbirds Go!\", the \"Joe 90\" theme is described as a \"dizzying piece of psychedelic pop art that could have been produced only in the late Sixties\". The closing credits are superimposed over images of objects such as Joe's BIG RAT spectacles, his WIN badge, and also his briefcase, gun, and transceiver; while the concepts for these images were photographic, the final versions were augmented with airbrush artwork.\n\nIn addition to the themes and tracks for the pilot, \"The Most Special Agent\", Gray composed incidental music for 20 additional episodes of \"Joe 90\". Music for the \"Joe 90\" episodes was recorded between 18 January and 27 September 1968, starting with the titles and the pilot in a session at the London Olympic Sound Studios and ending with one of the final instalments, \"See You Down There\" at CTS Studios. Scores were also recorded at Gray's residence in Esher, Surrey.\n\nGray's compositions for \"Joe 90\" occasionally required the hiring of guest talent. The piano music featured extensively in the episode \"International Concerto\" was performed by Robert Docker (the human hands seen in the close-up shots of Joe's playing belonged to Gray's son, Simon). \"Lone-Handed 90\" includes a recurring harmonica, played by the Canadian musician Tommy Reilly.\n\nA CD of the \"Joe 90\" soundtrack, running to 28 pieces, was released by Silva Screen Records in 2006. Awarding a rating of 3.5 stars out of five, AllRovi reviewer William Ruhlmann comments that the scores are \"not great writing\", but adds that Gray's work was \"perfectly adequate, if not inspired.\" Previous releases include a 45 rpm gramophone record, \"Title Theme from the ATV Series Joe 90\", also featuring various incidental music.\n\nIn comparison to \"Captain Scarlet and the Mysterons\", \"Joe 90\" features a smaller cast, voicing just five regular characters. Like \"Captain Scarlet\", the series has been viewed as more \"English-sounding\", the Andersons abandoning their stipulation dating from the production of \"Thunderbirds\" that the puppet cast be American and thus dispensing with the established format of their series' principal character being a \"square-jawed, fair-skinned male with a Mid-Atlantic accent\". Instead, in a manner similar to \"Captain Scarlet\", \"Joe 90\" focuses on the strong American supporting characters of Sam Loover and Shane Weston.\n\n\nSupporting characters were voiced by Alexander, Healy and Anderson as well as earlier Anderson contributors Gary Files, Martin King, Jeremy Wilkin, Shane Rimmer and (for one episode, \"Viva Cordova\") Liz Morgan. Rimmer and Morgan, however, are not credited in the closing titles. Files recalls that he felt honoured to be asked to rejoin the Andersons for another production following \"Captain Scarlet\", and that he was \"tickled pink\" to be performing with Davies, adding, \"I hated the way that so many so-called producers wouldn't meet his eye. He was Maigret forever, you see, in their eyes.\" Morgan, meanwhile, explains how she was contracted for her single voice role in \"Joe 90\": \"They needed a voice, they called around and everyone else was out shopping. So they called me in.\"\n\nIn the United Kingdom, the starts of the regional broadcasts were staggered, with \"Joe 90\" premiering on ATV Midlands and Tyne Tees in late September 1968 and moving on to LWT, Southern and Anglia shortly after. The series reached the Harlech and Channel regions in November and finally Granada on Christmas Day, although the first episode to air was the Christmas-themed \"The Unorthodox Shepherd\" rather than the pilot, \"The Most Special Agent\". Granada was one of several regions to broadcast the series under the alternative title \"The Adventures of Joe 90\". Although the series was re-run several times in various regions during the 1970s, it was not transmitted in the Yorkshire region until 1981, when it was secured by ITV for a syndicated run. In the United States, \"Joe 90\" was broadcast in first-run syndication in 1969.\n\n\"Joe 90\" was later purchased for early-morning network transmissions on BBC1 in 1994. Rights holder PolyGram cleared the programme for broadcast on the condition that the \"zooming\" \"Joe 90\" logo in the title sequence be replaced with a new static version to distinguish it from the logo for the American G.I. Joe toy brand, which, PolyGram believed, appeared too similar. The videotapes used for broadcast were 16 mm transfers of the 35 mm film and were edited for timeslot constraints, with the cold open re-arranged where applicable so that the titles now opened each episode, and the closing credits minimised to permit a Children's BBC presenter to read out viewer birthday cards. A separate 1994 run on Nickelodeon made none of these alterations to the 1960s material. With \"Thunderbirds\" and \"Captain Scarlet and the Mysterons\", the series commenced a run on the UK Sci Fi Channel in 2009.\n\nFor \"Joe 90\"<nowiki>'</nowiki>s original run, in some regions the end of the title sequence incorporated a zoom-in shot of Joe's WIN glasses accompanied by a voice-over provided by actor Tim Turner, stating, \"These are Joe 90's special glasses. Without them, he's a boy. Wearing them, he's an expert.\" This short speech, intended to warn child viewers not to put themselves at risk by imitating Joe's exploits, has been erroneously attributed to Keith Alexander on the \"Joe 90\" Region 2 DVD box set, on which it is a special feature.\n\nIn an episode guide to the Anderson TV series, science-fiction writer John Peel questions Mac's ethics in \"experimenting on\" Joe to further the development of the BIG RAT. On the subject of Joe as a secret agent he remarks, jokingly, \"Presumably there are no child labour laws in the future!\" La Rivière's attention is drawn to one of Mac's lines at the end of the pilot, in a scene that he considers \"amusing\"; the professor's admonition \"Don't come crying to me if you get hurt!\" represents his preparedness to \"abnegate all parental responsibility\" towards his adoptive son. Observing the series' subscription to \"wider themes in Cold War culture\", Cull likens the BIG RAT's powers to brainwashing, but concludes that it is a fundamentally \"benign\" technology. The more violent style introduced in \"Captain Scarlet and the Mysterons\" is sometimes evident in \"Joe 90\": in \"Hi-jacked\", Joe kills an enemy with a grenade, while in \"Project 90\", Professor McClaine is menaced by a drill poised to pulverise his head. On the subject of violence, episode director Desmond Saunders comments: \"There was an unpleasant side to it which I never really understood. There was something about it that was very strange and sinister.\"\n\nOn the other hand, producer David Lane praises the series for its increased humour following the dark tone of \"Captain Scarlet\" and sees \"Joe 90\" as much more family-oriented in comparison to its forerunner, summing up the series as \"a great little programme.\" Anthony Clark of the British Film Institute commends \"Joe 90\" for more effective characterisation than \"Captain Scarlet\", and also compliments the quality of its scripts and Barry Gray's musical score. La Rivière underlines a connection between the child protagonist and the theme of espionage, writing, \"The premise that drives \"Joe 90\" taps into the fantasy indulged by most boys that they, even at nine years old, can be James Bond.\" Writer John R. Cook agrees with La Rivière's points on audience self-identification, describes the series as a \"wish-fulfilment fantasy\" and suggests that the character of Joe is a mirror image of the target child viewer. Comparisons have been made to later franchises with child protagonists who are in fact operatives for intelligence agencies, such as Robert Rodriguez's \"Spy Kids\" films, and Anthony Horowitz's \"Alex Rider\" novels.\n\nLa Rivière noted the intimacy of the series and the predominantly male voice cast and characters, suggesting that \"Joe 90\" is \"very much a \"Boy's Own\" adventure.\" Out of the 30 episodes, only ten feature appearances from female characters, a fact which La Rivière attributes to the increased demands on Century 21 for its feature film productions, \"Thunderbird 6\" and \"Doppelgänger\". Peel suggests that the female absence leaves \"Joe 90\", with many other Anderson productions, inferior to previous Supermarionation effort \"Thunderbirds\", in which the character of Lady Penelope has a primary role in several episodes. Grouping \"Joe 90\" with the earlier \"Supercar\" and the subsequent \"The Secret Service\", Peel concludes, \"It is hardly coincidental that these tend to be the least-loved of [Anderson's] series; he had, after all, ignored half of his potential audience.\" For Peel, this return of the \"standard Anderson sexism\" is only one aspect of deterioration between \"Joe 90\" and previous productions. Peel challenges La Rivière's asserted \"kids play Bond\" theme, writing that, \"being a somewhat nerdy kid with glasses and brain implants was not really thrilling.\"\n\nPeel's view was contested by Anderson's and Cull's belief that the series, with its bespectacled main character of Joe McClaine, can increase the self-confidence of young viewers who wear glasses: \"Suddenly they were proud because they had something in common with Joe 90.\" Since the series' first appearance, the epithet \"Joe 90\" has become a popular term of endearment for both children and adults with glasses reminiscent of Joe's (such as snooker player Dennis Taylor). During UK repeats in the 1990s, similarities were drawn between Joe and then-Prime Minister John Major, also known for wearing large spectacles. Jeff Evans, writer of \"The Penguin TV Companion\", criticises the plot element of the glasses, writing, \"Joe simply dons a pair of scientific glasses, making him look like the class swot than a secret agent.\"\n\nCook reads further into the concept of child empowerment in \"Joe 90\", writing that the series creates a \"technological utopia\" around youth, remarking, \"Through the character of Joe, his brain hardwired at the start of each episode into the BIG RAT supercomputer, the young are shown to be literally at one with technology.\" He adds that the instant access to brain patterns that the BIG RAT affords to Joe may be interpreted as heralding the development of the Internet over a decade after \"Joe 90\" was produced. With his intellectual horizons broadened, Joe becomes the manifestation of \"homo superior\", yet his youthfulness grants him the power to change the fraught political world in ways that no adult could due to the limitations of their imagination. In this respect, Cook holds up \"Joe 90\" as a precursor to the 1970s television series \"The Tomorrow People\", which also concerned ideas of human transcendence in children. This idea, Cook says, was evident in the title of \"Joe 90\" itself: \"no longer is he a nine-year-old boy but instead his status and capacities have been multiplied tenfold to transform him into agent 'Joe 90', his name an appealing futuristic echo of the then distant year of 1990.\"\n\nUltimately, \"Joe 90\" has proven to be less successful than previous series made by Anderson. In the Anderson-related book, \"Supermarionation Classics\", the model work and scripts are praised, but it is conceded that the series \"failed to arouse more than a passing interest with some Anderson fans.\" Stephen Hulse refers to \"Joe 90\" as \"clearly the most child-oriented of the latter Anderson Supermarionation series\" and \"technically accomplished\", but \"one of the Anderson stable's lesser series\". However, its spy-fi theme led on to the final Supermarionation series, \"The Secret Service\", which too features an unconventional secret agent (a vicar, Father Stanley Unwin) and an intelligence organisation with a contracted name (BISHOP, an acronym for \"British Intelligence Service Headquarters, Operation Priest\").\nIn 1981, a compilation film of the \"Joe 90\" episodes \"The Most Special Agent\", \"Splashdown\", \"Attack of the Tiger\" and \"Arctic Adventure\", titled \"The Amazing Adventures of Joe 90\", was created under the supervision of Robert Mandell of ITC Entertainment's New York offices. Intended to boost American syndication sales, \"The Amazing Adventures of Joe 90\" is one of a number of composite films of Gerry Anderson productions, which were released both to stations and on home video under the promotional banner of \"Super Space Theater\". Material for \"The Most Special Agent\" was re-edited to remove the framing sequences set at Culver Bay Cottage and WIN Headquarters London, with the result that Joe's fictitious mission to steal the Russian prototype fighter appears to be a real assignment for the nine-year-old WIN agent. Despite each of the episodes in this compilation receiving a U certificate from the British Board of Film Classification (BBFC), \"The Amazing Adventures of Joe 90\" was rated PG.\n\nFrom the 1980s, the distribution rights to the ITC productions belonged to PolyGram Television. Subsequent sales were made to Carlton International in the late 1990s and finally Granada International which, through a merger with Carlton International in 2004, now forms ITV Global Entertainment, a division of ITV plc. During the 1990s, the possibility of a live-action film adaptation of \"Joe 90\" was mooted by PolyGram. The idea re-emerged in the 2000s, when in 2003 the magazine \"Variety\" reported that a film version was in the planning stages, to be produced by Disney. However, to date, the film proposal remains to be developed. In 2005, Anderson said of negotiations with Granada, \"We have regular meetings and although they are very polite and very nice, nothing ever happens.\"\n\nWhen \"I Love the '70s\", \"'80s\" and \"'90s\", three British pop culture nostalgia programmes, were broadcast on BBC Two in 2001, a set of \"Joe 90\"-themed \"trailers\" were filmed to precede instalments of the last of these series. In each of the three previews, the character of Joe is depicted entering the BIG RAT's \"Rat Trap\" to receive the brain pattern of a 1990s household name, from Oasis bandmember Liam Gallagher (representing 1990) to comedian Vic Reeves (1991) to the character of Garth (portrayed by Dana Carvey) from the 1992 film, \"Wayne's World\". On leaving the \"Rat Trap\", Joe has assumed the identity of each BIG RAT subject and acts and speaks using their mannerisms. Edited versions of the trailers missing the BBC Two voiceovers and logos are included as special feature material on the Region 2 release of the \"Joe 90\" DVD box set.\n\nAuthentic 1960s associated media for \"Joe 90\" included a Century 21 Toys range comprising friction-drive and battery-operated versions of Professor McClaine's Jet Air Car and Sam Loover's futuristic saloon. Also available were Joe's WIN briefcase (complete with replica gadgets and pistol) and his WIN badge (reading \"Most Special Agent\"). \"Joe 90\" was also given its own weekly comic, \"Joe 90 Top Secret\", which ran for 34 issues and narrated the TV episodes in strip form, while also including strips based on the TV series \"The Champions\" and \"Land of the Giants\". In September 1969, \"Joe 90 Top Secret\" merged with the established Anderson tie-in \"TV21\" (previously titled \"TV Century 21\"), which then came to be known as \"TV21 and Joe 90\". After a further 36 issues, \"Joe 90\" strips were dropped from the comic and the new title discontinued in favour of the original \"TV21\".\n\nThe 1990s were marked by a considerable interest in old TV series from the 1960s and 70s – \"Joe 90\" was one of those that was among the repeats and was also the subject of a strip series in the \"Funday Times\" section of \"The Sunday Times\". Strips from \"Joe 90 Top Secret\" were reprinted in a new publication, \"Joe 90\", which was launched to tie in with the 1994 BBC re-runs but which also, after just seven issues, merged into a related comic, on this occasion Fleetway's \"Thunderbirds\". Other \"Joe 90\" print media include 1968 and 1969 \"Joe 90\" annuals from Century 21 Publishing and two short paperback novels, \"Joe 90 and the Raiders\" (by Tod Sullivan) and \"Joe 90 in Revenge\" (by Howard Elson), published by May Fair Books.\n\nIn the United Kingdom, the earliest home releases of \"Joe 90\" in the 1980s were controlled by \"Channel 5\", later re-branded as \"PolyGram Video\". Released in an eight-volume series and re-packaged in 1992, the set included \"The Most Special Agent\", \"Splashdown\", \"Attack of the Tiger\" and \"Arctic Adventure\" in their re-edited forms from the 1981 compilation film \"The Amazing Adventures of Joe 90\", which itself received three video releases both in PAL and NTSC format between 1981 and 1986. The 1980s and 90s VHS releases used 16 mm prints, which were of a quality poorer than that of the original film.\n\nIn September 2002, a DVD box set of all 30 \"Joe 90\" episodes, sourced from a digital remaster of 35 mm film prints, was released in Region 2 by Carlton. The five component discs were also released individually at intervals between September 2002 and January 2003, and the episodes were also marketed in a new five-volume VHS package. A North American set from A&E debuted in July 2003 before a Region 4 version appeared in October. A French-language release of \"Joe 90 – Agent Très Spécial\" (English: \"Joe 90 – Very Special Agent\") hit the Canadian market in 2004. With these DVD releases, the component episodes of \"The Amazing Adventures of Joe 90\" were made commercially available in their unedited form for the first time.\n\n\n"}
{"id": "23877458", "url": "https://en.wikipedia.org/wiki?curid=23877458", "title": "Kingbright", "text": "Kingbright\n\nKingbright (), founded in 1980, is one of the leading manufacturers of light-emitting diodes (LEDs).\n\nHeadquartered in Taipei, Taiwan, Kingbright specializes in manufacturing LED-related products. Kingbright operates four production facilities in Shenzhen, China. The company has sales locations at United States, France, Germany, Taiwan, Korea, Japan, Malaysia, Hong Kong, Beijing, Shanghai, and Shenzhen. The company's North American operation is Kingbright USA, located in Los Angeles, California.\n\n\n"}
{"id": "10088199", "url": "https://en.wikipedia.org/wiki?curid=10088199", "title": "Magnetocapacitance", "text": "Magnetocapacitance\n\nMagnetocapacitance is a property of some dielectric or insulating materials and/or metal-insulator-metal heterostructures that exhibit a change in the value of their capacitance when an external magnetic field is applied to them. Magnetocapacitance can be an intrinsic property of some dielectric materials, such as multiferroic compounds like BiMnO, or can be a manifest of properties extrinsic to the dielectric but present in capacitance structures like Pd/AlO/Al.\n"}
{"id": "44726", "url": "https://en.wikipedia.org/wiki?curid=44726", "title": "Magnetoresistance", "text": "Magnetoresistance\n\nMagnetoresistance is the tendency of a material (preferably ferromagnetic) to change the value of its [electrical resistance] in an externally-applied magnetic field. There are a variety of effects that can be called magnetoresistance: some occur in bulk non-magnetic metals and semiconductors, such as geometrical magnetoresistance, Shubnikov de Haas oscillations, or the common positive magnetoresistance in metals. Other effects occur in magnetic metals, such as negative magnetoresistance in ferromagnets or anisotropic magnetoresistance (AMR). Finally, in multicomponent or multilayer systems (e.g. magnetic tunnel junctions), giant magnetoresistance (GMR), tunnel magnetoresistance (TMR), colossal magnetoresistance (CMR), and extraordinary magnetoresistance (EMR) can be observed.\n\nThe first magnetoresistive effect was discovered by William Thomson, better known as Lord Kelvin, in 1856, but he was unable to lower the electrical resistance of anything by more than 5%. Nowadays, systems, e.g. semimetals or concentric ring EMR structures, are known where a magnetic field can change resistance by orders of magnitude. As the resistance may depend on magnetic field through various mechanisms, it is useful to separately consider situations where it depends on magnetic field directly, e.g. geometric magnetoresistance and multiband magnetoresistance, and those where it does so indirectly through magnetisation, e.g. AMR, TMR.\n\nWilliam Thomson (Lord Kelvin) first discovered ordinary magnetoresistance in 1856. He experimented with pieces of iron and discovered that the resistance increases when the current is in the same direction as the magnetic force and decreases when the current is at 90° to the magnetic force. He then did the same experiment with nickel and found that it was affected in the same way but the magnitude of the effect was greater. This effect is referred to as anisotropic magnetoresistance (AMR).\n\nIn 2007, Albert Fert and Peter Grünberg were jointly awarded the Nobel Prize for the discovery of Giant Magnetoresistance.\n\nAn example of magnetoresistance due to direct action of magnetic field on electric current can be studied on a Corbino disc (see Figure).\nIt consists of a conducting annulus with perfectly conducting rims. Without a magnetic field, the battery drives a radial current between the rims. When a magnetic field perpendicular to the plane of the annulus is applied, (either into our out of the page) a circular component of current flows as well, due to the Lorentz force. A discussion of the disc is provided by Giuliani. Initial interest in this problem began with Boltzmann in 1886, and independently was re-examined by Corbino in 1911.\n\nIn a simple model, supposing the response to the Lorentz force is the same as for an electric field, the carrier velocity v is given by:\n\nwhere μ is the carrier mobility. Solving for the velocity, we find:\n\nwhere the effective reduction in mobility due to the B-field (for motion perpendicular to this field) is apparent. Electric current (proportional to the radial component of velocity) will decrease with increasing magnetic field and hence the resistance of the device will increase. Critically, this magnetoresistive scenario depends sensitively on the device geometry and current lines and it does not rely on magnetic materials.\n\nIn a semiconductor with a single carrier type, the magnetoresistance is proportional to (1 + (\"μB\")), where μ is the semiconductor mobility (units m·V·s or T) and \"B\" is the magnetic field (units teslas). Indium antimonide, an example of a high mobility semiconductor, could have an electron mobility above 4 m·V·s at 300 K. So in a 0.25 T field, for example the magnetoresistance increase would be 100%.\n\nThomson's experiments are an example of AMR, property of a material in which a dependence of electrical resistance on the angle between the direction of electric current and direction of magnetization is observed. The effect arises from the simultaneous action of magnetization and spin-orbit interaction and its detailed mechanism depends on the material. It can be for example due to a larger probability of s-d scattering of electrons in the direction of magnetization (which is controlled by the applied magnetic field). The net effect (in most materials) is that the electrical resistance has maximum value when the direction of current is parallel to the applied magnetic field. AMR of new materials is being investigated and magnitudes up to 50% have been observed in some ferromagnetic uranium compounds.\n\nIn polycrystalline ferromagnetic materials, the AMR can only depend on the angle formula_3 between the magnetization and current direction\nand (as long as the resistivity of the material can be described by a rank-two tensor), it must follow\n\nformula_4\n\nwhere formula_5 is the (longitudinal) resistivity of the film and formula_6 are the resistivities for formula_7 and formula_8, respectively. Associated with longitudinal resistivity, there is also transversal resistivity dubbed (somewhat confusingly[1]) the planar Hall effect. In monocrystals, resistivity formula_5 depends also on formula_10 individually.\n\nTo compensate for the non-linear characteristics and inability to detect the polarity of a magnetic field, the following structure is used for sensors. It consists of stripes of aluminum or gold placed on a thin film of permalloy (a ferromagnetic material exhibiting the AMR effect) inclined at an angle of 45°. This structure forces the current not to flow along the “easy axes” of thin film, but at an angle of 45°. The dependence of resistance now has a permanent offset which is linear around the null point. Because of its appearance, this sensor type is called 'barber pole'.\n\nThe AMR effect is used in a wide array of sensors for measurement of Earth's magnetic field (electronic compass), for electric current measuring (by measuring the magnetic field created around the conductor), for traffic detection and for linear position and angle sensing. The biggest AMR sensor manufacturers are Honeywell, NXP Semiconductors, STMicroelectronics, and Sensitec GmbH.\n\n"}
{"id": "12173993", "url": "https://en.wikipedia.org/wiki?curid=12173993", "title": "Military computers", "text": "Military computers\n\nThis article specifically addresses US armed forces military computers and their use.\n\nSome of the earliest computers were military computers.\nMilitary requirements for portability and ruggedness led to some of the earliest transistorized computers, such as the 1959 AN/MYK-1 (MOBIDIC),\nthe 1960 M18 FADAC,\nand the 1962 D-17B;\nthe earliest integrated-circuit based computer,\nthe 1964 D-37C;\nas well as one of the earliest laptop computers, the 1982 Grid Compass.\nMilitary requirements for a computer small enough to fit through a submarine's hatch led to the AN/UYK-1.\n\nTypically a military computer is much more robust than an industrial computer enclosure. Most electronics will be protected with a layer of conformal coating. There will be more structure inside to support the components, the plug-in cards will be individually supported and secured to assure they do not pop out of their sockets, the processor and heat sink will be secured, memory will be glued into their sockets, and so forth. This is to assure nothing moves during the shock events.\n\nThere are several differentiators between military computers and typical office or consumer computers:\n\n\nCost – Military computers are generally much more expensive than office/consumer computers. Consumer computers from manufacturers such as Dell are manufactured in very high quantities which leads to lower costs due to economy of scale. Military programs, on the other hand, can require small numbers of systems leading to higher costs. Military computers will typically also be constructed of more robust materials with more internal structure, more cooling fans, a more robust power supply, and so forth.\n\nIntended Environment – An office or consumer computer is intended for use in a very controlled shirt-sleeve environment with moderate temperatures and humidity and minimal dust. A military computer can be designed to operate in very adverse environments with extremes of temperature such as -20C to +65C operating, 5% to 95% humidity levels, and high dust loading in the air as well as other insults to the hardware. They may be required to operate in high salt environments such as on a ship or designed for high shock and vibration such as on a ship or submarine. Military computers may be intended for installation on aircraft in which case they need to be crash worthy and able to operate at high altitudes if in unpressurized aircraft. The same computer may be required to operate in Afghanistan as well as in Alaska with no change in the design.\n\nLong Term Availability – Military programs last years and identical replacement hardware may be required over the life of the program. Consumer computers are often driven by the latest and greatest to realize the highest possible performance, such as required to play games. The motherboard in a consumer grade computer may have an availability measured in months instead of years or decades. In a consumer level computer, over the lifetime of the product availability, it is not unheard of for all the components such as the motherboard, drives, BIOS, video board, etc., to be different from computer to computer. That is not acceptable in a military computer for which supporting documents have been created and systems tested and approved. \n\nArchitecture – There are many types of computer architecture. The most common that people know of is the PC as created by IBM. Many military computer systems are built around alternative plug-in bus structures such as VMEbus or Compact PCI. A military computer may not provide for plug-in cards and be in a dedicated form factor for a specific application such as installation on a UAV such as the Global Hawk. \n\nFeature Set – A military computer may have features not found on a consumer grade computer such as Circular connectors, hot swap power supplies, hot swap fans, custom front panel features such as LCD displays, and so forth.\n\nThe Armed Forces have many numerical designations for computers or other equipment, to guide the military buyer's choice of appropriate technology for their application. For instance, MIL-S-901D would indicate that the computer passed shock and vibration requirements of specific tests for Navy installation. Some of these tests are specific to application usage, such as barge explosion testing, which simulates a torpedo hit and subsequent high peak shock to a ship on which the computer is installed. The \"gold standard\" of testing for compliance with 901D is the Barge Test. A Barge Test is performed four times, each time placing 60 lbs HBX-1 explosive 24 feet under water, starting at 40 feet away, then at 30, 25 and finally 20 feet. In addition, the tests are performed in a fore-and-aft orientation to simulate an explosion at the bow or stern of the ship and athwartship to simulate an explosion by the side of the ship. A video of a barge test can be viewed.\n\nOther more common requirements are MIL-STD-810 for environmental testing such as storage and operating temperature, humidity, salt spray, dirt, etc. Another common specification is MIL-STD-461 for electromagnetic compatibility. There are specifications for workmanship, wiring, packaging, and so forth, that military computers are required to meet. \n\nMore on MIL Standards and Specifications at Defense Standard.\n\nTo meet the challenges of defending the U.S. cyber network, the U.S. military has taken steps to improve the security of devices connected to Department of Defense information networks. According to United States Cyber Command, \"Cyber threats demand new approaches to managing information, securing information, and ensuring our ability to operate.\"\n\nAll military computers must conform to the latest FIPS 140 standards (FIPS 140-2) which specify the latest requirements for cryptography modules on devices used throughout the U.S. government. FIPS 140-3, currently under development, will address new requirements to face existing threats, including software security and an additional level of security.\n\nTo address the risks associated with the increasing prevalence of commercial mobile devices (CMDs), a DoD Inspector General report from March 2013 identifies improvements necessary to track and configure commercial mobile devices to meet Army compliance standards. The report identifies existing gaps in tracking and sanitization for over 14,000 CMDs, recommending a \"clear and comprehensive policy to include requirements for reporting and tracking all commercial mobile devices purchased under pilot and non-pilot programs.\"\n\nThe progress of small-scale computer technology in military applications was initially slow due to concerns about security and the ability to survive rugged environments and enemy weaponry. PC-based technology in the 20th century was not robust enough to withstand combat conditions and severe environments.\n\nHazards in the field include water and corrosives, sand and wind, extreme temperatures, high shock and vibration, power interruptions, susceptibility to EMI/RFI radiation, etc. Also, operator interface was complex, and most operating systems were not fast in operation, or easy to learn and use in pressure situations.\n\nIn the last decade, improvements in design and operator interface have resulted in new mandates for the use of small computer technology in the military. Some of the improvements have migrated over from home and business computing. Others have migrated over from industrial computing, where designs for environments such as Zone 1 hazardous areas in oil & gas exploration have been modified for army and navy environments.\n\n\nIn the last 20 years, wide acceptance of small-scale computer technology in the military has occurred, and is likely to increase greatly. Confidence has improved in the ability of equipment to withstand combat and extreme environment conditions. Most importantly, modern combat has become a duel of speed. Faster and more technologically advanced weaponry demonstrates first-strike capability in current combat situations, which is likely only to encourage further implementation of computer technology into systems used in the US Armed Forces in the future.\n\nVarious branches of the military have mandated that future systems will be based on Zero Client or Thin Client technology.\n\n"}
{"id": "39645539", "url": "https://en.wikipedia.org/wiki?curid=39645539", "title": "Mycorestoration", "text": "Mycorestoration\n\nMycorestoration is the use of fungi to restore degraded environments. It is a multi-method approach to restore damaged habitats such as oil spill sites and logging roads, while also restoring the health of targeted forest sites that have been compromised in development. Mycorestoration is also used to control insect populations. It generally uses a four-tier approach of mycofiltration, mycoforestry, mycoremediation, and mycopesticides. The mycelia of a number of different gilled fungi are used in some of these applications.\n"}
{"id": "2991563", "url": "https://en.wikipedia.org/wiki?curid=2991563", "title": "Neutrodyne", "text": "Neutrodyne\n\nThe Neutrodyne radio receiver, invented in 1922 by Louis Hazeltine, was a particular type of tuned radio frequency (TRF) receiver, in which the instability-causing inter-electrode capacitance of the triode RF tubes is cancelled out or \"neutralized\". to prevent parasitic oscillations which caused \"squealing\" or \"howling\" noises in the speakers of early radio sets. In most designs, a small extra winding on each of the RF amplifiers' tuned anode coils was used to generate a small antiphase signal, which could be adjusted by special variable trim capacitors to cancel out the stray signal coupled to the grid via plate-to-grid capacitance. The Neutrodyne circuit was popular in radio receivers until the 1930s, when it was superseded by the superheterodyne receiver.\n\nThe circuit was developed about 1922 by Harold Wheeler who worked in Louis Hazeltine's laboratory at Stevens Institute of Technology, so Hazeltine is usually given the credit. The tuned radio frequency (TRF) receiver, one of the most popular radio receiver designs of the time, consisted of several tuned radio frequency (RF) amplifier stages, followed by a detector and several audio amplifier stages. A major defect of the TRF receiver was that, due to the high interelectrode capacitance of early triode vacuum tubes, feedback within the RF amplifier stages gave them a tendency to oscillate, creating unwanted radio frequency alternating currents. These parasitic oscillations mixed with the carrier wave in the detector, creating heterodynes (beat notes) in the audio frequency range, which were heard as annoying whistles and howls from the speaker. \n\nHazeltine's innovation was to add a circuit to each radio frequency amplifier stage which fed back a small amount of energy from the plate (output) circuit to the grid (input) circuit with opposite phase to cancel (\"neutralize\") the feedback which was causing the oscillation. This effectively prevented the high-pitched squeals that had plagued early radio sets. A group of more than 20 companies known as the Independent Radio Manufacturers Association licensed the circuit from Hazeltine and manufactured \"Neutrodyne\" receivers throughout the 1920s. At the time, RCA held a virtual monopoly over commercial radio receiver production due to its ownership of the rights to the Armstrong regenerative and superheterodyne circuits. The Neutrodyne ended this control, allowing competition. Compared to the technically superior superheterodyne the Neutrodyne was cheaper to build. As basically a TRF receiver, it was also considered easier for non-technical owners to use than the early superhets. After manufacture each tuned amplifier stage had to be \"neutralized\", adjusted to cancel feedback; after this the set would not produce the parasitic oscillations which caused the objectionable noises. By 1927 some ten million of these receivers had been sold to consumers in North America.\n\nBy the 1930s, advances in vacuum tube manufacturing had yielded the tetrode, which had reduced control grid to plate (Miller) capacitance. These advances made it possible to build TRF receivers that did not need neutralization, but also made Edwin Armstrong's superheterodyne design practical for domestic receivers. So the TRF circuit, including the Neutrodyne, became obsolete in radio receivers and was superseded by the superheterodyne design.\n\nThe Neutrodyne neutralization technique continues to be used in other applications to suppress parasitic oscillation, such as in RF power amplifiers in radio transmitters.\n\n\n"}
{"id": "14190258", "url": "https://en.wikipedia.org/wiki?curid=14190258", "title": "Operational database", "text": "Operational database\n\nOperational database management systems (also referred to as OLTP On Line Transaction Processing databases), are used to update data in real-time. These types of databases allow users to do more than simply view archived data. Operational databases allow you to modify that data (add, change or delete data), doing it in real-time. OLTP databases provide transactions as main abstraction to guarantee data consistency that guarantee the so-called ACID properties. Basically, the consistency of the data is guaranteed in the case of failures and/or concurrent access to the data.\n\nSince the early 90's, the operational database software market has been largely taken over by SQL engines. Today, the operational DBMS market (formerly OLTP) is evolving dramatically, with new, innovative entrants and incumbents supporting the growing use of unstructured data and NoSQL DBMS engines, as well as XML databases and NewSQL databases. NoSQL databases typically have focused on scalability and have renounced to data consistency by not providing transactions as OLTP system do. Operational databases are increasingly supporting distributed database architecture that can leverage distribution to provide high availability and fault tolerance through replication and scale out ability.\n\nThe growing role of operational databases in the IT industry is moving fast from legacy databases to real-time operational databases capable to handle distributed web and mobile demand and to address Big data challenges. Recognizing this, Gartner started to publish the Magic Quadrant for Operational Database Management Systems in October 2013.\n\nNotable operational databases include:\n\nOperational databases are used to store, manage and track real-time business information. For example, a company might have an operational database used to track warehouse/stock quantities. As customers order products from an online web store, an operational database can be used to keep track of how many items have been sold and when the company will need to reorder stock. An operational database stores information about the activities of an organization, for example customer relationship management transactions or financial operations, in a computer database.\n\nOperational databases allow a business to enter, gather, and retrieve large quantities of specific information, such as company legal data, financial data, call data records, personal employee information, sales data, customer data, data on assets and many other information. An important feature of storing information in an operational database is the ability to share information across the company and over the Internet. Operational databases can be used to manage mission-critical business data, to monitor activities, to audit suspicious transactions, or to review the history of dealings with a particular customer. They can also be part of the actual process of making and fulfilling a purchase, for example in e-commerce.\n\nIn data warehousing, the term is even more specific: the operational database is the one which is accessed by an operational system (for example a customer-facing website or the application used by the customer service department) to carry out regular operations of an organization. Operational databases usually use an online transaction processing database which is optimized for faster transaction processing (create, read, update and delete operations). An operational database is the source for a data warehouse.\n\n\n"}
{"id": "1101814", "url": "https://en.wikipedia.org/wiki?curid=1101814", "title": "RONJA", "text": "RONJA\n\nRONJA (Reasonable Optical Near Joint Access) is a free-space optical communication system originating in the Czech Republic, developed by Karel Kulhavý of Twibright Labs and released in 2001. It transmits data wirelessly using beams of light. Ronja can be used to create a 10 Mbit/s full duplex Ethernet point-to-point link. It has been estimated that 1000 to 2000 links have been built worldwide \n\nThe range of the basic configuration is . The device consists of a receiver and transmitter pipe (optical head) mounted on a sturdy adjustable holder. Two coaxial cables are used to connect the rooftop installation with a protocol translator installed in the house near a computer or switch. The range can be extended to by doubling or tripling the transmitter pipe.\n\nBuilding instructions, blueprints, and schematics are published under the GNU Free Documentation Licence. Only free software tools are used in the development. The author calls this level of freedom \"User Controlled Technology\". Ronja is a project of Twibright Labs.\n\nThe building instructions are written with an inexperienced builder in mind. Basic operations like drilling, soldering etc., are explained. Several techniques – drilling templates, detailed checks after soldering, testing procedures – are employed to minimize errors at critical places and help to speed up work. Printed circuit boards are downloadable ready for manufacture, with instructions for the fabhouse. People with no previous experience with building electronics have reported on the mailing list that the device ran on the first try.\n\n154 installations worldwide have been registered into a gallery with technical data and pictures.\n\nWith the brightest variant of Lumileds HPWT-BD00-F4000 LED and 130 mm diameter cheap Chinese magnifying glass lenses, the range is 1.4 km. The less bright, but easier to buy E4000 variant of HPWT-BD00 yields 1.3 km. The speed is always 10 Mbit/s full duplex regardless of the distance.\n\n\nBy definition, clear visibility between the transmitter and receiver is essential. If the beam is obscured in any way, the link will stop working. Typically, problems may occur during conditions of snow or dense fog. One device weighs 15.5 kg and requires 70 hours of building time. It requires an ability to set full duplex manually on the network card or switch to take advantage of full duplex, since it doesn't support autonegotiation. Must be plugged directly into PC or switch using the integral 1 m Ethernet cable.\n\nA complete RONJA system is made up of 2 transceivers: 2 optical transmitters and 2 optical receivers. They are assembled individually or as a combination. The complete system layout is shown in the block diagram.\n\nThe usual approach in FSO (Free Space Optics) preamplifiers is to employ a transimpedance amplifier. A transimpedance amplifier is a very sensitive broadband high-speed device featuring a feedback loop. This fact means the layout is plagued with stability problems and special compensation of PIN diode capacitance must be performed, therefore this doesn't allow selection of a wide range of cheap PIN photodiodes with varying capacitances.\n\nRonja however uses a feedbackless design where the PIN has a high working electrical resistance (100 kilohms) which together with the total input capacitance (roughly 8 pF, 5 pF PIN and 3 pF input MOSFET cascode) makes the device operate with a passband on a 6 dB/oct slope of low pass formed by PIN working resistance and total input capacitance. The signal is then immediately amplified to remove the danger of contamination by signal noise, and then a compensation of the 6 dB/oct slope is done by derivator element on the programming pins of an NE592 video amplifier. A surprisingly flat characteristic is obtained. If the PIN diode is equipped with 3 kΩ working resistor to operate in flat band mode, the range is reduced to about 30% due to thermal noise from the 3 kΩ resistor.\n\nThe HSDL4220 infrared LED is originally unsuitable for 10 Mbit/s operation. It has a bandwidth of 9 MHz, where 10 Mbit/s Manchester-modulated systems need bandwidth of around 16 MHz. Operation in a usual circuit with current drive would lead to substantial signal corruption and range reduction. Therefore, Twibright Labs developed a special driving technique consisting of driving the LED directly with 15-fold 74AC04 gate output in parallel with RF voltage applied current-unlimited directly to the LED through large capacitors. As the voltage to keep the nominal LED average current (100mA) varies with temperature and component tolerances, an AC-bypassed current sense resistor is put in series with the LED. A feedback loop measures voltage on this resistor and keeps it at a preset level by varying supply voltage of the 74AC04 gates. Therefore, the nominally digital 74AC04 is operating as a structured power CMOS switch completely in analog mode.\n\nThis way the LED junction is flooded and cleared of carriers as quickly as possible, basically by short circuit discharge. This pushes the speed of the LED to maximum, which makes the output optical signal fast enough so that the range/power ratio is the same as with the faster red HPWT-BD00-F4000 LED. The side effects of this brutal driving\ntechnique are: 1) the LED overshoots at the beginning of longer (5 MHz/1 MHz) impulses to about 2x brightness. This was measured to have no adverse effect on range. 2) A blocking ceramic capacitor bank backing up the 74AC04 switching array is crucial for correct operation, because charging and discharging the LED is done by short circuit. Under dimensioning this bank causes the leading and trailing edges of the optical output to grow longer.\n\nRonja Twister is an electronic interface for free space optical datalink based on counter and shift register chips. It is a part of the Ronja design. It is effectively an optical Ethernet transceiver without the optical drive part.\n\nThe original design has been superseded with Twister2 but the logic circuit remained the same.\n\nOne doctoral dissertation, 10 master's and bachelor's theses,\n\nthree high school graduation exams\n\nand one high school vocational activity\n\n18 master's and bachelor's theses,\n\nand one university semester project cite Ronja. Czech Technical University has built a 120 m educational link between the buildings of two faculties for teaching the students practically about FSO. One Czech private university dedicates over a page of their textbook to Ronja.\n\nAn additional 12 journal, conference and university articles are based largely on Ronja.\n\nSoderberg, studying Ronja sociologically, writes: \"Arguably, the first project that vindicated the methods and licensing schemes of free software development, applied those practices to open hardware development, and pulled off a state-of-the-art technology without any backing from universities or firms, was the Ronja project.\"\n\nThe whole toolchain is built strictly upon free tools and the source files are provided, free, under the GPL. This allows anyone to enter the development, start manufacture or invest into the technology without entry costs. Such costs normally can include software licence costs, time investment into resolution of compatibility issues between proprietary applications, or costs of intellectual property licence negotiations. The decision to conceive the project this way was inspired by observed organizational efficiency of Free Software.\n\nOn Christmas 2001, Ronja became the world's first 10 Mbit/s Free Space Optics device with free sources.\n\nExamples of tools used in development:\n\n\n\n"}
{"id": "44330879", "url": "https://en.wikipedia.org/wiki?curid=44330879", "title": "Ria Money Transfer", "text": "Ria Money Transfer\n\nRia Money Transfer is a subsidiary of Euronet Worldwide, Inc. which specializes in money remittances. Ria initiates transfers through a network of agents and company-owned stores located throughout North America, Latin America, Europe, Asia-Pacific, Africa and online.\n\nThe company opened its first store in 1987 and has since grown to become the third largest money transfer service in the world. Ria currently serves customers in 144 countries through more than 314,000 locations worldwide.\n\nRia began working with post offices in 2010 and on July 31, 2014, partnered with Postbank, which is owned by the Kenyan Government, allowing their customers to send money through any of the bank’s 99 branches, or receive it directly into their accounts. The company continues to expand its postal partnerships to reach more underbanked and underserved customers in hard to reach places.\n\nIn 2014 the company launched \"Walmart-2-Walmart Powered by Ria\", a Walmart money transfer service within the US. The service allows customers to transfer money to and from more than 4,600 stores at competitive prices. In March 2015, \"Walmart-2-Walmart Powered by Ria\" received the \"Best Cash Innovation\" Gold Award at the PYMNTS.com Innovator Awards. In 2016, the company expanded its relationship with Walmart to Latin America and partnered with Walmart Chile to offer money transfer services through selected Líder brand supermarkets.\n\n"}
{"id": "2213571", "url": "https://en.wikipedia.org/wiki?curid=2213571", "title": "Royal Ordnance Factory", "text": "Royal Ordnance Factory\n\nRoyal Ordnance Factories (ROFs) was the collective name of the UK government's munitions factories in and after World War II. Until privatisation in 1987 they were the responsibility of the Ministry of Supply and later the Ministry of Defence.\n\nPrior to the 1930s Britain's ordnance manufacturing capability had been concentrated within the Royal Arsenal, Woolwich. In the late nineteenth century the term 'Royal Ordnance Factories' began to be used collectively of the manufacturing departments of the Arsenal (principally the Royal Laboratory, Royal Gun Factory and Royal Carriage Works) which, though they shared the same site, operated independently of one another. This use of the term is seen in the name of the Royal Ordnance Factories Football Club (founded 1893) and it continued through the First World War. The emerging threat of aerial bombing, however, prompted the government to consider dispersing its ordnance factories around the country.\n\nThe majority of the ROFs were built in the re-armament period just before the start of the 1939–45 World War to enhance the capacity of the three ordnance sites that had continued in operation after the end of World War I: the Royal Arsenal, Woolwich, the Royal Gunpowder Factory (RGPF) Waltham Abbey, Essex and the Royal Small Arms Factory, (RSAF) Enfield. These three sites were in or near London and were considered to be vulnerable to aerial bombing from continental Europe.\n\nThe Royal Arsenal designed many of the ROFs and was also the agent for the construction of all of the Rifles ROFs, the Medium Machine ROF and the Small Arms Ammunition ROFs. The Ministry of Supply, the Ministry of Works and two other private companies were agents for the construction of the remaining ROFs.\n\nA number of World War II munitions factories in the UK were built and owned by Imperial Chemical Industries (ICI). These ICI Nobel Explosives owned factories were not considered part of the Ministry of Supply's Royal Ordnance Factory organisation and they were not called ROFs. ICI also managed munitions factories constructed with Ministry of Supply funding. These were known as \"Agency Factories\" and three of them became part of Royal Ordnance upon the ROFs' privatisation.\n\nSome of the ROF Filling Factories built later during World War II were government-owned but managed, as Agency Factories, by private companies unconnected with the explosives industry. For example, Joseph Lyons & Co ran ROF Elstow throughout the war. Other Filling Factories were run by Imperial Tobacco Co Ltd, Courtaulds Ltd, The Co-operative Wholesale Society (CWS), Metal Closures Ltd and Lever Brothers.\n\nThe new ROFs were to be built in areas regarded as \"relatively safe\", which until 1940 meant from Bristol in the south and then west of a line that ran from (roughly) Weston-super-Mare in Somerset northwards to Haltwhistle, Northumberland; and then northwestwards to Linlithgow in Scotland. The South, South East and East of England were regarded as \"dangerous\" and the Midlands area, including Birmingham as \"unsafe\". This definition of \"safe\" area was later changed, and in 1940 ignored in the case of ROF Chorley.\n\nSiting of the individual ROFs north and west of this line was of vital importance. ROFs involved with explosive manufacture or filling needed, on safety grounds, to be located away from centres of population. However they needed access to good transport links, such as railways; the availability of adequate workers within reasonable travelling distance; a plentiful guaranteed supply of clean process water; and (to avoid the danger of frozen explosives) tended to be located at or just above sea Level. Some ROFs located in Wales and Scotland were the result of political lobbying as these areas had high unemployment rates in the 1930s. The ROFs were guarded by what was to become the Ministry of Defence Police Force.\n\nThe Royal Ordnance Factories were set up with six generic types of factories:\n\nThe three main types were: Engineering, Filling and Explosives.\n\nThe largest ROFs tended to be the Explosive ROFs and the Filling Factories as these needed an explosives safeguarding zone around the perimeter of the factory; as well as separation, or reduced separation and traverses, between buildings. ROF Bishopton occupied over and ROF Chorley was .\n\nEach ROF tended to be self-contained, apart from its raw materials: with their own coal-fired power stations, for generating steam for heating and process use, and electricity via high-pressure steam turbines if needed; engineering workshops; plumbers and chemical plumbers; leather workers; electricians; buildings and works departments; housing and hostels for workers; canteens; laundries and medical centres.\n\nThe UK's ROFs were set up and operated as production factories. The design of explosives, propellants and munitions was carried out at separate government-owned research and development establishments such as the Research Department, which was initially based at the Royal Arsenal, Woolwich and then Fort Halstead, in Sevenoaks, Kent; and at PERME Waltham Abbey, Essex, which later moved to become RARDE Fort Halstead.\n\nA number of the ROFs were designated \"temporary\", for use during the war's duration only. They closed shortly after the end of World War II. Other ROFs were designated \"permanent\" and they remained open into more recent times. In 1957, a Defence White paper led to a reorganisation of the aircraft industry, a restructuring of the British Army and a concentration on missile systems. A number of the \"permanent\" ROFs closed in the late 1950s, after the end of the Korean War, and others closed in the 1970s. The largest of these, based at the Royal Arsenal in Woolwich, formally closed on 31 March 1967.\n\nThe temporary ROFs, or ROFs which closed in the 1950s and 1970s, tended to be taken over by other Government Departments. Some closed ROFs and Admiralty explosive sites, such as the Royal Navy Propellant Factory, Caerwent, were retained by the Ministry of Defence as ammunition storage areas; others became Government Industrial Estates or Trading Estates; others were used as brown field sites to build Prisons or Open Prisons.\n\nPart of ROF Thorp Arch became the Boston Spa depository of the British Library. Three of the seven hostels that served ROF Swynnerton became a Training School for the General Post Office (GPO) Telephones, which later became British Telecom. Now called Yarnfield Park Training and Conference Centre and run by Accenture. ROF Elstow was taken over by the CEGB and became a storage depot. The site has been cleared; and, as of 2008, is in the process of becoming the new town of Wixams.\n\nIn, July 1974, the Royal Ordnance Factories were set up as a Trading Fund, under the Government's Trading Funds Act 1973.\n\nAs part of its privatisation process in the 1980s, the UK Government transferred some of the, formerly separate, research and development capability of the Defence Research Establishments into the ROFs. Other parts of the UK's defence research and design capability were later closed down; remained with the UK Ministry of Defence, as Dstl; or became part of QinetiQ.\n\nOn 2 January 1985 the majority of the Royal Ordnance Factories were vested in the UK Government-owned company Royal Ordnance Plc; it was bought by British Aerospace in 1987. The Ministry of Defence Police left most of the ROFs on or within a few years of privatisation.\n\nThe small number of ROFs involved in nuclear weapons production, ROF Burghfield and ROF Cardiff, were removed from ROF management and did not pass over to Royal Ordnance upon privatisation. They were transferred to the control of AWRE; which later became the Atomic Weapons Establishment.\n\n\n"}
{"id": "6605364", "url": "https://en.wikipedia.org/wiki?curid=6605364", "title": "School bus crossing arm", "text": "School bus crossing arm\n\nA school bus crossing arm is a safety device intended to protect children from being struck while crossing in front of a school bus. \nTypically, school bus crossing arms are wire or plastic devices which extend from the front bumper on the right side of the bus while it is stopped for loading/unloading and form a barrier. The devices force children, who need to cross the road, to stand several feet in front of the bus itself before they can begin to cross the road. This ensures that the bus driver can see them as they cross, avoiding a common blind spot immediately in front of the bus, closest to the bumper. The crossing arm retracts flush against the bumper while not activated, such as when the bus is in motion.\n\nUnlike traffic warning lights and many other safety-related features typically found on school buses in the United States, crossing arms are not required by the Federal Motor Vehicle Safety Standards (FMVSS) for School Buses. Regulations for equipment and use vary widely on a state-by-state basis. In some places, they are optional at the discretion of a local school district or school bus contractor.\n\nThe 1990 death of 6 year old Elizabeth \"Betsy\" Kyung Lee Anderson, in Washington state, led to the installation of school bus crossing arms, also referred to as \"Betsy Bars\" or \"Betsy Gates\" on all Washington state school busses by 1992. The crossing arms, when extended, require students to cross at least 5 feet in front of the bus.\n\nIn Manitoba, Canada, provincial school buses have been required to have an extendable safety arm mounted on the bus since a seven-year-old boy died in 1996 in St. Norbert, after getting off his school bus. \n\n\n\nhttp://community.seattletimes.nwsource.com/archive/?date=19921023&slug=1520209"}
{"id": "43512313", "url": "https://en.wikipedia.org/wiki?curid=43512313", "title": "Shoe hanger", "text": "Shoe hanger\n\nA shoe hanger, also called a shoe display hanger, is commonly used to hang and display footwear in retail stores for the purpose of space efficient storage and to present footwear to customers. Shoe hangers have secondary functions of providing support for footwear and for displaying key information, such as style and shoe size. Shoe hangers come in a variety of styles for different display purposes and footwear types. The most common styles are wing, hook and clip designs, which are made from plastic.\n\nWing shoe hangers have an elongated base that terminates at the upper end with a hook. Usually a flat disc appears just below the hook where information about the footwear is displayed. The bottom end of the base terminates into a U-shape. Each arm of the U acts as an element where a shoe can be fitted. These arms often extend and loop back down on each other to create a wing, which offers more support for the shoe. In U-shaped and wing designs, each shoe of the pair is hung adjacent to each other. This design is most commonly used for flat soled footwear such as trainers, sandals, slippers and pumps.\n\nHook shoe hangers are more basic in their design and resemble small clothes hangers. The arms are far shorter and have a hook at their end. This design is commonly used to hang flip-flops or sandals.\n\nClip shoe hangers are used to hand and display boots. The base is short and terminates at the upper end with a hook. Directly below the hook is a flat disc that acts as a display for information and also as a grip to open the spring clip. The clip displays boots side on as opposed to other shoe hanger designs that display footwear front on.\n\nOne of the earliest patented shoe hangers was used for the purpose of hanging shoes in public service areas such as school, where it was important to be able to store shoes in an organised manner. It was also important to prominently display shoes so that the wearer would have easy access to them. Like modern shoe hangers in retail stores, its secondary function was to support shoes. The main difference between this design and modern shoe hangers was that the soles of the shoes would lie flat against the wall. This early shoe hanger, which was made from wire, was patented in 1926 by Jesse S. Harding.\n\nAs retail space became increasingly expensive it became necessary to display shoes and other clothing items in a more space efficient way. The first modern shoe hanger was filed by the Japanese Tsuneji Matsubara in 1970 and consisted of a base with an element for placing a pair of shoes adjacent to each other. Various improvements where made over the next few decades to accommodate the storage and display of different types of shoes, as seen in the wing, hook and clip designs.\n\n\n"}
{"id": "2488867", "url": "https://en.wikipedia.org/wiki?curid=2488867", "title": "Shorepower", "text": "Shorepower\n\nShore power or shore supply is the provision of shoreside electrical power to a ship at berth while its main and auxiliary engines are shut down. While the term denotes shore as opposed to off-shore, it is sometimes applied to aircraft or land-based vehicles (such as campers, heavy trucks with sleeping compartments and tour buses), which may plug into grid power when parked for idle reduction.\n\nThe source for land-based power may be grid power from an electric utility company, but also possibly an external remote generator. These generators may be powered by diesel or renewable energy sources such as wind or solar.\n\nShore power saves consumption of fuel that would otherwise be used to power vessels while in port, and eliminates the air pollution associated with consumption of that fuel. A port city may have anti-idling laws that require ships to use shore power. Use of shore power may facilitate maintenance of the ship's engines and generators, and reduces noise.\n\n\"Cold ironing\" is specifically a shipping industry term that came into use when all ships had coal-fired engines. When a ship tied up at port, there was no need to continue to feed the fire and the iron engines would literally cool down, eventually going completely cold – hence the term \"cold ironing\". If commercial ships can use shore-supplied power for services such as cargo handling, pumping, ventilation and lighting while in port, they need not run their own diesel engines, reducing air pollution emissions.\n\nOn small private boats, the power supply used is usually the same as regular household electric power. Some small craft may not need electric power while docked, or may replace on-board generators (if any) with solar panels sufficient to run a small battery-operated system. Boats that are connected to shorepower only need sufficient battery capacity to last until the next shorepower connection.\n\nBatteries on a vessel may be used to power an inverter to produce AC, which can be used for appliances that require AC power.\n\nShore power, as it relates to the trucking industry, is commonly referred to as \"Truck Stop Electrification\" (TSE). The US Environmental Protection Agency estimates that trucks plugging in versus idling on diesel fuel could save as much as $3240 annually. There are currently 138 truck stops in the USA that offer on-board systems (also called Shore power) or off-board systems (also called single system electrification) for an hourly fee.\nAuxiliary power units offer another alternative to both idling and shore power for trucks.\n\nSimilar to shore power for ships, a ground power unit (GPU) may be used to supply electric power for an aircraft on the ground, to sustain interior lighting, ventilation and other requirements before starting of the main engines or the aircraft auxiliary power unit (APU). It is also used by aircraft with APUs if the airport authority does not permit the usage of APUs at its docks or if the carrier wishes to save on the use of jet fuel (which APUs use). This may be a self-contained engine-generator set, or it may convert commercial power to the voltage and frequency needed for the aircraft.\n\n"}
{"id": "57970448", "url": "https://en.wikipedia.org/wiki?curid=57970448", "title": "Sillage (perfume)", "text": "Sillage (perfume)\n\nSillage (\\si.jaʒ\\) in perfume refers to the aura or trail created by a perfume when it is worn on the skin. It comes from the word in French for \"wake\" and can best be described as how a fragrance diffuses around the wearer. Sillage or diffusion in fragrances can also be called the \"projection\" of a fragrance.\n\nCompounds such as Hedione (methyl dihydrojasmonate) can be used to enhance the diffusivity of a fragrance. Hedione is a synthetic relative of methyl jasmonate, a naturally occurring compound in floral scents such as jasmine, tuberose and magnolia. Methyl jasmonate is also found in many other plant parts and is considered to be a signalling molecule. It is considered one of the compounds responsible for the projection of the scent in living flowers and was first fully characterised in between 1957 and 1962 in jasmine absolute (0.8%) by the fragrance chemist Edouard Demole who was working at Firmenich.\n\nThe first commercially successful fragrance to utilise Hedione was Eau Sauvage by perfumer Edmond Roudnitska for Christian Dior , launched in 1966. The addition of Hedione to a classically hesperidic fragrance construction created a dewy lemony magnolia-jasmine dimension without being directly floral, and gave it a new type of projection and transparency not experienced before in this type of perfume. This is considered to be the beginning of a new trend in perfumery towards transparency and projection.\n\nCompounds such as damascones, Iso-E super, linalool and some of the synthetic musks such as cashmeran may also be added to fragrances to enhance their diffusion and sillage.\n\nA fragrance does not need to be a heavy one to have a large sillage; fragrances such as Giorgio Beverly Hills for women and Poison (Dior) are very good examples of this. Eau Sauvage and L'eau d'Issey (Jaques Cavallier for Issey Miyake) have a large sillage but may be considered much lighter examples of this.\n\nSillage in a perfume could also be considered to be how a fragrance is perceived by others around the wearer and is enhanced by motion, ambient temperature as well as the inherent qualities of the skin. According to an article by Mookerjee, a fragrance is perceived by the diffusion of individual fragrance molecules. The rate of diffusion of these molecules in a fragrance however appears to be independent of their molecular weights, boiling points, odour thresholds and odour value.\n\nOnce a fragrance is applied to the skin, the skin itself becomes a substrate to the scent. The inherent scent of the individual skin, moisturisation of the skin, the behaviour of the microbiome of the skin, and the temperature of the surface of the skin that the fragrance is applied to will affect the sillage or diffusion of a perfume applied to it.\n"}
{"id": "23406183", "url": "https://en.wikipedia.org/wiki?curid=23406183", "title": "Stanhope (optical bijou)", "text": "Stanhope (optical bijou)\n\nStanhopes or Stanho-scopes are optical devices that enable the viewing of microphotographs without using a microscope. They were invented by René Dagron in 1857. Dagron bypassed the need for an expensive microscope to view the microscopic photographs by attaching the microphotograph at the end of a modified Stanhope lens. He called the devices \"bijoux photo-microscopiques\" or \"microscopic photo-jewelry\". In 1862, Dagron displayed the devices at the Exhibition in London, where he got an \"Honourable Mention\" and presented them to Queen Victoria. In 1864 Dagron became famous when he produced a stanhope optical viewer which enabled the viewing of a microphotograph , (equivalent in size to the head of a pin), that included the portraits of 450 people.\n\nIn 1851 John Benjamin Dancer invented microphotographs using a collodion process and a microscope converted to a camera. This resulted in a microphotograph about in area. The main disadvantage of Dancer's method was that the viewing of the microphotographs required a microscope which was at the time an expensive instrument. In 1857 Dagron solved the problem by inventing a method of mounting the microphotographs at the end of a small cylindrical lens. Dagron modified the Stanhope lens by sectioning the normally biconvex Stanhope lens and introducing a planar section so that the plane was located at the focal length of the convex side of the cylindrical lens. This produced a plano-convex lens, where Dagron was able to mount the microscopic photograph on the flat side of the lens using Canada balsam as adhesive. This arrangement enabled the picture to be focused.\n\nThe Stanhope optical viewers were also mounted inside the bows of violins by French violin maker Jean-Baptiste Vuillaume, probably using Dagron's methods and equipment. The violin Stanhopes featured the portraits of famous people such as Paganini, Tourte and Stradivari.\n\nThe sectioned lens could magnify the microphotograph three hundred times, so that the viewing of the microphotographs no longer required a bulky and expensive microscope. The modified Stanhope lens was small enough to be mounted in all manner of miniature artifacts such as rings, ivory miniatures, wooden toys etc. Dagron also designed a special microphotographic camera which could produce 450 exposures approximately on a wet collodion plate. \n\nDagron's efforts met with great success. The viewers were first introduced to the general public at the 1859 International Fair in Paris.\n\nThe success of his viewers enabled Dagron to purpose-build a factory dedicated to their production. As of June 1859, Dagron's factory was manufacturing the stanhopes, mounted in jewellery and souvenirs. In August 1859 he exhibited them at the International Exhibition in Paris where they met with great success. In 1862 he had 150 employees and was manufacturing 12,000 units a day.\n\nIn 1860 Dagron obtained the patent for his viewers under the title \"Bijoux Photomicroscopiques\". Dagron also developed mail order marketing techniques for his viewers.\n\nIn 1862 Dagron published his book \"Cylindres photo-microscopiques, montés et non montés sur bijoux\".\n\nIn the early twentieth century Eugène Reymond took control of Dagron's stanhope lens factory in Gex, France. He was succeeded in the management of the factory by his son Roger. In 1972 the factory, run by Roger Remond, produced the last stanhope lens made by the traditional methods. In 1998, after Roger's death, the workshop was closed and its equipment dismantled and sold. Stanhope lenses are still manufactured to this day, but they are not produced according to Dagron's methodology.\n\nIn modern times, the most common stanhopes are usually gold or silver crosses with Christian prayers in the microphotograph.\n"}
{"id": "476088", "url": "https://en.wikipedia.org/wiki?curid=476088", "title": "Susan Kare", "text": "Susan Kare\n\nSusan Kare (born February 5, 1954) is an artist and graphic designer who created many of the interface elements and typefaces for the Apple Macintosh in the 1980s. She was also Creative Director (and one of the original employees) at NeXT, the company formed by Steve Jobs after he left Apple in 1985. She has worked for Microsoft and IBM, and, more recently, Pinterest and Facebook.\n\nKare was born in Ithaca, New York, and is the sister of aerospace engineer Jordin Kare.\nShe graduated from Harriton High School in 1971, graduated \"summa cum laude\" with a B.A. in Art from Mount Holyoke College in 1975, and received a Ph.D. from New York University in 1978. She next moved to San Francisco and worked for the Fine Arts Museums. She is married and has three sons.\n\nKare joined Apple Computer after receiving a call from high-school friend Andy Hertzfeld in the early 1980s. A member of the original Apple Macintosh design team, she worked at Apple starting in 1982 (Badge #3978). Kare was originally hired into the Macintosh software group to design user interface graphics and fonts; her business cards read \"HI Macintosh Artist\". Later, she was a Creative Director in Apple Creative Services working for the Director of that organization, Tom Suiter.\n\nShe is the designer of many typefaces, icons, and original marketing material for the original Macintosh operating system. Descendants of her groundbreaking work can still be seen in many computer graphics tools and accessories, especially icons such as the Lasso, the Grabber, and the Paint Bucket. These designs created the first visual language for Apple's new point-and-click computing.\n\nKare was an early pioneer of pixel art. Her most recognizable works from her time with Apple are the Chicago typeface (the most prominent user-interface typeface seen in classic Mac OS interfaces from System 1 in 1984, to Mac OS9 in 1999, as well as the typeface used in the first four generations of the Apple iPod interface); the Geneva typeface; the original monospace Monaco typeface; \"Clarus the Dogcow\"; the \"Happy Mac\" icon (the smiling computer that welcomed Mac users when starting their machines), and the Command key symbol on Apple keyboards.\n\nAfter leaving Apple, Kare joined NeXT as a designer, working with clients such as Microsoft and IBM. Her projects for Microsoft included the card deck for Windows 3.0's solitaire game, as well as numerous icons and design elements for Windows 3.0. Many of her icons, such as those for Notepad and various Control Panels, remained essentially unchanged by Microsoft until Windows XP. For IBM, she produced icons and design elements for OS/2; for Eazel she contributed iconography to the Nautilus file manager.\n\nIn 2003, she became a member of the advisory board of Glam Media (now Mode Media).\n\nBetween 2006 and 2010, she produced icons for the \"Gifts\" feature of Facebook. Initially, profits from gift sales were donated to the Susan G. Komen for the Cure foundation. After Valentine's Day 2007, the gift selection was modified to include new and limited edition gifts that did not necessarily pertain to Valentine's Day. One of the gift icons, titled \"Big Kiss\" is also featured in some versions of Mac OS X as a user account picture.\n\nIn 2007, she designed the identity, icons and website for Chumby Industries, Inc. and their internet-enabled alarm clock whose interface she also designed.\n\nSince 2008, The Museum of Modern Art store in New York City has carried stationery and notebooks featuring her designs. In 2015 MoMA also acquired her notebooks of sketches that led to the early Mac icons.\n\nIn August 2012, she was called as an expert witness by Apple in the company's patent-infringement trial against industry competitor Samsung (\"see Apple Inc. v. Samsung Electronics Co.\").\n\nIn 2015, Kare was hired by Pinterest as a product design lead. she heads a digital design practice in San Francisco and sells limited-edition, signed fine-art prints.\n\nIn recognition of her design work, Kare was awarded the American Institute of Graphic Arts medal in April 2018.\n\n"}
{"id": "6027168", "url": "https://en.wikipedia.org/wiki?curid=6027168", "title": "Telepsychiatry", "text": "Telepsychiatry\n\nTelepsychiatry is the application of telemedicine to the specialty field of psychiatry. The term typically describes the delivery of psychiatric assessment and care through telecommunications technology, usually videoconferencing. Telepsychiatry services can be offered through intermediary companies that partner with facilities to increase care capacities, or by individual providers or provider groups. Most commonly, telepsychiatry encounters take place at medical facilities under the supervision of onsite staff, though at-home models are becoming accepted as long as they are in compliance with HIPAA standards.\n\nOne of the drivers behind telepsychiatry's growth in the United States has been a national shortage of psychiatrists, particularly in specialty areas such as child and adolescent psychiatry; telepsychiatry can allow fewer doctors to serve more patients by improving utilization of the psychiatrist's time. Telepsychiatry can also make it easier for psychiatrists to treat patients in rural or under-served areas by eliminating the need for either party to travel. The most common means of insurance coverage for telehealth services among the United States is to incorporate coverage into the Medicare program. Reimbursement for Medicare-covered services must satisfy federal requirements of efficiency, economy and quality of care. Since 1999, Medicare and Medicaid reimbursement for all kinds of telehealth services have expanded, requirements of providers have been reduced, and grants have been given to support telehealth program adoption. For 2014, the Center for Medicare (CMS) services does cover telemedicine services, including telepsychiatry in many areas.\n\nTelepsychiatry includes a variety of sub-specialties based on different contexts of service delivery.\n\nPsychiatric treatment of patients who are at home or in another private setting is called home-based telepsychiatry or direct-to-consumer telepsychiatry, and it can require only a webcam and high-speed internet service. However, in order to avoid the risk of violating the patient-provider relationship, issues of security and possible HIPAA violations, providers who wish to practice in-home telepsychiatry are best served doing so from within a secure, HIPPA compliant online platform.\n\nLed by psychiatrist Jill Afrin, South Carolina Department of Health Deaf Services Program has used home-based telepsychiatry as a part of its services since the mid-1990s.\n\nIndividual psychiatrists are adopting this method more and more with willing, interested patients, and it is an especially useful tool for consumers with limited mobility included the elderly and the disabled. Unfortunately, home-based telepsychiatry is not typically reimbursed by private payors or Medicaid, though many states are adopting measures into their legislation in the form of parity laws that would allow for it to be reimbursed in the future.\n\nForensic telepsychiatry is the use of a remote psychiatrist or nurse practitioner for psychiatry in a prison or correctional facility, including psychiatric assessment, medication consultation, suicide watch, pre-parole evaluations and more. Telepsychiatry can deliver significant cost savings to correctional facilities by eliminating the need for prisoners to be escorted to off-site appointments and psychiatric interventions.\n\nAs of 2008, guidelines are being developed for the provision of telepsychiatric consultation for emergency psychiatric patients, such as the evaluation of suicidal, homicidal, violent, psychotic, depressed, manic, and acutely anxious patients. However, emergency telepsychiatry services are already being provided to hospital emergency departments, jails, community mental health centers, substance abuse treatment facilities, and schools. Emergency telepsychiatry can ease staff shortages in overworked hospital emergency departments and increase patient throughput and ED disposition. Rather than employ expensive, short-term locum tenens doctors or have emergency department physicians evaluate the psychiatric stability of their patients, hospitals can use telepsychiatry to decrease costs and increase patient access to behavioral health evaluations by psychiatric specialists.\n\nCrisis telepsychiatry is also an efficient means of reducing the need for psychiatric boarding. Psychiatric boarding is when a mentally ill resident is detained, often in a hospital emergency department, while waiting for proper psychiatric treatment. With the increased throughput offered by telepsychiatry, psychiatric consumers enjoy reduced wait times and faster access to care.\n\nMany facilities that offer behavioral health care are turning to telepsychiatry providers to allow for an increased care capacity. With routine telepsychiatry, a consistent provider or small group of providers serve a regular caseload of consumers in previously scheduled blocks of time. Remote providers can be consulted for medication management, treatment team meetings, supervision, or to offer traditional psychiatric assessment and consultations.\n\nHaving access to remote providers allows facilities, especially those in rural areas that struggle to recruit and maintain providers, access to a greater variety of speciality care to offer their consumers.\n\nFacilities that use routine telepsychiatry include:\n\nCommunity Mental Health Centers (CMHCs)\nOutpatient clinics\nFederally Qualified Health Centers (FQHCs)\nUniversities and Schools\nResidential Programs\nNursing Homes\nAccountable Care Organizations (ACOs)\nSubstance Use Treatment Centers\nMilitary Bases\n\nHIPAA (the Health Insurance Portability and Accountability Act) is a United States federal law that establishes security and privacy standards for electronic medical information exchange, including telemental health services. In order to comply with HIPAA guidelines, many providers develop their own specialized videoconferencing services, since common third-party consumer solutions do not include sufficient security and privacy safeguards. There are also a growing number of HIPAA-compliant technologies available for telepsychiatry.\n\nIndia's large population and relatively small number of psychiatrists makes telepsychiatric service a good option for expanding access to mental health care. Telepsychiatry in India is still a young industry, but it is gradually growing, led by institutes such as the Post Graduate Institute of Medical Education and Research in Chandigarh and the Schizophrenia Research Foundation in Chennai.\n\nThe NHS has been slow at recognising the merits of telepsychiatry, perhaps because the regional set up of the NHS makes the provision non-location based virtual service difficult for the bureaucracy of a state owned model to implement, though there are now some NHS Mental Health Trusts such as Oxford NHS Foundation Trust who are starting to see the cost benefits and to set up services. There is a UK based group of consultant psychiatrists, Psychiatry-UK LLP, working in a chamber model, providing a national telepsychiatry service. However, though they are registered with the Care Quality Commission as a qualified provider of medical services and can be found on the NHS Choices website, they are, as yet, only seeing patients who self fund or who have suitable health insurance.\n\n\n\n"}
{"id": "22924230", "url": "https://en.wikipedia.org/wiki?curid=22924230", "title": "Test compression", "text": "Test compression\n\nTest compression is a technique used to reduce the time and cost of testing integrated circuits. The first ICs were tested with test vectors created by hand. It proved very difficult to get good coverage of potential faults, so Design for testability (DFT) based on scan and automatic test pattern generation (ATPG) were developed to explicitly test each gate and path in a design. These techniques were very successful at creating high-quality vectors for manufacturing test, with excellent test coverage. However, as chips got bigger the ratio of logic to be tested per pin increased dramatically, and the volume of scan test data started causing a significant increase in test time, and required tester memory. This raised the cost of testing.\n\nTest compression was developed to help address this problem. When an ATPG tool generates a test for a fault, or a set of faults, only a small percentage of scan cells need to take specific values. The rest of the scan chain is \"don't care\", and are usually filled with random values. Loading and unloading these vectors is not a very efficient use of tester time. Test compression takes advantage of the small number of significant values to reduce test data and test time. In general, the idea is to modify the design to increase the number of internal scan chains, each of shorter length. These chains are then driven by an on-chip decompressor, usually designed to allow continuous flow decompression where the internal scan chains are loaded as the data is delivered to the decompressor. Many different decompression methods can be used. One common choice is a linear finite state machine, where the compressed stimuli are computed by solving linear equations corresponding to internal scan cells with specified positions in partially specified test patterns. Experimental results show that for industrial circuits with test vectors and responses with very low fill rates, ranging from 3% to 0.2%, the test compression based on this method often results in compression ratios of 30 to 500 times.\n\nWith a large number of test chains, not all the outputs can be sent to the output pins. Therefore, a test response compactor is also required, which must be inserted between the internal scan chain outputs and the tester scan channel outputs. The compactor must be synchronized with the data decompressor, and must be capable of handling unknown (X) states. (Even if the input is fully specified by the decompressor, these can result from false and multi-cycle paths, for example.) Another design criteria for the test result compressor is that it should give good diagnostic capabilities, not just a yes/no answer.\n\n\n"}
{"id": "12225475", "url": "https://en.wikipedia.org/wiki?curid=12225475", "title": "The Cloisters (Letchworth)", "text": "The Cloisters (Letchworth)\n\nThe Cloisters in Letchworth Garden City, Hertfordshire in the UK was built in 1905 as an open-air school dedicated to Psychology and where students were taught skills from the Arts and Crafts movement. After a period of neglect during World War II The Cloisters became the North Hertfordshire Masonic Centre in 1951.\n\nThe Cloisters was built by Quaker Miss Annie Jane Lawrence (16 April 1863–3 August 1953), the daughter of Alfred Lawrence (1826 - 1875), who, with his brother Frederick, owned 'Lawrence Brothers, Smiths and Founders', and his wife Mary Elizabeth (née Ridge, 1838-1903). Her grandfather William Lawrence (1789-1855) came from humble origins but went on to be elected Master of the Worshipful Company of Carpenters in 1848 and an Alderman of the City of London, while her uncle Sir James Lawrence was Lord Mayor of London in 1868. Her younger brother was the Labour politician Frederick Pethick-Lawrence, 1st Baron Pethick-Lawrence (1871-1961). In 1881 she was a pupil at the School for Ladies in Hove in Sussex, while in 1895 she visited Boston in the USA. \n\nAs a young woman she undertook social work in the slums of London, which urged her on to action. Lawrence decided to build a centre dedicated as a school of thought from which adults could go out and effect social change. She chose Letchworth because it was the world's First Garden City; here she leased three acres of land on which she built The Cloisters and Cloisters Lodge, intending that the former should be a School of Philosophy while she lived in the latter.\n\nThe design reputedly came to Miss Lawrence in a dream. The building originally consisted of a large half-oval 'open-air room' called the `Cloisters Garth' with an open colonnade to the south and large glazed bays to the north; this was flanked by two wings, one housing the kitchen and store rooms and the other the cubicles and dressing rooms for an oval open-air swimming pool. It was designed according to the principles of the Arts and Crafts movement, and was built using marbles and other materials from all over Europe.\n\nOriginally built as an open-air school dedicated to Psychology, with accommodation for 20 students, students were encouraged to study \"how thought affects action and what causes and produces thought.\" Through healthy outdoor living it was intended that the students would develop healthy minds. The building was also designed to hold lectures, conferences, drama and musical performances as well as organ recitals. Students were also taught skills from the Arts and Crafts movement.\n\nDesigned by architect William Harrison Cowlishaw, building commenced on the site in 1905, and the building opened on 17 January 1907, having cost the then huge sum of £20,000. Miss Lawrence built a house for herself, 'Cloisters Lodge', alongside.\n\nA small community developed at \"The Cloisters\" dedicated to Theosophy. It became the base for the \"Alpha Union\" set up by J. Bruce Wallace of the Brotherhood Church, who organised summer schools and residential courses there. Miss Lawrence, although somewhat deaf, was a great music lover. On one occasion she brought The London Concert Orchestra, made up of 40 unemployed musicians, to play at the Cloisters Garth. This was part of an ongoing series of concerts that she organised attended by audiences of 1,000, making organ recitals, band and choral concerts a regular part of the new town's cultural life. The last concert was given by the Brotherhood Orchestra in 1939 on the day the Second World War broke out.\n\nThe Cloisters was commandeered by the Army during the Second World War and suffered damage. By 1948 Miss Lawrence found it increasingly difficult to maintain the building and to repair the damage caused by the Army. She initially offered it to the local Council, who turned it down. Eventually, she offered it to the local Freemasons who accepted it, turning it into the North Hertfordshire Masonic Centre. About sixty Masonic lodges and 'side degrees' meet at The Cloisters, and the centre is run and maintained by an elected Board of Trustees, The Lawrence Cloisters Trust. A Craft Lodge, the 'Cloisters Lodge No 7100' was formed there in 1951 to commemorate Miss Lawrence's generous gift of the building to Hertfordshire Freemasons.\n\nMiss Lawrence eventually moved out of Cloisters Lodge into a house on Willian Way in Letchworth but her increasing frailty led to her moving to St Catherine's Nursing Home where she died aged 90 on 3 August 1953. She is buried in the churchyard of All Saints Church in Willian in Hertfordshire. In her will she left £41,246 2s 6d to George Ernest Hinman (Solicitor) and her brother the Rt. Hon. Frederick William Pethick-Lawrence. \n\nThe Cloisters has been designated by Letchworth Garden City Corporation as one of the 'great historic buildings' of North Hertfordshire. It is a Grade II* listed building. In June 2013 The Cloisters appeared in episode 4 of ITV's five-part series \"Britain's Secret Homes\", placing it in 16th place out of 50 historic buildings in Britain selected by English Heritage.\n\nAlthough structurally sound, in October 2013 the building was put on English Heritage's 'At Risk Register' owing mainly to the poor condition of its roof tiles which have allowed water to cause damage.\n\n\n"}
{"id": "43114888", "url": "https://en.wikipedia.org/wiki?curid=43114888", "title": "The Lacassane Company", "text": "The Lacassane Company\n\nThe Lacassane Company is a land management company, with a goal of sustainable land management using an environmental management scheme that involves a host of tools including holistic management. Located primarily in Jefferson Davis and Cameron parish, with property in Ragley, Louisiana, the company headquarters is in Lake Charles, Louisiana.\n\nFounded in 1929, by eight Lake Charles area businessmen, with land purchased from Jim Gardiner. The company was formed with 2250 common shares of stock with share-holders including, W. P. Weber, H. G. Chalkley, C. O. Noble, Henry Pomeroy, George M. King and Frank Roberts, M. J. Muller, and purchased 21,000 acres that included farm machinery, implements, stock, and cattle bought for $380,000.00, that included what was the Lowery and Illinois plantations, that became known as \"The Illinois Plant\", and \"The Lowery Plant\".\n\nThe Lacassane company continued with the previous form of tenant farming, increasing the original cattle herd, establishing trapping, hunting, oil and gas leases, and then the wetlands mitigation project. The Lacassine National Wildlife Refuge was established in 1937, when the company sold south of the Illinois Plant to the United States Government for $51,774.00.\n\nThe Illinois Plant is called the Lacassane Coastal Prairie Mitigation Bank and the Ragley property, in conjunction with the \"Calcasieu Mitigation Bank\" and partnered with Ecosystem Investment Partners (EIP), is known as the Bill Jackson Longleaf Savannah Mitigation Bank. Both have been designated (through The Lacassane Company) by the Corps of Engineers as a mitigation bank providing ecosystem services to the public in the form of Environmental mitigation (compensatory mitigation) to ensure the no net loss wetlands policy is followed to prevent Biodiversity loss that keeps the greenhouse debt in check. The Lacassane Company partnered with The Coastal Plain Conservancy to hold conservation servitudes on the land. The banks are monitored and maintained by Wildlands, Inc., an environmental consulting and plant propagation company.\n\nThe company operations now include land leases for waterfowl (Waterfowl Limited Liability Company) and other hunting, cattle grazing, alligator hide and egg harvesting, oil and gas exploration, and wetland projects. A pumping system through canals, laterals, the Bell City ditch, the Lacassine Bayou and the Mermentau River provides irrigation for the farming operations. The company's SIC code (Lessors of Real Property, NEC) is 6519 and the NAICS CODE (Lessors of Other Real Estate Property) is 531190.\n\nIn 2006, The Lacassane Company became the parent company of \"Louisiana Native Seed Company\", that provides ecospecies such as Little Bluestem, Brownseed Paspalum, Florida Paspalum, Switchgrass, Partridge Pea, and Eastern Gamagrass and has consultants with experience in Agricultural science and botany. The Louisiana Native Seed Company is listed as a provider for the United States Department of Agriculture's Natural Resources Conservation Service.\n\nThe Lacassane Club was founded in 2013, as a subsidiary of The Lacassane Company, on 14,000 of the 21,000 acres owned by the company. The company offers personal and corporate hunting memberships that include access to the lodge, separate sleeping quarters called casitas and a 3 bedroom, 2 bath house called Jed’s Cabin. The staff includes the club manager, a head guide, 4 other guides, an executive chef, and a Zoology/Wildlife Management biologist.\n\n"}
{"id": "26856232", "url": "https://en.wikipedia.org/wiki?curid=26856232", "title": "Thor washing machine", "text": "Thor washing machine\n\nThe Thor washing machine was the first electric clothes washer sold commercially in the United States. Produced by the Chicago-based Hurley Electric Laundry Equipment Company, the 1907 Thor is believed to be the first electrically powered washer ever manufactured, crediting Hurley as the inventor of the first automatic washing machine. Designed by Hurley engineer Alva J. Fisher, a patent for the new electric Thor was issued on August 9, 1910, three years after its initial invention.\n\nThe idea of an automatic washing machine had been around for many years but these were crude mechanical efforts that typically involved a manually operated crank or similar design. In many ways, the patent of the new Thor washer sounds modern, even today. The patent states that a \"perforated cylinder is rotatably mounted within the tub containing the wash water\". A series of blades lifted the clothes as the cylinder rotated. After 8 rotations in one direction, the machine would reverse rotation to \"prevent the cloths from wadding up into a compact mass\". Drive belts attached to a Westinghouse motor connected to three wheels of different sizes, which moved the drum during operation. The design also included a clutch, which allowed the machine to switch direction, and an emergency stop rod. The new Thor washer was mass marketed throughout the United States beginning in 1908.\n\nThere is dispute over who was the first inventor of the automatic washer. A company called Nineteen Hundred Washing Machine Company of Binghamton, NY claims to have produced the first electric washer in 1906; a year before Thor's release. Additionally, it has been stated in various articles on the Internet that a Ford Motor Company employee invented the electric washer in late 19th century or early 20th century. Since Ford was incorporated in 1903, it seems unlikely that the Ford story is valid. Regardless, Thor remains one of the first (if not the first) company to manufacture and sell an automatic washing machine on a large scale.\n\nThor invented the tilt-a-whirl system in which the agitator, typically in the shape of disk, tilted back and forth within the washer drum while simultaneously rotating. The early 1930s tilt-a-whirl design was the first agitator to move water in both a horizontal and vertical motion. The 1936 version of the Thor tilt-a-whirl incorporated sculpted hands embossed on the agitator. At the time, some Thor dealers painted the fingernails of the hands on demonstration machines.\n\nIn the 1940s, Thor introduced the Automagic hybrid washer/dishwasher. The top-loading machine included both a removable clothes washing drum and a dish-washing drum. The Automagic was widely marketed but disappeared from the marketplace soon after its introduction, as many consumers soured on the idea of washing dirty clothing and dishes in the same machine.\n\nThe Thor trademark was acquired in 2002 by Los Angeles–based Appliances International, a supplier of washer dryer combos and stacking washers and dryers. Soon after the brand acquisition, the company introduced a new line of laundry appliances under the Thor brand.\n\n"}
{"id": "53862941", "url": "https://en.wikipedia.org/wiki?curid=53862941", "title": "Urbanization in the German Empire", "text": "Urbanization in the German Empire\n\nBetween 1871 and 1910, the German Empire experienced a period of both large-scale industrialization and large-scale urbanization. As a result of this movement of people from rural areas to cities, living and working conditions were often poor, setting the scene for the social conflict within Germany that was to follow.\n\nBetween 1871 and 1910, the number of Germans who lived in urban areas significantly increased. While the percentage of Germans who lived in communities (localities) with less than 2,000 people decreased from 64% to 40% between 1871 and 1910, the percentage of Germans who lived in communities with 20,000 to 99,999 people increased from 8% to 13% and the percentage of Germans who lived in communities with 100,000 or more people increased from 5% to 21% during the same time period.\n\nHowever, this urbanization occurred unevenly throughout Germany. Specifically, Germany's urbanization and industrialization was focused on the north and west while Germany's south and east retained their predominantly rural character. For instance, two-thirds of the population of both East Prussia and Posen Province (in the eastern part of Germany) lived in communities of less than 2,000 people in 1910 while only two-fifths of all Germans did and while only one-fifth of the population in both the Rhineland and Westphalia (in the western part of Germany) did. Overall, the most robust urbanization in Germany during this time period occurred in the Rhineland, Westphalia, Saxony, and Brandenburg. Similarly, industrial development in Germany during this time focused on the Ruhr, Saxony, and Silesia. Out of the total population in these provinces, 47% of Brandenburg's population, 40% of Westphalia's population, and 50% of the Rhineland's population lived in communities of 20,000 people or more in 1910.\n\nBetween 1875 and 1910, German cities experienced a huge increase in their population. During this time, the number of German cities with more than 10,000 residents increased from 271 to 576. Similarly, during this time, the number of German cities with a population of more than 200,000 people increased from 3 to 23 during the same time period. In addition, the German cities of Duisburg, Essen, and Kiel all had their populations increase by five times or more during this time period. Specifically, Duisburg's population increased from 37,380 to 229,438, Essen's population increased from 54,790 to 294,653, and Kiel's population increased from 37,246 to 211,627 during this time period.\n\nAs a result of Germany's urbanization, Germany's working-class often had to deal with miserable working and living conditions. In turn, this set the stage for intense social conflict within Germany as well as led to the rise of the German Social Democratic Party--the largest socialist party in the world during this time—thanks to large-scale worker support for it.\n"}
{"id": "32743", "url": "https://en.wikipedia.org/wiki?curid=32743", "title": "Voltmeter", "text": "Voltmeter\n\nA voltmeter is an instrument used for measuring electrical potential difference between two points in an electric circuit. Analog voltmeters move a pointer across a scale in proportion to the voltage of the circuit; digital voltmeters give a numerical display of voltage by use of an analog to digital converter. \n\nA voltmeter in a circuit diagram is represented by the letter \"V\" in a circle. \n\nVoltmeters are made in a wide range of styles. Instruments permanently mounted in a panel are used to monitor generators or other fixed apparatus. Portable instruments, usually equipped to also measure current and resistance in the form of a multimeter, are standard test instruments used in electrical and electronics work. Any measurement that can be converted to a voltage can be displayed on a meter that is suitably calibrated; for example, pressure, temperature, flow or level in a chemical process plant.\n\nGeneral purpose analog voltmeters may have an accuracy of a few percent of full scale, and are used with voltages from a fraction of a volt to several thousand volts. Digital meters can be made with high accuracy, typically better than 1%. Specially calibrated test instruments have higher accuracies, with laboratory instruments capable of measuring to accuracies of a few parts per million. Meters using amplifiers can measure tiny voltages of microvolts or less.\n\nPart of the problem of making an accurate voltmeter is that of calibration to check its accuracy. In laboratories, the Weston Cell is used as a standard voltage for precision work. Precision voltage references are available based on electronic circuits.\n\nA moving coil galvanometer can be used as a voltmeter by inserting a resistor in series with the instrument. The galvanometer has a coil of fine wire suspended in a strong magnetic field. When an electric current is applied, the interaction of the magnetic field of the coil and of the stationary magnet creates a torque, tending to make the coil rotate. The torque is proportional to the current through the coil. The coil rotates, compressing a spring that opposes the rotation. The deflection of the coil is thus proportional to the current, which in turn is proportional to the applied voltage, which is indicated by a pointer on a scale. \n\nOne of the design objectives of the instrument is to disturb the circuit as little as possible and so the instrument should draw a minimum of current to operate. This is achieved by using a sensitive galvanometer in series with a high resistance, and then the entire instrument is connected in parallel with the circuit examined.\n\nThe sensitivity of such a meter can be expressed as \"ohms per volt\", the number of ohms resistance in the meter circuit divided by the full scale measured value. For example, a meter with a sensitivity of 1000 ohms per volt would draw 1 milliampere at full scale voltage; if the full scale was 200 volts, the resistance at the instrument's terminals would be ohms and at full scale the meter would draw 1 milliampere from the circuit under test. For multi-range instruments, the input resistance varies as the instrument is switched to different ranges.\n\nMoving-coil instruments with a permanent-magnet field respond only to direct current. Measurement of AC voltage requires a rectifier in the circuit so that the coil deflects in only one direction. Some moving-coil instruments are also made with the zero position in the middle of the scale instead of at one end; these are useful if the voltage reverses its polarity.\n\nVoltmeters operating on the electrostatic principle use the mutual repulsion between two charged plates to deflect a pointer attached to a spring. Meters of this type draw negligible current but are sensitive to voltages over about 100 volts and work with either alternating or direct current.\n\nThe sensitivity and input resistance of a voltmeter can be increased if the current required to deflect the meter pointer is supplied by an amplifier and power supply instead of by the circuit under test. The electronic amplifier between input and meter gives two benefits; a rugged moving coil instrument can be used, since its sensitivity need not be high, and the input resistance can be made high, reducing the current drawn from the circuit under test. Amplified voltmeters often have an input resistance of 1, 10, or 20 megohms which is independent of the range selected. A once-popular form of this instrument used a vacuum tube in the amplifier circuit and so was called the vacuum tube voltmeter, or VTVM. These were almost always powered by the local AC line current and so were not particularly portable. Today these circuits use a solid-state amplifier using field-effect transistors, hence FET-VM, and appear in handheld digital multimeters as well as in bench and laboratory instruments. These are now so ubiquitous that they have largely replaced non-amplified multimeters except in the least expensive price ranges.\n\nMost VTVMs and FET-VMs handle DC voltage, AC voltage, and resistance measurements; modern FET-VMs add current measurements and often other functions as well. A specialized form of the VTVM or FET-VM is the AC voltmeter. These instruments are optimized for measuring AC voltage. They have much wider bandwidth and better sensitivity than a typical multifunction device.\n\nA digital voltmeter (DVM) measures an unknown input voltage by converting the voltage to a digital value and then displays the voltage in numeric form. DVMs are usually designed around a special type of analog-to-digital converter called an integrating converter.\n\nDVM measurement accuracy is affected by many factors, including temperature, input impedance, and DVM power supply voltage variations. Less expensive DVMs often have input resistance on the order of 10 MΩ. Precision DVMs can have input resistances of 1 GΩ or higher for the lower voltage ranges (e.g. less than 20 V). To ensure that a DVM's accuracy is within the manufacturer's specified tolerances, it must be periodically calibrated against a voltage standard such as the Weston cell.\n\nThe first digital voltmeter was invented and produced by Andrew Kay of Non-Linear Systems (and later founder of Kaypro) in 1954.\n\n\n"}
{"id": "11481321", "url": "https://en.wikipedia.org/wiki?curid=11481321", "title": "William Corin", "text": "William Corin\n\nWilliam Corin (13 October 1867- 2 March 1929) was an English-born electrical engineer, who undertook some of the early design of the Snowy Mountains Hydro-Electricity Scheme in Australia.\n\nHe was born in Kent, England and was educated at King's College School and University College, London. He worked as a civil engineer after graduating until 1891 when he became an electrical engineer. He migrated to Tasmania in 1895.\n\nIn Australia his interest in electrical engineering took off. He designed the Duck Reach Power Station and made early surveys for the Great Lakes schemes. In 1907 he was made Chief Electrical Engineer to the New South Wales Department of Public Works. He was in charge of the generation of thermal electricity. In 1915 he commenced writing a series of reports on a Snowy River scheme. In 1920 Corin estimated the cost of the Snowy River scheme at £2 million and that it would produce 1,500GwH of power. He worked for both the British and French Governments on projects in Fiji and New Caledonia. Finding working in the Government uncongenial he resigned in 1923 to undertake private consulting.\n\nHe died of cancer in Sydney in 1929 and is buried in the Northern Suburbs cemetery.\n\nCorin Dam in the Australian Capital Territory is named after him.\n\n"}
{"id": "668628", "url": "https://en.wikipedia.org/wiki?curid=668628", "title": "Window covering", "text": "Window covering\n\nWindow coverings are material used to cover a window to manage sunlight, to provide additional weatherproofing, to ensure privacy or sometimes security, or for purely decorative purposes.\n\nWindow coverings, especially in the American market, are usually on the interior side of windows, but exterior solutions are also available.\n\nTypes of coverings include:\n\n"}
{"id": "13907939", "url": "https://en.wikipedia.org/wiki?curid=13907939", "title": "Z-push", "text": "Z-push\n\nZ-Push (presumably Z is for Zarafa) is a FOSS implementation of the Microsoft Exchange ActiveSync protocol which is used to synchronize email, personal contacts and other items between a central server and a mobile device. Note the difference between this protocol, and an earlier (technologically unrelated) protocol named Microsoft ActiveSync.\n\nZ-Push enables any PHP-based groupware package to become fully syncable with any ActiveSync-compliant device.\n\nCurrently, Z-Push includes four backends: the IMAP and the Maildir backend for e-mail synchronization, the vCard backend for contact synchronization and one for the \"Zarafa\" package which is sold by allowing full synchronization of E-mail, Calendar, Contacts and Tasks.\n\nThere is also a 3rd party project that implements a Zimbra Backend allowing Z-push to be used with a ZCS server (Including opensource edition).\n\nSince 2.3.0, released in July 2016, significant performance improvements have been achieved, as well as significantly lower memory usage. Connecting to Outlook 2013 and 2016 via EAS is also officially supported. With the optional Kopano Outlook Extension (available only for paid subscribers of Zarafa/Kopano), additional Outlook features are enabled such as Out of Office replies, Notes synchronisation, opening of shared and public folders and synchronisation of the Global Address Book. \n\nZ-Push is under active development with new releases approximately every month including bug fixes, improvements and new features.\n\nThe Z-Push protocol is HTTP based, and uses WBXML (WAP Binary XML) as a communication layer, which is used for bi-directional communication between the PDA/cellular phone and the Server.\n\nInside the protocol there is everything you expect from a synchronization protocol: the process of sending items from one side to the other, while keeping track of what has already been sent. The Z-Push hides the complexity of handling these protocol requests to the backend developer, who only needs to implement various standard functions, like getting a list of items, and getting the data for a specific item. All that is needed is a good understanding of the WBXML object definitions and fields, and a developer can quite easily get the items of any groupware solutions onto the PDA/cellular phone.\n\nThe Z-Push has various performance and usability-related features; for example, the entire architecture of the project is based on the idea that only one message should ever have to be in memory at one time, even when the server is sending hundreds of messages to a PDA. This may sound easy, but in most XML-based applications, the XML result data is built in-memory before being serialized to the network - exactly the opposite to what Z-Push does, as data is streamed to the client while it is read from the backend. This not only improves already restricted memory usage in PHP, it also makes the progress bar on the client more user-friendly, as data starts arriving as soon as the synchronization request is made. Z-Push has provided a streaming WBXML encoder and decoder to make this happen.\n\nWhen a backend supports it, Z-Push can also make use of advanced features which bring server load down even lower, for example reading message changes directly from a 'diff' source, instead of comparing all the messages with whatever was in there last time. So if the groupware backend can provide a list of changes on-the-fly, then Z-Push can use this information almost instantaneously. Zarafa provides an incremental synchronization backend for its own MAPI-based solution here through their PHP-MAPI extension, enabling extremely low-load synchronizations.\n\n"}
