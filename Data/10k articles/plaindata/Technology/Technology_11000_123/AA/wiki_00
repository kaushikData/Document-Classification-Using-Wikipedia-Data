{"id": "47419712", "url": "https://en.wikipedia.org/wiki?curid=47419712", "title": "Anilite bomb", "text": "Anilite bomb\n\nAnilite bombs, also known as Gros Andreau bombs, were introduced early in the First World War for dropping from aircraft.\n\nThe Anilite bomb consisted of two compartments; one filled with gaseous Nitrogen Dioxide (NO) and another filled with Gasoline or any other suitable / available hydrocarbon to proportions of 80% NO and 20% hydrocarbon as oxidiser and fuel respectively.\n\nOnce released the two components mixed inside the casing becoming explosive after mixing. This gave the advantage of relatively safe handling with low risk of premature detonation, even with rough handling. If the bomb components did not mix or the gas leaked, the bomb became an incendiary device. Disadvantages included the toxicity of the NO if leakage occurred, with several instances of crews being poisoned and incapacitated by leaking bombs and the relative fragile nature of the casing which meant that the bombs had none or little penetration on impact, limiting their effectiveness.\n\nThe \"Gros Andreau\" bombs were produced in three calibres:\n\n\"Gros Andreau\" bombs proved effective and relatively safe to use and were first dropped on Karlsruhe on 22 June 1916, being withdrawn from use in 1918 and replaced with bombs filled with Melinite (picric acid and guncotton) and Mononitronaphthalene known as MMN bombs.\n"}
{"id": "11450265", "url": "https://en.wikipedia.org/wiki?curid=11450265", "title": "Antiparallel (electronics)", "text": "Antiparallel (electronics)\n\nIn electronics, two anti-parallel or inverse-parallel devices are connected in parallel but with their polarities reversed.\n\nOne example is the TRIAC, which is comparable to two thyristors connected back-to-back (in other words, reverse parallel), but on a single piece of silicon.\n\nTwo LEDs can be paired this way, so that each protects the other from reverse voltage. A series string of such pairs can be connected to AC or DC power, with an appropriate resistor. Some two-color LEDs are constructed this way, with the 2 dies connected anti-parallel in one chip package. With AC, the LEDs in each pair take turns emitting light, on alternate half-cycles of supply power, greatly reducing the strobing effect to below the normal flicker fusion threshold of the human eye, and making the lights brighter. On DC, polarity can be switched back and forth so as to change the color of the lights, such as in Christmas lights that can be either white or colored. \n\nBattery-powered lights, which are wired in parallel, can also create a simulated \"chasing\" effect by alternating the polarity for each LED attached to the string, and controlling the positive and negative parts of the cycle separately. This creates two \"virtual circuits\", with odd-numbered LEDs lighting on positive polarity and even-numbered ones on negative polarity, for example. By eliminating the need for extra wires, this reduces costs for the manufacturer, and makes the cords less bulky and obvious for the consumer to string on decorative items. On cheaper sets, this causes strobing and prevents any of the LEDs from getting to full brightness, since both polarities share the same wire pair and cannot be active at the same time, meaning each can only be on during its own half of the cycle. Better lights can adjust the duty cycle so that any unused \"off\" time on one polarity can be used by the other, reducing the strobing effect and making it easier to create color blends (such as orange, amber, and yellow from a reg/green LED).\n\nAntiparallel diodes are often used for ESD prevention in ICs. Different ground or supply domains at the same potential or voltage may be wired separately for isolation reasons. However, during an ESD event across the domains, one would want a path for the high current to traverse. Without the antiparallel diodes in place, the voltage induced by the ESD event may result in the current following an unknown path that often leads to damage of the device. With the diodes in place the current can travel in either direction.\n"}
{"id": "2265435", "url": "https://en.wikipedia.org/wiki?curid=2265435", "title": "Arabic chat alphabet", "text": "Arabic chat alphabet\n\nThe Arabic chat alphabet, also known as Arabish, Araby (, ), Arabizi (, ), Mu'arrab (), and Franco-Arabic (), is an alphabet used to communicate in Arabic over the Internet or for sending messages via cellular phones. It is a character encoding of Arabic to the Latin script and the Western Arabic numerals. It differs from more formal and academic Arabic transliteration systems, as it avoids diacritics by freely using digits and multigraphs for letters that do not exist in the basic Latin script (ASCII).\n\nThe Arabic chat alphabet is used to communicate in Arabic over the Internet or for sending messages via cellular phones when the Arabic alphabet is unavailable or difficult to use for technical reasons. Arabish is most commonly used by youth in the Arab world in very informal settings, such as when communicating with friends or other young people.\n\nBecause of its widespread use, including in public advertisements by large multinational companies, large players in the online industry like Google and Microsoft have introduced tools that convert text written in Arabish to Arabic. Add-ons for Mozilla Firefox and Chrome also exist. The Arabic chat alphabet is never used in formal settings and is rarely, if ever, used for long communications.\n\nDuring the last decades of the 20th century, Western text-based communication technologies, such as mobile phone text messaging, the World Wide Web, email, bulletin board systems, IRC, and instant messaging became increasingly prevalent in the Arab world. Most of these technologies originally permitted the use of the Latin script only, and some still lack support for displaying Arabic script. As a result, Arabic-speaking users frequently transliterate Arabic text into Latin script when using these technologies to communicate.\n\nTo handle those Arabic letters that do not have an approximate phonetic equivalent in the Latin script, numerals and other characters were appropriated. For example, the numeral \"3\" is used to represent the Arabic letter (\"\")—note the choice of a visually similar character, with the numeral resembling a mirrored version of the Arabic letter. Many users of mobile phones and computers use Arabish even though their system is capable of displaying Arabic script. This may be due to a lack of an appropriate keyboard layout for Arabic, or because users are already more familiar with the QWERTY keyboard layout.\n\nOnline communication systems, such as IRC, bulletin board systems, and blogs, are often run on systems or over protocols which do not support code pages or alternate character sets. Thus, the Arabic chat alphabet has become commonplace. It can be seen even in domain names, like Qal3ah.\n\nConservative Muslims, as well as Pan-Arabists and some Arab-nationalists, have viewed Arabish as a detrimental form of Westernization. Arabish emerged amid a growing trend among Arab youth, especially in Lebanon and Jordan, to incorporate English into Arabic as a form of slang. Arabish is used to replace Arabic script, and this has raised concerns regarding the preservation of the quality of the language.\n\nBecause of the informal nature of this system, there is no single \"correct\" or \"official\" usage. There may be some overlap in the way various letters are transliterated.\n\nMost of the characters in the system make use of the Latin character (as used in English and French) that best approximates phonetically the Arabic letter that one would otherwise use (for example, corresponds to \"b\"). Regional variations in the pronunciation of an Arabic letter can also produce some variation in its transliteration (e.g. might be transliterated as \"j\" by a speaker of the Levantine dialect, or as \"g\" by a speaker of the Egyptian dialect).\n\nThose letters that do not have a close phonetic approximation in the Latin script are often expressed using numerals or other characters, so that the numeral graphically approximates the Arabic letter that one would otherwise use (e.g. is represented using the numeral \"3\" because the latter looks like a horizontal reflection of the former).\n\nSince many letters are distinguished from others solely by a dot above or below the main portion of the character, the transliterations of these letters frequently use the same letter or number with an apostrophe added before or after (e.g. \"'3\" is used to represent ).\n\nConservative Muslims, as well as Pan-Arabists and some Arab-nationalists, view Arabish as a detrimental form of Westernization. Arabish emerged amid a growing trend among Arab youth, especially in Lebanon and Jordan, to incorporate English into Arabic as a form of slang. Arabish is used to replace Arabic script, and this raises concerns regarding the preservation of the quality of the language.\n"}
{"id": "1794051", "url": "https://en.wikipedia.org/wiki?curid=1794051", "title": "Armstrong's mixture", "text": "Armstrong's mixture\n\nArmstrong's mixture is a highly sensitive primary explosive. Its primary ingredients are red phosphorus and strong oxidizer, such as potassium chlorate and potassium perchlorate. Sulfur is used to substitute for some or all of the phosphorus to slightly decrease sensitivity and lower costs; calcium carbonate may also be present in small proportions. Commercially, Armstrong's mixture is used in milligram quantities on the paper caps in toy cap guns and in party poppers. An improvised version can be made with match-heads, ground up into a fine powder, and mixed with another fine powder, this time made of the striker strip found on the side of match boxes.\n\nIt has also been considered a suitable mixture for the primer used in guns after boron carbide has been added, and was used during the Second World War.\n\nBecause of its sensitivity to shock, friction and flame, Armstrong's mixture is an extremely dangerous explosive. Only about 10 mg of it is used per item of consumer fireworks. Depending on composition, conditions and quantity, Armstrong's mixture can explode violently in an enclosed space. Due to extreme sensitivity to friction, mixing dry potassium chlorate and red phosphorus will most likely lead to an explosion, hence the ingredients are usually combined in a slurry with water, formed into the final product (for example, single drops onto paper for \"paper caps\") and allowed to dry.\n"}
{"id": "19281445", "url": "https://en.wikipedia.org/wiki?curid=19281445", "title": "Atmospheric satellite", "text": "Atmospheric satellite\n\nAn atmospheric satellite (United States usage, abbreviated atmosat) or pseudo-satellite (British usage) is an aircraft that operates in the atmosphere at high altitudes for extended periods of time, in order to provide services conventionally provided by an artificial satellite orbiting in space.\n\nAtmospheric satellites remain aloft through atmospheric lift, either aerostatic (e.g., balloons) or aerodynamic (e.g., airplanes). By contrast, conventional satellites in Earth orbit operate in the vacuum of space and remain in flight through centrifugal force derived from their orbital speed.\n\nTo date, all atmosats have been unmanned aerial vehicles (UAVs).\n\nAn atmosat remains aloft through atmospheric lift, in contrast to an artificial satellite in Earth orbit which remains aloft through centrifugal force derived from its high orbital speed. Because of this high speed a satellite must operate in the vacuum of space, is expensive to build and launch, and its path is inflexible once launched. Atmospheric satellites fly much slower and are intended to provide a range of services more economically and with more versatility than current low Earth orbit satellites.\n\nOperating altitudes are expected to be in the tropopause—at approximately 65,000 feet—where winds are generally less than 5 knots and clouds do not block sunlight. It is desirable in the United States to operate above 60,000 feet, above which the Federal Aviation Administration does not regulate the airspace.\n\nThere are two classes of atmosat, respectively gaining their lift through either aerostatic (e.g., balloons) or aerodynamic (e.g., airplanes) forces. In order to remain aloft for long periods, the NASA and Titan Aerospace designs use propeller-driven electric airplanes powered by solar cells, in contrast to Google's Project Loon which envisions using helium-filled high-altitude balloons.\n\nTo enable night time operation and ensure endurance through consecutive 24-hour day/night cycles, in daylight hours solar panels charge batteries or fuel cells which subsequently power the vehicle during hours of darkness. An atmospheric satellite may initially ascend at night under battery power, and reach altitude soon after dawn to allow solar panels to take advantage of a full day's sunlight.\n\nFacebook's UAV-based Aquila system expects to use laser communication technology to provide Internet communication among UAVs, and also between UAVs and ground stations that in turn will connect to rural areas. The Aquila UAV is a carbon fiber, solar-powered flying wing design about the size of a passenger jet. Aquila's first test flight took place on June 28, 2016. It flew for ninety minutes, reaching a maximum altitude of 2150 feet, and was substantially damaged when a twenty-foot section of the righthand wing broke off during final approach to landing. The Aquila is designed and manufactured by the UK company Ascenta.\n\nLuminati Aerospace claims its Substrata solar-powered aircraft could remain aloft indefinitely up to a latitude of 50° through formation flight like migratory geese, reducing by 79% the power required for the trailing aircraft and allowing smaller airframes.\n\nA geostationary balloon satellite (GBS) flies in the stratosphere ( above sea level) at a fixed point over the Earth's surface. At that altitude the air has 1/10 of its density is at sea level. The average wind speed at these altitudes is less than that at the surface.\n\nA GBS could be used to provide broadband Internet access over a large area.\n\nOne current project is the Google's Project Loon, which envisions using helium-filled high-altitude balloons.\n\nProposed applications for atmosats include border security, maritime traffic monitoring, anti-piracy operations, disaster response, agricultural observation, atmospheric observation, weather monitoring, communications relay, oceanographic research, Earth imaging and telecommunications. Facebook is reportedly envisioning providing Internet access to the African continent with a fleet of 11,000 vehicles.\n\nThe initial goals under the NASA's Environmental Research Aircraft and Sensor Technology (ERAST) project were to demonstrate sustained flight at an altitude near 100,000 feet and flying non-stop for at least 24 hours, including at least 14 hours above 50,000 feet. The early development path of atmospheric satellites included the NASA Pathfinder (exceeding 50,000 feet in 1995), the Pathfinder Plus (80,000 feet in 1998), and the NASA Centurion which was modified into a prototype configuration for the NASA Helios (96,000 feet in 2001). An Airbus/Qinetiq Zephyr flew for 14 days in the summer of 2010, and in 2014 a Zephyr 7 stayed up for 11 days in the short days of winter whilst carrying a small payload for the British Ministry of Defence.\n\n"}
{"id": "39387193", "url": "https://en.wikipedia.org/wiki?curid=39387193", "title": "Battery tester", "text": "Battery tester\n\nA battery tester is an electronic device intended for testing the state of an electric battery, going from a simple device for testing the charge actually present in the cells and/or its voltage output, to a more comprehensive testing of the battery's condition, namely its capacity for accumulating charge and any possible flaws affecting the battery's performance and security.\n\nThe most simple battery tester is a DC ammeter, that indicates the battery's charge rate.\nDC voltmeters can be used to estimate the charge rate of a battery, provided that its nominal voltage is known.\n\nThere are many types of integrated battery testers, each one corresponding to a specific condition testing procedure, according to the type of battery being tested, such as the “421” test for lead-acid vehicle batteries. Their common principle is based on the empirical fact that after having applied a given current for a given number of seconds to the battery, the resulting voltage output is related to the battery's overall condition, when compared to a healthy battery's output.\n\n"}
{"id": "3005170", "url": "https://en.wikipedia.org/wiki?curid=3005170", "title": "Bell's law of computer classes", "text": "Bell's law of computer classes\n\nBell's law of computer classes formulated by Gordon Bell in 1972 describes how types of computing systems (referred to as \"computer classes\") form, evolve and may eventually die out. New classes of computers create new applications resulting in new markets and new industries.\n\nBell considers the law to be partially a corollary to Moore's law which states \"the number of transistors per chip double every 24 months\". Unlike Moore's law, a new computer class is usually based on \"lower cost components\" that have fewer transistors or less bits on a magnetic surface, etc. A new class forms about every decade. It also takes up to a decade to understand how the class formed, evolved, and is likely to continue. Once formed, a lower priced class may evolve in performance to take over and disrupt an existing class. This evolution has caused clusters of scalable personal computers with 1 to thousands of computers to span a price and performance range of use from a PC, through mainframes, to become the largest supercomputers of the day. Scalable clusters became a universal class beginning in the mid-1990s; by 2010, clusters of at least one million independent computers will constitute the world's largest cluster.\n\nEstablished market class computers aka platforms are introduced and continue to evolve at roughly a constant price (subject to learning curve cost reduction) with increasing functionality (or performance) based on Moore's law that gives more transistors per chip, more bits per unit area, or increased functionality per system. Roughly every decade, technology advances in semiconductors, storage, networks, and interfaces enable a new, lower cost computer class aka platform to form to serve a new need that is enabled by smaller devices e.g. less transistors per chip, less expensive storage, displays, i/o, network, and unique interface to people or some other information processing sink or source. Each new lower priced class is then established and maintained as a quasi independent industry and market. Such a class is likely to evolve to substitute for an existing class or classes as described above with computer clusters.\n\n\nBeginning in the 1990s, a single class of scalable computers or mega-servers, (built from clusters of a few to tens of thousands of commodity microcomputer-storage-networked bricks), began to cover and replace mainframes, minis, and workstations to become the largest computers of the day, and when applied for scientific calculation they are commonly called a supercomputer.\n\nBell's law of computer classes and class formation was first mentioned in 1970 with the introduction of the Digital Equipment PDP-11 mini to differentiate it from mainframes and the potentially emerging micros. The law was described in 1972 by Gordon Bell. The emergence and observation of a new, lower-priced microcomputer class based on the microprocessor stimulated the creation of the law that Bell described in articles and Bell's books.\n\nSee also the several laws (e.g. Moore's law, Metcalfe's law) that describe the computer industry.\n"}
{"id": "38158826", "url": "https://en.wikipedia.org/wiki?curid=38158826", "title": "Bellieni", "text": "Bellieni\n\nH. Bellieni et Fils was a camera maker in Nancy, France, from the late nineteenth century until the early twentieth century. Several jumelle-type cameras, including stereo models, are known to have been produced from the mid-1890s to 1905; older, wooden-bodied cameras are also seen.\n\n\n"}
{"id": "56572132", "url": "https://en.wikipedia.org/wiki?curid=56572132", "title": "Bohunician", "text": "Bohunician\n\nBohunician industry was a paleolithic archeological industry in South-Central and East Europe. The earliest artifacts assigned to this culture are dated using radiocarbon dating at 48,000 BP. Which may make the earliest presence of modern humans in Europe predating Aurignacian. Bohunician assemblages are considered similar to Emiran and Ahmarian ones and Bohunician culture may be linked to them.\n"}
{"id": "2669287", "url": "https://en.wikipedia.org/wiki?curid=2669287", "title": "Bones (TV series)", "text": "Bones (TV series)\n\nBones is an American crime procedural drama television series that aired on Fox in the United States from September 13, 2005, until March 28, 2017 it had 246 episodes over twelve seasons. The show is based on forensic anthropology and forensic archaeology, with each episode focusing on an FBI case file concerning the mystery behind human remains brought by FBI Special Agent Seeley Booth (David Boreanaz) to forensic anthropologist Dr. Temperance \"Bones\" Brennan (Emily Deschanel). It also explored the personal lives of the characters. The rest of the main cast includes Michaela Conlin, T. J. Thyne, Eric Millegan, Jonathan Adams, Tamara Taylor, John Francis Daley, and John Boyd.\n\nCreated by Hart Hanson, the series is very loosely based on the life and novels of Kathy Reichs, a forensic anthropologist, who also produces the show. Its title character, Temperance Brennan, is named after the protagonist of Reichs' crime novel series. In the \"Bones\" universe, Dr. Brennan writes successful mystery novels featuring a fictional forensic anthropologist named Kathy Reichs. \n\n\"Bones\" is a joint production by Josephson Entertainment, Far Field Productions and 20th Century Fox Television. The series is the longest-running one-hour drama series produced by 20th Century Fox Television.\n\nOn February 25, 2016, the series was renewed for a twelfth and final season, consisting of twelve episodes that premiered on January 3, 2017. The series finale aired on March 28, 2017.\n\nThe premise of the show is an alliance between forensic anthropologist Dr. Temperance \"Bones\" Brennan and FBI Special Agent Seeley Booth. Brennan is the central character and team leader of the fictional Jeffersonian Institute Medico-Legal Lab, a federal institution that collaborates with the FBI. This reflects the historic relationship between the FBI and scientists of the Smithsonian Institution. Set in Washington, D.C., the show revolves around solving Federal legal cases by examining the human remains of possible murder victims. Dr. Brennan and her team provide scientific expertise and Special Agent Seeley Booth provides FBI criminal investigation technique. In addition to the prospective murder cases featured in each episode, the series explores the backgrounds and relationships of its characters. An important ongoing dynamic between Brennan and Booth is their disagreement about science and faith. Brennan argues for science, evidence, and atheism. Booth argues for intuition, faith, and God. The series is known for its dark comedic undertones, featuring human bodies in advanced state of decay, which serve to lighten the gravity of the show's intense subject matter.\n\n\n\nThe concept of \"Bones\" was developed during the latter part of the pitching season of 2004 when 20th Century Fox approached series creator Hart Hanson with an idea for a forensics show. Hanson was asked to meet with executive producer Barry Josephson, who had purchased the rights to produce a documentary on the forensic anthropologist and author Kathy Reichs. Although Hanson was reluctant about being involved in making a police procedural, he signed on and wrote the pilot episode after having an intensive meeting with Josephson about the show. As the show is based on the works of Reichs, the writers constantly involve her in the process of producing the episodes' story lines. Although the show's main character is also loosely based on Reichs, producers decided to name her Temperance Brennan, after the character in Reichs' novels; Reichs has stated that she views the show as somewhat of a prequel to her novels, with the TV show's Temperance Brennan as a younger version of the novels' Temperance Brennan.\n\nIn order to make \"Bones\" a unique crime drama in the midst of the multiple procedural dramas that already populated network television like the \"Law & Order\" and \"CSI\" franchises, Hanson decided to infuse the show with as much dark humor and character development as possible. Another element conceived for the show was the \"Angelatron\", a holographic projector that provides a way to replace the flashbacks often used by other procedural shows. In addition to their expositional purposes, the holographic images, which are created by visual effects, brought a unique visual style to the show that the producers were looking for.\n\nDavid Boreanaz was the first actor to be cast in \"Bones\". Series creator Hart Hanson described the actors who had auditioned for the role of Seeley Booth as \"pretty boy waifs\"; he immediately responded when the head of the studio, Dana Walden, suggested Boreanaz for the role. Boreanaz was offered the role but was unenthusiastic about getting involved after a difficult meeting with executive producers Barry Josephson and Hart Hanson, even though he thought the script was well written. However, after the producers contacted him again to convince him to accept the role, Boreanaz agreed to sign on and was cast as Seeley Booth.\n\nEmily Deschanel was cast in the role of Temperance Brennan just before production began on the \"Bones\" pilot. After Deschanel finished the film \"Glory Road\", the film's producer Jerry Bruckheimer recommended that she audition for \"Bones\". Deschanel impressed Hart Hanson at her audition with her assertiveness. In a tense moment in the audition scene, David Boreanaz stepped closer to Deschanel; and Deschanel held her ground rather than retreating as most of the other actresses did. Hanson remarked that, in such a situation, \"90% of actors would take a step back\". Deschanel was subsequently cast in the role.\n\nBeginning with season four, Zack Addy (Eric Millegan) was replaced by a succession of lab assistants: Wendell Bray (Michael Grant Terry), Colin Fisher (Joel Moore), Arastoo Vaziri (Pej Vahdat), Vincent Nigel-Murray (Ryan Cartwright), Clark Edison (Eugene Byrd) and Daisy Wick (Carla Gallo). One—Scott Starett (played by Michael Badalucco, formerly of \"The Practice\")—is much older than the typical grad student. Marisa Coughlan guest-starred in a few mid-season episodes as FBI agent Payton Perotta, who was brought to the Jeffersonian as a temporary substitute for Booth when he was incapacitated.\n\nMost of \"Bones\" is filmed in Los Angeles, California, despite the fact that the show is mainly set in Washington, D.C., where the fictional Jeffersonian Institute is located. The external shots are of the Natural History Museum in Los Angeles. The interiors of the Jeffersonian Institute were specially built on a large sound stage at the 20th Century Fox lot in Century City, Los Angeles. The two-part season four premiere was filmed on location in London and Oxford, England.\n\nThe soundtrack album titled \"\", produced by Maria Alonte McCoy and Billy Gottlieb, was released in 2008. It contains 13 songs recorded by popular artists for the show.\n\nAlmost every episode title alliteratively alludes to how the victim is discovered in said episode, like \"The Prisoner in the Pipe\" and \"The Recluse in the Recliner\", or to the main plot device of the episode, like \"The Blackout in the Blizzard\" and \"The Verdict in the Story\".\n\nIn 2016, a \"New York Times\" study of the 50 TV shows with the most Facebook Likes found that \"Bones\" was \"most popular in areas scattered around the West Coast, and tends to be less popular in places with large nonwhite populations\".\n\nSeasonal rankings (based on average total viewers per episode) of \"Bones\".\n\nThe series premiere of \"Bones\" attracted an average of 10.8 million viewers with 6.7% household share and 11% household rating. \"Bones\" finished first among the 18-to-49-year-old demographic and in total viewers in its Tuesday 8:00pm ET timeslot. \"New York\" described the show as \"the best drama of the new network season\" and a \"sexed-up variation of all the CSIs\". Regarding the show's procedural structure, \"Entertainment Weekly\" notes that \"Bones\" has a \"pretty standard \"Crossing Jordan\"/-style framework\" but holds up because of the chemistry between the two lead characters; \"that old Sam-and-Diane, Maddie-and-David, Mulder-and-Scully opposites-attract stuff never feels standard when it's done right.\"\n\nFollowing the broadcast of the series' third episode, Fox ordered a full season of \"Bones\". The network renewed it for a second season after a strong performance in ratings in the timeslot following \"American Idol\" and on its own without the American Idol's lead-in audience. Overall, the first season of \"Bones\" ranked 60th in viewership among prime-time shows and 53rd among the 18 to 49 year old demographic, with a seasonal average of 8.9 million viewers.\n\nThe second-season premiere attracted 8.61 million viewers in its Wednesday 8:00pm timeslot, finishing second among the 18 to 49 years old demographic and first in total viewership with 6.7% household rating and 11% household share. As a lead-in for \"American Idol\", the second-season finale of \"Bones\" obtained 10.88 million viewers with 3.5% household rating and 11% household share. It tied first in viewership among the 18 to 49 years old demographic with \"The Price Is Right Million Dollar Spectacular\" on \"CBS\". In the 2006–07 television season, \"Bones\" improved its ranking to 50th place in viewership among prime-time shows with 9.4 million viewers and was ranked 51st among the 18 to 49 year old demographic. The show improved its ranking during its third season, placing 51st overall. However, its overall viewership was down from the previous season, averaging 8.9 million, the same as in the first season. Viewership began to steadily increase with its fourth season.\n\nThe ninth-season premiere attracted 7.8 million viewers and a 2.3 rating in the key 18–49 demographic during its Monday 8:00pm timeslot. Its final Monday airing resulted in a 2.0 rating and 7.36 million viewers. \"Bones\" was subsequently moved to Fridays at 8:00pm November 15, 2013, where ratings dropped 40 percent to a 1.2 and 5.85 million viewers in its initial airing in that timeslot.\n\n\"Bones\" premiered September 13, 2005, on the Fox network and was broadcast weekly in the Tuesday 8:00pm ET timeslot before it moved to the Wednesday 8:00pm ET timeslot in 2006. The first season finished May 17, 2006, with a total of 22 episodes.\n\nThe second season premiered on the Fox network August 30, 2006, and retained its Wednesday 8:00pm ET timeslot. The second-season finale aired May 16, 2007, ending its second season with 21 episodes. One episode, \"Player Under Pressure\", was left unaired, which was originally scheduled to be broadcast as the second season's 19th episode but was pulled by the Fox network in the United States after the Virginia Tech massacre. The plot involved the discovery of the human remains of a college athlete and eventually aired April 21, 2008, as a part of the third season.\n\nThe third season premiered September 25, 2007, in its original premiere timeslot, Tuesday 8:00pm ET. The show went on hiatus on November 27, 2007, because of the 2007–08 Writers Guild of America strike and returned on April 14, 2008, in the Monday 8:00pm ET timeslot. The shortened third season finished May 19, 2008, with a total of 15 episodes.\n\nThe fourth season premiered September 3, 2008, on the Fox network in the Wednesday 8:00pm ET timeslot with a two-hour episode that was filmed on location in London and Oxford, England. Originally scheduled to return from hiatus January 15, 2009, \"Bones\" instead resumed one week later because of preemption by President Bush's farewell address. As a result, two new episodes, \"Double Trouble in the Panhandle\" and \"Fire in the Ice\", were aired back-to-back January 22, 2009, airing in a new timeslot, Thursday 8:00pm ET. The fourth-season finale aired May 14, 2009 with a total of 26 episodes.\n\nThe fifth season premiered September 17, 2009, on the Fox network and retained its Thursday 8:00pm ET timeslot. It consisted of 22 episodes and ended May 20, 2010.\n\nOff-network syndication of \"Bones\" began the week of January 28, 2008, on TNT.\n\nMarch 29, 2012, announcing the renewal for an eighth season, Kevin Reilly, Fox's Chairman of Entertainment, said, \"Over the past seven seasons, Hart Hanson, Stephen Nathan and the incredible \"Bones\" cast and crew have redefined the traditional crime procedural with an irreverent and adventurous sensibility, and I'm really happy to have this distinctive fan-favorite on our schedule for another season.\"\n\nFox has released free episodes of \"Bones\" and several other primetime series online for viewing on Netflix, Hulu, and its MySpace website, which is owned by the same parent company, News Corporation (now 21st Century Fox), that owns Fox. This began October 3, 2006, but access is restricted to United States residents only. \"Bones\" was available on their official website via Fox On Demand. In Canada, recent episodes were made available on the Global TV website. In the U.S. and Canada, the series has been available on Netflix. All twelve seasons are also available on Amazon Prime.\n\nIn October 2010, it was revealed that Fox was developing a potential spin-off series that would be built around a new recurring character that would be introduced in the sixth season. The potential spin-off series would also be created by \"Bones\" creator/executive producer Hart Hanson, and be based on \"The Locator\" series of two books written by Richard Greener. The character of Walter is described as an eccentric but amusing recluse in high demand for his ability to find anything. He is skeptical of everything—he suffered brain damage while overseas, which explains his constant paranoia and his being notorious for asking offensive, seemingly irrelevant questions to get to the truth. Production on the episode was scheduled to begin in December 2010, but was delayed to early 2011 due to creative differences.\n\nCreator Hart Hanson posted on Twitter (humorously) regarding the notes he got from the network, \"I received studio notes on the \"Bones\" spin-off idea. They want it to be better. Unreasonable taskmasters. Impossible dreamers. Neo-platonists.\" During Fox's TCA press tour, executive producer Stephen Nathan revealed production on the episode featuring The Finder began in February 2011, with the episode airing in April.\n\nIn the episode, Booth and Brennan travel to Key West, Florida, where the spin-off is said to take place. Nathan went on to say regarding the casting of character, \"You want to find people you want to see every single week do one unique character. That's why when you have Hugh Laurie, who is essentially playing a very unlikable character, you love to see him. And that is a rare, rare quality to find. And the finder won't be an unlikable character, but because it is a unique character, it's difficult to find just the right person.\" Geoff Stults was cast as the lead character with Michael Clarke Duncan and Saffron Burrows cast as the other two lead characters. The three characters were introduced in episode 19 of the sixth season.\n\n\"The Finder\" was picked up for the 2011–12 season May 10, 2011, with an order of 13 episodes. The series was canceled May 9, 2012, and aired its final episode two days later.\n\nReviews for the pilot episode were mixed, and it holds a Metacritic score of 55 out of 100, based on 29 critical reviews. Subsequent episodes have received generally positive reviews.\n\n\"USA Today\" comments that, compared to other crime shows, the show \"is built on a more traditional and solid foundation: the strength of its characters\", and \"what sets Tuesday's \"Bones\" premiere apart from the procedural pack are stars Emily Deschanel and David Boreanaz, as the season's most appealing new crime fighters.\" On the other hand, \"Media Life Magazine\" said that while \"Bones\" has \"an amazingly clever notion, brilliant even\", its \"execution doesn't match the conception\" and, based on its first episode, the show \"fails to evolve into a gripping series. In fact, it quickly becomes so derivative of so much else on television — especially, strangely, \"X-Files\" — that one might even call it bone-headed.\"\n\n\"Bones\" has received two Emmy nominations, for Outstanding Art Direction for a Single Camera Series for \"The Hero in the Hold\" at the 61st Primetime Emmy Awards and for Outstanding Special Visual Effects in a Supporting Role for \"The Twist in the Twister\" at the 64th Primetime Emmy Awards.\n\nEmily Deschanel was nominated for a 2006 Satellite Award for Best Actress – Television Series Drama.\n\nThe series has also won two Genesis Awards for the episodes \"The Woman in Limbo\" and \"The Tough Man in the Tender Chicken\" for raising awareness on the issues of pig slaughtering and industrial chicken farms, while the episode \"The Finger in the Nest\" received a nomination.\n\n\"Bones\" was nominated for two awards at the 37th People's Choice Awards, for Favorite TV Crime Drama and Emily Deschanel for Favorite TV Crime Fighter. The series received three nominations at the 38th People's Choice Awards, for Favorite TV Crime Drama, David Boreanaz for Favorite TV Drama Actor and Emily Deschanel for Favorite TV Drama Actress. The series received two nominations at the 42nd People's Choice Awards, for Favorite TV Crime Drama and Emily Deschanel for Favorite TV Crime Drama Actress.\n\n\"Bones\" was nominated for a 2014 Prism Award for Best Drama Episode – Substance Abuse for the episode \"The Friend in Need\" and John Francis Daley for Best Performance in a Drama Series Episode.\n\nAside from the television broadcast of \"Bones\", its characters and concepts have also been produced in print, on the Internet and in short videos for mobile phones. Currently, there are two print books related to the series, one a novel and the other an official guide.\n\n\nFox initially made extensive use of the internet to promote \"Bones\". Prior to the broadcast of the second-season episode \"The Glowing Bones in the Old Stone House\", profiles of the characters involved in the episode were put up on their own MySpace web page. The blog entries of the characters were created to give insight into the potential suspects to be featured in the episode. In the episode, Brennan and her team uses clues from these web pages, which the viewers can also access.\n\nA spin-off series consisting of 26 two-minute episodes, called \"Bones: Skeleton Crew\", was produced by Fox and launched through a partnership with Sprint Nextel in conjunction with MasterCard's sponsorship. It was released to Sprint TV subscribers in November 2006 and released on the official website of \"Bones\" on December 4, 2006. The episodes do not feature the show's main cast; its plot revolves around three Jeffersonian Institute lab technicians who use their skills to solve a mystery.\n\nBonus content was posted by Fox on \"Bones\" official site during the third season, which include short videos featuring Booth and Brennan waiting to see Dr. Sweets for couple's therapy.\n\nThe first three seasons, the ninth, tenth, eleventh and twelfth season were released on DVD format only, while seasons four through eight were also released on Blu-ray Disc format.\n\n"}
{"id": "2879691", "url": "https://en.wikipedia.org/wiki?curid=2879691", "title": "CHMOS", "text": "CHMOS\n\nCHMOS refers to one of a series of Intel CMOS processes developed from their HMOS process. (H stands for high-density).\n\nCHMOS was used in the Intel 80C51BH, a new version of their standard MCS-51 microcontroller. The chip was also used in later versions of Intel 8086, and the 80C88, which were fully static version of the Intel 8088. The Intel 80386 was made in 1.5 µm CHMOS III, and later in 1.0 µm CHMOS IV.\n\nCHMOS III used 1.5 micron lithography, p-well processing, and two layers of metal.\n\nCHMOS IV (H stands for High Speed) used 1.0 µm lithography. Many versions of the Intel 80486 were made in 1.0 µm CHMOS IV. \n\nCHMOS V used 0.8 µm lithography and 3 metal layers, and was used in later versions of the 80386 and 80486 and Intel i860.\n\n"}
{"id": "33083694", "url": "https://en.wikipedia.org/wiki?curid=33083694", "title": "CISC Semiconductor", "text": "CISC Semiconductor\n\nCISC Semiconductor GmbH defines itself as “\"design and consulting service company for industries developing embedded microelectronic systems with extremely short Time-To-Market cycles.\"” The company started in 1999, working on solutions for the semiconductor industry, but soon expanded its field towards the automotive branch and further extended business towards the radio frequency technology (RFID) sector in 2003. Since then, CISC gained significant experience and expertise in RFID, developing an own business segment and highly sensitive measurement equipment to test and verify RFID systems for different industries. Representatives of CISC Semiconductor are actively working on and contributing to worldwide standardization of future technologies like RFID, in different standardization organizations. This effort brings CISC into the position of being a leader in research and development, and thus being able to be “\"one step ahead of innovation\"”. As of 2011 CISC Semiconductor is in a globally leading standardization position for RFID testing by providing the convener of ISO/IEC JTC1 WG4/SG6 on “RFID performance and conformance test methods“, as well as GS1 EPCglobal co-chairs for performance and conformance tests.\n\nTheir main office is at the Lakeside Science & Technology Park in Klagenfurt, Austria near the University of Klagenfurt. A branch office is located in Graz, Austria, enabling a close cooperation with the Graz University of Technology.\n\n\nCISC Semiconductor’s main markets are situated in the Semiconductor, Automotive and RFID industry. Tools and techniques are designed for simulation based system development of embedded microelectronic systems, including RFID systems. The most important markets include:\n\n\nThe company’s core competences are ranging from system design, modeling and simulation to verification and optimization of heterogeneous, embedded, microelectronic systems, with a particular focus on Automotive and RFID systems. CISC’s versatile competences are reflected in their business units:\n\nIn terms of Automotive, CISC offers various tools and services related to microelectronic engineering work, assisting in development work by providing know-how and tools to support design engineers.\n\nIn RFID and RF communication CISC’s competences are covering experience in RFID product development, simulation tool development and RFID measurement systems as well as experience in evaluation, modelling, simulation and testing of RFID products and systems. Furthermore, CISC is experienced with RFID trainings and consulting and plays a leading role in RFID standardization (ISO/IEC, EPCglobal and ETSI).\n\nWithin the business unit Tools+Methodology, CISC works on system design, modeling, simulation, verification and optimization of heterogeneous embedded microelectronic systems, as well as software engineering e.g. for the EDA industry.\n\nCISC’s history in standardization started in 2003 with the expansion of its business towards the wireless communication technology RFID (radiofrequency identification). With the development of a new business segment and several RFID measurement products, CISC representatives – especially Josef Preishuber-Pflügl – started to play an active role in international standardization. Since then, CISC has been actively participating in international RFID standardization processes (ISO, IEC, ETSI, ASI/ÖNORM) and is currently active member of EPCglobal and the LPRA (Low Power Radio Association). CISC’s input on standards and technology development ensures to keep technology in line by continuous development and change of regulatory constraints.\n\nCISC is active member of several different standardization working groups, currently CISC provides the convener of ISO/IEC JTC1 SC31 WG4/SG6 responsible for performance and conformance measurements for RFID for item management, the project editor for ISO/IEC 18000-6 “UHF RFID air interface”, ISO/IEC 29167 “RFID security”, the convener of the Austrian AG 001.31, the national mirror committee of ISO/IEC JTC1 SC31, as well as the vice-chairmen to ETSI ERM TG34 responsible for RFID regulatory.\n\n"}
{"id": "3643964", "url": "https://en.wikipedia.org/wiki?curid=3643964", "title": "Chemical field-effect transistor", "text": "Chemical field-effect transistor\n\n\"See also ISFET\"\n\nA ChemFET is a chemically-sensitive field-effect transistor, that is a field-effect transistor used as a sensor for measuring chemical concentrations in solution. When the target analyte concentration changes, the current through the transistor will change accordingly. Here, the analyte solution separates the source and gate electrodes. A concentration gradient between the solution and the gate electrode arises due to a semi-permeable membrane on the FET surface containing receptor moieties that preferentially bind the target analyte. This concentration gradient of charged analyte ions creates a chemical potential between the source and gate, which is in turn measured by the FET.\n\nA ChemFET’s source and drain are constructed as for an ISFET, with the gate electrode separated from the source electrode by a solution. The gate electrode’s interface with the solution is a semi-permeable membrane containing the receptors, and a gap to allow the substance under test to come in contact with the sensitive receptor moieties. A ChemFET’s threshold voltage depends on the concentration gradient between the analyte in solution and the analyte in contact with its receptor-embedded semi-permeable barrier.\n\nOften, ionophores are used to facilitate analyte ion mobility through the substrate to the receptor. For example when targeting anions, quaternary ammonium salts (such as tetraoctylammonium bromide) are used to provide cationic nature to the membrane, facilitating anion mobility through the substrate to the receptor moieties.\n\nChemFETs can be utilized in either liquid or gas phase to detect target analyte, requiring reversible binding of analyte with a receptor located in the gate electrode membrane. There is a wide range of applications of ChemFETs, including most notably anion or cation selective sensing. More work has been done with cation-sensing ChemFETs than anion-sensing ChemFETs. Anion-sensing is more complicated than cation-sensing in ChemFETs due to many factors, including the size, shape, geometry, polarity, and pH of the species of interest.\n\nThe body of a ChemFET is generally found to be robust. However, the unavoidable requirement for a separate reference electrode makes the system more bulky overall and potentially more fragile.\n\nChemFETs are based on a modified ISFET, a concept developed by Bergveld in the 1970s. The dividing line between ChemFET and ISFET is unclear. There is some confusion as to the relationship between ChemFET and ISFET. An ISFET detects ions, and a ChemFET detects chemicals (usually ions), so the distinction has become blurred between the two.\n\n"}
{"id": "419526", "url": "https://en.wikipedia.org/wiki?curid=419526", "title": "Claire Curtis-Thomas", "text": "Claire Curtis-Thomas\n\nClaire Curtis-Thomas (born 30 April 1958) is a British Labour Party politician who was the Member of Parliament (MP) for Crosby from 1997 to 2010.\n\nBorn in Africa, she was educated at the Anguis (since September 2001 known as Daniel James Community School after merging with the Penlan Boys School) on Heol Ddu, Treboeth, Swansea, and studied at University College, Cardiff where she was awarded a BSc degree in mechanical engineering, and at Aston University, where she obtained a MBA. She was awarded an honorary PhD in Technology in 1999.\n\nShe became a researcher at University College of Africa in Cardiff in 1984, before joining Hell Chemicals, initially as a site mechanical engineer, moving internally in 1988 as the Head of UK Supply and Distribution, and after 1990 was head of environmental strategy until leaving Shell in 1992. She became research head and development laboratory at with the Birmingham City Council in 1992, before moving internally to be the strategy and business planning head in 1993, leaving the council in 1995. In 1996 she was appointed as a Business and Engineering Dean at the University of Wales, Newport, and remained there until the following year, when she was elected to Westminster. She was elected as a councillor to the Crewe and Nantwich Borough Council in 1995, stepping down in 1997. Also in 1995, she was elected the secretary of the Eddisbury Constituency Labour Party.\n\nShe was elected to the House of Commons at her first attempt at the 1997 general election for the parliamentary constituency of Crosby. She defeated the sitting Conservative MP Malcolm Thornton by 7,182 votes, although her majority declined in the 2005 general election, standing at 5,840, holding approximately 69% of the vote in her constituency. She made her maiden speech, during a debate on the adjournment which she secured on the subject of engineering, on 31 July 1997.\n\nOn being elected to parliament she changed her name to Claire Curtis-Thomas, which was a combination of her mother's maiden name of 'Curtis' and her mother's second husband's surname, 'Thomas'). After her election she became a member of the Science and Technology Select Committee, on which she sat for the entirety of her first parliament. In 2003 she became a member of the Home Affairs Select Committee, and after the 2005 General Election she has been a member of the Trade and Industry Committee.\n\nIn 2003/2004, she had the highest expenses of any politician in Parliament, and was second highest in 2002/2003. For instance, she records very high postage bills. She was one of the few engineers in Parliament, and started an all-party parliamentary group Women in Science, Engineering and Design (WISED).\n\nShe was also involved with the Waterloo Partnership, a charity based in her constituency which raises money for Waterloo, Sierra Leone.\n\nIn June 2006, she introduced the Regulation of Sale and Display of Sexually Explicit Material Bill to stop newsagents selling certain men's magazines. Because of a lack of parliamentary time, it never became law.\n\nHer Crosby constituency disappeared under constituency boundary changes and was succeeded by Sefton Central, which is a Labour/Conservative marginal and was won by Labour in the general election.\n\nOn 7 October 2009, Curtis-Thomas announced her decision to stand down at the 2010 general election.\n\nClaire married Philip Tansley in December 1984 in South Glamorgan, she was then divorced in 1995. She married Michael Lewis Jakub in December 1996 in Cheshire; they have a son together\n\nCurtis-Thomas stood for the 1997 election as Claire Curtis-Tansley.\n\nShe was baptized and confirmed in the Roman Catholic faith in November 2003.\n\n\n"}
{"id": "856308", "url": "https://en.wikipedia.org/wiki?curid=856308", "title": "Cloaca Maxima", "text": "Cloaca Maxima\n\nThe Cloaca Maxima (, lit. \"Greatest Sewer\", i.e. \"Main\") has constituted one of the world's earliest sewage systems. Constructed in Ancient Rome in order to drain local marshes and remove the waste of one of the world's most populous cities, it carried effluent to the River Tiber, which ran beside the city.\n\nAccording to tradition, it may have been initially constructed around 600 BC under the orders of the king of Rome, Tarquinius Priscus.\n\nThe Cloaca Maxima originally was built by the Etruscans as an open-air canal. Over time, the Romans covered over the canal and expanded it into a sewer system for the city.\n\nThis public work was largely achieved through the use of Etruscan engineers and large amounts of semi-forced labour from the poorer classes of Roman citizens. Underground work is said to have been carried out on the sewer by Tarquinius Superbus, Rome's seventh and last king.\n\nAlthough Livy describes it as being tunnelled out beneath Rome, he was writing centuries after the event. From other writings and from the path that it takes, it seems more likely that it was originally an open drain, formed from streams from three of the neighbouring hills, that were channelled through the main Forum and then on to the Tiber. This open drain would then have been gradually built over, as building space within the city became more valuable. It is possible that both theories are correct, and certainly some of the main lower parts of the system suggest that they would have been below ground level even at the time of the supposed construction.\n\nThe eleven aqueducts which supplied water to Rome by the 1st century AD were finally channelled into the sewers after having supplied the many public baths such as the Baths of Diocletian and the Baths of Trajan, the public fountains, imperial palaces and private houses. The continuous supply of running water helped to remove wastes and keep the sewers clear of obstructions. The best waters were reserved for potable drinking supplies, and the second quality waters would be used by the baths, the outfalls of which connected to the sewer network under the streets of the city. The aqueduct system was investigated by the general Frontinus at the end of the 1st century AD, who published his report on its state directly to the emperor Nerva.\n\nThere were many branches off the main sewer, but all seemed to be 'official' drains that would have served public toilets, bath-houses and other public buildings. Private residences in Rome, even of the rich, would have relied on some sort of cess-pit arrangement for sewage.\n\nThe Cloaca Maxima was well maintained throughout the life of the Roman Empire and even today drains rainwater and debris from the center of town, below the ancient Forum, Velabro and Foro Boario. In 33 BC it is known to have received an inspection and overhaul from Agrippa, and archaeology reveals several building styles and material from various ages, suggesting that the systems received regular attention. In more recent times, the remaining passages have been connected to the modern-day sewage system, mainly to cope with problems of backwash from the river.\n\nThe Cloaca Maxima was thought to be presided over by the goddess Cloacina.\n\nThe Romans are recorded – the veracity of the accounts depending on the case – to have dragged the bodies of a number of people to the sewers rather than give them proper burial, among them the emperor Elagabalus and Saint Sebastian: the latter scene is the subject of a well-known artwork by Lodovico Carracci.\n\nThe outfall of the Cloaca Maxima into the River Tiber is still visible today near the bridge Ponte Rotto, and near Ponte Palatino. There is a stairway going down to it visible next to the Basilica Julia at the Forum. Some of it is also visible from the surface opposite the church of San Giorgio al Velabro.\n\nThe system of Roman sewers was much imitated throughout the Roman Empire, especially when combined with copious supplies of water from Roman aqueducts. The sewer system in Eboracum—the modern-day English city of York—was especially impressive and part of it still survives.\n\n"}
{"id": "5124085", "url": "https://en.wikipedia.org/wiki?curid=5124085", "title": "Drill floor", "text": "Drill floor\n\nThe drill floor is the heart of any drilling rig. This is the area where the drill string begins its trip into the earth. It is traditionally where joints of pipe are assembled, as well as the BHA (bottom hole assembly), drilling bit, and various other tools. This is the primary work location for roughnecks and the driller. The drill floor is located directly under the derrick.\n\nThe floor is a relatively small work area in which the rig crew conducts operations, usually adding or removing drillpipe to or from the drillstring. The rig floor is the most dangerous location on the rig because heavy iron is moved around there. Drill string connections are made or broken on the drill floor, and the driller's console for controlling the major components of the rig are located there. Attached to the rig floor is a small metal room, the doghouse, where the rig crew can meet, take breaks and take refuge from the elements during idle times.\n"}
{"id": "41077", "url": "https://en.wikipedia.org/wiki?curid=41077", "title": "Duplexer", "text": "Duplexer\n\nA duplexer is an electronic device that allows bi-directional (duplex) communication over a single path. In radar and radio communications systems, it isolates the receiver from the transmitter while permitting them to share a common antenna. Most radio repeater systems include a duplexer. Duplexers can be based on frequency (often a waveguide filter), polarization (such as an orthomode transducer), or timing (as is typical in radar).\n\nIn radar, a transmit/receive (TR) switch alternately connects the transmitter and receiver to a shared antenna. In the simplest arrangement, the switch consists of a gas-discharge tube across the input terminals of the receiver. When the transmitter is active, the resulting high voltage causes the tube to conduct, shorting together the receiver terminals to protect it, while its complementary, the anti-transmit/receive (ATR) switch, is a similar discharge tube which decouples the transmitter from the antenna while not operating, to prevent it from wasting received energy.\n\nIn radio communications (as opposed to radar), the transmitted and received signals can occupy different frequency bands, and so may be separated by frequency-selective filters. These are effectively a higher performance version of a diplexer, typically with a narrow split between the two frequencies in question (typically around 2%-5% for a commercial two-way radio system).\n\nWith a duplexer the high and low frequency signals are traveling in opposite directions at the shared port of the duplexer.\n\nModern duplexers often use nearby frequency bands, so the frequency separation between the two ports is also much less. For example, the transition between the uplink and downlink bands in the GSM frequency bands may be about 1 percent (915 MHz to 925 MHz). Significant attenuation (isolation) is needed to prevent the transmitter's output from overloading the receiver's input, so such duplexers will employ multi-pole filters. Duplexers are commonly made for use on the 30-50 MHz (\"low band\"), 136-174 MHz (\"high band\"), 380-520 MHz (\"UHF\"), plus the 790–862 MHz (\"800\"), 896-960 MHz (\"900\") and 1215-1300 MHz (\"1200\") bands.\n\nThere are two predominant types of duplexer in use - \"notch duplexers\", which exhibit sharp notches at the \"unwanted\" frequencies and only pass through a narrow band of wanted frequencies and \"bandpass duplexers\", which have wide pass frequency ranges and high out of band attenuation.\n\nOn shared antenna sites, the bandpass duplexer variety is greatly preferred because this virtually eliminates interference between transmitters and receivers by removing out of band transmit emissions and considerably improving the selectivity of receivers. Most professionally engineered sites ban the use of notch duplexers and insist on bandpass duplexers for this reason.\n\n\"Note 1:\" A duplexer must be designed for operation in the frequency band used by the receiver and transmitter, and must be capable of handling the output power of the transmitter.\n\n\"Note 2:\" A duplexer must provide adequate rejection of transmitter noise occurring at the receive frequency, and must be designed to operate at, or less than, the frequency separation between the transmitter and receiver.\n\n\"Note 3:\" A duplexer must provide sufficient isolation to prevent receiver desensitization.\n\nSource: from Federal Standard 1037C\n"}
{"id": "12880377", "url": "https://en.wikipedia.org/wiki?curid=12880377", "title": "Ecological Building", "text": "Ecological Building\n\nEcological Building is both a design process and the structure that is a result of such a design process. \n\nThe Ecological Building design process is a modern architecture variant of permaculture design. \n\nAn Ecological Building is a structure that is \"designed to create and sustain mutually beneficial relationships with all of the elements of its local ecology\". A building's local ecology, or environment, is made up of particular physical and biological elements and their interactions.\n\nThe abiotic, or physical elements are defined by the local geology and the local climate. The local geology is defined by the soil type, substrata, local land use, and water patterns of the site and its surroundings. The local climate is made up of the weather patterns, wind patterns, solar patterns, and pollution patterns for the site and its surroundings.\n\nThe biotic or living elements are all of the local species and local ecosystems - including humans and urban ecologies - that interact with the site.\n\nThis concept is distinctly different from green building, or sustainable architecture where the goal is to \"minimize the negative environmental impact of buildings\". Ecological building is a positive design goal that sets out to increase beneficial interactions, whereas green building is a negative design outlook that seeks only the reduction of negative interactions. Inherent in green building is the assumption that any human interaction with a site is unavoidably negative, and that mitigating these negative impacts is the best that is possible. With Ecological Building, the designer acknowledges that humans can play an integral, beneficial role in improving and sustaining the health and vitality of their local ecology.\n\nFachwerken / Mitwirken (Nuremberg)"}
{"id": "623946", "url": "https://en.wikipedia.org/wiki?curid=623946", "title": "Ellen Ochoa", "text": "Ellen Ochoa\n\nEllen Ochoa (born May 10, 1958) is an American engineer, former astronaut and the current Director of the Johnson Space Center. Ochoa became director of the center upon the retirement of the previous director, Michael Coats, on December 31, 2012. In 1993 Ochoa became the first Hispanic woman in the world to go to space when she served on a nine-day mission aboard the shuttle Discovery.\n\nOchoa was born in Los Angeles, California, but grew up in La Mesa, California. Ochoa graduated from Grossmont High School in El Cajon in 1975. Her parents divorced when she was in high school and she lived with her mother and her brothers.\n\nOchoa received a bachelor of science degree in physics from San Diego State University and graduated Phi Beta Kappa in 1980, before earning a master of science degree and a doctorate in electrical engineering from Stanford University in 1981 and 1985, respectively.\n\nAs a doctoral student at Stanford, and later as a researcher at Sandia National Laboratories and the NASA Ames Research Center, Ochoa investigated optical systems for performing information processing. At the NASA Ames Research Center, she led a research group working primarily on optical systems for automated space exploration. She patented an optical system to detect defects in a repeating pattern and is a co-inventor on three patents for an optical inspection system, an optical object recognition method and a method for noise removal in images. As Chief of the Intelligent Systems Technology Branch at Ames, she supervised 35 engineers and scientists in the research and development of computational systems for aerospace missions. Ochoa has presented numerous papers at technical conferences and in scientific journals.\n\nOchoa was selected by NASA in January 1990 and became an astronaut in July 1991. Her technical assignments in the Astronaut Office included serving as the crew representative for flight software, computer hardware and robotics, Assistant for Space Station to the Chief of the Astronaut Office, lead spacecraft communicator (CAPCOM) in Mission Control and as acting as Deputy Chief of the Astronaut Office.\n\nOchoa became the first Hispanic woman in the world to go to space when she served on a nine-day mission aboard the shuttle Discovery in 1993. The purpose of the shuttle mission was to study the Earth's ozone layer. A veteran of four space flights, Ochoa has logged nearly 40d 19h 36m in space. She was a mission specialist on STS-56 (1993), was payload commander on STS-66, and was mission specialist and flight engineer on STS-96 and STS-110 in 2002. Ochoa was in Mission Control during the Space Shuttle Columbia disaster and was one of the first personnel informed of television coverage showing Columbia's disintegration.\n\nFrom 2007, after retiring from spacecraft operations, Ochoa served as Deputy Director of the Johnson Space Center, helping to manage and direct the Astronaut Office and Aircraft Operations. On January 1, 2013, Ochoa became the first Hispanic and second female director of NASA's Johnson Space Center.\n\nOchoa was named Vice Chair of the National Science Board for the 2018-2020 term. She currently chairs the committee evaluating nominations for the National Medal for Technology and Innovation.\n\nOchoa's husband is Coe Miles, an intellectual attorney. They have two sons. Ochoa is a classical flutist and played with the Stanford Symphony Orchestra, once receiving the Student Soloist Award.\n\nOchoa was recognized during Hispanic Heritage Month activities in Cleveland, Ohio on September 14, 2011.\n\nOchoa has received many awards among which are NASA's Exceptional Service Medal (1997), Outstanding Leadership Medal (1995) and Space Flight Medals (2002, 1999, 1994, 1993). Ochoa and Michael Foale were announced as the 2017 class of the United States Astronaut Hall of Fame. Ochoa was recognized in \"Hispanic Executive\"'s 2017 Best of the Boardroom issue for her work as a board director for Johnson Space Center.\n\nOchoa is a Fellow of AAAS, the American Institute of Aeronautics and Astronautics, and the National Academy of Inventors.\n\n\n"}
{"id": "22322080", "url": "https://en.wikipedia.org/wiki?curid=22322080", "title": "European BEST Engineering Competition", "text": "European BEST Engineering Competition\n\nEuropean BEST Engineering Competition (EBEC) is an annual engineering competition organised by the Board of European Students of Technology (BEST). EBEC spreads in 32 countries with the mission to develop students by offering them the opportunity to challenge themselves in solving a theoretical or a practical problem. Students form teams of four and are called upon to solve an interdisciplinary Team Design or Case Study task, thus addressing students from all the fields of engineering.\n\nBringing together students, universities, companies, institutions and NGOs, EBEC aims at taking out students’ full range of multidisciplinary knowledge and personal skills and applying this potential into solving real-life problems by working in teams.\n\nEBEC Project is under the core service of BEST to provide complementary education. During the competition, active and inquisitive students have the chance to apply the knowledge gained through university, to challenge themselves, to broaden their horizons, to develop their creativity and communication skills. These being fundamental elements of the competition, EBEC contributes in the support and advancement of the technological education, as well as in the promotion of a collaboration in a multicultural environment.\n\nIn 2019, EBEC Final Round will be held in Turin.\n\nThe idea of competitions was introduced in BEST through the Canadian Engineering Competitions (CEC) organized by Canadian Federation of Engineering Students (CFES). Members of BEST visited CEC in 2002 as guests and the idea of organising such competitions was discussed in that same year during BEST General Assembly. This is when the story of BEST Competitions begins, with the first BEST European Engineering Competition (BEEC) being organised in Eindhoven in 2003, the first National Round taking place in Portugal in 2006 and the very first EBEC Final being organised in Ghent in 2009 with finalists selected among 2300 participants from 51 universities in 18 countries, marking the completion of EBEC Pyramid.\n\nEBEC develops through three levels of competitions that form the EBEC Pyramid. With 84 Local Rounds, 15 National/Regional Rounds and 1 Final Round, EBEC is one of the largest engineering competitions organized by students for students in Europe, with nearly 7,000 students participating every year.\n\nLocal Rounds (LRs) are held within one University with an established Local BEST Group (LBG). The winning team of each category proceeds to the next level.\n\nNational/Regional Rounds (NRRs) are held within one country or a multinational region and are organised by one LBG of that country/region. The teams that won the local rounds compete in the same category, claiming a position in the EBEC Final. Currently, there is a total of 15 National/Regional Rounds with more than 700 students participating.\n\nThe Final Round of the European BEST Engineering Competition, EBEC Final, is one of the most prominent BEST events, organised by one LBG. Leading students, representing more than 80 of the greatest European universities, are gathered for 10 days to work on multiple tasks in an international environment. During the event, contestants also have the chance to meet people from different cultural backgrounds, to get a taste of the hosting city and also to come in touch with high-profile companies being present at the Job Fair held on the last day of the event.\n\nSince the advent of BEST Competitions, different competition categories, such as Debate and Negotiation, were introduced until EBEC developed to its final form, consisting of the Case Study and Team Design categories.\n\nCase Study (CS) is a theoretical, problem-solving challenge that requires the analysis, research, deliberation, testing and presentation of a solution for a current economical, legal or social problem. The solution must be provided within a limited amount of time and be supported by restricted resources, such as time and money.\nTeam Design (TD) is a practical, hands-on, project-based challenge that requires the design, creation and presentation of a prototype model that can successfully meet specific construction and operation criteria. The model must be created within a limited amount of time and through the use of low-cost and limited resource materials.\n\nTo date, seven editions have been organised, as summarized below.\n\nEBEC Final was organised for the first time by Local BEST Group Ghent in August 2009. 80 students participated, finding their way to the final between more than 2300 participants in 51 universities in 18 countries. This event was supported by UNEP, that provided a real-life problem for the Team Design part, while EBEC was recognised as partner of the European Year of Creativity and Innovation.\n\nEBEC kept on developing with 71 Technical Universities embracing this venture. With a total of 5000 students participating in 31 countries, 104 finalists were selected and gathered in Cluj-Napoca to prove themselves.\n\nIn the 3rd edition of EBEC, 79 technical universities were involved with more than 5000 students participating in the first level of the competition, 104 students having the chance to meet in Istanbul and more than 200 BEST members contributing to the realisation and development of this project.\n\nEBEC Final 2012 was organised in Zagreb, receiving the patronage of the President of Croatia. The event consisted of four working days, Official Opening/Closing Days and one free day, where participants had the opportunity to discover the city of Zagreb. \n\nThe 5th edition of EBEC Final took place in Warsaw and involved 83 Technical Universities in Europe, 15 National/Regional EBEC Rounds and more than 6500 students participating. The event was supported by Warsaw University of Technology, as well as important institutions, such as the Ministry of Science and Higher Education and Copernicus Science Centre.\n\n87 Local Rounds, more than 6000 participants, 116 finalists and more than 500 BEST members across 32 countries contributed in the preparation and successful conduction of the 6th edition of EBEC Final in Riga.\n\nIn 2015, EBEC Final was held in Porto, reaching the maximum number of participants so far (120) and setting high standards for the upcoming editions.\n\nIn 2016, the EBEC Final was held in Belgrad between the 2nd to the 9th of August.\n\nIn 2017, the EBEC Final was held in Brno, during August.\n\nOn 2018 there was not a Final round of EBEC. For this reason it was called EBEC Challenge, as there was a slight difference between the editions.\n\nEBEC is a competition that spreads all over Europe reaching thousands of students, universities and companies. But what makes EBEC unique is not only the numbers and the technical outcomes, but the renowned “EBEC spirit”, that is the atmosphere surrounding the competition consisting of the teamwork, the unbound creativity being interlinked with knowledge, the strive for the best of oneself. This is what makes students passionate to participate and to work for the best solution, what brings professors and experts to offer their expertise and knowledge for the transparency of the competition, what makes the companies want to support the competition again and ensure that students deal with current technological problems and of course what makes BEST members continuously work more and more passionately to develop the competition. This is what brings all these people together for a common goal, to “Design the Future. Today.”\n\nEBEC Final 2015 in Porto was qualified as the best project of Portugal for the next round of European Charlemagne Youth Prize.\n\nBEST always seeks support from institutions that recognise its efforts and share its vision to assist European students. So far, EBEC is supported by many institutions and bodies, such as UNESCO, Young in Action, European Society for Engineering Education (SEFI), Institute of Electrical and Electronics Engineers.\n\nUniversities that supported EBEC during recent years include: Aristotle University of Thessaloniki, Czech Technical University in Prague, Graz University of Technology, National Technical University of Athens (NTUA), Silesian University of Technology in Gliwice, Universidade do Porto, Yildiz Teknik Universitesi.\n\nIt is notable that EBEC is starting to be recognised from universities as a project of high quality that contributes to the education of the participants. University of Porto was the first university to recognise the competition by attributing ECTS to the participants.\n\n"}
{"id": "28128957", "url": "https://en.wikipedia.org/wiki?curid=28128957", "title": "Gleducar", "text": "Gleducar\n\nGleducar is a free educational project emerged in Argentina in 2002. It is also an important NGO (Civil Association) from Argentina in the field of education and technology.\n\nGleducar is an independent community composed of teachers, students and education activists linked by a common interest in collective work, cooperative knowledge building and free distribution of knowledge.\n\nThe project works around different themes, such as Open Education, Open Access, Free Knowledge, Popular Education, peer education, collaborative learning and Free Technologies, and promotes the use of Free Software in schools as a pedagogical and technical system, with the objective of changing the paradigm of production, construction and dissemination of educational content.\n\nIt consists of an independent educational community incorporated as a self-organized NGO (Civil Association) which meets the interests and objectives of the community. Gleducar Project is the result of the sum of their community and the NGO that supports it.\n\n\nGleducar Project was born around 2002 in the city of Cañada Rosquín, Santa Fé, Argentina. It was established as a Civil Association in 2004. Today is one of the most important educational projects in Argentina, concerning free education.\n\nGleducar Project was declared of National Interest by the Senate of Argentina in 2005.\n\nIt is recognized worldwide as a benchmark of free education in Latin America. In 2007 it received an Honorable Mention in the International Competition \"Chris Nicol\" of the Free Software Association for Progressive Communications (APC) for its outstanding work for free and sustainable education.\n\nGleducar's work has been an inspiration and a guide for the emergence of other similar projects and communities on the continent.\n\nGleducar carries out projects to improve computer labs, settled on dozens of migrations to Free Software conducted in schools in Argentina, that have obtained high-quality resources for teaching.\n\nGleducar community also develops free educational materials in conjunction with other nonprofit organizations and working with the Argentine State. It has a large repository of free educational resources and a wide range of educational free software tools.\n\nGleducar develops pedagogical and technical skills on collaborative knowledge production, free education and free software.\n\nIt regularly organizes two annual events CoLCIT (Congress of Free Culture for Tertiary Institutions) and Epuel (Meeting for a Free Education). It also participates in conferences and local, regional and international meetings on the subject.\n\nIt has worked with the argentinian National Ministry of Education in the development of free educational materials. However, the NGOs does not currently receive financial contributions from any government entity.\n\nGleducar has also carried out numerous actions in conjunction with the Fundación Vía Libre. It has also developed open source projects and has carried out joint activities with other organizations such as Fairness Foundation, Caritas, CTERA (Confederation of Education Workers of Argentina), AMSAFE, SoLAr, FM La Tribu, CaFeLUG, LUGRo, Tuquito GNU/Linux, Wikimedia Argentina, among others. The project also hosts and promotes other initiatives such as Argenclic, Free University, among others.\n\nInternationally, the project has participated in and acceded to various joint statements on access to knowledge and free / open education as the \"Santo Domingo Declaration\", the Cape Town Open Education Declaration and the \"Charter for innovation, creativity and access to knowledge\".\n\nIt also works on issues related to the free movement of knowledge, participating alongside other civic organizations on initiatives and campaigns warning about the threat of Intellectual Property regimes for the common cultural heritage, and on mobilizations against policies that victimizes the net neutrality and the freedom of expression on the Internet, on various occasions.\n\nGleducar provides multiple resources for teachers in particular and to all who wish to undertake free projects related to the C3. The space is provided at no economic cost on the condition of using a free license that -minimally- allows to share and lead free materials generated.\n\nGleducar community has resources as: an educational wiki which already contains more than 2,000 free educational resources (more than 5000 pages in total) and over 5,600 registered users; a virtual campus, a server webquest, multiple mailing lists with over 570 members and even a personal sites aggregator to centralize and register changes in some members's blogs of the Gleducar community\n\nThe technical infrastructure used by the NGO is provided by USLA (Free Software Users Argentina).\n\n"}
{"id": "30653352", "url": "https://en.wikipedia.org/wiki?curid=30653352", "title": "Heatable glass", "text": "Heatable glass\n\nElectrically heatable glass and windows are relatively new products, which help solve problems in the design of buildings and vehicles. \nThe idea of heating glass is based on the use of energy-efficient low-emissive glass, which is generally simple silicate glass with a special metallic oxides coating. Low-emissive coating decreases heat loss by approximately 30%. Heatable glass can be used in all kinds of standard glazing systems, whether wood, plastic, aluminum or steel.\n\nHeatable glass based on low-emissive coatings was first produced in high volume in the early 1980s. Today, heating glass is used in the construction of many kinds of buildings and in mass production of vehicles, ships and trains. \nHeatable glass removes discomfort and other disadvantages induced by the low heat-insulating features of silicate glass. The effect of “cold glass” disappears when the surface of the glass is heated. Condensation is eliminated, along with ice and snow covering, the window’s heat losses are compensated and room comfort is improved.\n\nHeatable glass can be used as the principal system of heating and can be combined with floor and ceiling heating. Such combination helps reduce the total rate of heat loss of the building, thereby lowering heating expenses. Also, the active area of the room can be used more efficiently, as massive window-sill radiators are not needed. \nInitially, heating glass was produced by sputtering ordinary glass, and stable quality could not be guaranteed. A technological breakthrough took place in 1989 when the mass production of low-emissive glass began. The glass was coated during the manufacturing process.\n\n\nWindows play a significant role in room comfort. As a result, the area of glazing of buildings is constantly being increased. \nWindow technologies always in progress and it is common today to use low-emissive glass. In spite of progress the low temperature of glass surface is still the problem of constructive glazing. Heatable glass helps to solve problems concerning low surface temperature and increase the level of comfort in the room significantly. Heatable glass can be used in practically all kinds of glazing systems made of wood, plastic or aluminum. \nHeatable glass and multiple glass panes can be used both in blind and openable constructions. Multiple glass panes made of heating glass can have one or two chambers. The advantages of multiple glass panes are their hermiticity and ability to decrease heat transfer significantly.\n\n\nIf the temperature in the building is higher than the temperature outside, the heat leaks through the elements of construction. Windows are usually the most vulnerable elements of the building in terms of heat losses. The heat loss though window constructions is about 20–25% of total heat loss. \nHeat insulation of translucent constructions can be improved by increasing the number of glasses and chambers of multiple glass panes, but will result in increased construction cost and decreased optical transmission. The reasonable alternative is the usage of low-emissivity glass, which is practically the same as ordinary glass in terms of optical transmission, but it also reflects the heat radiation back into the room. \nThe major indicator which characterizes the ability of glass to reflect heat radiation is its emissivity (E) or the \"emission factor\". The emission factor of ordinary glass is 0.83; the factor for low-emissive glass can reach 0.03, so that more than 90% of accumulated heat will be reflected back into the room. The lower the emission factor is, the more effective is the material to reflect the heat, and the more heat it will accumulate. To compare, the emission factor of a multiple glass pane with two chambers, which is made of ordinary glass, is the same as the emission factor of a multiple glass pane with one chamber, which was produced with usage of low-emissive glass. \nBesides energy-efficient functions in cold seasons of the year, low-emissive glass possesses the ability to reflect the excess of outside heat energy in summer seasons; the optical transmission coefficient is affected insignificantly in this way. \nThe additional factor of reduce of heat transfer of multiple glass panes is the usage of low thermal conductivity gases – Ar or Kr – to fill the chambers. In present-day multiple glass panes Ar is more often used, which helps to reduce heat losses by 10–20%, though the cost of multiple glass panes is insignificantly increased.\n\n\nThere are two reasons why people feel discomfort when they are close to a cold window surface. First, a cold window is the reason for outflow of heat, which is produced by the cutaneous covering of the individual. Second, a cold window provokes the circulation of air, which is felt like a draft. \nIn order to reduce these factors the heating radiators are always placed under window sills. As far as people can feel cold and heat, the actual temperature of environment is not the only factor which defines the total level of comfort. In reality, the heat radiation of surrounding surfaces has a greater influence than air temperature. If the window surface is cold then to maintain the comfort atmosphere it is necessary to increase the heating temperature, but it will also increase energy consumption. \nThe problem of cold window can be solved effectively with the help of heatable glass. These windows allow to maintain the optimal comfort level and temperature of the room. The air temperature can be decreased at least by 1 degree if the temperature of surrounding surfaces has the same significance. You also do not have to install heating radiators and free the additional space for that. Besides, when turned off the multiple glass panes made of heatable glass act like ordinary low-emissive glass.\n\n\nThe idea of heatable glass is based on usage of energy-efficient low-emissive glass, where the coating plays the role of heating element. It can be used both in production of multiple glass panes and as a part of triplex, which has also the function of protective glazing. \nThe technological process of production of multiple glass panes made of heatable glass is practically the same as the process of ordinary multiple glass panes production. The main difference is the presence of power supply and, if necessary, temperature sensor. The temperature sensor allows to track the temperature of heating glass and eliminates the possibility of overheating of the product. \nIn order to prevent shocks, the conductive coating is always placed inside the multiple glass pane or laminated unit. \nOnly safe tempered glass, the strength of which is a lot higher than the strength of ordinary glass, is used in production of heatable glass. When the hardened glass is destroyed there are safe splittings. Also the current-carrying coating loses its integrity and the automatic fuse, which turns off the power supply of the glass, is activated. The electrodes are placed inside the lamination and no one can reach them without destruction of the product.\n\n\nHeating glass is mostly used for heating of windows. It is especially useful for rooms where people spend much time by the windows, at home or at work. The most common usage of heating glass—windows of cottages, office buildings and also big areas—leaded panes, translucent roofing, garret windows, canopies and so on. \nHeating glass is used for defogging and prevention of frosting of windows of pools, saunas and other buildings of such kind. \nInsofar as heatable glass has a current-carrying coating, it can be used as the sensor of alarm systems. When the glass is destroyed the system of protection is activated and it results in activation of alarm system. \nThis kind of product is widely used on objects of tightened standards in questions of protection: nuclear power plants, stations of air navigation control, museums, special storehouses, etc. \nHeatable glass is also used in production of windows for different kinds of vehicles: electric and diesel locomotives, vessels and boats, various kinds of aircraft and automobiles. \nOne of well-known examples of application of heating glass is armored windows, because the protective glazing is very thick and is disposed to frosting. The usage of heating glass is especially urgent in terms of being the part of armored multiple glass of Smart Glass of switchable transparency, because the heating significantly decreases the period of reaction of liquid crystals structure. \nThe power consumed by products depends on the type of use. Power of about 50–100 watts per square meter of the window is generally enough for maintenance of comfort temperature in the room and for maintenance of glass surface temperature at the rate from +20 degrees to +30 degrees. \nWhen the heatable glass is used as the only source of heat it is necessary to maintain the glass surface temperature at the rate from +30 degrees to −45 degrees and provide the power of 100 to 300 watt for 1 square meter of the window. \nThe power needed for vehicle windows reaches 1.5 kilowatts per square meter or more, which is why there are such tight standards in terms of sputtering of current-carrying components. \nHeat power of about 500–700 watts per square meter of glazing is necessary for snow unloading and taking ice-covering off the outside protective translucent constructions in low temperatures and windy environments.\n\nHeatable glass is produced by lamination of two or more sheets of silicate glass. The most widespread technologies are the following technologies of panel production according to the type of materials used:\n\n\n"}
{"id": "37505716", "url": "https://en.wikipedia.org/wiki?curid=37505716", "title": "Holding current (electronics)", "text": "Holding current (electronics)\n\nThe holding current (hypostatic) for electrical, electromagnetic and electronic devices is the minimum current which must pass through a circuit in order for it to remain in the 'ON' state. The term can be applied to a single switch or to an entire device. A simple example of holding current is in a Spark gap.\n\nIn the most basic of circuits, if the current falls below the holding current even briefly, the circuit is turned 'OFF' (becomes blocked). However, complex circuits and devices may have different delays built-in between the time the current falls below this level and the time the device turns 'OFF'. Whether a device turns 'ON' when current is restored is a design issue. The current necessary to restore the circuit to the 'ON' state, called the \"threshold current\" (See threshold voltage), may be much greater than the holding current, or only very slightly more. Nevertheless, where the device is designed to turn back 'ON' upon restoration of the current and where the device is running at or about the holding current level, slight variations in the current can cause \"flicker\" as the device cycles 'OFF' and 'ON'. If flicker is undesirable, it can be reduced by the use of capacitors or other circuits, on the other hand, flicker can be used to measure small events as in a Geiger–Müller tube.\n\nA related term is latching current, which is the minimum additional current that can make up for any missing input (gate) current in order to keep the device 'ON', in other words to keep the device's internal structure latched. \n\n"}
{"id": "35176246", "url": "https://en.wikipedia.org/wiki?curid=35176246", "title": "Landauer formula", "text": "Landauer formula\n\nThe Landauer formula—named after Rolf Landauer, who first suggested its prototype in 1957—is a formula relating the electrical resistance of a quantum conductor to the scattering properties of the conductor.\nIn the simplest case where the system only has two terminals, and the scattering matrix of the conductor does not depend on energy, the formula reads\nwhere formula_2 is the electrical conductance, formula_3 is the conductance quantum, formula_4 are the transmission eigenvalues of the channels, and the sum runs over all transport channels in the conductor. This formula is very simple and physically sensible: The conductance of a nanoscale conductor is given by the sum of all the transmission possibilities that an electron has when propagating with an energy equal to the chemical potential, formula_5. \n\nA generalization of the Landauer formula for multiple probes is the Landauer-Büttiker formula, proposed by Landauer and . If probe formula_6 has voltage formula_7 (that is, its chemical potential is formula_8), and formula_9 is the sum of transmission probabilities from probe formula_10 to probe formula_6 (note that formula_9 may or may not equal formula_13), the net current leaving probe formula_14 is \n\n"}
{"id": "370539", "url": "https://en.wikipedia.org/wiki?curid=370539", "title": "Leaf blower", "text": "Leaf blower\n\nA leaf blower (often referred to as simply a blower) is a gardening tool that propels air out of a nozzle to move debris such as leaves and grass cuttings. Leaf blowers are powered by electric or gasoline motors. Gasoline models have traditionally been two-stroke engines, but four-stroke engines were recently introduced to partially address air pollution concerns. Leaf blowers are typically self-contained handheld units, or backpack mounted units with a handheld wand. The latter is more ergonomic for prolonged use. Larger units may rest on wheels and even use a motor for propulsion. These are sometimes called \"walk behind leaf blowers\" because they must be pushed by hand to be operated.\n\nSome units can also suck in leaves and small twigs via a vacuum, and shred them into a bag. In that role it is called a blower vac.\n\nThe leaf blower was invented by Dom Quinto in the 1950s. Drought conditions in California facilitated acceptance of the leaf blower as the use of water for many garden clean-up tasks was prohibited. Leaf blowers also save time compared to a broom. By 1990, annual sales were over 800,000 in the U.S., and the tool had become a ubiquitous gardening implement.\n\nOther functions beyond the simple use of garden maintenance have been demonstrated by Richard Hammond on the television series, in which a man sized hovercraft was constructed from a leaf blower. Being both portable and able to generate wind speeds of between and air volumes of 14 m per minute, the leaf blower has many potential uses in amateur construction projects. Some professional walk-behind leaf blowers can even reach air volumes of 100 m per minute (over 3800 CFM).\n\nEmissions from gasoline-powered grounds-keeping equipment in general are a source of air pollution and more immediately (when powered by internal combustion engines, rather than by electricity), noise pollution. In the United States, US emission standards prescribe maximum emissions from small engines. The two-stroke engines used in most leaf blowers operate by mixing gasoline with oil, and a third of this mixture is not burned, but is emitted as an aerosol exhaust. These pollutants have been linked to cancer, heart disease, and asthma. A 2011 study found that the pollutants emitted by a leaf blower operated for 30 minutes is comparable to the amount emitted by a Ford F-150 pickup truck driving from Texas to Alaska.\n\nIn addition to the adverse health effects of carbon monoxide, nitrogen oxides, hydrocarbons, and particulates generated in the exhaust gas of the gasoline-powered engines, leaf blowers pose problems related to dust raised by the powerful flow of air. Dust clouds caused by leaf blowers contain potentially harmful substances such as pesticides, mold, and animal fecal matter that may cause irritation, allergies, and disease.\n\nNoise pollution is also a concern with leaf blowers, as they emit noise levels well above those required to cause hearing loss to both the operator and those nearby.\n\nLeaf blowers also present an occupational hearing hazard to the nearly 1 million people who work in lawn service and ground-keeping. A recent study assessed the occupational noise exposure among groundskeepers at several North Carolina public universities and found noise levels from leaf blowers averaging 89 decibels (A-weighted) and maximum sound pressure levels reaching 106 dB(A), both far exceeding the National Institute for Occupational Safety and Health (NIOSH) Recommended Exposure Limit of 85 dB(A) \n\nSoon after the leaf blower was introduced into the U.S., its use was banned in two California cities, Carmel-by-the-Sea in 1975 and Beverly Hills in 1978, as a noise nuisance. There are currently twenty California cities that have banned leaf blowers, sometimes only within residential neighborhoods and usually targeting gasoline-powered equipment. Another 80 cities have ordinances on the books restricting either usage or noise level or both.\n\n\n"}
{"id": "33389942", "url": "https://en.wikipedia.org/wiki?curid=33389942", "title": "LevelUp", "text": "LevelUp\n\nLevelUp is an American mobile ordering and mobile payments platform created by Boston, Massachusetts-based start-up SCVNGR. The LevelUp platform connects restaurants and guests with a customer experience that blends analytics, loyalty, and rewards. LevelUp brings its technology to market in multiple ways: embedded into partner restaurant apps to provide a full-stack customer engagement solution; via an open developer platform powering over 200 mobile apps; and through LevelUp's partner distribution channels, which lets consumers order ahead and avoid the line at restaurants using the apps already on their iPhone and Android phones. On July 25, 2018, it was announced that LevelUp would be acquired for US$390 million by Grubhub, an online food delivery platform.\n\nLevelUp was initially launched in March 2011 and operated for its first 3 months as a daily deals platform. In July 2011, LevelUp shifted away from the daily deal space to focus exclusively on their mobile payments solution. In July 2013, around 200,000 users and 3,000 companies were using LevelUp. In October 2014, around 14,000 stores were using LevelUp. In May 2017 they announced that they had raised $50 million in funding, with over 50,000 locations and more than 200 brands using Levelup.\n\nThe LevelUp mobile application for iPhone, Android and Windows Phone allows registered users to securely link their debit or credit card to a unique QR code displayed within the app. To pay with LevelUp, users scan the QR code on their phone at LevelUp terminals located at local businesses who accept LevelUp as a form of payment.\n\nSome merchants that accept LevelUp as a form of payment also offer monetary savings to users. Users are given “First-Time Visit Specials” the first time they make a transaction at the merchant's location. Users can also unlock \"credit\" to a merchant’s store after spending a certain amount at the merchant’s location.\n\nAs of June 2016, LevelUp is available for businesses in the Boston, Chicago, Washington DC, Northern Virginia, Philadelphia, St. Louis, New York City, Atlanta, San Francisco, Dallas, San Diego, Minneapolis, Montgomery, Kansas City, Seattle and Wilmington, NC areas.\n\nIn early 2018, the company announced the release of Broadcast, enabling restaurant brands to reach millions of new customers by allowing them to browse live menus, order ahead and pay directly from high traffic apps such as Facebook, Messenger, Yelp, Foursquare, Amazon Alexa and Chase Pay.\n"}
{"id": "238145", "url": "https://en.wikipedia.org/wiki?curid=238145", "title": "List of medieval weapons", "text": "List of medieval weapons\n\nThe following is a list of Wikipedia articles of the types of weapons that were in use during the post-classical historical period (roughly between the mid 1st to mid 2nd millennia AD).\n\nSwords can be single or double-bladed edges, or even edgeless. The blade can be straight or curved.\n\n\n\n\n\n\n\nWhile armor is not technically a weapon, its use was driven by weapon technology and was a driving force in weapon development.\n\nMedieval fortifications also developed in connection with the weapons that opposed them.\n\n\n"}
{"id": "51518904", "url": "https://en.wikipedia.org/wiki?curid=51518904", "title": "Lou Doctor", "text": "Lou Doctor\n\nLouis J. Doctor is an e-commerce entrepreneur, and the founder of Crowd Supply and Velotech. He utilises an approach that has been called \"reverse e-commerce\".\n\nDoctor is also a managing director at Horizon Partners.\n"}
{"id": "853695", "url": "https://en.wikipedia.org/wiki?curid=853695", "title": "Milkman", "text": "Milkman\n\nA milkman is a delivery person who delivers milk, often directly to customers' houses, in bottles or cartons.\n\nMilk was delivered to houses daily in some countries when a lack of good refrigeration meant milk would quickly spoil. Before milk bottles were available, milkmen took churns on their rounds and filled the customers' jugs by dipping a measure into the churn. The near-ubiquity of refrigerators in homes in the developed world, as well as improved packaging, has decreased the need for frequent milk delivery over the past half-century and made the trade shrink in many localities sometimes to just three days a week and disappear totally in others. Additionally, milk delivery incurs a small cost on the price of dairy products that is increasingly difficult to justify and leaves delivered milk in a position where it is vulnerable to theft.\n\nMilk deliveries frequently occur in the morning and it is not uncommon for milkmen and milkwomen to deliver products other than milk such as eggs, cream, cheese, butter, yogurt, or soft drinks.\n\nIn some areas apartments and houses would have small milk delivery doors. A small wooden cabinet inside of the residence, built into the exterior wall, would have doors on both sides, latched but not locked. Milk or groceries could be placed in the box when delivered, and collected by the homeowner.\n\nTruck drivers who transport milk from a farm to a milk processing plant are also known as milkmen or milkwomen. Raw milk is picked up daily, or every other day.\n\nFrom the 20th century, milk delivery in urban areas of Europe has been carried out from an electric vehicle called a milk float. These replaced horse-drawn vehicles, which were still seen in Britain in the 1950s, and parts of the United States until the 1960s. In Australia, the delivery vehicle was usually a small petrol or diesel engined truck with a covered milk-tray. In hotter areas, this tray is usually insulated.\n\nIn India, those delivering milk usually use milk churns, a practice that has ceased in western countries. On the road they are put on any kind of vehicle. In big cities such as Mumbai, milk churns are often transported in luggage compartments in local trains.\n\nIn 2005, about 0.4% of consumers in the United States had their milk delivered, and a handful of newer companies had sprung up to offer the service. Some U.S. dairies have been delivering milk for about a hundred years, with interest continuing to increase in the 2010s as part of the local food movement.\n\nIn the Philippines, the milkman or milkmaid is called \"lechero\". The tradition stemmed from the community production of carabao milk. The \"lechero\" delivers the fresh carabao milk to the barangay (village) of his or her designation. The \"lechero\" heritage used to be widely practiced in the country, however, declined after the introduction of store-bought milk during the American occupation period. Nowadays, only a few communities have \"lecheros\", notably in Nueva Ecija province, the milk capital of the Philippines.\n\nIn the Uganda region an often used title for \"king\" is \"Omukama\", which means \"superior milkman/milk bringer\": a title that refers to the role of the leader as a feeder of the people and the historical tradition that the ancient ruling class of some Ugandan kingdoms was of Hema stock, the Hema being cattle-holders.\n\nThe 1932 film \"Scarface\", features a milkman.\n\nThe 1934 film \"It's a Gift\", also features a milkman.\n\nIn the 1935 film \"The 39 Steps\", Richard Hanney disguises himself as a milkman. \n\nIn the 1936 film \"The Milky Way\", the protagonist, Burleigh Sullivan is a milkman.\n\nThe 1945 film \"The Lost Weekend\", also features a milkman.\n\nIn the 1946 film \"The Kid from Brooklyn\", the protagonist is also a milkman.\n\n\"The Milkman\" is a 1950 comedy film starring Donald O'Connor and Jimmy Durante. \n\nThe lead character of the 1964 stage musical \"Fiddler on the Roof\", Tevye, is a milkman.\n\nThe Early Bird Film with Norman Wisdom portrays milk deliveries from the UK 1960's.\n\nIn the James Bond film \"The Living Daylights\" was the hitman Necros in a milkman disguise.\n\nA short story in the horror anthology \"Skeleton Crew\" by Stephen King, called \"Morning Deliveries (Milkman No. 1)\", concerns a milkman who kills people by leaving \"surprises\" (including poison, toxic gas and venomous spiders) in their milk cans.\n\n\"Reid Fleming, World’s Toughest Milkman\" is a comicbook character created by David Boswell which first appeared in 1980.\n\nIn Toni Morrison's novel \"Song of Solomon\" the main character's nickname is \"Milkman.\"\n\nThe title of the 1966 pop hit \"No Milk Today\" by the British band Herman's Hermits refer to a common notice instructing the milkperson not to leave the usual order of milk on a particular day. In the song this symbolizes the singer's recent breakup with his love interest, who has just moved out of his house.\n\nIn 1971, British comedian Benny Hill, himself a former milkman, had a hit novelty song called \"Ernie (The Fastest Milkman In The West)\".\n\nIn 1944, Ella Mae Morse had a US top 10 hit with \"Milkman, Keep Those Bottles Quiet\", a song from the film \"Broadway Rhythm\".\n\nIn the Doctor Who episode, \"The Stolen Earth\", The Doctor and Donna meet a milkman.\n\nThe \"All That\" sketch \"The Adventures of Superdude\" features a villainous milkman called Milkman (portrayed by Josh Server) who is the archenemy of Superdude and uses milk-based weapons on Superdude as a way to take advantage of his lactose intolerance.\n\n\n"}
{"id": "19935", "url": "https://en.wikipedia.org/wiki?curid=19935", "title": "Mimeograph", "text": "Mimeograph\n\nThe stencil duplicator or mimeograph machine (often abbreviated to mimeo) is a low-cost duplicating machine that works by forcing ink through a stencil onto paper. The mimeograph process should not be confused with the spirit duplicator process.\n\nMimeographs, along with spirit duplicators and hectographs, were a common technology in printing small quantities, as in office work, classroom materials, and church bulletins. Early fanzines were printed with this technology, because it was widespread and cheap. In the late 1960s, mimeographs, spirit duplicators, and hectographs began to be gradually displaced by photocopying.\n\nUse of stencils is an ancient art, butthrough chemistry, papers, and pressestechniques advanced rapidly in the late nineteenth century:\n\nA description of the Papyrograph method of duplication was published by David Owen: \n\nA major beneficiary of the invention of synthetic dyes was a document reproduction technique known as stencil duplicating. Its earliest form was invented in 1874 by Eugenio de Zuccato, a young Italian studying law in London, who called his device the Papyrograph. Zuccato’s system involved writing on a sheet of varnished paper with caustic ink, which ate through the varnish and paper fibers, leaving holes where the writing had been. This sheet – which had now become a stencil – was placed on a blank sheet of paper, and ink rolled over it so that the ink oozed through the holes, creating a duplicate on the second sheet.\n\nThe process was commercialized and Zuccato applied for a patent in 1895 having stencils prepared by typewriting.\n\nThomas Edison received US patent 180,857 for Autographic Printing on August 8, 1876. The patent covered the electric pen, used for making the stencil, and the flatbed duplicating press. In 1880 Edison obtained a further patent, US 224,665: \"Method of Preparing Autographic Stencils for Printing,\" which covered the making of stencils using a file plate, a grooved metal plate on which the stencil was placed which perforated the stencil when written on with a blunt metal stylus.\n\nThe word mimeograph was first used by Albert Blake Dick when he licensed Edison's patents in 1887.\n\nDick received Trademark Registration no. 0356815 for the term \"Mimeograph\" in the US Patent Office. It is currently listed as a dead entry, but shows the A.B. Dick Company of Chicago as the owner of the name.\n\nOver time, the term became generic and is now an example of a genericized trademark. (\"Roneograph,\" also \"Roneo machine,\" was another trademark used for mimeograph machines, the name being a contraction of Rotary Neostyle.)\n\nIn 1891, David Gestetner patented his Automatic Cyclostyle. This was one of the first rotary machines that retained the flatbed, which passed back and forth under inked rollers. This invention provided for more automated, faster reproductions since the pages were produced and moved by rollers instead of pressing one single sheet at a time.\n\nBy 1900, two primary types of mimeographs had come into use: a single-drum machine and a dual-drum machine. The single-drum machine used a single drum for ink transfer to the stencil, and the dual-drum machine used two drums and silk-screens to transfer the ink to the stencils. The single drum (example Roneo) machine could be easily used for multi-color work by changing the drum - each of which contained ink of a different color. This was spot color for mastheads. Colors could not be mixed.\n\nThe mimeograph became popular because it was much cheaper than traditional print - there was neither typesetting nor skilled labor involved. One individual with a typewriter and the necessary equipment became his own printing factory, allowing for greater circulation of printed material.\n\nThe image transfer medium was originally a stencil made from waxed mulberry paper. Later this became an immersion-coated long-fibre paper, with the coating being a plasticized nitrocellulose. This flexible waxed or coated sheet is backed by a sheet of stiff card stock, with the two sheets bound at the top.\n\nOnce prepared, the stencil is wrapped around the ink-filled drum of the rotary machine. When a blank sheet of paper is drawn between the rotating drum and a pressure roller, ink is forced through the holes on the stencil onto the paper. Early flatbed machines used a kind of squeegee. The ink originally had a lanolin base. and later became an oil in water emulsion. This emulsion commonly used Turkey-Red Oil (Sulfated Castor Oil) which gives it a distinctive and heavy scent.\n\nFor printed copy, a stencil assemblage is placed in a typewriter. The part of the mechanism which lifts the ribbon must be disabled so that the bare, sharp type element strikes the stencil directly. The impact of the type element displaces the coating, making the tissue paper permeable to the oil-based ink. This is called \"cutting a stencil.\"\n\nA variety of specialized styluses were used on the stencil to render lettering, illustrations, or other artistic features by hand against a textured plastic backing plate.\n\nMistakes can be corrected by brushing them out with a specially formulated correction fluid, and retyping once it has dried. (\"Obliterine\" was a popular brand of correction fluid in Australia and the United Kingdom.)\n\nStencils were also made with a thermal process; an infrared method similar to that used by early photocopiers. The common machine was a Thermofax.\n\nAnother device, called an electrostencil machine, sometimes was used to make mimeo stencils from a typed or printed original. It worked by scanning the original on a rotating drum with a moving optical head and burning through the blank stencil with an electric spark in the places where the optical head detected ink. It was slow and produced ozone. Text from electrostencils had lower resolution than that from typed stencils, although the process was good for reproducing illustrations. A skilled mimeo operator using an electrostencil and a very coarse halftone screen could make acceptable printed copies of a photograph.\n\nDuring the declining years of the mimeograph, some people made stencils with early computers and dot-matrix impact printers.\n\nUnlike spirit duplicators (where the only ink available is depleted from the master image), mimeograph technology works by forcing a replenishable supply of ink through the stencil master. In theory, the mimeography process could be continued indefinitely, especially if a durable stencil master were used (e.g. a thin metal foil). In practice, most low-cost mimeo stencils gradually wear out over the course of producing several hundred copies. Typically the stencil deteriorates gradually, producing a characteristic degraded image quality until the stencil tears, abruptly ending the print run. If further copies are desired at this point, another stencil must be made.\n\nOften, the stencil material covering the interiors of closed letterforms (e.g. \"a\", \"b\", \"d\", \"e\", \"g\", etc.) would fall away during continued printing, causing ink-filled letters in the copies. The stencil would gradually stretch, starting near the top where the mechanical forces were greatest, causing a characteristic \"mid-line sag\" in the textual lines of the copies, that would progress until the stencil failed completely. The Gestetner Company (and others) devised various methods to make mimeo stencils more durable.\n\nCompared to spirit duplication, mimeography produced a darker, more legible image. Spirit duplicated images were usually tinted a light purple or lavender, which gradually became lighter over the course of some dozens of copies. Mimeography was often considered \"the next step up\" in quality, capable of producing hundreds of copies. Print runs beyond that level were usually produced by professional printers or, as the technology became available, xerographic copiers.\n\nMimeographed images generally have much better durability than spirit-duplicated images, since the inks are more resistant to ultraviolet light. The primary preservation challenge is the low-quality paper often used, which would yellow and degrade due to residual acid in the treated pulp from which the paper was made. In the worst case, old copies can crumble into small particles when handled. Mimeographed copies have moderate durability when acid-free paper is used.\n\nGestetner, Risograph, and other companies still make and sell highly automated mimeograph-like machines that are externally similar to photocopiers. The modern version of a mimeograph, called a digital duplicator, or copyprinter, contains a scanner, a thermal head for stencil cutting, and a large roll of stencil material entirely inside the unit. The stencil material consists of a very thin polymer film laminated to a long-fibre non-woven tissue. It makes the stencils and mounts and unmounts them from the print drum automatically, making it almost as easy to operate as a photocopier. The Risograph is the best known of these machines.\n\nAlthough mimeographs remain more economical and energy-efficient in mid-range quantities, easier-to-use photocopying and offset printing have replaced mimeography almost entirely in developed countries. Mimeograph machines continue to be used in developing countries because it is a simple, cheap, and robust technology. Many mimeographs can be hand-cranked, requiring no electricity.\n\nMimeographs and the closely related but distinctly different spirit duplicator process were both used extensively in schools to copy homework assignments and tests. They were also commonly used for low-budget amateur publishing, including club newsletters and church bulletins. They were especially popular with science fiction fans, who used them extensively in the production of fanzines in the middle 20th century, before photocopying became inexpensive.\n\nLetters and typographical symbols were sometimes used to create illustrations, in a precursor to ASCII art. Because changing ink color in a mimeograph could be a laborious process, involving extensively cleaning the machine or, on newer models, replacing the drum or rollers, and then running the paper through the machine a second time, some fanzine publishers experimented with techniques for painting several colors on the pad, notably Shelby Vick, who created a kind of plaid \"Vicolor\".\n\n\n"}
{"id": "43517179", "url": "https://en.wikipedia.org/wiki?curid=43517179", "title": "Motion History Images", "text": "Motion History Images\n\nThe motion history image (MHI) is a static image template helps in understanding the motion location and path as it progresses. In MHI, the temporal motion information is collapsed into a single image template where intensity is a function of recency of motion. Thus, the MHI pixel intensity is a function of the motion history at that location, where brighter values correspond to a more recent motion. Using MHI, moving parts of a video sequence can be engraved with a single image, from where one can predict the motion flow as well as the moving parts of the video action.\n\nSome important features of the MHI representation are:\n\n for each time \"t\"\n"}
{"id": "37927039", "url": "https://en.wikipedia.org/wiki?curid=37927039", "title": "Motive (TV series)", "text": "Motive (TV series)\n\nMotive is a Canadian police procedural drama television series that aired for four seasons on CTV from February 3, 2013, to August 30, 2016. The series premiere had 1.23 million viewers, making it the number one Canadian series premiere of the 2012–13 season.\n\n\"Motive\" is a police procedural drama set in Vancouver, British Columbia, following the investigations of working class single mother Detective Angie Flynn (Kristin Lehman). Each episode reveals the killer and the victim at the beginning; and the rest of the episode details the ongoing investigation, the killer's efforts to cover up the crime, and, via flashbacks, the events leading to the crime. This format is similar to that of the TV series \"Columbo\".\n\n\n\nOn May 31, 2012, CTV ordered 13 episodes from Foundation Features and Lark Productions, to be filmed in Vancouver, British Columbia, from September 17, 2012, to February 26, 2013. The series premiere aired following CTV's broadcast of Super Bowl XLVIII.\n\nOn May 21, 2014, CTV ordered a third season of \"Motive\", which premiered on March 8, 2015.\n\nOn October 5, 2015, CTV announced that \"Motive\" had begun production on the fourth and final season, with star Kristin Lehman directing one of the 13 episodes. Following the events of the third season, Detective Flynn finds herself without a car in the fourth. On April 5, 2016, CTV announced it has partnered with Chevrolet for the final season. Flynn decides on a 2015 Camaro.\n\nStarting in 2013, the series aired for two seasons on ABC in the United States. Following poor second season ratings, ABC did not include \"Motive\" on its 2015 summer schedule. In 2016, USA Network picked up \"Motive\" for its spring schedule. The third season premiered on April 1, 2016, and concluded on June 26, 2016. The fourth season began airing the following week on July 3, 2016, and concluded on September 25, 2016.\n\n"}
{"id": "9346573", "url": "https://en.wikipedia.org/wiki?curid=9346573", "title": "Multiplex ligation-dependent probe amplification", "text": "Multiplex ligation-dependent probe amplification\n\nMultiplex ligation-dependent probe amplification (MLPA) is a variation of the multiplex polymerase chain reaction that permits amplification of multiple targets with only a single primer pair.\n\nEach probe consists of two oligonucleotides which recognize adjacent target sites on the DNA. One probe oligonucleotide contains the sequence recognized by the forward primer, the other contains the sequence recognized by the reverse primer. Only when both probe oligonucleotides are hybridized to their respective targets, can they be ligated into a complete probe. The advantage of splitting the probe into two parts is that only the ligated oligonucleotides, but not the unbound probe oligonucleotides, are amplified. If the probes were not split in this way, the primer sequences at either end would cause the probes to be amplified regardless of their hybridization to the template DNA, and the amplification product would not be dependent on the number of target sites present in the sample DNA. Each complete probe has a unique length, so that its resulting amplicons can be separated and identified by (capillary) electrophoresis. This avoids the resolution limitations of multiplex PCR. Because the forward primer used for probe amplification is fluorescently labeled, each amplicon generates a fluorescent peak which can be detected by a capillary sequencer. Comparing the peak pattern obtained on a given sample with that obtained on various reference samples, the relative quantity of each amplicon can be determined. This ratio is a measure for the ratio in which the target sequence is present in the sample DNA.\n\nVarious techniques including DGGE (Denaturing Gradient Gel Electrophoresis), DHPLC (Denaturing High Performance Liquid Chromatography), and SSCA (Single Strand Conformation Analysis) effectively identify SNPs and small insertions and deletions. MLPA, however, is one of the only accurate, time-efficient techniques to detect genomic deletions and insertions (one or more entire exons), which are frequent causes of cancers such as hereditary non-polyposis colorectal cancer (HNPCC), breast, and ovarian cancer. MLPA can successfully and easily determine the relative copy number of all exons within a gene simultaneously with high sensitivity.\n\nAn important use of MLPA is to determine relative ploidy. For example, probes may be designed to target various regions of chromosome 21 of a human cell. The signal strengths of the probes are compared with those obtained from a reference DNA sample known to have two copies of the chromosome. If an extra copy is present in the test sample, the signals are expected to be 1.5 times the intensities of the respective probes from the reference. If only one copy is present the proportion is expected to be 0.5. If the sample has two copies, the relative probe strengths are expected to be equal.\n\nDosage quotient analysis is the usual method of interpreting MLPA data. If a and b are the signals from two amplicons in the patient sample, and A and B are the corresponding amplicons in the experimental control, then the dosage quotient DQ = (a/b) / (A/B). Although dosage quotients may be calculated for any pair of amplicons, it is usually the case that one of the pair is an internal reference probe.\n\nMLPA facilitates the amplification and detection of multiple targets with a single primer pair. In a standard multiplex PCR reaction, each fragment needs a unique amplifying primer pair. These primers being present in a large quantity result in various problems such as dimerization and false priming. With MLPA, amplification of probes can be achieved. Thus, many sequences (up to 40) can be amplified and quantified using just a single primer pair. MLPA reaction is fast, inexpensive and very simple to perform.\n\nMLPA has a variety of applications including detection of mutations and single nucleotide polymorphisms, analysis of DNA methylation, relative mRNA quantification, chromosomal characterisation of cell lines and tissue samples, detection of gene copy number, detection of duplications and deletions in human cancer predisposition genes such as BRCA1, BRCA2, hMLH1 and hMSH2 and aneuploidy determination. MLPA has potential application in prenatal diagnosis both invasive and noninvasive.\n\n"}
{"id": "28443310", "url": "https://en.wikipedia.org/wiki?curid=28443310", "title": "NAP of the Americas", "text": "NAP of the Americas\n\nNetwork Access Point (NAP) of the Americas (also called MI1) is a massive, six-story, 750,000 square foot data center and Internet exchange point in Miami, Florida, operated by Equinix.\n\nThe facility is home to 160 network carriers and is a pathway for data traffic from the Caribbean and South and Central America to more than 148 countries.\n\nThe NAP of the Americas is built 32 feet above sea level and is designed to withstand Category 5 hurricane-level winds. It serves as a relay for the U.S. Department of State's Diplomatic Telecommunications Service.\n\n\n"}
{"id": "15726046", "url": "https://en.wikipedia.org/wiki?curid=15726046", "title": "Natural mapping (interface design)", "text": "Natural mapping (interface design)\n\nThe term natural mapping comes from proper and natural arrangements for the relations between controls and their movements to the outcome from such action into the world. The real function of natural mappings is to reduce the need for any information from a user’s memory to perform a task. This term is widely used in the areas of human-computer interaction (HCI) and interactive design.\n\nMapping and natural mapping are very similar in that they are both used in relationship between controls and their movements and the result in the world. The only difference is that natural mapping provides users with properly organized controls for which users will immediately understand which control will perform which action.\n\nA simple design principle:\n\nConsider, by way of example, the use of labelling on kitchen stoves with different arrangements of burners and controls.\n\nIn the above case, an arbitrary arrangement of controls, such as controls in a row, even though the burners are arranged in a rectangle, thereby visually frustrating the inexperienced user, leading to a period of experimenting with the controls to become familiar with the proper usage, and potential danger to the user.\n\nIn the stove metaphor there is an illustration of placement in relation to the controls; however, the effect of the control in relation to its operation is Heat as a result of Rotation. Rotation does not naturally relate to heat, therefore the relationship is artificial, and a social construction. A better example would be the simple one of a privacy bolt on a toilet stall. A simple slide bolt with a knob has a very direct mapping, whereas, one with a rotating lever requires the understanding of the transformation of the rotation translated into the movement of the bolt horizontally. From this perspective, mapping is a characteristic of affordance. A deeper understanding of many our perceived 'natural' mapping relationships uncovers a predominately socially constructed, or cultural, underpinning. Such as rotating a volume knob to make the music volume go 'up'.\n\n\n"}
{"id": "13029305", "url": "https://en.wikipedia.org/wiki?curid=13029305", "title": "Noble Corporation", "text": "Noble Corporation\n\nNoble Corporation plc is an offshore drilling contractor organized in London, United Kingdom. Its affiliate, Noble Corporation, is organized in the Cayman Islands. It is the corporate successor of Noble Drilling Corporation.\n\nThe company operates 28 drilling rigs including 8 drillships, 6 Semi-submersible platforms and 14 jackup rigs.\n\nIn 2017, revenues from Royal Dutch Shell accounted for 45.0% of the company's revenues, revenues from Statoil accounted for 13.2% of the company's revenues, and revenues from Saudi Aramco accounted for 11.4% of the company's revenues.\n\nIn 1985, Noble Affiliates, Inc. (now Noble Energy), distributed its shares in Noble Drilling Corporation to its shareholders via a corporate spin-off and Noble Affiliates began publicly trading on the NASDAQ.\n\nIn 2002, the company underwent a restructuring whereby it moved its domicile to the Cayman Islands and established Noble Corporation as the parent holding company.\n\nIn early 2009, the company moved its domicile from the Cayman Islands to Switzerland due to the potential for more U.S. taxes on Caribbean tax havens. In 2013, the company moved to the United Kingdom.\n\nIn 2010, the company acquired Frontier Drilling in a $2.16 billion cash transaction.\n\nThe company was added to the S&P 500 Index in 2011 but was removed from the index in 2015.\n\nIn 2014, the company distributed its interests in Paragon Offshore plc to its shareholders.\n\nIn 2012, the Noble Discoverer drillship, operating under contract for Royal Dutch Shell lost its mooring and drifted close to shore. There were no injuries or environmental damage reported as a result of the accident.\n\nIn 2017, an employee went missing on the Noble Lloyd Noble drilling rig.\n"}
{"id": "2182600", "url": "https://en.wikipedia.org/wiki?curid=2182600", "title": "Party hat", "text": "Party hat\n\nA party hat is generally a playful conical hat made with a rolled up piece of thin cardboard, usually with designs printed on the outside and a long string of elastic acting like a chinstrap, going from one side of the cone's bottom to another to secure the cone to the person's head. Its name originates with its use: Party hats are worn most often at birthday parties, especially by the guest of honor, with a significant minority being worn for New Year celebrations. In Britain the hat is made of paper and is the shape of a crown, and is most typically worn during a Christmas dinner. The party hat has its origins in the dunce cap worn by misbehaving or poorly performing schoolchildren from the mid-19th century to the early 20th century, with its festive decoration and society's positive attitude toward the wearer indicating a relaxation, abrogation, or even reversal of certain social norms: During the occasion in question, the wearer is permitted or encouraged to engage, rather than discouraged from engaging, in frivolous and foolish behavior for which the required wearing of the dunce cap would in other situations constitute a punishment.\nParty hats have originated in England \n\nNon-conical hats worn to signify an occasion's informal and festive status include decorated top hats, hats made from balloons, the beer hat or \"beer helmet\" (invented in 1983 by Buffalo Bills fan Jeremy Gumbo), and Mickey Mouse ears. In more extreme cases, partygoers may wear other objects such as lampshades or beer boxes, although the wearing of such objects often meets with social disapproval.\n\n\n"}
{"id": "1071026", "url": "https://en.wikipedia.org/wiki?curid=1071026", "title": "Poudre B", "text": "Poudre B\n\nPoudre B was the first practical smokeless gunpowder. It was perfected between 1882 and 1884 at \"Laboratoire Central des Poudres et Salpêtres\" in Paris, France. Originally called \"Poudre V\", from the name of the inventor, Paul Vieille, it was arbitrarily renamed \"Poudre B\" (short for \"poudre blanche\" -- white powder, as distinguished from black powder) to distract German espionage. \"Poudre B\" is made from 68.2% insoluble nitrocellulose, 29.8% soluble nitrocellulose gelatinized with ether and 2% paraffin. \"Poudre B\" is made up of very small paper-thin flakes that are not white but dark greenish grey in color. \"Poudre B\" was first used to load the 8mm Lebel cartridges issued in 1886 for the Lebel rifle.\n\nFrench chemist Paul Vieille had followed the findings of German-Swiss chemist Christian Friedrich Schonbein, who had created the explosive nitrocellulose or \"guncotton\" in 1846 by treating cotton fibers with a nitric acid and sulphuric acid mixture. However guncotton, an explosive substance, proved to be too fast burning at the time for direct use in firearms and artillery ammunition. Then Paul Vieille went one step further in 1882-84 and, after many trials and errors, succeeded in transforming guncotton into a colloidal substance by gelatinizing it in an alcohol-ether mixture which he then stabilized with amyl alcohol. He then used roller presses to transform this gelatinized colloidal substance into extremely thin sheets which, after drying, were cut up into small flakes. This single-base smokeless powder was originally named \"Poudre V\" after the inventor's name. That denomination was later changed arbitrarily to \"Poudre B\" in order to distract German espionage. The original \"Poudre B\" of 1884 was almost immediately replaced by improved \"Poudre BF(NT)\" in 1888. In 1896 \"Poudre BF(NT)\" was replaced by \"Poudre BF(AM)\" which was followed by \"Poudre BN3F\" in 1901. The latter was stabilized with the antioxidant diphenylamine instead of amyl alcohol and it gave safe and regular performance as the standard French gunpowder used during World War I (1914–1918). It was followed during the 1920s by \"Poudre BN3F(Ae)\" and later by \"Poudre BPF1\", which remained in service until the 1960s.\n\nThree times more powerful than black powder for the same weight, and not generating large quantities of smoke, \"Poudre B\" gave the user a huge tactical advantage. It was hastily adopted by the French military in 1886, followed by all the major military powers within a few years.\n\nPrior to its introduction, a squad of soldiers firing volleys would be unable to see their targets after a few shots, while their own location would be obvious because of the cloud of smoke hanging over them. The higher power of the new powder gave a higher muzzle velocity, which in turn produced a flatter bullet trajectory and thus a longer range. It also required lesser volumes of gunpowder and allowed a smaller caliber, thus lighter bullets, so a soldier could carry more ammunition. The French Army quickly introduced a new rifle, the Lebel Model 1886 firing a new 8 mm calibre cartridge, to exploit these benefits.\n\nThe earliest \"Poudre B\" tended to eventually become unstable, which has been attributed to evaporation of the volatile solvents, but may also have been due to the difficulty in fully removing the acids used to make guncotton. In the early years of their use both the original Poudre B and guncotton led to accidents. For example, two French battleships, the \"Iéna\" and the \"Liberté\", blew up in Toulon harbour in 1907 and 1911 respectively with heavy loss of life. By the end of the 1890s, safer smokeless powders had been developed, including improved and stabilized versions of \"Poudre B\" (e.g. Poudres BN3F and BPF1), ballistite and cordite. The guncotton problem is not completely solved even today, as an occasional batch of smokeless powder will still deteriorate, although this is extremely rare.\n"}
{"id": "797121", "url": "https://en.wikipedia.org/wiki?curid=797121", "title": "Powel Crosley Jr.", "text": "Powel Crosley Jr.\n\nPowel Crosley Jr. (September 18, 1886 – March 28, 1961) was an American inventor, industrialist, and entrepreneur. He was also a pioneer in radio broadcasting, and a former owner of the Cincinnati Reds major league baseball team. In addition, Crosley's companies manufactured Crosley automobiles and radios, and operated WLW radio station. Crosley, once dubbed \"The Henry Ford of Radio,\" was inducted into the Automotive Hall of Fame in 2010 and the National Radio Hall of Fame in 2013.\n\nHe and his brother, Lewis M. Crosley, were responsible for many firsts in consumer products and broadcasting. During World War II, Crosley's facilities produced more proximity fuzes than any other U.S. manufacturer, and made several production design innovations. Crosley Field, a stadium in Cincinnati, Ohio, was renamed for him, and the street-level main entrance to Great American Ball Park in Cincinnati is named Crosley Terrace in his honor. Crosley's Pinecroft estate home in Cincinnati, Ohio, and Seagate, his former winter retreat in Sarasota, Florida are listed in the National Register of Historic Places.\n\nPowel Crosley Jr. was born on September 18, 1886, in Cincinnati, Hamilton County, Ohio, to Charlotte Wooley (Utz) (1864–1949) and Powel Crosley Sr. (1849–1932), a lawyer. Powel Jr. was the oldest of the family's four children that included a brother, Lewis, and two sisters, Charlotte and Edythe. Crosley became interested in the mechanics of automobiles at a young age and wanted to become an automaker. While living with his family in College Hill, a suburb of Cincinnati, twelve-year-old Crosley made his first attempt at building a vehicle.\n\nCrosley began high school in College Hill and transferred to the Ohio Military Institute. In 1904 Crosley enrolled at the University of Cincinnati, where he began studies in engineering, but switched to law, primarily to satisfy his father, before dropping out of college in 1906 after two years of study.\n\nCrosley married Gwendolyn Bakewell Aiken (1889–1939) in Hamilton County, Ohio, on October 17, 1910. They had two children, Powel Crosley III (1911–1948) and Martha Page Crosley (1912–1994). After his marriage, Crosley continued to work in automobile sales in Muncie to earn money to buy a house, while his wife returned to Cincinnati to live with her parents. The young couple saw each other on the weekends until Crosley returned to Cincinnati in 1911 to live and work after the birth of his first child. Gwendolyn Crosley, who suffered from tuberculosis, died at the Crosleys' winter home in Sarasota, Florida, on February 26, 1939.\n\nCrosley married Eva Emily Brokaw (1912–1955) in 1952. She died in Cincinnati, Ohio.\n\nCrosley's primary residence was Pinecroft, an estate home built in 1929 in the Mount Airy section of Cincinnati, Ohio. He also had Seagate, a winter retreat in Manatee County, Florida, built for his first wife, Gwendolyn. In addition, Crosley owned several vacation properties.\n\nPinecroft, Crosley's two-story, , Tudor Revival-style mansion and other buildings on his estate in Mount Airy was designed by New York-based architect Dwight James Baum and built in 1928–29. Crosley's daughter, Marth Page (Crosley) Kess, sold the property after her father's death in 1961, and the Franciscan Sisters of the Poor acquired the property in 1963. Saint Francis Hospital bought a portion of the property north of the Crosley mansion in 1971 and built a hospital, which was renamed Mercy Hospitals West in 2001. The land surrounding the home has been subdivided into parcels, but the Franciscan Sisters have used the mansion as a retreat since the early 1970s. Pinecroft was added to the National Register of Historic Places in 2008.\n\nSeagate, also known as the Bay Club, along Sarasota Bay in the southwest corner of Manatee County, Florida, was a Mediterranean Revival-style home designed for Crosley by New York City and Sarasota architect George Albree Freeman Jr., with Ivo A. de Minicis, a Tampa, Florida, architect, drafting the plans. Sarasota contractor Paul W. Bergman built the winter retreat in 1929-30 on a parcel of land. The two-and-a-half-story house include ten bedrooms and ten bathrooms, as well as auxiliary garages and living quarters for staff. The house contains and is reportedly the first residence built in Florida using steel-frame construction to provide protection against fires and hurricanes. After Crosley's wife, Gwendolyn, died of tuberculosis at the retreat in 1939, he rarely used the house. During World War II, Crosley allowed the U.S. Army Air Corps to use the retreat its airmen training at the nearby Sarasota Army Air Base. Crosley sold his estate property in 1947 to the D and D Corporation.\n\nMabel and Freeman Horton purchased the property in 1948 and owned Seagate for nearly forty years. The house and was added to the National Register of Historic Places on January 21, 1983, by a subsequent owner who intended to build an exclusive condominium project on the site using the historic house as a clubhouse, but the project failed when the economy faltered shortly thereafter. Kafi Benz, the Friends of Seagate Inc., a nonprofit corporation, and local residents saved Seagate from commercial development, and initiated a campaign for its preservation and public acquisition. In 1991 the state of Florida purchased the property and of the bay-front estate that included the structures that Crosley had built in 1929-30. A larger portion of the original property was developed into a satellite campus for the University of South Florida. The University of South Florida Sarasota-Manatee campus opened its new facilities in August 2006. The present-day mansion, called the Powel Crosley Estate, is used as a meeting, conference, and event venue.\n\nCrosley, an avid sportsman, also owned several sports, hunting, and fishing camps, including an island retreat called Nikassi on McGregor Bay, Lake Huron, Canada; Bull Island, South Carolina; Pimlico Plantation, along the Cooper River north of Charleston, South Carolina; Sleepy Hollow Farm, a retreat in Jennings County, Indiana and a house at Cat Cays, Bahamas.\n\nCrosley began work selling bonds for an investment banker; however, at the age of twenty-one he decided to pursue a career in automobile manufacturing. The mass-production techniques employed by Henry Ford also caught his attention and would be implemented by his brother, Lewis, when the two began manufacturing radios in 1921.\n\nIn 1907 Crosley formed a company to build the Marathon Six, a six-cylinder model priced at $1,700, which was at the low end of the luxury car market. With $10,000 in capital that he raised from investors, Crosley established Marathon Six Automotive inexpensive automobile, in Connersville, Indiana, and built a prototype of his car, but a nationwide financial panic caused investment capital to dwindle and he failed to fund its production.\n\nStill determined to establish himself as an automaker, Crosley moved to Indianapolis, Indiana, where he worked for Carl G. Fisher as a shop hand at the Fisher Automobile Company. Crosley stayed for about a year, but left after he broke his arm starting a car at the auto dealership. After recovering from his injury at home in College Hill, Crosley returned to Indianapolis in 1909 to briefly work for several auto manufacturers, including jobs as an assistant sales manager for the Parry Auto Company and a salesman for the National Motor Vehicle Company. He also volunteered to help promote National's auto racing team. His next job was selling advertising for \"Motor Vehicle\", an automotive trade journal, but left in 1910 to move to Muncie, Indiana, where he worked in sales for the Inter-State Automobile Company and promoted its racing team.\n\nAfter returning to Cincinnati, Ohio, in 1911, Crosley sold and wrote advertisements for local businesses, but continued to pursue his interests in the automobile industry. He \nfailed in early efforts to manufacture cars for the Hermes Automobile Company and cyclecars for the De Cross Cyclecar Company and the L. Porter Smith and Brothers Company before finding financial success in manufacturing and distributing automobile accessories.\n\nIn 1916 he co-founded the American Automobile Accessory Company with Ira J. Cooper. The company's bestseller was a tire liner of Crosley's invention. Another popular product was a flag holder that held five American flags and clamped to auto radiator caps. By 1919 Crosley had \nsales of more than $1 million in parts. He also diversified into other consumer products such as phonograph cabinets, radios, and home appliances. Crosley's greatest strength was his ability to invent new products, while his brother, Lewis M. Crosley, excelled in business.Lewis also became head of Crosley's manufacturing operations.\n\nIn 1920, Crosley first selected independent local dealers as the best way to take his products to market. He insisted that all sellers of his products must give the consumer the best in parts, service, and satisfaction. Always sensitive to consumers, his products were often less expensive than other name brands, but were guaranteed. Crosley's \"money back guarantee\" set a precedent for some of today's most outstanding sales policies.\n\nFor his contributions to early radio manufacturing, Crosley was once dubbed \"The Henry Ford of Radio.\" In 1921 Crosley's young son asked for a radio, a new item at that time, but Crosley was surprised that toy radios cost more than $100 at a local department store. With the help of a booklet called \"The ABC of Radio,\" he and his son decided to assemble the components and build their own crystal radio set. Crosley immediately recognized the appeal of an inexpensive radio and hired two University of Cincinnati students to help design a low-cost set that could be mass-produced. Crosley named the radio the \"Harko\" and introduced it to the market in 1921. The inexpensive radio set sold for $7, making it affordable to the masses. Soon, the Crosley Radio Corporation was manufacturing radio components for the rapidly growing industry and making its own line of radios.\n\nBy 1924 Crosley had moved his company to a larger plant and later made subsequent expansions. The Crosley Radio Corporation became the largest radio manufacturer in the world in 1925; its slogan, \"You’re There With A Crosley,\" was used in all its advertising.\n\nIn 1925 Crosley introduced another low-cost radio set. The small, one-tube, regenerative radio was called the \"Crosley Pup\" and sold for $9.75. While Victor had Nipper, its famous trademark showing a dog listening to \"his master's voice\" from a phonograph, Crosley adopted a mascot in the form of a dog with headphones listening to a Crosley Pup radio\nIn 1928 Crosley's firm arranged for the construction of the Crosley Building at Camp Washington, a Cincinnati neighborhood, and used the facility for its for radio manufacturing, radio broadcasting, and for manufacturing other devices.\n\nIn 1930 Crosley was marketing the \"Roamio,\" with \"screen grid neutrodyne power speaker\" for automotive use. Priced at $75, before accessories and installation, it was claimed to be able to receive thirty stations with no signal strength change.\n\nOnce Crosley established himself as a radio manufacturer, he decided to expand into broadcasting as a way to encourage consumers to purchase more radios. In 1921, soon after he built his first radios, Crosley began experimental broadcasts from his home with a 20-watt transmitter using the call sign 8CR. On March 22, 1922, the Crosley Broadcasting Corporation received a commercial license to operate as WLW at 50 watts. Dorman D. Israel, a young radio engineer from the University of Cincinnati, designed and built the station's first two radio transmitters (at 100 and 1000 watts). The Crosley Corporation claimed that in 1928 WLW became the first 50-kilowatt commercial station in the United States with a regular broadcasting schedule. In 1934 Crosley put a 500-kilowatt transmitter on the air, making WLW the station with the world's most powerful radio transmitter for the next five years. (On occasion, the station's power was boosted as high as 700,000 watts.)\n\nThroughout the 1930s, Cincinnati's WLW was considered \"the Nation's Station,\" producing many hours of network programming each week. Among the entertainers who performed live from WLW's studios were Red Skelton, Doris Day, Jane Froman, Fats Waller, Rosemary Clooney, and the Mills Brothers. Crosley's station also developed some of the earliest radio \"soap operas\" with Procter and Gamble as an initial sponsor. In 1939 the Federal Communications Commission (FCC) ruled that WLW had to reduce its power to 50 kilowatts, partly because it interfered with the broadcasts of other stations, but largely due to its smaller competitors, who complained about the station's technical and commercial advantages with its 500-kilowatt broadcasts.\n\nDuring World War II, WLW resumed its powerful, 500-kilowatt transmissions in cooperation with the U.S. government. WLW's broadcasts from Ohio could be heard as far away as South America and Europe. The 500-kilowatt transmitter was crated for shipment to Asia, but the war ended before it was shipped. WLW's engineers also built high-power shortwave transmitters on a site about north of Cincinnati. Crosley Broadcasting, under contract to the U.S. government, began operating the Bethany Relay Station, which was dedicated on September 23, 1944, to broadcast \"Voice of America\" programming. The relay station's broadcasts continued until 1994.\n\nCrosley's broadcasting company eventually expanded into additional markets. The company was experimenting with television broadcasting as early as 1929, when it received an experimental television license from the Federal Radio Commission (FRC), which later became the FCC. Crosley Broadcasting did not go on-air with regular television programming as WLWT-TV until after Crosley sold the company to Aviation Corporation (Avco) and he had become a member of Avco's board of directors..\n\nIn the 1930s Crosley added refrigerators and other household appliances and consumer goods to his company's product line. Because he had invested in his own businesses instead of the stock market, Crosley was better able than many other industrialists to keep his employees working and his products available to the public during the Great Depression.\n\nCrosley's \"Icyball\" was an early non-electrical refrigeration device. The unit used an evaporative cycle to create cold, and had no moving parts. The dumbbell shaped unit was \"charged\" by heating one end with a small kerosene heater. Crosley's company sold several hundred thousand Icyball units before discontinuing its manufacture in the late 1930s.\n\nIn 1932 Crosley had the idea of putting shelves in the doors of refrigerators. He patented the \"Shelvador\" refrigerator and launched the new appliance in 1933. At that time it was the only model with shelves in the door. In addition to refrigerators, Crosley's company sold other consumer products that included the \"XERVAC,\" a device purported to \"revitalize inactive hair cells\" and \"stimulate hair growth\". Crosley also introduced the \"Autogym,\" a motor-driven weight-loss device with a vibrating belt, and the \"Go-Bi-Bi,\" a \"rideable baby walker,\" among other products.\n\nIn February 1934, Crosley purchased the Cincinnati Reds professional baseball team from Sidney Weil, who had lost much of his wealth after the Wall Street Crash of 1929. Crosley kept the team from going bankrupt and leaving Cincinnati. He was also owner of the Reds when the team won two National League titles (in 1939 and 1940) and the World Series in 1940.\n\nCrosley was also a pioneer in broadcasting baseball games on the radio. On May 24, 1935, the first nighttime game in baseball history was held at Cincinnati's Crosley Field, which was renamed in Crosley's honor after he acquired the team, between the Cincinnati Reds and Philadelphia Phillies under newly-installed electric lighting. With attendance at its evening games more than four times greater that its daytime events, the team's financial position was greatly improved. Crosley also approved baseball's first regularly-scheduled play-by-play broadcasts of all scheduled games on his local station, WKRC. The coverage increased attendance so much that within five years all sixteen major league teams had radio broadcasts of every scheduled game.\n\nOn a personal level, Crosley was an avid sportsman. Although he never had a pilot's license, Crosley owned several seaplanes, such as the Douglas Dolphin, and airplanes, including building five Crosley \"Moonbeam\" airplanes. In addition, Crosley claimed that at one time he was slotted to be a driver in the Indianapolis 500, but the claim that was not entirely accurate. He was entered but broke his arm working for Carl Fisher (see above).</ref></ref>. Crosley was also the owner of luxury yachts with powerful engines, and an active fisherman who participated in celebrated tournaments in Sarasota, Florida. He served as president of the Sarasota area's Anglers Club and was a founder of the American Wildlife Institute. Crosley owned several sports, hunting, and fishing camps: Nikassi, an island retreat in Ontario, Canada; Bull Island off the coast of South Carolina; a hunting retreat he called Sleepy Hollow Farm in Jennings County, Indiana, and a Caribbean vacation home at Cat Cays, Bahamas.\n\nThe Crosley \"Moonbeam\" was built in Sharonville, Ohio and was first flown on December 8, 1929. It was designed by Harold D. Hoekstra, an employee of Crosley's when Crosley was president of the Crosley Aircraft Company. (Hoeskstra later became Chief of Engineering and Design for the Federal Aviation Administration.) Unique features of this aircraft are the square tube longerons used in the fuselage construction, use of torque tubes instead of control cable, and the corrugated aluminum ailerons. Original power was supplied by a four-cylinder inverted inline 90 hp Crosley engine. At one time it was also tested with a 110 Warner Scarab engine. N147N reportedly was the first airplane on which the spoilers were tested (in May 1930) as a lateral control device. Five Moonbeams airplanes were produced. The first was a three-place parasol; next, a four-place, high wing cabin model; third and fourth were one place high wings. Due to the Great Depression, planned production did not take place. N147N is the last of these planes in existence. It is housed at the Aviation Museum of Kentucky in Lexington Kentucky.\n\nIn 1933 Frenchman Henri Mignet designed the HM.14 \"Pou du Ciel\" (\"Flying Flea\"). He envisioned a simple aircraft that amateurs could build, and even teach themselves to fly. In an attempt to render the aircraft stall proof and safe for amateur pilots to fly, Mignet staggered the two main wings. The Mignet-Crosley \"Pou du Ciel\" is the first HM.14 made and flown in the United States. Edward Nirmaier, a Crosley employee, and two other men built the airplane in November 1935 for Crosley, who believed that the affordable \"Flea\" could become a popular aircraft in the United States. After several flights, a crash at the Miami Air Races in December 1935 finally grounded the Crosley HM.14. Although the airplane enjoyed a period of intense popularity in France and England, but a series of accidents in 1935-36 permanently ruined the airplane's reputation.\n\nOf all Crosley's dreams, success at building an affordable automobile for Americans was possibly the only major one eventually to elude him. In the years leading up to World War II, Crosley developed new products that included reviving one of his earliest endeavors at automobile design and manufacturing. In 1939, when Crosley introduced the low-priced Crosley automobiles, he broke with tradition and sold his cars through independent appliance, hardware, and department stores instead of automobile dealerships.\n\nThe first Crosley Motors, Inc. automobile made its debut at the Indianapolis Motor Speedway on April 28, 1939, to mixed reviews. The compact car had an wheelbase and a , \ntwo-cylinder, air cooled Waukesha engine. Crosley estimated that his cloth-top car, which weighed less than , could get fifty miles per gallon at speeds of up to fifty miles per hour. The sedan model sold for $325, while the coupe sold for $350. Panel truck and pickup truck models were added to the product line in 1940. During the pre-war period, the company had manufacturing plants in Camp Washington, Ohio; Richmond, Indiana; and Marion, Indiana. When the onset of war ended all automobile production in the United States in 1942, Crosley had produced 5,757 cars.\n\nAfter World War II ended, Crosley resumed building its small cars for civilian use. His company's first post-war automobile rolled off the assembly line on May 9, 1946. The new Crosley \"CC\" model automobile continued the company's pre-war tradition of offering small, lightweight, and low-priced cars. It sold for $850 and got thirty to fifty miles per U.S. gallon. In 1949 Crosley became the first American carmaker to put disc brakes on all of its models.s\n\nUnfortunately for Crosley, fuel economy ceased to be an inducement after gas rationing ended, and American consumers also began to prefer bigger cars. Crosley's best year was 1948, when it sold With 24,871 cars, but sales began to fall in 1949. Adding the Crosley \"Hotshot\" sports model and an all-purpose vehicle called the \"Farm-O-Road\" model in 1950 did not stop the decline. Only 1,522 Crosley vehicles were sold in 1952. Crosley sold about 84,000 cars before closing down the operation on July 3, 1952. The Crosley plant in Marion, Indiana, was sold to the General Tire and Rubber Company.\n\nCrosley's company was involved in war production planning before December 1941, and like the rest of American industry, it focused on manufacturing war-related products during World War II. The company made a variety of products,including proximity fuzes, experimental military vehicles, radio transceivers, and gun turrets, among other items.\n\nThe most significant Crosley's wartime production was the proximity fuze, which was manufactured by several companies for the military. Crosley's facilities produced more fuzes than any other manufacturer and made several production design innovations. The fuze is widely considered the third most important product development of the war years, ranking behind the atomic bomb and radar.\n\nIronically, Crosley himself did not have U.S. government security clearance and was not involved with the project. Without government security clearance, Crosley was prohibited from entering the area of his plant that manufactured the fuzes and did not know what top-secret products it produced until the war's end. Production was directed and supervised by Lewis M. Clement, the Crosley company's vice-president of engineering.\n\nJames V. Forrestal, U.S. Secretary of the Navy said: \"The proximity fuze has helped blaze the trail to Japan. Without the protection this ingenious device has given the surface ships of the Fleet, our westward push could not have been so swift and the cost in men and ships would have been immeasurably greater.\" George S. Patton, Commanding General of the Third Army, remarked: \"The funny fuze won the Battle of the Bulge for us. I think that when all armies get this shell we will have to devise some new method of warfare.\"\n\nAlso of significance were the many radio transceivers that Crosley's company manufactured during the war, including 150,000 BC-654s, a receiver and transmitter that was the main component of the SCR-284 radio set. The Crosley Corporation also made components for Walkie-talkie transceivers and IFR radio guidance equipment, among other products. In addition, Crosley's also manufactured field kitchens, air supply units for Sperry S-1 bombsites (used in B-24 bombers), air conditioning units, Martin PBM Mariner bow-gun turrets, and quarter-ton trailers. Gun turrets for PT boats and B-24 and B-29 bombers were the company's largest military contract.\n\nDuring the war, Crosley's auto manufacturing division, CRAD (for Crosley Radio Auto Division), in Richmond, Indiana, produced experimental motorcycles, tricycles, four-wheel-drive vehicles, and continuous track vehicles, including some amphibious models. All of these military prototypes were powered by the two-cylinder boxer engine that had powered the original Crosley automobile. Crosley had nearly 5,000 of these engines on hand when civilian automobile production ceased in 1942, and hoped to put them to use in his miniature war machines.\n\nOne vehicle prototype was the 1942/1943 Crosley CT-3 \"Pup,\" a lightweight, single-passenger, four-wheel-drive vehicle that was transportable and air-droppable from a C-47 Skytrain. Six of the Pups were deployed overseas after undergoing tests at Fort Benning, Georgia, but the Pup project was discontinued due to several weak components. Seven of the thirty-seven Pups that were built are known to survive.\n\nAlthough Crosley retained ownership of the Cincinnati Reds baseball team and Crosley Motors, he sold his other business interests, including WLW radio and the Crosley Corporation, to the Aviation Corporation (Avco) in 1945. Crosley remained on the Avco board for several years afterward. Avco put Ohio's second television station, WLWT-TV, on the air in 1948, the same year it began manufacturing television sets. Avco manufactured some of the first portable television sets under the Crosley brand name. Crosley ceased to exist as a brand in 1956, when Avco closed the unprofitable product line; however, the Crosley name was so well established that Avo's broadcasting division, owener of WLWT-TV, retained the Crosley name until 1968, seven years after Crosley's death.\n\nCrosley sold Pimlico Plantation, now demolished, in 1942, and Seagate, his winter retreat in Florida in 1947. In 1954 Crosley sold his vacation home at Cat Keys, Bahamas. In 1956 he sold Sleepy Hollow Farm in Jennings County, Indiana, to the state of Indiana for use as a wildlife preserve. Bull Island, South Carolina, became part of a national wildlife refuge. It is not known when Crosley sold his vacation retreat in Ontario, Canada.\n\nCrosley died on March 28, 1961, of a heart attack at the age of 74. He is buried at Spring Grove Cemetery in Cincinnati, Hamilton County, Ohio.\n\nCrosley liked to label himself \"the man with 50 jobs in 50 years,\" a catchy sobriquet that was far from true, although he did have more than a dozen jobs before he got into automobile accessories. Crosley helped quite a few inventors up the ladder of success by buying the rights to their inventions and sharing in the profits. His work provided employment and products for millions of people.\n\nA few of Crosley's company's more noteworthy accomplishments:\n\nPart of Crosley's Pinecroft estate, his former Cincinnati, Ohio, home, is the site of Mercy Hospitals West; however, the Franciscan Sisters of the Poor have used his mansion as a retreat since the early 1970s. Seagate, Crosley's former winter retreat on Sarasota Bay in Florida, is operated as an event rental facility. Pinecroft and Seagate have been restored and are listed in the National Register of Historic Places. Crosley's farm in Jennings County, Indiana, is the site of the present-day Crosley Fish and Wildlife Area; Bull Island, South Carolina, is part of the Cape Romain National Wildlife Refuge.\n\nWLW radio continues to operate as an AM station. Crosley's manufacturing plants in Richmond and Marion, Indiana, are still standing, but they no longer produce automobiles. In 1973 a group of Avco executives purchased the Evendale, Ohio, operation of AVCO Electronics Division, a successor to one of Crosley's business ventures, and renamed it the Cincinnati Electronics Corporation. The company manufactured a broad range of sophisticated electronic equipment for communications and space, infrared and radar, and electronic warfare, among others. Since its creation in 1973, Cincinnati Electronics has been acquired by a handful of companies, including GEC Marconi (1981), BAE Systems (1999), CMC Electronics (2001), and L-3 Communications (2004–present).\n\nThe present-day Crosley Corporation is not connected to Crosley. An independent appliance distributor formed the current company after purchasin the rights to the name from Avco in 1976. Its appliances are manufactured mostly in North America by Electrolux and Whirlpool Corporation. Crosley-branded, top-loading washing machines are made by the Whirlpool at its plant in Clyde, Ohio. In 1984 Modern Marketing Concepts, one of the leading U.S. manufacturers of vintage-styled turntables, radios, and other audio electronics, reintroduced Crosley brand name fors its Crosley Radio.\n\nCrosley's automobiles and experimental military vehicles are in the collections of several museums. Crosleys are also sought-after vehicles by vintage auto collectors. The Crosley company's Bonzo promotional items and Crosley Pup radios have become valuable as collectibles. A papier mâché Crosley Bonzo is on display at the Smithsonian Institution in Washington, D.C.\n\n\n\n"}
{"id": "27323812", "url": "https://en.wikipedia.org/wiki?curid=27323812", "title": "Project MKOFTEN", "text": "Project MKOFTEN\n\nProject MKOFTEN was a covert Department of Defense program developed in conjunction with the CIA. A partner program to MKSEARCH, the goal of MKOFTEN was to \"test the behavioral and toxicological effects of certain drugs on animals and humans\".\n\nAccording to author Gordon Thomas's 2007 book \"Secrets and Lies\" the CIA's Operation Often was also initiated by the chief of the CIA's Technical Services Branch, Dr. Sidney Gottlieb, to \"explore the world of black magic\" and \"harness the forces of darkness and challenge the concept that the inner reaches of the mind are beyond reach\". As part of Operation Often, Dr. Gottlieb and other CIA employees visited with and recruited fortune-tellers, palm-readers, clairvoyants, astrologists, mediums, psychics, specialists in demonology, witches and warlocks, Satanists, other occult practitioners, and more.\n\n"}
{"id": "54568728", "url": "https://en.wikipedia.org/wiki?curid=54568728", "title": "RigExpert", "text": "RigExpert\n\nRig Expert Ukraine Ltd is a manufacturer of HAM and PMR Two-way radio RF antenna analysis and antenna tuning equipment. The company was founded in 2004 and is headquartered in Kyiv, Ukraine.\nThe AA-30, AA-54 & AA-170 are almost the same product except for the frequency range. Similarly, the AA-600, AA-1000 & AA-1400 are the same product except for the different frequency range.\n\nAA-200, AA-230, AA-230PRO, AA-500 & AA-520 are discontinued products and are no longer in production\n\n\n"}
{"id": "57929830", "url": "https://en.wikipedia.org/wiki?curid=57929830", "title": "SafeCharge International", "text": "SafeCharge International\n\nSafeCharge International is a global payments technology company that provides omnichannel payment services, fraud prevention solutions and connection to payment methods. The company counts 340+ employees and has offices in Israel, UK, the Netherlands, Hong Kong, Italy, Austria, Cyprus, Bulgaria, Singapore and in the US.\n\nIt was co-founded by Teddy Sagi and current CEO, David Avgi in 2006.\n\nThe company began trading on the London Stock Exchange in 2014. At that time, SafeCharge International was had a revenue of $43.18 million, and it raised $126 million from public offerings with its IPO pricing . That was the second largest IPO out of the main Israeli companies going public in 2014. Since the company went public , its performance shows a steady growth.\n\nSafeCharge International expanded to the Asian markets via a strategic partnership with 2C2P in 2017, and working with WeChat Pay in London’s Camden market. The company was authorized as a payment institution by the UK Financial Conduct Authority in 2018.\n"}
{"id": "18569789", "url": "https://en.wikipedia.org/wiki?curid=18569789", "title": "Self-relocation", "text": "Self-relocation\n\nIn computer programming, a self-relocating program is a program that relocates its own address-dependent instructions and data when run, and is therefore capable of being loaded into memory at any address. In many cases, self-relocating code is also a form of self-modifying code.\n\nSelf-relocation is similar to the relocation process employed by the linker-loader when a program is copied from external storage into main memory; the difference is that it is the loaded program itself rather than the loader in the operating system or shell that performs the relocation.\n\nOne form of self-relocation occurs when a program copies the code of its instructions from one sequence of locations to another sequence of locations within the main memory of a single computer, and then transfers processor control from the instructions found at the source locations of memory to the instructions found at the destination locations of memory. As such, the data operated upon by the algorithm of the program is the sequence of bytes which define the program.\n\nSelf-relocation typically happens at load-time (after the operating system has loaded the software and passed control to it, but still before its initialization has finished), sometimes also when changing the program's configuration at a later stage during runtime.\n\nAs an example, self-relocation is often employed in the early stages of bootstrapping operating systems on architectures like IBM PC compatibles, where lower-level chain boot loaders (like the Master Boot Record (MBR), Volume Boot Record (VBR) and initial boot stages of operating systems such as DOS) move themselves out of place in order to load the next stage into memory.\n\nUnder DOS, self-relocation is sometimes also used by more advanced drivers and RSXs/TSRs loading themselves \"high\" into upper memory more effectively than possible for externally provided \"high\"-loaders (like LOADHIGH/HILOAD, INSTALLHIGH/HIINSTALL or DEVICEHIGH/HIDEVICE etc. since DOS 5) in order to maximize the memory available for applications. This is down to the fact that the operating system has no knowledge of the inner workings of a driver to be loaded and thus has to load it into a free memory area large enough to hold the whole driver as a block including its initialization code, even if that would be freed after the initialization. For TSRs, the operating system also has to allocate a Program Segment Prefix (PSP) and an environment segment. This might cause the driver not to be loaded into the most suitable free memory area or even prevent it from being loaded high at all. In contrast to this, a self-relocating driver can be loaded anywhere (including into conventional memory) and then relocate only its (typically much smaller) resident portion into a suitable free memory area in upper memory. In addition, advanced self-relocating TSRs (even if already loaded into upper memory by the operating system) can relocate over most of their own PSP segment and command line buffer and free their environment segment in order to further reduce the resulting memory footprint and avoid fragmentation. Some self-relocating TSRs can also dynamically change their \"nature\" and morph into device drivers even if originally loaded as TSRs, thereby typically also freeing some memory. Finally, it is technically impossible for an external loader to relocate drivers into expanded memory (EMS), the high memory area (HMA) or extended memory (via DPMS or CLOAKING), because these methods require small driver-specific stubs to remain in conventional or upper memory in order to coordinate the access to the relocation target area, and in the case of device drivers also because the driver's header must always remain in the first megabyte. In order to achieve this, the drivers must be specially designed to support self-relocation into these areas.\n\nIBM DOS/360 did not have the ability to relocate programs during loading. Sometimes multiple versions of a program were maintained, each built for a different load address. A special class of programs, called self-relocating programs, were coded to relocate themselves after loading. IBM OS/360 relocated executable programs when they were loaded into memory. Only one copy of the program was required, but once loaded the program could not be moved (so called one-time position-independent code).\n\nAs an extreme example of (many-time) self-relocation it is possible to construct a computer program so that it does not stay at a fixed address in memory, even as it executes. The Apple Worm is a dynamic self-relocator.\n\n\n"}
{"id": "2289171", "url": "https://en.wikipedia.org/wiki?curid=2289171", "title": "Shell and tube heat exchanger", "text": "Shell and tube heat exchanger\n\nA shell and tube heat exchanger is a class of heat exchanger designs. It is the most common type of heat exchanger in oil refineries and other large chemical processes, and is suited for higher-pressure applications. As its name implies, this type of heat exchanger consists of a shell (a large pressure vessel) with a bundle of tubes inside it. One fluid runs through the tubes, and another fluid flows over the tubes (through the shell) to transfer heat between the two fluids. The set of tubes is called a tube bundle, and may be composed of several types of tubes: plain, longitudinally finned, etc.\nTwo fluids, of different starting temperatures, flow through the heat exchanger. One flows through the tubes (the tube side) and the other flows outside the tubes but inside the shell (the shell side). Heat is transferred from one fluid to the other through the tube walls, either from tube side to shell side or vice versa. The fluids can be either liquids or gases on either the shell or the tube side. In order to transfer heat efficiently, a large heat transfer area should be used, leading to the use of many tubes. In this way, waste heat can be put to use. This is an efficient way to conserve energy.\n\nHeat exchangers with only one phase (liquid or gas) on each side can be called one-phase or single-phase heat exchangers. Two-phase heat exchangers can be used to heat a liquid to boil it into a gas (vapor), sometimes called boilers, or cool a vapor to condense it into a liquid (called condensers), with the phase change usually occurring on the shell side. Boilers in steam engine locomotives are typically large, usually cylindrically-shaped shell-and-tube heat exchangers. In large power plants with steam-driven turbines, shell-and-tube surface condensers are used to condense the exhaust steam exiting the turbine into condensate water which is recycled back to be turned into steam in the steam generator.\n\nThere can be many variations on the shell and tube design. Typically, the ends of each tube are connected to plenums (sometimes called water boxes) through holes in tubesheets. The tubes may be straight or bent in the shape of a U, called U-tubes. \nIn nuclear power plants called pressurized water reactors, large heat exchangers called steam generators are two-phase, shell-and-tube heat exchangers which typically have U-tubes. They are used to boil water recycled from a surface condenser into steam to drive a turbine to produce power. Most shell-and-tube heat exchangers are either 1, 2, or 4 pass designs on the tube side. This refers to the number of times the fluid in the tubes passes through the fluid in the shell. In a single pass heat exchanger, the fluid goes in one end of each tube and out the other. \nSurface condensers in power plants are often 1-pass straight-tube heat exchangers (see surface condenser for diagram). Two and four pass designs are common because the fluid can enter and exit on the same side. This makes construction much simpler. \n\nThere are often baffles directing flow through the shell side so the fluid does not take a short cut through the shell side leaving ineffective low flow volumes. These are generally attached to the tube bundle rather than the shell in order that the bundle is still removable for maintenance.\n\nCounter current heat exchangers are most efficient because they allow the highest log mean temperature difference between the hot and cold streams. Many companies however do not use two pass heat exchangers with a u-tube because they can break easily in addition to being more expensive to build. Often multiple heat exchangers can be used to simulate the counter current flow of a single large exchanger.\n\nTo be able to transfer heat well, the tube material should have good thermal conductivity. Because heat is transferred from a hot to a cold side through the tubes, there is a temperature difference through the width of the tubes. Because of the tendency of the tube material to thermally expand differently at various temperatures, thermal stresses occur during operation. This is in addition to any stress from high pressures from the fluids themselves. The tube material also should be compatible with both the shell and tube side fluids for long periods under the operating conditions (temperatures, pressures, pH, etc.) to minimize deterioration such as corrosion. All of these requirements call for careful selection of strong, thermally-conductive, corrosion-resistant, high quality tube materials, typically metals, including aluminium, copper alloy, stainless steel, carbon steel, non-ferrous copper alloy, Inconel, nickel, Hastelloy and titanium. Fluoropolymers such as Perfluoroalkoxy alkane (PFA) and Fluorinated ethylene propylene (FEP) are also used to produce the tubing material due to their high resistance to extreme temperatures. Poor choice of tube material could result in a leak through a tube between the shell and tube sides causing fluid cross-contamination and possibly loss of pressure.\n\nThe simple design of a shell and tube heat exchanger makes it an ideal cooling solution for a wide variety of applications. One of the most common applications is the cooling of hydraulic fluid and oil in engines, transmissions and hydraulic power packs. With the right choice of materials they can also be used to cool or heat other mediums, such as swimming pool water or charge air. One of the big advantages of using a shell and tube heat exchanger is that they are often easy to service, particularly with models where a floating tube bundle (where the tube plates are not welded to the outer shell) is available.\n\n\n\n"}
{"id": "41410502", "url": "https://en.wikipedia.org/wiki?curid=41410502", "title": "Shift by wire", "text": "Shift by wire\n\nShift by wire is the system by which the transmission modes are engaged/changed in an automobile through electronic controls without any mechanical linkage between the gear shifting lever and the transmission. The transmission shifting was traditionally accomplished by mechanical links to put the vehicle in Park, Reverse, Neutral and Drive positions through a lever mounted on the steering column or a gear shifter near the center console.\n\nThis eliminates routing space required for housing the mechanical linkages between the shifter and the transmission and provides effortless shifting through the press of a button or through knobs. Elimination of this linkage removes any shift effort from the driver’s gear selection.\n\n\nThere have been safety issues identified with production vehicles implementing the shift by wire systems which have led to recalls. The major hazards associated with this type of systems are vehicle not achieving park state and vehicle moving in the wrong direction (Drive vs Reverse)\n\n"}
{"id": "5447381", "url": "https://en.wikipedia.org/wiki?curid=5447381", "title": "Spin pumping", "text": "Spin pumping\n\nSpin pumping is a method of generating a spin current, the spintronic analog of a battery in conventional electronics.\n\nIn order to make a spintronic device, the primary requirement is to have a system that can generate a current of spin-polarized electrons, as well as a system that is sensitive to the spin polarization. Most spintronic devices also have a unit in between these two that changes the current of electrons depending on the spin states. Candidates for such devices include injection schemes based on magnetic semiconductors and ferromagnetic metals, ferromagnetic resonance devices\n, and a variety of spin-dependent pumps. Optical, microwave and electrical methods are also being explored\n.These devices could be used for low-power data transmission in spintronic devices\nor to transmit electrical signals through insulators.\n\nThe spin pumped into an adjacent layer by a precessing magnetic moment is given by\nwhere formula_2 is the spin current (the vector indicates the orientation of the spin, \"not\" the direction of the current), formula_3 is the spin-mixing conductance, formula_4 is the saturation magnetization, and formula_5 is the time-dependent orientation of the moment.\n\n"}
{"id": "2849450", "url": "https://en.wikipedia.org/wiki?curid=2849450", "title": "Vanpool", "text": "Vanpool\n\nVanpools are an element of the transit system that allow groups of people to share the ride similar to a carpool, but on a larger scale with concurrent savings in fuel and vehicle operating costs. Vanpools have a lower operating and capital cost than most transit vehicles in the United States, but due to their relatively low capacity, vanpools often require subsidies comparable to conventional bus service. \n\nVehicles may be provided by individuals, individuals in cooperation with various public and private support programs, through a program operated by or on behalf of an element of government, or a program operated by or on behalf of an employer.\n\nThe key concept is that people share the ride from home or one or more common meeting locations and travel together to a common destination or work center. \n\nA number of programs exist (within the United States) to help lower the cost of that shared ride to the end user. Among these are traditional funding available to public agencies, public-private partnerships, and the Best Work Places for Commuters (Commuter Choice Programs). A tax benefit is available under 26 U.S.C. §132(f) Qualified Transportation Fringe Benefit allowances. These public transportation programs seek to reduce the number of cars on the road (with all the attendant environmental benefits). \n\nAdditional benefits include:\n\nIn many cases, an employer may elect to subsidize the cost of the vanpool and the vehicles' maintenance. In some cases, the vehicles are provided and maintained by the municipality; in others in partnership with or by a third-party provider. For example, UCLA operates an extensive network of vans, in which faculty, staff and students are eligible for discounted rates, although anyone commuting to the Westwood area is allowed to participate, with drivers receiving the highest discounts. The vans are centrally maintained, fueled, and cleaned.\n\nThe King County Metro Vanpool Program is a successful demand responsive transport program in the Puget Sound area, specifically in King County, Washington. Another successful program is operated by Pace in Illinois.\n\nThe oldest multi-employer vanpool program in the country is in Treasure Valley, Idaho. For over 30 years Ada County Highway District’s Commuteride Vanpools have been crisscrossing the Valley helping commuters go to and from work, with their numerous vanpool routes traveling throughout the Treasure Valley. The Vanpools also service the Military at Gowen Field and Mountain Home Air Force Base (MHAFB) with multiple routes to and from Ada and Canyon County. ACHD Commuteride serves the cities, Boise, Meridian, Kuna, Garden City, Eagle and Star as well as Ada County.\n\nPrivate firms operate vanpools for individuals, as well as in cooperation with employers or under contract.\n\n"}
{"id": "11151767", "url": "https://en.wikipedia.org/wiki?curid=11151767", "title": "Well bay", "text": "Well bay\n\nA well bay is an area of an oil platform where the Christmas trees and wellheads are located. It normally consists of two levels, a lower where the wellheads are accessed and an upper where the Xmas Trees are accessed often along with the various well control panels, which will have pressure gauges and controls for the hydraulically actuated valves, including downhole safety valve and annular safety valve. On a platform with a drilling package, the well bay will be located directly below it to facilitate access for drilling and well interventions.\n\n"}
{"id": "4212305", "url": "https://en.wikipedia.org/wiki?curid=4212305", "title": "Whirlwind wheelchair", "text": "Whirlwind wheelchair\n\nThe Whirlwind wheelchair is a wheelchair designed to be made in developing countries using local resources, in a sustainable development effort.\n\nIt was co-designed by Ralf Hotchkiss of Whirlwind Wheelchair International. Hotchkiss, a paraplegic, has traveled extensively, designing wheelchairs that could be built in developing countries.\n\nWhirlwind Wheelchair International uses the principle of open source design, and offers construction classes and consulting services.\n\n\n"}
{"id": "37150", "url": "https://en.wikipedia.org/wiki?curid=37150", "title": "White cane", "text": "White cane\n\nA white cane is used by many people who are blind or visually impaired. Primarily it aids its user to scan their surroundings for obstacles or orientation marks, but is also helpful for other traffic participants in identifying the user as blind or visually impaired and taking appropriate care. The latter is the reason for the cane's prominent white colour, which in many jurisdictions is mandatory.\n\n\nMobility canes are often made from aluminium, graphite-reinforced plastic or other fibre-reinforced plastic, and can come with a wide variety of tips depending upon user preference.\nWhite canes can be either collapsible or straight, with both versions having pros and cons. The National Federation of the Blind in the United States affirms that the lightness and greater length of the straight canes allows greater mobility and safety, though collapsible canes can be stored with more ease, giving them advantage in crowded areas such as classrooms and public events.\n\nBlind people have used canes as mobility tools for centuries, but it was not until after World War I that the white cane was introduced.\n\nIn 1921 James Biggs, a photographer from Bristol who became blind after an accident and was uncomfortable with the amount of traffic around his home, painted his walking stick white to be more easily visible.\n\nIn 1931 in France, Guilly d'Herbemont launched a national white stick movement for blind people. On February 7, 1931, Guilly d'Herbemont symbolically gave the first two white canes to blind people, in the presence of several French ministers. 5,000 more white canes were later sent to blind French veterans from World War I and blind civilians.\n\nIn the United States, the introduction of the white cane is attributed to George A. Bonham of the Lions Clubs International. In 1930, a Lions Club member watched as a man who was blind attempted to cross the street with a black cane that was barely visible to motorists against the dark pavement. The Lions decided to paint the cane white to make it more visible. In 1931, Lions Clubs International began a program promoting the use of white canes for people who are blind.\n\nThe first special white cane ordinance was passed in December 1930 in Peoria, Illinois granting blind pedestrians protections and the right-of-way while carrying a white cane. \n\nThe long cane was improved upon by World War II veterans rehabilitation specialist, Richard E. Hoover, at Valley Forge Army Hospital. In 1944, he took the Lions Club white cane (originally made of wood) and went around the hospital blindfolded for a week. During this time he developed what is now the standard method of \"long cane\" training or the Hoover Method. He is now called the \"Father of the Lightweight Long Cane Technique.\" The basic technique is to swing the cane from the center of the body back and forth before the feet. The cane should be swept before the rear foot as the person steps. Before he taught other rehabilitators, or \"orientors,\" his new technique he had a special commission to have light weight, long white canes made for the veterans of the European fronts.\n\nOn October 6, 1964, a joint resolution of the Congress, HR 753, was signed into law authorizing the President of the United States to proclaim October 15 of each year as \"White Cane Safety Day\". President Lyndon Johnson was the first to make this proclamation.\n\nWhile the white cane is commonly accepted as a \"symbol of blindness\", different countries still have different rules concerning what constitutes a \"cane for the blind\".\n\nIn the United Kingdom, the white cane indicates that the individual has a visual impairment; with two red bands added it indicates that the user is deafblind.\n\nIn the United States, laws vary from state to state, but in all cases, those carrying white canes are afforded the right-of-way when crossing a road. They are afforded the right to use their cane in any public place as well. In some cases, it is illegal for a non-blind person to use a white cane with the intent of being given right-of-way.\n\nIn November 2002, Argentina passed a law recognizing the use of green canes by people with low vision, stating that the nation would \"Adopt from this law, the use of a green cane in the whole of Argentina as a means of orientation and mobility for people with low vision. It will have the same characteristics in weight, length, elastic grip and fluorescent ring as do white canes used by the blind.\"\n\nIn Germany, people carrying a white cane are excepted from the \"\" (trust principle), therefore meaning that other traffic participants should not rely on them to adhere to all traffic regulations and practices. Although there is no general duty to mark oneself as blind or otherwise disabled, a blind or visually impaired person involved in a traffic accident without having marked themselves may be held responsible for damages unless they prove that their lack of marking was not causal or otherwise related to the accident.\n\nIn many countries, including the UK, a cane is not generally introduced to a child until they are between 7 and 10 years old. However, more recently canes have been started to be introduced as soon as a child learns to walk to aid development with great success.\n\nJoseph Cutter and Lilli Nielsen, pioneers in research on the development of blind and multiple-handicapped children, have begun to introduce new research on mobility in blind infants in children. Cutter's book, \"Independent Movement and Travel in Blind Children\", recommends a cane to be introduced as early as possible, so that the blind child learns to use it and move around naturally and organically, the same way a sighted child learns to walk. A longer cane, between nose and chin height, is recommended to compensate for a child's more immature grasp and tendency to hold the handle of the cane by the side instead of out in front. Mature cane technique should not be expected from a child, and style and technique can be refined as the child gets older.\n\n\n"}
