{"id": "22780043", "url": "https://en.wikipedia.org/wiki?curid=22780043", "title": "ATNAA", "text": "ATNAA\n\nAn ATNAA (Antidote Treatment Nerve Agent Autoinjector) is any of a variety of autoinjectors in use with the US Armed Forces. An autoinjector is a medical device designed to deliver a single dose of a particular (typically life-saving) drug.\nMost autoinjectors are spring-loaded syringes. By design, autoinjectors are easy to use and are intended for self-administration by patients. The site of injection depends on the drug loaded, but it typically is administered into the thigh or the buttocks. The injectors were initially designed to overcome the hesitation associated with self-administration of the needle-based drug delivery device. It is the newer delivery vehicle to be used in lieu of the Mark I NAAK.\n\nThe ATNAA provides Atropine and Pralidoxime chloride in a single delivery system, although the two drugs are separate within the device. The use of the device is only to be administered in the extreme case of organophosphate poisoning. The delivery system is designed for use by military personnel only, and is only issued to DOD personnel that are considered to be in immediate danger of a chemical attack or work in a position (such as ordnance disposal) where there is a high likelihood of nerve agent exposure.\n\nDuring initial trials and submission to the FDA for approval, the United States Department of Defense requested that the FDA waive the requirement for a doctor's prescription for the ATNAA device; the rationale being that it would be issued en masse to troops. The waiver request was denied.\n\nAs a condition of approval, the FDA lists on the device label the precise instructions that are to be given to military personnel on dosage and administration. While each branch of the DoD typically provides their own tailored training, an example of the MK I NAAK and ATNAA are referenced in the United States Air Force Self Aid Buddy Care Training (SABC). SABC encompasses basic life support and limb-saving techniques to help wounded or injured personnel survive in medical emergencies until medical help is available.\n\n"}
{"id": "1919103", "url": "https://en.wikipedia.org/wiki?curid=1919103", "title": "Adsorption Method for Sampling of Dioxins and Furans", "text": "Adsorption Method for Sampling of Dioxins and Furans\n\nAdsorption Method for Sampling of Dioxins and Furans (AMESA) is an automatic system for \"continuous\" monitoring of emissions of dioxins and furans from industrial processes which require official approval in compliance with environmental regulations. Applications include refuse incinerators and hazardous material incinerators.\n\nA sample is continuously extracted from the gas stream being monitored using a titanium probe, which is water-cooled to below 70 °C. The sample flow rate is automatically adjusted to ensure isokinetic sampling (the velocity of gas entering the sampling system is equal to the velocity of the gas in the system under test). The sample gas is drawn through a quartz wool pre-filter and then across a cartridge filled with resin, such as XAD 2. The sample gas is also cooled to below 5 °C to condense and remove water vapour. All system parameters are recorded digitally during sampling. The resin cartridge and the condensate are removed at the end of a monitoring period, and the contents are analysed to determine levels of dioxins and furans.\n"}
{"id": "56902560", "url": "https://en.wikipedia.org/wiki?curid=56902560", "title": "Anton Paar", "text": "Anton Paar\n\nAnton Paar GmbH is an Austrian company based in Graz that develops, produces and sells analytical instruments for laboratories and process analytical technology. It also provides automation and robotics solutions. The company specializes in the production of instruments for measuring density, concentration, dissolved carbon dioxide, and in the fields of rheometry and material characterization. Many of Anton Paar's customers are beer and soft drink manufacturers as well as companies in the food, chemicals, and pharmaceutical industries.\n\nThe company was founded in 1922 by master locksmith Anton Paar as a one-man repair workshop. He developed a good reputation and made initial contacts with universities and research institutions. He trained his daughter, Margarete Platzer, in the 1920s to become a skilled locksmith. In 1932, she became the first master locksmith in Styria. Her talent in precision mechanics and precision engineering, together with the knowledge of Otto Kratky, formed the basis of Anton Paar's first scientific analytical instrument: the Kratky X-ray small-angle camera.\n\nFrom 1963 Ulrich Santner, the son-in-law of Margarete Platzer, took over the management. He laid the foundation for the company's future expansion in the field of metrology. He also intensified exchanges and contacts with universities and contributed to the transfer of new technologies from research to industry.\n\nIn 1997, his son-in-law Friedrich Santner joined the management. Since 2002 he is the sole managing director. The focus of his work was the organization of worldwide sales through his own subsidiaries. Under his leadership, new sales and service branches were founded and complementary metrology companies purchased.\n\nIn 2003, the family business was incorporated into a charitable foundation.\n\nOn February 12, 2018, Anton Paar acquired Quantachrome Instruments, a manufacturer of scientific instruments that specializes in the analysis of surface area, porosity, and density measurement.\n\n"}
{"id": "9015535", "url": "https://en.wikipedia.org/wiki?curid=9015535", "title": "Arnold Allen", "text": "Arnold Allen\n\nArnold Oral Allen was an American instructor, public speaker, and writer who worked at IBM and Hewlett-Packard, and specialized in the analysis and mathematical modelling of computer performance.\n\nAllen earned a Ph.D in Mathematics at UCLA in 1962 under Angus Taylor with a dissertation entitled \"Banach and Hilbert Spaces of Analytic Functions\", where he later lectured. At IBM, he taught at the Information Systems Management Institute in Los Angeles, California. Later, at Hewlett-Packard, he was a member of the Performance Technology Center, then a researcher at the Advanced Technology Group in Roseville, California.\n\nAllen was elected as a director of the Computer Measurement Group (CMG), and selected to be the keynote speaker at two international conferences. He was an invited speaker at the \"Sixth International Conference on Modelling Techniques and Tools for Computer Performance Evaluation\", held in Edinburgh, Scotland in September 1992.\n\nIn 1994, he received the Computer Measurement group's A. A. Michelson award for technical excellence and professional contributions as a teacher and inspirer of others.\n\nAllen is most well known as the author of the book, \"Probability, Statistics, and Queueing Theory with Computer Science Applications\". Originally published in 1978, and still in print in 2007, it is widely used as a university textbook, by practitioners of computer performance analysis, and by those wishing to apply probability, statistics and queueing theory techniques to solve problems in other fields, such as operations research, management science, engineering, and physics.\n\nAt IBM and Hewlett-Packard, Allen's students were typically systems engineers and project managers, not computer scientists. He encouraged them to improve upon the informal approaches to computer performance analysis that were (and still are) in common use, applying more formal methods and using mathematical models to predict how the performance of a computer system would behave as workloads increased. He began his 1994 book, Computer Performance Analysis with Mathematica, with this observation:\n\n\"The word \"performance\" in computer performance means the same thing that performance means in other contexts, that is, it means \"How well is the computer doing the work it is supposed to do?\"\"\n\nHe concluded the book by quoting George Bernard Shaw: \"The reasonable man adapts himself to the world; the unreasonable man persists in trying to adapt the world to himself. Therefore all progress depends on the unreasonable man\". Allen commented, \"I hope the reader fits Shaw's definition of unreasonable, and wants to change things for the better\".\n\n"}
{"id": "1029697", "url": "https://en.wikipedia.org/wiki?curid=1029697", "title": "Brood (comics)", "text": "Brood (comics)\n\nThe Brood are a fictional race of insectoid, parasitic, extraterrestrial beings appearing in American comic books published by Marvel Comics, especially \"Uncanny X-Men.\" Created by writer Chris Claremont and artist Dave Cockrum, they first appeared in \"Uncanny X-Men\" #155 (March 1982).\n\nThe Brood possess wings, fanged teeth and a stinging tail. They have a hive mentality and mindlessly follow a queen. To reproduce, they must infect other races with their eggs.\n\nAccording to Dave Cockrum, the Brood were originally conceived to serve as generic subordinates for the main villain of \"Uncanny X-Men\" #155: \"We had Deathbird in this particular story and Chris [Claremont] had written into the plot 'miscellaneous alien henchmen.' So I had drawn Deathbird standing in this building under construction and I just drew the most horrible looking thing I could think of next to her.\"\n\nDespite their resemblance to insects, the Brood have endoskeletons as well as exoskeletons. Also unlike insects, they have fanged jaws instead of mandibles. Their skulls are triangular and flat, with a birthmark (such as the battle axe which is most common in broodlings but are different for each Brood) between their large eyes. Their two front legs are actually long tentacles they can use to manipulate objects.\n\nDue to their natural body armor and teeth, the Brood are very dangerous in combat. In addition, they have stingers that can deliver either paralyzing or killing poison.\n\nThe Brood is governed by a supreme matriarchy, at the top of which sits the Brood Empress. The Brood reproduces asexually and therefore has no clear divisions of gender. Although the Brood lives by a caste system, some of the roles have proven to be flexible.\n\n\nNote: Surprisingly, the ability of an individual Brood to lay eggs does not necessarily make it a Queen. Several Warriors-Prime have been known to implant eggs in other hosts. This fluidity in the Brood caste system suggests the hierarchy is not as rigid as it initially appears.\n\nThe parasitic Brood have the ability to impregnate a host (of any lifeform) with an egg. Each host can only support one egg. When the embryo is grown, the host becomes a Brood, and is effectively dead.\n\nThey use a hive mind to pass memory to their hosts, which also passes an individual's knowledge, given to a broodling, to the hive and back to the queen, meaning newborn brood know what any member of a race knows. Until the embryo gains the host's body the embryo can only gain temporary control of the host, often without the host noticing as the host is unaware when it loses control.\n\nIf the host possesses any genetic powers, the resultant Brood will inherit them. The persona of the host once the Brood is \"born\" appears to be extinguished, but in some cases the host's will may be strong enough to survive and coexist with the Brood's.\n\nBrood also have the ability to morph into their host's original form and a hybrid form with characteristics of both the host and the Brood body.\n\nThe Brood are sadistic creatures that enjoy the suffering they intentionally cause others, especially the terror their infection causes their hosts. They have been compared to \"demons\".\n\nThe Brood have a civilization based on the typical communal insect societies, such as those of the bees and ants. The Queens are the absolute rulers, while the \"sleazoids\" do all the work; despite their evil, they never rebel against their Queens, perhaps due to the latter's telepathic abilities. However, the Queens have no allegiance to each other. They also have developed, or stolen, advanced technology.\n\nTheir true planet of origin is unknown, but such is their relentless evil, legend holds that the Brood originated not through natural evolution, but in otherworldly sorcery. They arrived in the Shi'ar galaxy long ago, and began infesting many worlds, becoming deadly enemies to the Sh'iar. In this galaxy they found certain large space-dwelling creatures that they decided to use as living starships. These include the whale-like Acanti, and the shark-like Starsharks. The Brood use a virus that effectively lobotomizes the creatures, then they use bionics to control them. The Brood hollow out part of the creatures (by eating them) and use the space created to live in, like termites eating a tree. This eventually kills the living ships, requiring them to capture new ones.\n\nOne of the Acanti they captured was of unusual size (its rib cage alone was the size of a mountain range.) They used it as their main base, and, when it died and crashed onto a planet, used it as their main city. The corpse was so large, it took centuries just to rot halfway. However, predators from the planet they landed on infested the area of the dead Acanti's brain, so the Brood avoided it.\n\nThe first Marvel hero to encounter the Brood was the Kree warrior Mar-Vell, who had been ordered to make contact with the stranded Grand Admiral Devros on a planet in the Absolom Sector, a region known to be infested with Brood. Mar-Vell's team, which includes the medic Una and Colonel Yon-Rogg, is ambushed by Brood warriors after landing on the planet and taken prisoner by the Brood-infected Devros. The colony's Brood Queen impregnates each captive with Brood embryos, but Mar-Vell and Una manage to escape, destroy both leaders of the Brood colony, and ridding themselves of their infections using Una's modified omni-wave projector which had been designed to eliminate Brood embryos. After rescuing Colonel Yon-Rogg, the trio escape the planet and are rescued by the Shi'ar royal Deathbird.\n\nDeathbird later allies with The Brood to gain their help deposing her sister Lilandra as ruler of their empire. As a reward for their help, Deathbird gives the Brood Lilandra, the X-Men, and the powerless Carol Danvers, along with Fang of the Imperial Guard, to use as hosts. The Brood infect the entire party, except for Danvers, who they perform experiments on because of her half-human/half-Kree genes. Wolverine's adamantium skeleton allowed his healing ability to purge him of the embryo, and he helps the others escape. He is unable to save Fang, who becomes a Brood warrior before they leave.\n\nThe Brood Queen orders her forces to find them, until she is contacted by the Queen embryo that is implanted in Cyclops. It explains that the X-Men are returning to Broodworld. Resigned to their dooms, the heroes help the Acanti race recover the racial Soul, a supernatural force that must be passed from one Acanti leader (\"The Prophet-Singer\") to the next. The Soul is located in a crystalline part of the dead Acanti Prophet-Singer's brain.\n\nThe Queen goes with her minions, and battles the X-Men, turning them into Brood warriors. The Prophet-Singer Soul is almost infected by the evil of the aliens, and Wolverine tries to mercy-kill his friends and the Queen but they are saved when Danvers, now a mighty being called Binary (due to experimentation), arrives and releases the Soul. Before it goes to its next host, the racial Soul cures the X-Men and Lilandra, and turns the Queen into a crystal statue. It also causes the Broodworld to explode, but the X-Men and their allies escape. Some of the Brood also manage to escape before the planet explodes. The new Prophet-Singer then leads the Acanti to safety in deep space. Returning to Earth with the Starjammers, the X-Men defeat and detain the Brood Queen infecting Charles Xavier in the first Earth-based confrontation with the Brood. The advanced medical facilities at the Starjammers' disposal are able to transfer the consciousness of Xavier from the Brood Queen's body to a new cloned body, enabling Xavier to walk again.\n\nA Brood-filled starshark later crashes on Earth, leading to the infection of several nearby humans by the Brood. One of the victims is allowed to live as a human assistant, but when he leads the aliens to some mutants, the Brood infect him and the mutants as well. It is revealed that the Brood can morph into the host's form or a hybrid of the two forms. In the course of the battle, an Earth woman named Hannah Connover is infected with a queen, though this problem would not develop until later.\n\nAnother branch of the Brood manage to land on Earth and infect more mutants, along with the Louisiana Assassins Guild of which X-Man Gambit is a member. The X-Men kill most of the infected people. They and Ghost Rider manage to rescue many of the Brood's other uninfected prisoners, only to have the \"Spirit of Vengeance\" become infected himself. Psylocke manages to separate Ghost Rider from the Brood host before it could kill Danny Ketch, the current host of the Ghost Rider, and he and the X-Men saved New Orleans.\n\nHannah Connover, previously infected with a Queen, soon begins to demonstrate attributes of Brood. She uses her new-found \"healing\" powers to become a faith healer and cure many people with her reverend husband, but secretly her Brood nature causes her to infect many people with embryos. Across the Galaxy, on the \"true\" Brood Homeworld, the Brood Empress sends her \"firstborn\" Imperial Assassins to kill Hannah for going against the Empress' wishes. Unable to stop future waves of Assassins from coming, the X-Man, Iceman, freezes Connover, putting her in suspended animation and causing the current firstborn to kill themselves, as in their minds the mission was accomplished. Connover is assumed to still be in suspended animation with her Queen host in the custody of the X-Men.\n\nDuring the \"Contest of Champions II\", the Brood abduct several heroes and pose as a benevolent species willing to give the heroes access to advanced technology after competing against each other in a series of contests. However, in reality, the Brood intend to use Rogue, infested with a Brood Queen, to absorb the powers of the contest winners and become unstoppable. Fortunately, Iron Man realizes that the Brood are drugging food to amplify aggression- relying on his armor's own life-support systems to prevent him succumbing to the 'infection'- and is able to uncover the plot.\n\nAlthough the Queen had already absorbed the powers and skills of the various contest winners- in the form of Captain America, Thor, the Hulk, Spider-Man, Jean Grey and the Scarlet Witch-, the remaining heroes managed to defeat her. The Brood Queen was extracted from Rogue with the aid of Carol Danvers, who forced the Brood Queen to flee by threatening to kill Rogue. After confirming that Rogue was cured, the heroes returned home.\n\nIt was also revealed that at some point in the dawn of civilization during the year 2610 BC, a spaceship filled with Brood crash landed in Egypt, marking the end of the second great dynasty. It also would have been the very end of days if not for Imhotep and a group of soldiers, among them En Sabah Nur, who were able to successfully fend off the invasion. Imhotep himself killed the Queen.\n\nThe Brood return to Earth in the \"Ms. Marvel\" series and battle Carol Danvers, who as Binary played a key role in their earlier defeat. Strangely enough, none of the Brood present recognize who she is, possibly because of her inability to fully access her cosmic powers, which also changed her physical appearance. The Brood are also stalked and summarily exterminated by the alien hunter called Cru, with whom Ms. Marvel also came into violent contact.\n\nDuring the invasion of Annihilus and his Annihilation Wave, the Brood were almost decimated and the species is now on the brink of extinction.\n\nSome Brood appear in the arena of planet Saakar in the \"Planet Hulk\" storyline of \"The Incredible Hulk\", one of them even becoming a main character. A Brood referred to as \"No-Name\", who becomes a genetic queen because their race is becoming rarer, becomes the lover of insect king Miek and also appears in \"World War Hulk\". When it is discovered that Miek was the one who let the Hulk's shuttle explode, No-Name and Hulk attack Miek. Near the end of the War the \"Earth Hive\", the shared consciousness of every insect on Earth, use Humbug as a Trojan Horse to deal a crippling blow to No-Name, rendering her infertile and poisoning the last generation of hivelings, growing in Humbug's body. No-Name is a rarity among the Brood, as she learned to feel compassion for other living beings.\n\nThe Brood reappeared once again in the pages of \"Astonishing X-Men\", however these Brood are revealed to be actual genetically-grown hybrids created by a geneticist known only as Kaga who started growing and redesigning them with missing data about post M-Day work on Henry McCoy's research computers.\n\nIn the 2011 \"Meanwhile\" storyline \"Astonishing X-Men\", S.W.O.R.D. scientists successfully find a way to remove a Brood embryo from a human host, but not before the Brood they are studying escape and attack, prompting a botched rescue mission led by Abigail Brand and another rescue mission led by the X-Men.\n\nGiven the chance to lower the Brood's numbers further, they discovered that the Annihilation event had caused the interstellar ecosystem to destabilize, since the Brood, dangerous as they are, served as natural predators for even worse species. These remaining species are now breeding out of control and present a greater threat than the Brood ever did. With no other choice, the X-Men act to prevent the Brood extinction. According to Bishop, there would be a race of benevolent Brood in the future, prompting the X-Men to willingly serve as Brood hosts, so that they could instill them with the same compassion felt by No-Name. After being connected with the hive-mind, the X-Men learned of a nearby Brood who was born with ability to feel compassion, making him the Brood equivalent of a mutant. While such Brood are typically destroyed upon birth, this one was permitted to live, given the Brood's dwindling numbers. After rescuing the Brood mutant and defeating the Brood in battle and allowing them to escape, the X-Men had their Brood embryos removed, to be raised aboard the Peak, with the Brood mutant acting as their mentor.\n\nThe 2012 X-Men subseries \"Wolverine and the X-Men\" featured a Broodling as a student at Wolverine's Jean Grey School for Higher Learning. Nicknamed \"Broo\" by Oya, the Broodling was a mutant, and both intelligent and non-violent able to wear clothing and glasses (which he felt made him look less frightening). Broo expressed a desire to join the Nova Corps. In a possible future timeline seen by Deathlok, Broo was a member of the X-Men alongside Oya, Quentin Quire, and Kid Gladiator.\n\nDuring the \"Age of Ultron\" storyline, it is revealed that while in a hidden S.H.I.E.L.D. substation decades in the past, the future-Wolverine released and was infected by a less menacing Brood. When he cut the embryo out of his body, the Brood Collective responded to the attack by altering the physical structure of all future Brood to the form it is now known for.\n\nDuring the \"Infinity\" storyline, the Brood Queen appeared as a member of the Galactic Council where she represents the Brood race. She later made a deal with J'son, the former Emperor of the Spartoi Empire which consisted in J'son surrendered the planet Spartax to the Brood, and in exchange J'son would acquire one planet for every ten worlds they conquered ever since.\n\nIn \"Spider-Man and the X-Men\", the Brood made a pact with the Symbiotes but ended up being betrayed and possessed until Spider-Man, with the help of the X-Men and S.W.O.R.D managed to defeat them.\n\nDuring \"The Black Vortex\" storyline, the Brood strike a deal with Mister Knife to takeover Spartax and use the entire planet as hosts. The plot is foiled once Kitty Pryde is cosmically powered by The Black Vortex and banishes the Brood from the planet.\n\nDario Agger and Roxxon Energy Corporation managed to obtain some Brood. Using some parasites on some wolves, Dario Agger and one of his scientists sent them to track down Weapon H. When Weapon H slayed them, Dario Agger had Brood Drones, Brood-infected Space Sharks, and a Brood-infected human riding an Acanti into attacking Weapon H. After the Brood Drones and Brood-Space Sharks are slain and the Acanti is knocked out, the Brood-infected human states to Weapon H that Roxxon wants to hire him. Weapon H stated that those who claim to help people will kill them anyway and has the Brood-infected human carry a message to Roxxon to leave him alone.\n\nThe following characters are either Brood and/or were turned into Brood:\n\n\nIn the \"Age of Apocalypse\" timeline, without the X-Men to aid them, part of the Shi'ar Imperium was consumed by the Brood, who infected its populace with Brood implants, including the still-captive Christopher Summers. Escaping to Earth, Summers fought to control his Brood implant, but was captured by Mister Sinister. Sinister turned him over to the Dark Beast, who then proceeded to experiment on him for years. Summers eventually escaped, and began infecting other humans (Including the AoA version of Joseph \"Robbie\" Robertson, as well as friends of Misty Knight and Colleen Wing). Ultimately, Corsair transformed into a Brood Queen and attempted to kill Alex but was killed by his son Cyclops. The Summers brothers cremated their father and indirectly deprived Sinister of the chance to carry out further tests on Brood DNA.\n\nIn Amalgam Comics, the Brood is combined with Brother Blood to form Brother Brood, and with the Cult of Blood to form the Cult of Brood. The Brood appear alongside Brother Brood, but are presented as supernatural rather than extraterrestrial.\n\nAccording to the time-traveling X-Man Bishop there are benign factions of Brood in the future. It is speculated that these \"good\" Brood are originated from Hannah Connover.\n\nIn \"JLA/Avengers\", the Brood have a brief cameo scene, where they are seen attacking Mongul and apparently invading Warworld as the two universes begin to come together.\n\nIn \"WildC.A.T.s/X-Men: The Silver Age\", alien hybrids of the Brood and Daemonites are created by Mister Sinister.\n\nIn the \"Ultimate Marvel\" universe, the Brood appeared as a Danger Room training exercise during the \"Tempest\" arc of \"Ultimate X-Men\". The Brood are later revealed to be creatures native to the mindscape, where the Shadow King dwells.\n\nIn \"\", taking place in a possible future, the Brood hatch a plan with Lilandra (possessed by Cassandra Nova). Nova plans to solidify her rule over Shi'ar space by smuggling an other-dimensional pure-Brood queen from an alternate universe. This realm is one where the X-Men failed to ever fight the Brood, they are described as 'pure'. This Brood Queen is implanted in Lilandra's sister, Deathbird.\n\n\n\n\n\n"}
{"id": "3482223", "url": "https://en.wikipedia.org/wiki?curid=3482223", "title": "Building science", "text": "Building science\n\nBuilding science is the collection of scientific knowledge that focuses on the analysis of the physical phenomena affecting buildings. \"Building physics, architectural science\" and \"applied physics\" are terms used for the knowledge domain that overlaps with building science.\n\nBuilding science traditionally includes the study of indoor thermal environment, indoor acoustic environment, indoor light environment, indoor air quality, and building resource use, including energy and building material use. These areas are studied in terms of physical principles, relationship to building occupant health, comfort, and productivity, and how they can be controlled by the building envelope and electrical and mechanical systems. The National Institute of Building Sciences (NIBS) additionally includes the areas of building information modeling, building commissioning, fire protection engineering, seismic design and resilient design within its scope.\n\nThe practical purpose of building science is to provide predictive capability to optimize the building performance and sustainability of new and existing buildings, understand or prevent building failures, and guide the design of new techniques and technologies.\n\nDuring the architectural design process, building science knowledge is used to inform design decisions to optimize building performance. Design decisions can be made based on knowledge of building science principles and established guidelines, such as the NIBS Whole Building Design Guide (WBDG) and the collection of ASHRAE Standards related to building science.\n\nComputational tools can be used during design to simulate building performance based on input information about the designed building envelope, lighting system, and mechanical system. Models can be used to predict energy use over the building life, solar heat and radiation distribution, air flow, and other physical phenomena within the building. These tools are valuable for evaluating a design and ensuring it will perform within an acceptable range before construction begins. Many of the available computational tools have the capability to analyze building performance goals and perform design optimization. The accuracy of the models is influenced by the modeler’s knowledge of building science principles and by the amount of validation performed for the specific program.\n\nWhen existing buildings are being evaluated, measurements and computational tools can be used to evaluate performance based on measured existing conditions. An array of in-field testing equipment can be used to measure temperature, moisture, sound levels, air pollutants, or other criteria. Standardized procedures for taking these measurements are provided in the Performance Measurement Protocols for Commercial Buildings. For example, thermal infrared (IR) imaging devices can be used to measure temperatures of building components while the building is in use. These measurements can be used to evaluate how the mechanical system is operating and if there are areas of anomalous heat gain or heat loss through the building envelope.\n\nMeasurements of conditions in existing buildings are used as part of post occupancy evaluations. Post occupancy evaluations may also include surveys of building occupants to gather data on occupant satisfaction and well-being and to gather qualitative data on building performance that may not have been captured by measurement devices.\n\nMany aspects of building science are the responsibility of the architect (in Canada, many architectural firms employ an architectural technologist for this purpose), often in collaboration with the engineering disciplines that have evolved to handle 'non-building envelope' building science concerns: \nCivil engineering, Structural engineering, Earthquake engineering, Geotechnical engineering, Mechanical engineering, Electrical engineering, Acoustic engineering, & fire code engineering. Even the interior designer will inevitably generate a few building science issues.\n\nIndoor environmental quality (IEQ) refers to the quality of a building’s environment in relation to the health and wellbeing of those who occupy space within it. IEQ is determined by many factors, including lighting, air quality, and damp conditions. Workers are often concerned that they have symptoms or health conditions from exposures to contaminants in the buildings where they work. One reason for this concern is that their symptoms often get better when they are not in the building. While research has shown that some respiratory symptoms and illnesses can be associated with damp buildings, it is still unclear what measurements of indoor contaminants show that workers are at risk for disease. In most instances where a worker and his or her physician suspect that the building environment is causing a specific health condition, the information available from medical tests and tests of the environment is not sufficient to establish which contaminants are responsible. Despite uncertainty about what to measure and how to interpret what is measured, research shows that building-related symptoms are associated with building characteristics, including dampness, cleanliness, and ventilation characteristics.\nIndoor environments are highly complex and building occupants may be exposed to a variety of contaminants (in the form of gases and particles) from office machines, cleaning products, construction activities, carpets and furnishings, perfumes, cigarette smoke, water-damaged building materials, microbial growth (fungal, mold, and bacterial), insects, and outdoor pollutants. Other factors such as indoor temperatures, relative humidity, and ventilation levels can also affect how individuals respond to the indoor environment.\nUnderstanding the sources of indoor environmental contaminants and controlling them can often help prevent or resolve building-related worker symptoms. Practical guidance for improving and maintaining the indoor environment is available.\n\nBuilding indoor environment covers the environmental aspects in the design, analysis, and operation of energy-efficient, healthy, and comfortable buildings. Fields of specialization include architecture, HVAC design, thermal comfort, indoor air quality (IAQ), lighting, acoustics, and control systems.\n\nThe mechanical systems, usually a sub-set of the broader Building Services, used to control the temperature, humidity, pressure and other select aspects of the indoor environment are often described as the Heating, Ventilating, and Air-Conditioning (HVAC) systems.\nThese systems have grown in complexity and importance (often consuming around 20% of the total budget in commercial buildings) as occupants demand tighter control of conditions, buildings become larger, and enclosures and passive measures became less important as a means of providing comfort.\n\nBuilding science includes the analysis of HVAC systems for both physical impacts (heat distribution, air velocities, relative humidities, etc.) and for effect on the comfort of the building's occupants. Because occupants' perceived comfort is dependent on factors such as current weather and the type of climate the building is located in, the needs for HVAC systems to provide comfortable conditions will vary across projects.\n\nThe building enclosure is the part of the building that separates the indoors from the outdoors. This includes the wall, roof, windows, slabs on grade, and joints between all of these. The comfort, productivity, and even health of building occupants in areas near the building enclosure (i.e., perimeter zones) are affected by outdoor influences such as noise, temperature, and solar radiation, and by their ability to control these influences. As part of its function, the enclosure must control (not necessarily block or stop) the flow of heat, air, vapor, solar radiation, insects, noise, etc.\nDaylight transmittance through glazed components of the facade can be analyzed to evaluate the reduced need for electric lighting.\n\nHigh Performance Facades Case Studies: \n\nPart of building science is the attempt to design buildings with consideration for the future and the resources and realities of tomorrow. This field may also be referred to as sustainable design.\n\nA push towards zero-energy building also known as Net-Zero Energy Building has been present in the Building Science field. The qualifications for Net Zero Energy Building Certification can be found on the Living Building Challenge website.\n\nThere are no professional architecture or engineering certifications for building science. It is currently a specialization within these broad areas of practice. In the US contractors certified by the Building Performance Institute, an independent organization, advertise that they operate businesses as Building Scientists. This is questionable due to their lack of scientific background and credentials. This is true in Canada for most of the Certified Energy Advisors. However, many of these trades and technologists require and receive some training in very specific areas of building science (e.g., air tightness, or thermal insulation).\n\n\"Building and Environment\": This international journal publishes original research papers and review articles related to building science, urban physics, and human interaction with the indoor and outdoor built environment. Impact Factor: 4.539 \n\n\"Energy and Buildings\": This international journal publishes articles with explicit links to energy use in buildings. The aim is to present new research results, and new proven practice aimed at reducing the energy needs of a building and improving indoor environment quality. Impact Factor: 4.457 \n\n\"Indoor Air:\" This international journal publishes papers reflecting the broad categories of interest in the field of indoor environment of non-industrial buildings, including health effects, thermal comfort, monitoring and modelling, source characterization, and ventilation and other environmental control techniques. Impact Factor: 4.396 \n\n\"Building Research and Information\": This journal focuses on buildings, building stocks and their supporting systems. Unique to BRI is a holistic and transdisciplinary approach to buildings, which acknowledges the complexity of the built environment and other systems over their life. Published articles utilize conceptual and evidence-based approaches which reflect the complexity and linkages between culture, environment, economy, society, organizations, quality of life, health, well-being, design and engineering of the built environment. Impact Factor 3.468 \n\n\"Journal of Building Performance Simulation\": This international, peer-reviewed journal publishes high quality research and state of the art “integrated” papers to promote scientifically thorough advancement of all the areas of non-structural performance of a building and particularly in heat, air, moisture transfer. Impact Factor: 2.603 \n\n\"Building Simulation\": This international journal publishes original, high quality, peer-reviewed research papers and review articles dealing with modeling and simulation of buildings including their systems. The goal is to promote the field of building science and technology to such a level that modeling will eventually be used in every aspect of building construction as a routine instead of an exception. Of particular interest are papers that reflect recent developments and applications of modeling tools and their impact on advances of building science and technology. Impact Factor: 1.673 \n\n\n"}
{"id": "6531297", "url": "https://en.wikipedia.org/wiki?curid=6531297", "title": "CO-OP Financial Services", "text": "CO-OP Financial Services\n\nCO-OP Financial Services, formerly known as CO-OP Network and also known as CU Cooperative Systems, Inc., is a company that operates an interbank network connecting the ATMs of credit unions in the United States, with locations also in Canada and certain United States Navy bases overseas. It is the largest credit union-owned interbank network in the US.\n\nIt is headquartered in Rancho Cucamonga, California.\n\n\nCO-OP Financial Services also provides what the company calls shared branching, through its subsidiary Service Center Corporation, acquired in 2002. Members of 1,800 credit unions can perform most teller transactions at any one of the network's 5,200 branches. This system was founded in 1975 by five Detroit-area credit unions to minimize costs associated with having their own branches.\n"}
{"id": "12793322", "url": "https://en.wikipedia.org/wiki?curid=12793322", "title": "CSC – IT Center for Science", "text": "CSC – IT Center for Science\n\nCSC – IT Center for Science Ltd. (also known as Finnish IT center for science) provides IT support and modeling, computing and information services for academia, research institutes and companies in Finland. It is owned entirely by the Finnish state, administered by the Ministry of Education and Culture, and operated on a non-profit principle. CSC has provided computational and network services since 1971.\n\nThe CSC servers create a common user environment. The service environment is made up of several supercomputers, database servers and information servers that are all linked together with a fast data transfer connection into one metacomputer. Extensive data and archive servers are available for saving results. \n\nCSC also manages the FUNET network, which is the Finnish national research and education network that provides fast internet access to universities and other academic institutions as well as some governmental agencies.\n\n"}
{"id": "931106", "url": "https://en.wikipedia.org/wiki?curid=931106", "title": "CTIA and GTIA", "text": "CTIA and GTIA\n\nColor Television Interface Adaptor (CTIA) and its successor Graphic Television Interface Adaptor (GTIA) are custom chips used in the Atari 8-bit family of computers and in the Atari 5200 console. In these systems, a CTIA or GTIA chip works together with ANTIC to produce video display. ANTIC generates the playfield graphics (text and bitmap) while CTIA/GTIA provides the color for the playfield and adds overlay objects known as player/missile graphics (sprites). Under the direction of Jay Miner, the CTIA/GTIA chips were designed by George McLeod with technical assistance of Steve Smith.\n\n\"Color Television Interface Adaptor\" and \"Graphic Television Interface Adaptor\" are names of the chips as stated in the Atari field service manual. Various publications named the chips differently, sometimes using the alternative spelling \"Adapter\" or \"Graphics\", or claiming that the \"C\" in \"CTIA\" stands for Colleen/Candy and \"G\" in \"GTIA\" is for George.\n\nAtari had built their first display driver chip, the Television Interface Adaptor but universally referred to as the TIA, as part of the Atari 2600 console. The TIA display logically consisted of two primary sets of objects, the \"players\" and \"missiles\" that represented moving objects, and the \"playfield\" which represented the static background image on which the action took place. The chip used data in memory registers to produce digital signals that were converted in realtime via a digital-to-analog converter and RF modulator to produce a television display.\n\nThe conventional way to draw the playfield is to use a bitmap held in a frame buffer, in which each memory location in the frame buffer represents one or more locations on the screen. In the case of the 2600, which normally used a resolution of 160x192 pixels, a frame buffer would need to have at least 160x192/8 = 3840 bytes of memory. Built in an era where RAM was very expensive, the TIA could not afford this solution.\n\nInstead, the system implemented a display system that used a single 20-bit memory register that could be copied or mirrored on the right half of the screen to make what was effectively a 40-bit display. Each location could be displayed in one of four colors, from a palette of 128 possible colors. The TIA also included several other display objects, the \"players\" and \"missiles\". These consisted of two 8-bit wide objects known as \"players\", a single 1-bit object known as the \"ball\", and two 1-bit \"missiles\". All of these objects could be moved to arbitrary horizontal locations via settings in other registers.\n\nThe key to the TIA system, and the 2600's low price, was that the system implemented only enough memory to draw a single line of the display, all of which held in registers. To draw an entire screen full of data, the user code would wait until the television display reached the right side of the screen and update the registers for the playfield and player/missiles to correctly reflect the next line on the display. This technique drew the screen line-by-line from program code on the ROM cartridge, a technique known as \"racing the beam\".\n\nAtari initially estimated that the 2600 would have short market lifetime of three years when it was designed in 1976, which meant the company would need a new design by 1979. Initially this new design was simply an updated 2600-like game console, and was built around a similar basic design, simply updated. Work on what would become the CTIA started in 1977, and aimed at delivering a system with twice the resolution and twice the number of colours. Moreover, by varying the number of colours in the playfield, much higher resolutions up to 320 pixels horizontally could be supported. Players and missiles were also updated, including four 8-bit players and four 2-bit missiles, but also allowing an additional mode to combine the four missiles into a fifth player.\n\nShortly after design began, the home computer revolution started in earnest in the later half of 1977. In response, Atari decided to release two versions of the new machine, a low-end model as a games console, and a high-end version as a home computer. In either role, a more complex playfield would be needed, especially support for character graphics in the computer role. Design of the CTIA was well advanced at this point, so instead of a redesign a clever solution was provided by adding a second chip that would effectively automate the process of racing the beam. Instead of the user's programming updating the CTIA's registers based on its interrupt timing, the new ANTIC would handle this chore, reading data from a framebuffer and feeding that to the CTIA on the fly.\n\nAs a result of these changes, the new chips provide greatly improved number and selection of graphics modes over the TIA. Instead of a single playfield mode with 20 or 40 bits of resolution, the CTIA/ANTIC pair can display six text modes and eight graphics modes with various resolutions and color depths, allowing the programmer to choose a balance between resolution, colours, and memory use for their display.\n\nThe original design of the CTIA chip also included three additional color interpretations of the normal graphics modes. This feature provides alternate expressions of ANTIC's high-resolution graphics modes presenting 1 bit per pixel, 2 colors with one-half color clock wide pixels as 4 bits per pixel, up to 16 colors, two-color clock wide pixels. This feature was ready before the computers' November 1979 debut, but was delayed so much in the development cycle that Atari had already ordered a batch of about 100,000 CTIA chips with the graphics modes missing. Not wanting to throw away the already-produced chips, the company decided to use them in the initial release of the Atari 400 and 800 models in the US market. The CTIA-equipped computers, lacking the 3 extra color modes, were shipped until October–November 1981. From this point, all new Atari units were equipped with the new chip, now called GTIA, that supported the new color interpretation modes.\n\nThe original Atari 800/400 operating system supported the GTIA alternate color interpretation modes from the start, which allowed for easy replacement of the CTIA with the GTIA once it was ready. Atari authorized service centers would install a GTIA chip in CTIA-equipped computers free of charge if the computer was under warranty; otherwise the replacement would cost $62.52.\n\nGTIA was also mounted in all later Atari XL and XE computers and Atari 5200 consoles.\n\nThe list below describes CTIA/GTIA's inherent hardware capabilities meaning the intended functionality of the hardware itself, not including results achieved by CPU-serviced interrupts or display kernels driving frequent register changes.\n\nCTIA/GTIA is a television interface device with the following features:\n\n\nAtari, Inc. intended to combine functions of the ANTIC and GTIA chips in one integrated circuit to reduce production costs of Atari computers and 5200 consoles. Two such prototype circuits were being developed, however none of them entered production.\n\nThe Atari 8-bit computers map CTIA/GTIA to the $D0xx page and the Atari 5200 console maps it to the $C0xx page.\n\nCTIA/GTIA provides 54 Read/Write registers controlling Player/Missile graphics, Playfield colors, joystick triggers, and console keys. Many CTIA/GTIA register addresses have dual purposes performing different functions as a Read vs a Write register. Therefore, no code should read Hardware registers expecting to retrieve the previously written value.\n\nThis problem is solved for many write registers by Operating System Shadow registers implemented in regular RAM as places to store the last value written to registers. Operating System Shadow registers are copied from RAM to the hardware registers during the Vertical Blank. Therefore, any write to hardware registers which have corresponding shadow registers will be overwritten by the value of the Shadow registers during the next Vertical Blank.\n\nSome Write registers do not have corresponding Shadow registers. They can be safely written by an application without the value being overwritten during the vertical blank. If the application needs to know the last state of the register then it is the responsibility of the application to remember what it wrote.\n\nOperating System Shadow registers also exist for some Read registers where reading the value directly from hardware at an unknown stage in the display cycle may return inconsistent results.\n\nIn the individual register listings below the following legend applies:\n\nThese registers specify the horizontal position in color clocks of the left edge (the high bit of the GRAF* byte patterns) of Player/Missile objects. Coordinates are always based on the display hardware's color clock engine, NOT simply the current Playfield display mode. This also means Player/Missile objects can be moved into overscan areas beyond the current Playfield mode.\n\nNote that while Missile objects bit patterns share the same byte for displayed pixels (GRAFM) each Missile can be independently positioned. When the \"fifth Player\" option is enabled (See PRIOR/GPRIOR register) turning the four Missiles into one \"Player\" the Missiles switch from displaying the color of the associated Player object to displaying the value of COLPF3. The new \"Player's\" position on screen must be set by specifying the position of each Missile individually.\n\nPlayer/Missile pixels are only rendered within the visible portions of the GTIA's pixel engine. Player/Missile objects are not rendered during the horizontal blank or the vertical blank. However, an object can be partially within the horizontal blank. The objects' pixels that fall outside of the horizontal blank are then within the visible portion of the display and can still register collisions. The horizontal position range of visible color clocks is $22/34 to $DD/221.\n\nTo remove a Player/Missile object from the visible display area horizontal positions (left) 0 and (right) $DE/222 (or greater) will insure no pixels are rendered regardless of the size of the Player/Missile object and so no unintentional collisions can be flagged.\n\nHorizontal Position of Player 0\n\nHorizontal Position of Player 1\n\nHorizontal Position of Player 2\n\nHorizontal Position of Player 3\n\nHorizontal Position of Missile 0\n\nHorizontal Position of Missile 1\n\nHorizontal Position of Missile 2\n\nHorizontal Position of Missile 3\nBelow are the color clock coordinates of the left and right edges of the possible Playfield sizes, useful when aligning Player/Missile objects to Playfield components:\n\nThree sizes can be chosen: Normal, Double, and Quad width. The left edge (See Horizontal Coordinates) is fixed and the size adjustment expands the Player or Missile toward the right in all cases.\n\n\nNote that in Quad size a single Player/Missile pixel is the same width as an Antic Mode 2 text character. Player/Missile priority selection mixed with Quad width Player Missile graphics can be used to create multiple text colors per Mode line.\n\nEach Player has its own size control register:\n\nSize of Player 0\n\nSize of Player 1\n\nSize of Player 2\n\nSize of Player 3\n\nPlayer size controls:\n\nValues:\n\nAll Missile sizes are controlled by one register, but each Missile can be sized independently of the others. When the \"fifth Player\" option is enabled (See PRIOR/GPRIOR register) turning the four Missiles into one \"Player\" the width is still set by specifying the size for each Missile individually.\n\nValues:\n\nEach Player object has its own 8-bit pattern register. Missile objects share one register with 2 bits per each Missile. Once a value is set it will continue to be displayed on each scan line. With no other intervention by CPU or ANTIC DMA to update the values the result is vertical stripe patterns the height of the screen including overscan areas. This mode of operation does not incur a CPU or DMA toll on the computer. It is useful for displaying alternate colored borders and vertical lines separating screen regions.\n\nGraphics pattern for Player 0\n\nGraphics pattern for Player 1\n\nGraphics pattern for Player 2\n\nGraphics pattern for Player 3\n\nEach Player is 8 bits (pixels) wide. Where a bit is set, a pixel is displayed in the color assigned to the color register associated to the Player. Where a bit is not set the Player object is transparent, showing Players, Missiles, Playfield pixels, or the background color. Pixel output begins at the horizontal position specified by the Player's HPOS value with the highest bit output first.\n\nGraphics pattern for all Missiles\n\nEach Missile is 2 bits (pixels) wide. Where a bit is set, a pixel is displayed in the color assigned to the color register for the Player associated to the Missile. When Fifth Player is enabled (see PRIOR/GPRIOR) the Missiles pixels all display COLPF3. Where a bit is not set the Missile object is transparent, showing Players, Missiles, Playfield pixels, or the background color. Pixel output begins at the horizontal position specified by the Missile's HPOS value with the highest bit output first.\n\nMissile Values:\n\nCTIA/GTIA has 60 bits providing automatic detection of collisions when Player, Missile, and Playfield pixels intersect. A single bit indicates a non-zero pixel of the Player/Missile object has intersected a pixel of a specific color register. There is no collision registered for pixels rendered using the background color register/value. This system provides instant, pixel-perfect overlap comparison without expensive CPU evaluation of bounding box or image bitmap masking.\n\nThe actual color value of an object is not considered. If Player, Missile, Playfield, and Background color registers are all the same value making the objects effectively \"invisible\", the intersections of objects will still register collisions. This is useful for making hidden or secret objects and walls.\n\nObscured intersections will also register collisions. If a Player object priority is behind a Playfield color register and another Player object priority is higher (foreground) than the Playfield, and the foreground Player pixels obscure both the Playfield and the Player object behind the Playfield, then the collision between the Playfield and both the background and foreground Player objects will register along with the collision between the foreground and background Player objects.\n\nNote that there is no Missile to Missile collision.\n\nPlayer/Missile collisions can only occur when Player/Missile object pixels occur within the visible portions of the display. Player/Missile objects are not rendered during the horizontal blank or the vertical blank. The range of visible color clocks is 34 to 221, and the visible scan lines range from line 8 through line 247. Player/Missile data outside of these coordinates are not rendered and will not register collisions. An object can be partially within the horizontal blank. The objects' pixels that fall outside of the horizontal blank are within the visible portion of the display and can still register collisions.\n\nTo remove a Player/Missile object from the visible display area horizontal positions (left) 0 and (right) 222 (or greater) will insure no pixels are rendered regardless of the size of the Player/Missile object and so no unintentional collisions can be flagged.\n\nFinally, Player, Missile, and Playfield objects collision detection is real-time, registering a collision as the image pixels are merged and output for display. Checking an object's collision bits before the object has been rendered by CTIA/GTIA will show no collision.\n\nOnce set, collisions remain in effect until cleared by writing to the HITCLR register. Effective collision response routines should occur after the targeted objects have been displayed, or at the end of a frame or during the Vertical Blank to react to the collisions and clear collisions before the next frame begins.\n\nBecause collisions are only a single bit, collisions are quite obviously not additive. No matter how many times and different locations a collision between pixels occurs within one frame there is only 1 bit to indicate there was a collision. A set collision bit informs a program that it can examine the related objects to identify collision locations and then decide how to react for each location.\n\nSince HITCLR and collision detection is real-time, Display List Interrupts can divide the display into sections with HITCLR used at the beginning of each section and separate collision evaluation at the end of each section.\n\nWhen the \"fifth Player\" option is enabled (See PRIOR/GPRIOR register) the only change is the Missiles 0 to 3 switch from displaying the color of the associated Player object to displaying the value of COLPF3. The new \"Player's\" collisions are still reported for the individual Missiles.\n\nEach bit indicates a pixel of the Player/Missile object has intersected a pixel of the specified Playfield color object. There is no collision registered for the background color.\n\nObscured intersections will also register collisions. If a Player/Missile object priority is behind a Playfield color register and another Player/Missile object priority is higher (foreground) than the Playfield, and the foreground Player/Missile pixels obscure both the Playfield and the Player/Missile object behind the Playfield, then the collision between the Playfield and both the background and foreground Player/Missile objects will register.\n\nHigh-resolution, 1/2 color clock pixel modes (ANTIC Modes 2, 3, and F) are treated differently. The \"background\" color rendered as COLPF2 where pixel values are 0 does not register a collision. High-resolution pixels are rendered as the luminance value from COLPF1. The pixels are grouped together in color clock-wide pairs (pixels 0 and 1, pixels 2 and 3, continuing to pixels 318 and 319). Where either pixel of the pair is 1 a collision is detected between the Player or Missile pixels and Playfield color COLPF2.\n\nGTIA modes 9 and 11 do not process playfield collisions. In GTIA mode 10 Playfield collisions will register where Playfield pixels use COLPF0 through COLPF3\n\nMissile 0 to Playfield collisions\n\nMissile 1 to Playfield collisions\n\nMissile 2 to Playfield collisions\n\nMissile 3 to Playfield collisions\n\nPlayer 0 to Playfield collisions\n\nPlayer 1 to Playfield collisions\n\nPlayer 2 to Playfield collisions\n\nPlayer 3 to Playfield collisions\n\nMissiles collide with Players and Playfields. There is no Missile to Missile collision.\n\nMissile 0 to Player collisions\n\nMissile 1 to Player collisions\n\nMissile 2 to Player collisions\n\nMissile 3 to Player collisions\n\nA collision between two players sets the collision bit in both Players' collision registers. When Player 0 and Player 1 collide, Player 0's collision bit for Player 1 is set, and Player 1's collision bit for Player 0 is set.\n\nA Player cannot collide with itself, so its bit is always 0.\n\nPlayer 0 to Player collisions\nPlayer 1 to Player collisions\nPlayer 2 to Player collisions\n\nPlayer 3 to Player collisions\n\nAll Player/Missile objects' pixels and all Playfield pixels in the default CTIA/GTIA color interpretation mode use indirection to specify color. Indirection means that the values of the pixel data do not directly specify the color, but point to another source of information for color. CTIA/GTIA contain hardware registers that set the values used for colors, and the pixels' information refer to these registers. The palette on the Atari is 8 luminance levels of 16 colors for a total 128 colors. The color indirection flexibility allows a program to tailor the screen's colors to fit the purpose of the program's display.\n\nAll hardware color registers have corresponding shadow registers.\n\nSHADOW: PCOLOR0 $02C0\n\nColor/luminance of Player and Missile 0.\n\nWhen GTIA 9-color mode is enabled (PRIOR/GPRIOR value $80) this register is used for the border and background (Playfield pixel value 0), rather than COLBK.\n\nSHADOW: PCOLOR1 $02C1\n\nColor/luminance of Player and Missile 1.\n\nSHADOW: PCOLOR2 $02C2\n\nColor/luminance of Player and Missile 2.\n\nSHADOW: PCOLOR3 $02C3\n\nColor/luminance of Player and Missile 3.\n\nSHADOW: COLOR0 $02C4\n\nColor/luminance of Playfield 0.\n\nSHADOW: COLOR1 $02C5\n\nColor/luminance of Playfield 1.\n\nThis register is used for the set pixels (value 1) in ANTIC text modes 2 and 3, and map mode F. Only the luminance portion is used and is OR'd with the color value of COLPF2. In other Character and Map modes this register provides the expected color and luminance for a pixel.\n\nSHADOW: COLOR2 $02C6\n\nColor/luminance of Playfield 2.\n\nThis register is used for Playfield background color of ANTIC text modes 2 and 3, and map mode F. That is, where pixel value 0 is used. In other Character and Map modes this register provides the expected color and luminance for a pixel.\n\nSHADOW: COLOR3 $02C7\n\nColor/luminance of Playfield 3\n\nCOLPF3 is available is several special circumstances: \n\nSHADOW: COLOR4 $02C8\n\nColor/luminance of Playfield background.\n\nThe background color is displayed where no other pixel occurs through the entire overscan display area. The following exceptions occur for the background:\n\nColor Registers' Bits:\n\nThe high nybble of the color register specifies one of 16 colors color ($00, $10, $20... to $F0). \nThe low nybble of the register specifies one of 16 luminance values ($00, $01, $02... to $0F).\n\nIn the normal color interpretation mode the lowest bit is not significant and only 8 luminance values are available ($00, $02, $04, $06, $08, $0A, $0C, $0E), so the complete color palette is 128 color values.\n\nIn GTIA color interpretation mode $4 (luminance-only mode) the full 16 bits of luminance values are available for Playfield pixels providing a palette of 256 colors. Any Player/Missile objects displayed in this mode are colored by indirection which still uses the 128 color palette.\n\nIn normal color interpretation mode the pixel values range from $0 to $3 ordinarily pointing to color registers COLBK, COLPF0, COLPF1, COLPF2 respectively. The color text modes also include options to use COLPF3 for certain ranges of character values. See ANTIC's graphics modes for more information.\n\nWhen Player/Missile graphics patterns are enabled for display where the graphics patterns bits are set the color displayed comes from the registers assigned to the objects.\n\nThere are exceptions for color generation and display: \n\nColor Registers' Use per ANTIC Character Modes:\n\nColor Registers' Use per ANTIC Map Modes:\n\nColor Registers' Use per GTIA Modes (ANTIC F):\n\nPlayer/Missile colors are always available for Player/Missile objects in all modes, though colors may be modified when the special GTIA modes (16 shades/16 color) are in effect.\n\nSHADOW: GPRIOR $026F\n\nThis register controls several CTIA/GTIA color management features: The GTIA Playfield color interpretation mode, Multi-Color Player objects, the Fifth Player, and Player/Missile/Playfield priority.\n\nGTIA Playfield Color Interpretations<br>\nCTIA includes only one default color interpretation mode for the ANTIC Playfield data stream. That is the basic functionality assumed in the majority of the ANTIC and CTIA/GTIA discussion unless otherwise noted. GTIA includes three alternate color interpretations modes for Playfield data. These modes work by pairing adjacent color clocks from ANTIC, thus the pixels output by GTIA are always two color clocks wide. Although these modes can be engaged while displaying any ANTIC Playfield Mode, the full color palette possible with these GTIA color processing options are only realized in the ANTIC Modes based on 1/2 color clock pixels (ANTIC modes 2, 3, F.) These GTIA options are most often used with a Mode F display. The special GTIA color processing modes also alter the display or behavior of Player/Missile graphics in various ways.\n\nThe color interpretation control is a global function of GTIA affecting the entire screen. GTIA is not inherently capable of mixing on one display the various GTIA color interpretation modes and the default CTIA mode needed for most ANTIC Playfields. Mixing color interpretation modes requires software writing to the PRIOR register as the display is generated (usually, by a Display List Interrupt).\n\nPRIOR bits 7 and 6 provide four values specifying the color interpretation modes:\n\n16 Shades<br>\nThis mode uses the COLBK register to specify the background color. Rather than using indirection, pixel values directly represent Luminance. This mode allows all four luminance bits to be used in the Atari color palette and so is capable of displaying 256 colors.\n\nPlayer/Missile graphics (without the fifth Player option) display properly in this mode, however collision detection with the Playfield is disabled. Playfield priority is always on the bottom. When the Missiles are switched to act as a fifth Player then where the Missile objects overlap the Playfield the Missile pixels luminance merges with the Playfield pixels' Luminance value.\n\n9 Color<br>\nUnlike the other two special GTIA modes, this mode is entirely driven by color indirection. All nine color registers work on the display for pixel values 0 through 8. The remaining 7 pixel values repeat previous color registers.\n\nThe pixels are delayed by one color clock (half a GTIA mode pixel) when output. This offset permits interesting effects. For an example, page flipping rapidly between this mode and a different GTIA mode produces a display with apparent higher resolution and greater number of colors.\n\nThis mode is unique in that is uses color register COLPM0 for the border and background (Playfield 0 value pixels) rather than COLBK.\n\nPlayer/Missile graphics display properly with the exception that Player/Missile 0 are not distinguishable from the background pixels, since they use the same color register, COLPM0. The Playfield pixels using the Player/Missile colors are modified by priority settings as if they were Player/Missile objects and so can affect the display of Players/Missiles. (See discussion later about Player/Missile/Playfield priorities).\n\nThe Playfield pixels using Player/Missile colors do not trigger collisions when Player/Missile objects overlay them. However, Player/Missile graphics overlapping Playfield colors COLPF0 to COLPF3 will trigger the expected collision.\n\n16 Colors<br>\nThis mode uses the COLBK register to specify the luminance of all Playfield pixels (values $1/1 through $F/15.) The least significant bit of the luminance value is not observed, so only the standard/CTIA 8 luminance values are available ($0, $2, $4, $6, $8, $A, $C, $E). Additionally, the background itself uses only the color component set in the COLBK register. The luminance value of the background is forced to 0. As with the Luminance mode indirection is disabled and pixel values directly represent a color.\n\nNote that the color component of the background also merges with the playfield pixels. Colors other than black for the background reduce the overall number of colors displayed in the mode.\nPlayer/Missile graphics (without the fifth Player option) display properly in this mode, however collision detection with the Playfield is disabled. Playfield priority is always on the bottom. When the Missiles are switched to act as a fifth Player then where the Missile objects overlap the Playfield the Missile pixels inherit the Playfield pixels' Color value.\n\nMulti-Color Player<br>\nPRIOR bit 5, value $20/32 enables Multi-Color Player objects. Where pixels of two Player/Missile objects overlap a third color appears. This is implemented by eliminating priority processing between pairs of Player/Missile objects resulting in CTIA/GTIA performing a bitwise OR of the two colored pixels to output a new color.\n\nExample: A Player pixel with color value $98/152 (blue) overlaps a Player pixel with color value $46/70 (red) resulting in a pixel color of $DE/228 (light green/yellow).\n\nThe Players/Missiles pairs capable of Multi-Color output: \n\nFifth Player<br>\nPRIOR bit 4, value $10/16 enables Missiles to become a fifth Player. No functional change occurs to the Missile other than the color processing of the Missiles. Normally the Missiles display using the color of the associated Player. When Fifth Player is enabled all Missiles display the color of Playfield 3 (COLPF3). Horizontal position, size, vertical delay, and Player/Missile collisions all continue to operate the same way. The priority of the Fifth Player for Player objects pixel intersections is COLPF3, but the Fifth Player's pixels have priority over all Playfield colors.\n\nThe color processing change also causes some exceptions for the Missiles' display in GTIA's alternative color modes:\n\nThe Fifth Player introduces an exception for Priority value $8 (bits 1000) (See Priority discussion below.)\n\nPriority<br>\nPRIOR bits 3 to 0 provide four Player/Missile and Playfield priority values that determine which pixel value is displayed when Player/Missile objects pixels and Playfield pixels intersect. The four values provide specific options listed in the Priority chart below. \"PM\" mean normal Player/Missile implementation without the Fifth Player. The Fifth Player, \"P5\", is shown where its priority occurs when it is enabled.\n\nThe chart is accurate for ANTIC Playfield Character and Map modes using the default (CTIA) color interpretation mode. GTIA color interpretation modes, and the ANTIC modes based on high-resolution, 1/2 color clock pixels behave differently (noted later).\n\nIf multiple bits are set, then where there is a conflict CTIA/GTIA outputs a black pixel—Note that black means actual black, not simply the background color, COLBK.\n\nAlthough the Fifth Player is displayed with the value of COLPF3, its priority is above all Playfield colors. This produces an exception for Priority value $8 (Bits 1000). In this mode Playfield 0 and 1 are higher priority than the Players, and the Players are higher priority than Playfield 2 and 3. Where Playfield 0 or 1 pixels intersect any Player pixel the result displayed is the Playfield pixel. However, if the Fifth player also intersects the same location, its value is shown over the Playfield causing it to appear as if Playfield 3 has the highest priority. If the Playfield 0 or 1 pixel is removed from this intersection then the Fifth Player's pixel has no Playfield pixel to override and so also falls behind the Player pixels.\n\nWhen the Priority bits are all 0 a different effect occurs—Player and Playfield pixels are logically OR'd together in the a manner similar to the Multi-Color Player feature. In this situation Players 0 and 1 pixels can mix with Playfield 0 and 1 pixels, and Players 2 and 3 pixels can mix with Playfield 2 and 3 pixels. Additionally, when the Multi-Color Player option is used the resulting merged Players' color can also mix with the Playfield producing more colors. When all color merging possibilities are considered, the CTIA/GTIA hardware can output 23 colors per scan line. Starting with the background color as the first color, the remaining 22 colors and color merges are possible:\n\nWhen Priority bits are all 0 the Missiles colors function the same way as the corresponding Players as described above. When Fifth Player is enabled, the Missile pixels cause the same color merging as shown for COLPF3 in the table above (colors 19 through 22).\n\nPriority And High-Resolution Modes<br>\nThe priority result differ for the Character and Map modes using high-resolution, 1/2 color clock pixels—ANTIC modes 2, 3, and F. These priority handling differences can be exploited to produce color text or graphics in these modes that are traditionally thought of as \"monochrome\".\n\nIn these ANTIC modes COLPF2 is output as the \"background\" of the Playfield and COLBK is output as the border around the Playfield. The graphics or glyph pixels are output using only the luminance component of COLPF1 mixed with the color component of the background (usually COLPF2).\n\nThe priority relationship between Players/Missiles, and COLPF2 work according to the priority chart below. Player/Missile pixels with higher priorities will replace COLPF2 as the \"background\" color. COLPF1 always has the highest priority and cannot be obscured by Players or Missiles. The glyph/graphics pixels use the color component of highest priority color (Playfield, Player, or Missile), and the luminance component of COLPF1. Note that this behavior is also consistent where Player/Missile priority conflicts result in true black for the \"background\". In effect, the color value CTIA/GTIA finally uses for the \"background\" color \"tints\" the COLPF1 foreground glyph/graphics pixels.\n\nVertical Delay P/M Graphics\n\nThis register is used to provide single scan line movement when Double Line Player/Missile resolution is enabled in ANTIC's DMACTL register. This works by masking ANTIC DMA updates to the GRAF* registers on even scan lines, causing the graphics pattern to shift down one scan line.\n\nSince Single Line resolution requires ANTIC DMA updates on each scan line and VDELAY masks the updates on even scan lines, then this bit reduces Single line Player/Missile resolution to Double line.\n\nGraphics Control\n\nGRACTL controls CTIA/GTIA's receipt of Player/Missile DMA data from ANTIC and toggles the mode of Joystick trigger input.\n\nReceipt of Player/Missile DMA data requires CTIA/GTIA be configured to receive the data. This is done with a pair of bits in GRACTL that match a pair of bits in ANTIC's DMACTL register that direct ANTIC to send Player data and Missile data. GRACTL's Bit 0 corresponds to DMACTL's Bit 2, enabling transfer of Missile data. GRACTL's Bit 1 corresponds to DMACTL's Bit 3, enabling transfer of Player data. These bits must be set for GTIA to receive Player/Missile data from ANTIC via DMA. When Player/Missile graphics are being operated directly by the CPU then these bits must be off.\n\nThe joystick trigger registers report the pressed/not pressed state in real-time. If a program's input polling may not be frequent enough to catch momentary joystick button presses, then the triggers can be set to lock in the closed/pressed state and remain in that state even after the button is released. Setting GRACTL Bit 2 enables the latching of all triggers. Clearing the bit returns the triggers to the unlatched, real-time behavior.\n\nClear Collisions\n\nAny write to this register clears all the Player/Missile collision detection bits.\n\nSHADOW: STRIG0 $0284\n\nJoystick 0 trigger\n\nSHADOW: STRIG1 $0285\n\nJoystick 1 trigger.\n\nSHADOW: STRIG2 $0286\n\nJoystick 2 trigger.\n\nSHADOW: STRIG3 $0287\n\nJoystick 3 trigger\n\nBits 7 through 1 are always 0. Bit 0 reports the state of the joystick trigger. Value 1 indicates the trigger is not pressed. Value 0 indicates the trigger is pressed.\n\nThe trigger registers report button presses in real-time. The button pressed state will instantly clear when the button is released.\n\nThe triggers may be configured to latch, that is, lock, in the pressed state and remain that way until specifically cleared. GRACTL bit 2 enables the latch behavior for all triggers. Clearing GRACTL bit 2 returns all triggers to real-time behavior.\n\nPAL flags.\n\nThis register reports the display standard for the system. When Bits 3 to 0 are set to 1 (value $f/15) the system is operating in NTSC. When the bits are zero the system is operating in PAL mode.\n\nConsole Speaker\n\nBit3 controls the internal speaker of the Atari 800/400. In later models the console speaker is removed and the sound is mixed with the regular POKEY audio signals for output to the monitor port and RF adapter. The Atari OS uses the console speaker to output the keyboard click and the bell/buzzer sound.\n\nThe Operating System sets the speaker bit during the Vertical Blank routine. Repeatedly writing 0 to the bit will produce a 60 Hz buzzing sound as the Vertical Blank resets the value. Useful tones can be generated using 6502 code effectively adding a fifth audio channel, albeit a channel requiring CPU time to maintain the audio tones.\n\nConsole Keys\n\nA bit is assigned to report the state of each of the special console keys, Start, Select, and Option. Bit value 0 indicates a key is pressed and 1 indicates the key is not pressed. Key/Bit values:\n\n\nA hardware \"sprite\" system is handled by CTIA/GTIA. The official ATARI name for the sprite system is \"Player/Missile Graphics\", since it was designed to reduce the need to manipulate display memory for fast-moving objects, such as the \"player\" and his weapons, \"missiles\", in a shoot 'em up game.\n\nA Player is essentially a glyph 8 pixels wide and 256 TV lines tall, and has two colors: the background (transparent) (codice_1 in the glyph) and the foreground (codice_2). A Missile object is similar, but only 2 pixels wide. CTIA/GTIA combines the Player/Missile objects' pixels with the Playfield pixels according to their priority. Transparent (codice_1) player pixels have no effect on the Playfield and display either a Playfield or background pixel without change. All Player/Missile objects' normal pixel width is one color clock. A register value can set the Player or Missile pixels' width to 1, 2, or 4 color clocks wide.\n\nThe Player/Missile implementation by CTIA/GTIA is similar to the TIA's. A Player is an 8-bit value or pattern at a specified horizontal position which automatically repeats for each scan line or until the pattern is changed in the register. Missiles are 2-bits wide and share one pattern register, so that four, 2-bit wide values occupy the 8-bit wide pattern register, but each missile has an independent horizontal position and size. Player/Missile objects extend the height of the display including the screen border. That is, the default implementation of Player/Missile graphics by CTIA/GTIA is a stripe down the screen. While seemingly limited this method facilitates Player/Missile graphics use as alternate colored vertical borders or separators on a display, and when priority values are set to put Player/Missile pixels behind playfield pixels they can be used to add additional colors to a display. All Players and Missiles set at maximum width and placed side by side can cover the entire normal width Playfield.\n\nCTIA/GTIA supports several options controlling Player/Missile color. The PRIOR/GPRIOR register value can switch the four Missiles between two color display options—each Missile (0 to 3) expresses the color of the associated Player object (0 to 3) or all Missiles show the color of register COLPF3/COLOR3. When Missiles are similarly colored they can be treated as a fifth player, but correct placement on screen still requires storing values in all four Missile Horizontal Position registers. PRIOR/GPRIOR also controls a feature that causes the overlapping pixels of two Players to generate a third color allowing multi-colored Player objects at the expense of reducing the number of available objects. Finally, PRIOR/GPRIOR can be used to change the foreground/background layering (called, \"priority\") of Player/Missile pixels vs Playfield pixels, and can create priority conflicts that predictably affect the colors displayed.\n\nThe conventional idea of a sprite with an image/pattern that varies vertically is also built into the Player/Missile graphics system. The ANTIC chip includes a feature to perform DMA to automatically feed new pixel patterns to CTIA/GTIA as the display is generated. This can be done for each scan line or every other scan line resulting in Player/Missile pixels one or two scan lines tall. In this way the Player/Missile object could be considered an extremely tall character in a font, 8 bits/pixels wide, by the height of the display.\n\nMoving the Player/Missile objects horizontally is as simple as changing a register in the CTIA/GTIA (in Atari BASIC, a single POKE statement moves a player or missile horizontally). Moving an object vertically is achieved by either block moving the definition of the glyph to a new location in the Player or Missile bitmap, or by rotating the entire Player/Missile bitmap (128 or 256 bytes). The worst case rotation of the entire bitmap is still quite fast in 6502 machine language, even though the 6502 lacks a block-move instruction found in the 8080. Since the sprite is exactly 128 or 256 bytes long, the indexing can be easily accommodated in a byte-wide register on the 6502. Atari BASIC lacks a high speed memory movement command and moving memory using BASIC PEEK()s and POKE(s) is painfully slow. Atari BASIC programs using Player/Missile graphics have other options for performing high speed memory moves. One method is calling a short machine language routine via the USR() function to perform the memory moves. Another option is utilizing a large string as the Player/Missile memory map and performing string copy commands which result in memory movement at machine language speed.\n\nCareful use of Player/Missile graphics with the other graphics features of the Atari hardware can make graphics programming, particularly games, significantly simpler.\n\nThe GTIA chip is backward compatible with the CTIA, and adds 3 color interpretations for the 14 \"normal\" ANTIC Playfield graphics modes. The normal color interpretation of the CTIA chip is limited, per scanline, to a maximum of 4 colors in Map modes or 5 colors in Text modes (plus 4 colors for Player/Missile graphics) unless special programming techniques are used. The three, new color interpretations in GTIA provide a theoretical total of 56 graphics modes (14 ANTIC modes multiplied by four possible color interpretations). However, only the graphics modes based on high-resolution, 1/2 color clock pixels (that is, Antic text modes 2, 3, and graphics mode F) are capable of fully expressing the color palettes of these 3 new color interpretations. The three additional color interpretations use the information in two color clocks (four bits) to generate a pixel in one of 16 color values. This changes a mode F display from 2 colors per pixel, 320 pixels horizontally, one scan line per mode line, to 16 colors and 80 pixels horizontally. The additional color interpretations allow the following:\n\nOf these modes, Atari BASIC Graphics 9 is particularly notable. It enables the Atari to display gray-scale digitized photographs, which despite their low resolution were very impressive at the time. Additionally, by allowing 16 shades of a single hue rather than the 8 shades available in other graphics modes, it increases the amount of different colors the Atari could display from 128 to 256. Unfortunately, this feature is limited for use in this mode only, which due to its low resolution was not widely used.\n\nThe Antic 2 and 3 text modes are capable of displaying the same color ranges as mode F graphics when using the GTIA's alternate color interpretations. However, since the pixel reduction also applies and turns 8 pixel wide, 2 color text into 2 pixel wide, 16 color blocks these modes are unsuitable for actual text, and so these graphics modes are not popular outside of demos. Effective use of the GTIA color interpretation feature with text modes requires a carefully constructed character set treating characters as pixels. This method allows display of an apparent GTIA \"high resolution\" graphics mode that would ordinarily occupy 8K of RAM to instead use only about 2K (1K for the character set, and 1K for the screen RAM and display list.)\n\nThe GTIA also fixed an error in CTIA that caused graphics to be misaligned by \"half a color clock\". The side effect of the fix was that programs that relied on color artifacts in high-resolution monochrome modes would show a different pair of colors.\n\nAtari owners can determine if their machine is equipped with the CTIA or GTIA by executing the BASIC command codice_4. If the screen blackens after execution, the machine is equipped with the new GTIA chip. If it stays blue, the machine has a CTIA chip instead.\n\nThe last Atari XE computers made for the Eastern European market were built in China. Many if not all have a buggy PAL GTIA chip. The luma values in Graphics 9 and higher are at fault, appearing as stripes. Replacing the chip fixes the problem. Also, there have been attempts to fix faulty GTIA chips with some external circuitry.\n\nThe following is a summary of hardware/feature limitations some of which can be circumvented by software (display list interrupts or display kernels.):\n\nThe display generator outputs pixels within the horizontal range $22/34 to $DD/221. (188 color clocks wide). Player/Missile pixels outside this range are not generated and so do not register collisions.\n\nFour Player objects 8 bits wide, and four Missile objects 2 bits wide per scan line.\n\nOne Player/Missile bit may be 1, 2, or 4 color clocks wide.\n\nWhile the additional GTIA color interpretation modes can be enabled for any kind of ANTIC Playfield graphics, the full range of color capability is available only in the ANTIC modes based on high-resolution, 1/2 color clock pixels (that is, ANTIC text modes 2, 3, and graphics mode F).\n\nThe color palette is 128 colors (8 shades of 16 colors) for normal color interpretation mode, and GTIA modes $8 and $C (BASIC GRAPHICS mode 10 and 11).\n\nThe color palette is 256 colors (16 shades of 16 colors) for GTIA color interpretation mode $4 (BASIC GRAPHICS mode 9).\n\n9 color registers are available—four for Players/Missiles, four for Playfield pixels, and one register (COLPF3) shared by the Playfield and Fifth Player option.\n\nThe color produced from merging multiple Players and merging Player/Missile and Playfield graphics is the result of binary OR'ing the two source colors which limits the resulting, merged color value.\nAll Playfield and Player/Missile color registers plus the Multi-Color Player option combined with the Priority option merging Player/Missile and Playfield colors generate 23 colors per scan line.\n\nWhile GTIA pixels have 16 values ($0 to $F) color interpretation mode $8 (BASIC GRAPHICS mode 10) can use only 9 pixel values—one per color register.\n\n\n"}
{"id": "2379771", "url": "https://en.wikipedia.org/wiki?curid=2379771", "title": "Cadmium zinc telluride", "text": "Cadmium zinc telluride\n\nCadmium zinc telluride, (CdZnTe) or CZT, is a compound of cadmium, zinc and tellurium or, more strictly speaking, an alloy of cadmium telluride and zinc telluride. A direct bandgap semiconductor, it is used in a variety of applications, including semiconductor radiation detectors, photorefractive gratings, electro-optic modulators, solar cells, and terahertz generation and detection. The band gap varies from approximately 1.4 to 2.2 eV, depending on composition.\n\nRadiation detectors using CZT can operate in direct-conversion (or photoconductive) mode at room temperature, unlike some other materials (particularly germanium) which require liquid nitrogen cooling. Their relative advantages include high sensitivity for x-rays and gamma-rays, due to the high atomic numbers of Cd and Te, and better energy resolution than scintillator detectors. CZT can be formed into different shapes for different radiation-detecting applications, and a variety of electrode geometries, such as coplanar grids and small pixel detectors, have been developed to provide unipolar (electron-only) operation, thereby improving energy resolution.\n\n\n"}
{"id": "30188363", "url": "https://en.wikipedia.org/wiki?curid=30188363", "title": "Cherry Mobile", "text": "Cherry Mobile\n\nCherry Mobile is a Philippine mobile phone and electronics brand by Cosmic Technologies, established by Maynard Ngu in 2009. In 2010, barely two years after it started, Cherry Mobile was voted IT Company of the Year in the 3rd CyberPress Awards, upstaging some of the country’s technology giants. The company imports mobile phones manufactured by original design manufacturers in China and markets them under the Cherry Mobile brand. Most of Cherry Mobile's current lineup come with Wi-Fi, capacitive touch screens and run on the Android, Windows, and Windows Phone operating systems; a line of smart feature phones running Firefox OS named Cherry Mobile Ace was also sold by the company for a time before it was later discontinued.\n\nApart from being the first legal mobile phone brand with dual and triple Subscriber Identity Module (SIM) systems in the Philippines, Cherry Mobile also marketed the first Windows-enabled phone in the country as a result of an exclusive partnership with Microsoft.\n\nIn 2013 Cherry Mobile expands it's market by distributing their products in Thailand and in Myanmar.\n\nIn 2015, Cosmic Technologies, along with rival mobile device company MyPhone, partnered with Google for the Android One initiative, both of them releasing their respective devices based on Google's reference designs using MediaTek's quad-core MT6582 system-on-chip.\n\nIn June 2015, the Cosmic Technologies announced that they will be launching two Windows Phone handsets preloaded with Windows 10 Mobile under their mobile phone brand, namely the Alpha Prime 4 and the Alpha Prime 5, which was released on the second half of 2015. Both devices come with a 1.1 GHz quad core Qualcomm Snapdragon 210, support for WCDMA 900/2100, FDD-LTE B1/B3/B5/B7, and a 2 megapixel front-facing camera. The Alpha Prime 4 will have a 4-inch screen, a 5 megapixel main camera, 4 GB of ROM, and 512 MB of RAM, while the Alpha Prime 5 will have a 5-inch screen, 8 megapixel main camera, 8 GB of ROM and 1 GB of RAM.\n\nCherry Mobile Flare S7 series' specifications and design were leaked ahead of their launch on October 12, 2018. Its design for the S7 Deluxe and Plus series were using the long notch from the iPhone XS\n\nIn 2015, Cosmic Technologies introduced a new smartphone brand named Cubix, with its first product being the Cubix Cube. The Cubix brand is Cosmic's response to the growing segment of budget-priced midrange phones, usually dominated by Chinese brands such as Lenovo. The Cubix Cube has 2 GB of RAM, 16 GB of ROM, a 13 megapixel rear camera and an 8 megapixel front camera. The phone also comes bundled with the Lazada app and is available through Lazada Philippines.\n\n"}
{"id": "1012806", "url": "https://en.wikipedia.org/wiki?curid=1012806", "title": "Click fraud", "text": "Click fraud\n\nClick fraud is a type of fraud that occurs on the Internet in pay-per-click (PPC) online advertising. In this type of advertising, the owners of websites that post the ads are paid an amount of money determined by how many visitors to the sites click on the ads. Fraud occurs when a person, automated script or computer program imitates a legitimate user of a web browser, clicking on such an ad without having an actual interest in the target of the ad's link. Click fraud is the subject of some controversy and increasing litigation due to the advertising networks being a key beneficiary of the fraud. \n\nMedia entrepreneur and journalist John Battelle describes click fraud as the intentionally malicious, \"decidedly black hat\" practice of publishers illegitimately gaming paid search advertising by employing robots or low-wage workers to repeatedly click on each AdSense ad on their sites, thereby generating money to be paid by the advertiser to the publisher and to Google.\n\nPPC advertising is an arrangement in which webmasters (operators of websites), acting as publishers, display clickable links from advertisers in exchange for a charge per click. As this industry evolved, a number of advertising networks developed, which acted as middlemen between these two groups (publishers and advertisers). Each time a (believed to be) valid Web user clicks on an ad, the advertiser pays the advertising network, which in turn pays the publisher a share of this money. This revenue-sharing system is seen as an incentive for click fraud.\n\nThe largest of the advertising networks, Google's AdWords/AdSense and Yahoo! Search Marketing, act in a dual role, since they are also publishers themselves (on their search engines). According to critics, this complex relationship may create a conflict of interest. This is because these companies lose money to undetected click fraud when paying out to the publisher but make more money when collecting fees from the advertiser. Because of the spread between what they collect and pay out, unfettered click fraud would create short-term profits for these companies. \n\nA secondary source of click fraud is non-contracting parties, who are not part of any pay-per-click agreement. This type of fraud is even harder to police, because perpetrators generally cannot be sued for breach of contract or charged criminally with fraud. Examples of non-contracting parties are:\n\nAdvertising networks may try to stop fraud by all parties but often do not know which clicks are legitimate. Unlike fraud committed by the publisher, it is difficult to know who should pay when past click fraud is found. Publishers resent having to pay refunds for something that is not their fault. However, advertisers are adamant that they should not have to pay for phony clicks.\n\nClick fraud can be as simple as one person starting a small Web site, becoming a publisher of ads, and clicking on those ads to generate revenue. Often the number of clicks and their value is so small that the fraud goes undetected. Publishers may claim that small amounts of such clicking is an accident, which is often the case.\n\nMuch larger-scale fraud also occurs. Those engaged in large-scale fraud will often run scripts which simulate a human clicking on ads in Web pages.\nHowever, huge numbers of clicks appearing to come from just one, or a small number of computers, or a single geographic area, look highly suspicious to the advertising network and advertisers.\nClicks coming from a computer known to be that of a publisher also look suspicious to those watching for click fraud. A person attempting large-scale fraud, from one computer, stands a good chance of being caught.\n\nOne type of fraud that circumvents detection based on IP patterns uses existing user traffic, turning this into clicks or impressions. Such an attack can be camouflaged from users by using 0-size iframes to display advertisements that are programmatically retrieved using JavaScript. It could also be camouflaged from advertisers and portals by ensuring that so-called \"reverse spiders\" are presented with a legitimate page, while human visitors are presented with a page that commits click fraud. The use of 0-size iframes and other techniques involving human visitors may also be combined with the use of incentivized traffic, where members of \"Paid to Read\" (PTR) sites are paid small amounts of money (often a fraction of a cent) to visit a website and/or click on keywords and search results, sometimes hundreds or thousands of times every day Some owners of PTR sites are members of PPC engines and may send many email ads to users who do search, while sending few ads to those who do not. They do this mainly because the charge per click on search results is often the only source of revenue to the site. This is known as forced searching, a practice that is frowned upon in the Get Paid To industry.\n\nOrganized crime can handle this by having many computers with their own Internet connections in different geographic locations. Often, scripts fail to mimic true human behavior, so organized crime networks use Trojan code to turn the average person's machines into zombie computers and use sporadic redirects or DNS cache poisoning to turn the oblivious user's actions into actions generating revenue for the scammer. It can be difficult for advertisers, advertising networks, and authorities to pursue cases against networks of people spread around multiple countries.\n\nImpression fraud is when falsely generated ad impressions affect an advertiser's account. In the case of click-through rate based auction models, the advertiser may be penalized for having an unacceptably low click-through for a given keyword. This involves making numerous searches for a keyword without clicking of the ad. Such ads are disabled automatically, enabling a competitor's lower-bid ad for the same keyword to continue, while several high bidders (on the first page of the search results) have been eliminated.\n\nA hit inflation attack is a kind of fraudulent method used by some advertisement publishers to earn unjustified revenue on the traffic they drive to the advertisers’ Web sites. It is more sophisticated and harder to detect than a simple inflation attack.\n\nThis process involves the collaboration of two counterparts, a dishonest publisher, P, and a dishonest Web site, S.\nWeb pages on S contain a script that redirects the customer to P's Web site, and this process is hidden from the customer. So, when user U retrieves a page on S, it would simulate a click or request to a page on P's site. P's site has two kinds of webpages: a manipulated version, and an original version. The manipulated version simulates a click or request to the advertisement, causing P to be credited for the click-through. P selectively determines whether to load the manipulated (and thus fraudulent) script to U's browser by checking if it was from S. This can be done through the Referrer field, which specifies the site from which the link to P was obtained. All requests from S will be loaded with the manipulated script, and thus the automatic and hidden request will be sent.\n\nThis attack will silently convert every innocent visit to S to a click on the advertisement on P's page. Even worse, P can be in collaboration with several dishonest Web sites, each of which can be in collaboration with several dishonest publishers. If the advertisement commissioner visits the Web site of P, the non-fraudulent page will be displayed, and thus P cannot be accused of being fraudulent. Without a reason for suspecting that such collaboration exists, the advertisement commissioner has to inspect all the Internet sites to detect such attacks, which is infeasible.\n\nAnother proposed method for detection of this type of fraud is through use of association rules.\n\nOne major factor that affects the ranking of websites in organic search results is the CTR (Click-through Rate). That is the ratio of clicks to impressions, or in other words how many times a search result is clicked on, as compared to the number of times the listing appears in search results.\n\nIn contrast to PPC fraud, where a competitor leverages the services of a botnet, or low cost labour, to generate false clicks, in this case the objective is to beggar thy competitor by making their CTR rate as low as possible, thereby diminishing their ranking factor (position from the top of search results).\n\nBad actors will therefore generate false clicks on organic search results that they wish to promote, while avoiding search results they wish to demote. This technique can effectively create a cartel of business services controlled by the same bad actor, or be used to promote a certain political opinion etc. The scale of this issue is unknown, but is certainly evident to many website developers who pay close attention to the statistics in webmaster tools.\n\n\nIn 2004, California resident Michael Anthony Bradley created Google Clique, a software program that he claimed could let spammers defraud Google out of millions of dollars in fraudulent clicks, which ultimately led to his arrest and indictment.\n\nBradley used technology that he created for his other companies that took him five years to develop. Using this technology, he was able to demonstrate that fraud was possible, and was impossible for Google to detect.\n\nBradley notified Google of this security flaw, and was willing to work with them to close up some of these holes. However, Bradley was offered $500,000 for his software and technology by some of the world's top spammers. With this information, Bradley thought he could put a price of $100,000 on his technology, and offered to sell Google all rights to his technology, and they could make the Internet a better and safer place.\n\nWhen Bradley showed up to Google's offices, he demonstrated the software for them, and when they asked what he wanted, he had stated that he would consult for free if they wanted to purchase the rights to his technology. He explained the prior offer of $500,000 and said he knew he could get it, but would settle for $100,000 if they wanted to work together.\n\nBradley returned to Google's offices and was met by United States Secret Service officers who were undercover. They kept asking him what he wanted, and they even pushed a check for $100,000 to him. Bradley stated that this felt like blackmail and was not comfortable with this, and pushed the money away. Just then the Secret Service came in and arrested him.\n\nAuthorities said he was arrested while trying to extort $100,000 from Google in exchange for handing over the program.\n\nCharges were dropped without explanation on November 22, 2006; both the US Attorney's office and Google declined to comment. \"Business Week\" suggests that Google was unwilling to cooperate with the prosecution, as it would be forced to disclose its click fraud detection techniques publicly.\n\nOn June 18, 2016, Fabio Gasperini, an Italian citizen, was extradited to the United States on click fraud charges. An indictment charged Gasperini with:\nAccording to the U.S. government, Gasperini set up and operated a botnet of over 140,000 computers around the world. This was the first click fraud trial in the United States. If convicted of all counts, Gasperini risked up to 70 years in jail.\n\nSimone Bertollini, an Italian-American lawyer, represented Gasperini at trial. On August 9th, 2017 a jury acquitted Gasperini of all the felony charges of the indictment. Gasperini was convicted of one misdemeanor count of obtaining information without a financial gain. Gasperini was sentenced to the statutory maximum of one year imprisonment, a $100,000 fine, and one year of supervised release following incarceration. Shortly after he was credited with time served and sent back to Italy. An appeal is currently pending.\n\nProving click fraud can be very difficult, since it is hard to know who is behind a computer and what their intentions are. Often the best an advertising network can do is to identify which clicks are most likely fraudulent and not charge the account of the advertiser. Even more sophisticated means of detection are used, but none are foolproof.\n\nThe Tuzhilin Report produced as part of a click fraud lawsuit settlement, has a detailed and comprehensive discussion of these issues. In particular, it defines \"the Fundamental Problem of invalid (fraudulent) clicks\":\n\nThe PPC industry is lobbying for tighter laws on the issue. Many hope to have laws that will cover those not bound by contracts.\n\nA number of companies are developing viable solutions for click fraud identification and are developing intermediary relationships with advertising networks. Such solutions fall into two categories:\n\n\nIn a 2007 interview in Forbes, Google click fraud czar Shuman Ghosemajumder said that one of the key challenges in click fraud detection by third-parties was access to data beyond clicks, notably, ad impression data.\n\nClick fraud is less likely in cost per action models.\n\nThe fact that the middlemen (search engines) have the upper hand in the operational definition of invalid clicks is the reason for the conflict of interest between advertisers and the middlemen, as described above. This is manifested in the Tuzhilin Report as described above. The Tuzhilin report did not publicly define invalid clicks and did not describe the operational definitions in detail. Rather, it gave a high-level picture of the fraud-detection system and argued that the operational definition of the search engine under investigations is \"reasonable\". One aim of the report was to preserve the privacy of the fraud-detection system in order to maintain its effectiveness. This prompted some researchers to conduct public research on how the middlemen can fight click fraud. Since such research is presumably not tainted by market forces, there is hope that this research can be adopted to assess how rigorous a middleman is in detecting click fraud in future law cases. The fear that this research can expose the internal fraud-detection system of middlemen still applies. An example of such research is that done by Metwally, Agrawal and El Abbadi at UCSB. Other work by Majumdar, Kulkarni, and Ravishankar at UC Riverside proposes protocols for the identification of fraudulent behavior by brokers and other intermediaries in content-delivery networks.\n\n"}
{"id": "19127147", "url": "https://en.wikipedia.org/wiki?curid=19127147", "title": "ColorChecker", "text": "ColorChecker\n\nThe ColorChecker Color Rendition Chart (often referred to by its original name, the Macbeth ColorChecker or simply Macbeth chart) is a color calibration target consisting of a cardboard-framed arrangement of 24 squares of painted samples. The ColorChecker was introduced in a 1976 paper by McCamy, Marcus, and Davidson in the \"Journal of Applied Photographic Engineering\". The chart’s color patches have spectral reflectances intended to mimic those of natural objects such as human skin, foliage, and flowers, to have consistent color appearance under a variety of lighting conditions, especially as detected by typical color photographic film, and to be stable over time.\n\nThe ColorChecker Classic chart is a rectangular card measuring about , or in its original incarnation about , an aspect ratio approximately the same as that of 35 mm film. It includes 24 patches in a 4 × 6 grid, each slightly under square, made of matte paint applied to smooth paper, and surrounded by a black border. Six of the patches form a uniform gray lightness scale, and another six are primary colors typical of chemical photographic processes – red, green, blue, cyan, magenta, and yellow. The remaining colors include approximations of medium light and medium dark human skin, blue sky, the front of a typical leaf, and a blue chicory flower. The rest were chosen arbitrarily to represent a gamut \"of general interest and utility for test purposes\", though the orange and yellow patches are similarly colored to typical oranges and lemons.\n\nThere is also a ColorCheckerPassport, a smaller version of the ColorChecker Classic with the same 24 chips but in a tri-fold version with some additional patches on two of the pages. Its dimensions are 125mm (H) × 90mm (W) × 9mm (T).\nThe pigments for ColorCheckerPassport were modified in November 2014, so the current available cards do not have exactly the same carnation, and hence RGB numbers, as before, and particularly are not the ones provided on next section.\n\nThe colors of the chart were described by McCamy \"et al.\" with colorimetric measurements using the CIE 1931 2° standard observer and Illuminant C, and also in terms of the Munsell color system. Using measured reflectance spectra, it is possible to derive CIELAB coordinates for Illuminants \"D\" and \"D\" and coordinates in sRGB.\n\nColor targets such as the ColorChecker can be captured by cameras and other color input devices, and the resulting images’ output can be compared to the original chart, or to reference measurements, to test the degree to which image acquisition reproduction systems and processes approximate the human visual system’s. It can also be used to color correct one photo with the chart in it (that may have a different color cast, for example due to a lighting coloration difference) to another \"reference\" photo with the chart in it. Because of its wide availability and use, its careful design, and its consistency, and because comprehensive spectrophotometric measurements are available, the ColorChecker has also been used in academic research into topics such as spectral imaging.\n\nX-Rite also sells a 140-patch chart called the ColorChecker Digital SG, and is intended for automated use with computer software to characterize digital cameras and scanners.\n\n\n"}
{"id": "47710", "url": "https://en.wikipedia.org/wiki?curid=47710", "title": "Cotton gin", "text": "Cotton gin\n\nA cotton gin is a machine that quickly and easily separates cotton fibers from their seeds, enabling much greater productivity than manual cotton separation. The fibers are then processed into various cotton goods such as linens, while any undamaged cotton is used largely for textiles like clothing. The separated seeds may be used to grow more cotton or to produce cottonseed oil. \nHandheld roller gins had been used in the Indian subcontinent since at earliest AD 500 and then in other regions. The Indian worm-gear roller gin, invented some time around the sixteenth century, has, according to Lakwete, remained virtually unchanged up to the present time. A modern mechanical cotton gin was created by American inventor Eli Whitney in 1793 and patented in 1794. Whitney's gin used a combination of a wire screen and small wire hooks to pull the cotton through, while brushes continuously removed the loose cotton lint to prevent jams. It revolutionized the cotton industry in the United States, but also led to the growth of slavery in the American South as the demand for cotton workers rapidly increased. The invention has thus been identified as an inadvertent contributing factor to the outbreak of the American Civil War. Modern automated cotton gins use multiple powered cleaning cylinders and saws, and offer far higher productivity than their hand-powered precursors.\n\nEli Whitney invented his cotton gin in 1793. He began to work on this project after moving to Georgia in search of work. Given that farmers were desperately searching for a way to make cotton farming profitable, a woman named Catharine Greene provided Whitney with funding to create the first cotton gin. Whitney created two cotton gins: a small one that could be hand-cranked and a large one that could be driven by a horse or water power. Thanks to the cotton gin, the amount of raw cotton yielded had doubled each decade after 1800.\n\nA single-roller cotton gin came into use in India by the 5th century. An improvement invented in India was the two-roller gin, known as the \"churka\", \"charki\", or \"wooden-worm-worked roller\".\nCotton fibers are produced in the seed pods (\"bolls\") of the cotton plant where the fibers (\"lint\") in the bolls are tightly interwoven with seeds. To make the fibers usable, the seeds and fibers must first be separated, a task which had been previously performed manually, with production of cotton requiring hours of labor for the separation. Many simple seed-removing devices had been invented, but until the innovation of the cotton gin, most required significant operator attention and worked only on a small scale.\n\nThe earliest versions of the cotton gin consisted of a single roller made of iron or wood and a flat piece of stone or wood. Evidence for this type of gin has been found in Africa, Asia, and North America. The first documentation of the cotton gin by contemporary scholars is found in the fifth century AD, in the form of Buddhist paintings depicting a single-roller gin in the Ajanta Caves in western India. These early gins were difficult to use and required a great deal of skill. A narrow single roller was necessary to expel the seeds from the cotton without crushing the seeds. The design was similar to that of a mealing stone, which was used to grind grain. The early history of the cotton gin is ambiguous, because archeologists likely mistook the cotton gin's parts for other tools.\n\nBetween the 12th and 14th centuries, dual-roller gins appeared in India and China. The Indian version of the dual-roller gin was prevalent throughout the Mediterranean cotton trade by the 16th century. This mechanical device was, in some areas, driven by water power.\n\nThe worm gear roller gin, which was invented in the Indian subcontinent during the early Delhi Sultanate era of the 13th to 14th centuries, came into use in the Mughal Empire some time around the 16th century, and is still used in the Indian subcontinent through to the present day. Another innovation, the incorporation of the crank handle in the cotton gin, first appeared some time during the late Delhi Sultanate or the early Mughal Empire. The incorporation of the worm gear and crank handle into the roller cotton gin led to greatly expanded Indian cotton textile production during the Mughal era.\n\nIt was reported that, with an Indian cotton gin, which is half machine and half tool, one man and one woman could clean 28 pounds of cotton per day. With a modified Forbes version, one man and a boy could produce 250 pounds per day. If oxen were used to power 16 of these machines, and a few people's labour was used to feed them, they could produce as much work as 750 people did formerly.\n\nThe Indian roller cotton gin, known as the \"churka\" or \"charkha\", was introduced to the United States in the mid-18th century, when it was adopted in the southern United States. The device was adopted for cleaning long-staple cotton, but was not suitable for the short-staple cotton that was more common in certain states such as Georgia. Several modifications were made to the Indian roller gin by Mr. Krebs in 1772 and Joseph Eve in 1788, but their uses remained limited to the long-staple variety, up until Eli Whitney's development of a short-staple cotton gin in 1793.\n\nEli Whitney (1765–1825) applied for a patent of his cotton gin on October 28, 1793; the patent was granted on March 14, 1794, but was not validated until 1807. Whitney's patent was assigned patent number 72X. There is slight controversy over whether the idea of the modern cotton gin and its constituent elements are correctly attributed to Eli Whitney. The popular image of Whitney inventing the cotton gin is attributed to an article on the subject written in the early 1870s and later reprinted in 1910 in \"The Library of Southern Literature\". In this article, the author claimed Catharine Littlefield Greene suggested to Whitney the use of a brush-like component instrumental in separating out the seeds and cotton. To date, Greene's role in the invention of the gin has not been verified independently.\n\nWhitney's cotton gin model was capable of cleaning of lint per day. The model consisted of a wooden cylinder surrounded by rows of slender spikes, which pulled the lint through the bars of a comb-like grid. The grids were closely spaced, preventing the seeds from passing through. Loose cotton was brushed off, preventing the mechanism from jamming.\n\nMany contemporary inventors attempted to develop a design that would process short staple cotton, and Hodgen Holmes, Robert Watkins, William Longstreet, and John Murray had all been issued patents for improvements to the cotton gin by 1796. However, the evidence indicates Whitney did invent the saw gin, for which he is famous. Although he spent many years in court attempting to enforce his patent against planters who made unauthorized copies, a change in patent law ultimately made his claim legally enforceable – too late for him to make much money from the device in the single year remaining before the patent expired.\n\nWhile Whitney's gin facilitated the cleaning of seeds from short-staple cotton, it damaged the fibers of (extra-long staple) cotton (\"Gossypium barbadense\"). In 1840 Fones McCarthy received a patent for a \"Smooth Cylinder Cotton-gin\", a roller gin. McCarthy's gin was marketed for use with both short-staple and extra-long staple cotton, but was particularly useful for processing long-staple cotton. After McCarthy's patent expired in 1861, McCarthy type gins were manufactured in Britain and sold around the world. McCarthy's gin was adopted for cleaning the Sea Island variety of extra-long staple cotton grown in Florida, Georgia and South Carolina. It cleaned cotton several times faster than the older gins, and, when powered by one horse, produced 150 to 200 pounds of lint a day. The McCarthy gin used a reciprocating knife to detach seed from the lint. Vibration caused by the reciprocating motion limited the speed at which the gin could operate. In the middle of the 20th Century gins using a rotating blade replaced ones using a reciprocating blade. These descendants of the McCarthy gin are the only gins now used for extra-long staple cotton in the United States.\n\nPrior to the introduction of the mechanical cotton gin, cotton had required considerable labor to clean and separate the fibers from the seeds. With Eli Whitney’s gin, cotton became a tremendously profitable business, creating many fortunes in the Antebellum South. Cities such as New Orleans, Louisiana; Mobile, Alabama; Charleston, South Carolina; and Galveston, Texas became major shipping ports, deriving substantial economic benefit from cotton raised throughout the South. Additionally, the greatly expanded supply of cotton created strong demand for textile machinery and improved machine designs that replaced wooden parts with metal. This led to the invention of many machine tools in the early 19th century.\n\nThe invention of the cotton gin caused massive growth in the production of cotton in the United States, concentrated mostly in the South. Cotton production expanded from 750,000 bales in 1830 to 2.85 million bales in 1850. As a result, the region became even more dependent on plantations and slavery, with plantation agriculture becoming the largest sector of its economy. While it took a single slave about ten hours to separate a single pound of fiber from the seeds, a team of two or three slaves using a cotton gin could produce around fifty pounds of cotton in just one day. The number of slaves rose in concert with the increase in cotton production, increasing from around 700,000 in 1790 to around 3.2 million in 1850. By 1860, black slave labor from the American South was providing two-thirds of the world’s supply of cotton, and up to 80% of the crucial British market. The cotton gin thus “transformed cotton as a crop and the American South into the globe's first agricultural powerhouse\".\nBecause of its inadvertent effect on American slavery, and on its ensuring that the South's economy developed in the direction of plantation-based agriculture (while encouraging the growth of the textile industry elsewhere, such as in the North), the invention of the cotton gin is frequently cited as one of the indirect causes of the American Civil War.\n\nIn modern cotton production, cotton arrives at industrial cotton gins either in trailers, in compressed rectangular \"modules\" weighing up to 10 metric tons each or in polyethylene wrapped round modules similar to a bale of hay produced during the picking process by the most recent generation of cotton pickers. Cotton arriving at the gin is sucked in via a pipe, approximately in diameter, that is swung over the cotton. This pipe is usually manually operated, but is increasingly automated in modern cotton plants. The need for trailers to haul the product to the gin has been drastically reduced since the introduction of modules. If the cotton is shipped in modules, the module feeder breaks the modules apart using spiked rollers and extracts the largest pieces of foreign material from the cotton. The module feeder's loose cotton is then sucked into the same starting point as the trailer cotton.\n\nThe cotton then enters a dryer, which removes excess moisture.The cylinder cleaner uses six or seven rotating, spiked cylinders to break up large clumps of cotton. Finer foreign material, such as soil and leaves, passes through rods or screens for removal. The stick machine uses centrifugal force to remove larger foreign matter, such as sticks and burrs, while the cotton is held by rapidly rotating saw cylinders.\n\nThe gin stand uses the teeth of rotating saws to pull the cotton through a series of \"ginning ribs\", which pull the fibers from the seeds which are too large to pass through the ribs. The cleaned seed is then removed from the gin via an auger conveyor system. The seed is reused for planting or is sent to an oil mill to be further processed into cottonseed oil and cottonseed meal. The lint cleaners again use saws and grid bars, this time to separate immature seeds and any remaining foreign matter from the fibers. The bale press then compresses the cotton into bales for storage and shipping. Modern gins can process up to of cotton per hour.\n\nModern cotton gins create a substantial amount of cotton gin residue (CGR) consisting of sticks, leaves, dirt, immature bolls, and cottonseed. Research is currently under way to investigate the use of this waste in producing ethanol. Due to fluctuations in the chemical composition in processing, there is difficulty in creating a consistent ethanol process, but there is potential to further maximize the utilization of waste in the cotton production.\n\n"}
{"id": "28945794", "url": "https://en.wikipedia.org/wiki?curid=28945794", "title": "Counterpoise (ground system)", "text": "Counterpoise (ground system)\n\nIn electronics and radio communication a counterpoise is a network of suspended horizontal wires or cables (or a metal screen), used as a substitute for an earth (ground) connection in a radio antenna system. It is used with radio transmitters or receivers when a normal earth ground cannot be used because of high soil resistance or when an antenna is mounted above ground level, for example, on a building. It usually consists of a single wire or network of horizontal wires, parallel to the ground, suspended above the ground under the antenna, connected to the receiver or transmitter's \"ground\" wire. The counterpoise functions as one plate of a large capacitor, with the conductive layers of the earth acting as the other plate.\n\nThe counterpoise evolved with the Marconi (monopole) antenna during the 1890s, the first decade of radio in the wireless telegraphy era, but it was particularly advocated by British radio pioneer Oliver Lodge, and patented by his associate Alexander Muirhead in 1907.\n\nCounterpoises are typically used in antenna systems for radio transmitters where a good earth ground connection cannot be constructed.\n\nMonopole antennas used at low frequencies, such as the mast radiator antennas used for AM broadcasting, require the radio transmitter to be electrically connected to the Earth under the antenna; this is called a \"ground\" (or \"earth\"). The ground must have a low electrical resistance, because any resistance in the ground connection will dissipate power from the transmitter. Low-resistance grounds for radio transmitters are normally constructed of a network of cables buried in the earth. However, in areas with dry, sandy or rocky soil the ground has a high resistance, so a low-resistance ground connection cannot be made. In these cases, a counterpoise is used. Another circumstance in which a counterpoise is used is when earth for a buried ground under the antenna mast is not available, such as in antennas located in a city or on top of a tall building.\n\nA common design for a counterpoise is a series of radial wires suspended a few feet above the ground, extending from the base of the antenna in all directions in a \"star\" pattern, connected at the centre. The counterpoise functions as one plate of a large capacitor, with the conductive layers in the earth as the other plate. Since the radio frequency alternating currents from the transmitter can pass through a capacitor, the counterpoise functions as a low-resistance ground connection. There should not be any closed loops in the wires of a counterpoise system, as the strong fields of the antenna will induce circular currents in it which will dissipate transmitter power.\n\nThe largest use of counterpoises is in transmitters on the low frequency (LF) and very low frequency (VLF) bands, as they are very sensitive to ground resistance. Because of the large wavelength of the radio waves, feasible antennas used at these frequencies are electrically short, their length is a small fraction of the wavelength. The radiation resistance of antennas (the resistance that represents power radiated as radio waves) drops as their length becomes small compared to a wavelength, so the radiation resistance of antennas on the LF and VLF bands is very low, often as low as one ohm or less. The other, larger resistances in the antenna-ground circuit can consume significant portions of the transmitter’s power. The largest resistance in the antenna-ground circuit is often the ground system, and the transmitter power is divided proportionally between it and the radiation resistance, so the resistance of the ground system has to be kept very low to minimize the \"wasted\" transmitter power.\n\nHowever, at low frequencies the resistance of even a good ground system in high conductivity soil can consume a major portion of the transmitter power. Another source of resistance is dielectric losses from the penetration of radio waves into the ground near the antenna due to the large skin depth at low frequencies. Therefore, particularly at VLF frequencies, large counterpoises are often used instead of buried grounds, to reduce the ground system resistance, allowing more of the transmitter power to be radiated.\n\nSometimes a counterpoise is combined with an ordinary ground, with the buried radial ground cables brought above ground near the base of the antenna to form a counterpoise. The area of the counterpoise around the base of the antenna is often covered with copper screening, to shield the ground to reduce ground currents.\n\nThe size of the counterpoise used for radio work depends on the wavelength of the transmit frequency. With a monopole antenna, the counterpoise functions as a ground plane, reflecting the radio waves radiated downward by the antenna. To perform adequately, the counterpoise should extend at least half a wavelength from the antenna tower in all directions. In designing a counterpoise for an AM radio station, for example, AM broadcast-band radio waves are a maximum of long. Therefore, the counterpoise should extend from the tower to make a circle in diameter.\n\n\n"}
{"id": "6639850", "url": "https://en.wikipedia.org/wiki?curid=6639850", "title": "Coupling (piping)", "text": "Coupling (piping)\n\nA coupling (or coupler) (used in piping or plumbing) is a very short length of pipe or tube, with a socket at one or both ends that allows two pipes or tubes to be joined, welded (steel), brazed or soldered (copper, brass etc.) together.\n\nAlternatively it is a short length of pipe with two female National pipe threads (NPT) (in North American terms, a coupler is a double female while a nipple is double male) or two male or female British standard pipe threads.\n\nIf the two ends of a coupling are different (e.g. one BSP threaded and one NPT threaded), then it is usually referred to as an adapter. Another variation is one plain socket and one threaded socket. Yet another variation would be 3/4\" NPT to 1/2\" NPT.\n\nWhen the two ends use the same connection method but are of a different size, the terms reducing coupling or reducer are used.\n\n\n\n"}
{"id": "5999928", "url": "https://en.wikipedia.org/wiki?curid=5999928", "title": "D. C. Heath and Company", "text": "D. C. Heath and Company\n\nD.C. Heath and Company was an American publishing company located at 125 Spring Street in Lexington, Massachusetts, specializing in textbooks. \n\nThe company was founded in Boston by Daniel Collamore Heath in 1885. D.C. Heath and Company was owned by Raytheon from 1966 to 1995. When Raytheon exited the textbook market, it sold the company to Houghton Mifflin. \n\nD.C. Heath started a small division of software editors to supplement the textbooks in the early 80's. However, the editors strived to make the software packages not dependent on the books. What impressed me was no compromising on quality just to support the texts. There were the test banks that allowed teachers to pick and choose questions for their quizzes and tests. However, development was further supported to empower teachers to create their own questions including a formula editor, tagging items by objectives, and including custom graphics in the question as well as in the answer key. All this was for the venerable Apple 2 then later Windows and Macintoshes. Many titles were commissioned for the areas of science, math, reading, social studies, and modern languages. These were not the mundane read and answer questions, but highly interactive original programs. D.C. Heath gave this group their own identity, Collamore Educational Publishing. Huge credit must be give to the software managing editor and the director of the division. They stated their vision and then let the editors make things happen. The editors were involved in all facets of the publishing process including contracts, development, design, publishing, marketing, and sales. The schools were just transitioning from the one computer classroom to the computer lab. In 1988 most of the software was being supported by William K. Bradford Publishing Company composed initially by D. C. Heath / Collamore personnel. \n\nPublications-(note: There are far more titles than are listed here) \n\n2. Software initial addition by Hal Wexler, software editor, 1984-1988 then transitioned to William K. Bradford Publishing Company.\n"}
{"id": "30438853", "url": "https://en.wikipedia.org/wiki?curid=30438853", "title": "Distortionmeter", "text": "Distortionmeter\n\nDistortionmeter (or more precisely distortion factor meter) is an electronic measuring instrument which displays the amount of distortion added to the original signal by an electronic circuit.\n\nHarmonic distortion is equivalent to adding harmonics to a signal. When a purely sinusoidal signal is in this way, a series of harmonics is superimposed on the original signal, and can be detected with suitable equipment.\nIf the input is \n\nThe normalized output is \n\nThe value of Total Harmonics Distortion (THD) is defined as the ratio of the harmonics to the fundamental; \n\nformula_3\n\nThis ratio can be given in dB or in percentage.\n\nA distortionmeter is actually a levelmeter with two switchable parallel circuits at the input. The first circuit measures the total signal at the output of a system. (For low distortion levels this will be almost equal to fundamental). That value is adjusted to read 100% or, equivalently, to 0 dB. The second circuit is a filter which removes (as much as practical) the fundamental frequency. This can be a notch filter, one which passes all but the fundamental, with negligible attenuation at other frequencies (including whatever harmonics might be present). Alternatively, if the distortion products are at higher frequencies, a highpass filter can be used if its cutoff rate is sufficiently steep to not affect the expected distortion products. The output of the filter is measured as a percentage of the fundamental, and the reported value will be the distortion value.\n\n"}
{"id": "12302784", "url": "https://en.wikipedia.org/wiki?curid=12302784", "title": "Dust reduction system", "text": "Dust reduction system\n\nA dust reduction system, or dust removal system, is used in several makes of digital cameras to remove dust from the image sensor. Every time lenses are changed, dust may enter the camera body and settle on the image sensor.\n\nDigital single-lens reflex cameras (DSLR) are particularly vulnerable to this issue, since the interior of the camera is exposed during lens changes unlike other forms of digital cameras, and the image sensor is fixed, unlike a film camera. Even the tiniest (micrometre-size) dust particles or other contaminants that settle on the face of the image sensor (individual pixels of which have dimensions on the order of ~5 micrometres) may cast shadows and thus become visible in the final image as more or less diffuse grey blobs, depending on aperture.\n\nDust may be generated by internal moving parts or may be moved by air currents within the camera. Some systems remove or clean the sensor by vibrating at a very high frequency—between 100 hertz and 50 kilohertz.\n\nDifferent manufactures employ their own version of dust abatement. One type uses a piezo crystal to vibrate a filter which covers the sensor. The second type moves the actual sensor—this may be supplemented by a controlled flow of air.\n\nThis system vibrates the thin filter surface that covers the image sensor many tens of thousands of times per second (35,000 to 50,000 hertz) to remove particles from the filter. The system consists of a very thin piece of filter glass placed in front of the image sensor; the area between the filter and the sensor is sealed, so no dust can enter. Whenever the camera is turned on, a piezoelectric driver induces a vibration in the filter glass, shaking dust off. A piece of adhesive located inside the camera traps removed dust.\n\nThe distance between the filter glass and the sensor also mitigates the problem of dust, since any dust that does adhere to the glass will be held further away from the sensor, and thus produce a larger, more diffuse, and less noticeable shadow. In practice, few Four Thirds system users report having any issues with sensor dust.\n\nOlympus invented the system, called a Supersonic Wave Filter (SSWF), and licensed it to Leica and Panasonic. Canon also uses this type of system. Nikon uses a similar system, and they refer to it as high resonance.\n\nThe SSWF has been included in all Olympus, Panasonic, and Leica Four Thirds DSLRs, and is often cited as a key advantage of the system by reviewers and users. One disadvantage to the implementation on all current Four Thirds cameras is that the SSWF is triggered whenever the camera is turned on, causing a delay of about 0.8 seconds before the camera is ready to shoot.\n\nThis type of system moves the actual sensor to help reduce dust. It vibrates the actual sensor at around 100 Hz. The amount of movement or sensor travel is larger than the higher-frequency filter vibrating types. A crude analogy to compare it to the piezo crystal filter method is something like hitting or banging the sensor to displace the contaminant, whereas the piezo vibrates a filter to make the particles fall off. The sensor may also utilize a negatively charged surface coating to reduce static and help repel negatively charged particles. Konica Minolta is credited with being the original developer of this type of system. Sony and Pentax incorporate sensor shifting in their cameras with dust reduction systems.\n\nThese problems are not as critical with film SLRs as the dust disappears as the film is wound on, but with DSLRs the image sensor always remains in the same place. Even with dust particles smaller than 1 micrometre (0.001 mm) and invisible to the human eye, once they land on the image sensor's surface they can degrade the quality of all the images taken thereafter. Furthermore, it can be a difficult task to remove the dust, sometimes making it necessary to send the camera in for servicing.\n\nThere are two main types of dust that can potentially degrade image quality: Dust particles that adhere through\nelectric force and dust particles that adhere through intermolecular force.\n\nMost of the contamination to be found on the image sensor surface is caused by dust particles as small as just one micrometre (0.001 mm) adhering to it through electrical charges. The particles themselves carry a positive static electric charge, while the image sensor is negatively charged, which makes them attract each other. The same phenomenon can be observed on the surface of LCD and CRT monitor screens.\n\nThe intermolecular force is weaker than electrostatic charges. However, it still attracts microscopic-sized dust to the image sensor with infinitesimal force. While earthing (grounding) the camera can help reduce the problem of electrostatic dust it does not reduce intermolecular attraction. If, for example, flour were drizzled into the camera, it would still adhere to the surface of earthed metal. This kind of dust is attracted by intermolecular force. Liquid also adheres to the image sensor by intermolecular force and such molecules adhere strongly due to their ability to get closer to the adhesion surface, making it harder for dust reduction systems to remove these type of contaminants completely. In such instances, wiping the optical elements in front of the image sensor with cleaning fluid may be necessary.\n\nOlympus was the first to include a dust reduction system on a DSLR, featuring their \"Supersonic Wave Filter\" (SSWF) dust reduction technology on the Olympus E-1 in 2003. All Olympus DSLRs with removable lenses have included this system, as have Panasonic's and Leica's DSLRs; both companies use Olympus technology. Olympus Corporation was awarded an innovation prize by the Japan Institute of Invention and Innovation (JIII) in 2010 for its invention of automatic dust reduction for digital cameras.\n\nBefore that Sigma was sealing the mirror box of their cameras with a protective filter behind the lens mount, preventing dust from entering the camera body.\n\nOther manufacturers, namely Sony (2006), Canon (2006), Pentax (2006), and Nikon (2007), followed suit with their own dust removal technologies. Each manufacturer uses a somewhat different system.\n\nThere have been several attempts by camera magazines to test the various dust reduction systems to see how effective they are. Pixinfo, Chasseur d’Images, and Camera Labs have all published their opinions, which can be summarized as saying that none of the systems are completely effective, but that the Olympus SSWF system is significantly better than most of the others, with the Nikon system perhaps a close second.\n\n\n"}
{"id": "4164396", "url": "https://en.wikipedia.org/wiki?curid=4164396", "title": "Ear trumpet", "text": "Ear trumpet\n\nEar trumpets are tubular or funnel-shaped devices which collect sound waves and lead them into the ear. They were used as hearing aids, resulting in a strengthening of the sound energy impact to the eardrum and thus improved hearing for a deaf or hard-of-hearing individual. Ear trumpets were made of sheet metal, silver, wood, snail shells or animal horns. They have largely been replaced in wealthier areas of the world by modern hearing aid technology that is much smaller and less obtrusive, albeit more expensive.\nThe use of ear trumpets for the partially deaf dates back to the 17th century. The earliest description of an ear trumpet was given by the French Jesuit priest and mathematician Jean Leurechon in his work \"Recreations mathématiques\" (1634). Polymath Athanasius Kircher also described a similar device in 1650.\nBy the late 18th century, their use was becoming increasingly common. Collapsible conical ear trumpets were made by instrument makers on a one-off basis for specific clients. Well-known models of the period included the Townsend Trumpet (made by the deaf educator John Townshend), the Reynolds Trumpet (specially built for painter Joshua Reynolds) and the Daubeney Trumpet.\n\nThe first firm to begin commercial production of the ear trumpet was established by Frederick C. Rein in London in 1800. In addition to producing ear trumpets, Rein also sold hearing fans and speaking tubes. These instruments helped amplify sounds, while still being portable. However, these devices were generally bulky and had to be physically supported from below. Later, smaller, hand-held ear trumpets and cones were used as hearing aids.\n\nRein was commissioned to design a special acoustic chair for the ailing King of Portugal, John VI of Portugal in 1819. The throne was designed with ornately carved arms that looked like the open mouths of lions. These holes acted as the receiving area for the acoustics, which were transmitted to the back of the throne via a speaking tube, and into the king's ear.\nFinally in the late 1800s, the acoustic horn, which was a tube that had two ends, a cone that captured sound, and was eventually made to fit in the ear.\n\nJohann Nepomuk Mälzel began manufacturing ear trumpets in the 1810s. He notably produced ear trumpets for Ludwig van Beethoven, who was starting to go deaf at the time. These are now kept in the Beethoven Museum in Bonn.\nToward the late 19th century, hidden hearing aids became increasingly popular. Rein pioneered many notable designs, including his 'acoustic headbands', where the hearing aid device was artfully concealed within the hair or headgear. Reins' \"Aurolese Phones\" were headbands, made in a variety of shapes, that incorporated sound collectors near the ear that would amplify the acoustics. Hearing aids were also hidden in couches, clothing, and accessories. This drive toward ever-increasing invisibility was often more about hiding the individual's disability from the public than about helping the individual cope with his problem.\n\nF. C. Rein and Son of London ended its ear trumpet-manufacturing activity in 1963, as both the first and last company of its kind.\n\nA Pinard horn is a type of stethoscope used by midwives that is designed similarly to an ear trumpet. It is a wooden cone about 8 inches long. The midwife presses the wide end of the horn against the pregnant woman's belly to monitor heart tones. Pinard horns were invented in France in the 19th century, and are still in use in many places worldwide.\n\n\n"}
{"id": "2312537", "url": "https://en.wikipedia.org/wiki?curid=2312537", "title": "European industry federation", "text": "European industry federation\n\nA European industry federation (EIF) is a trade union organisation operating at European sectoral level, comparable to and sometimes part of the global union federations. They are the social partners recognised by the European Commission as acting on behalf of employees in their sectors for the purposes of European social dialogue.\n\nThe following is a list of EIFs affiliated to the European Trade Union Confederation (ETUC):\n"}
{"id": "23355682", "url": "https://en.wikipedia.org/wiki?curid=23355682", "title": "Flux pumping", "text": "Flux pumping\n\nFlux pumping is a method for magnetising superconductors to fields in excess of 15 teslas. The method can be applied to any type II superconductor and exploits a fundamental property of superconductors. That is their ability to support and maintain currents on the length scale of the superconductor. Conventional magnetic materials are magnetised on a molecular scale which means that superconductors can maintain a flux density orders of magnitude bigger than conventional materials. Flux pumping is especially significant when one bears in mind that all other methods of magnetising superconductors require application of a magnetic flux density at least as high as the final required field. This is not true of flux pumping.\n\nAn electric current flowing in a loop of superconducting wire can persist indefinitely with no power source. In a normal conductor, an electric current may be visualized as a fluid of electrons moving across a heavy ionic lattice. The electrons are constantly colliding with the ions in the lattice, and during each collision some of the energy carried by the current is absorbed by the lattice and converted into heat, which is essentially the vibrational kinetic energy of the lattice ions. As a result, the energy carried by the current is constantly being dissipated. This is the phenomenon of electrical resistance.\n\nThe situation is different in a superconductor. In a conventional superconductor, the electronic fluid cannot be resolved into individual electrons. Instead, it consists of bound \"pairs\" of electrons known as Cooper pairs. This pairing is caused by an attractive force between electrons from the exchange of phonons. Due to quantum mechanics, the energy spectrum of this Cooper pair fluid possesses an \"energy gap\", meaning there is a minimum amount of energy Δ\"E\" that must be supplied in order to excite the fluid. Therefore, if Δ\"E\" is larger than the thermal energy of the lattice, given by \"kT\", where \"k\" is Boltzmann's constant and \"T\" is the temperature, the fluid will not be scattered by the lattice. The Cooper pair fluid is thus a superfluid, meaning it can flow without energy dissipation.\n\nIn a class of superconductors known as type II superconductors, including all known high-temperature superconductors, an extremely small amount of resistivity appears at temperatures not too far below the nominal superconducting transition when an electric current is applied in conjunction with a strong magnetic field, which may be caused by the electric current. This is due to the motion of vortices in the electronic superfluid, which dissipates some of the energy carried by the current. If the current is sufficiently small, the vortices are stationary, and the resistivity vanishes. The resistance due to this effect is tiny compared with that of non-superconducting materials, but must be taken into account in sensitive experiments.\n\nIn the method described here a magnetic field is swept across the superconductor in a magnetic wave. This field induces current according to Faraday's law of induction. As long as the direction of motion of the magnetic wave is constant then the current induced will always be in the same sense and successive waves will induce more and more current.\n\nTraditionally the magnetic wave would be generated either by physically moving a magnet or by an arrangement of coils switched in sequence, such as occurs on the stator of a three-phase motor. Flux Pumping is a solid state method where a material which changes magnetic state at a suitable magnetic ordering temperature is heated at its edge and the resultant thermal wave produces a magnetic wave which then magnetizes the superconductor. A superconducting flux pump should not be confused with a classical flux pump as described in Van Klundert et al.’s review.\n\nThe method described here has two unique features:\n\nThe system, as described, is actually a novel kind of heat engine in which thermal energy is being converted into magnetic energy.\n\nWhen a superconductor is placed in a weak external magnetic field H, the field penetrates the superconductor only a small distance \"λ\", called the London penetration depth, decaying exponentially to zero within the interior of the material. This is called the Meissner effect, and is a defining characteristic of superconductivity. For most superconductors, the London penetration depth is on the order of 100 nm.\n\nThe Meissner effect is sometimes confused with the kind of diamagnetism one would expect in a perfect electrical conductor: according to Lenz's law, when a \"changing\" magnetic field is applied to a conductor, it will induce an electric current in the conductor that creates an opposing magnetic field. In a perfect conductor, an arbitrarily large current can be induced, and the resulting magnetic field exactly cancels the applied field.\n\nThe Meissner effect is distinct from this because a superconductor expels \"all\" magnetic fields, not just those that are changing. Suppose we have a material in its normal state, containing a constant internal magnetic field. When the material is cooled below the critical temperature, we would observe the abrupt expulsion of the internal magnetic field, which we would not expect based on Lenz's law.\n\nThe Meissner effect was explained by the brothers Fritz and Heinz London, who showed that the electromagnetic free energy in a superconductor is minimized provided\n\nwhere H is the magnetic field and λ is the London penetration depth.\n\nThis equation, which is known as the London equation, predicts that the magnetic field in a superconductor decays exponentially from whatever value it possesses at the surface.\n\nIn 1962, the first commercial superconducting wire, a niobium-titanium alloy, was developed by researchers at Westinghouse, allowing the construction of the first practical superconducting magnets. In the same year, Josephson made the important theoretical prediction that a supercurrent can flow between two pieces of superconductor separated by a thin layer of insulator. This phenomenon, now called the Josephson effect, is exploited by superconducting devices such as SQUIDs. It is used in the most accurate available measurements of the magnetic flux quantum formula_2, and thus (coupled with the quantum Hall resistivity) for Planck's constant \"h\". Josephson was awarded the Nobel Prize for this work in 1973.\n\nThe most popular model used to describe superconductivity is the Bean or Critical State model and variations such as the Kim-Anderson model. However the Bean model assumes zero resistivity and that current is always induced at the critical current. A more useful model for engineering applications is the so-called E-J power law in which the field and the current are linked by the following equations:\n\nIn these equations if n = 1 then the conductor has linear resistivity such as is found in copper. The higher the n-value the closer we get to the critical state model. Also the higher the n-value then the \"better\" the superconductor as the lower the resistivity at a certain current. The E-J power law can be used to describe the phenomenon of flux-creep in which a superconductor gradually loses its magnetisation over time. This process is logarithmic and thus gets slower and slower and ultimately leads to very stable fields.\n\nThe potential of superconducting coils and bulk melt-processed YBCO single domains to maintain significant magnetic fields at cryogenic temperatures makes them particularly attractive for a variety of engineering applications including superconducting magnets, magnetic bearings and motors. It has already been shown that large fields can be obtained in single domain bulk samples at 77 K. A range of possible applications exist in the design of high power density electric motors.\n\nBefore such devices can be created a major problem needs to be overcome. Even though all of these devices use a superconductor in the role of a permanent magnet and even though the superconductor can trap potentially huge magnetic fields (greater than 10 T) the problem is the induction of the magnetic fields, this applies both to bulk and to coils operating in persistent mode. There are four possible known methods:\n\n\nAny of these methods could be used to magnetise the superconductor and this may be done either in situ or ex situ. Ideally the superconductors are magnetised in situ.\n\nThere are several reasons for this: first, if the superconductors should become demagnetised through (i) flux creep, (ii) repeatedly applied perpendicular fields or (iii) by loss of cooling then they may be re-magnetized without the need to disassemble the machine. Secondly, there are difficulties with handling very strongly magnetized material at cryogenic temperatures when assembling the machine. Thirdly, ex situ methods would require the machine to be assembled both cold and pre-magnetized and would offer significant design difficulties. Until room temperature superconductors can be prepared, the most efficient design of machine will therefore be one in which an in situ magnetizing fixture is included!\n\nThe first three methods all require a solenoid which can be switched on and off. In the first method an applied magnetic field is required equal to the required magnetic field, whilst the second and third approaches require fields at least two times greater. The final method, however, offers significant advantages since it achieves the final required field by repeated applications of a small field and can utilise a permanent magnet.\n\nIf we wish to pulse a field using, say, a 10 T magnet to magnetize a 30 mm × 10 mm sample then we can work out how big the solenoid needs to be. If it were possible to wind an appropriate coil using YBCO tape then, assuming an I of 70 A and a thickness of 100 μm, we would have 100 turns and 7 000 A turns. This would produce a B field of approximately 7 000/(20 × 10) × 4π × 10 = 0.4 T. To produce 10 T would require pulsing to 1 400 A! An alternative calculation would be to assume a J of say 5 × 10Am and a coil 1 cm in cross section. The field would then be 5 × 10 × 10 × (2 × 4π × 10) = 10 T. Clearly if the magnetisation fixture is not to occupy more room than the puck itself then a very high activation current would be required and either constraint makes in situ magnetization a very difficult proposition. What is required for in situ magnetisation is a magnetisation method in which a relatively small field of the order of milliteslas repeatedly applied is used to magnetize the superconductor.\n\nSuperconducting magnets are some of the most powerful electromagnets known. They are used in MRI and NMR machines, mass spectrometers, Magnetohydrodynamic Power Generation and beam-steering magnets used in particle accelerators. They can also be used for magnetic separation, where weakly magnetic particles are extracted from a background of less or non-magnetic particles, as in the pigment industries.\n\nOther early markets are arising where the relative efficiency, size and weight advantages of devices based on HTS outweigh the additional costs involved.\n\nPromising future applications include high-performance transformers, power storage devices, electric power transmission, electric motors (e.g. for vehicle propulsion, as in vactrains or maglev trains), magnetic levitation devices, and fault current limiters.\n\n\n"}
{"id": "12752221", "url": "https://en.wikipedia.org/wiki?curid=12752221", "title": "Fultograph", "text": "Fultograph\n\nThe fultograph was an early, clockwork image-receiving device, similar in function to fax machines. It took signals from the loudspeaker socket of a radio receiver and used an electrochemical process to darken areas of sensitised paper wrapped on a rotating drum. Invented by Otho Fulton, the system was used briefly in the late 1920s to broadcast images to homes by radio. The machines themselves were expensive (£22 15\"s\" 0\"d\" in 1928) and required a good receiver to operate.\n\nThe BBC broadcast Fultograph images in 759 programmes between 1929 and 1932. The Fultograph was the subject of an article in the British \"RadCom\" amateur radio magazine in October 2007.\n\n"}
{"id": "20160867", "url": "https://en.wikipedia.org/wiki?curid=20160867", "title": "Gleaner E", "text": "Gleaner E\n\nThe Gleaner E was a self-propelled combine harvester manufactured by the Gleaner Manufacturing Company while part of the Allis-Chalmers Manufacturing Company in the 1960s. 17,300 machines were manufactured in total from 1962 to 1969.\n\nThese harvesters utilized a 65 hp (49 kW) 226 cubic inch (3.7 Litre) 4-cylinder gasoline powered internal combustion engine.\n\nIn 1969 the Gleaner E was replaced by the Gleaner K which was nearly identical to the E III model but powered by a larger 250 cu in (4.1 Litre) General Motors 6-cylinder gasoline engine. An Allis Chalmers four cylinder diesel engine was offered as optional equipment on later K2 models after 1976. Gleaner K models were produced by Allis Chalmers until 1982 and remain popular with small scale farmers in the United States due to their simplicity, ease of maintenance, and their small size compared to modern harvesters. They are also widely used by agricultural researchers to harvest small test plots because unlike larger harvesters they can easily be transported between plots on a flatbed trailer towed by a heavy duty pickup truck.\n\n"}
{"id": "39900064", "url": "https://en.wikipedia.org/wiki?curid=39900064", "title": "Hale's Tours of the World", "text": "Hale's Tours of the World\n\nHale's Tours of the World were an attraction at amusement parks and similar venues in the early 20th Century. They were specially constructed spaces designed to simulate a railway journey.\n\nGeorge C. Hale was born on October 28, 1849. He was a well travelled and prolific inventor, engineer and Fire Chief of Kansas City, Missouri from 1882 until retiring in 1902. Interested in the use of film, Hale came across William Keefe who had conceived the idea of an imitation railway passenger car on a circular platform that would remain motionless as a panorama of images would revolve around the \"passengers\" to simulate the experience of a railway journey. \"Realism\" would be increased with staff providing a rocking motion, a wind machine and sound effects. Lacking the financial capital, Fell teamed with Judge Fred Gifford who introduced Fell to Hale who had possibly viewed the phantom rides on tours of Great Britain. The idea was patented in 1904; Hale and Gifford soon bought out Keefe's interest in the venture.\n\nThe first appearance of Hale's Tours were at the 1904 St. Louis Exhibition. The idea was further refined under the name Pleasure Railway. The idea then appeared in New York and Chicago and spread throughout the United States and Canada with an estimated 500 Hale's Tours appearing between 1906 and 1911. In 1906 Wade C. Gifford took Hale's Tours throughout the world in Mexico, the British Isles, Continental Europe and Hong Kong. \n\nThe American film producer and director Charles Urban, had an early franchise for Hale's Tours at 165 Oxford Street (later the Academy cinema), with main offices selling films and equipment a few hundred yards away on Wardour Street. The business seems to have been in liquidation by 1906, when the franchise for Hale's Tours was acquired by J. Henry Iles in December 1906, and a company named Hales Tours of the World Ltd. was set up in December of that year to take over the running of 165 Oxford Street. Further venues were opened in Nottingham, Manchester, Brighton, Leeds, Blackpool and Bristol.\n\nAs with most novelties, Hale's Tours gradually came to a decline as audiences tired of the idea. A lack of films lead to infrequent changes in program and some people felt uncomfortable being in a rocking carriage at close quarters with strangers.\n\nHales died on 14 July 1923.\n\nA Hale's Tour appears in the 1948 film \"Letter from an Unknown Woman\".\n\nHale's Tours: Ultrarealism in the Pre-1910 Motion Picture http://web.grinnell.edu/courses/spn/s02/SPN395-01/RAF/RAF06/RAF0603.pdf\n"}
{"id": "1086531", "url": "https://en.wikipedia.org/wiki?curid=1086531", "title": "Imaging radar", "text": "Imaging radar\n\nImaging radar is an application of radar which is used to create two-dimensional images, typically of landscapes. Imaging radar provides its light to illuminate an area on the ground and take a picture at radio wavelengths. It uses an antenna and digital computer storage to record its images. In a radar image, one can see only the energy that was reflected back towards the radar antenna. The radar moves along a flight path and the area illuminated by the radar, or footprint, is moved along the surface in a swath, building the image as it does so.\n\nDigital radar images are composed of many dots. Each pixel in the radar image represents the radar backscatter for that area on the ground: brighter areas represent high backscatter, darker areas represents low backscatter.\n\nThe traditional application of radar is to display the position and motion of typically highly reflective objects (such as aircraft or ships) by sending out a radiowave signal, and then detecting the direction and delay of the reflected signal. Imaging radar on the other hand attempts to form an image of one object (e.g. a landscape) by furthermore registering the intensity of the reflected signal to determine the amount of scattering (cf. Light scattering). The registered electromagnetic scattering is then mapped onto a two-dimensional plane, with points with a higher reflectivity getting assigned usually a brighter color, thus creating an image.\n\nSeveral techniques have evolved to do this. Generally they take advantage of the Doppler effect caused by the rotation or other motion of the object and by the changing view of the object brought about by the relative motion between the object and the back-scatter that is perceived by the radar of the object (typically, a plane) flying over the earth. Through recent improvements of the techniques, radar imaging is getting more accurate. Imaging radar has been used to map the Earth, other planets, asteroids, other celestial objects and to categorize targets for military systems.\n\nAn imaging radar is a kind of radar equipment which can be used for imaging. A typical radar technology includes emitting radio waves, receiving their reflection, and using this information to generate data. For an imaging radar, the returning waves are used to create an image. When the radio waves reflect off objects, this will make some changes in the radio waves and can provide data about the objects, including how far the waves traveled and what kind of objects they encountered. Using the acquired data, a computer can create a 3-D or 2-D image of the target.\n\nImaging radar has several advantages. It can operate in the presence of obstacles that obscure the target, and can penetrate ground (sand), water, or walls.\n\nApplications include: surface topography & crustal change; land use monitoring, agricultural monitoring, ice patrol, environmental monitoring;weather radar- storm monitoring, wind shear warning;medical microwave tomography; through wall radar imaging; 3-D measurements, etc.\n\nWall parameter estimation uses Utra Wide-Band radar systems. The handle M-sequence UWB radar with horn and circular antennas was used for data gathering and supporting the scanning method.\n\n3-D measurements are supplied by amplitude-modulated laser radars—Erim sensor and Perceptron sensor. In terms of speed and reliability for median-range operations, 3-D measurements have superior performance.\n\nCurrent radar imaging techniques rely mainly on synthetic aperture radar (SAR) and inverse synthetic aperture radar (ISAR) imaging. Emerging technology utilizes monopulse radar 3-D imaging.\n\nReal aperture radar(RAR) is a form of radar that transmits a narrow angle beam of pulse radio wave in the range direction at right angles to the flight direction and receives the backscattering from the targets which will be transformed to a radar image from the received signals.\n\nUsually the reflected pulse will be arranged in the order of return time from the targets, which corresponds to the range direction scanning.\n\nThe resolution in the range direction depends on the pulse width. The resolution in the azimuth direction is identical to the multiplication of beam width and the distance to a target.\n\nThe AVTIS radar is a 94 GHz real aperture 3D imaging radar. It uses Frequency-Modulated Continuous-Wave(FMCW) modulation and employs a mechanically scanned monostatic with sub-metre range resolution.\n\nLaser radar is a remote sensing technology that measures distance by illuminating a target with a laser and analyzing the reflected light.\n\nLaser radar is used for multi-dimensional imaging and information gathering. In all information gathering modes, lasers that transmit in the eye-safe region are required as well as sensitive receivers at these wavelengths.\n\n3-D imaging requires the capacity to measure the range to the first scatter within every pixel. Hence, an array of range counters is needed. A monolithic approach to an array of range counters is being developed. This technology must be coupled with highly sensitive detectors of eye-safe wavelengths.\n\nTo measure Doppler information requires a different type of detection scheme than is used for spatial imaging. The returned laser energy must be mixed with a local oscillator in a heterodyne system to allow extraction of the Doppler shift.\n\n3-D, Multi-wave and Multi-band, Imaging radar works in one of the two modes - as an arbitrary frequency(kHz-MHz), arbitrary wave radar, and as a C-band analog and digital mode radar. The system architecture of 3-D, Multi-wave and Multi-band, Imaging radar is shown in the figure.\n\nSynthetic-aperture radar (SAR) is a form of radar which moves a real aperture or antenna through a series of positions along the objects to provide distinctive long-term coherent-signal variations. This can be used to obtain higher resolution.\n\nSARs produce a two-dimensional (2-D) image. One dimension in the image is called range and is a measure of the \"line-of-sight\" distance from the radar to the object. Range is determined by measuring the time from transmission of a pulse to receiving the echo from a target. Also, range resolution is determined by the transmitted pulse width.The other dimension is called azimuth and is perpendicular to range. The ability of SAR of producing relatively fine azimuth resolution makes it different from other radars. To obtain fine azimuth resolution, a physically large antenna is needed to focus the transmitted and received energy into a sharp beam. The sharpness of the beam defines the azimuth resolution. An airborne radar could collect data while flying this distance and process the data as if it came from a physically long antenna. The distance the aircraft flies in synthesizing the antenna is known as the synthetic aperture. A narrow synthetic beamwidth results from the relatively long synthetic aperture, which gets finer resolution than a smaller physical antenna.\n\nInverse synthetic aperture radar (ISAR) is another kind of SAR system which can produce high-resolution on two- and three-dimensional images.\n\nAn ISAR system consists of a stationary radar antenna and a target scene that is undergoing some motion. ISAR is theoretically equivalent to SAR in that high-azimuth resolution is achieved via relative motion between the sensor and object, yet the ISAR moving target scene is usually made up of non cooperative objects.\n\nAlgorithms with more complex schemes for motion error correction are needed for ISAR imaging than those needed in SAR. ISAR technology uses the movement of the target rather than the emitter to make the synthetic aperture. ISAR radars are commonly\nused on vessels or aircraft and can provide a radar image of sufficient quality for target recognition. The ISAR image is often adequate to discriminate between various missiles, military aircraft, and civilian aircraft.\n\nRolling is side to side. Pitching is forward and backwards, yawing is turning left or right.\n\nMonopulse radar 3-D imaging technique uses 1-D range image and monopulse angle measurement to get the real coordinates of each scatterer. Using this technique, the image doesn’t vary with the change of the target’s movement. Monopulse radar 3-D imaging utilizes the ISAR techniques to separate scatterers in the Doppler domain and perform monopulse angle measurement.\n\nMonopulse radar 3-D imaging can obtain the 3 views of 3-D objects by using any two of the three parameters obtained from the azimuth difference beam, elevation difference beam and range measurement, which means the views of front, top and side can be azimuth-elevation, azimuth-range and elevation-range, respectively.\n\nMonopulse imaging generally adapts to near-range targets, and the image obtained by monopulse radar 3-D imaging is the physical image which is consistent with the real size of the object.\n\n\n"}
{"id": "58239632", "url": "https://en.wikipedia.org/wiki?curid=58239632", "title": "Instant payment", "text": "Instant payment\n\nInstant payment is a new way to exchange money and purchase services in seconds. Compared with wire transfers they allow the transfer of money from the payer bank account to the payee bank account almost immediately, instead of requiring few business days. \nThese payment systems have been developed (and are currently under development) worldwide as the need of faster and reliable transaction is the new economy common requirement.\n\nThe Euro Retail Payments Board (ERPB) has provided a widely accepted definition of instant payments, that are: \"electronic retail payment solutions available 24/7/365 and resulting in the immediate or close-to-immediate interbank clearing of the transaction and crediting of the payee’s account with confirmation to the payer (within seconds of payment initiation).\" \n\nThe growth of e-commerce has caused changes in people's spending patterns. Shopping is no longer confined to regular business hours, creating a new challenges for funds transfers. \nSimilarly, merchants require faster and more reliable money transfer systems to keep up with consumer demand.\n\nTraditional electronic payments like wire transfers, that perform the electronic funds transfer within few business days, are not in line with user expectations. \nIt is predicted that in the next year instant payments will become the standard for electronic fund transfers.\n\nA non-exhaustive list of systems available today, and in the near future, is the following:\n\n\nIn digital wallet based payment systems like PayPal, Apple Pay, AliPay, WeChat Pay etc. users receive immediate notification of the transaction, but funds are transferred at best in the next business day. \nThe settlement time depends on the underpinning payment method chosen by the customer, while for instant payment systems, the funds are transferred within seconds or minutes.\n\n\n"}
{"id": "48383315", "url": "https://en.wikipedia.org/wiki?curid=48383315", "title": "K. T. Keller", "text": "K. T. Keller\n\nKaufman Thuma Keller, commonly known as K. T. Keller (1885–1966), was an American corporate executive who served as the president of Chrysler Corporation from 1935 to 1950 and as its chairman of the board from 1950 to 1956. He is also known for proposing the creation of the Detroit Arsenal.\n\nKeller joined the General Motors Company (GMC) in 1911, and he worked as a general master mechanic for one of GMC's divisions, the Buick Motor Company from 1916 to 1919. He quickly rose to become a vice president of Chevrolet in 1921, later becoming a vice president for Chrysler.\n\nFrom 1935 to 1950 he served as Chrysler's president and then as its chairman of the board until 1956. Under his leadership, Chrysler became second among the world’s largest auto producers, with sales exceeding $1 billion in 1947. Further, Chrysler pioneered many of the \"engineering advances that are standard today, including high-compression engines and four-wheel hydraulic brakes.\"\n\nFollowing World War II, President Harry S. Truman appointed Keller as chairman of the President's Advisory Committee on the Merchant Marine in 1947. Truman also appointed Keller to serve as the director of the Office of Guided Missiles. In 1954, he was one of ten outstanding scientists and industrialists appointed to the newly formed Army Scientific Advisory Panel.\n\nKeller received many awards and honors during his lifetime. In 1946, President Truman awarded him the Medal for Merit for his contribution during World War II. In 1954, he received the Air Force Exceptional Service Award. The biggest honor he received occurred in October 1939, when \"Time\" honored Keller by not only writing a feature article about his work with Chrysler, but portraying him on the cover of its magazine.\n\n\n"}
{"id": "44317529", "url": "https://en.wikipedia.org/wiki?curid=44317529", "title": "Laminar flamelet model", "text": "Laminar flamelet model\n\nThe Laminar Flamelet Model is one of the methods of modelling turbulent combustions apart from SCRS, Eddy flamelet model and others. Combustion is a very important thermochemical process with significant material and aerodynamic implications and thus CFD modeling of combustion has become indispensable. The laminar flamelet model is basically for non pre-mixed fuel (the system in which the fuel and oxygen is supplied from two different pipes). The laminar flamelet model is different from the much more popular SCRS model as it includes the experimental information also which helps in bringing out much more intricate relationship between variables like temperature, mixture fractions and mass fractions.\n\nThe flamelet concept considers the turbulent flame as an aggregate of thin, laminar(Re<2000), locally one-dimensional flamelet structures present within the turbulent flow field. Counterflow diffusion flame is a common laminar flame which is used to represent a flamelet in a turbulent flow. Its geometry consists of opposed and axi-symmetric fuel and oxidizer jets. As the distance between the jets is decreased and/or the velocity of the jets is increased, the flame is strained and departs from its chemical equilibrium until it eventually extinguishes. The mass fraction of species and temperature fields can be measured or calculated in laminar counterflow diffusion flame experiments. When calculated, a self-similar solution exists, and the governing equations can be simplified to only one dimension i.e. along the axis of the fuel and oxidizer jets. It is in this direction where complex chemistry calculations can be performed affordably.\n\nThe following assumptions are made in the study of all the flamelet models:-\n\n1. While modelling only a single mixture fraction are allowed. Modelling of two-mixture-fraction flamelet models is not possible.\n\n2. It is assumed that the mixture fraction follow the β-function PDF, and scalar dissipation fluctuations are not considered.\n\n3. Empirically-based streams cannot be used.\n\nTo model a non-premixed combustion, governing equations for fluid elements are required.\nThe conservation equation for the species mass fraction is as follows:-\n\nLek→lewis number of kth species\nAnd the above formula was derived with keeping constant heat capacity. The energy equation with variable heat capacity:-\n\nAs can be seen from above formulas that the mass fraction and temperature are dependent on\n\n1. Mixture fraction Z\n\n2. Scalar dissipation χ\n\n3. Time\n\nMany a times we neglect the unsteady terms in above equation and assume the local flame structure having a balance between steady chemical equations and steady diffusion equation which result in Steady Laminar Flamelet Models(SLFM).\nFor this an average value of χ is computed known as \"favre\" value\n\nThe basic assumption of a SLFM model is that a turbulent flame front behaves locally as a one dimensional,steady and laminar which proves to be a very useful while reducing the situation to a much simpler terms but it does create problems as few of the effects are not accounted for.\n\nThe advantages of using this combustion model are as follows:-\n\n1. They have the advantage of showing strong coupling between chemical reactions and molecular transport.\n\n2. The steady laminar flamelet model is also used to predict chemical non-equilibrium due to aerodynamic straining of the flame by the turbulence.\n\nThe disadvantages of Steady Laminar Flamelet model due to above mentioned reason are:\n\n1. It doesn’t account for the curvature effects which can change the flame structure and is more detrimental while the structure hasn’t reached the quasi- steady state.\n\n2. Such Transient effects also arise in turbulent flow, the scalar dissipation experience a sudden change. As the flame structure take time to get stabilize.\n\nTo improve the above SLFM models, few more models has been proposed like Transient laminar flamelet model ( TLFM) by Ferreira .\n\n1. Versteeg H.K. and Malalasekera W., An introduction to computational fluid dynamics, .\n\n2. Stefano Giuseppe Piffaretti, Flame Age Model: a transient laminar flamelet approach for turbulent diffusion flames, A dissertation submitted to the\nSwiss Federal Institute Of Technology Zurich.\n\n3. N. Peters, Institut fur Technische Mechanik RWTH Aachen, Four Lectures on turbulent Combustion.\n"}
{"id": "3391379", "url": "https://en.wikipedia.org/wiki?curid=3391379", "title": "Lighting control system", "text": "Lighting control system\n\nA lighting control system is an intelligent network based lighting control solution that incorporates communication between various system inputs and outputs related to lighting control with the use of one or more central computing devices. Lighting control systems are widely used on both indoor and outdoor lighting of commercial, industrial, and residential spaces. Lighting control systems serve to provide the right amount of light where and when it is needed.\n\nLighting control systems are employed to maximize the energy savings from the lighting system, satisfy building codes, or comply with green building and energy conservation programs. Lighting control systems are often referred to under the term Smart Lighting.\n\nThe term \"lighting controls\" is typically used to indicate stand-alone control of the lighting within a space. This may include occupancy sensors, timeclocks, and photocells that are hard-wired to control fixed groups of lights independently. Adjustment occurs manually at each devices location. The efficiency of and market for residential lighting controls has been characterized by the Consortium for Energy Efficiency.\n\nThe term \"lighting control system\" refers to an intelligent networked system of devices related to lighting control. These devices may include relays, occupancy sensors, photocells, light control switches or touchscreens, and signals from other building systems (such as fire alarm or HVAC). Adjustment of the system occurs both at device locations and at central computer locations via software programs or other interface devices.\n\nThe major advantage of a lighting control system over stand-alone lighting controls or conventional manual switching is the ability to control individual lights or groups of lights from a single user interface device. This ability to control multiple light sources from a user device allows complex lighting scenes to be created. A room may have multiple scenes pre-set, each one created for different activities in the room. A major benefit of lighting control systems is reduced energy consumption. Longer lamp life is also gained when dimming and switching off lights when not in use. Wireless lighting control systems provide additional benefits including reduced installation costs and increased flexibility over where switches and sensors may be placed.\n\nLighting control systems typically provide the ability to automatically adjust a lighting device's output based on:\n\nChronological time schedules incorporate specific times of the day, week, month or year.\n\nSolar time schedules incorporate sunrise and sunset times, often used to switch outdoor lighting. Solar time scheduling requires that the location of the building be set. This is accomplished using the building's geographic location via either latitude and longitude or by picking the nearest city in a given database giving the approximate location and corresponding solar times.\n\nSpace occupancy is primarily determined with occupancy sensors.\n\nElectric lighting energy use can be adjusted by automatically dimming and/or switching electric lights in response to the level of available daylight. Reducing the amount of electric lighting used when daylight is available is known as daylight harvesting.\n\nAlarm conditions typically include inputs from other building systems such as the fire alarm or HVAC system, which may trigger an emergency 'all lights on' or ' all lights flashing' command for example.\n\nProgram logic can tie all of the above elements together using constructs such as if-then-else statements and logical operators.\n\nIn the 1980s there was a strong requirement to make commercial lighting more controllable so that it could become more energy efficient. Initially this was done with analog control, allowing fluorescent ballasts and dimmers to be controlled from a central source. This was a step in the right direction, but cabling was complicated and therefore not cost effective. \n\nTridonic was an early company to go digital with their broadcast protocols, DSI, in 1991. DSI was a basic protocol as it transmitted one control value to change the brightness of all the fixtures attached to the line. What made this protocol more attractive, and able to compete with the established analog option, was the simple wiring.\n\nThere are two types of lighting control systems which are:\n\nExamples for analog lighting control systems are:\n\nIn production lighting 0-10V system was replaced by analog multiplexed systems such as D54 and AMX192, which themselves have been almost completely replaced by DMX512. For dimmable fluorescent lamps (where it operates instead at 1-10 V, where 1 V is minimum and 0 V is off) the system is being replaced by DSI, which itself is in the process of being replaced by DALI.\n\nExamples for digital lighting control systems are:\n\nThose are all wired lighting control system. There is also a wireless lighting control system that is based on some standard protocols like MIDI, ZigBee, Bluetooth Mesh, and others.\n\nArchitectural lighting control systems can integrate with a theater's on-off and dimmer controls, and are often used for house lights and stage lighting, and can include worklights, rehearsal lighting, and lobby lighting. Control stations can be placed in several locations in the building and range in complexity from single buttons that bring up preset options-looks, to in-wall or desktop LCD touchscreen consoles. Much of the technology is related to residential and commercial lighting control systems.\n\nThe benefit of architectural lighting control systems in the theater is the ability for theater staff to turn worklights and house lights on and off without having to use a lighting control console. Alternately, the light designer can control these same lights with light cues from the lighting control console so that, for instance, the transition from houselights being up before a show starts and the first light cue of the show is controlled by one system.\n"}
{"id": "26557660", "url": "https://en.wikipedia.org/wiki?curid=26557660", "title": "List of the oldest newspapers", "text": "List of the oldest newspapers\n\nThis list of the oldest newspapers sorts the newspapers of the world by the date of their first publication. The earliest newspapers date to 17th century Europe when printed periodicals began rapidly to replace the practice of hand-writing newssheets. The emergence of the new media branch has to be seen in close connection with the simultaneous spread of the printing press from which the publishing press derives its name.\n\nNewspapers − apart from being printed − are typically expected to meet four criteria:\n\nThe French established the first newspaper in Africa in Mauritius in 1773.\n\nThe first recorded attempt to found a newspaper in South Asia was by William Bolts, a Dutchman in the employ of the British East India Company in September 1768 in Calcutta. The Company deported Bolts back to Europe before he could begin his newspaper.\n\n\n\n"}
{"id": "16762239", "url": "https://en.wikipedia.org/wiki?curid=16762239", "title": "MacSpeech Dictate", "text": "MacSpeech Dictate\n\nMacSpeech Dictate was a speech recognition program developed for Mac OS X by MacSpeech. The first version of MacSpeech Dictate was released in March 2008 after being showcased at the Macworld Conference & Expo in 2008 and winning the Macworld 2008 Best Of Show award. On September 20, 2010, Nuance Communications, which acquired MacSpeech in February 2010, released a new version of the product, renaming it \"Dragon Dictate for Mac\".\n\nMacSpeech Dictate ran as a Mac-native application. It used the Dragon speech recognition engine (v9 or v10), licensed from Nuance Communications. This is the same technology that powers speech recognition in Dragon NaturallySpeaking for the PC, although across platforms there are significant differences in features, functionality and integration. One major difference with MacSpeech Dictate was that it did not allow training by typing misrecognized words as Dragon NaturallySpeaking products do on Windows. Another notable difference was the lack of a transcription feature for recorded voice dictation, as found in NaturallySpeaking. MacSpeech released a separate product, MacSpeech Scribe, to handle this.\n\nMacSpeech Dictate Medical, a version with specialized vocabularies for doctors and dentists, was released in June 2009. MacSpeech Dictate Legal, with specialized vocabulary for lawyers, was released in July 2009. MacSpeech Dictate International, with support for speech recognition in English, French, German and Italian, was released in September 2009. Localized versions of MacSpeech Dictate are available in German, French and Italian.\n\nMacSpeech Dictate products used the highly successful and very accurate Dragon NaturallySpeaking speech recognition engine from Nuance Communications. In February 2010, MacSpeech Inc. was acquired by Nuance Communications, which continued development of native Mac speech recognition applications under the Dragon brand name.\n\nReviewing MacSpeech Dictate 1.0 in the New York Times in January 2008, David Pogue concluded:\n\nSo Dictate 1.0 is attractive, simple and Mac-like. It is not, however, as good as NaturallySpeaking 9.0 for Windows ($200). It lacks features like audio playback of what you said, a simple “add word” command, legal and medical versions, and non-English language kits.\n\nIt also lacks voice correction.\n\nWhen NatSpeak makes an error, you just say “Correct ‘ax a moron’ ” (or whatever it typed); and choose from a list of alternate transcriptions. The program not only corrects the error in your document, but also learns from its mistake. Over time, the accuracy edges ever closer to 100 percent.\n\nIn Dictate 1.0, however, you have to fix transcription errors by hand. The company intends to add voice correction in a 1.1 update; in the meantime, though, your accuracy won’t improve.\n\nThe late beta version I tested has some bugs. The company intends to get these fixed by the 1.0 version’s mid-February release.\n\nEven so, Dictate gets the big things — speed and accuracy — right, which may be enough for a lot of people. This program and the new Mac Office fill big holes in the Macintosh landscape — a landscape that’s looking brighter all the time.\n\nLater versions of the software added the features listed as lacking in David Pogue's initial review.\n\n\n"}
{"id": "52094190", "url": "https://en.wikipedia.org/wiki?curid=52094190", "title": "Mechanical paradox", "text": "Mechanical paradox\n\nThe mechanical paradox is an apparatus for studying physical paradoxes.\n\nThis apparatus, mounted on an elegant table, consists of a trapezoidal veneered wooden frame with two brass rails. A pair of brass cones joined at their bases by a wooden disk rests on the rails. When the double cone is placed at the low end of the frame, it automatically starts to roll upward, giving the impression of escaping the universal law of the gravitational force. Because of this phenomenon, astonishing in its seeming contradiction of common sense, the apparatus was often described as a \"mechanical paradox.\" In fact, the paradox is only apparent. This is due to the fact that the natural motion of bodies depends on that of their center of gravity, which has a natural tendency to descend. Since the rails diverge, the center of gravity of the double cone—when placed on the rotation axis at its maximum diameter—does not rise when the entire body seems to be moving upward; rather, the center is shifting downward. In its travel, the resting-points of the double cone on the rails converge toward its two apexes. As a result, the distance of the center of gravity from the horizontal plane decreases as the double cone rises.\n\nThe instrument is held in the Lorraine collections of the Museo Galileo in Florence.\n"}
{"id": "458096", "url": "https://en.wikipedia.org/wiki?curid=458096", "title": "Money order", "text": "Money order\n\nA money order is a payment order for a pre-specified amount of money. As it is required that the funds be prepaid for the amount shown on it, it is a more trusted method of payment than a check.\n\nThe money order system was established by a private firm in Great Britain in 1792, and was expensive and not very successful. Around 1836 it was sold to another private firm which lowered the fees, significantly increasing the popularity and usage of the system. The \nPost Office noted the success and profitability, and it took over the system in 1838. Fees were further reduced and usage increased further, making the money order system reasonably profitable. The only draw-back was the need to send an advance to the paying Post Office before payment could be tendered to the recipient of the order. This drawback was likely the primary incentive for establishment of the Postal Order System on 1 January 1881.\n\nA money order is purchased for the amount desired. In this way it is similar to a certified cheque. The main difference is that money orders are usually limited in maximum face value to some specified figure (for example, the United States Postal Service limits domestic postal money orders to US $1,000.00 ) while certified cheques are not. Money orders typically consist of two portions: the negotiable cheque for remittance to the payee, and a receipt or stub that the customer retains for his/her records. The amount is printed by machine or checkwriter on both portions, and similar documentation, either as a third hard copy or in electronic form and retained at the issuer and agent locations.\n\nMoney orders have limited acceptance in the insurance and brokerage industry because of concerns over money laundering. Because of provisions within the USA PATRIOT Act and the Bank Secrecy Act, money orders have far more regulatory processing requirements than personal cheques, cashier's cheques, or certified cheques.\n\nIn India, a money order is a service provided by the Indian Postal Service. A payer who wants to send money to a payee pays the amount and a small commission at a post office and receives a receipt for the same. The amount is then delivered as cash to the payee after a few days by a postal employee, at the address specified by the payer. A receipt from the payee is collected and delivered back to the payer at his address. This is more reliable and safer than sending cash in the mail.\n\nIt is commonly used for transferring funds to a payee who is in a remote, rural area, where banks may not be conveniently accessible or where many people may not use a bank account at all. Money orders are the most economical way of sending money in India for small amounts.\n\nIn the United States, money orders are typically sold by third parties such as the United States Postal Service, grocery stores, and convenience stores. Some financial service companies such as banks and credit unions may not charge for money orders to their clients. Money orders remain a trusted financial instrument. However, just because a particular business can issue a money order does not necessarily mean that they will cash them. The U.S. Postal Service issues money orders for a small charge at any location.\n\nThe United States Postal Service began selling money orders as an alternative to sending currency through the postal system in order to reduce post office robberies, an idea instituted by Montgomery Blair who was Postmaster-General 1861-1864. Money orders were later offered by many more vendors than just the postal service as a means to pay bills and send money internationally where there were not reliable banking or postal systems. Companies that now offer money orders include 7-11, QuikTrip, Cumberland Farms, Safeway, Western Union, MoneyGram, CVS, Wal-Mart, and 3T Solutions.\n\nObtaining a money order in the United States is simple, as they can be purchased with any form of money at any post office, and are sold at many other locations.\nThe US Postal Service's international money orders are accepted in 29 countries.\n\nPostal money orders (PMOs) are generally regarded as one of the most difficult financial documents to counterfeit. Security features include:\n\nDue to the increased public awareness of fraudulent US Postal Money Orders, counterfeiters are using these US Postal and other companies' (see above) money orders to dupe their victims. By obtaining their \"mark's\" postal zip code, they will draft bogus money orders based on whichever franchises will most likely be in their victim's home area as the familiarity of the store's name offers a sense of security. \n\nFurthermore, money orders are subject to erasing the name and writing in somebody else's name. Money order fraud is specifically not reimbursed by Western Union and the victim is required to go through local police to attempt to recover lost funds. In response, the perpetrator who cashed the money order can simply claim \"identity theft\" to the investigating detective.\n\nAn international money order is very similar in many aspects to a regular money order except that it can be used to make payments abroad. With it, a buyer can easily pay a seller for goods or services if he or she resides in another country. International money orders are often issued by a buyer's bank and bought in the currency that the seller accepts. International money orders are thought to be safer than sending currency through the post because there are various forms of identification required to cash an international money order, often including a signature and a form of photo identification.\n\nWhen purchasing an international money order, it is important to ensure that the specific type of money order is acceptable in the destination country. Several countries are very strict that the money order be on pink and yellow paper and bear the words \"international postal money order.\" In particular, the Japan Post (one of the largest banking institutions in the world) requires these features. Most other countries have taken this as a standard when there is any doubt of a document's authenticity.\n\nIn the last decade, a number of electronic alternatives to money orders have emerged and have, in some cases, supplanted money orders as the preferred cash transmission method. Many of these alternatives use the ubiquitous Visa/MasterCard payment systems to settle transactions. In Japan, the konbini system enables cash to cash transfers and is available at many of the thousands of convenience stores located in the country. In Italy, the PostePay system is offered through the Italian post office. In Ireland, 3V is offered through mobile top-up locations. In the United States, PaidByCash is offered at 60,000 grocery and convenience stores. In Bangladesh, mobile banking services enable electronic transfer of money as well as retail transactions.\nIn the United Kingdom, a number of credit card providers have started to provide pre-paid credit cards. These cards can be \"topped-up\" at any location that uses the Pay-Point system and also at the Post Office for the Post Office card. PayPal has their own branded pre-paid card which can be \"topped-up\" using a PayPal account or Pay-Points.\n\n"}
{"id": "9179644", "url": "https://en.wikipedia.org/wiki?curid=9179644", "title": "Multi-threshold CMOS", "text": "Multi-threshold CMOS\n\nMulti-threshold CMOS (MTCMOS) is a variation of CMOS chip technology which has transistors with multiple threshold voltages (V) in order to optimize delay or power. The V of a MOSFET is the gate voltage where an inversion layer forms at the interface between the insulating layer (oxide) and the substrate (body) of the transistor. Low V devices switch faster, and are therefore useful on critical delay paths to minimize clock periods. The penalty is that low V devices have substantially higher static leakage power. High V devices are used on non-critical paths to reduce static leakage power without incurring a delay penalty. Typical high V devices reduce static leakage by 10 times compared with low V devices. \n\nOne method of creating devices with multiple threshold voltages is to apply different bias voltages (Vb) to the base or bulk terminal of the transistors. Other methods involve adjusting the gate oxide thickness, gate oxide dielectric constant (material type), or dopant concentration in the channel region beneath the gate oxide. \n\nA common method of fabricating multi-threshold CMOS involves simply adding additional photolithography and ion implantation steps. For a given fabrication process, the V is adjusted by altering the concentration of dopant atoms in the channel region beneath the gate oxide. Typically, the concentration is adjusted by ion implantation method. For example, photolithography methods are applied to cover all devices except the p-MOSFETs with photoresist. Ion implantation is then completed, with ions of the chosen dopant type penetrating the gate oxide in areas where no photoresist is present. The photoresist is then stripped. Photolithography methods are again applied to cover all devices except the n-MOSFETs. Another implantation is then completed using a different dopant type, with ions penetrating the gate oxide. The photoresist is stripped. At some point during the subsequent fabrication process, implanted ions are activated by annealing at an elevated temperature.\n\nIn principle, any number of threshold voltage transistors can be produced. For CMOS having two threshold voltages, one additional photomasking and implantation step is required for each of p-MOSFET and n-MOSFET. For fabrication of normal, low, and high V CMOS, four additional steps are required relative to conventional single-V CMOS.\n\nThe most common implementation of MTCMOS for reducing power makes use of sleep transistors. Logic is supplied by a virtual power rail. Low V devices are used in the logic where fast switching speed is important. High V devices connecting the power rails and virtual power rails are turned on in active mode, off in sleep mode. High V devices are used as sleep transistors to reduce static leakage power.\n\nThe design of the power switch which turns on and off the power supply to the logic gates is essential to low-voltage, high-speed circuit techniques such as MTCMOS. The speed, area, and power of a logic circuit are influenced by the characteristics of the power switch. \n\nIn a \"coarse-grained\" approach, high V sleep transistors gate the power to entire logic blocks. The sleep signal is de-asserted during active mode, causing the transistor to turn on and provide virtual power (ground) to the low V logic. The sleep signal is asserted during sleep mode, causing the transistor to turn off and disconnect power (ground) from the low V logic. The drawbacks of this approach are that:\n\n\nIn a \"fine-grained\" approach, high V sleep transistors are incorporated within every gate. Low V transistors are used for the pull-up and pull-down networks, and a high V transistor is used to gate the leakage current between the two networks. This approach eliminates problems of logic block partitioning and sleep transistor sizing. However, a large amount of area overhead is added due both to inclusion of additional transistors in every Boolean gate, and in creating a sleep signal distribution tree. \n\nAn intermediate approach is to incorporate high V sleep transistors into threshold gates having more complicated function. Since fewer such threshold gates are required to implement any arbitrary function compared to Boolean gates, incorporating MTCMOS into each gate requires less area overhead. Examples of threshold gates having more complicated function are found with Null Convention Logic and Sleep Convention Logic. Some art is required to implement MTCMOS without causing glitches or other problems.\n"}
{"id": "30781788", "url": "https://en.wikipedia.org/wiki?curid=30781788", "title": "National Board of Boiler and Pressure Vessel Inspectors", "text": "National Board of Boiler and Pressure Vessel Inspectors\n\nThe National Board of Boiler and Pressure Vessel Inspectors (NBBI) is composed of chief boiler and pressure vessel inspectors representing states, cities, and provinces enforcing pressure equipment laws and regulations. Created to prevent death, injury and destruction, these laws and regulations represent the collective input of National Board members.\n\nDuring the past ten years, over six million pressure equipment inspections were performed in North America. Of that total, there were more than 556,000 violations, or more than 556,000 potential accidents that were prevented: almost one out of every ten pieces of equipment inspected. For the general public, the importance of thoroughly trained and specially commissioned inspectors is of critical significance: every person in the civilized world comes within close proximity of pressure equipment several times each day.\n\nSteam drove the Industrial Revolution during the mid-19th century. At this point in history, conversion of water was considered both good and bad: good in the sense it powered industrial progress, and bad in that boilers used in the conversion process employed new and unproven technology. According to the American Society of Mechanical Engineers (ASME):\n\nAs catastrophic casualties continued into the early 20th century, the ASME developed its boiler code in 1915. While the code provided a solid reference of construction standards, it lacked an important component: the authority to regulate. This was complicated by existence of local and state jurisdictions having their own codes and standards. The result was a patchwork of confusion having no basis in consistency.\n\nOn December 2, 1919, Ohio Chief Inspector Carl Myers met with chief inspectors from other jurisdictions to discuss creation of a board of inspector representatives from each of the existing jurisdictions. Hence, the genesis of The National Board of Boiler and Pressure Vessel Inspectors.\n\nThere are no longer 50,000 deaths caused by pressure equipment each year. However, if not properly maintained and inspected, boilers and pressure vessels can be lethal, and in some instances, catastrophic. \n\nFor example, rupture of a typical home hot-water tank generates the equivalent of 0.16 pounds of nitroglycerin. Translated, that is enough force to send the average car (weighing 2,500 pounds) to a height of nearly – or more than the elevation of a 14-story apartment building starting with a lift-off velocity of 85 miles per hour. When a similar hot-water tank explodes, its volume expands approximately 1,600 times. That is comparable to taking a trash can and causing it to fill a 12’ x 11’ living room with an ceiling in a split second. A large industrial boiler has the capacity to level an entire city block.\n\nEvery year the National Board hosts hundreds of boiler and pressure equipment professionals from around the world. National Board training facilities are located on a wooded campus in Columbus, Ohio. Attendees are taught by professionals actively involved with codes and standards development. Class sizes are limited for more interaction and individualized attention. The very latest equipment and instructional aides are employed along with the opportunity to acquire hands-on experience. National Board training reflects credibility and reputation as a third party evolving from the regulation and enforcement side of the boiler and pressure vessel industry.\n\nRegistering a pressure-retaining item with the National Board requires certain uniform quality standards be achieved certifying the manufacturing, testing, and inspection process. This certification acknowledges to owners, users, and public safety jurisdictional authorities registered items have been inspected by National Board-commissioned inspectors and built to required standards. The purpose of National Board registration is to promote safety and document specific equipment design and construction details for future use. It takes place when the manufacturer submits data reports to the National Board for items stamped with National Board numbers. \n\nA data report is similar to a birth certificate. Among the information included are: date of manufacture, materials of construction, specific details regarding design, and certification statements by both the manufacturer and inspector. Registration is required by most US jurisdictions for installation of pressure equipment. Registered pressure relief devices are stamped with a National Board NB Mark. For the manufacturer, data reports provide an essential form of customer service over the life of the equipment – a value-added quality of significant worth to the owner or user. Since the process began in 1921, there have been over 45 million data reports registered with the National Board.\n\nEach year, representatives from around the world travel to the National Board Testing Laboratory north of Columbus, Ohio. The purpose: to accurately measure the performance of their company’s pressure relieving devices. \n\nTested products undergo independent certification of function and capacity. A pressure relief device meeting new construction standards and specifications permits the manufacturer to apply the National Board NB mark to new equipment. Capacity certification signifies equipment designs have been thoroughly reviewed. Additionally, it indicates the quality system has been audited and the equipment meets internationally recognized standards for preventing potential overpressure conditions in boilers and pressure vessels.\n\nTesting is also performed to evaluate a company’s ability to properly repair pressure relief valves. Accredited repair organizations qualify to stamp the National Board VR symbol on repair nameplates.\n\nThe National Board lab supports industry research and development by testing new designs, serving as a comparative standard for other laboratories, validating new concepts, and – upon jurisdiction request – assist in boiler and pressure vessel incident investigations.\n\nRepairs and alterations are essential in maintaining pressure equipment integrity. These can vary from simple welded repairs to the repair of safety relief valves. The National Board administers three accreditation programs for organizations performing repairs and alterations. Accreditation involves a thorough evaluation of the organization’s quality system manual including a demonstration of its ability to implement the system. Authorized repair organizations are issued symbol stamps for application to equipment nameplates signifying the integrity of work performed. \n\nThe R Certificate of Authorization is issued to an accredited organization performing repairs and alterations to pressure-retaining items. The VR Certificate of Authorization is provided for repairs and modification to pressure relief device (R and VR stamps are required in a number of U.S. jurisdictions.) The NR Certificate of Authorization is issued for repairs and replacement of nuclear components. All National Board code symbol stamps are registered trademarks of The National Board of Boiler and Pressure Vessel Inspectors.\n\nAs the flagship publication of the National Board, the \"National Board Inspection Code\" (NBIC) is a consensus document created by an evolving committee of pressure equipment professionals. Distributed biennially, the NBIC provides rules, information, and guidance to manufacturers, jurisdictions, inspectors, repair organizations, owner-users, installers, contractors, as well as other individuals and organizations performing or involved in post-construction activities. The objective is to provide uniform administration of rules pertaining to pressure equipment items.\n\nThe NBIC was first published in 1945 and is today the \"only\" standard recognized worldwide for inservice repair and alteration of boilers and pressure vessels. Approved as an American National Standard (ANSI) in August 1987, the National Board Inspection Code has been adopted by a number of states and jurisdictions, as well as federal regulatory agencies, including the United States Department of Transportation. \n\nProposed changes are made available for public review and comment thus allowing industry, academia, regulatory and jurisdictional agencies, and the public-at-large to contribute to NBIC development. It is available in hard copy and through a subscription basis on the Internet.\n\nNational Board’s technical journal is distributed worldwide three times annually. In addition to articles of interest to the pressure equipment industry, the \"BULLETIN\" provides an up-close look at jurisdiction chief inspectors; timely updates on National Board member changes; helpful tips on equipment inspection, repairs and alterations; industry case histories; and a comprehensive listing of jurisdiction law and regulation amendments. Readers also find technical perspective by National Board staff and guest columnists, a complete listing of offerings from the training department, and the latest violations tracking data.\n\nEach spring, the General Meeting is conducted in conjunction with the American Society of Mechanical Engineers to address important issues relative to the safe installation, operation, maintenance, construction, repair, and inspection of boilers and pressure vessels. Attendees include boiler and pressure vessel inspectors, mechanical engineers, engineering consultants, equipment manufacturers, representatives of repair organizations, operators, owners and users of boilers and pressure vessels, labor officials, welding professionals, insurance industry representatives, and government safety personnel.\n\nFocus of the week-long event is an exchange of expertise and technical insight shared by other attendees, as well as making contacts and participating in numerous industry and committee meetings. General session presentations cover a wide range of pressure equipment topics such as safe operation, maintenance and repair, safety valves - as well as other unit components – testing codes and standards, risks and reliability, and training.\n\nThe National Board annually offers up to two $12,000 scholarships to select college students meeting eligibility standards. Application period extends from September 1 to February 28. Scholarships are available to the children, step-children, grandchildren, or great-grandchildren of past or present National Board Commissioned Inspectors (living or deceased). These are also available to children of past or present National Board employees (living or deceased). To be considered, a student must be enrolled full-time at an accredited U.S. or Canadian college or university, plan to be enrolled for the upcoming academic year, major in pressure equipment-related or closely related engineering discipline, possess a cumulative 3.0 GPA or higher (4.0 scale), and be either a U.S. or Canadian citizen. A letter of recommendation from a current National Board member is also required.\n\n"}
{"id": "1496447", "url": "https://en.wikipedia.org/wiki?curid=1496447", "title": "OKB", "text": "OKB\n\nOKB is a transliteration of the Russian initials of \"Опытное конструкторское бюро\" – \"Opytnoye Konstruktorskoye Buro\", meaning Experimental Design Bureau. During the Soviet era, OKBs were closed institutions working on design and prototyping of advanced technology, usually for military applications.\n\nA bureau was officially identified by a number, and often semi-officially by the name of its lead designer – for example, OKB-51 was led by Pavel Sukhoi, and it eventually became known as the OKB of Sukhoi. Successful and famous bureaus often retained this name even after the death or replacement of their designers.\n\nThese relatively small state-run organisations were not intended for the mass production of aircraft, rockets, or other vehicles or equipment which they designed. However, they usually had the facilities and resources to construct prototypes. Designs accepted by the state were then assigned to factories for mass production.\n\nAfter the collapse of the Soviet Union, many OKBs became Scientific Production Organizations (Научно-производственное объединение) abbreviated to NPO. There were some attempts to merge them in the 1990s, and there were widespread amalgamations in 2001–2006 to create \"national champions\", such as Almaz-Antey to consolidate SAM development.\n\n\nAviation.ru - \"OKBs\"\n\n"}
{"id": "28885698", "url": "https://en.wikipedia.org/wiki?curid=28885698", "title": "Old Pal Tackle box", "text": "Old Pal Tackle box\n\nOld Pal was a line of tackle boxes produced by the Woodstream Corporation from the 1950s to the 1980s in Lititz, Pennsylvania. The Old Pal tackle box appeared on the television show \"Emergency!\", and was used as the drug box throughout the show. Old Pal tackle boxes came in many sizes and materials; some were metal and the later models were hard durable plastic. Many of the boxes can be found circulating through the auction website Ebay. One notable owner is Disastrous the famous Scotch actor, fisherman, sailor, and (rarely mentioned) bandsman.\n\n"}
{"id": "463149", "url": "https://en.wikipedia.org/wiki?curid=463149", "title": "Open Financial Exchange", "text": "Open Financial Exchange\n\nOpen Financial Exchange (OFX) is a data-stream format for exchanging financial information that evolved from Microsoft's Open Financial Connectivity (OFC) and Intuit's Open Exchange file formats.\n\nMicrosoft, Intuit and CheckFree announced the OFX standard on 16 January 1997. The first OFX specification, version 1.0, was released on 14 February 1997. The specification allows for bank- and application-specific extensions, although only a subset is necessary to describe a financial transaction.\n\nVersions 1.0 through 1.6 relied on SGML for data exchange, but later versions are XML based. According to the main OFX site, \"The specification is freely licensed, allowing any software developer to design an interface that will be supported on the front-end.\"\n\nMany banks in the US let customers use personal financial management software to automatically download their bank statements in OFX format, but most Canadian, United Kingdom and Australian banks do not allow this (though in Australia, CBA does export OFX and QIF files).\n\nQFX is a proprietary variant of OFX used in Intuit's products. In the Intuit Products, OFX is used for Direct Connect and QFX for Web Connect. Direct Connect allows personal financial management software to connect directly to a bank OFX server, whereas in Web Connect, the user needs to log in and manually download a .qfx file and import it into Quicken.\n\n\n"}
{"id": "24631889", "url": "https://en.wikipedia.org/wiki?curid=24631889", "title": "PICMG 1.2", "text": "PICMG 1.2\n\nPICMG 1.2 is a specification by PICMG that standardizes both mechanical and electrical interfaces to support a standard form factor PCI computer system. PICMG 1.2 defines a single board computer in a passive backplane architecture with either two PCI/PCI-X busses or a single PCI/PCI-X bus. It is similar to PICMG 1.0 but removes the ISA bus.\n\nAdopted : 1/23/2002\n\nCurrent Revision : 1.0\n"}
{"id": "34111257", "url": "https://en.wikipedia.org/wiki?curid=34111257", "title": "Phase detector characteristic", "text": "Phase detector characteristic\n\nA phase detector characteristic is a function of phase difference describing the output of the phase detector.\n\nFor the analysis of Phase detector it is usually considered the models\nof PD in signal (time) domain and phase-frequency domain.\nIn this case for constructing of an adequate nonlinear mathematical model of PD in phase-frequency domain it is necessary to find the characteristic of phase detector.\nThe inputs of PD are high-frequency signals and the output contains a low-frequency error correction signal, corresponding to a phase difference of input signals. For the suppression of high-frequency component of the output of PD (if such component exists) a low-pass filter is applied. The\ncharacteristic of PD is the dependence of the signal at the\noutput of PD (in the phase-frequency domain) on the difference of phases \nat the input of PD.\n\nThis characteristic of PD depends on the realization of PD and the types of waveforms of signals. Consideration of PD characteristic allows to apply averaging methods for high frequency oscillations and to pass from analysis and simulation of non autonomous models of phase synchronization systems in time domain to analysis and simulation of autonomous dynamical models in phase-frequency domain \n\nConsider a classical phase detector implemented with analog multiplier and low-pass filter.\n\nHere formula_1 and formula_2 denote high-frequency signals, piecewise differentiable functions formula_3, formula_4 represent waveforms of input signals, formula_5 denote phases, and formula_6 denotes the output of the filter.\nIf formula_7 and formula_5 satisfy the high frequency conditions (see ) then phase detector characteristic formula_9 is calculated in such a way that time-domain model filter output \nand filter output for phase-frequency domain model \nare almost equal:\n\nConsider a simple case of harmonic waveforms formula_13 formula_14 and integration filter.\nStandard engineering assumption is that the filter removes\nthe upper sideband formula_16 from\nthe input but leaves the lower sideband formula_17\nwithout change.\n\nConsequently, the PD characteristic in the case of sinusoidal waveforms is\n\nConsider high-frequency square-wave signals formula_19 and formula_20.\nFor this signals it was found that similar thing takes place.\nThe characteristic for the case of square waveforms is\n\nLet us consider general case of piecewise-differentiable waveforms formula_22, formula_4.\n\nThis class of functions can be expanded in Fourier series.\nDenote by\nthe Fourier coefficients of formula_3 and formula_4.\nThen the phase detector characteristic is\n\nObviously, the PD characteristic formula_30 is periodic, continuous, and bounded on formula_31.\n\nModeling method based on this result is described in \n"}
{"id": "88378", "url": "https://en.wikipedia.org/wiki?curid=88378", "title": "Pressed flower craft", "text": "Pressed flower craft\n\nPressed flower craft consists of drying flower petals and leaves in a flower press to flatten and exclude light and moisture. Pressing flowers makes them appear flat, and there is often a change in color, ranging from faded colors to a greater intensity of vibrant colors. It has long been practiced as an art form in China and in Japan, where it is known as Oshibana (). Outside of Asia, the art gained popularity in England during the Victorian era and has experienced a revival in the last 30 years or so. It is currently used in Australia and in the United States by some recognized artists, including Cellestine Hannemann and Janie Gross.\n\nThe pressed flowers and leaves can be used in a variety of craft projects. They are often mounted on special paper, such as handmade paper, Ingres paper, Japanese paper, or paper decorated by marbling. With meticulous attention to detail, each leaf and flower is glued onto a precise location. With a creative approach to the use of materials, a leaf becomes a tree and petals form mountains.\n\nWashes of watercolor painting are sometimes applied to the backing paper before the pressed material is attached. Pressed material may also be mounted on fabrics, such as velvet, silk, linen or cotton.\n\nPetals and leaves can be applied to wood furnishings using the technique of Decoupage.\n\nThe Pressed Flower Craft Guild was established in 1983 by Joyce Fenton (a pressed flower artist) and Bill Edwardes (who devised the method of framing pressed flower pictures adopted by the Guild). It has members throughout the UK and in other parts of the world.\n\nIts stated aims are:\n\nProficiency awards in a number of different specialities may be attained after the Gold Award is gained. The Overseas Advanced Award may be taken by overseas members who have gained the Grade I and Grade II awards. A different subject for both of these awards is announced annually.\n\nThe IPFAS in an international pressed flower organization that promotes pressed flower art and offers education and holds competitions. It has members from over 20 nations (as of 2010) including Japan, the United Kingdom, United States, France, Germany, Mexico, and Australia. It was founded in 1999 by Nobuo Sugino, a Japanese pressed flower artist and President of Japan Wonderful Oshibana Club.\n\nThe WWPFG was established in July 2001. In November 2008, the guild was incorporated in North Carolina, USA, as a public educational non-profit organisation.\n\n\n"}
{"id": "176654", "url": "https://en.wikipedia.org/wiki?curid=176654", "title": "Roll film", "text": "Roll film\n\nRollfilm or roll film is any type of spool-wound photographic film protected from white light exposure by a paper backing, as opposed to film which is protected from exposure and wound forward in a cartridge. The term originated in contrast to sheet film. Confusingly, roll film was originally often referred to as \"cartridge\" film because of its resemblance to a shotgun cartridge.\n\nThe opaque backing paper allows roll film to be loaded in daylight. It is typically printed with frame number markings which can be viewed through a small red window at the rear of the camera. A spool of roll film is usually loaded on one side of the camera and pulled across to an identical take up spool on the other side of the shutter as exposures are made. When the roll is fully exposed, the take up spool is removed for processing and the empty spool on which the film was originally wound is moved to the other side, becoming the take up spool for the next roll of film.\n\nIn 1881 a farmer in Cambria, Wisconsin, Peter Houston, invented the first roll film camera. His younger brother David, filed the patents for various components of Peter's camera. \nDavid Henderson Houston (born June 14, 1841; died May 6, 1906), originally from Cambria, Wisconsin, patented the first holders for flexible roll film. Houston moved to Hunter in Dakota Territory in 1880. He was issued an 1881 patent for a roll film holder which he licensed to George Eastman (it was used in Eastman's Kodak 1888 box camera). Houston sold the patent (and an 1886 revision) outright to Eastman for $5000 in 1889. Houston continued developing the camera, creating 21 patents for cameras or camera parts between 1881 and 1902. In 1912 his estate transferred the remainder of his patents to Eastman.\n\nIn 1998, Fujifilm introduced a film identification system for 120 and 220 format roll film called \"Barcode System\" (with logo \"|||B\"). The barcode encoding the film format and length as well as the film speed and type is located on the sticker between the emulsion carrying film and the backing paper. This 13-bit barcode is optically scanned by newer medium format cameras like the Fujifilm GA645i Professional, GA645Wi Professional, GA645Zi Professional, GX645AF Professional, GX680III Professional, GX680IIIS Professional, Hasselblad H1, H2, H2F and H3D Model I with HM 16-32 as well as by the Contax 645 AF.\n\n"}
{"id": "43089362", "url": "https://en.wikipedia.org/wiki?curid=43089362", "title": "Surface energy transfer", "text": "Surface energy transfer\n\nSurface energy transfer (SET) is a dipole-surface energy transfer process involving metallic surface and molecular dipole.\n\nThe SET rate follows the inverse of the fourth power of the distance\nwhere formula_2 is the donor emission lifetime, formula_3 is the distance between donor-acceptor, and formula_4 is the distance at which SET efficiency decreases to 50% (i.e., equal probability of energy transfer and spontaneous emission).\n\nThe energy transfer efficiency also follows a similar form\n\nDue to the fourth power dependence SET can cover a distance more than 15 nm, which is almost twice the efficiency of FRET. Theoretically predicted in 1978 by Chance \"et al.\" it was proved experimentally in 2000s by different workers.\n\nThe efficiency of SET as nanoruler has been used in live cells.\n\nGold nano particles are frequently used in these studies as the nanoparticle surface.\n\n"}
{"id": "45486157", "url": "https://en.wikipedia.org/wiki?curid=45486157", "title": "Test loop translator", "text": "Test loop translator\n\nA test loop translator (TLT) is a type of radio frequency converter or heterodyne, used to translate between uplink and downlink segments (generally in the same band), to allow for \"loop-back\" testing and calibration of a satellite ground station without the need to interface with the satellite. \nThe test loop translator is an extremely valuable tool for evaluating the performance of satellite earth stations. It allows the user to carry out analysis, alignment and system testing without incurring satellite airtime costs and the risk of interfering with other satellite users. Thus, it has applications during equipment development, qualification, troubleshooting and in-service routine monitoring.\nTLTs generally contain a fixed or preset local oscillator (LO) and a preset gain, though the LO and gain may be adjustable in some models. Most models have a negative gain (i.e., a loss), with -15 dB being the most common value. Test loop translators can cover one or more of the satellite communication bands S, C, X, K, DBS, and K.\n"}
{"id": "36110078", "url": "https://en.wikipedia.org/wiki?curid=36110078", "title": "Thomas Henry Sumpter Walker", "text": "Thomas Henry Sumpter Walker\n\nThomas Henry Sumpter Walker aka \"T. H. S. Walker\", (*29 June 1856 in Cambridge; † 1 May 1936) was an English bicycle racing pioneer who lived and worked for decades in Germany.\n\nThomas Walker bought his first bicycle in 1868. From 1869 until 1872 he attended the Rugby School. In 1873 he moved to Bad Godesberg in Germany.\n\nHaving obtained his school certificate he left Germany to join the 22nd Middlesex Rifle Volunteer Corps. After having served his time he got hired by the \"Howe Machine Company\". Among other products this firm manufactured bicycles. As a representative he returned to Germany. Eventually he represented in Berlin also \"St. George's Engineering\", \"Rudge-Whitworth\" and \"Quadrant Tricycle\". The year 1882 marked the beginning of his career as a sportsman. As an amateur he competed in several international bicycle races, including events in Prague, London and throughout Germany. Based on his reputation he was chosen to organise meetings and races. As his achievements were increasingly recognised he also put up national championships and represented the German cyclist internationally. Also he created the first German publication for organised cyclist, which started as \"Das Velociped\" and was later continued as \"Der Radfahrer\". Moreover he published a variety of leaflets and books including George Lacy Hillier's \"The Art of Cycle Racing\" which got released in English, German and French.\n\nIn 1889, at the age of 34, Walker finished his commitment because he was tired of the permanent quarreling among German cyclists. Disappointed by the fact that he'd encountered xenophobic attitudes he dropped his plan to become a German citizen and left Germany for good. For a certain time he lived in Bristol. Later he moved to London. In September 1936 \"The London Gazette\" published an advertisement by a solicitor from London who hereby tried to find relatives of the deceased.\n\n"}
{"id": "14723101", "url": "https://en.wikipedia.org/wiki?curid=14723101", "title": "Todd Siler", "text": "Todd Siler\n\nTodd Siler (born August 23, 1953) is an American multimedia artist, author, educator, and inventor, equally well known for his art and for his work in creativity research. A graduate of Bowdoin College, he became the first visual artist to be granted a PhD from MIT (interdisciplinary studies in Psychology and Art, 1986). Siler began advocating the full integration of the arts and sciences in the 1970s and is the founder of the ArtScience Program and movement.\n\nIn the early 1980s, Siler made an extensive study of genius across numerous disciplines to see what, if anything, such highly creative people as Einstein and Rachmaninoff have, or more importantly do, in common. Although such inquiries are standard, Siler's work went further than any work before or since in examining how methods used by highly creative people might work on the neurological and cellular level. \"Creativity is any unconditioned response,\" is typical of Siler's approach, which both validates and challenges the work of luminaries in the field such as Howard Gardner, Mihaly Csikszentmihalyi and Robert Root-Bernstein (Encyclopedia of Creativity, 1999). These theories were elaborated in two books, \"Breaking The Mind Barrier: The ArtScience of Neurocosmology\" (Simon & Schuster, 1990; Touchstone Books, 1992), which is largely intended for scholars, and \"Think Like A Genius\" (Bantam Books, 1997; Transworld, 1998) written for the general reader. Siler has developed these theories into proprietary programs which are used extensively in schools and corporations.\n\nThe son of an aspiring concert pianist and bio-medical researcher, as a child, Siler was a prodigy in the fine arts, often using highly detailed drawings to express his ideas on integrating the arts and sciences. He studied art as an undergraduate, spending a year \"apprenticed\" in the studio of American artist Leonard Baskin. In his 20s Siler was part of the same SoHo art scene which launched Julian Schnabel, Francesco Clemente and David Salle. Today, Siler's artworks are in numerous public collections including the Solomon R. Guggenheim Museum, The Metropolitan Museum of Art (20th Century Collection), The Museum of Modern Art in New York City, the Pushkin Museum of Fine Arts in Moscow, and The Israel Museum in Jerusalem.\n\nIn 2006, Siler used a multimedia exhibition at New York's Ronald Feldman Gallery to present his proposal for the nature-inspired \"Fractal Reactor,\" which offers an environmental-friendly, alternative method of using controlled nuclear fusion for energy purposes. While the actual processes used by the fractal reactor, rely on highly sophisticated physical and mathematical formulations, its principles re-examine the hypotheses behind nuclear fusion in novel ways. This proposal has been taken up by the International Atomic Energy Agency for further study.\n\nAs an artist who has championed the study of science, Siler worked with the Cherry Creek School District (Colorado) to pioneer experiential learning methodologies based on the understanding and creation of systems of metaphor. These methodologies have since spread to both public and private schools in America and abroad. Siler was instrumental in developing the interdisciplinary curriculum for one of the world's most respected schools for the gifted, The Israel Arts and Science Academy (IASA) in Jerusalem.\n\nThese programs have become popular with Fortune 500 companies as a way of promoting out-of-the-box thinking. Siler has unusual credibility in the corporate community as in addition to being a successful artist and scholar, he holds a number of patents on a wide range of inventions, including a widely used computer-graphics input device and textile printing machinery.\n\nIn 2011 he became the recipient of the Leonardo da Vinci World Award of Arts in recognition for his extraordinarily creative and innovative contributions to contemporary and visual arts, for stimulating creativity, inspiring innovation and uniting art and science to enrich the experience of creative learning.\n\n"}
{"id": "4560311", "url": "https://en.wikipedia.org/wiki?curid=4560311", "title": "Uses of compost", "text": "Uses of compost\n\nUses of compost describes the range of beneficial uses for compost. Compost is a versatile product resulting from composting - the biodegradation of organic waste at household, community or citywide level. Composting can be carried out at the household level, in garden composters or in composting toilets, or at municipal level at centralised composting plants. The method of producing the compost has an influence on its possible uses in terms of quantity and quality considerations.\n\nThe basic use of compost is conditioning and fertilizing soil by the addition of humus, nutrients and beneficial soil bacteria, with a wide range of specific applications.\n\nOn the open ground, for growing wheat, corn, soybeans, and similar crops, compost can be broadcast across the top of the soil using spreader trucks or spreaders pulled behind a tractor. It is expected that the spread layer is very thin (approximately 6 mm (0.25 in.)) and worked into the soil prior to planting. However, application rates of 25 mm (one in.) or more are not unusual when trying to rebuild poor soils or control erosion. Due to the extremely high cost of compost per unit of nutrients in the western world (such as the United States) on-farm use is relatively rare since rates over 4 tons/acre can not be afforded. This is unfortunate and results from over-emphasis on \"recycling organic matter\" than on \"sustainable nutrients\". In other countries such as Germany, where compost distribution and spreading are partially subsidized in the original waste fees, compost is used more frequently on open ground, but only on the premise of nutrient \"sustainability\"\n\nIn plasticulture, strawberries, tomatoes, peppers, melons, and other fruits and vegetables are often grown under plastic to control temperature, retain moisture and control weeds. Compost may be banded (applied in strips along rows) and worked into the soil prior to bedding and planting, be applied at the same time the beds are constructed and plastic laid down, or used as a \"top dressing\".\n\nMany crops are not seeded directly in the field but are started in seed trays in a greenhouse (see transplanting). When the seedlings reach a certain stage of growth, they are transplanted in the field. Compost can be used as an ingredient in the mix used to grow the seedlings, but is not normally used as the only planting substrate. The crop to be grown and the seeds' sensitivity to nutrients, salts, etc. dictates the ratio of the blend, and maturity is important to insure that oxygen deprivation will not occur or that no lingering phyto-toxins remain.\n\nCompost is used in horticulture in a wide range of contexts. In raised bed gardening, compost can be mixed with sand, clay, aged sawdust, and other materials to create an enriched mix for landscape beds or raised-bed gardens. Compost should be no more than 30 percent of the total mix. Use a high quality mature compost to avoid nutrient and oxygen competition with plants.\n\nIn a container garden, as in bedding mixes, compost may be a beneficial ingredient in potting media, used up to 30 percent of the total mix, depending on salinity and maturity. It is considered a partial substitute for peat moss, but generally lacks the porosity and water-holding capacity of peat so must be used in limited percentages. The nutrient content of compost can also reduce the need for supplemental chemical fertilizers, although this has to be determined in each situation.\n\nExcavated areas around the foundation of new buildings are backfilled when construction is complete, but these planting zones may contain rubble, residues of toxic chemicals, and other undesirable substances. Removing the backfill and replacing it with a soil/compost mix will improve soil structure and give foundation plantings a healthier start.\n\nTwo or more inches of compost can be used alone or in conjunction with conventional mulch products to keep root zones cool, conserve moisture, and act as a slow-release fertilizer, provided the product is coarsely textured and mature. For a weed barrier, double or triple the depth of compost can be used, placed on top of a thick layer of newspapers, to replace geomembrane weed barriers. This is obviously only true if the compost is weed free; many are not.\n\nFor trees and shrubs, mixes of \"well aged\" compost with the native soils can be used as backfill. Immature composts may cause settling and young root disturbance due to oxygen deprivation. Seasonally, top dress with compost to the drip line and rake into the soil.\n\nTo establish new turf areas (lawns, recreation fields, golf courses), compost can be applied prior to seeding or sodding and work into the soil. Compost can seasonally be used to top dress and may also be raked into the soil. Some turf farms also use compost, growing grass in a couple of inches of the material to prevent topsoil loss.\n\nTopsoil loss is a serious ecological issue. The use of compost to control sediment run-off and fight erosion is a relatively new technology, now being adopted by local authorities, developers, farmers, and other major disturbers of soil as another tool to reduce topsoil loss.\n\nA layer of compost spread over a disturbed area of soil is called a compost blanket. With a high water-holding capacity, compost is not tilled into the soil but remains on the surface to temper the impact of rainfall. Even small amounts can help, but typical recommendations call for a 5 cm (2 in.) layer to insure adequate surface coverage. The blanket can also be directly planted into.\n\nCompost berms and socks are used alone or in conjunction with compost blankets to mitigate the impact of high volume water discharges and flows. Compost berms are more aesthetically pleasing than silt fences and eliminate the need to remove the berm when the project is complete. Over time, a compost berm simply biodegrades and returns to the earth. As the name implies, a compost sock is a mesh tube stuffed with compost. Socks stand up better to heavy equipment, can be anchored in place, and are easily removed/reused. If a biodegradable fiber is used for the sock, it can also be left in place to biodegrade. This is rarely if ever practiced, however, since it defeats the idea of the sock.\n\nAdditional special uses for compost include use as a planting media for constructed or artificial wetlands, as a cap for a landfill cell when it is closed to encourage vegetation and reduce erosion, and as erosion control along streambanks to restore functionality and beauty to riparian zones while possibly mitigating future damage.\n\nEPA Class A and B guidelines in the United States were developed solely to manage the processing and beneficial reuse of sludge, also now called biosolids, following the US EPA ban of ocean dumping. About 26 American states now require composts to be processed according to these federal protocols for pathogen and vector control, even though the application to non-sludge materials has not been scientifically tested. An example is that green waste composts are used at much higher rates than sludge composts were ever anticipated to be applied at. U.K guidelines also exist regarding compost quality, as well as Canadian, Australian, and the various European states.\n\nIn the United States, some compost manufacturers participate in a testing program offered by a private lobbying organization called the U.S. Composting Council. The USCC was originally established in 1991 by Procter & Gamble to promote composting of disposable diapers, following state mandates to ban diapers in landfills, which caused a national uproar. Ultimately the idea of composting diapers was abandoned, partly since it was not proven scientifically to be possible, and mostly because the concept was a marketing stunt in the first place. After this, composting emphasis shifted back to recycling organic wastes previously destined for landfills. There are no bonafide quality standards in America, but the USCC sells a seal called \"Seal of Testing Assurance\" (also called \"STA\"). For a considerable fee, the applicant may display the USCC logo on products, agreeing to volunteer to customers a current laboratory analysis that includes parameters such as nutrients, respiration rate, salt content, pH, and limited other indicators. However, the STA program is not ISO approved, and is a financially beneficial activity for the private USCC, an organization that does disclose its books (in 2009 USCC earned $65,000 from STA fees). Some argue that the existence of STA means EPA or USDA do not have to regulate composts.\n\n"}
{"id": "51347632", "url": "https://en.wikipedia.org/wiki?curid=51347632", "title": "Xiangning Zhang", "text": "Xiangning Zhang\n\nXiangning \"Forrest\" Zhang 张向宁 (born in 1972) is a Chinese information technology entrepreneur, angel investor, and venture capitalist. He is among the first group of Internet entrepreneurs in China, and also the founder, former Chairman and CEO of HiChina Corporation (WWW.NET.CN) as well as the Chairman and CEO of Tixa Internet Technology Corporation. Zhang foresaw the potential of the Internet and provided domain name registration and web hosting services from the end of 1995, leading HiChina to earn the largest share of the Chinese market in this field. Later he started his second company, Tixa, and continued to innovate while holding positions in organizations like the All-China Youth Federation and the China Council for International Investment Promotion.\n\nZhang enrolled in Beijing Normal University at 16 years old. His paper on the Principle of Relativity written at 17 years old impressed a professor at the Massachusetts Institute of Technology (MIT), but shortly afterwards, he willingly stopped schooling at 18 years old.\n\nBetween 1990 and 1994, Zhang went on to experience the role of a normal staff member and the transition from failure to success in founding businesses. Helping Zhang gain operation experience and a deep understanding for company management, his work expanded through industries such as international trade, shipping, tourism, bid management, equipment development, computer exhibition, and more. At 22 years old, Zhang became one of the few Chinese youth entrepreneurs.\n\nIn 1994, Zhang foresaw the potential of the Internet. Initially interested in the Bulletin Board System (BBS), he started to create one of his own. He eventually decided to suspend all of his other businesses and focus on the Internet industry.\n\nAt the end of 1995, Zhang founded HiChina Co. (Chinese: 万网) (WWW.NET.CN), which was to become the largest domain name registration and web hosting service company in China. As the Chairman and CEO, Zhang led the startup company from a size of a few more than 10 staff members to a size of thousands.\n\nIn January 2000, Zhang initiated the \"Net.cn Plan\", associating with Sina, Sohu, NetEase, Changhong, Kelon, Computer World, China Info World (CIW), China Internet Network Information Center, China Information Association, and 37 other notable companies and organizations, and created the \"Zhong Guo Qi Ye Shang Wang Fu Wu Lian Meng\" (Translated as: Internet Services for China Businesses Alliance). Zhang also announced that the year 2000 would be China's \"Qi Ye Shang Wang Nian\" (Translated as: Year of Internet Utilization for Businesses).\n\nDuring HiChina's development, Zhang led HiChina through two rounds of equity trades with major investors including IDG Ventures, TPG Newbridge, and more.\n\nIn 1998, Zhang completed his Master's degree at Huazhong University of Science and Technology.\n\nIn November 2001, Zhang chose to abdicate his position, only remaining as a shareholder, and collaborated with his long-time friends to found a new company \"VeryE.com\". VeryE would create a brand new Internet business model, making the Internet more practical.\n\nIn February 2004, VeryE obtained investment from Sumitomo Mitsui Banking Corporation, Japan Asia Investment Co. (JAIC), MIH Investments, and became Tixa Co. (WWW.TIXA.COM).\n\nIn September 2009, Alibaba acquired HiChina and later built it into Alibaba Cloud (Aliyun), maintaining its position as the top domain name registration and web hosting service company in China.\n\n\n\n"}
{"id": "3541533", "url": "https://en.wikipedia.org/wiki?curid=3541533", "title": "Yarder", "text": "Yarder\n\nA yarder is piece of logging equipment which uses a system of cables to pull or fly logs from the stump to the landing. It generally consists of an engine, drums, and spar, but has a range of configurations and variations such as the Swing yarder.\n\nThe early yarders were steam powered. They traveled on railroads, known as \"dummylines\" and the felled trees were dragged or \"skidded\" to the railroad where they were later loaded onto rail cars. Popular brands were: Willamette, Skagit, Washington, Tyee, or Lidgerwood and Clyde built by Clyde Ironworks in Duluth, Minnesota. Although these machines appear to be large and cumbersome, they were true workhorses of their day. The Clyde was capable of retrieving logs from four different points at the same time. Each cable, or lead, was approximately 1000 feet in length. Once the logs were attached and a clearance signal was sent for retrieval, they could be skidded at a speed of 1000 feet per minute (1MPH = 88 fpm = 26.8 meter per minute) this is around 10 mph. Working conditions around these machines were very dangerous.\n\nAfter the logs were hauled to the landing, a separate machine called a loader, loaded them with tongs or heel-booms onto rail cars or diesel engine trucks. Loaders were sometimes called duplexes as they had two steam engines to control the tongs. A large machine with a yarder and loader combined is called a unit. In the 1950s steam engines were replaced by diesel engine, and war-surplus tank chassis, which provided a frame for the new yarders, now that rail mounted ones were obsolete.\n\nCable logging used primarily on the U.S. west coast uses a yarder, loaders and grapple yarders.\n\n"}
