{"id": "347027", "url": "https://en.wikipedia.org/wiki?curid=347027", "title": "Amorphous metal", "text": "Amorphous metal\n\nAn amorphous metal (also known as metallic glass or glassy metal) is a solid metallic material, usually an alloy, with disordered atomic-scale structure. Most metals are crystalline in their solid state, which means they have a highly ordered arrangement of atoms. Amorphous metals are non-crystalline, and have a glass-like structure. But unlike common glasses, such as window glass, which are typically electrical insulators, amorphous metals have good electrical conductivity. There are several ways in which amorphous metals can be produced, including extremely rapid cooling, physical vapor deposition, solid-state reaction, ion irradiation, and mechanical alloying.\n\nIn the past, small batches of amorphous metals have been produced through a variety of quick-cooling methods. For instance, amorphous metal ribbons have been produced by sputtering molten metal onto a spinning metal disk (melt spinning). The rapid cooling, on the order of millions of degrees Celsius a second, is too fast for crystals to form and the material is \"locked\" in a glassy state. More recently a number of alloys with critical cooling rates low enough to allow formation of amorphous structure in thick layers (over 1 millimeter) have been produced; these are known as bulk metallic glasses (BMG). More recently, batches of amorphous steel with three times the strength of conventional steel alloys have been produced.\n\nThe first reported metallic glass was an alloy (AuSi) produced at Caltech by W. Klement (Jr.), Willens and Duwez in 1960. This and other early glass-forming alloys had to be cooled extremely rapidly (on the order of one megakelvin per second, 10Â K/s) to avoid crystallization. An important consequence of this was that metallic glasses could only be produced in a limited number of forms (typically ribbons, foils, or wires) in which one dimension was small so that heat could be extracted quickly enough to achieve the necessary cooling rate. As a result, metallic glass specimens (with a few exceptions) were limited to thicknesses of less than one hundred micrometers.\n\nIn 1969, an alloy of 77.5% palladium, 6% copper, and 16.5% silicon was found to have critical cooling rate between 100 and 1000 K/s.\n\nIn 1976, H. Liebermann and C. Graham developed a new method of manufacturing thin ribbons of amorphous metal on a supercooled fast-spinning wheel. This was an alloy of iron, nickel, phosphorus and boron. The material, known as \"Metglas\", was commercialized in the early 1980s and is used for low-loss power distribution transformers (Amorphous metal transformer). Metglas-2605 is composed of 80% iron and 20% boron, has Curie temperature of and a room temperature saturation magnetization of 1.56 teslas.\n\nIn the early 1980s, glassy ingots with diameter were produced from the alloy of 55% palladium, 22.5% lead, and 22.5% antimony, by surface etching followed with heating-cooling cycles. Using boron oxide flux, the achievable thickness was increased to a centimeter.\n\nResearch in Tohoku University and Caltech yielded multicomponent alloys based on lanthanum, magnesium, zirconium, palladium, iron, copper, and titanium, with critical cooling rate between 1 K/s to 100 K/s, comparable to oxide glasses.\n\nIn 1988, alloys of lanthanum, aluminium, and copper ore were found to be highly glass-forming. Al-based metallic glasses containing Scandium exhibited a record-type tensile mechanical strength of about 1500 MPa.\n\nIn the 1990s new alloys were developed that form glasses at cooling rates as low as one kelvin per second. These cooling rates can be achieved by simple casting into metallic molds. These \"bulk\" amorphous alloys can be cast into parts of up to several centimeters in thickness (the maximum thickness depending on the alloy) while retaining an amorphous structure. The best glass-forming alloys are based on zirconium and palladium, but alloys based on iron, titanium, copper, magnesium, and other metals are also known. Many amorphous alloys are formed by exploiting a phenomenon called the \"confusion\" effect. Such alloys contain so many different elements (often four or more) that upon cooling at sufficiently fast rates, the constituent atoms simply cannot coordinate themselves into the equilibrium crystalline state before their mobility is stopped. In this way, the random disordered state of the atoms is \"locked in\".\n\nIn 1992, the commercial amorphous alloy, Vitreloy 1 (41.2% Zr, 13.8% Ti, 12.5% Cu, 10% Ni, and 22.5% Be), was developed at Caltech, as a part of Department of Energy and NASA research of new aerospace materials. More variants followed.\n\nIn 2004, bulk amorphous steel was successfully produced by two groups: one at Oak Ridge National Laboratory, who refers to their product as \"glassy steel\", and the other at the University of Virginia, calling theirs \"DARVA-Glass 101\". The product is non-magnetic at room temperature and significantly stronger than conventional steel, though a long research and development process remains before the introduction of the material into public or military use.\n\nIn 2018 a team at SLAC National Accelerator Laboratory, the National Institute of Standards and Technology (NIST) and Northwestern University reported the use of artificial intelligence to predict and evaluate samples of 20,000 different likely metallic glass alloys in a year. Their methods promise to speed up research and time to market for new amorphous metals alloys.\n\nAmorphous metal is usually an alloy rather than a pure metal. The alloys contain atoms of significantly different sizes, leading to low free volume (and therefore up to orders of magnitude higher viscosity than other metals and alloys) in molten state. The viscosity prevents the atoms moving enough to form an ordered lattice. The material structure also results in low shrinkage during cooling, and resistance to plastic deformation. The absence of grain boundaries, the weak spots of crystalline materials, leads to better resistance to wearand corrosion. Amorphous metals, while technically glasses, are also much tougher and less brittle than oxide glasses and ceramics.\n\nThermal conductivity of amorphous materials is lower than that of crystalline metal. As formation of amorphous structure relies on fast cooling, this limits the maximum achievable thickness of amorphous structures.\n\nTo achieve formation of amorphous structure even during slower cooling, the alloy has to be made of three or more components, leading to complex crystal units with higher potential energy and lower chance of formation. The atomic radius of the components has to be significantly different (over 12%), to achieve high packing density and low free volume. The combination of components should have negative heat of mixing, inhibiting crystal nucleation and prolonging the time the molten metal stays in supercooled state.\n\nThe alloys of boron, silicon, phosphorus, and other glass formers with magnetic metals (iron, cobalt, nickel) have high magnetic susceptibility, with low coercivity and high electrical resistance. Usually the conductivity of a metallic glass is of the same low order of magnitude as of a molten metal just above the melting point. The high resistance leads to low losses by eddy currents when subjected to alternating magnetic fields, a property useful for e.g. transformer magnetic cores. Their low coercivity also contributes to low loss.\n\nAmorphous metals have higher tensile yield strengths and higher elastic strain limits than polycrystalline metal alloys, but their ductilities and fatigue strengths are lower. Amorphous alloys have a variety of potentially useful properties. In particular, they tend to be stronger than crystalline alloys of similar chemical composition, and they can sustain larger reversible (\"elastic\") deformations than crystalline alloys. Amorphous metals derive their strength directly from their non-crystalline structure, which does not have any of the defects (such as dislocations) that limit the strength of crystalline alloys. One modern amorphous metal, known as Vitreloy, has a tensile strength that is almost twice that of high-grade titanium. However, metallic glasses at room temperature are not ductile and tend to fail suddenly when loaded in tension, which limits the material applicability in reliability-critical applications, as the impending failure is not evident. Therefore, there is considerable interest in producing metal matrix composites consisting of a metallic glass matrix containing dendritic particles or fibers of a ductile crystalline metal.\n\nPerhaps the most useful property of bulk amorphous alloys is that they are true glasses, which means that they soften and flow upon heating. This allows for easy processing, such as by injection molding, in much the same way as polymers. As a result, amorphous alloys have been commercialized for use in sports equipment, medical devices, and as cases for electronic equipment.\n\nThin films of amorphous metals can be deposited via high velocity oxygen fuel technique as protective coatings.\n\nCurrently the most important application is due to the special magnetic properties of some ferromagnetic metallic glasses. The low magnetization loss is used in high efficiency transformers (amorphous metal transformer) at line frequency and some higher frequency transformers. Amorphous steel is a very brittle material which makes it difficult to punch into motor laminations. Also electronic article surveillance (such as theft control passive ID tags,) often uses metallic glasses because of these magnetic properties.\n\nAmorphous metals exhibit unique softening behavior above their glass transition and this softening has been increasingly explored for thermoplastic forming of metallic glasses. Such low softening temperature allows for developing simple methods for making composites of nanoparticles (e.g. carbon nanotubes) and BMGs. It has been shown that metallic glasses can be patterned on extremely small length scales ranging from 10Â nm to several millimeters. This may solve the problems of nanoimprint lithography where expensive nano-molds made of silicon break easily. Nano-molds made from metallic glasses are easy to fabricate and more durable than silicon molds. The superior electronic, thermal and mechanical properties of BMGs compared to polymers make them a good option for developing nanocomposites for electronic application such as field electron emission devices.\n\nTiCuPdZr is believed to be noncarcinogenic, is about three times stronger than titanium, and its elastic modulus nearly matches bones. It has a high wear resistance and does not produce abrasion powder. The alloy does not undergo shrinkage on solidification. A surface structure can be generated that is biologically attachable by surface modification using laser pulses, allowing better joining with bone.\n\nMgZnCa, rapidly cooled to achieve amorphous structure, is being investigated, at Lehigh University, as a biomaterial for implantation into bones as screws, pins, or plates, to fix fractures. Unlike traditional steel or titanium, this material dissolves in organisms at a rate of roughly 1 millimeter per month and is replaced with bone tissue. This speed can be adjusted by varying the content of zinc.\n\nBulk metallic glasses (BMGs) have now been modeled using atomic scale simulations (within the density functional theory framework) in a similar manner to high entropy alloys. This has allowed predictions to be made about their behavior, stability and many more properties. As such, new BMG systems can be tested, and tailored systems; fit for a specific purpose (e.g. bone replacement or aero-engine component) without as much empirical searching of the phase space and experimental trial and error.\n\n\n"}
{"id": "1137568", "url": "https://en.wikipedia.org/wiki?curid=1137568", "title": "Artificial gravity", "text": "Artificial gravity\n\nArtificial gravity (sometimes referred to as pseudogravity) is the creation of an inertial force that mimics the effects of a gravitational force, usually by rotation. \nArtificial gravity, or rotational gravity, is thus the appearance of a centrifugal force in a rotating frame of reference (the transmission of centripetal acceleration via normal force in the non-rotating frame of reference), as opposed to the force experienced in linear acceleration, which by the equivalence principle is indistinguishable from gravity.\nIn a more general sense, \"artificial gravity\" may also refer to the effect of linear acceleration, e.g. by means of a rocket engine.. \n\nRotational simulated gravity has been used in simulations to help astronauts train for extreme conditions. \nRotational simulated gravity has been proposed as a solution in manned spaceflight to the adverse health effects caused by prolonged weightlessness. \nHowever, there are no current practical outer space applications of artificial gravity for humans due to concerns about the size and cost of a spacecraft necessary to produce a useful centripetal acceleration comparable to the gravitational field strength on Earth (g).\n\nArtificial gravity can be created using a centripetal force. A centripetal force directed towards the center of the turn is required for any object to move in a circular path. \nIn the context of a rotating space station it is the normal force provided by spacecraft's hull that acts as centripetal force. Thus, the \"gravity\" force felt by an object the centrifugal force perceived in the rotating frame of reference as pointing \"downwards\" towards the hull.\nIn accordance with Newton's Third Law the value of little \"g\" (the perceived \"downward\" acceleration) is equal in magnitude and opposite in direction\nto the centripetal acceleration.\n\nFrom the point of view of people rotating with the habitat, artificial gravity by rotation behaves in some ways similarly to normal gravity but with the following differences:\n\nThis form of artificial gravity has additional engineering issues:\n\nThe engineering challenges of creating a rotating spacecraft are comparatively modest to any other proposed approach. Theoretical spacecraft designs using artificial gravity have a great number of variants with intrinsic problems and advantages. The formula for the centripetal force implies that the radius of rotation grows with the square of the rotating spacecraft period, so a doubling of the period requires a fourfold increase in the radius of rotation. For example, to produce standard gravity, = with a rotating spacecraft period of 15Â s, the radius of rotation would have to be , while a period of 30Â s would require it to be . To reduce mass, the support along the diameter could consist of nothing but a cable connecting two sections of the spaceship. Among the possible solutions include a habitat module and a counterweight consisting of every other part of the spacecraft, alternatively two habitatable modules of similar weight could be attached to one another.\n\nWhatever design is chosen, it would be necessary for the spacecraft to possess some means to quickly transfer ballast from one section to another, otherwise even small shifts in mass could cause a substantial shift in the spacecraft's axis, which would result in a dangerous \"wobble.\" One possible solution would be to engineer the spacecraft's plumbing system to serve this purpose, using drinking water and/or waste water as the ballast. \n\nIt is not yet known whether exposure to high gravity for short periods of time is as beneficial to health as continuous exposure to normal gravity. It is also not known how effective low levels of gravity would be at countering the adverse effects on health of weightlessness. Artificial gravity at 0.1\"g\" and a rotating spacecraft period of 30Â s would require a radius of only . Likewise, at a radius of 10Â m, a period of just over 6Â s would be required to produce standard gravity (at the hips; gravity would be 11% higher at the feet), while 4.5Â s would produce 2\"g\". If brief exposure to high gravity can negate the harmful effects of weightlessness, then a small centrifuge could be used as an exercise area.\n\nThe Gemini 11 mission attempted to produce artificial gravity by rotating the capsule around the Agena Target Vehicle to which it was attached by a 36-meter tether. They were able to generate a small amount of artificial gravity, about 0.00015 g, by firing their side thrusters to slowly rotate the combined craft like a slow-motion pair of bolas.\nThe resultant force was too small to be felt by either astronaut, but objects were observed moving towards the \"floor\" of the capsule.\n\nIt should be pointed out that the Gemini 8 mission achieved artificial gravity for a few minutes. This, however, was due to an accident. The acceleration forces upon the crew were so high (~ 4g's) that the mission had to be urgently terminated.\n\nArtificial gravity has been suggested as a solution to the various health risks associated with spaceflight. In 1964, the Soviet space program believed that a human could not survive more than 14 days in space due to a fear that the heart and blood vessels would be unable to adapt to the weightless conditions. This fear was eventually discovered to be unfounded as spaceflights have now lasted up to 438 consecutive days, with missions aboard the international space station commonly lasting 6 months. However, the question of human safety in space did launch an investigation into the physical effects of prolonged exposure to weightlessness. In June 1991, a Spacelab Life Sciences 1 flight performed 18 experiments on two men and two women over a period of nine days. In an environment without gravity, it was concluded that the response of white blood cells and muscle mass decreased. Additionally, within the first 24 hours spent in a weightless environment, blood volume decreased by 10%. Upon return to earth, the effects of prolonged weightlessness continue to affect the human body as fluids pool back to the lower body, the heart rate rises, a drop in blood pressure occurs and there is a reduced ability to exercise.\n\nArtificial gravity, due to its ability to mimic the behavior of gravity on the human body has been suggested as one of the most encompassing manners of combating the physical effects inherent with weightless environments. Other measures that have been suggested as symptomatic treatments include exercise, diet and penguin suits. However, criticism of those methods lays in the fact that they do not fully eliminate the health problems and require a variety of solutions to address all issues. Artificial gravity, in contrast, would remove the weightlessness inherent with space travel. By implementing artificial gravity, space travelers would never have to experience weightlessness or the associated side effects. Especially in a modern-day six-month journey to Mars, exposure to artificial gravity is suggested in either a continuous or intermittent form to prevent extreme debilitation to the astronauts during travel.\n\nA number of proposals have incorporated artificial gravity into their design:\n\n\nSome of the reasons that artificial gravity remains unused today in spaceflight trace back to the problems inherent in implementation. One of the realistic methods of creating artificial gravity is a centripetal force pulling a person towards a relative floor. In that model, however, issues arise in the size of the spacecraft. As expressed by John Page and Matthew Francis, the smaller a spacecraft, the more rapid the rotation that is required. As such, to simulate gravity, it would be more ideal to utilize a larger spacecraft that rotates very slowly. The requirements on size in comparison to rotation are due to the different magnitude of forces the body can experience if the rotation is too tight. Additionally, questions remain as to what the best way to initially set the rotating motion in place without disturbing the stability of the whole spacecraft's orbit. At the moment, there is not a ship massive enough to meet the rotation requirements, and the costs associated with building, maintaining, and launching such a craft are extensive.\n\nIn general, with the limited health effects present in shorter spaceflights, as well as the high cost of research, application of artificial gravity is often stunted and sporadic.\n\nSeveral science fiction novels, films and series have featured artificial gravity production. In the movie , a rotating centrifuge in the \"Discovery\" spacecraft provides artificial gravity. \nIn the novel \"The Martian\", the \"Hermes\" spacecraft achieves artificial gravity by design; it employs a ringed structure, at whose periphery forces around 40% of Earth's gravity are experienced, similar to Mars's gravity. The movie \"Interstellar\" features a spacecraft called the \"Endurance\" that can rotate on its center axis to create artificial gravity, controlled by retro thrusters on the ship.\n\nHigh-G training is done by aviators and astronauts who are subject to high levels of acceleration ('G') in large-radius centrifuges. It is designed to prevent a \"g-induced loss Of consciousness\" (abbreviated G-LOC), a situation when \"g\"-forces move the blood away from the brain to the extent that consciousness is lost. Incidents of acceleration-induced loss of consciousness have caused fatal accidents in aircraft capable of sustaining high-\"g\" for considerable periods.\n\nIn amusement parks, pendulum rides and centrifuges provide rotational force. Roller coasters also do, whenever they go over dips, humps, or loops. When going over a hill, time in which zero or negative gravity is felt is called air time, or \"airtime\", which can be divided into \"floater air time\" (for zero gravity) and \"ejector air time\" (for negative gravity).\n\nLinear acceleration, even at a low level, can provide sufficient g-force to provide useful benefits. A spacecraft under constant acceleration in a straight line would give the appearance of a gravitational pull in the direction opposite of the acceleration. This \"pull\" that would cause a loose object to \"fall\" towards the hull of the spacecraft is actually a manifestation of the inertia of the objects inside the spacecraft, in accordance with Newton's first law. \nFurther, the \"gravity\" felt by an object pressed against the hull of the spacecraft is simply the reaction force of the object on the hull reacting to the acceleration force of the hull on the object, in accordance with Newton's Third Law and somewhat similar to the effect on an object pressed against the hull of a spacecraft rotating as outlined above. Unlike an artificial gravity based on rotation, linear acceleration gives the appearance of a gravity field which is both uniform throughout the spacecraft and without the disadvantage of additional fictitious forces.\n\nSome chemical reaction rockets can at least temporarily provide enough acceleration to overcome Earth's gravity and could thus provide linear acceleration to emulate Earth's g-force. However, since all such rockets provide this acceleration by expelling reaction mass such an acceleration would only be temporary, until the limited supply of rocket fuel had been spent.\n\nNevertheless, constant linear acceleration is desirable since in addition to providing artificial gravity it could theoretically provide relatively short flight times around the solar system. For example, if a propulsion technique able to support 1\"g\" of acceleration continuously were available, a spaceship accelerating (and then decelerating for the second half of the journey) at 1\"g\" would reach Mars within a few days. Similarly, a hypothetical space travel using constant acceleration of 1\"g\" for one year would reach relativistic speeds and allow for a round trip to the nearest star, Proxima Centauri.\n\nAs such, low-impulse but long-term linear acceleration has been proposed for various interplanetary missions. For example, even heavy (100 ton) cargo payloads to Mars could be transported to Mars in and retain approximately 55 percent of the LEO vehicle mass upon arrival into a Mars orbit, providing a low-gravity gradient to the spacecraft during the entire journey.\n\nA propulsion system with a very high specific impulse (that is, good efficiency in the use of reaction mass that must be carried along and used for propulsion on the journey) could accelerate more slowly producing useful levels of artificial gravity for long periods of time. A variety of electric propulsion systems provide examples. Two examples of this long-duration, low-thrust, high-impulse propulsion that have either been practically used on spacecraft or are planned in for near-term in-space use are Hall effect thrusters and Variable Specific Impulse Magnetoplasma Rockets (VASIMR). Both provide very high specific impulse but relatively low thrust, compared to the more typical chemical reaction rockets. They are thus ideally suited for long-duration firings which would provide limited amounts of, but long-term, milli-g levels of artificial gravity in spacecraft.\n\nIn a number of science fiction plots, acceleration is used to produce artificial gravity for interstellar spacecraft, propelled by as yet theoretical or hypothetical means.\n\nThis effect of linear acceleration is well understood, and is routinely used for 0\"g\" cryogenic fluid management for post-launch (subsequent) in-space firings of upper stage rockets.\n\nRoller coasters, especially launched roller coasters or those that rely on electromagnetic propulsion, can provide linear acceleration \"gravity\", and so can relatively high acceleration vehicles, such as sports cars. Linear acceleration can be used to provide air-time on roller coasters and other thrill rides.\n\nA similar effect to gravity can be created through diamagnetism. It requires magnets with extremely powerful magnetic fields. Such devices have been able to levitate at most a small mouse, producing a 1 \"g\" field to cancel that of the Earth's. \n\nSufficiently powerful magnets require either expensive cryogenics to keep them superconductive or several megawatts of power.\n\nWith such extremely strong magnetic fields, safety for use with humans is unclear. In addition, it would involve avoiding any ferromagnetic or paramagnetic materials near the strong magnetic field that is required for diamagnetism to be evident.\n\nFacilities using diamagnetism may prove workable for laboratories simulating low gravity conditions here on Earth. A mouse has been levitated against Earth's gravity, creating a condition similar to microgravity. Lower forces may also be generated to simulate a condition similar to lunar or Martian gravity with small model organisms.\n\n\"Weightless Wonder\" is the nickname for the NASA aircraft that flies parabolic trajectories and briefly provides a nearly weightless environment in which to train astronauts, conduct research, and film motion pictures. The parabolic trajectory creates a vertical linear acceleration which matches that of gravity, giving zero-g for a short time, usually 20â30 seconds, followed by approximately 1.8g for a similar period. The nickname Vomit Comet is also used to refer to motion sickness that is often experienced by the aircraft passengers during these parabolic trajectories. Such reduced gravity aircraft are nowadays operated by several organizations worldwide.\n\nA Neutral Buoyancy Laboratory (NBL) is an astronaut training facility, such as the Sonny Carter Training Facility at the NASA Johnson Space Center in Houston, Texas. The NBL is a large indoor pool of water, the largest in the world, in which astronauts may perform simulated EVA tasks in preparation for space missions. The NBL contains full-sized mock-ups of the Space Shuttle cargo bay, flight payloads, and the International Space Station (ISS).\n\nThe principle of neutral buoyancy is used to simulate the weightless environment of space. The suited astronauts are lowered into the pool using an overhead crane and their weight is adjusted by support divers so that they experience no buoyant force and no rotational moment about their center of mass. The suits worn in the NBL are down-rated from fully flight-rated EMU suits like those in use on the space shuttle and International Space Station.\n\nThe NBL tank is in length, wide, and deep, and contains 6.2 million gallons (23.5 million litres) of water. Divers breathe nitrox while working in the tank.\n\nNeutral buoyancy in a pool is not weightlessness, since the balance organs in the inner ear still sense the up-down direction of gravity. Also, there is a significant amount of drag presented by water. Generally, drag effects are minimized by doing tasks slowly in the water. Another difference between neutral buoyancy simulation in a pool and actual EVA during spaceflight is that the temperature of the pool and the lighting conditions are maintained constant.\n\nIn science fiction, artificial gravity (or cancellation of gravity) or \"paragravity\" is sometimes present in spacecraft that are neither rotating nor accelerating. At present, there is no confirmed technique that can simulate gravity other than actual mass or acceleration. There have been many claims over the years of such a device. Eugene Podkletnov, a Russian engineer, has claimed since the early 1990s to have made such a device consisting of a spinning superconductor producing a powerful \"gravitomagnetic field\", but there has been no verification or even negative results from third parties. In 2006, a research group funded by ESA claimed to have created a similar device that demonstrated positive results for the production of gravitomagnetism, although it produced only 0.0001\"g\". This result has not been replicated. String theory predicts that gravity and electromagnetism unify in hidden dimensions and that extremely short photons can enter those dimensions.\n\n"}
{"id": "4641671", "url": "https://en.wikipedia.org/wiki?curid=4641671", "title": "Artificial urinary bladder", "text": "Artificial urinary bladder\n\nThe two main methods for replacing bladder function involve either redirecting urine flow or replacing the bladder \"in situ\". Replacement can be done with an artificial urinary bladder, an artificial organ.\n\nOn January 30, 1999, scientists announced that a lab-grown bladder had been successfully transplanted into dogs. These artificial bladders worked well for almost a year in the dogs. In 2000, a new procedure for creating artificial bladders for humans was developed. This procedure is called an orthotopic neobladder procedure. This procedure involves shaping a part (usually 35 to 40Â inches) of a patient's small intestine to form a new bladder; however, these bladders made of intestinal tissues produced unpleasant side-effects.\n\nIn 2006, the first publication of experimental transplantation of bioengineered bladders appeared in \"The Lancet\". The trial involved seven people with spina bifida between the ages of four and nineteen who had been followed for up to five years after surgery to determine long-term effects. The bladders were prepared and the trial run by a team of biologists at the Wake Forest University School of Medicine and Boston Children's Hospital led by Professor Anthony Atala.\n\nBioengineered organs which rely on a patient's own cells, autologous constructs, are not subject to transplant rejection, unlike transplants from human or animal donors.\n\nThe current standard for repairing a damaged urinary bladder involves partial or complete replacement using tissue from the small intestine.\n\n"}
{"id": "2654792", "url": "https://en.wikipedia.org/wiki?curid=2654792", "title": "Berlyn Brixner", "text": "Berlyn Brixner\n\nBerlyn B. Brixner (May 21, 1911 â August 1, 2009) was an American photographer. He was the head photographer for the Trinity test, the first detonation of a nuclear weapon in July 1945. Brixner was positioned away from the explosion and had 50 cameras of varying speeds running from different locations to capture the shot in full motion.\n\nHe was born in El Paso, Texas on May 21, 1911. His mother had graduated from Western New Mexico University in 1898 and taught school in various small southern New Mexico communities. His father was a power systems engineer for a mining company, and had worked in Chile, Mexico and the Fanny Mine in Mogollon, New Mexico until the Army commandeered its boxcar-size generators at the beginning of World War II turning Mogollon into a ghost town.\n\nOn December 11, 1932, he fell into Kilbourne Hole near Lanark, New Mexico and broke his ankle.\nBrixner attended the University of Texas at Austin for four years without earning a degree, then worked and studied photography under Willis W. Waite, who operated a pathology laboratory in El Paso. In 1936, Brixner worked as a regional photographer with the Soil Conservation Service at its four-state headquarters in Albuquerque, New Mexico. He married his first wife, Betty, around 1940. His two daughters, Annette (born 1942) and Kathleen (born 1943) were born in Albuquerque. During World War II, he was hired at the Los Alamos National Laboratory to work on photography problems connected with the Manhattan Project in the Optics Engineering and High Speed Photography Group in Los Alamos under the direction of Professor Julian Mack, the group invented and constructed extremely high speed cameras.\n\nBrixner was assigned to shoot movies in 16-millimeter black-and-white film, from every angle and distance and at every available speed, of an unknown event beginning with the brightest flash ever produced on Earth. \"The theoretical people had calculated a some 10-sun brightness. So that was easy,\" said Brixner. \"All I had to do was go out and point my camera at the sun and take some pictures. Ten times that was easy to calculate.\"\n\nAt ignition, Brixner remembers \"The whole filter seemed to light up as bright as the sun. I was temporarily blinded. I looked to the side. The Oscura mountains were as bright as day. I saw this tremendous ball of fire, and it was rising. I was just spellbound! I followed it as it rose. Then it dawned on me. I'm the photographer! I've gotta get that ball of fire.\" He jogged the camera up. He said: \"There was no sound! It all took place in absolute silence.\"\n\nAfter the war, he stayed on at Los Alamos National Laboratory until retirement as head of the optical group. In 1956 he married Audrey Chew (1915â1996) who was from Washington, DC. Berlyn Brixner died peacefully in Albuquerque on August 1, 2009. He was 98.\n\nHis papers are archived at Los Alamos National Laboratory. He authored or co-authored over 45 papers describing major developments in camera engineering, optical instrumentation and fabrication techniques. His optical lens design was used to construct a high resolution telescope mounted on the Mariner 1969 and 1970 spacecraft to Mars. He received the DuPont gold medal from the Society of Motion Picture and Television Engineers and the Robert Gordon Memorial Award from the Society of Photo-Optical Instrumentation Engineers.\n\n\n\n\n"}
{"id": "7458892", "url": "https://en.wikipedia.org/wiki?curid=7458892", "title": "Biasing", "text": "Biasing\n\nBiasing in electronics means establishing predetermined voltages or currents at various points of an electronic circuit for the purpose of establishing proper operating conditions in electronic components. Many electronic devices such as diodes, transistors and vacuum tubes, whose function is processing time-varying (AC) signals also require a steady (DC) current or voltage to operate correctly â a \"bias\". The AC signal applied to them is superposed on this DC bias current or voltage. The operating point of a device, also known as bias point, quiescent point, or Q-point, is the DC voltage or current at a specified terminal of an active device (a transistor or vacuum tube) with no input signal applied. A bias circuit is a portion of the device's circuit which supplies this steady current or voltage.\n\nIn electronics, \"bias\" usually refers to a fixed DC voltage or current applied to a terminal of an electronic component such as a diode, transistor or vacuum tube in a circuit in which alternating current (AC) signals are also present, in order to establish proper operating conditions for the component. For example, a bias voltage is applied to a transistor in an electronic amplifier to allow the transistor to operate in a particular region of its transconductance curve. For vacuum tubes, a grid bias voltage is often applied to the grid electrodes for the same reason.\n\nIn electronic engineering, the term \"bias\" has the following meanings:\n\nIn magnetic tape recording, the term \"bias\" is also used for a high-frequency signal added to the audio signal and applied to the recording head, to improve the quality of the recording on the tape. This is called tape bias.\n\nLinear circuits involving transistors typically require specific DC voltages and currents for correct operation, which can be achieved using a biasing circuit. As an example of the need for careful biasing, consider a transistor amplifier. In linear amplifiers, a small input signal gives larger output signal without any change in shape (low distortion): the input signal causes the output signal to vary up and down about the Q-point in a manner strictly proportional to the input. However, because the relationship between input and output for a transistor is not linear across its full operating range, the transistor amplifier only approximates linear operation. For low distortion, the transistor must be biased so the output signal swing does not drive the transistor into a region of extremely nonlinear operation. For a bipolar junction transistor amplifier, this requirement means that the transistor must stay in the active mode, and avoid cut-off or saturation. The same requirement applies to a MOSFET amplifier, although the terminology differs a little: the MOSFET must stay in the active mode, and avoid cutoff or ohmic operation.\n\nFor bipolar junction transistors the bias point is chosen to keep the transistor operating in the active mode, using a variety of circuit techniques, establishing the Q-point DC voltage and current. A small signal is then applied on top of the Q-point bias voltage, thereby either modulating or switching the current, depending on the purpose of the circuit.\n\nThe quiescent point of operation is typically near the middle of the DC load line, so as to obtain the maximum available peak-to-peak signal amplitude without distortion due to clipping. The process of obtaining an appropriate DC collector current at a certain DC collector voltage by setting up the operating point is called biasing.\n\nAfter establishing the operating point, when an input signal is applied, the output signal should not move the transistor either to saturation or to cut-off. However, this unwanted shift still might occur, due to the following reasons:\n\n\nTo avoid a shift of Q-point, bias-stabilization is necessary. Various biasing circuits can be used for this purpose.\n\n\"Grid bias\" is the DC voltage provided at the control grid of an electron tube relative to the cathode for the purpose of establishing the zero input signal or steady state operating condition of the tube.\n\n\nMethods of obtaining grid bias:\n\nCombinations of bias methods may be used on the same tube.\n\nElectret microphone elements typically include a junction field-effect transistor as an impedance converter to drive other electronics within a few meters of the microphone. The operating current of this JFET is typically 0.1 to 0.5Â mA and is often referred to as bias, which is different from the phantom power interface which supplies 48 volts to operate the backplate of a traditional condenser microphone. Electret microphone bias is sometimes supplied on a separate conductor.\n\n\n\n"}
{"id": "27383608", "url": "https://en.wikipedia.org/wiki?curid=27383608", "title": "Blue Bloods (TV series)", "text": "Blue Bloods (TV series)\n\nBlue Bloods is an American police procedural fictional drama series that airs on CBS. The series is filmed on location in New York City with occasional references to nearby suburbs. The series debuted on September 24, 2010, with episodes airing on Fridays following \"\" before being moved to Wednesdays at 10:00Â p.m. Eastern and Pacific time and 9:00Â p.m. Central and Mountain time for a four-week tryout. After four weeks, it returned to its original Friday 10:00Â p.m. Eastern time slot, where it has remained since. On April 18, 2018, CBS renewed the series for a ninth season, which premiered on September 28, 2018.\n\nThe series follows the Reagan family of police officers with the New York City Police Department (NYPD). Francis Xavier \"Frank\" Reagan is the Police Commissioner. Frank's oldest son Danny is an NYPD detective, his youngest son Jamie is an NYPD police officer (later sergeant), and his daughter Erin is an assistant district attorney. Frank's second-oldest son Joe was murdered by a crooked cop in the line of duty in events that pre-date the series, when he was involved with the Federal Bureau of Investigation (FBI) investigating a group of corrupt NYPD cops. Frank's father Henry is a former NYPD beat cop who rose through the ranks to Police Commissioner. Each member of the family represents a different aspect of police work or the legal process: Frank as the commissioner, Danny as the detective, Jamie as the beat cop and Erin as the prosecutor. Additionally, while each person's story might occasionally interweave with another's, the show also follows the relationships with their respective partners and colleagues. Frank with Garrett Moore, the NYPD Deputy Commissioner of Public Information and \"de facto\" Chief of Staff, Detective 1st Grade Abigail Baker, the Primary Aide to the Commissioner, and later Lieutenant Sidney Gormley, the Special Assistant to the Commissioner and \"de facto\" Chief of Department; Danny with Detective Maria Baez; Jamie with Officer Edit \"Eddie\" Janko; and Erin with Detective Anthony Abetemarco in the DA's office.\n\nBoth Henry's and Frank's wives are deceased. Erin is divorced and has one college student daughter, Nicky, who lives with her. Danny is a widower with two sons named Jack and Sean; he was married to Linda, a nurse who was killed offscreen between seasons 7 and 8. Jamie is unmarried, though romantic feelings develop between him and his current partner, Eddie Janko; they acknowledge their mutual attraction during season 7 and get engaged at the end of season 8.\n\nA running plot feature of the show entails Sunday dinner around the table in Frank Reagan's Bay Ridge home at 8070 Harbor View Terrace. Four generations gather here to eat and discuss their week, and the rule is that no one can take a first bite before every last family member is present and seated, and grace is said. Discussion of the week's events then ensues, often including the adults' current work cases, and controversies therefrom are often discussed and hashed out by the entire family. The grandchildren have questions which adults answer sometimes to humor them, sometimes to give their viewpoints. At times the dialogue becomes heated and emotionally charged, but the family always remains loving and united in spite of any differences they encounter. The Sunday dinner has been a Reagan family tradition for more than a century, and is well known to their friends and neighbors.\n\nSelleck mentioned that he was drawn to the project because of the strong pilot script and that he was concerned about becoming involved in an ongoing series because he did not want to compromise his commitment to the Jesse Stone television movies. Beginning January 19, 2011, \"Blue Bloods\" was moved to Wednesdays at 10Â p.m. for a four-week tryout. In February 2011, the series returned to its Friday night time slot due to the series premiere of \"\". In the UK, the show helped launch Sky Atlantic by premiering on Tuesday, February 1, 2011, at 10:30Â p.m. In Australia, the show premiered in February 2011 in a primetime Wednesday slot on \"Network TEN\", then from February 2013 for season two was moved from the network's primary SD channel to its HD channel \"one\" alongside \"White Collar\", then on August 26, 2013, the show moved back to the primary SD channel on Mondays for season three. In New Zealand, the show premieres on July 13, 2013 on \"TV3\"\" \"replacing in a primetime Saturday slot after \"\".\n\nOn October 20, 2012, CBS announced that Jennifer Esposito was being dropped from the series, with her character, Detective Curatola, being placed on indefinite leave of absence. Esposito and CBS had been at odds over her limited availability for work after the actress was diagnosed with celiac disease. She had missed a week after collapsing on set earlier in the season. According to Esposito, CBS challenged her request for a reduced work schedule, and further, kept her from finding work elsewhere. In a press release from CBS, the network said that Esposito had \"informed us that she is only available to work on a very limited part-time schedule. As a result, she's unable to perform the demands of her role and we regretfully had to put her character on a leave of absence.\" Curatola's last appearance was in the season three episode \"Nightmares\".\n\nAccording to TV by the Numbers, by committing itself to a third season of \"Blue Bloods\", CBS all but guaranteed a fourth season, because, as a CBS Television Studios production, CBS had a financial incentive to produce at least the minimum number of episodes needed for stripped syndication. On March 27, 2013, CBS made it official by renewing \"Blue Bloods\" for a fourth season to begin in the fall of 2013.\n\n\"Blue Bloods\" was renewed for a fifth season on March 13, 2014, and has since been renewed for seasons 6, 7, 8, and 9.\n\nA house in Bay Ridge in Brooklyn is used for exterior shots of the Reagan house.\n\nThe series received $79 million in tax breaks from the state of New York for its seasons four through seven.\n\nIn Australia, the series premiered on Network Ten's primary channel on February 2, 2011. Due to poor ratings of the first season, Ten moved the series to its HD channel One, which premiered the second season on October 31, 2011. Due to decreased viewership of Ten's primary channel, season three moved back from August 26, 2013, and season four returned on March 17, 2014.\n\nIn New Zealand, the series premiered on TV3 on July 13, 2013.\n\nIn the United Kingdom and Ireland, Sky Atlantic premiered the series on February 1, 2011, with season two returning on October 4, 2011, season three on December 13, 2012, and season four on November 28, 2013. Season five premiered on January 21, 2015.\n\nReruns of \"Blue Bloods\" air nationally in the United States on Ion Television and WGN America, the latter of which airs the show daily. CBS Television Distribution currently distributes the series to local outlets on weekends, with two episodes being offered.\nRepeats of \"Blue Bloods\" air on Pick in the UK.\nThe show airs twice a day on TV2 Charlie in Denmark.\n\nReviewers have praised the series' on-location shooting. The New York \"Daily News\" praised Selleck's performance as Frank Reagan, while also praising the family dinner scene for discussion of morally complex issues.\n\nThe \"Daily News\" drew comparisons between Selleck's characters Jesse Stone and Frank Reagan, saying that both Reagan and Stone are moral, principled men of few words. In \"Entertainment Weekly\"s annual Fall TV Preview, Ken Tucker named \"Blue Bloods\" one of the \"5 Best New Shows\" of 2010.\n\nThe pilot episode garnered 15.246 million viewers because of the Live + 7 days after with viewings from DVRs. Without the DVR adding to the live viewing the show's debut episode garnered a total of 13.02 million viewers.\n\nOverall, the ratings have been steady for the series, after the pilot episode. The lowest viewer total of 8.88 million occurred in episode eight of season seven (which ran against Game 3 of the 2016 World Series), and is to date the only episode that had fewer than 9 million initial-broadcast viewers. The second-season finale on May 11, 2012, was watched by 10.73 million viewers and received a 1.2/4 ratings/share among adults ages 18â49. The highest (non-pilot) live viewership of 12.93 million occurred in episode 14 of season four (\"Manhattan Queens\"), which was the series' 81st episode.\n\nIn January 2013, CBS announced via press release that \"Blue Bloods\" was the first scripted Friday series in 10 years to average at least 13 million viewers (live + 7), gaining eight percent more viewers than the previous year. The in 2001 had 14.78 million viewers, while the in 2003 had 14.92 million.\n\n"}
{"id": "38606873", "url": "https://en.wikipedia.org/wiki?curid=38606873", "title": "Bond-out processor", "text": "Bond-out processor\n\nA bond-out processor is an emulation processor that takes the place of the microcontroller or microprocessor in the target board while an application is being developed.\n\nBond-out processors have internal signals and bus brought out to external pins. The term bond-out derives from connecting (or bonding) the emulation circuitry to these external pins. These devices are designed to be used within an in-circuit emulator and are not typically used in any other kind of system.\n\nBond-out pins were marked as no-connects in the first devices produced by Intel, and were usually not connected to anything on the ordinary production silicon. Later bond-out versions of the microprocessor were produced in a bigger package to provide more signals and functionality.\n\nBond-out processors provides capabilities far beyond those of a simple ROM monitor. A ROM monitor is a firmware program that runs instead of the application code and provides a connection to a host computer to carry out debugging functions. In general the ROM monitor uses part of the processor resources and shares the memory with the user code. \n\nBond-out processors can handle complex breakpoints (even in ROM), real-time traces of processor activity, and no use of target resources. But this extra functionality comes at a high cost, as bond-outs have to be produced for in-circuit emulators only.\n\nTherefore, sometimes solutions similar to bond-outs are implemented with an ASIC or FPGA or a faster RISC processor that imitates the core processor code execution and peripherals.\n\n"}
{"id": "45146", "url": "https://en.wikipedia.org/wiki?curid=45146", "title": "CP/M", "text": "CP/M\n\nCP/M, originally standing for Control Program/Monitor and later Control Program for Microcomputers, is a mass-market operating system created for Intel 8080/85-based microcomputers by Gary Kildall of Digital Research, Inc. Initially confined to single-tasking on 8-bit processors and no more than 64 kilobytes of memory, later versions of CP/M added multi-user variations and were migrated to 16-bit processors.\n\nThe combination of CP/M and S-100 bus computers was loosely patterned on the MITS Altair, an early standard in the microcomputer industry. This computer platform was widely used in business through the late 1970s and into the mid-1980s. CP/M increased the market size for both hardware and software by greatly reducing the amount of programming required to install an application on a new manufacturer's computer. An important driver of software innovation was the advent of (comparatively) low-cost microcomputers running CP/M, as independent programmers and hackers bought them and shared their creations in user groups. CP/M was displaced by DOS soon after the 1981 introduction of the IBM PC.\n\nA minimal 8-bit CP/M system would contain the following components:\n\nThe only hardware system that CP/M, as sold by Digital Research, would support was the Intel 8080 Development System. Manufacturers of CP/M-compatible systems customized portions of the operating system for their own combination of installed memory, disk drives, and console devices. CP/M would also run on systems based on the Zilog Z80 processor since the Z80 was compatible with 8080 code. While the Digital Research distributed core of CP/M (BDOS, CCP, core transient commands) did not use any of the Z80-specific instructions, many Z80-based systems used Z80 code in the system-specific BIOS, and many applications were dedicated to Z80-based CP/M machines.\n\nOn most machines the bootstrap was a minimal bootloader in ROM combined with some means of minimal bank switching or a means of injecting code on the bus (since the 8080 needs to see boot code at Address 0 for start-up, while CP/M needs RAM there); for others, this bootstrap had to be entered into memory using front-panel controls each time the system was started.\n\nCP/M used the 7-bit ASCII set. The other 128 characters made possible by the 8-bit byte were not standardized. For example, one Kaypro used them for Greek characters, and Osborne machines used the 8th bit set to indicate an underlined character. WordStar used the 8th bit as an end-of-word marker. International CP/M systems most commonly used the ISO 646 norm for localized character sets, replacing certain ASCII characters with localized characters rather than adding them beyond the 7-bit boundary.\n\nIn the 8-bit versions, while running, the CP/M operating system loaded into memory had three components:\nThe BIOS and BDOS were memory-resident, while the CCP was memory-resident unless overwritten by an application, in which case it was automatically reloaded after the application finished running. A number of transient commands for standard utilities were also provided. The transient commands resided in files with the extension .COM on disk.\n\nThe BIOS directly controlled hardware components other than the CPU and main memory. It contained functions such as character input and output and the reading and writing of disk sectors. The BDOS implemented the CP/M file system and some input/output abstractions (such as redirection) on top of the BIOS. The CCP took user commands and either executed them directly (internal commands such as DIR to show a directory or ERA to delete a file) or loaded and started an executable file of the given name (transient commands such as PIP.COM to copy files or STAT.COM to show various file and system information). Third-party applications for CP/M were also essentially transient commands.\n\nThe BDOS, CCP and standard transient commands were (ideally) the same in all installations of a particular revision of CP/M, but the BIOS portion was always adapted to the particular hardware. Adding memory to a computer, for example, meant that the CP/M system had to be reinstalled with an updated BIOS capable of addressing the additional memory. A utility was provided to patch the supplied BIOS, BDOS and CCP to allow them to be run from higher memory. Once installed, the operating system (BIOS, BDOS and CCP) was stored in reserved areas at the beginning of any disk which would be used to boot the system. On start-up, the bootloader (usually contained in a ROM firmware chip) would load the operating system from the disk in drive codice_1.\n\nBy modern standards CP/M was primitive, owing to the extreme constraints on program size. With version 1.0 there was no provision for detecting a changed disk. If a user changed disks without manually rereading the disk directory the system would write on the new disk using the old disk's directory information, ruining the data stored on the disk. From version 1.1 or 1.2 onwards, changing a disk then trying to write to it before its directory was read would cause a fatal error to be signalled. This avoided overwriting the disk but required a reboot and loss of the data that was to be stored on disk.\n\nThe majority of the complexity in CP/M was isolated in the BDOS, and to a lesser extent, the CCP and transient commands. This meant that by porting the limited number of simple routines in the BIOS to a particular hardware platform, the entire OS would work. This significantly reduced the development time needed to support new machines, and was one of the main reasons for CP/M's widespread use. Today this sort of abstraction is common to most OSs (a hardware abstraction layer), but at the time of CP/M's birth, OSs were typically intended to run on only one machine platform, and multilayer designs were considered unnecessary.\n\nThe Console Command Processor, or CCP, accepted input from the keyboard and conveyed results to the terminal. CP/M itself would work with either a printing terminal or a video terminal. All CP/M commands had to be typed in on the command line. The console would most often display the codice_2 prompt, to indicate the current default disk drive. When used with a video terminal, this would usually be followed by a blinking cursor supplied by the terminal. The CCP would await input from the user.\n\nA CCP internal command, of the form drive letter followed by a colon, could be used to select the default drive. For example, typing codice_3 and pressing enter at the command prompt would change the default drive to B, and the command prompt would then become codice_4 to indicate this change.\n\nCP/M's command-line interface was patterned after the operating systems from Digital Equipment, such as RT-11 for the PDP-11 and OS/8 for the PDP-8.\n\nCommands took the form of a keyword followed by a list of parameters separated by spaces or special characters. Similar to a Unix shell builtin, if an internal command was recognized, it was carried out by the CCP itself. Otherwise it would attempt to find an executable file on the currently logged disk drive and (in later versions) user area, load it, and pass it any additional parameters from the command line. These were referred to as \"transient\" programs. On completion, CP/M would reload the part of the CCP that had been overwritten by application programs â this allowed transient programs a larger memory space.\n\nThe commands themselves could sometimes be obscure. For instance, the command to duplicate files was named codice_5 (Peripheral-Interchange-Program), the name of the old DEC utility used for that purpose. The format of parameters given to a program was not standardized, so that there was no single option character that differentiated options from file names. Different programs could and did use different characters.\n\nThe Basic Disk Operating System, or BDOS, provided access to such operations as opening a file, output to the console, or printing. Application programs would load processor registers with a function code for the operation, and addresses for parameters or memory buffers, and call a fixed address in memory. Since the address was the same independent of the amount of memory in the system, application programs would run the same way for any type or configuration of hardware.\n\nThe Basic Input Output System, or BIOS, provided the lowest level functions required by the operating system.\n\nThese included reading or writing single characters to the system console and reading or writing a sector of data from the disk. The BDOS handled some of the buffering of data from the diskette, but before CP/M 3.0 it assumed a disk sector size fixed at 128 bytes, as used on single-density 8-inch floppy disks. Since most 5.25-inch disk formats used larger sectors, the blocking and deblocking and the management of a disk buffer area was handled by model-specific code in the BIOS.\n\nCustomization was required because hardware choices were not constrained by compatibility with any one popular standard. For example, some manufacturers used a separate computer terminal, while others designed a built-in integrated video display system. Serial ports for printers and modems could use different types of UART chips, and port addresses were not fixed. Some machines used memory-mapped I/O instead of the 8080 I/O address space. All of these variations in the hardware were concealed from other modules of the system by use of the BIOS, which used standard entry points for the services required to run CP/M such as character I/O or accessing a disk block. Since support for serial communication to a modem was very rudimentary in the BIOS or may have been absent altogether, it was common practice for CP/M programs that used modems to have a user-installed overlay containing all the code required to access a particular machine's serial port.\n\nFile names were specified as a string of up to eight characters, followed by a period, followed by a file name extension of up to three characters (\"8.3\" filename format). The extension usually identified the type of the file. For example, codice_6 indicated an executable program file, and codice_7 indicated a file containing ASCII text.\n\nEach disk drive was identified by a drive letter, for example drive codice_8 and drive codice_9. To refer to a file on a specific drive, the drive letter was prefixed to the file name, separated by a colon, e.g. codice_10. With no drive letter prefixed, access was to files on the current default drive.\n\nThe \"8.3\" filename format and the drive letter + colon combination survived into MS-DOS. The drive letter + colon convention has survived in Microsoft Windows and is still used today. In addition, file extensions (for example, codice_11, codice_12 (a compiled file), or codice_13 (an executable file)) are still used to identify file types on many operating systems.\n\nFile size was specified as the number of 128 byte \"records\" (directly corresponding to disk sectors on 8-inch drives) occupied by a file on the disk. There was no generally supported way of specifying byte-exact file sizes. The current size of a file was maintained in the file's File Control Block (FCB) by the operating system. Since many application programs (such as text editors) prefer to deal with files as sequences of characters rather than as sequences of records, by convention text files were terminated with a control-Z character (ASCII SUB, hexadecimal 1A). Determining the end of a text file therefore involved examining the last record of the file to locate the terminating control-Z. This also meant that inserting a control-Z character into the middle of a file usually had the effect of truncating the text contents of the file.\n\nWith the advent of larger removable and fixed disk drives, disk de-blocking formulas were employed which resulted in more disk blocks per logical file allocation block. While this allowed for larger file sizes, it also meant that the smallest file which could be allocated increased in size from 1KB (on single-density drives) to 2KB (on double-density drives) and so on, up to 32KB for a file containing only a single byte. This made for inefficient use of disk space if the disk contained a large number of small files.\n\nFile modification time stamps were not supported in releases up to CP/M 2.2, but were an optional feature in MP/M and CP/M 3.0.\n\nCP/M 2.2 had no subdirectories in the file structure, but provided 16 numbered user areas to organize files on a disk. To change user one had to simply type \"User X\" at the command prompt, X being the number of the user wanted; security was non-existent and not believed to be necessary. The user area concept was to make the single-user version of CP/M somewhat compatible with multi-user MP/M systems. A common patch for the CP/M and derivative operating systems was to make one user area accessible to the user independent of the currently set user area. A USER command allowed the user area to be changed to any area from 0 to 15. User 0 was the default. If one changed to another user, such as USER 1, the material saved on the disk for this user would only be available to USER 1; USER 2 would not be able to see it or access it. However, files stored in the USER 0 area were accessible to all other users; their location was specified with a prefatory path, since the files of USER 0 were only visible to someone logged in as USER 0. The user area feature arguably had little utility on small floppy disks, but it was useful for organizing files on machines with hard drives. The intent of the feature was to ease use of the same computer for different tasks. For example, a secretary could do data entry, then, after switching USER areas, another employee could use the machine to do billing without their files intermixing.\n\nThe read/write memory between address 0100 hexadecimal and the lowest address of the BDOS was the \"Transient Program Area\" (TPA) available for CP/M application programs. Although all Z80 and 8080 processors could address 64 kilobytes of memory, the amount available for application programs could vary, depending on the design of the particular computer. Some computers used large parts of the address space for such things as BIOS ROMs, or video display memory. As a result, some systems had more TPA memory available than others. Bank switching was a common technique that allowed systems to have a large TPA while switching out ROM or video memory space as needed. CP/M 3.0 allowed parts of the BDOS to be in bank-switched memory as well.\n\nCP/M came with a Dynamic Debugging Tool, nicknamed DDT (after the insecticide, i.e. a bug-killer), which allowed memory and program modules to be examined and manipulated, and allowed a program to be executed one step at a time.\n\nCP/M originally did not support the equivalent of terminate and stay resident program as under DOS. Programmers could write software that could intercept certain operating system calls and extend or alter their functionality. Using this capability, programmers developed and sold auxiliary desk accessory programs, such as SmartKey, a keyboard utility to assign any string of bytes to any key.\nCP/MÂ 3, however, added support for dynamically loadable Resident System Extensions (RSX), and similar solutions (like RSM) were also retrofitted to CP/M 2.2 systems by third-parties.\n\nAlthough CP/M provided some hardware abstraction to standardize the interface to disk I/O or console I/O, typically application programs still required installation to make use of all the features of such equipment as printers and terminals. Often these were controlled by escape sequences which had to be altered for different devices. For example, the escape sequence to select bold face on a printer would have differed among manufacturers, and sometimes among models within a manufacturer's range. This procedure was not defined by the operating system; a user would typically run an installation program that would either allow selection from a range of devices, or else allow feature-by-feature editing of the escape sequences required to access a function. This had to be repeated for each application program, since there was no central operating system service provided for these devices.\n\nThe initializing codes for each model of printer had to be written into the application. To use a program such as Wordstar with more than one printer (say, a fast dot matrix printer or a slower but presentation-quality daisy wheel printer), a separate version of Wordstar had to be prepared, and one had to load the Wordstar version that corresponded to the printer selected (and exiting and reloading to change printers).\n\nGary Kildall originally developed CP/M during 1974, as an operating system to run on an Intel Intellec-8 development system, equipped with a Shugart Associates 8-inch floppy disk drive interfaced via a custom floppy disk controller. It was written in Kildall's own PL/M (\"Programming Language for Microcomputers\"). Various aspects of CP/M were influenced by the TOPS-10 operating system of the DECsystem-10 mainframe computer, which Kildall had used as a development environment.\n\nCP/M originally stood for \"Control Program/Monitor\", a name which implies a resident monitorâa primitive precursor to the operating system. However, during the conversion of CP/M to a commercial product, trademark registration documents filed in November 1977 gave the product's name as \"Control Program for Microcomputers\". The CP/M name follows a prevailing naming scheme of the time, as in Kildall's PL/M language, and Prime Computer's PL/P (\"Programming Language for Prime\"), both suggesting IBM's PL/I; and IBM's CP/CMS operating system, which Kildall had used when working at the Naval Postgraduate School.\n\nThis renaming of CP/M was part of a larger effort by Kildall and his wife/business partner to convert Kildall's personal project of CP/M and the Intel-contracted PL/M compiler into a commercial enterprise. The Kildalls intended to establish the Digital Research brand and its product lines as synonymous with \"microcomputer\" in the consumer's mind, similar to what IBM and Microsoft together later successfully accomplished in making \"personal computer\" synonymous with their product offerings. Intergalactic Digital Research, Inc. was later renamed via a corporation change-of-name filing to Digital Research, Inc.\n\nCompanies chose to support CP/M because of its large library of software. The Xerox 820 ran the operating system because \"where there are literally thousands of programs written for it, it would be unwise not to take advantage of it\", Xerox said. By 1984 Columbia University used the same source code to build Kermit binaries for more than a dozen different CP/M systems, plus a generic version. The operating system was described as a \"software bus\", allowing multiple programs to interact with different hardware in a standardized way. Programs written for CP/M were typically portable among different machines, usually requiring only the specification of the escape sequences for control of the screen and printer. This portability made CP/M popular, and much more software was written for CP/M than for operating systems that ran on only one brand of hardware. One restriction on portability was that certain programs used the extended instruction set of the Z80 processor and would not operate on an 8080 or 8085 processor. Another was graphics routines, especially in games and graphics programs, which were generally machine-specific as they used direct hardware access for speed, bypassing the OS and BIOS (this was also a common problem in early DOS machines).\n\nBill Gates claimed that the Apple II family with a Z-80 SoftCard was the single most-popular CP/M hardware platform. Many different brands of machines ran the operating system, some notable examples being the Altair 8800, the IMSAI 8080, the Osborne 1 and Kaypro luggables, and MSX computers. The best-selling CP/M-capable system of all time was probably the Amstrad PCW. In the UK, CP/M was also available on Research Machines educational computers (with the CP/M source code published as an educational resource), and for the BBC Micro when equipped with a Z80 co-processor. Furthermore, it was available for the Amstrad CPC series, the Commodore 128, TRS-80, and later models of the ZX Spectrum. CP/M 3 was also used on the NIAT, a custom handheld computer designed for A.C. Nielsen's internal use with 1Â Mbytes of SSD memory.\n\nWordStar, one of the first widely used word processors, and dBase, an early and popular database program for microcomputers, were originally written for CP/M. Two early outliners, KAMAS (Knowledge and Mind Amplification System) and its cut-down successor Out-Think (without programming facilities and retooled for 8080/V20 compatibility) were also written for CP/M, though later rewritten for MS-DOS. Turbo Pascal, the ancestor of Borland Delphi, and Multiplan, the ancestor of Microsoft Excel, also debuted on CP/M before MS-DOS versions became available. Visicalc, the first-ever spreadsheet program, was made available for CP/M. Another company, Sorcim, created its SuperCalc spreadsheet for CP/M, which would go on to become the market leader and de facto standard on CP/M. Supercalc would go on to be a competitor in the spreadsheet market in the MS-DOS world. AutoCAD, a CAD application from Autodesk debuted on CP/M. A host of compilers and interpreters for popular programming languages of the time (such as BASIC, Borland's Turbo Pascal and FORTRAN) were available, among them several of the earliest Microsoft products.\n\nCP/M software often came with installers that adapted it to a wide variety of computers. Most forms of copy protection were ineffective on the operating system, and the source code for BASIC programs was easily accessible. The lack of standardized graphics support limited video games, but various character and text-based games were ported, such as \"Telengard\", \"Gorillas (video game)\", \"Hamurabi\", \"Lunar Lander\", along with early interactive fiction including the Zork series and \"Colossal Cave Adventure\". A text adventure specialist, Infocom was one of the few publishers to consistently release their games in CP/M format. Lifeboat Associates started collecting and distributing user-written \"free\" software. One of the first was XMODEM, which allowed reliable file transfers via modem and phone line. The last significant program native to CP/M was the outline processor KAMAS.\n\nNo standard 5 1/4-inch CP/M disk format existed. While the 8-inch single-density floppy disk format (so-called \"distribution format\") was standardized, various 5 1/4-inch formats were used depending on the characteristics of particular systems and to some degree the choices of the designers. CP/M supported options to control the size of reserved and directory areas on the disk, and the mapping between logical disk sectors (as seen by CP/M programs) and physical sectors as allocated on the disk. There were many ways to customize these parameters for every system but once they had been set, no standardized way existed for a system to load parameters from a disk formatted on another system.\n\nJRT Pascal, for example, provided versions on -inch disk for North Star, Osborne, Apple, Heath hard sector and soft sector, and Superbrain, and a single 8-inch version. Certain disk formats were more popular than others. The Xerox 820's became widely supported, for example; much software was available on it, and other computers such as the Kaypro II were compatible with the format. No single manufacturer, however, prevailed in the -inch era of CP/M use, and disk formats were often not portable between hardware manufacturers. A software manufacturer had to prepare a separate version of the program for each brand of hardware on which it was to run. With some manufacturers (Kaypro is an example), there was not even standardization across the company's different models. Because of this situation, disk format translation programs, which allowed a machine to read many different formats, became popular and reduced the confusion, as did programs like Kermit which allowed transfer of data and programs from one machine to another using the serial ports that most CP/M machines had.\n\nThe degree of portability between different CP/M machines depended on the type of disk drive and controller used since many different floppy types existed in the CP/M era in both 8\" and 5.25\" format. Disks could be hard or soft sectored, single or double density, single or double sided, 35 track, 40 track, 77 track, or 80 track, and the sector layout and size could vary widely as well. Although translation programs could allow the user to read disk types from different machines, it also depended on the drive type and controller. By 1982, soft sector, single sided, 40 track 5.25\" disks had become the most popular format to distribute CP/M software on as they were used by the most common consumer-level machines of that time such as the Apple II, TRS-80, Osborne 1, Kaypro II, and IBM PC. A translation program allowed the user to read any disks on his machine that had a similar format--for example, the Kaypro II could read TRS-80, Osborne, IBM PC, and Epson disks. Other disk types such as 80 track or hard sectored were completely impossible to read. The first half of double sided disks (like the Epson QX-10's) could be read because CP/M accessed disk tracks sequentially with track 0 being the first (outermost) track of side 1 and track 79 (on a 40 track disk) being the last (innermost) track of side 2. Apple II users could not use anything but Apple's GCR format and so had to obtain CP/M software on Apple format disks or else transfer it via serial link.\n\nThe fragmented CP/M market, requiring distributors either to stock multiple formats of disks or to invest in multiformat duplication equipment, compared with the more standardized IBM PC disk formats, was a contributing factor to the rapid obsolescence of CP/M after 1981.\n\nOne of the last notable CP/M capable machines to appear was the Commodore 128 in 1985, which had a Z80 for CP/M support in addition to its native mode using a 6502-derivative CPU. Using CP/M required either a 1571 or 1581 disk drive which could read soft sector 40 track MFM format disks.\n\nThe C128 and MSX were the only machines that supported the use of CP/M with 3.5\" floppy disks.\n\nAlthough graphics-capable S-100 systems existed from the commercialization of the S-100 bus, CP/M did not provide any standardized graphics support until 1982 with GSX (Graphics System Extension). Owing to the small memory available, graphics was never a common feature associated with 8-bit CP/M operating systems. Most systems could only display rudimentary ASCII art charts and diagrams in text mode or by using a custom character set.\n\nIn 1979, a multi-user compatible derivative of CP/M was released. MP/M allowed multiple users to connect to a single computer, using multiple terminals to provide each user with a screen and keyboard. Later versions ran on 16-bit processors.\n\nThe last 8-bit version of CP/M was version 3, often called CP/M Plus, released in 1983. It incorporated the bank switching memory management of MP/M in a single-user single-task operating system compatible with CP/M 2.2 applications. CP/M 3 could therefore use more than 64 kB of memory on an 8080 or Z80 processor. The system could be configured to support date stamping of files. The operating system distribution software also included a relocating assembler and linker. CP/M 3 was available for the last generation of 8-bit computers, notably the Amstrad PCW, the ZX Spectrum +3, the Commodore 128, and the Radio Shack TRS-80 Model 4.\n\nThere were versions of CP/M for some 16-bit CPUs as well.\n\nThe first version in the 16-bit family was CP/M-86 for the Intel 8086, which was soon followed by CP/M-68K for the Motorola 68000. The original version of CP/M-68K in 1982 was written in Pascal/MT+68k, but it was ported to C later on.\nAt this point the original 8-bit CP/M became known by the retronym CP/M-80 to avoid confusion. In ca. 1982/1983 there was also a port to the Zilog Z8000 for the Olivetti M20, written in C, named CP/M-8000.\n\nCP/M-86 was expected to be the standard operating system of the new IBM PCs, but DRI and IBM were unable to negotiate development and licensing terms. IBM turned to Microsoft instead, and Microsoft delivered PC DOS based on a CP/M clone, 86-DOS. Although CP/M-86 became an option for the IBM PC after DRI threatened legal action, it never overtook Microsoft's system.\n\nWhen Digital Equipment Corporation (DEC) put out the Rainbow 100 to compete with IBM, it came with CP/M-80 using a Z80 chip, CP/M-86 or MS-DOS using an 8088 microprocessor, or CP/M-86/80 using both. The Z80 and 8088 CPUs ran concurrently. A benefit of the Rainbow was that it could continue to run 8-bit CP/M software, preserving a user's possibly sizable investment as they moved into the 16-bit world of MS-DOS.\n\nCP/M-68K, already running on the Motorola EXORmacs systems, was initially to be used in the Atari ST computer, but Atari decided to go with a newer disk operating system called GEMDOS. CP/M-68K was also used on the SORD M68 and M68MX computers.\n\nThese 16-bit versions of CP/M required application programs to be re-compiled for the new CPUs or if they were written in assembly language, to be translated using tools like Digital Research's XLT86, a program written by Gary Kildall in 1981, which translated .ASM source code for the Intel 8080 processor into .A86 source code for the Intel 8086. Using global data flow analysis on 8080 register usage, the translator would also optimize the output for code size and take care of calling conventions, so that CP/M-80 and MP/M-80 programs could be ported to the CP/M-86 and MP/M-86 platforms automatically. XLAT86 itself was written in PL/I-80 and was available for CP/M-80 platforms as well as for DEC VMS (for VAX 11/750 or 11/780).\n\nMany expected that CP/M would be the standard operating system for 16-bit computers. In 1980 IBM approached Digital Research, at Bill Gates' suggestion, to license a forthcoming version of CP/M for its new product, the IBM Personal Computer. Upon the failure to obtain a signed non-disclosure agreement, the talks failed, and IBM instead contracted with Microsoft to provide an operating system. The resulting product, MS-DOS, soon began outselling CP/M.\n\nMany of the basic concepts and mechanisms of early versions of MS-DOS resembled those of CP/M. Internals like file-handling data structures were identical, and both referred to disk drives with a letter (codice_1, codice_3, etc.). MS-DOS's main innovation was its FAT file system. This similarity made it easier to port popular CP/M software like WordStar and dBase. However, CP/M's concept of separate user areas for files on the same disk was never ported to MS-DOS. Since MS-DOS had access to more memory (as few IBM PCs were sold with less than 64 KB of memory, while CP/M could run in 16 KB if necessary), more commands were built into the command-line shell, making MS-DOS somewhat faster and easier to use on floppy-based computers.\n\nAlthough one of the first peripherals for the IBM PC was a SoftCard-like expansion card that let it run 8-bit CP/M software, CP/M rapidly lost market share as the microcomputing market moved to the IBM-compatible platform, and it never regained its former popularity. \"Byte\" magazine, at the time one of the leading industry magazines for microcomputers, essentially ceased covering CP/M products within a few years of the introduction of the IBM PC. For example, in 1983 there were still a few advertisements for S-100 boards and articles on CP/M software, but by 1987 these were no longer found in the magazine. A 1984 article in \"InfoWorld\" stated that efforts to introduce CP/M to the home market had been largely unsuccessful and most CP/M software was too expensive for home users, and in 1986 stated that Kaypro had stopped production of their 8-bit CP/M-based models to concentrate on sales of MS-DOS compatible systems, long after most other vendors had ceased production of new equipment and software for CP/M.\n\nLater versions of CP/M-86 made significant strides in performance and usability and were made compatible with MS-DOS. To reflect this compatibility the name was changed, and CP/M-86 became DOS Plus, which in turn became DR-DOS.\n\nZCPR (the Z80 Command Processor Replacement) was introduced on February 2, 1982 as a drop-in replacement for the standard Digital Research console command processor (CCP) and was initially written by a group of computer hobbyists who called themselves \"The CCP Group\". They were Frank Wancho, Keith Petersen (the archivist behind Simtel at the time), Ron Fowler, Charlie Strom, Bob Mathias, and Richard Conn. Richard was, in fact, the driving force in this group (all of whom maintained contact through email).\n\nZCPR1 was released on a disk put out by SIG/M (Special Interest Group/Microcomputers), a part of the Amateur Computer Club of New Jersey.\n\nZCPR2 was released on February 14, 1983. It was released as a set of ten disks from SIG/M. ZCPR2 was upgraded to 2.3, and also was released in 8080 code, permitting the use of ZCPR2 on 8080 and 8085 systems.\n\nZCPR3 was released on Bastille Day, July 14, 1984, as a set of nine disks from SIG/M. The code for ZCPR3 could also be compiled (with reduced features) for the 8080 and would run on systems that did not have the requisite Z80 microprocessor.\n\nIn January 1987, Richard Conn stopped developing ZCPR, and Echelon asked Jay Sage (who already had a privately enhanced ZCPR 3.1) to continue work on it. Thus, ZCPR 3.3 was developed and released. ZCPR 3.3 no longer supported the 8080 series of microprocessors, and added the most features of any upgrade in the ZCPR line.\n\nFeatures of ZCPR as of version 3 included:\n\n\nZCPR3.3 also included a full complement of utilities with considerably extended capabilities. While enthusiastically supported by the CP/M user base of the time, ZCPR alone was insufficient to slow the demise of CP/M.\n\nA number of behaviors exhibited by Microsoft Windows are a result of backward compatibility with MS-DOS, which in turn attempted some backward compatibility with CP/M. The drive letter and 8.3 filename conventions in MS-DOS (and early Windows versions) were originally adopted from CP/M. The wildcard matching characters used by Windows (? and *) are based on those of CP/M, as are the reserved filenames used to redirect output to a printer (\"PRN:\"), and the console (\"CON:\"). The drive names A and B were used to designate the two floppy disk drives that CP/M systems typically used; when hard drives appeared they were designated C, which survived into MS-DOS as the codice_16 command prompt. Also, the control character ^Z marking the end of some text files can also be attributed to CP/M.\n\nIn 1997 and 1998 Caldera released some CP/M 2.2 binaries and source code under an open source license, also allowing the redistribution and modification of further collected Digital Research files related to the CP/M and MP/M families through Tim Olmstead's \"The Unofficial CP/M Web site\" since 1997. After Olmstead's death on 2001-09-12, the distribution license was refreshed and expanded by Lineo, who had meanwhile become the owner of those Digital Research assets, on 2001-10-19.\nIn October 2014, to mark the 40th anniversary of the first presentation of CP/M, the Computer History Museum released early source code versions of CP/M.\n\n\n\n"}
{"id": "6101721", "url": "https://en.wikipedia.org/wiki?curid=6101721", "title": "Clamper (electronics)", "text": "Clamper (electronics)\n\nA clamper is an electronic circuit that fixes either the positive or the negative peak excursions of a signal to a defined value by shifting its DC value. The clamper does not restrict the peak-to-peak excursion of the signal, it moves the whole signal up or down so as to place the peaks at the reference level. A diode clamp (a simple, common type) consists of a diode, which conducts electric current in only one direction and prevents the signal exceeding the reference value; and a capacitor, which provides a DC offset from the stored charge. The capacitor forms a time constant with the resistor load, which determines the range of frequencies over which the clamper will be effective.\n\nA clamping circuit (also known as a clamper) will bind the upper or lower extreme of a waveform to a fixed DC voltage level. These circuits are also known as DC voltage restorers. Clampers can be constructed in both positive and negative polarities. When unbiased, clamping circuits will fix the voltage lower limit (or upper limit, in the case of negative clampers) to 0 volts. These circuits clamp a peak of a waveform to a specific DC level compared with a capacitively-coupled signal, which swings about its average DC level.\n\nThe clamping network is one that will \"clamp\" a signal to a different dc level. The network must have a capacitor, a diode, and a resistive element, but it can also employ an independent dc supply to introduce an additional shift. The magnitude of R and C must be chosen such that the time constant RC is large enough to ensure that the voltage across the capacitor does not discharge significantly during the interval the diode is nonconducting.\n\nClamp circuits are categorised by their operation; negative or positive, and biased or unbiased. A positive clamp circuit(negative peak clamper) outputs a purely positive waveform from an input signal; it offsets the input signal so that all of the waveform is greater than 0Â V. A negative clamp is the opposite of thisâthis clamp outputs a purely negative waveform from an input signal. A bias voltage between the diode and ground offsets the output voltage by that amount.\n\nFor example, an input signal of peak value 5Â V (V = 5Â V) is applied to a positive clamp with a bias of 3Â V (V = 3Â V), the peak output voltage will be:\n\nNote that the peak to peak excursion remains at 2V\n\nIn the negative cycle of the input AC signal, the diode is forward biased and conducts, charging the capacitor to the peak negative value of V. During the positive cycle, the diode is reverse biased and thus does not conduct. The output voltage is therefore equal to the voltage stored in the capacitor plus the input voltage, so V = V + V. This is also called a Villard circuit.\n\nA negative unbiased clamp is the opposite of the equivalent positive clamp. In the positive cycle of the input AC signal, the diode is forward biased and conducts, charging the capacitor to the peak positive value of V. During the negative cycle, the diode is reverse biased and thus does not conduct. The output voltage is therefore equal to the voltage stored in the capacitor plus the input voltage again, so V = V â V.\n\nA positive biased voltage clamp is identical to an equivalent unbiased clamp but with the output voltage offset by the bias amount V. Thus, V = V + (V + V).\n\nA negative biased voltage clamp is likewise identical to an equivalent unbiased clamp but with the output voltage offset in the negative direction by the bias amount V. Thus, V = V â (V + V).\n\nThe figure shows an op-amp clamp circuit with a non-zero reference clamping voltage. The advantage here is that the clamping level is at precisely the reference voltage. There is no need to take into account the forward volt drop of the diode (which is necessary in the preceding simple circuits as this adds to the reference voltage). The effect of the diode volt drop on the circuit output will be divided down by the gain of the amplifier, resulting in an insignificant error. The circuit also has a great improvement in linearity at small input signals in comparison to the simple diode circuit and is largely unaffected by changes in the load.\n\nClamping can be used to adapt an input signal to a device that cannot make use of or may be damaged by the signal range of the original input.\n\nDuring the first negative phase of the AC input voltage, the capacitor in a positive clamper circuit charges rapidly. As \"V\" becomes positive, the capacitor serves as a voltage doubler; since it has stored the equivalent of \"V\" during the negative cycle, it provides nearly that voltage during the positive cycle. This essentially doubles the voltage seen by the load. As \"V\" becomes negative, the capacitor acts as a battery of the same voltage of \"V\". The voltage source and the capacitor counteract each other, resulting in a net voltage of zero as seen by the load.\n\nFor passive type clampers with a capacitor, followed by a diode in parallel with the load, the load can significantly affect perfomance. The magnitude of \"R\" and \"C\" are chosen so that the time constant, formula_1, is large enough to ensure that the voltage across the capacitor does not discharge significantly during the diode's non-conducting interval. A load resistance that is too low (heavy load) will partially discharge the capacitor and cause the waveform peaks to drift off the intended clamp voltage. This effect is greatest at low frequecies. At a higher frequency, there is less time between cycles for the capacitor to discharge.\n\nThe capacitor cannot be made arbitrarily large to overcome load discharge. During the conducting interval, the capacitor must be recharged. The time taken to do this is governed by a different time constant, this time set by the capacitance and the internal impedance of the driving circuit. Since the peak voltage is reached in one quarter cycle and then starts to fall again, the capacitor must be recharged in a quarter cycle. This requirement calls for a low value of capacitance.\n\nThe two conflicting requirements for capacitance value may be irreconcilable in applications with a high driving impedance and low load impedance. In such cases, an active circuit must be used such as the op-amp circuit described above.\n\nBy using a voltage source and resistor, the clamper can be biased to bind the output voltage to a different value. The voltage supplied to the potentiometer will be equal to the offset from zero (assuming an ideal diode) in the case of either a positive or negative clamper (the clamper type will determine the direction of the offset). If a negative voltage is supplied to either positive or negative, the waveform will cross the x-axis and be bound to a value of this magnitude on the opposite side. Zener diodes can also be used in place of a voltage source and potentiometer, hence setting the offset at the Zener voltage.\n\nClamping circuits were common in analog television receivers. These sets have a DC restorer circuit, which returns the voltage of the video signal during the 'back porch' of the line blanking (retrace) period to 0Â V. Low frequency interference, especially power line hum, induced onto the signal spoils the rendering of the image, and in extreme cases causes the set to lose synchronization. This interference can be effectively removed via this method.\n\n\n"}
{"id": "42579502", "url": "https://en.wikipedia.org/wiki?curid=42579502", "title": "Clover Network", "text": "Clover Network\n\nClover is a cloud-based Android point of sale (POS) platform that was launched in April 2012. The company is headquartered in Sunnyvale, CA. Clover was acquired on December 28, 2012 by First Data Corporation. Bank of America Merchant Services was the first to announce it would sell Clover to its merchant base in October 2013. PNC Merchant Services was the second to announce it would sell Clover to its merchant base.\n\nClover was incorporated in October 15, 2010 and raised $5.5M on November 1, 2010. The lead investor was Sutter Hill Ventures with participation from Andreessen Horowitz and a number of angel investors. The company was founded by John Beatty, Leonard Speiser, and Kelvin Zheng. The company secured an additional $3M convertible note from Sutter Hill Ventures in April 2012.\n\nClover launched its first hardware solution, the Clover Station, in 2013 and began shipping it in January 2014. Clover opened its App Marketplace to third party developers in 2014. Gyft announced the launch of its Gift Cloud Service on Clover in April 2014.\n\nFirst Data announced the sale of 17,000 Clover Stations six months after the release of the product. This put Clover ahead of Square Stand in terms of total units shipped.\n\nOn September 9, 2014 Clover announced its work with Apple to support Apple Pay via its Android POS Platform. Clover described the functionality in a blog post that was featured on the home page of Hacker News.\n\nClover announced its second hardware product, the Clover Mobile, at the Money 20/20 conference in November 2014. On March 19, 2015, Clover Mobile won the Gold medal for the Best POS innovation at the PYMNTS.com Innovator Awards ceremony. Clover Mobile features a complete EMVCo compliant payments solution with an on-screen pin-pad, that is fully PCI PTS 4.0 compliant. It also features an EMVCo compliant Contactless payment interface which enables it to support Apple Pay among other contactless payment modes. The device has an ergonomic handle with an integrated barcode scanner - with an external wireless mobile printer accessory designed to keep the device lightweight and reduce fatigue.\n\nClover's third hardware product, the Clover Mini was launched by First Data on June 16, 2015. Like Clover Mobile, Clover Mini is a PCI PTS 4.0 approved, EMVCo compliant contact and contactless next generation payment device. Clover Mini has a built-in receipt printer and is designed to be a terminal replacement with the support for advanced payments interfaces, while also allowing Merchants to install apps from the Clover App Market. Clover Mini also features an USB hub allowing easy interface of peripherals such as external barcode scanners, cash drawer and a Merchant keypad (for cases where the Mini is Customer-facing).\n\nOn December 28, 2012, Clover entered into a merger agreement with First Data Corporation. In a radio interview with Wharton professor Rahul Kapoor the structure of the acquisition was explained. An operating agreement was put into place whereby Clover would receive $100M in funding to run independently. First Data would be responsible for sales and support, while Clover built all new payment hardware/software for the company. The deal utilized a \"Founder's Clause\" that would trigger a very large penalty payment if the parent company interfered with operations of Clover. Thirty two months after the acquisition Clover was mentioned 88 times in the S-1 filed by First Data Corporation.\n"}
{"id": "40957", "url": "https://en.wikipedia.org/wiki?curid=40957", "title": "Control communications", "text": "Control communications\n\nIn telecommunication, control communications is the branch of technology devoted to the design, development, and application of communications facilities used specifically for control purposes, such as for controlling (a) industrial processes, (b) movement of resources, (c) electric power generation, distribution, and utilization, (d) communications networks, and (e) transportation systems.\n"}
{"id": "2989336", "url": "https://en.wikipedia.org/wiki?curid=2989336", "title": "DBc", "text": "DBc\n\ndBc (decibels relative to the carrier) is the power ratio of a signal to a carrier signal, expressed in decibels. For example, phase noise is expressed in dBc/Hz at a given frequency offset from the carrier. dBc can also be used as a measurement of Spurious-Free Dynamic Range (SFDR) between the desired signal and unwanted spurious outputs resulting from the use of signal converters such as a digital-to-analog converter or a frequency mixer. \n\nIf the dBc figure is positive, then the relative signal strength is greater than the carrier signal strength. If the dBc figure is negative, then the relative signal strength is less than carrier signal strength.\n\nAlthough the decibel (dB) is permitted for use alongside SI units, the dBc is not.\n\n"}
{"id": "7578771", "url": "https://en.wikipedia.org/wiki?curid=7578771", "title": "Engineering psychology", "text": "Engineering psychology\n\nEngineering psychology, also known as Human Factors Engineering, is the science of human behavior and capability, applied to the design and operation of systems and technology.\nAs an applied field of psychology and an interdisciplinary part of ergonomics, it aims to improve the relationships between people and machines by redesigning equipment, interactions, or the environment in which they take place. The work of an engineering psychologist is often described as making the relationship more \"user-friendly.\"\n\nEngineering psychology was created from within experimental psychology. Engineering psychology started during World War I (1914). The reason why this subject was developed during this time was because many of Americaâs weapons were failing; bombs not falling in the right place to weapons attacking normal marine life. The fault was traced back to human errors. One of the first designs to be built to restrain human error was the use of psychoacoustics by S.S. Stevens and L.L. Beranek were two of the first American psychologists called upon to help change how people and machinery worked together. One of their first assignments was to try and reduce noise levels in military aircraft. The work was directed at improving intelligibility of military communication systems and appeared to have been very successful. However it was not until after August 1945 that levels of research in engineering psychology began to increase significantly. This occurred because the research that started in 1940 now began to show.\n\nLillian Gilbreth combined the talents of an engineer, psychologist and mother of twelve. Her appreciation of human factors made her successful in the implementation of time and motion studies and scientific management. She went on to pioneer ergonomics in the kitchen, inventing the pedal bin, for example.\n\nIn Britain, the two world wars generated much formal study of human factors which affected the efficiency of munitions output and warfare. In World War I, the Health of Munitions Workers Committee was created in 1915. This made recommendations based upon studies of the effects of overwork on efficiency which resulted in policies of providing breaks and limiting hours of work, including avoidance of work on Sunday. The Industrial Fatigue Research Board was created in 1918 to take this work forward. In WW2, researchers at Cambridge University such as Frederic Bartlett and Kenneth Craik started work on the operation of equipment in 1939 and this resulted in the creation of the Unit for Research in Applied Psychology in 1944.\n\n\nAlthough the comparability of these terms and many others have been a topic of debate, the differences of these fields can be seen in the applications of the respective fields.\n\nEngineering psychology is concerned with the adaptation of the equipment and environment to people, based upon their psychological capacities and limitations with the objective of improving overall system performance, involving human and machine elements Engineering psychologists strive to match equipment requirements with the capabilities of human operators by changing the design of the equipment. An example of this matching was the redesign of the mailbags used by letter carriers. Engineering psychologists discovered that mailbag with a waist-support strap, and a double bag that requires the use of both shoulders, reduces muscle fatigue. Another example involves the cumulative trauma disorders grocery checkout workers suffered as the result of repetitive wrist movements using electronic scanners. Engineering psychologists found that the optimal checkout station design would allow for workers to easily use either hand to distribute the workload between both wrists.\n\nThe field of ergonomics is based on scientific studies of ordinary people in work situations and is applied to the design of processes and machines, to the layout of work places, to methods of work, and to the control of the physical environment, in order to achieve greater efficiency of both men and machines An example of an ergonomics study is the evaluation of the effects of screwdriver handle shape, surface material and workpiece orientation on torque performance, finger force distribution and muscle activity in a maximum screwdriving torque task. Another example of an ergonomics study is the effects of shoe traction and obstacle height on friction. Similarly, many topics in ergonomics deal with the actual science of matching man to equipment and encompasses narrower fields such as Engineering Psychology.\n\nAt one point in time, the term human factors was used in place of ergonomics in Europe. Human factors involve interdisciplinary scientific research and studies to seek to realize greater recognition and understanding of the worker's characteristics, needs, abilities, and limitations when the procedures and products of technology are being designed. This field utilizes knowledge from several fields such as mechanical engineering, psychology, and industrial engineering to design instruments.\n\nHuman factors is broader than engineering psychology, which is focused specifically on designing systems that accommodate the information-processing capabilities of the brain.\n\nAlthough the work in the respective fields differ, there are some similarities between these. These fields share the same objectives which are to optimize the effectiveness and efficiency with which human activities are conducted as well as to improve the general quality of life through increased safety, reduced fatigue and stress, increased comfort, and satisfaction.\n\nEngineering psychologists contribute to the design of a variety of products, including dental and surgical tools, cameras, toothbrushes and car-seats. They have been involved in the re-design of the mailbags used by letter carriers. More than 20% of letter carriers suffer from musculoskeletal injury such as lower back pain from carrying mailbags slung over their shoulders. A mailbag with a waist-support strap, and a double bag that requires the use of both shoulders, has been shown to reduce muscle fatigue.\n\nResearch by engineering psychologists has demonstrated that using cell-phones while driving degrades performance by increasing driver reaction time, particularly among older drivers, and can lead to higher accident risk among drivers of all ages. Research findings such as these have supported governmental regulation of cell-phone use.\n\n"}
{"id": "9245367", "url": "https://en.wikipedia.org/wiki?curid=9245367", "title": "European Route of Industrial Heritage", "text": "European Route of Industrial Heritage\n\nThe European Route of Industrial Heritage (ERIH) is a network (theme route) of the most important industrial heritage sites in Europe. This is a tourism industry information initiative to present a network of industrial heritage sites across Europe. The aim of the project is to create interest for the common European Heritage of the Industrialisation and its remains. ERIH also wants to promote regions, towns and sites showing the industrial history and market them as visitor attractions in the leisure and tourism industry.\n\nThe â virtual â main route is built by the so-called Anchor Points. These are Industrial Heritage sites which are the historically most important and most attractive for visitors. The route leads through 13 countries thus far (in 2014): United Kingdom, the Netherlands, Belgium, Luxembourg, Germany, France, Spain, Italy, Czech Republic, Poland, Sweden, Norway and Denmark.\n\nThe anchor sites in are:\n\nRegional Routes (like the Route der Industriekultur in the Ruhr) cover regions as where industrial history has left its mark. Currently (2017) there are seventeen:\n\n\nThirteen European Theme Routes show the diversity of industrial landscapes all over Europe and the common roots of industrial history:\n"}
{"id": "11747061", "url": "https://en.wikipedia.org/wiki?curid=11747061", "title": "Fermentas", "text": "Fermentas\n\nFermentas was a biotechnology company specializing in the discovery and production of molecular biology products for life science research and diagnostics. Since 2010, Fermentas has been part of Thermo Fisher Scientific.\n\nIn 2003, Fermentas consolidated its international business and set up a controlling enterprise, Fermentas International in Canada. Fermentas International became the shareholder of Fermentas in Vilnius, Lithuania, and its joint ventures in the United States, Canada and Germany (as well as Fermentas China established in 2009).\n\nIn 2010, Fermentas International was acquired by Thermo Fisher Scientific. Fermentas, and all of its enterprises, became part of the Analytical Technology segment of the Thermo Fisher.\n\nFermentas has principal manufacturing operations in Vilnius, Lithuania.\n\nFermentas is a producer of molecular biology products and is known for its restriction enzymes and DNA ladders and molecular weight markers.\n\nMain products are FastDigest and conventional restriction enzymes, DNA/RNA modifying enzymes, transfection reagents, nucleotides and primers, products for PCR and RT-PCR, molecular cloning, nucleic acid purification, in vitro transcription, molecular labeling and detection, DNA, RNA, protein electrophoresis.\n\nAll Fermentasâ products are produced in Class D clean-room facilities, qualified and certified as per EU directives and ISPE guidelines, which are prerequisite for GMP manufacturing. The company is operating under ISO9001, ISO13485 quality and ISO14001 environmental management systems.\n\n"}
{"id": "5813818", "url": "https://en.wikipedia.org/wiki?curid=5813818", "title": "Fernanda ViÃ©gas", "text": "Fernanda ViÃ©gas\n\nFernanda Bertini ViÃ©gas (born 1971) is a Brazilian scientist and designer, whose work focuses on the social, collaborative and artistic aspects of information visualization.\n\nViÃ©gas received a Ph.D. in Media Arts and Sciences from the MIT Media Lab in 2005. The same year she began work at the Cambridge, Massachusetts location of IBM's Thomas J. Watson Research Center as part of the Visual Communication Lab.\n\nIn April 2010, she and Martin M. Wattenberg started a new venture called Flowing Media, Inc., to focus on visualization aimed at consumers and mass audiences. Four months later, both of them joined Google as the co-leaders of the Google's \"Big Picture\" data visualization group in Cambridge, Massachusetts.\n\nViÃ©gas began her research while at the MIT Media Lab, focusing on graphical interfaces for online communication. Her Chat Circles system introduced ideas such as proximity-based filtering of conversation and a visual archive of chat history displaying the overall rhythm and form of a conversation. Her email visualization designs (including PostHistory and Themail) are the foundation for many other systems; her findings on how visualizations are often used for storytelling influenced subsequent work on the collaborative aspects of visualization. While at MIT, she also studied usage of Usenet and blogs.\n\nA second stream of work, in partnership with Martin Wattenberg, centers on collective intelligence and the public use of data visualization.\n\nHer work with visualizations such as History Flow and Chromogram led to some of the earliest publications on the dynamics of Wikipedia, including the first scientific study of the repair of vandalism.\n\nViÃ©gas is one of the founders of IBM's experimental Many Eyes website, created in 2007, which seeks to make visualization technology accessible to the public. In addition to broad uptake from individuals, the technology from Many Eyes has been used by nonprofits and news outlets such as the New York Times Visualization Lab.\n\nViÃ©gas is also known for her artistic work, which explores the medium of visualization for explorations of emotionally charged digital data. An early example is Artifacts of the Presence Era, an interactive installation at the Boston Center for the Arts in 2003, which featured a video-based timeline of visitor interactions with the museum. She often works with Martin Wattenberg to visualize emotionally charged information. An example of these works is their piece \"Web Seer\", which is a visualization of Google Suggest. The Fleshmap series (started in 2008) uses visualization to portray aspects of sensuality, and includes work on the web, video, and installations. In 2012, she launched the Wind Map project, which displays continuously updated forecasts of wind patterns across the United States.\n\n\n"}
{"id": "5933973", "url": "https://en.wikipedia.org/wiki?curid=5933973", "title": "Fire screen", "text": "Fire screen\n\nA fire screen sheet began as a form of furniture that acted as a shield between the occupants of a room and the fireplace, and its primary function was to reduce the discomfort of excessive heat from a log fire. Early firescreens were generally shaped as flat panels standing on attached feet, or as adjustable shield-shaped panels mounted on tripod table legs.\n\nFirescreens in the modern home have become decorative shields of sheet metal, glass, or wire mesh that can be placed in front of a fireplace opening to protect the room from open flames and flying embers that may be emitted by the fire.\n\nFire screens were used to cover the fireplace when nothing was burning inside it, and make it look more decorative.\nThe \"three-panel\" fire screen, which covers the fireplace almost completely, has two side panels angled away from the central panel. It is an effective way of providing decoration in a room.\n\nThe \"horse screen\", or \"cheval screen\" (cheval is the French word for horse) was in common use from the 18th century. It is a wide screen having two feet on each side, the arrangement of the feet giving the screen its name. Placed in front of the unused fireplace, the decorated screen improves the appearance of a room. Screens are decorated with embroidery, papier machÃ©, painted wood or perhaps stained glass; the frame and feet might be carved.\n\nThe \"pole screen\" also began to appear in the 18th century. It is a smaller screen placed on a vertical pole which is mounted on a tripod; placed between a lit fire and an occupant of the room, the screen can be adjusted up or down to shield the person's face from the heat. The screen might be rectangular or a more decorous shape, and is decorated perhaps with embroidery, lacquer or paint.\n\nThe \"banner\" screen is similar to a pole screen; instead of a solid screen there is a loose piece of silk or embroidery, weighted with tassels on the lower edge; like a banner, it is supported from the top edge by a crossbar connected to a pole.\n"}
{"id": "44993448", "url": "https://en.wikipedia.org/wiki?curid=44993448", "title": "Flow graph (mathematics)", "text": "Flow graph (mathematics)\n\nA flow graph is a form of digraph associated with a set of linear algebraic or differential equations:\n\nAlthough this definition uses the terms \"signal flow graph\" and \"flow graph\" interchangeably, the term \"signal flow graph\" is most often used to designate the Mason signal-flow graph, Mason being the originator of this terminology in his work on electrical networks. Likewise, some authors use the term \"flow graph\" to refer strictly to the Coates flow graph. According to Henley & Williams:\n\nA designation \"flow graph\" that includes both the Mason graph and the Coates graph, and a variety of other forms of such graphs appears useful, and agrees with Abrahams and Coverley's and with Henley and Williams' approach.\n\nA directed network â also known as a flow network â is a particular type of flow graph. A \"network\" is a graph with real numbers associated with each of its edges, and if the graph is a digraph, the result is a \"directed network\". A flow graph is more general than a directed network, in that the edges may be associated with \"gains, branch gains\" or \"transmittances\", or even functions of the Laplace operator \"s\", in which case they are called \"transfer functions\".\n\nThere is a close relationship between graphs and matrices and between digraphs and matrices. \"The algebraic theory of matrices can be brought to bear on graph theory to obtain results elegantly\", and conversely, graph-theoretic approaches based upon flow graphs are used for the solution of linear algebraic equations.\n\nAn example of a flow graph connected to some starting equations is presented.\n\nThe set of equations should be consistent and linearly independent. An example of such a set is:\n\nConsistency and independence of the equations in the set is established because the determinant of coefficients is non-zero, so a solution can be found using Cramer's rule.\n\nUsing the examples from the subsection Elements of signal flow graphs, we construct the graph In the figure, a signal-flow graph in this case. To check that the graph does represent the equations given, go to node \"x\". Look at the arrows incoming to this node (colored green for emphasis) and the weights attached to them. The equation for \"x\" is satisfied by equating it to the sum of the nodes attached to the incoming arrows multiplied by the weights attached to these arrows. Likewise, the red arrows and their weights provide the equation for \"x\", and the blue arrows for \"x\".\n\nAnother example is the general case of three simultaneous equations with unspecified coefficients:\nTo set up the flow graph, the equations are recast so each identifies a single variable by adding it to each side. For example:\nUsing the diagram and summing the incident branches into \"x\" this equation is seen to be satisfied.\n\nAs all three variables enter these recast equations in a symmetrical fashion, the symmetry is retained in the graph by placing each variable at the corner of an equilateral triangle. Rotating the figure 120Â° simply permutes the indices. This construction can be extended to more variables by placing the node for each variable at the vertex of a regular polygon with as many vertices as there are variables.\n\nOf course, to be meaningful the coefficients are restricted to values such that the equations are independent and consistent.\n\n\n"}
{"id": "31309377", "url": "https://en.wikipedia.org/wiki?curid=31309377", "title": "Food spoilage", "text": "Food spoilage\n\nSpoilage is the process in which food deteriorates to the point in which it is not edible to humans or its quality of edibility becomes reduced. Various external forces are responsible for the spoilage of food. Food that is capable of spoiling is referred to as perishable food.\n\nHarvested foods decompose from the moment they are harvested due to attacks from enzymes, oxidation and microorganisms. These include bacteria, mold, yeast, moisture, temperature and chemical reaction. \n\nBacteria can be responsible for the spoilage of food. When bacteria breaks down the food, acids and other waste products are created in the process. While the bacteria itself may or may not be harmful, the waste products may be unpleasant to taste or may even be harmful to one's health.\n\nYeasts can be responsible for the decomposition of food with a high sugar content. The same effect is useful in the production of various types of food and beverages, such as bread, yogurt, cider, and alcoholic beverages.\n\nSigns of food spoilage may include an appearance different from the food in its fresh form, such as a change in color, a change in texture, an unpleasant odour, or an undesirable taste. The item may become softer than normal. If mold occurs, it is often visible externally on the item.\n\nSpoilage bacteria do not normally cause \"food poisoning\"; typically, the microorganisms that cause foodborne illnesses are odorless and flavourless, and otherwise undetectable outside the lab.\nEating deteriorated food could not be considered safe due to mycotoxins or microbial wastes. Some pathogenic bacteria, such as \"Clostridium perfringens\" and \"Bacillus cereus\", are capable of causing spoilage.\n\nA number of methods of prevention can be used that can either totally prevent, delay, or otherwise reduce food spoilage.\n\nA food rotation system uses the first in first out method (FIFO), which ensures that the first item purchased is the first item consumed. \n\nPreservatives can expand the shelf life of food and can lengthen the time long enough for it to be harvested, processed, sold, and kept in the consumer's home for a reasonable length of time.\n\nRefrigeration can increase the shelf life of certain foods and beverages, though with most items, it does not indefinitely expand it. Freezing can preserve food even longer, though even freezing has limitations.\n\nA high-quality vacuum flask (thermos) will keep coffee, soup, and other boiling-hot foods above the danger zone (140F/58C), at which bacteria grow most rapidly, for over 24 hours.\n\nCanning of food can preserve food for a particularly long period of time, whether canned at home or commercially. Canned food is vacuum packed in order to keep oxygen out of the can that is needed to allow bacteria to break it down. Canning does have limitations, and does not preserve the food indefinitely.\n\nLactic acid fermentation also preserves food and prevents spoilage.\n\n"}
{"id": "41038479", "url": "https://en.wikipedia.org/wiki?curid=41038479", "title": "High-definition fiber tracking", "text": "High-definition fiber tracking\n\nHigh definition fiber tracking (HDFT) is a tractography technique where data from MRI scanners is processed through computer algorithms to reveal the detailed wiring of the brain and to pinpoint fiber tracts. Each tract contains millions of neuronal connections. HDFT is based on data acquired from diffusion spectrum imaging and processed by generalized q-sampling imaging. The technique makes it possible to virtually dissect 40 major fiber tracts in the brain. The HDFT scan is consistent with brain anatomy unlike diffusion tensor imaging (DTI). Thus, the use of HDFT is essential in pinpointing damaged neural connections.\n\nTraditional DTI uses six diffusivity characteristics to model how water molecules diffuse in brain tissues and makes axonal fiber tracking possible. However, DTI had a major limitation in resolving axons from different tracts intersected and crossed en route to their target. In 2009, Learning Research & Development Center (LRDC) at University of Pittsburgh launched the 2009 Pittsburgh Brain Competition to invite the best research team to work on this problem. A price of $10,000 was offered to the team that could track optic radiations, and teams from 168 countries took part in the competition. A winning team from Taiwan revealed Meyerâs loop, which no other team had successfully tracked. The technique was further developed as HDFT between the University of Pittsburgh and Carnegie Mellon University.\n\nHDFT has been applied to traumatic brain injury (TBI) to identify which brain connections have been broken and which are still intact. HDFT allows neurosurgeons to localize fiber breaks caused by traumatic brain injuries to provide better diagnoses and prognoses. HDFT can also be used to determine the optimal surgical approach for difficult-to-reach tumors and vascular malformations.\n\n\n"}
{"id": "53676588", "url": "https://en.wikipedia.org/wiki?curid=53676588", "title": "ID Finance", "text": "ID Finance\n\nID Finance is a financial technology company founded in 2012 by Boris Batine and Alexander Dunaev. It is predominantly focused on emerging markets, which are characterized by noncompetitive financial services, limited availability of credit and high barriers to entry.\n\nID Finance's proprietary IT and risk management system collects and analyses thousands of data points in order to process loan applications in real time over the internet.\n\nID Finance is headquartered in Barcelona and employs 380 staff. It is providing loans to under the brand MoneyMan in Russia, Spain, Kazakhstan, Georgia, Poland, Brazil and Mexico. By having a presence in multiple markets it is better able to manage risk. It is the largest online lender in Russia and the CIS region. It reached profitability in 2015 and saw revenues treble to $68m in 2016. As of February 2017 the company was issuing 50,000 loans a month. The company also plans to launch in Colombia and Peru. According to reports, a growing number of ID Finance's customers are Gold or Platinum credit card users that are seeking more competitive rates.\n\nEquity investors include Emery Capital Venture Fund and Russian food magnate Vadim Dymov.\n\nIn February 2017 ID Finance secured $50 million in debt financing including $15 million from TransKapitalBank.\n\nIn December 2017 the company began a $170m programme of bond issuances, successfully raising $8.5m via exchange-traded bonds.\n"}
{"id": "53017273", "url": "https://en.wikipedia.org/wiki?curid=53017273", "title": "James May: The Reassembler", "text": "James May: The Reassembler\n\nJames May: The Reassembler is a BBC Four documentary programme focusing on the reassembly of various pieces of technology from the past. The host James May discusses the item in question whilst documenting how long it has taken to reassemble the device throughout each episode.\n\nSeason 1 has also been shown on Quest, in a 1 hour format with commercials.\n\n"}
{"id": "19892614", "url": "https://en.wikipedia.org/wiki?curid=19892614", "title": "Key relevance", "text": "Key relevance\n\nIn master locksmithing, key relevance is the measurable difference between an original key and a copy made of that key, either from a wax impression or directly from the original, and how similar the two keys are in size and shape. It can also refer to the measurable difference between a key and the size required to fit and operate the keyway of its paired lock.\n\nNo two copies of keys are exactly the same, even if they were both made from key blanks that are struck from the same mould or cut from the same duplicating/milling machine with no changes to the bitting settings in between. Even under these favorable circumstances, there will be minute differences between the two key shapes, though their key relevance is extremely high.\n\nIn all machining work, there are measurable amounts of difference between the design specification of an object, and its actual manufactured size. In locksmithing, the allowable tolerance is decided by the range of minute differences between a key's size and shape in comparison to the size and shape required to turn the tumblers within the lock. Key relevance is the measure of similarity between the key and the optimal size needed to fit the lock, or it is the similarity between a duplicate key and the original it is seeking to replicate.\n\nKey relevance cannot be deduced from a key code, since the key code merely refers to a central authoritative source for designed shapes and sizes of keys.\n\nTypical modern keys require a key relevance of approximately to (accuracy within 0.75% to 1.75%) in order to operate.\n\n\"Key relevance\" may also be applied to types of electronic locks when used to refer to similarities in magnetic signatures or radio codes, though truly digital-coded radio signals do not have key relevance ratings because they must have precise unlocking codes, with no degree of difference whatsoever in order to operate.\n\nThe term has also occasionally been co-opted by broader mechanical engineering to refer to the similarity of fit between two interlocking machined parts, although \"tolerance\" or \"backlash\" has gained far wider adoption and acceptance in usage.\n\n"}
{"id": "1911105", "url": "https://en.wikipedia.org/wiki?curid=1911105", "title": "Klabin", "text": "Klabin\n\nKlabin is the biggest paper producer, exporter and recycler in Brazil. The Company is headquartered in Sao Paulo\n\nIt is the leading manufacturer of packaging paper and board, corrugated boxes, industrial sacks and timber in logs. It has 17 industrial plants in Brazil and one in Argentina. Self-sufficient in wood, it has 218 thousand hectares of planted forests and 183 thousand hectares of preserved native woodlands. It was the first company from the pulp and paper sector in the Southern Hemisphere to have its forests certified by the FSC (Forest Stewardship Council), attesting to the fact that the company runs its activities within the highest possible standards of environmental conservation and socioeconomic sustainability. The History of Klabin starts with the arrival of two families of Lithuanian immigrants to Brazil; Klabin and Lafer.\n\n\n"}
{"id": "55183923", "url": "https://en.wikipedia.org/wiki?curid=55183923", "title": "Leonardo da Vinci Medal", "text": "Leonardo da Vinci Medal\n\nThe Leonardo da Vinci Medal is the highest award of the Society for the History of Technology (SHOT) established in 1962. In general this award is granted annually to people, who have contributed outstandingly to the history of technology through research, teaching, publication or other activities. The prize consists of a certificate and a medal.\n\n"}
{"id": "339482", "url": "https://en.wikipedia.org/wiki?curid=339482", "title": "Limelight", "text": "Limelight\n\nLimelight (also known as Drummond light or calcium light) is a type of stage lighting once used in theatres and music halls. An intense illumination is created when an oxyhydrogen flame is directed at a cylinder of quicklime (calcium oxide), which can be heated to before melting. The light is produced by a combination of incandescence and candoluminescence. Although it has long since been replaced by electric lighting, the term has nonetheless survived, as someone in the public eye is still said to be \"in the limelight\". The actual lights are called \"limes\", a term which has been transferred to electrical equivalents.\n\nThe limelight effect was discovered in the 1820s by Goldsworthy Gurney, based on his work with the \"oxy-hydrogen blowpipe\", credit for which is normally given to Robert Hare. In 1825, a Scottish engineer, Thomas Drummond (1797â1840), saw a demonstration of the effect by Michael Faraday and realized that the light would be useful for surveying. Drummond built a working version in 1826, and the device is sometimes called the \"Drummond light\" after him.\n\nThe earliest known use of limelight at a public performance was outdoors, over Herne Bay Pier, Kent, on the night of 3 October 1836 to illuminate a juggling performance by magician Ching Lau Lauro. This performance was part of the celebrations following the laying of the foundation stone of the Clock Tower. The advertising leaflet called it \"koniaphostic\" light and announced that \"the whole pier is overwhelmed with a flood of beautiful white light\". Limelight was first used for indoor stage illumination in the Covent Garden Theatre in London in 1837 and enjoyed widespread use in theatres around the world in the 1860s and 1870s.\nLimelights were employed to highlight solo performers in the same manner as modern followspots (spotlights).\nLimelight was replaced by electric arc lighting in the late 19th century.\n\n\n"}
{"id": "14400425", "url": "https://en.wikipedia.org/wiki?curid=14400425", "title": "List of daggers", "text": "List of daggers\n\nThe following is a list of daggers.\n\n\n\n\nMilitary issue or commercial designs, 1918 to present.\n\n"}
{"id": "1254375", "url": "https://en.wikipedia.org/wiki?curid=1254375", "title": "List of glassware", "text": "List of glassware\n\nThe list of glasswares includes drinking vessels (drinkware) and tableware used to set a table for eating a meal, general glass items such as vases, and glasses used in the catering industry. It does not include laboratory glassware.\n\nDrinkware, beverageware (in other words, cups) is a general term for a vessel intended to contain beverages or liquid foods for drinking or consumption. \n\nThe word \"cup\" comes from Middle English \"cuppe\", from Old English, from Late Latin \"cuppa\", drinking vessel, perhaps variant of Latin \"cupa\", tub, cask. The first known use of the word cup is before the 12th century.\n\nTumblers are flat-bottomed drinking glasses.\n\n\n\n"}
{"id": "2575602", "url": "https://en.wikipedia.org/wiki?curid=2575602", "title": "Match report", "text": "Match report\n\nIn metadata, a match report is a report that compares two distinct data dictionaries and creates a list of the data elements that have been identified as semantically equivalent.\n\nMatch reports are critical for systems that wish to automatically exchange data such as intelligent software agents. If one computer system is requesting a report from a remote system that uses a distinct data dictionary and all of the data elements on the report manifest are included in the match report the report request can be executed.\n\nMatch reports are useful if data dictionaries use a metadata tagging system such as the UDEF.\n\n"}
{"id": "312671", "url": "https://en.wikipedia.org/wiki?curid=312671", "title": "Mechanosynthesis", "text": "Mechanosynthesis\n\nMechanosynthesis is a term for hypothetical chemical syntheses in which reaction outcomes are determined by the use of mechanical constraints to direct reactive molecules to specific molecular sites. There are presently no non-biological chemical syntheses which achieve this aim. Some atomic placement has been achieved with scanning tunnelling microscopes.\n\nIn conventional chemical synthesis or chemosynthesis, reactive molecules encounter one another through random thermal motion in a liquid or vapor. In a hypothesized process of mechanosynthesis, reactive molecules would be attached to molecular mechanical systems, and their encounters would result from mechanical motions bringing them together in planned sequences, positions, and orientations. It is envisioned that mechanosynthesis would avoid unwanted reactions by keeping potential reactants apart, and would strongly favor desired reactions by holding reactants together in optimal orientations for many molecular vibration cycles. In biology, the ribosome provides an example of a programmable mechanosynthetic device.\n\nA non-biological form of mechanochemistry has been performed at cryogenic temperatures using scanning tunneling microscopes. So far, such devices provide the closest approach to fabrication tools for molecular engineering. Broader exploitation of mechanosynthesis awaits more advanced technology for constructing molecular machine systems, with ribosome-like systems as an attractive early objective.\n\nMuch of the excitement regarding advanced mechanosynthesis regards its potential use in assembly of molecular-scale devices. Such techniques appear to have many applications in medicine, aviation, resource extraction, manufacturing and warfare.\n\nMost theoretical explorations of advanced machines of this kind have focused on using carbon, because of the many strong bonds it can form, the many types of chemistry these bonds permit, and utility of these bonds in medical and mechanical applications. Carbon forms diamond, for example, which if cheaply available, would be an excellent material for many machines.\n\nIt has been suggested, notably by K. Eric Drexler, that mechanosynthesis will be fundamental to molecular manufacturing based on nanofactories capable of building macroscopic objects with atomic precision. The potential for these has been disputed, notably by Nobel Laureate Richard Smalley (who proposed and then critiqued an unworkable approach based on small fingers).\n\nThe Nanofactory Collaboration, founded by Robert Freitas and Ralph Merkle in 2000, is a focused ongoing effort involving 23 researchers from 10 organizations and 4 countries that is developing a practical research agenda specifically aimed at positionally controlled diamond mechanosynthesis and diamondoid nanofactory development.\n\nIn practice, getting exactly one molecule to a known place on the microscope's tip is possible, but has proven difficult to automate. Since practical products require at least several hundred million atoms, this technique has not yet proven practical in forming a real product.\n\nThe goal of one line of mechanoassembly research focuses on overcoming these problems by calibration, and selection of appropriate synthesis reactions. Some suggest attempting to develop a specialized, very small (roughly 1,000 nanometers on a side) machine tool that can build copies of itself using mechanochemical means, under the control of an external computer. In the literature, such a tool is called an assembler or molecular assembler. Once assemblers exist, geometric growth (directing copies to make copies) could reduce the cost of assemblers rapidly. Control by an external computer should then permit large groups of assemblers to construct large, useful projects to atomic precision. One such project would combine molecular-level conveyor belts with permanently mounted assemblers to produce a factory.\n\nIn part to resolve this and related questions about the dangers of industrial accidents and popular fears of runaway events equivalent to Chernobyl and Bhopal disasters, and the more remote issue of ecophagy, grey goo and green goo (various potential disasters arising from runaway replicators, which could be built using mechanosynthesis) the UK Royal Society and UK Royal Academy of Engineering in 2003 commissioned a study to deal with these issues and larger social and ecological implications, led by mechanical engineering professor Ann Dowling. This was anticipated by some to take a strong position on these problems and potentials ââ and suggest any development path to a general theory of so-called mechanosynthesis. However, the Royal Society's nanotech report did not address molecular manufacturing at all, except to dismiss it along with grey goo.\n\nCurrent technical proposals for nanofactories do not include self-replicating nanorobots, and recent ethical guidelines would prohibit development of unconstrained self-replication capabilities in nanomachines.\n\nThere is a growing body of peer-reviewed theoretical work on synthesizing diamond by mechanically removing/adding hydrogen atoms and depositing carbon atoms (a process known as diamond mechanosynthesis or DMS). \nFor example, the 2006 paper in this continuing research effort by Freitas, Merkle and their collaborators reports that the most-studied mechanosynthesis tooltip motif (DCB6Ge) successfully places a C carbon dimer on a C(110) diamond surface at both 300 K (room temperature) and 80 K (liquid nitrogen temperature), and that the silicon variant (DCB6Si) also works at 80 K but not at 300 K. These tooltips are intended to be used only in carefully controlled environments (e.g., vacuum). Maximum acceptable limits for tooltip translational and rotational misplacement errors are reported in paper IIIâtooltips must be positioned with great accuracy to avoid bonding the dimer incorrectly. Over 100,000 CPU hours were invested in this study.\n\nThe DCB6Ge tooltip motif, initially described at a Foresight Conference in 2002, was the first complete tooltip ever proposed for diamond mechanosynthesis and remains the only tooltip motif that has been successfully simulated for its intended function on a full 200-atom diamond surface. Although an early paper gives a predicted placement speed of 1 dimer per second for this tooltip, this limit was imposed by the slow speed of recharging the tool using an inefficient recharging method and is not based on any inherent limitation in the speed of use of a charged tooltip. Additionally, no sensing means was proposed for discriminating among the three possible outcomes of an attempted dimer placementâdeposition at the correct location, deposition at the wrong location, and failure to place the dimer at allâbecause the initial proposal was to position the tooltip by dead reckoning, with the proper reaction assured by designing appropriate chemical energetics and relative bond strengths for the tooltip-surface interaction.\n\nMore recent theoretical work analyzes a complete set of nine molecular tools made from hydrogen, carbon and germanium able to (a) synthesize all tools in the set (b) recharge all tools in the set from appropriate feedstock molecules and (c) synthesize a wide range of stiff hydrocarbons (diamond, graphite, fullerenes, and the like). All required reactions are analyzed using standard ab initio quantum chemistry methods.\n\nFurther research to consider alternate tips will require time-consuming computational chemistry and difficult laboratory work.\nIn the early 2000s, a typical experimental arrangement was to attach a molecule to the tip of an atomic force microscope, and then use the microscope's precise positioning abilities to push the molecule on the tip into another on a substrate. Since the angles and distances can be precisely controlled, and the reaction occurs in a vacuum, novel chemical compounds and arrangements are possible.\n\nThe technique of moving single atoms mechanically was proposed by Eric Drexler in his 1986 book The Engines of Creation.\n\nIn 1988, researchers at IBM's ZÃ¼rich Research Institute successfully spelled the letters \"IBM\" in xenon atoms on a cryogenic copper surface, grossly validating the approach. Since then, a number of research projects have undertaken to use similar techniques to store computer data in a compact fashion. More recently the technique has been used to explore novel physical chemistries, sometimes using lasers to excite the tips to particular energy states, or examine the quantum chemistry of particular chemical bonds.\n\nIn 1999, an experimentally proved methodology called feature-oriented scanning (FOS) was suggested. The feature-oriented scanning methodology allows precisely controlling the position of the probe of a scanning probe microscope (SPM) on an atomic surface at room temperature. The suggested methodology supports fully automatic control of single- and multiprobe instruments in solving tasks of mechanosynthesis and bottom-up nanofabrication.\n\nIn 2003, Oyabu \"et al.\" reported the first instance of purely mechanical-based covalent bond-making and bond-breaking, i.e., the first experimental demonstration of true mechanosynthesisâalbeit with silicon rather than carbon atoms.\n\nIn 2005, the first patent application on diamond mechanosynthesis was filed.\n\nIn 2008, a $3.1 million grant was proposed to fund the development of a proof-of-principle mechanosynthesis system.\n\nSee also molecular nanotechnology, a more general explanation of the possible products, and discussion of other assembly techniques.\n\n"}
{"id": "31326967", "url": "https://en.wikipedia.org/wiki?curid=31326967", "title": "Modular data center", "text": "Modular data center\n\nA modular data center system is a portable method of deploying data center capacity. A modular data center can be placed anywhere data capacity is needed.\n\nModular data center systems consist of purpose-engineered modules and components to offer scalable data center capacity with multiple power and cooling options. Modules can be shipped to be added, integrated or retrofitted into an existing data center or combined into a system of modules. Modular data centers typically consist of standardized components.\n\nModular data centers are often marketed as converged infrastructure, promoting economies of scale and efficient energy usage, including considerations regarding the external environment. A module can be treated as a single unit for U.S. Federal Communications Commission compliance certification rather than all discrete systems.\nPatents have been taken out on variations.\n\nModular data centers typically come in two types of form factors. The more common type, referred to as \"containerized data centers\" or \"portable modular data centers\", fits data center equipment (servers, storage and networking equipment) into a standard shipping container, which is then transported to a desired location. Containerized data centers typically come outfitted with their own cooling systems. Cisco makes an example of this type of data center, called the Cisco Containerized Data Center.\n\nAnother form of modular data center fits data center equipment into a facility composed of prefabricated components that can be quickly built on a site and added to as capacity is needed. For example, HPâs version of this type of modular data center is constructed of sheet metal components that are formed into four data center halls linked by a central operating building.\n\n"}
{"id": "54068149", "url": "https://en.wikipedia.org/wiki?curid=54068149", "title": "National Information Technology Authority-Uganda", "text": "National Information Technology Authority-Uganda\n\nThe National Information and Technology Authority - Uganda (NITA-U) is an autonomous government parastatal under the Ministry of ICT and National Guidance in Uganda, mandated to coordinate, promote and monitor Information and Technology developments in Uganda within the context of National Social and Economic development.\n\nThe headquarters of NITA-U are located on Lugogo Rotary Avenue in Kampala district. The coordinates are Latitude:0.3331848; Longitude:32.5994181.\n\nThe National Information Technology Authority - Uganda was initiated under the NITA- U Act, in 2009. It is generally overseen by the Ministry of ICT and National Guidance.\nThe Authority provides technical support and expert guidance to critical Government Information Technology systems in some of the following ways:\n\nThe Authority constitutes the following key people and the respective directorates they head;\n\nThe NBI/EGI Project is Composed to two (2) Components;\n\nThe major aims of the Project are;\n\n\n"}
{"id": "28776425", "url": "https://en.wikipedia.org/wiki?curid=28776425", "title": "Network video recorder", "text": "Network video recorder\n\nA network video recorder (NVR) is a specialized computer system that includes a software program that records video in a digital format to a disk drive, USB flash drive, SD memory card or other mass storage device. An NVR contains no dedicated video capture hardware. However, the software is typically run on a dedicated device, usually with an embedded operating system. Alternatively, to help support increased functionality and serviceability, standard operating systems are used with standard processors and video management software. An NVR is typically deployed in an IP video surveillance system.\n\nNetwork video recorders are distinct from digital video recorders (DVR) as their input is from a network rather than a direct connection to a video capture card or tuner. Video on a DVR is encoded and processed at the DVR, while video on an NVR is encoded and processed at the camera, then streamed to the NVR for storage or remote viewing.\nAdditional processing may be done at the NVR, such as further compression or tagging with meta data.\n\nHybrid NVR/DVR surveillance systems exist which incorporate functions of both NVR and DVR; these are considered a form of NVR. \n\nNVR home surveillance systems are generally wireless, tend to be easy to set up, can be accessed through a web browser, and allow the user to be notified by email if an alarm is triggered.\n\n"}
{"id": "5273082", "url": "https://en.wikipedia.org/wiki?curid=5273082", "title": "Nissan Motors vs. Nissan Computer", "text": "Nissan Motors vs. Nissan Computer\n\nNissan Motors vs. Nissan Computer was a lengthy court case between the two parties over use of the name Nissan and the domain name nissan.com. The case has received national attention in the U.S.\n\nBeginning in the late 1970s, Datsun began progressively fitting its cars with small \"Nissan\" and \"Datsun by Nissan\" badges. The company eventually changed its branding at 1,100 Datsun dealerships. In autumn 1981, Datsun announced that its name would be changed in the United States. Between 1982 and 1986, the company transitioned from its \"Datsun, We Are Driven!\" to its \"The Name is Nissan\" campaign. Five years after the name change program was over, cars in some export markets continued to display badges bearing both names and \"Datsun\" still remained more familiar than \"Nissan\".\n\nIn 1980, Uzi Nissan founded Nissan Foreign Car, an automobile service, in Raleigh, North Carolina. In 1987, Uzi Nissan founded Nissan International, Ltd, an import/export company that traded primarily in heavy equipment and computers. On 14 May 1991, Uzi Nissan founded Nissan Computer Corporation, which provides sales and service of personal computers, servers, and computer parts, as well as internet hosting and development. Nissan Computer registered \"nissan.com\" for its use on 4 June 1994, five years prior to Nissan Motor Corporation's interest in the domain.\n\nNissan Motors considered Nissan Computer's use of the name to be trademark dilution, and laid claim to the domain by alleging cyber squatting. However, Nissan Computer was named after its owner, Uzi Nissan. Following the outcome of the case, Nissan Motors uses the name \"nissanusa.com\" for its U.S. website.\n"}
{"id": "6294914", "url": "https://en.wikipedia.org/wiki?curid=6294914", "title": "Orbital Debris Co-ordination Working Group", "text": "Orbital Debris Co-ordination Working Group\n\nThe Orbital Debris Co-ordination Working Group (ODCWG) is one of the working groups of the International Organization for Standardization's Technical Committee 20/Subcommittee 14 TC20/SC14 \"Spacecraft Systems and Operations\".\n\nIt was formed by unanimous agreement at the May 2003 Plenary meeting of TC20/SC14. The text of the resolution is as follows:\n\nThe terms of reference for the ODCWG state that the aims of the group are as follows:\n\nCurrently six standards projects are in development, and a further seven project proposals are being prepared. The first debris mitigation standards were expected in 2008, with more International Standards, technical specifications or technical reports expected to be published through to 2011-2012.\n\n\n"}
{"id": "54497735", "url": "https://en.wikipedia.org/wiki?curid=54497735", "title": "Philippine Nuclear Research Institute", "text": "Philippine Nuclear Research Institute\n\nThe Philippine Nuclear Research Institute (PNRI) is a government agency under the Department of Science and Technology mandated to undertake research and development activities in the peaceful uses of nuclear energy, institute regulations on the said uses, and carry out the enforcement of said regulations to protect the health and safety of radiation workers and the general public.\n\nThe Philippine Nuclear Research Institute (PNRI) is an agency of the government that is authorized to regulate the safe and peaceful applications of nuclear science and technology in the Philippines.\n\nUnder Executive Order 128, the PNRI is mandated to perform the following functions:\n\nUnder Executive Order 128, the PNRI is headed by a Director assisted by a Deputy Director. It is composed of four technical divisions and one administrative/finance division.\n\nThe five divisions provide the Institute with research, nuclear-related, policy development, budgetary assistance, and technology development services respectively:\n263 permanent positions make up the PNRI organization.\n\nIn the year 1958, the Philippine Atomic Energy Commission, which would later be known as PAEC, was established following the Republic Act No. 2067. This R.A. is also known as the \"Science Act of 1958.\" In the early 1960s, the PAEC built the Philippine Research Reactor-1, the first nuclear reactor in the Philippines. The \"Atomic Energy Regulatory and Liability Act of 1968\" established the regulatory function and mandate of the PAEC whereas on December 13, 1974, Presidential Decree No. 606 established the PAEC as an independent and autonomous body. Three years later, the Presidential Decree 1206 of October 6, 1977 created the Ministry of Energy (MOE). From the MoE, the Philippine Atomic Energy Commission was transferred back to the Office of the President Executive under Order No. 613 on August 15, 1980 and transferred again to the Office of the Prime Minister under Executive Order No. 708 of July 2, 1981. In 1984, the PAEC was placed within the administrative administration of the Department of Science and Technology under the Executive Order No. 784. The Philippine Atomic Energy Commission became the Philippine Nuclear Research Institute (PNRI) in 1987.\n\nIn 1995, the trial of the Sterile insect technique (SIT) held in Guimaras was successful. In the succeeding year, Dr. William G. Padolina, Secretary of the Department of Science and Technology, served as the president of the 40th General Conference of the International Atomic Energy Agency (IAEA). In celebration of the centennial of the discovery of radioactivity of 1997, the second Philippine Nuclear Congress was held in Manila.\n\nAt the beginning of the 21st century, the PNRI's Radiological Emergency Preparedness and Response Plan was approved in the year 2000. In 2001, the first Positron emission tomography (PET) was licensed by the PNRI at St. Luke's Medical Center. During the years 2001-2005, a PVP carrageenan hydrogel dressing for burns and wounds was developed by the PNRI as well as the development of the mutant ornamental plants Kamuning dwarf mutant (Murraya 'Ibarra Santos'), Dracaena 'Marea' and Cordyline 'Medina'. In 2005, the PNRI was designated as the collaborating center for Studies on Harmful Algal Blooms by the IAEA. The next year, the Philippine Research Reactor at the PNRI was chosen by the IAEA to be the training platform to demonstrate the decommissioning process technique under the Research Reactor Decommissioning Demonstration Project (R2D2P). The 9th Forum for Nuclear Cooperation in Asia Ministerial Level Meeting was hosted in the Philippines in 2008, the same year of the 50th Founding Anniversary of the Philippine Nuclear Research Institute.\n\nThe Philippines was named one of the three pilot countries for the IAEA Water Availability Enhancement Project (IWAVE) in 2010. The National Nuclear Security Plan and the IAEA INSSP also became operational at this time. In 2011, the Member States engaged in an RCA Regional project to study the disaster impact on the marine environment. The data was compiled in the Asia and Pacific Marine Radioactivity Database (ASPAMARD) which was managed by the Philippines through the PNRI. This was made in response to the Fukushima Daiichi nuclear disaster. In 2012, the Technetium-99m Generator Facility was commissioned. During 2013, the conditioning and storage of Spent High Activity Radioactive Sources (SHARS) was put to attention when the Philippines together with the IAEA and the South Africa Nuclear Energy Cooperation (NESCA) worked in a tripartite cooperation. In 2014, the PNRI Electron Beam Facility was inaugurated and the PNRI was able to conduct its first full exhibit of Filipino applications of nuclear science and technology at the 58th IAEA General Conference in Vienna, Austria\n\n\n\n\nThis division includes the Plant Mutation Breeding Facility, which aims for the improvement of mutation breeding of important crops. Data is gathered to compare mutants with original plants. Procedures are also undertaken for asexual propagation and testing the pre-germination of seeds.\n\nThe Plant tissue culture Laboratory aids projects in mutation induction for tissue propagation.\n\nThe Soil Science and Plant Nutrition Laboratory is for the research and development of technologies for soil, water, and crop management packages through the use of an isotope tracer and nuclear techniques. The goal is to enhance agricultural productivity while conserving natural resources for sustainable crop production.\n\n\nThe PNRI houses the Mossbauer Effect Spectrometry (MES) system, which studies nuclear structure with the absorption and re-emission of gamma rays. The other two systems are the X-ray fluorescence Spectrometry (XRF) and X-ray fluorescence Diffractometry (XRD). The XRF is a non-destructive analytical technique used to determine the composition of materials. On the other hand, the XRD, also an analytical technique, is used for phase identification of a crystalline material and provides information on the unit cell dimensions. It is more widely used for the identification of unknown crystalline materials such as minerals and inorganic compounds.\n\nFor the Cytogenetics Research Laboratory, this facility monitors and calculates the accidental (or occupational) exposure of workers and/or researchers who have been exposed to ionizing radiation through the analyses of blood samples. The Microbiological Service Laboratory performs the bioburden and sterility testing of medical devices.\n\nThe Radiation Crosslinking Laboratory practices covalent bonding with one or more polymers and imparting improved mechanical and functional properties in the result of cross-linking products. Next is the Radiation Degradation Laboratory. This facility analyzes degradation products through gel permeation chromatography and separates different molecular weight fractions by tangential flow filtration. Another facility is the Radiation-Induced Graft Polymerization Laboratory which specializes Graft polymerization as a method for the medification of a material's chemical and physical properties. Electron beam and gamma irradiation are utilized to create active sites for grafting.\n\nFor quantitative measurements, the Radioactivity Measurement Laboratory measures low level radioactivity in soil erosion studies and toxicity assay for red tide toxins by using detectors to identify and quantify alpha, beta and gamma spectrometries.\n\nAnother is the Radioassay Laboratory, which established the Radiological and Receptor Binding Assay (RBA); a method used for measuring toxicity in red tide. The last laboratory for the Chemistry Research Section is the Radiometric Dating Laboratory. This facility is a sediment dating laboratory used to study both the history of pollution in a certain area and the sedimentation rate and processes in coastal areas, lakes, rivers, and dams.\n\n\nStored in the Environmental Monitoring Laboratory are nuclear instruments used to measure low - level radioactivity collected from different types of environmental samples in various parts of the Philippines. Among the instruments stored here are the Co-Axial High Purity Germanium (HPGe) detector which is a type of semiconductor detector used specifically for gamma spectroscopy as well as x-ray spectroscopy.\n\nIn case of emergencies that may lead to an extensive spread of radioactive materials, an On-line Environmental Radiation Monitoring System provides real-time data of the radiation levels nationwide is at work.\n\n\nThe Nuclear Materials Research Facility uses gamma ray spectrometers to observe particles found in a certain concentration or venue.\n\n\nThe first facility is the Electron Beam Irradiation Facility. Through irradiation caused by Electron Beams, it is used for the sterilization of food and medical devices as well as for refining electrical components such as wires and semiconductors. Electron Beams emit radiation faster than gamma rays. An average gamma ray would take hours to irradiate an object while an electron beam may take only seconds. The next is the Gammacell-220, that is used for irradiating small samples of objects and in regulating dosimeters. The last facility is the Multipurpose Irradiation Facility. It is multi-purpose gamma ray irradiator which may be used for various applications such as elimination of harmful bacteria, improvement of agriculture and sterilization of equipment.\n\n\nThe Technetium-99m (Tc-99m) Generator Facility domestically produces Technetium 99m (Tc-99m), a radioisotope necessary for the creation of radiopharmaceuticals. The domestic production of this isotope will allow it to be sold in the Philippines at a cheaper price and with a greater supply.\n\n\nThe Isotope Radio Mass Spectrometry Facility (IRMS) analyzes substances such as water and records the stable isotopes found in the substance. The other facility under this section is the Nuclear Analytical Techniques Laboratory which handles the research and development of topics revolving around nuclear and nuclear-related techniques.\n\nThe PNRI offers several services related to nuclear energy for professionals and PNRI employees.\n\nFor their Irradiation Services, these are offered for food irradiation, for medical products sterilization and for research purposes.\n\n\nThe Nuclear Analytical Techniques Applications (NATA) Services is for radioactivity measurements and elemental determinations are provided to analyze the usage of nuclear techniques.\n\nThe Cytogenetic Analysis for Radiological Reassurance is for the monitoring or calculation of accidental or occupational exposure of clients who are exposed to gamma radiation through blood sampling.\n\nFor Microbiological Testing, bioburden and sterility testing of devices of medicine are offered using the ISO 11137.2 to establish a dose of radiation sterilization.\n\nThis scanning technique called the Gamma-ray Column Scanning Technique is for Industries is a service is to assist industries through the inspection and investigation utilizing the Gamma Ray Column Scanning Technology.\n\nIn the Radiometric / Gamma ray Spectrometry, gamma ray spectrometers are used for geological mapping, radiogenic mineral exploration, hydrothermal alteration detection, radiogenic and chemical element pollution studies, and superficial structural discontinuity detection.\n\nThe Nuclear Information Services, disseminates information on nuclear science and technology to the general public.\n\nThe Engineering Services of the PNRI offers Instrument Repair Diagnostics, Decommissioning of Cobalt-60 Teletheraphy Machine, and Radioactive Waste Management.\n\nAs for the Regulation of Nuclear Transportation, this ensures certified parties follow nuclear transportation regulations as well as issuance of certificates for nuclear transportation both national and domestic.\n\nThrough the Nuclear Training Courses (NTC), the PNRI is able to conduct training sources for different agencies, companies, industries, institutions, academe, and public. These include provisions of training courses in the field of nuclear science and technology, radiation safety, and non-destructive testing techniques.\nFurthermore, they offer On-the-Job Training Opportunities, students and technologies who would like to use nuclear apparatuses and working with researchers in the PNRI, different divisions offer training opportunities as requested.\n\nLastly, their Non-Destructive Training (NDT) Courses is the opportunity for practice of different nuclear-related courses. These are generally catered those who are willing to learn an in-depth knowledge on nuclear sciences.\n\nThe PNRI is experimenting on crop production with mutation breeding; wherein plant breeders use various techniques, and mutagens such as radiation or chemicals, to improve the crops' individual yields and develop new varieties of crop. Radiation can induce hereditary changes, or mutations, in irradiated planting materials. Another development is the Carrageenan PGP as Plant Food Supplement where the radiation-induced degradation of natural polymers like Carrageenan PGP is performed to yield oligosaccharides: natural bioactive agents that act as Plant Food Supplements. Another technique is Radiation Processing, involving the exposure of materials to ionizing radiation by either gamma radiation or electron beam.\n\nThe PNRI also practices \"Irradiation for Food Safety and Quality\": food irradiation prolongs the shelf-life of certain food and agricultural products, destroys counterproductive bacteria and microorganisms, and can disinfest grains such as rice and corn. The \"Precision Farming Methods with Stable Isotope Techniques,\" are done to improve soil test value and to provide fertilizer recommendations by using analyses based primarily on the N and C isotopes and the Soil Moisture Neutron Probe.\n\nThe PNRI helps with insect control in the Philippines through regulation or eradication. This was modeled after similar experiments done to pests in Kume Island and the Okinawa Prefecture in Japan. The regulation/eradication is done by collecting the pests, such as fruit flies, and then exposing them to gamma radiation in order to sterilize them. These sterile pests are then released back into nature and help prevent reproduction.\n\nThe PNRI developed a Polyvinyl Pyrrolidone (PVP) Carrageenan dressing: a fully permanent gel in a form of a sheet that is 3-4mm thick and containing over 90% water, used to treat burns, wounds, and bedsores. It is made from Polyvinylpyrrolidone, a water-soluble polymer, and Carrageenan, a seaweed polysaccharide by the means of Radiation Processing to effect cross-linking and to sterilize the product into a final form. Through the process of Radiation Processing, Radiation-Sterilized Honey Alginate Wound Dressing was also developed; which is for exudating burns and wounds. It is made from local honey and Sodium Alginate.\n\nThe PNRI utilizes nuclear techniques in addressing problems in air pollution, algal bloom, and water resources management through isotope-based techniques, analytical nuclear techniques, and nuclear-based techniques in algal bloom studies such as: Nuclear assay in red tide toxin analysis and Lead-210 dating method. The PNRI also took environmental radioactivity measurements following the Fukushima Daiichi nuclear disaster in part of its radiological surveillance program for public protection and safety. The PNRI aims to assess the environmental impact of the radioactive discharges of the accident and their possible effects on human health through soil, sediment, and seawater analysis for Anthropogenic Radionuclidesâindicators of the nuclear power plant accident.\n\nIn the 1990s, the PNRI identified the occurrence of Rare-earth element (REE) deposits in the northwestern part of Palawan through earlier geo-chemical surveys and studies. Considered as strategic minerals, REEs are supportive elements in the production of electronics and in the renewable energy industry. From 2013-2016, The PNRI undertook a project that was a combined verification stream sediment and radiometric survey to identify and recommend a detailed evaluation of prospective sites. The collected samples were analyzed for REE and Thorium using X-ray fluorescence (XRF) and Uranium determination using Fluorimetry, including Atomic absorption spectroscopy for the other trace elements of economic value.\n\n\n\n\n"}
{"id": "293585", "url": "https://en.wikipedia.org/wiki?curid=293585", "title": "RealDoll", "text": "RealDoll\n\nThe RealDoll is a life-size sex doll (also considered a mannequin) manufactured by Abyss Creations, LLC in San Marcos, California, and sold worldwide. It has a poseable PVC skeleton with steel joints and silicone flesh.\n\nThe RealDolls are designed to recreate the appearance, texture, and weight of the human female and male form. Their primary function is to serve as sex partners. This activity can be accompanied by certain preparations such as dressing them up in different types of clothing, changing wigs or makeup, and even adjusting body temperature by use of electric blankets or baths.\n\nEarly prototypes were made from solid latex with an interior skeleton, but construction of the outer material was later switched to silicone. In June 2009, Abyss Creations switched from tin cure silicone to platinum silicone, which resulted in dolls that are less prone to tears and compression marks than older RealDolls. \n\nThe current incarnation of the female RealDoll product was introduced in 1996. In 2003, Abyss introduced the \"Face-X\" system, allowing any face to be interchangeable with any body. Multiple faces can then be attached one at a time to a single doll by the owner. As of 2011, there are nine female bodies and 16 female faces in the first product line. In 2009 the RealDollÂ 2 was introduced, which feature removable inserts for the mouth and vagina and faces that attach by magnets instead of Velcro. The line started with two female bodies and three female faces and as of 2013 offers ten faces and three bodies. Another female body is in development.\n\n\"Charlie\", the first male RealDoll, was retired in 2008 and replaced with two body types and three head types. \"Transgender\" dolls may also be purchased from the company, although these must be custom ordered. Abyss also sells silicone toys such as female torso products and dildos among other offerings.\n\nFor a time, the company also offered customizations such as robotic hip actuators, finger skeletons (instead of the usual finger wires) and computer-controlled speech feedback, but these expensive options are no longer available.\n\n\n\n"}
{"id": "4418536", "url": "https://en.wikipedia.org/wiki?curid=4418536", "title": "Rebuilt: The Human Body Shop", "text": "Rebuilt: The Human Body Shop\n\n\"Rebuilt: The Human Body Shop\" takes viewers into the world of orthotics and prosthetics.\n\nEach episode features three people in the process of acquiring new limbs and intrudes on their emotional experiences.\n\n\n\n"}
{"id": "58911296", "url": "https://en.wikipedia.org/wiki?curid=58911296", "title": "Rosalind Rickaby", "text": "Rosalind Rickaby\n\nRosalind \"Ros\" Emily Majors Rickaby is a professor of biogeochemistry at the Department of Earth Sciences, University of Oxford and a Governing Body Fellow of Wolfson College.\n\nRickaby received her M.A. in Natural Sciences from Magdalene College, University of Cambridge in 1995 and her Ph.D. from the Universityâs Department of Earth Sciences in 1999, under the supervision of Harry Elderfield. She spent two years as a post-doctoral researcher at the Department of Earth and Planetary Sciences at Harvard University working with with Dan Schrag.\n\nRickaby began as a faculty member of the Department of Earth Sciences at the University of Oxford following her postdoc at Harvard. Her research centers around paleoceanography and biogeochemical cycling in the oceans through deep time, with a focus on using fossil shells of marine micro-organisms as proxies to reconstruct past climate change. Her research group uses a variety of geochemical methods, including the analysis of trace element and isotopic ratios, to understand the biochemical behavior of paleoproxies, such as coccolithophores.She is also an author of the book \"\" along with Bob Williams. \n\n"}
{"id": "41679", "url": "https://en.wikipedia.org/wiki?curid=41679", "title": "Schematic", "text": "Schematic\n\nA schematic, or schematic diagram, is a representation of the elements of a system using abstract, graphic symbols rather than realistic pictures. A schematic usually omits all details that are not relevant to the information the schematic is intended to convey, and may add unrealistic elements that aid comprehension. For example, a subway map intended for passengers may represent a subway station with a dot; the dot does not resemble the actual station at all but gives the viewer information without unnecessary visual clutter. A schematic diagram of a chemical process uses symbols to represent the vessels, piping, valves, pumps, and other equipment of the system, emphasizing their interconnection paths and suppressing physical details. In an electronic circuit diagram, the layout of the symbols may not resemble the layout in the circuit. In the schematic diagram, the symbolic elements are arranged to be more easily interpreted by the viewer.\n\nSchematics and other types of diagrams, e.g.,\nA semi-schematic diagram combines some of the abstraction of a purely schematic diagram with other elements displayed as realistically as possible, for various reasons. It is a compromise between a purely abstract diagram (e.g. the schematic of the Washington Metro) and an exclusively realistic representation (e.g. the corresponding aerial view of Washington).\n\nIn electrical and electronic industry, a schematic diagram is often used to describe the design of equipment. Schematic diagrams are often used for the maintenance and repair of electronic and electromechanical systems. Original schematics were done by hand, using standardized templates or pre-printed adhesive symbols, but today electronic design automation software (EDA or \"electrical CAD\") is often used.\n\nIn electronic design automation, until the 1980s schematics were virtually the only formal representation for circuits. More recently, with the progress of computer technology, other representations were introduced and specialized computer languages were developed, since with the explosive growth of the complexity of electronic circuits, traditional schematics are becoming less practical. For example, hardware description languages are indispensable for modern digital circuit design.\n\nSchematics for electronic circuits are prepared by designers using EDA (electronic design automation) tools called schematic capture tools or schematic entry tools. These tools go beyond simple drawing of devices and connections. Usually they are integrated into the whole IC design flow and linked to other EDA tools for verification and simulation of the circuit under design.\n\nIn electric power systems design, a schematic drawing called a one-line diagram is frequently used to represent substations, distribution systems or even whole electrical power grids. These diagrams simplify and compress the details that would be repeated on each phase of a three-phase system, showing only one element instead of three. Electrical diagrams for switchgear often have common device functions designate by standard function numbers.\n\nSchematic diagrams are used extensively in repair manuals to help users understand the interconnections of parts, and to provide graphical instruction to assist in dismantling and rebuilding mechanical assemblies. Many automotive and motorcycle repair manuals devote a significant number of pages to schematic diagrams.\n\n"}
{"id": "3761693", "url": "https://en.wikipedia.org/wiki?curid=3761693", "title": "Semantic broker", "text": "Semantic broker\n\nA semantic broker is a computer service that automatically provides semantic mapper services. A semantic broker is frequently part of a semantic middleware system that leverage semantic equivalence statements. To qualify as a semantic broker product a system must be able to automatically extract data from a message and use semantic equvalance statements to transform this into another namespace.\n\n"}
{"id": "504188", "url": "https://en.wikipedia.org/wiki?curid=504188", "title": "Settle (furniture)", "text": "Settle (furniture)\n\nA settle is a wooden bench, usually with arms and a high back, long enough to accommodate three or four sitters.\n\nSettles are commonly movable, but occasionally fixed. The settle shares with the chest and the chair the distinction of great antiquity. Its high back was a protection from the draughts of medieval buildings, protection which was sometimes increased by the addition of winged ends or a wooden canopy. It was most frequently placed near the fire in the common sitting-room.\n\nConstructed of oak or other hardwood, it was extremely heavy, solid, and durable. Few English examples of earlier date than the middle of the 16th century are extant; survivals from the Jacobean period are more numerous. Settles of the more expensive type were often elaborately carved or incised; others were divided into plain panels. They were commonly used in farmhouse kitchens or manorial halls. Its vogue did not long outlast the first half of the 18th century.\n\nElaborate specimens of oak settles with very tall backs, sometimes a cupboard built into them, or a box under the seat, are referred to as \"monks settles\", but Frederick Robinson writing in 1905 was of the opinion that none of them were of any great age. Two pre-reformation settles of which he was aware are in Winchester Cathedral. Neither of them contain cupboards or boxes.\n\nRobinson also describes a specimen of a settle and table combination, with a chest in the seat that was made in the 17th century (see right). \"The flanges in the back have a long slit in the lower half, into which fits a peg on the inside of the back of the arm. The back is raised and drawn forwards to serve as a table top as far as the play of the pegs in the slit allows\". This type of furniture is also called a monks bench.\n\nThe English architect and designer William Burges designed the Zodiac settle, made between 1869 and 1870. The settle is painted and illustrated with dancing Zodiac signs, and adorned with inlaid pieces of glass crystal and vellum.\n\n\n\n"}
{"id": "25219833", "url": "https://en.wikipedia.org/wiki?curid=25219833", "title": "Shared transport", "text": "Shared transport\n\nShared transport is a term for describing a demand-driven vehicle-sharing arrangement, in which travelers share a vehicle either simultaneously (e.g. ride-sharing) or over time (e.g. carsharing or bike sharing), and in the process share the cost of the journey, thereby creating a hybrid between private vehicle use and mass or public transport.\n\nShared transport systems include carsharing (also called car clubs in the UK), bicycle sharing (also known as PBS or Public Bicycle Systems), carpools and vanpools (aka ride-sharing or lift-sharing), real-time ridesharing, slugging, casual carpooling, community buses and vans, demand responsive transit (DRT), paratransit, a range of taxi projects and even hitchhiking and its numerous variants. \n\nShared transport is taking on increasing importance as a key strategy for reducing greenhouse gas and other emissions from the transport sector in the face of the global climate emergency by finding ways of getting more intensive use of vehicles on the road.\n\nA somewhat different form of shared transport is the \"shared taxi\", a vehicle which follows a predetermined route and takes anybody waiting for it, more like a bus than a taxi.\n\n\nAuto rickshaws carry people and goods in many developing countries. Also known as a three-wheeler, Samosa, tempo, tuk-tuk, trishaw, auto, rickshaw, autorick, bajaj, rick, tricycle, mototaxi, baby taxi or lapa in popular parlance, they are motorized version of the traditional pulled rickshaw or cycle rickshaw. They are an essential form of urban transport, both as vehicles for hire and for private use, in many developing countries, and a form of novelty transport in many Eastern countries.\n\n"}
{"id": "35896864", "url": "https://en.wikipedia.org/wiki?curid=35896864", "title": "Silent butler", "text": "Silent butler\n\nA silent butler, sometimes called an ash butler, is a small container, often of base metal, sometimes silver or silverplate, with a handle and hinged cover, used for collecting ashes or crumbs. They were more common prior to the modern period, and enjoyed some popularity being made as a home construction project in the US. They are now often considered collector's items, or are valued for their retro appeal.\n"}
{"id": "2597872", "url": "https://en.wikipedia.org/wiki?curid=2597872", "title": "Stamped concrete", "text": "Stamped concrete\n\nStamped concrete is concrete that is patterned and/or textured or embossed to resemble brick, slate, flagstone, stone, tile, wood, and various other patterns and textures. Stamped concrete is commonly used for patios, sidewalks, driveways, pool decks, and interior flooring. The ability of stamped concrete to resemble other building materials makes stamped concrete a less expensive alternative to using those other authentic materials such as stone, slate or brick.\n\nThere are three procedures used in stamped concrete which separate it from other concrete procedures; the addition of a base color, the addition of an accent color, and stamping a pattern into the concrete. These three procedures provide stamped concrete with a color and shape similar to the natural building material. It also is longer-lasting than paved stone, and still resembles the look.\n\nThe base color is the primary color used in stamped concrete. The base color is chosen to reflect the color of the natural building material. The base color is produced by adding a color hardener to the concrete. Color hardener is a powder pigment used to dye the concrete.\n\nThe color hardener can be applied using one of two procedures; integral color or cast-on color. Integral color is the procedure where the entire volume of concrete is dyed the base color. The entire volume of concrete is colored by adding the color hardener to the concrete truck, and allowing all the concrete in the truck to be dyed. Cast-on color is the procedure where the surface of the concrete is dyed the base color. The surface of the concrete is colored by spreading the color hardener onto the surface of the wet concrete and floating the powder into the top layer of the wet concrete.\n\nConcrete can be colored in many ways; color hardener, integral liquid or powder, acid stains to name a few. The process of integrally coloring the concrete offers the advantage of the entire volume being colored; however, the surface strength is not increased as with the use of color hardener. Dry shake color hardener is another popular way to color concrete. You broadcast the hardener on the concrete as soon as it is floated for the first time. After letting the bleed water soak into the hardener you float and trowel it in. This method only covers the surface about 3/16 of an inch but it gives the concrete surface a longer wear life.\n\nThe accent color is the secondary color used in stamped concrete. The secondary color is used to produce texture and show additional building materials (e.g. grout) in the stamped concrete. The accent color is produced by applying color release to the concrete. Color release has two purposes - 1) It is a pigment used to color the concrete and 2) It is a non-adhesive used to prevent the concrete stamps from sticking to the concrete.\n\nThe color release can be applied in one of two procedures based on the two forms it is manufactured in: powdered (cast-on color release made up of calcium-releasing powders that repel water); or liquid - which is a light aromatic-based solvent, spray-on color release. Cast-on color release is a procedure where the powder color release is applied by spreading the color release on the surface of the concrete before the concrete is stamped. Spray-on color release is a procedure where liquid color release is sprayed on the bottom of the concrete stamps before the concrete is stamped.\n\nThe pattern is the shape of the surface of the stamped concrete. The pattern reflects the shape of the natural building material. The pattern is made by imprinting the concrete shortly after it has been poured with a \"concrete stamp\". Most modern concrete stamps are made of polyurethane, but older \"cookie cutter\" style stamps were made of various metals. The old style stamps lacked the capabilities of forming natural stone texture.\n\nConcrete stamping is the procedure which uses the concrete stamps to make the pattern in the stamped concrete. Concrete stamps are placed on the concrete after the color release has been applied. The concrete stamps are pushed into the concrete and then removed to leave the pattern in the stamped concrete.\n\nIn most cases concrete stamping is made to look like ordinary building products such as flagstone, brick, natural stone, etc.\n\nStamping concrete increased in popularity in the 1970s when it was first introduced in the World of Concrete. Builders saw it as a new way to satisfy the customer and make their budget work simultaneously. This technique of stamping concrete has been done since at least the 1950s. When stamping concrete first began, there were very few choices of design and colors. However, as the industry grew more stamping patterns were being created along with many different types of stains. Another advantage to using stamped concrete is that it can be applied to many different surfaces and textures, such as driveways, highways, patios, decks, and even floors inside the home.\n\n"}
{"id": "40376062", "url": "https://en.wikipedia.org/wiki?curid=40376062", "title": "Terra Santa Agro", "text": "Terra Santa Agro\n\nTerra Santa Agro is a publicly traded Brazilian agribusiness company founded in 2003 in Rio de Janeiro as Brasil Ecodiesel, a biodiesel producer. In 2011, after the merger with Maeda Agroindustrial and Vanguarda ParticipaÃ§Ãµes, the company changed its name to Vanguarda Agro and moved its headquarters to Nova Mutum in Mato Grosso. \n\nAs an agricultural commodities producer focused on the production of soybean, corn, and cotton, and on land appreciation, it operates 7 production units strategically located in the Brazilian state of Mato Grosso, and manages a total area of approximately 156.6 thousand hectares.\n\nThe Company also holds equipment and other assets that complement its agricultural operations. These include:\n\n\nThe Companyâs shares are listed on the BM&FBovespaâs \"Novo Mercado\" segment, the highest corporate governance level of Brazilian stock markets.\n"}
{"id": "41880174", "url": "https://en.wikipedia.org/wiki?curid=41880174", "title": "The Brits Who Built the Modern World", "text": "The Brits Who Built the Modern World\n\nThe Brits Who Built the Modern World is a British factual television series that was first broadcast on BBC Four from 13 to 27 February 2014. The three-part series tells the story of British architects Richard Rogers, Norman Foster, Nicholas Grimshaw, Michael Hopkins and Terry Farrell.\n\nThe series was produced with the Open University and a Royal Institute of British Architects exhibition (which also included Lady Patty Hopkins). The exhibition was open between 13 February and 27 May 2014.\n\nThe series was the subject of controversy when the BBC were accused of removing Patty Hopkins from a photograph of the architects, used to illustrate the third programme of the series. Patty Hopkins had been a full partner in the Hopkins company from the outset. The BBC were accused of ignoring women architects, though the BBC responded by saying that at the start of making the series, the director met with both Michael and Patricia Hopkins to discuss their involvement in the series.\n\n"}
{"id": "304688", "url": "https://en.wikipedia.org/wiki?curid=304688", "title": "Video 2000", "text": "Video 2000\n\nVideo 2000 (also known as V2000, with the tape standard Video Compact Cassette, or VCC) is a consumer videocassette system and analogue recording standard developed by Philips and Grundig to compete with JVC's VHS and Sony's Betamax video technologies. Designed for the PAL colour television standard (some models additionally handled SECAM), distribution of Video 2000 products began in 1979 exclusively in Europe, South Africa and Argentina and ended in 1988.\n\nVideo 2000 was presented at the International Radio Exhibition in Berlin in 1979 and succeeded Philips's earlier Video Cassette Recording (VCR) format and its derivatives (VCR-LP and Grundig's SVR). Although some early models and advertising featured a mirror-image 'VCR' badge based on the older systems' logo, Video 2000 was an entirely new (and incompatible) format that incorporated many technical innovations. Despite this, the format was not a major success and was eventually discontinued, having lost out to the rival VHS system in the videotape format war.\n\nPhilips named the videotape standard Video Compact Cassette (VCC) to complement its landmark audio Compact Cassette format introduced in 1963, but the format itself was marketed under the trademark Video 2000.\n\nDespite the name, VCCs are marginally larger than VHS cassettes âshorter, but thicker and deeper. They have two co-planar reels containing half-inch (12.5mm) wide chromium dioxide magnetic tape. The format utilised only half (6.25mm) of the half-inch tape on a given 'side', and so it is occasionally referred to erroneously as a quarter-inch tape format despite its physical tape width. The cassette can then be flipped over to use the other half of the tape, thus doubling playing time.\n\nThe tape is totally enclosed when not in use. Unlike competing formats that have spaces in the cassette for the tape loading mechanism to be inserted, thus exposing the delicate magnetic tape surface, VCCs have a retractable sheath that covers such space. The sheath is retracted as a tape is inserted into the machine and only then can the tape cover be raised to fully expose the tape.\n\nWhile VHS and Beta tapes have a break-off tab to protect recordings from erasure (as in audio Compact Cassettes and, once broken, the cavity left by the missing tab must be covered or filled before the tape can be reused), VCCs employ a reversible solution: a switch on the tape edge can be turned to red/orange to protect the recordings, and back to black/brown (depending on the colour of the cassette housing) to re-record. The switch covers/uncovers a hole along the tape edge which is detected by a sensor in the machine.\n\nThe tape edge features six such holes along each side of the tape, detected by sensors on the cassette's underside. The left-hand cluster includes the write-protection hole. The right-hand cluster of three is used (by various permutations of open/closed status) to tell the machine the total tape running time. This was employed in second generation machines such as Grundig's Video 2x4 Super to provide a real-time tape counter: upon insertion of the tape the machine moves the tape forward and then backward by a small amount and monitors the comparative angular speed of the reels. This is looked up in a data table for the known total tape length and the hours and minutes used are then displayed. A similar technique was later used on Video8, MiniDV and MicroMV cassettes. Some later VHS machines also featured this ability although it did not work with VHS-C cassettes. NOTE: When Grundig began marketing VHS recorders, its VS2XX series machines employed a similar system, whereby barcoded stickers attached to the tape edge indicated the total tape length to the machine so that it could calculate the time used.\n\nA hole between the two spools enables a pin in the VCR to pass right through the cassette. This pin releases ratchets within the cassette that prevent the tape accidentally becoming slack in transit. The VCR's eject function includes a tape tensioning action prior to the cassette being ejected.\n\nDynamic Track Following (DTF) eliminated the need for a separate control track, and enabled the video heads to accurately follow the recorded tracks on the tape during playback. Therefore, by design V2000 machines does not require a video tracking control (however, Grundig's model 1600 lacked DTF). During record, a sequence of four pilot frequencies is recorded with the video signal. During playback, if a video head reads an adjacent track, a voltage of up to Â± 150 volts is applied to the piezo-electric material on which the video heads are mounted. The flexible head mount enables the head to accurately read every track (i.e., no noisebars), at up to seven times normal speed forwards, and fives times in reverse. VHS developers JVC later introduced a so-called \"Dynamic Drum\" in a few top-of-the-range devices.\n\nV2000 is able to play both fields of the image in still frame mode, providing full vertical resolution whereas VHS and Betamax could only reproduce one field, giving only half of the normal vertical resolution. A real advantage of DTF on all but the very first V2000 models is the ability to provide picture search without noise bars across the screen, a feature domestic VHS or Betamax machines were only ever able to approach by introducing complex multi-head drums.\n\nAt the time of its launch, Video 2000 also offered several innovative features unmatched by the competing formats VHS and Betamax:\nLinear stereo sound was available on some models, though both VHS and Betamax were offering hifi stereo sound with near-CD quality by the mid 1980s.\n\nAfter displaying their VR2000 prototype at trade shows and to the media Philips released the first Video 2000 VCR, the VR2020, in the United Kingdom in 1979. Philips models were re-badged as Pye and ITT, amongst others, and even re-skinned as Bang & Olufsen, whilst Grundig models were re-badged as Siemens.\n\nA key intention of the V2000 format, thanks to DTF, was tape compatibility. A tape from any machine should play perfectly on any other machine. Unfortunately, when the VR2020 reached the shops it was discovered that its audio head was 2.5mm out of position compared to that on Grundig's Video 2x4. This meant that the sound would be out of sync with the picture when played back on the other type of machine. Both manufacturers' production lines hastily moved the audio head 1.25mm to a common position, but compatibility issues remained for recordings made on the first generation of machines. Furthermore, the required close tolerances and fragility of the DTF system resulted in significant inter-machine compatibility issues which were never fully resolved.\n\nFurthermore, there were initially some compatibility issues with the tape interchange, allegedly even with the lap synchrony, if cassettes recorded with Philips equipment were to be played on Grundig machines.\n\nAlthough Philips and Grundig agreed on a common tape format, they came up with machines that were radically different mechanically. Building on its experience with DVD And VCR, Grundig machines featured a Betamax-style loading ring to gently pull the tape around the video heads in a 'U-wrap' over all three generations of their recorders, which was effective, simple and economical.\n\nPhilips, however, used an 'M-wrap' similar to that in VHS machines. Cables pulled pins along tracks in order to pull the tape into the transport path. This drive was large, heavy and very complex to produce, although having the head drum, capstan, audio/erase heads and the respective drive motors all mounted on a common base plate meant it was produced to high tolerances. Philips referred to this assembly as the âmicroworldâ. Second and third generation Philips drives replaced the cable-pulled pins with a reliable lever mechanism.\n\nIn addition, first and second generation Grundig recorders had a high failure rate due to thin-walled plastic connectors between the drive and the motor connection board.\n\nCompared to VHS, production costs were a big problem with Video 2000. For their second-generation series Philips developed a completely new drive, with the tape pulled against the heads in an formation by pins on lever arms. This drive was very compact, with very high quality and (compared to VHS) very fast response times, but still had five direct-drive motors (head drum, capstan, two belt drive and a threading and cassette compartment). Due to their compact nature, these last Philips drives were quite expensive to service but defects in the mechanism are quite rare, except for the rubber pressure roller which is easy to change.\n\nOther improvements include reduced outer dimensions and weight and addition of a SCART audio/video connector.\n\nPhilips and Grundig intended Video 2000 to improve on the perceived failings of the VHS and Betamax formats whilst providing the potential for further developments. However, the format was withdrawn before many of these possibilities appeared on the market.\n\nThe prototype Video Mini Cassette was a compact version of the VCC (analogous to VHS-C) that was playable in existing machines using a full-sized cassette adaptor. Published photos clearly show the nomenclature VMC120, suggesting that 60 minutes per side were possible (compared to 20 minutes total initially for VHS-C), but Philips retired Video 2000 before the development was ready for market. The 108x72x21mm Video Mini Cassette was somewhat larger than VHS-C's (92x59x23mm). However, the cassette - as well as the adaptor - left the tape noticeably exposed to mishandling.\n\nHifi sound was never marketed although photos of the Philips VR2870 were published in 1985, shortly before the format's demise. This would have recorded pulse-code modulated (PCM) audio in the data track, offering the format another advantage over VHS/Beta as the hifi track would be independent of the visuals, and so could be re-recorded or dubbed as became possible later with Video8.\n\nRumours also circulated in the press of an auto-reverse machine shortly before the format was retired. Technically this would have been a major challenge to enable a single head drum to scan both 'sides' of the tape at the correct angle.\n\nAlongside the write-protect hole were two that were never used. One was slated to indicate the tape formulation as higher coercivity tapes were to be introduced for the 'Super 2000' hi-band version of the format. The flexibility of this system also allowed for metal tape to be introduced for the digital version 'Digital 2000', also in the early stages of development as the format was canceled. Internal documents suggested the cassette abbreviations VSC and VDC to be used, respectively, for the two developments.\n\nAlthough Video 2000 was technologically superior to the competition in several ways, it could not compete with VHS and Betamax's key advantages:\nAdditionally, Video 2000 never achieved the picture quality (optimally adjusted devices) of the predecessor systems VCR, VCR-LP or SVR because of the video writing speed.\n\nIn 1985 Grundig started development and production of VHS recorders alongside V2000, and in 1986 Philips announced that it would discontinue the manufacturing of Video 2000 recorders, focusing instead on VHS exclusively. Their first home-grown VHS recorder - VR6560 - was virtually a clone of the VR2324, using the VHS format tapes and the usual VHS M-wrap. Curiously, when Philips launched its second-generation home-grown VHS recorders (VR6467, VR6760, etc.) they pioneered the VHS 'U-wrap' (known colloquially as the \"Charly\" deck) and this was used in many Philips-built machines well into the first half of the 1990s. Grundig also used U-wrap in its own VHS decks for a short while before using Panasonic-manufactured decks.\n\nRecorders in the format were manufactured by Philips and Grundig and marketed additionally by Pye, ITT, Bang and Olufsen, Aristona, Erres, Radiola, Siera and Siemens.\n\n\n"}
{"id": "4764556", "url": "https://en.wikipedia.org/wiki?curid=4764556", "title": "W86", "text": "W86\n\nThe W86 was an American thermonuclear warhead with earth-penetrating characteristics (a nuclear bunker buster), intended for use on the Pershing II IRBM missile.\n\nThe W86 design was canceled in September 1980 when the Pershing II missile mission shifted from destroying hardened targets to targeting soft targets at greater range. The W85 warhead, which had been developed in parallel with the W86, was used for all production Pershing II missiles.\n\nPublished design details on the W86 are vague; it has been stated that the W86 and W85 were yet more derivatives of the B61 bomb family (along with the W80, W81, and W84). All of those weapons have a maximum diameter of roughly . The later B61-11 bomb is another earth-penetrating bomb type.\n\n\n\n"}
{"id": "28774446", "url": "https://en.wikipedia.org/wiki?curid=28774446", "title": "Water Data Transfer Format", "text": "Water Data Transfer Format\n\nWater Data Transfer Format (WDTF) is a data delivery standard implemented by the Australian Bureau of Meteorology (BoM) that was jointly developed with the CSIRO. The standard, released in 2009, specifies both the format of and the techniques used to deliver Australian water data measurements to the BoM. Some private organisations and government agencies in Australia that collect water data are mandated to deliver it to the BoM according to the (Australian) Water Act 2007.\n\nAn external meteorological data source that delivers data in WDTF-compliant forms is the CSIRO Land & Water's Automatic Weatherstation Network. Data from this weatherstation network can be viewed in a web browser, downloaded at text values in CSV format, downloaded in a condensed XML format for machine-to-machine communications, or downloaded as WDTF-compliant data.\nThe use of WDTF is an example of work in the field of Irrigation Informatics.\n\n"}
