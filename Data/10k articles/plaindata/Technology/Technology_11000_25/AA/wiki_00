{"id": "6362086", "url": "https://en.wikipedia.org/wiki?curid=6362086", "title": "Advanced Wireless Services", "text": "Advanced Wireless Services\n\nAdvanced Wireless Services (AWS) is a wireless telecommunications spectrum band used for mobile voice and data services, video, and messaging. AWS is used in the United States, Argentina, Canada, Colombia, Mexico, Chile, Paraguay, Peru, Ecuador, Uruguay and Venezuela. It replaces some of the spectrum formerly allocated to Multipoint Multichannel Distribution Service (MMDS), sometimes referred to as Wireless Cable, that existed from 2150 to 2162 MHz.\n\nThe AWS band uses microwave frequencies in several segments: from 1695 to 2200 MHz. The service is intended to be used by mobile devices such as wireless phones for mobile voice, data, and messaging services. Most manufacturers of smartphone mobile handsets provide versions of their phones that include radios that can communicate using the AWS spectrum. Though initially limited, device support for AWS has steadily improved the longer the band has been in general use, with most high-end and many mid-range handsets supporting it over HSPA, LTE, or both.\n\nThe AWS band defined in 2002 (AWS-1), used microwave frequencies in two segments, from 1710 to 1755 MHz for uplink, and from 2110 to 2155 MHz for downlink. The service is intended to be used by mobile devices such as wireless phones for mobile voice, data, and messaging services. Most manufacturers of smartphone mobile handsets provide versions of their phones that include radios that can communicate using the AWS spectrum. Since for downlink AWS uses a subset of UMTS frequency band I (2100 MHz) some UMTS2100 capable handsets do detect AWS networks but cannot register on them due to the difference in uplink frequencies (1710–1755 MHz for AWS vs. 1920–1980 MHz for UMTS2100).\n\nThough initially limited, device support for AWS has steadily improved the longer the frequency has been in general use, with most high-end and many mid-range handsets supporting it over HSPA, LTE, or both. In Canada, almost all available LTE handsets support AWS as it was the first frequency over which LTE was offered there, and was still the most commonly supported frequency for LTE in Canada as of 2014-08-21.\n\nIn 2012 the [FCC] released rules for the 'H' block (AWS-2), covering the frequencies 1915-1920 MHz and 1995-2000 MHz.\n\nIn 2013 they regulated the AWS-3 Block, covering bands 1695-1710 MHz, 1755-1780 MHz and 2155-2180 MHz.\n\nIn 2012 there was a proposal regarding the AWS-4 Block, which regulated use of 2000-2020 MHz and 2180-2200 MHz. These were initially proposed for use with the Mobile Satellite System (MSS), but later more uses were introduced\n\nIn Canada, Industry Canada held the auction for AWS spectrum in 2008. Freedom Mobile (formerly Wind Mobile) had licensed AWS spectrum in every province, and began offering voice and data services on December 16, 2009. Its Saskatchewan and Manitoba spectrum was later sold off to Sasktel and MTS, respectively. Freedom only operates in British Columbia, Alberta and Ontario, although they have roaming agreements with Rogers, Telus and Bell at extra cost.\n\nMobilicity also used the AWS spectrum and began offering services in May 2010, operating in similar areas as Wind but with a smaller network footprint. Its AWS network was combined with Rogers when the latter company acquired Mobilicity in 2015.\n\nQuebecor licensed AWS spectrum throughout the province of Quebec and began offering service with its Vidéotron Mobile brand on September 9, 2010.\n\nShaw Communications licensed AWS spectrum in western Canada and northern Ontario, began to build some infrastructure for providing wireless phone service, but subsequently decided to cancel further development and did not launch this service. The licenses were eventually sold to Rogers, with some transferred to Wind. Shaw re-entered the mobile services market when it acquired Wind Mobile in 2016.\n\nHalifax-based EastLink obtained licenses in eastern Canada, with a small amount of spectrum bought in Ontario and Alberta, and is currently building up infrastructure to launch mobile phone and data services in Nova Scotia and PEI in 2012. This Service has since launched and is available in numerous markets around Atlantic Canada with roaming through Rogers and Bell.\n\nRogers Wireless, Bell Mobility, Telus Mobility, SaskTel, and Manitoba Telecom Services (MTS) all received licenses for AWS spectrum, which they are now using for their LTE networks. Freedom Mobile has subsequently refarmed some spectrum in their UMTS network and deployed LTE on bands 4, and 66.\n\nIn the United States, the service is administered by the Federal Communications Commission. The licenses were broken up into 6 blocks (A-F). Block A consisted of 734 Cellular Market Areas (CMA). Blocks B and C were each divided into 176 Economic Areas (EA), sometimes referred to as BEA by the FCC. Blocks D, E, and F were each broken up into 12 Regional Economic Area Groupings (REAG), sometimes referred to as REA by the FCC.\nBidding for this new spectrum started on August 9, 2006 and the majority of the frequency blocks were sold to T-Mobile USA to deploy their 3G wireless network in the United States. This move effectively killed the former MMDS and/or Wireless Cable service in the United States.\n\nThe following mobile network operators are known to use AWS. Indicated in the list are the launch dates and city.\n\n\n\nPrimary network\n\nPrimary network and LTE\n\nLTE only\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "40706", "url": "https://en.wikipedia.org/wiki?curid=40706", "title": "Alarm sensor", "text": "Alarm sensor\n\nIn telecommunication, the term alarm sensor has the following meanings: \n\n1. In communications systems, a device that can sense an abnormal condition within the system and provide a signal indicating the presence or nature of the abnormality to either a local or remote alarm indicator, and (b) may detect events ranging from a simple contact opening or closure to a time-phased automatic shutdown and restart cycle. \n\n2. In a physical security system, an approved device used to indicate a change in the physical environment of a facility or a part thereof. \n\n3. In electronic security systems, a physical device or change/presence of any electronic signal/logic which causes trigger to electronic circuit to perform application specific operation. In electronic alarm systems the use of this trigger event done by such devices is to turn on the alarm or siren producing sound and/or perform a security calling through telephone lines.\n\n\"Note:\" Alarm sensors may also be redundant or chained, such as when one alarm sensor is used to protect the housing, cabling, or power protected by another alarm sensor.\n\nSource: from Federal Standard 1037C and from MIL-STD-188 and from TRISHAM Software Systems\n"}
{"id": "1989889", "url": "https://en.wikipedia.org/wiki?curid=1989889", "title": "Amkor Technology", "text": "Amkor Technology\n\nAmkor Technology, Inc. is a semiconductor product packaging and test services provider. The company has been headquartered in Tempe, Arizona, since 2005, when it was moved from West Chester, Pennsylvania, United States. The company was founded in 1969 and, as of 2017, had approximately 29,300 employees worldwide and a reported $4.19 billion in sales.\n\nWith factories in China, Japan, Korea, Malaysia, Philippines, Portugal and Taiwan, Amkor is a leading player in the semiconductor industry. It packages and tests integrated circuits (ICs) for chip manufacturers.\n\nIn February 2016, Amkor fully acquired J-Devices Corp., the largest outsourced semiconductor assembly and test provider in Japan. \n\nAmkor Technology has competitiveness for chip assembly by thermal compression as well as wafer level packaging. In September 2018, Amkor Technology opened manufacturing and test plant at Longtan Science Park in Taiwan.\n\n"}
{"id": "35888911", "url": "https://en.wikipedia.org/wiki?curid=35888911", "title": "Arctic (company)", "text": "Arctic (company)\n\nArctic GmbH, formerly known as Arctic Cooling, is a Swiss-founded manufacturer of computer cooling components, mainly CPU and graphics card coolers, case fans and thermal compound. Since 2010, Arctic expanded its business by starting a range of products to cater other consumer demands beyond that of computer cooling hardware. Nowadays, Arctic also offers various consumer products—spanning audio, home entertainment and computer peripherals. In 2012, Arctic was nominated as one of the finalists in the annual PCR Awards.\n\nFounded in 2001, Arctic has offices in Germany, Hong Kong and the United States and cooperates with different production facilities in China. Arctic products are distributed worldwide through distributors, dealers and retailers. The United States, United Kingdom and Germany are Arctic’s major markets. The company has also collaborated with leading graphics card brands such as HIS, Inno3D, PowerColor VTX3D and Sapphire in the development of OEM cooling equipment.\n\nIn 2001, Arctic Cooling was founded in Switzerland by Magnus Huber and Gebhard Scherrer. As the company name suggested, in the past, the business focused entirely on computer cooling solutions. Today, in order to expand the business into other areas especially in consumer electronics, by 2010, it began to develop a diverse range of products that spans beyond cooling into computer peripherals, audio products and home entertainment PCs. For this reason, Arctic Cooling was changed to Arctic in 2010. Since 17.11.2015 Arctic Switzerland AG is in liquidation.\n\nBeing the company's original focus, Arctic designs and manufactures cooling products for computer hardware. with broad compatibility. The company owned a number of patents for its fan and cooler designs as well as for special technologies used in the air coolers including PWM sharing, low noise impeller, cross blow and anti-vibration technologies.\n\nFreezer\n\nFreezer is a trademark of ARCTIC for its line of CPU coolers. It includes both the air cooler based on a heatpipe architecture as well as the water cooling solutions.\n\nAlpine\n\nAlpine is a trademark of ARCTIC for its line of CPU coolers based on Aluminum extrusion heatsink. It includes both active and passive coolers.\n\n\"Accelero\" is a trademark of ARCTIC (formerly Arctic Cooling) for its line of graphics card coolers. The Accelero line of coolers are targeted to high-end graphics cards based on GPUs from Nvidia and AMD. The Accelero series utilizes different types of cooling technologies namely air cooling, passive cooling as well as air and liquid combined cooling to offer different options for different customers. In 2006, Accelero X1 and Accelero X2 are the first VGA coolers introduced in the series by the manufacturer. The Accelero series has collaborated in a number of OEM projects with motherboard and video card manufacturers to develop customized graphics card cooling solutions. ARCTIC is the first video card cooler manufacturer to use a copper base for their heatsinks.<br>\nIn May 2012, ARCTIC released the Accelero Hybrid, which is claimed to be the world’s first graphics card cooler with integrated air and liquid solution in the market.\n\nGraphics card coolers are generally served as an upgrade or replacement for the stock cooler in order to reduce noise, temperature and enhance the overclocking capability of the GPU. ARCTIC’s heatsinks are claimed to provide quiet, high performance cooling, which also makes the Accelero series one of the most popular graphics card coolers in the market.\n\nAmong the company's array of thermal compound, the MX-4 received the Top Product Award from the German magazine PC Games Hardware.\n\n\nFusion is Arctic's brand name for various cooling and data storage products, including the Fusion 550- EU, Fusion 550RF, Fusion 550R, Fusion 550F and Fusion 1TB (external hard drive), and the Fusion 1TB data storage device.\n\nArctic started to develop its audio products such as speakers, headphones and headsets since 2010 and it has expanded to wireless audio system near the end of 2011.\n\nIn June 2011, Arctic entered the HTPC market with its first mini HTPC, MC001 Entertainment Center Series, which was first introduced in Computex 2011. In 2012, Arctic introduced more advanced models with MC101 Series, AMD Trinity-powered HTPCs which are aimed for multimedia users. The MC101 Series features AMD Trinity A8/A10 APU, AMD Radeon HD 3D graphics, up to 1TB hard disk storage, SSD, up to 8GB DDR3 memory and built-in TV tuner.\n\nOn top of the entertainment centers, Arctic offers as well an audio gateway that works as a Windows Media Center Extender : Audio Relay. It is not DLNA certified but is compatible with the protocol.\n\nDue to limited commercial success, this product line has been discontinued.\n\nArctic offers a selection of computer peripherals including keyboards, mice, USB fans, etc. In the end of 2011, the brand started to offer Apple accessories.\n\nThe Power series offers various USB travel adapters, car chargers and batteries.\n\nThe company has claimed several trademarks and patents for the name and technology applied to their products. Some of the air coolers and case fans produced by the company feature a patent design of the fan holder to achieve vibration absorption and elimination of the buzzing sound when the fan is running. The Freezer 7 Pro features 4 rubber connectors which serve as a vibration damper to absorb the vibration of the running fan and prevent the vibration from transferring to the heatsink and the case. The Arctic F Pro PWM employs the same technology to absorb vibration and prevent it from transmitting across other components within the case.\n\nArctic claims to be the patent holder of the PWM Sharing Technology, namely PST, which shares a single PWM signal with all the other PWM controlled devices connected to the motherboard to control all fan speeds and enhance the noise level according to the load.\n\nThe Freezer 13 PRO CO employs the patented Cross-Blow technology by the use of an extra fan installed at the bottom of the heatsink to give a boost of cooling performance to the surrounding components, including Northbridge and voltage regulators.\n\nThe company utilizes its patented passive cooling technology (DE 20200600) in the Accelero S1 PLUS to enhance the level of natural convection from the GPU by letting more air to pass through the aluminum fins so that heat will be dissipated more efficiently.\n\nSome of the key products including Freezer, Accelero, Alpine, Fusion and Silentium series are registered trademarks in the EU and the US.\n\nIn 2011, Arctic has started to engage its end consumers through the means of social media (e.g. Facebook ) to reinforce the brand’s awareness. The company was also a platinum partner with PCR Retail Boot Camp – a new conference and expo for the UK PC and IT channel.\n\nArctic also produces cooling solutions for several graphic card manufactures; in most cases improving cooling beyond the OEM cooler. These include but are not limited to:\n\n\nDue to the strategic mistake of bundleing the MC001 with Microsoft Windows 7 and the high price the MC001 was very badly sold. To increase sales, on 5 February 2013, Arctic announced their new partnership with OpenELEC. Arctic worked with OpenELEC together and combined a fully passive cooled Entertainment system - the MC001 media centre (US and EU version) equipped with the latest XBMC 12 (OpenELEC 3.0) platform. Arctic and OpenELEC were planning on their next release, aimed to provide a more dedicated builds for the Arctic MC001 systems. Shortly after partnershipping with OpenELEC, the development of passively cooled media centers was abandoned.\n\nArctic was reportedly planning to file a lawsuit against Advanced Micro Devices (\"AMD\") for the infringement of its trademark \"Fusion\", the name that AMD used to describe its series of APUs which integrate x86 processing cores with Radeon stream processors on the same piece of silicon. In light of the lawsuit, AMD has announced earlier in 2012 its plans to drop its Fusion branding in favor of the Heterogeneous Systems Architecture (HSA). On 23 January 2013, Arctic announced that the company and AMD have arrived at a mutual agreement in settling the \"Fusion\" trademark dispute without any disclosure of the terms.\n"}
{"id": "198669", "url": "https://en.wikipedia.org/wiki?curid=198669", "title": "Avid Technology", "text": "Avid Technology\n\nAvid Technology (often known as Avid, stylized as ▲▼❚▶) is an American technology and multimedia company founded in August 1987 by Bill Warner, based in Burlington, Massachusetts. It specializes in audio and video; specifically, digital non-linear editing (NLE) systems, management and distribution services.\n\nAvid products are now used in the television and video industry to create television shows, feature films, and commercials. Media Composer, a professional software-based non-linear editing system, is Avid's flagship product.\n\nAvid was founded by a marketing manager from Apollo Computer, Bill Warner, a prototype of their first digital nonlinear editing system (the Avid/1) was shown in a private suite at the National Association of Broadcasters (NAB) convention in April 1988. The Avid/1 was based on an Apple Macintosh II computer, with special hardware and software of Avid's own design installed.\n\nAt the NAB show in April 1989, the Avid/1 was publicly introduced. It was \"the biggest shake-up in editing since Melies played around with time and sequences in the early 1900s\". By the early 1990s, Avid products began to replace such tools as the Moviola, Steenbeck, and KEM flatbed editors, allowing editors to handle their film creations with greater ease. The first feature film edited using the Avid was \"Let's Kill All the Lawyers\" in 1992, directed by Ron Senkowski. The film was edited at 30fps NTSC rate, then used Avid MediaMatch to generate a negative cutlist from the EDL. The first feature film edited natively at 24fps with what was to become the Avid Film Composer was \"Emerson Park\". The first studio film to be edited at 24fps was \"Lost in Yonkers\", directed by Martha Coolidge. By 1994 only three feature films used the new digital editing system. By 1995 dozens had switched to Avid, and it signaled the beginning of the end of cutting celluloid. In 1996 Walter Murch accepted the Academy Award for editing \"The English Patient\" (which also won best picture), which he cut on the Avid. This was the first Editing Oscar awarded to a digitally edited film (although the final print was still created with traditional negative cutting).\n\nIn 1994 Avid introduced Open Media Framework (OMF) as an open standard file format for sharing media and related metadata. In recent years the company has extended its business expertise through several acquisitions and internal investments towards the full palette of multimedia generation products including those to store and manage media files. In 2006 Avid launched new products such as Avid Interplay and Unity Isis. Avid used to be considered just a \"video editing\" company, but now has consolidated a well-rounded multimedia generation technology company.\n\nIn the past, Avid has released home versions of their professional line of editors, such as Xpress DV and lower cost professional versions such as Xpress Pro. Additionally, Avid Free DV was available as a free download, providing an introduction to the Media Composer interface, but in a limited version. All of these have now been discontinued as the core Media Composer product has been lowered in price and is now heavily discounted for academic/student use.\n\nOn March 29, 1999, Avid Technology, Inc. adjusted the amount originally allocated to IPR&D and restated its third quarter 1998 consolidated financial statements accordingly, considering the SEC's views.\n\nIn 1993, the National Academy of Television Arts & Sciences awarded Avid Technology and all of the company's initial employees with a technical Emmy award for Outstanding Engineering Development for the Avid Media Composer video editing system.\n\nOn March 21, 1999, at the 71st Academy Awards, Avid Technology Inc. was awarded an Oscar for the concept, system design and engineering of the Avid Film Composer for motion picture editing which was accepted by founder Bill Warner.\n\n"}
{"id": "428558", "url": "https://en.wikipedia.org/wiki?curid=428558", "title": "Bicycle pump", "text": "Bicycle pump\n\nA bicycle pump is a type of positive-displacement air pump specifically designed for inflating bicycle tires. It has a connection or adapter for use with one or both of the two most common types of valves used on bicycles, Schrader or Presta. A third type of valve called the Dunlop (or Woods) valve exists, but tubes with these valves can be filled using a Presta pump.\n\nSeveral basic types are available:\n\nIn its most basic form, a bicycle pump functions via a hand-operated piston. During up-stroke, this piston draws air through a one-way valve into the pump from outside. During down-stroke, the piston then displaces air from the pump into the bicycle tire. Most floor pumps, also commonly called track pumps, have a built-in pressure gauge to indicate tire pressure.\n\nElectrically-operated pumps intended to inflate car tires (as available in most service stations) can in principle be used to inflate a bicycle tire if the right type of connection is available. Some such pumps are designed to cut off before a suitable pressure (much higher for a bicycle than a car tire), and will much under inflate the tire. Others may not cut off, but deliver a high rate of flow to fill the larger car tire, with a risk of over inflating and bursting a bicycle tire unless it is stopped with split-second timing.\n\nInflating tubeless tires requires an initial surge of air to seat the bead, and specialized pumps are available specifically for this task.\n\nIt is not known when the first bicycle pump was invented, but it is believed to have been in or around 1887, which is when the first inflatable tire or pneumatic tire was produced by John Boyd Dunlop of Scotland. The first bicycle pump consisted of a metal cylinder that had a metal rod running down the middle of it. This would have forced the air out of the cylinder and then sucked in new air when the metal rod was pulled up again. Many modern pumps use a very similar method, while some, such as the electric pumps, use an automated pumping mechanism.\n\nThe bicycle pump compresses air. When the cylinder is compressed, air is pushed down the tube of the pump and then into the tire via the valve, which is forced open by the pressure of the air. When the handle is pulled up again, the valve shuts off automatically so that the air cannot escape from the tire, and new air is forced back into the cylinder so the process can be repeated. Some pumps have a gauge that shows the pressure of air that is forced into the tire. Once the tire is at the correct pressure, the pump valve can be removed from the tire, and the cap can be replaced on the tire valve.\n\nThere are two main types of tire valves to which the bicycle tire pump attaches. These are the Presta valve and the Schrader valve. Some pumps fit both types of valves, whereas others do not, but adapters are available that enable the pump to fit any type of valve. \"All valves adjustable connecting systems,\" also known as AVACS, enable the pump to fit any type of valve found on a bicycle, and it also has the capacity to fit onto other universal inflatable products, such as balls, paddling pools, and rubber rings. The AVACS feature is commonly available on pump models and can also be bought as a separate valve attachment.\nIt also works by repeatedly pulling and pushing activities of the Piston.\n\nThere are three main types of bicycle pumps\n\n\nAlso known as a floor or track pump. To operate, the user rests the base of the pump on the floor, resting feet at the base, and pulls and pushes full strokes with handles. An additional tube must connect the pump to the fill valve, which may create dead volume.\n\nThere are two basic types: tubed and integral. The tubed type requires a separate tube to connect the pump to the valve. These have the advantage that they are cheap, but are inefficient compared to other pumps. They also have a lot of joints from which air can escape.\n\nIntegral pumps have a hole in the side with a rubber washer that fits round the valve. This is frequently compressed on to the valve by an extra lever. Because it is well sealed, rigid and has little dead volume, this type of pump is very efficient. An 8\" integral will typically pump faster than an 18\" tubed. These type of pumps will run the risk of shredding the tube valves, especially on those with presta valves.\n\nA simple pump has a cupped fiber or plastic piston. On the forward stroke the air pushes the sides of the cup against the cylinder, so forming a seal; it provides its own valve. Then this piston can push the air out of the hole at the far end.\n\nSome of the most efficient pumps are double action pumps. By sealing the piston in the cylinder at both ends they can force air into the tire on both strokes.\n\nPumps can be fitted to a bracket on the bike frame, either a clamp on, screw on, or a braze-on peg, or they can be carried in a pannier or other bag on the bike, or they can be carried by the rider in a backpack, pocket, etc.\n\nMini pumps (or compact pumps) are usually hand pumps that have been made very small and light, so that they can readily be carried on a bicycle for emergency use; they can fit in pockets, saddle bags, or even include a water bottle bracket. Because of their small size, the volume of air that these pumps can provide is somewhat limited compared to a floor pump, so quite a lot of pumping may be needed.\n\nThese pumps are often not specifically designed for bicycle use. They do not generate very high pressures so do not work well for narrow road-bike tires, but are fine for large low-pressure tires as found on mountain bikes.\n\nBecause they are designed for cars they fit Schrader valves. If the bicycle has Presta valves a small brass reducer is required in order to use the pump.\n\nGas-filled cylinders for bicycles have an unclear history but appear to have appeared between the two World Wars. One story says they were made \"by a rider after watching a café proprietor charge up the glass of beer he had ordered from a bottle of carbon dioxide.\"\n\nThe pumps generally used liquefied gas that could not be replaced at home. A later version, which had more success, used a cartridge sold originally for drinks siphons. A lever broke the cartridge and enough gas escaped to inflate a tire.\n\nModern gas pumps are often used by mountain bike or road bike racers who need to save weight, and to save time if they puncture during a race. They can be a one-time pump or a pump that can be fitted with a replacement cartridge. Most pumps use carbon dioxide and standard-threaded 16g CO canisters. Carbon dioxide leaks out of a rubber inner tube more rapidly than air - despite its larger size, the CO molecule is slightly soluble in rubber and a tire may go flat within a few days.\n\nTwelve-volt air compressors made for automobile tires are also compatible with bicycle tires. A portable jump-starter for automobiles can sometimes be used to power these types of pumps. Even non-standard do-it-yourself (DIY) 12-volt electric systems that are primarily for bicycle lighting are sometimes used to power these pumps when cigar lighter receptacles are installed. A main advantage to electric pumps is that recent ones take up less space than hand or foot pumps, which makes them suitable for well-equipped bicycles with DIY 12-volt electric systems to use when basket space is of the essence.\n\nThe pressure rating of tires is usually stamped somewhere on the sidewall. This may be in psi (pounds per square inch) or bar. The pressure rating could be indicated as \"Maximum Pressure,\" or \"Inflate to . . . \" and will usually give a range (for example, 90-120 psi, or 35-60 psi). Inflating to the lower number in the pressure range will increase traction and make the ride more comfortable. Inflating to the higher number will make the ride more efficient and will decrease the chances of getting a flat tire but a firmer ride must be expected.\n\nThe maximum pressure, or how much air the pump can force into a tire, is an important consideration. The pump needs to match or exceed the stated air pressure the tires can handle. If the maximum air pressure is too low, it will not be able to adequately inflate the tires, no matter how hard it is used.\n\n"}
{"id": "2681207", "url": "https://en.wikipedia.org/wiki?curid=2681207", "title": "BiteStrip", "text": "BiteStrip\n\nThe BiteStrip is a disposable self-use home test for Sleep Bruxism. Its indications correlate well with comparable indications from formal sleep lab studies.\n\nThe device is a miniature but complete sleep EMG (electromyography) monitor, including two pre-gelled EMG electrodes, an amplifier, a micro-processor-based real time data acquisition and analysis hardware and software, and a permanent chemical display unit. The entire system is integrated on a small piece of lightweight plastic film attached to the user’s cheek. By analyzing the jaw muscles’ EMG waveforms in real time during the night and presenting the result on the built-in display, the device doesn’t need a large data memory, and the downloading and analysis phases, common to all current sleep recorders, are eliminated. Test results indicate the number of bruxing events detected per hour of sleep (bruxing index or BI). This indication appears as permanent numbers encoded onto the electro-chemical display for easy reading. The device itself can serve as the medical record of the test. Self-test letters indicate technical and clinical validity of the study.\n\nSleep Bruxism (SB) is a serious medical disorder, characterized by involuntary grinding and clenching of teeth during sleep. It is often accompanied by unpleasant grinding sounds heard by the bed-partner or roommate. Symptoms include wearing of teeth, temporomandibular joint (TMJ) dysfunction or pain, chewing difficulties, headaches and daytime sleepiness. The prevalence of SB is estimated at 14%-20% in children and 8% in adults. Diagnosis of SB is usually based on clinical examination and patient history. However, none of the signs and symptoms may be considered conclusive. Another alternative has been to send the patient to a sleep lab for an overnight test.\n"}
{"id": "47104487", "url": "https://en.wikipedia.org/wiki?curid=47104487", "title": "Boynton Bicycle Railroad", "text": "Boynton Bicycle Railroad\n\nThe Boynton Bicycle Railroad was a monorail in Brooklyn on Long Island, New York. It ran on a single load-bearing rail at ground level, but with a wooden overhead stabilising rail engaged by a pair of horizontally opposed wheels. The railway operated for only two years, but the design was adopted elsewhere.\n\nThe concept was invented by E. Moody Boynton, who hoped that this would eventually replace the conventional rail road, because it was cheaper to build and could be used for a double track on the space available for a conventional single track right of way.\n\nAccording to the \"Scientific American\" of 28 March 1891, the steam locomotive and cars were in regular and continuous operation for passenger service during several weeks in the summer of 1890. The service was provided between the Gravesend and Coney Island areas of Brooklyn, on an abandoned section of an old standard gauge track of the Sea Beach and Brighton Railroad. The first locomotive weighed nine tons, and had two 10 by 12 inch cylinders, the piston rods of both being connected with cranks on each side the single 6 foot driving wheel, and the front of the locomotive being also supported by two 38 inch pony wheels, one behind the other. These wheels had double flanges, to contact with either side of the track rail, as also had similarly arranged pairs of 38 inch wheels arranged under and housed in the floors near each end of the passenger cars.”\n\nA heavier locomotive was especially designed for this method of traction, and built for use on a street railway. It weighed 16 tons and had a pair of 5 foot drivers. The crank was only seven inches in length, and the engine was designed to readily make 600 revolutions a minute, and maintain a speed of 100 miles an hour with a full train of passenger cars.\n\nIn a true line with, and fifteen feet directly above, the face of the track rail was the lower face of a guide rail, supported from posts arranged along the side of the track, and on the sides of this guide rail run pairs of rubber-faced trolley wheels attached to the top of the locomotive and the cars. The guide rail was a simple stringer of yellow pine, 4¼ by 8 inches in section, and the standards on which the wheels are journaled were placed far enough apart to allow a space of 6 inches between the continuous faces of each pair of wheels, thus affording 1¾ inches for lateral play, or sidewise movement toward or from the guard rail, it being designed that the guide rail shall be arranged in the exact line of the true center of gravity of the cars and locomotive. The standards were bolted to six-inch wide strap iron attached to and extending across the top of the car.\n\nThe switching arrangement was remarkably simple. In addition to an ordinary track switch, in which, however, the switch bar is made to throw only one rail, a connection was made by means of a vertical rod and upper switch bar with a shifting section of the guide rail, whereby, on the moving of the track rail and the setting of the signal, the guide rail was simultaneously moved, the adjustment being effected and both being locked in position according to the methods usual in ordinary railway practice.\n\nThe passenger cars were each two stories in height, each story being divided lengthwise into nine separate compartments, each of which seated comfortably four passengers, thus providing seats for 72 passengers in each car. Each compartment had its own sliding door, and all the doors on the same floor of the car were connected by rods at the top and bottom with a lever in convenient reach of the brakeman, by whom the doors are all opened and closed simultaneously. The compartments were each four feet wide and five feet long, the seats facing each other. Only one rail of the old single track was used, as only one guide rail had been erected, except at the ends of the route, for switching purposes, but the width of the cars and motor was such that it only required the erection of another guide rail, for the utilizing of the other track rail, to form a regular double-track road of the Boynton pattern.\n\nThe section of railroad on which this system has been operated was only 2 miles long, in which distance the curves were considerable, but, although they were mostly in one direction, the indications of wear upon the traction wheels, and upon the guide rail and trolley wheels, were hardly perceptible. During a portion of the season, when the summer travel to Coney Island was at its height, trains were run on regular schedule time, 50 three-car-trains daily each way, carrying from one to three hundred passengers per trip.\n\nAt least four different locomotive designs for the Boynton Bicycle Railroad were produced but it is unclear how many were actually built. \nThe freight locomotive resembles a Double Fairlie.\n\n\n"}
{"id": "20077025", "url": "https://en.wikipedia.org/wiki?curid=20077025", "title": "Carrier-to-noise-density ratio", "text": "Carrier-to-noise-density ratio\n\nIn satellite communications, carrier-to-noise-density ratio (C/N) is the ratio of the carrier power \"C\" to the noise power density \"N\", expressed in dB-Hz.\nWhen considering only the receiver as a source of noise, it is called carrier-to-receiver-noise-density ratio.\n\nIt determines whether a receiver can lock on to the carrier and if the information encoded in the signal can be retrieved, given the amount of noise present in the received signal. The carrier-to-receiver noise density ratio is usually expressed in dBHz.\n\nThe noise power density, \"N\"=\"kT\", is the receiver noise power per hertz, which can be written in terms of the Boltzmann constant \"k\" (in joules per kelvin) and the noise temperature \"T\" (in kelvins). \n\n\n"}
{"id": "43454774", "url": "https://en.wikipedia.org/wiki?curid=43454774", "title": "Coldharbour Mill Working Wool Museum", "text": "Coldharbour Mill Working Wool Museum\n\nColdharbour Mill, near the village of Uffculme in Devon, England, is one of the oldest woollen textile mills in the world, having been in continuous production since 1797. The mill was one of a number owned by Fox Brothers, and is designated by English Heritage as a Grade II* listed building.\n\nColdharbour Mill can be found just off junction 27 of the M5 motorway near the village of Uffculme, and near to the border with Somerset. The headquarters for the mill was at Tonedale in Wellington. The water provided by the nearby River Culm was a prime factor in Thomas Fox's decision to purchase the existing grist mill. In 1797 he wrote to his brother \"I have purchased the premises at Uffculme for eleven hundred guineas, which I do not think dear as they include about fifteen acres of very fine meadow land. The buildings are but middling, but the stream good.\"\nThe roads in the area at the time were very poor, and finished cloth had to be carried by pack horses to the nearby ports of Topsham and Exeter, or by carrier's cart to Bridgwater, Bristol and London (a twelve-day journey).\n\nIt appears that there has been a mill of some description near the Coldharbour site since Saxon times. The Domesday Book recording two mills in the Uffculme area.\n\nAt its peak the company employed approximately 5,000 people and owned and operated nine mills and factories in Somerset, Devon, and Oxfordshire. One of the most notable satellite mills was that of William Bliss & Sons, built in 1872 after a disastrous fire in the original mill. Located in Chipping Norton, the William Bliss site was one of the grandest mills in England, complete with reading room, chapel and workers cottages. Fox Brothers bought it in 1920. \nThe main Tonedale site in Wellington was the largest integrated mill site in the South West of England, covering 10 acres of land and forming the hub of the Fox Brothers woollen manufacturing 'empire'. It is believed to have been the only 'Twin Vertical Woollen Factory' in the world - that is, making both worsted and woollen products, and controlling the entire process from fleece to finished cloth in-house.\n\nThe ancestors of the mill owners, the Fox family (no relation to George Fox, founder of the Religious Society of Friends, or Quakers) and the Were family, were early Quaker converts. During George Fox's first visit to Devonshire in 1655, he went to the house of Nicholas Tripe and his wife, who became 'convinced'. Their daughter, Anstice, married George Croker of Plymouth, and they were much persecuted for their beliefs. Their daughter Tabitha married Francis Fox of St. Germans, Cornwall, a serge maker. The family remained in Cornwall, becoming merchants and shipping agents, and in 1745 the grandson of Francis and Tabitha, Edward Fox of Eggeshall near Wadebridge, married Anne Were, the daughter of a Wellington sergemaker, Thomas Were. (In 1749, Edward's cousin, George Croker Fox, married Mary Were, the sister of Anne). Thomas Were was a very successful manufacturer, and had inherited the WRE trademark, which certified the quality of his cloth. His great-great grandfather John Were of Pinksmoor was credited with owning a fulling mill. During one of the visits of Edward to his father-in-law, it was suggested that one of Edward and Anne's sons should join the Wellington woollen manufacturing business. After four years study overseas, Edward's son Thomas Fox moved to Wellington, and became a partner of Were and Company in 1772, aged 25. Thomas and his wife Sarah Smith, built in 1801, then lived in, Tone Dale House, Wellington - the house is still lived in by a Fox, five generations later, by Ben and Victoria Fox. In 1826, when his sons were partners (the Weres having relinquished their shares), the business was renamed Fox Brothers.\n\nThe family were prominent in local affairs, and subscribed £1,044 5s 6d in shares in the Grand Western Canal between 1809 and 1813, Thomas having considered the original proposal of 1792 with considerable reticence: \"People hereaway seem now as much too eager to engage in Canals as they have been too backward for many years. The almost incredible sum of £900,000 was lately subscribed at Wells in about two hours for cutting one from Taunton to Bristol. Whilst this delyrium continues the writer is neither disposed to subscribe himself nor to recommend his friends doing it, as he doubts whilst such money pours in on them in such abundance it may be badly husbanded.\"\n\nIn 1787, Were and Company ran short of ready cash, and decided to print their own bank notes - effectively \"promises to pay\". On 30 October, Thomas printed 500 notes of five guineas each. The notes were well received by local businesses. In 1797, an invasion scare resulted in a shortage of gold and cash, and Thomas Fox issued 3,000 five guinea notes, and seventy six £20 notes in order to enable his business to continue its expansion.\nThe Fox, Fowler and Company bank eventually had over fifty branches in the West Country, and was authorised to issue its own bank notes until 1921, the year it was taken over by Lloyd's Bank - itself founded by a Quaker, Sampson Lloyd. One of the original £5 notes is on display at Tone Dale House, the family home which Thomas Fox built, in 1801. \n\nExeter was the centre of the mediaeval woollen trade in England, with cloth being exported to the Continental markets of France, Holland and Germany. Kersey, a sturdy cloth, was superseded by serge, so that by 1681 95% of the Exeter cloth export was serge. As already mentioned, the Were family were major suppliers of serge cloth to the Continent, especially Holland. Whilst using the ports of London and Bristol as well, Topsham was a major port for the Were export trade. We have a contemporary description of the Exeter trade in serge by Celia Fiennes (1662–1741): \n\nHowever, the French Revolution and the invasion of Flanders in 1793 caused very serious difficulties for Exeter cloth merchants, and in 1794 the Weres were forced to cancel major worsted yarn orders.\nNapoleon's Italian campaigns of 1796-7 closed the Italian market for English cloth, and then Spain entered the war as an ally of France, leading to the confiscation of Exeter cloth. Only six vessels cleared from the Exe with cloth in 1797, and two in 1798; a far cry from the 1768 despatch of 330,414 pieces of cloth. Some Exeter merchants, such as Barings, moved to London - the Weres changed production to \"long ells\", a fine white serge, for the East India Company. A letter from Thomas Fox to Green and Walford, factors, is to be found in the Fox Brothers Letter Book archive: \nWith the termination of the monopoly of the East India Company's charter in 1833 through the Government of India Act 1833 (3 & 4 Will. 4 c. 85), the trade in long ells to China declined, and Thomas Fox developed the production of flannel, which was sold in the home market and to America. Following his Quaker beliefs, Thomas Fox refused to sell flannel to the East India Company when he heard it would be used in the manufacture of cartridges.\nIn 1881, as a result of losses in the First Boer War, a Parliamentary Commission sought to equip the army with khaki uniform. Fox Brothers decided to bid for the contract, reasoning that the new contract for 5,000 puttees would save lives, as well as create employment. Fox Brothers went on to be the major producer of puttees, manufacturing some 850 miles of the cloth in World War One.\n\nIn support of the flannel emphasis, in 1865 Coldharbour Mill moved over to producing worsted yarn rather than woollen yarn. This necessitated the need for more power to drive new combing machines. (Worsted yarn is made from sheep with long hair fleeces and the wool has to be combed to ensure that all the fibres are parallel.)\n\nColdharbour Mill classifies itself as \"a working wool museum\" and as such runs its museum machinery to demonstrate how woollen products were made. The demonstration products (including worsted yarn, tartan cloth,and rugs) are made available for sale. The mill has four registered tartans - Devon Original (1284), Devon Companion (1283), Somerset (831), and Blackdown Hills (6711).\n\nEnglish Heritage wrote a Historic Buildings Report (B/065/2001) about the mill complex, and described the site as \"probably one of the best-preserved textile mill complexes in the country. It retains the full range of buildings and power system features which characterised the development of the 19th century textile mill with much of the machinery that was used at the site in the 20th century.\"\n\nColdharbour Mill was primarily always used for the production of wool yarn for the weaving frames of the Wellington mill. The original grist mill was probably a three storey building, and the original sale notice of 1788 states \"The Stream divides in two Parts, one at each End of the House, and they are in a Manner two separate Mills, under the same Roof\". A legal dispute of 1834 contains a detailed map of the water courses, which are in their existing positions, with a leat to the front and rear of the grist mill.\n\nThe foundations of the main mill were mentioned in a letter of 15 April 1799, stating that they were 50 feet from the grist mill - further away than the current building, but at a point where the wall is thicker today. The mill building of 39 feet wide by 123 feet long was very large for its time. Thomas Fox wrote to his machinery supplier describing how the new mill building was to operate: \nAn inventory of 1802 suggests that spinning was to be carried out on hand-powered spinning jennies, with a waterwheel (costing £450) powering the carding machines. By 1816 the mill contained worsted spinning frames, and in 1822 a new waterwheel costing £1,500 had been installed.\n\nThe main mill building was expanded at various times, with a two story extension added to the north; a fireproof stone staircase to the east; a wheelhouse; a fourth floor to the main building; and an adjacent combing shed built over the tail race leat flowing from the waterwheel.\n\nSteam power came to the mill in three major building phases. In 1865 a beam engine house was built, together with a boiler house and the first chimney on the site. In the 1890s a second beam engine arrived, the boiler house was extended and the existing chimney built, and then in 1910 the existing horizontal engine was fitted, the Green's economiser house added and the boiler house extended again.\nThe mill site contains a wide variety of subsidiary buildings, including stabling, a linhay, a gas retort house (see below), a carpenter's workshop, an air raid shelter from WWII, worker's cottages, and the manager's house. The tail race which takes the water away from the waterwheel is unusual in that it runs under the combing shed in a wide culvert, before briefly reappearing and then running underground again for some 200 metres.\n\nColdharbour Mill is unusual to have used both water and steam power right up to the time of its demise as a commercial venture. The water power was believed to have been used for the night shift up until 1978.\n\nThe English Heritage report states \"it is possible that the wheel pit and parts of the wheel itself are the remnants of the new wheel which was recorded in the Stock Book of 1822...and should be considered of considerable historic significance\". The cast and wrought iron high breast-shot wheel is 18 feet in diameter by 14 feet wide, with 48 buckets. It is part of a very unusual survival of a combined water and steam powered drive system, as it continued to be used after the 1910 addition of the horizontal steam engine, and the drive mechanism is still in place. The wheel is turning most days.\n\nThomas Fox's brother Edward was part owner of a Cornish mine, and was instrumental in installing an early example of a Boulton and Watt engine. Edward told Thomas about this new technology, with the result that James Watt was invited to Wellington in 1782, just six months after his patent for the sun and planet gear that allowed reciprocating motion. Thomas missed the meeting, but wrote to him afterwards: \nHowever, the visit came to nothing, partly due to the very high cost of coal, and partly due to the discontent about mechanisation, which would culminate in the later Luddite Movement. A letter in 1785 from Thomas states \nThus it was that the first mechanised wool spinning machinery, purchased from Backhouse of Darlington, were powered by horses. These arrived in September 1791, bringing the Industrial Revolution to the West Country. It was evident from a letter of 1786 that Thomas wished he could have had a cheap source of coal, as in those parts of the country fed by canals: \n\nAlthough the Wellington mill purchased a steam engine for £90 (and a £20 boiler) in 1840, Coldharbour Mill did not get a steam engine until 1865, by which time the Bristol and Exeter Railway was supplying cheap coal to Tiverton Junction. The mill has two Lancashire boilers in the boiler house, only one of which is still operational. Initially a 25 hp beam engine was installed, followed by a second beam engine in the 1890s (possibly 1896). A Pollit & Wigzell 300 hp, cross compound engine superseded the beam engines in 1910 and continued in use, along with the water wheel, until Fox Brothers closed the mill in April 1981. Today the cross compound steam engine remains fully operational and runs regularly at steam up weekends. It drives the shafting on all five floors of the mill via an operational rope drive. In 1993 a salvaged 1867 Kittoe and Brotherhood beam engine was painstakingly restored and installed at the mill, in one of the disused beam engine sheds. \n\nThe mill contains a number of other steam powered exhibits, including a working Ashworth fire pump, already at Coldharbour, but repaired in 1984 with components from Bliss Mill; a very rare example of a low pressure wagon boiler dating from the late 1700s; and a (non-operational) steam powered flue fan.\n\nColdharbour Mill also had a small water turbine for electricity generation, which used the 14 foot head of water between the upper leat and the tail race. No references have been found to it in any register of Devon hydro-electric schemes, and it was unlikely to have generated any more than 3 kW peak power. The exit is visible today in the tail race leat, but nothing else is believed to exist.\n\nTwo generators were installed in the beam engine house after the beam engines were removed and these were driven by the Pollit and Wigzell engine. The flat belt pulley system is still in existence. It is believed that this was used for lighting rather than running the machines.\n\nColdharbour Mill generated its own coal gas on site for lighting the mill (and thus enabling the machinery to be run all night). Although the retorts have been removed and disposed of, the Gas Retort house which housed the retort bench is still standing. In fact, the original gas retorts have been discovered in the leat, where they served as weir components. English Heritage classifies the late 19th century Gas Retort House as \"a very rare survival\" of gas-making facilities.\n\nAt the time of its closure in April 1981, Coldharbour Mill still had its textile machinery in position. The majority of these machines have been preserved (though not all are exhibited), and have been augmented with weaving machines rescued from the closure of the Tonedale site. The lowest part of the site, the level 1 combing shed, dealt with the initial cleaning and combing of the unwashed wool. The process involved a number of separate stages, each with a specialised machine. The eight opening gill machines (made by Taylor Wadsworth & Co)opened up the fleeces and prepared the wool for washing in a large back-washer with steam heated rollers. Following the washing, further gill boxes produced successively combed fibres, which were passed to a circular Noble combing machine. This machine separated the fibres into long \"Tops\" and the short poor quality fibres. Although these machines are preserved on site, they are no longer in use today. British wool tops are purchased in, dyed into standard colours, and then up to ten strands of tops are fed into the Intersecting Gill Box (manufactured by Prince Smith and Stells in 1959). The gill box starts the process of drawing out the fibres, and also enables new colours to be created by blending together the standard colours. The output of the gill box is termed a sliver. This particular machine has a mechanism to ensure the weight of the sliver is constant, which is important to ensure the final yarn thickness is constant. \nThe next process is to draw the slivers out further, and to give the fibres a small twist to strengthen the resulting slubbing such that it can be wound onto a bobbin. At Coldharbour Mill, this is demonstrated on a Price Smith and Stells draw box of 1959. The bobbins from this machine are then placed in a further draw box by Prince Smith and Stells, this time an 1898 machine, and the thread from a pair of bobbins is drawn out to a seventh of its diameter, and given a light twist. If this output is to be used for Aran yarn production, it is termed a roving, and is sent on to the spinning frame. However, if the slubbing is for double knitting yarn, the slubbing must go through another reduction on a draw box.\n\nThe museum is owned and run by a not-for-profit charitable trust, Registered Charity No. 1123386. It has a number of educational programmes for schools including Victorian Drama; Materials & Fibres; and Britain at War.\n\nThe mill is home to a number of other exhibits:\n\n"}
{"id": "750772", "url": "https://en.wikipedia.org/wiki?curid=750772", "title": "Cooling tower", "text": "Cooling tower\n\nA cooling tower is a heat rejection device that rejects waste heat to the atmosphere through the cooling of a water stream to a lower temperature. Cooling towers may either use the evaporation of water to remove process heat and cool the working fluid to near the wet-bulb air temperature or, in the case of \"closed circuit dry cooling towers\", rely solely on air to cool the working fluid to near the dry-bulb air temperature.\n\nCommon applications include cooling the circulating water used in oil refineries, petrochemical and other chemical plants, thermal power stations and HVAC systems for cooling buildings. The classification is based on the type of air induction into the tower: the main types of cooling towers are natural draft and induced draft cooling towers.\n\nCooling towers vary in size from small roof-top units to very large hyperboloid structures (as in the adjacent image) that can be up to tall and in diameter, or rectangular structures that can be over tall and long. The hyperboloid cooling towers are often associated with nuclear power plants, although they are also used in some coal-fired plants and to some extent in some large chemical and other industrial plants. Although these large towers are very prominent, the vast majority of cooling towers are much smaller, including many units installed on or near buildings to discharge heat from air conditioning.\n\nCooling towers originated in the 19th century through the development of condensers for use with the steam engine. Condensers use relatively cool water, via various means, to condense the steam coming out of the cylinders or turbines. This reduces the back pressure, which in turn reduces the steam consumption, and thus the fuel consumption, while at the same time increasing power and recycling boiler-water. However the condensers require an ample supply of cooling water, without which they are impractical. The consumption of cooling water by inland processing and power plants is estimated to reduce power availability for the majority of thermal power plants by 2040–2069. While water usage is not an issue with marine engines, it forms a significant limitation for many land-based systems.\n\nBy the turn of the 20th century, several evaporative methods of recycling cooling water were in use in areas lacking an established water supply, as well as in urban locations where municipal water mains may not be of sufficient supply; reliable in times of demand; or otherwise adequate to meet cooling needs. In areas with available land, the systems took the form of cooling ponds; in areas with limited land, such as in cities, they took the form of cooling towers.\n\nThese early towers were positioned either on the rooftops of buildings or as free-standing structures, supplied with air by fans or relying on natural airflow. An American engineering textbook from 1911 described one design as \"a circular or rectangular shell of light plate—in effect, a chimney stack much shortened vertically (20 to 40 ft. high) and very much enlarged laterally. At the top is a set of distributing troughs, to which the water from the condenser must be pumped; from these it trickles down over \"mats\" made of wooden slats or woven wire screens, which fill the space within the tower.\"\n\nA hyperboloid cooling tower was patented by the Dutch engineers Frederik van Iterson and Gerard Kuypers in 1918. The first hyperboloid cooling towers were built in 1918 near Heerlen. The first ones in the United Kingdom were built in 1924 at Lister Drive power station in Liverpool, England, to cool water used at a coal-fired electrical power station.\n\nAn HVAC (heating, ventilating, and air conditioning) cooling tower is used to dispose of (\"reject\") unwanted heat from a chiller. Water-cooled chillers are normally more energy efficient than air-cooled chillers due to heat rejection to tower water at or near wet-bulb temperatures. Air-cooled chillers must reject heat at the higher dry-bulb temperature, and thus have a lower average reverse-Carnot cycle effectiveness. In areas with a hot climate, large office buildings, hospitals, and schools typically use one or more cooling towers as part of their air conditioning systems. Generally, industrial cooling towers are much larger than HVAC towers.\n\nHVAC use of a cooling tower pairs the cooling tower with a water-cooled chiller or water-cooled condenser. A \"ton\" of air-conditioning is defined as the removal of 12,000 BTU/hour (3500 W). The \"equivalent ton\" on the cooling tower side actually rejects about 15,000 BTU/hour (4400 W) due to the additional waste heat-equivalent of the energy needed to drive the chiller's compressor. This \"equivalent ton\" is defined as the heat rejection in cooling 3 US gallons/minute (1,500 pound/hour) of water 10 °F (6 °C), which amounts to 15,000 BTU/hour, assuming a chiller coefficient of performance (COP) of 4.0. This COP is equivalent to an energy efficiency ratio (EER) of 14.\n\nCooling towers are also used in HVAC systems that have multiple water source heat pumps that share a common piping \"water loop\". In this type of system, the water circulating inside the water loop removes heat from the condenser of the heat pumps whenever the heat pumps are working in the cooling mode, then the externally mounted cooling tower is used to remove heat from the water loop and reject it to the atmosphere. By contrast, when the heat pumps are working in heating mode, the condensers draw heat out of the loop water and reject it into the space to be heated. When the water loop is being used primarily to supply heat to the building, the cooling tower is normally shut down (and may be drained or winterized to prevent freeze damage), and heat is supplied by other means, usually from separate boilers.\n\nIndustrial cooling towers can be used to remove heat from various sources such as machinery or heated process material. The primary use of large, industrial cooling towers is to remove the heat absorbed in the circulating cooling water systems used in power plants, petroleum refineries, petrochemical plants, natural gas processing plants, food processing plants, semi-conductor plants, and for other industrial facilities such as in condensers of distillation columns, for cooling liquid in crystallization, etc. The circulation rate of cooling water in a typical 700 MW coal-fired power plant with a cooling tower amounts to about 71,600 cubic metres an hour (315,000 US gallons per minute) and the circulating water requires a supply water make-up rate of perhaps 5 percent (i.e., 3,600 cubic metres an hour).\n\nIf that same plant had no cooling tower and used once-through cooling water, it would require about 100,000 cubic metres an hour A large cooling water intake typically kills millions of fish and larvae annually, as the organisms are impinged on the intake screens. A large amount of water would have to be continuously returned to the ocean, lake or river from which it was obtained and continuously re-supplied to the plant. Furthermore, discharging large amounts of hot water may raise the temperature of the receiving river or lake to an unacceptable level for the local ecosystem. Elevated water temperatures can kill fish and other aquatic organisms (see \"thermal pollution\"), or can also cause an increase in undesirable organisms such as invasive species of zebra mussels or algae. A cooling tower serves to dissipate the heat into the atmosphere instead and wind and air diffusion spreads the heat over a much larger area than hot water can distribute heat in a body of water. Evaporative cooling water cannot be used for subsequent purposes (other than rain somewhere), whereas surface-only cooling water can be re-used.\nSome coal-fired and nuclear power plants located in coastal areas do make use of once-through ocean water. But even there, the offshore discharge water outlet requires very careful design to avoid environmental problems.\n\nPetroleum refineries also have very large cooling tower systems. A typical large refinery processing 40,000 metric tonnes of crude oil per day ( per day) circulates about 80,000 cubic metres of water per hour through its cooling tower system.\n\nThe world's tallest cooling tower is the tall cooling tower of Kalisindh Thermal Power Station in Jhalawar, Rajasthan, India.\n\nThese types of cooling towers are factory preassembled, and can be simply transported on trucks, as they are compact machines. The capacity of package type towers is limited and, for that reason, they are usually preferred by facilities with low heat rejection requirements such as food processing plants, textile plants, some chemical processing plants, or buildings like hospitals, hotels, malls, automotive factories etc.\n\nDue to their frequent use in or near residential areas, sound level control is a relatively more important issue for package type cooling towers.\n\nFacilities such as power plants, steel processing plants, petroleum refineries, or petrochemical plants usually install field erected type cooling towers due to their greater capacity for heat rejection. Field erected towers are usually much larger in size compared to the package type cooling towers.\n\nA typical field erected cooling tower has a pultruded fiber-reinforced plastic (FRP) structure, FRP cladding, a mechanical unit for air draft, drift eliminator, and fill.\n\nWith respect to the heat transfer mechanism employed, the main types are:\n\nIn a wet cooling tower (or open circuit cooling tower), the warm water can be cooled to a temperature \"lower\" than the ambient air dry-bulb temperature, if the air is relatively dry (see dew point and psychrometrics). As ambient air is drawn past a flow of water, a small portion of the water evaporates, and the energy required to evaporate that portion of the water is taken from the remaining mass of water, thus reducing its temperature. Approximately 970 BTU of heat energy is absorbed for each pound of evaporated water (2 MJ/kg). Evaporation results in saturated air conditions, lowering the temperature of the water processed by the tower to a value close to wet-bulb temperature, which is lower than the ambient dry-bulb temperature, the difference determined by the initial humidity of the ambient air.\n\nTo achieve better performance (more cooling), a medium called \"fill\" is used to increase the surface area and the time of contact between the air and water flows. \"Splash fill\" consists of material placed to interrupt the water flow causing splashing. \"Film fill\" is composed of thin sheets of material (usually PVC) upon which the water flows. Both methods create increased surface area and time of contact between the fluid (water) and the gas (air), to improve heat transfer.\n\nWith respect to drawing air through the tower, there are three types of cooling towers:\n\nHyperboloid (sometimes incorrectly known as hyperbolic) cooling towers have become the design standard for all natural-draft cooling towers because of their structural strength and minimum usage of material. The hyperboloid shape also aids in accelerating the upward convective air flow, improving cooling efficiency. These designs are popularly associated with nuclear power plants. However, this association is misleading, as the same kind of cooling towers are often used at large coal-fired power plants as well. Conversely, not all nuclear power plants have cooling towers, and some instead cool their heat exchangers with lake, river or ocean water.\n\nThermal efficiencies up to 92% have been observed in hybrid cooling towers.\n\nTypically lower initial and long-term cost, mostly due to pump requirements.\nCrossflow is a design in which the air flow is directed perpendicular to the water flow (see diagram at left). Air flow enters one or more vertical faces of the cooling tower to meet the fill material. Water flows (perpendicular to the air) through the fill by gravity. The air continues through the fill and thus past the water flow into an open plenum volume. Lastly, a fan forces the air out into the atmosphere.\n\nA \"distribution\" or \"hot water basin\" consisting of a deep pan with holes or \"nozzles\" in its bottom is located near the top of a crossflow tower. Gravity distributes the water through the nozzles uniformly across the fill material.\n\nAdvantages of the crossflow design:\n\nDisadvantages of the crossflow design:\n\nIn a counterflow design, the air flow is directly opposite to the water flow (see diagram at left). Air flow first enters an open area beneath the fill media, and is then drawn up vertically. The water is sprayed through pressurized nozzles near the top of the tower, and then flows downward through the fill, opposite to the air flow.\nAdvantages of the counterflow design:\n\nDisadvantages of the counterflow design:\n\nCommon aspects of both designs:\n\nBoth crossflow and counterflow designs can be used in natural draft and in mechanical draft cooling towers.\n\nQuantitatively, the material balance around a wet, evaporative cooling tower system is governed by the operational variables of make-up volumetric flow rate, evaporation and windage losses, draw-off rate, and the concentration cycles.\n\nIn the adjacent diagram, water pumped from the tower basin is the cooling water routed through the process coolers and condensers in an industrial facility. The cool water absorbs heat from the hot process streams which need to be cooled or condensed, and the absorbed heat warms the circulating water (C). The warm water returns to the top of the cooling tower and trickles downward over the fill material inside the tower. As it trickles down, it contacts ambient air rising up through the tower either by natural draft or by forced draft using large fans in the tower. That contact causes a small amount of the water to be lost as windage/drift (W) and some of the water (E) to evaporate. The heat required to evaporate the water is derived from the water itself, which cools the water back to the original basin water temperature and the water is then ready to recirculate. The evaporated water leaves its dissolved salts behind in the bulk of the water which has not been evaporated, thus raising the salt concentration in the circulating cooling water. To prevent the salt concentration of the water from becoming too high, a portion of the water is drawn off/blown down (D) for disposal. Fresh water make-up (M) is supplied to the tower basin to compensate for the loss of evaporated water, the windage loss water and the draw-off water.\n\nUsing these flow rates and concentration dimensional units:\nA water balance around the entire system is then:\n\nSince the evaporated water (E) has no salts, a chloride balance around the system is:\n\nand, therefore:\n\nFrom a simplified heat balance around the cooling tower:\n\nWindage (or drift) losses (W) is the amount of total tower water flow that is entrained in the flow of air to the atmosphere. From large-scale industrial cooling towers, in the absence of manufacturer's data, it may be assumed to be:\n\nCycle of concentration represents the accumulation of dissolved minerals in the recirculating cooling water. Discharge of draw-off (or blowdown) is used principally to control the buildup of these minerals.\n\nThe chemistry of the make-up water, including the amount of dissolved minerals, can vary widely. Make-up waters low in dissolved minerals such as those from surface water supplies (lakes, rivers etc.) tend to be aggressive to metals (corrosive). Make-up waters from ground water supplies (such as wells) are usually higher in minerals, and tend to be scaling (deposit minerals). Increasing the amount of minerals present in the water by cycling can make water less aggressive to piping; however, excessive levels of minerals can cause scaling problems.\n\nConcentration cycles in the majority of cooling towers usually range from 3 to 7. In the United States, many water supplies use well water which has significant levels of dissolved solids. On the other hand, one of the largest water supplies, for New York City, has a surface rainwater source quite low in minerals; thus cooling towers in that city are often allowed to concentrate to 7 or more cycles of concentration.\n\nSince higher cycles of concentration represent less make-up water, water conservation efforts may focus on increasing cycles of concentration. Highly treated recycled water may be an effective means of reducing cooling tower consumption of potable water, in regions where potable water is scarce.\n\nSurfaces with any visible biofilm (i.e., slime) should be cleaned.\n\nDisinfectant and other chemical levels in cooling towers and hot tubs should be continuously maintained and regularly monitored.\n\nRegular checks of water quality (specifically the aerobic bacteria levels) using dipslides should be taken as the presence of other organisms can support legionella by producing the organic nutrients that it needs to thrive.\n\nBesides treating the circulating cooling water in large industrial cooling tower systems to minimize scaling and fouling, the water should be filtered to remove particulates, and also be dosed with biocides and algaecides to prevent growths that could interfere with the continuous flow of the water. Under certain conditions, a biofilm of micro-organisms such as bacteria, fungi and algae can grow very rapidly in the cooling water, and can reduce the heat transfer efficiency of the cooling tower. Biofilm can be reduced or prevented by using chlorine or other chemicals. A normal industrial practice is to use two biocides, such as oxidizing and non-oxidizing types to complement each other's strengths and weaknesses, and to ensure a broader spectrum of attack. In most cases, a continual low level oxidizing biocide is used, then alternating to a periodic shock dose of non-oxidizing biocides.\n\nAnother very important reason for using biocides in cooling towers is to prevent the growth of \"Legionella\", including species that cause legionellosis or Legionnaires' disease, most notably \"L. pneumophila\", or \"Mycobacterium avium\". The various \"Legionella\" species are the cause of Legionnaires' disease in humans and transmission is via exposure to aerosols—the inhalation of mist droplets containing the bacteria. Common sources of \"Legionella\" include cooling towers used in open recirculating evaporative cooling water systems, domestic hot water systems, fountains, and similar disseminators that tap into a public water supply. Natural sources include freshwater ponds and creeks.\n\nFrench researchers found that \"Legionella\" bacteria travelled up to through the air from a large contaminated cooling tower at a petrochemical plant in Pas-de-Calais, France. That outbreak killed 21 of the 86 people who had a laboratory-confirmed infection.\n\nDrift (or windage) is the term for water droplets of the process flow allowed to escape in the cooling tower discharge. Drift eliminators are used in order to hold drift rates typically to 0.001–0.005% of the circulating flow rate. A typical drift eliminator provides multiple directional changes of airflow to prevent the escape of water droplets. A well-designed and well-fitted drift eliminator can greatly reduce water loss and potential for \"Legionella\" or water treatment chemical exposure.\n\nThe CDC does not recommend that health-care facilities regularly test for the \"Legionella pneumophila\" bacteria. Scheduled microbiologic monitoring for \"Legionella\" remains controversial because its presence is not necessarily evidence of a potential for causing disease. The CDC recommends aggressive disinfection measures for cleaning and maintaining devices known to transmit \"Legionella\", but does not recommend regularly-scheduled microbiologic assays for the bacteria. However, scheduled monitoring of potable water within a hospital might be considered in certain settings where persons are highly susceptible to illness and mortality from \"Legionella\" infection (e.g. hematopoietic stem cell transplantation units, or solid organ transplant units). Also, after an outbreak of legionellosis, health officials agree that monitoring is necessary to identify the source and to evaluate the efficacy of biocides or other prevention measures.\n\nStudies have found \"Legionella\" in 40% to 60% of cooling towers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnder certain ambient conditions, plumes of water vapor (fog) can be seen rising out of the discharge from a cooling tower, and can be mistaken as smoke from a fire. If the outdoor air is at or near saturation, and the tower adds more water to the air, saturated air with liquid water droplets can be discharged, which is seen as fog. This phenomenon typically occurs on cool, humid days, but is rare in many climates. Fog and clouds associated with cooling towers can be described as homogenitus, as with other clouds of man-made origin, such as contrails and ship tracks.\n\nThis phenomenon can be prevented by decreasing the relative humidity of the saturated discharge air. For that purpose, in hybrid towers, saturated discharge air is mixed with heated low relative humidity air. Some air enters the tower above drift eliminator level, passing through heat exchangers. The relative humidity of the dry air is even more decreased instantly as being heated while entering the tower. The discharged mixture has a relatively lower relative humidity and the fog is invisible.\n\nWhen wet cooling towers with seawater make-up are installed in various industries located in or near coastal areas, the drift of fine droplets emitted from the cooling towers contain nearly 6% sodium chloride which deposits on the nearby land areas. This deposition of sodium salts on the nearby agriculture/vegetative lands can convert them into sodic saline or sodic alkaline soils depending on the nature of the soil and enhance the sodicity of ground and surface water. The salt deposition problem from such cooling towers aggravates where national pollution control standards are not imposed or not implemented to minimize the drift emissions from wet cooling towers using seawater make-up.\n\nRespirable suspended particulate matter, of less than 10 micrometers (µm) in size, can be present in the drift from cooling towers. Larger particles above 10 µm in size are generally filtered out in the nose and throat via cilia and mucus but particulate matter smaller than 10 µm, referred to as PM, can settle in the bronchi and lungs and cause health problems. Similarly, particles smaller than 2.5 µm, (PM), tend to penetrate into the gas exchange regions of the lung, and very small particles (less than 100 nanometers) may pass through the lungs to affect other organs. Though the total particulate emissions from wet cooling towers with fresh water make-up is much less, they contain more PM and PM than the total emissions from wet cooling towers with sea water make-up. This is due to lesser salt content in fresh water drift (below 2,000 ppm) compared to the salt content of sea water drift (60,000 ppm).\n\nAt some modern power stations equipped with flue gas purification, such as the Großkrotzenburg Power Station and the Rostock Power Station, the cooling tower is also used as a flue-gas stack (industrial chimney), thus saving the cost of a separate chimney structure. At plants without flue gas purification, problems with corrosion may occur, due to reactions of raw flue gas with water to form acids.\n\nSometimes, natural draft cooling towers are constructed with structural steel in place of concrete (RCC) when the construction time of natural draft cooling tower is exceeding the construction time of the rest of the plant or the local soil is of poor strength to bear the heavy weight of RCC cooling towers or cement prices are higher at a site to opt for cheaper natural draft cooling towers made of structural steel.\n\nSome cooling towers (such as smaller building air conditioning systems) are shut down seasonally, drained, and winterized to prevent freeze damage.\n\nDuring the winter, other sites continuously operate cooling towers with water leaving the tower. Basin heaters, tower draindown, and other freeze protection methods are often employed in cold climates. Operational cooling towers with malfunctions can freeze during very cold weather. Typically, freezing starts at the corners of a cooling tower with a reduced or absent heat load. Severe freezing conditions can create growing volumes of ice, resulting in increased structural loads which can cause structural damage or collapse.\n\nTo prevent freezing, the following procedures are used:\n\nCooling towers constructed in whole or in part of combustible materials can support internal fire propagation. Such fires can become very intense, due to the high surface-volume ratio of the towers, and fires can be further intensified by natural convection or fan-assisted draft. The resulting damage can be sufficiently severe to require the replacement of the entire cell or tower structure. For this reason, some codes and standards recommend that combustible cooling towers be provided with an automatic fire sprinkler system. Fires can propagate internally within the tower structure when the cell is not in operation (such as for maintenance or construction), and even while the tower is in operation, especially those of the induced-draft type, because of the existence of relatively dry areas within the towers.\n\nBeing very large structures, cooling towers are susceptible to wind damage, and several spectacular failures have occurred in the past. At Ferrybridge power station on 1 November 1965, the station was the site of a major structural failure, when three of the cooling towers collapsed owing to vibrations in winds. Although the structures had been built to withstand higher wind speeds, the shape of the cooling towers caused westerly winds to be funnelled into the towers themselves, creating a vortex. Three out of the original eight cooling towers were destroyed, and the remaining five were severely damaged. The towers were later rebuilt and all eight cooling towers were strengthened to tolerate adverse weather conditions. Building codes were changed to include improved structural support, and wind tunnel tests were introduced to check tower structures and configuration.\n\n"}
{"id": "2035450", "url": "https://en.wikipedia.org/wiki?curid=2035450", "title": "Cruck", "text": "Cruck\n\nA cruck or crook frame is a curved timber, one of a pair, which supports the roof of a building, used particularly in England. This type of timber framing consists of long, generally naturally curved, timber members that lean inwards and form the ridge of the roof. These posts are then generally secured by a horizontal beam which then forms an \"A\" shape. Several of these \"crooks\" are constructed on the ground and then lifted into position. They are then joined together by either solid walls or cross beams which aid in preventing racking (the action of each individual frame going out of square with the rest of the frame, and thus risking collapse).\n\nThe term \"crook\" or \"cruck\" comes from Middle English \"crok(e)\", from Old Norse \"krāka\", meaning \"hook\". This is also the origin of the word \"crooked\", meaning bent, twisted or deformed, and also the crook used by shepherds and symbolically by bishops.\n\nCrucks were chiefly in use in the medieval period for structures such as large tithe barns. However, these bent timbers were comparatively rare, as they were also in high demand for the ship building industry. \n\nWhere naturally curved timbers were convenient and available, carpenters continued to use them at much later dates. For instance, base crucks are found in the roofs of the residential range of Staple Inn Buildings, Nos. 337 – 338, High Holborn, London. This is dated by documented records to 1586, with significant alterations in 1886 (under Alfred Waterhouse) and further restorations in 1936, and 1954–55. Despite these changes, an authority on English Historic Carpentry, Cecil Hewett, has stated that these 16th-century crucks are original.\n\nThe large main barn of the manor house Barlow Woodseats Hall features what is claimed to be the longest continuously roofed cruck barn in Derbyshire, and possibly even in the United Kingdom.\n\nAn example of a Yorkshire cruck barn complete with a heather thatched roof can be found in Appletreewick. The crucks or cruck \"blades\" are a single oak tree riven (split) in two to form an equally shaped A frame.\n\nRare examples of cruck framing are found on continental Europe such as in Belgium, Flanders, Northern France and the Corrèze region of France.\n\nNo cruck frames are known to have been built in America though there are rare examples of what may be an upper cruck or knee rafters. \n\nDuring the current revival of green oak framing for new building work, which has occurred mainly since approximately 1980 in the UK, genuine cruck frames have quite often been included in traditionally carpentered structures. \n\nThere are also some fine, historically authentic reconstructions. For instance, Tithe Barn, Pilton, Glastonbury, whose original roof was destroyed by lightning, has been carefully rebuilt in 2005 from curved oaks. The necessary trees were sought out, using special templates, in English woodlands.\n\n\nThe apex of a cruck frame also helps to define the style and region of the cruck. Different types include the butt apex, halved, housed, yoke, and crossed forms.\n\n\n\n"}
{"id": "12401257", "url": "https://en.wikipedia.org/wiki?curid=12401257", "title": "DIN 41612", "text": "DIN 41612\n\nDIN 41612 is a DIN standard for electrical connectors that are widely used in rack based electrical systems. Standardisation of the connectors is a pre-requisite for open systems, where users expect components from different suppliers to operate together. The most widely known use of DIN 41612 connectors is in the VMEbus system. They were also used by NuBus. The standard has subsequently been upgraded to international standards IEC 60603-2 and EN 60603-2.\n\nDIN 41612 connectors are used in STEbus,\nFuturebus, VMEbus, Multibus II, NuBus, VXI Bus,\neurocard TRAM motherboards,\nand Europe Card Bus,\nall of which typically use male DIN 41612 connectors on Eurocards plugged into female DIN 41612 on the backplane in a 19-inch rack chassis.\n\nThe standard describes connectors which may have one, two or three rows of contacts, which are labelled as rows a, b and c. Two row connectors may use rows a+b or rows a+c. The connectors may have 16 or 32 columns, which means that the possible permutations allow 16, 32, 48, 64 or 96 contacts. The rows and columns are on a 0.1 inch (2.54 mm) grid pitch. Insertion and removal force are controlled, and three durability grades are available.\n\nOften the female DIN 41612 connectors have press fit contacts rather than solder pin contacts, to avoid thermal shock to the backplane.\n\nThe headline performance of the connectors is a 2 amp per pin current carrying capacity, and 500 volt working voltage. Both these figures may need to be de-rated according to safety requirements or environmental conditions.\n\nThe DIN 41612 specification identifies 3 different classes or \"levels\"; it's more complicated than this, but, essentially: class 1 is good for 500 mating cycles; class 2 is good for 400 mating cycles, and, class 3 is good for 50 mating cycles.\n\n"}
{"id": "45534038", "url": "https://en.wikipedia.org/wiki?curid=45534038", "title": "Door phone", "text": "Door phone\n\nA door phone or door bell phone is a set of electrical and electronic elements used to handle two-way communication (street to home) in houses, apartments or villas. The device is connected to a secure communication system used to control the opening of the door giving access to any kind of buildings, offices, or apartment blocks. They are so widely used in the mentioned structures, that nowadays, they form part of the standard electrical installation of most buildings.\n\nThe simplest version is a simple intercom that establishes a communication between the street door and the house. In buildings where there are more than one \"door phone\" plate (located at the outside of the building's door entrance) has a certain number of buttons, depending on the number of flats existing in each building, in addition to the loudspeaker.\n\nA door phone in its most basic version is a two-way intercom allowing communication from the street to the house, with the possibility of driving an electric strike to unlock and open the door, allowing access to the interior of the building.\n\nIn places where there is more than one dwelling, the caller street, also called \" Push Plate \" or simply \" Plate Street \", is composed of a number of buttons is (usually one per housing) with the label next to the house or apartment number or name of the tenant.\n\nThe door phone plate is located at the outside of the entrance of each building, is composed by a matrix of buttons each of them driving a Buzzer located in a unit inside each home, which has a button to activate the electric strike.\n\nThere are several installation systems, the most traditional of the 4 + 1, (I.E.: four wires for power, communication and door system and one for the call).\n\nGoing a little further in time, we find video door phones featuring a video installation apart from the classical audio.\nIn these cases, the intercom plate has the same structure as the previous version but features a video monitor connected to a surveillance camera that allows inspecting the person who pressed the button and part of the surrounding area.\n\nAccess control systems. Some manufacturers have also adopted the possibility of opening the door via a keyboard. By introducing a numerical secret code the electric strike is operated. Although there is also the possibility of being used as an access control with either a magnetic card or a smart card.\n\nAmong other classifications there is one with three different types of door phones that differ mainly in the number of wires required in their installation, having some parallelism between different manufacturers, that use different communication protocols but a similar wiring topology:\n\n"}
{"id": "15054348", "url": "https://en.wikipedia.org/wiki?curid=15054348", "title": "Dual-phase steel", "text": "Dual-phase steel\n\nDual-phase steel (DP steel) is a high-strength steel that has a ferritic–martensitic microstructure.\nDP steels are produced from low or medium carbon steels that are quenched from a temperature above A but below A determined from continuous cooling transformation diagram.\nThis results in a microstructure consisting of a soft ferrite matrix containing islands of martensite as the secondary phase (martensite increases the tensile strength).\nTherefore, the overall behaviour of DP steels is governed by the volume fraction, morphology (size, aspect ratio, interconnectivity, etc.), the grain size and the carbon content.\nFor achieving these microstructures, DP steels typically contain 0.06–0.15 wt.% C and 1.5-3% Mn (the former strengthens the martensite, and the latter causes solid solution strengthening in ferrite, while both stabilize the austenite), Cr & Mo (to retard pearlite or bainite formation), Si (to promote ferrite transformation), V and Nb (for precipitation strengthening and microstructure refinement).\nThe desire to produce high strength steels with formability greater than microalloyed steel led the development of DP steels in the 1970s. \n\nDP steels have high ultimate tensile strength (UTS, enabled by the martensite) combined with low initial yielding stress (provided by the ferrite phase), high early-stage strain hardening and macroscopically homogeneous plastic flow (enabled through the absence of Lüders effects).\nThese features render DP steels ideal materials for automotive-related sheet forming operations. \n\nThe steel melt is produced in an oxygen top blowing process in the converter, and undergoes an alloy treatment in the secondary metallurgy phase. The product is aluminium-killed steel, with high tensile strength achieved by the composition with manganese, chromium and silicon.\n\nTheir advantages are as follows:\n\n\nDue to these properties DP steels are often used for automotive body panels, wheels, and bumpers.\n\n\n"}
{"id": "2677544", "url": "https://en.wikipedia.org/wiki?curid=2677544", "title": "Döbereiner's lamp", "text": "Döbereiner's lamp\n\nDöbereiner's lamp, also called a \"tinderbox\" (\"Feuerzeug\"), is a lighter invented in 1823 by the German chemist Johann Wolfgang Döbereiner; the lighter is based on the Fürstenberger lighter and was in production until ca. 1880. In the jar, similar to the Kipp's apparatus, zinc metal reacts with dilute sulfuric acid to produce hydrogen gas. When a valve is opened, a jet of hydrogen is released onto a platinum sponge. The sponge catalyzes a reaction with atmospheric oxygen, which heats the catalyst and ignites the hydrogen, producing a gentle flame.\nIt was commercialized for lighting fires and pipes. It's said that in 1820s over a million of the \"tinderboxes\" were sold.\n\nExamples of the lighter are exhibited in the Deutsches Museum and in the old pharmacy at Heidelberg Castle.\n\n\n"}
{"id": "3034047", "url": "https://en.wikipedia.org/wiki?curid=3034047", "title": "Ecrasite", "text": "Ecrasite\n\nEcrasite is an explosive material which is unaffected by moisture, shock or fire. It is a mixture of ammonium salts of cresol, phenol and various nitrocresols and nitrophenols principally trinitrocresol and picric acid. It was invented in 1888-1889 by Siersch and Kubin, and used in Austria-Hungary to load artillery shells.\n\nEcrasite is prepared by the partial nitration of a crude mixture of cresol and phenol with a mixture of concentrated sulfuric and nitric acids and the neutralisation of the product with ammonia to produce a crude salt similar to ammonium picrate.\n\nEcrasite is a bright yellow solid. It is waxy to touch and melts at about 100 °C. When subjected to open flame, it burns without detonation, unless confined. It is insensitive to friction. It requires a detonator for initiation. Its general adoption was hindered by several unexplained explosions during loading into shells, which might have been caused by creation of unstable metal salts of trinitrocresol and/or trinitrophenol when the explosive came in contact with metals or alloys such as copper, brass (widely used for manufacturing detonator parts) and possibly other ones.\n"}
{"id": "963967", "url": "https://en.wikipedia.org/wiki?curid=963967", "title": "Fireplace", "text": "Fireplace\n\nA fireplace is a structure made of brick, stone or metal designed to contain a fire. Fireplaces are used for the relaxing ambiance they create and for heating a room. Modern fireplaces vary in heat efficiency, depending on the design.\n\nHistorically they were used for heating a dwelling, cooking, and heating water for laundry and domestic uses. A fire is contained in a firebox or firepit; a chimney or other flue allows exhaust to escape. A fireplace may have the following: a foundation, a hearth, a firebox, a mantelpiece; a chimney crane (used in kitchen and laundry fireplaces), a grate, a lintel, a lintel bar,home overmantel, a damper, a smoke chamber, a throat, and a flue.\n\nOn the exterior there is often a corbeled brick crown, in which the projecting courses of brick act as a drip course to keep rainwater from running down the exterior walls. A cap, hood, or shroud serves to keep rainwater out of the exterior of the chimney; rain in the chimney is a much greater problem in chimneys lined with impervious flue tiles or metal liners than with the traditional masonry chimney, which soaks up all but the most violent rain. Some chimneys have a spark arrestor incorporated into the crown or cap.\n\nOrganizations like the Environmental Protection Agency and the Washington Department of Ecology warn that, according to various studies, fireplaces can pose a significant health risk. The EPA writes \"Smoke may smell good, but it's not good for you.\"\n\nMasonry and prefabricated fireplaces can be fueled by wood, natural gas, biomass and propane fuel sources. Ventless Fireplaces (duct free/room-venting fireplaces) are fueled by either gel, liquid propane, bottled gas or natural gas. In the United States, some states and local counties have laws restricting these types of fireplaces. They must be properly sized to the area to be heated. There are also air quality control issues due to the amount of moisture they release into the room air, and oxygen sensor and carbon monoxide sensors are safety essentials. Direct vent fireplaces are fueled by either liquid propane or natural gas. They are completely sealed from the area that is heated, and vent all exhaust gasses to the exterior of the structure.\n\nChimney and flue types:\n\nNewly constructed flues may feature a chase cover, a cap, and a spark arrestor at the top to keep small animals out and to prevent sparks from being broadcast into the atmosphere. All fireplaces require trained gas service members to carry out installations.\n\nA wide range of accessories are used with fireplaces, which range between countries, regions, and historical periods. For the interior, common in recent Western cultures include grates, fireguards, log boxes, andirons, pellet baskets, and fire dogs, all of which cradle fuel and accelerate burning. A grate (or fire grate) is a frame, usually of iron bars, to retain fuel for a fire. Heavy metal firebacks are sometimes used to capture and re-radiate heat, to protect the back of the fireplace, and as decoration. Fenders are low metal frames set in front of the fireplace to contain embers, soot and ash. For fireplace tending, tools include pokers, bellows, tongs, shovels, brushes and tool stands. Other wider accessories can include log baskets, companion sets, coal buckets, cabinet accessories and more. \n\nAncient fire pits were sometimes built in the ground, within caves, or in the center of a hut or dwelling. Evidence of prehistoric, man-made fires exists on all five inhabited continents. The disadvantage of early indoor fire pits was that they produced toxic and/or irritating smoke inside the dwelling.\n\nFire pits developed into raised hearths in buildings, but venting smoke depended on open windows or holes in roofs. The medieval great hall typically had a centrally located hearth, where an open fire burned with the smoke rising to the vent in the roof. Louvers were developed during the Middle Ages to allow the roof vents to be covered so rain and snow would not enter.\n\nAlso during the Middle Ages, smoke canopies were invented to prevent smoke from spreading through a room and vent it out through a wall or roof. These could be placed against stone walls, instead of taking up the middle of the room, and this allowed smaller rooms to be heated.\n\nChimneys were invented in northern Europe in the 11th or 12th centuries and largely fixed the problem of fumes, more reliably venting smoke outside. They made it possible to give the fireplace a draft, and also made it possible to put fireplaces in multiple rooms in buildings conveniently. They did not come into general use immediately, however, as they were expensive to build and maintain.\n\nIn 1678 Prince Rupert, nephew of Charles I, raised the grate of the fireplace, improving the airflow and venting system. The 18th century saw two important developments in the history of fireplaces. Benjamin Franklin developed a convection chamber for the fireplace that greatly improved the efficiency of fireplaces and wood stoves. He also improved the airflow by pulling air from a basement and venting out a longer area at the top. In the later 18th century, Count Rumford designed a fireplace with a tall, shallow firebox that was better at drawing the smoke up and out of the building. The shallow design also improved greatly the amount of radiant heat projected into the room. Rumford's design is the foundation for modern fireplaces.\n\nThe Aesthetic movement of the 1870s and 1880s took on a more traditional spectra based on stone and deflected unnecessary ornamentation. Rather it relied on simple designs with little unnecessary ornamentation. In the 1890s the Aesthetic movement gave way to the Arts and Crafts movement, where the emphasis was still placed on providing quality stone. Stone fireplaces at this time were a symbol of prosperity, which to some degree is still the notion today. \n\nOver time, the purpose of fireplaces has changed from one of necessity to one of visual interest. Early ones were more fire pits than modern fireplaces. They were used for warmth on cold days and nights, as well as for cooking. They also served as a gathering place within the home. These fire pits were usually centered within a room, allowing more people to gather around it.\n\nMany flaws were found in early fireplace designs. Along with the Industrial Revolution, came large scale housing developments, necessitating a standardization of fireplaces. The most renowned fireplace designers of this time were the Adam Brothers. They perfected a style of fireplace design that was used for generations. It was smaller, more brightly lit, with an emphasis on the quality of the materials used in their construction, instead of their size.\n\nBy the 1800s most new fireplaces were made up of two parts, the surround and the insert. The surround consisted of the mantlepiece and sides supports, usually in wood, marble or granite. The insert was where the fire burned, and was constructed of cast iron often backed with decorative tiles. As well as providing heat, the fireplaces of the Victorian era were thought to add a cozy ambiance to homes.\n\nSome fireplace units incorporate a blower which transfers more of the fireplace's heat to the air via convection, resulting in a more evenly heated space and a lower heating load. Fireplace efficiency can also be increased with the use of a fireback, a piece of metal that sits behind the fire and reflects heat back into the room. Firebacks are traditionally made from cast iron, but are also made from stainless steel. Efficiency is a complicated concept though with open hearth fireplaces. Most efficiency tests consider just the effect of heating of the air. An open fireplace is not, and never was, intended to heat the air. A fireplace with a fireback is a radiant heater, and has done so since the 15th century. The best way to gauge the output of a fireplace is if you notice you're turning the thermostat up or down.\n\nMost older fireplaces have a relatively low efficiency rating. Standard, modern, wood-burning masonry fireplaces though have an efficiency rating of at least 80% (legal minimum requirement for example in Salzburg/Austria). To improve efficiency, fireplaces can also be modified by inserting special heavy fireboxes designed to burn much cleaner and can reach efficiencies as high as 80% in heating the air. These modified fireplaces are often equipped with a large fire window, enabling an efficient heating process in two phases. During the first phase the initial heat is provided through a large glass window while the fire is burning. During this time the structure, built of refractory bricks, absorbs the heat. This heat is then evenly radiated for many hours during the second phase. Masonry fireplaces without a glass fire window only provide heat radiated from its surface. Depending on outside temperatures 1 to 2 daily firings are sufficient to ensure a constant room temperature.\n\nIn a literature review published in the journal of Toxicology and Environmental Health, J.T. Zelikoff concludes that there are a wide variety of health risks posed by residential wood combustion. She writes:\n\nThe Washington Department of Ecology also published a booklet explaining why wood smoke can be dangerous. It explains that human lung and respiratory systems are unable to filter particles emitted by wood combustion, which penetrate deeply into the lungs. For months, cancer causing chemicals can continue to cause changes and structural damage within the respiratory system. Young children, seniors, pregnant women, smokers and individuals with respiratory disorders are most vulnerable. Wood smoke can cause disease and even death in children because it is associated with lower respiratory tract infections. Home fireplaces have caused fatal carbon monoxide poisoning.\n\nSeveral of these terms may be compounded with chimney or fireplace such as \"chimney-back\".\n\n\n"}
{"id": "23658272", "url": "https://en.wikipedia.org/wiki?curid=23658272", "title": "Flight control modes", "text": "Flight control modes\n\nAircraft with fly-by-wire flight controls require computer-controlled flight control modes that are capable of determining the operational mode (computational law) of the aircraft.\n\nA reduction of electronic flight control can be caused by the failure of a computational device, such as the flight control computer or an information providing device, such as the ADIRU.\n\nElectronic flight control systems (EFCS) also provide augmentation in normal flight, such as increased protection of the aircraft from overstress or providing a more comfortable flight for passengers by recognizing and correcting for turbulence and providing yaw damping.\n\nTwo aircraft manufacturers produce commercial passenger aircraft with primary flight computers that can perform under different flight control modes (or laws). The most well-known are the \"normal\", \"alternate\", \"direct\" and \"mechanical laws\" of the Airbus A320-A380.\n\nBoeing's fly-by-wire system is used in the Boeing 777, Boeing 787 Dreamliner and Boeing 747-8.\n\nThese newer aircraft use electronic control systems to increase safety and performance while saving aircraft weight. These electronic systems are lighter than the old mechanical systems and can also protect the aircraft from overstress situations, allowing designers to reduce over-engineered components, which further reduces the aircraft's weight.\n\nAirbus aircraft designs after the A300/A310 are almost completely controlled by fly-by-wire equipment. These newer aircraft, including the A320, A330, A340, A350 and A380 operate under Airbus flight control laws. The flight controls on the Airbus A330, for example, are all electronically controlled and hydraulically activated. Some surfaces, such as the rudder, can also be mechanically controlled. In normal flight, the computers act to prevent excessive forces in pitch and roll. \nThe aircraft is controlled by three primary control computers (captain's, first officer's, and standby) and two secondary control computers (captain's and first officer's). In addition there are two flight control data computers (FCDC) that read information from the sensors, such as air data (airspeed, altitude). This is fed along with GPS data, into three redundant processing units known as air data inertial reference units (ADIRUs) that act both as an air data reference and inertial reference. ADIRUs are part of the air data inertial reference system, which, on the Airbus is linked to eight air data modules: three are linked to pitot tubes and five are linked to static sources. Information from the ADIRU is fed into one of several flight control computers (primary and secondary flight control). The computers also receive information from the control surfaces of the aircraft and from the pilots aircraft control devices and autopilot. Information from these computers is sent both to the pilot's primary flight display and also to the control surfaces.\n\nThere are four named flight control laws, however \"alternate law\" consists of two modes, \"alternate law 1\" and \"alternate law 2\". Each of these modes have different sub modes: ground mode, flight mode and flare, plus a back-up \"mechanical law\".\n\n\"Normal law\" differs depending on the stage of flight. These include:\n\n\nDuring the transition from take-off to cruise there is a 5-second transition, from descent to flare there is a two-second transition, and from flare to ground there is another 2 second transition in \"normal law\".\n\nThe aircraft behaves as in direct mode: the autotrim feature is turned off and there is a direct response of the elevators to the sidestick inputs. The horizontal stabilizer is set to 4° up but manual settings (e.g. for center of gravity) override this setting. After the wheels leave the ground, a 5-second transition occurs where \"normal law – flight mode\" takes over from \"ground mode\".\n\nThe flight mode of \"normal law\" provides five types of protection: pitch attitude, load factor limitations, high speed, high-AOA and bank angle. \"Flight mode\" is operational from take-off, until shortly before the aircraft lands, around 100 feet above ground level. It can be lost prematurely as a result of pilot commands or system failures. Loss of \"normal law\" as a result of a system failure results in \"alternate law 1\" or \"2\".\n\nUnlike conventional controls, in \"normal law\" vertical side stick movement corresponds to a load factor proportional to stick deflection independent of aircraft speed. When the stick is neutral and the load factor is 1g, the aircraft remains in level flight without the pilot changing the elevator trim. Horizontal side stick movement commands a roll rate, and the aircraft maintains a proper pitch angle once a turn has been established, up to 33° bank. The system prevents further trim up when the angle of attack is excessive, the load factor exceeds 1.3g, or when the bank angle exceeds 33°.\n\nAlpha protection (α-Prot) prevents stalling and guards against the effects of windshear. The protection engages when the angle of attack is between α-Prot and α-Max and limits the angle of attack commanded by the pilot's sidestick or, if autopilot is engaged, it disengages the autopilot.\nHigh speed protection will automatically recover from an overspeed. There are two speed limitations for high altitude aircraft, V (maximum operational velocity) and M (maximum operational Mach) the two speeds are the same at approximately 31,000 feet, below which overspeed is determined by V and above which by M.\n\nThis mode is automatically engaged when the radar altimeter indicates 100 feet above ground. At 50 feet the aircraft trims the nose slightly down. During the flare, \"normal law\" provides high-AOA protection and bank angle protection. The load factor is permitted to be from 2.5g to −1g, or 2.0g to 0g when slats are extended. Pitch attitude is limited from −15° to +30°, and upper limit is further reduced to +25° as the aircraft slows.\n\nThere are four reconfiguration modes for the Airbus fly-by-wire aircraft: \"alternate law 1\", \"alternate law 2\", \"direct law\" and \"mechanical law\". The ground mode and flare modes for \"alternate law\" are identical to those modes for \"normal law\".\n\nAlternate law 1 (ALT1) mode combines a \"normal law\" lateral mode with the load factor, bank angle protections retained. High angle of attack protection may be lost and low energy (level flight stall) protection is lost. High speed and high angle of attack protections enter alternative law mode.\n\nALT1 may be entered if there are faults in the horizontal stabilizer, an elevator, yaw-damper actuation, slat or flap sensor, or a single air data reference fault.\n\nAlternate law 2 (ALT2) loses \"normal law\" lateral mode (replaced by roll direct mode and yaw alternate mode) along with pitch attitude protection, bank angle protection and low energy protection. Load factor protection is retained. High angle of attack and high speed protections are retained unless the reason for \"alternate law 2\" mode is the failure of two air-data references or if the two remaining air data references disagree.\n\nALT2 mode is entered when 2 engines flame out (on dual engine aircraft), faults in two inertial or air-data references, with the autopilot being lost, except with an ADR disagreement. This mode may also be entered with an all spoilers fault, certain ailerons fault, or pedal transducers fault.\n\n\"Direct law\" (DIR) introduces a direct stick-to-control surfaces relationship: control surface motion is directly related to the sidestick and rudder pedal motion. The trimmable horizontal stabilizer can only be controlled by the manual trim wheel. All protections are lost, and the maximum deflection of the elevators is limited for each configuration as a function of the current aircraft centre of gravity. This aims to create a compromise between adequate pitch control with a forward C.G. and not-too-sensitive control with an aft C.G.\n\nDIR is entered if there is failure of three inertial reference units or the primary flight computers, faults in two elevators, or flame-out in two engines (on a two-engine aircraft) when the captain's primary flight computer is also inoperable.\n\nIn the \"mechanical law\" back-up mode, pitch is controlled by the mechanical trim system and lateral direction is controlled by the rudder pedals operating the rudder mechanically.\n\nThe fly-by-wire electronic flight control system of the Boeing 777 differs from the Airbus EFCS. The design principle is to provide a system that responds similarly to a mechanically controlled system. Because the system is controlled electronically the flight control system can provide flight envelope protection.\n\nThe electronic system is subdivided between 2 levels, the 4 actuator control electronics (ACE) and the 3 primary flight computers (PFC). The ACEs control actuators (from those on pilot controls to control surface controls and the PFC). The role of the PFC is to calculate the control laws and provide feedback forces, pilot information and warnings.\n\nThe flight control system on the 777 is designed to restrict control authority beyond certain range by increasing the back pressure once the desired limit is reached. This is done via electronically controlled backdrive actuators (controlled by ACE). The protections and augmentations are: bank angle protection, turn compensation, stall protection, over-speed protection, pitch control, stability augmentation and thrust asymmetry compensation. The design philosophy is: \"to inform the pilot that the command being given would put the aircraft outside of its normal operating envelope, but the ability to do so is not precluded.\"\n\nIn \"normal mode\" the PFCs transmit actuator commands to the ACEs, which convert them into analog servo commands. Full functionality is provided, including all enhanced performance, envelope protection and ride quality features.\n\nBoeing \"secondary mode\" is comparable to the Airbus \"alternate law\", with the PFCs supplying commands to the ACEs. However, EFCS functionality is reduced, including loss of flight envelope protection. Like the Airbus system, this state is entered when a number of failures occur in the EFCS or interfacing systems (e.g. ADIRU or SAARU).\n"}
{"id": "55424384", "url": "https://en.wikipedia.org/wiki?curid=55424384", "title": "French Fab", "text": "French Fab\n\nFrench Fab is an accreditation award created 2 October 2017 in order to federate French companies and to promote French industries throughout the world.\n\nThe French government has been inspired by the French Tech created in 2013 to promote startups and French IT.\n\nThe award has been created 2 October 2017 by Bruno Le Maire, Minister of Finance.\n\n"}
{"id": "57384948", "url": "https://en.wikipedia.org/wiki?curid=57384948", "title": "Heinz Faßmann", "text": "Heinz Faßmann\n\nHeinz Faßmann (born August 13, 1955 in Düsseldorf) is an Austrian politician and professor of human geography and land-use planning at the University of Vienna. Since December 2017, Faßmann has been serving as the minister of education in the Kurz cabinet. Faßmann is considered to be aligned with the Austrian People's Party but holds no formal party membership or affiliation.\n\nHeinz Faßmann was born on August 13, 1955 in Düsseldorf, Germany.\n\nHe spent his childhood and adolescence in Vienna.\n\nFaßmann attended primary school from 1962 to 1966 and gymnasium from 1966 to 1974.\n\nFaßmann studied geography and social and economic history at the University of Vienna, graduating with a PhD in 1980.\nFrom 1980 to 1981, he was engaged in postgraduate work in sociology at the Institute for Advanced Studies.\n\nFrom 1981 to 1992, Faßmann was a researcher with the Austrian Academy of Sciences. He spent his academy years on the academy's commission on land-use planning (). On the strength of his commission work, he was appointed director of the Institute of City and Regional Planning () in 1992.\n\nFour years later, in 1996, Faßmann left Vienna and the academy to become a C4 professor of human geography and geoinformatics at the Technical University of Munich.\nAnother four years later, in 2000, Faßmann returned to Vienna to teach human geography and land-use planning at his alma mater. In 2011, he was made the university's vice rector, a post he held until his transition into politics in 2017.\nHe also served as a member of the senate of the University of Vienna from 2000 to 2006 and as the dean of the Faculty of Geosciences from 2006 to 2011.\n\nIn addition to his academic work, Faßmann held positions on various committees and sat on the supervisory boards of a number of companies, including one university spin-off incubator.\nFaßmann ran a spin-off of his own, the Heinz Faßmann Projektentwicklung KG, from 2013 to 2018.\nHe was director of the Expert Commission of German Foundations on Integration and Migration () from 2009 to 2017.\nIn 2006 he was, for the second time, appointed Director of the Institute of City and Regional Planning.\n\nFaßmann routinely acted as a political consultant and advisor to the Austrian government. He became chairman of the Ministry of Europe, Integration and Foreign Affairs Expert Commission on Integration () in 2010.\nHe has been sitting on the Ministry of the Interior Migration Council for Austria (),\nlater renamed Migration Commission (),\nsince 2014.\n\nAlthough Faßmann had never held elected office and never officially attached himself to any political party, he was invited to join the Kurz cabinet as the minister of education.\nWhen the cabinet took office on December 18, 2017, Faßmann was appointed minister of education and science. Following a reshuffling of ministerial responsibilities − a move regularly made by new parliamentary majority leaders − he became minister of education, science and research on January 8, 2018.\nFaßmann has resigned his positions as a vice rector, as the chairman of the commission, and as a member of the council. For the duration of his term in cabinet office, he has also suspended his position as the director of the Institute of City and Regional Planning.\n\nBorn a German, Heinz Faßmann has been a naturalized Austrian citizen since 1994.\n\nHe is married and has two children.\n\nFaßmann stands well over two meters tall. Correcting media reports that erroneously cited even greater numbers, Faßmann states his height at 203 cm.\n\n\n"}
{"id": "37413113", "url": "https://en.wikipedia.org/wiki?curid=37413113", "title": "History of Lance Armstrong doping allegations", "text": "History of Lance Armstrong doping allegations\n\nFor much of the second phase of his career, American cyclist Lance Armstrong faced constant allegations of doping. Armstrong consistently denied allegations of using performance enhancing drugs until a partial confession during a broadcast interview with Oprah Winfrey in January 2013.\n\nArmstrong has been criticized for his disagreements with outspoken opponents of doping such as journalist Paul Kimmage and cyclist Christophe Bassons. Bassons wrote a number of articles for a French newspaper during the 1999 Tour de France which made references to doping in the peloton. Subsequently, Armstrong had an altercation with Bassons during that Tour where Bassons said he rode up alongside on the Alpe d'Huez stage to tell him \"it was a mistake to speak out the way I do and [Armstrong] asked why I was doing it. I told him that I'm thinking of the next generation of riders. Then he said 'Why don't you leave, then?'\" Armstrong later confirmed Bassons's story. On the main evening news on TF1, a French television station, Armstrong said: \"His accusations aren't good for cycling, for his team, for me, for anybody. If he thinks cycling works like that, he's wrong and he would be better off going home.\"\n\nKimmage, a professional cyclist in the 1980s who later became a sports journalist, referred to Armstrong as a \"cancer in cycling\". He also asked Armstrong questions in relation to his \"admiration for dopers\" at a press conference at the Tour of California in 2009, provoking a scathing reaction from Armstrong. This spat continued and is exemplified by Kimmage's articles in the British newspaper \"The Sunday Times\". David Walsh, a \"Sunday Times\" reporter referred to by Armstrong as a \"little troll,\" revealed in a 2001 story that Armstrong had ties to controversial Italian doctor Michele Ferrari. Two years later, Walsh's book \"L.A. Confidentiel\" alleged, based on testimony by Armstrong's former masseuse Emma O'Reilly, that clandestine trips were made to pick up and deliver doping products to Armstrong's team.\n\nUntil his 2013 admission, Armstrong continually denied using illegal performance-enhancing drugs and described himself as the most tested athlete in the world. However, a 1999 urine sample showed traces of corticosteroid; a medical certificate showed he used an approved cream for saddle sores which contained the substance. O'Reilly claimed that team officials conspired with a compliant doctor to falsify Armstrong's prescription, and that Armstrong never had the condition. She also claimed that, on other occasions, she was asked to dispose of used syringes for Armstrong and pick up strange parcels for the team.\n\nFrom his return to cycling in the fall of 2008 through March 2009, Armstrong submitted to twenty-four unannounced drug tests by various anti-doping authorities. All of the tests were negative for performance-enhancing drugs.\n\nU.S. federal prosecutors pursued allegations of doping by Armstrong from 2010–2012. The effort convened a grand jury to investigate doping charges, including: taking statements under oath from Armstrong's former team members and other associates; met with officials from France, Belgium, Spain, and Italy; and requested samples from the French Anti-Doping Agency (AFLD). The investigation was led by Jeff Novitzky, an investigator from the United States Anti-Doping Agency (USADA), who also investigated suspicions of steroid use by baseball players Barry Bonds and Roger Clemens. The probe was terminated on February 3, 2012 with no charges filed.\n\nArmstrong faced accusations of doping as early as the 1999 Tour de France. Specifically, many European papers contended his victory in Stage 9, where he seemingly ascended the Alps with almost no difficulty, could not have been possible through natural means. Armstrong adamantly denied this, and the American press generally supported him. Armstrong contended, among other things, that it would have made no sense for him to dope since he lived in France for most of the year. France has long had some of the strictest anti-doping laws in the world.\n\nArmstrong was criticized for working with controversial Italian trainer Michele Ferrari, who later claimed that the two were introduced by Belgian cyclist Eddy Merckx in 1995. American cyclist Greg LeMond described himself as \"devastated\" on hearing of them working together, while Tour de France organizer Jean-Marie Leblanc said, \"I am not happy the two names are mixed.\" Following Ferrari's later-overturned conviction for \"sporting fraud\" and \"abuse of the medical profession\", Armstrong suspended his professional relationship with him, saying that he had \"zero tolerance for anyone convicted of using or facilitating the use of performance-enhancing drugs\" and denying that Ferrari had ever \"suggested, prescribed or provided me with any performance-enhancing drugs.\"\n\nFerrari was later absolved of all charges by an Italian appeals court of the sporting fraud charges as well as charges of abusing his medical license to write prescriptions. The court stated that it overturned his conviction \"because the facts do not exist\" to support the charges. Ferrari, however, is still banned from practicing medicine with cyclists by the Italian Cycling Federation. According to Italian authorities, Armstrong met with Ferrari as recently as 2010 in a country outside of Italy.\n\nIn 2004, Walsh and fellow reporter Pierre Ballester published a French-language book alleging Armstrong had used performance-enhancing drugs, entitled \"L. A. Confidentiel – Les secrets de Lance Armstrong\". On top of O'Reilly's aforementioned allegations, another figure in the book, Steve Swart, claims he and other riders, including Armstrong, began using drugs in 1995 while members of the Motorola team; this claim was denied by other team members.\n\nAllegations in the book were reprinted in \"The Sunday Times\" in a story by deputy sports editor Alan English in June 2004. Armstrong sued for libel, and the paper settled out of court after a High Court judge in a pre-trial ruling stated that the article \"meant accusation of guilt and not simply reasonable grounds to suspect.\" The newspaper's lawyers stated, \"\"The Sunday Times\" has confirmed to Mr. Armstrong that it never intended to accuse him of being guilty of taking any performance-enhancing drugs and sincerely apologized for any such impression.\" Ballester and Walsh subsequently published \"L.A. Official\" and \"Le Sale Tour\" (\"The Dirty Trick\"), further pressing their claims that Armstrong used performance-enhancing drugs throughout his career.\n\nOn March 31, 2005, Mike Anderson, Armstrong's personal assistant of two years, filed a brief in Travis County District Court in Texas, as part of a legal battle following his termination in November 2004. In the brief, Anderson claimed that he discovered a box of androstenone while cleaning a bathroom in Armstrong's apartment in Girona, Spain. Androstenone is not a banned substance, and Anderson stated in a subsequent deposition that he had no direct knowledge of Armstrong using such substances. Armstrong denied the claim and issued a counter-suit. The two men reached an out-of-court settlement in November 2005; the terms of the agreement were not disclosed.\n\nIn June 2006, the French newspaper \"Le Monde\" reported claims by Frankie and Betsy Andreu during a deposition that Armstrong had admitted using performance-enhancing drugs to his physician just after brain surgery in 1996. The Andreus' testimony was related to litigation between Armstrong and SCA Promotions, a Dallas-based insurer who balked at paying a US$5 million bonus for Armstrong's sixth Tour victory due to the claims raised in \"L. A. Confidentiel\". The suit was settled out of court, with SCA paying Armstrong and Tailwind Sports US$7.5 million, to cover the bonus plus interest and lawyers' fees. Betsy Andreu's testimony stated, \"And so the doctor asked him a few questions, not many, and then one of the questions he asked was... have you ever used any performance-enhancing drugs? And Lance said yes. And the doctor asked, what were they? And Lance said, growth hormone, cortisone, EPO, steroids and testosterone.\"\n\nArmstrong suggested Betsy may have been confused by possible mention of his post-operative treatment, which included steroids and EPO that are taken to counteract the side effects of intensive chemotherapy. The Andreus' allegation was not supported by the eight other people present, including Armstrong's doctor Craig Nichols, or his medical history. According to LeMond, he had recorded a conversation, transcribed for review by NPR, in which Armstrong's contact at Oaklety, Inc., Stephanie McIlvain, corroborated Betsy's account. However, McIlvain contradicted LeMond's allegations and denied under oath that the incident ever occurred.\n\nIn July 2006, the \"Los Angeles Times\" published a story on the allegations raised in the SCA case. The report cited evidence at the trial, including the results of an AFLD test and an analysis of these results by an expert witness. According to the \"Times\", Australian researcher Michael Ashenden testified that Armstrong's levels rose and fell, consistent with a series of injections during the 1999 Tour. Ashenden, a paid expert retained by SCA Promotions, told arbitrators the results painted a \"compelling picture\" that the world's most famous cyclist \"used EPO in the '99 Tour.\"\n\nSCA president Bob Hamman knew his chances of winning the suit were slim, since the language in SCA's contract with Armstrong stipulated that the money had to be paid. However, he believed that the testimony and evidence would prove that Armstrong was a doper, and would be enough to trigger an investigation by sporting authorities. His hunch proved right; before the \"Times\" ran its story on the case, USADA general counsel Travis Tygart contacted Hamman and asked to see the evidence he'd gleaned.\n\nAshenden's findings were disputed by the Vrijman report, which pointed to procedural and privacy issues in dismissing the AFLD test results. The \"Times\" article also provided information on testimony given by Swart, the Andreus, and an instant messaging conversation between Frankie Andreu and Jonathan Vaughters regarding blood-doping in the peloton. Vaughters signed a statement disavowing the comments and stating he had \"no personal knowledge that any team in the Tour de France, including Armstrong's Discovery team in 2005, engaged in any prohibited conduct whatsoever.\" Frankie signed a statement affirming the conversation took place as indicated on the instant messaging logs submitted to the court. The SCA trial was settled out of court, and while no verdict or finding of facts was rendered, Armstrong regarded the outcome as proof that the doping allegations were baseless.\n\nOn May 20, 2010, former U.S. Postal teammate Floyd Landis – who had previously been stripped of the 2006 Tour title after a positive drug test – accused Armstrong of doping in 2002 and 2003. Landis also claimed that Postal team director Johan Bruyneel had bribed Hein Verbruggen, former president of the Union Cycliste Internationale (UCI), to keep quiet about a positive Armstrong test in 2002. Landis admitted there was no documentation supporting his claims. However, in July 2010 the president of the UCI, Pat McQuaid, confirmed that Armstrong made two donations to cycling's governing body: $25,000 in 2002, used by the juniors anti-doping program, and $100,000 in 2005, to buy a blood testing machine.\n\nLandis also maintained that he witnessed Armstrong receiving multiple blood transfusions, and dispensing testosterone patches to his teammates. On May 25, 2010, the UCI disputed Landis's claims, insisting that \"none of the tests revealed the presence of EPO in the samples taken from riders at the 2001 Tour of Switzerland.\" According to ESPN, \"Landis claimed that Armstrong tested positive while winning in 2002, a timeline Armstrong himself said left him 'confused,' because he did not compete in the event in 2002.\"\n\nIn May 2011, former Armstrong teammate Tyler Hamilton told CBS News that he and Armstrong had together taken EPO before and during the 1999, 2000, and 2001 Tours de France. Armstrong's attorney, Mark Fabiani, responded that Hamilton was lying. The accompanying \"60 Minutes\" investigation alleged that two other former Armstrong teammates, Frankie Andreu and George Hincapie, have told federal investigators that they witnessed Armstrong taking banned substances, including EPO, or supplied Armstrong with such substances. Fabiani stated in response that, \"We have no way of knowing what happened in the grand jury and so can't comment on these anonymously sourced reports.\" Hamilton further claimed that Armstrong tested positive for EPO during the 2001 Tour de Suisse; \"60 Minutes\" reported that the UCI intervened to conceal those test results, and that donations from Armstrong totaling US$125,000 may have played into said actions.\n\nMartial Saugy, chief of the Swiss anti-doping agency, later confirmed that they found four urine samples suspicious of EPO use at the 2001 race, but said there was no \"positive test\" and claimed not to know whether the suspicious results belonged to Armstrong. As a result, Armstrong's lawyers demanded an apology from \"60 Minutes\". Instead of apologizing, CBS News chairman Jeff Fager said the network stood by its report as \"truthful, accurate and fair\", and added that the suspicious tests which Saugy confirmed to exist have been linked to Armstrong \"by a number of international officials\".\n\nOn February 2, 2012, U.S. federal prosecutors officially dropped their criminal investigation with no charges.\n\nInterviewed by the BBC, Hamilton again insisted that he and Armstrong had routinely doped together. Dismissing the fact that Armstrong had passed numerous drug tests, Hamilton said that he himself had also passed hundreds of drug tests while doping. In the documentary \"The World According to Lance Armstrong\", attorney Jeffrey Tillotson, who represented SCA Promotions in the 2005 lawsuit, stated that he thought that the evidence collected by his legal team showed that Armstrong had been using performance-enhancing drugs since the beginning of his career.\n\nOn October 10, 2012, USADA claimed that Armstrong was part of \"the most sophisticated, professionalized and successful doping program that sport has ever seen,\" in advance of issuing its long-awaited report detailing the evidence it acquired. Based on evidence received during the Armstrong investigation and its prosecutions, USADA forwarded its \"reasoned decision\" document and supporting information to the UCI, the World Anti-Doping Agency, and the World Triathlon Corporation. Tygart claimed that:\nIn December 2012, Armstrong and his attorney, Tim Herman, held a secret meeting at the Denver offices of former Colorado governor Bill Ritter in an attempt to negotiate a reduction of Armstrong's lifetime ban down to one year. The talks fell apart when Armstrong refused to cooperate with Tygart.\n\nOn August 23, 2005, \"L'Équipe\", a major French daily sports newspaper, reported on its front page under the headline \"le mensonge Armstrong\" (\"The Armstrong Lie\") that 6 urine samples taken from the cyclist during the prologue and five stages of the 1999 Tour de France, frozen and stored since at \"Laboratoire national de dépistage du dopage de Châtenay-Malabry\" (LNDD), had tested positive for erythropoietin (EPO) in recent retesting conducted as part of a research project into EPO testing methods.\n\nArmstrong immediately replied on his website, saying, \"Unfortunately, the witch hunt continues and tomorrow's article is nothing short of tabloid journalism. The paper even admits in its own article that the science in question here is faulty and that I have no way to defend myself. They state: 'There will therefore be no counter-exam nor regulatory prosecutions, in a strict sense, since defendant's rights cannot be respected.' I will simply restate what I have said many times: I have never taken performance enhancing drugs.\"\n\nIn October 2008, the AFLD gave Armstrong the opportunity to have samples taken during the 1999 Tours de France retested. Armstrong immediately refused, saying, \"the samples have not been maintained properly.\" Head of AFLD Pierre Bordry stated: \"Scientifically there is no problem to analyze these samples – everything is correct\" and \"If the analysis is clean it would have been very good for him. But he doesn't want to do it and that's his problem.\"\nIn October 2005, in response to calls from the International Olympic Committee and the World Anti-Doping Agency (WADA) for an independent investigation, the UCI appointed Dutch lawyer Emile Vrijman to investigate the handling of urine tests by the French national anti-doping laboratory, LNDD. Vrijman was head of the Dutch anti-doping agency for ten years; since then he has worked as a defense attorney defending high-profile athletes against doping charges. Vrijman's report cleared Armstrong because of improper handling and testing. The report said tests on urine samples were conducted improperly and fell so short of scientific standards that it was \"completely irresponsible\" to suggest they \"constitute evidence of anything.\"\n\nThe recommendation of the commission's report was no disciplinary action against any rider on the basis of LNDD research. It also called upon the WADA and LNDD to submit themselves to an investigation by an outside independent authority. The WADA rejected these conclusions stating \"The Vrijman report is so lacking in professionalism and objectivity that it borders on farcical.\" The IOC Ethics Commission subsequently censured Dick Pound, the President of WADA and a member of the IOC, for his statements in the media that suggested wrongdoing by Armstrong.\n\nIn April 2009, Ashenden said that \"the LNDD absolutely had no way of knowing athlete identity from the sample they're given. They have a number on them, but that's never linked to an athlete's name. The only group that had both the number and the athlete's name is the federation, in this case it was the UCI.\" He added \"There was only two conceivable ways that synthetic EPO could've gotten into those samples. One, is that Lance Armstrong used EPO during the '99 Tour. The other way it could've got in the urine was if, as Lance Armstrong seems to believe, the laboratory spiked those samples. Now, that's an extraordinary claim, and there's never ever been any evidence the laboratory has ever spiked an athlete's sample, even during the Cold War, where you would've thought there was a real political motive to frame an athlete from a different country. There's never been any suggestion that it happened.\"\n\nDr. Michael Ashenden's statements are at odds with the findings of the Vrijman report. \"According to Mr. Ressiot, the manner in which the LNDD had structured the results table of its report – i.e. listing the sequence of each of the batches, as well as the exact number of urine samples per batch, in the same (chronological) order as the stages of the 1999 Tour de France they were collected at – was already sufficient to allow him to determine the exact stage these urine samples referred to and subsequently the identity of the riders who were tested at that stage.\" The Vrijman report also says \"Le Monde of July 21 and 23, 1999 reveal that the press knew the contents of original doping forms of the 1999 Tour de France\".\n\nIn an interview with Oprah Winfrey that aired January 17 and 18, 2013, on the Oprah Winfrey Network, Armstrong confessed that he has used banned performance-enhancing drugs throughout much of his cycling career, most recently in 2005. He admitted that he used erythropoietin and human growth hormone, and that he had blood doped as well as falsifying documents saying he passed drug tests. Doping helped him for each of his seven Tour de France wins, Armstrong told Winfrey. According to USADA, samples from Armstrong taken in 2009 and 2010 as well are \"fully consistent with blood manipulation including EPO use and/or blood transfusions\". Armstrong is fighting to avoid paying millions of dollars in prize money back.\n\nIn a 2016 speech to University of Colorado, Boulder professor Roger A. Pielke, Jr.'s Introduction to Sports Governance class, Armstrong stated he began doping in \"late Spring of 1995\".\n\n"}
{"id": "4988264", "url": "https://en.wikipedia.org/wiki?curid=4988264", "title": "IMS Associates, Inc.", "text": "IMS Associates, Inc.\n\nIMS Associates, Inc., or IMSAI, was a microcomputer company, responsible for one of the earliest successes in personal computing, the IMSAI 8080. The company was founded in 1973 by William Millard and was based in San Leandro, California. Their first product launch was the IMSAI 8080 in 1975. One of the company's subsidiaries was the ill-fated ComputerLand. IMS stood for \"Information Management Sciences\".\n\nIMS Associates required all executives and key employees to take the EST Standard Training. \"Forbes\" considered Millard's requirements - which placed a heavy emphasis on self-actualization and encouraged vast discrepancies between executives and staff - were a key contributor to the downfall of the company, and Paul Freiberger and Michael Swaine concurred in \"\", noting that Millard's EST-induced unwillingness to admit a task might be impossible was a key factor in IMSAI's demise. \n\nIn May 1972, William Millard began business individually as \"IMS Associates\" (IMS) in the area of computer consultancy and engineering, using his home as an office. The work done by IMS was similar to that Millard had done previously for the city and county of San Francisco. By 1973, Millard founded IMS Associates, Inc. Millard soon found capital for his business, and received several contracts, all for software. IMS provided advanced engineering and software management to mainframe users, including business and the United States Government.\n\nIn 1974, IMS was contacted by a client which wanted a \"workstation system\" that could complete jobs for any General Motors new-car dealership. IMS planned a system including a terminal, small computer, printer, and special software. Five of these work stations were to have common access to a hard disk, which would be controlled by a small computer. Eventually product development was stopped. Millard and his chief engineer Joe Killian turned to the microprocessor.\n\nIntel had announced the 8080 chip, and compared to the 4004 to which IMS Associates had been first introduced, the 8080 looked like a \"real computer\". Full-scale development of the IMSAI 8080 was put into action, and by October 1975 an ad was placed in \"Popular Electronics\", receiving positive reactions. IMS shipped the first IMSAI 8080 kits on 16 December 1975 and shortly after turned to fully assembled units. Between 17,000 and 20,000 units were eventually produced, with an additional 2500 produced under the Fischer-Freitas name thereafter.\n\nIn 1976, as IMS had completed its transition from a consultancy firm into a manufacturing firm, the name of the company was changed to IMSAI Manufacturing Corporation.\n\nThe release of the Z80 by Zilog in 1976 quickly put an end to the dominance of 8080 machines as the new chip had an improved instruction set, could be clocked at faster speeds, and had on-chip DRAM refresh. IMSAI sales quickly plummeted and so in 1977 Millard decided to take the company through another transition, this time from a computer manufacturing company to a computer retailer. He established a chain of franchised retail outlets, initially called Computer Shack (the name was changed to ComputerLand following legal threats from Radio Shack).\n\nComputerLand retailed not only IMSAI 8080s, but also computers from companies including Apple, North Star, and Cromemco. The 8080 sold poorly in comparison, and IMSAI developed the IMSAI VDP-80, an all-in-one computer which worked poorly. Many franchise dealers refused to retail most IMSAI products except those that retained popularity including the IMSAI 8080. With most of the IMSAI resources stripped to fund ComputerLand's expansion, and with Millard's attention diverted, IMS Associates, Inc. went into a \"tailspin\", and filed for bankruptcy in October 1979.\n\nThe trademark was eventually acquired by Thomas \"Todd\" Fischer and Nancy Freitas (former early employees who undertook continued support after the parent company folded), now doing business as Fischer-Freitas Company (since October 1978), who continued manufacturing and service support under their newly acquired and trade marked IMSAI badge (such as the IMSAI Series Two), and continue support to this day. ComputerLand stores continued to prosper retailing IBM computers until IBM abandoned the 8-bit ISA bus in 1984; the franchises became independent following a series of bitter and costly legal battles with Millard.\n\n\n"}
{"id": "26131239", "url": "https://en.wikipedia.org/wiki?curid=26131239", "title": "Improved Military Rifle", "text": "Improved Military Rifle\n\nImproved military rifle propellants are tubular nitrocellulose propellants evolved from World War I through World War II for loading military and commercial ammunition and sold to civilians for reloading rifle ammunition for hunting and target shooting. These propellants were DuPont modifications of United States artillery propellants. DuPont miniaturized the large artillery grains to form military rifle propellants suitable for use in small arms. These were improved during the first world war to be more efficient in rimless military cartridges replacing earlier rimmed rifle cartridges. Four-digit numbers identified experimental propellants, and a few successful varieties warranted extensive production by several manufacturers. Some were used almost exclusively for military contracts, or commercial ammunition production, but a few have been distributed for civilian use in handloading. Improved military rifle propellants are coated with dinitrotoluene (DNT) to slow initial burning and graphite to minimize static electricity during blending and loading. They contain 0.6% diphenylamine as a stabilizer and 1% potassium sulfate to reduce muzzle flash.\nJohn Bernadou patented a single-base propellant while working at the Naval Torpedo Station in 1897. Bernadou's colloid of nitrocellulose with ether and alcohol was formulated for the reaction pressures generated within naval artillery. The colloid was extruded in dense cylinders with longitudinal perforations to decompose in accordance with Piobert's law. If all external surfaces of the grain are ignited simultaneously, the grain reacts inward from the outside of the cylinder (creating a reaction area of decreasing size), and outward from each perforation (creating a reaction area of increasing size.) Propellant decomposition is initiated by heat causing the colloid to melt and form bubbles of reactive gas which decompose in a luminous exothermic reaction after the bubbles burst. Rate of reaction is controlled by heat transfer through the temperature gradient from the luminous reacting gas through the bubbles to the intact colloid. Heat transfer (and rate of reaction) is faster if the bubbles are under pressure, because heat transfer is more efficient through smaller bubbles. These propellants may not react satisfactorily at low pressures within the oxygen-deficient atmosphere of a gun barrel.\n\nThe United States Navy licensed use of the patent to DuPont for production of artillery propellant for ships operating in the Atlantic, and to California Powder Works for ships operating in the Pacific. The United States Army also used Bernadou's propellant for artillery and for the new M1903 Springfield service rifle in 1909 with the M1906 bullet. Grain size varied with bore diameter. While artillery grain dimensions might be several inches or centimeters, the standard grains of military rifle propellant were long and in diameter. The Army identified this military rifle propellant as Pyro DG (for diphenylamine, graphited), and 500 tons per day were manufactured by various plants through the first world war.\n\nMilitary rifle propellant was manufactured in batches in a procedure taking about two weeks from treating cotton linters with nitric acid, through curing the extruded grains to evaporate excess ether and alcohol, and finally coating the dried grains with DNT and graphite. Each batch had somewhat different reaction rates, so testing was required to determine the appropriate charge to generate required reaction pressure in the intended cartridge. Test results were forwarded to the factory or arsenal assembling cartridges. Propellants packaged in small sheet metal canisters for sale to civilians were labeled \"Military Rifle Powder\" to distinguish the product from low-density \"bulk\" propellants intended to react at lower pressures in shotguns or pistols and from \"Sporting Rifle Powder\" for early lever action rifles unable to withstand the pressures of 20th-century service rifle cartridges. Charges of low-density \"bulk\" propellants were often similar to the volumes of gunpowder used in older firearms and reaction rates were less variable at low pressures appropriate for those cartridges; but each batch of military rifle propellant required a different canister label specifying the batch or lot number with the tested charge weight to generate appropriate reaction pressure in intended cartridges.\n\nOrders from countries fighting World War I required determining charges for different European military rifle cartridges, and production volume supported research for improvements. Improved military rifle propellants included a longitudinal perforation converting each grain to a tube with a progressive burning interior surface allowing a more consistent gas generation rate through the reaction period. Early propellants were identified by a two-digit number. As the number of experimental variations increased, each improved military rifle propellant was identified by a four-digit number. In addition to the canisters available from DuPont, the Director of Civilian Marksmanship (DCM) sold surplus improved military rifle propellants to members of the National Rifle Association. By 1936 improved DuPont process control produced batches conforming to published reloading data rather than requiring different charge specifications for each batch; and those propellants have remained in production. Non-conforming batches were used to load commercial and military cartridges following traditional testing procedures.\n\nWartime temporarily interrupted production of civilian specification propellants, as major quantities of new specifications were manufactured. Number 4831 was used to load navy anti-aircraft machine gun ammunition, and number 4895 was used to load United States service rifle ammunition. As these propellants became military surplus after the war, large quantities of different batches were blended together to make products with uniform average performance for sale to civilians. Manufacture of these specifications for civilian use resumed after military surplus had been exhausted; but reaction characteristics were slightly different from the products distributed from military surplus supplies.\n\nIMR® is a registered trademark of the IMR Powder Company assigned to the Hodgdon Powder Company, which markets powders under that name.\n\n\n\n"}
{"id": "46964830", "url": "https://en.wikipedia.org/wiki?curid=46964830", "title": "Inkjet technology", "text": "Inkjet technology\n\nInkjet technology is a method for depositing liquid droplets on a substrate. It was originally developed for the publishing industry, but has become a popular method in digital fabrication of electronic and mechanical devices. Although both terms, \"inkjet technology\" and \"inkjet printing\", are commonly used interchangeably, inkjet printing usually refers to the publishing industry, used for printing graphical content, while inkjet technology usually refers to the general purpose fabrication via inkjetting.\n\nInks must have high conductivity, high oxidation resistance and low sintering temperature.\n\nVarious drop formation technologies exist, and can be classified into two main types: continuous inkjet (CIJ) and drop-on-demand (DOD).\n\nWhile CIJ has a straightforward drop creation and sophisticated drop trajectory manipulation, DOD has sophisticated drop creation and no trajectory manipulation.\nIn this method, drops of ink are released individually, on demand, by a voltage signal. Released drops fall vertically without any trajectory manipulation. Commercial printheads can have tens to thousands of nozzles.\n\nThe two leading technologies for forcing ink out of a nozzle on demand are thermal DOD and piezoelectric DOD. Additional technologies include electrospray, acoustic discharge, electrostatic membrane and thermal bimorph.\n\nPiezoelectric DOD was invented in the 1970s. One disadvantage of the piezo-DOD method is that jettable inks must have viscosity and surface tension within a relatively strict range.\n\nThermal DOD was introduced in the 1980s by Canon and Hewlett-Packard.\n\nOne disadvantage of this method is that the variety of inks compatible with TIJ is essentially limited, because this method is compatible with inks that have high vapour pressure, low boiling point and high kogation stability. Water being such a solvent, limited the popularity of this method for non-industrial photo printing only, where water-based inks are used.\n\nIn this method, a column of ink is released continuously from the nozzle. The ink column spontaneously breaks into separate drops due to the Plateau–Rayleigh flow instability. The formed ink drops are either deflected by an electric field towards the desired location on the substrate, or collected for reuse. CIJ printheads can be either have a single jet (nozzle) or multiple jets (nozzles).\n\nOne disadvantage of the CIJ method is the need for solvent monitoring. Since only a small fraction of the ink is being used for actual printing, solvent must be continually added to the recycled ink to compensate the evaporation that takes place during flight of the recycled drops.\n\nAnother disadvantage is the need for ink additives. Since this method is based on electrostatic deflection, ink additives, such as potassium thiocyanate, may deteriorate the performance of the printed devices.\n\nThe printhead must have heating capability to print metallic alloys such as lead, tin, indium, zinc and aluminium. The process of printing of low-melting point metals is called \"direct melt printing\".\n\nThe printed material is sometimes only one step in the process, which may include deposition of a precursor followed by a catalyst, sintering, photonic curing, electroless plating etc., to give the final result.\n\n\nThe ink must be liquid, but may also contain small solids if they do not cause clogging. The solid particles should be smaller than 1/10 of the nozzle diameter to avoid clogging.\n\nDrop formation is governed by two physical properties: surface tension and viscosity. The surface tension forms ejected drops into spheres, in accordance with Plateau–Rayleigh instability. The viscosity can be optimized at jet time by using an appropriate printhead temperature.\n\nGenerally, lower viscosity allows better droplet formation and in practice only liquids with viscosities of 2-50 mPa s can be printed. More precisely, liquids whose Ohnesorge number is larger than 0.1 and smaller than 1 are jettable.\n\n\n"}
{"id": "25390654", "url": "https://en.wikipedia.org/wiki?curid=25390654", "title": "Intermittent pneumatic compression", "text": "Intermittent pneumatic compression\n\nIntermittent pneumatic compression is a therapeutic technique used in medical devices that include an air pump and inflatable auxiliary sleeves, gloves or boots in a system designed to improve venous circulation in the limbs of patients who suffer edema or the risk of deep vein thrombosis (DVT) or pulmonary embolism (PE).\n\nIn use, an inflatable jacket (sleeve, glove or boot) encloses the limb requiring treatment, and pressure lines are connected between the jacket and the air pump. When activated, the pump fills the air chambers of the jacket in order to pressurize the tissues in the limb, thereby forcing fluids, such as blood and lymph, out of the pressurized area. A short time later, the pressure is reduced, allowing increased blood flow back into the limb.\n\nThe primary functional aim of the device “is to squeeze blood from the underlying deep veins, which, assuming that the valves are competent, will be displaced proximally.” When the inflatable sleeves deflate, the veins will replenish with blood. The intermittent compressions of the sleeves will ensure the movement of venous blood.\n\nSequential compression devices (SCD) utilize sleeves with separated areas or pockets of inflation, which works to squeeze on the appendage in a “milking action.” The most distal areas will initially inflate, and the subsequent pockets will follow in the same manner.\n\nSequential calf compression and graduated compression stockings are currently the preferred prophylaxis in neurosurgery for the prevention of DVT and pulmonary embolism, sometimes in combination with low molecular weight heparins or unfractionated heparin.\n\nIntraoperative SCD-therapy is recommended during prolonged laparoscopic surgery to counter altered venous blood return from the lower extremities and consequent cardiac depression caused by pneumoperitoneum (inflation of the abdomen with carbon dioxide).\n"}
{"id": "57255044", "url": "https://en.wikipedia.org/wiki?curid=57255044", "title": "James Shaw Jr.", "text": "James Shaw Jr.\n\nJames Shaw Jr. is an American electrical technician. Shaw is known for disarming a gunman armed with an AR-15 style rifle during the Nashville Waffle House shooting in Antioch, Tennessee and saving lives as a result.\n\nJames Shaw Jr. was born and raised in Nashville, Tennessee. He attended Hunters Lane High School. Shaw graduated from Tennessee State University with a degree in Electrical Engineering. He later studied online at Brightwood College and earned an Electrical Technician diploma.\n\nShaw began working in the electrical field at the age of 18. In an interview, Shaw stated that \"... I've been destroying stuff and putting it back together since I was a little kid. I would take stuff apart and just hope I could reassemble it.\" He began working at AT&T as a wire technician after graduating from Tennessee State University. At AT&T, Shaw installs internet, television, and phone connections in residences. He also manages his own company, Imount Electric.\n\nWhen the shooting started, he ran to the bathroom—a bullet grazed his elbow and he realized there was no exit—so when he saw the gunman point the rifle down, perhaps to reload or fix a jam, he lunged and wrestled the weapon away from him. The gunman was captured by police a few days later. While four people died in the attack, it is likely that Shaw's action saved the lives of more restaurant patrons. Afterwards, Shaw established a GoFundMe campaign to raise money for the families of the four victims. Total donations raised by Shaw surpassed in early May 2018. While he has been described as a hero by numerous people, including Tennessee authorities, he has said that he does not think of himself in those terms:\n\nShaw was honored by the president of Tennessee State University who praised Shaw for his \"bravery and courage\" and for saving many lives. He was honored by Tennessee lawmakers who passed a joint resolution honoring Shaw's actions. He appeared on national cable TV news shows and was honored at a game featuring the Nashville Predators. Shaw appeared on \"The Ellen DeGeneres Show\" where he met NBA basketball star Dwyane Wade. \"The Steve Harvey Show\" sent him on a trip to the island of Barbados. In May 2018, Shaw met with Emma González, a survivor of the shooting at Marjory Stoneman Douglas High School and anti-gun activist who helped to found Never Again MSD. He is a graduate of Tennessee State University which is setting up a scholarship fund in his name. At the 2018 MTV Movie & TV Awards, after Chadwick Boseman won an award for \"Best Superhero\" for his work in \"Black Panther,\" he called Shaw up to the stage and gave him the award. Shaw was presented a Gold Vail Award by Randall L. Stephenson, the Chief Executive Officer of AT&T. In 2018, he was received a BET Humanitarian Award.\n\nShaw is a vegan. He has a daughter.\n"}
{"id": "56594103", "url": "https://en.wikipedia.org/wiki?curid=56594103", "title": "Jean-Baptiste-Claude Sené", "text": "Jean-Baptiste-Claude Sené\n\nJean-Baptiste-Claude Séne (1747-1803) was a French furniture maker in the 18th century, primarily during the reign of Louis XVI. He came from a noted family of \"menuisiers\", or furniture craftsmen. cabinet makers. His grandfather Jean established a workshop, which was inherited by his father Claude I (1724-1792), who earned the title of a master craftsman in 1743, and made chairs and armchairs for Louis XV. Jean-Baptiste-CLaude became a master in 1769. His younger brother,Claude II (called Sené the younger), also became a master in 1769, and both made chairs for Louis XVI. \n\nIn 1785b Jean-Baptiste-Claude received the title of \"fournissur\" to the royal furniture depot, and made chairs, armchairs, stools, fireplace screens and beds for the Palace of Versailles. the Palace of Fontainebleau, and the Palace of Saint-Cloud. His works were prominent in the private apartments of Marie-Antoinette. \n\n\n"}
{"id": "16463190", "url": "https://en.wikipedia.org/wiki?curid=16463190", "title": "Kaqusha Jashari", "text": "Kaqusha Jashari\n\nKaqusha Jashari (born 16 August 1946) is a Kosovo Albanian politician and engineer by profession. She is a member of the Assembly of Kosovo on the Democratic Party of Kosovo list since 2007.\n\nFrom 1986 until November 1988, she and Azem Vllasi were the two leading Kosovo politicians. In November 1988, they were both dismissed in the \"anti-bureaucratic revolution\" because of their unwillingness to accept the constitutional amendments curbing Kosovo's autonomy, and were replaced by proxies of Slobodan Milošević, the leader of the League of Communists of Serbia at the time.\n\nKaqusha Fejzullahu was born in Skenderaj, the daughter of Halil Fejzullahu. The family had an apartment in Bulevar kralja Aleksandra, Belgrade.\n\nIn May 1988 Jashari replaced Azem Vllasi as the President of the Provential Committee of the League of Communists of Kosovo. It seems that Serbia \"accepted\" her as it was said at the time it that her mother was Montenegrin.\n\nFrom 17 to 21 October there were Albanian protests throughout Kosovo against the changing of status of the SAP Kosovo. On 17 November 1988, Jashari and Vllasi were forced to resign and Rahman Morina was elected President of the Provential Committee on 27 January 1989 by the Presidium of the Provential Committee. This sparked new protests by Albanian youths and workers. They were both dismissed because of their unwillingness to accept the constitutional amendments curbing Kosovo's autonomy, and were replaced by proxies of Slobodan Milošević, the leader of the League of Communists of Serbia at the time.\n\nOn 20 October 1990 Marko Orlandić and Jashari guested the gathering of Serbs and Montenegrins in Kosovo Polje, which was not met with positive reactions.\n\nShe was the president of the Social Democratic Party of Kosovo (PSDK) from 1991 until 2008, when she was succeeded by the former prime minister and Kosovo Liberation Army (KLA) guerilla leader Agim Çeku.\n\nAccording to Radmila Vuličević from Pristina, Jashari's father seized her apartment in Pristina in June 1999. Jashari currently lives in the apartment.\n\n"}
{"id": "49548566", "url": "https://en.wikipedia.org/wiki?curid=49548566", "title": "Librem", "text": "Librem\n\nLibrem is a line of computers manufactured by Purism, SPC featuring free (libre) software. The laptop line is designed to protect privacy and freedom by providing no non-free (proprietary) software in the operating system or kernel, avoiding the Intel Active Management Technology, and gradually freeing and securing firmware. Librem laptops feature hardware kill switches for the microphone, webcam, Bluetooth and Wi-Fi.\n\nIn 2014 Purism launched a crowdfunding campaign on Crowd Supply to fund the creation of the Librem 15 laptop. The Librem 15 was marketed as a modern alternative to traditional free-hardware laptops, which tend to use older hardware. The 15 in the name refers to the 15\" screen size. The campaign succeeded, after extending the original campaign, and the laptops were shipped to backers. In a second revision of the laptop hardware kill switches were introduced.\n\nAfter the successful launch of the 15\" laptop, purism created another campaign on Crowd Supply for a 13\" model laptop, called the Librem 13. The laptop was created with hardware kill switches, similar to the revision 2 of the 15. The campaign also got funded and the laptops shipped\n\n, Purism had two laptop models in production, the Librem 13 (version 2) and Librem 15 (version 3). Purism had already announced in December 2016 it would be preparing to move from a build to order production approach to a \"ship from inventory\" model with the new batches of Librem 13 and 15. \n\nA convertible tablet-to-laptop model, the Librem 11, was under development . Purism Chief Executive Officer, Todd Weaver, stated that work on the Librem 11 would continue after the planned 2019 release of the Librem 5 smartphone.\n\nIn 2017, Purism started a crowdfunding campaign for \"Librem 5\", a smartphone aimed not only to run purely on the free software provided in PureOS, but to \"[focus] on security by design and privacy protection by default\". Purism claimed that the phone would become \"the world's first ever IP-native mobile handset, using end-to-end encrypted decentralized communication.\" Purism cooperated with KDE and GNOME in its development of Librem 5.\n\nPlans for security on the Librem 5 include separation of the CPU from the baseband processor, which, according to \"Linux Magazine\", would make Librem 5 unique in comparison to other mobile phones. Hardware kill switches for Wi-Fi and Bluetooth communication and the phone's camera, microphone, and baseband processor are also planned.\n\nThe default operating system planned for Librem 5 is Purism's PureOS, a Debian GNU/Linux derivative, with a choice of either GNOME or KDE Plasma Mobile as the desktop environments. Ubuntu Touch is also planned as a standard option for the operating system of Librem 5.\n\nPurism announced on 4 September 2018 that the launch date of Librem 5 would be April 2019, later than initially planned, because of two hardware bugs and the Europe/North America holiday season. The two \"silicon bugs\" in components provided by NXP Semiconductors caused extreme battery draining, discharging the phone in about an hour. Development kits for software developers were planned for release in October 2018, mostly unaffected by the bug, since developers would normally connect the device to a mains power outlet rather than rely on the phone battery.\n\nAnnounced on 20 September 2018, the Librem Key is a hardware USB security token with multiple features, including integration with a tamper-evident Heads BIOS, that ensures a Librem laptop BIOS was not maliciously altered since the last laptop launch. Also a one-time password storage with 3x HOTP (<nowiki>RFC 4226</nowiki>) and 15 x TOTP (<nowiki>RFC 6238</nowiki>) and a integrated password manager (16 entries), 40 kbit/s true random number generator, and a tamper-resistant smart card. The key supports type A USB 2.0, has dimensions of 48 x 19 x 7 mm, and weights 6 g.\n\nInitially planning to preload its Librem laptops with the Trisquel operating system, Purism eventually moved off the Trisquel platform to rebase onto Debian for the 2.0 release of its PureOS Linux operating system. As an alternative to PureOS, Librem laptops were originally announced as purchasable with Qubes preloaded, but in July, 2017 Librem announced that Qubes was no longer an option for new orders. In December 2017 the Free Software Foundation added PureOS to its list of endorsed GNU/Linux distributions.\n\nIn 2015, Purism began research to port the Librem 13 to coreboot but the effort was initially stalled. By the end of the year, a coreboot developer completed an initial port of the Librem 13 and submitted it for review. In December 2016, hardware enablement developer Youness Alaoui joined Purism and was tasked to complete the coreboot port for the original Librem 13 and prepare a port for the second revision of the device. Since summer 2017, new Librem laptops are shipped with coreboot as their standard BIOS, while some older models can be updated.\n"}
{"id": "8644283", "url": "https://en.wikipedia.org/wiki?curid=8644283", "title": "Membrane oxygenator", "text": "Membrane oxygenator\n\nA membrane oxygenator is a device used to add oxygen to, and remove carbon dioxide from the blood. It can be used in two principal modes: to imitate the function of the lungs in cardiopulmonary bypass (CPB), and to oxygenate blood in longer term life support, termed extracorporeal membrane oxygenation, ECMO. A membrane oxygenator consists of a thin gas permeable membrane separating the blood and gas flows in the CPB circuit; oxygen diffuses from the gas side into the blood, and carbon dioxide diffuses from the blood into the gas for disposal.\n\nThe history of the oxygenator, or artificial lung, dates back to 1885, with the first demonstration of a disc oxygenator, on which blood was exposed to the atmosphere on rotating discs by Von Frey and Gruber. These pioneers noted the dangers of blood streaming, foaming and clotting. In the 1920s and 30s, research into developing extracorporeal oxygenation continued. Working independently, Brukhonenko in the USSR and John Heysham Gibbon in the USA demonstrated the feasibility of extracorporeal oxygenation. Brukhonenko used excised dog lungs while Gibbon used a direct contact drum type oxygenator, perfusing cats for up to 25 minutes in the 1930s.\n\nGibbon’s pioneering work was rewarded in May 1953 with the first successful cardiopulmonary bypass operation. The oxygenator was of the stationary film type, in which oxygen was exposed to a film of blood as it flowed over a series of stainless steel plates.\n\nThe disadvantages of direct contact between the blood and air were well recognized, and the less traumatic membrane oxygenator was developed to overcome these. The first membrane artificial lung was demonstrated in 1955 by the group led by Willem Kolff and in 1956 the first disposable membrane oxygenator removed the need for time consuming cleaning before re-use. No patent was filed as Kolff believed that doctors should make technology available to all, without mind to profit.\n\nThe early artificial lungs used relatively impermeable polyethylene or Teflon homogeneous membranes, and it was not until more highly permeable silicone rubber membranes were introduced in the 1960s (and as hollow fibres in 1971) that the membrane oxygenator became commercially successful. The introduction of microporous hollow fibres with very low resistance to mass transfer revolutionized the design of membrane modules, as the limiting factor to oxygenator performance became the blood resistance. Current designs of oxygenator typically use an extraluminal flow regime, where the blood flows outside the gas filled hollow fibers, for short term life support, while only the homogeneous membranes are approved for long term use.\n\n\n"}
{"id": "1561380", "url": "https://en.wikipedia.org/wiki?curid=1561380", "title": "Nasal cannula", "text": "Nasal cannula\n\nThe nasal cannula (NC) is a device used to deliver supplemental oxygen or increased airflow to a patient or person in need of respiratory help. This device consists of a lightweight tube which on one end splits into two prongs which are placed in the nostrils and from which a mixture of air and oxygen flows. The other end of the tube is connected to an oxygen supply such as a portable oxygen generator, or a wall connection in a hospital via a flowmeter. The cannula is generally attached to the patient by way of the tube hooking around the patient's ears or by elastic head band. The earliest, and most widely used form of adult nasal cannula carries 1–5 litres of oxygen per minute.\n\nCannulae with smaller prongs intended for infant or neonatal use can carry less than one litre per minute. Flow rates of up to 60 litres of air/oxygen per minute can be delivered through wider bore humidified nasal cannula.\n\nThe nasal cannula was invented by Wilfred Jones and patented in 1949 by his employer, BOC.\n\nA nasal cannula is generally used wherever small amounts of supplemental oxygen are required, without rigid control of respiration, such as in oxygen therapy. Most cannulae can only provide oxygen at low flow rates—up to 5 litres per minute (L/min)—delivering an oxygen concentration of 28–44%. Rates above 5 L/min can result in discomfort to the patient, drying of the nasal passages, and possibly nose bleeds (epistaxis). Also with flow rates above 6 L/min, the laminar flow becomes turbulent and the oxygen therapy being delivered is only as effective as delivering 5-6 L/min.\n\nThe nasal cannula is often used in elderly patients or patients who can benefit from oxygen therapy but do not require it to self respirate. These patients do not need oxygen to the degree of wearing a non-rebreather mask. It is especially useful in those patients where vasoconstriction could negatively impact their condition, such as those suffering from strokes.\n\nA nasal cannula may also be used by pilots and passengers in small, unpressurized aircraft that do not exceed certain altitudes. The cannula provides extra oxygen to compensate for the lower oxygen content available for breathing at the low ambient air pressures of high altitude, preventing hypoxia. Special aviation cannula systems are manufactured for this purpose.\n\nSince the early 2000s, with the introduction of nasal cannula which uses heated humidification for respiratory gas humidification, flows above 6 LPM have become possible without the associated discomfort, and with the added benefit of improving mucociliary clearance.\n\n\"(See: High-flow therapy)\"\nHigh flows of an air/oxygen blend can be administered via a nasal cannula to accurately deliver high volume of oxygen therapy. Respiratory gas humidification allows the high flows to be delivered comfortably via the cannula. Nasal high flow therapy can be used as an effective alternative to face mask oxygen and allows the patient to continue to talk, eat and drink while receiving the therapy.\nDefinition: Non-invasive delivery of oxygen air mixture delivered via a nasal cannula at flows that exceed the patient’s inspiratory flow demands with gas that has been optimally conditioned by warming and humidifying the gas to close to 100% relative humidity at body temperature.\n\n"}
{"id": "1083341", "url": "https://en.wikipedia.org/wiki?curid=1083341", "title": "Night soil", "text": "Night soil\n\nNight soil is a historically used euphemism for human excreta collected from cesspools, privies, pail closets, pit latrines, privy middens, septic tanks, etc. This material was removed from the immediate area, usually at night, by workers employed in this trade. Sometimes it could be transported out of towns and sold on as a fertilizer. \n\nAnother definition is \"untreated excreta transported without water (e.g. via containers or buckets)\". The term \"night soil\" is largely an archaic word, used in historical contexts. The modern term is \"fecal sludge\"; fecal sludge management is an ongoing challenge, particularly in developing countries.\n\nNight soil was produced as a result of a sanitation system in areas without sewer systems or septic tanks. In this system of waste management, the human feces are collected without dilution with water.\n\nFeces were excreted into a container such as a chamber pot, and sometimes collected in the container with urine and other waste (\"slops\", hence slopping out). The excrement in the pail was often covered with earth (soil), which may have contributed to the term \"night soil.\" Often the deposition or excretion occurred within the residence, such as in a shophouse. This system may still be used in isolated rural areas or in urban slums in developing countries. The material was collected for temporary storage and disposed of depending on local custom.\n\nDisposal has varied through time. In urban areas, a night soil collector arrived regularly, at varying time periods depending on the supply and demand for night soil collection. Usually this occurred during the night, giving the night soil its name.\n\nIn isolated rural areas such as in farms, the household usually disposed of the night soil themselves.\n\nHuman excreta may be attractive as fertilizer because of the high demand for fertilizer and the relative availability of the material to create night soil. In areas where native soil is of poor quality, the local population may weigh the risk of using night soil.\n\nThe use of unprocessed human feces as fertilizer is a risky practice as it may contain disease-causing pathogens. Nevertheless, in some developing nations it is still widespread. Common parasitic worm infections, such as ascariasis, in these countries are linked to night soil use in agriculture, because the helminth eggs are in feces and can thus be transmitted from one infected person to another person (fecal-oral transmission of disease).\n\nThese risks are reduced by proper fecal sludge management, e.g. via composting. The safe reduction of human excreta into compost is possible. Some municipalities create compost from the sewage sludge, but then recommend that it only be used on flower beds, not vegetable gardens. Some claims have been made that this is dangerous or inappropriate without the expensive removal of heavy metals.\n\nThe use of sewage as fertilizer was common in ancient Attica. The sewage system of ancient Athens collected the sewage of the city in a large reservoir and then channelled it to the Cephissus river valley for use as fertilizer.\n\nThe term is known, or even infamous, among the generations that were born in parts of China or Chinatowns (depending on the development of the infrastructure) before 1960. Post-World War II Chinatown, Singapore, before the independence of Singapore, utilized night-soil collection as a primary means of waste disposal, especially as much of the infrastructure was damaged and took a long time to rebuild following the Battle of Singapore and subsequent Japanese Occupation of Singapore. Following the development of the economy and the standard of living after independence, the night soil system in Singapore is now merely a curious anecdote from the time of colonial rule when new systems developed.\n\nThe collection method is generally very manual and heavily relies on close human contact with the waste. During the Nationalist era when the Kuomintang ruled mainland China, as well as Chinatown in Singapore, the night soil collector usually arrived with spare and relatively empty honey buckets to exchange for the full honey buckets. The method of transporting the honey buckets from individual households to collection centers was very similar to delivering water supplies by an unskilled laborer, with the exception that the item being transported was not at all potable and it was being delivered \"from\" the household, rather than \"to\" the household. The collector would hang full honey buckets onto each end of a pole he carried on his shoulder and then proceeded to carry it through the streets until he reached the collection point.\n\nHong Kong has a similar euphemism for night soil collection, \"dàoyèxiāng\", which literally means \"emptying nocturnal fragrance\".\n\nThe reuse of feces as fertilizer was common in Japan. In Edo city, compost merchants gathered feces to sell for farmers. That was good additional income for apartment owners. Human excreta of rich people were sold at higher prices because their diet was better; presumably, more nutrients remained in their excreta. Various historic documents dating from the 9th century detail the disposal procedures for toilet waste.\n\nSelling human waste products as fertilizers became much less common after World War II, both for sanitary reasons and because of the proliferation of chemical fertilizers, and less than 1% is used for night soil fertilization. The presence of the United States occupying force, by whom the use of human waste as fertilizer was seen as unhygienic and suspect, was also a contributing factor: \"the Occupationaires condemned the practice, and tried to prevent their compatriots from eating vegetables and fruit from the local markets\".\n\nVarious Mesoamerican civilizations used human feces to fertilize their crops. The Aztecs, in particular, are well known for their famous chinampas, artificial islands made of mud and human waste used to grow crops that could be harvested up to 7 times a year. Current research has placed the origins of chinampas in an Aztec town of Culhuacan in the year 1100 C.E. They were constructed by first fencing an area between 30 m x 2.5 m and 91 m x 9 m, using wattle. Then filled in with mud, sediment, feces and decaying vegetation. To stabilize the chinampas, trees were often planted on the corners, primarily \"āhuexōtl\" (\"Salix bonplandiana\") or \"āhuēhuētl\" (\"Taxodium mucronatum\"). Chinampas were very common before Spanish conquest and are still found in Mexico today.\n\nA gong farmer was the term used in Tudor England for a person employed to remove human excrement from privies and cesspits. Gong farmers were only allowed to work at night and the waste they collected had to be taken outside the city or town boundaries.\n\nThe rapid industrialisation of England during the nineteenth century led to mass urbanisation, over-crowding, and epidemics. One response was the development of the \"Rochdale system\", in which the town council arranged for the collection of the night soil from the outhouse attached to each dwelling or group of dwellings (see pail closet). A later response was the passage of the Public Health Act 1875, which led to the creation of byelaws regarding housing, mandating one outhouse per house. These were still \"earth closets\" (not water closets i.e. WCs) and so still depended on the \"night soil men\" or \"nightmen\".\n\nPeople responsible for the disposal of night soil are considered untouchables in India. The practice of untouchability was banned by law when India gained independence, but the tradition widely persists as the law is difficult to enforce. This \"manual scavenging\" is now illegal in all Indian states.\n\nThe Indian government's Union Ministry for Social Justice and Empowerment stated in 2003 that 676,000 people were employed in the manual collection of human waste in India. Social organizations have estimated that up to 1.3 million Indians collect such waste. Further, workers in the collection of human waste were confined to marriage amongst themselves, thereby leading to a waste-collecting caste, which passes its profession on from generation to generation.\n\nEmployment of Manual Scavengers and Creation of Dry Latrines (Prohibition) Act 1993 has made manual scavenging illegal.\n\nModern Japan still has areas with ongoing night soil collection and disposal. The Japanese name for the \"outhouse within the house\" style toilet, where night soil is collected for disposal, is \"kumitori benjo\" (汲み取り便所). The proper disposal or recycling of sewage remains an important research area that is highly political.\n\n\n"}
{"id": "26734587", "url": "https://en.wikipedia.org/wiki?curid=26734587", "title": "Orthotics", "text": "Orthotics\n\nOrthotics () is a medical specialty that focuses on the design and application of orthoses. An (plural: \"orthoses\") is \"an externally applied device used to modify the structural and functional characteristics of the neuromuscular and skeletal system\". An orthotist is the primary medical clinician responsible for the prescription, manufacture and management of orthoses.\nAn orthosis may be used to:\nOrthotics combines knowledge of anatomy and physiology, pathophysiology, biomechanics and engineering. Patients who benefit from an orthosis may have a condition such as spina bifida or cerebral palsy, or have experienced a spinal cord injury or stroke. Equally, orthoses are sometimes used prophylactically or to optimise performance in sport.\n\nOrthoses were traditionally made by following a tracing of the extremity with measurements to assist in creating a well-fitted device. Subsequently, the advent of plastics as a material of choice for construction necessitated the idea of creating a plaster of Paris mould of the body part in question. This method is still extensively used throughout the industry. Currently, CAD/CAM, CNC machines and 3D printing are involved in orthotic manufacture.\n\nOrthoses are made from various types of materials including thermoplastics, carbon fibre, metals, elastic, EVA, fabric or a combination of similar materials. Some designs may be purchased at a local retailer; others are more specific and require a prescription from a physician, who will fit the orthosis according to the patient's requirements. Over-the-counter braces are basic and available in multiple sizes. They are generally slid on or strapped on with Velcro, and are held tightly in place. One of the purposes of these braces is injury protection.\n\nUnder the International Standard terminology, orthoses are classified by an acronym describing the anatomical joints which they contain. For example, an ankle foot orthosis ('AFO') is applied to the foot and ankle, a thoracolumbosacral orthosis ('TLSO') affects the thoracic, lumbar and sacral regions of the spine. It is also useful to describe the function of the orthosis. Use of the International Standard is promoted to reduce the widespread variation in description of orthoses, which is often a barrier to interpretation of research studies.\n\nUpper-limb (or upper extremity) orthoses are mechanical or electromechanical devices applied externally to the arm or segments thereof in order to restore or improve function, or structural characteristics of the arm segments encumbered by the device. In general, musculoskeletal problems that may be alleviated by the use of upper limb orthoses include those resulting from trauma or disease (arthritis for example). They may also be beneficial in aiding individuals who have suffered a neurological impairment such as stroke, spinal cord injury, or peripheral neuropathy.\n\n\nA lower-limb orthosis is an external device applied to a lower-body segment to improve function by controlling motion, providing support through stabilizing gait, reducing pain through transferring load to another area, correcting flexible deformities, and preventing progression of fixed deformities. The term caliper or calipers remains in widespread use for lower-limb orthoses in the United Kingdom.\n\nFoot orthoses (commonly called \"orthotics\") are devices inserted into shoes to provide support for the foot by redistributing ground reaction forces acting on the foot joints while standing, walking or running. They may be either pre-moulded (also called pre-fabricated) or custom made according to a cast or impression of the foot. A great body of information exists within the orthotic literature describing their medical use for people with foot problems as well as the impact \"orthotics\" can have on foot, knee, hip, and spine deformities. They are used by everyone from athletes to the elderly to accommodate biomechanical deformities and a variety of soft tissue conditions. Custom-made foot orthoses are effective at reducing pain for people with painful high-arched feet, and may be effective for people with rheumatoid arthritis, plantar fasciitis or hallux valgus (\"bunions\"). For children with juvenile idiopathic arthritis (JIA) custom-made and pre-fabricated foot orthoses may also reduce foot pain. Foot orthoses may also be used in conjunction with properly fitted orthopaedic footwear in the prevention of foot ulcers in the at-risk diabetic foot.\n\nAn ankle-foot orthosis (AFO) is an orthosis or brace that encumbers the ankle and foot. AFOs are externally applied and intended to control position and motion of the ankle, compensate for weakness, or correct deformities. AFOs can be used to support weak limbs, or to position a limb with contracted muscles into a more normal position. They are also used to immobilize the ankle and lower leg in the presence of arthritis or fracture, and to correct foot drop; an AFO is also known as a foot-drop brace.\nAnkle-foot orthoses are the most commonly used orthoses, making up about 26% of all orthoses provided in the United States. According to a review of Medicare payment data from 2001 to 2006, the base cost of an AFO was about $500 to $700. An AFO is generally constructed of lightweight polypropylene-based plastic in the shape of an \"L\", with the upright portion behind the calf and the lower portion running under the foot. They are attached to the calf with a strap, and are made to fit inside accommodative shoes. The unbroken \"L\" shape of some designs provides rigidity, while other designs (with a jointed ankle) provide different types of control.\n\nObtaining a good fit with an AFO involves one of two approaches:\nThe International Red Cross recognizes four major types of AFOs:\n\nThe International Committee of the Red Cross published its manufacturing guidelines for ankle-foot orthoses in 2006. Its intent is to provide standardized procedures for the manufacture of high-quality modern, durable and economical devices to people with disabilities throughout the world.\n\nA custom-made ankle/foot orthosis for the treatment of patients having plantar ulcers is disclosed, which comprises a rigid L-shaped support member and a rigid anterior support shell hingedly articulated to the L-shaped support member. The plantar portion of the L-shaped member further comprises at least one ulcer-protecting hollow spatially located for fitted placement in inferior adjacency to a user's plantar ulcer, thus allowing the user to transfer the user's weight away from the plantar ulcer and facilitating plantar ulcer treatment. The anterior support shell is designed for lateral hinged attachment to the L-shaped member to take advantage of medial tibial flare structure for enhancing the weight-bearing properties of the disclosed orthosis. A flexible, polyethylene hinge member hingedly attaches the anterior support shell to the L-shaped member and securing straps securely attach the anterior support shell in fixed, weight-bearing relation about the proximal, anterior portion of the user's lower leg.\n\nA knee-ankle-foot orthosis (KAFO) is an orthosis that encumbers the knee, ankle and foot. Motion at all three of these lower limb areas is affected by a KAFO and can include stopping motion, limiting motion, or assisting motion in any or all of the three planes of motion in a human joint: sagittal, coronal, and axial. Mechanical hinges, as well as electrically controlled hinges have been used. Various materials for fabrication of a KAFO include but are not limited to metals, plastics, fabrics, and leather. Conditions that might benefit from the use of a KAFO include paralysis, joint laxity or arthritis, fracture, and others.\nAlthough not as widely used as knee orthoses, KAFOs can make a real difference in the life of a paralyzed person, helping them to walk therapeutically or, in the case of polio patients, on a community level. These devices are expensive and require maintenance. Some research is being done to enhance the design; even NASA helped spearhead the development of a special knee joint for KAFOs.\n\nA knee orthosis (KO) or knee brace is a brace that extends above and below the knee joint and is generally worn to support or align the knee. In the case of diseases causing neurological or muscular impairment of muscles surrounding the knee, a KO can prevent flexion or extension instability of the knee. In the case of conditions affecting the ligaments or cartilage of the knee, a KO can provide stabilization to the knee by replacing the function of these injured or damaged parts. For instance, knee braces can be used to relieve pressure from the part of the knee joint affected by diseases such as arthritis or osteoarthritis by realigning the knee joint into valgus or varus. In this way a KO may help reduce osteoarthritis pain, however, there is no clear evidence to advise people with osteoarthritis of the knee about the most effective orthosis to use or the best approach to rehabilitation. A knee brace is not meant to treat an injury or disease on its own, but is used as a component of treatment along with drugs, physical therapy and possibly surgery. When used properly, a knee brace may help an individual to stay active by enhancing the position and movement of the knee or reducing pain.\n\nProphylactic braces are used primarily by athletes participating in contact sports. Evidence about prophylactic knee braces, the ones football linemen wear that are often rigid with a knee hinge, indicates they are ineffective in reducing anterior cruciate ligament tears, but may be helpful in resisting medial and lateral collateral ligament tears.\n\nFunctional braces are designed for use by people who have already experienced a knee injury and need support to recover from it. They are also indicated to help people who are suffering from pain associated with arthritis. They are intended to reduce the rotation of the knee and support stability. They reduce the chance of hyperextension, and increase the agility and strength of the knee. The majority of these are made of elastic. They are the least expensive of all braces and are easily found in a variety of sizes.\n\nRehabilitation braces are used to limit the movement of the knee in both medial and lateral directions- these braces often have an adjustable range of motion stop potential for limiting flexion and extension following ACL reconstruction. They are primarily used after injury or surgery to immobilize the leg. They are larger in size than other braces, due to their function.\n\nScoliosis, a condition describing an abnormal curvature of the spine, may in certain cases be treated with spinal orthoses, such as the Milwaukee brace, the Boston brace, and Charleston bending brace. As this condition develops most commonly in adolescent females who are undergoing their pubertal growth spurt, compliance with wearing is these orthoses is hampered by the concern these individuals have about changes in appearance and restriction caused by wearing these orthoses.\nSpinal orthoses may also be used in the treatment of spinal fractures. A Jewett brace, for instance, may be used to facilitate healing of an anterior wedge fracture involving the T10 to L3 vertebrae. A body jacket may be used to stabilize more involved fractures of the spine. The halo brace is a cervical thoracic orthosis used to immobilize the cervical spine, usually following fracture. The halo brace allows the least cervical motion of all cervical orthoses currently in use; it was first developed by Vernon L. Nickel at Rancho Los Amigos National Rehabilitation Center in 1955.\n\nOrthotists are healthcare professionals who specialize in the provision of orthoses. In the United States, orthotists work by prescription from a licensed healthcare provider. Physical therapists are not legally authorized to prescribe orthoses in the U.S. In the U.K., orthotists will often accept open referrals for orthotic assessment without a specific prescription from doctors or other healthcare professionals.\n\nIn Canada, a Certified Orthotist CO(c) provides clinical assessment, treatment plan development, patient management, technical design, and fabrication of custom orthoses to maximize patient outcomes. To become CBCPO certified through Orthotics Prosthetics Canada (OPC) an applicant must successfully meet the following requirements:\n- be fluent in French or English; \n- be a Canadian citizen or legal landed immigrant; \n- graduate from an OPC approved post-secondary clinical Prosthetic and Orthotic program; \n- complete a minimum 3450 hours of Residency in Orthotics under the direct supervision of a Canadian certified orthotist; \n- successfully challenge the written, oral and practical national certification exams.\n\nUpon successful completion of the national certification exams, candidates are conferred the designation of Canadian Certified Orthotist CO(c).\n\nIn the UK orthotists assess patients, and where appropriate design and fit orthoses for any part of the body. Registration is with the Health and Care Professions Council and BAPO - The British Association of Prosthetists and Orthotists. The training is a B.Sc.(Hons) in Prosthetics and Orthotics at either the University of Salford or University of Strathclyde. New graduates are therefore eligible to work as an orthotist and/or prosthetist.\n\nPodiatrists are the other profession involved with foot orthotic provision. They are also registered with the Health and Care Professions Council . Podiatrists assess gait to provide orthotics to improve foot function and alignment or may use orthoses to redistribute stress on pressure areas for those with diabetes or rheumatoid arthritis.\n\nA licensed orthotist is an orthotist who is recognized by the particular state in which they are licensed to have met basic standards of proficiency, as determined by examination and experience to adequately and safely contribute to the health of the residents of that state. An American Board of Certification certified orthotist has met certain standards; these include a degree in orthotics, completion of a one-year residency at an approved clinical site, and passing a rigorous three-part exam. A certified orthotist (CO) is an orthotist who has passed the certification standards of the American Board of Certification in Orthotics, Prosthetics and Pedorthics. Other credentialing bodies who are involved in orthotics include the Board for Orthotic Certification, the pharmaceutical industry, the Pedorthic Footcare Association, and various of the professional associations who work with athletic trainers, physical and occupational therapists, and orthopedic technologists/cast technicians.\n\nFour universities including the Iran University of Medical Science, Isfahan University of Medical Science, University of Social Welfare and Rehabilitation Sciences and Iran Red Crescent University confer bachelor of science in the Prosthetics and Orthotics. Three universities including Isfahan University of Medical Science, the Iran University of Medical Science and University of Social Welfare and Rehabilitation Science also confer M.Sc. and Ph.D. New bachelor graduates are eligible to work as an orthotist and prosthetist after registration in the Medical Council of Iran.\n\n\n"}
{"id": "26758980", "url": "https://en.wikipedia.org/wiki?curid=26758980", "title": "Outline of television broadcasting", "text": "Outline of television broadcasting\n\nThe following outline is provided as an overview of and topical guide to television broadcasting:\n\nTelevision broadcasting: form of broadcasting in which a television signal is transmitted by radio waves from a terrestrial (Earth based) transmitter of a television station to TV receivers having an antenna. \n\nTelevision broadcasting can be described as all of the following:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a list of topics related to television broadcasting.\n"}
{"id": "50056803", "url": "https://en.wikipedia.org/wiki?curid=50056803", "title": "Pakathon", "text": "Pakathon\n\nPakathon is a Boston-based, registered global non-profit organization with eight global chapters in four countries: Pakistan, the United States, Canada, and Australia. Founded in 2013, Pakathon aims to create jobs through innovation and entrepreneurship by mobilizing the Pakistani diaspora. By working to foster a global network of like-minded individuals, investors and entrepreneurs, Pakathon aims to solve problems in sectors such as education, healthcare, gender equality, energy and law.\n\nThe organization's flagship event is a weekend-long hackathon hosted in chapter cities where students and professionals develop and pitch their projects to a panel of mentors. Top-performing teams are then invited to present their ideas to a global audience where they compete for funding, mentorship, and additional support, with the aim of advancing their ideas from conception to reality. Two winning teams are then selected: one from Pakistan and another from the international host cities.\n\nIn 2015, Pakathon partnered with the Higher Education Commission of Pakistan to expand Pakathon's innovation-centred programming in up to 50 Pakistani universities.\n\nPakathon's first winning team conceived the idea for ProCheck, a serialization system for authentic medicines in Pakistan. ProCheck allows customers and patients to verify authentic medicines using their mobile phones via text messaging. In November, 2015 ProCheck partnered with Ferozsons Labs, a leading manufacturer of pharmaceuticals in Pakistan, to serialize 35 million units of medicine using ProCheck's track and trace solution. As a result, more than 50,000 patients across the country will be able to distinguish between genuine and counterfeit medicines.\n"}
{"id": "35097299", "url": "https://en.wikipedia.org/wiki?curid=35097299", "title": "Portmanteau (luggage)", "text": "Portmanteau (luggage)\n\nA portmanteau is a piece of luggage, usually made of leather and opening into two equal parts. Some were large, upright, and hinged at the back and enabled hanging up clothes in one half, while others are much smaller bags (such as Gladstone bags) with two equally sized compartments. The word derives from the French word \"portemanteau\" (from \"porter\", \"to carry\", and \"manteau\", \"coat\"), which nowadays means a coat rack but was in the past also used to refer to a traveling case or bag for clothes.\n"}
{"id": "57720434", "url": "https://en.wikipedia.org/wiki?curid=57720434", "title": "Posted oil price", "text": "Posted oil price\n\nThe posted price of oil was the price that oil companies offered to purchase oil from oil-producing governments. This price was set by the oil companies and used to calculate the share of oil revenues that oil producing countries would receive. Between 1957 and 1972, the posted price was greater than the market price of crude oil. Between 1961 and 1970 the market price hovered between $1.30 and $1.50 per barrel, while the posted price was a constant $1.80.\n\nBefore Standard Oil was broken up by the United States Supreme Court in 1911, they used to \"post\" the price they were willing to pay for crude oil. Until 1895, crude oil was sold on the exchange at Oil City, Pennsylvania, but in January 1935 the Seep Purchasing Company, which purchased 80% of Pennsylvania and Ohios crude oil production for Standard Oil, posted a notice that the price of oil would \"be as high as the market of the world will justify, but will not necessarily be the price bill on the exchange for certificate oil\".\n\nThe system continued after 1911 where large buyers would post a fixed buying price. Historically, the posted price in the Middle East, or Venezuela was calculated based on the CIF New York price, which was itself based on the FOB Gulf of Mexico price. In other words, the posted price in New York was the FOB Gulf of Mexico price plus the cost of freight. Thus, if the posted price per barrel of crude oil was $3.16 in New York, after deducting the freight between New York and Kuwait ($1.19) and the US tax on oil ($0.11 per barrel), the posted price in Kuwait would be $1.87 per barrel.\n\nBy the 1950s, fixed royalty payments had been replaced by 50-50 profit sharing arrangements following the 1950 ARAMCO deal. The posted price, sometimes also called the published price, is not the price that is actually received by oil producing governments. This price, historically set by the oil companies and later by OPEC, is used to calculate the taxes and royalties that will be paid to governments under profit-sharing agreements. The use of the posted price as a pricing basis for the 50-50 share agreements varied from country to country; the posted price was not adopted as the basis in Saudi Arabia until 1955, but had been used in Iraq since the Iraqi governments revised agreement with the Iraqi Petroleum Company was negotiated in 1952.\n\nAfter oil companies lowered the posted price, five oil producing countries formed OPEC in 1960: Venezuela, Saudi Arabia, Kuwait, Iran and Iraq. In 1970 the price of crude oil was still $1.35 and the supply of oil exceeded demand. Due to a decline in value of the US dollar relative to gold in 1971, the Tehran Agreement of 1971 was amended to include an 8.49% increase in the posted price of oil. Additionally, the amended agreement further stipulated that every quarter the posting price would be adjusted based on an index that would be calculated by comparing value changes in the currencies of nine major industrialized countries relative to the dollar. However, this indexing arrangement was canceled after the 1973 oil crisis. The Tripoli Agreement of 1971 was signed by the OPEC members who exported across the Mediterranean, rather than through the Persian Gulf — namely Libya, Algeria and Iraq; not only did the posted price increase, but the profit sharing arrangement went from 50-50 to 55-45 in favor of the producing countries.\n\nDuring the 1973 oil crisis, OPEC turned down an offer to increase the posted price by 15%, instead raising it from $3.00 to $5.11 per barrel. Arab oil producers also decided to cut production and embargoed the United States. Soon after, at a meeting in Tehran in January 1974, OPEC raised the posted price again — this time, to $11.65 per barrel. Between the start of the Arab-Israeli War of 1973 and the Tehran meeting the posted price of oil increased four-fold. \n\nBetween 1973 and 1980, the value of exports from Arab oil-producing countries increased from less than $23 billion to $220 billion. Saudi Arabia's exports rose from $8 billion to $108 billion in same time period. The oil-producing countries agreed to return to normal production levels in March 1974, and the price of crude oil \"froze\" until 1979. Although the posted price in 1978 was $12.70, after calculating for inflation and currency fluctuations, the real price of oil purchased with Japanese yen was $6.67 in 1978, compared to $9.56 in 1974. \n\nA number of studies were published in the 1980s that attempted to show causality between OPEC's the posted price and the market price of oil.\n\nThe following table shows variations in the posted price and the revenue received by OPEC countries between 1964 and 1975 (in US dollars):\n"}
{"id": "3786139", "url": "https://en.wikipedia.org/wiki?curid=3786139", "title": "Pyrotechnician", "text": "Pyrotechnician\n\nA pyrotechnician is a person who is responsible for the safe storage, handling, and functioning of pyrotechnics and pyrotechnic devices. Although the term is generally used in reference to individuals who operate pyrotechnics in the entertainment industry, it can include all individuals who regularly handle explosives. However, individuals who handle more powerful materials for commercial, demolition, or military applications are generally referred to as explosive technicians.\n\nIt is becoming more common in different countries and regions for individuals involved with the storage, handling and operation of pyrotechnics to have some form of license or certification. The specific requirements for certification, along with any restrictions or prohibitions, however, can vary wildly from one jurisdiction to the next. Most jurisdictions do require some type of minimum professional fireworks training before being allowed to apply for certification.\n\nIn Australia, individuals using, handling or storing pyrotechnics must receive government certification. All individuals must meet certain minimum requirements, including being at least 18 years of age and having received approved training, including working under the direct supervision of a licensed pyrotechnician, before being issued a license. All pyrotechnicians are required to maintain a log of their experience and to submit this log annually during the license renewal process.\n\nThere is a push to form a recreational pyrotechnic association, ARPA (Australian Recreational Pyrotechnic Association) that would benefit those who do not wish to shoot professional displays or acquire a professional license and would rather discharge smaller consumer fireworks that do not pose a higher risk as the professional devices. The association still has guidelines and requirements that form part of a safety strategy, but less training and expenses would be the benefit of the association as individuals do not need the training that the professional devices require.\n\nIn Canada, individuals using, handling or storing pyrotechnics must receive government certification, administered by the Explosives Regulatory Division of Natural Resources Canada. All individuals must meet certain minimum requirements, including being at least 18 years of age and having received government training, before being issued a license. All pyrotechnicians, of any classification or certification, are required to maintain a log of their experience. \n\nCertification for the use of proximate (indoor) pyrotechnics in Canada is separate from certification for display fireworks, and different government training and certification must be obtained.\n\nThe available licenses for the use of proximate pyrotechnics are sub-divided into one of 6 categories.\n\nThe Pyrotechnician classification is the primary classification in Canada. Individuals may use a restricted list of pyrotechnics individually, but may fabricate, handle, and set up a broader list of pyrotechnics typically prohibited to the pyrotechnician under the direct supervision of a higher-licensed individual. After two years of verifiable experience with a sufficient assortment of pyrotechnics, pyrotechnicians may apply for advancement to Senior Pyrotechnician certification. Certification is valid for five years.\n\nThe Senior Pyrotechnician classification is the intermediate classification in Canada. Individuals can supervise other pyrotechnicians and use any pyrotechnic device or product approved for use in Canada, and may also fabricate, handle, and set up a special-purpose or custom-fabricated pyrotechnics typically prohibited while under the direct supervision of a Special Effects Technician. After two years of verifiable experience with a sufficient assortment of pyrotechnics, pyrotechnicians may apply for advancement to Special Effects Technician certification. Certification is valid for five years.\n\nThe Special Effects Technician classification is the most advanced classification in Canada. Individuals may use any pyrotechnic device or product approved for use in Canada, and may also fabricate, handle, and set up a special-purpose or custom-fabricated pyrotechnics device typically prohibited. A Special Effects Technician may also apply for special certification to use restricted material, such as detonating cord and exploding bolts. Certification is valid for five years.\n\nAuthorities Having Jurisdiction (AHJ) are individuals certified by the Explosive Regulatory Division to inspect and approve pyrotechnic events, including members of agencies associated with pyrotechnics such as fire departments and police forces. Unlike other certification classes, Authorities Having Jurisdiction are not required to pay fees to attend training sessions.\n\nPyrotechnicians based outside of Canada participating in the production of a pyrotechnics special effects event in Canada are required to obtain a Visitor Card at either a Pyrotechnician or Special Effects Technician equivalent level. \nA certified Canadian technician of the applicable class must also be employed for the production. Visitor Cards are valid for one year.\n\nAlternately, pyrotechnicians based outside of Canada may apply for Canadian certification, provided they meet the minimum requirements and attend the Explosive Regulatory Division certification training.\n\nThere is no national certification or licensing requirements for pyrotechnicians in the United States, however, many individual states implement their own certification or licensing programs. Not all states have reciprocity agreements in regards to honouring another state's certification, so it is not uncommon for pyrotechnicians to have certifications from multiple states. Certain states require certification to store, handle and operate fireworks but not proximate (indoor) pyrotechnics; other states use the same certification for all pyrotechnics.\n\nProfessional pyrotechnic companies typically have licenses issued by the Bureau of Alcohol, Tobacco, Firearms and Explosives with regard to manufacturer and distribution of pyrotechnic materials. A special category of driver's license is required for anyone who wishes to transport pyrotechnic materials by vehicle.\n\n\n"}
{"id": "52305608", "url": "https://en.wikipedia.org/wiki?curid=52305608", "title": "Reply (company)", "text": "Reply (company)\n\nReply is a company that specialises in consulting, system integration and digital services, with a focus on the design and implementation of solutions based on the web and social networks.\n\nIn 2017 Reply had a revenue of €884.4 million and over 6,000 employees.\n\nFounded in 1996 in Turin, Italy, Reply utilises a network model consisting of companies operating in different sectors, including big data, cloud computing, digital media and the internet of things.\n\nSince 2006 the company has expanded its operations in Europe, particularly in England and Germany, opening new offices as well as relying on existing acquisitions.\n\nReply’s revenue increased from €33.3 million in 2000, the year the company was listed on the STAR segment of the Italian Stock Exchange (Borsa Italiana), to €705.6 million in 2015.\n\nReply operates in the following sectors:\n\nReply group official website\n"}
{"id": "16821000", "url": "https://en.wikipedia.org/wiki?curid=16821000", "title": "Road space rationing", "text": "Road space rationing\n\nRoad space rationing, also known as alternate-day travel, driving restriction, no-drive days, (; ; ) is a travel demand management strategy aimed to reduce the negative externalities generated by urban air pollution or peak urban travel demand in excess of available supply or road capacity, through artificially restricting demand (vehicle travel) by rationing the scarce common good road capacity, especially during the peak periods or during peak pollution events. This objective is achieved by restricting traffic access into an urban cordon area, city center (CBD), or district based upon the last digits of the license number on pre-established days and during certain periods, usually, the peak hours.\n\nThe practical implementation of this traffic restraint policy is common in Latin America, and in many cases, the road rationing has as a main goal the reduction of air pollution, such as the cases of México City, and Santiago, Chile. São Paulo, with a fleet of 6 million vehicles in 2007, is the largest metropolis in the world with such a travel restriction, implemented first in 1996 as measured to mitigate air pollution, and thereafter made permanent in 1997 to relieve traffic congestion. More recent implementations in Costa Rica and Honduras have had the objective of reducing oil consumption, due to the high impact this import has on the economy of small countries, and considering the steep increases in oil prices that began in 2003. Bogotá, Quito, and La Paz, Bolivia also have similar driving restriction schemes in place.\n\nAfter a temporary implementation of road space rationing to reduce air pollution in Beijing during the 2008 Summer Olympics, local officials put in place several permanent rationing schemes to improve the city's air quality. , another 11 Chinese cities have similar restriction schemes in place. Also, temporary driving restrictions to reduce cars on the streets by half during severe pollution events have been implemented in Paris and surrounding suburbs in March 2014, March 2015, and December 2016; in Beijing twice in December 2015, and one more time in December 2016; and also in Rome and Milan for several days in December 2015. A similar alternate-day travel temporary scheme was implemented in New Delhi as a two-week trial in January 2016. A temporary ban on diesel cars was implemented in Oslo on municipal roads in January 2017.\n\nThe earliest known implementation of road space rationing took place in Rome, as carriages and carts pulled by horses created serious congestion problems in several Roman cities. In 45 B.C. Julius Caesar declared the center of Rome off-limits between 6 a.m. and 4 p.m. to all vehicles except for carriages transporting priests, officials, visitors, and high-ranking citizens.\n\nSchemes rationing access based on number plate have mixed results. If used infrequently or temporarily the alternate-day travel policy can have some impact. However, if used as a long term measure, inequality issues might arise, as wealthier people can afford to own two cars with opposite-parity number plates, to circumvent any restrictions, with the second vehicle being often older and therefore more polluting. Cities such as Tehran which have used such schemes are now looking to more sustainable methods of traffic and emissions control, such as low emission zone or traffic limited zones as used in Europe. Access regulations have often been found to be effective, in reducing congestion, traffic and pollution.\n\nRoad space rationing based on license numbers has been implemented in cities such as Athens (1982), Santiago, Chile (1986 and extended 2001), México City (1989), Metro Manila (1995), São Paulo (1997), Bogotá, Colombia (1998), La Paz, Bolivia (2003), San José, Costa Rica, (2005) countrywide in Honduras (2008), and Quito, Ecuador (2010). All these cities restrain a percentage of vehicles every weekday during rush hours or for the entire day. When the restriction is based in two digits a theoretical 20% reduction of traffic is expected. Cities with serious air quality problems, such as México City and Santiago use more digits to achieve greater reductions in air pollution, and even the prohibition can be for more than one day a week. In Bogotá, Colombia from 2009 the plate restriction was extended from peak periods to the whole day (from 06:00 to 20:00 hours) in the whole city.\n\nBertrand Delanoë, the mayor of Paris, proposed to impose a complete ban on motor vehicles in the city's inner districts, with exemptions only for residents, businesses, and the disabled, as a three-part plan to implement during a seven-year period. This proposal was made in 2005, in the context of Paris' bid to host the 2012 Summer Olympics which ended up being won by London.\n\nDuring the discussions regarding the proposal to introduce congestion pricing in New York, the commission created in 2007 by the New York State Legislature to evaluate other traffic relief options, considered road space rationing based on license plates as an alternative to congestion pricing. The proposal stalled in April 2008 as the legislature decided not to vote the proposed plan.\n\nRising traffic in Athens during the 90s' led to the development of the Dactylius (Greek: Δακτύλιος, meaning ring) around central Athens. The Dactylius does not require drivers to pay in order to enter the areas subjected to the Dactylius' restrictions. Instead, the scheme depends on the parity of the date and of the vehicle's registration plate, the vehicle type as well as the time of the week/month. There are three Dactylius areas: The Inner, the Outer and the Green, each one with its own policies.\n\nThen mayor Enrique Peñalosa, introduced in Bogotá, Colombia in 1998 a driving restriction program, \"Pico y placa\" (literally in Spanish: \"peak and license plate\") to reduce traffic congestion during rush hours. The system restricts traffic access into a pre-established urban area for vehicles with license plate numbers ending in certain digits on pre-established days and during certain hours based on the last digit of the licence plate numbers. Initially the system restricted traffic between 7:00 - 9:00 a.m. and between 5:30 - 7:30 p.m., Monday through Friday, for two days for each registered vehicle.\n\nThen mayor Antanas Mockus extended the restriction for one hour in 2001. A complementary program called \"El Pico y Placa Ambiental\" (literally in Spanish: \"environmental peak and license plate\") was implemented by then mayor Luis Eduardo Garzón in 2006, expanding the restriction to public transportation vehicles, including both bus and taxi services. Four numbers were restricted every day for private use vehicles, and two for public transportation vehicles. Since 2002 Bogotá's scheme switched the combinations of days and numbers every year, making it harder to circumvent the restriction by buying another car.\n\nStarting in February 2009, then mayor Samuel Moreno Rojas extended the restriction from 6 a.m. to 8 p.m., Monday through Friday. This extension was issued as a temporary measure until public works related with the Transmilenio BRT were completed. In July 2012, then mayor Gustavo Petro reduced the hours of the restriction from 14 to 7 hours per day, to restrict access only between 6:00- 8:30 a.m. and between 3:00 - 7:30 p.m. In addition, five neighborhoods were released from the restriction, Usme, Rafael Uribe Uribe, Tunjuelito, San Cristóbal and Ciudad Bolívar. Also, under the modified scheme five ending numbers are restricted every day between Monday and Friday, license plated ending on odd-numbers are restricted on odd-days of the month, and even-numbers on even-days.\n\nSince December 2014, exempted vehicles include passenger cars with three or more passengers including the driver; properly registered vehicles for use by people with disabilities; all-electric vehicles; emergency vehicles, such police cars, ambulances, and fire trucks; properly identified public utilities vehicles, traffic control and towing vehicles; school buses; motorcycles; cash-in-transit armored vehicles; funeral vehicles; and press, judiciary, diplomatic, presidential motorcade, and security scort vehicles.\n\nSimilar schemes with the same name have been implemented in several Colombian cities, including Armenia, Barranquilla, Bucaramanga, Cali, Cartagena, Manizales, Medellín, and Pereira.\n\nMexico City started in November 1989 its driving restriction program, \"Hoy no Circula\" (literally in Spanish: \"today [your car] does not circulate\", known as \"No-drive days\"), which consisted of prohibiting the circulation of 20% of vehicles from Monday to Friday depending on the last digit of their license plates. Record levels of ozone and other airborne pollutants led the city government to implement the scheme. The program was planned to apply only during the winter, when air pollution is at its worst. Winter follows the rainy season when thermal inversion, an atmospheric condition which traps smog and pollution close to the ground, increases air pollution noticeably. However, the program was made permanent at the end of the 1990 winter season.\n\nThe program is intended to improve the air quality of Mexico City. The restriction is based on the last digit of the license plate. Two numbers are restricted to travel every day between from 5 a.m. to 10 p.m. The restrictions apply to the entire Mexico City metropolitan area, therefore, a similar coordinated program operates within the State of México, including the 18 neighboring municipalities which surrounds Mexico City on three sides: Atizapán de Zaragoza, Coacalco de Berriozabal, Cuautitlán, Cuautitlán Izcalli, Chalco, Chimalhuacan, Chicoloapan, Ecatepec de Morelos, Huixquilucan, Ixtapaluca, La Paz, Naucalpan de Juárez, Nezahualcóyotl, Nicolás Romero, Tecámac, Tlalnepantla de Baz, Tultitlán and Valle de Chalco Solidaridad.\n\nHoy No Circula is coupled with an exhaust monitoring program, known as \"Verificación\" in Spanish (verification), whereby a car's pollutant emissions are analyzed every six months. A colored sticker based on a vehicle’s license plate number is affixed to each vehicle following an emissions test, indicating whether a vehicle is exempt from the program or not. Hybrid electric vehicles and electric vehicles are exempted. There are other restrictions that are applicable to non-local vehicles and foreigners. In June 2015, the Supreme Court of Justice of the Nation ruled in favor a constitutional challenge, and ordered that passenger cars with model year older than 2007 shall be restricted based on their actual tailpipe emissions, and not on how old the car is.\n\nTaxis, buses, police cars, ambulances, fire trucks, commercial vehicles operating with liquid propane gas, and commercial vehicles transporting perishable goods are exempt. In 2008 the scheme in Mexico City was extended to limit driving into the city one Saturday every month, but only applies to cars that are more than 10-years old (sticker 2). Foreign-plated vehicles without emissions stickers are restricted all day every Saturday. Similar alternate-day travel restriction programs have been implemented in Pachuca, Puebla and Toluca.\n\nThe driving restriction program was initially successful in reducing pollution levels, as carbon monoxide (CO) fell by 11%. Compliance with the program is near universal. However, as the restriction was made permanent residents began buying second cars to get around the ban, usually used and old polluting cars. A 2008 study from the University of Michigan found that there is no evidence that the restrictions have improved air quality. Evidence from additional sources indicates that the restrictions led to an increase in the total number of vehicles in circulation and the long-term impact of the scheme on CO levels has been a 13% rise.\n\nAn alternate-day travel scheme was introduced in San José, Costa Rica, in August 2005. The goal of the restriction program was to reduced oil consumption with the purpose of mitigating the negative effects of high international oil prices in the Costa Rican economy. The program was implemented when the price of an oil barrel was at up from in early 2003. Fuel and oil imports represented in 2007 a 5.6% of the country's GDP, up from 2% ten years earlier.\n\nInitially the driving restriction was applied to enter the capital's central business district and the scheme is based on the last number of the license plate, restricting two numbers per day, Monday through Friday. The travel restrictions were issued initially only during the rush hours, from 7:00-8:30 a.m and 4:00-5:30 p.m.\n\nIn June 2008 the restricted cordon area was expanded until San José's Boulevard Circunvalación peripheral ring. The expansion aim was to attain further reductions in oil consumption, as oil prices continued to rise and reached per barrel in June 2008. Government official estimated that at this price, oil imports will reach , representing about a third of the country's export earnings and about 9% of the country's GDP estimated for 2008. As the implemented scheme only reduced fuel consumption by 5%, the government decided to expand the restriction hours beginning in July 2008. The road space rationing was expanded to 13 hours for passenger cars and light-duty commercial vehicles, from 6 a.m. through 7 p.m. Heavy-duty commercial vehicles were included in the restriction program but only during rush hours.\n\nThe alternate-day driving program was suspended in June 2009 as a result of a Constitutional Court ruling in favor a constitutional challenge. The court ruled that the policy infringed the constitutional freedom of movement right, and also that the economic sanctions for this violation were not supported by the existing legal framework. The temporary end of the program resulted in heavy traffic jams all over the city, as traffic volumes within the restricted cordon area increased by 20% to 25%. The driving restriction scheme was restored one month later by an Executive Decree based on amendments included in the Traffic Law passed by Congress in 2008. Nevertheless, this time the program was justified on San José's worsening traffic congestion levels instead of the economic impact of high oil prices.\n\nSince October 2012 hybrid electric vehicles and all-electric cars are exempted from the driving restriction as part of Costa Rica's government policy to promote the use of clean energy in the country. As a result of multiple legal challenges, traffic engineering authorities decided to conduct periodic effectiveness evaluations of the program. Traffic authorities announced in October 2014 that the results of six studies found that the alternate-day travel restriction reduced the number of vehicles entering downtown San José every working day between 14% to 16%.\n\nDrving restrictions were introduced in Santiago in 1986, as a measure to mitigate Chile's capital high levels of air pollution. The scheme is based on the last number of the license plate, and two number are banned from entering the city between Monday and Friday.\n\nInitially the restriction applied only to passenger vehicles without a catalytic converter with the aim to reduced particulate matter emissions. During critical air pollution events, classified as alert, pre-emergency or emergency, the number of cars restricted to travel are increased by adding additional last number plates. Beginning with pre-emergency state, the restriction might include vehicles with catalytic converter. As the number of registered vehicles with the emission control device surpassed those without it, authorities decided in 2008 to increase the number of vehicles restricted to enter every day to the city, by increasing the restricction to the four last digits instead of the initial two. This scheme produces a theoritical reduction of 40% of vehicles entering the city on a work day.\n\nSão Paulo is the largest metropolis in the world with a permanent alternate-day travel restriction (). The scheme was first implemented in 1995 as a trial on a voluntary basis, and then as a mandatory restriction implemented in August 1996 to mitigate air pollution, and thereafter made permanent in June 1997 to relieve traffic congestion. The driving restriction applies to passenger cars and commercial vehicles, and it is based on the last digit of the license plate. Two numbers are restricted to travel every day between 7 a.m. to 10 a.m. and 5 p.m. to 8 p.m. from Monday through Friday.\n\nVehicles exempted from the restriction include buses and other urban transportation vehicles, school buses, ambulances and other medical services vehicles, mail and fire cars and trucks, police and military vehicles, cash-in-transit armored vehicles, vehicles delivering perishable food products, properly registered vehicles for use by people with disabilities, and other public utility vehicles. In May 2014 the City Council approved a law to exempt from the restriction plug-in electric vehicles, hybrid electric vehicles and fuel-cell vehicles with a license plate registered in the city. The benefits for electric-drive vehicles went into effect in September 2015.\n\nThe no-drive day in Jakarta is known as \"Ganjil Genap\" (even and odd [days] in Indonesian). This policy imitates what has been done in Beijing during Olympics. Policeman checks license plate's last digit on the entrance of avenue or expressway. A car with even license plate (for example B 1000 LA, obsolete plate of Idris Sardi car) is allowed to pass in \"even days\", but will be fined when they pass that road in \"odd days\". Drivers might get even/odd days information from social media to radio stations. \nThe \"Ganjil Genap\" first time implemented in Sudirman Avenue in 27 July 2016. \"Ganjil Genap\" was implemented due to \"3-in-1 policy\" termination started from 16 May 2016 by former Jakarta Governor Basuki Tjahaja Purnama (or Ahok). 3-in-1 policy banned vehicles containing less than 3 passengers. 3-in-1 existed for 12 years and a half (23 December 2003-16 May 2016), but abandoned by Basuki due to a trick known as \"joki 3 in 1\". He said that \"joki 3 in 1\" practice often exploited kids, just to fill the car to 3 people when passing checkpoints. Police authorities also admitted that control of 3-in-1 was harder than license plate checking, due to \"joki 3-in-1\" hack.\nIn 2018, license plate checking also applied on expressways bound to Jakarta suburban area (Jabodetabek). License plates are checked in toll booths Bekasi, Cibubur (border of Jakarta and Depok) and Tangerang. Only overcrowded toll booths apply this ban. Existing \"Ganjil Genap\" in Jakarta was extended from 6am starts from 23 April 2018.\n\nIn May 2016, Paris launched a scheme called \"Paris Respire\" (literally \"Paris breathes\") as part of which certain areas of the city are closed to vehicular traffic on Sundays and public holidays.\n\nTemporary driving restrictions were imposed in Beijing from December 8 to 10, 2015, as part of the smog mitigation measures provided for in Beijing’s red alert for hazardous smog, the first such alert issued ever. The smog alert system was put in place in 2013, and a red alert should go into effect if there is a prediction that the air quality index will stay over 200 for more than 72 hours. On the evening of December 7 the index was 253 according to Beijing’s authorities. Under a red alert half of the city’s cars are ordered off the streets through a temporary alternate-day travel scheme based on the cars' license plate numbers. Only cars with even-numbered license plates were allowed on the roads during the first day of the restriction. Electric cars are not subject to the driving restriction, as a government incentive to promote the use of cleaner vehicles.\n\nAccording to the Ministry of Environmental Protection, the combined effect of all the restrictions imposed reduced pollutant emissions in Beijing by 30% during day one of the city's first red alert for smog. Environmentalist from Beijing University of Technology estimated that without the measures, the density of PM would have risen by 10% in that period. A second red alert for pollution was issued on December 18, 2015. Temporary driving restrictions were imposed for four days, beginning at 7 a.m. on December 19 and ending on the 23rd at midnight.\n\nOn 16 December 2016, Beijing authorities declared a five-day pollution “red alert” due to a heavy pollution event. Among other measures, about half the cars were restricted through a temporary alternate-day travel scheme, and older and “dirty” high-emissions vehicles were forbidden to circulate. Public transport services in the city were increased, with about 3,600 buses on duty. The Ministry of Environmental Protection reported that 21 other cities across north and central China had also declared pollution red alerts, including Tianjin, Shijiazhuang, Taiyuan, and Zhengzhou. The red alert was lifted on 22 December 2016 as the winds cleared pollution away the night before.\n\nIn December 2015, several Italian cities implemented temporary driving restrictions due to severe air pollution levels. The restrictions were issued in Rome, Milan and other cities in the Lombardy region, including Pavia, Buccinasco, Cesano Boscone, Cernusco sul Naviglio, Bresso, Cinisello Balsamo, Cormano, Corsico, Cusano Milanino, Paderno Dugnano and Sesto San Giovanni. Italy had the most pollution-related deaths in Europe in 2012. Over 84,000 people in the country died prematurely owing to bad air quality, according to the European Environment Agency (EEA).\n\nMilan was named as Europe's most polluted city in 2008 and remains among the worst on the continent. City officials have limited traffic on several occasions since 2007 to curb bad air quality. Due to record high air pollution levels, Milanese authorities ban cars, motorcycles and scooters for six hours a day, between 10 a.m. and 4 p.m. for three days during the last week of December 2015. Local authorities introduced a special \"anti-smog\" all-day public transport ticket for (~). Neighboring towns and municipalities in the Lombardy region, including Pavia, Cinisello Balsamo, Paderno Dugnano, and Sesto San Giovanni, also implemented the temporary driving restriction. Electric vehicles and carsharing cars are exempted from the ban.\n\nAn alternate-day travel scheme was implemented in Rome to curb severe air pollution in the city, which has high concentrations of particulate matter and nitrogen dioxide. The scheme was in force for several days during the last two weeks of December 2015. The driving restriction is based on the last digits of license plate numbers and was implemented for a total of nine hours, from 7:30 to 12:30 and 16:30 until 20:30. To promote ridership by public transportation, (~) single-ride transit tickets became passes valid all day. Environmentally friendly vehicles, such as hybrids and natural gas vehicles are exempt from the restriction. The most polluting vehicles, such as gasoline-powered cars compliant with Euro 0 and 1 standards, and diesel cars up to Euro 2, can not enter the city independently of the number plate. Rome authorities resorted to limit traffic in the city on several occasions during the fall of 2015 due to high air pollution. \n\nIn January 2017 a combination of cold, still winter weather and poor air quality prompted Oslo city authorities to ban diesel-powered cars from municipal roads to combat rising air pollution for at least two days. The ban did not apply on the national motorways. This was the first time ever Oslo implemented a ban of this type after the city council agreed on the use of such a measure in February 2016. The diesel ban went into effect from 6 a.m. until 10.p.m on 17 January 2017, and motorists violating the ban were fined 1,500 kroner (~ ). The temporary ban scheduled for 18 January was lifted after officials said the weather forecast indicated that higher altitude winds would clear the air.\n\nThe restriction did not apply to heavy vehicles with Euro VI technology, gasoline-powered cars, electric cars and plug-in hybrid vehicles, emergency vehicles, goods transport, diplomatic vehicles, handicap transport, public service vehicles, and cars carrying a patient to a doctor appointment. The restriction angered some motorists, who were encouraged by Norwegian authorities in 2006 to opt for diesel vehicles, which at the time were considered a better environmental choice than gasoline-powered cars.\n\nOn March 17, 2014, a partial driving restriction was imposed in Paris and its inner suburbs based on license plate numbers. The measure was issued by the city government in order to mitigate a peak in air pollution, caused by particulate matter (PM 10) attributable to vehicle emissions. Cars with even-numbered license plates and commercial vehicles over 3.5 tons were banned from entering the city from 5:30 a.m. until midnight. Electric and hybrid cars, natural gas-powered vehicles and carpools with three or more passengers were exempted. Only once before this type of restriction had been implemented in the city for one day in 1997. The week before the traffic restriction was imposed, the government also reduced speed limits around Paris by per hour, provided all public transportation for free, and the short-term subscriptions of the Vélib bikesharing program, and the first hour of the Autolib carsharing service were free. The measure was not extended to the following day due to the improvement of air quality.\n\nAnother peak in air pollution affected Paris and Northern France in mid March 2015. The Mayor of Paris, Anne Hidalgo, requested the central government to implement a driving restriction to mitigate the problem. The pollution index in Paris at 93 micrograms per cubic meter (mcg/m3) on Friday 20, 2015, due to increase amounts of pollutant PM10. The accepted limit for PM10 is set at 50 mcg/m3, and the safe limit or alert threshold is set at 80 mcg/m3. As the pollution episode continued on Saturday 21 according to Airparif measurements, the central government imposed a driving restriction on Monday 23 affecting cars with even-numbered license plates and commercial vehicles over 3.5 tons. Taxis, ambulances, carpools with three or more passengers, electric cars and other environmentally friendly vehicles were exempted. As in the 2014 episode, complementary measures were implemented including reduced speed limits in the city, free public transportation, free residential parking, and free short-term use for subscribers of bike and carsharing services. The restriction was implemented in Paris and 22 towns located in the administrative region of Île-de-France.\n\nIn early December 2016, Paris and its surrounding region suffered for a week the longest and most intense winter pollution episode in 10 years. A driving restriction went into effect in Paris and 22 surrounding towns for four days, from Tuesday 6 to Friday 9, due to the persistence of pollution of fine particles and nitrogen dioxide. The restriction was extended for the first time in the cities of Villeurbanne and Lyon on Friday December 9. This was the fourth time in twenty years that alternating traffic is implemented in the capital, but the first time it is maintained for several days in a row.\n\nAccording to the World Health Organization, in 2014, New Delhi had the most polluted air of about 1,600 cities the organization tracked around the world. According to India's Central Pollution Control Board, the city's air pollution had been in the severe category on nearly three-quarter of the days in November 2015. The Delhi High Court asked the government to take action to curb air pollution on 30 November 2015.\n\nIn an attempt to mitigate severe air pollution in New Delhi, which gets worst during the winter, a temporary alternate-day travel scheme for cars using the odd- and even-numbered license plates system was announced by Delhi government in December 2015. In addition, trucks were allowed to enter India's capital only after 11 p.m., two hours later than the existing restriction. Almost 9 million vehicles are registered in Delhi. The driving restriction scheme went into effect as a trial for an initial period of 15 days, from 1 to 15 January 2016. The restriction was in force from 8 a.m. till 8 p.m., and traffic was not restricted on Sundays. The scheme was expected to take more than a million private cars off the road every day.\n\nPublic transportation service was increased during the restriction period. A total of 27 exemptions to the restriction were allowed by the government, including all motorcycles, benefiting more than 5 million motorcyclists, all female drivers traveling alone, to ensure women’s security, and several categories of official vehicles, including those of high-ranking officials. During the first day of the restriction there was acceptance by the general population.\n\nA petition was filed in the Delhi High Court against the government in order to stop the implementation of the restriction driving scheme. On 9 December 2015, the Court decided to put on hold the analysis of the petition until more details of the scheme are defined by the government, and considering that no official notification has been issued by a public agency. A hearing was scheduled for December 23 for further anaylisis. The most contested exemption is the one for female drivers, and a legal petition was filed. A court had to decide whether it is discriminatory to allow women to drive around Delhi while some of males will be forced to leave their vehicles at home.\n\nOn 16 December 2015, the Supreme Court of India mandated several restrictions to curb pollution. Among the measures, the court banned the sale of new cars that have diesel engines and sport utility vehicles with an engine greater than 2000 cc until 31 March 2016. The court also ordered all taxis in the Delhi region to switch to compressed natural gas by 1 March 2016. Transportation vehicles than are more than 10 years old were banned from entering the capital.\n\nOn July 20, 2008, Beijing implemented a temporary road space rationing scheme based on plate numbers in order to significantly improve air quality in the city during the 2008 Summer Olympics. Enforcement was carried out through an automated traffic surveillance network. The rationing was in effect for two months, between July 20 to September 20, as the Olympics were followed by the Paralympics from September 6 until 17. The restrictions on car use was implemented on alternate days depending on the plates ending in odd or even numbers. This measure was expected to take 45% of the 3.3 million car fleet off the streets. In addition, 300,000 heavy polluting vehicles were banned from July 1, and the measure also prohibited access to most vehicles coming from outside Beijing. Authorities decided to compensate car owners for the inconvenience, by exempting them from payment of vehicle taxes for three months.\n\nA pilot test was conducted in August 2007 for four days, restricting driving for a third of Beijing's fleet, some 1.3 million vehicles. A 40% daily reduction of vehicle emissions was reported. A previous test carried out in November 2006 during the Sino-African Summit show reductions of 40% in NOx auto emissions.\n\nThe driving restriction during the Olympics was so successful in cleaning the air and relieving traffic congestion, that a modified version of the scheme was made permanent afterward in October 2008, now banning 20% of the vehicles on a given weekday instead of half the vehicles as implemented during the Olympics. Also a ban on heavy trucks from entering the city during the day was implemented, and the oldest most polluting automobiles, called \"yellow-label\" cars, after the sticker fixed to their windshields, are banned from entering the city center. In July 2009 a nationwide car scrappage program was implemented offering rebates for trade in old heavy polluting cars and trucks for new ones. , in addition to Beijing, another 11 Chinese cities have similar restriction schemes in place.\n\nThe 2012 Summer Olympics organization, with support from the Mayor of London office, announced in 2007 that they are planning auto exclusion zones around all venues, including London, Birmingham, Manchester, Newcastle upon Tyne, Glasgow and Cardiff. London authorities hope this measure will work as an experiment to change the public's travel behavior, allowing thereafter a shift from automobile to mass transit or bicycling. This severe policy has been publicized as the \"First Car-free Olympics\". During the peak events, the Olympics expect a crowd of 800,000 people. Those attending will have to travel by public transport, mainly through the Underground, or by bicycle or on foot.\n\nTransport economists consider road space rationing a variation of road pricing, and an alternative to congestion pricing, but road space rationing is considered more equitable by some, as the restrictions force all drivers to reduce auto travel, while congestion pricing restrains less those who can afford paying the congestion charge. Nevertheless, high-income users can often avoid the restrictions by owning a second car. Moreover, congestion pricing (unlike rationing) acts \"to allocate a scarce resource to its most valuable use, as evinced by users' willingness to pay for the resource\". While some \"opponents of congestion pricing fear that tolled roads will be used only by people with high income. But preliminary evidence suggests that the new toll lanes in California are used by people of all income groups. The ability to get somewhere fast and reliably is valued in a variety of circumstances. Not everyone will need or want to incur a toll on a daily basis, but on occasions when getting somewhere quickly is necessary, the option of paying to save time is valuable to people at all income levels.\"\n\nA more recent idea for automobile travel restrictions, proposed by some transport economists to avoid inequality and revenue allocation issues, is to implement a rationing of peak period travel but through revenue-neutral credit-based congestion pricing. This concept is similar to the existing system of emissions trading of carbon credits, proposed by the Kyoto Protocol to curb greenhouse emissions. Metropolitan area or city residents, or the taxpayers, will have the option to use the local government-issued mobility rights or congestion credits for themselves, or to trade or sell them to anyone willing to continue traveling by automobile beyond the personal quota. This trading system will allow direct benefits to be accrued by those users shifting to public transportation or by those reducing their peak-hour travel rather than the government.\n\n\n"}
{"id": "15007619", "url": "https://en.wikipedia.org/wiki?curid=15007619", "title": "Scotford Upgrader", "text": "Scotford Upgrader\n\nThe Shell Scotford Upgrader is an oilsand upgrader, a facility which processes crude bitumen (extra-heavy crude oil) from oil sands into a wide range of synthetic crude oils. The upgrader is owned by Athabasca Oil Sands Project (AOSP), a joint venture of Shell Canada Energy (60%), Marathon Oil Sands L.P. (20%) and Chevron Canada Limited (20%). The facility is located in the industrial development of Scotford, just to the northeast of Fort Saskatchewan, Alberta in the Edmonton Capital Region. \n\nThe Scotford Upgrader is a part of a larger site known as Shell Scotford located 40 km northeast of Edmonton, Alberta. Shell Scotford comprises three operating units: the Upgrader, a Refinery, and a Chemical plant. The Scotford Cogeneration Plant is also located on the site. Currently, work is being done on the first Upgrader expansion. In 1984, Shell opened both the Refinery and Chemical plant on the Scotford site. As one of North America's most modern and efficient refineries, the Scotford Refinery was the first to exclusively process synthetic crude oil from Alberta’s oil sands. Benzene that is produced during the refining process is sent to the adjacent Chemical plant and is used in the production of Styrene Monomer, a chemical needed to make many of the hard plastics people use daily. In 2000, a glycols plant was opened at Scotford Chemicals. Much of the output of the Scotford Upgrader is sold to the Scotford Refinery. Both light and heavy crudes are also sold to Shell's Sarnia Refinery in Ontario. The rest of the synthetic crude is sold to the general marketplace.\n\nIn 1891, a group of immigrants from Galicia, Austria settled on the land south of the North Saskatchewan River, near the South Victoria Trail. Philip Krebs, along with his son John, settled on the north side of South Victoria Trail. Their home became a popular stopping place for those travelling along the trail. Besides being a hospitable natured man, John was fluent in four European languages (German, English, Polish, and Ukrainian) and could speak Cree - making him popular with those who stopped by. \n\nWhen the Canadian Northern Railway was being built into Fort Saskatchewan, Philip Krebs’ homestead was a natural place for a stop. In 1905, a loading station was erected there, and on the siding of the building was the name “Scotford” (named after \"Walter Scott\" and \"Alexander Rutherford\", the premiers of the two provinces – Saskatchewan and Alberta - that were formed that same year). The area is still referred to by that name.\n\nThe Scotford Upgrader has a rated processing capacity of , but has at times pumped out more than . It was shut down after being damaged in a fire 19 November 2007. The production was resumed in December 2007.\n\nThe facility uses hydrogen addition to convert the bitumen from Shell's Muskeg River Mine in the Athabasca oil sands into refinery-ready sweet, light crude oil. The Muskeg River Mine is the first commercial unit using Shell's \"Enhance\" froth treatment technology — a process for removing sand, fine clay and water from oil sands froth to make clean bitumen suitable for upgrading via hydrogen addition. \n\nAccording to Shell, the hydrogenation process is well suited to the very clean bitumen produced at the Muskeg River Mine, and results in the upgrader producing more light crude oil than it inputs in the form of heavy bitumen. It also produces lower levels of sulfur dioxide emissions than the alternative coking method which removes carbon to produce petroleum coke as a by-product. The Scotford Upgrader has its own hydrogen manufacturing unit and produces most of the hydrogen required for the hydrogen-addition process.\n\nThe Scotford Upgrader capacity was expanded by by March 2010, an increase of 60% in capacity. In May 2007, the US$9 billion to US$11.3 billion expansion contract was awarded to TIC, Bantrel Constructors, PCL & KBR. KBR built 160 modules and performed construction work for the Atmospheric and Vacuum (A&V) unit and Sulphur Recovery Unit (SRU). Bantrel completed the tank farm, Utilities, Waterblock and Flare units, PCL completed the Residue Hydroconversion Complex (RHC) and TIC constructed the Hydrogen Manufacturing Unit (HMU) .\n\n\n"}
{"id": "32109234", "url": "https://en.wikipedia.org/wiki?curid=32109234", "title": "Sense on Cents", "text": "Sense on Cents\n\nSense on Cents is a financial website and blog. It is authored and managed by Larry Doyle, a 23-year Wall Street veteran, and is found at www.senseoncents.com.\n\nSense on Cents was launched in January 2009. With close to 1 million visitors in three plus years, Doyle's writing emphasizes investor education and protection with regard to the economy, markets, and finance. In the blog, Doyle covers an extensive array of topics and writes in a style which is understandable for those with little to no financial/market experience to graduate level professors. He has addressed at length the scam embedded in auction-rate securities. Doyle referred to the marketing and distribution by Wall Street of auction-rate securities as \"the single greatest fraud ever perpetrated on investors\". He has also raised questions about Wall Street's self-regulatory organization, the Financial Industry Regulatory Authority (FINRA), in the blog.\n\nDoyle's writing at \"Sense on Cents\" has led to his first book, \"In Bed with Wall Street: The Conspiracy Crippling Our Global Economy\", to be published by Palgrave Macmillan in January 2014.\n\nDoyle previously worked in the mortgage business, starting in 1983, for First Boston (as a mortgage-backed securities trader), Bear Stearns (where he was a senior managing director), and Union Bank of Switzerland (where he was head of mortgage trading), Bank of America, and JP Morgan Chase, where he was the National Sales Manager for Securitized Products.\n\n"}
{"id": "2273690", "url": "https://en.wikipedia.org/wiki?curid=2273690", "title": "Signal/One", "text": "Signal/One\n\nSignal/One was a manufacturer of high performance SSB and CW HF radio communications transceivers initially based in St. Petersburg, Florida, United States. \n\nSignal/One's parent company was Electronic Communications, Inc. (ECI), a military division of NCR Corporation located in St. Petersburg, Florida. Key Signal/One executives were general manager Dick Ehrhorn (amateur radio call sign W4ETO), and project engineer Don Fowler (W4YET). Beginning in the 1960s with the Signal/One CX7, (\"S1\", as they were called) the company made radios that were priced well above the competition and offered many advanced features for the time, such as passband tuning, broadband transmission, dual receive, built-in IAMBIC keyer, electronic digital read out, solid state design, QSK and RF clipping. A Signal/One radio was said to be a complete high performance, station in a box.\n\nWhile marketed to the affluent radio amateur, it has been suggested that the primary market for Signal/One, like Collins, was military, State Department, and government communications. Although prized for the performance and advanced engineering, Signal/One's products did not sell as well as hoped, and the company gradually fell on hard times. From the 1970s though the 1990s, every few years, Signal/One was spun off, sold, and resurfaced at another location.\n\nThe surviving Signal/One products are sought after and actively collected. These include the CX7, CX7A, CX7B, CX11 and Milspec models. The last Signal/One radio was a re-engineered ICOM IC-781. Information available indicates there were 1152 Signal Ones built: 850 CX7, 112 CX11, 168 MS1030 (number of \"C\" versions is not known), 6 MilSpec1030C, 15 MilSpec1030CI Icom IC-781 conversions and 1 Milspec1030E DSP Icom IC-756 Pro conversion.\n\n"}
{"id": "14629784", "url": "https://en.wikipedia.org/wiki?curid=14629784", "title": "Society of American Military Engineers", "text": "Society of American Military Engineers\n\nFounded in 1920, the Society of American Military Engineers (SAME) unites public and private sector individuals and organizations from across the architecture, engineering, construction, environmental, facility management, contracting and acquisition fields and related disciplines in support of the United States' national security.\n\nSAME connects architects, engineers and builders in the public sector and private industry, uniting them to improve individual and collective capabilities to provide the capability and prepare for and overcome natural and man-made disasters, acts of terrorism and to improve security at home and abroad. \n\nThat goal grew from the United States' experiences in World War I in which more than 11,000 civilian engineers were called to duty upon the US entering the conflict. Returning home after \"the war to end war,\" many feared the sector would lose this collective knowledge and the cooperation between public and private sectors that proved vital to combat success. Industry and military leaders vowed to capitalize on the technical lessons and camaraderie shared during their battlefield experiences. \n\nIn 1919, Maj. Gen. William M. Black, USA, the Army's Chief of Engineers, appointed a nine-officer board to consider the formation of an \"association of engineers\" that would preserve, and expand upon, connections formed in war and promote the advancement of engineering and its related professions. Early in 1920, the first SAME posts were established, providing former colleagues and new engineers opportunities to connect face-to-face, and establishing post-to-community relationships across the United States.\n\nThe original nine-member board appointed by Maj. Gen. Black also arranged the donation of \"Professional Memoirs\", a magazine published by the Engineer Bureau since 1909, and its assets, to SAME with the blessing of Gen. John J. \"Blackjack\" Pershing, USA. Those memoirs were subsequently renamed \"The Military Engineer\", which has been continuously published since it debuted in 1920.\n\nU.S. Vice President Charles G. Dawes served as SAME's 8th president. The year before assuming his role as president of SAME, Dawes was awarded the 1925 Nobel Peace Prize for his work on German reparations in 1924.\n\nDue to its close ties with the uniformed services of the United States, several branches of the military and the Public Health Service allow its members to wear the SAME ribbon on the uniform after all military and foreign decorations and awards. \n\nHeadquartered in Alexandria, Va., SAME provides its more than 30,000 members extensive opportunities for training, education and professional development through a robust offering of conferences, workshops, networking events and publications. With a membership that includes recent service academy graduates and retired engineering officers, project managers and corporate executives, uniformed and public sector professionals and private sector experts, SAME bridges the gaps between critical stakeholders to help secure our nation.\n\nSAME consists of 105 Posts and more than 50 Student Chapters and Field Chapters around the world along with a headquarters staff. Nationally, the organization is led by a volunteer Board of Direction that comprises five National Officers, 17 Regional Vice Presidents, the Chairs of the Mission Committees & Councils and 12 Elected Directors who serve three-year terms and are elected in groups of four annually.\n\nSAME membership is open to anyone in the U.S. and abroad.\n\n"}
{"id": "17419999", "url": "https://en.wikipedia.org/wiki?curid=17419999", "title": "Temenos Group", "text": "Temenos Group\n\nTemenos AG (SWX: TEMN) is a company specialising in enterprise software for banks and financial services, with its headquarters in Geneva, Switzerland. Temenos was initially created in 1993, and has been listed on the Swiss Stock exchange since 2001.\n\nFounded in 1993 and listed on the Swiss Stock Exchange (SIX: TEMN), Temenos AG is a provider of banking software systems to retail, corporate, universal, private, Islamic, microfinance and community banks. Headquartered in Geneva, Switzerland, and with 63 offices in 42 countries, Temenos serves over 3,000 financial institutions in 145 countries across the world. It claims to be used by 41 of the top 50 banks worldwide.\n\nThe company was started in November 1993, by George Koukis having acquired the rights to GLOBUS, the successful banking software platform developed by a team of technical and banking experts in 1988. The company was Temenos, in reference to a lecture on Money given by Hans-Wolfgang Frick at the Temenos Academy (1992), and continued to develop and market GLOBUS.\n\nIn 2001, Temenos went public, and is listed on the main segment of the SWX Swiss Exchange (TEMN). Also in 2001, Temenos acquired a mainframe core banking application aimed at high-end retail banks, originally developed by IBM, and now marketed as Temenos Corebanking.\n\nOn the 30th September 2003 Temenos launched T24. T24 was based on GLOBUS, but with a state of the art banking technology platform. This is the result of 3 years of development effort and an investment of more than USD 24 million.\n\nIn 2011, George Koukis stepped down as Chairman and became a Non-executive director, and Andreas Andreades became Chairman. \n\nin 2016, Temenos won the Banking Technology Readers’ Choice Award for “Best emerging/innovative technology product” for Temenos MarketPlace.\n\nAsian Private Banker awarded Temenos as the “Best Integrated Front Office Solution” for WealthSuite.\n\nIn July 2017, Temenos was recognized as a Market Leader by Gartner in their report \"Magic Quadrant for Global Retail Core Banking\".\n\nIn April 2018, Temenos won a contract to provide its financial technology platform for Nordic-Baltic telecommunications company Telia’s digital banking services. \n\nTemenos has been awarded the \"Payments Innovation of the Year\" award by the business website FStech on March 2018.\n\nIn March 2018, Temenos' Islamic Banking Suite was named the \"Best Islamic Banking and Finance Software Solution\" at the 2018 World Islamic Finance Awards.\n"}
{"id": "31254322", "url": "https://en.wikipedia.org/wiki?curid=31254322", "title": "Toyota Commemorative Museum of Industry and Technology", "text": "Toyota Commemorative Museum of Industry and Technology\n\nThe , also known as Toyota Tecno Museum, is a technology museum located in Nishi-ku in the city of Nagoya, central Japan.\n\nToyota started as a textile firm and evolved over decades into an international automobile producer. The museum was established in June, 1994 and is housed in an old red-brick textile factory. Its display starts with textile looms and then gradually goes over into the history of cars. Also featured are high-tech robots.\n\nAccess by public transport is Sako Station on the Meitetsu line or Kamejima Station by the Higashiyama Line.\n\n\n"}
{"id": "55988548", "url": "https://en.wikipedia.org/wiki?curid=55988548", "title": "TrackInsight", "text": "TrackInsight\n\nTrackInsight is a free exchange-traded funds analysis platform founded in 2014 by Jean-René Giraud and Arnaud Izard.\n\nThe service provides ratings and comparison tools on exchange-traded funds distributed in North America, Europe and Asia.\n\nOn October 17, 2016, TrackInsight raised €2,500,000 from NewAlpha Asset Management and Aviva Group.\n\n"}
{"id": "34193404", "url": "https://en.wikipedia.org/wiki?curid=34193404", "title": "Virtual learning environment", "text": "Virtual learning environment\n\nA virtual learning environment (VLE) in educational technology is a Web-based platform for the digital aspects of courses of study, usually within educational institutions. They present resources, activities and interactions within a course structure and provide for the different stages of assessment. VLEs also usually report on participation; and have some level of integration with other institutional systems.\n\nFor teachers and instructors who edit them, VLEs may have a de facto role as authoring and design environments. VLEs have been adopted by almost all higher education institutions in the English-speaking world.\n\nThe following are the main components required for the best virtual learning learning environment or online education curriculum to take place.\n\nVLE learning platforms commonly allow: \n\nA VLE may include some or all of the following elements:\n\nA VLE is normally not designed for a specific course or subject, but is capable of supporting multiple courses over the full range of the academic program, giving a consistent interface within the institution and—to some degree—with other institutions using the system. The virtual learning environment supports the worldwide exchange of information between a user and the learning institute he or she is currently enrolled in through digital mediums like e-mail, chat rooms, web 2.0 sites or a forum.\n\nOne of the processes to enhance the learning experience was the virtual resource room, which is student centered, works in a self-paced format, and which encourages students to take responsibility for their own learning. In virtual mode, the materials are available in the form of computer aided learning program, lecture notes, special self-assessment module. Another mechanism for student to student interactions in a form of simple discussion forum is by using a novel link cyber tutor. This allows the students with an email account to connect with course content and the staff with their doubts and related questions. The students are able to contact the staff without a face to face visit which saves the on campus time. The staff remains anonymous which allows for the several staff to act as a cyber tutor during the course. The student do not remain anonymous although their email address are cryptic enough to mask their identity. Students can discuss about the exams, lab reports, posters, lectures, technical help with downloading materials. The evaluation of the use of Virtual resource room is done by surveys, focus groups and online feedback forms. The students have 24 hours of access to the learning material in a day which suits their life styles.\n\nComputerized learning systems have been referred to as electronic educational technology, e-learning, learning platform or learning management system. The major difference is that VLE and LMS are applications, whereas the Learning Platform shares characteristics with an Operating System where different educational web-based applications can be run on the platform.\n\nThe terms virtual learning environment (VLE) and learning platform are generically used to describe a range of integrated web-based applications that provide teachers, learners, parents and others involved in education with information, tools and resources to support and enhance educational delivery and management. These terms are broadly synonymous with 'managed learning environments' (MLEs) and 'managed virtual learning environments' (MVLEs).\n\nThe applications that form part of these online services can include web pages, email, message boards and discussion forums, text and video conferencing, shared diaries, online social areas, as well as assessment, management and tracking tools.\n\nThe term learning platform refers to a range of tools and services often described using terms such as educational extranet, VLE, LMS, ILMS and LCMS providing learning and content management. The term learning platform also includes the personal learning environment (PLE) or personal online learning space (POLS), including tools and systems that allow the development and management of eportfolios.\nThe specific functionality associated with any implementation of a learning platform will vary depending upon the needs of the users and can be achieved by bringing together a range of features from different software solutions either commercially available, open source, self-built or available as free to use web services. These tools are delivered together via a cohesive user environment with a single entry point, through integration achieved by technical standards.\n\n\nThe term LMS can also mean \"library management system\" (which is now more commonly referred to as integrated library system, or ILS).\n\nMiddle School and High School use VLEs in order to:\n\n\nInstitutions of higher and further education use VLEs in order to:\n\n\nOnline learners performed modestly better, on average, than those learning the same material through traditional face-to-face instruction. VLEs are able to provide opportunities for relations between learners by the use of discussion forums. Through this, e-learning helps eliminate barriers that have the potential of hindering participation including the fear of talking to other learners. They can compensate for scarcities of academic staff, including instructors or teachers.\n\nVLEs are supposed to support many 21st century skills, including:\n\nBoth supporters and critics of virtual learning environments recognize the importance of the development of such skills, including creativity, communication, and knowledge application; however, the controversy lies in whether or not virtual learning environments are practical for both teachers and students.\n\nCritics of VLE worry about the disconnect that can occur between the teacher and students, as well as between student to student. Virtual Learning Environments does not provide students with face-to-face interaction and therefore, can deprive students of opportunities for better communication and deeper understanding. Educators also have concerns pertaining to a student's computer literacy skills and access to quality technology. Both can create a challenge for students to succeed in a Virtual Learning Environment. A study among Indian students has suggested that a negative experience with virtual learning environments can leave \"the learner with a passive, un-engaging experience, leading to incomplete learning and low performance\". \n\nThe VLE leads to a reported higher computer self-efficacy, while participants report being less satisfied with the learning process that is achieved in the Virtual Learning Environment.\n\nMost VLEs support the Shareable Content Object Reference Model (SCORM) as a standard, but there are no commonly used standards that define how the learner's performance within a course can be transferred from one VLE to another.\n\nThere are also standards for sharing content such as those defined by the IMS Global Consortium. Local bodies such as in the schools sector in the UK the DCSF via Becta have additionally defined a learning platform \"conformance framework\" to encourage interoperability.\n\nVirtual learning environments are not limited only to students and learners in university level studies. There are many virtual learning environments for students in grades K-12. These systems are also particularly suited for the needs of independent educational programs, charter schools and home-based education.\n\nAs virtual teaching and learning becomes more deeply integrated into curricula, it is important to assess the quality and rigor of virtual programs. The Virtual Learning Program Standards provide a framework for identifying key areas for effective teaching and learning in Virtual Learning Programs throughout the Northeast and the nation.\n\nEducators need benchmark tools to assess a virtual learning environment as a viable means of education.\n\nWalker developed a survey instrument known as the Distance Education Learning Environment Survey (DELES), which is accessible to students anywhere. DELES examines instructor support, student interaction and collaboration, personal relevance, authentic learning, active learning, and student autonomy.\n\nHarnish and Reeves provide a systematic criteria approach based on training, implementation, system usage, communication, and support.\n\n\n"}
{"id": "43523586", "url": "https://en.wikipedia.org/wiki?curid=43523586", "title": "Wayfinding software", "text": "Wayfinding software\n\nWayfinding software is a self-service computer program that helps users to find a location, usually used indoors. This software is usually installed on interactive kiosks or smartphones.\n"}
