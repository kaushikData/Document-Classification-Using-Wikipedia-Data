{"id": "54711947", "url": "https://en.wikipedia.org/wiki?curid=54711947", "title": "3D structure change detection", "text": "3D structure change detection\n\n3D Structure Change Detection is a type of Change detection (GIS) processes for GIS (geographical information systems). It is a process that measures how the volume of a particular area have changed between two or more time periods. A high-spatial resolution Digital elevation model (DEM) that provides accurate 4-d (space and time) structural information over area of interest is required to compute such changes. In production, two or more DEMs that cover the same area are used to monitor topographic changes of area. By comparing the DEMs made at different times, structure of terrain changes can be realized by the ground elevation difference from DEMs. Details, occurring time and accuracy of such changes are strongly relied on the resolution, quality of DEMs. In general, the problem of involves whether or not a change has occurred, or whether several changes have occurred. Such structure changes detection has been widely used to assess urban growth, impact of natural disasters like earthquake, volcano and battle damage assessment.\n\n\n"}
{"id": "13798231", "url": "https://en.wikipedia.org/wiki?curid=13798231", "title": "Acorn Computers (2006)", "text": "Acorn Computers (2006)\n\nAcorn Computers Ltd was a company based in Nottingham, England in the United Kingdom. It licensed, in early 2006, the dormant Acorn Computers trademark from French company Aristide & Co Antiquaire De Marques. This company sold IBM PC compatible computers and had no connection to ARM.\n\nIn April 2006, internet news portals claimed that the company was to relaunch. The new company announced its range before the 2006 \"Computer Trade Show\", held at the NEC near Birmingham, UK. At the show, the company distributed leaflets inviting people to \"be part of one of the most exciting brand re-launches in UK history\" by joining its reseller program.\n\nThe company sold a range of laptop computers. The systems used Microsoft Windows rather than the RISC OS operating system developed by the original Acorn Computers and this incarnation of Acorn did not support or license any technologies or products of the original, apart from the name and trademark.\n\nThe reuse of the Acorn Computers Ltd name caused an amount of confusion and controversy, particularly amongst users of the original company's products.\n\nOn 24 July 2006, Nominet's Dispute Resolution Service (DRS) ruled that the domain name acorncomputers.co.uk should be transferred to the new Acorn from computer enthusiast Roy Johnson. The company made a complaint to the DRS contending that the \"use of Acorn Computers' company name is illegal and has caused much confusion and continues to do so which is detrimental to [Acorn] and extremely misleading\". Despite the fact that Johnson appeared to have been operating the website since at least 2001, five years before the new Acorn was registered as a company, Nominet ruled in favour of Acorn, as Johnson had not maintained an accurate record of his postal address, and mail to Johnson's registered address was returned by Royal Mail marked 'addressee has gone away'.\n\nAcorn Computers Ltd failed to file any accounts at Companies House, and so was struck off the limited companies register and dissolved in December 2009.\n"}
{"id": "7348443", "url": "https://en.wikipedia.org/wiki?curid=7348443", "title": "Air blaster", "text": "Air blaster\n\nAn air blaster or air cannon is a de-clogging device composed of two main elements: a pressure vessel (storing air pressure) and a triggering mechanism (high speed release of compressed air). They are permanently installed on silos, bins and hoppers walls for all powdery forms of materials, and are used to prevent caking and allowing maximum storage capacity. They are also used in the entertainment industry for shock value in Halloween haunts and other attractions.\n\nAir blasters do not need any specific air supply. Available plant air is enough with a minimum of 4 bar air pressure (60 psi or 400 kPa), although 5 to 6 bar are preferred for better results (75 to 90 psi). The average air consumption is moderate, and depends on the number of firings per hour, size of the pressure vessel, and number of air cannons installed. For instance, a 50-liter air cannon consumes 0.60 Nm³/hour at 6 bar air pressure (90 psi or 600 kPa), with 2 firings per hour.\n\nThe compressed air contained in the pressure vessel is instantly released, and the achieved blast, called the impact force, evacuates material sticking to the walls (rat holing), as well as breaking potential bridging thanks to the shock wave obtained.\n\nThe blasts are usually organized by using an automatic sequencer.\n\n\nAn efficient air blaster should be designed to ensure:\n\nUsually 2 versions exist\nA high temperature version: mainly for heat exchanger and cooler applications to remove clogging and to avoid costly plant stoppages and downtime.\nA low temperature version: to eliminate build-up and dead stock for powdery and granular materials thus preventing caking and allowing optimization of storage capacity.\n\nAir blasters solves problem occurring in cement factories among other industries, with blockages occurring in preheater towers (Kiln inlet, Cyclones, riser ducts...etc.) and in grate coolers, thus providing substantial savings.\n\n"}
{"id": "10833027", "url": "https://en.wikipedia.org/wiki?curid=10833027", "title": "Alexander Christakis", "text": "Alexander Christakis\n\nAlexander (Aleco) Christakis (; born 1937) is a Greek American social scientist, systems scientist and cyberneticist, former faculty member of several Universities, organizational consultant and member of the Club of Rome, known for his \"study and design of social systems\".\n\nChristakis came to the United States in 1956, and received a BA in theoretical physics at Princeton University and a Ph.D. in theoretical nuclear physics at Yale. Later on he proceeded studying urban planning, and systems science until 1970.\n\nAfter the birth of his son Nicholas he returned to Greece and joined the architectural firm of Doxiadis Associates which was specialized in architecture and town planning. In 1968 he was co-founder of the Club of Rome, where he was a collaborator of Aurelio Peccei, Erich Jantsch and Hasan Özbekhan. Since the 1970s Christakis has served on the faculties of Yale University, Georgetown University, University of Athens, and the University of Virginia. He also spent five years at George Mason University as the Director of the Center for Interactive Management headed by John N. Warfield. In 1989 Christakis founded his own management consultancy firm CWA Ltd. dedicated to apply Interactive Management principles, and he has been CEO ever since.\n\nChristakis serves on the Editorial Boards of several journals, including \"Systems Research and Behavioral Sciences\", \"Systems: Journal of Transdisciplinary Systems Science\", and the \"Journal of Applied Systems Studies\".\n\nIn 2002, Dr. Christakis served as President of the International Society for the Systems Sciences. He also serves as President of the Institute for 21st Century Agoras, a non-profit organization dedicated to the evolution of civic, global, and institutional capacity for coordinated democratic decision-making using systems principles (Co-Laboratories of Democracy.) The Institute for 21st Century Agoras is credited for the formalization of the science of Structured dialogic design in its present form. Dr. Christakis is also a partner with Dialogic Design International, a consulting firm that deploys SDD in systemic design approaches for complex organizational and systems problems.\n\nIn 2007 he served on the Board of Directors of the Americans for Indian Opportunity (AIO), of Future Worlds Center, and also as an Advisor to the Ambassadors Program of AIO which serves indigenous people around the world.\n\nHe has received numerous awards and distinctions including the Demosophia Award, the Creative Programming Award from the National University Continuing Education Association and the most prestigious medal of the Hellenic Society for Systemic Studies to name a few.\n\nChristakis has been invited to support the peace process in Cyprus and the Middle East. He and Laouris led the implementation of series of mass scale dialogues using the Structured dialogic design process in the Civil Society Dialogue project in Cyprus and in the Act Beyond Borders project in Middle East.\n\nChristakis has published over 100 papers on the management of complexity in refereed journals. He is also the co-author of three books on \"Technology Assessment\". Books:\n\nArticles and papers, a selection:\n\n"}
{"id": "1925126", "url": "https://en.wikipedia.org/wiki?curid=1925126", "title": "Amine gas treating", "text": "Amine gas treating\n\nAmine gas treating, also known as amine scrubbing, gas sweetening and acid gas removal, refers to a group of processes that use aqueous solutions of various alkylamines (commonly referred to simply as amines) to remove hydrogen sulfide (HS) and carbon dioxide (CO) from gases. It is a common unit process used in refineries, and is also used in petrochemical plants, natural gas processing plants and other industries.\n\nProcesses within oil refineries or chemical processing plants that remove hydrogen sulfide are referred to as \"sweetening\" processes because the odor of the processed products is improved by the absence of hydrogen sulfide. An alternative to the use of amines involves membrane technology. However, membrane separation is less attractive due to the relatively high capital and operating costs as well as other technical factors.\n\nMany different amines are used in gas treating:\n\nThe most commonly used amines in industrial plants are the alkanolamines DEA, MEA, and MDEA. These amines are also used in many oil refineries to remove sour gases from liquid hydrocarbons such as liquified petroleum gas (LPG).\n\nGases containing or both and are commonly referred to as \"sour gases\" or \"acid gases\" in the hydrocarbon processing industries.\n\nThe chemistry involved in the amine treating of such gases varies somewhat with the particular amine being used. For one of the more common amines, monoethanolamine (MEA) denoted as RNH, the chemistry may be expressed as:\n\nA typical amine gas treating process (the Girbotol process, as shown in the flow diagram below) includes an absorber unit and a regenerator unit as well as accessory equipment. In the absorber, the downflowing amine solution absorbs and from the upflowing sour gas to produce a sweetened gas stream (i.e., a gas free of hydrogen sulfide and carbon dioxide) as a product and an amine solution rich in the absorbed acid gases. The resultant \"rich\" amine is then routed into the regenerator (a stripper with a reboiler) to produce regenerated or \"lean\" amine that is recycled for reuse in the absorber. The stripped overhead gas from the regenerator is concentrated and .\n\nAlternative stripper configurations include matrix, internal exchange, flashing feed, and multipressure with split feed. Many of these configurations offer more energy efficiency for specific solvents or operating conditions. Vacuum operation favors solvents with low heats of absorption while operation at normal pressure favors solvents with high heats of absorption. Solvents with high heats of absorption require less energy for stripping from temperature swing at fixed capacity. The matrix stripper recovers 40% of at a higher pressure and does not have inefficiencies associated with multipressure stripper. Energy and costs are reduced since the reboiler duty cycle is slightly less than normal pressure stripper. An Internal Exchange stripper has a smaller ratio of water vapor to in the overheads stream, and therefore less steam is required. The multipressure configuration with split feed reduces the flow into the bottom section, which also reduces the equivalent work. Flashing feed requires less heat input because it uses the latent heat of water vapor to help strip some of the in the rich stream entering the stripper at the bottom of the column. The multipressure configuration is more attractive for solvents with a higher heats of absorption.\n\nThe amine concentration in the absorbent aqueous solution is an important parameter in the design and operation of an amine gas treating process. Depending on which one of the following four amines the unit was designed to use and what gases it was designed to remove, these are some typical amine concentrations, expressed as weight percent of pure amine in the aqueous solution:\n\nThe choice of amine concentration in the circulating aqueous solution depends upon a number of factors and may be quite arbitrary. It is usually made simply on the basis of experience. The factors involved include whether the amine unit is treating raw natural gas or petroleum refinery by-product gases that contain relatively low concentrations of both HS and CO or whether the unit is treating gases with a high percentage of CO such as the offgas from the steam reforming process used in ammonia production or the flue gases from power plants.\n\nBoth HS and CO are acid gases and hence corrosive to carbon steel. However, in an amine treating unit, CO is the stronger acid of the two. HS forms a film of iron sulfide on the surface of the steel that acts to protect the steel. When treating gases with a high percentage of CO, corrosion inhibitors are often used and that permits the use of higher concentrations of amine in the circulating solution.\n\nAnother factor involved in choosing an amine concentration is the relative solubility of HS and CO in the selected amine. The choice of the type of amine will affect the required circulation rate of amine solution, the energy consumption for the regeneration and the ability to selectively remove either HS alone or CO alone if desired. For more information about selecting the amine concentration, the reader is referred to Kohl and Nielsen's book.\n\nMEA and DEA are primary and secondary amines. They are very reactive and can effectively remove a high volume of gas due to a high reaction rate. However, due to stoichiometry, the loading capacity is limited to 0.5 mol CO per mole of amine. MEA and DEA also require a large amount of energy to strip the CO during regeneration, which can be up to 70% of total operating costs. They are also more corrosive and chemically unstable compared to other amines.\n\nIn oil refineries, that stripped gas is mostly HS, much of which often comes from a sulfur-removing process called hydrodesulfurization. This HS-rich stripped gas stream is then usually routed into a Claus process to convert it into elemental sulfur. In fact, the vast majority of the 64,000,000 metric tons of sulfur produced worldwide in 2005 was byproduct sulfur from refineries and other hydrocarbon processing plants. Another sulfur-removing process is the WSA Process which recovers sulfur in any form as concentrated sulfuric acid. In some plants, more than one amine absorber unit may share a common regenerator unit. \nThe current emphasis on removing CO from the flue gases emitted by fossil fuel power plants has led to much interest in using amines for removing CO. (See also: Carbon capture and storage and Conventional coal-fired power plant.)\n\nIn the specific case of the industrial synthesis of ammonia, for the steam reforming process of hydrocarbons to produce gaseous hydrogen, amine treating is one of the commonly used processes for removing excess carbon dioxide in the final purification of the gaseous hydrogen.\n\nIn the biogas production it is sometimes necessary to remove carbon dioxide from the biogas to make it comparable with the natural. The removal of the sometimes high content of hydrogen sulfide is necessary to prevent corrosion of metallic parts after burning the bio gas.\n\nAmines are used to remove CO in various areas ranging from natural gas production to the food and beverage industry, and have been for over sixty years.\n\nThere are multiple classifications of amines, each of which has different characteristics relevant to CO capture. For example, Monoethanolamine (MEA) reacts strongly with acid gases like CO and has a fast reaction time and an ability to remove high percentages of CO, even at the low CO concentrations. Typically, Monoethanolamine (MEA) can capture 85% to 90% of the CO from the flue gas of a coal-fired plant, which is one of the most effective solvent to capture CO.\n\nChallenges of carbon capture using amine include:\n\nThe partial pressure is the driving force to transfer CO into the liquid phase. Under the low pressure, this transfer is hard to achieve without increasing the reboiler’s heat duty, which will result in higher cost.\n\nPrimary and secondary amines, for example, MEA and DEA, will react with CO and form degradation products. O from the inlet gas will cause degradation as well. The degraded amine is no longer able to capture CO, which decreases the overall carbon capture efficiency.\n\nCurrently, variety of amine mixtures are being synthesized and tested to achieve a more desirable set of overall properties for use in CO capture systems. One major focus is on lowering the energy required for solvent regeneration, which has a major impact on process costs. However, there are tradeoffs to consider. For example, the energy required for regeneration is typically related to the driving forces for achieving high capture capacities. Thus, reducing the regeneration energy can lower the driving force and thereby increase the amount of solvent and size of absorber needed to capture a given amount of CO, thus, increasing the capital cost.\n\n\n"}
{"id": "2994", "url": "https://en.wikipedia.org/wiki?curid=2994", "title": "Anemometer", "text": "Anemometer\n\nAn anemometer is a device used for measuring wind speed, and is also a common weather station instrument. The term is derived from the Greek word \"anemos\", which means wind, and is used to describe any wind speed instrument used in meteorology. The first known description of an anemometer was given by Leon Battista Alberti in 1450.\n\nThe anemometer has changed little since its development in the 15th century. Leon Battista Alberti (1404–1472) is said to have invented the first mechanical anemometer around 1450. In following centuries, numerous others, including Robert Hooke\n(1635–1703), developed their own versions, with some being mistakenly credited as the inventor. In 1846, John Thomas Romney Robinson (1792–1882) improved upon the design by using four hemispherical cups and mechanical wheels. In 1926, Canadian meteorologist John Patterson (January 3, 1872 – February 22, 1956) developed a three-cup anemometer, which was improved by Brevoort and Joiner in 1935. In 1991, Derek Weston added the ability to measure wind direction. In 1994, Andrews Pflitsch developed the sonic anemometer.\n\nA simple type of anemometer was invented in 1845 by Dr. John Thomas Romney Robinson, of Armagh Observatory. It consisted of four hemispherical cups mounted on horizontal arms, which were mounted on a vertical shaft. The air flow past the cups in any horizontal direction turned the shaft at a rate that was roughly proportional to the wind speed. Therefore, counting the turns of the shaft over a set time interval produced a value proportional to the average wind speed for a wide range of speeds. It is also called a rotational anemometer.\n\nOn an anemometer with four cups, it is easy to see that since the cups are arranged symmetrically on the end of the arms, the wind always has the hollow of one cup presented to it and is blowing on the back of the cup on the opposite end of the cross. Since a hollow hemisphere has a drag coefficient of .38 on the spherical side and 1.42 on the hollow side, more force is generated on the cup that is presenting its hollow side to the wind. Because of this asymmetrical force, torque is generated on the axis of the anemometer, causing it to spin.\n\nTheoretically, the speed of rotation of the anemometer should be proportional to the wind speed, because the force produced on an object is proportional to the speed of the fluid flowing past it, but other factors influence the rotational speed, including turbulence produced by the apparatus, increasing drag in opposition to the torque that is produced by the cups and support arms, and friction of the mount point. When Robinson first designed his anemometer, he asserted that the cups moved one-third of the speed of the wind, unaffected by the cup size or arm length. This was apparently confirmed by some early independent experiments, but it was incorrect. Instead, the ratio of the speed of the wind and that of the cups, the \"anemometer factor\", depends on the dimensions of the cups and arms, and may have a value between two and a little over three. Every previous experiment involving an anemometer had to be repeated after the error was discovered.\n\nThe three-cup anemometer developed by the Canadian John Patterson in 1926 and subsequent cup improvements by Brevoort & Joiner of the United States in 1935 led to a cupwheel design with a nearly linear response and had an error of less than 3% up to . Patterson found that each cup produced maximum torque when it was at 45° to the wind flow. The three-cup anemometer also had a more constant torque and responded more quickly to gusts than the four-cup anemometer.\n\nThe three-cup anemometer was further modified by the Australian Dr. Derek Weston in 1991 to measure both wind direction and wind speed. Weston added a tag to one cup, which causes the cupwheel speed to increase and decrease as the tag moves alternately with and against the wind. Wind direction is calculated from these cyclical changes in cupwheel speed, while wind speed is determined from the average cupwheel speed.\n\nThree-cup anemometers are currently used as the industry standard for wind resource assessment studies & practice.\n\nOne of the other forms of mechanical velocity anemometer is the \"vane anemometer\". It may be described as a windmill or a propeller anemometer. Unlike the Robinson anemometer, whose axis of rotation is vertical, the vane anemometer must have its axis parallel to the direction of the wind and therefore horizontal. Furthermore, since the wind varies in direction and the axis has to follow its changes, a wind vane or some other contrivance to fulfill the same purpose must be employed.\n\nA \"vane anemometer\" thus combines a propeller and a tail on the same axis to obtain accurate and precise wind speed and direction measurements from the same instrument. The speed of the fan is measured by a rev counter and converted to a windspeed by an electronic chip. Hence, volumetric flow rate may be calculated if the cross-sectional area is known.\n\nIn cases where the direction of the air motion is always the same, as in ventilating shafts of mines and buildings, wind vanes known as air meters are employed, and give satisfactory results.\n\nHot wire anemometers use a very fine wire (on the order of several micrometres) electrically heated to some temperature above the ambient. Air flowing past the wire cools the wire. As the electrical resistance of most metals is dependent upon the temperature of the metal (tungsten is a popular choice for hot-wires), a relationship can be obtained between the resistance of the wire and the flow speed.\n\nSeveral ways of implementing this exist, and hot-wire devices can be further classified as CCA (constant current anemometer), CVA (constant voltage anemometer) and CTA (constant-temperature anemometer). The voltage output from these anemometers is thus the result of some sort of circuit within the device trying to maintain the specific variable (current, voltage or temperature) constant, following Ohm's law.\n\nAdditionally, PWM (pulse-width modulation) anemometers are also used, wherein the velocity is inferred by the time length of a repeating pulse of current that brings the wire up to a specified resistance and then stops until a threshold \"floor\" is reached, at which time the pulse is sent again.\n\nHot-wire anemometers, while extremely delicate, have extremely high frequency-response and fine spatial resolution compared to other measurement methods, and as such are almost universally employed for the detailed study of turbulent flows, or any flow in which rapid velocity fluctuations are of interest.\n\nAn industrial version of the fine-wire anemometer is the thermal flow meter, which follows the same concept, but uses two pins or strings to monitor the variation in temperature. The strings contain fine wires, but encasing the wires makes them much more durable and capable of accurately measuring air, gas, and emissions flow in pipes, ducts, and stacks. Industrial applications often contain dirt that will damage the classic hot-wire anemometer.\n\nIn laser Doppler velocimetry, laser Doppler anemometers use a beam of light from a laser that is divided into two beams, with one propagated out of the anemometer. Particulates (or deliberately introduced seed material) flowing along with air molecules near where the beam exits reflect, or backscatter, the light back into a detector, where it is measured relative to the original laser beam. When the particles are in great motion, they produce a Doppler shift for measuring wind speed in the laser light, which is used to calculate the speed of the particles, and therefore the air around the anemometer.\n\nUltrasonic anemometers, first developed in the 1950s, use ultrasonic sound waves to measure wind velocity. They measure wind speed based on the time of flight of sonic pulses between pairs of transducers. Measurements from pairs of transducers can be combined to yield a measurement of velocity in 1-, 2-, or 3-dimensional flow. The spatial resolution is given by the path length between transducers, which is typically 10 to 20 cm. Ultrasonic anemometers can take measurements with very fine temporal resolution, 20 Hz or better, which makes them well suited for turbulence measurements. The lack of moving parts makes them appropriate for long-term use in exposed automated weather stations and weather buoys where the accuracy and reliability of traditional cup-and-vane anemometers are adversely affected by salty air or dust. Their main disadvantage is the distortion of the air flow by the structure supporting the transducers, which requires a correction based upon wind tunnel measurements to minimize the effect. An international standard for this process, ISO 16622 \"Meteorology—Ultrasonic anemometers/thermometers—Acceptance test methods for mean wind measurements\" is in general circulation. Another disadvantage is lower accuracy due to precipitation, where rain drops may vary the speed of sound.\n\nSince the speed of sound varies with temperature, and is virtually stable with pressure change, ultrasonic anemometers are also used as thermometers.\n\nTwo-dimensional (wind speed and wind direction) sonic anemometers are used in applications such as weather stations, ship navigation, aviation, weather buoys and wind turbines. Monitoring wind turbines usually requires a refresh rate of wind speed measurements of 3 Hz, easily achieved by sonic anemometers. Three-dimensional sonic anemometers are widely used to measure gas emissions and ecosystem fluxes using the eddy covariance method when used with fast-response infrared gas analyzers or laser-based analyzers.\n\nTwo-dimensional wind sensors are of two types:\n\nAcoustic resonance anemometers are a more recent variant of sonic anemometer. The technology was invented by Dr Savvas Kapartis and patented in 2000. Whereas conventional sonic anemometers rely on time of flight measurement, acoustic resonance sensors use resonating acoustic (ultrasonic) waves within a small purpose-built cavity in order to perform their measurement.\nBuilt into the cavity is an array of ultrasonic transducers, which are used to create the separate standing-wave patterns at ultrasonic frequencies. As wind passes through the cavity, a change in the wave's property occurs (phase shift). By measuring the amount of phase shift in the received signals by each transducer, and then by mathematically processing the data, the sensor is able to provide an accurate horizontal measurement of wind speed and direction.\n\nAcoustic resonance technology enables measurement within a small cavity, the sensors therefore tend to be typically smaller in size than other ultrasonic sensors. The small size of acoustic resonance anemometers makes them physically strong and easy to heat and therefore resistant to icing. This combination of features means that they achieve high levels of data availability and are well suited to wind turbine control and to other uses that require small robust sensors such as battlefield meteorology. One issue with this sensor type is measurement accuracy when compared to a calibrated mechanical sensor. For many end uses, this weakness is compensated for by the sensor's longevity and the fact that it does not require re-calibrating once installed.\n\nA common anemometer for basic use is constructed from a ping-pong ball attached to a string. When the wind blows horizontally, it presses on and moves the ball; because ping-pong balls are very lightweight, they move easily in light winds. Measuring the angle between the string-ball apparatus and the vertical gives an estimate of the wind speed.\n\nThis type of anemometer is mostly used for middle-school level instruction, which most students make on their own, but a similar device was also flown on Phoenix Mars Lander.\n\nThe first designs of anemometers that measure the pressure were divided into plate and tube classes.\n\nThese are the first modern anemometers. They consist of a flat plate suspended from the top so that the wind deflects the plate. In 1450, the Italian art architect Leon Battista Alberti invented the first mechanical anemometer; in 1664 it was re-invented by Robert Hooke (who is often mistakenly considered the inventor of the first anemometer). Later versions of this form consisted of a flat plate, either square or circular, which is kept normal to the wind by a wind vane. The pressure of the wind on its face is balanced by a spring. The compression of the spring determines the actual force which the wind is exerting on the plate, and this is either read off on a suitable gauge, or on a recorder. Instruments of this kind do not respond to light winds, are inaccurate for high wind readings, and are slow at responding to variable winds. Plate anemometers have been used to trigger high wind alarms on bridges.\n\nJames Lind's anemometer of 1775 consisted of a glass U tube containing a liquid manometer (pressure gauge), with one end bent in a horizontal direction to face the wind and the other vertical end remains parallel to the wind flow. Though the Lind was not the first it was the most practical and best known anemometer of this type. If the wind blows into the mouth of a tube it causes an increase of pressure on one side of the manometer. The wind over the open end of a vertical tube causes little change in pressure on the other side of the manometer. The resulting elevation difference in the two legs of the U tube is an indication of the wind speed. However, an accurate measurement requires that the wind speed be directly into the open end of the tube; small departures from the true direction of the wind causes large variations in the reading.\n\nThe successful metal pressure tube anemometer of William Henry Dines in 1892 utilized the same pressure difference between the open mouth of a straight tube facing the wind and a ring of small holes in a vertical tube which is closed at the upper end. Both are mounted at the same height. The pressure differences on which the action depends are very small, and special means are required to register them. The recorder consists of a float in a sealed chamber partially filled with water. The pipe from the straight tube is connected to the top of the sealed chamber and the pipe from the small tubes is directed into the bottom inside the float. Since the pressure difference determines the vertical position of the float this is a measure of the wind speed.\n\nThe great advantage of the tube anemometer lies in the fact that the exposed part can be mounted on a high pole, and requires no oiling or attention for years; and the registering part can be placed in any convenient position. Two connecting tubes are required. It might appear at first sight as though one connection would serve, but the differences in pressure on which these instruments depend are so minute, that the pressure of the air in the room where the recording part is placed has to be considered. Thus if the instrument depends on the pressure or suction effect alone, and this pressure or suction is measured against the air pressure in an ordinary room, in which the doors and windows are carefully closed and a newspaper is then burnt up the chimney, an effect may be produced equal to a wind of 10 mi/h (16 km/h); and the opening of a window in rough weather, or the opening of a door, may entirely alter the registration.\n\nWhile the Dines anemometer had an error of only 1% at , it did not respond very well to low winds due to the poor response of the flat plate vane required to turn the head into the wind. In 1918 an aerodynamic vane with eight times the torque of the flat plate overcame this problem.\n\nModern tube anemometers use the same principle as in the Dines anemometer but using a different design. The implementation uses a pitot-static tube which is a pitot tube with two ports, pitot and static, that is normally used in measuring the airspeed of aircraft. The pitot port measures the dynamic pressure of the open mouth of a tube with pointed head facing wind, and the static port measures the static pressure from small holes along the side on that tube. The pitot tube is connected to a tail so that it always makes the tube's head to face the wind. Additionally, the tube is heated to prevent rime ice formation on the tube. There are two lines from the tube down to the devices to measure the difference in pressure of the two lines. The measurement devices can be manometers, pressure transducers, or analog chart recorders.\n\nIn the tube anemometer the dynamic pressure is actually being measured, although the scale is usually graduated as a velocity scale. If the actual air density differs from the calibration value, due to differing temperature, elevation or barometric pressure, a correction is required to obtain the actual wind speed. Approximately 1.5% (1.6% above 6,000 feet) should be added to the velocity recorded by a tube anemometer for each 1000 ft (5% for each kilometer) above sea-level.\n\nAt airports, it is essential to have accurate wind data under all conditions, including freezing precipitation. Anemometry is also required in monitoring and controlling the operation of wind turbines, which in cold environments are prone to in-cloud icing. Icing alters the aerodynamics of an anemometer and may entirely block it from operating. Therefore, anemometers used in these applications must be internally heated. Both cup anemometers and sonic anemometers are presently available with heated versions.\n\nIn order for wind speeds to be comparable from location to location, the effect of the terrain needs to be considered, especially in regard to height. Other considerations are the presence of trees, and both natural canyons and artificial canyons (urban buildings). The standard anemometer height in open rural terrain is 10 meters.\n\n\n"}
{"id": "56059680", "url": "https://en.wikipedia.org/wiki?curid=56059680", "title": "Ball pump needle", "text": "Ball pump needle\n\nA ball pump needle is a metal device in which air passes through it, from an inflating pump to a ball of sports. Such as the ones of football, soccer, volleyball, basketball, rugby, handball or waterpolo. Using a lubricant facilitates insertion of the inflation needle. A ball needle is related to a ball pump.\n"}
{"id": "7802082", "url": "https://en.wikipedia.org/wiki?curid=7802082", "title": "Capacitor voltage transformer", "text": "Capacitor voltage transformer\n\nA capacitor voltage transformer (CVT or CCVT), is a transformer used in power systems to step down extra high voltage signals and provide a low voltage signal, for metering or operating a protective relay.\n\nIn its most basic form, the device consists of three parts: two capacitors across which the transmission line signal is split, an inductive element to tune the device to the line frequency, and a voltage transformer to isolate and further step down the voltage for metering devices or protective relay.\n\nThe tuning of the divider to the line frequency makes the overall division ratio less sensitive to changes in the burden of the connected metering or protection devices. The device has at least four terminals: a terminal for connection to the high voltage signal, a ground terminal, and two secondary terminals which connect to the instrumentation or protective relay.\n\nCapacitor C is often constructed as a stack of smaller capacitors connected in series. This provides a large voltage drop across C and a relatively small voltage drop across C. As the majority of the voltage drop is on C, this reduces the required insulation level of the voltage transformer. This makes CVTs more economical than the wound voltage transformers under high voltage (over 100kV), as the latter one requires more winding and materials.\n\nThe CVT is also useful in communication systems. CVTs in combination with wave traps are used for filtering high-frequency communication signals from power frequency. This forms a carrier communication network throughout the transmission network, to communicate between substations. The CVT is installed at a point after Lightning Arrester and before Wave trap.\n\n\n"}
{"id": "36541240", "url": "https://en.wikipedia.org/wiki?curid=36541240", "title": "Ceramic mixing technology", "text": "Ceramic mixing technology\n\nCeramic mixing technology is used to mix and blend ceramics to create end products such as: ceramic powder blends, injection molding feedstock, electronics, decorative finishes, refractory linings, batteries and fuel cells, thermally conductive pastes, investment casting slurries, dental ceramics and advanced composites.\n\nA wide range of equipment is available for these requirements.\n\n"}
{"id": "3320853", "url": "https://en.wikipedia.org/wiki?curid=3320853", "title": "Chemical process", "text": "Chemical process\n\nIn a scientific sense, a chemical process is a method or means of somehow changing one or more chemicals or chemical compounds. Such a chemical process can occur by itself or be caused by an outside force, and involves a chemical reaction of some sort. In an \"engineering\" sense, a chemical process is a method intended to be used in manufacturing or on an industrial scale (see Industrial process) to change the composition of chemical(s) or material(s), usually using technology similar or related to that used in chemical plants or the chemical industry. \n\nNeither of these definitions are exact in the sense that one can always tell definitively what is a chemical process and what is not; they are practical definitions. There is also significant overlap in these two definition variations. Because of the inexactness of the definition, chemists and other scientists use the term \"chemical process\" only in a general sense or in the engineering sense. However, in the \"process (engineering)\" sense, the term \"chemical process\" is used extensively. The rest of the article will cover the engineering type of chemical process. \n\nAlthough this type of chemical process may sometimes involve only one step, often multiple steps, referred to as unit operations, are involved. In a plant, each of the unit operations commonly occur in individual vessels or sections of the plant called units. Often, one or more chemical reactions are involved, but other ways of changing chemical (or material) composition may be used, such as mixing or separation processes. The process steps may be sequential in time or sequential in space along a stream of flowing or moving material; see Chemical plant. For a given amount of a feed (input) material or product (output) material, an expected amount of material can be determined at key steps in the process from empirical data and material balance calculations. These amounts can be scaled up or down to suit the desired capacity or operation of a particular chemical plant built for such a process. More than one chemical plant may use the same chemical process, each plant perhaps at differently scaled capacities. \nChemical processes like distillation and crystallization go back to alchemy in Alexandria, Egypt.\n\nSuch chemical processes can be illustrated generally as block flow diagrams or in more detail as process flow diagrams. Block flow diagrams show the units as blocks and the streams flowing between them as connecting lines with arrowheads to show direction of flow. \n\nIn addition to chemical plants for producing chemicals, chemical processes with similar technology and equipment are also used in oil refining and other refineries, natural gas processing, polymer and pharmaceutical manufacturing, food processing, and water and wastewater treatment.\n\nUnit processing is the basic processing in chemical engineering. Together with unit operations it forms the main principle of the varied chemical industries. Each genre of unit processing follows the same chemical law much as each genre of unit operations follows the same physical law.\n\nChemical engineering unit processing consists of the following important processes:\n\n\n"}
{"id": "49315050", "url": "https://en.wikipedia.org/wiki?curid=49315050", "title": "Combustion tap-off cycle", "text": "Combustion tap-off cycle\n\nThe combustion tap-off cycle is a power cycle of a bipropellant rocket engine. The cycle routes hot gases from the main combustion chamber of the rocket engine and routes them through engine turbopump turbines to pump fuel, then is exhausted. Since not all fuel flows through the throat into the nozzle, the tap-off cycle is considered an open-cycle engine. The cycle is comparable to a gas-generator cycle engine with turbines driven by main combustion chamber exhaust rather than a separate gas generator or preburner.\n\nThe J-2S rocket engine, a cancelled engine developed by NASA, used the combustion tap-off cycle and was first successfully tested in 1969.\n\nBy 2013, Blue Origin, with their New Shepard launch vehicle, had successfully flight-tested the BE-3 engine using a tap-off cycle. According to Blue Origin, the cycle is particularly suited to human spaceflight due to its simplicity, with only one combustion chamber and a less stressful engine shutdown process. However, engine startup is more complicated, and due to its nature of feeding gases from the main combustion chamber into the turbopumps, the turbine must be built to withstand higher-than-normal temperatures. In contrast, the upper-stage variant of the BE-3, the BE-3U, uses an expander cycle to power the turbopump, and will be used on the upper stage of the New Glenn launch vehicle.\n\n"}
{"id": "4820337", "url": "https://en.wikipedia.org/wiki?curid=4820337", "title": "Computer repair technician", "text": "Computer repair technician\n\nA computer repair technician is a person who repairs and maintains computers and servers. The technician's responsibilities may extend to include building or configuring new hardware, installing and updating software packages, and creating and maintaining computer networks.\n\nComputer technicians work in a variety of settings, encompassing both the public and private sectors. Because of the relatively brief existence of the profession, institutions offer certificate and degree programs designed to prepare new technicians, but computer repairs are frequently performed by experienced and certified technicians who have little formal training in the field like private sectors \n\nA repair technician might work in a corporate information technology department, a central service center, or a retail computer sales environment. A public sector technician might work in the military, national security or law enforcement communities, health or public safety field, or an educational institution. Despite the vast variety of work environments, all computer technicians perform similar physical and investigative processes, including technical support. Experienced technicians might specialize in fields such as data recovery, system administration, or information systems. Some technicians are self-employed or own a firm that provides services in a regional area. Some are subcontracted as freelancers or consultants. This type of technician ranges from hobbyists and enthusiasts to those who work professionally in the field.\n\nComputer malfunctions can range from a minor setting that is incorrect, to spyware, viruses, and as far as replacing hardware and an entire operating system. Some technicians provide on-site services, usually at an hourly rate. Others can provide services off-site, where the client can drop their computers and other devices off at the repair shop. Some have pickup and drop off services for convenience. Some technicians may also take back old equipment for recycling (In the EU, this is required under WEEE rules).\n\nWhile computer hardware configurations vary widely, a \"Computer OEM & Repair\" technician will work with five general categories of hardware; desktop computers, laptops, servers, computer clusters and smartphones / mobile computing. Technicians also work with and occasionally repair a range of peripherals, including input devices (like keyboards, mice, and scanners), output devices (like displays, printers, and speakers), and data storage devices such as internal and external hard drives and disk arrays. Technicians involved in system administration might also work with networking hardware, including routers, switches, fiber optics, and wireless networks.\nOEM= Original Equipment Manufacturer.\n\nWhen possible, repair technicians protect the computer user's data and settings, so that, after repair, the user will not have lost any data and can fully use the device with little interruption. Addressing the issue, the technician could take action as minor as adjusting one or several settings or preferences, but could also apply more involved techniques like installing, uninstalling, or reinstalling various software packages.\n\nA reliable, but somewhat more complicated procedure for addressing software issues is known as a restore (also referred to as imaging, and/or reimaging), in which the computer's original installation image (including operating system and original applications) is reapplied to a formatted hard drive. Anything unique such as settings or personal files will be destroyed if not backed up on external media, as this reverts everything back to its original unused state. The computer technician can only reimage if there is an image of the hard drive for that computer either in a separate partition or stored elsewhere.\n\nOn a Microsoft Windows system, if there is a restore point that was saved (normally saved on the hard drive of the computer) then the Windows Registry can be restored to that point, sometimes solving problems that have arisen after the time the restore point was created.\n\nIn Texas, computer companies and professionals are required to have private investigators’ licenses if they access computer data for purposes other than diagnosis or repair. Texas Occupations Code, Chapter 1702 section 104, subsection 4(b).\n\n"}
{"id": "14924041", "url": "https://en.wikipedia.org/wiki?curid=14924041", "title": "Cyber electronic warfare", "text": "Cyber electronic warfare\n\nCyber electronic warfare (cyber EW) is a form of electronic warfare. Cyber EW is any military action involving the use of electromagnetic energy to control the domain characterized by the use of electronics and the electromagnetic spectrum to use exchange data via networked systems and associated physical infrastructures.\n\nCyber EW consists of the following three activities: cyber electronic attack (cyber EA), cyber electronic protection (cyber EP), and cyber electronic warfare support (cyber ES). These three activities are defined as follows:\n\nThe four terms suggested above are generated from electronic warfare.\n\n\n\n"}
{"id": "22209667", "url": "https://en.wikipedia.org/wiki?curid=22209667", "title": "Docter Optics", "text": "Docter Optics\n\nDocter Optics is a German manufacturer of quality sports optics, including binoculars, rifle scopes, spotting scopes, red dot sights, flashlights and reading glasses. Its headquarters are in Eisfeld, Thuringia, Germany, where most of the products are developed and manufactured. Docter is part of the Analytik Jena Group.\n\nThe company grew out of binocular production by the forerunner of Carl Zeiss AG before World War Two. In 1952 a part of the company VEB Carl Zeiss Jena was established in Eisfeld in what was then East Germany. Starting from tasks as supplier of parts and pre-assembling products for the Zeiss factories in Jena the company continuously developed over the following years to become a producer of precision-engineered optical consumer goods and industrial products. Besides that VEB Carl Zeiss Jena produced military optics for the armed forces of the German Democratic Republic (GDR) like the NVA DF 7x40 and NVA EDF 7x40 roof prism binoculars.\n\nA year after the German reunification, the Eisfeld plant of the Jenoptik Carl Zeiss Jena GmbH, which employed 550 staff, was taken over by Bernhard Docter, who lent his name to the company and products. The company now traded under the name Docter-Optic-Eisfeld GmbH and continued with the production of binoculars, riflescopes, spotting scopes, magnifying glasses and opto-electronic measurement equipment. The company Docter-Optic in Wetzlar declared bankruptcy for the whole business on 15 November 1995.\n\nOn 1 May 1997 a part of the business with 40 staff was continued by Analytik Jena GmbH, to which traditionally good relations existed. Already the first year after the takeover was completed with very good turnover and results. By 2006 Docter Optics employed 100 staff.\n\n"}
{"id": "12793856", "url": "https://en.wikipedia.org/wiki?curid=12793856", "title": "Elements Software", "text": "Elements Software\n\nElements Software, formerly known as Helveta, is a supply chain management software company based in Abingdon, Oxfordshire, England. With revenues of £2.1 million in 2010, it has been recognized for its work in forestry conservation, specifically illegal logging prevention.\n\nPatrick Newton founded Helveta in Abingdon, Oxfordshire in 2004. The company developed supply-chain management software Control Intelligence (CI) World to allow customers to track assets. Supported industries include biofuel, food processing, and forestry. In 2010 Helveta's revenues reached £2.1 million.\n\nIn 2008, the Forest Development Authority hired Helveta and SGS, an inspection firm, to develop a technology barcode scanning system to track lumber in Liberia, where the United Nations banned the logging trade in 2003. Helveta also earned clients in the Democratic Republic of the Congo and Cameroon, where a similar project ran from 2008 to 2011. The Cameroon project met many of its goals but failed to secure a long-term commitment in part due to changing government authorities.\n\nKarim Peer is the Chief Executive Officer (CEO) after replacing Newton in 2011.\n\nThe Financial Times recognized Helveta as a \"Boldness in Business\" 2012 award winner for finding profit-making opportunities in sustainable development. In 2015, Helveta was selected as a top ten finalist in the EY Startup Challenge.\n\n"}
{"id": "547866", "url": "https://en.wikipedia.org/wiki?curid=547866", "title": "Engineers Without Borders – International", "text": "Engineers Without Borders – International\n\nEngineers Without Borders International (EWB-I) is an association of individual Engineers Without Borders/Ingenieurs Sans Frontieres groups.  EWB-I facilitates collaboration and the exchange of information among the member groups. EWB-I helps its member groups develop their capacity to assist underserved communities in their respective countries and around the world.\n\nEWB-I is a virtual organization with staff located in both the United States and South Africa.  EWB-I is run by an international board, composed of representatives of the EWB/ISF groups.\n\nThe member groups of EWB-I support the association’s vision for \"“A sustainable world where engineering enables long-term positive social and global development for the benefit of people and the environment everywhere.”\"  EWB-I seeks to promote collaboration so that collectively we can achieve more than the sum of our parts and fulfill our mission “To be the beating heart of the engineering movement for sustainable global development, building and evolving engineering capacity throughout the world.\n\nProjects conducted by individual EWB/ISF member groups are grassroots and small and are not usually addressed by in-country consulting firms. Through their work and projects, EWB-I member groups contribute to meeting the UN Millennium Development Goals (MDGs) and Sustainable Development Goals (SDGs) through capacity building in their projects. EWB-I also endorses the Earth Charter and the Universal Declaration of Human Rights. \n\nThe EWB Movement-EWB-UK Chief Executive Katie Creswell Maynard talks about the EWB movement and a specific project undertaken by EWB-UK. \n\nWhile each member group is fully independent and autonomous, EWB-I provides a platform for its member groups, affiliates, and outside organizations \n\n\nEWB-International also facilitates the start-up of new groups in the areas where none currently exist.  EWB-I consists of member groups, provisional member groups, and start-up groups. Membership requires that all member groups adhere to high professional and ethical standards as stated in the EWB-I Bylaws. \n\n\nEngineers Without Borders International is not in any way affiliated with Doctors Without Borders, which is a registered trademark of Bureau International de Medecins Sans Frontieres.\n\n\n"}
{"id": "44382509", "url": "https://en.wikipedia.org/wiki?curid=44382509", "title": "Example-centric programming", "text": "Example-centric programming\n\nExample-centric programming is an approach to software development that helps the user to create software by locating and modifying small examples into a larger whole. That approach can be helped by tools that allow an integrated development environment (IDE) to show code examples or API documentation related to coding behaviors occurring in the IDE. “Borrow” tactics are often employed from online sources, by programmers leaving the IDE to troubleshoot.\nThe purpose of example-centric programming is to reduce the time spent by developers searching online. Ideally, in example-centric programming, the user interface integrates with help module examples for assistance without programmers leaving the IDE. The idea for this type of “instant documentation” is to reduce programming interruptions. The usage of this feature is not limited to experts, as some novices reap the benefits of an integrated knowledge base, without resorting to frequent web searches or browsing.\n\nThe growth of the web has fundamentally changed the way software is built. Vast increase in information resources and the democratization of access and distribution are main factors in the development of example-centric programming for end-user development. Tutorials are available on the web in seconds thus broadening the space of who writes it: designers, scientists, or hobbyists. By 2012 13 million program as a part of their job, yet only three million of those are actual professional programmers.\nPrevalence of online code repositories, documentation, blogs and forums—enables programmers to build applications iteratively searching for, modifying, and combining examples.\n\nUsing the web is integral to an opportunistic approach to programming when focusing on speed and ease of development over code robustness and maintainability. There is a widespread use of the web by programmers, novices and experts alike, to prototype, ideate, and discover.\n\nTo develop software quickly programmers often mash up various existing systems. As part of this process, programmers must often search for suitable components and learn new skills, thus they began using the web for this purpose.\n\nWhen developing software programmers spend 19% of their programming time on the web. Individuals use the web to accomplish several different kinds of activities. The intentions behind web use vary in form and time spent. Programmers spend most of the time learning a new concept, the least time is spent reminding themselves of details of a concept they already know, and in between they use the web to clarify their existing knowledge.\n\nExample-centric programming tries to solve the issue of having to get out of the development environment to look for references and examples while programming. For instance, traditionally, to find API documentation and sample code, programmers will either visit the language reference website of they go to search engines and make API specific queries. When trying to learn something new, programmers use web tutorials for just-in-time learning. Additionally, programmers deliberately choose not to remember complicated syntax and instead use the web as an external memory that can be accessed when needed.\n\nSome of the benefits of example-centric programming include:\n\nEmergence can be defined as a process whereby larger entities, patterns, and regularities arise through interactions among smaller or simpler entities that themselves do not exhibit such properties. The extensive amount of code publicly available on the web can be used to find this type of patterns and regularities. By modeling how developers use programming languages in practices, algorithms for finding common idioms and detecting unlikely code can be created.\n\nThis process is limited to the amount of code that programmers are willing and able to share. Because people write more code than they share online there is a lot of duplicated effort. To fully use the power of the crowd, the effort required to publish code online should be reduced.\n\nBlueprint is a plugin for Adobe Flash Builder that automatically augments queries with code context, presents a code-centric view of search results, embeds the search experience into the editor, and retains a link between copied code and its source. It is designed to help programmers with web searches and allow them to easily remember forgotten details and clarify existing knowledge.\n\nIt displays results from a varied set of web pages enabling users to browse and evaluate search results rapidly.\n\nBlueprint is task-specific, meaning that it will specifically search for examples in the programming language.\n\nRedprint is a browser-based development environment for PHP that integrates API specific \"instant example\" and \"instant documentation\" display interfaces. The prototype IDE was developed by Anant Bhardwaj, then at Stanford University on the premise that task-specific example interfaces leave programmers having to understand the example code that has been found, and thus Redprint also includes an API specific search interface. The API specific search interface searches for relevant API specific examples and documentation.\n\nCodex is a knowledge base that records common practices for Ruby. Uses crowdsourced data from developers and searches all code, looking for patterns, that way if someone is coding in a strange way, Codex lets them know that they are doing something wrong.\n\nCodex uses statistical linting to find poorly written code, or code which is syntactically different from well written code, and warn the user, pattern annotation to automatically discover common programming idioms and annotate them with metadata using crowdsourcing, and library generation to construct a utility package that encapsulates emergent software practice.\n\nA codelet is a block of example code an interactive helper widget that assists the user in understanding and integrating the example.\n\nBing Code Search is an extension to Microsoft Visual Studio developed by a team made of people from Visual Studio, Bing and Microsoft Research that allows developers to search code examples and documentation from Bing directly from IntelliSense.\n\nBing Code Search gathers its code samples from MSDN, StackOverflow, Dotnetperls and CSharp411.\n\nCodota helps developers find typical Java code examples by analyzing millions of code snippets available on sites such as GitHub and StackOverflow. Codota ranks these examples by criteria such as commonality of the coding patterns, credibility of the origin and clarity of the code.\nThe Codota plugin for the IntelliJ IDEA and Android Studio IDEs allows developers to get code examples for using Java and android APIs without having to leave their editor.\n\n\n"}
{"id": "30445334", "url": "https://en.wikipedia.org/wiki?curid=30445334", "title": "Fixes that fail", "text": "Fixes that fail\n\nFixes that fail is a system archetype that in system dynamics is used to describe and analyze a situation, where a fix effective in the short-term creates side effects for the long-term behaviour of the system and may result in the need of even more fixes. This archetype may be also known as fixes that backfire or corrective actions that fail. It resembles the Shifting the burden archetype.\n\nIn a \"fixes that fail\" scenario the encounter of a problem is faced by a corrective action or fix that seems to solve the issue. However, this action leads to some unforeseen consequences. They form then a feedback loop that either worsens the original problem or creates a related one.\nIn system dynamics this is described by a circles of causality (Fig. 1) as a system consisting of two feedback loops. One is the balancing feedback loop B1 of the corrective action, the second is the reinforcing feedback loop R2 of the unintended consequences. These influence the problem with a delay and therefore make it difficult to recognize the source of the new rise of the problem.\n\nRepresentation of the long-term disadvantages of the scenario can be seen on Fig. 2. Although the symptoms go through a decrease when fixes are applied, the overall crisis threshold rises. \nA representation with a stock and flow diagram of this archetype is on Fig. 3.\n\nThe fix influences the number of problems present in the system proportionally to the fix factor and the problems to be resolved. When activated by the action variable, the fix lowers the problems, thus creating a balancing loop. However, each fix also starts a delayed consequence which adds to the problems proportionally to the consequence factor and the fix applied. Combined, these create a growing number of problems to be dealt with.\n\nAs an archetype, it helps to gain insight into patterns that are present in the system and thereby also give the tools to change these patterns. In the case of \"Fixes that fail\", the warning sign is a problem which reappears although fixes were applied. It is crucial to recognize that the fix only adds to the overall deteriorating state and does not solve the problem. To identify this pattern, it is needed to consider a connection between the symptoms and the fixes we apply to solve them, which can be very difficult to do. In management this can be present as a \"hero-scapegoat\" cycle. While the manager who applied the fix gets promoted for diminishing the problem. A new manager must face the returning problem symptom and may be punished for failing to do his job. Then a new hero is found who temporarily solves the problem symptoms. The delay of the reinforcing loop makes it difficult to recognize the causal relation between the fix applied to the symptoms and the new problems arising. What then seems to be a series of successes in short-term then are steps towards failure on the long-term.\n\nSome typical ways of thinking associated with the pattern are:\nThey can serve as a warning that this archetype is present or will be. \n\nIf this pattern is recognized, then there are multiple possibilities how to react, depending on which leverage point is addressed:\n\nA few common examples of the pattern. The situation describes always the starting point to which a fix is applied. This bears then the consequences which are confronted again with a new fix.\n\n\n\n\n\n\n"}
{"id": "31825004", "url": "https://en.wikipedia.org/wiki?curid=31825004", "title": "Forestry and Agricultural Biotechnology Institute", "text": "Forestry and Agricultural Biotechnology Institute\n\nThe Forestry and Agricultural Biotechnology Institute (FABI) (Pretoria) was established in 1997 and is located on the University of Pretoria campus. The goal of the institute is to help the development of novel food and fibre crops, that will clearly contribute to global economic development and food security.\n\nFABI was involved in 2011 in the completion of the eucalyptus tree genome (\"Eucalyptus grandis\").\n\n\n"}
{"id": "38108415", "url": "https://en.wikipedia.org/wiki?curid=38108415", "title": "FreePBX Distro", "text": "FreePBX Distro\n\nThe FreePBX Distro is a freeware unified communications software system that consists of a graphical user interface (GUI) for configuring, controlling, and managing Asterisk PBX software. The FreePBX Distro includes packages that offer VoIP, PBX, Fax,IVR, voice-mail and email functions.\n\nThe FreePBX Distro Linux distribution is based on CentOS, which maintains binary compatibility with Red Hat Enterprise Linux. FreePBX has contributed to the popularity of Asterisk.\n\nThe Official FreePBX Distro is installed from a CD-ROM image available by web download, that includes the system CentOS, Asterisk, FreePBX GUI and assorted dependencies.\n\nThe FreePBX Distro has built in support for cards from multiple vendors, including Digium, OpenVox, Alto, Rhino Equipment, Xorcom and Sangoma.\n\nThe FreePBX Distro supports a large number of phone models via open-source modules.\n\nFreePBX made its debut in 2004 as the AMP project (Asterisk Management Portal). The FreePBX Distro was released in 2011 as an All-In-One solution for building a PBX using Asterisk, CentOS and FreePBX.\n\nFreePBX has over 1 million active production PBXs and over 20,000 new systems added each month. Supported PBX phone manufacturers include Aastra Technologies, Algo, AND, Audiocodes, Cisco Systems, Cyberdata, Digium, Grandstream, Mitel, Panasonic, Polycom, Sangoma, Snom, Xorcom and Yealink.\n\nThe core telephony engine is Asterisk (PBX), as configured by the Open Source FreePBX GUI .\n\nThe version numbering system summarizes the versions of core components of the FreePBX Distro. As an example, the \"FreePBX Distro 2.210.62-1\" version string has the following components. The first number (2) represents the major track number. This second number (210) refers to \"FreePBX 2.10\" GUI. The third number (62) refers to \"CentOS 6.2\", and the final number (1) is used as the minor release revision of this major track number.\n\nThe latest stable release is FreePBX Distro Stable SNG7-PBX-64bit-1805-1 based on these main components:\nShell update scripts for each major release track are available on wiki.freepbx.org.\n\n"}
{"id": "55632237", "url": "https://en.wikipedia.org/wiki?curid=55632237", "title": "GreenMantra Technologies", "text": "GreenMantra Technologies\n\nGreenMantra Technologies is a Canadian clean technology company based in Brantford, Ontario, that converts recycled waste plastics into polymers, additives, and other chemicals for industrial use.\n\nGreenMantra Technologies has developed a thermo-catalytic system and patented manufacturing process that allows the conversion of waste plastics such as grocery bags, film and foam, into high-value waxes, polymers and other specialty chemicals. The company’s end products, which are sold under the Ceranovus brand name, are polyethylene and polypropylene additives that can be custom-formulated for specific physical and performance characteristics. These products have applications in coatings, inks, plastics processing, adhesives, rubber compounding and asphalt modification for roofing and paving. GreenMantra's original technologies were developed by Dr. Anil Kumar and Pushkar Kumar. \n\nIn 2018, the company will be launching a line of styrenic polymers.\n\n\n\n"}
{"id": "963313", "url": "https://en.wikipedia.org/wiki?curid=963313", "title": "Hemp", "text": "Hemp\n\nHemp, or industrial hemp (from Old English \"hænep\"), typically found in the northern hemisphere, is a variety of the \"Cannabis sativa\" plant species that is grown specifically for the industrial uses of its derived products. It is one of the fastest growing plants and was one of the first plants to be spun into usable fiber 10,000 years ago. It can be refined into a variety of commercial items including paper, textiles, clothing, biodegradable plastics, paint, insulation, biofuel, food, and animal feed.\n\nAlthough cannabis as a drug and industrial hemp both derive from the species \"Cannabis sativa\" and contain the psychoactive component tetrahydrocannabinol (THC), they are distinct strains with unique phytochemical compositions and uses. Hemp has lower concentrations of THC and higher concentrations of cannabidiol (CBD), which decreases or eliminates its psychoactive effects. The legality of industrial hemp varies widely between countries. Some governments regulate the concentration of THC and permit only hemp that is bred with an especially low THC content.\n\nThe etymology is uncertain but there appears to be no common Proto-Indo-European source for the various forms of the word; the Greek term ' is the oldest attested form, which may have been borrowed from an earlier Scythian or Thracian word. Then it appears to have been borrowed into Latin, and separately into Slavic and from there into Baltic, Finnish, and Germanic languages. Following Grimm's law, the \"k\" would have changed to \"h\" with the first Germanic sound shift, after which it may have been adapted into the Old English form, '. However, this theory assumes that hemp was not widely spread among different societies until after it was already being used as a psychoactive drug, which Adams and Mallory (1997) believe to be unlikely based on archaeological evidence. Barber (1991) however, argued that the spread of the name \"kannabis\" was due to its historically more recent drug use, starting from the south, around Iran, whereas non-THC varieties of hemp are older and prehistoric. Another possible source of origin is Assyrian \"qunnabu\", which was the name for a source of oil, fiber, and medicine in the 1st millennium BC.\n\nCognates of hemp in other Germanic languages include Dutch \"hennep\", Danish and Norwegian \"hamp\", German \"Hanf\", and Swedish \"hampa\".\n\nHemp is used to make a variety of commercial and industrial products including rope, textiles, clothing, shoes, food, paper, bioplastics, insulation, and biofuel. The bast fibers can be used to make textiles that are 100% hemp, but they are commonly blended with other fibers, such as flax, cotton or silk, as well as virgin and recycled polyester, to make woven fabrics for apparel and furnishings. The inner two fibers of the plant are more woody and typically have industrial applications, such as mulch, animal bedding and litter. When oxidized (often erroneously referred to as \"drying\"), hemp oil from the seeds becomes solid and can be used in the manufacture of oil-based paints, in creams as a moisturizing agent, for cooking, and in plastics. Hemp seeds have been used in bird feed mix as well. A survey in 2003 showed that more than 95% of hemp seed sold in the European Union was used in animal and bird feed.\n\nHemp seeds can be eaten raw, ground into hemp meal, sprouted or made into dried sprout powder. The leaves of the hemp plant can be consumed raw in salads. Hemp seeds can also be made into a liquid and used for baking or for beverages such as hemp milk, hemp juice, and tea. Hemp oil is cold-pressed from the seed and is high in unsaturated fatty acids.\n\nIn 2011, the U.S. imported $11.5 million worth of hemp products, mostly driven by growth in the demand for hemp seed and hemp oil for use as ingredients in foods such as granola.\n\nIn the UK, the Department for Environment, Food and Rural Affairs treats hemp as a purely non-food crop, but with proper licensing and proof of less than 0.2% THC concentration, hemp seeds can be imported for sowing or for sale as a food or food ingredient. In the U.S., imported hemp can be used legally in food products and, , was typically sold in health food stores or through mail order.\n\nA 100-gram portion of hulled hemp seeds supplies 586 calories. They contain 5% water, 5% carbohydrates, 49% total fat, and 31% protein. Hemp seeds are notable in providing 64% of the Daily Value (DV) of protein per 100-gram serving. Hemp seeds are a rich source of dietary fiber (20% DV), B vitamins, and the dietary minerals manganese (362% DV), phosphorus (236% DV), magnesium (197% DV), zinc (104% DV), and iron (61% DV). About 73% of the energy in hempseed is in the form of fats and essential fatty acids, mainly polyunsaturated fatty acids, linoleic, oleic, and alpha-linolenic acids.\n\nHempseed's amino acid profile is comparable to other sources of protein such as meat, milk, eggs and soy. Protein digestibility-corrected amino acid scores (PDCAAS), which attempt to measure the degree to which a food for humans is a \"complete protein\", were 0.49–0.53 for whole hemp seed, 0.46–0.51 for hempseed meal, and 0.63–0.66 for hulled hempseed.\n\nHemp oil oxidizes and turns rancid within a short period of time if not stored properly; its shelf life is extended when it is stored in a dark airtight container and refrigerated.\n\nHemp fiber has been used extensively throughout history, with production climaxing soon after being introduced to the New World. For centuries, items ranging from rope, to fabrics, to industrial materials were made from hemp fiber. Hemp was also commonly used to make sail canvas. The word \"canvas\" is derived from the word \"cannabis\". Pure hemp has a texture similar to linen. Because of its versatility for use in a variety of products, today hemp is used in a number of consumer goods, including clothing, shoes, accessories, dog collars, and home wares.\nConcrete-like blocks made with hemp and lime have been used as an insulating material for construction. Such blocks are not strong enough to be used for structural elements; they must be supported by a brick, wood, or steel frame. However, hemp fibres are extremely strong and durable, and have been shown to be usable as a replacement for wood for many jobs, including creating very durable and breathable homes. The most common use of hemp lime in building is by casting the hemp and lime mix while wet around a timber frame with temporary shuttering, and tamping the mix to form a firm mass; after the removal of the temporary shuttering, the solidified hemp mix is then ready to be plastered with a lime plaster.\n\nThe first example of the use of hempcrete was in 1986 in France with the renovation of the Maison de la Turquie in Nogent-sur-Seine by the innovator Charles Rasetti. In the UK hemp lime was first used in 2000 for the construction of two test dwellings in Haverhill. Designed by Modece Architects, who pioneered hemp's use in UK construction, the hemp houses were monitored in comparison with other standard dwellings by BRE. Completed in 2009, the Renewable House is one of the most technologically advanced made from hemp-based materials. The first US home made of hemp-based materials was completed in August 2010 in Asheville, North Carolina.\n\nA panellized system of hemp-lime panels for use in building construction is currently under test in a European Union-funded research collaboration led by the University of Bath. The panels are being designed to assure high-quality construction, rapid on-site erection, optimal hygrothermal performance from day one, and energy- and resource-efficient buildings. The 36-month-long work programme aims to refine product and manufacturing protocols, produce data for certification and marketing, warranty, insurance cover, and availability of finance. It also includes the development of markets in Britain, France, and Spain.\n\nHemp is used as an internal plaster and is a mixture of hemp hurd (shive) mixed with larger proportions of a lime-based binder. Hemp plaster has insulative qualities.\n\nA mixture of fiberglass, hemp fiber, kenaf, and flax has been used since 2002 to make composite panels for automobiles. The choice of which bast fiber to use is primarily based on cost and availability.\nVarious car makers are beginning to use hemp in their cars, including Audi, BMW, Ford, GM, Chrysler, Honda, Iveco, Lotus, Mercedes, Mitsubishi, Porsche, Saturn, Volkswagen and Volvo. For example, the Lotus Eco Elise\nand the Mercedes C-Class both contain hemp (up to 20 kg in each car in the case of the latter).\n\nHemp paper are paper varieties consisting exclusively or to a large extent from pulp obtained from fibers of industrial hemp. The products are mainly specialty papers such as cigarette paper, banknotes and technical filter papers . Compared to wood pulp, hemp pulp offers a four to five times longer fibre, a significantly lower lignin fraction as well as a higher tear resistance and tensile strength. However, production costs are about four times higher than for paper from wood, so hemp paper could not be used for mass applications as printing, writing and packaging paper.\n\nHemp jewelry is the product of knotting hemp twine through the practice of macramé. Hemp jewellery includes bracelets, necklaces, anklets, rings, watches and other adornments. Some jewellery features beads made from crystals, glass, stone, wood and bones. The hemp twine varies in thickness and comes in a variety of colors. There are many different stitches used to create hemp jewellery, however, the half knot and full knot stitches are most common.\n\nIn recent years, hemp has been growing in popularity as a material used in shoes. Today you can find boots, athletic shoes, sandals, and dress shoes that are made with 100% hemp fiber, or textiles that blend hemp fibers with materials such as cotton, jute, virgin polyester, and recycled polyester. The strength of hemp fibers makes it an ideal material for shoes because it's durable. In addition, it's breathable and naturally antimicrobial, so it doesn't hold on to odors. Because hemp can be grown sustainably, shoes, clothing, and accessories made with hemp are representative of the sustainable fashion movement.\n\nHemp rope was used in the age of sailing ships, though the rope had to be protected by tarring, since hemp rope has a propensity for breaking from rot, as the capillary effect of the rope-woven fibers tended to hold liquid at the interior, while seeming dry from the outside. Tarring was a labor-intensive process, and earned sailors the nickname \"Jack Tar\". Hemp rope was phased out when Manila, which does not require tarring, became widely available. Manila is sometimes referred to as Manila hemp, but is not related to hemp; it is abacá, a species of banana.\n\nHemp shives are the core of the stem, hemp hurds are broken parts of the core. In the EU, they are used for animal bedding (horses, for instance), or for horticultural mulch. Industrial hemp is much more profitable if both fibers and shives (or even seeds) can be used.\n\nHemp can be used as a \"mop crop\" to clear impurities out of wastewater, such as sewage effluent, excessive phosphorus from chicken litter, or other unwanted substances or chemicals. Additionally, hemp is being used to clean contaminants at the Chernobyl nuclear disaster site, by way of a process which is known as phytoremediation—the process of clearing radioisotopes and a variety of other toxins from the soil, water, and air.\n\nHemp crops are tall, have thick foliage, and can be planted densely, and thus can be grown as a smother crop to kill tough weeds. Using hemp this way can help farmers avoid the use of herbicides, gain organic certification, and gain the benefits of crop rotation. However, due to the plant's rapid and dense growth characteristics, some jurisdictions consider hemp a prohibited and noxious weed, much like Scotch Broom.\n\nBiodiesel can be made from the oils in hemp seeds and stalks; this product is sometimes called \"hempoline\". Alcohol fuel (ethanol or, less commonly, methanol) can be made by fermenting the whole plant.\n\nFiltered hemp oil can be used directly to power diesel engines. In 1892, Rudolf Diesel invented the diesel engine, which he intended to power \"by a variety of fuels, especially vegetable and seed oils, which earlier were used for oil lamps, i.e. the Argand lamp.\"\n\nProduction of vehicle fuel from hemp is very small. Commercial biodiesel and biogas is typically produced from cereals, coconuts, palmseeds and cheaper raw materials like garbage, wastewater, dead plant and animal material, animal feces and kitchen waste.\n\nTraditionally, hemp stalks would be water-retted first before the fibers were beaten off the inner hurd by hand, a process known as scutching. As mechanical technology evolved, separating the fiber from the core was accomplished by crushing rollers and brush rollers that would produce a nearly clean fiber. After the Marijuana Tax Act was implemented in 1938, the technology for separating the fibers from the core remained \"frozen in time\".\n\nOnly in 1997, did Ireland, parts of the Commonwealth and other countries begin to legally grow industrial hemp again. Iterations of the 1930s decorticator have been met with limited success, along with steam explosion and chemical processing known as thermomechanical pulping.\n\nHemp is usually planted between March and May in the northern hemisphere, between September and November in the southern hemisphere. It matures in about three to four months.\n\nMillennia of selective breeding have resulted in varieties that display a wide range of traits; e.g. suited for a particular environments/latitudes, producing different ratios and compositions of terpenoids and cannabinoids (CBD, THC, CBG, CBC, CBN...etc.), fibre quality, oil/seed yield etc. Hemp grown for fiber is planted closely, resulting in tall, slender plants with long fibers.\n\nUse of industrial hemp plant and its cultivation was commonplace until the 1900s, when it was associated with its genetic sibling aka Drug-Type Cannabis species (which contain higher levels of psychoactive THC). Influential groups misconstrued hemp as a dangerous 'drug', even though it is not a 'drug' and it has the potential to be a sustainable & profitable alternative crop.\n\nIn the United States, the public's perception of hemp as marijuana has blocked hemp from becoming a useful crop and product,\" in spite of its vital importance prior to World War II. Ideally, according to Britain's Department for Environment, Food and Rural Affairs, the herb should be desiccated and harvested towards the end of flowering. This early cropping reduces the seed yield but improves the fiber yield and quality. In these strains of industrial hemp* the tetrahydrocannabinol (THC) content would have been very low.\n\nThe seeds are sown from mid-April to mid-May with grain drills to 4–6 cm sowing depth. Hemp needs less fertilizer than corn does. A total of 60–150 kg of nitrogen, 40–140 kg phosphorus (PO) and 75–200 kg of potassium [5] per acre for hemp fiber made before sowing and again later, maybe three to four weeks. When practiced, especially in France double use of fiber and seed fertilization with nitrogen doses up to 100 kg / ha rather low. Organic fertilizers such as manure can utilize industrial hemp well. Neither weeds nor crop protection measures are necessary.\n\nIn contrast to cannabis for medical use, varieties grown for fiber and seed have less than 0.3% THC and are unsuitable for producing hashish and marijuana. Present in industrial hemp, cannabidiol is a major constituent among some 560 compounds found in hemp.\n\n\"Cannabis sativa\" L. subsp. \"sativa\" var. \"sativa\" is the variety grown for industrial use, while \"C. sativa\" subsp. \"indica\" generally has poor fiber quality and female buds from this variety are primarily used for recreational and medicinal purposes. The major differences between the two types of plants are the appearance, and the amount of Δ-tetrahydrocannabinol (THC) secreted in a resinous mixture by epidermal hairs called glandular trichomes, although they can also be distinguished genetically. Oilseed and fiber varieties of \"Cannabis\" approved for industrial hemp production produce only minute amounts of this psychoactive drug, not enough for any physical or psychological effects. Typically, hemp contains below 0.3% THC, while cultivars of \"Cannabis\" grown for medicinal or recreational use can contain anywhere from 2% to over 20%.\n\nSmallholder plots are usually harvested by hand. The plants are cut at 2 to 3 cm above the soil and left on the ground to dry. Mechanical harvesting is now common, using specially adapted cutter-binders or simpler cutters.\n\nThe cut hemp is laid in swathes to dry for up to four days. This was traditionally followed by \"retting\", either water retting (the bundled hemp floats in water) or dew retting (the hemp remains on the ground and is affected by the moisture in dew, and by molds and bacterial action). \n\nFor profitable hemp farming, particularly deep, humus-rich, nutrient-rich soil with controlled water flow is preferable. Waterlogged acidic, compressed or extremely light (sandy) soils primarily affect the early development of plants. Steep and high altitudes of more than 400 m above sea level are best avoided. Hemp is relatively insensitive to cold temperatures and can withstand frost down to −5 °C. Seeds can germinate down to 1–3 °C. Hemp needs a lot of heat, so earlier varieties come to maturation. The water requirement is 300–500 l/kg dry matter. This is around 1/14th that of cotton, which takes between 7,000 and 29,000 l/kg, according to WWF. Roots can grow up to 3 feet into the soil and use water from deeper soil layers.\n\nHemp benefits crops grown after it. So, it is generally grown before winter cereals. Advantageous changes are high weed suppression, soil loosening by the large hemp root system, and the positive effect on soil tilth. Since hemp is very self-compatible, it can also be grown several years in a row in the same fields (monoculture).\n\nHemp plants can be vulnerable to various pathogens, including bacteria, fungi, nematodes, viruses and other miscellaneous pathogens. Such diseases often lead to reduced fiber quality, stunted growth, and death of the plant. These diseases rarely affect the yield of a hemp field, so hemp production is not traditionally dependent on the use of pesticides.\n\nHemp is considered by a 1998 study in \"Environmental Economics\" to be environmentally friendly due to a decrease of land use and other environmental impacts, indicating a possible decrease of ecological footprint in a US context compared to typical benchmarks. A 2010 study, however, that compared the production of paper specifically from hemp and eucalyptus concluded that \"industrial hemp presents higher environmental impacts than eucalyptus paper\"; however, the article also highlights that \"there is scope for improving industrial hemp paper production\". Hemp is also claimed to require few pesticides and no herbicides, and it has been called a carbon negative raw material.\nResults indicate that high yield of hemp may require high total nutrient levels (field plus fertilizer nutrients) similar to a high yielding wheat crop.\n\nThe world-leading producer of hemp is China, which produces more than 70% of the world output. France ranks second with about a quarter of the world production. Smaller production occurs in the rest of Europe, Chile, and North Korea. Over 30 countries produce industrial hemp, including Australia, Austria, Canada, Chile, China, Denmark, Egypt, Finland, Great Britain, Germany, Greece, Hungary, India, Italy, Japan, Korea, Netherlands, New Zealand, Poland, Portugal, Romania, Russia, Slovenia, Spain, Sweden, Switzerland, Thailand, Turkey and Ukraine.\n\nThe United Kingdom and Germany resumed commercial production in the 1990s. British production is mostly used as bedding for horses; other uses are under development. Companies in Canada, the UK, the United States, and Germany, among many others, process hemp seed into a growing range of food products and cosmetics; many traditional growing countries still continue to produce textile-grade fibre.\nAir-dried stem yields in Ontario have from 1998 and onward ranged from 2.6-14.0 tonnes of dry, retted stalks per hectare (1-5.5 t/ac) at 12% moisture. Yields in Kent County, have averaged 8.75 t/ha (3.5 t/ac). Northern Ontario crops averaged 6.1 t/ha (2.5 t/ac) in 1998. Statistic for the European Union for 2008 to 2010 say that the average yield of hemp straw has varied between 6.3 and 7.3 ton per ha. Only a part of that is bast fiber. Around one tonne of bast fiber and 2-3 tonnes of core material can be decorticated from 3-4 tonnes of good-quality, dry-retted straw. For an annual yield of this level is it in Ontario recommended to add nitrogen (N):70–110 kg/ha, phosphate (PO): up to 80 kg/ha and potash (KO): 40–90 kg/ha.\nThe average yield of dry hemp stalks in Europe was 6 ton/ha (2.4 ton/ac) in 2001 and 2002.\n\nFAO argue that an optimum yield of hemp fiber is more than 2 tonnes per ha, while average yields are around 650 kg/ha.\n\nIn the Australian states of Tasmania, Victoria, Queensland, New South Wales, and most recently, South Australia, the state governments have issued licences to grow hemp for industrial use. The first to initiate modern research into the potential of cannabis was the state of Tasmania, which pioneered the licensing of hemp during the early 1990s. The state of Victoria was an early adopter in 1998, and has reissued the regulation in 2008.\n\nQueensland has allowed industrial production under licence since 2002, where the issuance is controlled under the Drugs Misuse Act 1986.\nNew South Wales now issues licences under a law, the Hemp Industry Regulations Act 2008 (No 58), that came into effect as of 6 November 2008.\nMost recently, South Wales legalized industrial hemp under South Australia’s Industrial Hemp Act 2017, which commenced on 12 November 2017.\n\nCommercial production (including cultivation) of industrial hemp has been permitted in Canada since 1998 under licenses and authorization issued by Health Canada (9,725 ha in 2004, 5450 ha in 2009).\n\nIn the early 1990s, industrial hemp agriculture in North America began with the Hemp Awareness Committee at the University of Manitoba. The Committee worked with the provincial government to get research and development assistance, and was able to obtain test plot permits from the Canadian government. Their efforts led to the legalization of industrial hemp (hemp with only minute amounts of tetrahydrocannabinol) in Canada and the first harvest in 1998.\n\nIn 2017, the cultivated area for hemp in the Prairie provinces include Saskatchewan with more than , Alberta with , and Manitoba with . Canadian hemp is cultivated mostly for its food value as hulled hemp seeds, hemp oils and hemp protein powders, with only a small fraction devoted to production of hemp fiber used for construction and insulation.\n\nFrance is Europe's biggest producer (and the world's second largest producer) with 8,000 hectares cultivated. 70-80% of the hemp fibre produced in 2003 was used for specialty pulp for cigarette papers and technical applications. About 15% was used in the automotive sector, and 5-6% was used for insulation mats. About 95% of hurds were used as animal bedding, while almost 5% was used in the building sector. In 2010/2011, a total of was cultivated with hemp in the EU, a decline compared with previous year.\n\nFrom the 1950s to the 1980s, the Soviet Union was the world's largest producer of hemp (3,000 km in 1970). The main production areas were in Ukraine, the Kursk and Orel regions of Russia, and near the Polish border. Since its inception in 1931, the Hemp Breeding Department at the Institute of Bast Crops in Hlukhiv (Glukhov), Ukraine, has been one of the world's largest centers for developing new hemp varieties, focusing on improving fiber quality, per-hectare yields, and low THC content.\n\nAfter the collapse of the Soviet Union, the commercial cultivation of hemp declined sharply. However, at least an estimated 2.5 million acres of hemp grow wild in the Russian Far East and Black Sea regions.\n\nIn the United Kingdom, cultivation licences are issued by the Home Office under the Misuse of Drugs Act 1971. When grown for nondrug purposes, hemp is referred to as industrial hemp, and a common product is fibre for use in a wide variety of products, as well as the seed for nutritional aspects and for the oil. Feral hemp or ditch weed is usually a naturalized fibre or oilseed strain of \"Cannabis\" that has escaped from cultivation and is self-seeding.\n\nHemp was made illegal to grow without a permit in the U.S. under the Controlled Substances Act passed in 1970 because of its relation to marijuana, and any imported hemp products must meet a zero tolerance level. Some states have made the cultivation of industrial hemp legal, but farmers in many states have not yet begun to grow it because of resistance from the federal Drug Enforcement Administration, making \"large-scale hemp growing\" in the United States \"not viable\" as late as 2013. In 2013, after the legalization of cannabis in the state, several farmers in Colorado planted and harvested several acres of hemp, bringing in the first hemp crop in the United States in over half a century. Colorado, Vermont, California, and North Dakota have passed laws enabling hemp licensure. All four states are waiting for permission to grow hemp from the DEA. Currently, Oregon has licensed industrial hemp . Congress included a provision in the Agricultural Act of 2014 that allowed colleges and state agencies to grow and conduct research on hemp in states where it is legal. Hemp production in Kentucky, formerly the United States' leading producer, resumed in 2014. Hemp production in North Carolina resumed in 2017, and in Washington State the same year. By the end of 2017, at least 34 U.S. states had industrial hemp programs. In 2018, New York began taking strides in industrial hemp production, along with hemp research pilot programs at Cornell University, Binghamton University and SUNY Morrisville.\n\nAs of 2015 the hemp industry estimated that annual sales of hemp products were around US$600 million annually; hemp seeds have been the major force driving this growth.\n\nYet, even with this progress, hemp businesses seem to face difficulty expanding in the US as they face challenges in traditional marketing and sales approaches. According to a case study done by \"Forbes\", hemp businesses and startups have had difficulty marketing and selling non-psychoactive hemp products, as some online advertising platforms and financial institutions do not distinguish between hemp and marijuana.\n\nHemp is possibly one of the earliest plants to be cultivated. An archeological site in the Oki Islands near Japan contained cannabis achenes from about 8000 BC, probably signifying use of the plant. Hemp use archaeologically dates back to the Neolithic Age in China, with hemp fiber imprints found on Yangshao culture pottery dating from the 5th millennium BC. The Chinese later used hemp to make clothes, shoes, ropes, and an early form of paper. The classical Greek historian Herodotus (ca. 480 BC) reported that the inhabitants of Scythia would often inhale the vapors of hemp-seed smoke, both as ritual and for their own pleasurable recreation.\n\nTextile expert Elizabeth Wayland Barber summarizes the historical evidence that \"Cannabis sativa\", \"grew and was known in the Neolithic period all across the northern latitudes, from Europe (Germany, Switzerland, Austria, Romania, Ukraine) to East Asia (Tibet and China),\" but, \"textile use of Cannabis sativa does not surface for certain in the West until relatively late, namely the Iron Age.\"\n\"I strongly suspect, however, that what catapulted hemp to sudden fame and fortune as a cultigen and caused it to spread rapidly westwards in the first millennium B.C. was the spread of the habit of pot-smoking from somewhere in south-central Asia, where the drug-bearing variety of the plant originally occurred. The linguistic evidence strongly supports this theory, both as to time and direction of spread and as to cause.\"\n\nJews living in Palestine in the 2nd century were familiar with the cultivation of hemp, as witnessed by a reference to it in the Mishna (\"Kil'ayim\" 2:5) as a variety of plant, along with Arum, that sometimes takes as many as three years to grow from a seedling. In late medieval Germany and Italy, hemp was employed in cooked dishes, as filling in pies and tortes, or boiled in a soup. Hemp in later Europe was mainly cultivated for its fibers, and was used for ropes on many ships, including those of Christopher Columbus. The use of hemp as a cloth was centered largely in the countryside, with higher quality textiles being available in the towns.\n\nThe Spaniards brought hemp to the Americas and cultivated it in Chile starting about 1545. Similar attempts were made in Peru, Colombia, and Mexico, but only in Chile did the crop find success. In July 1605, Samuel Champlain reported the use of grass and hemp clothing by the (Wampanoag) people of Cape Cod and the (Nauset) people of Plymouth Bay told him they harvested hemp in their region where it grew wild to a height of 4 to 5 ft.\n\nGeorge Washington pushed for the growth of hemp and even grew hemp himself, as it was a cash crop commonly used to make rope and fabric. In May 1765 he noted in his diary about the sowing of seeds each day until mid-April. Then he recounts the harvest in October which he grew 27 bushels that year.\n\nThere is some speculation that George Washington smoked the flower of the cannabis plant in order to achieve a recreational high (\"Like all farmers, Washington probably sampled the quality and potency of what he grew, and he may have used this hemp to treat his chronic tooth aches\"), but there is no evidence in any of his writings that he grew hemp for anything other than industrial purposes. It is sometimes supposed that an excerpt from Washington's diary, which reads \"Began to the Male from the Female hemp at Do.&—rather too late\" is evidence that he was trying to grow female plants for the THC found in the flowers. However, the editorial remark accompanying the diary states that \"This may arise from their [the male] being coarser, and the stalks larger\" In subsequent days, he describes soaking the hemp (to make the fibers usable) and harvesting the seeds, suggesting that he was growing hemp for industrial purposes, not recreational.\n\nGeorge Washington also imported the Indian Hemp plant from Asia, which was used for fiber and, by some growers, for intoxicating resin production. In a letter to William Pearce who managed the plants for him Washington says, \"What was done with the Indian Hemp plant from last summer? It ought, all of it, to be sown again; that not only a stock of seed sufficient for my own purposes might have been raised, but to have disseminated seed to others; as it is more valuable than common hemp.\"\n\nAdditional presidents known to have farmed hemp include Thomas Jefferson, James Madison, James Monroe, Andrew Jackson, Zachary Taylor, and Franklin Pierce.\nHistorically, hemp production had made up a significant portion of antebellum Kentucky's economy. Before the American Civil War, many slaves worked on plantations producing hemp.\n\nIn 1937, the Marihuana Tax Act of 1937 was passed in the United States, levying a tax on anyone who dealt commercially in cannabis, hemp, or marijuana. The passing of the Act to destroy the US hemp industry has been disputed to involve businessmen Andrew Mellon, Randolph Hearst and the Du Pont family.\n\nOne claim is that Hearst believed that his extensive timber holdings were threatened by the invention of the decorticator which he feared would allow hemp to become a cheap substitute for the paper pulp used for newspaper. Historical research indicates this fear was unfounded because improvements of the decorticators in the 1930s – machines that separated the fibers from the hemp stem – could not make hemp fiber a cheaper substitute for fibers from other sources. Further, decorticators did not perform satisfactorily in commercial production.\n\nAnother claim is that Mellon, Secretary of the Treasury and the wealthiest man in America at that time, had invested heavily in DuPont's new synthetic fiber, nylon, and believed that the replacement of the traditional resource, hemp, was integral to the new product's success. The company DuPont and many industrial historians dispute a link between nylon and hemp, nylon became immediately a scarce commodity. Nylon had characteristics that could be used for toothbrushes (sold from 1938) and very thin nylon fiber could compete with silk and rayon in various textiles normally not produced from hemp fiber, such as very thin stockings for women.\n\nHemp was used extensively by the United States during World War II to make uniforms, canvas, and rope. Much of the hemp used was cultivated in Kentucky and the Midwest. During World War II, the U.S. produced a short 1942 film, \"Hemp for Victory\", promoting hemp as a necessary crop to win the war.\n\nHemp has been grown for millennia in Asia and the Middle East for its fibre. Commercial production of hemp in the West took off in the eighteenth century, but was grown in the sixteenth century in eastern England. Because of colonial and naval expansion of the era, economies needed large quantities of hemp for rope and oakum. In the early 1940s, world production of hemp fiber ranged from 250 000 to 350 000 metric tonnes, Russia was the biggest producer.\n\nIn Western Europe, the cultivation of hemp was not legally banned by the 1930s, but the commercial cultivation stopped by then, due to decreased demand compared to increasingly popular artificial fibers. Speculation about the potential for commercial cultivation of hemp in large quantities has been criticized due to successful competition from other fibers for many products. The world production of hemp fiber fell from over 300,000 metric tons 1961 to about 75,000 metric tons in the early 1990s and has after that been stable at that level.\n\nIn Japan, hemp was historically used as paper and a fiber crop. There is archaeological evidence cannabis was used for clothing and the seeds were eaten in Japan back to the Jōmon period (10,000 to 300 BC). Many Kimono designs portray hemp, or \"asa\" (), as a beautiful plant. In 1948, marijuana was restricted as a narcotic drug. The ban on marijuana imposed by the United States authorities was alien to Japanese culture, as the drug had never been widely used in Japan before. Though these laws against marijuana are some of the world's strictest, allowing five years imprisonment for possession of the drug, they exempt hemp growers, whose crop is used to make robes for Buddhist monks and loincloths for Sumo wrestlers. Because marijuana use in Japan has doubled in the past decade, these exemptions have recently been called into question.\n\nThe cultivation of hemp in Portuguese lands began around the fourteenth century onwards, it was raw material for the preparation of rope and plugs for the Portuguese ships. Colonies for factories for the production of flax hemp, such as the Royal Flax Hemp Factory in Brazil.\n\nAfter the Restoration of Independence in 1640, in order to recover the ailing Portuguese naval fleet, were encouraged its cultivation as the Royal Decree of D. John IV in 1656. At that time its cultivation was carried out in Trás-os-Montes, Zone Tower Moncorvo, more precisely in Vilariça Valley, fertile land for any crop irrigation, and a very large area, flat and very fertile culture still wide until the last century grew up tobacco, a plant that needs a large space to expand and grow, the area lies in the valley of Serra de Bornes.\n\nIn 1971, the cultivation of hemp became illegal, and the production was substantially reduced. Because of EU regulations 1308/70, 619/71 and 1164/89, this law was revoked (for some certified seed varieties).\n\n"}
{"id": "184978", "url": "https://en.wikipedia.org/wiki?curid=184978", "title": "Home automation", "text": "Home automation\n\nHome automation or domotics is building automation for a home, called a smart home or smart house. A home automation system will control lighting, climate, entertainment systems, and appliances. It may also include home security such as access control and alarm systems. When connected with the Internet, home devices are an important constituent of the Internet of Things.\n\nA home automation system typically connects controlled devices to a central hub or \"gateway\". The user interface for control of the system uses either wall-mounted terminals, tablet or desktop computers, a mobile phone application, or a Web interface, that may also be accessible off-site through the Internet.\n\nWhile there are many competing vendors, there are very few worldwide accepted industry standards and the smart home space is heavily fragmented. Manufacturers often prevent independent implementations by withholding documentation and by litigation.\n\nThe home automation market was worth US$5.77 billion in 2013, predicted to reach a market value of US$12.81 billion by the year 2020.\n\nEarly home automation began with labour-saving machines. Self-contained electric or gas powered home appliances became viable in the 1900s with the introduction of electric power distribution and led to the introduction of washing machines (1904), water heaters (1889), refrigerators, sewing machines, dishwashers, and clothes dryers.\n\nIn 1975, the first general purpose home automation network technology, X10, was developed. It is a communication protocol for electronic devices. It primarily uses electric power transmission wiring for signalling and control, where the signals involve brief radio frequency bursts of digital data, and remains the most widely available. By 1978, X10 products included a 16 channel command console, a lamp module, and an appliance module. Soon after came the wall switch module and the first X10 timer.\n\nBy 2012, in the United States, according to ABI Research, 1.5 million home automation systems were installed.\n\nAccording to Li et al. (2016) there are three generations of home automation:\n\n\nThe word \"\"domotics\" (and \"domotica\"\" when used as a verb) is a contraction of the Latin word for a home (\"domus\") and the word \"robotics\".\n\n\nIn a review of home automation devices, \"Consumer Reports\" found two main concerns for consumers:\n\nMicrosoft Research found in 2011, that home automation could involve high cost of ownership, inflexibility of interconnected devices, and poor manageability.\n\nHistorically systems have been sold as complete systems where the consumer relies on one vendor for the entire system including the hardware, the communications protocol, the central hub, and the user interface. However, there are now open hardware and open source software systems which can be used instead of or with proprietary hardware.\n\nHome automation suffers from platform fragmentation and lack of technical standards a situation where the variety of home automation devices, in terms of both hardware variations and differences in the software running on them, makes the task of developing applications that work consistently between different inconsistent technology ecosystems hard. Customers may be hesitant to bet their IoT future on proprietary software or hardware devices that use proprietary protocols that may fade or become difficult to customize and interconnect.\n\nThe nature of home automation devices can also be a problem for security, since patches to bugs found in the core operating system often do not reach users of older and lower-price devices. One set of researchers say that the failure of vendors to support older devices with patches and updates leaves more than 87% of active devices vulnerable.\n\n"}
{"id": "398546", "url": "https://en.wikipedia.org/wiki?curid=398546", "title": "Howard Hughes Medical Institute", "text": "Howard Hughes Medical Institute\n\nThe Howard Hughes Medical Institute (HHMI) is an American non-profit medical research organization based in Chevy Chase, Maryland. It was founded by the American businessman Howard Hughes in 1953. It is one of the largest private funding organizations for biological and medical research in the United States. HHMI spends about $1 million per HHMI Investigator per year, which amounts to annual investment in biomedical research of about $825 million. The institute has an endowment of $22.6 billion, making it the second-wealthiest philanthropic organization in the United States and the second-best endowed medical research foundation in the world. HHMI is the former owner of the Hughes Aircraft Company - an American aerospace firm which was divested to various firms over time.\n\nThe institute was formed with the goal of basic research including trying to understand, in Hughes's words, \"the genesis of life itself.\" Despite its principles, in the early days it was generally viewed as a tax haven for Hughes's huge personal fortune. Hughes was HHMI's sole trustee and transferred his stock of Hughes Aircraft to the institute, in effect turning the large defense contractor into a tax-exempt charity. For many years the Institute grappled with maintaining its non-profit status; the Internal Revenue Service challenged its \"charitable\" status which made it tax exempt. Partly in response to such claims, starting in the late 1950s it began funding 47 investigators researching at eight different institutions; however, it remained a modest enterprise for several decades.\n\nThe institute was initially in Miami, Florida in 1953. Hughes's internist, Verne Mason, who treated Hughes after his 1946 plane crash, was chairman of the institute's medical advisory committee. The institute moved to Coconut Grove, Florida, in the mid-1970s and then to Bethesda, Maryland, in 1976. In 1993 the institute moved to its headquarters in Chevy Chase, Maryland.\n\nIt was not until after Hughes's death in 1976 that the institute's profile increased from an annual budget of $4 million in 1975 to $15 million in 1978 and prior to Hughes Aircraft sale the number had peaked to $200 million per year. At the time of the sale Hughes Aircraft employed 75,000 people and vast amounts of money from the approximate annual revenue of $6 billion dollars were put into Hughes Aircraft internal research and development rather than the medical institute. Most of the money for the medical institute came from the operations at Ground System Group responsible for providing Air Defense Systems to NATO, Pacific Rim, and the USA. In this period it focused its mission on genetics, immunology, and molecular biology. Since Hughes died without a will as the sole trustee of the HHMI, the institute was involved in lengthy court proceedings to determine whether it would benefit from Hughes's fortune. In April 1984, a court appointed new trustees for the institute's holdings. In January 1985 the trustees announced they would sell Hughes Aircraft by private sale or public stock offering. On June 5, 1985 General Motors (GM) was announced as the winner of a secretive five-month, sealed-bid auction. The purchase was completed on December 20, 1985 for an estimated $5.2 billion, $2.7 billion in cash and the rest in 50 million shares of GM Class H stock. The proceeds caused the institute to grow dramatically.\n\nHHMI completed the building of a new research campus in Ashburn, Virginia called Janelia Research Campus in October 2006. It is modeled after AT&T's Bell Labs and the Medical Research Council's Laboratory of Molecular Biology. With a main laboratory building nearly long, it contains of enclosed space, used primarily for research. The campus also features apartments for visiting researchers.\n\nIn 2007, HHMI and the publisher Elsevier announced they have established an agreement to make author manuscripts of HHMI research articles published in Elsevier and Cell Press journals publicly available six months following final publication. The agreement takes effect for articles published after September 1, 2007. In 2008, the Trustees of the Howard Hughes Medical Institute selected Robert Tjian as the new president of HHMI. In 2009, HHMI awarded fifty researchers, as part of the HHMI Early Career Scientist Competition. In 2016, the HHMI Trustees selected Erin K. O'Shea, a previous Vice President and Chief Scientific Officer at the institute, the new president of HHMI.\n\n\n the Howard Hughes Medical Institute had assets of $22,588,928,000.\nFunding details :\n\n"}
{"id": "41246", "url": "https://en.wikipedia.org/wiki?curid=41246", "title": "Hybrid coil", "text": "Hybrid coil\n\nA hybrid coil (or bridge transformer, or sometimes hybrid) is a transformer that has three windings, and which is designed to be configured as a circuit having four ports that are conjugate in pairs. \n\nA signal arriving at one port is divided equally between the two adjacent ports but does not appear at the opposite port. In the schematic diagram, the signal into W splits between X and Z, and no signal passes to Y. Similarly, signals into X split to W and Y with none to Z, etc. \n\nCorrect operation requires matched characteristic impedance at all four ports. Hybrids are a class of directional coupler in which the input port power is split equally between the two output ports. Forms of hybrid other than transfomer coils are possible; any format of directional coupler can be designed to be a hybrid. These formats include transmission lines and waveguides.\n\nThe primary use of a voiceband hybrid coil is to convert between 2-wire and 4-wire operation in sequential sections of a communications circuit, for example in a four-wire terminating set. Such conversion was necessary when repeaters were introduced in a 2-wire circuit, a frequent practice at early 20th century telephony. Without hybrids, the output of one amplifier feeds directly into the input of the other, resulting in a howling situation (upper diagram). By using hybrids, the outputs and inputs are isolated, resulting in correct 2-wire repeater operation. Late in the century, this practice became rare but hybrids continued in use in line cards.\n\nHybrids are realized using transformers. Two versions of transformer hybrids were used, the single transformer version providing unbalanced outputs with one end grounded, and the double transformer version providing balanced ports. \n\nFor use in 2-wire repeaters, the single transformer version suffices, since amplifiers in the repeaters have grounded inputs and outputs. X, Y, and Z share a common ground. As shown at left, signal into W, the 2-wire port, will appear at X and Z. But since Y is bridged from center of coil to center of X and Z, no signal appears. Signal into X will appear at W and Y. But signal at Z is the difference of what appears at Y and, through the transformer coil, at W, which is zero. Similar reasoning proves both pairs, W & Y, X & Z, are conjugates.\n\nWhen both the 2-wire and the 4-wire circuits must be balanced, double transformer hybrids are used, as shown at right. Signal into port W splits between X and Z, but due to reversed connection to the windings, cancel at port Y. Signal into port X goes to W and Y. But due to reversed connection to ports W and Y, Z gets no signal. Thus the pairs, W & Y, X & Z, are conjugates.\n\nTelephone hybrids are used in telephone exchanges to convert the 4-wire appearance to the 2-wire last mile connection to the subscriber's telephone. A different kind of hybrid is used in telephone handsets to convert the four wires of the transmitter (earpiece) and receiver (microphhone) to the 2-wire line connection. This kind of hybrid is more commonly called an induction coil due to its derivation from high-voltage induction coils. It does not produce a high voltage, but like the high-voltage variety, it is a step-up transformer in order to impedance match the low-impedance carbon button transmitter to the higher impedance parts of the system. The simple induction coil later evolved into a form of hybrid as a sidetone reduction measure, or volume of microphone output that was fed back to the earpiece. Without this, the phone user's own voice would be louder in the earpiece than the other party's. Today, the transformer version of the hybrid has been replaced by resistor networks and compact IC versions, which use integrated circuit electronics to do the job of the hybrid coil.\n\nRadio-frequency hybrids are used to split radio signals, including television. The splitter divides the antenna signal to feed multiple receivers.\n\n\n"}
{"id": "37268014", "url": "https://en.wikipedia.org/wiki?curid=37268014", "title": "JVC HR-3300", "text": "JVC HR-3300\n\nThe JVC HR-3300 VIDSTAR is the world's first VHS-based VCR to be released to the market, introduced by the president of JVC at the Okura Hotel on September 9, 1976. Sales started in Japan under the name Victor HR-3300 on 31 October 1976. Foreign sales followed in 1977 with the HR-3300U in the United States, and HR-3300EK in the United Kingdom.\n\nIn 2008, the HR-3300 became the first VCR to be registered with the National Museum of Nature and Science, based in Tokyo, Japan. It was noted as one of the 85 most disruptive ideas by Business Week in 2014.\n\nThe first VCR system (strictly, \"video tape recorder\" as there was no cassette) sold directly to home users was 1963's Telcan from the UK, but this was not a commercial success. Sony's CV-2000 was a complete system based on commercial -inch tape on open reels, requiring the user to thread the tape around the helical scan heads. In order to conserve tape, the system recorded every other frame of the television signal, producing 200-line output. Similar models from Ampex and RCA followed that year. The number of video tape recorders continued to increase during the late 1960s, leading to the EIAJ-1 standard for -inch tape on a 7 inch reel. The follow-up EIAJ-2 built the take-up reel into the VCR body.\n\nIn September 1971, Sony introduced the U-matic format, aimed at professional users, which replaced the open reels with a cassette. The next year Philips introduced the Video Cassette Recording format specifically for home users. Over the next five years, a number of companies introduced similar cassette-based home formats, all of which were incompatible. Among the better known examples are Sanyo's V-Cord from 1974, Sony's Betamax from 1975, and Panasonic's VX from 1975.\n\nJVC engineers Yuma Shiraishi and Shizuo Takano led the effort in developing the VHS tape format starting in 1971. The project started off by designing guidelines for VHS, creating a matrix on a blackboard called the VHS Development Matrix. Included in the matrix was a list of objectives in building a home video recording unit. The HR-3300 is a result of these objectives.\n\nSoon after the matrix was produced, the commercial video recording industry in Japan took a financial hit. As a result, JVC cut its budgets and restructured its video division - even going as far as shelving the VHS project. However, despite the lack of funding for the VHS project, Takano and Shiraishi continued to work on the project in secrecy within the video division. By 1973, the two engineers successfully produced a functional prototype of the HR-3300.\n\nThe first HR-3300 was released in 1976. These early units used two large rotary knobs for tuning television signals for recording, one for VHF and another for UHF. Separate antenna inputs and pass-throughs were provided for both frequencies, as well composite video in and out via RCA jacks. An electronic timer with four seven-segment displays was located in the lower left of the front panel allowed the user to automatically record programs, one event up to 24 hours in the future. It also included a mechanical three-digit counter, similar to those on audio cassette recorders.\n\nFor the US and UK release the next year, the system was updated by replacing the mechanical tuning dials with a push-button system with eight pre-selected channels. A panel in the top flipped up to access small mechanical tuning dials for each of the eight channels. Push-button tuning was relatively rare at this time. The UK model was also released under the Ferguson brand name.\n\nIn December 1974, Sony attempted to standardize their Betamax format by inviting Matsushita (Panasonic) and JVC to license the system. Apparently to their surprise, both companies refused. At the time, Matsushita not only sold through its own Panasonic brand, but was the majority shareholder in JVC as well. Through 1976 Sony was unrivalled in the VCR market, selling 30,000 units in the US alone.\n\nThe HR-3300 was introduced late in 1976 with one crucial feature, the ability to hold two hours of video on a single cassette. This made the format able to record an entire movie. JVC licensed the VHS format as an open standard, and in January 1977 there were VHS products from four other Japanese companies on the market.\n\nIn February Sony once again started to look for licensors for the Betamax format, and joined forces with US-based Zenith Electronics. Matsushita then started looking for US partners as well, and formed an alliance with RCA. RCA was interested, but stated that the format should be extended to allow recordings of five to six hours. Victor refused to compromise on picture quality by slowing down the tape speed, but Matsushita produced a prototype of such a system, and RCA announced they were going with VHS in March 1977. Simply re-badging units made by Matsushita in Japan, by 1978, RCA held 36% of the VCR market, and VHS was on its way to becoming a \"de facto\" standard.\n\nBeing the very first VHS-based VCR, the HR-3300 is the result of the \"VHS Development Matrix\" in terms of ease of servicing. Almost every component of this VCR can be purchased at any electronic surplus store. \n"}
{"id": "6309533", "url": "https://en.wikipedia.org/wiki?curid=6309533", "title": "Kraft Foods Banbury", "text": "Kraft Foods Banbury\n\nKraft Foods in the Ruscote ward of Banbury, Oxfordshire, England is a large food and coffee producing factory in the north of the town.\n\nBuilt in 1964, it was partly due to the London overspill. Kraft Foods Banbury is the Kraft Foods centre of manufacturing with the Kraft UK headquarters located at Uxbridge. The factory is still sometimes known as General Foods after the American company which originally owned the building, before 'GF' as it is commonly known was taken over by Kraft.\n\nDuring October 2006, a block of Kraft Foods that was being prepared for demolition caught on fire and remained on fire for most of the day.\n\nThere was a non-lethal fire at the coffee plant on Tuesday 7 December 2010.\n\nIn Spring 2010, a truckload of Kenco Coffee was stolen by a driver who conned his way into the plant.\n\nWith the split of Kraft General Foods into Mondelez International and Kraft Foods in October 2012, this factory site became part of new company Mondelez International\n\n"}
{"id": "907754", "url": "https://en.wikipedia.org/wiki?curid=907754", "title": "List of EDA companies", "text": "List of EDA companies\n\nA list of electronic design automation (EDA) companies.\n\n"}
{"id": "24039538", "url": "https://en.wikipedia.org/wiki?curid=24039538", "title": "List of wireless sensor nodes", "text": "List of wireless sensor nodes\n\nA sensor node, also known as a mote (chiefly in North America), is a node in a sensor network that is capable of performing some processing\"\", gathering sensory information and communicating with other connected nodes in the network. A mote is a node but a node is not always a mote.\n\n"}
{"id": "43016848", "url": "https://en.wikipedia.org/wiki?curid=43016848", "title": "Ludwig Prandtl Ring", "text": "Ludwig Prandtl Ring\n\nThe Ludwig Prandtl Ring is the highest award of the Deutsche Gesellschaft für Luft- und Raumfahrt (German Society for Aeronautics and Astronautics), awarded \"for outstanding contribution in the field of aerospace engineering.\" The award is named in honour of Ludwig Prandtl.\n\n"}
{"id": "8120129", "url": "https://en.wikipedia.org/wiki?curid=8120129", "title": "Maynard Webb", "text": "Maynard Webb\n\nMaynard G. Webb Jr. (born 1955) is an American business person and is the author of the \"New York Times\" bestseller \"Rebooting Work: Transform How You Work in the Age of Entrepreneurship\", and the national bestseller \"Dear Founder: Letters of Advice for Anyone who Leads, Manages, or Wants to Start a Business.\" A long-time technology executive and angel investor, a board member of Salesforce, VISA, Everwise and Chairman of the Board of Directors at Yahoo!. Webb founded Webb Investment Network in 2010 and is the former CEO of LiveOps and former COO of eBay.<ref name=\"NYT DealBook 2/12\"></ref>\n\nWebb received his Bachelor's Degree in criminal justice from Florida Atlantic University. After graduation, he took a security guard job at IBM. He later held management and leadership positions at Bay Networks, Quantum Corporation, and Thomas-Conrad Corporation and was Senior Vice President and Chief Information Officer at Gateway, Inc.\n\nFrom 1999 to 2006, Webb held various titles at eBay, including President of Technology and Chief Operating Officer. During his tenure, eBay grew from $140 million in revenue to over $4.5 billion in 2005 as the employee base expanded from 250 to more than 12,000. Webb served as CEO of LiveOps from 2006 to 2011, a cloud-based call center services company. During that time, LiveOps was named one of Forbes’ Ten Hot Start-Ups (2009), expanded into the enterprise market, generated more capital than it had originally raised, and expanded its board with executives from Symantec, Hewlett-Packard, PeopleSoft, and eBay.\n\nWith Carlye Adler, Webb authored a New York Times best-selling book entitled \"Rebooting Work: Transform How You Work in the Age of Entrepreneurship\", which was published in January 2013. The book focuses on how work models developed a century ago are out of sync today, identifies four mindsets about work, and explains how to leverage technology to change how we work. Over the years, Webb has blogged about entrepreneurship and work in the Internet economy. Webb's second book, \"Dear Founder: Letters of Advice for Anyone Who Leads, Manages, or Wants to Start a Business\", with a Foreword by Howard Schultz, former executive chairman and CEO of Starbucks, was published by St. Martin's Press on September 11, 2018. \n\nWebb is a long-time angel investor who has helped fund such companies as Okta, Zuora, Rypple, GOAT, Hipmunk, PagerDuty, Turo, AppLovin, and Diffbot. In 2010, he founded Webb Investment Network (WIN) for early-stage investing in ecommerce, mobile, cloud computing, and enterprise startups. Startups that WIN funds have access to a network of 89 industry experts from companies such as Google, PayPal, Oracle, and Hewlett-Packard. The network was built from Webb’s business connections. WIN is considered to be part of a trend of smaller, early-stage funds that are indirectly challenging the traditional venture capital model.\n\nWebb and his wife Irene founded the Webb Family Foundation in 2004, an organization dedicated to “promoting meritocracy through helping underdogs in society meet their full potential.” Through grants, the foundation has supported disaster relief, youth mentoring, cancer research, education, and other organizations.\n"}
{"id": "18582230", "url": "https://en.wikipedia.org/wiki?curid=18582230", "title": "Methane", "text": "Methane\n\nMethane ( or ) is a chemical compound with the chemical formula (one atom of carbon and four atoms of hydrogen). It is a group-14 hydride and the simplest alkane, and is the main constituent of natural gas. The relative abundance of methane on Earth makes it an attractive fuel, though capturing and storing it poses challenges due to its gaseous state under normal conditions for temperature and pressure.\n\nNatural methane is found both below ground and under the sea floor. When it reaches the surface and the atmosphere, it is known as atmospheric methane. The Earth's atmospheric methane concentration has increased by about 150% since 1750, and it accounts for 20% of the total radiative forcing from all of the long-lived and globally mixed greenhouse gases.\n\nIn November 1776, methane was first scientifically identified by Italian physicist Alessandro Volta in the marshes of Lake Maggiore straddling Italy and Switzerland. Volta was inspired to search for the substance after reading a paper written by Benjamin Franklin about \"flammable air\". Volta collected the gas rising from the marsh, and by 1778 had isolated the pure gas. He also demonstrated that the gas could be ignited with an electric spark.\n\nThe name \"methane\" was coined in 1866 by the German chemist August Wilhelm von Hofmann. The name was derived from methanol.\n\nEtymologically, the word \"\"methane\" is coined from the chemical suffix \"-ane\", which denotes substances belonging to the alkane family; and the word \"methyl\", which is derived from the German \"methyl\" (A.D.1840) or directly from the French \"méthyle\" which is a back-formation from the French \"méthylène\"\" (corresponding to English \"methylene\"), the root of which is coined from the Greek \"methy\" (related to English \"mead\") and \"hyle\" (meaning \"wood\"). The radical is named after this because it was first detected in wood alcohol. The chemical suffix \"\"-ane\" is from the coordinating chemical suffix \"-ine\" which is from Latin feminine suffix \"-ina\"\" which is applied to represent abstracts. The coordination of \"-ane\", \"-ene\", \"-one\", etc. was proposed in 1866 by German chemist August Wilhelm von Hofmann (1818-1892).\n\nMethane is a tetrahedral molecule with four equivalent C–H bonds. Its electronic structure is described by four bonding molecular orbitals (MOs) resulting from the overlap of the valence orbitals on C and H. The lowest energy MO is the result of the overlap of the 2s orbital on carbon with the in-phase combination of the 1s orbitals on the four hydrogen atoms. Above this energy level is a triply degenerate set of MOs that involve overlap of the 2p orbitals on carbon with various linear combinations of the 1s orbitals on hydrogen. The resulting \"three-over-one\" bonding scheme is consistent with photoelectron spectroscopic measurements.\n\nAt room temperature and standard pressure, methane is a colorless, odorless gas. The familiar smell of natural gas as used in homes is achieved by the addition of an odorant, usually blends containing tert-butylthiol, as a safety measure. Methane has a boiling point of −164 °C (−257.8 °F) at a pressure of one atmosphere. As a gas it is flammable over a range of concentrations (5.4–17%) in air at standard pressure.\n\nSolid methane exists in several modifications. Presently nine are known. Cooling methane at normal pressure results in the formation of methane I. This substance crystallizes in the cubic system (space group Fmm). The positions of the hydrogen atoms are not fixed in methane I, i.e. methane molecules may rotate freely. Therefore, it is a plastic crystal.\n\nThe primary chemical reactions of methane are combustion, steam reforming to syngas, and halogenation. In general, methane reactions are difficult to control. Partial oxidation to methanol, for example, is challenging because the reaction typically progresses all the way to carbon dioxide and water even with an insufficient supply of oxygen. The enzyme methane monooxygenase produces methanol from methane, but cannot be used for industrial-scale reactions. Some homogeneously catalyzed systems and heterogeneous systems have been developed, but all have significant drawbacks. These generally operate by generating protected products which are shielded from overoxidation. Examples include the Catalytica system, copper zeolites, and iron zeolites stabilizing the Alpha-Oxygen active site.\n\nLike other hydrocarbons, methane is a very weak acid. Its pKa in DMSO is estimated to be 56. It cannot be deprotonated in solution, but the conjugate base is known in forms such as methyllithium.\n\nA variety of positive ions derived from methane have been observed, mostly as unstable species in low-pressure gas mixtures. These include methenium or methyl cation , methane cation , and methanium or protonated methane . Some of these have been detected in outer space. Methanium can also be produced as diluted solutions from methane with superacids. Cations with higher charge, such as and , have been studied theoretically and conjectured to be stable.\n\nDespite the strength of its C–H bonds, there is intense interest in catalysts that facilitate C–H bond activation in methane (and other lower numbered alkanes).\n\nMethane's heat of combustion is 55.5 MJ/kg. Combustion of methane is a multiple step reaction summarized as follows:\nPeters four-step chemistry is a systematically reduced four-step chemistry which explains the burning of methane.\n\nGiven appropriate conditions, methane reacts with as follows:\n\nwhere X is a halogen: fluorine (F), chlorine (Cl), bromine (Br), or iodine (I). This mechanism for this process is called free radical halogenation. It is initiated when UV light or some other radical initiator produces a halogen atom. A two-step chain reaction ensues in which the halogen atom abstracts a hydrogen atom from a methane molecule, resulting in the formation of a hydrogen halide molecule and a methyl radical (CH•). The methyl radical then reacts with a molecule of the halogen to form a molecule of the halomethane, with a new halogen atom as byproduct. Similar reactions can occur on the halogenated product, leading to replacement of additional hydrogen atoms by halogen atoms with dihalomethane, trihalomethane, and ultimately, tetrahlomethane structures, depending upon reaction conditions and the halogen-to-methane ratio.\n\nMethane is used in industrial chemical processes and may be transported as a refrigerated liquid (liquefied natural gas, or LNG). While leaks from a refrigerated liquid container are initially heavier than air due to the increased density of the cold gas, the gas at ambient temperature is lighter than air. Gas pipelines distribute large amounts of natural gas, of which methane is the principal component.\n\nMethane is used as a fuel for ovens, homes, water heaters, kilns, automobiles, turbines, and other things. Activated carbon is used to store methane. Liquid methane is also used as a rocket fuel when combined with liquid oxygen, as in the BE-4 and Raptor engines.\n\nMethane is important for electricity generation by burning it as a fuel in a gas turbine or steam generator. Compared to other hydrocarbon fuels, methane produces less carbon dioxide for each unit of heat released. At about 891 kJ/mol, methane's heat of combustion is lower than any other hydrocarbon but the ratio of the heat of combustion (891 kJ/mol) to the molecular mass (16.0 g/mol, of which 12.0 g/mol is carbon) shows that methane, being the simplest hydrocarbon, produces more heat per mass unit (55.7 kJ/g) than other complex hydrocarbons. In many cities, methane is piped into homes for domestic heating and cooking. In this context it is usually known as natural gas, which is considered to have an energy content of 39 megajoules per cubic meter, or 1,000 BTU per standard cubic foot.\n\nMethane in the form of compressed natural gas is used as a vehicle fuel and is claimed to be more environmentally friendly than other fossil fuels such as gasoline/petrol and diesel. Research into adsorption methods of methane storage for use as an automotive fuel has been conducted.\n\nLiquefied natural gas (LNG) is natural gas (predominantly methane, CH) that has been converted to liquid form for ease of storage or transport.\n\nLiquefied natural gas occupies about 1/600th the volume of natural gas in the gaseous state at room temperature and atmospheric pressure. It is odorless, colorless, non-toxic and non-corrosive. Hazards include flammability after vaporization into a gaseous state, freezing, and asphyxia.\n\nThe liquefaction process involves removal of certain components, such as dust, acid gases, helium, water, and heavy hydrocarbons, which could cause difficulty downstream. The natural gas is then condensed into a liquid at close to atmospheric pressure (maximum transport pressure set at around ) by cooling it to approximately .\n\nLNG achieves a higher reduction in volume than compressed natural gas (CNG) so that the energy density of LNG is 2.4 times greater than that of CNG or 60% that of diesel fuel. This makes LNG cost efficient to transport over long distances where pipelines do not exist. Specially designed cryogenic sea vessels (LNG carriers) or cryogenic road tankers are used for its transport. Even if pressurized, methane must be cooled below its critical temperature of in order to be liquefied.\n\nLNG, when it is not highly refined for special uses, is principally used for transporting natural gas to markets, where it is regasified and distributed as pipeline natural gas. It can be used in LNG-fueled road vehicles. \nHowever, it remains more common to design vehicles to use compressed natural gas. , the relatively higher cost of LNG production and the need to store LNG in more expensive cryogenic tanks had slowed widespread commercial use.\n\nNatural gas located far from its user base is often released into the atmosphere or flared. Some is subjected to gas to liquids technologies (GTL) to produce liquid fuels, which are more readily transported than methane.\n\nRefined liquid methane is used as a rocket fuel. Methane is reported to offer the advantage over kerosene of depositing less carbon on the internal parts of rocket motors, reducing the difficulty of re-use of boosters.\n\nMethane is abundant in many parts of the Solar system and potentially could be harvested on the surface of another solar-system body (in particular, using methane production from local materials found on Mars or Titan), providing fuel for a return journey.\n\nNatural gas, which is mostly composed of methane, is used to produce hydrogen gas on an industrial scale. Steam Methane Reforming, or SMR, is the most common method of producing commercial bulk hydrogen gas.\nMore than 50 million metric tons are produced annually worldwide (2013), principally from SMR of natural gas. Much of this hydrogen is used in petroleum refineries, in the production of chemicals and in food processing. Very large quantities of hydrogen are used in the industrial synthesis of ammonia. \n\nAt high temperatures (700 – 1100 °C) and in the presence of a metal-based catalyst (nickel), steam reacts with methane to yield carbon monoxide and hydrogen: \n\nAdditional hydrogen is obtained by reacting the CO with water via the water-gas shift reaction.\n\nThe first reaction is strongly endothermic (consumes heat, ΔH= 206 kJ/mol), the second reaction is mildly exothermic (produces heat, ΔH= -41 kJ/mol).\nMethane is also subjected to free-radical chlorination in the production of chloromethanes, although methanol is a more typical precursor.\n\nThere are two main routes for geological methane generation, organic (thermogenic), and inorganic (abiotic, meaning non-living). Thermally generated methane, referred to as thermogenic, originates from deeper sedimentary strata. Thermogenic methane (CH) formation occurs due to the breakup of organic matter, forced by elevated temperatures and pressures. This type of methane is considered to be the primary methane type in sedimentary basins, and from an economical perspective the most important source of natural gas. Thermogenic methane components are generally considered to be relic (from an earlier time). Generally, formation of thermogenic methane (at depth) can occur through organic matter breakup, or organic synthesis. Both ways can involve microorganisms (methanogenesis) but may also occur inorganically. The involved anaerobic and aerobic processes can also consume methane, with and without microorganisms. The more important source of methane at depth (crystalline bedrock) is abiotic. Abiotic means that the methane formation took place involving inorganic compounds, without biological activity, magmatic or created at low temperatures and pressures through water-rock reactions.\n\nNaturally occurring methane is mainly produced by microbial methanogenesis. This multistep process is used by microorganisms as an energy source. The net reaction is\nThe final step in the process is catalyzed by the enzyme Coenzyme-B sulfoethylthiotransferase. Methanogenesis is a form of anaerobic respiration used by organisms that occupy landfill, ruminants (for example cows or cattle), and the guts of termites.\n\nIt is uncertain whether plants are a source of methane emissions.\n\nThere are many technological methane production methods. However, as methane is the main component in natural gas and a major energy source, there is little industrial incentive to prepare it from other sources, although some plants use coal to gas processes (see below). Methane created from biomass in industrial plants via biological route is called biogas. A more synthetic method to produce methane is hydrogenating carbon dioxide through the Sabatier process. Methane is also a side product of the hydrogenation of carbon monoxide in the Fischer–Tropsch process, which is practiced on a large scale to produce longer-chain molecules than methane.\n\nExample of large-scale coal-to-methane gasification is the Great Plains Synfuels plant, started in 1984 in Beulah, North Dakota as a way to develop abundant local resources of low-grade lignite, a resource that is otherwise very hard to transport for its weight, ash content, low calorific value and propensity to spontaneous combustion during storage and transport.\n\nPower to methane is a technology that uses electrical power to produce hydrogen from water by electrolysis and uses the Sabatier reaction to combine hydrogen with carbon dioxide to produce methane. As of 2016, this is mostly under development and not in large-scale use. Theoretically, the process could be used as a buffer for excess and off-peak power generated by highly fluctuating wind generators and solar arrays. However, as currently very large amounts of natural gas are used in power plants (e.g. CCGT) to produce electric energy, the losses in efficiency are not acceptable.\n\nMethane can be produced by protonation of methyl lithium and methylmagnesium iodide. In practice, a requirement for pure methane will be filled with a steel gas bottle from standard suppliers.\n\nMethane was discovered and isolated by Alessandro Volta between 1776 and 1778 when studying marsh gas from Lake Maggiore. It is the major component of natural gas, about 87% by volume. The major source of methane is extraction from geological deposits known as natural gas fields, with coal seam gas extraction becoming a major source (see Coal bed methane extraction, a method for extracting methane from a coal deposit, while enhanced coal bed methane recovery is a method of recovering methane from non-mineable coal seams). It is associated with other hydrocarbon fuels, and sometimes accompanied by helium and nitrogen. Methane is produced at shallow levels (low pressure) by anaerobic decay of organic matter and reworked methane from deep under the Earth's surface. In general, the sediments that generate natural gas are buried deeper and at higher temperatures than those that contain oil.\n\nMethane is generally transported in bulk by pipeline in its natural gas form, or LNG carriers in its liquefied form; few countries transport it by truck.\n\nApart from gas fields, an alternative method of obtaining methane is via biogas generated by the fermentation of organic matter including manure, wastewater sludge, municipal solid waste (including landfills), or any other biodegradable feedstock, under anaerobic conditions. Rice fields also generate large amounts of methane during plant growth. Methane hydrates/clathrates (ice-like combinations of methane and water on the sea floor, found in vast quantities) are a potential future source of methane. Cattle belch methane accounts for 16% of the world's annual methane emissions to the atmosphere. One study reported that the livestock sector in general (primarily cattle, chickens, and pigs) produces 37% of all human-induced methane. Early research has found a number of medical treatments and dietary adjustments that help slightly limit the production of methane in ruminants. A 2009 study found that at a conservative estimate, at least 51% of global greenhouse gas emissions were attributable to the life cycle and supply chain of livestock products, meaning all meat, dairy, and by-products, and their transportation. More recently, a 2013 study estimated that livestock accounted for 44 percent of human-induced methane and 14.5 percent of human-induced greenhouse gas emissions. Many efforts are underway to reduce livestock methane production and trap the gas to use as energy. The state of California has been particularly active in this area.\n\nPaleoclimatology research published in \"Current Biology\" suggests that flatulence from dinosaurs may have warmed the Earth.\n\nIn 2010, methane levels in the Arctic were measured at 1850 nmol/mol. This level is over twice as high as at any time in the last 400,000 years. Historic methane concentrations in the world's atmosphere have ranged between 300 and 400 nmol/mol during glacial periods commonly known as ice ages, and between 600 and 700 nmol/mol during the warm interglacial periods. The Earth's oceans are a potential important source of Arctic methane.\n\nMethane is an important greenhouse gas with a global warming potential of 34 compared to CO over a 100-year period, and 72 over a 20-year period.\n\nThe Earth's atmospheric methane concentration has increased by about 150% since 1750, and it accounts for 20% of the total radiative forcing from all of the long-lived and globally mixed greenhouse gases (these gases don't include water vapor which is by far the largest component of the greenhouse effect).\n\nMethane is essentially insoluble in water, but significant deposits of methane clathrate have been found under sediments on the ocean floors of Earth at large depths. Estimates consider up to 15,000 gigatonnes of carbon may be stored in the form of clathrates (hydrates) in the ocean floor, not accounting for abiotic methane, a relatively newly discovered source of methane, formed below the ocean floor, in the earth crust. It has been suggested, that today's methane emission regime from the ocean floor, is potentially similar to that during the PETM.\n\nArctic methane release from permafrost and methane clathrates is an expected consequence and further cause of global warming.\n\nThere is a group of bacteria that drive methane oxidation with nitrite as the oxidant, the anaerobic oxidation of methane.\n\nMethane is nontoxic, yet it is extremely flammable and may form explosive mixtures with air. Methane is also an asphyxiant if the oxygen concentration is reduced to below about 16% by displacement, as most people can tolerate a reduction from 21% to 16% without ill effects. The concentration of methane at which asphyxiation risk becomes significant is much higher than the 5–15% concentration in a flammable or explosive mixture. Methane off-gas can penetrate the interiors of buildings near landfills and expose occupants to significant levels of methane. Some buildings have specially engineered recovery systems below their basements to actively capture this gas and vent it away from the building.\n\nMethane gas explosions are responsible for many deadly mining disasters. A methane gas explosion was the cause of the Upper Big Branch coal mine disaster in West Virginia on April 5, 2010, killing 29.\n\nMethane has been detected or is believed to exist on all planets of the solar system and most of the larger moons. With the possible exception of Mars, it is believed to have come from abiotic processes.\nOn June 7th of 2018, NASA disclosed in a press conference that its \"Curiosity\" rover had documented seasonal fluctuations of atmospheric methane levels on Mars. These fluctuations peaked at the end of the Martian summer at 0.6 parts per billion.\n\nMethane has been proposed as a possible rocket propellant on future Mars missions due in part to the possibility of synthesizing it on the planet by in situ resource utilization. An adaptation of the Sabatier methanation reaction may be used with a mixed catalyst bed and a reverse water-gas shift in a single reactor to produce methane from the raw materials available on Mars, utilizing water from the Martian subsoil and carbon dioxide in the Martian atmosphere.\n\nMethane could be produced by a non-biological process called ’'serpentinization\" involving water, carbon dioxide, and the mineral olivine, which is known to be common on Mars.\n\n"}
{"id": "567726", "url": "https://en.wikipedia.org/wiki?curid=567726", "title": "Mower", "text": "Mower\n\nA mower is a person or machine that cuts (mows) grass or other plants that grow on the ground. Usually mowing is distinguished from reaping, which uses similar implements, but is the traditional term for harvesting grain crops, e.g. with reapers and combines.\n\nA smaller mower used for lawns and sports grounds (playing fields) is called a \"lawn mower\" or \"grounds mower\", which is often self-powered, or may also be small enough to be pushed by the operator. Grounds mowers have reel or rotary cutters. \nLarger mowers or \"mower-conditioners\" are mainly used to cut grass (or other crops) for hay or silage and often place the cut material into rows, which are referred to as \"windrows\". \"Swathers\" (or \"windrowers\") are also used to cut grass (and grain crops). Prior to the invention and adoption of mechanized mowers, (and today in places where use a mower is impractical or uneconomical), grass and grain crops were cut by hand using scythes or sickles.\n\nLarger mowers are usually \"ganged\" (equipped with a number or gang of similar cutting units), so they can adapt individually to ground contours. They may be powered and drawn by a tractor or draft animals. The cutting units can be mounted underneath the tractor between the front and rear wheels, mounted on the back with a three-point hitch or pulled behind the tractor as a trailer. There are also dedicated self-propelled cutting machines, which often have the mower units mounted at the front and sides for easy visibility by the driver. \"Boom\" or \"side-arm\" mowers are mounted on long hydraulic arms, similar to a backhoe arm, which allows the tractor to mow steep banks or around objects while remaining on a safer surface.\n\nThe cutting mechanism in a mower may be one of several different designs:\n\n\"Sickle mowers\", also called \"reciprocating mowers\", \"bar mowers\", \"sickle-bar mowers\", or \"finger-bar mowers\", have a long (typically six to seven and a half feet) bar on which are mounted fingers with stationary guardplates. In a channel on the bar there is a reciprocating sickle with very sharp sickle sections (triangular blades). The sickle bar is driven back and forth along the channel. The grass, or other plant matter, is cut between the sharp edges of the sickle sections and the finger-plates (this action can be likened to an electric hair clipper).\n\nThe bar rides on the ground, supported on a skid at the inner end, and it can be tilted to adjust the height of the cut. A spring-loaded board at the outer end of the bar guides the cut hay away from the uncut hay. The so-formed channel, between cut and uncut material, allows the mower skid to ride in the channel and cut only uncut grass cleanly on the next swath. These were the first successful horse-drawn mowers on farms and the general principles still guide the design of modern mowers.\n\nRotary mowers, also called \"drum mowers\", have a rapidly rotating bar, or disks mounted on a bar, with sharpened edges that cut the crop. When these mowers are tractor-mounted they are easily capable of mowing grass at up to 20 miles per hour (32 km/h) in good conditions. Some models are designed to be mounted in double and triple sets on a tractor, one in the front and one at each side, thus able to cut up to 20 foot (6 metre) swaths.\n\nIn rough cutting conditions, the blades attached to the disks are swivelled to absorb blows from obstructions. Mostly these are rear-mounted units and in some countries are called \"scrub cutters\". Self-powered mowers of this type are used for rougher grass in gardening and other land maintenance.\n\n\"Reel mowers\", also called \"cylinder mowers\" (familiar as the hand-pushed or self-powered cylinder lawn mower), have a horizontally rotating cylindrical reel composed of helical blades, each of which in turn runs past a horizontal cutter-bar, producing a continuous scissor action. The bar is held at an adjustable level just above the ground and the reel runs at a speed dependent on the forward movement speed of the machine, driven by wheels running on the ground (or in self-powered applications by a motor). The cut grass may be gathered in a collection bin.\n\nThis type of mower is used to produce consistently short and even grass on bowling greens, lawns, parks and sports grounds. When pulled by a tractor (or formerly by a horse), these mowers are often ganged into sets of three, five or more, to form a \"gang mower\". A well-designed reel mower can cut quite tangled and thick tall grass, but this type works best on fairly short, upright vegetation, as taller vegetation tends to be rolled flat rather than cut.\n\nHome reel mowers have certain benefits over motor-powered mowers as they are quieter and not dependent on any extra form of power besides the person doing the mowing. This is useful not only to lessen dependence on other types of power which may have availability issues, but also lessens the impact on the environment.\n\nFlail mowers have a number of small blades on the end of chains attached to a horizontal axis. The cutting is carried out by the ax-like heads striking the grass at speed. These types are used on rough ground, where the blades may frequently be fouled by other objects, or on tougher vegetation than grass, such as brush (scrub). Due to the length of the chains and the higher weight of the blades, they are better at cutting thick brush than other mowers, because of the relatively high inertia of the blades. In some types the cut material may be gathered in a collection bin. As a boom mower (see above), a flail mower may be used in an upright position for trimming the sides of hedges, when it is often called a \"hedge-cutter\".\n\nDrum mowers have their horizontally-mounted cutting blades attached to the outside of a relatively large diameter disc fixed to the bottom of a smaller diameter drum and are principally designed for cutting lighter crops, such as grass, very quickly. The drive mechanism is top-mounted and often in the form of fully enclosed, bevel geared drive shafts.\n\n"}
{"id": "5841553", "url": "https://en.wikipedia.org/wiki?curid=5841553", "title": "Mum (deodorant)", "text": "Mum (deodorant)\n\nMum was the first brand of commercial deodorant. Containing a zinc compound as its active ingredient, it was developed and patented by Edna Murphey in Philadelphia in 1888. It was named for the term \"mum\" meaning \"to keep silent\" as in the popular phrase \"Mum's the word\" Mum was originally sold as a cream in a jar and applied with the fingertips. The small company was bought by Bristol-Myers in 1931.\n\nIn the late 1940s, an employee (Helen Diserens) developed an applicator based on the newly invented ball-point pen. In 1952, the company began marketing the product under the name Ban Roll-On. In 1958, the product was launched in the United Kingdom and the Commonwealth of Nations as Mum Rollette.\n\nThe product was briefly withdrawn from the market in the United States, but is again widely available.\nIt is popular in Australia, Germany, Mexico, Peru, Singapore, South Africa, the United Kingdom and Venezuela. In the Philippines, it was sold in the 1970s and 1980s but has since been withdrawn.\n\nBan is now owned by Kao Corporation.\n\n"}
{"id": "5678122", "url": "https://en.wikipedia.org/wiki?curid=5678122", "title": "NERC Data Centres", "text": "NERC Data Centres\n\nThe Natural Environment Research Council (NERC) has seven subject-based environmental data centres (EDCs) to store and distribute data from its own research programmes and data that are of general use to the environmental research community. These data centres are sometimes called the NERC designated Data Centres.\n\nThe NERC Environmental Data Centres and their areas of responsibility are as follows: \n\nThe data centres hold data from environmental scientists working in the UK and around the world.\nThey provide access to a comprehensive data and information resource about our environment, through the NERC Data Catalog Service. CEDA hosts the NERC Data Catalog Service , data may also be cataloged from certain NERC Data Centres and also at data.gov.uk.\nAccess to these data is freely available to students, researchers and stakeholders, as well as business users and policy makers, to help them understand the environment in which we live.\n\nEach data centre works to build user confidence, using common data formats and noting sources, contexts, and degrees of accuracy. They combine expertise in the scientific collection of information, state-of-the-art data management and preservation techniques, making them an important national asset.\n\nThe NERC Data Policy is commitment to support the long-term management of environmental data and also outlines the roles and responsibilities of all those involved in collecting and managing environmental data. The NERC Data Centres provide support and guidance in data management to those funded by NERC, are responsible for the long-term management of data and provide access to NERC's holdings of environmental data.\n\nThe data policy is consistent with legal frameworks, such as the Environmental Information Regulations 2004, the INSPIRE Regulations 2009 and contractual arrangements with other bodies where, for example, NERC holds data on their behalf but does not own the intellectual property rights.\n\nTo reflect NERC's continuing commitment to openness and transparency in the research process, and in support of the government's developing agenda on open access to public data, the NERC Data Policy has been substantially revised, and this new version of NERC's Data Policy came into force in January 2011.\n\nThe NERC charging regime recognises two classes of data:\n\n\nThe licence charges that NERC can levy for the supply of data and information are governed by HM Treasury guidance and Government regulations.\nDuring 2010 NERC aimed to move to a position where it will supply its data for free for all teaching and research activities, apart from large or complex requests where they may make a nominal handling charge.\n\nThe NERC Science Information Strategy (SIS) was created to provide the framework for NERC to work more closely and effectively with its scientific communities, both internal and external, in delivering data and information management services to support its five-year science strategy, the Next Generation Science for Planet Earth 2007-2012.\n\nThe Science Information Strategy is being implemented in three phases, beginning April 2010. Each of these phases will be made up of a number of smaller projects. Project/Phase 1 will concentrate on determining user requirements, documenting existing holdings, agreeing an information architecture, updating the NERC Data Policy and improving the data discovery tools.\n\nThe benefits of the SIS implementation for science include the increased competence of data centre staff to inform the science community, policy and decision makers nationally and internationally on best practice, standards development and the establishment of new projects and initiatives (e.g. EU Framework projects):enhancing NERC's national and international reputation and increased capability to conduct multi-disciplinary, integrated science.\n\n"}
{"id": "48214291", "url": "https://en.wikipedia.org/wiki?curid=48214291", "title": "Polyglot persistence", "text": "Polyglot persistence\n\nPolyglot persistence is the concept of using different data storage technologies to handle different data storage needs within a given software application. Polyglot programming, a term coined by Neal Ford in 2006, expresses the idea that computer applications should be written in a mix of different programming languages, in order to take advantage of the fact that different languages are suitable for tackling different problems. Complex applications combine different types of problems, so picking the right language for each job may be more productive than trying to solve all aspects of the problem using a single language. This same concept can be applied to databases, that an application can communicate with different databases, using each for what it is best at to achieve an end goal, hence the term \"polyglot persistence\".\n\nThere are numerous databases available to solve different problems. Using a single database to satisfy all of a program's requirements can result in a non-performant, \"jack of all trades, master of none\" solution. Relational databases, for example, are good at enforcing relationships that exist between various data tables. To discover a relationship or to find data from different tables that belong to the same object, an SQL join operation can be used. This might work when the data is smaller in size, but becomes problematic when the data involved grows larger. A graph database might solve the problem of relationships in case of Big Data, but it might not solve the problem of database transactions, which are provided by RDBM systems. Instead, a NoSQL document database might be used to store unstructured data for that particular part of the problem. Thus different problems are solved by different database systems, all within the same application.\n\n"}
{"id": "250851", "url": "https://en.wikipedia.org/wiki?curid=250851", "title": "Portable computer", "text": "Portable computer\n\nA portable computer was a computer designed to be easily moved from one place to another and included a display and keyboard. \nThe first commercially sold portable was the 50 pound IBM 5100, introduced 1975. The next major portables were Osborne's 24 pound CP/M-based Osborne 1 (1981) and Compaq's 28 pound 100% IBM PC compatible Compaq Portable (1983). These \"luggable\"s lacked the next technological advance, not requiring an external power source; that feature was introduced by the laptop. Laptops were followed by lighter models, so that in the 2000s mobile devices and by 2007 smartphones made the term almost meaningless. The 2010s introduced wearable computers such as smartwatches. \n\nPortable computers, by their nature, were generally microcomputers. Larger portable computers were commonly known as 'Lunchbox' or 'Luggable' computers. They were also called a 'Portable Workstation' or 'Portable PC'. In Japan they were often called from \"bento\".\n\nPortable computers, more narrowly defined, were distinct from desktop replacement computers in that they usually were constructed from full-specification desktop components, and did not incorporate features associated with laptops or mobile devices. A portable computer in this usage, versus a laptop or other mobile computing device, had a standard motherboard or backplane providing plug-in slots for add-in cards. This allowed mission specific cards such as test, A/D, or communication protocol (IEEE-488, 1553) to be installed. Portable computers also provided for more disk storage by using standard disk drives and providing for multiple drives.\n\nIn 1973, the IBM Palo Alto Scientific Center developed a portable computer prototype called SCAMP (Special Computer APL Machine Portable) based on the IBM PALM processor with a Philips compact cassette drive, small CRT and full function keyboard. SCAMP emulated an IBM 1130 minicomputer in order to run APL\\1130. In 1973, APL was generally available only on mainframe computers, and most desktop sized microcomputers such as the Wang 2200 or HP 9800 offered only BASIC. Because SCAMP was the first to emulate APL\\1130 performance on a portable, single user computer, \"PC Magazine\" in 1983 designated SCAMP a \"revolutionary concept\" and \"the world's first personal computer\".\n\nSuccessful demonstrations of the 1973 SCAMP prototype led to the first commercial IBM 5100 portable microcomputer launched in 1975. The product incorporated an IBM PALM processor, 5 inch CRT, full function keyboard and the ability to be programmed in both APL and BASIC for engineers, analysts, statisticians and other business problem-solvers. IBM referred to its PALM processor as a microprocessor, though they used that term to mean a processor that executes microcode to implement a higher-level instruction set, rather than its conventional definition of a complete processor on a single silicon integrated circuit. In the late 1960s, such a machine would have been nearly as large as two desks and would have weighed about half a ton. In comparison, the IBM 5100 weighed about 53 pounds (24 kg and very portable for that time).\n\nThe MIT Suitcase Computer, constructed in 1975, was the first known microprocessor-based portable computer. It was based on the Motorola 6800. Constructed in a Samsonite suitcase approximately 20\"x30\"x8\" and weighing approximately 20 lbs., it had 4K of SRAM, a serial port to accept downloaded software and connect to a modem, a keyboard and a 40-column thermal printer taken from a cash register. Built by student David Emberson in the MIT Digital Systems Laboratory as a thesis project, it never entered production. It is currently in the collection of Dr. Hoo-Min D. Toong.\n\nXerox NoteTaker, developed in 1976 at Xerox PARC, was a precursor to later portable computers from Osborne Computer Corporation and Compaq, though it remained a prototype and did not enter production. \n\nAn early portable computer was manufactured in 1979 by GM Research, a small company in Santa Monica, California. The machine which was designed and patented by James Murez. It was called the Micro Star and later changed the name to The Small One. Although Xerox claims to have designed the first such system, the machine by Murez predated anything on the market or that had been documented in any publication at the time hence the patent was issued. As early as 1979, the U.S. Government was contracting to purchase these machines. Other major customers included Sandia Labs, General Dynamics, BBN (featured on the cover of their annual report in 1980 as the C.A.T. system) and several dozen private individuals and companies around the world. In 1979, Adam Osborne viewed the machine along with several hundred other visitors at the first computer show that was sponsored by the IEEE Westec in Los Angeles. Later that year the machine was also shown at the first COMDEX show.\n\nThe portable micro computer the \"Portal\" of the French company R2E Micral CCMC officially appeared in September 1980 at the Sicob show in Paris. The Portal was a portable microcomputer designed and marketed by the studies and developments department of the french firm R2E Micral in 1980 at the request of the company CCMC specializing in payroll and accounting. The Portal was based on an intel 8085 processor, 8-bit, clocked at 2 MHz. It was equipped with a central 64 KB RAM, a keyboard with 58 alpha numeric keys and 11 numeric keys (separate blocks), a 32-character screen, a floppy disk: capacity = 140 000 characters, of a thermal printer: speed = 28 characters / sec, an asynchronous channel, a synchronous channel, a 220 V power supply. Designed for an operating temperature of 15 °C to 35 °C, it weighed 12 kg and its dimensions were 45 × 45 × 15 cm. It provided total mobility. Its operating system was Prolog. A few hundred were sold between 1980 and 1983.\n\nThe first mass-produced microprocessor-based portable computer released in 1981 was the Osborne 1, developed by Osborne, which owed much to the NoteTaker's design.\n\nAnother early portable computer released in 1982 was the Kaypro. \n\nIn January 1983, the first IBM PC compatible portable computer (and the first 100% IBM PC compatible, or \"clone,\" of any kind) was the Compaq Portable. \n\nThe first full-color portable computer was the Commodore SX-64 in January 1984. \n\nApple Inc. introduced and released Macintosh Portable in 1989.\n\nSmaller portable computers are also known as mobile computers. They are referred to by their more specific terms:\n\n\nPortable computers have been increasing in popularity over the past decade, as they do not restrict the user in terms of mobility as a desktop computer would. Wireless access to the Internet, extended battery life and more comfortable ergonomics have been factors driving this increase in popularity. All-in-One PCs such as the iMac can also be considered portable computers and often have handles built into the case. Home theater computers (HTPC) tend to be much smaller than desktop computers, but still require a power source.\n\n"}
{"id": "31668309", "url": "https://en.wikipedia.org/wiki?curid=31668309", "title": "Remote and virtual tower", "text": "Remote and virtual tower\n\nRemote and virtual tower (RVT) is a new concept where the air traffic service (ATS) at an airport is performed somewhere else than in the local control tower.\n\nThe first remote tower implementation providing aerodrome ATS was approved and introduced into operations in Sweden in April 2015, with further\nimplementations in other EASA Member States well underway. (EASA, 2017)\n\nThe air traffic control officer (ATCO) or aerodrome flight information services officer (AFISO) will be re-located to a remote tower centre (RTC) from where they will provide the ATS.\n\nThe RVT concept is aiming at providing:\n\n\nThe full range of air traffic services defined in ICAO Documents 4444, 9426 and EUROCONTROL’s Manual for AFIS will still be provided remotely by an ATCO or AFISO. The airspace users should be provided with the appropriate level of services as if the ATS were provided locally at the airport.\n\nThe SESAR Joint Undertaking projects are looking at RVT concepts, based on either one person controlling one airport, or one person controlling multiple airports.\n\nThe basic concept, formerly known as virtual towers, was introduced by Deutsches Zentrum für Luft- und Raumfahrt e.V. (DLR) in 2002 and describes a remote ATC control room with video-sensor based surveillance instead of 'out-of-the-window' view from a real tower. The initial trials of remote ATS, for low and medium-density airports, have been based on optical sensors (cameras), providing the ATCOs at the RTC with a high-quality real-time image of the runway, the airport ramp (APRON) and the very nearby airspace. These real-time images are displayed at large monitors providing up to 360-degree view.\n\nBeside the live video feed from the airport, the ATCOs have available the same air traffic management computer systems as they would have in a local control tower building, being voice communication systems, meteorological systems, flight plan systems, and Surveillance display systems. The level of equipage might depend on whether it is a controlled TWR service, or a Flight Information Service being provided at the specific airport.\nDepending on the complexity of the airport, the traffic densities, and weather conditions, it might be preferable to complement the optical images with an advanced surface movement guidance and control system (A-SMGCS) with signal inputs from surface movement radar (SMR) and/or Local Area Multilateration (LAM).\n\nThe RVT concept is under development, besides of other former research & development inititaives (e.g. by DLR, DFS, LFV, Searidge Technologies, SAAB, FREQUENTIS, Indra Sistemas or the FP6 EU project ART, etc.), as part of the SESAR Joint Undertaking (SJU), where work package 6 develops the operational concepts, while Work Package 12 develops the corresponding technology to enable the RVT functionality.\n\nThere will be carried out live SESAR validation trials at a few selected airports in Germany, Spain (ENAIRE), Norway (Avinor) and Sweden (Luftfartsverket) as part of SESAR Joint Undertaking Projects 06.08.04 and 06.09.03 during the years 2012–2015.\n\nAirservices Australia intends to evaluate RVT technology from Saab Group at Alice Springs airport in Central Australia from late 2012, with the control centre placed in Adelaide.\n\nIn March 2009, Saab Group and Luftfartsverket (LFV) carried out a live shadow mode demonstration of their existing remote tower concept.\nThis demonstration took place at a remote tower centre facility established at Malmö air traffic control centre (ATCC), controlling a flight in and out of Angelholm airport (ICAO:ESTA) in southern Sweden. As a contingency mechanism during this trial, the local control tower at Angelholm was staffed by ATCOs.\n\nIn 2010 DLR carried out the first human in the loop remote tower center simulation, whereas a remote controller operated traffic at two different low frequented airports simoultanouesly. Despite several biases the controllers' situation awareness was over-average and their workload remained in average range and operational feasibility could be shown the first time.\n\nDLR Institute of Flight Guidance, Saab Group, Luftfartsverket, Indra and DFS have been the major driving forces behind the Remote Tower development, and are all represented in the SESAR Joint Undertaking projects, SAAB through North European ATM Industry Group (NATMIG) and LFV NORACON.\n\nDuring ATC Global in Amsterdam in March 2011, SESAR Joint Undertaking had a ceremony where Project 6.9.3 'Remote & Virtual Tower' was given the award for 'most advanced for deployment'. The price was presented by Executive Director of SESAR Joint Undertaking Mr Patrick Ky, and received by Project 6.9.3 Project Manager Mr Göran Lindqvist, NORACON.\n\nAs of 21 April 2015 12:00 am, the airport of Örnsköldsvik/Gideå (OER/ESNO) is run using remote ATC services from Sundsvall/Midlanda (SDL/ESNN). This is reported to be the first production deployment of RVT in the world. The system was tested at Leesburg Executive Airport in summer 2015.\n\nOn 1 October 2015 the FAA announced Fort Collins-Loveland Municipal Airport as the first official FAA approved Virtual Air Traffic Control Tower test site in the United States. The equipment necessary for the testing is expected to be installed at the Fort Collins-Loveland Municipal Airport by spring of 2016, with initial testing and assessments of the new virtual technology commencing shortly thereafter. \n\nThe main benefits of RVT is expected to be on cost efficiency.\n\nThe cost savings originate from the following factors:\n\n\nThere is also a great potential to better and more cost efficiently serve flights which either are scheduled outside the core opening hours of the airport, or by being able to serve non scheduled traffic (ambulance flights and search-and-rescue helicopters) with an air traffic service during night time when a smaller airports would normally be closed.\n\nIn 2014 the European Organisation for Civil Aviation Equipment (EUROCAE) founded the Working Group (WG) 100 “Remote and Virtual Tower”. The WG-100 was launched under the Chair of the German Aerospace Center - DLR and EUROCONTROL in the Secretary role. WG-100 further consists of active contributors (air navigation service manufatcurers & service providers) from more than 30 companies worldwide and acts in close coordination with EASA, ICAO, SESAR, and the most recent SESAR2020 project \"PJ05 Remote Tower\". The group was tasked as a first step to develop standards for remote towers optical systems. In September 2016 the ED-240 MINIMUM AVIATION SYSTEM PERFORMANCE SPECIFICATION (MASPS) FOR REMOTE TOWER OPTICAL SYSTEMS document was published. These MASPS are applicable to all optical sensor configurations (visible, as well as infrared spectrum) to be used for the implementation of the remote provision of ATS to an aerodrome, encompassing the whole chain from sensor to display. This standard should help vendors and customers to quantify an optimal operational system performance and to verify it in a standardised way. For the time being the WG-100 work focuses on an extension of the current MASPS (revision A) to include ‘visual tracking’ and automatic Pan-Tilt-Zoom (PTZ) camera object following technologies. ‘Visual tracking’ is understood as the augmentation of the display of objects on the visual presentation by using information obtained only by image processing of the video from the optical sensors for the purpose of increasing the operator’s situation awareness. The PTZ Object Following function attaches the PTZ camera to a moving target and persistently follows and displays it automatically. The MASPS ED-240A are expected to be published by 2018.\n\nWhile some may argue, there are strong similarities between the concept of RVT, and the criteria for disruptive innovations as defined by Clayton Christensen and Michael Raynor in the book \"Innovators Solution\". A closer examination of the technology and its practical use would indicate that it is more appropriately categorized as a sustainable innovation, marking an evolution in aerodrome control by supplanting visual observation with a surveillance system.\n\n\n"}
{"id": "8358059", "url": "https://en.wikipedia.org/wiki?curid=8358059", "title": "René Soetens", "text": "René Soetens\n\nRené John Soetens (born 7 September 1948) was a member of the House of Commons of Canada from 1988 to 1993. His background was in business and sales.\n\nRene was elected to Town of Ajax Council in 1980 and re-elected 1982 and 1985.\nHe was elected in the 1988 federal election at the Ontario electoral district for the Progressive Conservative party. He served in the 34th Canadian Parliament but lost to Dan McTeague of the Liberal Party in the 1993 federal election.\n\nSoetens also made an unsuccessful bid to return to national Parliament in the 2004 federal election at the Ajax—Pickering electoral district.\nHe is president and owner of Con-Test, a national certification company specializing in the testing of controlled environments found in research facilities, hospitals, pharmaceutical drug manufacturing and associated cleanrooms.\n"}
{"id": "13653181", "url": "https://en.wikipedia.org/wiki?curid=13653181", "title": "Rewrite (programming)", "text": "Rewrite (programming)\n\nA rewrite in computer programming is the act or result of re-implementing a large portion of existing functionality without re-use of its source code or writing inscription. When the rewrite is not using existing code at all, it is common to speak of a rewrite from scratch.\n\nA piece of software is typically rewritten when one or more of the following apply:\n\nSeveral software engineers, such as Joel Spolsky have warned against total rewrites, especially under schedule constraints or competitive pressures. While developers may initially welcome the chance to correct historical design mistakes, a rewrite also discards those parts of the design that work as required. A rewrite commits the development team to deliver not just new features, but all those that exist in the previous code, while potentially introducing new bugs or regressions of previously fixed bugs. A rewrite also interferes with the tracking of unfixed bugs in the old version.\n\nThe incremental rewrite is an alternative approach, in which developers gradually replace the existing code with calls into a new implementation, expanding that implementation until it fully replaces the old one. This approach avoids a broad loss of functionality during the rewrite. Cleanroom software engineering is another approach, which requires the team to work from an exhaustive written specification of the software's functionality, without access to its code.\n\nNetscape's project to improve HTML layout in Navigator 4 has been cited as an example of a failed rewrite. The new layout engine (Gecko) had developed independently from Navigator and did not integrate readily with Navigator's code; hence Navigator itself was rewritten around the new engine, breaking many existing features and delaying release by several months. Meanwhile, Microsoft focused on incremental improvements to Internet Explorer and did not face the same obstacles. Ironically, Navigator itself was a successful cleanroom rewrite of NCSA Mosaic overseen by that program's developers. See Browser wars.\n\n\nSome projects mentioning major rewrites in their history:\n\n"}
{"id": "12800416", "url": "https://en.wikipedia.org/wiki?curid=12800416", "title": "Standby generator", "text": "Standby generator\n\nA standby generator is a back-up electrical system that operates automatically. Within seconds of a utility outage an automatic transfer switch senses the power loss, commands the generator to start and then transfers the electrical load to the generator. The standby generator begins supplying power to the circuits. After utility power returns, the automatic transfer switch transfers the electrical load back to the utility and signals the standby generator to shut off. It then returns to standby mode where it awaits the next outage. To ensure a proper response to an outage, a standby generator runs weekly self-tests. Most units run on diesel, natural gas, or liquid propane gas.\n\nAutomatic standby generator systems may be required by building codes for critical safety systems such as elevators in high-rise buildings, fire protection systems, standby lighting, or medical and life support equipment. Residential standby generators are increasingly common, providing backup electrical power to HVAC systems, security systems, and household appliances such as refrigerators, stoves, and water heaters.\n\n\n"}
{"id": "38949621", "url": "https://en.wikipedia.org/wiki?curid=38949621", "title": "Storage water heater", "text": "Storage water heater\n\nA storage water heater, or a hot water system (HWS), is a domestic water heating appliance that uses a hot water storage tank to maximize heating capacity and provide instantaneous delivery of hot water. Conventional storage water heaters use a variety of fuels, including natural gas, propane, fuel oil, and electricity. Less conventional water heating technologies, such as heat pump water heaters and solar water heaters, can also be categorized as storage water heaters.\n\nSolar heat is clean and renewable. This is the most modern system. Increasingly, solar powered water heaters are being used. Their solar collectors are installed outside dwellings, typically on the roof or walls or nearby, and the potable hot water storage tank is typically a pre-existing or new conventional water heater, or a water heater specifically designed for solar thermal.\n\nThe most basic solar thermal models are the direct-gain type, in which the potable water is directly sent into the collector. Many such systems are said to use \"integrated collector storage\" (ICS), as direct-gain systems typically have storage integrated within the collector. Heating water directly is inherently more efficient than heating it indirectly via heat exchangers, but such systems offer very limited freeze protection (if any), can easily heat water to temperatures unsafe for domestic use, and ICS systems suffer from severe heat loss on cold nights and cold, cloudy days.\n\nBy contrast, \"indirect\" or \"closed-loop\" systems do not allow potable water through the panels, but rather pump a heat transfer fluid (either water or a water/antifreeze mix) through the panels. After collecting heat in the panels, the heat transfer fluid flows through a heat exchanger, transferring its heat to the potable hot water. When the panels are cooler than the storage tank or when the storage tank has already reached its maximum temperature, the controller in closed-loop systems will stop the circulation pumps. In a \"drainback\" system, the water drains into a storage tank contained in conditioned or semi-conditioned space, protected from freezing temperatures. With antifreeze systems, however, the pump \"must\" be run if the panel temperature gets too hot (to prevent degradation of the antifreeze) or too cold (to prevent the water/antifreeze mixture from freezing.)\n\nFlat panel collectors are typically used in closed-loop systems. Flat panels, which often resemble skylights, are the most durable type of collector, and they also have the best performance for systems designed for temperatures within of ambient temperature. Flat panels are regularly used in both pure water and antifreeze systems.\n\nAnother type of solar collector is the \"evacuated tube collector\", which are intended for cold climates that do not experience severe hail and/or applications where high temperatures are needed (i.e., over ). Placed in a rack, evacuated tube collectors form a row of glass tubes, each containing absorption fins attached to a central heat-conducting rod (copper or condensation-driven). The \"evacuated\" description refers to the vacuum created in the glass tubes during the manufacturing process, which results in very low heat loss and lets evacuated tube systems achieve extreme temperatures, far in excess of water's boiling point.\n\nNatural gas and propane storage water heaters operate identically with a gas or propane burner located at the bottom of the storage tank heating the water. Fuel oil fired storage water heaters are configured similarly by igniting a vaporizing mist of oil and air with an electric spark. \n\nEmissions from fossil fuel fired water heaters are expelled using a variety of venting technologies. Atmospheric vented systems use room air as combustion air and exhaust air. The exhaust air is expelled through the exhaust flue by buoyancy forces resulting from the combustion. Power vent models operate similarly to atmospheric vent systems, but an exhaust fan is added to aid in the expulsion of combustion gases. Direct vent systems do not use room air for combustion; instead, buoyancy forces air from the outside through the water heater combustion system and finally exhausts the combustion gases to the outside. Powered direct-vent systems include an exhaust fan to aid in the expulsion of combustion gasses.\n\nAs fossil fuels, burning wood causes greenhouse effect gases. However, wood is a renewable source of energy. A sustainable heat system would be to use solar heat in the summer, and the minimum of wood in the winter, thanks to maximum insulation.\n\nMost electric water heaters use electric resistance elements to heat the water in the storage tank using two electric resistance elements, which are located at the bottom and top of the storage tank. Each element is controlled by an independent thermostat. In two element tanks the lower element provides recovery from standby losses and the upper element provides heating during periods of large hot water use. Some resistance water heaters contain only a lower element.\n\nElectrical water heaters that store hot water can be a good match for an intelligent electrical power distribution system, heating when the electrical grid load is low and turning off when the load is high. This could be implemented by allowing the power supplier to send loadshedding requests, or by the use of real-time energy pricing. See Economy 7.\n\nHeat pump water heaters use an air source heat pump to transfer thermal energy from the air around the unit into the storage tank. Electric resistance element(s) are typically included to provide backup heating if the heat pump cannot provide sufficient heating capacity.\n\nMost of the energy used in making electricity is lost as heat.\n\nIt is possible to send electricity more than 200 km, but moving thermal energy becomes impractical beyond about 1 km. The whole thermal system can be thought of as a way to transport heat over long distances.\n\n"}
{"id": "33913831", "url": "https://en.wikipedia.org/wiki?curid=33913831", "title": "Sugarcane harvester", "text": "Sugarcane harvester\n\nA sugarcane harvester is a large piece of agricultural machinery used to harvest and partially process sugarcane.\n\nThe machine, originally developed in the 1920s, remains similar in function and design to the combine harvester. Essentially a storage vessel on a truck with a mechanical extension, the machine cuts the stalks at the base, strips the leaves off, and then cuts the cane into segments. These are then deposited into either the on-board container, or a separate vehicle traveling alongside. Waste material is then ejected back onto the field, where it acts as fertilizer.\n\n\n"}
{"id": "2349153", "url": "https://en.wikipedia.org/wiki?curid=2349153", "title": "Survival knife", "text": "Survival knife\n\nSurvival knives are knives intended for survival purposes in a wilderness environment, often in an emergency when the user has lost most of his/her main equipment. Military units issue some type of survival knife to pilots in the event their plane may be shot down. Survival knives can be used for trapping, skinning, wood cutting, wood carving, and other uses. Hunters, hikers, and outdoor sport enthusiasts use survival knives. Some survival knives are heavy-bladed and thick. Other survival knives are lightweight or fold in order to save weight and bulk as part of a larger survival kit. Their functions often include serving as a hunting knife. Features, such as hollow handles that could be used as storage space for matches or similar small items, began gaining popularity in the 1980s. Custom or semi-custom makers such as Americans Jimmy Lile, Bo Randall, and Chris Reeve are often credited with inventing those features.\n\nPrior to the late 19th century, outdoorsmen and military personnel did not use knives that were notably different from the knives used by butchers. Blades were relatively thin and the handles were often no more than two wooden slabs riveted to the tang. Serrations appeared on knives in the 19th century for use as a wood saw or fish scaler. Around the turn of the century, Webster L. Marble introduced the modern concept of the \"hunting knife.\" These knives incorporated heavier blades, crossguards, and pommels. They very much resembled miniaturized Bowie knives. Case, Cattaraugus, and other cutlery manufacturers soon introduced similar knives of their own and it is from these that the modern concept of the survival knife is descended. These knives, along with machetes and bolos constituted survival knives as used by military, explorers, and outdoorsmen up through at least the 1930s.\n\nDuring WWII, survival knives were issued to aircraft crew, as it was a real possibility that these personnel might be shot down over wilderness or behind enemy lines. Lifeboats aboard naval vessels also frequently contained survival kits including knives. These knives varied in design from one branch of the service to another and from one nation to another. The majority of them were simply commercial knives purchased in bulk by the military. From the Vietnam-era and to present, purpose-built survival knives evolved. One of Randall's designs which became a popular fighting knife for troops in Vietnam was the Number 14 \"Attack\" Model. During Vietnam, Randall received feedback from a Combat Surgeon in the US Army's 94th Medical Detachment named Captain George Ingraham. Ingraham's request was for serrations on the spine to cut through the fuselage of downed aircraft to rescue trapped personnel and a hollow handle to allow storage of survival gear. Randall made the changes and the result was the first of the modern survival knives.\n\nSome militaries (including Finland, the People's Republic of China, Great Britain, Germany, Soviet Union and United States) have redesigned the bayonet used with their issued rifle to include survival knife features. Historically, bayonets had functioned poorly as field knives, due to being designed primarily to turn a rifle into a thrusting weapon and only secondarily (if at all) to work as a field knife. The newer models function more acceptably for mundane tasks while remaining capable of being attached to the muzzle of a rifle.\n\nSurvival knives are designed for work such as setting traps, cutting branches, carving wood, and skinning animals. Most survival knives have fixed blades that are 10 cm to 20 cm (3.9 - 7.9 inches) long with a full thick tang. Survival knives made by Aitor, Lile, Parrish, Randall, or Reeve have hollow handles, which allow the user to store additional equipment in the handle. Some of these knives feature a compass in the cap. A hollow handle survival knife may have reduced strength and may break more easily when performing tasks such as chopping or batoning.\n\nOn some survival knives, the spine or back of the blade is flat; allowing it to make a good hitting platform when pounding it with a hard stick to aid in splitting wood. Other models such as Lile's and Parrish's feature a serrated spine or in the case of Rob Simonich's and Strider Knives, a band (strapping) breaker near the tip. For survival knives that have a distinct flat spine with sharp 90 degree edges, the user can strike the spine edges against a ferrocerium rod to create sparks for fire starting purposes.\n\nIf the knife has a strong tip, then the knife can also be used as a weapon for self defense. Some knives even include holes in the handle such that the knife can be fastened to a long stick and then it functions as a spear tip. The capability to mount the knife on a stick to create a spear allows the user to better hunt wildlife at a safer distance.\n\nThe handle material of survival knives differs from one to the next and is determined primarily by user preference. Handle materials can be hardened rubber, wood, bone (horn), aluminium, polymer, or even metal, such as stainless steel in the case of the Aitor Jungle King I, or tool steel as used in the Chris Reeve One-Piece line. Makers like Lile, Strider, and Parrish often wrap these metal handled knives with cord which can be used in survival situations and in daily use provides a more comfortable and reliable grip. In a situation where the knife handle breaks, if the knife is full tang, users can also wrap the handle area of the tang with cord to create a functional makeshift handle and not lose the functionality of the knife.\n\nThe serrations seen on Aircrew Survival Egress Knife (ASEK) are intended to allow air crewmen to cut their way free through the relatively thin metal skin of a crashed helicopter or airplane. Those knives that do include functional saw-teeth still suffer from lack of blade length, limiting the thickness of what can be cut when used as a saw.\n\nThe AKM Type I bayonet (introduced in 1959) was revolutionary design. It has a Bowie style (clip-point) blade with saw-teeth along the spine, and can be used as a multi-purpose survival knife and wire-cutter when combined with its steel scabbard. This design was copied by other nations and formed the basis of the US M9 bayonet. The AK-74 bayonet 6Kh5 (introduced in 1983) represents a further refinement of the AKM bayonet. \"It introduced a radical blade cross-section, that has a flat milled on one side near the edge and a corresponding flat milled on the opposite side near the false edge.\" The blade has a new spear point and an improved one-piece moulded plastic grip, making it a more effective fighting knife. It also has saw-teeth on the false edge and the usual hole for use as a wire-cutter. The wire cutting versions of the AK bayonets each have an electrically insulated handle and an electrically insulated part of the scabbard, so it can be used to cut an electrified wire.\n\n\n"}
{"id": "2726769", "url": "https://en.wikipedia.org/wiki?curid=2726769", "title": "Systems management", "text": "Systems management\n\nSystems management refers to enterprise-wide administration of distributed systems including (and commonly in practice) computer systems. Systems management is strongly influenced by network management initiatives in telecommunications. The application performance management (APM) technologies are now a subset of Systems management. Maximum productivity can be achieved more efficiently through event correlation, system automation and predictive analysis which is now all part of APM.\n\nCentralized management has a time and effort trade-off that is related to the size of the company, the expertise of the IT staff, and the amount of technology being used:\n\nSystems management may involve one or more of the following tasks:\n\nFunctional groups are provided according to International Telecommunication Union Telecommunication Standardization Sector (ITU-T) Common management information protocol (X.700) standard. This framework is also known as Fault, Configuration, Accounting, Performance, Security (FCAPS).\n\n\nHowever this standard should not be treated as comprehensive, there are obvious omissions. Some are recently emerging sectors, some are implied and some are just not listed. The primary ones are:\n\nPerformance management functions can also be split into end-to-end performance measuring and infrastructure component measuring functions. Another recently emerging sector is operational intelligence (OI) which focuses on real-time monitoring of business events that relate to business processes, not unlike business activity monitoring (BAM).\n\n\nSchools that offer or have offered degrees in the field of systems management include the University of Southern California, the University of Denver, Capitol Technology University, and Florida Institute of Technology.\n\n\n"}
{"id": "7454236", "url": "https://en.wikipedia.org/wiki?curid=7454236", "title": "Tetrathiafulvalene", "text": "Tetrathiafulvalene\n\nTetrathiafulvalene is an organosulfur compound with the formula (HCSC). Studies on this heterocyclic compound contributed to the development of molecular electronics. TTF is related to the hydrocarbon fulvalene, (CH), by replacement of four CH groups with sulfur atoms. Over 10,000 scientific publications discuss TTF and its derivatives.\n\nThe high level of interest in TTFs has spawned the development of many syntheses of TTF and its analogues. Most preparations entail the coupling of cyclic CS building blocks such as 1,3-dithiole-2-thiones or the related 1,3-dithiole-2-ones. For TTF itself, the synthesis begins with the trithiocarbonate HCSCS, which is S-methylated and then reduced to give HCSCH(SCH), which is treated as follows:\n\nBulk TTF itself has unremarkable electrical properties. Distinctive properties are, however, associated with salts of its oxidized derivatives, such as salts derived from TTF.\n\nThe high electrical conductivity of TTF salts can be attributed to the following features of TTF: (i) its planarity, which allows π-π stacking of its oxidized derivatives, (ii) its high symmetry, which promotes charge delocalization, thereby minimizing coulombic repulsions, and (iii) its ability to undergo oxidation at mild potentials to give a stable radical cation. Electrochemical measurements show that TTF can be oxidized twice reversibly:\n\nEach dithiolylidene ring in TTF has 7π electrons: 2 for each sulfur atom, 1 for each sp carbon atom. Thus, oxidation converts each ring to an aromatic 6π-electron configuration, consequently leaving the central double bond essentially a single bond, as all π-electrons occupy ring orbitals.\n\nThe salt was reported to be a semiconductor in 1972. Subsequently, the charge-transfer salt [TTF]TCNQ was shown to be a narrow band gap semiconductor. X-ray diffraction studies of [TTF][TCNQ] revealed stacks of partially oxidized TTF molecules adjacent to anionic stacks of TCNQ molecules. This “segregated stack” motif was unexpected and is responsible for the distinctive electrical properties, i.e. high and anisotropic electrical conductivity. Since these early discoveries, numerous analogues of TTF have been prepared. Well studied analogues include tetramethyltetrathiafulvalene (MeTTF), tetramethylselenafulvalenes (TMTSFs), and bis(ethylenedithio)tetrathiafulvalene (BEDT-TTF, CAS [66946-48-3]). Several tetramethyltetrathiafulvalene salts (called Fabre salts) are of some relevance as organic superconductors.\n\n\n"}
{"id": "179124", "url": "https://en.wikipedia.org/wiki?curid=179124", "title": "Threshold pledge system", "text": "Threshold pledge system\n\nThe threshold pledge or fund and release system is a way of making a fundraising pledge as a group of individuals, often involving charitable goals or financing the provision of a public good. An amount of money is set as the goal or \"threshold\" to reach for the specified purpose and interested individuals will pitch in, but the money at first either remains with the pledgers or is held in escrow.\n\nWhen the threshold is reached, the pledges are called in (or transferred from the escrow fund) and a contract is formed so that the collective good is supplied; a variant is that the money is collected when the good is actually delivered. If the threshold is not reached by a certain date (or perhaps if no contract is ever signed, etc.), the pledges are either never collected or, if held in escrow, are simply returned to the pledgers. In economics, this type of model is known as an assurance contract.\n\nThis system is most often applied to creative works, both for financing new productions and for buying out existing works; in the latter cases, it is sometimes known as ransom publishing model or Street Performer Protocol (SPP).\n\nStreet Performer Protocol is an early description of a type of threshold pledge system. SPP is the threshold pledge system encouraging the creation of creative works in the public domain or copylefted, described by Steven Schear and separately by cryptographers John Kelsey and Bruce Schneier. This assumes that current forms of copyright and business models of the creative industries will become increasingly inefficient or unworkable in the future, because of the ease of copying and distribution of digital information.\n\nUnder the Street Performer Protocol, the artist announces that when a certain amount of money is received in escrow, the artist will release a work (book, music, software, etc.) into the public domain or under a free content license. Interested donors make their donations to a publisher, who contracts with the artist for the work's creation and keeps the donations in escrow, identified by their donors, until the work is released.\n\nIf the artist releases the work on time, the artist receives payment from the escrow fund. If not, the publisher repays the donors, possibly with interest. As detailed above, contributions may also be refunded if the threshold is not reached within a reasonable expiring date. The assessed threshold also includes a fee which compensates the publisher for costs and assumption of risks.\n\nThe publisher may act like a traditional publisher, by soliciting sample works and deciding which ones to support, or it may serve only as an escrow agent and not care about the quality of the works (like a vanity press). \n\nIn software, source code escrow is a publishing model that applies the SPP to source code (often involving existing proprietary software) which is eventually released under an open source or free software license.\n\nThe Street Performer Protocol is a natural extension of the much older idea of funding the production of written or creative works through agreements between groups of potential readers or users.\n\nThe first illustrated edition of John Milton's \"Paradise Lost\" was published under a subscription system; and Mozart and Beethoven, among other composers, used subscriptions to premiere concerts and first print editions of their works. Unlike today's meaning of \"subscription\", this meant that a fixed number of people had to sign up and pay some amount before the concert could take place or the printing press started.\n\nThese three (piano) concertos K413-415 ... formed an important milestone in his career, being the first in the series of great concertos that he wrote for Vienna, and the first to be published in a printed edition. Initially, however, he followed the usual practice of making them available in manuscript copies. Mozart advertised for subscribers in January 1783: \"These three concertos, which can be performed with full orchestra including wind instruments, or only a quattro, that is with 2 violins, 1 viola and violoncello, will be available at the beginning of April to those who have subscribed for them (beautifully copied, and supervised by the composer himself).\" Six months later, Mozart complained that it was taking a long time to secure enough subscribers. This was despite the fact that he had meanwhile scored a great success on two fronts:…\n\nHowever, there are a number of differences between this traditional model and the SPP. The most important difference is that traditionally, the subscribers would be among the first to get access and would do so with the understanding that the work would likely always be a \"rare\" good; thus, there was some status in owning a copy, as well as the prestige of being among the patrons. Additionally, subscriptions were generally sold at a set price, but some wealthy subscribers may have given more in order to be a patron. In the modern Street Performer Protocol, each funder chooses the amount they want to pay, and the work is released to the public and freely reproduced.\n\nIn 1970, Stephen Breyer argued for the importance of this model in \"The Uneasy Case for Copyright\".\n\nThe Street Performer Protocol was successfully used to release the source code and brand name of the Blender 3D animation program. After NaN Technologies BV went bankrupt in 2002, the copyright and trademark rights to Blender went to the newly created NaN Holding BV. The newly created Blender Foundation campaigned for donations to obtain the right to release the software as free and open source under the GNU General Public License. NaN Holding BV set the price tag at 100,000 euros. More than 1,300 users became members and donated more than 50 euros each, in addition to anonymous users, non-membership individual donations and companies. On October 13, 2002, Blender was released on the Internet as free/open source software.\n\nVariations of the SPP include the Rational Street Performer Protocol and the Wall Street Performer Protocol.\n\n\n\n"}
{"id": "27275174", "url": "https://en.wikipedia.org/wiki?curid=27275174", "title": "Windhoeker Maschinenfabrik", "text": "Windhoeker Maschinenfabrik\n\nWindhoeker Maschinenfabrik (WMF) is a Namibian defense contractor located in Windhoek, Namibia and operated by the Namibian Defence Force. WMF designs and manufactures monocoque V-hull armoured combat vehicles. WMF is responsible for the armoured bodywork fabrication while engines, transmissions and axles are built by MAN.\n\nWindhoeker Maschinenfabrik was founded in 1939. It later provided equipment for the South African Defence Force and South West African Territorial Force. In 1977 WMF began manufacturing Mine Protected Vehicles. After the Independence of Namibia from South Africa in 1989, WMF was nationalized in 1998. The company functions as a subsidiary of Namibian Defence Force. Currently Victor Simunja is the Managing Director.\n\nWMF performs the following services:\n\nWMF produces the following 4x4 Mine protected vehicles (MPV)\n\nHumanitarian Engineering and Consulting (HEC), the R&D branch for MgM, has designed a special series of WerWolf called the HEC-Wolf, which specifically supports the requirements of Humanitarian Demining operations. The HEC-Wolf is somewhat de-militarized in that it cannot carry the wide range of weapons which normally are associated with this vehicle.\n\nWMFs customers are:\n\n13 Bessemer Street,\n\nSouthern Industrial Area\n\nWindhoek, Namibia\n\n\nP.O. Box 5013\n\nWindhoek, Namibia\n\n\n"}
{"id": "4555540", "url": "https://en.wikipedia.org/wiki?curid=4555540", "title": "Window fan", "text": "Window fan\n\nA window fan is a fan designed to be placed inside the frame of a window. Window fans have been used for many decades to maintain comfortable temperatures and ventilation within one's home, and operate at a tiny fraction of the cost of central air conditioning.\n\nWindow fans are designed to fit into open windows, and often have expandable side panels to cover the entire window opening. Most units have either one large fan, or two small fans mounted side by side.\n\nMost window fans have two or three speeds, and some are electrically reversible so that the user can switch between intake and exhaust modes.\n\nManufacturers have added features to their models over the years. Some fans contain thermostats which will turn the fan off if the temperature drops to a certain point, or adjustable grilles to direct the flow of air in a particular direction. Other fans offer sophisticated electronic features such as a remote control, automatic shut-off timers, and dimmer-like variable-speed motor controls.\n\nMost window fans today are made of plastic. Many older fans, however, are made of metal and are more solid. The more powerful window fans can be used to circulate air throughout a house, acting as a whole-house fan.\n\n"}
{"id": "53093944", "url": "https://en.wikipedia.org/wiki?curid=53093944", "title": "Word Attack!", "text": "Word Attack!\n\nWord Attack! is a 1984 educational video game by Davidson & Associates. An updated version was released entitled \"Word Attack! Plus.\" The game found similar success to the Davidson titles \"Spell It\" and \"Reading and Me.\"\n\nThe game consists of a series of language-based \"drill-and-practice\" minigames and exercises. In one, the player uses the arrow keys to fire at correct answer \"targets\". The nine programs are aimed at grades 4-12.\n\nBy October 13, 1984, the game had sustained an 11-week streak on the Billboard Charts for Top Educational Computer Software. It re-entered the charts on November 3, 1984 at #9, and again on November 24, 1984 at #7. By June 15, 1985, the game had stayed on the list for a total of 44 weeks, and was currently at #8.\n\n\"The Computer in Reading and Language Arts\" criticised the mechanic of shooting the correct multiple choice answers, deeming the graphics to be out of place in an educational title. \"InfoWorld\" deemed it well-organised and flexible, adding that it is an effective teaching tool.\n"}
