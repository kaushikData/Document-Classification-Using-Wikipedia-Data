{"id": "2163963", "url": "https://en.wikipedia.org/wiki?curid=2163963", "title": "Air traffic control radar beacon system", "text": "Air traffic control radar beacon system\n\nThe air traffic control radar beacon system (ATCRBS) is a system used in air traffic control (ATC) to enhance surveillance radar monitoring and separation of air traffic. It consists of a rotating ground antenna and transponders in aircraft. The ground antenna sweeps a narrow vertical beam of microwaves around the airspace. When the beam strikes an aircraft, the transponder transmits a return signal back giving information such as the flight number designation and altitude of the aircraft. ATCRBS assists air traffic control (ATC) surveillance radars by acquiring information about the aircraft being monitored, and providing this information to the radar controllers. The controllers can use the information to identify radar returns from aircraft (known as \"targets\") and to distinguish those returns from ground clutter.\n\nThe system consists of transponders, installed in aircraft, and secondary surveillance radars (SSRs), installed at air traffic control facilities. The SSR is sometimes co-located with the primary surveillance radar, or PSR. These two radar systems work in conjunction to produce a synchronized surveillance picture. The SSR transmits interrogations and listens for any replies. Transponders that receive an interrogation decode it, decide whether to reply, and then respond with the requested information when appropriate. Note that in common informal usage, the term \"SSR\" is sometimes used to refer to the entire ATCRBS system, however this term (as found in technical publications) properly refers only to the ground radar itself.\n\nAn ATC ground station consists of two radar systems and their associated support components. The most prominent component is the PSR. It is also referred to as \"skin paint radar\" because it shows not synthetic or alpha-numeric target symbols, but bright (or colored) blips or areas on the radar screen produced by the RF energy reflections from the target's \"skin.\" This is a non-cooperative process, no additional avionic devices are needed. The radar detects and displays reflective objects within the radar's operating range. Weather radar data is displayed in skin paint mode. The primary surveillance radar is subject to the radar equation that says signal strength drops off as the fourth power of distance to the target. Objects detected using the PSR are known as primary targets.\n\nThe second system is the secondary surveillance radar, or SSR, which depends on a cooperating transponder installed on the aircraft being tracked. The transponder emits a signal when it is interrogated by the secondary radar. In a transponder based system signals drop off as the inverse square of the distance to the target, instead of the fourth power in primary radars. As a result, effective range is greatly increased for a given power level. The transponder can also send encoded information about the aircraft, such as identity and altitude.\n\nThe SSR is equipped with a main antenna, and an omnidirectional \"Omni\" antenna at many older sites. Newer antennas (as in the adjacent picture), are grouped as a left and right antenna, and each side connects to a hybrid device which combines the signals into sum and difference channels. Still other sites have both the sum and difference antenna, and an Omni antenna. Surveillance aircraft, e.g. AWACS, have only the sum and difference antennas, but can also be space stabilized by phase shifting the beam down or up when pitched or rolled. The SSR antenna is typically fitted to the PSR antenna, so they point in the same direction as the antennas rotate. The omnidirectional antenna is mounted near and high, usually on top of the radome if equipped. Mode-S interrogators require the sum and difference channels to provide the monopulse capability to measure the off-boresight angle of the transponder reply.\n\nThe SSR repetitively transmits interrogations as the rotating radar antenna scans the sky. The interrogation specifies what type of information a replying transponder should send by using a system of modes. There have been a number of modes used historically, but four are in common use today: mode 1, mode 2, mode 3/A, and mode C. Mode 1 is used to sort military targets during phases of a mission. Mode 2 is used to identify military aircraft missions. Mode 3/A is used to identify each aircraft in the radar's coverage area. Mode C is used to request/report an aircraft's altitude.\n\nTwo other modes, mode 4 and mode S, are not considered part of the ATCRBS system, but they use the same transmit and receive hardware. Mode 4 is used by military aircraft for the Identification Friend or Foe (IFF) system. Mode S is a discrete selective interrogation, rather than a general broadcast, that facilitates TCAS for civilian aircraft. Mode S transponders ignore interrogations not addressed with their unique identity code, reducing channel congestion. At a typical SSR radar installation, ATCRBS, IFF, and mode S interrogations will all be transmitted in an interlaced fashion. Some military facilities and/or aircraft will also utilize Mode S.\n\nReturns from both radars at the ground station are transmitted to the ATC facility using a microwave link, a coaxial link, or (with newer radars) a digitizer and a modem. Once received at the ATC facility, a computer system known as a radar data processor associates the reply information with the proper primary target and displays it next to the target on the radar scope.\n\nThe equipment installed in the aircraft is considerably simpler, consisting of the transponder itself, usually mounted in the instrument panel or avionics rack, and a small L band UHF antenna, mounted on the bottom of the fuselage. Many commercial aircraft also have an antenna on the top of the fuselage, and either or both antennas can be selected by the flight crew.\n\nTypical installations also include an altitude encoder, which is a small device connected to both the transponder and the aircraft's static system. It provides the aircraft's pressure altitude to the transponder, so that it may relay the information to the ATC facility. The encoder uses 11 wires to pass altitude information to the transponder in the form of a Gillham Code, a modified binary Gray code.\nThe transponder has a small required set of controls and is simple to operate. It has a method to enter the four-digit transponder code, also known as a \"beacon code\" or \"squawk code\", and a control to transmit an \"ident\", which is done at the controller's request (see SPI pulse below). Transponders typically have 4 operating modes: Off, Standby, On (Mode-A), and Alt (Mode-C). On and Alt mode differ only in that the On mode inhibits transmitting any altitude information. Standby mode allows the unit to remain powered and warmed up but inhibits any replies, since the radar is used for searching the aircraft and exact location of aircraft.\n\nThe steps involved in performing an ATCRBS interrogation are as follows: First, the ATCRBS interrogator periodically interrogates aircraft on a frequency of 1030 MHz. This is done through a rotating or scanning antenna at the radar's assigned Pulse Repetition Frequency (PRF). Interrogations are typically performed at 450 - 500 interrogations/second. Once an interrogation has been transmitted, it travels through space (at the speed of light) in the direction the antenna is pointing until an aircraft is reached.\n\nWhen the aircraft receives the interrogation, the aircraft transponder will send a reply on 1090 MHz after a 3.0 μs delay indicating the requested information. The interrogator's processor will then decode the reply and identify the aircraft. The range of the aircraft is determined from the delay between the reply and the interrogation. The azimuth of the aircraft is determined from the direction the antenna is pointing when the first reply was received, until the last reply is received. This window of azimuth values is then divided by two to give the calculated \"centroid\" azimuth. The errors in this algorithm cause the aircraft to jitter across the controllers scope, and is referred to as \"track jitter.\" The jitter problem makes software tracking algorithms problematic, and is the reason why monopulse was implemented.\n\nInterrogations consist of three pulses, 0.8 μs in duration, referred to as P1, P2 and P3. The timing between pulses P1 and P3 determines the mode (or question) of the interrogation, and thus what the nature of the reply should be. P2 is used in side-lobe suppression, explained later.\n\nMode 3/A uses a P1 to P3 spacing of 8.0 μs, and is used to request the \"beacon code\", which was assigned to the aircraft by the controller to identify it. Mode C uses a spacing of 21 μs, and requests the aircraft's pressure altitude, provided by the altitude encoder. Mode 2 uses a spacing of 5 μs and requests the aircraft to transmit its Military identification code. The latter is only assigned to Military aircraft and so only a small percentage of aircraft actually reply to a mode 2 interrogation.\n\nReplies to interrogations consist of 15 time slots, each 1.45 μs in width. The reply is encoded by the presence or absence of a 0.45 μs pulse in each slot. These are labeled as follows:\n\nF1 C1 A1 C2 A2 C4 A4 X B1 D1 B2 D2 B4 D4 F2 SPI\n\nThe F1 and F2 pulses are framing pulses, and are always transmitted by the aircraft transponder. They are used by the interrogator to identify legitimate replies. These are spaced 20.3 μs apart.\n\nThe A4, A2, A1, B4, B2, B1, C4, C2, C1, D4, D2, D1 pulses constitute the \"information\" contained in the reply. These bits are used in different ways for each interrogation mode.\n\nFor mode A, each digit in the transponder code (A, B, C, or D) may be a number from zero to seven. These octal digits are transmitted as groups of three pulses each, the A slots reserved for the first digit, B for the second, and so on.\n\nIn a mode C reply, the altitude is encoded by a \"Gillham interface\", Gillham code, which uses Gray code. The Gillham interface is capable of representing a wide range of altitudes, in increments. The altitude transmitted is pressure altitude, and corrected for altimeter setting at the ATC facility. If no encoder is attached, the transponder may optionally transmit only framing pulses (most modern transponders do).\n\nIn a mode 3 reply, the information is the same as a mode A reply in that there are 4 digits transmitted between 0 and 7. The term mode 3 is utilized by the military, whereas mode A is the civilian term.\n\nThe X bit is currently only used for test targets. This bit was originally transmitted by BOMARC missiles that were used as air-launched test targets. This bit may be used by drone aircraft.\n\nThe SPI pulse is positioned 4.35μs past the F2 pulse (3 time slots) and is used as a \"Special Identification Pulse\". The SPI pulse is turned on by the \"identity control\" on the transponder in the aircraft cockpit when requested by air traffic control. The air traffic controller can request the pilot to ident, and when the identity control is activated, the SPI bit will be added to the reply for about 20 seconds (two to four rotations of the interrogator antenna) thereby highlighting the track on the controllers display.\n\nThe SSR's directional antenna is never perfect; inevitably it will \"leak\" lower levels of RF energy in off-axis directions. These are known as side lobes. When aircraft are close to the ground station, the side lobe signals are often strong enough to elicit a reply from their transponders when the antenna is not pointing at them. This can cause \"ghosting\", where an aircraft's target may appear in more than one location on the radar scope. In extreme cases, an effect known as \"ring-around\" occurs, where the transponder replies to excess resulting in an arc or circle of replies centered on the radar site.\n\nTo combat these effects, side lobe suppression (SLS) is used. SLS employs a third pulse, P2, spaced 2μs after P1. This pulse is transmitted from the omnidirectional antenna (or the antenna difference channel) by the ground station, rather than from the directional antenna (or the sum channel). The power output from the omnidirectional antenna is calibrated so that, when received by an aircraft, the P2 pulse is stronger than either P1 or P3, \"except\" when the directional antenna is pointing directly at the aircraft. By comparing the relative strengths of P2 and P1, airborne transponders can determine whether or not the antenna is pointing at the aircraft when the interrogation was received. The power to the difference antenna pattern (for systems so equipped) is not adjusted from that of the P1 and P3 pulses. Algorithms are used in the ground receivers to delete replies on the edge of the two beam patterns.\n\nTo combat these effects more recently, side lobe suppression (SLS) is still used, but differently. The new and improved SLS employs a third pulse, spaced 2μs either before P3 (a new P2 position) or after P3 (which should be called P4 and appears in the Mode S radar and TCAS specifications). This pulse is transmitted from the directional antenna at the ground station, and the power output of this pulse is the same strength as the P1 and P3 pulses. The action to be taken is specified in the new and improved C74c as:\n\n2.6 Decoding Performance.\nc. Side-lobe Suppression. The transponder must be suppressed for a period of 35 ±10 microseconds following receipt of a pulse pair of proper spacing and suppression action must be capable of being reinitiated for the full duration within 2 microseconds after the end of any suppression period. The transponder must be suppressed with a 99 percent efficiency over a received signal amplitude range between 3 db above minimum triggering level and 50 db above that level and upon receipt of properly spaced interrogations when the received amplitude of P2 is equal to or in excess of the received amplitude of P1 and spaced 2.0 ±0.15 microsecond from P3.\n\nAny requirement at the transponder to detect and act upon a P2 pulse 2μs after P1 has been removed from the new and improved TSO C74c specification.\n\nMost \"modern\" transponders (manufactured since 1973) have an \"SLS\" circuit which suppresses reply on receipt of any two pulses in any interrogation spaced 2.0 microseconds apart that are above the MTL Minimum Triggering Level threshold of the receiver amplitude discriminator (P1->P2 or P2->P3 or P3->P4). This approach was used to comply with the original C74c and but also complies with the provisions of the new and improved C74c.\n\nThe FAA refers to the non-responsiveness of new and improved TSO C74c compliant transponders to Mode S compatible radars and TCAS as \"The Terra Problem\", and has issued Airworthiness Directives (ADs) against various transponder manufacturers, over the years, at various times on no predictable schedule. The ghosting and ring around problems have recurred on the more modern radars.\n\nTo combat these effects most recently, great emphasis is placed upon software solutions. It is highly likely that one of those software algorithms was the proximate cause of a mid-air collision recently, as one airplane was reported at showing its altitude as the pre-flight paper filed flight plan, and not the altitude assigned by the ATC controller (see the reports and observations contained in the below reference ATC Controlled Airplane Passenger Study of how radar worked).\nSee the reference section below for errors in performance standards for ATCRBS transponders in the US.\n\nSee the reference section below for FAA Technician Study of in-situ transponders.\n\nThe beacon code and altitude were historically displayed verbatim on the radar scope next to the target, however modernization has extended the radar data processor with a flight data processor, or FDP. The FDP automatically assigns beacon codes to flight plans, and when that beacon code is received from an aircraft, the computer can associate it with flight plan information to display immediately useful data, such as aircraft callsign, the aircraft's next navigational fix, assigned and current altitude, etc. near the target in a \"data block\".\nAlthough the ATCRBS does not display aircraft heading.\n\nMode S, or \"mode select\", despite also being called a mode, is actually a radically improved system intended to replace ATCRBS altogether. A few countries have mandated mode S, and many other countries, including the United States, have begun phasing out ATCRBS in favor of this system. Mode S is designed to be fully backward compatible with existing ATCRBS technology.\n\nMode S, despite being called a replacement transponder system for ATCRBS, is actually a data packet protocol which can be used to augment ATCRBS transponder positioning equipment (radar and TCAS).\n\nOne major improvement of Mode S is the ability to interrogate a single aircraft at a time. With old ATCRBS technology, all aircraft within the beam pattern of the interrogating station will reply. In an airspace with multiple interrogation stations, ATCRBS transponders in aircraft can be overwhelmed. By interrogating one aircraft at a time, workload on the aircraft transponder is greatly reduced.\n\nThe second major improvement is increased azimuth accuracy. With PSRs and old SSRs, azimuth of the aircraft is determined by the half split (centroid) method. The half split method is computed by recording the azimuth of the first and last replies from the aircraft, as the radar beam sweeps past its position. Then the midpoint between the start and stop azimuth is used for aircraft position. With MSSR (monopulse secondary surveillance radar) and Mode S, the radar can use the information of one reply to determine azimuth. This is calculated based on the RF phase of the aircraft reply, as determined by the sum and difference antenna elements, and is called monopulse. This monopulse method results in superior azimuth resolution, and removes target jitter from the display.\n\nThe Mode S system also includes a more robust communications protocol, for a wider variety of information exchange. At this time, this capability is becoming mandatory across Europe with some states already requiring its use.\n\n\n\n"}
{"id": "27452961", "url": "https://en.wikipedia.org/wiki?curid=27452961", "title": "American Council of Engineering Companies", "text": "American Council of Engineering Companies\n\nThe American Council of Engineering Companies (ACEC) is the oldest and largest business association of engineering companies. It is organized as a federation of 52 state and regional councils with national headquarters in Washington, D.C., comprising thousands of engineering practices throughout the country. It administers extensive lobbying and education programs.\n\nACEC traces its roots to the Association of Architectural Engineers, founded in New York City in 1905 to promote the business interests of consulting engineers. The organization, which was based on individual memberships, changed its name to the American Institute of Consulting Engineers (AICE) in 1909. Similar organizations soon sprung up in many states.\n\nIn 1956, representatives from 10 state associations created a national Consulting Engineers Council (CEC), representing engineering firms rather than individuals. By 1960, CEC had grown to 29 state member organizations, representing more than 1,000 firms.\n\nIn 1973, CEC and AICE merged to create the American Consulting Engineers Council (ACEC). To accommodate the individual members of AICE, the new organization created a College of Fellows membership category.\n\nACEC’s advocacy activities covered a broad range of issues, sometimes necessitating the creation of spin-off groups to pursue specific goals. For example, in 1986, ACEC founded the American Tort Reform Association (ATRA) to lobby for the reform of unfair liability statutes nationwide.\n\nIn 2000, ACEC changed its name to the American Council of Engineering Companies to reflect both its firm-based membership and the increasingly diversified and multi-disciplinary nature of engineering practices, including design-build practices.\n\nIn 2006, President George W. Bush gave a major mid-term address at the ACEC Annual Convention in Washington, D.C., recognizing the organization for its public policy advocacy.\n\nIn 2012, along with the American Public Works Association (APWA) and American Society of Civil Engineers (ASCE), ACEC founded the Institute for Sustainable Infrastructure (ISI), which has developed the Envision® sustainability rating system for infrastructure works.\n\nACEC advocates for the business interests of its member firms before legislatures, executive agencies, courts, and in public media. \n\nQualifications-Based Selection (QBS). In 1972, the Council was a prime mover in the passage of the A/E Selection Procedures Act, also known as the Brooks Act, which requires that the U.S. Federal Government procure engineering and architecture services through Qualifications-Based Selection (QBS) rather than solely by price. Most states have adopted similar legislation. ACEC today seeks to bolster and expand the reach of QBS as a business “best practice” to ensure innovation, successful performance and public safety. \n\nContracting Out. The Council is a strong proponent of both federal and state contracting out of engineering services, asserting that such work is not “inherently a governmental” and can be best and most economically performed by the private sector.\n\nIn 2000, ACEC was a key advocate of the \"Thomas Amendment\" in the Water Resources and Development Act, which limits the extent to which the U.S. Army Corps of Engineers can compete with private engineering firms in municipal works such as schools, hospitals, and utilities.\n\nIn 2016, an ACEC-sponsored contracting out study, conducted by New York University, concluded that contracting out of engineering services by state departments of transportation provided demonstrable savings over having those same services performed by DOT in-house operations.\n\nAcquisition Regulations. In 2011, ACEC led a large business coalition that won repeal of the 3 percent withholding provision on federal, state, and local contracts. \n\nThe Council has also: secured reforms in Federal Acquisition Regulations (FAR) pertaining to overhead and audit requirements; removed the mandatory 10 percent retainage on fixed-private federal architectural/engineering (A/E) contracts; expanded opportunities for small firms to compete for Department of Defense contracts; exempted A/E services from Project Labor Agreements on federal projects; and secured reforms to the federal design-build competition process. \n\nInfrastructure Investment. ACEC has been a leading proponent of increased infrastructure investment in surface transportation, water, aviation and energy.\n\nACEC has supported the passage of every long-term surface transportation program, including the \"Intermodal Surface Transportation Efficiency Act\" (ISTEA) (1991), \"Transportation Equity Act for the 21st Century\" (TEA-21) (1998), \"Safe, Accountable, Flexible, Efficient Transportation Equity Act: A Legacy for Users\" (SAFETEA-LU) (2005), \"Moving Ahead for Progress in the 21st Century Act\" (MAP-21) (2012), and \"Fixing America’s Surface Transportation Act\" (FAST Act) (2015).\n\nTax Reform. In 2004, ACEC helped win passage of a 9 percent tax deduction for engineering and other firms as part of the American Jobs Creation Act; in 2011, defeated a mandate for filing 1099 forms for every purchase of goods valued at more than $600; and in 2016, secured the extension of key tax benefits for engineering firms, including the R&D tax credit, bonus depreciation, small business expensing, and renewable energy tax credits. The Council also continues to seek protection of the cash method of accounting.\n\nRisk Management. ACEC helps member firms in understanding and managing risk, and advocates for legislative and regulatory reforms to properly control and fairly allocate risk.\n\nIn recent years, ACEC has won judicial or legislative victories on issues including indemnification provisions, the duty to defend, the Economic Loss Doctrine, and the professional standard of care.\nEach year, ACEC conducts annual Professional Liability Insurance Surveys of member and carriers to gauge current market conditions.\n\nThe American Council of Engineering Companies Political Action Committee (ACEC/PAC) is a $1 million-plus annual PAC, the largest PAC in the A/E industry and among the top 3 percent of all federal PACs.\n\nDuring the 2014-2016 election cycle, the PAC contributed more than $2 million to Congressional campaigns and had a 97 percent win record.\n\nACEC/PAC is bipartisan, funded solely by ACEC member contributions, and supports pro-business candidates.\n\nThe Council holds more than 100 online classes annually, covering a wide range of business management and engineering topics.\n\nACEC's Senior Executives Institute (SEI) provides advanced management, leadership and public policy training for firm leaders. More than 500 executives have participated in the program.\n\nIn cooperation with the Federal Highway Administration, ACEC holds regular on-site educational workshops on Federal Acquisition Regulations.\n\nACEC publishes \"Engineering Inc.\", a bi-monthly print and online magazine focusing on business issues. \n\n\"Last Word\" is the Council's weekly online membership newsletter.\n\nThe quarterly \"ACEC Engineering Business Index\" surveys member CEOs on the health of the engineering industry.\n\nThe Engineers Joint Contract Document Committee (EJCDC) is a collaboration of ACEC, ASCE, and the National Society of Professional Engineers (NSPE) to develop and disseminate standard contract documents for use in design and construction projects. \n\nACEC hosts two conferences per year. Each spring, the ACEC Annual Convention and Legislative Summit is held in Washington, D.C., featuring political speakers and member visits to congressional offices, as well as business education. \n\nThe ACEC Fall Conference is held in different locations each year and focuses on business practice issues, markets, and political developments. \n• Engineering Excellence Awards—Introduced in 1967, the Engineering Excellence Awards (EEA) program annually honors outstanding engineering accomplishments.\n\n• Community Service Awards—Given annually to member firm principals who have made outstanding contributions to the quality of life in their community.\n\n• Distinguished Award of Merit—The Council's highest award, given to individuals for exemplary achievement. Recipients include former Presidents Dwight D. Eisenhower and Herbert Hoover, General Lucius Clay, Admiral Hyman G. Rickover, Carl Sagan, W. Edwards Deming, and Neil Armstrong.\n\n• QBS Awards Program—Co-sponsored with NSPE, the QBS Awards recognize public and private entities that make exemplary use of the qualifications-based selection (QBS) process at the state and local levels.\n\n• Young Professional of the Year Award—This award promotes the accomplishments of young engineers by highlighting their engineering contributions and the resulting impact on society.\n\n• ACEC also awards six student scholarships annually.\n"}
{"id": "1880985", "url": "https://en.wikipedia.org/wiki?curid=1880985", "title": "Backchannel", "text": "Backchannel\n\nBackchannel is the practice of using networked computers to maintain a real-time online conversation alongside the primary group activity or live spoken remarks. The term was coined in the field of linguistics to describe listeners' behaviours during verbal communication. (See Backchannel (linguistics).)\n\nThe term \"backchannel\" generally refers to online conversation about the conference topic or speaker. Occasionally backchannel provides audience members a chance to fact-check the presentation.\n\nFirst growing in popularity at technology conferences, backchannel is increasingly a factor in education where WiFi connections and laptop computers allow participants to use ordinary chat like IRC or AIM to actively communicate during presentation. More recent research include works where the backchannel is brought publicly visible, such as the ClassCommons, backchan.nl and Fragmented Social Mirror.\n\nTwitter is also widely used today by audiences to create backchannels during broadcasting of content or at conferences. For example, television drama, other forms of entertainment and magazine programs. This practice is often also called live tweeting. Many conferences nowadays also have a hashtag that can be used by the participants to share notes and experiences; furthermore such hashtags can be user generated.\n\nVictor Yngve first used the phrase \"back channel\" in 1970 in a linguistic meaning, in the following passage: \"In fact, both the person who has the turn and his partner are simultaneously engaged in both speaking and listening. This is because of the existence of what I call the back channel, over which the person who has the turn receives short messages such as 'yes' and 'uh-huh' without relinquishing the turn.\"\n\nSuch systems were widely imagined and tested in late 1990s and early 2000s. These cases include researcher's installations on conferences and classroom settings. The first famous instance of backchannel communications influencing a talk occurred on March 26, 2002, at the PC Forum conference, when Qwest CEO Joe Nacchio famously lamented the difficulties of raising capital. Journalists Dan Gillmor and Doc Searls posted accounts, from the audience, in real-time, to their weblogs. Buzz Bruggeman, a reader of Gillmor's, emailed information about a recent sizable transaction that had made Nacchio very wealthy; both Gillmor and Searls updated their weblogs with that information.\n\nIn her article referring to the \"Parallel Channel,\" PC Forum host Esther Dyson wrote, \"around that point, the audience turned hostile.\" Many commentators later attributed the audience's hostility to the information people shared while surfing and communicating on their laptops during Nacchio's remarks.\n\nResearch has demonstrated that backchannels help participants to feel as contributing members, not passive followers and make them feel more social. However, the research is mixed on the nature of this discussions, and especially regarding social interaction on the backchannels: some cases report vast interaction where as others highlight that interaction on the platform was considered low. There are indicators that these tools however engage different members of the audience to provide their input.\n\nSince its inception in 1998 at Argonne National Laboratory, the Internet2 initiative known as the Access Grid (a large-format presentation, video conferencing and interactive environment) has used backchannel communications to permit the node operators to pass URLs for display at another site, troubleshoot problems and even discuss what's for lunch at their location. The Access Grid backchannel has evolved from the use of a MOO to XMPP.\n\nIn 2009 Purdue University developed a tool called Hotseat that enabled students to comment on the course lectures in near real-time using social networking tools such as Facebook and Twitter.\n\nUsing a backchannel for educational purposes can function as a formal class activity or even an independent discussion without instructor participation and awareness. Aside from the normal discussion, a backchannel can also be used for note taking, asking questions, offering suggestions on different topics, and sharing resources with other students and faculty members. There are many different media networks out there that can be used as a backchannel. Including Twitter, Facebook, Yammer and Instant Messaging.\n\nJoichi Ito's HeckleBot includes an LED text panel displaying phrases sent from the chat room to catch the attention of the speaker or audience. The USC Interactive Media Division has experimented with \"Google Jockeys\" to feed visual information and search results between the speakers and the backchannel, projected on multiple screens surrounding their seminars. Software like SubEthaEdit allows for more formal backchannel: collaborative notetaking. In 2007 the Building Learning Communities Conference in Boston, Massachusetts used tools such as Twitter and Skype to create backchannels that included participants who were not on location and at times in remote parts of the world. At times presenters were not aware of the backchannel and other occasions the presenters themselves were involved in the backchannel.\n\n"}
{"id": "629006", "url": "https://en.wikipedia.org/wiki?curid=629006", "title": "Baker Hughes", "text": "Baker Hughes\n\nBaker Hughes, a GE company (BHGE) is an international industrial service company and one of the world's largest oil field services companies. As of July 2017 Baker Hughes is 62.5% owned by General Electric Company and 37.5% publicly traded (NYSE:BHGE). It operates in more than 120 countries, providing the oil and gas industry with products and services for oil drilling, formation evaluation, completion, production and reservoir consulting. Baker Hughes, a GE Company has its headquarters in the legacy BHI headquarters in Houston, Texas.\n\nBHGE is the combination of many companies that have developed and introduced technology to serve the petroleum service industry. Their combined history dates back to the early 1900s. During its history, Baker Hughes has acquired and assimilated numerous oilfield pioneers including: Brown Oil Tools, CTC, EDECO, and Elder Oil Tools (completions); Milchem and Newpark (drilling fluids); EXLOG (mud logging); Eastman Christensen and Drilex (directional drilling and diamond drill bits); Teleco (measurement while drilling); Tri-State and Wilson (fishing tools and services); Aquaness, Chemlink and Petrolite (specialty chemicals), Western Atlas (seismic exploration, well logging), BJ Services Company (pressure pumping).\nThe Hughes Tool Company was founded in 1908 by business partners Walter Benona Sharp and Howard R. Hughes, Sr., father of Howard R. Hughes, Jr.. That year, Hughes, Sr. and Sharp developed the first two-cone drill bit, designed to enable rotary drilling in harder, deeper formations than was possible with earlier fishtail bits. They conducted two secret tests on a drilling rig in Goose Creek, Texas. Each time, Hughes asked the drilling crew to leave the rig floor, pulled the bit from a locked wooden box, and then his associates ran the bit into the hole. The drill pipe twisted off on the first test, but the second was extremely successful. In 1909, the Sharp & Hughes bit was granted a U.S. patent. In the same year, the partners formed the Sharp-Hughes Tool Company in Houston, Texas to manufacture the bit in a rented space measuring 20 by .\n\nAfter Walter Sharp died in 1912, Hughes purchased Sharp's half of the business. The company was renamed Hughes Tool Company in 1915, and Hughes, Jr. inherited it after his father's death in 1924. Through the 1950s and 1960s, Hughes Tool Company remained a private enterprise, owned by Hughes. While Hughes was engaged in his Hollywood and aviation enterprises, managers in Houston, such as Fred Ayers and Maynard Montrose, kept the tool company growing through technical innovation and international expansion. In 1958, the Engineering and Research Laboratory was enlarged to accommodate six laboratory sections that housed specialized instruments, such as a direct reading spectrometer and x-ray diffractometer. In 1959, Hughes introduced self-lubricating, sealed bearing rock bits. After collecting data from thousands of bit runs, Hughes introduced the first comprehensive guides to efficient drilling practices in 1960; 1964 saw the introduction of the X-Line rock bits, combining new cutting structure designs and hydraulic jets.\n\nBaker International was formed by Reuben C. Baker, who developed a Casing shoe, that revolutionized cable tool drilling. In July 1907, R.C. Baker, a 34-year-old inventor and entrepreneur in Coalinga, California, was granted a U.S. patent for a casing shoe that enabled drillers to efficiently run casing and cement it in oil wells. This innovation launched the business that would become Baker Oil Tools and Baker Hughes Incorporated. Mr. Baker had arrived in the California oilfield in 1895 with 95 cents in his pocket and dreams of making his fortune in the Los Angeles oil boom. Subsequently, he hauled oil for drillers with a team of horses and became a drilling contractor and an oil wildcatter before achieving success as an innovator in oilfield equipment. In 1928, Baker Casing Shoe Company changed its name to Baker Oil Tools, Inc., to reflect its product line of completion, cementing and fishing equipment.\n\nIn early 1956, during one of the most successful periods in the company's history, Reuben C. Baker retired as President of Baker Oil Tools. A few weeks later, he died after a brief illness at the age of 85 and was succeeded by his long-time associate Ted Sutter. Although he only had three years of formal education, Mr. Baker had been granted 150 patents. In 1965, Mr. Sutter was succeeded by E.H. \"Hubie\" Clark, who would become the first Baker Hughes chairman of the board in 1987; during its 80-year history before the Baker Hughes merger, Baker had only three chief executives.\n\nINTEQ also originally incorporated the drilling fluids division of Baker Hughes which consisted of Milpark and others. This division was called 'INTEQ drilling fluids' which provided the premier brands in oil and gas well drilling muds and wellbore cleaning fluids. In 2003, these product lines were spun off to form the separate entity of Baker Hughes Drilling Fluids (BHDF), with INTEQ continuing as the Drilling and Evaluation (D&E) company. INTEQ provides directional Drilling, MWD/LWD, surface logging (Mudlogging) and coring services.\n\nThe company's flagship brand has been the AutoTrak rotary steerable drilling system which was a pioneering directional drilling tool and has been responsible for the company's relatively strong market share in the past few years. Introduced in 1997 with Agip S.p.A., the tool is fundamentally different compared to contemporary rivals such as the PowerDrive and the GeoPilot employing the hybrid technique of \"pushing and pointing (vectoring) the bit\" rather than only \"pointing the bit\" or only \"pushing the bit\".\n\nIn 1987, Baker International acquired and merged with Hughes Tool Company to form Baker Hughes Incorporated. Shortly after in 1992 Baker Hughes acquired Christensen Diamond Products and merged it with Hughes Tool Company to form the drilling and evaluation division, Hughes Christensen.\n\nAfter the merger, Hughes Christensen introduced the AR Series, the newest antiwhirl technology capable of penetrating a much wider variety of tough formations without the catastrophic cutter fracture experienced by conventional PDC bits. AR Series bits were designed to resist bit whirl by directing load forces through low-friction gauge pads.\nBy 1995, Hughes Christensen's Gold Series PDC line increased drilling efficiency by reducing the frictional forces that can accumulate in front of the cutting edge, reducing the energy required to remove the rock. A year later patented ChipMaster PDCs, known for their efficiency and durability, were built on the success of the Eggbeater product line.\nHughes Christensen next introduced the Genesis HCM bits for steerable motors with patented EZSteer depth-of-cut control technology. This same technology was adapted to Genesis HCR bits for rotary steerable systems, such as the Baker Hughes AutoTrak rotary closed loop system. Genesis ZX PDCs followed with new Zenith cutters.\n\nIn 2000, Baker Hughes Incorporated and Schlumberger formed a joint venture called WesternGeco. The Joint venture was signed for a period of five years, and merged Baker's Western Geophysical and Schlumberger's Geco-Prakla, the two leading seismic interpretation companies of the time. Due to diminishing exploration markets, new marginal oil fields, and low barrel prices the worldwide business of seismic exploration was surviving on just the corporate strength of the two big service companies. The only new technology that was being introduced at the time was the 4-dimensional seismic survey monitoring. In 2006, Baker Hughes announced it was selling its 30% share of the WesternGeco joint venture to Schlumberger for $2.4 billion in cash.\n\nIn 2008, Baker Hughes Incorporated joined the PetroSkills Alliance. Member companies came together to create detailed skill and Competency Maps, which act as a guide for the 200+ short courses, taught to industry professionals in over 40 locations worldwide. Competency Maps are an analysis tool and software application that allows users to assess their skills base to identify gaps in their training, areas needing improvement or mastered skill areas within upstream, downstream, and HSE petroleum subject disciplines.\n\nIn 2007, Baker Hughes Incorporated pleaded guilty in U.S. federal court to violations of the Foreign Corrupt Practices Act (FCPA), including bribing oil-related industry officials in Russia, Uzbekistan, Angola, Indonesia, and Nigeria. Under the settlement, a unit of the Houston-based company pleaded guilty to violations of the Foreign Corrupt Practices Act (FCPA) for payments made between 2001 and 2003 to a commercial agent retained in 2000 in connection with a project in Kazakhstan. After bribes were paid, Baker Hughes was awarded an oil-services contract in a Karachaganak, Kazakhstan field that generated $219 million in revenues from 2001 to 2006.\n\nDuring the annual period of negotiations between trade unions and their employer counterparts in Norway 114 Baker Hughes employees were called to strike action in June 2012. Due to their key positions in the oil industry this strike by SAFE-organized Baker Hughes Incorporated employees would affect more than a dozen offshore oil installations.\n\nIn November 2014, it was announced that Baker Hughes Incorporated had entered talks with Halliburton over a merger deal valued at $34.6 Billion. If carried out, it would have been the largest merger in the history of the industry. Halliburton proposed the acquisition of Baker Hughes, which would unite two of the largest U.S. providers of oil field services. Under the terms of the transaction, Baker Hughes shareholders would have received 1.12 shares of Halliburton common stock and $19.00 in cash for each share of Baker Hughes stock they own. The merger was approved by both Companies Stockholders and was waiting on approval from several jurisdictions, such as the US Department of justice, on concerns on how it will affect the level of competition, prices and consumer welfare for the global oilfield service providing industry. Undoubtedly, the merger would have led to increased consolidation in the oil services market. Also, officials believed that the asset sale would not help the combined company retain competitiveness, as smaller buyers would not utilize their larger rivals’ assets efficiently. The merger had a deadline of the end of April 2016 after which, if a decision had not been made, both companies could walk away from the deal if they chose. At the beginning of May 2016, the day after the deadline expired, Baker Hughes and Halliburton announced the termination of the merger agreement.\n\nAt the end of October 2016, it was announced that General Electric was under negotiations for a deal valued at about $30 billion to combine GE Oil and Gas with Baker Hughes Incorporated. The transaction would create a publicly-traded entity controlled by GE. The deal was cleared by the EU in May 2017, and by the DOJ in June 2017. In December 2016, it was announced that Baker Hughes Incorporated would be dividing off its North America Land Pressure Pumping division to form a new BJ Services as part of its divestment agreement with GE. The new BJ Services company will also be formed by a merger with ALLIED Services and ALTCEM. The merger agreement was approved by shareholders at the end of June 2017.\n\nBHGE has acquired various different companies, allowing for a wider range of services to customers. These legacy companies were divided into specialised divisions, each responsible for a specific area of oilfield service expertise.\n\n\n\nIn 2009 Baker Hughes adopted a new business model that divides the company to the following business segments:\n\n\n\nIn 1929, Cicero C. Brown organized Brown Oil Tools in Houston, and patented the first liner hanger in 1937. Liner hangers enable drillers to lengthen their casing strings without having the liner pipe extend all the way to the surface. This saves capital cost and reduces weight borne by offshore platforms. Hughes Tool Company acquired Brown Oil Tools in 1978. In 1970, Baker Oil Tools acquired Lynes, Inc., which produced liner hangers and other completion equipment. In 1978, Baker Oil Tools introduced the Bakerline casing liner hanger. In 1985, the FlexLock Liner Hanger was introduced, extending the performance range and functionality of liner hanger systems. In 1987, the Brown liner hanger technology was merged into Baker Oil Tools. In 1992, BOT introduced the ZXP Liner Hanger Packer, with expandable metal seals, which set the stage for development of expandable screens, casing systems and liner hangers.\n\nIn 1994, Baker Oil Tools introduced multilateral completion systems, which enabled operators to install completion tools and perform selective intervention work in multiple horizontal sections from a common main wellbore.\n\nOn August 31, 2009, the company announced an intention to purchase BJ Services Company in a $5.5 billion stock and cash deal. Greenhill & Co. advised on the transaction. On April 28, 2010, it was announced that Baker Hughes' acquisition of BJ Services had been finalized with some conditions.\n\n\nIn 1931, Max B. Miller devised a drilling mud using a white clay as a weighting material. To market the new mud, he formed The Milwhite Company in Texas. In the mid-1930s, the company mined barites in conjunction with the Magnet Cove Barium Corporation (later called Magcobar). After a hiatus during World War II, the company resumed grinding operations using barite from a mine in Missouri and conducted mud sales through independent distributors. After 1956 Milwhite Mud Sales Company built its own sales network. In 1963 the company acquired the Aquaness chemical company, and in 1964 the combination became Milchem Incorporated. In 1971, Baker Oil Tools acquired Milchem. In 1985, Baker International acquired the drilling fluids division of Newpark Resources and merged it with Milchem's mud division to form Milpark. Meanwhile, in 1942, Oil Base Drilling Company was founded by George Miller, and made its first application of oil base mud. The company was acquired by Hughes Tool Company in 1979, and renamed Hughes Drilling Fluids in 1982. In 1987, when Baker Hughes was formed, Hughes Drilling Fluids was merged into Milpark, and in 1993, Milpark became a product line within Baker Hughes INTEQ. Baker Hughes Drilling Fluids was established as a stand-alone division in 2004.\n\n\nIn 1952, in Sacramento, California a group of Stanford University engineering and geology graduates founded Exploration Logging Company (EXLOG) to provide geologic mud logging services from mobile logging units using technical innovations in hot-wire gas detection. Vern Jones was the company's first president. EXLOG would become a world leader in surface logging, rig instrumentation and data acquisition. Baker International acquired EXLOG in 1972, and invested in its expansion. By 1982, the company had more than 200 logging units and 1,000 geologists on staff. Its broad expertise in geological services would eventually become the Surface Logging Service product line of Baker Hughes INTEQ.\n\n\nIn 1929, H. John Eastman introduced \"controlled directional drilling\" in Huntington Beach, California, using whipstocks and magnetic survey instruments to deflect the drill pipe from shore-based rigs to reach oil deposits offshore. In 1934, Mr. Eastman gained notoriety, and respect for directional drilling techniques, when he drilled the world's first relief well to control a blowout in Conroe, Texas, that had been on fire for more than a year.\n\nIn 1957, Christensen Diamond Products opened its manufacturing plant in Celle, Germany. The facility built diamond core heads and drilling bits and soon began producing stabilizers, drilling jars and other equipment. In 1977, the Celle engineering and manufacturing team introduced the Navi-Drill line of downhole drilling motors, which has led the drilling industry in performance and reliability for three decades. Other innovations developed in Celle include the industry's first steerable motor system, and the AutoTrak Rotary Closed Loop System. In 2007, the Celle Technology Center became Baker Hughes' leading research and engineering facility in the Eastern Hemisphere.\n\n\nTeleco Oilfield Services Inc was founded in 1972 and introduced the world's first MWD tool in 1978. Schlumberger introduced the LWD service in 1980.\nThe legacy MWD company Teleco Oilfield Services Inc. was integrated into a new division, to be known as Eastman Teleco in 1992. The division then merged the directional drilling products and services once marketed by the Eastman Christensen division with Teleco's measurement-while-drilling (MWD) services. In January 1992, Baker Hughes agreed to purchase Teleco from Sonat Inc. for $200 million cash, preferred stock and royalty from future sales of Teleco's \"triple combo\" sensors.\n\nBefore the acquisition, Teleco was recognized as the world's leader in MWD, with an estimated $140 million in revenues, of which about $120 million were from MWD alone. Eastman Teleco was then combined with others to form INTEQ in 1993.\n\n\nTri-State and Wilson companies were acquired by Baker Oil Tools and merged to the company's Wellbore Intervention division. Tri-State was a multi-national technology leader on products like milling operation, spears, and packer retrieving.\n\n\nWilliam Barnickel's Tret-O-Lite business in 1920 had outgrown his initial manufacturing plant, so he built a new one in Webster Groves, Missouri. The ingenious new facility had six times the capacity of the old plant and was built on a hillside so that raw materials were unloaded from a railroad line on the top of the hill, and the chemicals flowed through the plant using the force of gravity. Finished product was loaded on rail cars at the bottom of the hill. In 1922, the company sold 10,815 drums of Tret-O-Lite demulsifier, representing a recovery of of oil from produced oil/water emulsion. In 1923, Mr. Barnickel died at age of 45, of a perforated ulcer, and John S. Lehmann succeeded him as Tret-O-Lite president.\n\nMeanwhile, Frederick Cottrell and James Speed were developing electrostatic methods for separating oil from water. In 1911, Allen C. Wright formed the Petroleum Rectifying Company of California (PETRECO), which built electric dehydrating plants—based on Contrell's and Speed's inventions—to serve California oilfields. By 1922, Petreco had 417 treaters in operation, but was running into competition from Barnickel and his chemical process. In 1930, as the worldwide Depression began, the two competing companies—PETRECO and Tret-O-Lite—merged to form Petrolite\n\n\nIn 1932, Bill Lane and Walt Wells invented bullet gun perforating and formed the Lane-Wells Company in Vernon, California. They performed their first job on Union Oil's La Merced #17 well in Los Angeles. The company that would become Western Atlas (later Baker Atlas) grew quickly and added other wireline services, including the gamma ray log in 1939 and the neutron log in 1941, which were developed by Well Surveys Inc., an affiliated company. In 1948, a Lane-Wells crew performed the company's 100,000th job on La Merced #17, the site of the first perforating run.\n\nIn 1963, Baker Atlas predecessor Lane-Wells introduced the Neutron Lifetime Log service, providing the ability to detect oil through well casing, and initiating the line of Baker Atlas pulsed-neutron logging tools for cased hole logging and reservoir monitoring. It took another five years for competitors to introduce a comparable service. Beginning in 1948, Well Surveys Inc. physicist Arthur Youmans led the team of engineers and scientists to develop this technology. The highly complex instrument included a miniaturized particle generator and sensors to detect and analyze sub-atomic particles. Mr. Youmans went on to become Vice President of Research and Engineering for Dresser Atlas.\n\nIn 1968, Lane-Wells and the Pan Geo Atlas Corporation (PGAC) merged to form Dresser Atlas, a name chosen to “position” the company as more than a perforating provider and as part parent company of Dresser Industries. A competitor with Lane-Wells but possessing deeper expertise and an international reputation in open hole logging, PGAC was the perfect merger partner to form an integrated wireline services company. Since its inception, Lane-Wells had generated most of its income from perforating services, but log interpretation had narrowed down producing zones, resulting in fewer perforations and less revenue. During the oil slump of the 1999, Western Atlas was acquired by Baker Hughes Incorporated and the wireline division was created within the company rebranded as Baker Atlas. Western Geophysical was meanwhile allied with GecoPrakla of Schlumberger and later combined into a separate business entity called WesternGeco.\n\n\n"}
{"id": "44275267", "url": "https://en.wikipedia.org/wiki?curid=44275267", "title": "Bangalore", "text": "Bangalore\n\nBangalore (), officially known as Bengaluru (), is the capital of the Indian state of Karnataka. It has a population of over ten million, making it a megacity and the third most populous city and fifth most populous urban agglomeration in India. It is located in southern India on the Deccan Plateau at an elevation of over above sea level, which is the highest among India's major cities.\n\nA succession of South Indian dynasties, the Western Gangas, the Cholas and the Hoysalas, ruled the present region of Bangalore until in 1537 CE, Kempé Gowdā – a feudal ruler under the Vijayanagara Empire – established a mud fort considered to be the foundation of modern Bangalore. In 1638, the Marāthās conquered and ruled Bangalore for almost 50 years, after which the Mughals captured and sold the city to the Mysore Kingdom of the Wadiyar dynasty. It was captured by the British after victory in the Fourth Anglo-Mysore War (1799), who returned administrative control of the city to the Maharaja of Mysore. The old city developed in the dominions of the Maharaja of Mysore and was made capital of the Princely State of Mysore, which existed as a nominally sovereign entity of the British Raj.\n\nIn 1809, the British shifted their cantonment to Bangalore, outside the old city, and a town grew up around it, which was governed as part of British India. Following India's independence in 1947, Bangalore became the capital of Mysore State, and remained capital when the new Indian state of Karnataka was formed in 1956. The two urban settlements of Bangalore – city and cantonment – which had developed as independent entities merged into a single urban centre in 1949. The existing Kannada name, \"Bengalūru\", was declared the official name of the city in 2006.\n\nBangalore is sometimes referred to as the \"Silicon Valley of India\" (or \"IT capital of India\") because of its role as the nation's leading information technology (IT) exporter. Indian technological organisations ISRO, Infosys, Wipro and HAL are headquartered in the city. A demographically diverse city, Bangalore is the second fastest-growing major metropolis in India. Bangalore has one of the most highly educated workforces in the world. It is home to many educational and research institutions in India, such as Indian Institute of Science (IISc), Indian Institute of Management (Bangalore) (IIMB), International Institute of Information Technology, Bangalore (IIITB), National Institute of Fashion Technology, Bangalore, National Institute of Design, Bangalore (NID R&D Campus), National Law School of India University (NLSIU) and National Institute of Mental Health and Neurosciences (NIMHANS). Numerous state-owned aerospace and defence organisations, such as Bharat Electronics, Hindustan Aeronautics and National Aerospace Laboratories are located in the city. The city also houses the Kannada film industry.\n\nThe name \"Bangalore\" represents an anglicised version of the Kannada language name, \"Bengalūru\" ಬೆಂಗಳೂರು . It is the name of a village near Kodegehalli and was copied by Kempegowda to the city of Bangalore. The earliest reference to the name \"Bengalūru\" was found in a ninth-century Western Ganga Dynasty stone inscription on a \"\"vīra gallu\" () (literally, \"hero stone\", a rock edict extolling the virtues of a warrior). In this inscription found in Begur, \"Bengalūrū\" is referred to as a place in which a battle was fought in 890 CE. It states that the place was part of the Ganga Kingdom until 1004 and was known as \"Bengaval-uru\", the \"City of Guards\" in Halegannada (Old Kannada).\n\nAn apocryphal story recounts that the 12th century Hoysala king Veera Ballala II, while on a hunting expedition, lost his way in the forest. Tired and hungry, he came across a poor old woman who served him boiled beans. The grateful king named the place \"benda-kaal-uru\" (literally, \"town of boiled beans\"), which eventually evolved into \"Bengalūru\". Suryanath Kamath has put forward an explanation of a possible floral origin of the name, being derived from \"benga\", the Kannada term for \"Pterocarpus marsupium\" (also known as the Indian Kino Tree), a species of dry and moist deciduous trees, that grew abundantly in the region.\n\nOn 11 December 2005, the Government of Karnataka announced that it had accepted a proposal by Jnanpith Award winner U. R. Ananthamurthy to rename Bangalore to \"Bengalūru\". On 27 September 2006, the Bruhat Bangalore Mahanagara Palike (BBMP) passed a resolution to implement the proposed name change. The government of Karnataka accepted the proposal, and it was decided to officially implement the name change from 1 November 2006. The Union government approved this request, along with name changes for 11 other Karnataka cities, in October 2014, whence Bangalore was renamed to \"Bengaluru\" on 1 November 2014.\n\nA discovery of Stone Age artefacts during the 2001 census of India at Jalahalli, Sidhapura and Jadigenahalli, all of which are located on Bangalore's outskirts today, suggest probable human settlement around 4,000 BCE. Around 1,000 BCE (Iron Age), burial grounds were established at Koramangala and Chikkajala on the outskirts of Bangalore. Coins of the Roman emperors Augustus, Tiberius, and Claudius found at Yeswanthpur and HAL indicate that Bangalore was involved in trans-oceanic trade with ancient civilisations in 27 BCE.\n\nThe region of modern-day Bangalore was part of several successive South Indian kingdoms. Between the fourth and the tenth centuries, the Bangalore region was ruled by the Western Ganga Dynasty of Karnataka, the first dynasty to set up effective control over the region. According to Edgar Thurston there were twenty eight kings who ruled Gangavadi from the start of the Christian era until its conquest by the Cholas. These kings belonged to two distinct dynasties: the earlier line of the \"Solar race\" which had a succession of seven kings of the Ratti or Reddi tribe, and the later line of the Ganga race. The Western Gangas ruled the region initially as a sovereign power (350–550), and later as feudatories of the Chalukyas of Badami, followed by the Rashtrakutas until the tenth century. The Begur Nageshwara Temple was commissioned around 860, during the reign of the Western Ganga King Ereganga Nitimarga I and extended by his successor Nitimarga II. Around 1004, during the reign of Raja Raja Chola I, the Cholas defeated the Western Gangas under the command of the crown prince Rajendra Chola I, and captured Bangalore. During this period, the Bangalore region witnessed the migration of many groups — warriors, administrators, traders, artisans, pastorals, cultivators, and religious personnel from Tamil Nadu and other Kannada speaking regions. The Chokkanathaswamy temple at Domlur, the Aigandapura complex near Hesaraghatta, Mukthi Natheshwara Temple at Binnamangala, Choleshwara Temple at Begur, Someshwara Temple at Madiwala, date from the Chola era.\n\nIn 1117, the Hoysala king Vishnuvardhana defeated the Cholas in the Battle of Talakad in south Karnataka, and extended its rule over the region. Vishnuvardhana expelled the Cholas from all parts of Mysore state. By the end of the 13th century, Bangalore became a source of contention between two warring cousins, the Hoysala ruler Veera Ballala III of Halebidu and Ramanatha, who administered from the Hoysala held territory in Tamil Nadu. Veera Ballala III had appointed a civic head at Hudi (now within Bangalore Municipal Corporation limits), thus promoting the village to the status of a town. After Veera Ballala III's death in 1343, the next empire to rule the region was the Vijayanagara Empire, which itself saw the rise of four dynasties, the Sangamas (1336–1485), the Saluvas (1485–1491), the Tuluvas (1491–1565), and the Aravidu (1565–1646). During the reign of the Vijayanagara Empire, Achyuta Deva Raya of the Tuluva Dynasty raised the Shivasamudra Dam across the Arkavati river at Hesaraghatta, whose reservoir is the present city's supply of regular piped water.\n\nModern Bangalore was begun in 1537 by a vassal of the Vijayanagara Empire, Kempe Gowda I, who aligned with the Vijayanagara empire to campaign against Gangaraja (whom he defeated and expelled to Kanchi), and who built a mud-brick fort for the people at the site that would become the central part of modern Bangalore. Kempe Gowda was restricted by rules made by Achuta Deva Raya, who feared the potential power of Kempe Gowda and did not allow a formidable stone fort. Kempe Gowda referred to the new town as his \"gandubhūmi\" or \"Land of Heroes\". Within the fort, the town was divided into smaller divisions—each called a \"pete\" (). The town had two main streets—Chikkapeté Street, which ran east-west, and Doddapeté Street, which ran north-south. Their intersection formed the Doddapeté Square—the heart of Bangalore. Kempe Gowda I's successor, Kempe Gowda II, built four towers that marked Bangalore's boundary. During the Vijayanagara rule, many saints and poets referred to Bangalore as \"Devarāyanagara\" and \"Kalyānapura\" or \"Kalyānapuri\" (\"Auspicious City\").\n\nAfter the fall of the Vijayanagara Empire in 1565 in the Battle of Talikota, Bangalore's rule changed hands several times. Kempe Gowda declared independence, then in 1638, a large Adil Shahi Bijapur army led by Ranadulla Khan and accompanied by his second in command Shāhji Bhōnslé defeated Kempe Gowda III, and Bangalore was given to Shāhji as a \"jagir\" (feudal estate). In 1687, the Mughal general Kasim Khan, under orders from Aurangzeb, defeated Ekoji I, son of Shāhji, and sold Bangalore to Chikkadevaraja Wodeyar (1673–1704), the then ruler of the Kingdom of Mysore for three lakh rupees. After the death of Krishnaraja Wodeyar II in 1759, Hyder Ali, Commander-in-Chief of the Mysore Army, proclaimed himself the \"de facto\" ruler of the Kingdom of Mysore. Hyder Ali is credited with building the Delhi and Mysore gates at the northern and southern ends of the city in 1760. The kingdom later passed to Hyder Ali's son Tipu Sultan. Hyder and Tipu contributed towards the beautification of the city by building Lal Bagh Botanical Gardens in 1760. Under them, Bangalore developed into a commercial and military centre of strategic importance.\n\nThe Bangalore fort was captured by the British armies under Lord Cornwallis on 21 March 1791 during the Third Anglo-Mysore War and formed a centre for British resistance against Tipu Sultan. Following Tipu's death in the Fourth Anglo-Mysore War (1799), the British returned administrative control of the Bangalore \"pētē\" to the Maharaja of Mysore and was incorporated into the Princely State of Mysore, which existed as a nominally sovereign entity of the British Raj. The old city (\"pētē\") developed in the dominions of the Maharaja of Mysore. The Residency of Mysore State was first established in Mysore City in 1799 and later shifted to Bangalore in 1804. It was abolished in 1843 only to be revived in 1881 at Bangalore and to be closed down permanently in 1947, with Indian independence. The British found Bangalore to be a pleasant and appropriate place to station their garrison and therefore moved their cantonment to Bangalore from Seringapatam in 1809 near Ulsoor, about north-east of the city. A town grew up around the cantonment, by absorbing several villages in the area. The new centre had its own municipal and administrative apparatus, though technically it was a British enclave within the territory of the Wodeyar Kings of the Princely State of Mysore. Two important developments which contributed to the rapid growth of the city, include the introduction of telegraph connections to all major Indian cities in 1853 and a rail connection to Madras, in 1864.\n\nIn the 19th century, Bangalore essentially became a twin city, with the \"pētē\", whose residents were predominantly Kannadigas and the cantonment created by the British. Throughout the 19th century, the Cantonment gradually expanded and acquired a distinct cultural and political salience as it was governed directly by the British and was known as the Civil and Military Station of Bangalore. While it remained in the princely territory of Mysore, Cantonment had a large military presence and a cosmopolitan civilian population that came from outside the princely state of Mysore, including British and Anglo-Indians army officers.\n\nBangalore was hit by a plague epidemic in 1898 that claimed nearly 3,500 lives. The crisis caused by the outbreak catalysed the city's sanitation process. Telephone lines were laid to help co-ordinate anti-plague operations. Regulations for building new houses with proper sanitation facilities came into effect. A health officer was appointed and the city divided into four wards for better co-ordination. Victoria Hospital was inaugurated in 1900 by Lord Curzon, the then Governor-General of British India. New extensions in Malleswaram and Basavanagudi were developed in the north and south of the pētē. In 1903, motor vehicles came to be introduced in Bangalore. In 1906, Bangalore became one of the first cities in India to have electricity from hydro power, powered by the hydroelectric plant situated in Shivanasamudra. The Indian Institute of Science was established in 1909, which subsequently played a major role in developing the city as a science research hub. In 1912, the Bangalore torpedo, a defensive explosive weapon widely used in World War I and World War II, was devised in Bangalore by British army officer Captain McClintock of the Madras Sappers and Miners.\n\nBangalore's reputation as the \"Garden City of India\" began in 1927 with the Silver Jubilee celebrations of the rule of Krishnaraja Wodeyar IV. Several projects such as the construction of parks, public buildings and hospitals were instituted to improve the city. Bangalore played an important role during the Indian independence movement. Mahatma Gandhi visited the city in 1927 and 1934 and addressed public meetings here. In 1926, the labour unrest in Binny Mills due to demand by textile workers for payment of bonus resulted in lathi charging and police firing, resulting in the death of four workers, and several injuries. In July 1928, there were notable communal disturbances in Bangalore, when a Ganesh idol was removed from a school compound in the Sultanpet area of Bangalore. In 1940, the first flight between Bangalore and Bombay took off, which placed the city on India's urban map.\n\nAfter India's independence in August 1947, Bangalore remained in the newly carved Mysore State of which the Maharaja of Mysore was the \"Rajapramukh\" (appointed governor). The \"City Improvement Trust\" was formed in 1945, and in 1949, the \"City\" and the \"Cantonment\" merged to form the Bangalore City Corporation. The Government of Karnataka later constituted the Bangalore Development Authority in 1976 to co-ordinate the activities of these two bodies. Public sector employment and education provided opportunities for Kannadigas from the rest of the state to migrate to the city. Bangalore experienced rapid growth in the decades 1941–51 and 1971–81, which saw the arrival of many immigrants from northern Karnataka. By 1961, Bangalore had become the sixth largest city in India, with a population of 1,207,000. In the decades that followed, Bangalore's manufacturing base continued to expand with the establishment of private companies such as MICO (Motor Industries Company), which set up its manufacturing plant in the city.\n\nBy the 1980s, it was clear that urbanisation had spilled over the current boundaries, and in 1986, the Bangalore Metropolitan Region Development Authority, was established to co-ordinate the development of the entire region as a single unit. On 8 February 1981, a major fire broke out at Venus Circus in Bangalore, where more than 92 lives were lost, the majority of them being children. Bangalore experienced a growth in its real estate market in the 1980s and 1990s, spurred by capital investors from other parts of the country who converted Bangalore's large plots and colonial bungalows into multi-storied apartments. In 1985, Texas Instruments became the first multinational corporation to set up base in Bangalore. Other information technology companies followed suit and by the end of the 20th century, Bangalore had established itself as the \"Silicon Valley of India\". Today, Bangalore is India's third most populous city. During the 21st century, Bangalore has suffered terrorist attacks in 2008, 2010, and 2013.\n\nBangalore lies in the southeast of the South Indian state of Karnataka. It is in the heart of the Mysore Plateau (a region of the larger Precambrian Deccan Plateau) at an average elevation of . It is located at and covers an area of . The majority of the city of Bangalore lies in the Bangalore Urban district of Karnataka and the surrounding rural areas are a part of the Bangalore Rural district. The Government of Karnataka has carved out the new district of Ramanagara from the old Bangalore Rural district.\n\nThe topology of Bangalore is generally flat, though the western parts of the city are hilly. The highest point is Vidyaranyapura Doddabettahalli, which is and is situated to the north-west of the city. No major rivers run through the city, although the Arkavathi and South Pennar cross paths at the Nandi Hills, to the north. River Vrishabhavathi, a minor tributary of the Arkavathi, arises within the city at Basavanagudi and flows through the city. The rivers Arkavathi and Vrishabhavathi together carry much of Bangalore's sewage. A sewerage system, constructed in 1922, covers of the city and connects with five sewage treatment centres located in the periphery of Bangalore.\n\nIn the 16th century, Kempe Gowda I constructed many lakes to meet the town's water requirements. The Kempambudhi Kere, since overrun by modern development, was prominent among those lakes. In the earlier half of 20th century, the Nandi Hills waterworks was commissioned by Sir Mirza Ismail (Diwan of Mysore, 1926–41 CE) to provide a water supply to the city. Currently, the river Kaveri provides around 80% of the total water supply to the city with the remaining 20% being obtained from the Thippagondanahalli and Hesaraghatta reservoirs of the Arkavathi river. Bangalore receives 800 million litres (211 million US gallons) of water a day, more than any other Indian city. However, Bangalore sometimes does face water shortages, especially during summer- more so in the years of low rainfall. A random sampling study of the Air Quality Index (AQI) of twenty stations within the city indicated scores that ranged from 76 to 314, suggesting heavy to severe air pollution around areas of traffic concentration.\n\nBangalore has a handful of freshwater lakes and water tanks, the largest of which are Madivala tank, Hebbal lake, Ulsoor lake, Yediyur Lake and Sankey Tank. Groundwater occurs in silty to sandy layers of the alluvial sediments. The Peninsular Gneissic Complex (PGC) is the most dominant rock unit in the area and includes granites, gneisses and migmatites, while the soils of Bangalore consist of red laterite and red, fine loamy to clayey soils.\n\nVegetation in the city is primarily in the form of large deciduous canopy and minority coconut trees. Though Bangalore has been classified as a part of the seismic zone II (a stable zone), it has experienced quakes of magnitude as high as 4.5.\n\nBangalore has a tropical savanna climate (Köppen climate classification \"Aw\") with distinct wet and dry seasons. Due to its high elevation, Bangalore usually enjoys a more moderate climate throughout the year, although occasional heat waves can make summer somewhat uncomfortable. The coolest month is January with an average low temperature of and the hottest month is April with an average high temperature of . The highest temperature ever recorded in Bangalore is (recorded on 24 April 2016) as there was a strong El Nino in 2016 There were also unofficial records of on that day. The lowest ever recorded is in January 1884. Winter temperatures rarely drop below , and summer temperatures seldom exceed . Bangalore receives rainfall from both the northeast and the southwest monsoons and the wettest months are September, October and August, in that order. The summer heat is moderated by fairly frequent thunderstorms, which occasionally cause power outages and local flooding. Most of the rainfall occurs during late afternoon/evening or night and rain before noon is infrequent. November 2015 (290.4 mm) was recorded as one of the wettest months in Bangalore with heavy rains causing severe flooding in some areas, and closure of a number of organisations for over a couple of days.\nThe heaviest rainfall recorded in a 24-hour period is recorded on 1 October 1997.\n\nWith a population estimated to be between 10,456,000 and 12,339,000,\nup from 8.5 million at the 2011 census, Bangalore is a megacity, and the third most populous city in India and the 18th most populous city in the world. Bangalore was the fastest-growing Indian metropolis after New Delhi between 1991 and 2001, with a growth rate of 38% during the decade. Residents of Bangalore are referred to as \"Bangaloreans\" in English and \"Bengaloorinavaru or Bengaloorigaru\" in Kannada. People from other states have migrated to Bangalore.\n\nAccording to the 2011 census of India, 78.9% of Bangalore's population is Hindu, a little less than the national average. Muslims comprise 13.9% of the population, roughly the same as their national average. Christians and Jains account for 5.6% and 1.0% of the population, respectively, double that of their national averages. The city has a literacy rate of 89%. Roughly 10% of Bangalore's population lives in slums.—a relatively low proportion when compared to other cities in the developing world such as Mumbai (50%) and Nairobi (60%). The 2008 National Crime Records Bureau statistics indicate that Bangalore accounts for 8.5% of the total crimes reported from 35 major cities in India which is an increase in the crime rate when compared to the number of crimes fifteen years ago.\n\nBangalore suffers from the same major urbanisation problems seen in many fast-growing cities in developing countries: rapidly escalating social inequality, mass displacement and dispossession, proliferation of slum settlements, and epidemic public health crisis due to severe water shortage and sewage problems in poor and working-class neighbourhoods.\n\nBangalore is a multilingual city. The official language in Bangalore is Kannada. Other languages such as English, Telugu, Tamil, Hindi, Malayalam, Urdu are also spoken widely. The Kannada language spoken in Bangalore is a form of Kannada called as 'Old Mysuru Kannada' which is also used in most of the southern part of Karnataka state. A vernacular dialect of this, known as Bangalore Kannada, is spoken among the youth in Bangalore and the adjoining Mysore regions. English (as an Indian dialect) is extensively spoken and is the principal language of the professional and business class.\n\nThe major communities of Bangalore who share a long history in the city other than the Kannadigas are the Telugus and Tamilians, who migrated to Bangalore in search of a better livelihood. Already in the 16th century, Bangalore had few speakers of Tamil and Telugu, who spoke Kannada to carry out low profile jobs. However the Telugu Speaking Morasu Vokkaligas are the Native people of Bangalore Telugu-speaking people initially came to Bangalore on invitation by the Mysore royalty (a few of them have lineage dating back to Krishnadevaraya).\n\nOther native communities are the Tuluvas and the Konkanis of coastal Karnataka, the Kodavas of the Kodagu district of Karnataka. The migrant communities are Maharashtrians, Punjabis, Rajasthanis, Gujaratis, Tamilians, Telugus, Malayali, Odias, Sindhis, and Bengalis. Bangalore once had a large Anglo-Indian population, the second largest after Calcutta. Today, there are around 10,000 Anglo-Indians in Bangalore. Christians form a sizeable section of Bangalorean society, with migrant forming the majority of the Christian population, while Kannada Catholics, Mangalorean Catholics, Syro-Malabar Nasranis and others form the rest of the population. Muslims form a very diverse population, consisting of Dakhini and Urdu-speaking Muslims, Kutchi Memons, Labbay and Mappilas.\n\nAccording to 2011 census following is the distribution of Bangalore district people as per their mother tongue.\n\nKannada 45.47%, Tamil 14.2%, Telugu 13.99%, Urdu 12.11%, Hindi 5.51%, Malayalam 2.94%, Marathi 1.92%, Konkani 0.67%, Bengali 0.64%, Oriya 0.52%, Tulu 0.49%, Gujarati 0.47%.\n\nThe Bruhat Bangalore Mahanagara Palike (BBMP, \"Greater Bangalore Municipal Corporation\") is in charge of the civic administration of the city. It was formed in 2007 by merging 100 wards of the erstwhile \"Bangalore Mahanagara Palike\", with seven neighbouring City Municipal Councils, one Town Municipal Council and 110 villages around Bangalore. The number of wards increased to 198 in 2009. The BBMP is run by a city council composed of 250 members, including 198 corporators representing each of the wards of the city and 52 other elected representatives, consisting of members of Parliament and the state legislature. Elections to the council are held once every five years, with results being decided by popular vote. Members contesting elections to the council usually represent one or more of the state's political parties. A mayor and deputy mayor are also elected from among the elected members of the council. Elections to the BBMP were held on 28 March 2010, after a gap of three and a half years since the expiry of the previous elected body's term, and the Bharatiya Janata Party was voted into power – the first time it had ever won a civic poll in the city. Indian National Congress councillor Sampath Raj became the city's mayor in September 2017, the vote having been boycotted by the BJP. In September 2018, Indian National Congress councillor Gangambike Mallikarjun was elected as the mayor of Bengaluru and took charge from the outgoing Mayor, Sampath Raj.\n\nBangalore's rapid growth has created several problems relating to traffic congestion and infrastructural obsolescence that the Bangalore Mahanagara Palike has found challenging to address. The unplanned nature of growth in the city resulted in massive traffic gridlocks that the municipality attempted to ease by constructing a flyover system and by imposing one-way traffic systems. Some of the flyovers and one-ways mitigated the traffic situation moderately but were unable to adequately address the disproportionate growth of city traffic. A 2003 \"Battelle Environmental Evaluation System\" (BEES) evaluation of Bangalore's physical, biological and socioeconomic parameters indicated that Bangalore's water quality and terrestrial and aquatic ecosystems were close to \"ideal\", while the city's socioeconomic parameters (traffic, quality of life) aire quality and noise pollution scored poorly. The BBMP works in conjunction with the Bangalore Development Authority (BDA) and the Agenda for Bangalore's Infrastructure and Development Task Force (ABIDe) to design and implement civic and infrastructural projects.\n\nThe Bangalore City Police (BCP) has seven geographic zones, includes the Traffic Police, the City Armed Reserve, the Central Crime Branch and the City Crime Record Bureau and runs 86 police stations, including two all-women police stations. As capital of the state of Karnataka, Bangalore houses important state government facilities such as the Karnataka High Court, the Vidhana Soudha (the home of the Karnataka state legislature) and Raj Bhavan (the residence of the Governor of Karnataka). Bangalore contributes four members to the lower house of the Indian Parliament, the \"Lok Sabha\", from its four constituencies: Bangalore Rural, Bangalore Central, Bangalore North, and Bangalore South, and 28 members to the Karnataka Legislative Assembly.\n\nElectricity in Bangalore is regulated through the Bangalore Electricity Supply Company (BESCOM), while water supply and sanitation facilities are provided by the Bangalore Water Supply and Sewerage Board (BWSSB).\n\nThe city has offices of the Consulate General of Germany, France, Japan Israel, British Deputy High Commission, along with honorary consulates of Ireland, Finland, Switzerland, Maldives, Mongolia, Sri Lanka and Peru. It also has a trade office of Canada and a virtual Consulate of the United States.\n\nBangalore generates about 3,000 tonnes of solid waste per day, of which about 1,139 tonnes are collected and sent to composting units such as the Karnataka Composting Development Corporation. The remaining solid waste collected by the municipality is dumped in open spaces or on roadsides outside the city. In 2008, Bangalore produced around 2,500 metric tonnes of solid waste, and increased to 5000 metric tonnes in 2012, which is transported from collection units located near Hesaraghatta Lake, to the garbage dumping sites. The city suffers significantly with dust pollution, hazardous waste disposal, and disorganised, unscientific waste retrievals. The IT hub, Whitefield region is the most polluted area in Bangalore. Recently a study found that over 36% of diesel vehicles in the city exceed the national limit for emissions.\n\nAccording to a 2012 report submitted to the World Bank by Karnataka Slum Clearance Board, Bangalore had 862 slums from total of around 2000 slums in Karnataka. The families living in the slum were not ready to move into the temporary shelters. 42% of the households migrated from different parts of India like Chennai, Hyderabad and most of North India, and 43% of the households had remained in the slums for over 10 years. The Karnataka Municipality, works to shift 300 families annually to newly constructed buildings. One-third of these slum clearance projects lacked basic service connections, 60% of slum dwellers lacked complete water supply lines and shared BWSSB water supply.\n\nΙn 2012 Bangalore generated 2.1 million tonnes of Municipal Solid Waste (195.4 kg/cap/yr). The waste management scenario in the state of Karnataka is regulated by the Karnataka State Pollution Control Board (KSPCB) under the aegis of the Central Pollution Control Board (CPCB) which is a Central Government entity. As part of their Waste Management Guidelines the Government of Karnataka through the Karnataka State Pollution Control Board (KSPCB) has authorised a few well-established companies to manage the biomedical waste and hazardous waste in the state of Karnataka.\n\nRecent estimates of the economy of Bangalore's metropolitan area have ranged from $45 to $83 billion (PPP GDP), and have ranked it either fourth- or fifth-most productive metro area of India. The value of city's exports totalling in 2004–05. With an economic growth of 10.3%, Bangalore is the second fastest-growing major metropolis in India, and is also the country's fourth largest fast-moving consumer goods (FMCG) market. \"Forbes\" considers Bangalore one of \"The Next Decade's Fastest-Growing Cities\". The city is the third largest hub for high-net-worth individuals and is home to over 10,000-dollar millionaires and about 60,000 super-rich people who have an investment surplus of and respectively.\n\nThe headquarters of several public sector undertakings such as Bharat Electronics Limited (BEL), Hindustan Aeronautics Limited (HAL), National Aerospace Laboratories (NAL), Bharat Heavy Electricals Limited (BHEL), Bharat Earth Movers Limited (BEML), Central Manufacturing Technology Institute (CMTI) and HMT (formerly Hindustan Machine Tools) are located in Bangalore. In June 1972 the Indian Space Research Organisation (ISRO) was established under the Department of Space and headquartered in the city. Bangalore also houses several research and development centres for many firms such as ABB, Airbus, Bosch, Boeing, GE, GM, Google, Liebherr-Aerospace, Microsoft, Mercedes-Benz, Nokia, Oracle, Philips, Shell, Toyota and Tyco.\n\nBangalore is called as the \"Silicon Valley of India\" because of the large number of information technology companies located in the city which contributed 33% of India's IT exports in 2006–07. Bangalore's IT industry is divided into three main clusters – Software Technology Parks of India (STPI); International Tech Park, Bangalore (ITPB); and Electronics City. UB City, the headquarters of the United Breweries Group, is a high-end commercial zone. Infosys and Wipro, India's third and fourth largest software companies are headquartered in Bangalore, as are many of the global \"SEI-CMM Level 5 Companies\".\n\nThe growth of IT has presented the city with unique challenges. Ideological clashes sometimes occur between the city's IT moguls, who demand an improvement in the city's infrastructure, and the state government, whose electoral base is primarily the people in rural Karnataka. The encouragement of high-tech industry in Bangalore, for example, has not favoured local employment development, but has instead increased land values and forced out small enterprise. The state has also resisted the massive investments required to reverse the rapid decline in city transport which has already begun to drive new and expanding businesses to other centres across India. Bangalore is a hub for biotechnology related industry in India and in the year 2005, around 47% of the 265 biotechnology companies in India were located here; including Biocon, India's largest biotechnology company.\n\nBangalore is served by Kempegowda International Airport , located at Devanahalli, about from the city centre. It was formerly called Bangalore International Airport. The airport started operations from 24 May 2008 and is a private airport managed by a consortium led by the GVK Group. The city was earlier served by the HAL Airport at Vimanapura, a residential locality in the eastern part of the city. The airport is third busiest in India after Delhi and Mumbai in terms of passenger traffic and the number of air traffic movements (ATMs). Taxis and air conditioned Volvo buses operated by BMTC connect the airport with the city.\n\nA rapid transit system called the \"Namma Metro\" is being built in stages. Initially opened with the stretch from Baiyappanahalli to MG Road in 2011, phase 1 covering a distance of for the North-South and East-West lines was made operational by June 2017. Phase 2 of the metro covering is under construction and includes two new lines along with the extension of the existing North-South and East-West lines. There are also plans to extend the North-South line to the airport, covering a distance of . It is expected to be operational by 2021.\n\nBangalore is a divisional headquarters in the South Western Railway zone of the Indian Railways. There are four major railway stations in the city: \"Krantiveera Sangolli Rayanna Railway Station\", Bangalore Cantonment railway station, Yeshwantapur junction and Krishnarajapuram railway station, with railway lines towards Jolarpettai in the east, Chikballapur in the north-east, Guntakal in the north, Tumkur in the northwest, Hassan in the west, Mysore in the southwest and Salem in the south. There is also a railway line from Baiyappanahalli to Vimanapura which is no more in use. Though Bangalore has no commuter rail at present, there have been demands for a suburban rail service keeping in mind the large number of employees working in the IT corridor areas of Whitefield, Outer Ring Road and Electronics City.\n\nThe Rail Wheel Factory is Asia's second largest manufacturer of wheel and axle for railways and is headquartered in Yelahanka, Bangalore.\n\nBuses operated by Bangalore Metropolitan Transport Corporation (BMTC) are an important and reliable means of public transport available in the city. While commuters can buy tickets on boarding these buses, BMTC also provides an option of a bus pass to frequent users. BMTC runs air-conditioned luxury buses on major routes, and also operates shuttle services from various parts of the city to Kempegowda International Airport . The BMTC also has a mobile app that provides real-time location of a bus using the global positioning system of the user's mobile device. The Karnataka State Road Transport Corporation operates 6,918 buses on 6,352 schedules, connecting Bangalore with other parts of Karnataka as well as other neighbouring states. The main bus depots that KSRTC maintains are the Kempegowda Bus Station, locally known as \"Majestic bus stand\", where most of the out station buses ply from. Some of the KSRTC buses to Tamil Nadu, Telangana and Andhra Pradesh ply from Shantinagar Bus Station, Satellite Bus Station at Mysore road and Baiyappanahalli satellite bus station. BMTC and KSRTC were the first operators in India to introduce Volvo city buses and intracity coaches in India.\nThree-wheeled, yellow and black or yellow and green auto-rickshaws, referred to as \"autos\", are a popular form of transport. They are metered and can accommodate up to three passengers. Taxis, commonly called \"City Taxis\", are usually available too, but they are only available on call or by online based services. Taxis are metered and are generally more expensive than auto-rickshaws.\n\nThere are currently 1,250 vehicles being registered daily on an average in Bangalore RTOs. The total number of vehicles as on date are 44 lakh vehicles, with a road length of .\nBangalore is known as the \"Garden City of India\" because of its greenery, broad streets and the presence of many public parks, such as Lal Bagh and Cubbon Park. Bangalore is sometimes called as the \"Pub Capital of India\" and the \"Rock/Metal Capital of India\" because of its underground music scene and it is one of the premier places to hold international rock concerts. In May 2012, Lonely Planet ranked Bangalore 3rd among the world's top 10 cities to visit.\n\nBangalore is also home to many vegan-friendly restaurants and vegan activism groups, and has been named as India's most vegan-friendly city by PETA India.\n\nBiannual flower shows are held at the Lal Bagh Gardens during the week of Republic Day (26 January) and Independence Day (15 August). Bangalore Karaga or \"Karaga Shaktyotsava\" is one of the most important and oldest festivals of Bangalore dedicated to the Hindu Goddess Draupadi. It is celebrated annually by the Thigala community, over a period of nine days in the month of March or April. The Someshwara Car festival is an annual procession of the idol of the Halasuru Someshwara Temple (Ulsoor) led by the Vokkaligas, a major land holding community in the southern Karnataka, occurring in April. Karnataka Rajyotsava is widely celebrated on 1 November and is a public holiday in the city, to mark the formation of Karnataka state on 1 November 1956. Other popular festivals in Bangalore are Ugadi, Ram Navami, Eid ul-Fitr, Ganesh Chaturthi, St. Mary's feast, Dasara, Deepawali and Christmas.\n\nThe diversity of cuisine is reflective of the social and economic diversity of Bangalore. Bangalore has a wide and varied mix of restaurant types and cuisines and Bangaloreans deem eating out as an intrinsic part of their culture. Roadside vendors, tea stalls, and South Indian, North Indian, Chinese and Western fast food are all very popular in the city. Udupi restaurants are very popular and serve predominantly vegetarian, regional cuisine.\n\nBangalore did not have an effective contemporary art representation, as compared to Delhi and Mumbai, until recently during the 1990s, several art galleries sprang up, notable being the government established National Gallery of Modern Art. Bangalore's international art festival, \"Art Bangalore\", was established in 2010.\n\nKannada literature appears to have flourished in Bangalore even before Kempe Gowda laid the foundations of the city. During the 18th and 19th centuries, Kannada literature was enriched by the \"Vachanas\" (a form of rhythmic writing) composed by the heads of the Veerashaiva Mathas (monastery) in Bangalore. As a cosmopolitan city, Bangalore has also encouraged the growth of Telugu, Urdu, and English literatures. The headquarters of the Kannada Sahitya Parishat, a nonprofit organisation that promotes the Kannada language, is located in Bangalore. The city has its own literary festival, known as the \"Bangalore Literature Festival\", which was inaugurated in 2012.\n\nThe cartoon gallery is located in the heart of Bangalore, dedicated to the art of cartooning, is the first of its kind in India. Every month the gallery is conducting fresh cartoon exhibition of various professional as well as amateur cartoonist. The gallery has been organised by the Indian Institute of Cartoonists based in Bangalore that serves to promote and preserve the work of eminent cartoonists in India. the Institute has organised more than one hundred exhibitions of cartoons.\n\nBangalore is home to the Kannada film industry, which churns out about 80 Kannada movies each year. Bangalore also has a very active and vibrant theatre culture with popular theatres being Ravindra Kalakshetra and the more recently opened Ranga Shankara The city has a vibrant English and foreign language theatre scene with places like Ranga Shankara and Chowdiah Memorial Hall leading the way in hosting performances leading to the establishment of the Amateur film industry.\n\nKannada theatre is very popular in Bangalore, and consists mostly of political satire and light comedy. Plays are organised mostly by community organisations, but there are some amateur groups which stage plays in Kannada. Drama companies touring India under the auspicies of the British Council and Max Müller Bhavan also stage performances in the city frequently. The Alliance Française de Bangalore also hosts numerous plays through the year.\n\nBangalore is also a major centre of Indian classical music and dance. The cultural scene is very diverse due to Bangalore's mixed ethnic groups, which is reflected in its music concerts, dance performances and plays. Performances of Carnatic (South Indian) and Hindustani (North Indian) classical music, and dance forms like Bharat Natyam, Kuchipudi, Kathakali, Kathak, and Odissi are very popular. Yakshagana, a theatre art indigenous to coastal Karnataka is often played in town halls. The two main music seasons in Bangalore are in April–May during the Ram Navami festival, and in September–October during the Dusshera festival, when music activities by cultural organisations are at their peak. Though both classical and contemporary music are played in Bangalore, the dominant music genre in urban Bangalore is rock music. Bangalore has its own subgenre of music, \"Bangalore Rock\", which is an amalgamation of classic rock, hard rock and heavy metal, with a bit of jazz and blues in it. Notable bands from Bangalore include Raghu Dixit Project, Kryptos, Inner Sanctum, Agam, All the fat children, and Swaratma.\n\nThe city hosted the Miss World 1996 beauty pageant.\n\nUntil the early 19th century, education in Bangalore was mainly run by religious leaders and restricted to students of that religion. The western system of education was introduced during the rule of Mummadi Krishnaraja Wodeyar. Subsequently, the British Wesleyan Mission established the first English school in 1832 known as Wesleyan Canarese School. The fathers of the Paris Foreign Missions established the St. Joseph's European School in 1858. The Bangalore High School was started by the Mysore Government in 1858 and the Bishop Cotton Boys' School was started in 1865. In 1945 when World War II came to an end, King George Royal Indian Military Colleges was started at Bangalore by King George VI; the school is popularly known as Bangalore Military School\n\nIn post-independent India, schools for young children (16 months–5 years) are called nursery, kindergarten or play school which are broadly based on Montessori or multiple intelligence methodology of education. Primary and secondary education in Bangalore is offered by various schools which are affiliated to one of the boards of education, such as the Secondary School Leaving Certificate (SSLC), Indian Certificate of Secondary Education (ICSE), Central Board for Secondary Education (CBSE), International Baccalaureate (IB), International General Certificate of Secondary Education (IGCSE) and National Institute of Open Schooling (NIOS). Schools in Bangalore are either government run or are private (both aided and un-aided by the government). Bangalore has a significant number of international schools due to expats and IT crowd. After completing their secondary education, students either attend Pre University (PUC) or continue High School in one of three \"streams\" – Arts, Commerce or Science. Alternatively, students may also enroll in Diploma courses. Upon completing the required coursework, students enroll in general or professional degrees in universities through lateral entry.\n\nBelow are some of the historical schools in Bangalore and their year of establishment.\n\nBangalore University, established in 1886, provides affiliation to over 500 colleges, with a total student enrolment exceeding 300,000. The university has two campuses within Bangalore – Jnanabharathi and Central College. University Visvesvaraya College of Engineering was established in the year 1917, by Sir M. Visvesvaraya, At present, the UVCE is the only engineering college under the Bangalore University. Bangalore also has many private Engineering Colleges affiliated to Visvesvaraya Technological University.\n\nSome of the institutes in Bangalore are:\n\nSome of the professional institutes located in Bangalore are \n\nPrivate universities in Bangalore include institutes like Christ University, Jain University, PES University and M. S. Ramaiah University of Applied Sciences.\nBangalore medical colleges include St. John's Medical College (SJMC) and Bangalore Medical College and Research Institute (BMCRI). The M. P. Birla Institute of Fundamental Research has a branch located in Bangalore.\n\nThe first printing press in Bangalore was established in 1840 in Kannada by the Wesleyan Christian Mission. In 1859, \"Bangalore Herald\" became the first English bi-weekly newspaper to be published in Bangalore and in 1860, \"Mysore Vrittanta Bodhini\" became the first Kannada newspaper to be circulated in Bangalore. Currently, \"Vijaya Karnataka\" and \"The Times of India\" are the most widely circulated Kannada and English newspapers in Bangalore respectively, closely followed by the \"Prajavani\" and \"Deccan Herald\" both owned by the Printers (Mysore) Limited – the largest print media house in Karnataka. Other circulated newspapers are \"Vijayvani\",\"Vishwavani\",\"Kannadaprabha\",\"Sanjevani\", \"Bangalore Mirror\",\"Udayavani\" provide localised news updates. On the web, Explocity provides listings information in Bangalore.\n\nBangalore got its first radio station when All India Radio, the official broadcaster for the Indian Government, started broadcasting from its Bangalore station on 2 November 1955. The radio transmission was AM, until in 2001, Radio City became the first private channel in India to start transmitting FM radio from Bangalore. In recent years, a number of FM channels have started broadcasting from Bangalore. The city probably has India's oldest Amateur (Ham) Radio Club – Bangalore Amateur Radio Club (VU2ARC), which was established in 1959.\n\nBangalore got its first look at television when Doordarshan established a relay centre here and started relaying programs from 1 November 1981. A production centre was established in the Doordarshan's Bangalore office in 1983, thereby allowing the introduction of a news program in Kannada on 19 November 1983. Doordarshan also launched a Kannada satellite channel on 15 August 1991 which is now named DD Chandana. The advent of private satellite channels in Bangalore started in September 1991 when Star TV started to broadcast its channels. Though the number of satellite TV channels available for viewing in Bangalore has grown over the years, the cable operators play a major role in the availability of these channels, which has led to occasional conflicts. Direct To Home (DTH) services are also available in Bangalore now.\n\nThe first Internet service provider in Bangalore was STPI, Bangalore which started offering internet services in early 1990s. This Internet service was, however, restricted to corporates until VSNL started offering dial-up internet services to the general public at the end of 1995. Currently, Bangalore has the largest number of broadband Internet connections in India.\n\nNamma Wifi is a free municipal wireless network in Bangalore, the first free Wifi in India. It began operation on 24 January 2014. Service is available at M.G. Road, Brigade Road, and other locations. The service is operated by D-VoiS and is paid for by the State Government. Bangalore was the first city in India to have the 4th Generation Network (4G) for Mobile.\n\nCricket and football are by far the most popular sports in the city. Bangalore has many parks and gardens that provide excellent pitches for impromptu games. A significant number of national cricketers have come from Bangalore, including former captains Rahul Dravid and Anil Kumble. Some of the other notable players from the city who have represented India include Gundappa Vishwanath, Syed Kirmani, E. A. S. Prasanna, B. S. Chandrasekhar, Roger Binny, Venkatesh Prasad, Sunil Joshi, Robin Uthappa, Vinay Kumar, KL Rahul, Karun Nair, Brijesh Patel and Stuart Binny. Bangalore's international cricket stadium is the M. Chinnaswamy Stadium, which has a seating capacity of 55,000 and has hosted matches during the 1987 Cricket World Cup, 1996 Cricket World Cup and the 2011 Cricket World Cup. The Chinnaswamy Stadium is the home of India's National Cricket Academy.\n\nThe Indian Premier League franchise Royal Challengers Bangalore and the Indian Super League club Bengaluru FC are based in the city. The city hosted some games of the 2014 Unity World Cup.\n\nThe city hosts the Women's Tennis Association (WTA) Bangalore Open tournament annually. Beginning September 2008, Bangalore has also been hosting the Kingfisher Airlines Tennis Open ATP tournament annually.\n\nThe city is home to the Bangalore rugby football club (BRFC). Bangalore has a number of elite clubs, like Century Club, The Bangalore Golf Club, the Bowring Institute and the exclusive Bangalore Club, which counts among its previous members Winston Churchill and the Maharaja of Mysore. The Hindustan Aeronautics Limited SC is based in Bangalore.\n\nIndia's Davis Cup team members, Mahesh Bhupathi and Rohan Bopanna reside in Bangalore. Other sports personalities from Bangalore include national swimming champion Nisha Millet, world snooker champion Pankaj Advani and former All England Open badminton champion Prakash Padukone.\n\nBangalore is home to Bengaluru Beast, 2017 vice champion of India's top professional basketball division, the UBA Pro Basketball League.\n\nThe city hast hosted some games of the 2014 Unity World Cup.\n\n\n\n\n"}
{"id": "14633213", "url": "https://en.wikipedia.org/wiki?curid=14633213", "title": "BigBelly", "text": "BigBelly\n\nBigbelly was originally a solar powered, rubbish-compacting bin, manufactured by U.S. company Bigbelly Solar for use in public spaces such as parks, beaches, amusement parks, universities, retail properties, grocery industry and food service operators. The bin was designed and originally manufactured in Needham, Massachusetts by Seahorse Power, a company set up in 2003 with the aim of reducing fossil fuel consumption. Due to the bin's commercial success, Seahorse Power changed its name to BigBelly Solar.\n\nAlthough solar power is still an important feature, the company has since created self-powered stations for use where sun may not be available such as under a convenience store's dispenser canopy and AC powered stations for applications such as corporate cafeterias.\n\nThe bin has a capacity of 567 litres. Its compaction mechanism exerts 5.3kN of force, increasing the bin's effective capacity by five. The compaction mechanism is chain-driven, using no hydraulic fluids. Maintenance consists of lubricating the front door lock annually. The mechanism runs on a standard 12 volt battery, which is kept charged by the solar panel. The battery reserve lasts for approximately three weeks. Wireless technology-enabled units report their status into the CLEAN (Collection, Logistics, Efficiency and Notification system) dashboard that gives waste management and administration insights for monitoring and route optimization. BigBelly Solar also provides companion recycling units that allow cities, parks and universities to collect single-stream or separated recyclable materials in public spaces.\n\nThe first machine was installed in Vail, Colorado in 2004. Other locations to since use the bin include Seattle, Washington; Cincinnati, Ohio; Boston, Massachusetts; Provincetown, Massachusetts; Quincy, Massachusetts; Chicago, Illinois; Dallas, Texas; El Paso, Texas; Baltimore, Maryland; Philadelphia, Pennsylvania; Ventura, California; Oakland, California; San Diego, California; San Francisco, California; New York City; Aberystwyth; as well as Amsterdam, the Netherlands; Aberdeen, Scotland; London, England; Lower Mainland, Canada; Hamburg, Germany; Münster, Germany; Uppsala, Sweden; Reykjavík, Iceland; Oxford; Brighton and Hove and Lausanne, Switzerland at the Rolex Learning Center. Some private-sector customers include Microsoft, Google, Kaiser Permanente, ShopRite, Wegmans, CBRE and Costco.\n\n\n"}
{"id": "23850671", "url": "https://en.wikipedia.org/wiki?curid=23850671", "title": "Boring (earth)", "text": "Boring (earth)\n\nBoring is drilling a hole, tunnel, or well in the earth.\n\nBoring is used for various applications in geology, agriculture, hydrology, civil engineering, and mineral exploration. Today, most earth drilling serves one of the following purposes:\n\n\nUnlike drilling in other materials where the aim is to create a hole for some purpose, often the case of drilling or coring is to get an understanding of the ground/lithology. This may be done for prospecting to identify and quantify an ore body for mining, or to determining the type of foundations needed for a building or raised structure, or for underground structures, including tunnels and deep basements where an understanding of the ground is vital to determining how to excavate and the support philosophy. Drilling is also used in vertical and inclined shaft construction.\n\nBorehole drilling has a long history. Han Dynasty China (202 BC – 220 AD) used deep borehole drilling for mining and other projects. Chinese borehole sites could reach as deep as 600 m (2000 ft). \n\nWhen drilling in stone, one must pay particular attention to the type of material. There are three different classifications of drill bits used for drilling into stone: soft, medium, and hard. Soft formation rock bits are used in unconsolidated sands, clays, and soft limestones, etc. Medium formation bits are used in dolomites, limestones, and shale, while hard formation bits are used in hard shale, mudstones, granite, limestones and other hard and/or abrasive formations.\n\nSoft ground drilling can be undertaken using a rotary auger or wash boring techniques, while rock drilling often use methods such as NMLC which allow for recovery of a core of material which can be examined to determine the strength, degree of weathering, understanding of any how intact the rock is (RQD) and identify any discontinuities or other planes of weakness.\n\nTesting of the material in boreholes is also possible. In soft ground the standard penetration test can be used to determine the strength of the material. In rock in-situ stress testing using hydrofracturing or overcoring, Acoustic Televiewer can be used to map discontinuities to determine their orientation. It is also possible once a borehole is complete to measure the permeability. Samples of water and material are also taken for examination and lab testing.\n\nIn 1961 the United States began Project Mohole, an ambitious attempt to drill through the Earth's crust into the Mohorovičić discontinuity. The project was discontinued due to high cost. \n\nThe Kola Superdeep Borehole was a similar project of the USSR in the 1970s and early 1980s the USSR attempted to drill a hole through the crust, to sample the Mohorovicic Discontinuity. The deepest hole ever drilled failed not because of lack of money or time, but because of rock physics at depth. At approximately 12,000 metres depth, rock begins to act more like a plastic solid than a rigid solid. Rock temperatures of several hundred degrees Celsius, required that the drilling fluid be refrigerated before being sent to the cutting face of the drill. As the drill bits burnt out and were removed for replacement, the hole simply flowed closed, and the rock had to be re-drilled. The hole was scrapped.\n\nFurther attempts are planned by American consortia and further Russian attempts in Finland.\n\nIce cores are drilled by hollow bits, in much the same way that sediment cores are drilled. When all that is needed is the hole, hot water drill technology may be used to melt holes in ice or snow for both Arctic and Antarctic research purposes. Equipment for such a method is also lightweight when drilling deep holes, compared to traditional drilling equipment. Hot water drilling has been used successfully in the IceCube Neutrino Detector and Antarctic Muon And Neutrino Detector Array projects to drill as deep as 2,450 meters.\n\n\n"}
{"id": "59017898", "url": "https://en.wikipedia.org/wiki?curid=59017898", "title": "CloudBolt", "text": "CloudBolt\n\nCloudBolt is a hybrid cloud management platform developed by CloudBolt Software for deploying and managing virtual machines (VMs), applications, and other IT resources, both in public clouds (e.g., AWS, MS Azure, GCP) and in private data centers (e.g., VMware, OpenStack).\n\nThe platform was developed by Auggy da Rocha and Bernard Sanders. Auggy began work on a prototype of the platform in 2010, calling it SmartCloud 1.0. Together they created a generalized solution, which was released in August of 2011 as SmartCloud 2.0. The early version focused on simple installs and upgrades of virtual machines, and building an extensible product that customers could use as a platform for integrating with other technologies.\n\nIn 2012, they renamed their company to CloudBolt Software (to avoid a name conflict with an IBM offering), and released version CloudBolt Command and Control 3.0. In 2014, the product name \"CloudBolt Command and Control (C2)\" was simplified to \"CloudBolt\".\n\nProduct Timeline:\n\n\nThe company received Series A funding from Insight Venture Partners in July 2018\n\n\n"}
{"id": "49480932", "url": "https://en.wikipedia.org/wiki?curid=49480932", "title": "Contextual documentation", "text": "Contextual documentation\n\nContextual documentation is an information block approach to writing in-situ documentation. It becomes particularly useful when dealing with in-situ documentation delivered to the software GUI, to devise a matrix of required help to users in a particular situation or context. \nThis concept is based on DITA, where small topics are written when needed asking the right questions:\nThis is an editorial matrix, a content guideline as sorts. By no means are all items to be written exhaustively as if they were a form to be filled\n"}
{"id": "6557", "url": "https://en.wikipedia.org/wiki?curid=6557", "title": "Control unit", "text": "Control unit\n\nThe control unit (CU) is a component of a computer's central processing unit (CPU) that directs the operation of the processor. It tells the computer's memory, arithmetic out.logic unit and input and output devices how to respond to the instructions that have been sent to the processor.\n\nIt directs the operation of the other units by providing timing and control signals.\nMost computer resources are managed by the CU. It directs the flow of data between the CPU and the other devices. John von Neumann included the control unit as part of the von Neumann architecture. In modern computer designs, the control unit is typically an internal part of the CPU with its overall role and operation unchanged since its introduction.\n\nThe Control unit (CU) is digital circuitry contained within the processor that coordinates the sequence of data movements into, out of, and between a processor's many sub-units. The result of these routed data movements through various digital circuits (sub-units) within the processor produces the manipulated data expected by a software instruction (loaded earlier, likely from memory). It controls (conducts) data flow inside the processor and additionally provides several external control signals to the rest of the computer to further direct data and instructions to/from processor external destinations (i.e. memory).\n\nExamples of devices that require a CU are CPUs and graphics processing units (GPUs). The CU receives external instructions or commands which it converts into a sequence of control signals that the CU applies to the data path to implement a sequence of register-transfer level operations.\n\nMore precisely, the Control Unit (CU) is generally a sizable collection of complex digital circuitry interconnecting and directing the many execution units (i.e. ALU, data buffers, registers) contained within a CPU. The CU is normally the first CPU unit to accept from an externally stored computer program, a single instruction (based on the CPU's instruction set). The CU then decodes this individual instruction into several sequential steps (fetching addresses/data from registers/ memory, managing execution [i.e. data sent to the ALU or I/O], and storing the resulting data back into registers/memory) that controls and coordinates the CPU's inner works to properly manipulate the data. The design of these sequential steps are based on the needs of each instruction and can range in number of steps, the order of execution, and which units are enabled.\n\nThus by only using a program of set instructions in memory, the CU will configure all the CPU's data flows as needed to manipulate the data correctly between instructions. This results in a computer that could run a complete program and require no human intervention to make hardware changes between instructions (as had to be done when using only punch cards for computations before stored programmed computers with CUs were invented). These detailed steps from the CU dictate which of the CPU's interconnecting hardware control signals to enable/disable or which CPU units are selected/de-selected and the unit's proper order of execution as required by the instruction's operation to produce the desired manipulated data. Additionally, the CU's orderly hardware coordination properly sequences these control signals then configures the many hardware units comprising the CPU, directing how data should also be moved, changed, and stored outside the CPU (i.e. memory) according to the instruction's objective.\n\nDepending on the type of instruction entering the CU, the order and number of sequential steps produced by the CU could vary the selection and configuration of which parts of the CPU's hardware are utilized to achieve the instruction's objective (mainly moving, storing, and modifying data within the CPU). This one feature, that efficiently uses just software instructions to control/select/configure a computer's CPU hardware (via the CU) and eventually manipulates a program's data, is a significant reason most modern computers are flexible and universal when running various programs. As compared to some 1930s or 1940s computers without a proper CU, they often required rewiring their hardware when changing programs. This CU instruction decode process is then repeated when the Program Counter is incremented to the next stored program address and the new instruction enters the CU from that address, and so on till the programs end.\n\nOther more advanced forms of Control Units manage the translation of instructions (but not the data containing portion) into several micro-instructions and the CU manages the scheduling of the micro-instructions between the selected execution units to which the data is then channeled and changed according to the execution unit's function (i.e., ALU contains several functions). On some processors, the Control Unit may be further broken down into additional units, such as an instruction unit or scheduling unit to handle scheduling, or a retirement unit to deal with results coming from the instruction pipeline. Again, the Control Unit orchestrates the main functions of the CPU: carrying out stored instructions in the software program then directing the flow of data throughout the computer based upon these instructions (roughly likened to how traffic lights will systematically control the flow of cars [containing data] to different locations within the traffic grid [CPU] until it parks at the desired parking spot [memory address/register]. The car occupants [data] then go into the building [execution unit] and comes back changed in some way then get back into the car and returns to another location via the controlled traffic grid).\n\nHardwired control units are implemented through use of combinational logic units, featuring a finite number of gates that can generate specific results based on the instructions that were used to invoke those responses. Hardwired control units are generally faster than microprogrammed designs.\n\nTheir design uses a fixed architecture—it requires changes in the wiring if the instruction set is modified or changed.\nThis architecture is preferred in reduced instruction \nset computers (RISC) as they use a simpler instruction set.\n\nA controller that uses this approach can operate at high speed; however, it has little flexibility, and the complexity of the instruction set it can implement is limited.\n\nThe hardwired approach has become less popular as computers have evolved. Previously, control units for CPUs used ad-hoc logic, and they were difficult to design.\n\nThe idea of microprogramming was introduced by Maurice Wilkes in 1951 as an intermediate level to execute computer program instructions. Microprograms were organized as a sequence of \"microinstructions\" and stored in special control memory. The algorithm for the microprogram control unit,unlike the hardwired control unit, is usually specified by flowchart description. The main advantage of the microprogram control unit is the simplicity of its structure. Outputs of the controller are organized in microinstructions and they can be easily replaced.\n\n"}
{"id": "19749632", "url": "https://en.wikipedia.org/wiki?curid=19749632", "title": "Drag harrow", "text": "Drag harrow\n\nA drag harrow, a type of spring-tooth harrow, is a largely outdated type of soil cultivation implement that is used to smooth the ground as well as loosen it after it has been plowed and packed. It uses many flexible iron teeth usually arranged into three rows. It has no hydraulic functionality and has to be raised/adjusted with one or multiple manual levers. It is a largely outdated piece of farm equipment, having been replaced by more modern disc harrows and hydraulically operated field cultivators.\n\nA drag harrow is used to loosen and even out soil after it has been plowed and packed. It pulls up large rocks which may then be picked up manually and put in the tractor's stone box to remove from the field. The drag harrow also kills some weeds that may be present, but it is not very efficient in doing so due to its highly flexible teeth, hence it is not one of its primary functions.\n\nThe drag harrow is not often used in modern farming as other harrows have proven to be more suitable, such as the disc harrow. Another reason they are not often used is because they cannot be controlled hydraulically, meaning that the operator is required to dismount from the tractor to adjust it or unclog it. However it is used as a drag behind several other implements such as a rod weeder. Due to their low cost and simplicity, drag harrows are still used widely by small farmers.\n\nDrag harrows can be a name used for several different types of equipment. A spike tooth harrow or flex harrow is often called a drag harrow and is in use extensively throughout the US for seedbed preparation and for grooming grassland pastures. See also:\n"}
{"id": "21475252", "url": "https://en.wikipedia.org/wiki?curid=21475252", "title": "English Electric/BAC Lightning (book)", "text": "English Electric/BAC Lightning (book)\n\nThe English Electric/BAC Lightning () is an aviation book by British military historian and author Bruce Barrymore Halpenny about the English Electric Lightning. It was published by Osprey Publishing as part of their Air Combat series. It was a best seller in Grimsby, the home town of the Lightning.\n\nThe author, known for his books on airfields and aircraft, spent nine months researching the Lightnings with the pilots of 5 and 11 Squadrons and Binbrook's own Lightning Training Flight. To gather information for the book, the author talked to men like, Sqdn Ldr Dave Carden (with 3,000 plus hours, the most experienced Lightning pilot in the world) and fellow pilots and ground staff.\n\nIncluded in the book are several anecdotes, one being a pilot's super sonic flight causing \"anger and dismay\". Thirty minutes after landing at RAF Binbrook, the hapless airman took a telephone call... as Flt Lt Normant Want recalls in the book,\n\nIt was from the Mayor of Bridlington who was complaining about an aircraft that had dropped a supersonic boom on Bridlington and the surrounding area. The boom had broken lots of windows. But the thing that really annoyed him was that the sonic boom sounded just like the signal to launch the lifeboat - which was now in the middle of the North Sea. The Mayor was not too impressed with all this. I did the hot-foot dance routine that most young lads go through when the've been nicked. It was about half a day later that the Ground Controlled Interception controller let me off the hook. He 'phoned to tell me that he had been responsible and that it wasn't the Mayor of Bridlington after all!\n\nThe book itself gives an insight into the workings of RAF Binbrook, its Lightnings, and the men that fly and maintain them. Sqdn Ldr Dave Carden takes the reader on a typically \"hair-raising mission\", while another section is devoted to a pilot's experiences when his aircraft caught fire and crashed into the sea off Flamborough Head in 1981. It also deals with the Quick Reaction Alert shed, where two fully armed Lightnings and their pilots were on constant standby to intercept Russian aircraft which used to sometimes fly to within 100 miles of Spurn Point.\n"}
{"id": "774020", "url": "https://en.wikipedia.org/wiki?curid=774020", "title": "Eurasian Land Bridge", "text": "Eurasian Land Bridge\n\nThe Eurasian Land Bridge (, \"Yevraziyskiy sukhoputniy most\"), sometimes called the New Silk Road (Новый шёлковый путь, \"Noviy shyolkoviy put<nowiki>'</nowiki>\"), or Belt and Road Initiative is the rail transport route for moving freight and passengers overland between Pacific seaports in the Russian Far East and China and seaports in Europe. The route, a transcontinental railroad and rail land bridge, currently comprises the Trans-Siberian Railway, which runs through Russia and is sometimes called the Northern East-West Corridor, and the New Eurasian Land Bridge or Second Eurasian Continental Bridge, running through China and Kazakhstan. As of November 2007, about 1% of the $600 billion in goods shipped from Asia to Europe each year were delivered by inland transport routes.\n\nCompleted in 1916, the Trans-Siberian connects Moscow with Russian Pacific seaports such as Vladivostok. From the 1960s until the early 1990s the railway served as the primary land bridge between Asia and Europe, until several factors caused the use of the railway for transcontinental freight to dwindle. One factor is that the railways of the former Soviet Union use a wider rail gauge than most of the rest of Europe as well as China. Recently, however, the Trans-Siberian has regained ground as a viable land route between the two continents.\n\nChina's rail system had long linked to the Trans-Siberian via northeastern China and Mongolia. In 1990 China added a link between its rail system and the Trans-Siberian via Kazakhstan. China calls its uninterrupted rail link between the port city of Lianyungang and Kazakhstan the \"New Eurasian Land Bridge\" or \"Second Eurasian Continental Bridge\". In addition to Kazakhstan, the railways connect with other countries in Central Asia and the Middle East, including Iran. With the October 2013 completion of the rail link across the Bosphorus under the Marmaray project the New Eurasian Land Bridge now theoretically connects to Europe via Central and South Asia.\n\nProposed expansion of the Eurasian Land Bridge includes construction of a railway across Kazakhstan that is the same gauge as Chinese railways, rail links to India, Burma, Thailand, Malaysia and elsewhere in Southeast Asia, construction of a rail tunnel and highway bridge across the Bering Strait to connect the Trans-Siberian to the North American rail system, and construction of a rail tunnel between South Korea and Japan. The United Nations has proposed further expansion of the Eurasian Land Bridge, including the Trans-Asian Railway project.\n\nCommercial traffic between Europe and Asia took place along the Silk Road from at least the 2nd millennium BC. The Silk Road was not a specific thoroughfare, but a general route used by traders to travel, much of it by land, between the two continents along the Eurasian Steppes through Central Asia. The route was used to exchange goods, ideas and people primarily between China and India and the Mediterranean and helped create a single-world system of trade between the civilisations of Europe and Asia.\n\nExports from Asia transported along the Silk Road included fabrics, carpets, furs, weapons, utensils, metals, farm produce, livestock and slaves. Civilisations active in trading during the road's history included Scythia, Ancient and Byzantine Greece, the Han and Tang dynasties, Parthia, Rouran, Sogdiana, Göktürks, Xiongnu, Yuezhi and the Mongol Empire.\n\nBeginning in the 5th century AD, new land routes between Asia and Europe developed further to the north, in the Rus'. Many of these routes passed through Yugra and extended to the Baltic region. The Khazars, Volga Bulgaria, and the Rus' Khaganate were active in trading along the northern trade routes.\n\nTraffic along the southern Silk Road routes greatly diminished with the Fall of Constantinople in the 15th century and development of the sea route around the Cape of Good Hope in the 16th century. By the 18th century, European influence on trade and new national boundaries severely restricted the movement of traders along all land routes between Europe and China, and overland trade between East Asia and Europe virtually disappeared.\n\nThe Trans-Siberian Railway and its various associated branches and supporting lines, completed in 1916, established the first rail connection between Europe and Asia, from Moscow to Vladivostok. The line, at , is the longest rail line in the world.\nThe Trans-Siberian connects the Russian Pacific ports of Vladivostok and Nakhodka with Moscow. Rail links at Moscow allow passengers and freight to connect to train lines running further west into Europe. By making further transfers, passengers and freight can eventually reach Western European seaports. The Trans-Siberian also connects with North Korea (e.g. via Dandong in Northeastern China, or directly at Khasan south of Vladivostok).\n\nA fully electrified and double-tracked line, the Trans-Siberian Railway line is capable of transporting around 100 million tons of freight annually. The line can handle up to 200,000 TEU of containerized international transit freight per year.\n\nA more northerly east-west route across Siberia, parallel to the Trans-Siberian line and known as the Baikal–Amur Mainline was mostly completed in 1989. It terminates at the Pacific ports of Vanino and Sovetskaya Gavan. Although this line is comparatively little used (the management mentions 6 million tons of freight per year, not indicating the year), the management expects the line to be fully used in the foreseeable future for oil and copper ore export, and has plans to double-track it.\n\nWhile the Trans-Siberian has always been used by the Czarist, Soviet and modern Russian government to project political power into their territories in Asia, in the 1960s it was opened by the USSR as an international trade route connecting the Western Pacific with Europe. Freight shipments on the Trans-Siberian, however, experienced increasing problems over time with dilapidated rail infrastructures, theft, damaged freight, late trains, inflated freight fees, uncertain scheduling for return of containers and geopolitical tension. As a result, use of the railway for international trade declined to almost zero by the 1990s.\n\nAccording to Hofstra University, as of 2001 there was renewed interest in using the Trans-Siberian as a route across Asia to Europe. Also, the Trans-Siberian links directly to railways which ultimately connect, via Finland and Sweden, to the year-round ice-free port of Narvik in Norway. At Narvik, freight can be transshipped to ships to cross the Atlantic to North America. Rail links from Russia also connect to Rotterdam, but may encounter greater congestion along this route with resulting delays. The trade route between the east coast of North America and eastern Russia using the Trans-Siberian is often called the Northern East West Freight Corridor.\n\nIn an effort to attract use of the Trans-Siberian to transport goods from Japan, China, and Korea to Europe, in the mid-1990s Russia lowered tariffs on freight using the railway. As a result, freight volume over the rail line doubled in 1999 and 2000.\n\nIn February and March 2011, Japan's Ministry of Land, Infrastructure, Transport and Tourism sponsored a test of the route by shipping roof tiles to Europe via the Trans-Siberian. The tiles were transported by ship from Hamada, Shimane to Vladivostok, then by the railway to Moscow. The transit time was expected to be 30 days, in comparison with the 50 days on average it takes to ship cargo by ship from Hamada to ports in western Russia. If successful, the ministry would use the results of the test to encourage other Japanese companies to utilize the Trans-Siberian over the sea route.\n\nIn 2011, a direct container rail service began carrying car parts from Leipzig, Germany, to inland Shenyang, China, through Siberia in 23 days, every day.\n\nIn 2013 a direct container, pallet, and general cargo rail service began, from Łódź, Poland, to inland Chengdu, China, through Siberia in 14 days, 3 days each week.\n\nAccording to Russian statistics, the amount of international container shipments transiting annually through Russia over the Trans-Siberian has grown by a factor of 7 between 2009 and 2014, reaching 131,000 TEU (55,000 physical containers) in 2014. \n\nBelarusian Railways reported similar statistics: in 2014, the volume of direct container traffic from China to Western Europe crossing Belarus amounted to 40,600 TEU, on 25 direct container train routes. This constituted over 20% of Belarusian Railway's entire volume of container transportation that year, 193,100 TEU. While significant, and growing, this is still much less than 0.1% of the number of containers that travel via China's sea ports (some 170 million TEU).\n\nThe original Moscow–Vladivostok route, completed in 1904, cut across China's northeastern provinces, or Manchuria; the section of the railway located within China was known as the Chinese Eastern Railway. While the more northerly Trans-Siberian route, located entirely on Russian soil, was completed in 1916, the former Chinese Eastern Railway route continues as an important connector between the two countries' railway networks.\n\nThe western border point (Zabaykalsk/Manzhouli) and the line connecting it to the Trans-Siberian main line, are now being upgraded, with the goal of enabling the railway by 2010 to pass 30 freight trains in each direction across the border, each one up to 71 cars long. The cross-border freight volume at this rail crossing is expected to reach 25.5 million tons by 2010. Besides cargo (principally, Russian oil exported to China), this crossing sees a direct weekly passenger train, Moscow–Beijing, as well as some local passenger trains.\nThe eastern border point of the former Chinese Eastern Railway, at Suifenhe/Grodekovo, sees significant use as well, with over 8 million tons of freight crossing the border there in 2007, and regular cross-border passenger service.\n\nA third, little-known and less used, rail connection between Russia and China was built farther south, between \nHunchun (in China's Jilin province) and Russian Makhalino (a station on the Ussuriysk–Khasan–North Korean border line, before Khasan). It began operating in February 2000, and saw only a minor amount of traffic (678 railcars of lumber) over the next two years. The line was closed in 2002–03, briefly reopened in 2003, and closed again in September 2004. On 15 February 2011, the two companies who own the line, Northeast Asia Railway Group, a Chinese company, and JSC Golden Link, a Russian company, signed an agreement to resume operations on the line in May 2011.\n\nIn November 2008, the transport ministries of Russia and the China signed an agreement about creating one more link between the railway systems of the two countries. It will involve a railway bridge between across the Amur (Heilong) River, connecting Tongjiang in China's Heilongjiang province with Nizhneleninskoye in Russia's Jewish Autonomous Oblast. On 4 November 2010, the project director, Wang Jin, told Xinhua News Agency that construction on the bridge would begin in January 2011.\n\nThe Trans-Mongolian line, connecting Ulan-Ude on the Trans-Siberian with China's Erenhot via the Mongolian capital Ulaanbaatar, both serves as a crucial link to the outside world for landlocked Mongolia, and the shortest connection between the Trans-Siberian Railway and Beijing. This line's capacity, however, is limited by its being single-track.\n\nWhile the USSR had long been connected with China via the rail links in Northeastern China and Mongolia, since the 1950s plans existed to connect the two countries' rail networks at the Kazakhstan/Xinjiang border. The Soviets completed their line from Aktogay (a station on the Turksib in eastern Kazakhstan) to their border station Druzhba (now Dostyk), but the construction on the Chinese side stopped because of the Sino-Soviet split of the 1960s. In 1985 construction commenced on the Northern Xinjiang Railway to link the Chinese and Russian rail networks via Kazakhstan. The section between Ürümqi and Alashankou was completed on 16 September 1990, linking the railway lines of the two countries at Dostyk. In July 1991 the first goods train traveled along the line from China to Kazakhstan's then-capital of Almaty. In December 2009, a second rail link from China was built to the Kazakhstan border at Khorgos. The Jinghe–Yining–Khorgos Railway forks off of the Northern Xinjiang Railway at Jinghe and approaches Kazakhstan from the Ili River Valley. A rail link on Kazakh side will extend the line to Saryozek by 2013. The rail link through the Korgas Pass was completed in December 2012.\n\nBecause Kazakhstan was once a member of the USSR, its rail system connects with and carries the same rail gauge as the Russian rail system, as well as the other Central Asian republics of Turkmenistan, Uzbekistan, Kyrgyzstan and Tajikistan.\n\nFrom Kazakhstan, four major north-south railways connect with the Russian rail system. Two connect with the Trans-Siberian Railway (the Turksib and the Shu–Astana–Petropavl meridional line) while the other two (the Trans-Aral Railway, and the connection via Atyrau and Astrakhan Oblast) go directly to European Russia. These links to the Russian rail system are sometimes called the Eurasian Railway. Kazakhstan plays an important role in the \"New Silk Road\" initiative, known as \"One Belt, One Road\" linking China and Europe through Central Asia and Russia.\n\nA new direction of the Silk Road was launched in January 2016 and included the Ukraine – Georgia – Azerbaijan – Kazakhstan – China route.\n\nKazakhstan's infrastructure development program Nurly Zhol was developed in line with the New Silk Road Initiative. President of Kazakhstan Nursultan Nazarbayev even noted that Nurly zhol was a part of the New Silk Road Economic Belt.\n\nThere are 3 main routes for container services from China to Europe: Eastern route from Vostochny Port (Russia), northern route from west China via Manzhouli/Zabaikalsk border stations and southern route from east China via Dostyk border station, through which totally 25k TEU has been transported on rail by 2014. \n\nIn January 2008 China and Germany inaugurated a long-distance freight train service between Beijing and Hamburg. Travelling a total of , the train uses the China Railways and the Trans-Mongolian line to travel from Xiangtan (in Hunan Province) to Ulaanbaatar, where it then continues north to the Trans-Siberian. After reaching the end of the Trans-Siberian at Moscow the train continues to Germany via rail links in Belarus and Poland. Total transit time is 15 days, as compared with the 30 days average it would take for the freight to make the same journey by ship. The first train of 50 containers, carrying a mixed load of clothes, ceramics and electronics (for the Fujitsu company), travelled on tracks operated by six different railways.\n\nHartmut Mehdorn, chairman of Deutsche Bahn (DB), stated in March 2008 that regularly scheduled, weekly China-Germany freight services should be in operation by 2010. In April 2009, however, DB postponed the service indefinitely because of the global economic crisis.\n\nAnother test run, from Chongqing to Duisburg via Alashankou crossing, Kazakhstan, Russia, Belarus, and Poland took place in March–April 2011, covering in 16 days. It was again said by DB that if there is enough demand, the service can be made regular already in 2011, As of March 2014, the Chonqing-Duisburg route makes three weekly services carrying up to 50 40-foot-long containers.\n\nThe transportation authorities in another industrial center of central China, Wuhan, plan to organize regular runs of direct freight train between Wuhan and European destinations (Czech Republic, Poland, Germany) starting in April 2014. Plans call for the service starting from 1–2 trains per month in April–June 2014, gradually increasing the frequency to 1–2 trains per week in 2015. A new customs facility is under construction in Wuhan's Wujiashan (吴家山) industrial area; after its planned opening in October 2015, exports from the Wuhan region will be able to clear Chinese customs there, instead of Alashankou.\n\nBy 2016, the freight rail service between a number of container terminals in China and their counterparts in Europe has become fairly regular. Between some city pairs, there is one train per week.\n\nBoth with respect to cost and speed, the China-Europe direct train service is in between the air and sea options. While it is estimated that the overall volume of goods moving between China and Europe by rail is not going to surpass 1–2% of the sea cargo volume, it may eat significantly into the air cargo volume.\n\nThe service is typically used for valuable and somewhat time-sensitive cargo where the time advantage of rail over ship is essential, which, however, is heavy enough to make the cost saving vs. air transport noticeable. Typical cargoes include complex machinery and spare parts (in both directions), as well as high-end groceries and consumer goods (primarily toward China). While major customers ship their products by full container load, freight forwarders also make it possible to send less-than-container shipments.\n\nOn the European end of the freight route, the rail terminal near Duisburg's river port (Ruhrort) is a major destination. According to a 2018 report, Duisburg is either the destination, or one of the destinations, of some 80% of all direct China-Western Europe cargo trains.\n\nThe New Eurasian Land Bridge, also called the New Eurasian Continental Bridge, is the name given to China's rail link with Central Asia. The route includes China's east-west railways which, in addition to the Beijiang line, are the Longhai Railway and the Lanzhou–Xinjiang Railway. Together, the railways create an uninterrupted rail link between the port city of Lianyungang in Jiangsu province and Kazakhstan. In 1995 the Chinese and Kazakhstan governments signed an agreement which allows the latter to use Lianyungang as its primary seaport for exports and imports, and the former intends for Lianyungang to serve as the designated starting point for the New Eurasian Land Bridge.\n\nFrom Almaty in Kazakhstan, the railway extends to Tashkent and Samarkand, Uzbekistan and then to Tejen, Turkmenistan. From Tejen, another line continues to Ashgabat, the capital of Turkmenistan. After Ashgabat, the line ends at Türkmenbaşy, Turkmenistan, a port on the Caspian Sea. (After a direct rail link between Kazakhstan and Turkmenistan opened, it became possible for the freight to bypass Uzbekistan, which has poor relations with its neighbors).\n\nIn 1996 a branch railway from Tejen was constructed across the border with Iran (at Serakhs) and linked to the Islamic Republic of Iran Railways. The link potentially enables rail freight from China to reach ports on the Persian Gulf and via other train lines, to reach into the Caucasus and Turkey. \nIn 2016, direct container train service was inaugurated on this route, between Yiwu (Zhejiang Province) and Teheran; the trip takes 14 days.\n\nThe central Asian route did not extend all the way into Europe until October 2013 when the rail link across the Bosphorus though the Marmaray link was opened. Iranian rail lines use gauge, requiring freight cars transiting from China into Iran to change wheel gauges twice. The train ferry across Lake Van is also a capacity restriction.\n\nChinese state media claims that the New Eurasian Land/Continental Bridge extends from Lianyungang to Rotterdam, a distance of . The exact route used to connect the two cities, whether through Mongolia or Kazakhstan, however, is unclear.\n\nAn alternative way from China to Europe is via Turkey. The route from China follows Kazakhstan, Uzbekistan, Turkmenistan, Iran, Turkey. Due to longer distance, insufficient service and border crossings, this route has never been used for transports from China to Europe. However, a number of projects may strengthen this route in the future. \n\nThe Marmaray project is one of them, connecting Europe and Asia via a tunnel under the Bosphorus. After the completion of the project, a continuous run of trains will be possible between Asia and Europe, which is now done by rail ferry service. But Marmaray tunnel, which will give very limited service to freight trains due to dense public transport via tunnel and which will be closed for dangerous goods, may not be able to change the current traffic. \n\nAnother project is Baku–Tbilisi–Kars railway project, which will be able to shorten the route via the Caspian Sea by bypassing Iran. The new railway lines constructed in Kazakhstan will make it shorter. The new route, in this case, will be China–Kazakhstan–Azerbaijan–Georgia–Turkey.\n\nFormer countries of the USSR, as well as Mongolia, use a track gauge of . The international standard rail gauge used in most of Europe and China is . As a result, trains cannot run from China or European countries into or out of the former USSR without changing bogies. Large facilities to carry out this procedure exist at most border crossing between the \"Russian\" and \"standard\" gauge territories (e.g., at Zabaykalsk or Erenhot)\nChanging the bogies on a rail car takes hours and special, heavy equipment. In many cases (especially, containerized freight), freight is transshipped from one train to another instead of changing the bogies. As of 2016, this is what's usually done with China-Europe container trains at places such as Khorgos; it is reported that containers can be moved from one train to another in as little as 47 minutes. In the case of liquids, frozen goods and hazardous materials, however, the bogies are usually changed.\n\nIt has been suggested that on some lines variable gauge axles would achieve significant time savings in comparison to bogie exchange. Their implementation however would involve a much higher capital cost, requiring either retrofitting or replacement of existing bogies.\n\nOn 10 March 2004 the Kazakhstan Railway Company Ltd announced that it was looking for investors to fund the construction of a railway stretching from China across Kazakhstan to the Caspian Sea that would be the same gauge as Chinese railways. Thus, the railway would allow trains from China to cross Kazakhstan without having to change bogies. The reported construction cost of the new railway was $3.5 billion. Chinese media reported that the railway would complete the link between China and Europe via central Asia, but it is unclear where the actual link to Europe would be. Also unclear is whether construction has yet to begin on the project.\n\nThe governments of India and Burma have proposed building, with China's cooperation, a link to the Eurasian Land Bridge that would start in India or Burma and connect to the Chinese rail system in Yunnan. The route would allow freight from India and Burma to travel overland to Europe. The link would also give rail access for China to the Indian Ocean. One proposed starting point for the route is Kyaukpyu. The governments of Thailand and Malaysia are also studying the feasibility of establishing rail links with China.\n\nBoth Russia and China are seeking to establish a permanent rail link with South Korea by way of North Korea to allow South Korean goods to be shipped to Europe via the Eurasian Land Bridge. According to Choi Yeon-Hye, a professor of marketing and management at the Korea National Railway College, a rail connection from Busan to Rotterdam would cut shipping time from 26 to 16 days and save $800 per container of freight. As part of its plan to link the Trans-Siberian to North and South Korea, Russia rebuilt its railink from Khasan to Rajin, finishing in October 2011.\n\nThe South Korean government announced on 2 December 2009 that it would conduct an economic and technical study on the feasibility of constructing undersea tunnels for transporting goods and people to and from the country directly to Kyushu, Japan and Shandong, China.\n\nThe United Nations Development Programme has advocated greater regional integration along the Eurasian Land Bridge, including development of rail links between the countries of South and Southeast Asia and Central Asia, called the Trans-Asian Railway project. Chinese leaders have called for the establishment of free trade zones at both ends of the Eurasian Land Bridge to facilitate development. Said Khalid Malik, United Nations Resident Coordinator in China, \"If this comes true, it will enable the continental bridge to play its due role in enhancing co-operation between Asia and Europe, and promoting world peace and development.\"\n\nIn 2010 and 2011, China announced plans to finance expansion of the rail systems in Laos, Thailand, Myanmar, Cambodia, and Vietnam and connect them to China's rail system via Kunming. The plans include construction of a high-speed rail line from Kunming to Vientiane, beginning in April 2011, with a possible future extension to Bangkok.\n\nOn 15 December 2011, Russian Prime Minister Vladimir Putin announced that a rail link was being considered between Sakhalin Island and Japan. The rail line, constructed in an undersea tunnel, would link Japan to the Trans-Siberian.\n\nIn April 2007 the Russian government announced that it was considering building a double track broad gauge rail tunnel under the Bering Strait between Chukotka and Alaska. The tunnel, as projected, would be long and would include oil and gas pipelines, fiber optic cables and power lines. The tunnel project was estimated to cost $65 billion and take 15–20 years to build. In addition to the Russian government, sponsors of the project apparently include Transneft and RAO United Energy Systems.\n\nThe project, as envisioned, would connect the Trans-Siberian via Komsomolsk-on-Amur/Yakutsk in Siberian Russia with the North American rail network (gauge to be widened) at Fort Nelson, British Columbia, Canada, a distance of . A significant hurdle for the project is that the nearest major road to the Russian end of the tunnel is away. In addition, Alaska has no direct rail link to either Canada or the contiguous United States. Other leaders, including Wally Hickel, Lyndon LaRouche, Sun Myung Moon, and the 14th Dalai Lama have also advocated the construction of a tunnel or bridge across the strait.\n\nIt was reported in the press in March 2007 that China intends to build a high-speed rail link between China and Western Europe with the possibility of a high-speed rail journey from Beijing to London taking just two days.\n\nIn February 2011, the Chinese government announced that it would jointly sponsor the construction of a high-speed rail line between Astana and Almaty in Kazakhstan. The announced completion date was 2015.\n\n\n\n\n"}
{"id": "44510561", "url": "https://en.wikipedia.org/wiki?curid=44510561", "title": "Eurocom Corporation", "text": "Eurocom Corporation\n\nEurocom Corporation is a Canadian computer developer of high performance notebooks and laptops.\n\nEurocom was founded in 1989 as a company designing desktop replacement notebooks. To achieve this they used CPUs intended for desktop computers in their notebooks. In May 2013 Eurocom began to sell laptops through Future Shop's online retail store.\n\nEurocom structures laptop design and building around units that it claims are \"highly configurable and easily upgradable.\" Another Eurocom philosophy is \"creating computers that push technology forward\" and the company claims to have a series of industry firsts as a result. Eurocom offers a series of specialized computers such as Trusted Platform Module notebooks, and Mobile Servers.\nEurocom has been awarded the \"Intel Form Factor Solution Innovation Award\" In addition to other awards from various publications.\n\n"}
{"id": "53806103", "url": "https://en.wikipedia.org/wiki?curid=53806103", "title": "Fabric shaver", "text": "Fabric shaver\n\nThe fabric shaver (also known as a lint shaver or fuzz remover) is a handheld electrical device that has a rotating blade underneath a blade net. It allows users to remove fuzz on a fabric without damaging the fabric. It can be applied on different fabric-made items such as bedding, curtain or carpet, but mostly used for removing fuzz on a clothes, especially sweaters, hoodies, or clothes made by wool, angora or cashmere.\nThe fabric shaver can be powered by two different ways; using a battery charger or a dry cell. To use the fabric shaver, all the user has to do is check its battery and then press on/off button.\n\nIt is very useful in removing fur formed on woolen clothes due to prolonged usage,\n"}
{"id": "57661926", "url": "https://en.wikipedia.org/wiki?curid=57661926", "title": "Garrison engineer", "text": "Garrison engineer\n\nA garrison engineer holds the rank of executive engineer in the Indian Defense Service of Engineers. Garrisson Engineer is a designation which is held either by a civilian officer of M.E.S. or by a military officer of the rank of Lt Col. The Garrison engineer's staff includes at least three assistant garrison engineers, a barrack stores officer, an accounts officer, and junior engineers. Responsibilities of the position include procuring contracts and executing them. One of the elected members of the cantonment board has to be a former garrison engineer.\n\nThe first Indian to become an officiating garrison engineer was Rai Bahadur Pandit Ram Prasad Tewari, who served in the Military Engineer Services from 1880 to 1920.\n\nhttps://www.quora.com/How-is-the-post-of-Garrison-engineer-in-MES-and-that-of-QS-and-C-through-IES-different-What-are-the-basic-differences-in-work-lifestyle-and-postings\n"}
{"id": "386519", "url": "https://en.wikipedia.org/wiki?curid=386519", "title": "History of computing", "text": "History of computing\n\nThe history of computing is longer than the history of computing hardware and modern computing technology and includes the history of methods intended for pen and paper or for chalk and slate, with or without the aid of tables. \n\nDigital computing is intimately tied to the representation of numbers. But long before abstractions like \"the number\" arose, there were mathematical concepts to serve the purposes of civilization. These concepts are implicit in concrete practices such as :\n\nEventually, the concept of numbers became concrete and familiar enough for counting to arise, at times with sing-song mnemonics to teach sequences to others. All known languages have words for at least \"one\" and \"two\" (although this is disputed: see Piraha language), and even some animals like the blackbird can distinguish a surprising number of items.\n\nAdvances in the numeral system and mathematical notation eventually led to the discovery of mathematical operations such as addition, subtraction, multiplication, division, squaring, square root, and so forth. Eventually the operations were formalized, and concepts about the operations became understood well enough to be stated formally, and even proven. See, for example, Euclid's algorithm for finding the greatest common divisor of two numbers.\n\nBy the High Middle Ages, the positional Hindu–Arabic numeral system had reached Europe, which allowed for systematic computation of numbers. During this period, the representation of a calculation on paper actually allowed calculation of mathematical expressions, and the tabulation of mathematical functions such as the square root and the common logarithm (for use in multiplication and division) and the trigonometric functions. By the time of Isaac Newton's research, paper or vellum was an important computing resource, and even in our present time, researchers like Enrico Fermi would cover random scraps of paper with calculation, to satisfy their curiosity about an equation. Even into the period of programmable calculators, Richard Feynman would unhesitatingly compute any steps which overflowed the memory of the calculators, by hand, just to learn the answer.\n\nThe earliest known tool for use in computation is the Sumerian abacus, and it was thought to have been invented in Babylon c. 2700–2300 BC. Its original style of usage was by lines drawn in sand with pebbles. Abaci, of a more modern design, are still used as calculation tools today. This was the first known computer and most advanced system of calculation known to date - preceding Greek methods by 2,000 years.\n\nIn c. 1050–771 BC, the south-pointing chariot was invented in ancient China. It was the first known geared mechanism to use a differential gear, which was later used in analog computers. The Chinese also invented a more sophisticated abacus from around the 2nd century BC known as the Chinese abacus.\n\nIn the 5th century BC in ancient India, the grammarian Pāṇini formulated the grammar of Sanskrit in 3959 rules known as the Ashtadhyayi which was highly systematized and technical. Panini used metarules, transformations and recursions.\n\nIn the 3rd century BC, Archimedes used the mechanical principle of balance (see Archimedes Palimpsest#Mathematical content) to calculate mathematical problems, such as the number of grains of sand in the universe (\"The sand reckoner\"), which also required a recursive notation for numbers (e.g., the myriad myriad).\n\nAround 200 BC the development of gears had made it possible to create devices in which the positions of wheels would correspond to positions of astronomical objects. By about 100 AD Hero of Alexandria had described an odometer-like device that could be driven automatically and could effectively count in digital form. But it was not until the 1600s that mechanical devices for digital computation appear to have actually been built.\n\nThe Antikythera mechanism is believed to be the earliest known mechanical analog computer. It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to \"circa\" 100 BC.\n\nMechanical analog computer devices appeared again a thousand years later in the medieval Islamic world and were developed by Muslim astronomers, such as the mechanical geared astrolabe by Abū Rayhān al-Bīrūnī, and the torquetum by Jabir ibn Aflah. According to Simon Singh, Muslim mathematicians also made important advances in cryptography, such as the development of cryptanalysis and frequency analysis by Alkindus. Programmable machines were also invented by Muslim engineers, such as the automatic flute player by the Banū Mūsā brothers, and Al-Jazari's humanoid robots and \"castle clock\", which is considered to be the first programmable analog computer.\n\nDuring the Middle Ages, several European philosophers made attempts to produce analog computer devices. Influenced by the Arabs and Scholasticism, Majorcan philosopher Ramon Llull (1232–1315) devoted a great part of his life to defining and designing several \"logical machines\" that, by combining simple and undeniable philosophical truths, could produce all possible knowledge. These machines were never actually built, as they were more of a thought experiment to produce new knowledge in systematic ways; although they could make simple logical operations, they still needed a human being for the interpretation of results. Moreover, they lacked a versatile architecture, each machine serving only very concrete purposes. In spite of this, Llull's work had a strong influence on Gottfried Leibniz (early 18th century), who developed his ideas further, and built several calculating tools using them.\n\nIndeed, when John Napier discovered logarithms for computational purposes in the early 17th century, there followed a period of considerable progress by inventors and scientists in making calculating tools. The apex of this early era of formal computing can be seen in the difference engine and its successor the analytical engine (which was never completely constructed but was designed in detail), both by Charles Babbage. The analytical engine combined concepts from his work and that of others to create a device that if constructed as designed would have possessed many properties of a modern electronic computer. These properties include such features as an internal \"scratch memory\" equivalent to RAM, multiple forms of output including a bell, a graph-plotter, and simple printer, and a programmable input-output \"hard\" memory of punch cards which it could modify as well as read. The key advancement which Babbage's devices possessed beyond those created before his was that each component of the device was independent of the rest of the machine, much like the components of a modern electronic computer. This was a fundamental shift in thought; previous computational devices served only a single purpose, but had to be at best disassembled and reconfigured to solve a new problem. Babbage's devices could be reprogramed to solve new problems by the entry of new data, and act upon previous calculations within the same series of instructions. Ada Lovelace took this concept one step further, by creating a program for the analytical engine to calculate Bernoulli numbers, a complex calculation requiring a recursive algorithm. This is considered to be the first example of a true computer program, a series of instructions that act upon data not known in full until the program is run.\n\nSeveral examples of analog computation survived into recent times. A planimeter is a device which does integrals, using distance as the analog quantity. Until the 1980s, HVAC systems used air both as the analog quantity and the controlling element. Unlike modern digital computers, analog computers are not very flexible, and need to be reconfigured (i.e., reprogrammed) manually to switch them from working on one problem to another. Analog computers had an advantage over early digital computers in that they could be used to solve complex problems using behavioral analogues while the earliest attempts at digital computers were quite limited.\nSince computers were rare in this era, the solutions were often \"hard-coded\" into paper forms such as nomograms, which could then produce analog solutions to these problems, such as the distribution of pressures and temperatures in a heating system.\n\nNone of the early computational devices were really computers in the modern sense, and it took considerable advancement in mathematics and theory before the first modern computers could be designed.\n\nThe first recorded idea of using digital electronics for computing was the 1931 paper \"The Use of Thyratrons for High Speed Automatic Counting of Physical Phenomena\" by C. E. Wynn-Williams. From 1934 to 1936, NEC engineer Akira Nakashima published a series of papers introducing switching circuit theory, using digital electronics for Boolean algebraic operations, influencing Claude Shannon's seminal 1938 paper \"A Symbolic Analysis of Relay and Switching Circuits\".\n\nThe 1937 Atanasoff–Berry computer design was the first digital electronic computer (though not programmable), and the Z3 computer from 1941, by German inventor Konrad Zuse was the first working programmable, fully automatic computing machine.\n\nAlan Turing modelled computation in terms of a one-dimensional storage tape, leading to the idea of the Turing machine and Turing-complete programming systems.\n\nDuring World War II, ballistics computing was done by women, who were hired as \"computers.\" The term computer remained one that referred to mostly women (now seen as \"operator\") until 1945, after which it took on the modern definition of machinery it presently holds.\n\nThe ENIAC (Electronic Numerical Integrator And Computer) was the first electronic general-purpose computer, announced to the public in 1946. It was Turing-complete, digital, and capable of being reprogrammed to solve a full range of computing problems. Women implemented the programming for machines like the ENIAC, and men created the hardware.\n\nThe Manchester Baby was the first electronic stored-program computer. It was built at the Victoria University of Manchester by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948. The first stored-program transistor computer was the ETL Mark III, developed by Japan's Electrotechnical Laboratory from 1954 to 1956.\n\nThe microprocessor was introduced with the Intel 4004. It began with the \"Busicom Project\" as Masatoshi Shima's three-chip CPU design in 1968, before Sharp's Tadashi Sasaki conceived of a single-chip CPU design, which he discussed with Busicom and Intel in 1968. The Intel 4004 was then developed as a single-chip microprocessor from 1969 to 1970, led by Intel's Marcian Hoff and Federico Faggin and Busicom's Masatoshi Shima. The microprocessor led to the development of microcomputers, and the microcomputer revolution.\n\nThe 1980s brought about significant advances with microprocessor that greatly impacted the fields of engineering and other sciences. The Motorola 68000 microprocessor had a processing speed that was far superior to the other microprocessors being used at the time. Because of this, having a newer, faster microprocessor allowed for the newer microcomputers that came along after to be more efficient in the amount of computing they were able to do. This was evident in the 1983 release of the Apple computer Lisa. Lisa was the first personal computer with graphical user interface (GUI) that was sold commercially, she ran on the Motorola 68000, dual floppy disk drives, a 5 MB hard drive and had 1MB of RAM . After successfully launching Lisa, a year later Apple released its first Macintosh computer still running on the Motorola 68000 microprocessor. Another advancement because of microprocessors came from Texas Instruments. Texas Instruments first introduced their TMS9900 processor in June 1976. They then used their microprocessor in their TI 99/4 computer.\n\nLate 1980s and beginning in the early 1990s we see more advances with actual computers to aid with actual computing. In 1990, Apple released the Macintosh Portable, it was heavy weighing and extremely expensive. It was not met with great success and was discontinued only two years later. That same year Intel introduced the Touchstone Delta supercomputer, which had 512 microprocessors. This technological advancement was very significant as it was used as a model for some of the fastest multi-processors systems in the world. It was even used a prototype for Caltech researchers who used the model for projects like real time processing of satellite images and simulating molecular models for various fields of research.\n\nStarting with known special cases, the calculation of logarithms and trigonometric functions can be performed by looking up numbers in a mathematical table, and interpolating between known cases. For small enough differences, this linear operation was accurate enough for use in navigation and astronomy in the Age of Exploration. The uses of interpolation have thrived in the past 500 years: by the twentieth century Leslie Comrie and W.J. Eckert systematized the use of interpolation in tables of numbers for punch card calculation.\n\nThe numerical solution of differential equations, notably the Navier-Stokes equations was an important stimulus to computing,\nwith Lewis Fry Richardson's numerical approach to solving differential equations. The first computerised weather forecast was performed in 1950 by a team composed of American meteorologists Jule Charney, Philip Thompson, Larry Gates, and Norwegian meteorologist Ragnar Fjørtoft, applied mathematician John von Neumann, and ENIAC programmer Klara Dan von Neumann. To this day, some of the most powerful computer systems on Earth are used for weather forecasts.\n\nBy the late 1960s, computer systems could perform symbolic algebraic manipulations well enough to pass college-level calculus courses.\n\n\n\n\n"}
{"id": "40973575", "url": "https://en.wikipedia.org/wiki?curid=40973575", "title": "History of smart antennas", "text": "History of smart antennas\n\nThe first smart antennas were developed for military communications and intelligence gathering. The growth of cellular telephone in the 1980s attracted interest in commercial applications. The upgrade to digital radio technology in the mobile phone, indoor wireless network, and satellite broadcasting industries created new opportunities for smart antennas in the 1990s, culminating in the development of the MIMO (multiple-input multiple-output) technology used in 4G wireless networks.\n\nThe earliest success at tracking and controlling wireless signals relied on the antennas’ physical configuration and motion. The German inventor and physicist Karl F. Braun demonstrated beamforming for the first time in 1905. Braun created a phased array by positioning three antennas to reinforce radiation in one direction and diminish radiation in other directions. Guglielmo Marconi experimented with directional antennas in 1906.\nDirectional antennas were rotated to detect and track enemy forces during World War I. The British admiralty used goniometers (radio compasses) to track the German fleet. Edwin H. Armstrong invented the superheterodyne receiver to detect the high frequency noise generated by German warplanes’ ignition systems. The war ended before Armstrong’s creation was ready to help direct antiaircraft fire.\nMultiple elements (a fed dipole, a director, and reflectors) were assembled in the 1920s to create narrow transmit and receive antenna patterns. The Yagi-Uda array, better known as the Yagi antenna, is still widely used. Edmond Bruce and Harald T. Friis developed directional antennas for shortwave and microwave frequencies during the 1930s.\n\nAT&T’s decision to use microwave to carry inter-city telephone traffic led to the first large-scale commercial deployment of directional antennas (based on Friis’ horn reflector design) in 1947. Directional antennas with alternating polarization enabled a single pair of frequencies to be reused over many consecutive hops. Microwave links are less expensive to deploy and maintain than coaxial cable links.\n\nThe first mechanically scanned phased array radar (using a rotating Yagi antenna) was demonstrated in the 1930s. The first electronically scanned radars used electromechanical devices (such as mechanical tuners or switches) to steer the antenna’s beam.\n\nGermany built the Wullenweber circular array for direction finding during the early years of World War II. The Wullenweber could electronically scan the horizon 360° and determine the direction of any signal with reasonably good accuracy. Circular arrays were enhanced during the Cold War for eavesdropping purposes.\nThe American Physicist Luis Walter Alvarez developed the first ground-controlled approach (GCA) system for landing aircraft in bad weather based on an electronically steered microwave phased array antenna. Alvarez tested and deployed the system in England in 1943. Near the end of the war, Germany’s GEMA built an early warning phased array radar system (the PESA Mammut 1) to detect targets up to 300 km away. The polyrod fire control antenna was developed by Bell Laboratories in 1947 using cascaded phase shifters controlled by a rotary switch (spinning at ten revolutions per second) to create a continuous scanning beam.\n\nA major push to meet national security response time and coverage requirements called for the development of an all-electronic steerable planar phased array radar. The USSR’s launch of Sputnik in 1957 suggested the need for ground-based satellite surveillance systems. Bendix Corporation responded by building its Electronically Steerable Array Radar (ESAR) in 1960. Enhanced beamforming techniques, such as multiple-beam Butler matrices, were developed for detecting and tracking objects in space.\n\nThe launch of Explorer 1 by the United States in 1958 suggested another application: space-based radar systems for detecting and tracking aircraft, ships, armored vehicles, ballistic missiles, and cruise missiles. These systems required the development of special techniques for canceling the radar clutter seen from space, nulling ground-based jammers, and compensating for Doppler shifts experienced by fast-moving satellites.\n\nSpace-based radar systems spurred the development of smaller, lighter weight, and less costly components: monolithic microwave integrated circuits (MMICs) for operation at frequencies in the 1 GHz to 30 GHz (microwave) and 30 GHz to 300 GHz (millimeter wave) ranges. The high power levels needed for detection are easier to achieve at microwave frequencies. The narrow beams required for high resolution target tracking are best achieved at millimeter wave frequencies. Companies such as Texas Instruments, Raytheon, RCA, Westinghouse, General Electric, and Hughes Electronics participated in the early development of MMICs.\n\nThe first all-solid state radar was built for the United States Marines in 1972 by General Electric. It was a mobile 3-D radar system with its array mounted on a rotating platform for scanning the horizon. The first all-solid state phased array radar was the PAVE PAWS (precision acquisition vehicle entry - phased array warning system) UHF radar built in 1978 for the United States Air Force.\nPhased array antennas are also used in radio astronomy. Karl Jansky, discoverer of the radio waves emanating from the Milky Way galaxy, used a Bruce array for experiments he conducted in 1931. Modern phased array radio telescopes typically consist of a number of small, interconnected antennas such as the Murchison Widefield Array in Australia, constructed in 2012.\n\nL. C. van Atta was first to describe a retrodirective antenna, which redirects (rather than reflects) a signal back in the direction from which it came, in his 1959 patent. The signal can be modulated by the redirecting host for purposes such as radio-frequency identification and traffic control (radar target echo enhancement).\nThe first adaptive array, the side-lobe canceller, was developed by Paul Howells and Sid Applebaum at General Electric in 1959 to suppress radar jamming signals. Building on Norbert Wiener’s work with analog filters, in 1960 Stanford University professor Bernard Widrow and PhD student Ted Hoff developed the least mean squares (LMS) algorithm that automatically adjusts an antenna’s directivity pattern to reinforce desired signals.\nTed Compton at Ohio State University developed an adaptive antenna technique for recovering direct sequence spread spectrum signals in the presence of narrowband co-channel interference. Compton’s method, reported in 1974, only requires knowledge of the desired signal’s pseudorandom noise (PN) code—not its direction of arrival. In the late 1970s, Kesh Bakhru and Don Torrieri developed the maximin algorithm for recovering frequency hopping signals in the presence of narrowband co-channel interference.\nA 1977 paper by Bell Labs researchers Douglas O. Reudink and Yu S. Yeh described the advantages of scanning spot beams for satellites. The authors estimated that scanning spot beams could save 20 dB in link budget which in turn could be used to reduce transmit power, increase communication capacity, and decrease the size of earth-station antennas. Satellite spot beams are used today by direct broadcast satellite systems such as DirecTV and Dish Network.\n\nThe Strategic Defense Initiative (SDI), proposed in 1983, became a major source of funding for technology research in several areas. The algorithms developed to track intercontinental ballistic missiles and direct x-ray laser weapons were particularly relevant to smart antennas.\n\nThese are antenna arrays with multi channels \"digital beamforming\", usually by using FFT.\n\nThe theory of the 'digital antenna arrays' (DAA) started to emerge as a theory of multichannel estimation. Its origins go\nback into methods developed in the 1920s that were used to determine direction of the arrival of radio signals by a set of\ntwo antennas based on the phase difference or amplitudes of their output voltages. Thus, the assessment of the directions of\narrival of a single signal was conducted according to pointedtype indicator readings or according to the Lissajous curves,\ndrawn by beam on the oscilloscope screen.\n\nIn the late 1940s this approach caused the emergence of the theory of three-channel antenna analyzers that provided the\nsolution to the problem of signal separation of air target and “antipode” reflected from the underlying surface by solving\nsystem of equations which were obtained with the help of complex voltages of three-channel signal mix.\n\nThe growing complexity of solving such radar challenges, as well as the need to implement effective signal processing by\nthe end of the 1950s predetermined the use of electronic computers in this field. For example, in 1957, Ben S. Meltont\nand Leslie F. Bailey published a very significant article in this field, where authors offered options of implementation of algebraic operations for signal processing with the help of electronic circuits, their equivalents, with the aim to develop signal correlator on the base of certain analogue computer.\n\nThe replacement of analogue computer facilities by digital technologies three years after in 1960 was embodied in the\nidea of using high-speed computers to solve directional finding problems, initially to locate earthquake epicenter. B. A. Bolt\nwas one of the first who implemented this idea in practice, he has developed a program for IBM 704 for seismic direction finding based on the method of least squares. Almost simultaneously a similar approach was used by Flinn, research fellow of the Australian National University.\n\nDespite the fact that in the mentioned experiments the interface between sensors and computer was implemented with the help of data input cards, such decision was a decisive step on the way of the appearance of the DAA. Then, there was only to solve the problem of direct digital data, obtained from sensing elements, input into computer, excluding the stage of preparation of punch card and operator assistance as a surplus link.\n\nApparently, it was Polikarpov B.I. who first drew attention to the potential possibilities of multichannel analyzers in the\nformer USSR Polikarpov B.I. shows the principal possibility of signal sources\nresolution with an angular distance less than aperture angle of the antenna system.\n\nHowever, a specific solution to the problem of superRayleigh resolution of the emission sources was proposed by Varyukhin V.A. and Zablotskiy M.A. only in 1962, they invented corresponding method of measuring of directions to sources of electromagnetic field. This method was based on the processing of information contained in the distribution of complex voltage amplitudes at the outputs of amplitude, phase and phase-amplitude multichannel analyzers and it permitted to determine the angular coordinates of sources within the width of the main lobe of the receiving antenna system.\n\nFurther Varyukhin V.A. developed a general theory of multichannel analyzers, based on the processing of information contained in the distribution of complex voltage amplitudes at the outputs of the digital antenna array. An important milestone in the recognition of the scientific results of Varyukhin V.A. was the defence of his doctor of science dissertation, held in 1967.\n\nA distinctive feature of developed by him theoretical foundations is the maximum automation of the process of assessment of the coordinates and parameters of signals, whereas an approach based on the generation of the response function of seismic multichannel analyzer and assessment of its resolution capabilities on the basis of visual impressions was just arisen at that time. What is meant here is a Capon method and developed further multiple signal classification (MUSIC), Estimation of signal parameters via rotational invariance techniques (ESPRIT) methods and other projection methods of spectral estimation.\nOf course, it is ungrateful to make a conclusion about the priority and importance of various alternative scientific\napproaches in the process of development of a general theory of the DAA, taking into account classified nature of the\nmajority works and the lack of the possibility to study scientific heritage of that time, even taking into account\nInternet. Proposed here historical journey only slightly raised the veil of time over the true development of scientific\nresearch and its main aim was to point general niche and time frame of the inception of the theory of multichannel analysis\nthrough the lens of historical background. A detailed presentation of the historical stages of development of the DAA theory deserves standalone consideration.\n\nA 1979 paper by Ralph O. Schmidt of Electromagnetic Systems Laboratory (ESL, a supplier of strategic reconnaissance systems) described the multiple signal classification (MUSIC) algorithm for estimating signals’ angle of arrival. Schmidt used a signal subspace method based on geometric modeling to derive a solution assuming the absence of noise and then extended the method to provide a good approximation in the presence of noise. Schmidt’s paper became the most cited and his signal subspace method became the focus of ongoing research.\n\nJack Winters showed in 1984 that received signals from multiple antennas can be combined (using the optimum combining technique) to reduce co-channel interference in digital mobile networks. Up to this time, antenna diversity had only been used to mitigate multipath fading. However, digital mobile networks would not become common for another ten years.\n\nRichard Roy developed the Estimation of signal parameters via rotational invariance techniques (ESPRIT) algorithm in 1987. ESPRIT is a more efficient and higher resolution algorithm than MUSIC for estimating signals’ angle of arrival.\nBrian Agee and John Treichler developed the constant modulus algorithm (CMA) for blind equalization of analog FM and telephone signals in 1983. CMA relies on knowledge of the signal’s waveform rather than channel state information or training signals. Agee extended the CMA to adaptive antenna arrays over the next few years.\n\nDuring the 1990s companies such as Applied Signal Technology (AST) developed airborne systems to intercept digital cellular phone calls and text messages for law enforcement and national security purposes. While an airborne system can eavesdrop on a mobile user anywhere in a cellular network, it will receive all mobile stations reusing the same user and control frequencies at roughly the same power level. Adaptive antenna beamforming and interference cancellation techniques are used to focus on the target user. AST was acquired by Raytheon in 2011.\n\nIn 1947 Douglas H. Ring wrote a Bell Laboratories internal memorandum describing a new way to increase the capacity of metropolitan radio networks. Ring proposed dividing a city into geographic cells, using low power transmitters with omnidirectional antennas, and reusing frequencies in non-adjacent cells. Ring’s cellular radio scheme did not become practical until the arrival of integrated circuits in the 1970s.\n\nAs the number of mobile phone subscribers grew in the 1980s and 1990s researchers investigated new ways to increase mobile phone network capacity. Directional antennas were used to divide cells into sectors. In 1989, Simon Swales at Bristol University in the United Kingdom proposed methods for increasing the number of simultaneous users on the same frequency. Receive signals can be distinguished based on differences in their direction-of-arrival at the cell site antenna array. Transmit signals can be aimed at the intended recipient using beamforming. Soren Anderson in Sweden presented a similar scheme based on computer simulations the following year.\nRichard Roy and Björn Ottersten at Arraycomm patented a spatial division multiple access method for wireless communication systems in the early 1990s. This technology was employed in Arraycomm’s IntelliCell product line.\n\nRichard Roy and French entrepreneur Arnaud Saffari founded ArrayComm in 1992 and recruited Marty Cooper, who led the Motorola group that developed the first portable cell phone, to head the company. ArrayComm’s smart antennas were designed to increase the capacity of wireless networks employing time division duplex (TDD) such as the PHS (Personal Handy-phone System) networks that were deployed throughout Asia.\nBell Labs researcher Douglas O. Reudink founded Metawave Communications, a maker of switched beam antennas for cellular telephone networks, in 1995. Metawave claimed that by focusing capacity on areas with the highest traffic it could boost cell capacity up to 75%. Though Metawave managed to sell switched beam antennas to at least one major carrier, the company went out of business in 2004.\nIn 1997, AT&T Wireless Group announced plans to offer fixed wireless service at speeds up to 512 kbit/s. Project Angel promised non-line of sight (NLOS) coverage using beamforming and orthogonal frequency division multiplexing (OFDM). Service was launched in ten cities in 2000. However, by 2002 AT&T sold its fixed wireless service business to Netro Corp.\n\nSmart antenna research led to the development of 4G MIMO. Conventional smart antenna techniques (such as diversity and beamforming) deliver incremental gains in spectral efficiency. 4G MIMO exploits natural multipath propagation to multiply spectral efficiency.\n\nResearchers studying the transmission of multiple signals over different wires in the same cable bundle helped create a theoretical foundation for 4G MIMO. Specifically, techniques for cancelling the effects of crosstalk using knowledge of the source signals were investigated. The “wireline MIMO” researchers included Lane H. Brandenburg and Aaron D. Wyner (1974),\nWim van Etten (1970s), Jack Salz (1985), and Alexandra Duel-Hallen (1992). Though optimizing the transmission of multiple data streams over different wire pairs in the same bundle requires compensating for crosstalk, the transmission of multiple data streams over different wireless paths due to multipath propagation is a far greater challenge because the signals become mixed up in time, space, and frequency.\n\nGreg Raleigh’s 1996 paper was first to propose a method for multiplying the capacity of point-to-point wireless links using multiple co-located antennas at each end of a link in the presence of multipath propagation. The paper provided a rigorous mathematical proof of MIMO capacity based on a precise channel model and identified OFDM as the most efficient air interface for use with MIMO. The paper was submitted to the IEEE in April 1996 and presented in November at the 1996 Global Communications Conference in London. Raleigh also filed two patent applications for MIMO in August of the same year.\n\nRaleigh discovered that multipath propagation could be exploited to multiply link capacity after developing an improved channel model that showed how multipath propagation affects signal waveforms. The model took into account factors including radio propagation geometry (natural and man-made objects serving as “local reflectors” and “dominant reflectors”), antenna array steering, angle of arrival, and delay spread.\nBell Labs researcher Gerard J. Foschini’s paper submitted in September 1996 and published in October of the same year also theorized that MIMO could be used to significantly increase the capacity of point-to-point wireless links. Bell Labs demonstrated a prototype MIMO system based on its BLAST (Bell Laboratories Layered Space-Time) technology in late 1998.\nSpace-time block code (also known as the Alamouti code) was developed by Siavash Alamouti and is widely used in MIMO-OFDM systems. Alamouti’s 1998 paper showed that the benefits of receive diversity can also be achieved using a combination of transmit diversity and space-time block codes. A key advantage of transmit diversity is that it does not require multiple antennas and RF chains in handsets.\n\nOFDM emerged in the 1950s when engineers at Collins Radio Company found that a series of non-contiguous sub-channels are less vulnerable to inter-symbol interference (ISI). OFDM was studied more systematically by Robert W. Chang in 1966. Chang used Fourier transforms to ensure orthogonality. Sidney Darlington proposed use of the discrete Fourier transform (DFT) in 1970. Stephen B. Weinstein and Paul M. Ebert used a discrete Fourier transform (DFT) to perform baseband modulation and demodulation in 1971.\nDial-up modems developed by Gandalf Technologies and Telebit in the 1970s and 1980s used OFDM to achieve higher speeds. Amati Communications Corp. used its discrete multi-tone (DMT) form of OFDM to transmit data at higher speeds over phone lines also carrying phone calls in digital subscriber line (DSL) applications. OFDM is part of the digital audio broadcasting (DAB) and digital video broadcasting (DVB) standards developed in Europe. OFDM is also used in the 802.11a and 802.11g wireless LAN standards.\n\nGreg Raleigh, V. K. Jones, and Michael Pollack founded Clarity Wireless in 1996. The company built a prototype MIMO-OFDM fixed wireless link running 100 Mbit/s in 20 MHz of spectrum in the 5.8 GHz band, and demonstrated error-free operation over six miles with one watt of transmit power. Cisco Systems acquired Clarity Wireless in 1998 for its non-line of sight, vector OFDM (VOFDM) technology. The Broadband Wireless Industry Forum (BWIF) was created in 1999 to develop a VOFDM standard.\nArogyaswami Paulraj founded Iospan Wireless in late 1998 to develop MIMO-OFDM products. Iospan was acquired by Intel in 2003. Neither Clarity Wireless nor Iospan Wireless shipped MIMO-OFDM products before being acquired.\n\nGreg Raleigh and V. K. Jones founded Airgo Networks in 2001 to develop MIMO-OFDM chipsets for wireless LANs. In 2004, Airgo became the first company to ship MIMO-OFDM products. Qualcomm acquired Airgo Networks in late 2006.\nSurendra Babu Mandava and Arogyaswami Paulraj founded Beceem Communications in 2004 to produce MIMO-OFDM chipsets for WiMAX. The company was acquired by Broadcom in 2010.\nThe Institute of Electrical and Electronics Engineers (IEEE) created a task group in late 2003 to develop a wireless LAN standard delivering at least 100 Mbit/s of user data throughput. There were two major competing proposals: TGn Sync was backed by companies including Intel and Philips, and WWiSE was supported by companies including Airgo Networks, Broadcom, and Texas Instruments. Both groups agreed that the 802.11n standard would be based on MIMO-OFDM with 20 MHz and 40 MHz channel options. TGn Sync, WWiSE, and a third proposal (MITMOT, backed by Motorola and Mitsubishi) were merged to create what was called the Joint Proposal. The final 802.11n standard supported speeds up to 600 Mbit/s (using four simultaneous data streams) and was published in late 2009.\nWiMAX was developed as an alternative to cellular standards, is based on the 802.16e standard, and uses MIMO-OFDM to deliver speeds up to 138 Mbit/s. The more advanced 802.16m standard enabled download speeds up to 1 Gbit/s. A nationwide WiMAX network was built in the United States by Clearwire, a subsidiary of Sprint-Nextel, covering 130 million pops by mid-2012. Clearwire subsequently announced plans to deploy LTE (the cellular 4G standard) covering 31 cities by mid-2013.\nThe first 4G cellular standard was proposed by NTT DoCoMo in 2004. Long term evolution (LTE) is based on MIMO-OFDM and continues to be developed by the 3rd Generation Partnership Project (3GPP). LTE specifies downlink rates up to 300 Mbit/s, uplink rates up to 75 Mbit/s, and quality of service parameters such as low latency. LTE Advanced adds support for picocells, femtocells, and multi-carrier channels up to 100 MHz wide. LTE has been embraced by both GSM/UMTS and CDMA operators.\n\nThe first LTE services were launched in Oslo and Stockholm by TeliaSonera in 2009. Deployment is most advanced in the United States, where all four Tier 1 operators have or are constructing nationwide LTE networks. There are currently more than 222 LTE networks in 83 countries operational with approximately 126 million connections (devices).\n\nThe 802.11ac wireless LAN standard was proposed to deliver speeds of 1 Gbit/s and faster. Development of the specification began in 2011 and is expected to be completed by 2014. 802.11ac uses the 5 GHz band, defines channels up to 160 MHz wide, supports up to 8 simultaneous MIMO data streams, and delivers raw data rates up to nearly 7 Gbit/s. A number of products based on 802.11ac draft specifications are now available.\n\nFifth generation (5G) mobile network concepts are in the exploratory stage. Commercialization is expected by the early 2020s. In March 2013, NTT DoCoMo tested a10 Gbit/s uplink using 400 MHz in the 11 GHz band. In May 2013, Samsung announced that it is experimenting in the 28 GHz band using base stations with up to 64 antennas and has achieved 1 Gbit/s at distances up to 2 kilometers. Samsung claims the technology could deliver tens of Gbit/s under favorable conditions.\nResearch papers suggest that 5G networks are likely to consist of small distributed cells operating at frequencies up to 90 GHz using “massive MIMO.” According to Jakob Hoydis of Bell Laboratories, Alcatel-Lucent, Germany, “Network densification is the only solution to the capacity crunch.” This could involve two-tier networks (“HetNets”) using existing cellular base stations to ensure broad coverage and high mobility and interspersed small cells for capacity and indoor service. Massive MIMO would also be employed in high-speed backhaul links.\n\n"}
{"id": "41192267", "url": "https://en.wikipedia.org/wiki?curid=41192267", "title": "Honeywell Primus", "text": "Honeywell Primus\n\nHoneywell Primus is a range of Electronic Flight Instrument System (EFIS) glass cockpits manufactured by Honeywell Aerospace.\nEach system is composed of multiple display units used as primary flight display and multi-function display.\n\n\"Primus 1000\" is used on:\n\n\"Primus 2000\" and \"Primus 2000XP\" are used on:\n\n\"Primus Elite\" is an upgrade to older SPZ-8000 series and \"Primus 1000\" and \"2000/2000XP\" flight decks. The upgrade include replacing the CRT display with the new light weight LCD displays. The Primus Elite displays also include enhanced capability of SVS (Synthetic Vision System), Jeppesen Charts, Enhanced with XM weather, airports, Navaids, TAF, METARs, Geopolitical boundary, Airways, Airspace information, NOTAMs and many more features. The Multi function display will have cursor control device (CCD) to select the various above listed options.\n\n\"Primus Apex\" is based on the \"Primus Epic\" and is designed for single-pilot turboprop aircraft and very light jets.\n\nIt is installed in:\n\n\"Primus Epic\" and \"Primus Epic 2\" are designed for two-crew business or regional jets.\n\nThey are used on:\n\nDassault's Enhanced Avionics System (EASy) was jointly developed with Honeywell and is based on the \"Primus Epic\".\n\nGulfstream Aerospace's \"PlaneView\" cockpit is also based on the \"Primus Epic\".\n\n\"Primus Apex\" flight deck competes with Garmin G1000 and Avidyne Entegra while \"Primus Epic\" competes with Rockwell Collins \"Pro Line\" on larger aircraft.\n\n"}
{"id": "34107794", "url": "https://en.wikipedia.org/wiki?curid=34107794", "title": "Kolskaya (jack-up rig)", "text": "Kolskaya (jack-up rig)\n\nKolskaya was a jack-up rig operating in the Russian Far East. It was built by Rauma-Repola in Pori, Finland in 1985 and was owned by the Russian company \"ArktikmorNeftegazRazvedka\" (AMNGR), a subsidiary of Zarubezhneft.\n\n\"Kolskaya\" was an independent leg cantilever type jack-up rig. It was long and wide, and could accommodate up to 102 people. Its rated water depth for operations was . Its drilling depth was .\n\nOn December 18, 2011 the rig, which was under tow during a fierce storm, capsized and sank in the Sea of Okhotsk. It was being towed by the icebreaker \"Magadan\" and the tugboat \"Neftegaz-55\" having just completed an exploration well for Gazprom off the Kamchatka Peninsula. The incident happened some off the coast of Sakhalin island, in waters more than deep. The towing operation was illegal since the platform’s manufacturer explicitly stated that “towing is prohibited in the winter, in winter seasonal zones.” \n\nA search and rescue effort began as soon as the rig sank and was halted five days later on December 22. Of the 67 people known to have been aboard \"Kolskaya\", 14 had been rescued and 36 more were listed as missing. Only 17 bodies had been recovered. With 53 declared missing or dead, it was the largest number of casualties in an accident the Russian oil sector has ever experienced.\n\n"}
{"id": "6936785", "url": "https://en.wikipedia.org/wiki?curid=6936785", "title": "Limoges porcelain", "text": "Limoges porcelain\n\nLimoges porcelain is hard-paste porcelain produced by factories in and around the city of Limoges, France beginning in the late 18th century, but does not refer to a particular manufacturer. By about 1830 Limoges, which was close to the areas where suitable clay was found, had replaced Paris as the main centre for private porcelain factories, although the state-owned Sèvres porcelain near Paris remained dominant at the very top of the market. Limoges has maintained this position to the present day. \n\nLimoges had strong antecedents in the production of decorative objects. The city was the most famous European centre of vitreous enamel production in the 12th century, and Limoges enamel was known as \"Opus de Limogia\" or \"Labor Limogiae\".\n\nLimoges had also been the site of a minor industry producing plain faience earthenware since the 1730s. \n\nThe manufacturing of hard-paste porcelain at Limoges was established by Turgot in 1771 following the discovery of local supplies of kaolin and a material similar to petuntse in the economically distressed area at Saint-Yrieix-la-Perche, near Limoges. The materials, which were quarried beginning in 1768, were used to produce hard-paste porcelain similar to Chinese porcelain. \n\nA manufactory at Limoges was placed under the patronage of the comte d'Artois, brother of Louis XVI, and was later purchased by the King in 1784, apparently with the idea of producing hard-paste bodies for decoration at Sèvres, although this never happened.\n\nAfter the French Revolution a number of private factories were established at Limoges, including Bernardaud and Haviland & Co.\n\nLimoges maintains the position it established in the 19th century as the premier manufacturing city of porcelain in France.\n\nCounterfeiting of Limoges porcelain has been documented for decades.\n\n\n"}
{"id": "4220348", "url": "https://en.wikipedia.org/wiki?curid=4220348", "title": "List of English inventions and discoveries", "text": "List of English inventions and discoveries\n\nEnglish inventions and discoveries are objects, processes or techniques invented, innovated or discovered, partially or entirely, in England by a person from England (that is, someone born in England – including to non-English parents – or born abroad with at least one English parent and who had the majority of their education or career in England). Often, things discovered for the first time are also called inventions and in many cases, there is no clear line between the two.\n\nThe following is a list of inventions, innovations or discoveries known or generally recognised to be English.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1835 – Der Adler. First steam locomotive in Germany. Built by George & Robert Stephenson in Newcastle.\n\n\n\n\n\n\n"}
{"id": "1444981", "url": "https://en.wikipedia.org/wiki?curid=1444981", "title": "Micro process engineering", "text": "Micro process engineering\n\nMicro process engineering is the science of conducting chemical or\nphysical processes (unit operations) inside small volumina,\ntypically inside channels with diameters of less than 1 mm\n(microchannels) or other structures with sub-millimeter dimensions. \nThese processes are usually carried out in continuous\nflow mode, as opposed to batch production, allowing a throughput\nhigh enough to make micro process engineering a tool for chemical\nproduction. Micro process engineering is therefore not to be confused\nwith microchemistry, which deals with very small overall quantities of\nmatter.\n\nThe subfield of micro process engineering that deals with chemical\nreactions, carried out in microstructured reactors or\n\"microreactors\", is also known as\nmicroreaction technology.\n\nThe unique advantages of microstructured reactors or \"microreactors\" are enhanced heat transfer due to the large surface area-to-volume ratio, and enhanced\nmass transfer. For example, the length scale of diffusion\nprocesses is comparable to that of microchannels or even shorter, and efficient mixing\nof reactants can be achieved during very short times (typically\nmilliseconds). The good heat transfer properties allow a precise\ntemperature control of reactions. For example, highly \nexothermic reactions \ncan be conducted almost isothermally when the\nmicrostructured reactor contains a second set of microchannels (\"cooling\npassage\"), fluidically separated from the reaction channels (\"reaction\npassage\"), through which a flow of cold fluid with sufficiently high\nheat capacity is maintained. It is also possible to change the temperature of\nmicrostructured reactors very rapidly to intentionally achieve a non-isothermal behaviour.\n\nWhile the dimensions of the individual channels are small, a micro\nprocess engineering device (\"microstructured reactor\") can contain many\nthousands of such channels, and the overall size of a microstructured\nreactor can be on the scale of meters. The objective of micro process\nengineering is not primarily to miniaturize production plants,\nbut to increase yields and selectivities of chemical reactions, thus\nreducing the cost of chemical production. This goal can be achieved by\neither using chemical reactions that cannot be conducted in larger\nvolumina, or by running chemical reactions at parameters (temperatures,\npressures, concentrations) that are inaccessible in larger volumina due\nto safety constraints. For example, the detonation of the\nstoichiometric mixture of two volume unit of hydrogen gas and\none volume unit of oxygen gas does not propagate in microchannels\nwith a sufficiently small diameter. This property is referred to as the\n\"intrinsic safety\" of microstructured reactors. The improvement of yields\nand selectivities by using novel reactions or running reactions at more\nextreme parameters is known as \"process intensification\".\n\nHistorically, micro process engineering originated around the 1980s,\nwhen mechanical micromachining methods developed for the fabrication of\nuranium isotope separation nozzles were first applied to the\nmanufacturing of compact heat exchangers at the \nKarlsruhe (Nuclear) Research Center.\n\n"}
{"id": "35552932", "url": "https://en.wikipedia.org/wiki?curid=35552932", "title": "Mikhail Agursky", "text": "Mikhail Agursky\n\nMikhail Samuilovich Agursky (; 1933 – 21 August 1991), real name Melik Samuilovich Agursky (), was a Jewish sovietologist, cybernetic and historian of National Bolshevism. Agursky was a member of the Soviet Academy of Sciences and Hebrew University of Jerusalem. \nMikhail Agursky was the pen name of Melik Agursky. Other variations of the name are \"Melir\" () and \"Melib\" ().\nHe was the son of a famous revolutionary, a historian and party leader Samuel (Shmuel) Haimovich Agursky (1884-1947). In 1955 he married Vera Feodorovna Kondratieva.\n\nMikhail Agursky was born as Melik Samuilovich Agursky (Мэ́лик Самуилович Агу́рский) in Moscow in 1933 to a Jewish family. His father Samuel Agursky was revolutionary and historian. Mikhail Agursky received an education in engineering and defended a dissertation on cybernetics. In 1975 he emigrated to Israel. Agursky became a Fellow of the Soviet and East European Research Center at the Hebrew University of Jerusalem. His book \"The Ideology of National Bolshevism\" was published in Paris in 1980. On 21 August 1991 Agursky was found dead on in his hotel room in Moscow.\n\n"}
{"id": "35014091", "url": "https://en.wikipedia.org/wiki?curid=35014091", "title": "Moustapha Alassane, cinéaste du possible", "text": "Moustapha Alassane, cinéaste du possible\n\nMoustapha Alassane, cinéaste du possible is a 2009 documentary film.\n\nMoustapha Alassane is a living legend in African cinema. His adventures take us to the era of \"pre-cinema\", to the times of magical lantern and Chinese shadows. He is the first director of Nigerien cinema and animation films in Africa. He tells very old stories with current technology, but he also narrates the most current events with the most archaic means. This documentary not only tells the adventure of a human being and an extraordinary professional, but the memories of a generation, the history of a country, Niger in its golden age of cinema.\n"}
{"id": "6519136", "url": "https://en.wikipedia.org/wiki?curid=6519136", "title": "Multibanco", "text": "Multibanco\n\nMultibanco is an interbank network in Portugal owned and operated by Sociedade Interbancária de Serviços S.A., or SIBS, that links the ATMs of 27 banks in Portugal, totaling 12,700 machines as of December 2014. The bank members of Multibanco control the SIBS. Multibanco is a fully integrated interbank network. One of the most notable characteristics of Multibanco is the wide range of services that can be utilised through its machines.\n\nMultibanco in itself does not only encompass ATMs. It has a fully-fledged EFTPOS network called Multibanco Automatic Payment, and is also a provider of mobile phone and Internet banking services through the TeleMultibanco and MBNet services respectively. It is also the provider of the Via Verde automated toll payment service.\n\nThe Caixa Automático Multibanco (Multibanco ATM) was the first project developed by SIBS and started operating on September 2, 1985, with the installation of 12 terminals in the cities of Lisbon and Porto.\n\nIn 1995, the network had 3,745 ATMs; in 2006 had more than 11,200; and as of December 2014 it had 12,700, on which an average of more than 75 million operations are made every month.\n\nMultibanco is known for having more functionality than standard ATMs in other countries. Initially, the machines only offered withdrawal of cash, checking of balances and checking of recent transactions. Later, features such as service and shopping payments, Prepaid mobile phone top-up and show ticket sales, among others, were introduced. Nowadays, 60 different services are offered by the Multibanco network.\n\nFrom 2001 to 2005, the average number of transactions by year is over 630 million. The number has been rising every year; 719 million were recorded in 2005. This has since grown substantially, with 900 million operations recorded in 2014.\n\nMultibanco allows users with a local (Portuguese) bank account to withdraw a maximum of 400 Euros daily (holders of foreign accounts can withdraw more, depending on their bank). If the user fails to enter the correct PIN after three attempts, the ATM will not return the card. If the user fails to take the money out of the ATM after a certain period of time, the ATM takes back the cash to prevent theft. A recent security measure has been introduced under the form of a tinting mechanism that releases ink and stains the money in the event of forced opening of the ATM. When in possession of a tinted bill, one must report it to the police (for further investigation, as it is likely to be from a stolen ATM) and can later have it traded at a bank for a non-tinted one, without additional charges. In addition to security cameras, many ATM terminals include a rear-view mirror above the interactive screen, and the money withdrawal slot is located at approximately waist-level, for quick and more secluded storage of the money.\n\nThe Multibanco Automatic Payment is an EFTPOS network which began operating in 1987. It allows electronic payments made at the point of sale, through the use of a system accepted card. Therefore, this service allows real time payments protected with security measures, with 266,000 such terminals on the network as of 2014. In 2014, 781 million payments were made using this system.\n\nMB PHONE was launched as TeleMultibanco in 1996 and rebranded to the current designation in 2008. It allows its users to check their account balance and transactions, make transfers, service and shopping payments, and request cheque books through their mobile phones. In 2009, around 3 million transactions were made by the 300,000 mobile phone clients using this system.\n\nMB NET allows users to make payments through the Internet and it is set up for online shopping. Over 1 million payments were made using MB NET in 2009.\nMB NET allows the creation of virtual credit cards to be used online. Depending on the card system, one will get an American Express, VISA or MasterCard virtual card. For example, if the card is a VISA or Visa Electron then the generated cards by MB NET are VISAs. This system is compatible with most payment systems, including PayPal and all places that accept credit cards as payment.\n\nTo generate a virtual credit card one must login to the MB NET site and enter a limit. Then the site instantly gives the credit card number and security codes (CVC2 and CVV2) to pay for your online purchase. The number is also valid for offline purchases. The virtual cards allow for normal credit card interactions; either payment or crediting the holder's account. There are two types of virtual card—one for one-time use, and one meant for multiple uses.\n\nAll cards are given a usable lifetime of between 1 and 12 months. More specifically, they are valid until the end of the month following the one when it is requested/issued. The great advantage of MB NET is that the customer's real payment details are never known to the vendor and the card's limit can be set according to the purchase's value. Even if a vendor or a hacker tries to extort a higher amount, the defined limit prevents it. Shopping online is hence made safer.\n\nRegistration to use MB NET is required. To do so, one must go to a Multibanco ATM or one's home banking website (if the bank allows that operation on such a medium). The client sets a password but the username is given by the system. Registration links the MB NET service to a debit or credit card of one of the SIBS associated banking institutions that support it.\n\nThe automated toll payment service Via Verde is also operated by SIBS. Host to Host is the name given to the system that allows users to perform ATM transactions through their own bank site, enabling online banking which has been active since 2000.\n\n\n"}
{"id": "43595729", "url": "https://en.wikipedia.org/wiki?curid=43595729", "title": "NanoRacks CubeSat Deployer", "text": "NanoRacks CubeSat Deployer\n\nThe NanoRacks CubeSat Deployer (NRCSD) is a device to deploy CubeSats into orbit from the International Space Station.\n\nCurrently, there are two CubeSat deployers on board the International Space Station (ISS): The Japanese Experiment Module (JEM) Small Satellite Orbital Deployer (J-SSOD) and the NanoRacks CubeSat Deployer (NRCSD). The J-SSOD is the first of its kind to deploy small satellites from the International Space Station. The NRCSD is the first commercially operated small satellite deployer from the ISS, maximizing full capabilities of each airlock cycle of deployments.\n\nCubeSats belong to a class of research spacecraft called nanosatellites. The basic cube-shaped satellites measure on each side, weigh less than , and have a volume of about , although CubeSats are built, and deployed, that are multiples of 10 cm in length.\n\n, one method of getting CubeSats to orbit is to transport them aboard a larger spacecraft as part of a cargo load to a larger space station. When this is done, deploying the CubeSats into orbit as a separate artificial satellite requires a special apparatus, such as the NanoRacks CubeSat Deployer. The NRCSD is put into position to be grabbed by one of the ISS's robotic arms, which then places the CubeSat deployer into the correct position externally mounted to the ISS to be able to release the miniature satellites into proper orbit.\n\nThe International Space Station was designed to be used as both a microgravity laboratory, as well as a launch pad for low-Earth-orbit services. The Japanese Space Agency's (JAXA) Kibo ISS module includes a small satellite-deployment system called the J-SSOD.\n\nNanoRacks, via its Space Act Agreement with NASA, deployed a CubeSat using the J-SSOD. Seeing the emerging market demand for CubeSats, NanoRacks self-funded its own ISS deployer, with the permission of both NASA and JAXA. NanoRacks evolved away from the J-SSOD due to the small number of satellites that could be deployed in one airlock cycle and their desire to maximize the capacity of each airlock cycle. The J-SSOD used a full airlock cycle to only launch 6U. The NanoRacks CubeSat Deployer uses two airlock cycles, each holding 8 deployers. Each deployer is capable of holding 6U, allowing a total of 48U per airlock cycle.\n\nDeploying CubeSats from ISS has a number of benefits. Launching the vehicles aboard the logistics carrier of ISS visiting vehicle reduces the vibration and loads they have to encounter during launch. In addition, they can be packed in protective materials so that the probability of CubeSat damage during launch is reduced significantly. In addition, for earth observation satellites, such as those of Planet Labs, the lower orbit of the ISS orbit, at roughly 400 km, is an advantage. In addition, the lower orbit allows a natural decay of the satellites, thus reducing the build-up of orbital debris.\n\nThe Japanese Experiment Module Small Satellite Orbital Deployer (J-SSOD) is the first of its kind to deploy small satellites from the International Space Station. The facility provides a unique satellite install case to the Japanese Experiment Module (JEM) Remote Manipulator System (RMS) for deploying small, CubeSat, satellites from the ISS. The J-SSOD holds up to 3 small one-unit (1U, 10 x 10 x 10 centimeters) small CubeSats per satellite install case, 6 in total, though other sizes up to 55 x 55 x 35 cm may also be used. Each pre-packed satellite install case is loaded by crewmembers onto the Multi-Purpose Experiment Platform (MPEP) within the JEM habitable volume. The MPEP platform is then attached to the JEM Slide Table inside the JEM airlock for transfer to the JEMRMS and space environment. The JEMRMS grapples and maneuvers the MPEP and J-SSOD to a predefined deployment orientation and then jettisons the small CubeSat satellites.\n\nThe MPEP is a platform that acts as an interface between operations inside and outside the ISS, and the J-SSOD mechanism is installed on this platform. On July 21, 2012, JAXA launched the Kounotori 3 (HTV-3) cargo spacecraft to the ISS on Expedition 33. The J-SSOD was a payload on this flight along with five CubeSats that were planned to be deployed by the J-SSOD mounted on the JEMRMS (JEM- Remote Manipulator System), a robotic arm, later in 2012. The five CubeSats were deployed successfully on Oct. 4, 2012 by the JAXA astronaut Akihiko Hoshide using the newly installed J-SSOD. This represented the first deployment service of J-SSOD.\n\nIn October 2013, NanoRacks became the first company to coordinate the deployment of small satellites (CubeSats/nanosatellites) from the ISS via the airlock in the Japanese KIBO module. This deployment was done by NanoRacks using J-SSOD. NanoRacks' first customer was FPT University of Hanoi, Vietnam. Their F-1 CubeSat was developed by young engineers and students at FSpace laboratory at FPT University of Hanoi. The mission of F-1 was to \"survive\" the space environment for one month, measuring temperature and magnetic data while taking low-resolution photos of Earth.\n\nIn 2013, NanoRacks sought permission from NASA to develop their own hardware and CubeSat/SmallSat deployer to use over the JEM- Small Satellite Deployer. NanoRacks brought leadership to the American small satellite industry by building a larger deployer capable of deploying 48U of satellites. NanoRacks designed, manufactured, and tested the deployer for NASA and JAXA approval to reach the International Space Station.\n\nThe NanoRacks CubeSat Deployer was launched on January 9, 2014, on the Orbital Sciences Cygnus CRS Orb-1 mission along with 33 small satellites.\n\nThe NanoRacks CubeSat Deployer (NRCSD) is a self-contained CubeSat deployer system that mechanically and electrically isolates CubeSats from the ISS, cargo resupply vehicles, and ISS crew. The NRCSD design is compliant with NASA ISS flight safety requirements and is space-qualified.\n\nThe NRCSD is a rectangular tube that consists of anodized aluminum plates, base-plate assembly, access panels, and deployer doors. The NRCSD deployer doors are located on the forward end, the base-plate assembly is located on the aft end, and access panels are provided on the top. The inside walls of the NRCSD are a smooth-bore design to minimize and/or preclude hang-up or jamming of CubeSat appendages during deployment, should these become released prematurely. However, deployable systems shall be designed such that there is no intentional contact with the inside walls of the NRCSD.\n\nFor a deployment, the platform is moved outside via the Kibo Module's Airlock and slide table that allows the JEMRMS to move the deployers to the correct orientation for the satellite release and also provides command and control to the deployers. Each NRCSD (NanoRacks CubeSat Deployer) is capable of holding six CubeSat Units, allowing it to launch two 3U satellites or a number of 2U and 1U satellites.\n\nQuad-M, Inc. developed the CubeSat Deployer to be compliant with the Cal Poly standard. It was redesigned and manufactured to NanoRacks' specification for use on the International Space Station.\n\nQuad-M performed an initial design analysis to ensure a compliant design. The structural analysis included a modal analysis to evaluate vibration response, and the thermal analysis included calculations to evaluate different door coating options and an initial transient thermal analysis to estimate. In addition, Quad-M performed development tests for: the door release, the CSD/CubeSat Deployment test, random vibration test, and temperature cycling.\n\nCubeSat integration begins with unpacking the CSD from the shipping container and then removing the Base Plate Assembly from the rear of the CSD. Next, the CubeSat is inserted from the rear and is slid up snug against the doors. Additional CubeSats are then inserted from the rear in the same progress. The Base Plate Assembly is then reinstalled. Four jack screws are then adjusted with the Pusher Plate and locked. The Containment Bolt is then removed, and the deployer is packed for shipment.\n\nLaunch vehicle: Orbital Sciences Cygnus (Orb-1)\n\nLaunch date: January 9, 2014\n\nTotal number of CubeSats: 33\n\nPlanet Labs: Doves, Flock 1A (28)\n\nPurpose: These 28 3U CubeSats are working to build an Earth-observation constellation based solely on CubeSats. The CubeSats contain batteries that provide power to the various systems in each Dove. Each satellite has an optical telescope for acquiring high-resolution images of earth. Each satellites uses an X-band system for the downlink of acquired images and systems telemetry at data rates of 120Mbit/s.\n\nNanoSatisfi: ArduSat (1)\n\nPurpose: This 2U CubeSat will provide a platform for students and space enthusiasts to run space-based Arduino experiments. This is a follow up of ArduSat-1 launched in November 2013.\n\nKaunas University of Technology: LitSat-1 (1)\n\nPurpose: To use low-cost open-source hardware and software for its flight computers that will control the satellite payload. The CubeSat carries a VGA camera, a GPS receiver, a linear transponder, and an AX-25 packet radio transponder.\n\nVilnius University: Lituanica SAT-1 (1)\n\nPurpose: One of Lithuania's first satellites (together with LitSat-1). This CubeSat is equipped with a low resolution VGA camera, GPS receiver, 9k6 AX25 FSK telemetry beacon, UHF CW beacon, and a 150 mW V/U FM mode voice repeater. The satellite will transmit payload and sensor data images and three Lithuanian words.\n\nSouthern Stars: SkyCube (1)\n\nPurpose: This crowd-funded 2-Kilogram 1U satellites that features deployable solar panels, four cameras, and communication antennas that are used to receive messages from Earth that are then transmitted at pre-determined times.\n\nUniversity of Peru: UAPSat-1 (1)\n\nPurpose: This 1U CubeSat uses body-mounted solar panels for power-generation. It is equipped with a minicomputer, radio transmitters/receivers, a power control module, and a basic attitude control system. The satellite will transmit telemetry data and temperature sensor readings from inside and outside the spacecraft.\n\nLaunch vehicle: Orbital Sciences Cygnus (Orb-2)\n\nLaunch date: July 13, 2014\n\nTotal number of CubeSats: 32\n\nPlanet Labs: Doves, Flock 1A (28)\n\nPurpose: These 28 3U CubeSats are working to build an Earth-observation constellation based solely on CubeSats. The CubeSats contain batteries that provide power to the various systems in each Dove. Each satellite has an optical telescope for acquiring high-resolution images of earth. Each satellites uses an X-band system for the downlink of acquired images and systems telemetry at data rates of 120Mbit/s \n\nNASA Ames Research Center/San Jose State University: TechEdSat-4 (1)\n\nPurpose: This satellites uses commercial off-the-shelf components to provide the basic satellites functions such as commanding, power generation & supply, and communications with the other two units of the satellites. The CubeSat will fly and Exo-Brake to orbit that is deployed once the satellite is released to demonstrate a Passive De-Orbit System for satellites.\n\nMIT Lincoln Laboratory: MicroMAS: Microsized Microwave Atmospheric Satellite (1)\n\nPurpose: This satellite carries a nine-channel passive microwave radiometer to demonstrate miniaturized radiometer technology in space for application in ultra-compact spacecraft systems such as high performance multi-band sounder for future weather satellites.\n\nGEARSSAAT (1)\n\nPurpose: This satellite is equipped with Globalstar communications terminals that will perform studies involving the Globalstar communications satellite constellation.\n\nLambda Team: Lambdasat (1)\n\nPurpose: The spacecraft will conduct technical demonstration of the satellite bus in the radiation environment in space and track systems degradation. The satellite also carries and Automatic Identification System (AIS) for tracking sea vessels around the globe, and a science experiment that looks at Graphene in space.\n"}
{"id": "8969490", "url": "https://en.wikipedia.org/wiki?curid=8969490", "title": "Open Forum of Cambodia", "text": "Open Forum of Cambodia\n\nThe Open Forum of Cambodia (OFC) was formed in 1994 and is a not-for-profit organisation that aims to promote dialogue in Cambodian society.\n\nIt provided the first e-mail service in the country in 1994 to encourage dialogue and address social concerns. Its programme includes paper-based publications, in Khmer and English which according to its website included until January 2007 the editorialized weekly overview of all local newspapers reflecting the diverse points of views on political and social issues; electronic communication.\" It also promoted its website, and advocacy through research. OFC has also played an important role in the localisation of Free Software into the Khmer language.\n\nOFC's Nobert Klein edited a weekly press review of the Cambodian language press in English for 10 years. It was a 16-page publication. OFC has faced a funding crunch in recent months.\n\n"}
{"id": "18704175", "url": "https://en.wikipedia.org/wiki?curid=18704175", "title": "Orkney Wireless Museum", "text": "Orkney Wireless Museum\n\nThe Orkney Wireless Museum in Kirkwall, Orkney, Scotland, houses a collection of domestic and military wireless equipment. It developed from the private collection of the late Jim MacDonald from St Margaret's Hope and marks the importance of wireless communications in Orkney during World War II.\n\nThe Orkney Wireless Museum is located at Kiln Corner, Kirkwall in Orkney and is run on an entirely voluntary basis. It is registered as a Charitable Company. The Friends of the museum, from all over the world, help raise funds for the museum. The local branch of the society organises volunteers to man the Museum every day of the season, from April to September.\n\nThe displays and photographic archive bear testament to the strategic and military importance of Orkney during World War II. In the collection there is much reference to the Home Fleet in Scapa Flow, Orkney. The museum demonstrates the importance of wireless communications and Radar to the civilian and military populations.\n\nThe Amateur Radio call sign GB2OWM is frequently activated at the museum. During the Orkney Science Festival every September, worldwide contacts are made and QSL cards are subsequently exchanged with the stations contacted.\n\nThe museum has an extensive collection of early domestic radio and wartime communications equipment. Most of the equipment was built in the UK and USA. The museum also houses early advertising and posters. An archive of photographs depicts wartime forces and includes coverage of the building of the Churchill Barriers which were built primarily as naval defences to protect the anchorage at Scapa Flow.\n\nSome examples of exhibits:\n\n\nJim MacDonald had a love of wireless sets and gathered an extensive and varied collection of domestic and military wireless equipment. The Museum was founded in 1983 when he was persuaded to display his collection in what he called \"An Orkney Wireless Museum\" in the family home in St Margaret's Hope, South Ronaldsay. After he died of cancer in 1988, his family carried out his wishes to develop the museum.\nIn June 1990 a registered family Trust was set up to run the museum. In 1994, the Museum was admitted as a member of the Museums and Galleries Commission of Great Britain. The Museum was awarded a Certificate from the Royal Naval Amateur Radio Society \"For work in collecting, preserving and displaying the radio and electronic heritage of Scapa Flow and the Orkney Isles\"\n\nIn 1997, the collection moved to Kiln Corner, Kirkwall which allowed a greater number of exhibits to be displayed. The Museum has received many financial donations.\n\nJames \"Jim\" MacDonald (1927 - 1988) was born and brought up on South Ronaldsay. He was locally educated and served his apprenticeship as an electrician and radio repairman. During his boyhood, wireless was rapidly developing and he started to gather old, interesting sets which were becoming redundant. This became the nucleus for his collection which includes such rare and interesting examples as \"Kit Sets\" and the very, very expensive models by such famous names as Marconi, Cossor, MacMichael, K.B., Lowe and Amplion.\n\nWith the outbreak of war in 1939, he was listed as in a reserved occupation, and worked for the War Office as a civilian in the great Naval Base at Lyness. He was also employed at other RAF sites such as Netherbutton, with one of the world's first active operational Radar installations. (This was actually the prototype, modified, which was shipped from England, as the one being built for Orkney was not nearly ready. When Orkney's was finished, it was sent to Dover where it was to play such a vital role in the Battle of Britain).\n\nAfter the war, Jim collected as many examples of relevant equipment as he could; many operations were still classified as Top Secret. Another problem to collection was that equipment was deliberately destroyed in order to prevent flooding the market with surplus goods (as had happened after the First World War). The collected equipment had played a vital part in the Battle of the Atlantic and North Sea, and has all seen active service.\n\n\n"}
{"id": "1093053", "url": "https://en.wikipedia.org/wiki?curid=1093053", "title": "Pedal keyboard", "text": "Pedal keyboard\n\nA pedalboard (also called a pedal keyboard, pedal clavier, or, with electronic instruments, a bass pedalboard) is a keyboard played with the feet that is usually used to produce the low-pitched bass line of a piece of music. A pedalboard has long, narrow lever-style keys laid out in the same semitone scalar pattern as a manual keyboard, with longer keys for C, D, E, F, G, A and B, and shorter, higher keys for C, D, F, G and A. Training in pedal technique is part of standard organ pedagogy in church music and art music.\n\nPedalboards are found at the base of the console of most pipe organs, theatre organs, and electronic organs. Standalone pedalboards such as the 1970s-era Moog Taurus bass pedals are occasionally used in progressive rock and fusion music. In the 21st century, MIDI pedalboard controllers are used with synthesizers, electronic Hammond-style organs, and with digital pipe organs. Pedalboards are also used with pedal pianos and with some harpsichords, clavichords, and carillons (church bells).\n\nThe first use of pedals on a pipe organ grew out of the need to hold bass drone notes, to support the polyphonic musical styles that predominated in the Renaissance. Indeed, the term pedal point, which refers to a prolonged bass tone under changing upper harmonies, derives from the use of the organ pedalboard to hold sustained bass notes. These earliest pedals were wooden stubs nicknamed \"mushrooms\", which were placed at the height of the feet. These pedals, which used simple pull-downs connected directly to the manual keys, are found in organs dating to the 13th century. The pedals on French organs were composed of short stubs of wood projecting out of the floor, which were mounted in pedalboards that could be either flat or tilted. Organists were unable to play anything but simple bass lines or slow-moving plainsong melodies on these short stub-type pedals. Organist E. Power Biggs, in the liner notes for his album \"Organs of Spain\" noted that \"One can learn to play them, but fluent pedal work is impossible\".\n\nThere were two approaches used for the accidental notes (colloquially referred to as the \"black\" notes). The first approach can be seen in the 1361 Halberstadt organ, which uses shorter black keys placed above the white keys. Other organs positioned the black keys on the same level and depth as the white keys. The first pedal keyboards only had three or four notes. Eventually, organ designers augmented this range by using eight notes, an approach now called a \"short octave\" keyboard, because it does not include accidental notes such as C, D, F, G, and A. The 17th-century north German organ builder Arp Schnitger used an F and G in the lowest octave of the manuals and pedal keyboards, but not a C and D. From the 16th to 18th centuries, short octave keyboards were also used in the lowest octave of upper manual keyboards.\n\nBy the 14th century, organ designers were building separate windchests for the pedal division, to supply the pipes with the large amount of wind that bass notes need to speak. These windchests were often built into tall structures called \"organ towers\". Until the 15th century, most pedal keyboards only triggered the existing Hauptwerk pipes already used by the upper manual keyboards. Beginning in the 15th century, some organ designers began giving pedal keyboards their own set of pipes and stops. In the 15th and 16th centuries, the pedal division usually consisted of a few 8′ ranks and a single 16′ rank. By the early 17th century, pedal divisions became more complex, with a richer variety of pipes and tones. Nevertheless, the pedal division was usually inconsistent from one country to another.\n\nBy the beginning of the 17th century, organ designers began to give pedalboards on large organs a larger range, encompassing twenty-eight to thirty notes. As well, German organ designers began to use longer, narrower pedals, with a wider space between the pedals. By this point, most pedals were given a smoother lever-action by including a fulcrum at the back of each pedal. These design changes allowed performers to play more complex, fast-moving pedal lines. This gave rise to the dramatic pedal solos found in German organ works from composers from the Lutheran Organ School, such as J.S. Bach. In Bach's organ music the cantus firmus melody, which is usually a hymn tune, is often performed in the pedal, using a reed stop to make it stand out.\n\nSeveral sources, including an encyclopedia on the organ, claim that the pedalboard design improvements of the 17th century allowed the organist to actuate the pedals either with the toe of the foot or with the heel. However, organist Ton Koopman argues that\n\"Bach's complete oeuvre [can be played] with the pedal technique of his time, in other words without the use of the heel.\" Koopman claims that in \"Bach's day toe and heel pedalling was not yet known, as is evident from his organ works, in which all the pedal parts can be played with the toe.\" What evolved as \"German\" pedal technique in the late 18th and early 19th century promoted heel-and-toe pedaling, while the \"French\" style was predicated on \"toe only\" pedal technique.\n\nIn the 17th and 18th century, pedalboards were rare in England. A critic for the \"New York Times\" in 1895 argued that this may explain why Handel's published organ works are generally lighter-sounding than those of J.S. Bach. In the 17th and 18th centuries, the pedal part of organ music was rarely given its own staff. Instead, the organ part would be put into two staves, which were mostly used for the upper and lower manual parts. When the composer wanted a part played with the pedal keyboard, they marked \"Pedal\", \"Ped.\", or simply \"P\". Often, composers omitted these signs, and player had to decide if the range of all the parts or the lowest part was appropriate for the pedal keyboard. This lack of specification is in keeping with many other aspects of Baroque musical performance practice, such as the use of improvised chords by organists and harpsichord players in the figured bass tradition and the use of improvised ornaments by solo singers and instrumentalists.\n\nIn the late 1820s, the pedalboard was still fairly unfamiliar in the UK. In the organ at the Church of St James at Bermondsey in 1829, \"a finger [manual] keyboard was added for those unable to play with their feet.\" If an organist was performing a piece with a pedal part, \"an assistant was needed to play the bottom line of the finger keyboard, offset on the bass side of the console.\" In 1855 in England, Henry Willis patented a concave design for the pedalboard that also radiated the ends keyboard outward and used longer keys, bringing the end keys closer to the performer. This design became common in the UK and in the US in the late 19th century, and by 1903, the American Guild of Organists (AGO) adopted it as their standard.\n\nIn the 19th century and early 20th century, the pedal division also underwent changes. The pedal divisions of the Baroque era often included a small number of higher-pitched stops, which allowed performers to perform higher melodies on the pedalboard. In the 19th century and early 20th century, organ designers omitted most of these higher-pitched stops, and used pedal divisions which were dominated by 8′ and 16′ stops. This design change, which coincided with the musical trend for music with a deep, rich bass part, meant that players used the pedalboard mainly for bass parts.\n\nBy the mid-19th century, the pedal part of organ music was increasingly given its own staff, which meant that composers and transcribers began writing organ music in three-stave systems (upper manual, lower manual, and pedal keyboard). Whereas early organ composers left the way that pedal keyboard lines were played to the player's discretion, in the later 19th century, composers began to indicate specific foot actions.\n\nIn addition to telling the organist whether to use the left or right foot, symbols indicate whether they should use the toe or heel. A \"^\" symbol indicates the toe, and a \"u\" or \"o\" indicates the heel. Symbols below notes indicate the left foot, and above notes indicates the right foot.\n\nSwedish organist L. Nilson published a method for the pedal keyboard, the English translation of which was titled \"A System of Technical Studies in Pedal Playing for the Organ\" (Schirmer, 1904). Nilson lamented that it \"...is a melancholy fact that only very few eminent organists since Bach's time have made it their business to lift pedal-playing out of its primitive confusion...\" (page 1 of Preface). He argued that the great organ pedagogues such as Kittel and Abbe Vogler did not make any efforts to improve the \"...system of playing on the pedals\". Nilson makes one exception from this critique: the organ method of J. Lemmens, who he praises as having reformed pedal playing by introducing \"...sound principles of execution\" (page 2 of Preface). Nilson's pedal method includes scale and arpeggio studies, polyphonic studies with both feet playing in contrary motion, studies written in parallel octaves, and studies written in thirds.\n\nIn the 1990s, standalone electronic MIDI controller pedalboards became widely available on the market. MIDI pedalboards do not produce any tones by themselves, and so they must be connected to a MIDI-compatible electronic keyboard or MIDI sound module and an amplified loudspeaker to produce musical tones. In the 1990s and 21st century, some churches began using electronic-trigger equipped pedalboards for the 16′ and 32′ stops. The MIDI information from the electronic pedalboard sensors triggers pipe organ sounds from digital sound modules (e.g., Wicks CM-100, Ahlborn Archive Modules, or Walker Technical sound generation), which are then amplified through loudspeakers.\n\nThese MIDI systems can be much less expensive than metal or wooden bass pipes, which are very costly to purchase and install, due to their heavy weight (up to one ton per pipe), large size, and need for large amounts of wind. Another rationale for using MIDI systems is that it may be easier to get a focused sound with a MIDI system, because all of the bass tone emanates from a single speaker or set of speakers. With traditional pipes, it can be difficult to give the pedal division a focused sound, because the large pipes tend to be spread out over the entire organ pipe chest.\n\nThis cost-saving measure has been the subject of controversy in the organ scene. Advocates of MIDI pedal divisions argue that a good quality MIDI system produces a better tone than an inexpensive set of bass pipes with money-saving \"shortcuts\" such as using stopped pipes and resultant tones to reduce the number of required pipes. However, critics dislike the way that the use of MIDI pedal divisions blends electronically amplified lower voices with the natural, wind-driven upper ranks. Willi Apel and Peter Williams argue that by definition, an organ must make its sound by air flowing through pipes. Some critics argue that the bass tone from a MIDI pedal division, which comes from an amplified 12-inch subwoofer, is not as \"natural\" and \"open-sounding\" as the vibrations from a massive, wind-driven 32-foot pipe.\n\nPedalboards range in size from 13 notes on small spinet organs designed for in-home use (an octave, conventionally C–C) to 32 notes (two and a half octaves, C–G) on church or concert organs. Modern pipe organs typically have 30- or 32-note pedalboards, while some electronic organs and many older pipe organs have 25-note pedalboards.\n\nBesides the number of pedals, the two main identifying aspects of a pedalboard are:\n\nExact design specifications for pedalboards are published in Great Britain by the RCO, in the United States by the AGO (which requires a design similar to the RCO's), and in Germany by the BDO (which allows both 30- and 32-note pedalboards, of both concave/radiating and concave/parallel varieties).\n\nIn an organ with more than one keyboard, the stops and the ranks that the stops control are separated into different divisions, in which the ranks of pipes are grouped together so that they make a \"focused\" or coherent sound. The pedal division, which is played from the pedal keyboard, usually includes more stops of 16′ pitch. The sound of the pedal division is generally voiced so that the pedal division complements the sound of the great division. Common 16′ stops found in the pedal division include the 16′ Bourdon, the 16′ Principal, and the 16′ Trombone. Eight foot stops include the 8′ Open Diapason. Pedal divisions may also include higher-register stops, such as the 4′ Choral Bass or various mixtures. When pedal parts are performed, a 16′ stop is usually paired with an 8′ one to provide more definition. For pedal parts that need accentuation, such as the Cantus Firmus melody in a 17th-century organ piece, many organs have a nasal-sounding reed stop in the pedal division, or a 4′ Principal designated on the stop knob as \"Choralbass\".\n\nA few pedalboards have a pedal divide system that lets the organist split the pedalboard at its midpoint. With this system, an organist can play a melody with the right foot and a bass part with the left. The divided pedal is a type of coupler. It allows the sounds played on the pedals to be split, so the lower octave (principally that of the left foot) plays stops from the pedal division while the upper half (played by the right foot), plays stops from one of the manual divisions. The choice of manual is at the discretion of the performer, as is the 'split point' of the system.\n\nThe system can be found on the organs of Gloucester Cathedral, having been added by Nicholson & Co (Worcester) Ltd/David Briggs and Truro Cathedral, having been added by Mander Organs/David Briggs, as well as on the new nave console of Ripon Cathedral. The system as found in Truro Cathedral operates like this:\n\nIn some organs, a wooden panel called a \"kickboard\" or \"kneeboard\" is installed above the pedalboard, between the pedals and the lowest manual keyboard. Expression pedals, coupler controls and toe studs (to activate stops or stop combinations) may be located on or set into the kickboard. Expression pedals are used to open and close shades or shutters that enclose the pipes of a given division. Combination pistons are used to make rapid stop changes from the console on organs with electric stop action. Toe studs are pistons that can be operated by the feet, which change either the pedal stops or the entire organ.\n\nIn some organs, a \"pedalboard check\" mechanism serves as a safety catch, to shut off the pedalboard keys. The mechanism prevents accidental foot contact with the pedalboard from sounding notes in a section written only for the upper manuals.\n\nThe works of Dutch composer, organist, and pedagogue Jan Pieterszoon Sweelinck (1562–1621) contain the earliest example of an independent part for the pedal, rather than a sustained bass drone. His work straddled the end of the Renaissance and beginning of the Baroque eras, and he helped establish the north German organ tradition.\n\nDieterich Buxtehude (1637–1707), who was the most renowned composer of his time, was famous for his \"...virtuosity and innovation at the pedal board.\" The young Johann Sebastian Bach was influenced by Buxtehude, who used the pedal board \"as a full-fledged keyboard and devot[ed] virtuoso passages to it.\" J.S. Bach used the pedal to perform the melody in works such as his setting of the Christmas hymn, In Dulci Jubilo, in which the main theme in the tenor voice is played in the pedal on a higher-pitched stop. Bach also wrote compositions that use the pedal for dramatic virtuoso displays of scales and figurated passage-work in preludes, toccatas, fantasias and fugues.\n\nThere are a small number of organ compositions that are written solely for the pedal keyboard. English organist and composer George Thalben-Ball (1896–1987) wrote a piece entitled “Variations on a Theme by Paganini” for pedal keyboard. Based on Paganini's “Caprice No. 24”, a virtuoso work for solo violin, it includes pedal glissandi, leaps from one end of the pedalboard to the other, and four-note chords.\n\nFirmin Swinnen (1885–1972) was a Belgian organist who became famous in the US in the 1920s for his theater organ improvisations during silent films. Swinnen wrote a pedal cadenza for an arrangement of Widor's \"Fifth Symphony\". The cadenza was published separately by \"The American Organist\". The publisher promoted the cadenza it as the \"most daring, the most musical Pedal Cadenza obtainable\"; this praise is corroborated by reviewers who were at the performance, who remarked at the complex footwork required by the work. The symphony was performed 29 times during the week of its premiere, to \"...literally screaming audiences...who had never seen such a sight as an organist up on a lift [platform] in the spotlight playing with his feet alone\".\n\nAlthough the pedalboard is most frequently used for the bass part, composers from the 17th century to the present have often used it for higher parts as well. In his serene \"Le Banquet Céleste\" Olivier Messiaen places the tune, registered for 4′ flute (and higher mutation ranks), in the pedals.\n\nFrom the early 20th century, composers have increasingly demanded an advanced pedal technique at the organ. Performers display their virtuosity in such works as Wilhelm Middelschulte's \"Perpetuum mobile\", Leo Sowerby's \"Pageant\" (1931), and Jeanne Demessieux's \"Six études\", Op. 5 (1944), which recall the dramatic organ pedal solos of the Baroque era.\n\nPedal keyboards were developed for the clavichord and harpsichords during the Baroque era so that organists could practise the pedal parts of their organ repertoire when they had no-one available to work the bellows for a church organ or, in the wintertime, to avoid having to practice on a church organ in an unheated church. Johann Sebastian Bach owned a pedal harpsichord and his organ trio sonatas BWV 525-530, Passacaglia and Fugue in C minor BWV 582, Toccata and Fugue in D minor BWV 565, and other works sound well when played on the instrument.\n\nThe pedal piano (or pedalier piano) is a kind of piano that includes a pedalboard\n\nThere are two types of pedal piano:\nWolfgang Amadeus Mozart owned a fortepiano with independent pedals, built for him in 1785. Robert Schumann had an upright pedal piano with 29 notes. In the 21st century, pedal pianos, the Doppio Borgato are made in the Borgato workshop in Italy. The bass pedalboard has 37 notes, A0 to A3 (rather than the standard 30 or 32 on an organ).\n\nSome large carillon systems for playing church bells include a pedalboard for the lowest-pitched bells. Carillon pedal keys activate a pull-down coupler that visibly moves the keys of the manual clavier and heavy clappers for the largest bells. These keys resemble the \"button keys\" of early organs, and are played by the player's toes. Because this non-legato technique involves no sliding, shoes with leather soles are not required.\n\nAfter jazz organist Jimmy Smith popularized the Hammond organ in jazz in the 1950s, many jazz pianists \"... who thought that getting organ-ized would be a snap ...\" realized that the Hammond \"... B-3 required not only a strong left hand, but studied coordination on the pedals to create the strong and solid \"jazz bass\" feel.\" Barbara Dennerlein combines advanced pedalboard techniques with agile playing on the manuals. Jazz organists from more recent decades typically perform the bass line with their left hand on one of the keyboards, rather than by using the pedalboard. Organists who play the bassline on the lower manual may do short taps on the bass pedals – often on the tonic of a tune's key and in the lowest register of the pedalboard – to simulate the low, resonant sound of a plucked upright bass string.\n\nIn popular music, pedaling style may be more varied and idiosyncratic, in part because jazz or pop organists may be self-taught. Also, pedaling styles may differ due to the design of electromechanical organs and spinet organs, many of which have shorter pedalboards designed to play primarily with the left foot, so that the right foot can control a volume (swell) pedal.\nIn the 1970s, some progressive rock groups such as Yes, Pink Floyd, Genesis, Atomic Rooster and Rush used standalone Moog Taurus bass pedalboard synths. The Taurus generated a bass tone for amplification by a bass amp. Other groups, such as Led Zeppelin and Van Der Graaf Generator used the bass pedals of the Hammond organ in place of a bass guitar for several of their recordings and for live performances.\n\nOther users included metal and hard rock bands such as Yngwie Malmsteen, Styx, and Francis Buchholz of the Scorpions, and Justin Harris of Menomena. Ex-Genesis guitarist Steve Hackett had a set mounted waist high, which his brother, John Hackett, played with his hands for the intro of \"Clocks – The Angel Of Mons\" from the album Spectral Mornings. Adam Jones of Tool uses the Moog Taurus along with an Access Virus B synth to trigger live effects. The keyboardist for the rock group Emerson, Lake & Palmer took this idea to its logical conclusion by performing all of the first movement, and part of the second of \"The Three Fates\" on the organ of Royal Festival Hall in London.\n\nAs well, some pop groups (e.g., The Police, Muse, U2) and fusion bands have used bass pedalboards to produce sounds in the bass range. They are most commonly used by keyboard players as an adjunct to keyboards, but can be played in combination with other instruments (e.g., by the bass guitar or electric guitar player), or by themselves.\n\nStandalone pedalboards usually have a 13-note range and short pedals, which limits the types of basslines to fairly simple passages. A group's bass guitarist or electric guitarist playing the pedalboard from a standing position can only use one foot at a time, which further limits what they can play. The BASYN analog bass synthesizer is a two-VCO analog synthesizer with a 13-note \"button board\"—with momentary push-button switches in place of pedals. Another variant used in rock bands is a bass pedalboard laid out as a tabulature representation of part of the four strings of an electric bass guitar.\n\nIn the 1990s, standalone electronic MIDI controller pedalboards became widely available. Unlike the Moog Taurus pedalboards, MIDI pedalboards do not produce tones by themselves, but control a MIDI-compatible electronic keyboard or MIDI sequencer. In jazz organ trios, a keyboardist using this type of pedalboard usually connects it to a MIDI-compatible electronic Hammond organ-style keyboard. On modern electronic synthesizers such as the Yamaha Electone, the pedals are not limited to traditional bass notes but may instead produce many different sounds, including high-register tones. While MIDI pedalboards are typically used for musical sounds, since they use MIDI, technically the footpedals could be used to trigger lights or other electronic elements of a show.\n\nMIDI pedalboards offer a range of features. Some MIDI pedalboards contain velocity-sensitive triggers, which produce MIDI velocity information for musical dynamics. MIDI pedalboards such as the 13-note Roland PK-5 include a row of MIDI toe switches above the pedal keyboard, so the performer can select preset tones or MIDI channels or change the octave. Larger 25-note Roland pedalboards also include an expression pedal for controlling the volume or other parameters. In the 2000s, controller designer Keith McMillen developed a 13-note velocity-sensitive pedalboard with a USB output that can be connected to a MIDI-equipped synthesizer. McMillen's pedalboard differs from other pedalboards in that it senses a variety of types of velocity and pressure, which the user can program to cause different effects on the synthesizer patch. McMillen's pedalboard can be programmed so that pressing an individual pedal triggers chords (up to five simultaneous notes), which a one person band could use to provide accompaniment for live shows. The Roland PK-9 and the Hammond XPK-200 are 20-note pedalboards that sound from low C to a high G. The Nord PedalKeys is a 27-note pedalboard, going from a low C to a high D. As compared with a 25-note pedalboard, the PedalKeys adds a high C# and a high D.\n\nSome MIDI pedalboards are designed for the church pipe organ market, which means that they use AGO specifications such as a 32-note range. Most pipe organ-style MIDI pedalboards are too unwieldy for transportation, so they are typically installed under the upper manuals. However, a German company makes a MIDI pedalboard with a hinge in the middle and wheels on the underside for easy transport. Since AGO-specification MIDI pedalboards are often priced in between US$1000 and US$3000, some amateur home organists make DIY MIDI pedalboards by retrofitting an old pedalboard with MIDI. Due to the popularity of theater organs and Hammond organs during the 1950s and 1960s, many organ parts are on the market—including pedalboards (often with less than 32 notes, such as 20 or 25 notes) that cost under US$300. After the pedalboard is cleaned up and glass reed switches are repaired or replaced, the pedal contacts are soldered into a keyboard matrix circuit-equipped MIDI encoder, which then connects to any MIDI device to produce the sound of an organ or other instrument.\n\n"}
{"id": "34637922", "url": "https://en.wikipedia.org/wiki?curid=34637922", "title": "Polimedia", "text": "Polimedia\n\nPolimedia is a system designed at the Universitat Politècnica de València for the creation of high resolution multimedia educational content in a cheap and easy way.\n\nThe specification consists of a mini-recording studio with a video camera, a computer for the author, and a computer for the capture and encoding process that produces an mp4 video with the author's video and the digital content presented in the computer. The screenshot of the teacher's computer can be replaced by an overhead camera image or the output of an interactive screen if required.\n\nThe system is controlled by a technician (which requires minimal training) and creates videos without the need of postproduction, so the video is available almost in real time (with the only delay of the encoding time). The videos are stored on a local server system managed by a simple database and, in the latest versions, can be exported to YouTube and iTunesU.\n\nThe system has been running since 2003 at the UPV, where they have recorded over 16,500 videos, and has been implemented for the production of educational content in several Spanish universities such as Universitat Autònoma de Barcelona (UAB), the Polytechnic University of Cartagena,(UPTC) and the University of La Laguna (ULL). It has also been implemented in various organizations and universities around the world, including the University of São Paulo and the University of Honduras Global UNITEC.\n\nThe system was awarded one of the International Forum of Digital Content (FICOD) prizes in 2009 for the use of digital content to improve citizen services.\n"}
{"id": "4622869", "url": "https://en.wikipedia.org/wiki?curid=4622869", "title": "Post pounder", "text": "Post pounder\n\nA post pounder, post driver, post knocker or fence driver is a tool used for driving fence posts and similar items into land surfaces. It consists of a heavy steel pipe which is closed at one end and has handles welded onto the sides. It is normally used by one person, but larger versions may require two.\n\nAn early type was developed in 1933 by the Pennsylvania Department of Transportation to aid the installation of fencing along roads and highways. \n\nThe open end is put over the top of the post to be driven, then the tool is lifted and dropped repeatedly onto the top of the post. The closed end strikes the top of the post, driving it into the ground. Unlike a maul it can be used easily on tall posts, such as those used for deer fencing. Because the tube guides the striking force consistently in line with the post and strikes it across its full width, a post pounder usually damages the top of the post less than a maul. It is not suitable for posts which will be shorter than the length of the tool, or for posts which do not fit easily into the tube because of bulges or curves, or their over-large diameter. The tool is difficult to use on slanting posts.\n\nThe post pounder can also be used to remove t-posts. To do so, put the closed end of the post pounder on the ground about 6 inches from the t-post. It should be on the side of the post with the knobs. Push back on the t-post away from the pounder, then push the top of the pounder so the rim sets beneath the lowest knob possible on the t-post. With the pounder lodged beneath the t-post knob, pull the t-post back to the starting position and it will lift up a few inches. Repeat process until t-post can be removed. \n\nSingle-person post pounders tend to be around , but weight varies with model types. The diameter depends on the size of the post to be pounded. They can be dangerous to handle, especially if the lower edge of the pounder catches on the top of the post, when it can pivot towards the user and strike them on the head.\n\nHearing protection and a hard hat are often recommended.\n"}
{"id": "36960704", "url": "https://en.wikipedia.org/wiki?curid=36960704", "title": "Promecarb", "text": "Promecarb\n\nPromecarb (chemical formula: CHNO) is a chemical compound previously used as an insecticide.\n"}
{"id": "14495904", "url": "https://en.wikipedia.org/wiki?curid=14495904", "title": "Ralf Brown's Interrupt List", "text": "Ralf Brown's Interrupt List\n\nRalf Brown's Interrupt List (aka RBIL, x86 Interrupt List, MS-DOS Interrupt List or INTER) is a comprehensive list of interrupts, calls, hooks, interfaces, data structures, CMOS settings, memory and port addresses, as well as processor opcodes and special function registers for x86 machines (including many clones) from the very start of the PC era in 1981 up to 2000, most of it still applying to PCs today.\n\nThe list covers operating systems, device drivers, and application software; both documented and undocumented information including bugs, incompatibilities, shortcomings, and workarounds, with version, locale, and date information, often at a detail level far beyond that found in the contemporary literature. A large part of it covers system BIOSes and internals of operating systems such as DOS, OS/2, and Windows, as well as their interactions.\n\nIt has been a widely used resource by IBM PC system developers as well as application programmers in the pre-Windows era. Parts of the compiled information have been used for and in the creation of various books on systems programming. As such it has proven to be an important resource in developing various closed and open source operating systems, including Linux and FreeDOS. Today it is still used as a reference to BIOS calls and to develop programs for DOS as well as other system-level software.\n\nThe project is the result of the research and collaborative effort of more than 650 listed contributors worldwide over a period of 15 years, of which about 290 provided significant information (and some 55 of them even more than once). The original list was created in January 1985 by Janet Jack and others, and, named \"Interrupt List for MS-DOS\", it was subsequently maintained and mailed to requestors on Usenet by Ross M. Greenberg until 1986. Since October 1987 it is maintained by Ralf D. Brown, a researcher at Carnegie Mellon University's Language Technologies Institute. In order to establish one comprehensive reference information from several other compilations was merged into the list. Over the years, Michael A. Shiels, Tim Farley, Matthias Paul, Robin Walker, Wolfgang Lierz and Tamura Jones became major contributors to the project, providing information all over the list. The project was also expanded to include other PC development related information and therefore absorbed a number of independently maintained lists on PC I/O ports (by Wim Osterholt and Matthias Paul), BIOS CMOS memory contents (by Padgett Peterson), processor opcodes (by Alex V. Potemkin) and bugs (by Harald Feldmann). Brown and Paul also conducted several systematic surveys on specific hard- and software details among a number of dedicated user groups in order to validate some info and to help fill some gaps in the list.\n\nOriginally, the list was distributed in an archive named INTERRUP in various compression formats as well as in form of diffs. The distribution file name was changed to include a version in the form INTERnyy (with n = issue number, and yy = 2-digit release year) in 1988. In mid 1989 the distribution settled to only use ZIP compression. When the archive reached the size of a 360 KB floppy in June 1991, the distribution split into several files following a INTERrrp.ZIP naming scheme (with rr = revision starting with 26 for version 91.3, and p = part indicator of the package starting with letter A). Officially named \"MS-DOS Interrupt List\" and \"x86 Interrupt List\" (abbreviated as \"INTER\") by its maintainer, the community coined the unofficial name \"Ralf Brown's Interrupt List\" (abbreviated as \"RBIL\") in the 1990s.\n\nThe publication is currently at revision 61 as of 17 July 2000 with almost 8 MB of ASCII text including close to entries plus about tables, fully cross linked, which would result in more than 3700 pages (at 60 lines per page) of condensed information when printed. Of this, the interrupt list itself makes up some 5.5 MB for more than 2500 pages printed.\n\nWhile the project is not officially abandoned and the website is still maintained (as of 2017), new releases have not been forthcoming for a very long time, despite the fact that information was still pending for release even before the INTER61 release in 2000. New releases were planned for at several times in 2001 and 2002, but when they did not materialize, portions of the new information on DOS and PC internals provided by Paul were circulated in preliminary form in the development community for peer-review and to assist in operating system development.\n\n\n"}
{"id": "34791483", "url": "https://en.wikipedia.org/wiki?curid=34791483", "title": "Reckoning board", "text": "Reckoning board\n\nThe reckoning board, also called a memory board or hole board, could be used on its own as a basic counting device or used with an abacus for engineering.\n\nThere were two types of reckoning board. The older type was a simple 10 × 10 grid of holes. A peg would be inserted into a hole and moved along, starting from the top and working downwards. It was used as a memory aid when counting certain units. For every sack of grain or bar of steel, the peg would be moved forward and after a day or a week, the total number counted could be seen.\n\nThe more advanced type had columns of holes, with the columns indicating place value.\n\n\n"}
{"id": "99326", "url": "https://en.wikipedia.org/wiki?curid=99326", "title": "Richard Hamming", "text": "Richard Hamming\n\nRichard Wesley Hamming (Chicago, Illinois, February 11, 1915 – Monterey, California, January 7, 1998) was an American mathematician whose work had many implications for computer engineering and telecommunications. His contributions include the Hamming code (which makes use of a Hamming matrix), the Hamming window, Hamming numbers, sphere-packing (or Hamming bound), and the Hamming distance.\n\nBorn in Chicago, Hamming attended University of Chicago, University of Nebraska and the University of Illinois at Urbana–Champaign, where he wrote his doctoral thesis in mathematics under the supervision of Waldemar Trjitzinsky (1901–1973). In April 1945 he joined the Manhattan Project at the Los Alamos Laboratory, where he programmed the IBM calculating machines that computed the solution to equations provided by the project's physicists. He left to join the Bell Telephone Laboratories in 1946. Over the next fifteen years he was involved in nearly all of the Laboratories' most prominent achievements.\n\nAfter retiring from the Bell Labs in 1976, Hamming took a position at the Naval Postgraduate School in Monterey, California, where he worked as an adjunct professor and senior lecturer in computer science, and devoted himself to teaching and writing books. He delivered his last lecture in December 1997, just a few weeks before he died from a heart attack on January 7, 1998.\n\nRichard Wesley Hamming was born in Chicago, Illinois, on February 11, 1915, the son of Richard J. Hamming, a credit manager, and Mabel G. Redfield. He grew up in Chicago, where he attended Crane Technical High School and Crane Junior College.\n\nHamming initially wanted to study engineering, but money was scarce during the Great Depression, and the only scholarship offer he received came from the University of Chicago, which had no engineering school. Instead, he became a science student, majoring in mathematics, and received his Bachelor of Science degree in 1937. He later considered this a fortunate turn of events. \"As an engineer,\" he said, \"I would have been the guy going down manholes instead of having the excitement of frontier research work.\"\n\nHe went on to earn a Master of Arts degree from the University of Nebraska in 1939, and then entered the University of Illinois at Urbana–Champaign, where he wrote his doctoral thesis on \"Some Problems in the Boundary Value Theory of Linear Differential Equations\" under the supervision of Waldemar Trjitzinsky. His thesis was an extension of Trjitzinsky's work in that area. He looked at Green's function and further developed Jacob Tamarkin's methods for obtaining characteristic solutions. While he was a graduate student, he discovered and read George Boole's \"The Laws of Thought\".\n\nThe University of Illinois at Urbana–Champaign awarded Hamming his Doctor of Philosophy in 1942, and he became an Instructor in Mathematics there. He married Wanda Little, a fellow student, on September 5, 1942, immediately after she was awarded her own Master of Arts in English literature. They would remain married until his death, but had no children. In 1944, he became an Assistant Professor at the J.B. Speed Scientific School at the University of Louisville in Louisville, Kentucky.\n\nWith World War II still ongoing, Hamming left Louisville in April 1945 to work on the Manhattan Project at the Los Alamos Laboratory, in Hans Bethe's division, programming the IBM calculating machines that computed the solution to equations provided by the project's physicists. His wife Wanda soon followed, taking a job at Los Alamos as a human computer, working for Bethe and Edward Teller. Hamming later recalled that:\n\nHamming remained at Los Alamos until 1946, when he accepted a post at the Bell Telephone Laboratories (BTL). For the trip to New Jersey, he bought Klaus Fuchs's old car. When he later sold it just weeks before Fuchs was unmasked as a spy, the FBI regarded the timing as suspicious enough to interrogate Hamming. Although Hamming described his role at Los Alamos as being that of a \"computer janitor\", he saw computer simulations of experiments that would have been impossible to perform in a laboratory. \"And when I had time to think about it,\" he later recalled, \"I realized that it meant that science was going to be changed\".\n\nAt the Bell Labs Hamming shared an office for a time with Claude Shannon. The Mathematical Research Department also included John Tukey and Los Alamos veterans Donald Ling and Brockway McMillan. Shannon, Ling, McMillan and Hamming came to call themselves the Young Turks. \"We were first-class troublemakers,\" Hamming later recalled. \"We did unconventional things in unconventional ways and still got valuable results. Thus management had to tolerate us and let us alone a lot of the time.\"\n\nAlthough Hamming had been hired to work on elasticity theory, he still spent much of his time with the calculating machines. Before he went home on one Friday in 1947, he set the machines to perform a long and complex series of calculations over the weekend, only to find when he arrived on Monday morning that an error had occurred early in the process and the calculation had errored off. Digital machines manipulated information as sequences of zeroes and ones, units of information that Tukey would christen \"bits\". If a single bit in a sequence was wrong, then the whole sequence would be. To detect this, a parity bit was used to verify the correctness of each sequence. \"If the computer can tell when an error has occurred,\" Hamming reasoned, \"surely there is a way of telling where the error is so that the computer can correct the error itself.\"\n\nHamming set himself the task of solving this problem, which he realised would have an enormous range of applications. Each bit can only be a zero or a one, so if you know which bit is wrong, then it can be corrected. In a landmark paper published in 1950, he introduced a concept of the number of positions in which two code words differ, and therefore how many changes are required to transform one code word into another, which is today known as the Hamming distance. Hamming thereby created a family of mathematical error-correcting code, which are called Hamming codes. This not only solved an important problem in telecommunications and computer science, it opened up a whole new field of study.\n\nThe Hamming bound, also known as the sphere-packing or volume bound is a limit on the parameters of an arbitrary block code. It is from an interpretation in terms of sphere packing in the Hamming distance into the space of all possible words. It gives an important limitation on the efficiency with which any error-correcting code can utilize the space in which its code words are embedded. A code which attains the Hamming bound is said to be a perfect code. Hamming codes are perfect codes.\n\nReturning to differential equations, Hamming studied means of numerically integrating them. A popular approach at the time was Milne's Method, attributed to Arthur Milne. This had the drawback of being unstable, so that under certain conditions the result could be swamped by roundoff noise. Hamming developed an improved version, the Hamming predictor-corrector. This was in use for many years, but has since been superseded by the Adams method. He did extensive research into digital filters, devising a new filter, the Hamming window, and eventually writing an entire book on the subject, \"Digital Filters\" (1977).\n\nDuring the 1950s, he programmed one of the earliest computers, the IBM 650, and with Ruth A. Weiss developed the L2 programming language, one of the earliest computer languages, in 1956. It was widely used within the Bell Labs, and also by external users, who knew it as Bell 2. It was superseded by Fortran when the Bell Labs' IBM 650 were replaced by the IBM 704 in 1957.\n\nIn \"A Discipline of Programming\" (1967), Edsger Dijkstra attributed to Hamming the problem of efficiently finding regular numbers. The problem became known as \"Hamming's problem\", and the regular numbers are often referred to as Hamming numbers in Computer Science, although he did not discover them.\n\nThroughout his time at Bell Labs, Hamming avoided management responsibilities. He was promoted to management positions several times, but always managed to make these only temporary. \"I knew in a sense that by avoiding management,\" he later recalled, \"I was not doing my duty by the organization. That is one of my biggest failures.\"\n\nHamming served as president of the Association for Computing Machinery from 1958 to 1960. In 1960, he predicted that one day half of the Bell Lab's budget would be spent on computing. None of his colleagues thought that it would ever be so high, but his forecast actually proved to be too low. His philosophy on scientific computing appeared as the motto of his \"Numerical Methods for Scientists and Engineers\" (1962): \n\nIn later life, Hamming became interested in teaching. Between 1960 and 1976, when he left the Bell labs, he held visiting or adjunct professorships at Stanford University, Stevens Institute of Technology, the City College of New York, the University of California at Irvine and Princeton University. As a Young Turk, Hamming had resented older scientists who had used up space and resources that would have been put to much better use by the young Turks. Looking at a commemorative poster of the Bell Labs' valued achievements, he noted that he had worked on or been associated with nearly all of those listed in the first half of his career at Bell Labs, but none in the second. He therefore resolved to retire in 1976, after thirty years.\n\nIn 1976 he moved to the Naval Postgraduate School in Monterey, California, where he worked as an Adjunct Professor and senior lecturer in computer science. He gave up research, and concentrated on teaching and writing books. He noted that:\n\nHamming attempted to rectify the situation with a new text, \"Methods of Mathematics Applied to Calculus, Probability, and Statistics\" (1985). In 1993, he remarked that \"when I left BTL, I knew that that was the end of my scientific career. When I retire from here, in another sense, it's really the end.\" And so it proved. He became Professor Emeritus in June 1997, and delivered his last lecture in December 1997, just a few weeks before his death from a heart attack on January 7, 1998. He was survived by his wife Wanda.\n\n\nThe IEEE Richard W. Hamming Medal, named after him, is an award given annually by the Institute of Electrical and Electronics Engineers (IEEE), for \"exceptional contributions to information sciences, systems and technology\", and he was the first recipient of this medal. The reverse side of the medal depicts a Hamming parity check matrix for a Hamming error-correcting code.\n\n\n\n"}
{"id": "51920739", "url": "https://en.wikipedia.org/wiki?curid=51920739", "title": "Shlomo Kramer", "text": "Shlomo Kramer\n\nShlomo Kramer (), is an Israeli information technology entrepreneur and investor. He is the co-founder of cyber-security companies Check Point and Imperva, as well as Cato Networks, a cloud-based network security provider. In addition, the long list of startups in which he has invested and/or taken an active role as an executive or board member includes WatchDox, Trusteer, Palo Alto Networks, Exabeam, Indegy, LightCyber, Lacoon Mobile Security and more.\n\nShlomo Kramer has been actively involved with technology all his life. As a youth, he worked on mainframes and sold video games. He landed his first job – selling personal computers at a Tel Aviv shop – at age 15. After discovering that one of the shop’s best-selling games was developed and marketed by a 17-year-old in Britain who had set up his own company, Kramer knew that he “wanted to be like him — an entrepreneur, even though it would be quite a few years before I knew the word.” \n\nKramer served in the Israel Defense Forces’ Unit 8200, a crack cybersecurity and intelligence team whose operations include gathering, analyzing and decrypting data; over the years, the unit has produced many of Israel’s top high-tech entrepreneurs. After completing his military service, Kramer earned a master’s degree in Computer Science from the Hebrew University of Jerusalem and a bachelor’s degree in Mathematics and Computer Science from Tel Aviv University.\n\nKramer, who has been called “the godfather of Israeli cybersecurity,” is a serial high-tech investor and entrepreneur with “a long track record of success\". In 1993, he co-founded Check Point Software Technologies along with Gil Shwed and Marius Nacht; the company introduced the first firewall to the commercial market and went on to become “a world leader in protecting the information that flows round the Internet, and a flagship of Israel's high-tech industry\". Kramer left Check Point in 1998 and used the money from the sale of his stake to strike out on his own as an entrepreneur and investor in numerous startups.\n\nIn 2002, Kramer founded his second startup, Imperva, together with Mickey Boodaei and Amichai Shulman. Imperva moved away from perimeter defenses such as firewalls and instead deployed its software to protect against hackers and business-data theft by identifying and preventing attacks before they find their way to the inside of an organization.\n\nThe company’s initial public offering on the New York Stock Exchange raised $90 million, with its shares gaining 33% on its first day of trading on 9 November 2011. In 2014, Imperva acquired Skyfence, a cloud security gateway startup in which Kramer was a lead investor, and bought the shares it did not already own in Incapsula, a cloud-based website performance and security service in which it had already invested. The acquisitions helped Imperva extend its data security strategy throughout the cloud.\n\nKramer’s belief in the cloud as the next big development in cybersecurity led him to establish Cato Networks in 2016, together with former Imperva colleague Gur Shatz. Cato Networks’ software integrates all the elements of an organization’s network – including branch locations, data centers, mobile users and more – into one encrypted network in the cloud. This means the enterprise is no longer tied to an array of location-bound appliances to protect its data.\n\nIn addition to co-founding Check Point, Imperva and Cato Networks, Kramer has invested in many startups, mostly in the field of data security.\nA partial list of his investments since 2000 includes: Insert; SafeBreach; LightCyber; TopSpin Security; Exabeam; Fundbox; Splacer; Indegy; Comilion; Lacoon Mobile Security; Alicanto ; WatchDox ; Sumo Logic; Worklight; Trusteer; Business Layers and Aqua Security.\n\nIn 2006, Kramer was selected by Network World magazine as one of 20 luminaries who changed the network industry. In 2008, he was named CEO of the Year by SC Magazine. In 2013, Kramer was inducted into the Infosecurity Europe Hall of Fame.\n\nBased in Israel, Kramer enjoys deep-sea diving and photography. He was a weightlifting champion in his youth.\n\n\n"}
{"id": "16284001", "url": "https://en.wikipedia.org/wiki?curid=16284001", "title": "Signed overpunch", "text": "Signed overpunch\n\nA signed overpunch is a code used to store the sign of a number by changing the last digit. It is used in character data on IBM mainframes by languages such as COBOL, PL/I, and RPG. Its purpose is to save a character that would otherwise be used by the sign digit. The code is derived from the Hollerith Punched Card Code, where both a digit and a sign can be entered in the same card column. Character data which may contain overpunches is called \"zoned decimal\".\n\nThe codice_1 instruction on IBM System/360 architecture machines converts the sign of a zoned decimal number when converting to \"packed decimal\",\nand the corresponding codice_2 instruction will set the correct overpunched sign of its zoned decimal output.\n\nPL/I uses the codice_3 attribute to declare zoned decimal data with a signed overpunch. Each character in a numeric picture except codice_4, which indicates the position of the assumed decimal point, represents a digit. A picture character of codice_5, codice_6, or codice_7 indicates a digit position which may contain an overpunch. codice_5 indicates that the position will contain {–I if positive and }–R if negative. codice_6 indicates that the position will contain {–I if positive and 0-9 if negative. codice_7 indicates that the position will contain 0–9 if positive and }–R if negative.\n\nFor example codice_11 describes a four-character numeric field. The first position may be blank or will contain a digit 0–9. The next two positions will contain digits, and the fourth position will contain 0–9 for a positive number and }–R for negative.\n\nAssigning the value 1021 to the above picture will store the characters \"1021\" in memory; assigning -1021 will store \"102J\".\n\n10} is -100<BR>\n45A is 451\n\nDecimal points are usually implied and not explicitly stated in the text. Using numbers with two decimal digits:\n\n1000} is -100.00\n\nCOBOL representation of signed overpunch characters \"is not standardized in ASCII, and different compilers use different overpunch codes.\" In most cases, \"the representation is not the same as the result of converting an EBCDIC Signed field to ASCII with a translation table.\" PL/I compilers on ASCII systems use the same set of characters as in EBCDIC to represent overpunches.\n"}
{"id": "2150852", "url": "https://en.wikipedia.org/wiki?curid=2150852", "title": "SipXecs", "text": "SipXecs\n\nsipXecs is an open-source enterprise communications system. It was initially developed as a proprietary voice over IP telephony server in 2003 by Pingtel Corporation in Boston, MA, and later extended with additional collaboration capabilities in the SIPfoundry project. Its core feature is a software implementation of the Session Initiation Protocol (SIP), which makes it an IP based communications system (IP PBX).\n\nsipXecs is often compared to other open source telephony and softswitch solutions such as Asterisk, FreeSWITCH, and the SIP Express Router, but the design of sipXecs is substantially different from Asterisk and FreeSWITCH.\n\nDevelopment of sipXecs was started in 2003 by Pingtel Corp, a Boston, MA based venture backed company. In 2004, Pingtel adopted an open-source business model and contributed the codebase to the not-for-profit organization SIPfoundry. It has been an open source project since then.\n\nPingtel's assets were acquired by Bluesocket in July 2007. In August 2008 the Pingtel assets were acquired from Bluesocket by Nortel. Subsequent to the acquisition by Nortel, Nortel released the SCS500 product based on sipXecs. SCS500 was positioned as an open and software-only telephony server for the SMB market up to 500 users and received some recognition. It was later renamed SCS and positioned as an Enterprise Communications System.\n\nSubsequent to the Nortel bankruptcy and the acquisition of the Nortel assets by Avaya, Avaya has backed away from SCS in an effort to rationalize its product portfolio post acquisition of Nortel. The sipXecs solution continued to be used as the basis for the Avaya Live cloud based communications service.\n\nIn April 2010 the founders of SIPfoundry founded eZuce. eZuce commercialized the sipXecs solution in close cooperation with SIPfoundry and the open source community.\n\nsipXecs is typically deployed by enterprises wanting to replace a traditional private branch exchange (PBX) with a software-based solution that can be produced in a cloud environment. In addition to telephony features, sipXecs offers collaboration capabilities such as enterprise instant messaging and conferencing.\n\nsipXecs is designed as a software-only, distributed cloud application. It runs on the Linux operating system CentOS or RHEL on either virtualized or physical servers. A minimum configuration allows running all of the sipXecs components on a single server, including database, all available services, and the sipXecs management. Global clusters can be built using built-in auto-configuration capabilities from the centralized management system.\n\nsipXecs uses MongoDB as a distributed and partition tolerant database for global transactions, includes CFEngine for orchestration of clusters and JasperReports for reporting. The management and configuration system is based on the Spring Framework. sipXecs includes FreeSWITCH as its media server and Openfire for presence and instant messaging services.\n\nsipXecs follows standards such as Session Initiation Protocol (SIP), SRTP, Extensible Messaging and Presence Protocol (XMPP), SIP and XMPP over TLS, and several Web standards including WebRTC, WebSOCKET and Representational State Transfer (REST).\n\neZuce, Inc.holds the US patent on the architecture referred to as SIP Session Oriented Architecture or SSOA.\n\nAdoption of open source solutions is difficult to quantify. SIPfoundry claims on its Web site that over one million users have downloaded sipXecs in small and large companies worldwide.\n\nAmazon.com was an early adopter of sipXecs. This initial 5,000 user deployment expanded considerably in the following years.\n\nOnRelay, a company in the UK, selected sipXecs for its fixed-mobile convergence solution sold to carriers.\n\nColorado State University and Cedarville University of Ohio committed to sipXecs in 2010.\n\nRed Hat deployed a commercial version of sipXecs from eZuce globally in 2012.\n\nUnder the SIPfoundry Higher Education Program (HEP) and as of 2014 Lafayette College, St. Mary's University, Messiah College, Colorado School of Mines, Carthage College deployed sipXecs to replace their respective PBX systems.\n\nsipXecs is used by small and large enterprises ranging up to about 20,000 users per cluster. SIPfoundry lists the following users on its Web site: Brevard County FL, Dutch Police, Easter Seals, Siemens Transportation, British Airways.\n\nsipXecs is available for Red Hat Linux and CentOS. It runs virtualized in different cloud environments such as the Amazon Elastic Compute Cloud, the Google Compute Engine, the HP Cloud, IBM SoftLayer, VMware vCloud and VMware ESX, OpenStack environments, and clouds from other vendors using these technologies.\n\nSIPfoundry distributes the sipXecs source code under the Affero General Public License (AGPL).\n\nMany different corporate and individual contributors contributed to sipXecs, including Pingtel, Bluesocket, Nortel, Avaya, and eZuce as some of the larger corporate contributors representing 864,791 lines of code. In addition, the sipXecs solution includes many other open source components. SIPfoundry holds Copyright on all derivative work. Contributions to sipXecs are made under a Contributor Agreement, which grants SIPfoundry shared Copyright with the original author on all contributed code.\n\nsipXecs supports a wide range of SIP compatible hardware, such as PSTN gateways, desk phones, softphones and mobile phone applications. A plug n'play auto-configuration capability is available for phones from currently (software release 14.04) 18 different vendors.\n\nThe sipXecs system represents a reference implementation of the SIP standard. It was used at SIPIT interoperability events organized by the SIP Forum to test interoperability of SIP solutions from many different vendors.\n\n\n"}
{"id": "9898816", "url": "https://en.wikipedia.org/wiki?curid=9898816", "title": "Sling Media", "text": "Sling Media\n\nSling Media Inc. is a technology company that develops placeshifting and Smart TV solutions for consumers, multiple-system operators and set top box manufacturers. The company is based in Foster City, California, and was a subsidiary of EchoStar Corporation (acquired in the fall of 2007). Their initial product, the Slingbox, debuted on the US market on July 1, 2005. The EchoStar business unit was part of a corporate assets exchange with Dish Network at the beginning of 2017 and now operates as Dish Technologies Corporation under Dish Network.\n\nThe company was founded in 2004 by brothers Blake and Jason Krikorian from San Francisco, along with Bhupen Shah, who had the relationships to help establish Sling's presence in Bangalore. The idea for Sling originated during the 2002 Major League Baseball season, when the Krikorian brothers, who are dedicated San Francisco Giants fans, often traveled far from home, and faced missing the best games of the season.\n\nOn September 24, 2007, EchoStar announced an agreement to acquire Sling Media for approximately $380 million USD. Blake and Jason left the company in January 2009.\n\nOn August 3, 2016, Blake Krikorian died of a heart attack at age 48.\n\nThe original Slingbox, now referred to as the Slingbox Classic, was released July 1, 2005. It was designed by Yves Béhar, and had the appearance of a \"foil-wrapped chocolate bar.\"\n\nImprovement came with the introduction of the second-generation line of Sling Media products: the Slingbox AV, the Slingbox Tuner, and the Slingbox Pro. While the Slingbox AV became a simplified unit with s-video and composite inputs only, the Slingbox Tuner provided service for the other end of the spectrum, with only a single coaxial input for use by basic cable and antenna-only applications. The Slingbox Pro introduced a four input design, combining the capabilities of the AV and Tuner units while also allowing for the connection of high definition sources with the use of an accessory cable adding component and digital audio inputs.\n\nIn 2007, Sling introduced the Slingbox SOLO, a third generation box that was a \"streamlined version of the Slingbox Pro\". It provided a high quality standard definition video stream and a lower price point. This model was followed up \nin 2008 by the Slingbox PRO-HD, a high-end device that supported placeshifting HDTV (1080i) video and currently is the only Slingbox to include an ATSC tuner for over-the-air HDTV broadcasts. The Slingbox SOLO was also later repackaged as the Slingbox 120 for special vertical and international markets.\n\nAn unknown number of the third generation Slingboxes were susceptible to the capacitor plague. While many enthusiasts replaced these capacitors on their own, Sling later addressed these issues in support. Remanufactured and refurbished third-generation Slingboxes have been fixed as well.\n\nIn October 2012, Sling Media launched the Slingbox 350 and 500 to replace the Slingbox SOLO and PRO-HD. With the digital television transition in the United States, the desirability of a standard definition focused product no longer existed in Sling's main market. Therefore, both boxes include HDTV capability, though the ATSC digital tuner that was included in the PRO-HD was not included in either Slingbox. The Slingbox 350 is the base product, with one SD/HD audio-video input (composite or component) and an ethernet port to connect to the Internet.\n\nThe Slingbox 500 was positioned as a platform for next-generation Smart TV capabilities. In addition to placeshifting, the Slingbox 500 included streaming apps from Dish Digital, including Dishworld and Blockbuster On-Demand, as well as the ability to manage and view personal media, including video. The Slingbox 500 had Wi-Fi networking and HDMI passthrough capabilities. However, because of restrictive HDCP DRM, Sling still recommends that customers use component cables for placeshifting.\n\nIn July 2014, Sling Media launched the award-winning Slingbox M1 to replace the Slingbox 350. With the introduction of this Slingbox, the entire line of Slingbox hardware now support Wi-Fi connectivity. In addition, users were now able to configure a Slingbox M1 using the Slingplayer for iPad, iPhone and Android phone, as well as the Slingplayer for Desktop that was reintroduced at the same time.\n\nAlso introduced in July 2014, SlingTV was a free upgrade to the Slingbox 500 that provides a graphical user interface and information overlays for living room TV viewing in addition to providing place shifting capabilities. The interface also provides recommendations based on aggregate viewing data, popularity and social activity. It was released in September 2014 to current Slingbox 500 customers.\n\nWith the licensing of the Sling brand to Dish Network for the Sling Television OTT service, the SlingTV box reverted to the Slingbox 500 product name.\n\nIn 2008, Sling introduced the SlingCatcher, a hardware device to view content from a remote Slingbox, as well as personal media. The product garnered mixed reviews from its limited capabilities, including no support for HDTV, complex nature and its price.\n\nBecause early Slingboxes did not support Wi-Fi, connecting them to a network was difficult if a customer did not have an ethernet jack near their set top box. To address these needs, Sling released the SlingLink line of power line adapters. With the release of a Wi-Fi-enabled Slingbox 500, the product was discontinued and is no longer supported.\n\nWhen the original Slingbox Classic was launched, customers used free downloadable software for Windows or Macintosh to access video that was streamed from the Slingbox. That application was phased out with the introduction of the Slingbox Watch website, which utilized an NPAPI plug-in for video streaming. Because of the discontinuation of the NPAPI support on Chrome and demand from customers, Sling reintroduced the Slingplayer for Desktop application for Windows and Apple Mac OS X with the launch of the Slingbox M1. However, the relaunched desktop application is only compatible with the Slingbox M1 models. In addition, Sling has also added banner ads and periodic pre-roll video ads when using the Watch website.\n\nCustomers can also purchase the Slingplayer apps for their mobile smartphones and tablets. Supported platforms include iOS (iPhone and iPad), Android (phones and tablets), Kindle Fire and Microsoft Windows 8.1 tablets. Previously supported platforms include Blackberry, Palm OS and Symbian.\n\nSlingplayer apps also have the ability to \"cast\" their video stream to a TV screen through a Smart TV streaming box. Supported platforms include:\n\nThere were several native apps for Connected Devices that can view and control a remote Slingbox. Those devices included:\n\nThese apps have been deprecated as of July 2014 and will no longer be supported.\n\nIn addition to developing products and services for consumers, Sling also provides multiple-system operators and set top box manufacturers a solution for mobile viewing of licensed content and the integration of Smart TV technologies. These capabilities include an SDK, cloud infrastructure and engineering resources.\n\nSling's technology is currently embedded in the following products:\n\nEarly in its history, the Slingbox caused widespread speculation of its possible legal implications. High on the list of issues cited by critics, was the ability to provide a loophole around proximity control, potentially allowing people outside the approved viewing area for events, especially sports, in which distribution traditionally has been restricted by time and region. However, the practice of placeshifting is not unlike that of timeshifting, which has been upheld in courts across the world due to the personal nature of a timeshifted rebroadcast, which is deemed \"non-infringing fair use\". Furthermore, Sling Media's technology limits access to a single authorized user, which prevents unauthorized or multi-user access thereby maintaining the personal nature of the placeshifted content.\n\nIronically, given the controversy, the retail Slingbox hardware has found an unexpected niche market in television broadcasting. Broadcast engineers at several TV stations have installed them at remote \"towercam\" locations to observe traffic and weather conditions. KPIX-TV in San Francisco has several connected with wireless networking, using EV-DO via a cellular network (mobile phone) provider. This costs only a few hundred dollars for each site, versus well over ten thousand for a setup with a remote pickup unit and auxiliary broadcast licenses. However, the system is not yet reliable or broadband enough to handle live remote broadcasts.\n\nCable TV providers are also using it to provide proof of performance for companies that run TV ads on their systems. It is also used with Amateur television transmissions. There are also hosted Slingbox services where the slingbox and set top box are hosted in a data centre on behalf of the user. This means that the management of the devices is done by the host and that the user can access TV streams from their hosted Slingbox wherever, whenever and whatever device (PC, Mobile, TV) they want.\n\nDuring the 2007 Consumer Electronics Show, the firm announced a future feature named Clip+Sling. It allows users to share clips of their favorite TV shows with each other through a hosted Web service. The announcement was made during Leslie Moonves' keynote speech.\n\nOn December 2, 2008, Sling Media announced the public launch of Sling.com, an online video entertainment destination. Users can go to Sling.com to watch clips, TV shows, films, news and sports. This includes video programming from over 90 content providers spanning 150 content brands. In addition to the on-demand offerings, Slingbox owners could connect to their Slingbox through the website, making their Slingboxes available without a software client download.\n\nHowever, Clip+Sling was never launched and Sling.com eventually became the home of the web-based Watch player. This signaled Sling Media's intent at the time to migrate users from using desktop software to Sling.com to access to their Slingbox.\n\nIn January 2013, EchoStar and Sling Media sued Belkin and Monsoon Multimedia for infringing on five patents related to placeshifting. In March 2013, Sling Media also initiated a complaint with the ITC to block the importing of Belkin and Monsoon Media products related to the @TV and Vulkano products, respectively. The ITC complaint also targeted chips from C2 Microsystems.\n\nIn May 2013, Belkin and Sling Media settled their portion of the suit. In December 2013, the ITC closed out the case and barred Monsoon Multimedia products from being imported into the US.\n\n\n"}
{"id": "456147", "url": "https://en.wikipedia.org/wiki?curid=456147", "title": "Speargun", "text": "Speargun\n\nA speargun is an underwater fishing implement designed to launch a spear at fish or other underwater animals or targets. Spearguns are used in sport fishing and underwater target shooting. The two basic types are pneumatic and those powered by rubber bands. Spear types come in a number of varieties including threaded, break-away and lined. Floats and buoys are common accessories when targeting larger fish.\n\nThe basic components of a speargun are a spear, a stock or barrel, and a handle or grip containing a trigger mechanism. Spearguns are usually from long, round or roughly rectangular from in diameter/width.\n\nThe two most common types are:\n\nRubber powered spearguns come in two types: those made from wood, and those of sealed tubing metal or composite and/or a combination of these materials construction: referred to as the Stock or Barrel respectively.The wooden type is often referred to as a Hawaiian sling.\n\nThose of a tubular barrel variety have separate \"muzzles\" fitting in or over one end of the tube, for attaching the rubber bands; while others (usually those made of wood), mainly have the bands passing through a horizontal slot in the stock.\n\nA rubber powered gun, besides the barrel, has the following parts:\n\nA Pneumatic gun differs from a rubber model in that it has a thicker spear that goes inside a sealed internal barrel encased in a hollow sealed outer casing that contains the air which is at ambient pressure until it is pumped up by hand to a pressure usually equal to one strong rubber band. The trigger mechanism (which is at the back of the gun) has a handle/grip below it or midway up the gun. Once at the required pressure the spear is forced down the barrel to engage the trigger mechanism and is then loaded and ready. There will usually be a strong line at least twice the length of the gun connecting the spear to the front of the gun. When loaded great caution must be exercised as the speargun is now a dangerous weapon. Without a trigger, rubber bands or air pressure the speargun cannot be loaded and therefore cannot function.\n\nSpears and spearguns have various uses:\n\nAll spearguns have a trigger mechanism that holds a spear in place along the barrel.\n\nTraditionally, rear-handle spearguns are popular in Europe and mid-handle guns were used in North America, however as spearfishing has developed as an international sport these distinctions have blurred.\n\nSouth African speargun manufacturers have improved speargun designs with the use of a rail along the barrel that prevents the spear from flexing under pressure from the rubber bands.\n\nThe speargun can have:\n\nThreaded shafts allow for different tips to be used by having them screw on or off. Some tips include a basic short tip with a folding flopper and others use longer tips with a rotating feature, usually with multiple floppers. When larger fish are targeted a break-away tip is recommended (not to be confused with the break-away setup below) as it allows some portion, usually , of the tip to \"break-away\" from the main shaft after it penetrates a fish allowing the shaft to \"fall out\" of the fish but maintain the fish connection by either a small steel cable or high-strength spectra or kevlar line.\n\nHawaiian shafts are the simplest shaft as the tip is shaped on the end and a folding barb installed directly behind the tip to keep the fish from sliding off. There are different length barbs and sometimes multiples (sometimes referred to as a Florida rig).\n\nIn this setup, the most common, the speargun is attached to the shaft by a line. The line is usually a heavy monofilament ( test), spectra, or other spectra-like material (braided line) with 180–270+ kg (400–600+ lb) rating. The line wraps around the bottom or side of the gun upon loading and releases via a mechanical release or tension release to allow the line to travel with the spear after firing. There are typically two different methods of connecting the line to the spear shaft, either a slide ring that travels up and down the shaft to machined stopping points or via a hole drilled through some portion of the shaft (usually the rear). This is the most common method for taking fish by speargun and the majority of the other types of setups use the mentioned connection options to the shaft. Some newer setups offer an option to quickly attach line to, or disconnect from, a shaft making for a line or free shaft option on a single speargun.\n\nHere the spear gun is connected to a buoy via a float rope. After spearing a fish, the spearfisher detaches the float rope and uses a speed stick (a metal spike) attached to the float rope to thread the fish onto the float rope through its gills. The fish will then gradually slide up the float rope as the diver swims until it rests underneath the buoy. When shooting larger fish, the diver can let go of his gun and play the fish from the float line, giving the fish more room to tire and preventing it from tearing off the spear or dragging the spearfisher under the water.\n\nHere the spear shaft is connected directly to the buoy. It is loosely fitted to the gun as well while hunting, but after the spear is fired its force of movement detaches the line from the gun. The spearfisher is then able to subdue the fish from the buoy or float line while retaining possession of his gun. This is for two reasons. Firstly, the gun can be used to push off sharks or signal the boat driver, and secondly to prevent loss of the gun should the fish break the line, or should the spearfisher lose grip on the buoy. This setup can be used in conjunction with elastic bungee-style rope and a body board style float with locking cleats. This respectively maintains constant pressure on the fish and allows the spearfisher to rest while being towed around. They can then gather the bungee line as the fish tires and lock it off in order to gradually pull the fish closer.\n\nHere the gun has a line reel like on a fishing rod. After spearing a fish, the reel unwinds, allowing the spearfisher room to play the fish. Reel setups are useful when ocean structures such as built up reef or kelp gardens prevent the spearfisher from towing a buoy.\n\nHere the spear shaft is not connected to a gun or buoy. This is more commonly associated with spearfishing on scuba where excess cable or line can be problematic. Also it can be used in the areas with exceptionally clear water, where underwater hunter can track his shaft after firing it. It is not used in turbid waters with poor visibility.\n\nHere, the spear shaft is connected to monofilament or other small high strength line, which is wrapped around the speargun, then connected to a larger, easier to handle rope which the diver holds in his hand. On the larger rope there is a loop on the end big enough for the divers thumb. The rope is placed on the thumb, then wrapped around the back of the hand. In this setup the spear is also totally free from the gun, allowing the diver to work the fish using the large rope, while keeping his gun safe. If the fish is too large to hang on to, the diver can tie it off or release it, only losing his spear and rope. This is used usually when fishing around oil rigs, or other hazardous diving such as hunting larger fish or in poor visibility.\n\n\n\n"}
{"id": "1673229", "url": "https://en.wikipedia.org/wiki?curid=1673229", "title": "Stealth mode", "text": "Stealth mode\n\nIn business, stealth mode is a company's temporary state of secretiveness, usually undertaken to avoid alerting competitors to a pending product launch or other business initiative. A stealth product is a product a company develops in secret, and a stealth company is a new company that avoids initial disclosure as to its existence, purpose, products, personnel, funding, brand name, or other important attributes. The term stealth innovation has been applied to individual projects and ideas that are developed in secret inside a company.\n\nWhereas secrecy is the historical norm in many fields of business, start-up companies often thrive on publicity and open sharing of information. Openness is common to the business culture of Silicon Valley and other technology centers, with competitors freely exchanging news of discoveries, products under development, and other company news. There is intense media interest in some business sectors, with even relatively small funding rounds covered in specialized press. Public relations is considered useful to attract interest from talent, customers, and investors, and to promote the careers of the people involved. Additionally, competitors often collaborate on projects, or buy each other's products.\n\nSome companies nevertheless avoid publicity in fields that are ordinarily not secretive. Among the reasons, a small, relatively unfunded company may wish to avoid giving companies with more resources time to develop competing technologies. The very announcement that a larger or better-known company is working on a competing product may damp interest in the smaller upstart. If the innovative company has no realistic means of protecting its new intellectual property, it may seek to obtain a \"first-mover advantage\" by waiting until the company or its products are ready to sell before they are announced. This gives as long a lead as possible before others may copy its products, distribution channels, brand, or other business advantages. Conversely, companies with a protectable new technology may nevertheless wish to wait until they have filed or obtained a patent.\n\nWhen an entire company is in stealth mode it may attempt to mislead the public about its true company goals. For example, it may give code names to its pending products. It may operate a corporate website that does not disclose its personnel or location. New companies may operate under a temporary \"stealth name\" that does not disclose its field of business.\n\nTo enforce stealthy behavior, companies often require employees to sign non-disclosure agreements, and strictly control who may speak with the media.\n\nAt the in-company level, stealth mode can also refer to a new project or idea that is kept secret, not just from external parties, but also from internal stakeholders in order to avoid a (premature) dismissal of the idea. Key behaviors can include soliciting informal project sponsors, engaging in covert testing of the concept, freeing up extra resources and building a \"cover story\" for the project.\n"}
{"id": "545863", "url": "https://en.wikipedia.org/wiki?curid=545863", "title": "Step response", "text": "Step response\n\nThe step response of a system in a given initial state consists of the time evolution of its outputs when its control inputs are Heaviside step functions. In electronic engineering and control theory, step response is the time behaviour of the outputs of a general system when its inputs change from zero to one in a very short time. The concept can be extended to the abstract mathematical notion of a dynamical system using an evolution parameter.\n\nFrom a practical standpoint, knowing how the system responds to a sudden input is important because large and possibly fast deviations from the long term steady state may have extreme effects on the component itself and on other portions of the overall system dependent on this component. In addition, the overall system cannot act until the component's output settles down to some vicinity of its final state, delaying the overall system response. Formally, knowing the step response of a dynamical system gives information on the stability of such a system, and on its ability to reach one stationary state when starting from another.\n\nInstead of frequency response, system performance may be specified in terms of parameters describing time-dependence of response. The step response can be described by the following quantities related to its time behavior,\n\n\nIn the case of linear dynamic systems, much can be inferred about the system from these characteristics. Below the step response of a simple two-pole amplifier is presented, and some of these terms are illustrated.\n\nThis section describes the step response of a simple negative feedback amplifier shown in Figure 1. The feedback amplifier consists of a main open-loop amplifier of gain \"A\" and a feedback loop governed by a feedback factor β. This feedback amplifier is analyzed to determine how its step response depends upon the time constants governing the response of the main amplifier, and upon the amount of feedback used.\n\nA negative-feedback amplifier has gain given by (see negative feedback amplifier):\n\nwhere \"A\" = open-loop gain, \"A\" = closed-loop gain (the gain with negative feedback present) and β = feedback factor.\n\nIn many cases, the forward amplifier can be sufficiently well modeled in terms of a single dominant pole of time constant τ, that it, as an open-loop gain given by:\n\nwith zero-frequency gain \"A\" and angular frequency ω = 2π\"f\". This forward amplifier has unit step response\n\nan exponential approach from 0 toward the new equilibrium value of \"A\".\n\nThe one-pole amplifier's transfer function leads to the closed-loop gain:\n\nThis closed-loop gain is of the same form as the open-loop gain: a one-pole filter. Its step response is of the same form: an exponential decay toward the new equilibrium value. But the time constant of the closed-loop step function is τ / (1 + β \"A\"), so it is faster than the forward amplifier's response by a factor of 1 + β \"A\":\n\nAs the feedback factor β is increased, the step response will get faster, until the original assumption of one dominant pole is no longer accurate. If there is a second pole, then as the closed-loop time constant approaches the time constant of the second pole, a two-pole analysis is needed.\n\nIn the case that the open-loop gain has two poles (two time constants, τ, τ), the step response is a bit more complicated. The open-loop gain is given by:\n\nwith zero-frequency gain \"A\" and angular frequency ω = 2π\"f\".\n\nThe two-pole amplifier's transfer function leads to the closed-loop gain:\n\nThe time dependence of the amplifier is easy to discover by switching variables to \"s\" = \"j\"ω, whereupon the gain becomes:\n\nThe poles of this expression (that is, the zeros of the denominator) occur at:\n\nwhich shows for large enough values of βA\" the square root becomes the square root of a negative number, that is the square root becomes imaginary, and the pole positions are complex conjugate numbers, either \"s\" or \"s\"; see Figure 2:\n\nwith\n\nand\nUsing polar coordinates with the magnitude of the radius to the roots given by |\"s\"| (Figure 2):\n\nand the angular coordinate φ is given by:\n\nTables of Laplace transforms show that the time response of such a system is composed of combinations of the two functions:\n\nwhich is to say, the solutions are damped oscillations in time. In particular, the unit step response of the system is:\n\nwhich simplifies to\n\nwhen \"A\" tends to infinity and the feedback factor β is one.\n\nNotice that the damping of the response is set by ρ, that is, by the time constants of the open-loop amplifier. In contrast, the frequency of oscillation is set by μ, that is, by the feedback parameter through β\"A\". Because ρ is a sum of reciprocals of time constants, it is interesting to notice that ρ is dominated by the \"shorter\" of the two.\n\nFigure 3 shows the time response to a unit step input for three values of the parameter μ. It can be seen that the frequency of oscillation increases with μ, but the oscillations are contained between the two asymptotes set by the exponentials [ 1 − exp (−ρt) ] and [ 1 + exp(−ρt) ]. These asymptotes are determined by ρ and therefore by the time constants of the open-loop amplifier, independent of feedback.\n\nThe phenomenon of oscillation about the final value is called ringing. The overshoot is the maximum swing above final value, and clearly increases with μ. Likewise, the undershoot is the minimum swing below final value, again increasing with μ. The settling time is the time for departures from final value to sink below some specified level, say 10% of final value.\n\nThe dependence of settling time upon μ is not obvious, and the approximation of a two-pole system probably is not accurate enough to make any real-world conclusions about feedback dependence of settling time. However, the asymptotes [ 1 − exp (−ρt) ] and [ 1 + exp (−ρt) ] clearly impact settling time, and they are controlled by the time constants of the open-loop amplifier, particularly the shorter of the two time constants. That suggests that a specification on settling time must be met by appropriate design of the open-loop amplifier.\n\nThe two major conclusions from this analysis are: \n\nAs an aside, it may be noted that real-world departures from this linear two-pole model occur due to two major complications: first, real amplifiers have more than two poles, as well as zeros; and second, real amplifiers are nonlinear, so their step response changes with signal amplitude.\nHow overshoot may be controlled by appropriate parameter choices is discussed next.\n\nUsing the equations above, the amount of overshoot can be found by differentiating the step response and finding its maximum value. The result for maximum step response \"S\" is:\n\nThe final value of the step response is 1, so the exponential is the actual overshoot itself. It is clear the overshoot is zero if μ = 0, which is the condition:\n\nThis quadratic is solved for the ratio of time constants by setting \"x\" = ( τ / τ ) with the result\n\nBecause β \"A\" » 1, the 1 in the square root can be dropped, and the result is\n\nIn words, the first time constant must be much larger than the second. To be more adventurous than a design allowing for no overshoot we can introduce a factor α in the above relation:\n\nand let α be set by the amount of overshoot that is acceptable.\n\nFigure 4 illustrates the procedure. Comparing the top panel (α = 4) with the lower panel (α = 0.5) shows lower values for α increase the rate of response, but increase overshoot. The case α = 2 (center panel) is the \"maximally flat\" design that shows no peaking in the Bode gain vs. frequency plot. That design has the rule of thumb built-in safety margin to deal with non-ideal realities like multiple poles (or zeros), nonlinearity (signal amplitude dependence) and manufacturing variations, any of which can lead to too much overshoot. The adjustment of the pole separation (that is, setting α) is the subject of frequency compensation, and one such method is pole splitting.\n\nThe amplitude of ringing in the step response in Figure 3 is governed by the damping factor exp ( −ρ t ). That is, if we specify some acceptable step response deviation from final value, say Δ, that is:\n\nthis condition is satisfied regardless of the value of β \"A\" provided the time is longer than the settling time, say \"t\", given by:\n\nwhere the τ » τ is applicable because of the overshoot control condition, which makes τ = \"αβA\" τ. Often the settling time condition is referred to by saying the settling period is inversely proportional to the unity gain bandwidth, because 1/(2π τ) is close to this bandwidth for an amplifier with typical dominant pole compensation. However, this result is more precise than this rule of thumb. As an example of this formula, if Δ = 1/e = 1.8 %, the settling time condition is \"t\" = 8 τ.\n\nIn general, control of overshoot sets the time constant ratio, and settling time \"t\" sets τ.\n\nThis method uses significant points of the step response. There is no need to guess tangents to the measures Signal. The equations are derived using numerical simulations, determining some significant ratios and fitting parameters of nonlinear equations. See also .\n\nHere the steps:\n\n\nformula_36\n\nformula_37\n\nformula_38\n\nformula_39\n\nformula_40\n\nformula_41\n\nNext, the choice of pole ratio τ/τ is related to the phase margin of the feedback amplifier. The procedure outlined in the Bode plot article is followed. Figure 5 is the Bode gain plot for the two-pole amplifier in the range of frequencies up to the second pole position. The assumption behind Figure 5 is that the frequency \"f\" lies between the lowest pole at \"f\" = 1/(2πτ) and the second pole at \"f\" = 1/(2πτ). As indicated in Figure 5, this condition is satisfied for values of α ≥ 1.\n\nUsing Figure 5 the frequency (denoted by \"f\") is found where the loop gain β\"A\" satisfies the unity gain or 0 dB condition, as defined by:\n\nThe slope of the downward leg of the gain plot is (20 dB/decade); for every factor of ten increase in frequency, the gain drops by the same factor:\n\nThe phase margin is the departure of the phase at \"f\" from −180°. Thus, the margin is:\n\nBecause \"f\" / \"f\" = \"βA\" » 1, the term in \"f\" is 90°. That makes the phase margin:\n\nIn particular, for case α = 1, φ = 45°, and for α = 2, φ = 63.4°. Sansen recommends α = 3, φ = 71.6° as a \"good safety position to start with\".\n\nIf α is increased by shortening τ, the settling time \"t\" also is shortened. If α is increased by lengthening τ, the settling time \"t\" is little altered. More commonly, both τ \"and\" τ change, for example if the technique of pole splitting is used.\n\nAs an aside, for an amplifier with more than two poles, the diagram of Figure 5 still may be made to fit the Bode plots by making \"f\" a fitting parameter, referred to as an \"equivalent second pole\" position.\n\nThis section provides a formal mathematical definition of step response in terms of the abstract mathematical concept of a dynamical system formula_48: all notations and assumptions required for the following description are listed here. \n\nFor a general dynamical system, the step response is defined as follows:\n\nIt is the evolution function when the control inputs (or source term, or forcing inputs) are Heaviside functions: the notation emphasizes this concept showing \"H\"(\"t\") as a subscript.\n\nFor a linear time-invariant black box, let formula_56 for notational convenience: the step response can be obtained by convolution of the Heaviside step function control and the impulse response \"h\"(\"t\") of the system itself\n\nwhich for an LTI system is equivalent to just integrating the latter. Conversely, for an LTI system, the derivative of the step response yields the impulse response:\n\nHowever, these simple relations are not true for a non-linear or time-variant system.\n\n\n"}
{"id": "30204464", "url": "https://en.wikipedia.org/wiki?curid=30204464", "title": "Sunstone (medieval)", "text": "Sunstone (medieval)\n\nThe sunstone () is a type of mineral attested in several 13th–14th century written sources in Iceland, one of which describes its use to locate the sun in a completely overcast sky. Sunstones are also mentioned in the inventories of several churches and one monastery in 14th–15th century Iceland and Germany.\n\nA theory exists that the sunstone had polarizing attributes and was used as a navigation instrument by seafarers in the Viking Age. A stone found in 2013 off Alderney, in the wreck of a 16th-century warship, may lend evidence of the existence of sunstones as navigational devices.\n\nOne medieval source in Iceland, \"Rauðúlfs þáttr\", mentions the sunstone as a mineral by means of which the sun could be located in an overcast and snowy sky by holding it up and noting where it emitted, reflected or transmitted light (\"hvar geislaði úr honum\"). Sunstones are also mentioned in \"Hrafns saga Sveinbjarnarsonar\" (13th century) and in church and monastic inventories (14th–15th century) without discussing their attributes. The sunstone texts of \"Hrafns saga Sveinbjarnarsonar\" were copied to all four versions of the medieval hagiography \"Guðmundar saga góða\".\n\nThe description in \"Rauðúlfs þáttr\" of the use of the sunstone is as follows:\nTwo of the original medieval texts on the sunstone are allegorical. \"Hrafns saga Sveinbjarnarsonar\" contains a burst of purely allegorical material associated with Hrafn’s slaying. This involves a celestial vision with three highly cosmological knights, recalling the horsemen of the Apocalypse. It has been suggested that the horsemen of Hrafns saga contain allegorical allusions to the winter solstice and the four elements as an omen of Hrafn’s death, where the sunstone also appears.\n\n\"Rauðúlfs þáttr\", a tale of Saint Olav, and the only medieval source mentioning how the sunstone was used, is a thoroughly allegorical work. A round and rotating house visited by Olav has been interpreted as a model of the cosmos and the human soul, as well as a prefiguration of the Church. The intention of the author was to achieve an apotheosis of St. Olav, through placing him in the symbolic seat of Christ. The house belongs to the genre of \"abodes of the sun,\" which seemed widespread in medieval literature. St. Olav used the sunstone to confirm the time reckoning skill of his host right after leaving this allegorical house. He held the sunstone up against the snowy and completely overcast sky and noted where light was emitted from it (the Icelandic words used do not make it clear whether the light was reflected by the stone, emitted by it or transmitted through it). It has been suggested that in \"Rauðúlfs þáttr\" the sunstone was used as a symbol of the Virgin, following a widespread tradition in which the virgin birth of Christ is compared with glass letting a ray of the sun through.\n\nThe allegories of the above-mentioned texts exploit the symbolic value of the sunstone, but the church and monastic inventories, however, show that something called sunstones did exist as physical objects in Iceland. The presence of the sunstone in \"Rauðúlfs þáttr\" may be entirely symbolic but its use is described in sufficient detail to show that the idea of using a stone to find the sun's position in overcast conditions was commonplace.\n\nDanish archaeologist Thorkild Ramskou posited that the sunstone could have been one of the minerals (cordierite or Iceland spar) that polarize light and by which the azimuth of the sun can be determined in a partly overcast sky or when the sun is just below the horizon. The principle is used by many animals and polar flights applied the idea before more advanced techniques became available. Ramskou further conjectured that the sunstone could have aided navigation in the open sea in the Viking period. This idea has become very popular, and research as to how a sunstone could be used in nautical navigation continues.\n\nResearch in 2011 by Ropars et al., confirms that one can identify the direction of the sun to within a few degrees in both cloudy and twilight conditions using the sunstone and the naked eye. The process involves moving the stone across the visual field to reveal a yellow entoptic pattern on the fovea of the eye. Alternatively a dot can be placed on top of crystal so that when you look at it from below, two dots appear, because the light is “depolarised” and fractured along different axes. The crystal can then be rotated until the two points have the same luminosity. The angle of the top face now gives the direction of the sun. Attempts to replicate this work in both Scotland and off the coast of Turkey by science journalist Matt Kaplan and mineralogists at the British Geological Survey in 2014 failed. Kaplan communicated extensively with Ropars and neither could understand why the samples of Iceland spar that were being used during the trials did not reveal the sun's direction.\n\nThe recovery of an Iceland spar sunstone from an Elizabethan ship which sank near Alderney in 1592 suggests the possibility that the navigational technology may have persisted after the invention of the magnetic compass. Although the stone was found near a navigational instrument, its use remains uncertain.\n\nBeyond nautical navigation, a polarizing crystal would have been useful as a sundial, especially at high latitudes with extended hours of twilight, in mountainous areas, or in partly overcast conditions. This use would require the polarizing crystal to be used in conjunction with known landmarks; churches and monasteries would have valued such an object as an aid to keep track of the canonical hours.\n\nA Hungarian team proposed that a sun compass artifact with crystals might have allowed Vikings to guide their boats at night too. A type of crystal they called a sunstone can use scattered sun light from below the horizon as a guide. What they suggest is that calcite stone crystals similar to one found amongst navigational tools on a sixteenth century sunken ship were used in combination with Haidinger's brush. If so, the Vikings could have used them in the northern latitudes where during the summer it never goes completely dark. In areas of confused magnetic deviation (such as the Labrador coast) a sunstone would have been a more reliable guide than a magnetic compass.\n\n\n"}
{"id": "48837233", "url": "https://en.wikipedia.org/wiki?curid=48837233", "title": "Sustainable electronics", "text": "Sustainable electronics\n\nSustainable electronics are electronic products made with no toxic chemicals, recyclable parts, and reduced carbon emissions during production. \"Sustainability is still very new, emerging business concept. Because of that, we lack uniform guidelines or standards applicable per industry sector that can help companies establish best practices.\" \n\nAccording to Rank a Brand Electronics Green Fair Ranking Report in 2014, none of the electronic brands met all of their green requirements for level A. The only company to reach level B was Fairphone, who met 60% of their standards. Level C was awarded to Apple and Nokia with 45% and 40% respectively. The majority of the researched electronic brands were put into level D. These brands include Sony, Acer, Dell HP, Samsung, Motorola, Philips, Blackberry, Lenovo, Toshiba. They met less than 35% of the Rank a Brand criteria. Rank a Brand generalizes their findings into 4 main categories: reporting on sustainability, climate protection, ecology, and fair labor. All brands report on sustainability. Nokia is the top brand for climate protection, and Fairphone is the top brand for both ecology and fair labor. Apple, although criticized for their sustainability efforts, is a strong second in all of these categories apart from ecology.\n\nCompanies who receive an E label are stamped with Electronic Greenwashing Alert, which means that consumer can not clearly find or understand their sustainability information and might find them self confused or misguided.\n\nMany hazardous chemicals and materials are used in the production of electronics. These substances are further outlined in this page about electronic waste substances.\n\nNo brand in 2014 had completely eliminated use of Phthalates, beryllium, antimony, BFRs, and PVC in their productions, but Nokia and Motorola have the best track record by eliminating 3 out of 5 above mentioned chemicals. \n\n\"Sustainable ICT will enable us to protect and enhance human health and well-being and the environment over generations while minimizing the adverse life-cycle impacts of devices, infrastructure and services.\" \n\nElectronics contain many chemicals that are known to cause issues with human health. A lot of these chemicals also easily seep into the environment, whether it be in soil, water or the air. A lot of e-waste is exported to third world countries such as China and India, where the waste is put in a landfill and the chemicals are allowed to seep into the environment. In the U.S. in 2011 only about 25% of e-waste was actually recycled. By using sustainable electronics principles, such as Green Engineering, chemicals can be prevented from entering electronics in the first place, or can be removed properly once a product has reached the end of its life cycle.\n\nThe generation of natural bio-composites based electronics would remove the need for corrosive acids, currently used in Electronic waste recycling to recover precious metals. In developing countries, the use of these chemicals is very common as it is cheap, however. These acids, primarily hydrochloric acid and nitric acid, create massive amounts of leaching which require further processing to prevent pollution. Their unregulated use is harmful for both the environment and the workers that utilize it. The use of bio generated composites removes the need for acid digestion in the recycling process as current plastic based recycling methods suffice at collecting the recoverable metals.\n\nGreen engineering is the process of using sustainable materials and methods to create products that can be used for long periods of time and can be taken apart and reused; ultimately fostering a sustainable way to build and use technology. Green engineering works to find solutions to the waste and hazardous materials that are frequently used in the building of technology today. The goals of green engineering are to use materials that will “conserve and improve natural ecosystems while protecting human health and well-being.” Along with this, the EPA wants to have incentives to motivate companies and developers to have green engineering in mind when they produce their products. They want green engineering to become the norm as technology moves forward. The development of green engineering across communities and across the globe will promote a more sustainable way of life as humans continue to rely on technology to improve their daily lives.\n\n"}
{"id": "18418194", "url": "https://en.wikipedia.org/wiki?curid=18418194", "title": "The Benefit Company", "text": "The Benefit Company\n\nThe Benefit Company (TBC) is the local switch in the Kingdom of Bahrain handling ATM and POS transactions among other services. Established in 1997 with a special license from the Central Bank of Bahrain as \"Provider of Ancillary Services to the Financial Sector\", it is the only financial network of its kind in the country.\n\nBenefit's initial services included connecting the ATM services of the local banks in Bahrain and handling the settlements. In addition, it connected the POS terminals to all of the local issuers in the Kingdom.\n\nIn addition to local switching, Benefit is also connected to GCC Net, the main switch operating on all GCC countries. A connection was also established with Shetab in 2005; linking all BENEFIT users with Iran's only switch.\n\nIn 2006, the company offered the payment gateway service, integrating more the local banks onto one payment hub allowing banks and their merchants to perform online transactions.\n\nIn May 2012, Benefit started running the nation's check truncation system. Thus, enabling 29 banks in the Kingdom to settle checks the same day.\n\n\n\n"}
{"id": "16948325", "url": "https://en.wikipedia.org/wiki?curid=16948325", "title": "The Blue Ribbon SoundWorks", "text": "The Blue Ribbon SoundWorks\n\nThe Blue Ribbon SoundWorks was a software company in the United States. The company produced several digital audio products for the Amiga, including Bars & Pipes, a sequencer described by Sound on Sound as \"the ultimate in Amiga sequencing\", and SuperJAM!, a music composition tool. Blue Ribbon also produced the One Stop Music Shop, a hardware MIDI interface and synthesizer based on the E-mu Proteus. Other early products included Who! What! When! Where!, a personal information manager. It was founded by Melissa Jordan Grey and Todor Fay, who went on to found NewBlue, a video technology company.\n\nBlue Ribbon was acquired by Microsoft in 1995, and Microsoft subsequently merged Blue Ribbon's technology with DirectSound. After the acquisition, Microsoft made Blue Ribbon's Amiga products available for free download on CompuServe while discontinuing official support.\n"}
{"id": "857262", "url": "https://en.wikipedia.org/wiki?curid=857262", "title": "Tristram Cary", "text": "Tristram Cary\n\nTristram Ogilvie Cary, OAM (14 May 192524 April 2008) was a pioneering English-Australian composer. He was also active as a teacher and music critic.\n\nCary was born in Oxford, England, and educated at the Dragon School in Oxford and Westminster School in London. He was the third son and child of a pianist and the novelist Joyce Cary, author of \"Mister Johnson\". While working as a radar engineer for the Royal Navy during World War II, he independently developed his own conception of electronic and tape music, and is regarded as among the earliest pioneers of these musical forms.\n\nFollowing World War II, he created one of the first electronic music studios, later travelling around Europe to meet the small numbers of other early pioneers of electronic music and composition. He studied arts at the University of Oxford and went on to study composition, conducting, piano, viola and horn at Trinity College London.\n\nWith Peter Zinovieff and David Cockerell, he founded Electronic Music Studios (London) Ltd, which created the first commercially available portable synthesiser, the EMS VCS 3, and was then involved in production of such distinctive EMS products as the EMS Synthi 100.\n\nIn 1967 he created an electronic music studio at the Royal College of Music. This led to an invitation from the University of Melbourne in 1973 for a lecture tour, which in turn led to an invitation to become the Visiting Composer at the University of Adelaide in 1974. He remained there as a lecturer until 1986. He also wrote music criticism for \"The Australian\".\n\nHis concert works of note include a Sonata for guitar (1959), \"Continuum\" for tape (1969), a cantata \"Peccata Mundi\" (1972), \"Contours and Densities at First Hill\" for orchestra (1972), a Nonet (1979), String Quartet No. 2 (1985) and \"The Dancing Girls\" for orchestra (1991).\n\nCary is also particularly well known for his film and television music. He wrote music for the science fiction television series \"Doctor Who\" (including the first Dalek story), as well as the score for the Ealing comedy \"The Ladykillers\" (1955). Later film scores included \"Quatermass and the Pit\" (1967) and \"Blood from the Mummy's Tomb\" (1971), both for Hammer. He also composed the score for the ABC TV animated version of \"A Christmas Carol\". and the children's Animated special \"Katya and the Nutcracker\n\nCary was one of the first British composers to work in musique concrète. In 1967 he created the first electronic music studio of the Royal College of Music. He built another at his home in Suffolk, which he transported to Australia when he emigrated there, and incorporated it into the University of Adelaide where he worked as a lecturer until 1986.\n\nHe provided the visual design for the EMS VCS3 synthesizer.\n\nCary died in Adelaide, South Australia on 24 April 2008, aged 82.\n\nCary won the 1977 Albert H. Maggs Composition Award. He was awarded the Medal of the Order of Australia in 1991 in recognition of service to music. He also received the 2005 lifetime achievement award from the Adelaide Critics' Circle for his contribution to music in England and Australia.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "14614753", "url": "https://en.wikipedia.org/wiki?curid=14614753", "title": "Uniform Customs and Practice for Documentary Credits", "text": "Uniform Customs and Practice for Documentary Credits\n\nThe Uniform Customs and Practice for Documentary Credits (UCP) is a set of rules on the issuance and use of letters of credit. The UCP is utilized by bankers and commercial parties in more than 175 countries in trade finance. Some 11-15% of international trade utilizes letters of credit, totaling over a trillion dollars (US) each year.\n\nHistorically, the commercial parties, particularly banks, have developed the techniques and methods for handling letters of credit in international trade finance. This practice has been standardized by the ICC (International Chamber of Commerce) by publishing the UCP in 1933 and subsequently updating it throughout the years. The ICC has developed and moulded the UCP by regular revisions, the current version being the UCP600. The result is the most successful international attempt at unifying rules ever, as the UCP has substantially universal effect. The latest revision was approved by the Banking Commission of the ICC at its meeting in Paris on 25 October 2006. This latest version, called the UCP600, formally commenced on 1 July 2007.\n\nA significant function of the ICC is the preparation and promotion of its uniform rules of practice. The ICC’s aim is to provide a codification of international practice occasionally selecting the best practice after ample debate and consideration. The ICC rules of practice are designed by bankers and merchants and not by legislatures with political and local considerations. The rules accordingly demonstrate the needs, customs and practices of business. Because the rules are incorporated voluntarily into contracts, the rules are flexible while providing a stable base for international review, including judicial scrutiny. International revision is thus facilitated permitting the incorporation of the changing practices of the commercial parties. ICC, which was established in 1919, had as its primary objective facilitating the flow of international trade at a time when nationalism and protectionism threatened the easing of world trade. It was in that spirit that the UCP were first introduced – to alleviate the confusion caused by individual countries’ promoting their own national rules on letter of credit practice. The aim was to create a set of contractual rules that would establish uniformity in practice, so that there would be less need to cope with often conflicting national regulations. The universal acceptance of the UCP by practitioners in countries with widely divergent economic and judicial systems is a testament to the rules’ success.\n\nThe latest (July 2007) revision of UCP is the sixth revision of the rules since they were first promulgated in 1933. It is the outcome of more than three years of work by the ICC's Commission on Banking Technique and Practice.\n\nThe UCP remain the most successful set of private rules for trade ever developed. A range of individuals and groups contributed to the current revision including: the UCP Drafting Group, which waded through more than 5000 individual comments before arriving at this final text; the UCP Consulting Group, consisting of members from more than 25 countries, which served as the advisory body; the more than 400 members of the ICC Commission on Banking Technique and Practice who made pertinent suggestions for changes in the text; and 130 ICC National Committees worldwide which took an active role in consolidating comments from their members.\n\nDuring the revision process, notice was taken of the considerable work that had been completed in creating the International Standard Banking Practice for the Examination of Documents under Documentary Credits (ISBP), ICC Publication 745. This publication has evolved into a necessary companion to the UCP for determining compliance of documents with the terms of letters of credit. It is the expectation of the Drafting Group and the Banking Commission that the application of the principles contained in the ISBP, including subsequent revisions thereof, will continue during the time UCP 600 is in force. At the time UCP 600 is implemented, there will be an updated version of the ISBP (the most recent one being the 2013 revision) to bring its contents in line with the substance and style of the new rules.\nNote that UCP600 does not automatically apply to a credit if the credit is silent as to which set of rules it is subject to. A credit issued by SWIFT MT700 is no longer subject by default to the current UCP; it has to be indicated in field 40E, which is designated for specifying the \"applicable rules\".\n\nWhere a credit is issued subject to UCP600, the credit will be interpreted in accordance with the entire set of 39 articles contained in UCP600. However, exceptions to the rules can be made by express modification or exclusion. For example, the parties to a credit may agree that the rest of the credit shall remain valid despite the beneficiary's failure to deliver an installment. In such case, the credit has to nullify the effect of article 32 of UCP600, such as by wording the credit as: \"The credit will continue to be available for the remaining installments notwithstanding the beneficiary's failure to present complied documents of an installment in accordance with the installment schedule.\"\n\nThe eUCP was developed as a supplement to UCP due to the sense at the time that banks and corporates together with the transport and insurance industries were ready to use electronic commerce. The hope and expectation that surrounded the development of eUCP has failed the UCP600 and it will remain as a supplement albeit slightly amended to identify its relationship with UCP600.\n\nAn updated version of the eUCP came into effect on 1 July 2007 to coincide the commencement of the UCP600. There are no substantive changes to the eUCP, merely references to the UCP600.\n\nThe Certificate for Documentary Credit Specialists (CDCS) is the leading qualification for documentary credit specialists. Recognised worldwide as a benchmark of competence for international practitioners, it enables documentary credit specialists to demonstrate practical knowledge and understanding of the complex issues associated with documentary credit practice such as:\n\n\nCDCS was developed by the Institute of Financial Services and Bankers Association for Finance and Trade (formerly IFSA), in partnership with the International Chamber of Commerce (ICC). The qualification was first examined in 1999 and has seen a rapid growth in the uptake of the programme across the world.\n\nThe Certificate is examined in over 30 countries each year and is taught through distance learning and self-study over a four-month period. The CDCS assessment involves a three-hour multiple-choice examination of 70 questions, designed to test knowledge and its application to real-life situations. Once a practitioner has achieved the qualification, they have the right to add the professional designation of ‘CDCS’ after their name for a period of three years.\n\nAfter the three-year period a process of Re-Certification is required where the professional has to provide evidence of Continued Professional Development to maintain the accreditation or re-sit the examination.\n\nQualification syllabus and specification can be found at www.CDCSinternational.org.\n\n"}
{"id": "1862254", "url": "https://en.wikipedia.org/wiki?curid=1862254", "title": "Vessel traffic service", "text": "Vessel traffic service\n\nA vessel traffic service (VTS) is a marine traffic monitoring system established by harbour or port authorities, similar to air traffic control for aircraft. Typical VTS systems use radar, closed-circuit television (CCTV), VHF radiotelephony and automatic identification system to keep track of vessel movements and provide navigational safety in a limited geographical area.\n\nThe VTS guidelines (see VTS Manual IALA, Edition 6, Chapter 12, for general reference) require that the VTS authority should be provided with sufficient staff, appropriately qualified, suitably trained and capable of performing the tasks required, taking into consideration the type and level of services to be provided in conformity with the current IMO guidelines on the subject.\n\nIALA Recommendation V-103 is the Recommendation on Standards for Training and Certification of VTS Personnel. There are four associated model courses V103/1 to V-103/4 which are approved by IMO and should be used when training VTS personnel for the VTS qualifications.\n\nAn information service is a service to ensure that essential information becomes available in time for on-board navigational decision-making.\n\nThe information service is provided by broadcasting information at fixed times and intervals or when deemed necessary by the VTS or at the request of a vessel, and may include for example reports on the position, identity and intentions of other traffic; waterway conditions; weather; hazards; or any other factors that may influence the vessel's transit.\n\nA traffic organization service is a service to prevent the development of dangerous maritime traffic situations and to provide for the safe and efficient movement of vessel traffic within the VTS area.\n\nThe traffic organization service concerns the operational management of traffic and the forward planning of vessel movements to prevent congestion and dangerous situations, and is particularly relevant in times of high traffic density or when the movement of special transports may affect the flow of other traffic. The service may also include establishing and operating a system of traffic clearances or VTS sailing plans or both in relation to priority of movements, allocation of space, mandatory reporting of movements in the VTS area, routes to be followed, speed limits to be observed or other appropriate measures which are considered necessary by the VTS authority.\n\nA navigational assistance service is a service to assist on-board navigational decision-making and to monitor its effects.\n\nThe navigational assistance service is especially important in difficult navigational or meteorological circumstances or in case of defects or deficiencies. This service is normally rendered at the request of a vessel or by the VTS when deemed necessary.\n\n\n"}
