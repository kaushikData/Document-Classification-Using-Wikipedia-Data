{"id": "1280825", "url": "https://en.wikipedia.org/wiki?curid=1280825", "title": "Alexanderson alternator", "text": "Alexanderson alternator\n\nAn Alexanderson alternator is a rotating machine invented by Ernst Alexanderson in 1904 for the generation of high-frequency alternating current for use as a radio transmitter. It was one of the first devices capable of generating the continuous radio waves needed for transmission of amplitude modulation (sound) by radio. It was used from about 1910 in a few \"superpower\" longwave radiotelegraphy stations to transmit transoceanic message traffic by Morse code to similar stations all over the world.\n\nAlthough obsolete by the early 1920s due to the development of vacuum-tube transmitters, the Alexanderson alternator continued to be used until World War II. It is on the list of IEEE Milestones as a key achievement in electrical engineering.\n\nAfter radio waves were discovered in 1887, the first generation of radio transmitters, the spark gap transmitters, produced strings of \"damped waves\", pulses of radio waves which died out to zero quickly. By the 1890s it was realized that damped waves had disadvantages; their energy was spread over a broad frequency bandwidth so transmitters on different frequencies interfered with each other, and they could not be modulated with an audio signal to transmit sound. Efforts were made to invent transmitters that would produce \"continuous waves\", a sinusoidal alternating current at a single frequency.\n\nIn an 1891 lecture, Frederick Thomas Trouton pointed out that, if an electrical alternator were run at a great enough cycle speed (that is, if it turned fast enough and was built with a large enough number of magnetic poles on its armature) it would generate continuous waves at radio frequency. Starting with Elihu Thomson in 1889, a series of researchers built high frequency alternators, Nikola Tesla (1891, 15 kHz), Salomons and Pyke (1891, 9 kHz), Parsons and Ewing (1892, 14 kHz.), Siemens (5 kHz), B. G. Lamme (1902, 10 kHz), but none was able to reach the frequencies required for radio transmission, above 20 kHz.\n\nIn 1904, Reginald Fessenden contracted with General Electric for an alternator that generated a frequency of 100,000 hertz for continuous wave radio. The alternator was designed by Ernst Alexanderson. The Alexanderson alternator was extensively used for long-wave radio communications by shore stations, but was too large and heavy to be installed on most ships. In 1906 the first 50-kilowatt alternators were delivered. One was to Reginald Fessenden at Brant Rock, Massachusetts, another to John Hays Hammond, Jr. in Gloucester, Massachusetts and another to the American Marconi Company in New Brunswick, New Jersey.\n\nAlexanderson would receive a patent in 1911 for his device. The Alexanderson alternator followed Fessenden's rotary spark-gap transmitter as the second radio transmitter to be modulated to carry the human voice. Until the invention of vacuum-tube (valve) oscillators in 1913 such as the Armstrong oscillator, the Alexanderson alternator was an important high-power radio transmitter, and allowed amplitude modulation radio transmission of the human voice. The last remaining operable Alexanderson alternator is at the VLF transmitter Grimeton in Sweden and was in regular service until 1996. It continues to be operated for a few minutes on Alexanderson Day, which is either the last Sunday in June or first Sunday in July every year.\n\nStarting in 1942 four stations were operated by US Navy: the station at Haiku, Hawaii until 1958, Bolinas until 1946, Marion, and Tuckerton (both until 1948). Two alternators were shipped to Hawaii in 1942, one each from Marion, MA and Bolinas, CA. Haiku received one. The other went to Guam but returned to Haiku after World War 2. Haiku began operation of the first 200 kW alternator in 1943. The second alternator went into operation at Haiku in 1949. Both alternators were sold for salvage in 1969, possibly to Kreger Company of California. The Marion station was transferred in 1949 to the US Air Force and used until 1957 for the transmission of weather forecasts to the arctic as well as for the Basen to Greenland, Labrador, and Iceland. One of the alternators was scrapped in 1961 and another one was handed over to the US office of standard, it now resides in a Smithsonian Institution warehouse. The two machines in Brazil were never used because of organizational problems there. They were returned to Radio Central after 1946.\n\nThe Alexanderson alternator works similarly to an AC electric generator, but generates higher-frequency current, in the very low frequency (VLF) radio frequency range. The rotor has no conductive windings or electrical connections; it consists of a solid disc of high tensile strength magnetic steel, with narrow slots cut in its circumference to create a series of narrow \"teeth\" that function as magnetic poles. The space between the teeth is filled with nonmagnetic material, to give the rotor a smooth surface to decrease aerodynamic drag. The rotor is turned at a high speed by an electric motor.\n\nThe machine operates by variable reluctance (similar to an electric guitar pickup), changing the magnetic flux linking two coils. The periphery of the rotor is embraced by a circular iron stator with a C-shaped cross-section, divided into narrow poles, the same number as the rotor has, carrying two sets of coils. One set of coils is energized with direct current and produces a magnetic field in the air gap in the stator, which passes axially (sideways) through the rotor.\n\nAs the rotor turns, alternately either an iron section of the disk is in the gap between each pair of stator poles, allowing a high magnetic flux to cross the gap, or else a non-magnetic slot is in the stator gap, allowing less magnetic flux to pass. Thus the magnetic flux through the stator varies sinusoidally at a rapid rate. These changes in flux induce a radio-frequency voltage in a second set of coils on the stator.\n\nThe RF collector coils are all interconnected by an output transformer, whose secondary winding is connected to the antenna circuit. Modulation or telegraph keying of the radio frequency energy was done by a magnetic amplifier, which was also used for amplitude modulation and voice transmissions.\n\nThe frequency of the current generated by an Alexanderson alternator in hertz is the product of the number of rotor poles and the revolutions per second. Higher radio frequencies thus require more poles, a higher rotational speed, or both. Alexanderson alternators were used to produce radio waves in the very low frequency (VLF) range, for transcontinental wireless communication. A typical alternator with an output frequency of 100 kHz had 300 poles and rotated at 20,000 revolutions per minute (RPM) (330 revolutions per second). To produce high power, the clearance between the rotor and stator had to be kept to only 1 mm. The manufacture of precision machines rotating at such high speeds presented many new problems, and Alexanderson transmitters were bulky and very expensive.\n\nThe output frequency of the transmitter is proportional to the speed of the rotor. To keep the frequency constant, the speed of the electric motor turning it was controlled with a feedback loop. In one method, a sample of the output signal is applied to a high-Q tuned circuit, whose resonant frequency is slightly above the output frequency. The generator's frequency falls on the \"skirt\" of the LC circuit's impedance curve, where the impedance increases rapidly with frequency. The output of the LC circuit is rectified, and the resulting voltage is compared with a constant reference voltage to produce a feedback signal to control the motor speed. If the output frequency gets too high, the impedance presented by the LC circuit increases, and the amplitude of the RF signal getting through the LC circuit drops. The feedback signal to the motor drops, and the motor slows down. Thus the alternator output frequency is \"locked\" to the tuned circuit resonant frequency.\n\nThe sets were built to operate at wavelengths of 10,500 to 24,000 meters (28.57 to 12.5 KHz) . This was accomplished by three design variables. The alternators were built with\n1220 or 976 or 772 poles . Three gear boxes were available with ratios of 2.675- 2.973 and 3.324 and the 900 RPM driving motor was operated at slips of 4% to 20%, giving speeds of 864 to 720 RPM . Transmitters installed in Europe, operating on 50 cycle power, had a wavelength range of 12, 500 to 28, 800 meters because of the lower speed of the driving motor.\n\nA large Alexanderson alternator might produce 500 kW of output radio-frequency energy and would be water- or oil-cooled. One such machine had 600 pole pairs in the stator winding, and the rotor was driven at 2170 RPM, for an output frequency near 21.7 kHz. To obtain higher frequencies, higher rotor speeds were required, up to 20,000 RPM.\n\nAlong with the arc converter invented in 1903, the Alexanderson alternator was one of the first radio transmitters that generated continuous waves. In contrast, the earlier spark-gap transmitters generated a string of damped waves. These were electrically \"noisy\"; the energy of the transmitter was spread over a wide frequency range, so they interfered with other transmissions and operated inefficiently. With a continuous-wave transmitter, all of the energy was concentrated within narrow frequency band, so for a given output power they could communicate over longer distances. In addition, continuous waves could be modulated with an audio signal to carry sound. The Alexanderson alternator was one of the first transmitters to be used for AM transmission.\n\nThe Alexanderson alternator produced \"purer\" continuous waves than the arc converter, whose nonsinusoidal output generated significant harmonics, so the alternator was preferred for long-distance telegraphy.\n\nBecause of the extremely high rotational speed compared to a conventional alternator, the Alexanderson alternator required continuous maintenance by skilled personnel. Efficient lubrication and oil or water cooling was essential for reliability which was difficult to achieve with the lubricants available at the time. In fact, early editions of the Royal Navy's \"Admiralty Handbook of Wireless Telegraphy\" cover this in considerable detail, mostly as an explanation as to why the navy did not use that particular technology. However, the US Navy did.\n\nOther major problems were that changing the operating frequency was a lengthy and complicated process, and unlike a spark transmitter, the carrier signal could not be switched on and off at will. The latter problem greatly complicated \"listening through\" (that is, stopping the transmission to listen for any answer). There was also the risk that it would allow enemy vessels to detect the presence of the ship.\n\nBecause of the limits of the number of poles and rotational speed of a machine, the Alexanderson alternator is capable of generating transmission frequencies up to around 600kHz in the lower Medium wave band, with Shortwave and higher frequencies being physically impossible.\n\n\n"}
{"id": "27222453", "url": "https://en.wikipedia.org/wiki?curid=27222453", "title": "Alfa Omega", "text": "Alfa Omega\n\nAlfa Omega is a Romanian broadcaster with a satellite channel and an IPTV station. It started as a distributor of programming working with more than 50 stations in both cable and terrestrial networks inside of Romania.\nIt now also has operations in Moldova.\n"}
{"id": "48326624", "url": "https://en.wikipedia.org/wiki?curid=48326624", "title": "All Power Labs", "text": "All Power Labs\n\nAll Power Labs (APL) is a renewable energy company based in Berkeley, California. APL designs and manufactures biomass gasifiers and builds and markets small-scale (15 kW - 150 kW) electrical generators fueled by these gasifiers. In 2013, All Power Labs reached an installed base of 500 machines in approximately 40 countries. As of 2015, APL employed 30 staff, including engineering, manufacturing, management, sales, and technical support staff, on the site of the former Shipyard, an approximately 20,000 sq.ft. facility that includes APL’s offices, R&D, manufacturing and production facilities. In 2015 it established a board of directors to which it added Daniel Kammen and Tom Dinwoodie.\n\nJim Mason and Jessica Hobbs founded APL in 2007 on the site of, and based on, the work of the former Shipyard. The Shipyard was a collaborative art-development space established in 2001 by Mason in a dozen shipping-container workspaces assembled around a small machine shop in West Berkeley. Containers were rented out to a mix of artists, engineers and scientists he selected mostly from among his collaborators on art projects for the Burning Man festival. Unable to get approval for a grid-power connection due to zoning issues, the Shipyard community assembled an off-grid power system combining 2 kW of used photovoltaic panels, a 4000 amp hour surplus telecommunication battery bank, a pair of used diesel generators running on biodiesel made in an on-site biodiesel-reactor facility, and began experimenting with Biomass Gasification as a potential power source.\n\nAPL's initial goal was education and experimentation in open-source alternative energy technologies in an attempt to create a do-it-yourself (DIY) power-hacking culture. Within the first year, the company limited its focus to the open-source development of biomass gasification technology, and began to design and manufacture a range of open-source DIY Gasifier Experimenters Kits (GEK), whose plans and cad files were made available online.\n\nDuring a 24 hour period ending on September 29, 2018, at the APL facility in Berkeley, California, APL along with the Skysource/Skywater Alliance, successfully won the $1.5 million Water XPRIZE by producing over 2,000 liters of water in one day at a final cost of less than 2 cents per liter using only renewable energy. An APL PP30 Biomass Gasifier Genset powered three Skywater 300 atmospheric water generators to extract water from the atmosphere and the additional residual moisture generated by the drying of the woodchips used to fuel the gasifier.\n\nIn 2010, APL began to manufacture an integrated biomass gasifier-genset named the Power Pallet in 10 kW and 20 kW ratings. By the end of 2013 the GEK kits and 10 kW version were abandoned along with the company's open-source ethos with its release of a 20 kW unit using a proprietary gasifier design based on Mason’s patents. In late 2016, the company’s principle product was the PP30 Power Pallet 25 kW biomass genset which included Combined Heat and Power (CHP) and Grid-tied Electrical System features as standard equipment as well as optional accessories. A principal segment of their market is directed at addressing energy poverty in the developing world. In 2015, using a California Energy Commission (CEC) grant intended to incentivize forest-fire remediation in California's Sierra Nevada Mountains, APL continued development of a 150 kW version of a shipping-container-based genset named the Powertainer that was initially built as a 100 kW rated prototype in 2012 with the assistance of a US Department of Energy grant.\n\nBiomass Gasification uses high temperatures in an low-oxygen environment to covert woody feedstocks into a Syngas fuel composed predominantly of flammable hydrogen (H2) and carbon monoxide (CO) gases. When waste biomass is used as the feedstock and the biochar byproduct is sequestered, such as when used as a soil amendment, the operation results in Negative carbon dioxide emission. APL's gasifiers use a variety of lignocellulosic biomass (woody biomass) such as wood chips, nut shells, and other agricultural bi-products as feedstock, at the rate of approximately 1kg/kWh.\n"}
{"id": "27781511", "url": "https://en.wikipedia.org/wiki?curid=27781511", "title": "Antique Airplane Association", "text": "Antique Airplane Association\n\nThe Antique Airplane Association, Inc. (AAA) is the oldest antique airplane association in the world. The AAA formed in August 1953 by Robert L. Taylor, via a classified ad in Flying magazine and a few people with a deep-seated interest in old airplanes, for the purpose to fly, to preserve, to share and to promote the early flying machines.\n\nThe AAA operates from Antique Airfield in Blakesburg, Iowa and serves as the headquarters for its 50 chapter organizations and 7000 members from throughout the United States and 20 foreign countries.\n\nFounder and president of the national Antique Airplane Association as well as current Chairman of the Board of the closely affiliated Airpower Museum Inc., Taylor states on the national club website \"At that time (1953) no other association existed that had a specific interest in antique and classic airplanes. No aviation historical groups had yet been formed. The AAA was organized to \"Keep the Antiques Flying\" and this basic premise has always been our main interest and primary function. We do provide our membership with aviation history and memories of the important parts our own members have played in this fascinating subject.\"\n\nThe AAA of the late 50s was incorporated by Ken Cook, at the time serving as the AAA vice president. During this time Cook published a new version of the \"AAA News\" and would later rename it the \"American Airman\" magazine. In 1961, Cook withdrew his involvement with the AAA.\nIn the 1960s, the remaining parts stock for LeBlond aircraft engines were sold to the AAA. There are a few parts left, but mostly what's left are engine drawings. In 1970 the AAA Board of Directors approved the necessary changes required of the AAA in order to be chartered as a non-profit corporation in the state of Iowa and has operated as such since that time.\n"}
{"id": "9384886", "url": "https://en.wikipedia.org/wiki?curid=9384886", "title": "Burn-in oven", "text": "Burn-in oven\n\nBurn-in ovens are designed for dynamic and static burn-in of integrated circuits and other electronic devices, including laser diodes. Typical sizes are from under ten to over , with air or nitrogen configurations. Operating temperatures can go over , and can use both single and multiple temperature settings.\n\nBurn-in oven applications can be used in numerous different applications such as high-dissipation forward bias, high-temperature reverse bias, dynamic and static burn-in of microprocessors and other semiconductor devices.\n\nBurn-in ovens are considered a type of batch oven. Other types of batch ovens are bench/laboratory, reach-in, walk in/truck in, and clean process.\n\nOne company builds systems designed for burn-in of low power laser diodes up to 1A and high power laser diodes up to 300A.\n"}
{"id": "12912093", "url": "https://en.wikipedia.org/wiki?curid=12912093", "title": "Canadian Agricultural Safety Association", "text": "Canadian Agricultural Safety Association\n\nThe Canadian Agricultural Safety Association was established in 1993 in response to an identified need for a national farm safety networking and coordinating agency to address problems of illness, injuries and accidental death in farmers, their families and agricultural workers. Since then, CASA has worked to improve the health and safety conditions of those who live and work on Canadian farms.\n\nCASA is funded by Agriculture and Agri-Food Canada's Renewal Chapter and has applied for similar support under the federal government's Growing Forward program for 2009–2013.\n\nIn the past, CASA acted as facilitator and enabler. For instance, in 2007-08, CASA extended $637,365 to 20 organizations for 24 farm safety projects, directly reaching more than 376,000 producers across Canada.\n\nIn the future, CASA will operate as a national health and safety initiator and service provider.\n\nCanada's agricultural industry is one of the top three most hazardous industries in which to work. And yet, farmers themselves believe their safety habits are better than fair. Farm Credit Canada recently conducted a national survey to determine farmer's attitudes toward safety. The final report is a self-assessment of how farmers visualize their safety performance. They gave themselves a \"B\".\n\nCASA has dedicated itself to close the gap between what they believe and what is actually occurring.\n\nAt CASA's Safety Summit of October 2008, in Saskatoon, there was consensus that CASA must move forward with the proposed strategic plan. Many of the participants committed, on behalf of their organizations, to support the strategy with time and resources.\n\nContributing to a sector that is competitive and innovative:\nEnabling a sector that contributes to society's priorities:\nBuilding a sector that is proactive in managing risks:\n\nCASA is led by a seven-person Board of Directors elected for three-year terms by the 32 member Council. The Board elects its Executive annually. Visit the CASA/ACSA website for information about the current Board of Directors.\n\nCanadian Agricultural Injury Reporting (CAIR) formerly the Canadian Agricultural Injury Surveillance Program (CAISP) - is an integrated national surveillance project of the Canadian Agricultural Safety Association that guides and informs the national agricultural health and safety agenda.\n\nCAISP's latest national report Agricultural Injuries in Canada for 1990–2005 can be downloaded free.\n\nIt describes the occurrence of fatal agricultural injuries in Canada by age group and mechanism of injury. There were 1,769 agricultural fatalities in Canada from 1990 to 2005. Overall, more than half of the agricultural fatalities were due to four machine-related causes: machine rollovers, machine runovers, machine entanglements and traffic collisions. The top five causes of agricultural fatalities in Canada were machine rollovers (20.5%), machine runovers (18.6%), machine entanglements (8.3%), traffic collisions (7.3%), and being pinned or struck by a machine (7.0%).\n\nTheir recent fifteen-year summary reports on fatal agricultural rollovers, runovers, animal injuries, injuries to older adults and injuries to children can be downloaded here.\n\nUnder the Federal-Provincial-Territorial Framework Agreement on Agricultural and Agri-Food Policy, the Canadian Agricultural Safety Association (CASA) has been recognized as the association to provide a forum for directing and coordinating activities with a national, multi-provincial or provincial scope specifically related to farm safety and rural health issues.\n\nIn 2008-09, CASA approved $283,642.50 for 16 projects under CASHP.\n\nCASA supports the Progressive Agriculture Safety Days in Canada. This educational program has helped children across Canada learn about dangers and potential deadly outcomes of unsafe behaviour on the farm.\n\n\"When an individual or community partners with the Progressive Agriculture Safety Day program, they are provided planning resources to help each Safety Day become a success\" says Susan Reynolds, Executive Director-Programs of the Progressive Agriculture Safety Day. \"Through our program, each lead coordinator is provided training on how to organize a Safety Day, and is offered year-round support.\"\n\nEach safety day is organized locally, allowing communities to meets local priorities in their area. Children rotate among different safety stations, allowing each child the opportunity to participate in interactive and engaging activities.\"\n\nIn 2009, 72 Safety Days are planned for Alberta, Saskatchewan, Manitoba, Ontario, Quebec, Nova Scotia and New Brunswick involving more than 13,000 youth and over 3000 adult volunteers!\n\nFounded by The Progressive Farmer magazine in the U.S. in 1995, the program trains and provides the resources that local communities need to conduct one-day safety programs that are age-appropriate, hands-on, fun, and safe for children. While the basic program reaches children ages of 8 to 13, safety days may also be conducted for 4- to 7-year-olds or even entire families.\n\nThe mission of the Progressive Agriculture Safety Days is to make farm and ranch life safer and healthier for all children through education and training.\n\n"}
{"id": "42969262", "url": "https://en.wikipedia.org/wiki?curid=42969262", "title": "Cerego", "text": "Cerego\n\nCerego is an adaptive learning technology platform based on principles of neuroscience and cognitive science. Cerego’s patented technology uses the scientific method of spaced rehearsal as the basis for memory retention for content available via their website.\n\nCerego has partnered and collaborated with various organizations and institutions including The Bill & Melinda Gates Foundation, EdX, Cengage Learning and Fabien Cousteau’s Mission 31.\n\nIn 2009, Cerego launched a new Facebook app called Smart.fm Brainspeed that scans the information in the profiles of the user's friends and then creates a quiz around their personal information, in order to assess the memory power of the users.\n\nIn 2013, Cerego partnered with Elsevier to provide nursing and healthcare students an adaptive learning solution for their educational content.\n\nIn 2014, Cerego was awarded a grant by the Bill & Melinda Gates Foundation to provide next-generation digital courseware designed to reach more than 1 million low-income students and disadvantaged learners in undergraduate courses by 2018.\n\nPreliminary results from a study conducted at Excelsior College in 2014 indicated that using Cerego can help increase grades when studying math and biology online.\n\n\n"}
{"id": "37701754", "url": "https://en.wikipedia.org/wiki?curid=37701754", "title": "Click Frenzy", "text": "Click Frenzy\n\nClick Frenzy is an Australian online sales initiative inspired by and based on a similar format to the United States shopping event Cyber Monday. The inaugural Click Frenzy event was launched on Tuesday, 20 November 2012 with heavy media and online promotion in the lead up, with organisers arranging sales partnerships with many of Australia's leading retailers, and had given multiple assurances prior to the event that they would be able to support the anticipated heavy traffic of the event. However the event's website crashed within moments of launching at 7pm AEST and was unavailable for most of the night, leading to a backlash from frustrated customers. \n\nDespite the inauspicious start, Click Frenzy events continue to be held in the years following, with the event estimating $189 million in sales and the site receiving 1 million visitors in 2014.\n\nThe inaugural Click Frenzy event was heavily promoted in Australian media including the major television networks and online newspapers. Numerous Australian and international retailers and brands were involved, including Myer, Bing Lee, Microsoft, Toys R Us, Dell, Target and Priceline. Organisers boasted of their preparedness to deal with the expected popularity. \"We're expecting up to 1 million site visits [to clickfrenzy.com.au] and we're prepared for this,\" the spokesperson said. However the site failed almost immediately after the sale period starting. There was a rapid backlash from the Australian public, with trending of the #clickfail hashtag on Twitter and creation of various memes mocking the event.\n\nClick Frenzy co-founder Grant Arnott appeared on Channel 9's Today show the next morning expressing disappointment at the website's initial failure.\n\nCommentators have suggested this incident could further damage the struggling Australian retail sector, and also hurt the credibility of other online product and service providers. Speculation has been raised that the event was a scam to profit from the collection of users' personal information and premiums paid by participating retailers.\n\nSome retailers reportedly paid up to $33,000 for an advertisement at the site, and are asking for refunds. However, other retailers such as Windsor Smith, Booktopia and EzyDVD have been very happy with the increase in sales and traffic that Click Frenzy has driven to their websites. Although the Click Frenzy site itself failed, consumers bypassed the Click Frenzy site and shopped directly with the retailers involved.\n\n"}
{"id": "50354864", "url": "https://en.wikipedia.org/wiki?curid=50354864", "title": "Club Nokia", "text": "Club Nokia\n\nClub Nokia was a mobile internet digital distribution portal, similar to an App Store, operated by Nokia to provide special offers, paid-for ringtones, picture messages and game content directly to members. Following resistance from its mobile operator customers, Nokia partially closed the service and the brand became solely a consumer service and loyalty portal.\n\nClub Nokia was originally launched in 1997 to provide detailed product information and support about Nokia products. In 1999 Club Nokia was developed into an integral multi-channel personalised service accessible by WAP, SMS or the World Wide Web, spawning a new industry for the provision of mobile content. Consumers could join Club Nokia after buying a new Nokia device. To download content, users were required to purchase credits obtained from authorised Nokia dealerships. Content included additional game levels for e.g. \"Space Impact\".The picture messaging service was launched in Finland in December 1999. In 2000, Amazon partnered with Nokia to enable purchasing of books from Amazon's catalogue via Club Nokia with WAP enabled mobile phones.\n\nIn August 2000, Nokia signed a deal with music publisher EMI to provide EMI-owned songs as ringtones, available from the Club Nokia website or by sending an SMS message. By November 2001, over 10 million consumers were subscribed to Club Nokia, and the enterprise was forecast to yield €1 billion in revenue by 2004. However, the EMI deal proved controversial as it placed Nokia in direct competition with the mobile operators' own branded portals (e.g. Vodafone live! or T-Zones), who relied on the booming ringtones market for revenue and were wary of Nokia gaining a mobile content monopoly through Club Nokia as Microsoft had done in computing software. Nokia argued customers used the carriers' mobile data to download content, but network operators remained resistant. As a result, Nokia announced in September 2004 that the service for selling ringtones would close down, never having become the commercial success it was forecast to be, and Club Nokia became solely a customer service, loyalty and news portal. On the back of investments made into Club Nokia, Nokia launched a new service Preminet to its operators, designed to distribute certified Java- and Symbian-based mobile software to make cell-phone applications easier to buy, sell, and distribute.\n\nIn late 2007 the Club Nokia service was rebranded \"My Nokia\". Nokia launched a new direct-to-consumer service in 2006 called Nokia Content Discoverer. The term \"Club Nokia\" was since re-used as the name of a concert venue in Los Angeles, which has now been renamed The Novo by Microsoft.\n\n"}
{"id": "559066", "url": "https://en.wikipedia.org/wiki?curid=559066", "title": "Cobra probe", "text": "Cobra probe\n\nA Cobra probe is a device to measure the pressure and velocity components of a moving fluid. It is a multi-holed pressure probe with rotational axis of the probe shaft coplanar with the measurement plane of the instrument. Because of this geometry, when the instrument is rotated around the shaft's axis, the measurement elements of the probe remain in the same location. The name cobra probe comes from the shape of the probe head which gives it this property.\n\nCobra probes come in three-, four-, and five-hole configurations, the former used for two-dimensional flow measurement, the latter two for three-dimensional flow measurement. In the three-hole kind of instrument, there are two yaw direction tubes which are chamfered and silver soldered symmetrically on the two sides of a pitot tube. It is otherwise similar to the other kinds of yawmeters. In the four- and five-hole configurations, the central pitot tube is surrounded by three or four chamfered tubes, respectively.\n"}
{"id": "31743423", "url": "https://en.wikipedia.org/wiki?curid=31743423", "title": "Defense industrial base", "text": "Defense industrial base\n\nThe term defense industrial base (or DIB), also known as the defense industrial and technological base, is used in political science to refer to a government's industrial assets that are of direct or indirect importance for the production of equipment for a country's armed forces. It is loosely associated with realism, which views the state as the preponderant guarantor of security, and frequently features as an element of grand strategy and defense policy, as well as diplomacy.\n\nA commonly cited example of a defense industrial base is that of the United States, where, given the onset of the Cold War accompanied by the outbreak of the Korean War, the maintenance \"of a peacetime defense industry of significant proportions was an unprecedented event.\"\n\nResearchers and public figures critical of close ties among legislators, militaries and the defense industrial base due to a government's monopoly on demand for products of the latter employ the concept of the military–industrial complex to critique these connections. Early studies of interest group representation in the US referred to these ties as exemplary of the iron triangle phenomenon.\n\n\n"}
{"id": "17197823", "url": "https://en.wikipedia.org/wiki?curid=17197823", "title": "Ecuador composting method", "text": "Ecuador composting method\n\nThe Ecuador composting method is a common composting practice in the lowlands of Ecuador and Peru. The compost pile is embedded on the tree trunk or banana stalks, with a pale erected in the middle. Organic matter is placed in layers on the trunks or stalks, each layer being covered by mud, or inlaid via different types of organic matter. When the pile is about 1.2 meters high, it is watered and covered by big leaves. After some time, when the compost pile settles down, the central pale is removed for aeration. This composting method is typically done in a small-scale, by indigenous villagers.\n"}
{"id": "344136", "url": "https://en.wikipedia.org/wiki?curid=344136", "title": "Electrical termination", "text": "Electrical termination\n\nIn electronics, electrical termination is the practice of ending a transmission line with a device that matches the characteristic impedance of the line. This is intended to prevent signals from reflecting off the end of the transmission line. Reflections at the ends of unterminated transmission lines cause distortion which can produce ambiguous digital signal levels and mis-operation of digital systems. Reflections in analog signal systems cause such effects as video ghosting, or power loss in radio transmitter transmission lines.\n\nSignal termination often requires the installation of a terminator at the beginning and end of a wire or cable to prevent an RF signal from being reflected back from each end, causing interference, or power loss. The terminator is usually placed at the end of a transmission line or daisy chain bus (such as in SCSI), and is designed to match the AC impedance of the cable and hence minimize signal reflections, and power losses. Less commonly, a terminator is also placed at the driving end of the wire or cable, if not already part of the signal-generating equipment.\n\nRadio frequency currents tend to reflect from discontinuities in the cable such as connectors and joints, and travel back down the cable toward the source causing interference as primary reflections. Secondary reflections can also occur at the cable start, allowing interference to persist as repeated echoes of old data. These reflections also act as bottlenecks, preventing the signal power from reaching the destination.\n\nTransmission line cables require impedance matching to carry electromagnetic signals with minimal reflections and power losses. The distinguishing feature of most transmission line cables is that they have uniform cross sectional dimensions along their length, giving them a uniform electrical characteristic impedance. Signal terminators are designed to specifically match the characteristic impedances at both cable ends. For many systems, the terminator is a resistor, with a value chosen to match the characteristic impedance of the transmission line, and chosen to have acceptably low parasitic inductance and capacitance at the frequencies relevant to the system. Examples include 75 ohm resistors often used to terminate 75-ohm video transmission coaxial cables.\n\nTypes of transmission line cables include balanced line such as ladder line, and twisted pairs (Cat-6 Ethernet, Parallel SCSI, ADSL, Landline Phone, XLR audio, USB, Firewire, Serial); and unbalanced lines such as coaxial cable (Radio antenna, CATV, 10BASE5 Ethernet).\n\nPassive terminators often consist of a single resistor, however significantly reactive loads may require other passive components such as inductors, capacitors, or transformers.\n\nActive terminators consist of a voltage regulator that keeps the voltage used for the terminating resistor(s) at a constant level.\n\nForced perfect termination (FPT) can be used on single ended buses where diodes remove over and undershoot conditions. The signal is locked between two actively regulated voltage levels, which results in superior performance over a standard active terminator.\n\nAll parallel SCSI units use terminators. SCSI is primarily used for storage and backup. An active terminator is a type of single ended SCSI terminator with a built-in voltage regulator to compensate for variations in terminator power.\n\nController area network, commonly known as CAN Bus, uses terminators consisting of a 120 ohm resistor.\n\nDummy loads are commonly used in HF to EHF frequency circuits.\n\n10BASE2 networks absolutely must have proper termination with a 50 ohm BNC terminator. If the bus network is not properly terminated, too much power will be reflected, causing all of the computers on the bus to lose network connectivity.\n\nA terminating resistor for a television coaxial cable is often in the form of a cap, threaded to screw onto an F connector. Antenna cables are sometimes used for internet connections; however RG-6 should not be used for 10BASE2 (which should use RG-58) as the impedance mismatch can cause phasing problems with the baseband signal.\n\nThe Digital Equipment Corporation minicomputer Unibus systems used terminator cards with 178 Ω pullup resistors on the multi-drop address and data lines, and 383 Ω on the single-drop signal lines.\n\nTerminating resistors values of 78.7 ohms 2 watt 1% are used on the MIL-STD-1553 bus. At the two ends of the bus, resistors connect between the positive (high) and negative (low) signal wires either in internally terminated bus couplers or external connectorized terminators.\n\nThe MIL-STD-1553B bus must be terminated at both ends to minimize the effects of signal reflections that can cause waveform distortion and disruption or intermittent communications failures.\n\nOptionally, a high-impedance terminator (1000 to 3000 ohms) may be used in vehicle applications to simulate a future load from an unspecified device.\n\nConnectorized terminators are available with or without safety chains.\n\n\n"}
{"id": "31002280", "url": "https://en.wikipedia.org/wiki?curid=31002280", "title": "Electrochemical energy conversion", "text": "Electrochemical energy conversion\n\nElectrochemical energy conversion is a field of energy technology concerned with electrochemical methods of energy conversion including fuel cells and photoelectrochemical. This field of technology also includes electrical storage devices like batteries and supercapacitors. It is increasingly important in context of automotive propulsion systems. There has been the creation of more powerful, longer running batteries allowing longer run times for electric vehicles. These systems would include the energy conversion fuel cells and photoelectrochemical mentioned above.\n\n\n"}
{"id": "7614026", "url": "https://en.wikipedia.org/wiki?curid=7614026", "title": "Electropalatography", "text": "Electropalatography\n\nElectropalatography (EPG) is a technique used to monitor contacts between the tongue and hard palate, particularly during articulation and speech.\n\nA custom-made artificial palate is moulded to fit against a speaker's hard palate. The artificial palate contains electrodes exposed to the lingual surface. When contact occurs between the tongue surface and any of the electrodes, particularly between the lateral margins of the tongue and the borders of the hard palate, electronic signals are sent to an external processing unit. EPG provides dynamic real-time visual feedback of the location and timing of tongue contacts with the hard palate.\n\nThis procedure can record details of tongue activity during speech. It can provide direct articulatory information that children can use in therapy to monitor and improve their articulation patterns. Visual feedback is very important in the success of treating deaf children.\n\nElectropalatography was originally conceptualized and developed as a tool for phonetics research to improve upon traditional palatography methods. Both military and academic language researchers used early electropalatography tools to obtain accurate information regarding tongue-to-palate contact in a number of foreign languages.\n\nEarly EPG devices used direct current electricity to power the sensors, which were activated by moisture sensors on mouthpieces. Mouthpieces (electropalates) originally closely resembled modern dental impression plates. Mouthpieces became more customized over time, which allowed for more accurate research. Fig. 1 shows a typical electropalate from the Reading system.\n\nEPG added significant insight into academic understanding of articulatory phonetics. In the 1960s and 1970s a number of independent individuals and companies recognized EPG's potential for pedagogical and therapeutic applications. Despite the multiple attempts to reverse engineer EPG tools for speech therapy, most companies failed to commercialize EPG effectively. EPG tools remain fairly expensive tools for speech therapy and phonetics research, though the information they provide are difficult to obtain using other methods of visual feedback of articulation.\n\nAlthough much of the development of EPG has been dedicated to clinical applications, it has been used in a number of laboratory phonetic investigations. Stone (1997) lists three main areas of research:\n\nWhen electropalatography is used for speech research, the data from tongue-palate contact is sampled by the controlling computer at up to 100 frames per second, and can then be printed out. An example of the printout can be seen in Fig.2, where the sequence runs from top to bottom, and where the 'O' symbol indicates contact and '.' indicates no contact. The utterance shown is 'catkin' /kæt.kɪn/; the sample numbered 344 shows when the /t/ closure is complete, and at frame 350 there is a complete velar closure. The alveolar closure is released at 351. The articulatory overlap (which is inaudible) is thus clearly shown.\nIndividual frames of EPG contact data may be used to illustrate descriptions of consonant articulations, and this is done by Cruttenden for all the English (RP) consonants. In some research, multiple repetitions may be summed to produce graphical representations of tongue-palate contact in a way that minimizes effects of random variation on single tokens. This was done by Farnetani in studies of Italian and French coarticulation.\n\nThree primary manufacturers of EPG tools exist: CompleteSpeech in the United States, Articulate Technologies, and Rose Medical Solutions in Great Britain. Completespeech is a private company that specializes in speech therapy oriented EPG tools, branded as The SmartPalate System. The SmartPalate System uses a standard size sensor sheet with 126 sensors that is fitted to individual mouthpiece molds. Articulate technologies provides both speech therapy and research oriented EPG mouthpieces, branded as The Reading Palate and The Articulate Palate. Articulate technologies EPG sensors are places by hand for individual users' mouthpieces. The Rose Medical system, branded as LinguaGraph, also uses the Reading Palate.\n\nElectropalatography has been studied in a variety of populations, including children with cleft palate, children with Down syndrome, children who are deaf, children with cochlear implants, children with cerebral palsy and adults with Parkinson's disease. Therapy has proved to be successful in tested populations. Longitudinal studies with large sample sizes are needed to determine the long-term success of therapy.\n"}
{"id": "13956770", "url": "https://en.wikipedia.org/wiki?curid=13956770", "title": "FAC-System", "text": "FAC-System\n\nFAC-System is a mechanical construction set, invented by Swedish artist Mark Sylwan in 1952. It uses mainly metallic parts, and extensively uses steel rod. The company continued in business , supplying kits stated to give industry a basic construction system which gives the same freedom in 3D design as is available from the drawing board and CAD/CAM. Kits are suitable, and priced as, for laboratory work and teaching.\n\n"}
{"id": "38404986", "url": "https://en.wikipedia.org/wiki?curid=38404986", "title": "Fleksy", "text": "Fleksy\n\nFleksy is a third party virtual keyboard and input method for mobile devices, which is private and improves traditional tap-typing input speed and accuracy through enhanced auto-correction and gesture control. It uses error-correcting algorithms that analyze the region where the user touches the keyboard and feeds this through a language model, which calculates and identifies the intended word. Swiping gestures are used to control common functions, such as space, delete, and word correction. Blind and visually impaired users have utilized Fleksy for eyes-free typing through muscle memory. Available on Android and iOS, it was first commercially available on the iPhone as a download from Apple's App Store in July 2012, and is available in more than 40 languages.\n\nWith ‘on device profiling’ capable of mapping behaviours across any app, users' data is anonymous and remains private. \n\nOfficially the world’s fastest keyboard by the Guinness Book of Records , Fleksy offers instant access to content while typing, such as restaurants, music, videos and more. \n\nFleksy was developed by Fleksy Inc., a company founded in 2011. On June 15, 2016, Fleksy announced that the team had moved on to Pinterest. Since July 2017, the product is managed and developed by Thingthing, a competitor keyboard .\n\nFleksy’s auto-correct algorithm functions by combining analysis of user typing patterns and linguistic context. Analysis of tap locations (rather than letters selected) affords it the ability to remain tolerant of drifting errors and allows the user to type on an invisible keyboard or even off the keyboard in some instances. As a result, Fleksy has been embraced by the visually-impaired community. Most notably, the software has been considered for the \"Story of the Year\" of the Technology Year in Review for 2012 by the American Foundation for the Blind.\n\nThere is evidence that the software could potentially allow sighted people to blind-type on a touchscreen. Quentin Stafford-Fraser said on his website: \"I found I could type whole sentences immediately, without looking at the keyboard\".\n\nFleksy not only offers next word predictions, as found in competitors such as Swype, SwiftKey and other keyboards, but also next-service predictions, which is unique in the keyboard space.\n\nFleksy also utilizes a gesture-based interface that can be used for some common functions, such as deleting a word, space, backspace, and choosing a word correction.\n\nFleksy is available on Android and iOS. Its latest update made it available for iOS 10 and all Android versions. \n\nFleksy is available on certain platforms in 40 languages and QWERTY, AZERTY, QWERTZ, DVORAK, and Colemak layouts.\n\nFleksy has received a number of awards since its release in July 2012:\n"}
{"id": "27963614", "url": "https://en.wikipedia.org/wiki?curid=27963614", "title": "Gateway Touch Pad", "text": "Gateway Touch Pad\n\nThe Gateway Touch Pad was an Internet appliance released on November 10, 2000 at a price of USD599.\n\nThe Touch Pad was built with a chassis (containing speakers) around a 10-inch LCD display with a touch screen for input and controlling basic functions. The CPU was the Transmeta Crusoe.\n\nThe operating system was Mobile Linux and software included the Instant AOL GUI and a version of Netscape's browser using the Gecko engine.\n\nThe unit was designed to sit on a table, kitchen counter, or desk and intended to be used solely for access to the Internet or email. It was sold bundled with AOL Internet access.\n\nDue to slow sales it was removed from the market in October 2001.\n\n"}
{"id": "7221768", "url": "https://en.wikipedia.org/wiki?curid=7221768", "title": "Gigapan", "text": "Gigapan\n\nGigaPan Systems is a global, privately held technology company that provides hardware, software, and services to create and share high-resolution, interactive gigapixel panoramic images. The company is headquartered in Portland, Oregon.\n\nGigaPan Systems was founded in 2008 as a collaborative project between Carnegie Mellon University and NASA’s Ames Research Center with support from Google. The original GigaPan robotic hardware and related software were devised for NASA's Mars Spirit and Opportunity Rovers, to capture high-definition panoramas of Mars. The development team was led by Randy Sargent, a senior systems scientist at Carnegie Mellon West and the NASA Ames Research Center in Moffett Field, Calif., and Illah Nourbakhsh, associate professor of robotics at Carnegie Mellon in Pittsburgh.\n\nThe project has since grown into an independent company offering solutions for capturing gigapixel images.\n\nGigaPan Systems combines high-definition images and panoramas with sharing, tagging and zooming capabilities. The company offers a solution for shooting, viewing and exploring high-resolution gigapixel images in a single system. The GigaPan system includes:\n\n\n\n"}
{"id": "56593335", "url": "https://en.wikipedia.org/wiki?curid=56593335", "title": "Handling course", "text": "Handling course\n\nThe handling course is a section of vehicle industry proving grounds which provides the possibility to examine vehicle behaviour, vehicle manoeuvrability, and technical settings under controlled no-traffic circumstances. This track, which typically forms a loop, is often compared to smaller racecourses; it is, however strictly designed for testing. The layout of its environment imitates motorway circumstances.\n\nIn most cases handling courses consist of two parts: low and high speed courses, both enable the same types of tests, only at differing speeds. Handling courses may also be categorised either as wet or dry courses according to the availability of the option for track wetting.\n\nThe alternative routes of the handling courses provide diverse track options for the tests. As the course sections have varying radius of curvature, handling courses are suitable for testing all vehicles types. Several types of specially paved shorter sections may also belong to handling courses depending on the given proving ground. These may be curved, straight or even wavy asphalt track sections, furthermore straight or curving sections with varying sideway slopes. Occasionally even degraded surfaces requiring maintenance are built deliberately in order to increase the number of options; furthermore sections suitable for conducting aquaplaning tests may also be inserted. The option of wetting of the latter sections is usually provided by using nozzles. The safety of testing is ensured b the crash spaces designed around the curves.\n\nVehicle behaviour or manoeuvrability refers to the testing of the connection between steering and the running gear, the different assistance appliances (ESP, ABS, ASR), and other systems influencing the dynamics of vehicles. As testing options are unlimited, handling courses are suitable for all situations related to the dynamics of modern vehicles.\n\nCurrently most proving grounds in Europe feature handling courses. The most well-known examples are the Boxberg Proving Ground constructed by Bosch (Germany) and the Vaitoudden Winter Test Centre in Sweden. A test track featuring a handling course is currently under construction in Zalaegerszeg, Hungary as well.\n\nThe handling course at the Boxberg Proving Ground consists of two parts. One part is designed for higher speeds suitable for testing all vehicles types thanks to its radius of curvature varying between 48 m and 101 m. The other part is designed with radius of curvature varying between 15 m and 115 m, and has inclinations which increase its difficulty. Several sections of this part are wettable, and it also has a circular surface for drifting. It is not suitable for testing heavy-duty vehicles and buses, but it has a track unit imitating a mountain tunnel, which is suitable for simulating windy circumstances.\n\nThe handling course at the Vaitoudden Winter Test Centre may be divided into two parts, one with snowy surface, the other with frozen. The total length of the snowy surface is 2470 m, its width is approx. 5 m, it contains 13 curves, and due to the snow its grip is medium (μ <0,3). The frozen surface is also divided into two sections. The total length of the first section is approx. 1850 m, its width is approx. 5 m, it contains 8 curves, and the grip of the ice is medium (μ <0,3). The second section is approx. 3000 m long and has the same width as the first section, it contains 30 curves and its grip is also the same as that of the first.\n\nThe handling course at the Papenburg Test Centre is a small scale replica of the racecourse at Hockenheim with a length of 2.6 km and a width of 10 m, and may be divided into two independent units.\n\nThe handling course will be a basic component of the test track currently under construction near Zalaegerszeg. The external handling course designed for high speeds will have a total length of 2032 m and a width of 12 m. Its straight sections and curve radius enable reaching a speed of 120 km/h. The 1334 m long and 6 m wide internal handling course will be designed for the same tests at lower speeds of maximum 60 km/h.\n\nFurthermore the handling course will contain several specially paved section inserts, such as pothole and eroded sections, basalt pavement, straight, curved and wavy pavements, hilly road sections and a section with a 5% sideway slope. A basalt pavement disc will also be built suitable for testing drifting and skidding, as well as a section insert which enables the simulation of sliding on a wet road.\n"}
{"id": "1125423", "url": "https://en.wikipedia.org/wiki?curid=1125423", "title": "Hip replacement", "text": "Hip replacement\n\nHip replacement is a surgical procedure in which the hip joint is replaced by a prosthetic implant, that is, a hip prosthesis. Hip replacement surgery can be performed as a total replacement or a hemi (half) replacement. Such joint replacement orthopaedic surgery is generally conducted to relieve arthritis pain or in some hip fractures. A total hip replacement (total hip arthroplasty) consists of replacing both the acetabulum and the femoral head while hemiarthroplasty generally only replaces the femoral head. Hip replacement is currently one of the most common orthopaedic operations, though patient satisfaction short- and long-term varies widely. The average cost of a total hip replacement in 2012 was $40,364 in the United States, and about $7,700 to $12,000 in most European countries.\n\nTotal hip replacement is most commonly used to treat joint failure caused by osteoarthritis. Other indications include rheumatoid arthritis, avascular necrosis, traumatic arthritis, protrusio acetabuli, certain hip fractures, benign and malignant bone tumors, arthritis associated with Paget's disease, ankylosing spondylitis and juvenile rheumatoid arthritis. The aims of the procedure are pain relief and improvement in hip function. Hip replacement is usually considered only after other therapies, such as physical therapy and pain medications, have failed.\n\nThe modern artificial joint owes much to the 1962 work of Sir John Charnley at Wrightington Hospital. His work in the field of tribology resulted in a design that almost completely replaced the other designs by the 1970s. Charnley's design consisted of three parts:\n\nThe replacement joint, which was known as the Low Friction Arthroplasty, was lubricated with synovial fluid. The small femoral head () was chosen for Charnley's belief that it would have lower friction against the acetabular component and thus wear out the acetabulum more slowly. Unfortunately, the smaller head dislocated more easily. Alternative designs with larger heads such as the Mueller prosthesis were proposed. Stability was improved, but acetabular wear and subsequent failure rates were increased with these designs. The Teflon acetabular components of Charnley's early designs failed within a year or two of implantation. This prompted a search for a more suitable material. A German salesman showed a polyethylene gear sample to Charnley's machinist, sparking the idea to use this material for the acetabular component. The ultra high molecular weight polyethylene or UHMWPE acetabular component was introduced in 1962. Charnley's other major contribution was to use polymethylmethacrylate (PMMA) bone cement to attach the two components to the bone. For over two decades, the Charnley Low Friction Arthroplasty, and derivative designs were the most used systems in the world. It formed the basis for all modern hip implants.\n\nThe Exeter hip stem was developed in the United Kingdom during the same time as the Charnley device. Its development occurred following a collaboration between Orthopaedic Surgeon Robin Ling and University of Exeter engineer Clive Lee and it was first implanted at the Princess Elizabeth Orthopaedic Hospital in Exeter in 1970. The Exeter Hip is a cemented device, but with a slightly different stem geometry. Both designs have shown excellent long-term durability when properly placed and are still widely used in slightly modified versions.\n\nEarly implant designs had the potential to loosen from their attachment to the bones, typically becoming painful ten to twelve years after placement. In addition, erosion of the bone around the implant was seen on x-rays. Initially, surgeons believed this was caused by an abnormal reaction to the cement holding the implant in place. That belief prompted a search for an alternative method to attach the implants. The Austin Moore device had a small hole in the stem into which bone graft was placed before implanting the stem. It was hoped bone would then grow through the window over time and hold the stem in position. Success was unpredictable and the fixation not very robust. In the early 1980s, surgeons in the United States applied a coating of small beads to the Austin Moore device and implanted it without cement. The beads were constructed so that gaps between beads matched the size of the pores in native bone. Over time, bone cells from the patient would grow into these spaces and fix the stem in position. The stem was modified slightly to fit more tightly into the femoral canal, resulting in the Anatomic Medullary Locking (AML) stem design. With time, other forms of stem surface treatment and stem geometry have been developed and improved.\n\nInitial hip designs were made of a one-piece femoral component and a one-piece acetabular component. Current designs have a femoral stem and separate head piece. Using an independent head allows the surgeon to adjust leg length (some heads seat more or less onto the stem) and to select from various materials from which the head is formed. A modern acetabulum component is also made up of two parts: a metal shell with a coating for bone attachment and a separate liner. First the shell is placed. Its position can be adjusted, unlike the original cemented cup design which are fixed in place once the cement sets. When proper positioning of the metal shell is obtained, the surgeon may select a liner made from various materials.\n\nTo combat loosening caused by polyethylene wear debris, hip manufacturers developed improved and novel materials for the acetabular liners. Ceramic heads mated with regular polyethylene liners or a ceramic liner were the first significant alternative. Metal liners to mate with a metal head were also developed. At the same time these designs were being developed, the problems that caused polyethylene wear were determined and manufacturing of this material improved. Highly crosslinked UHMWPE was introduced in the late 1990s. The most recent data comparing the various bearing surfaces has shown no clinically significant differences in their performance. Potential early problems with each material are discussed below. Performance data after 20 or 30 years may be needed to demonstrate significant differences in the devices. All newer materials allow use of larger diameter femoral heads. Use of larger heads significantly decreases the chance of the hip dislocating, which remains the greatest complication of the surgery.\n\nWhen currently available implants are used, cemented stems tend to have a better longevity than uncemented stems. No significant difference is observed in the clinical performance of the various methods of surface treatment of uncemented devices. Uncemented stems are selected for patients with good quality bone that can resist the forces needed to drive the stem in tightly. Cemented devices are typically selected for patients with poor quality bone who are at risk of fracture during stem insertion. Cemented stems are less expensive due to lower manufacturing cost, but require good surgical technique to place them correctly. Uncemented stems can cause pain with activity in up to 20% of patients during the first year after placement as the bone adapts to the device. This is rarely seen with cemented stems.\n\nOnce an uncommon operation reserved for frail patients with a limited life expectancy, hip replacement is now common, even among active athletes including race car drivers Bobby Labonte and Dale Jarrett, and the 8-time Major-winning American golfer Tom Watson, who shot a 67 in the opening round of the Masters Tournament in the year following his operation.\n\nThere are several incisions, defined by their relation to the gluteus medius. The approaches are posterior (Moore), lateral (Hardinge or Liverpool), antero-lateral (Watson-Jones), anterior (Smith-Petersen) and greater trochanter osteotomy. There is no compelling evidence in the literature for any particular approach, but consensus of professional opinion favours either modified anterolateral (Watson-Jones) or posterior approach.\n\nThe \"posterior (Moore or Southern) approach\" accesses the joint and capsule through the back, taking piriformis muscle and the short external rotators of the femur. This approach gives excellent access to the acetabulum and femur and preserves the hip abductors and thus minimizes the risk of abductor dysfunction post operatively. It has the advantage of becoming a more extensile approach if needed. Critics cite a higher dislocation rate, although repair of the capsule, piriformis and the short external rotators along with use of modern large diameter head balls reduces this risk.\n\nThe \"lateral approach\" is also commonly used for hip replacement. The approach requires elevation of the hip abductors (gluteus medius and gluteus minimus) to access the joint. The abductors may be lifted up by osteotomy of the greater trochanter and reapplying it afterwards using wires (as per Charnley), or may be divided at their tendinous portion, or through the functional tendon (as per Hardinge) and repaired using sutures. Although this approach has a lower dislocation risk than the posterior approach, critics note that occasionally the abductor muscles do not heal back on, leading to pain and weakness which is often very difficult to treat.\n\nThe \"anterolateral approach\" develops the interval between the tensor fasciae latae and the gluteus medius. The Gluteus medius, gluteus minimus and hip capsule are detached from the anterior (front) for the greater trochanter and femoral neck and then repaired with heavy suture after the replacement of the joint.\n\nThe \"anterior approach\" uses an interval between the sartorius muscle and tensor fasciae latae. Dr. Joel Matta and Dr. Bert Thomas have adapted this approach, which was commonly used for pelvic fracture repair surgery, for use when performing hip replacement. When used with older hip implant systems that had a small diameter head, dislocation rates were reduced compared to surgery performed through a posterior approach. With modern implant designs, dislocation rates are similar between the anterior and posterior approaches. The anterior approach has been shown in studies to variably improve early functional recovery, with possible complications of femoral component loosening and early revision compared to other approaches\n\nThe dual incision approach and other minimally invasive surgery seeks to reduce soft tissue damage through reducing the size of the incision. However, component positioning accuracy and visualization of the bone structures can be significantly impaired as the approaches get smaller. This can result in unintended fractures and soft tissue injury. The majority of current orthopedic surgeons use a \"minimally invasive\" approach compared to traditional approaches which were quite large comparatively.\n\nComputer-assisted surgery and robotic surgery techniques are also available to guide the surgeon to provide enhanced accuracy. Several commercial CAS and robotic systems are available for use worldwide. Improved patient outcomes and reduced complications have not been demonstrated when these systems are used when compared to standard techniques.\n\nThe prosthetic implant used in hip replacement consists of three parts: the acetabular cup, the femoral component, and the articular interface. Options exist for different people and indications. The evidence for a number of newer devices is not very good, including: ceramic-on-ceramic bearings, modular femoral necks, and uncemented monoblock cups. Correct selection of the prosthesis is important.\n\nThe acetabular cup is the component which is placed into the acetabulum (hip socket). Cartilage and bone are removed from the acetabulum and the acetabular cup is attached using friction or cement. Some acetabular cups are one piece, while others are modular. One-piece (monobloc) shells are either UHMWPE (ultra-high-molecular-weight polyethylene) or metal, they have their articular surface machined on the inside surface of the cup and do not rely on a locking mechanism to hold a liner in place. A monobloc polyethylene cup is cemented in place while a metal cup is held in place by a metal coating on the outside of the cup. Modular cups consist of two pieces, a shell and liner. The shell is made of metal; the outside has a porous coating while the inside contains a locking mechanism designed to accept a liner. Two types of porous coating used to form a friction fit are sintered beads and a foam metal design to mimic the trabeculae of cancellous bone and initial stability is influenced by under-reaming and insertion force. Permanent fixation is achieved as bone grows onto or into the porous coating. Screws can be used to lag the shell to the bone providing even more fixation. Polyethylene liners are placed into the shell and connected by a rim locking mechanism; ceramic and metal liners are attached with a Morse taper.\n\nThe femoral component is the component that fits in the femur (thigh bone). Bone is removed and the femur is shaped to accept the femoral stem with attached prosthetic femoral head (ball). There are two types of fixation: cemented and uncemented. Cemented stems use acrylic bone cement to form a mantle between the stem and to the bone. Uncemented stems use friction, shape and surface coatings to stimulate bone to remodel and bond to the implant. Stems are made of multiple materials (titanium, cobalt chromium, stainless steel, and polymer composites) and they can be monolithic or modular. Modular components consist of different head dimensions and/or modular neck orientations; these attach via a taper similar to a Morse taper. These options allow for variability in leg length, offset and version. Femoral heads are made of metal or ceramic material. Metal heads, made of cobalt chromium for hardness, are machined to size and then polished to reduce wear of the socket liner. Ceramic heads are more smooth than polished metal heads, have a lower coefficient of friction than a cobalt chrome head, and in theory will wear down the socket liner more slowly. As of early 2011, follow-up studies in patients have not demonstrated significant reductions in wear rates between the various types of femoral heads on the market. Ceramic implants are more brittle and may break after being implanted.\n\nThe articular interface is not part of either implant, rather it is the area between the acetabular cup and femoral component. The articular interface of the hip is a simple ball and socket joint. Size, material properties and machining tolerances at the articular interface can be selected based on patient demand to optimise implant function and longevity whilst mitigating associated risks. The interface size is measured by the outside diameter of the head or the inside diameter of the socket. Common sizes of femoral heads are , and . While was common in the first modern prostheses, now even larger sizes are available from 38 to over 54 mm. Larger-diameter heads lead to increased stability and range of motion whilst lowering the risk of dislocation. At the same time they are also subject to higher stresses such as friction and inertia. Different combinations of materials have different physical properties which can be coupled to reduce the amount of wear debris generated by friction. Typical pairings of materials include metal on polyethylene (MOP), metal on crosslinked polyethylene (MOXP), ceramic on ceramic (COC), ceramic on crosslinked polyethylene (COXP) and metal on metal (MOM). Each combination has different advantages and disadvantages.\n\nPost-operative projectional radiography is routinely performed to ensure proper configuration of hip prostheses.\n\nThe direction of the acetabular cup influences the range of motion of the leg, and also affects the risk of dislocation. For this purpose, the \"acetabular inclination\" and the \"acetabular anteversion\" are measurements of cup angulation in the coronal plane and the sagittal plane, respectively.\n\nRisks and complications in hip replacement are similar to those associated with all joint replacements. They can include infection, dislocation, limb length inequality, loosening, impingement, osteolysis, metal sensitivity, nerve palsy, chronic pain and death. Weight loss surgery before a hip replacement does not appear to change outcomes.\n\nInfection is one of the most common causes for revision of a total hip replacement, along with loosening and dislocation. The incidence of infection in primary hip replacement is around 1% or less in the United States. Risk factors for infection include obesity, diabetes, smoking, immunosuppressive medications or diseases, and history of infection.\n\nModern diagnosis of infection around a total knee replacement is based on the Musculoskeletal Infection Society (MSIS) criteria. They are:\n\n1.There is a sinus tract communicating with the prosthesis; or\n2. A pathogen is isolated by culture from at least two separate tissue or fluid samples obtained from the affected prosthetic joint; \nor\n\nFour of the following six criteria exist:\n\n1.Elevated serum erythrocyte sedimentation rate (ESR>30mm/hr) and serum C-reactive protein (CRP>10 mg/L) concentration,\n\n2.Elevated synovial leukocyte count,\n\n3.Elevated synovial neutrophil percentage (PMN%),\n\n4.Presence of purulence in the affected joint,\n\n5.Isolation of a microorganism in one culture of periprosthetic tissue or fluid, or\n\n6.Greater than five neutrophils per high-power field in five high-power fields observed from histologic analysis of periprosthetic tissue at ×400 magnification.\n\nNone of the above laboratory tests has 100% sensitivity or specificity for diagnosing infection. Specificity improves when the tests are performed in patients in whom clinical suspicion exists. ESR and CRP remain good 1st line tests for screening (high sensitivity, low specificity). Aspiration of the joint remains the test with the highest specificity for confirming infection.\n\nDislocation is the most common complication of hip replacement surgery. The most common causes vary by the duration since the surgery.\n\nHip prosthesis dislocation mostly occurs in the first 3 months after insertion, mainly because of incomplete scar formation and relaxed soft tissues. It takes eight to twelve weeks for the soft tissues injured or cut during surgery to heal. During this period, the hip ball can come out of the socket. The chance of this is diminished if less tissue is cut, if the tissue cut is repaired and if large diameter head balls are used.\n\nDislocations occurring between 3 months and 5 years after insertion usually occur due to malposition of the components, or dysfunction of nearby muscles.\n\nRisk factors of late dislocation (after 5 years) mainly include:\n\nSurgeons who perform more of the operations each year tend to have fewer patients dislocate. Doing the surgery from an anterior approach seems to lower dislocation rates when small diameter heads are used, but the benefit has not been shown when compared to modern posterior incisions with the use of larger diameter heads. The use of larger diameter head size does in it self decrease the risk of dislocation, even though this correlation is only found in head sizes up to 28 mm, thereafter no additional decrease in dislocation rate is found. Patients can decrease the risk further by keeping the leg out of certain positions during the first few months after surgery.\n\nMost adults prior to a hip replacement have a limb length inequality of 0–2 cm which they were born with and which causes no clinical deficits. It is common for patients to feel a limb length inequality after total hip replacement. Sometimes the leg seems long immediately after surgery when in fact both are equal length. An arthritic hip can develop contractures that make the leg behave as if it is short. When these are relieved with replacement surgery and normal motion and function are restored, the body feels that the limb is now longer than it was. This feeling usually subsides by 6 months after surgery as the body adjusts to the new hip joint. The cause of this feeling is variable, and usually related to abductor muscle weakness, pelvic obliquity, and minor lengthening of the hip during surgery (<1 cm) to achieve stability and restore the joint to pre-arthritic mechanics. If the limb length difference remains bothersome to the patient more than 6 months after surgery, a shoe lift can be used. Only in extreme cases is surgery required for correction.\n\nBones with internal fixation devices in situ are at risk of periprosthetic fractures at the end of the implant, an area of relative mechanical stress. Post-operative femoral fractures are graded by the Vancouver classification.\n\nVenous thrombosis such as deep vein thrombosis and pulmonary embolism are relatively common following hip replacement surgery. Standard treatment with anticoagulants is for 7–10 days; however treatment for more than 21 days may be superior. Research from 2013 has on the other hand suggested that anticoagulants in otherwise healthy patients undergoing a so-called fast track protocol with hospital stays under five days, might only be necessary while in the hospital.\n\nSome physicians and patients may consider having an ultrasonography for deep vein thrombosis after hip replacement. However, this kind of screening should only be done when indicated because to perform it routinely would be unnecessary health care.\n\nMany long-term problems with hip replacements are the result of osteolysis. This is the loss of bone caused by the body's reaction to polyethylene wear debris, fine bits of plastic that come off the cup liner over time. An inflammatory process causes bone resorption that may lead to subsequent loosening of the hip implants and even fractures in the bone around the implants. In an attempt to eliminate the generation of wear particles, ceramic bearing surfaces are being used in the hope that they will have less wear and less osteolysis with better long-term results. Metal cup liners joined with metal heads (metal-on-metal hip arthroplasty) were also developed for similar reasons. In the lab these show excellent wear characteristics and benefit from a different mode of lubrication. At the same time that these two bearing surfaces were being developed, highly cross linked polyethylene plastic liners were also developed. The greater cross linking significantly reduces the amount of plastic wear debris given off over time. The newer ceramic and metal prostheses do not always have the long-term track record of established metal on poly bearings. Ceramic pieces can break leading to catastrophic failure. This occurs in about 2% of the implants placed. They may also cause an audible, high pitched squeaking noise with activity. Metal-on-metal arthroplasty releases metal debris into the body raising concerns about the potential dangers of these accumulating over time. Highly cross linked polyethylene is not as strong as regular polyethylene. These plastic liners can crack or break free of the metal shell that holds them.\n\nOn radiography, it is normal to see thin radiolucent areas of less than 2 mm around hip prosthesis components, or between a cement mantle and bone. However, these may still indicate loosening of the prosthesis if they are new or changing, and areas greater than 2 mm may be harmless if they are stable. The most important prognostic factors of cemented cups are absence of radiolucent lines in DeLee and Charnley zone I, as well as adequate cement mantle thickness. In the first year after insertion of uncemented femoral stems, it is normal to have mild subsidence (less than 10 mm). The direct anterior approach has been shown to itself be a risk factor for early femoral component loosening.\n\nConcerns are being raised about the metal sensitivity and potential dangers of metal particulate debris. New publications have demonstrated development of \"pseudotumors\", soft tissue masses containing necrotic tissue, around the hip joint. It appears these masses are more common in women and these patients show a higher level of iron in the blood. The cause is unknown and is probably multifactorial. There may be a toxic reaction to an excess of particulate metal wear debris or a hypersensitivity reaction to a normal amount of metal debris.\n\nMetal hypersensitivity is a well-established phenomenon and is common, affecting about 10–15% of the population. Contact with metals can cause immune reactions such as skin hives, eczema, redness and itching. Although little is known about the short- and long-term pharmacodynamics and bioavailability of circulating metal degradation products in vivo, there have been many reports of immunologic type responses temporally associated with implantation of metal components. Individual case reports link hypersensitivity immune reactions with adverse performance of metallic clinical cardiovascular, orthopedic and plastic surgical and dental implants.\n\nMost hip replacements consist of cobalt and chromium alloys, or titanium. Stainless steel is no longer used. All implants release their constituent ions into the blood. Typically these are excreted in the urine, but in certain individuals the ions can accumulate in the body. In implants which involve metal-on-metal contact, microscopic fragments of cobalt and chromium can be absorbed into the patient's bloodstream. There are reports of cobalt toxicity with hip replacement patients.\n\nPost operative sciatic nerve palsy is another possible complication. The incidence of this complication is low. Femoral nerve palsy is another but much more rare complication. Both of these will typically resolve over time, but the healing process is slow. Patients with pre-existing nerve injury are at greater risk of experiencing this complication and are also slower to recover.\n\nA few patients who have had a hip replacement suffer chronic pain after the surgery. Groin pain can develop if the muscle that raises the hip (iliopsoas) rubs against the edge of the acetabular cup. Bursitis can develop at the trochanter where a surgical scar crosses the bone, or if the femoral component used pushes the leg out to the side too far. Also some patients can experience pain in cold or damp weather. Incision made in the front of the hip (anterior approach) can cut a nerve running down the thigh leading to numbness in the thigh and occasionally chronic pain at the point where the nerve was cut (a neuroma).\n\nThe rate of death for elective hip replacements is significantly less than 1%.\n\nBy 2010, reports in the orthopaedic literature increasingly cited the problem of early failure of metal on metal prostheses in a small percentage of patients. Failures may relate to release of minute metallic particles or metal ions from wear of the implants, causing pain and disability severe enough to require revision surgery in 1–3% of patients. Design deficits of some prothesis models, especially with heat-treated alloys and a lack of special surgical experience accounting for most of the failures. In 2010, surgeons at medical centers such as the Mayo Clinic reported reducing their use of metal-on-metal implants by 80 percent over the previous year in favor of those made from other materials, like combinations of metal and plastic. The cause of these failures remain controversial, and may include both design factors, technique factors, and factors related to patient immune responses (allergy type reactions). In the United Kingdom the Medicines and Healthcare Products Regulatory Agency commenced an annual monitoring regime for metal-on-metal hip replacement patients from May 2010. Data which are shown in The Australian Orthopaedic Association's 2008 National Joint replacement registry, a record of nearly every hip implanted in that country over the previous 10 years, tracked 6,773 BHR (Birmingham Hip Resurfacing) Hips and found that less than one-third of one percent may have been revised due to the patient's reaction to the metal component. Other similar metal-on-metal designs have not fared as well, where some reports show 76% to 100% of the people with these metal-on-metal implants and have aseptic implant failures requiring revision also have evidence of histological inflammation accompanied by extensive lymphocyte infiltrates, characteristic of delayed type hypersensitivity responses. It is not clear to what extent this phenomenon negatively affects orthopedic patients. However, for patients presenting with signs of an allergic reactions, evaluation for sensitivity should be conducted. Removal of the device that is not needed should be considered, since removal may alleviate the symptoms. Patients who have allergic reactions to cheap jewelry are more likely to have reactions to orthopedic implants. There is increasing awareness of the phenomenon of metal sensitivity and many surgeons now take this into account when planning which implant is optimal for each patient.\n\nOn March 12, 2012, \"The Lancet\" published a study, based on data from the National Joint Registry of England and Wales, finding that metal-on-metal hip implants failed at much greater rates than other types of hip implants and calling for a ban on all metal-on-metal hips. The analysis of 402,051 hip replacements showed that 6.2% of metal-on-metal hip implants had failed within five years, compared to 1.7% of metal-on-plastic and 2.3% of ceramic-on-ceramic hip implants. Each increase in head size of metal-on-metal hip implants was associated with a 2% increase of failure. Surgeons of the British Hip Society are recommending that large head metal-on-metal implants should no longer be performed.\n\nOn February 10, 2011, the U.S. FDA issued an advisory on metal-metal hip implants, stating it was continuing to gather and review all available information about metal-on-metal hip systems. On June 27–28, 2012, an advisory panel met to decide whether to impose new standards, taking into account findings of the study in \"The Lancet\". No new standards, such as routine checking of blood metal ion levels, were set, but guidance was updated. Currently, FDA has not required hip implants to be tested in clinical trials before they can be sold in the U.S. Instead, companies making new hip implants only need to prove that they are \"substantially equivalent\" to other hip implants already on the market. The exception is metal-on-metal implants, which were not tested in clinical trials but because of the high revision rate of metal-on-metal hips, in the future the FDA has stated that clinical trials will be required for approval and that post-market studies will be required to keep metal on metal hip implants on the market.\n\nThe first line approach as an alternative to hip replacement is conservative management which involves a multimodal approach of oral medication, injections, activity modification and physical therapy. Conservative management can prevent or delay the need for hip replacement.\n\nPreoperative education is currently an important part of patient care. There is some evidence that it may slightly reduce anxiety before hip or knee replacement, with low risk of negative effects.\n\nHemiarthroplasty is a surgical procedure which replaces one half of the joint with an artificial surface and leaves the other part in its natural (pre-operative) state. This class of procedure is most commonly performed on the hip after a subcapital (just below the head) fracture of the neck of the femur (a hip fracture). The procedure is performed by removing the head of the femur and replacing it with a metal or composite prosthesis. The most commonly used prosthesis designs are the Austin Moore prosthesis and the Thompson Prosthesis. More recently a composite of metal and HDPE which forms two interphases (bipolar prosthesis) has also been used. The monopolar prosthesis has not been shown to have any advantage over bipolar designs. The procedure is recommended only for elderly and frail patients, due to their lower life expectancy and activity level. This is because with the passage of time the prosthesis tends to loosen or to erode the acetabulum.\n\nHip resurfacing is an alternative to hip replacement surgery. It has been used in Europe for over seventeen years and become a common procedure. Health-related quality of life measures are markedly improved and patient satisfaction is favorable after hip resurfacing arthroplasty.\n\nThe minimally invasive hip resurfacing procedure is a further refinement to hip resurfacing.\n\nCurrent alternatives also include viscosupplementation, or the injection of artificial lubricants into the joint. Use of these medications in the hip is off label. The cost of treatment is typically not covered by health insurance organizations.\n\nSome believe that the future of osteoarthritis treatment is bioengineering, targeting the growth and/or repair of the damaged, arthritic joint. Centeno et al. have reported on the partial regeneration of an arthritic human hip joint using mesenchymal stem cells in one patient. It is yet to be shown that this result will apply to a larger group of patients and result in significant benefits. The FDA has stated that this procedure is being practiced without conforming to regulations, but Centeno claims that it is exempt from FDA regulation. It has not been shown in controlled clinical trials to be effective, and costs over $7,000.\n\nTotal hip replacement incidence varies in developed countries between 30 (Romania) and 290 (Germany) procedures per 100,000 population per year. Approximately 0.8% of Americans have undergone the procedure.\n\nAccording to the International Federation of Healthcare Plans, the average cost of a total hip replacement in 2012 was $40,364 in the United States, $11,889 in the United Kingdom, $10,987 in France, $9,574 in Switzerland, and $7,731 in Spain. In the United States, the average cost of a total hip replacement varies widely by geographic region, ranging from $11,327 (Birmingham, Alabama) to $73,927 (Boston, Massachusetts).\n\nThe earliest recorded attempts at hip replacement were carried out in Germany in 1891 by Themistocles Gluck (1853–1942), who used ivory to replace the femoral head (the ball on the femur), attaching it with nickel-plated screws, Plaster of Paris, and glue.\n\nOn September 28, 1940 at Columbia Hospital in Columbia, South Carolina, American surgeon Dr. Austin T. Moore (1899–1963) performed the first metallic hip replacement surgery. The original prosthesis he designed was a proximal femoral replacement, with a large fixed head made of the cobalt-chrome alloy Vitallium. It was about a foot in length and bolted to the resected end of the femoral shaft (hemiarthroplasty). A later version, the so-called Austin Moore Prosthesis which was introduced in 1952, is still in use today, although rarely. Like modern hip implants, it is inserted into the medullary canal of the femur, and depends on bone growth through a hole in the stem for long-term attachment.\n\n\n"}
{"id": "53534159", "url": "https://en.wikipedia.org/wiki?curid=53534159", "title": "Jane Grimson", "text": "Jane Grimson\n\nJane Grimson (née Wright), is a Scottish-born computer engineer. She is Fellow Emerita and Pro-Chancellor at Trinity College Dublin.\n\nGrimson attended Alexandra College Dublin. She was the first woman to graduate in engineering from Trinity College Dublin obtaining a first class honors degree and gold medal in 1970. She received a Masters in Computer Science from the University of Toronto in 1971, and a PhD from the University of Edinburgh in 1981.\n\nIn 1980, Grimson was appointed to a Lectureship in Computer Science at Trinity College Dublin where she spent her entire academic career, holding a Personal Chair in Health Informatics prior to her retirement in 2014. Her major research interests are in Health Informatics, a field concerned with the application of Information and Communications Technology to improve the quality and safety of healthcare.\n\nGrimson served as Dean of Engineering and Systems Sciences from 1996-1999, as pro-Dean of Research in 2001 and as Vice-Provost from 2001-2005, being the first woman to ever take these roles. She was appointed Pro-Chancellor of the University of Dublin in 2016.\n\nA chartered Fellow of the Institution of Engineers of Ireland (now Engineers Ireland) and a EUR ING, Professor Grimson served as President of Engineers Ireland from 1999-2000, again the first woman to hold this role. She is a Fellow and Past-President (2002) of the Irish Academy of Engineering and of the Irish Computer Society (2000-2004). She was President of the Healthcare Informatics Society of Ireland from 1999-2006. Professor Grimson was partially seconded to the newly established Health Information and Quality Authority (HIQA) as its first Director of Health Information in 2007, where she led the development of national standards for health information. In 2014, she was appointed Acting Chief Executive of HIQA, just prior to her retirement.\n\nShe has served on numerous boards including Science Foundation Ireland, the Energy Research Council, the European Research Advisory Board, and the Mary Robinson Foundation - Climate Justice and was Chair of the Irish Research Council for Science, Engineering and Technology.\n\nProfessor Grimson is an outspoken advocate for the advancement of women in engineering and in research more broadly. She helped to establish WiSER (the Centre for Women in Science and Engineering Research) at Trinity College Dublin, and also chaired a Department of Education and Science committee aiming to increase female representation in Science, Engineering and Technology. She also chaired the Gender Equality Task Force at NUI Galway from 2015-2016.\n\nProfessor Grimson was elected as an International Fellow of the Royal Academy of Engineering in 2004. She was awarded the O'Moore Medal in 2007 in recognition of her contribution to the field of Health Informatics. In 2009, she was elected as a member of the Royal Irish Academy. In 2017 she was awarded an honorary degree by NUI Galway.\n\nProfessor Grimson's father was Professor Bill Wright, Chair of Civil Engineering in Trinity College Dublin. She married a fellow Engineering student at Trinity, Bill Grimson.\n"}
{"id": "14741403", "url": "https://en.wikipedia.org/wiki?curid=14741403", "title": "LDK Solar Co", "text": "LDK Solar Co\n\nLDK Solar Co. Ltd., located in Xinyu City, Jiangxi province in the People's Republic of China, manufactures multicrystalline solar wafers used in solar cells. Additionally, LDK provides wafering services for both monocrystalline and multicrystalline wafers to companies who provide their own ingot stock. LDK's principal customers have included CSI, Chinalight Solar Co., Ltd., Solarfun Power Holdings Co., Ltd., Solartech Energy Corp., Solland Solar Energy B.V., and Suntech Power Holdings Co., Ltd. The company has sold wafers to Chinalight primarily pursuant to short-term sales contracts, and monthly and quarterly purchase orders.\n\nTheir distribution network for solar products covers over 43 distributors and wholesalers, across over 15 different countries.\n\nXiaofeng Peng (born c.1975) founded LDK Solar (nicknamed Light) in July 2005 and is its chairman and chief executive officer. Mr. Peng first founded Suzhou Liouxin in March 1997, and was its chief executive officer until February 2006. Suzhou Liouxin manufactured personal protective equipment products like gloves and employed 12,000. He considered adding solar cell wafers to its product line when he realized that no Chinese company was producing them. In 2005, Peng invested $30 million of his own money and $80 million of venture financing into building factories.\n\nMr. Peng graduated from Jiangxi Foreign Trade School in 1993 with a diploma in international business and from Beijing University Guanghua School of Management with an executive MBA degree in 2002. His net worth was $2.5 billion in 2008.\n\n2013 Xiaofeng Peng tried to run Ecommerce, Set up FFdms Co., tried C2B ecommerce model, but was not able to put in much time. In January 2014 given FFdms owed suppliers and employees salary scandals, a lot of illegal dismissal of employees lawsuits. April 2014 Xiaofeng Peng, as the respondent, requested access to personal bankruptcy liquidation proceedings.\n\nLDK has stated an annualized capacity of 1.46 GW as of December 31, 2008, with plans to expand to 2.3 GW per year by the end of 2009. And has achieved 3GW at the end of 2010\n\nLDK has contracted with Fluor Corp., an American engineering firm, for construction of a 15,000 Ton per Year polysilicon plant. The first 5000-ton train went into production in July 2009, with the second 5000 train scheduled for Q3 and third and final train by the end of 2010.\n\nA Second, 1000 Ton per Year Polysilicon Plant was Bought by LDK from Sunways AG, and entered Production Phase at the end of 2008. On January 16, 2009 LDK announced the completion of their first polysilicon production run from their Sunways Plant .\n\nGrid Parity Presentation\n\n\n"}
{"id": "159032", "url": "https://en.wikipedia.org/wiki?curid=159032", "title": "Lead programmer", "text": "Lead programmer\n\nA lead programmer is a software engineer in charge of one or more software projects. Alternative titles include \"development lead\", \"technical lead\", \"lead software engineer\", \"software design engineer lead\" (SDE lead), \"software development manager\", \"software manager\", or \"lead application developer\". When primarily contributing in a high-level enterprise software design role, the title \"software architect\" (or similar) is often used.\n\nA lead programmer's exact responsibilities vary from company to company, but in general he or she is responsible for the underlying architecture for the software program, as well as for overseeing the work being done by any other software engineers working on the project. A lead programmer will typically also act as a mentor for new or lower-level software developers or programmers, as well as for all the members on the development team.\n\nAlthough the responsibilities are primarily technical, lead programmers also generally serve as an interface between the programmers and management, have ownership of development plans and have supervisorial responsibilities in delegating work and ensuring that software projects come in on time and under budget. Lead programmers also serve as technical advisers to management and provide programming perspective on requirements. Typically a lead programmer will oversee a development team of between two and ten programmers, with three to five often considered the ideal size. Teams larger than ten programmers tend to become unmanageable without additional structure. A lead programmer normally reports to a manager with overall project or section responsibility, such as a director or product unit manager (PUM).\n\nLead programmers are usually trained in software programming, although do not necessarily hold formal degrees in the subject, and may learn management responsibilities either on the job or through short courses. Because their primary training is usually technical rather than managerial, lead programmers traditionally see themselves as part of the technical staff of a company rather than as part of management. \n"}
{"id": "498411", "url": "https://en.wikipedia.org/wiki?curid=498411", "title": "List of firearms", "text": "List of firearms\n\nThis is an extensive list of small arms—including pistols, shotguns, sniper rifles, submachine guns, personal defense weapons, assault rifles, battle rifles, designated marksman rifles, carbines, machine guns, flamethrowers, multiple barrel firearms, grenade launchers, and anti-tank rifles—that includes variants.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "7468499", "url": "https://en.wikipedia.org/wiki?curid=7468499", "title": "Marcel Loncin Research Prize", "text": "Marcel Loncin Research Prize\n\nThe Marcel Loncin Research Prize was established in 1994. It is awarded by the Institute of Food Technologists (IFT) in even-numbered years to fund basic chemistry, physics, and/or engineering research applied to food processing and improving food quality. It is named for Marcel Loncin (1920-1994), a Belgian-born, French chemical engineer who did food engineering research while a professor at the Centre d'Enseignement et de Recherches des Industries Alimentaries (CERIA) and afterwards at the Food Engineering Department of the Universität Karlsruhe (TH), Germany. It was the third and final IFT award as of 2006 that has been named for a then-living person.\n\nAward winners receive USD 50,000 in two annual installments and a plaque from the Marcin Loncin Endowment Fund of the IFT.\n\n\n"}
{"id": "741886", "url": "https://en.wikipedia.org/wiki?curid=741886", "title": "Meg Whitman", "text": "Meg Whitman\n\nMargaret Cushing \"Meg\" Whitman (born August 4, 1956) is an American business executive, political activist, and philanthropist. Whitman served as President and Chief Executive Officer of Hewlett Packard Enterprise. Whitman was a senior member of Mitt Romney's presidential campaigns in both 2008 and 2012 and ran for governor of California as a Republican but supported Hillary Clinton in 2016.\n\nA native of Cold Spring Harbor, a hamlet of Huntington, New York, Whitman is a graduate of Princeton University and Harvard Business School. Whitman served as an executive in The Walt Disney Company, where she was Vice President of Strategic Planning throughout the 1980s. In the 1990s, she served as an executive for DreamWorks, Procter & Gamble, and Hasbro.\n\nWhitman served as President and Chief Executive Officer of eBay, from 1998 to 2008. During Whitman's 10 years with the company, she oversaw its expansion from 30 employees and $4 million in annual revenue, to more than 15,000 employees and $8 billion in annual revenue. In 2014, Whitman was named 20th in \"Forbes\" List of the 100 Most Powerful Women in the World.\n\nIn 2008, Whitman was cited by \"The New York Times\" as among the women most likely to become the first female President of the United States. In February 2009, Whitman announced her candidacy for Governor of California, becoming the third woman in a 20-year period to run for the office. Whitman won the Republican primary in June 2010. The fifth-wealthiest woman in California with a net worth of $1.3 billion in 2010, she spent more of her own money on the race than any other political candidate spent on a single election in American history, spending $144 million of her own fortune and $178.5 million in total, including money from donors. Whitman was defeated by Democratic former Governor Jerry Brown in the 2010 California gubernatorial election by 54% to 41%.\n\nWhitman was born in Cold Spring Harbor, New York, the daughter of Margaret Cushing (née Goodhue) and Hendricks Hallett Whitman, Jr. Her patrilineal great-great-great-grandfather, Elnathan Whitman, was a member of the Nova Scotia House of Assembly. Through her father, Whitman is also a great-great-granddaughter of U.S. Senator Charles B. Farwell, of Illinois. On her mother's side, she is a great-granddaughter of historian and jurist Munroe Smith and a great-great-granddaughter of General Henry S. Huidekoper. Her paternal grandmother, born Adelaide Chatfield-Taylor, was the daughter of writer Hobart Chatfield-Taylor and the sister of economist Wayne Chatfield-Taylor.\n\nWhitman attended Cold Spring Harbor High School in Cold Spring Harbor, New York, graduating after three years in 1974. In her memoirs, she says she was in the top 10 of her class. She wanted to be a doctor, so she studied math and science at Princeton University. However, after spending a summer selling advertisements for a magazine, she changed over to the study of economics, earning a B.A. with honors in 1977. Whitman then obtained an M.B.A. from Harvard Business School in 1979.\n\nWhitman is married to Griffith Harsh IV, a neurosurgeon at Stanford University Medical Center. They have two sons. She has lived in Atherton, California, since March 1998. Whitman College, a residential college completed in 2007 at Princeton University, was named for Meg Whitman following her $30 million donation.\n\nBeginning her career in 1979 as a brand manager at Procter & Gamble in Cincinnati, Ohio. Whitman later moved on to work as a consultant at Bain & Company's San Francisco office. She then rose through the ranks to achieve the position of senior vice president.\n\nWhitman became vice president of strategic planning at The Walt Disney Company in 1989. Two years later she joined the Stride Rite Corporation, before becoming president and CEO of Florists' Transworld Delivery in 1995.\n\nAs Hasbro's Playskool Division General Manager, she oversaw global management and marketing of two children's brands, Playskool and Mr. Potato Head starting in January 1997. She also imported the UK's children's television show Teletubbies into the U.S.\n\nWhitman joined eBay in March 1998, when it had 30 employees and revenues of approximately $4 million. During her time as CEO, the company grew to approximately 15,000 employees and $8 billion in annual revenue by 2008. Originally, when Whitman had joined eBay, she found the website as a simple black and white webpage with courier font. On her first day, the site crashed for eight hours. She believed the site to be confusing and began by building a new executive team. Whitman organized the company by splitting it into twenty-three business categories. She then assigned executives to each, including some 35,000 subcategories. In 2004, Whitman made several key changes in her management team. Jeff Jordan took over PayPal, Matt Bannick took control of international operations and Bill Cobb was placed in control of U.S. operations, which has the colorful U.S. logo, while each international site has unique branding.\nWhitman picked John J. Donahoe for eBay in March 2005 as President of eBay Marketplaces, responsible for all elements of eBay's global ecommerce businesses.\n\nDuring Whitman's tenure as CEO, eBay completed the purchase of Skype for $4.1B in cash and stock in September 2005. eBay later admitted that it had overpaid and, in 2009, eBay sold Skype to a group of investors led by Silver Lake Partners at a valuation of $2.75B. In 2011, after the first papers were filed for a possible IPO, Microsoft purchased Skype for US$8.5B.\n\nIn June 2007, while preparing for an interview with Reuters, Whitman allegedly shoved her subordinate, communications employee Young Mi Kim. Of the incident, Whitman related, \"In any high-pressure working environment, tensions can surface.\" Kim also stated, \"Yes, we had an unfortunate incident, but we resolved it in a way that speaks well for her and for eBay.\" The matter was resolved after a $200,000 settlement.\n\nWhitman resigned as CEO of eBay in November 2007, but remained on the board and served as an advisor to new CEO John Donahoe until late 2008. She was inducted into the U.S. Business Hall of Fame in 2008. \"I've said for some time that 10 years is roughly the right time to stay at the helm at a company like ours\", she said in an interview with the \"San Francisco Chronicle\", adding that \"it's time for new leadership, a new perspective and a new vision.\"\n\nWhitman has received numerous awards and accolades for her work at eBay. On more than one occasion, she was named among the top five most powerful women by \"Fortune\" magazine. \"Harvard Business Review\" named her the eighth-best-performing CEO of the past decade and the \"Financial Times\" named her as one of the 50 faces that shaped the decade.\n\nWhitman also served on the board of directors of the eBay Foundation, Summit Public Schools, Procter & Gamble and DreamWorks SKG, until early 2009. She was appointed to the board of Goldman Sachs in October 2001 and then resigned in December 2002, amidst controversy that she had received shares in several public offerings managed by Goldman Sachs, although she denied any wrongdoing. (see Ties to Goldman Sachs for further detail). In March 2011, she was appointed a part-time special adviser at venture capital firm Kleiner Perkins Caufield & Byers.\n\nShe has also joined the boards of Zipcar and Teach For America, and re-joined the board of Procter & Gamble. Whitman has also been a member of the board at Survey Monkey and Immortals.\n\nIn January 2011, Whitman joined Hewlett-Packard's (HP) board of directors. She was named CEO on September 22, 2011. As well as renewing focus on HP's Research & Development division, Whitman's major decision during her first year as CEO has been to retain and recommit the firm to the PC business that her predecessor announced he was considering discarding.\n\nIn May 2013, Bloomberg L.P. named Whitman \"Most Underachieving CEO\" along with Apple's CEO Tim Cook (ranked 12th) and IBM's Virginia Rometty (ranked 10th) -- whose stocks have all turned in the worst numbers relative to the broader market since the beginning of each CEO's tenure. HP's stock led the list by underperforming by 30 percentage points since Whitman took the job.\n\nOn July 26, 2017, Whitman stepped down as Chairwoman of HP Inc.'s board of directors, while remaining as CEO of Hewlett Packard Enterprise (HPE). Whitman fought off further rumours around her position at HPE, where she was quoted by \"The New York Times\" \"So let me make this as clear as I can. I am fully committed to HPE and plan to remain the company's C.E.O. We have a lot of work still to do at HPE and I am not going anywhere\" \n\nOn November 21, 2017 it was announced Whitman was stepping aside as the CEO of HPE effectively February 1, 2018 with HPE president Antonio Neri taking over as CEO.\n\nWhitman founded a charitable foundation with husband Harsh on December 21, 2006, by donating to it 300,000 shares of eBay stock worth $9.4 million. By the end of its first year of operation, the Griffith R. Harsh IV and Margaret C Whitman Charitable Foundation had $46 million in assets and has disbursed $125,000 to charitable causes. Most of the money disbursed went to the Environmental Defense Fund. In 2010, Warren Buffett asked Whitman to join the Giving Pledge in which billionaires would commit to donating half of their money to charity, and Whitman declined. In 2011, the foundation donated $2.5 million to Summit Public Schools, which operates several charter schools in the San Jose area.\n\nWhitman was a supporter of former Massachusetts Governor Mitt Romney's presidential campaign in 2008 and was on his national finance team. She was also listed as finance co-chair of Romney's exploratory committee. After Romney stepped out of the race and endorsed John McCain, Whitman joined McCain's presidential campaign as a national co-chair. McCain mentioned Whitman as a possible Secretary of the Treasury during the second presidential debate in 2008, but lost the election to Barack Obama.\n\nDuring the 2012 Republican primaries, Whitman endorsed Mitt Romney, who praised her. Whitman's name was mentioned as a possible cabinet member in a Romney administration before he lost to Obama.\n\nDuring the 2016 Republican primaries, Whitman was finance co-chair of Chris Christie's presidential campaign. After Christie withdrew from the race and subsequently endorsed Donald Trump, Whitman criticized it as \"an astonishing display of political opportunism\" and called on other Christie donors to reject Trump, whom she compared to Adolf Hitler and Benito Mussolini. In August, Whitman endorsed Democrat Hillary Clinton's presidential campaign, stating that to vote for Trump \"out of party loyalty alone would be to endorse a candidacy that I believe has exploited anger, grievance, xenophobia and racial division\". Acknowledging policy differences with Clinton, Whitman nonetheless praised her \"temperament, global experience and commitment to America's bedrock national values\". She called on all Republicans \"to put country first before party\" and added that she would support the campaign financially.\n\nOn February 10, 2009, Whitman announced she would run for governor of California in the 2010 election. Her campaign was largely self-funded. She spent more of her own money on this effort than any other self-funded political candidate in U.S. history and ultimately lost to Jerry Brown.\n\nAccording to final reports, Whitman spent $144 million from her own personal funds. \"The Daily Caller\" said that her \"penchant for throwing money around is well known in California political circles\". The comment came in connection with the disclosure that her campaign had paid far above market rates for advertising on a conservative political blog. The blog's founder, addressing the issue of a possible attempt to influence the blog's content, said he had made clear to the Whitman campaign that \"advertising and editorial are two very different things\".\n\nIn June 2010, Whitman released a political ad, \"A Lifetime in Politics A Legacy of Failure\", which seemingly contained one image of the FAIL Blog website, making it appear in the ad as if Jerry Brown had been the subject of one of the website's namesake \"fails\". Ben Huh, founder of the Cheezburger Network, of which failblog.org is a part, demanded an apology and the removal of the video, stating that the image was faked, and that the website is non-partisan and has never endorsed a particular political candidate or party.\n\nOn November 2, 2010, at 11:35 pm, Whitman conceded the election to her opponent, Jerry Brown, stating \"We've come up a little short.\"\n\nIn 2010, \"The Sacramento Bee\" reported that Whitman did not vote for 28 years, after reviewing her voting records in California. Whitman has described her voting record as \"inexcusable\", apologized for it, and stated that she is happy to discuss the matter. Whitman answered questions about her record in September, replying, \"And I think the reason for many years, I wasn't as engaged in the political process and should have been.\"\n\nIn September 2010, Nicky Diaz Santillan revealed that she was employed in the Whitman household as a housekeeper and nanny from 2000–2009 despite her status as an illegal worker. Whitman's campaign released documents which she says Santillan provided prior to her employment including a driver's license, social security ID, and application. Santillan says Whitman knew she was undocumented, producing a 2003 letter from the Social Security Administration stating that her Social Security number did not match her name. Whitman initially stated that they \"never received those letters\", however, after a hand-written note on the document was shown, believed to be from Whitman's husband, they acknowledged they may have received it, but forgot. Santillan's attorney, Gloria Allred, states that Santillan was fired for the sake of the campaign. Whitman's campaign maintains that this is a political attack, stating that Allred is a Jerry Brown supporter. Brown, Allred and Santillan all deny this. Crystal Williams, Director of the American Immigration Lawyers Association stated \"Not only is accepting the documents all the law required [Whitman] to do, but there's a counterbalancing anti-discrimination law that keeps her from probing further or demanding different documents.\" Others disagree; Immigration lawyer Greg Siskind states Whitman was the employer, and the documents by law needed to be signed by her but were not, nor did they have a social security number on them; the \"Fort Worth Star-Telegram\" noted that Whitman \"hired her, paid her and had direct contact with her for nine years\", so should have known her legal status. The \"Los Angeles Times\" noted that Latino voters were more likely interested that Whitman treated Santillan \"like a piece of garbage\" when the maid asked for help finding an immigration attorney, and Whitman allegedly stated \"you don't know me and I don't know you\".\n\nGoldman Sachs, whose executives donated $100,000 to the Whitman campaign, manages a part of Whitman's fortune. As CEO of eBay, Whitman earned approximately $1.78 million resulting from a practice known as spinning whereby executives who did business with Goldman Sachs could reap profits by getting early deals before the public on hot IPOs offered by the bank. Whitman later resigned from the Goldman Sachs board after some expressed concern over her receiving shares from Goldman Sachs. In commenting on Whitman's resignation from the Goldman Sachs board, eBay spokesman Henry Gomez told \"The Wall Street Journal\" at the time that, \"If we wanted to use Goldman's services, she doesn't want there to be even the slightest perception of any conflict. She's doing this because she thinks quite highly of the firm.\" While Whitman was on Goldman Sachs' board, she served on the compensation committee, which approved multimillion-dollar bonus packages for then-CEO Henry Paulson and his top aides. Public domain documents reveal that Whitman has a multimillion-dollar stake in 21 investment funds managed by Goldman Sachs. Given Goldman Sachs' major investments in California state finances, all these ties to Goldman Sachs led to considerable controversy during the gubernatorial campaign. In response, Whitman vowed to eliminate any potential conflicts of interest, and publicly stated that she would immediately sell her Goldman Sachs stock and put her Goldman Sachs-managed investments in a blind trust if elected governor.\n\nWhile running for governor, Whitman emphasized three major areas: job creation, reduced state government spending, and reform of the state's K-12 educational system. She argued that it is best to start only a few things and finish them, instead of starting a lot of things and finishing few of them.\n\nWhitman said that if elected, on her first day she would have suspended AB 32, the Global Warming Solutions Act of 2006, for a year to study its potential economic implications. AB 32 requires the state to cut greenhouse gas emissions by 30 percent by 2020. At the state Republican Convention in March 2010, Whitman described California Republican Governor Arnold Schwarzenegger's climate change bill as a \"job-killer\". Whitman opposed Proposition 23, which would delay the global warming law AB 32 until California's unemployment falls to 5.5 percent and stays there for a year, stating that the proposition did not reasonably balance the need to protect jobs with the need to preserve environment.\n\nOn water issues, Whitman opposed further restrictions on water supply in the Central Valley, and she suggested that President Obama should overturn a federal judge's ruling under provisions in the Endangered Species Act which reduced water supplies another 5% to 7%.\n\nWhitman said that Arizona's approach to illegal immigration with Arizona SB 1070 is wrong and that there are better ways to solve the problem. She said that, if she had lived in California in 1994, she would have voted against Proposition 187 concerning illegal immigrants. In an op-ed during her gubernatorial campaign, Whitman wrote, \"Clearly, when examining our positions on immigration, there is very little over which Jerry Brown and I disagree\".\n\nShe stated that illegal immigrant students should be prohibited from attending state-funded institutions of higher education. Currently, California state law permits this. In 2009, Whitman called for \"a path to legalization\" of illegal immigrants. In a 2010 interview on television station KTLA, Whitman said, \"I want to hold employers accountable for hiring only documented workers.\"\n\nDuring the 2010 California gubernatorial election, Whitman supported California's Proposition 8, which reversed \"In re Marriage Cases\" and defined marriage as a union between one man and one woman in the state. Whitman also criticized Governor Arnold Schwarzenegger and Attorney General Jerry Brown for not defending Proposition 8 in the federal judicial system. However, on February 26, 2013, Whitman confirmed that she had reversed that opinion. Whitman stated, \"At the time, I believed the people of California had weighed in on this question and that overturning the will of the people was the wrong approach,\" and \"The facts and arguments presented during the legal process since then have had a profound impact on my thinking.\" Whitman also believes that gay and lesbian couples should be permitted to adopt children. Whitman supports abortion rights.\n\nWhitman has said that the legalization of marijuana is not what any law enforcement person would suggest for any reason and that \"this is the worst idea [she has] ever seen.\"\n\nWhitman does not support the proposed and partially voter-funded California High-Speed Rail project. In a letter to the \"Sacramento Bee\" Whitman's spokeswoman Sarah Pompei said, \"Meg believes the state cannot afford the costs associated with high-speed rail due to our current fiscal crisis.\" Her opponent Jerry Brown is in favor of the project.\n\nWhitman has made monetary donations to various candidates and political action committees (PAC). While these have gone to both Republicans and Democrats, the donations are weighted to Republicans. Though Whitman has contributed to a few Democrats, including Senator Barbara Boxer; donating $4,000 to her campaign and serving on the \"Friends of Boxer\" committee in 2004, she donated more than $225,000 during the same period to Republicans, eBay's PAC and to Americans for a Republican Majority, the PAC of former Representative Tom DeLay.\n\nIn 2017, Whitman was the Commencement speaker for Carnegie Mellon University and was awarded an honorary doctorate degree.\n\n\n"}
{"id": "1850771", "url": "https://en.wikipedia.org/wiki?curid=1850771", "title": "Modified atmosphere", "text": "Modified atmosphere\n\nModified atmosphere is the practice of modifying the composition of the internal atmosphere of a package (commonly food packages, drugs, etc.) in order to improve the shelf life.The need for this technology for food arises from the short shelf life of food products such as meat, fish, poultry, and dairy in the presence of oxygen. In food, oxygen is readily available for lipid oxidation reactions. Oxygen also helps maintain high respiration rates of fresh produce, which contribute to shortened shelf life. From a microbiological aspect, oxygen encourages the growth of aerobic spoilage microorganisms. Therefore, the reduction of oxygen and its replacement with other gases can reduce or delay oxidation reactions and microbiological spoilage. Oxygen scavengers may also be used to reduce browning due to lipid oxidation by halting the auto-oxidative chemical process.\n\nThe modification process generally lowers the amount of oxygen (O) in the headspace of the package. Oxygen can be replaced with nitrogen (N), a comparatively inert gas, or carbon dioxide (CO).\n\nA stable atmosphere of gases inside the packaging can be achieved using active techniques, such as gas flushing and compensated vacuum, or passively by designing “breathable” films.\n\nThe first recorded beneficial effects of using modified atmosphere date back to 1821. Jacques Etienne Berard, a professor at the School of Pharmacy in Montpellier, France, reported delayed ripening of fruit and increased shelf life in low-oxygen storage conditions. Controlled Atmosphere Storage (CAS) was used from the 1930's when ships transporting fresh apples and pears had high levels of CO in their holding rooms in order to increase the shelf life of the product. In the 1970s MA packages reached the stores when bacon and fish were sold in retail packs in Mexico. Since then development has been continuous and interest in MAP has grown due to consumer demand.\n\nAtmosphere within the package can be modified passively or actively. In passive modified atmosphere packaging (MAP), the high concentration of CO and low O levels in the package is achieved over time as a result of respiration of the product and gas transmission rates of the packaging film. This method is commonly used for fresh respiring fruits and vegetables. Reducing O and increasing CO slows down respiration rate, conserves stored energy, and therefore extended shelf life. On the other hand, active MA involves the use of active systems such as O and CO scavengers or emitters, moisture absorbers, ethylene scavengers, ethanol emitters and gas flushing in the packaging film or container to modify the atmosphere within the package. \n\nThe mixture of gases selected for a MAP package depends on the type of product, the packaging materials and the storage temperature. The atmosphere in an MA package consists mainly of adjusted amounts of N, O, and CO Reduction of O promotes delay in deteriorative reactions in foods such as lipid oxidation, browning reactions and growth of spoilage organisms. Low O levels of 3-5% are used to slow down respiration rate in fruits and vegetables. In the case of red meat, however, high levels of O (∼80%) are used to reduce oxidation of myoglobin and maintain an attractive bright red color of the meat. Meat color enhancement is not required for pork, poultry and cooked meats; therefore, a higher concentration of CO is used to extend the shelf life. Levels higher than 10% of CO are phytotoxic for fruit and vegetables, so CO is maintained below this level. N is mostly used as a filler gas to prevent pack collapse. In addition, it is also used to prevent oxidative rancidity in packaged products such as snack foods by displacing atmospheric air, especially oxygen, therefore extending shelf life. The use of noble gases such as Helium (He), Argon (Ar) and Xenon (Xe) to replace N as the balancing gas in MAP can also be used to preserve and extend the shelf life of fresh and minimally processed fruits and vegetables. Their beneficial effects are due to their higher solubility and diffusivity in water, making them more effective in displacing O from cellular sites and enzymatic O receptors. \n\nThere has been a debate regarding the use of carbon monoxide (CO) in the packaging of red meat due to its possible toxic effect on packaging workers. Its use results in a more stable red color of carboxymyoglobin in meat, which leads to another concern that it can mask evidence of spoilage in the product.\n\nLow O and high CO concentrations in packages are effective in limiting the growth of Gram negative bacteria, molds and aerobic microorganisms, such as \"Pseudomonas\" spp. High O combined with high CO could have bacteriostatic and bactericidal effects by suppression of aerobes by high CO and anaerobes by high O. CO has the ability to penetrate bacterial membrane and affect intracellular pH. Therefore, lag phase and generation time of spoilage microorganisms are increased resulting in shelf life extension of refrigerated foods. Since the growth of spoilage microorganisms are suppressed by MAP, the ability of the pathogens to grow is potentially increased. Microorganisms that can survive under low oxygen environment such as \"Campylobacter jejuni\", \"Clostridium botulinum\", \"E. coli\", \"Salmonella\", \"Listeria\" and \"Aeromonas hydophila\" are of major concern for MA packaged products. Products may appear organoleptically acceptable due to the delayed growth of the spoilage microorganisms but might contain harmful pathogens. This risk can be minimized by use of additional hurdles such as temperature control (maintain temperature below 3 degrees C), lowering water activity (less than 0.92), reducing pH (below 4.5) or addition or preservatives such as nitrite to delay metabolic activity and growth of pathogens.\n\nFlexible films are commonly used for products such as fresh produce, meats, fish and bread seeing as they provide suitable permeability for gases and water vapor to reach the desired atmosphere. Pre-formed trays are formed and sent to the food packaging facility where they are filled. The package headspace then undergoes modification and sealing. Pre-formed trays are usually more flexible and allow for a broader range of sizes as opposed to thermoformed packaging materials as different tray sizes and colors can be handled without the risk of damaging the package. Thermoformed packaging however is received in the food packaging facility as a roll of sheets. Each sheet is subjected to heat and pressure, and is formed at the packaging station. Following the forming, the package is filled with the product, and then sealed. The advantages that thermoformed packaging materials have over pre-formed trays are mainly cost-related: thermoformed packaging uses 30% to 50% less material, and they are transported as rolls of material. This will amount in significant reduction of manufacturing and transportation costs.\n\nWhen selecting packaging films for MAP of fruits and vegetables the main characteristics to consider are gas permeability, water vapour transmission rate, mechanical properties, transparency, type of package and sealing reliability. Traditionally used packaging films like LDPE (low-density polyethylene), PVC (polyvinyl chloride), EVA (ethylene-vinyl acetate) and OPP (oriented polypropylene) are not permeable enough for highly respiring products like fresh-cut produce, mushrooms and broccoli. As fruits and vegetables are respiring products, there is a need to transmit gases through the film. Films designed with these properties are called permeable films. Other films, called barrier films, are designed to prevent the exchange of gases and are mainly used with non-respiring products like meat and fish.\n\nMAP films developed to control the humidity level as well as the gas composition in the sealed package are beneficial for the prolonged storage of fresh fruits, vegetables and herbs that are sensitive to moisture. These films are commonly referred to as modified atmosphere/modified humidity packaging (MA/MH)films.\n\nIn using form-fill-seal packaging machines, the main function is to place the product in a flexible pouch suitable for the desired characteristics of the final product. These pouches can either be pre-formed or thermoformed. The food is introduced into the pouch, the composition of the headspace atmosphere is changed within the package; it is then heat sealed. These types of machines are typically called pillow-wrap, which horizontally or vertically form, fill and seal the product. Form-fill-seal packaging machines are usually used for large scale operations.\n\nIn contrast, chamber machines are used for batch processes. A filled pre-formed wrap is filled with the product and introduced into a cavity. The cavity is closed and vacuum is then pulled on the chamber and the modified atmosphere is inserted as desired. Sealing of the package is done through heated sealing bars, and the product is then removed. This batch process is labor intensive and thus requires a longer period of time; however, it is relatively cheaper than packaging machines which are automated.\n\nAdditionally, snorkel machines are used to modify the atmosphere within a package after the food has been filled. The product is placed in the packaging material and positioned into the machine without the need of a chamber. A nozzle, which is the snorkel, is then inserted into the packaging material. It pulls a vacuum and then flushes the modified atmosphere into the package. The nozzle is removed and the package is heat sealed. This method is suitable for bulk and large operations.\n\nMany products such as red meat, seafood, minimally processed fruits and vegetables, salads, pasta, cheese, bakery goods, poultry, cooked and cured meats, ready meals and dried foods are packaged under MA. A summary of optimal gas mixtures for MA products are shown in Table 1.\n\nTable 1. Modified Atmosphere Packaging for different food products and optimal gas mixtures\n\n"}
{"id": "6525423", "url": "https://en.wikipedia.org/wiki?curid=6525423", "title": "NAV-CO2 system", "text": "NAV-CO2 system\n\nNon-flammable Alcohol Vapor in Carbon Dioxide systems (NAV-CO2 System) were developed in Japan in the 1990s to sanitize hospitals and ambulances.\nThe NAV-CO2 uses CO (carbon dioxide) as a propellant to dispense a 58% isopropyl alcohol solution in a heated stream. The procedure uses alcohol in an atomized vapor, and reaches nooks, crannies, and crevices that may be beyond the reach of other disinfecting methods.\n\nThe use of carbon dioxide as a propellant serves to displace ambient oxygen (one of the elements needed to support combustion) and eliminates the risk of explosion. The carbon dioxide and atomized alcohol evaporate at room temperature leaving no residue.\n\n"}
{"id": "11625477", "url": "https://en.wikipedia.org/wiki?curid=11625477", "title": "Oil heater", "text": "Oil heater\n\nAn oil heater, also known as an oil-filled heater, oil-filled radiator, or column heater, is a common form of convection heater used in domestic heating. Although filled with oil, it is electrically heated and does not involve burning any oil fuel; the oil is used as a heat reservoir (buffer), not as a fuel.\n\nOil heaters consist of metal columns with cavities, inside which a heat transfer oil flows freely around the heater. A heating element at the base of the heater heats up the oil, which then flows around the cavities of the heater by convection. The oil has a relatively high specific heat capacity and high boiling point. The high specific heat capacity allows the oil to effectively transfer thermal energy from the heating element, while the oil's high boiling point allows it to remain in the liquid phase for the purpose of heating, so that the heater does not have to be a high pressure vessel.\n\nThe heating element heats the oil, which transfers heat to the metal wall through convection, through the walls via conduction, then to the surroundings via air convection and thermal radiation. The columns of oil heaters are typically constructed as thin fins, such that the surface area of the metal columns is large relative to the amount of oil and element which provides the heat. A large surface area allows more air to be in contact with the heater at any point in time, allowing for the heat to be transferred more effectively, which results in a surface temperature which is safe enough to touch. The relatively large specific heat capacity of the oil and metal parts means this type of heater takes a few minutes to heat up and cool down, providing a short-term thermal store.\n\nAlthough oil heaters are more expensive to run and provide far less spatial heating than gas heaters, they are still commonly used in bedrooms and other small-to-medium-sized enclosed areas. This is because gas heaters, especially when unflued, are not suitable for bedroom use - gas heaters cannot be used in confined spaces due to the reduced oxygen, and the emissions produced. This leaves electrically powered heaters, such as oil heaters and fan heaters, as the only alternative.\n\nSeveral efficiency metrics can be measured in regard to heaters, such as the efficiency of heating a room with a given amount of power, and the efficiency of the electrical generator which powers the heater and power loss from transporting the electricity over power lines. Measures may also consider how well a heater keeps the temperature of a space above a certain point. Such a measure would find inefficiencies in heating an already warm room. Many heaters (the majority of available models) are equipped with a thermostat to prevent this inefficient heating, which in turn reduces running costs. This feature was much more common in oil heaters than in the cheaper fan heaters until recently; thus many older oil heaters will be cheaper and more efficient to run than their contemporary fan heaters that lack the thermostat.\n\nTypical oil heaters range in power consumption/output from 300 to 2400 watts, and their length and number of columns is roughly proportional to their power rating. A 2400 watt oil heater is usually approximately 1 meter (3.3 feet) in length.\n\nAll electric resistance heaters are 100% efficient, with operating costs determined by their wattage and the length of operating time. A 500 watt heater will take twice as long to reach the same thermostat setting as a 1000 watt unit; the total consumption of electricity is the same for both.\n\nBy contrast, an electrical heat pump used for home heating typically has an efficiency well above 100%, expressed as its coefficient of performance.\n\nThe primary risk of oil heaters is that of fire and burns. In both regards they are generally more dangerous than hydronics and air conditioning, but less dangerous than electric fan heaters or bar radiators; this is due to the surface temperature of each type of heater.\n\nMost modern small heaters have some form of tilt sensor to cut power if they are knocked over or placed on an unstable surface. This can reduce the risk of fire if a heater is knocked over.\n\nFrom a safety standpoint, it is best to avoid having any object within three feet of an oil heater. Using an oil heater to dry clothes is not recommended by any modern manufacturer. There is a substantial fire risk if flammable materials such as clothing or bedding are left near a heater, especially synthetic fabrics such as polyester, which can melt or burn. Even though the surface temperature of the heater in normal operation is quite low, the extra thermal resistance of the clothing on the heater can cause its surface temperature to rise to the material's autoignition temperature. It should also be noted that some oil heaters contain strong warnings to avoid operation in damp areas (such as bathrooms or laundry rooms) because the moisture and humidity can damage components of the heater itself.\n\nOil heaters have been known to explode when their thermal fuses fail to shut them off. This can cause fire, thick black smoke, unpleasant odors, oil on walls and other surfaces, and disfiguring scalding.\n\nSome companies offer oil heaters with a fan, to increase the air flow over the heater. Since it is constantly bringing the colder air from the room into contact with the heater, that can improve the rate of heat flow from the heater into the room. The rate of heat flow from the heater into the air in contact with it is higher when there is a greater temperature difference between the heater and the air.\n"}
{"id": "758808", "url": "https://en.wikipedia.org/wiki?curid=758808", "title": "Paul Ambrose Oliver", "text": "Paul Ambrose Oliver\n\nPaul Ambrose Oliver (July 18, 1831 – May 17, 1912) was an American explosives inventor, American Civil War Union Army captain and staff officer who was appointed to the brevet grade of brigadier general and Medal of Honor recipient. He was born on the \"Louisiana\", his father's merchant ship, during one of its voyages in the English Channel. Before the Civil War, he worked as a shipping merchant.\n\nIn January 1862, Oliver joined the 12th New York Volunteer Infantry as a second lieutenant. During the war, he served as an aide to no fewer than four generals, including Daniel Butterfield, George Meade, Joseph Hooker and Gouverneur K. Warren. While he accepted a promotion to captain in April 1864, he declined further promotions. On March 8, 1865, President Abraham Lincoln nominated Oliver for appointment as a brevet brigadier general to rank from March 8, 1865 and the U.S. Senate confirmed the appointment on March 10, 1865. Oliver resigned his commission on May 6, 1865.\n\nOliver was an inventor and powder manufacturer after the Civil War.\n\nPaul Ambrose Oliver died May 17, 1912 at Laurel Run, Pennsylvania. He was buried at Green-Wood Cemetery, Brooklyn, New York.\n\nRank and organization: Captain, Company D, 12th New York Infantry. Place and date: Resaca, Ga., 15 May 1864. Entered service at: New York, N.Y. Born: 18 July 1831, at sea in the English Channel. Date of issue: 12 October 1892.\n\nWhile acting as aide assisted in preventing a disaster caused by Union troops firing into each other.\n\n\n"}
{"id": "24066379", "url": "https://en.wikipedia.org/wiki?curid=24066379", "title": "Planka.nu", "text": "Planka.nu\n\nPlanka.nu is a network of organizations in Sweden and Norway promoting tax-financed zero-fare public transport with chapters in Stockholm, Gothenburg, Skåne, Östergötland and Oslo. Planka.nu was founded in 2001 by the Swedish Anarcho-syndicalist Youth Federation in response to the increasingly expensive ticket prices in the public transport system in Stockholm. The campaign has received much attention because of the controversial methods used to promote free public transport: Planka.nu encourage people to fare-dodge in the public transport,\n\nIn 2001, the Stockholm chapter of the Swedish Anarcho-syndicalist Youth Federation () launched the campaign Planka.nu (literally, \"fare-dodge.now\") as a reaction to a rise in ticket prices by the Stockholm County Council. The domain name .nu, which belongs to the\nWith growing membership and interest, the campaign was detached from the SUF to form an independent organization. Local organizations have since started up in Gothenburg in 2003, Östergötland in 2005 and Skåne in 2009. A sister organization, \"pumm.it\" has been created in Helsinki, Finland. In November 2008, Planka.nu released Freepublictransports.com, a global forum for the free public transport movement. \n\nTicket fares in Stockholm have increased dramatically over time. By one measure—single ticket price for a journey—Stockholm has the most expensive-to-use public transport in the world, as of March 2009.\n\nIn the tradition of the Italian autonomists, Planka.nu advocate self-reduction, where the price of the service is determined by the consumer. Whereas in other cases this is done in agreement with (at least some) employees of the service provider, in the case of Planka.nu fare-dodgers are encouraged to evade fares without seeking permission. \n\nSince its conception Planka.nu has broadened its methods and now functions as a think tank with public transport, urban planning and climate change as its main areas of focus, as well as an insurance fund. The network also uses standard leftist methods, such as rallies, adbusting, and pamphlets. Together with refugee rights organization Ingen människa är illegal, the Swedish chapter of No one is illegal, Planka.nu provides tickets for immigrants living in Sweden without asylum, since a paperless immigrant might otherwise be reported to the police and deported if caught riding without a ticket.\n\nFailing to purchase a ticket or traveling using an incorrect pass is subject to a penalty fare of 1200 SEK in the public transport systems of Stockholm, Gothenburg and Östergötland. In return for a membership fee (paid monthly or biannually) members are refunded by Planka.nu if fined, through the solidarity fund \"P-kassan\". Net income from p-kassan is used in the aforementioned partnership with \"No one is illegal\" and to finance the production of flyers, stickers, folders, web pages and other information material. All members of Planka.nu work pro-bono. \n\nIn December 2009, Västtrafik accused the Gothenburg chapter of operating an unauthorized insurance.\n\nPlanka.nu and its organizers have seen substantial criticism, primarily by politicians and transportation companies in each city. The network has been reported to the police for incitement by Storstockholms Lokaltrafik (SL), Västtrafik and ÖstgötaTrafiken, who believe that Planka.nu engages in illegal activities. As of today, no charges have been made. Planka.nu themselves believe that their activities do not conflict with existing legislation.\n\nMany critics believe that fare evasion leads to reduced funds and higher costs for public transport, which must be offset by higher prices for those who do pay their fares.\n\nIn 2004, the Stockholm University Student Union received media attention after first permitting a meeting of the network on its premises and later barring Planka.nu when criticism was voiced.\n\nPlanka.nu has released four larger policy papers: \"Highway to Hell?\" criticizing the plans for the express beltway Förbifart Stockholm around Stockholm, \"Travel Doesn't Have to Cost the Earth\" proposing a set of measures to make the transport sector in Stockholm more climate-friendly, \"Till varje pris?\" (\"At any cost?\") a report about the costs of ticketing in the public transport system in Stockholm, and \"Trafikmaktsordningen\" (\"The Trafic Power Structure\") regarding the hierarchies of transportation and how it relates to social mobility and class.\n\n\n"}
{"id": "544400", "url": "https://en.wikipedia.org/wiki?curid=544400", "title": "Polybius square", "text": "Polybius square\n\nIn cryptography, the Polybius square, also known as the Polybius checkerboard, is a device invented by the Ancient Greeks Cleoxenus and Democleitus, and perfected by the Ancient Greek historian and scholar Polybius, for fractionating plaintext characters so that they can be represented by a smaller set of symbols.\n\nThe original square used the Greek alphabet, but it can be used with any other alphabet. In fact, it has also been used with Japanese hiragana (see cryptography in Japan). With the modern English alphabet, this is the typical form:\n\nEach letter is then represented by its coordinates in the grid. For example, \"BAT\" becomes \"12 11 44\". Because 26 characters do not quite fit in a square, it is rounded down to the next lowest square number by combining two letters (usually C and K or sometimes I and J). (Polybius had no such problem because the Greek alphabet has 24 letters.) Alternatively, the ten digits could be added, and 36 characters would be put into a 6 × 6 grid.\n\nSuch a larger grid might also be used for the Cyrillic alphabet (the most common variant has 33 letters, but some have fewer and some have up to 37).\n\nPolybius did not originally conceive of his device as a cipher so much as an aid to telegraphy; he suggested the symbols could be signalled by holding up pairs of sets of torches. It has also been used, in the form of the \"knock code\" to signal messages between cells in prisons by tapping the numbers on pipes or walls. \nIt is said to have been used by nihilist prisoners of the Russian Czars and also by US prisoners of war during the Vietnam War.\n\nArthur Koestler describes the code being used by political prisoners of Stalin in the 1930s in his anti-totalitarian novel \"Darkness at Noon\". (Koestler had been a prisoner-of-war during the Spanish Civil War.) \nIndeed, it can be signalled in many simple ways (flashing lamps, blasts of sound, drums, smoke signals) and is much easier to learn than more sophisticated codes like the Morse code. However, it is also somewhat less efficient than more complex codes.\n\nThe simple representation also lends itself to steganography. The figures from one to five can be indicated by knots in a string, stitches on a quilt, contiguous letters before a wider space or many other ways.\n\nThe Polybius cipher can be used with a keyword like the Playfair cipher. By itself the Polybius square is not terribly secure, even if used with a mixed alphabet. The pairs of digits, taken together, just form a simple substitution in which the symbols happen to be pairs of digits. In this sense it is just another encoding which can be cracked with simple frequency analysis.\nHowever a Polybius square offers the possibility of fractionation, leading toward Claude E. Shannon's confusion and diffusion. As such, it is a useful component in several ciphers such as the ADFGVX cipher, the Nihilist cipher, and the bifid cipher.\n\n"}
{"id": "5071884", "url": "https://en.wikipedia.org/wiki?curid=5071884", "title": "Potluck", "text": "Potluck\n\nA potluck is a communal gathering where each guest or group contributes a different, often homemade dish of food to be shared.\n\nOther names for a \"potluck\" include: potluck dinner, spread, Jacob's join, Jacob's supper, faith supper, covered-dish-supper, dish party, bring-and-share, shared lunch, pitch-in, bring-a-plate, dish-to-pass, fuddle, fellowship meal, and carry-in.\n\nWhile there exists some disagreement as to its origin, two principal theories exist: the combination of the English \"pot\" and \"luck\" or the North American indigenous communal meal \"potlatch\".\n\nThe word \"pot-luck\" appears in the 16th century English work of Thomas Nashe, and used to mean \"food provided for an unexpected or uninvited guest, the luck of the pot.\" The modern execution of a \"communal meal, where guests bring their own food,\" most likely originated in the 1930s during the Depression . \n\nThe alternative origin of the word is associated with a tradition common to the Tlingit and other indigenous peoples of the Pacific Northwest, called a potlatch, and is considered by opponents of this theory to be an eggcorn or malapropism.\n\nPotluck dinners are events where the attendees bring a dish to a meal. Potluck dinners are often organized by religious or community groups, since they simplify the meal planning and distribute the costs among the participants. Smaller, more informal get-togethers with distributed food preparation may also be called potlucks. The only traditional rule is that each dish be large enough to be shared among a good portion (but not necessarily all) of the anticipated guests. In some cases each participant agrees ahead of time to bring a single course, and the result is a multi-course meal. Guests may bring in any form of food, ranging from the main course to desserts. In the United States, potlucks are associated with crockpot dishes, casseroles (often called hot dishes in the upper Midwest), dessert bars, and jello salads. Traditionally, potlucks were a simple combination of dishes brought together by event attendees without a general theme. Recent times have seen the growth of themed dinners for parties or special occasions.\n\n\n"}
{"id": "10985546", "url": "https://en.wikipedia.org/wiki?curid=10985546", "title": "RCA tape cartridge", "text": "RCA tape cartridge\n\nThe RCA tape cartridge (also known as the \"Magazine Loading Cartridge\") is a magnetic tape audio format that was designed to offer stereo quarter-inch reel-to-reel tape recording quality in a convenient format for the consumer market. It was introduced in 1958, following four years of development. This timing coincided with the launch of the stereophonic phonograph record. It was introduced to the market by RCA in 1958.\n\nThe main advantage of the RCA tape cartridge over reel-to-reel machines is convenience. The user is not required to handle unruly tape ends and thread the tape through the machine before use, making the medium of magnetic tape more friendly to casual users. In addition, since the cartridge carries both supply and take-up reels, the cartridge does not have to be rewound before the tape is removed from the machine and stored. Because of these conveniences, the RCA tape cartridge system did see some success in schools, particularly in student language learning labs.\n\nThe same design concept would later be used in the more successful Compact Cassette, introduced by Philips in 1962. \nSimilar to the Compact Cassette, the RCA cartridges are reversible so that either side can be played. An auto-reverse mechanism in some models allows the tape to run continuously. Equal to 8-track tape and Stereo-Pak, the tape runs at a standard speed of 3.75 inches per second (IPS). This is double the speed of the Compact Cassette and half of the top speed of consumer reel-to-reel tape recorders, which usually offer both 3.75 IPS and 7.5 IPS speeds. Such consumer reel-to-reel machines are capable of superior audio performance, but only at the faster speed.\n\nThe RCA tape cartridge format offers four discrete audio tracks that provide a typical playtime of 30 minutes of stereo sound per side, or double that for monophonic sound. Some models can also play and record at 1.875 IPS, doubling playing time with a significant reduction in sound quality. This speed was of too low quality for music on these machines, but was acceptable for voice recording.\n\nWith two interleaved stereo pairs, the track format and speed of the RCA tape cartridge is the same as that of consumer reel-to-reel stereo tape recorders run at 3.75 IPS. It is possible to dismantle the cartridge, spool the tape onto an open reel, and play it on such a machine. In fact, RCA offered an adapter for their Sound Tape Cartridge machines to enable them to both play back and record traditional reels of tape up to 5 inches in reel diameter.\n\nUnlike the Compact Cassette, the RCA tape cartridge incorporates a brake to prevent the tape hubs from moving when the cartridge is not in a player. Small slot windows extend from the tape hubs toward the outside of the cartridge so that the amount of tape visible on each spool can be seen.\n\nDespite its convenience the RCA tape cartridge was not much of a success. RCA was slow to produce machines for the home market. They were also slow to license prerecorded music tapes for home playback. Cost was also an issue, with a single cartridge costing US$4.50 in 1960 ($ with inflation today) compared to a 1,200 foot (365 m) reel of tape, which cost $3.50 ($ today). The format disappeared from retail stores by 1964.\n\nThe physical tape width and speed of the tape and even the size of the RCA tape cartridge is similar to, though incompatible with, Sony's Elcaset system, introduced in 1976. That system also failed to achieve much market acceptance and was soon withdrawn.\n\n"}
{"id": "27274102", "url": "https://en.wikipedia.org/wiki?curid=27274102", "title": "Relief well", "text": "Relief well\n\nIn the natural gas and petroleum industry, a relief well is drilled to intersect an oil or gas well that has experienced a blowout. Specialized liquid, such as heavy (dense) drilling mud followed by cement, can then be pumped down the relief well in order to stop the flow from the reservoir in the damaged well.\n\nIn flood control, a different type of relief well is used adjacent to earthen levees to relieve the pressure on the lake or river side of the levee and thus to prevent the collapse of the levee. The greater flow of water in the water source, typically during a flood, creates a pressure gradient such that more water infiltrates the soil of the levee. Water may then flow through the soil towards the dry side of the levee, resulting in sand boil, liquefaction of the soil, and ultimately destruction of the levee. Relief wells act like valves to relieve the water pressure and allow excess water to be diverted safely, for example, to a canal. Relief wells can prevent sand boils from occurring by relieving the water pressure as described.\n\nThe first use of a relief well was in Texas in the mid-1930s when one was drilled to pump water into an oil well that had cratered and caught on fire.\n\n"}
{"id": "15715514", "url": "https://en.wikipedia.org/wiki?curid=15715514", "title": "Resende Nuclear Fuel Factory", "text": "Resende Nuclear Fuel Factory\n\nThe Nuclear Fuel Factory (FCN) is located near Resende, state of Rio de Janeiro, comprising three units, and has a production capacity of 280 tons of uranium per year. At present, FCN was modernized and produces at the Components and Assembly Unit the fuel rods and fuel elements needed for Brazilian nuclear reactors. The Reconversion and Pellets Production Unit is operating since 1999 with a capacity of 160 tons of UO2 pellets/year. The UO2 reconversion line uses the AUC process. The Nuclear Fuel Factory also produces other fuel element components, such as top and bottom nozzles, spacer grids and end plugs for export demands. Previously, Brazil supplied the uranium, which is transported to Canada where it’s converted into hexafluoride gas, and then to the United Kingdom for enrichment before it returns to Brazil for fabrication into fuel elements.\n\nThe unit has an in-house nuclear safety program and an external one, for environmental monitoring. A nuclear accounting system, internally implemented, required by the Brazilian Nuclear Energy Commission (CNEN) and supervised by the International Atomic Energy Agency (IAEA) continuously performs a balance of the material in processing with a precision reaching tenths of milligrams.\n\nIn 2018 a seventh cascade of centrifuges was inaugurated, increasing enrichment capacity by 25%. This gives the plant the capacity to produce about 50% of the annual fuel requirements of Angra Nuclear Power Plant unit 1. The new cascade is the first part of a ten cascade expansion plan.\n\nBy late 2003 the International Atomic Energy Agency (IAEA) was negotiating with the Brazilian Government to ensure that the new uranium enrichment facility, due to begin operating in 2005, was properly safeguarded.\n\nIn April 2004 the Brazilian Government denied access for the IAEA inspectors to the uranium enrichment facility being built in Resende. The plant, scheduled to begin operation in October 2004, remains subject to IAEA inspections aimed at making sure it is not used for producing weapons-grade material. In February and March 2004 Brazil refused to let IAEA inspectors see equipment in the plant, citing a need to protect proprietary information. The IAEA had dispatched inspectors to Resende who found significant portions of the facility and its contents shielded from view. Walls had been built and coverings are draped over equipment.\n\nBy November 2004 the IAEA was able to reach an agreement in principle with the Brazilian government on a safeguards approach to verify the enrichment facilities in Brazil, at the Resende facility. This approach would enable the IAEA to do credible inspections but at the same time take care of Brazil's need to protect certain commercial sensitivity inside the facility.\n\n"}
{"id": "1971934", "url": "https://en.wikipedia.org/wiki?curid=1971934", "title": "Roborior", "text": "Roborior\n\nRoborior is a robot manufactured by the robotics company Tmsuk and marketed by Sanyo. It is used both for lighting and guarding homes. Roborior is roughly the size of a watermelon and can produce different hues of color ranging from blue, purple, and orange. The Roborior is also equipped with a digital video camera that can stream live video directly to the owner's cell phone if it detects an intruder. The Roborior can be controlled remotely with a hand set, much like a Remote control vehicle, as well. It was introduced in Japan in late 2005 and was priced at 280,000 Japanese yen. The name is a portmanteau of robot and interior.\n\n"}
{"id": "43173625", "url": "https://en.wikipedia.org/wiki?curid=43173625", "title": "Routing in cellular networks", "text": "Routing in cellular networks\n\nNetwork routing in a cellular network deals with the challenges of traditional telephony such as switching and call setup. \n\nMost cellular network routing issues in different cells can be attributed to the multiple access methods used for transmission. The location of each mobile phone must be known to reuse a given band of frequencies in different cells and forms space-division multiple access (SDMA).\n\nFDMA is one of the multiple access methods used in cellular networks. 50 MHz blocks of communication channel are assigned, which lie in radio frequency range and contain an equal number of uplinks (terminal to base station) and downlinks (base station to terminal). One or more bidirectional channels are carried by 10-90 band pairs. The digital networks additionally make use of either CDMA or TDMA methods.\n\nA special service called mobility management provides this application. Terminals (handsets) can move from one place to another during the call and therefore requires the call to be handed over from one channel to another. Soft handover uses the same frequency channel. The same terminals can operate in the same area covered by different service providers, which is known as roaming.\n\n"}
{"id": "15734342", "url": "https://en.wikipedia.org/wiki?curid=15734342", "title": "Single buoy mooring", "text": "Single buoy mooring\n\nA Single buoy mooring (SrM) (also known as single-point mooring or SPM) is a loading buoy anchored offshore, that serves as a mooring point and interconnect for tankers loading or offloading gas or liquid products. SPMs are the link between geostatic subsea manifold connections and weathervaning tankers. They are capable of handling any tonnage ship, even very large crude carriers (VLCC) where no alternative facility is available.\n\nIn shallow water SPMs are used to load and unload crude oil and refined products from inshore and offshore oilfields or refineries, usually through some form of storage system. These buoys are usually suitable for use by all types of oil tanker. In deep water oil fields, SPMs are usually used to load crude oil direct from the production platforms, where there are economic reasons not to run a pipeline to the shore. These moorings usually supply to dedicated tankers which can moor without assistance.\n\nSeveral types of single point mooring are in use.\n\nA commonly used configuration is the catenary anchor leg mooring (CALM), which can be capable of handling very large crude carriers. This configuration uses six or eight heavy anchor chains placed radially around the buoy, of a tonnage to suit the designed load, each about long, and attached to an anchor or pile to provide the required holding power. The anchor chains are pre-tensioned to ensure that the buoy is held in position above the PLEM. As the load from the tanker is applied, the heavy chains on the far side straighten and lift off the seabed to apply the balancing load. Under full design load there is still some of chain lying on the bottom. The flexible hose riser may be in one of three basic configurations, all designed to accommodate tidal depth variation and lateral displacement due to mooring loads. In all cases the hose curvature changes to accommodate lateral and vertical movement of the buoy, and the hoses are supported at near neutral buoyancy by floats along the length. These are:\n\nLess commonly used configurations include:\n\nThere are four groups of parts in the total mooring system: the body of the buoy, mooring and anchoring elements, product transfer system and other components.\n\nThe buoy body may be supported on static legs attached to the seabed, with a rotating part above water level connected to the (off)loading tanker. The two sections are linked by a roller bearing, referred to as the \"main bearing\". Alternatively the buoy body may be held in place by multiple radiating anchor chains. The moored tanker can freely weather vane around the buoy and find a stable position due to this arrangement.\n\nMoorings fix the buoy to the sea bed. Buoy design must account for the behaviour of the buoy given applicable wind, wave and current conditions and tanker tonnages. This determines the optimum mooring arrangement and size of the various mooring leg components. Anchoring points are greatly dependent on local soil condition.\n\n\nA tanker is moored to a buoy by means of a hawser arrangement. Oil Companies International Marine Forum (OCIMF) standards are available for mooring systems.\n\nThe hawser arrangement usually consist of nylon rope, which is shackled to an integrated mooring uni-joint on the buoy deck. At the tanker end of the hawser, a chafe chain is connected to prevent damage from the tanker fairlead. A load pin can be applied to the mooring uni-joint on the buoy deck to measure hawser loads.\n\nHawser systems use either one or two ropes depending on the largest tonnage of vessel which would be moored to the buoy. The ropes would either be single-leg or grommet leg type ropes. These are usually connected to an OCIMF chafe chain on the export tanker side (either type A or B depending on the maximum tonnage of the tanker and the mooring loads). This chafe chain would then be held in the chain stopper on board the export tanker.\n\nA basic hawser system would consist of the following (working from the buoy outwards):\n\nBuoy-side shackle and bridle assembly for connection to the padeye on the buoy;\nMooring hawser shackle;\nMooring hawser;\nChafe chain assembly;\nSupport buoy;\nPick-up / messenger lines;\nMarker buoy for retrieval from the water.\n\nUnder OCIMF recommendations, the hawser arrangement would normally be purchased as a full assembly from a manufacturer.\n\nThe heart of each buoy is the product transfer system. From a geostatic location, e.g. a pipeline end manifold (PLEM) located on the seabed, this system transfers products to the offtake tanker.\n\nThe basic product transfer system components are:\n\nThe risers are flexible hoses that connect the subsea piping to the buoy. Configuration of these risers can vary depending on water depth, sea conditions, buoy motions, etc.\n\nFloating hose string(s) connect the buoy to the offloading tanker. The hose string can be equipped with a breakaway coupling to prevent rupture of hoses/hawser and subsequent oil spills.\n\nThe product swivel is the connection between the geostatic and the rotating parts of the buoy. The swivel enables an offloading tanker to rotate with respect to the mooring buoy. Product swivels range in size depending on the capacity of attached piping and risers. Product swivels can provide one or several independent paths for fluids, gases, electrical signals or power. Swivels are equipped with a multiple seal arrangement to minimise the possibility of leakage of product into the environment.\n\nOther possible components of SPMs are:\n\n"}
{"id": "43196817", "url": "https://en.wikipedia.org/wiki?curid=43196817", "title": "SpeechSchool.TV", "text": "SpeechSchool.TV\n\nSpeechSchool.TV is an online speech training service based in New Zealand and the United Kingdom. It provides a subscription based Internet TV channel to provide a Standard English accent training service to students worldwide. It was founded in 2009 for the purpose of teaching the English accent to foreign students and others who are interested in improving their accent. It has won the \"Technium Challenge award\" for New Zealand.\n\nThe company provides two different courses in speech training; \"The Master Speaker English Accent programme\" which is focused on accent reduction to help develop clear English speaking skills without mumbling or stuttering, and the other is \"The Master Communicator programme\" to help develop presentation, confidence and public speaking skills.\n"}
{"id": "16955480", "url": "https://en.wikipedia.org/wiki?curid=16955480", "title": "Stormwater detention vault", "text": "Stormwater detention vault\n\nA stormwater detention vault is an underground structure designed to manage excess stormwater runoff on a developed site, often in an urban setting. This type of best management practice may be selected when there is insufficient space on the site to infiltrate the runoff or build a surface facility such as a detention basin or retention basin.\n\nDetention vaults manage stormwater \"quantity\" flowing to nearby surface waters. They help prevent flooding and can reduce erosion in rivers and streams. They do not provide treatment to improve water quality, though some are attached to a media filter bank to remove pollutants.\n\nUnderground stormwater detention allows for high volume storage of runoff in a small footprint area. The storage vessels can be made from a variety of materials, including corrugated metal pipe, aluminum, steel, plastic, fiberglass, pre-cast or poured-in-place concrete.\n\nThe vault is typically buried under a parking lot or other open land on the site. In the latter case, this underground vault may be preferable to a surface detention pond if other uses are intended for the land (e.g. a pedestrian plaza or park). In other situations, a vault is used because installing a pond might pose other problems, such as attracting unwanted waterfowl or other animals. In some sites, a vault may be installed in the basement of a building, such as a parking garage. Tunnels may be bored to serve as detention vaults. Tunnels may be cheaper than basins, as they do not require pumps to move the water.\n\nThe outlet is generally a restricted-flow drain from the detention vessel, with a weir for containing detritus. Detention vessels delay water's delivery downstream, and possibly creates a later water level peak post-rainfall. It is important to consider timing of water release and the types of reservoirs feeding a waterway.\n\n"}
{"id": "56199829", "url": "https://en.wikipedia.org/wiki?curid=56199829", "title": "Technology mining", "text": "Technology mining\n\nTech mining or technology mining refers to applying text mining methods to technical documents. For patent analysis purposes, it is named ‘patent mining’. Porter, as one of the pioneers in technology mining, defined ‘tech mining’ in his book as follows: “the application of text mining tools to science and technology information, informed by understanding of technological innovation processes.” Therefore, tech mining has two significant characteristics: 1) using ‘text mining tools’, 2) applying these tools to ‘technology management’. Also, technology mining can be considered as one of technology intelligence branches.\n\nTechnology mining have many applications including R&D portfolio selection, R&D project initiation, new product development, strategic technology planning, technology roadmapping, etc. Tech miner should communicate closely with target users what technological issue they have, and how they want to address the issues. The number of published papers and the number of citations in technology mining area illustrates a hyperbolically progress; there is a jump in the number of publications after 2005 and a huge rise in the number of citations after 2012.\n\nGlobal TechMining Conference\n\nTech Mining website\n\nVantagePoint\n\nVantagePoint Institute website\n"}
{"id": "54576556", "url": "https://en.wikipedia.org/wiki?curid=54576556", "title": "The Art of Shaving", "text": "The Art of Shaving\n\nThe Art of Shaving is a United States retail business of high-end men's shaving and skin care accessories. The first store was founded by Eric Malka and Myriam Zaoui in Manhattan in 1996.\n\nThe store was successful and opened a second shop on Madison Avenue. The brand developed a line of natural shaving products. The company operates 17 stores across the United States. 10 new stores were opened in 2007.\n\nThe company entered a partnership with razor company Gillette with some The Art of Shaving franchises owned by Gillette. \n\nProcter & Gamble purchased The Art of Shaving in 2009.\n"}
{"id": "22898551", "url": "https://en.wikipedia.org/wiki?curid=22898551", "title": "Thermodynamics of nanostructures", "text": "Thermodynamics of nanostructures\n\nAs the devices continue to shrink further into the sub-100 nm range following the trend predicted by Moore’s law, the topic of thermal properties and transport in such nanoscale devices becomes increasingly important. Display of great potential by nanostructures for thermoelectric applications also motivates the studies of thermal transport in such devices. These fields, however, generate two contradictory demands: high thermal conductivity to deal with heating issues in sub-100 nm devices and low thermal conductivity for thermoelectric applications. These issues can be addressed with phonon engineering, once nanoscale thermal behaviors have been studied and understood.\n\nIn general two carrier types can contribute to thermal conductivity - electrons and phonons. In nanostructures phonons usually dominate and the phonon properties of the structure become of a particular importance for thermal conductivity. These phonon properties include: phonon group velocity, phonon scattering mechanisms, heat capacity, Grüneisen parameter. Unlike bulk materials, nanoscale devices have thermal properties which are complicated by boundary effects due to small size. It has been shown that in some cases phonon-boundary scattering effects dominate the thermal conduction processes, reducing thermal conductivity.\n\nDepending on the nanostructure size, the phonon mean free path values (Λ) may be comparable or larger than the object size, formula_1. When formula_1 is larger than the phonon mean free path, Umklapp scattering process limits thermal conductivity (regime of diffusive thermal conductivity). When formula_1 is comparable to or smaller than the mean free path (which is of the order 1 µm for carbon nanostructures), the continuous energy model used for bulk materials no longer applies and nonlocal and nonequilibrium aspects to heat transfer also need to be considered. In this case phonons in defectless structure could propagate without scattering and thermal conductivity becomes ballistic (similar to ballistic conductivity). More severe changes in thermal behavior are observed when the feature size formula_1 shrinks further down to the wavelength of phonons.\n\nThe first measurement of thermal conductivity in silicon nanowires was published in 2003. Two important features were pointed out: 1) The measured thermal conductivities are significantly lower than that of the bulk Si and, as the wire diameter is decreased, the corresponding thermal conductivity is reduced. 2) As the wire diameter is reduced, the phonon boundary scattering dominates over phonon–phonon Umklapp scattering, which decreases the thermal conductivity with an increase in temperature.\n\nFor 56 nm and 115 nm wires \"k ~ T\" dependence was observed, while for 37 nm wire \"k ~ T\" dependence and for 22 nm wire \"k ~ T\" dependence were observed. Chen \"et al.\" has shown that the one-dimensional cross-over for 20 nm Si nanowire occurs around 8K, while the phenomenon was observed for temperature values greater than 20K. Therefore, the reason of such behaviour is not in the confinement experienced by phonons so that three-dimensional structures display two-dimensional or one-dimensional behavior.\n\nAssuming that Boltzmann transport equation is valid, thermal conductivity can be written as:\n\nwhere C is the heat capacity, v is the group velocity and formula_6 is the relaxation time. Note that this assumption breaks down when the dimensions of the system are comparable to or smaller than the wavelength of the phonons responsible for thermal transport. In our case, phonon wavelengths are generally in the 1 nm range and the nanowires under consideration are within tens of nanometers range, the assumption is valid.\n\nDifferent phonon mode contributions to heat conduction can be extracted from analysis of the experimental data for silicon nanowires of different diameters to extract the \"C·v\" product for analysis. It was shown that all phonon modes contributing to thermal transport are excited well below the Si Debye temperature (645 K).\n\nFrom the thermal conductivity equation, one can write the product \"C·v\" for each isotropic phonon branch \"i\".\n\nwhere formula_8 and formula_9 is the phonon phase velocity, which is less sensitive to phonon dispersions than the group velocity \"v\".\n\nMany models of phonon thermal transport ignores the effects of transverse acoustic phonons (TA) at high frequency due to their small group velocity. (Optical phonon contributions are also ignored for the same reason.) However, upper branch of TA phonons have non-zero group velocity at the Brillouin zone boundary along the Γ-Κ direction and, in fact, behave similarly to the longitudinal acoustic phonons (LA) and can contribute to the heat transport.\n\nThen, the possible phonon modes contributing to heat conduction are both LA and TA phonons at low and high frequencies. Using the corresponding dispersion curves, the \"C·v\" product can then be calculated and fitted to the experimental data. The best fit was found when contribution of high-frequency TA phonons is accounted as 70% of the product at room temperature. The remaining 30% is contributed by the LA and TA phonons at low-frequency.\n\nThermal conductivity in nanowires can be computed based on complete phonon dispersions instead of the linearlized dispersion relations commonly used to calculate thermal conductivity in bulk materials.\n\nAssuming the phonon transport is diffusive and Boltzmann transport equation (BTE) is valid, nanowire thermal conductance \"G(T)\" can be defined as:\n\nwhere the variable α represents discrete quantum numbers associated with sub-bands found in one-dimensional phonon dispersion relations, \"f\" represents the Bose-Einstein distribution, \"v\" is the phonon velocity in the \"z\" direction and λ is the phonon relaxation length along the direction of the wire length.\nThermal conductivity is then expressed as:\n\nwhere \"S\" is the cross sectional area of the wire, \"a\" is the lattice constant.\n\nIt was shown that, using this formula and atomistically computed phonon dispersions (with interatomic potentials developed in ), it is possible to predictively calculate lattice thermal conductivity curves for nanowires, in good agreement with experiments. On the other hand, it was not possible to obtain correct results with the approximated Callaway formula. These results are expected to apply to ”nanowhiskers” for which phonon confinement effects are unimportant. Si nanowires wider than ~35 nm are within this category.\n\nFor large diameter nanowires, theoretical models assuming the nanowire diameters are comparable to the mean free path and that the mean free path is independent of phonon frequency have been able to closely match the experimental results. But for very thin nanowires whose dimensions are comparable to the dominant phonon wavelength, a new model is required. The study in has shown that in such cases, the phonon-boundary scattering is dependent on frequency. The new mean free path is then should be used:\n\nHere, \"l\" is the mean free path (same as Λ). The parameter \"h\" is length scale associated with the disordered region, \"d\" is the diameter, \"N(ω)\" is number of modes at frequency ω, and \"B\" is a constant related to the disorder region.\n\nThermal conductance is then calculated using the Landauer formula:\n\nAs nanoscale graphitic structures, carbon nanotubes are of great interest for their thermal properties. The low-temperature specific heat and thermal conductivity show direct evidence of 1-D quantization of the phonon band structure. Modeling of the low-temperature specific heat allows determination of the on-tube phonon velocity, the splitting of phonon subbands on a single tube, and the interaction between neighboring tubes in a bundle.\n\nMeasurements show a single-wall carbon nanotubes (SWNTs) room-temperature thermal conductivity about 3500 W/(m·K), and over 3000 W/(m·K) for individual multiwalled carbon nanotubes (MWNTs). It is difficult to replicate these properties on the macroscale due to imperfect contact between individual CNTs, and so tangible objects from CNTs such as films or fibres have reached only up to 1500 W/(m·K) so far. Addition of nanotubes to epoxy resin can double the thermal conductivity for a loading of only 1%, showing that nanotube composite materials may be useful for thermal management applications.\n\nThermal conductivity in CNT is mainly due to phonons rather than electrons so the Wiedemann–Franz law is not applicable.\n\nIn general, the thermal conductivity is a tensor quality, but for this discussion, it is only important to consider the diagonal elements:\n\nwhere C is the specific heat, and \"v\" and formula_6 are the group velocity and relaxation time of a given phonon state.\n\nAt low temperatures (T is far less than Debye temperature), the relaxation time is determined by scattering of fixed impurities, defects, sample boundaries, etc. and is roughly constant. Therefore, in ordinary materials, the low-temperature thermal conductivity has the same temperature dependence as the specific heat. However, in anisotropic materials, this relationship does not strictly hold. Because the contribution of each state is weighted by the scattering time and the square of the velocity, the thermal conductivity preferentially samples states with large velocity and scattering time. For instance, in graphite, the thermal conductivity parallel to the basal planes is only weakly dependent on the interlayer phonons. In SWNT bundles, it is likely that \"k(T)\" depends only on the on-tube phonons, rather than the intertube modes.\n\nThermal conductivity is of particular interest in low-dimensional systems. For CNT, represented as 1-D ballistic electronic channel, the electronic conductance is quantized, with a universal value of\n\nSimilarly, for a single ballistic 1-D channel, the thermal conductance is independent of materials parameters, and there exists a quantum of thermal conductance, which is linear in temperature:\n\nPossible conditions for observation of this quantum were examined by Rego and Kirczenow. In 1999, Keith Schwab, Erik Henriksen, John Worlock, and Michael Roukes carried out a series of experimental measurements that enabled first observation of the thermal conductance quantum. The measurements employed suspended nanostructures coupled to sensitive dc SQUID measurement devices. In 2008, a colorized electron micrograph of one of the Caltech devices was acquired for the permanent collection of the Museum of Modern Art in New York.\n\nAt high temperatures, three-phonon Umklapp scattering begins to limit the phonon relaxation time. Therefore, the phonon thermal conductivity displays a peak and decreases with increasing temperature. Umklapp scattering requires production of a phonon beyond the Brillouin zone boundary; because of the high Debye temperature of diamond and graphite, the peak in the thermal conductivity of these materials is near 100 K, significantly higher than for most other materials. In less crystalline forms of graphite, such as carbon fibers, the peak in \"k(T)\" occurs at higher temperatures, because defect scattering remains dominant over Umklapp scattering to higher temperature. In low-dimensional systems, it is difficult to conserve both energy and momentum for Umklapp processes, and so it may be possible that Umklapp scattering is suppressed in nanotubes relative to 2-D or 3-D forms of carbon.\n\nBerber \"et al.\" have calculated the phonon thermal conductivity of isolated nanotubes. The value \"k(T)\" peaks near 100 K, and then decreases with increasing temperature. The value of \"k(T)\" at the peak (37,000 W/(m·K)) is comparable to the highest thermal conductivity ever measured (41,000 W/(m·K) for an isotopically pure diamond sample at 104 K). Even at room temperature, the thermal conductivity is quite high (6600 W/(m·K)), exceeding the reported room-temperature thermal conductivity of isotopically pure diamond by almost a factor of 2.\n\nIn graphite, the interlayer interactions quench the thermal conductivity by nearly 1 order of magnitude . It is likely that the same process occurs in nanotube bundles . Thus it is significant that the coupling between tubes in bundles is weaker than expected . It may be that this weak coupling, which is problematic for mechanical applications of nanotubes, is an advantage for thermal applications.\n\nThe phonon density of states is to calculated through band structure of isolated nanotubes, which is studied in Saito \"et al.\"\nand Sanchez-Portal \"et al.\"\nWhen a graphene sheet is ‘‘rolled’’ into a nanotube, the 2-D band structure folds into a large number of 1-D subbands. In a (10,10) tube, for instance, the six phonon bands (three acoustic and three optical) of the graphene sheet become 66 separate 1-D subbands. A direct result of this folding is that the nanotube density of states has a number of sharp peaks due to 1-D van Hove singularities, which are absent in graphene and graphite. Despite the presence of these singularities, the overall density of states is similar at high energies, so that the high temperature specific heat should be roughly equal as well. This is to be expected: the high-energy phonons are more reflective of carbon–carbon bonding than the geometry of the graphene sheet.\n\nThin films are prevalent in the micro and nanoelectronics industry for the fabrication of sensors, actuators and transistors; thus, thermal transport properties affect the performance and reliability of many structures such as transistors, solid-state lasers, sensors, and actuators. Although these devices are traditionally made from bulk crystalline material (silicon), they often contain thin films of oxides, polysilicon, metal, as well as superlattices such as thin-film stacks of GaAs/AlGaAs for lasers.\n\nSilicon-on-insulator (SOI) films with silicon thicknesses of 0.05 µm to 10 µm above a buried silicon dioxide layer are increasingly popular for semiconductor devices due to the increased dielectric isolation associated with SOI/ SOI wafers contain a thin-layer of silicon on an oxide layer and a thin-film of single-crystal silicon, which reduces the effective thermal conductivity of the material by up to 50% as compared to bulk silicon, due to phonon-interface scattering and defects and dislocations in the crystalline structure. Previous studies by Asheghi \"et al.\", show a similar trend. Other studies of thin-films show similar thermal effects .\n\nThermal properties associated with superlattices are critical in the development of semiconductor lasers. Heat conduction of superlattices is less understood than homogeneous thin films. It is theorized that superlattices have a lower thermal conductivity due to impurities from lattice mismatches and at the heterojunctions. Phonon-interface scattering at heterojunctions needs to be considered in this case; fully elastic scattering underestimates the heat conduction, while fully inelastic scattering overestimates the heat conduction. For example, a Si/Ge thin-film superlattice has a greater decrease in thermal conductivity than an AlAs/GaAs film stack due to increased lattice mismatch. A simple estimate of heat conduction of superlattices is:\n\nwhere \"C\" and \"C\" are the corresponding heat capacity of film1 and film2 respectively, \"v\" and \"v\" are the acoustic propagation velocities in film1 and film2, and \"d1\" and \"d2\" are the thicknesses of film1 and film2. This model neglects scattering within the layers and assumes fully diffuse, inelastic scattering.\n\nPolycrystalline films are common in semiconductor devices, as the gate electrode of a field-effect transistor is often made of polycrystalline silicon. If the polysilicon grain sizes are small, internal scattering from grain boundaries can overwhelm the effects of film-boundary scattering. Also, grain boundaries contain more impurities, which result in impurity scattering. Likewise, disordered or amorphous films will experience a severe reduction of thermal conductivity, since the small grain size results in numerous grain-boundary scattering effects. Different deposition methods of amorphous films will result in differences in impurities and grain sizes.\n\nThe simplest approach to modeling phonon scattering at grain boundaries is to increase the scattering rate by introducing this equation:\n\nwhere B is a dimensionless parameter that correlates with the phonon reflection coefficient at the grain boundaries, \"d\" is the characteristic grain size, and \"v\" is the phonon velocity through the material. A more formal approach to estimating the scattering rate is:\n\nwhere \"v\" is the dimensionless grain-boundary scattering strength, defined as\n\nHere formula_22 is the cross-section of a grain-boundary area, and \"ν\" is the density of the grain boundary area.\n\nThere are two approaches to experimentally determine the thermal conductivity of thin films. The goal of experimental metrology of thermal conductivity of thin films is to attain an accurate thermal measurement without disturbing the properties of the thin-film.\n\nElectrical heating is used for thin films which have a lower thermal conductivity than the substrate; it is fairly accurate in measuring out-of-plane conductivity. Often, a resistive heater and thermistor is fabricated on the sample film using a highly conductive metal, such as aluminium. The most straightforward approach would be to apply a steady-state current and measure the change in temperature of adjacent thermistors. A more versatile approach uses an AC signal applied to the electrodes. The third harmonic of the AC signal reveals heating and temperature fluctuations of the material.\n\nLaser heating is a non-contact metrology method, which uses picosecond and nanosecond laser pulses to deliver thermal energy to the substrate. Laser heating uses a pump-probe mechanism; the pump beam introduces energy to the thin-film, as the probe beam picks up the characteristics of how the energy propagates through the film. Laser heating is advantageous because the energy delivered to the film can be precisely controlled; furthermore, the short heating duration decouples the thermal conductivity of the thin film from the substrate .\n"}
{"id": "40779484", "url": "https://en.wikipedia.org/wiki?curid=40779484", "title": "Transit Wireless", "text": "Transit Wireless\n\nTransit Wireless is an American telecommunication company founded in 2005, based in New York City. It specializes in building wireless communication infrastructure using distributed antenna system networks to provide Wi-Fi and cellular phone coverage in the places that are unreachable by traditional cellular phone services such as in the underground portions of the New York City Subway. In 2010, the company was injected with financial support from infrastructure company BAI Communications for its first project with the New York City Transit Authority, which consisted of adding wireless access to subway stations. The company is now a subsidiary of BAI Communications.\n"}
{"id": "27748997", "url": "https://en.wikipedia.org/wiki?curid=27748997", "title": "Tree spade", "text": "Tree spade\n\nA tree spade is a specialized machine that mechanizes the transplanting of large plants whose hand-powered transplanting (using traditional spades, wagons, and other equipment) would be prohibitively laborious. These include large bushes and small or medium trees. By bringing mechanized power to what was formerly only a manual process, tree spades do for transplanting what tractors and combine harvesters do for agriculture, and what excavators and other heavy equipment do for construction. Today, tree spades are widely used in the tree nursery industry to increase production rates, and in the landscaping industry for tree removal and transplanting.\n\nA typical machine consists of a number of blades (generally 3 or 4, but single or dual blade designs also exist) that encircle the tree, digging into the ground and then lifting the entire tree, including its roots and soil (in what is termed a \"root ball\"), out of the ground and replanting or transplanting the whole tree in the designated area. Similar machines can also be used for placing trees in pots or baskets for commercial tree nursery operations. Tree spades are available in a wide variety of sizes and designs that are made to suit varied soil conditions and customer requirements. Typical design variations can include straight vs curved blades, root ball diameter, root ball depth, and root ball cone angle.\n\nThe machines can be mounted on a tractor or loader for moving trees a short distance, or on the back of a truck for relocating trees over the highway a long distance away. For tree nursery operations, tree spades are also sometimes mounted on excavators, though this is less common due to the weight capacity requirements of manipulating the machine along with an excavated tree. For transport on public roads, the spade tilts the tree to a near-horizontal position in order to clear overhead bridges and cables.\n\n\n"}
{"id": "37099647", "url": "https://en.wikipedia.org/wiki?curid=37099647", "title": "University of Montenegro Faculty of Biotechnology", "text": "University of Montenegro Faculty of Biotechnology\n\nThe University of Montenegro Biotechnical Faculty (Montenegrin: Biotehnički fakultet Univerziteta Crne Gore \"Биотехнички факултет Универзитета Црне Горе\") is one of the educational institutions of the University of Montenegro. The building is located in Podgorica, at the University campus.\n\nScientific research in agriculture in Montenegro was established in 1937, when the Ministry of Agriculture of the Kingdom of Yugoslavia decided to establish the experimental State Research Station for Southern Cultures in Bar. After World War II, the founding of new institutions and renewal of previously existing ones followed, such as: \n\nThe Agricultural Institute was established in 1961, as a result of merging the above-mentioned institutions. It functioned under that name until 1997, when it was transformed into the Biotechnical Institute by including the \"Forestry Sector\" into one unique scientific research institution. By establishing the Studies of Agriculture, in 2005, the Institute developed into a faculty, and in 2008 formally changed its name into the Biotechnical Faculty.\n\nThe Faculty of Biotechnology has adequately equipped classrooms and laboratories situated in the Faculty's buildings in Podgorica, Bar and Bijelo Polje, as well as experimental plots for a part of students’ professional practice and organization of Faculty production.\n\nThe undergraduate academic studies are divided in two study groups:\n\nThe specialist academic studies at the Faculty have four study groups:\n\nThe master studies of Plant production are organized on the following three groups:\n"}
{"id": "42019836", "url": "https://en.wikipedia.org/wiki?curid=42019836", "title": "Varenye", "text": "Varenye\n\nVarenye or varenie (, , ) is a traditional whole-fruit preserve, widespread in Eastern Europe (Russia, Ukraine, Belarus), as well as the Baltic region (, , ). It is made by cooking berries, other fruits, or more rarely nuts, vegetables, or flowers, in sugar syrup. In some traditional recipes, other sweeteners such as honey or treacle are used instead of or in addition to sugar.\n\nVarenye is similar to jam except the fruits are not macerated, and no gelling agent is added. It is characterized by a thick but transparent syrup having the natural colour of the fruits.\n\nThe making of varenye requires a careful balance between cooking, or sometimes steeping in the hot sugar mixture for just enough time to allow the flavor to be extracted from the fruit, and sugar to penetrate the fruit, and cooking too long that fruit will break down and liquefy. Some fruits with tough skins require cooking for many hours, while others are suitable for making \"five-minute varenye\" (, \"varenye-pyatiminutka\"). For the latter, dry sugar is spread over raw fruit in layers and left for several hours to steep into the fruit. The resulting mixture is then heated for just about five minutes.\n\nThe most popular types of varenye are made from locally available berries and fruits, such as sour cherries, strawberries, raspberries, apricots and apples. In general, virtually any kind of fruit, as well as some culinary vegetables, nuts, pine cones, and rose petals are used.\n\nVarenye is enjoyed as a dessert and condiment, in particular as a topping for pancakes (bliny, oladyi, syrniki), as a filling for pies (pirogi and pirozhki), dumplings (vareniki), cakes and cookies, and as a sweetener for tea. It is also used as a spread on bread, though due to its liquid consistency it is not well suited for that. Finally, it is eaten on its own as a sweet.\n\n\"Varenye\" is an old Slavic word which is used in East Slavic languages in a more general sense to refer to any type of sweet fruit preserve. The word has common etymological roots with the words denoting cooking, boiling, brewing, or stewing (, , ).\n\nIn literary translations, especially of children's books, into Russian, the term is often used to replace less-common loanwords, such as jam, confiture or marmalade. Examples are the translations of Alice's Adventures in Wonderland, Harry Potter, The Adventures of Tom Sawyer, and the animated movies about Karlsson-on-the-roof.\n\nThe same is true when translating \"from\" Russian. For instance, the making of raspberry varenie is described in Leo Tolstoy's novel \"Anna Karenina\" (VI-2). In her classic translation, Constance Garnett refers to the activity as \"jam-making\".\n\nIn the popular Soviet children's book \"A tale about a war secret, about the boy Nipper-Pipper, and his word of honour\" by Arkady Gaidar the antihero Little Baddun betrays his friends for \"a barrel of varenye and a basket of biscuits\" (; again, in English translation \"jam\" is used instead of varenye). This phrase became an idiomatic expression for betrayal or selling out in Russian, similar to thirty pieces of silver.\n\nThe more general usage of the term varenye in Russian includes a number of related local specialties.\n\nIn the preparation of \"raw varenye\" (, \"syroye varenye\") the heating is omitted completely. The recipes usually include grating of raw berries or other fruits and mixing them with sugar.\n\nIn pre-revolutionary Russia, \"dry varenye\" (, \"sukhoe varenye\") referred to a local variety of candied fruits (fruit confit) obtained by extracting fruits from varenye syrup and drying them. Kiev (today the capital of Ukraine) was particularly famous for this delicacy.\n\nSimilar confections are also made in Transcaucasia and in some regions of Central and South Asia, where they are called murabba.\n"}
{"id": "53759905", "url": "https://en.wikipedia.org/wiki?curid=53759905", "title": "Williams spray equation", "text": "Williams spray equation\n\nIn combustion, the Williams spray equation, also known as the Williams–Boltzmann equation, describes the statistical evolution of sprays contained in another fluid, analogous to the Boltzmann equation for the molecules, named after Forman A. Williams, who derived the equation in 1958.\n\nThe sprays are assumed to be spherical with radius formula_1, even though the assumption is valid for solid particles(liquid droplets) when their shape has no consequence on the combustion. For liquid droplets to be nearly spherical, the spray has to be dilute(total volume occupied by the sprays is much less than the volume of the gas) and the Weber number formula_2, where formula_3 is the gas density, formula_4 is the spray droplet velocity, formula_5 is the gas velocity and formula_6 is the surface tension of the liquid spray, should be formula_7.\n\nThe equation is described by a number density function formula_8, which represents the probable number of spray particles (droplets) of chemical species formula_9 (of formula_10 total species), that one can find with radii between formula_1 and formula_12, located in the spatial range between formula_13 and formula_14, traveling with a velocity in between formula_4 and formula_16, having the temperature in between formula_17 and formula_18 at time formula_19. Then the spray equation for the evolution of this density function is given by\n\nwhere\n\nThis model for the rocket motor was developed by Probert, Williams and Tanasawa. It is reasonable to neglect formula_31, for distances not very close to the spray atomizer, where major portion of combustion occurs. Consider a one-dimensional liquid-propellent rocket motor situated at formula_32, where fuel is sprayed. Neglecting formula_33(density function is defined without the temperature so accordingly dimensions of formula_34 changes) and due to the fact that the mean flow is parallel to formula_35 axis, the steady spray equation reduces to\n\nwhere formula_37 is the velocity in formula_35 direction. Integrating with respect to the velocity results\n\nThe contribution from the last term (spray acceleration term) becomes zero (using Divergence theorem) since formula_40 when formula_41 is very large, which is typically the case in rocket motors. The drop size rate formula_42 is well modeled using vaporization mechanisms as\n\nwhere formula_44 is independent of formula_1, but can depend on the surrounding gas. Defining the number of droplets per unit volume per unit radius and average quantities averaged over velocities,\n\nthe equation becomes\n\nIf further assumed that formula_48 is independent of formula_1, and with a transformed coordinate\n\nformula_50\n\nIf the combustion chamber has varying cross-section area formula_51, a known function for formula_52 and with area formula_53 at the spraying location, then the solution is given by\n\nwhere formula_55 are the number distribution and mean velocity at formula_32 respectively.\n\n"}
