{"id": "299813", "url": "https://en.wikipedia.org/wiki?curid=299813", "title": "Acoustical engineering", "text": "Acoustical engineering\n\nAcoustical engineering (also known as acoustic engineering) is the branch of engineering dealing with sound and vibration. It is the application of acoustics, the science of sound and vibration, in technology. Acoustical engineers are typically concerned with the design, analysis and control of sound.\n\nOne goal of acoustical engineering can be the reduction of unwanted noise, which is referred to as noise control. Unwanted noise can have significant impacts on animal and human health and well-being, reduce attainment by pupils in schools, and cause hearing loss. Noise control principles are implemented into technology and design in a variety of ways, including control by redesigning sound sources, the design of noise barriers, sound absorbers, suppressors, and buffer zones, and the use of hearing protection (earmuffs or earplugs).\n\nBut acoustical engineering is not just about noise control; it also covers positive uses of sound, from the use of ultrasound in medicine to the programming of digital sound synthesizers, and from designing a concert hall to enhance the sound of an orchestra to specifying a railway station's sound system so announcements are intelligible.\n\nAcoustic engineers usually possess a bachelor's degree or higher qualification in acoustics, physics or another engineering discipline. Practicing as an acoustic engineer usually requires a bachelor's degree with significant scientific and mathematical content. Acoustic engineers might work in acoustic consultancy, specializing in particular fields, such as architectural acoustics, environmental noise or vibration control. In other industries, acoustic engineers might: design automobile sound systems; investigate human response to sounds, such as urban soundscapes and domestic appliances; develop audio signal processing software for mixing desks, and design loudspeakers and microphones for mobile phones. Acousticians are also involved in researching and understanding sound scientifically. Some positions, such as faculty require a Doctor of Philosophy.\n\nIn most countries, a degree in acoustics can represent the first step towards professional certification and the degree program may be certified by a professional body. After completing a certified degree program the engineer must satisfy a range of requirements before being certified. Once certified, the engineer is designated the title of Chartered Engineer (in most Commonwealth countries).\n\nThe listed subdisciplines are loosely based on the PACS (Physics and Astronomy Classification Scheme) coding used by the Acoustical Society of America.\n\nAeroacoustics is concerned with how noise is generated by the movement of air, for instance via turbulence, and how sound propagates through the fluid air. Aeroacoustics plays an important role in understanding how noise is generated by aircraft and wind turbines, as well as exploring how wind musical instruments work.\n\nAudio signal processing is the electronic manipulation of audio signals using analog and digital signal processing.\n\nAudio signal processing is done for a variety of reasons such as:\n\nAudio engineers develop and use audio signal processing algorithms.\n\nArchitectural acoustics (also known as building acoustics) is the science and engineering of achieving a good sound within a building. Architectural acoustics can be about achieving good speech intelligibility in a theatre, restaurant or railway station, enhancing the quality of music in a concert hall or recording studio, or suppressing noise to make offices and homes more productive and pleasant places to work and live. Architectural acoustic design is usually done by acoustic consultants.\n\nBioacoustics usually concerns the scientific study of sound production and hearing in animals. It can include: acoustic communication and associated animal behaviour and evolution of species; how sound is produced by animals; the auditory mechanisms and neurophysiology of animals; the use of sound to monitor animal populations, and the effect of man-made noise on animals.\n\nThis branch of acoustic engineering deals with the design of headphones, microphones, loudspeakers, sound systems, sound reproduction and recording. There has been a rapid increase in the use of portable electronic devices which can reproduce sound and rely on electroacoustic engineering, e.g. mobile phones, portable media players, and tablet computers.\n\nEnvironmental acoustics is concerned with the control of noise and vibrations caused by traffic, aircraft, industrial equipment, recreational activities and anything else that might be considered a nuisance. Acoustical engineers concerned with environmental acoustics face the challenge of measuring or predicting likely noise levels, determining an acceptable level for that noise, and determining how the noise can be controlled. Environmental acoustics work is usually done by acoustic consultants or those working in environmental health. Recent research work has put a strong emphasis on soundscapes, the positive use of sound (e.g. fountains, bird song), and the preservation of tranquility.\n\nMusical acoustics is concerned with researching and describing the physics of music and its perception – how sounds employed as music work. This includes: the function and design of musical instruments including electronic synthesizers; the human voice (the physics and neurophysiology of singing); computer analysis of music and composition; the clinical use of music in music therapy, and the perception and cognition of music.\n\nNoise control is a set of strategies to reduce noise pollution by reducing noise at its source, by inhibiting sound propagation using noise barriers or similar, or by the use of ear protection (earmuffs or earplugs). Control at the source is the most cost-effective way of providing noise control. Noise control engineering applied to cars and trucks is known as noise, vibration, and harshness (NVH). Other techniques to reduce product noise include vibration isolation, application of acoustic absorbent and acoustic enclosures. Acoustical engineering can go beyond noise control to look at what is the best sound for a product, for instance, manipulating the sound of door closures on automobiles.\n\nPsychoacoustics tries to explain how humans respond to what they hear, whether that is an annoying noise or beautiful music. In many branches of acoustic engineering, a human listener is a final arbitrator as to whether a design is successful, for instance, whether sound localisation works in a surround sound system. \"Psychoacoustics seeks to reconcile acoustical stimuli and all the scientific, objective, and physical properties that surround them, with the physiological and psychological responses evoked by them.\"\n\nSpeech is a major area of study for acoustical engineering, including the production, processing and perception of speech. This can include physics, physiology, psychology, audio signal processing and linguistics. Speech recognition and speech synthesis are two important aspects of the machine processing of speech. Ensuring speech is transmitted intelligibly, efficiently and with high quality; in rooms, through public address systems and through telephone systems are other important areas of study.\n\nUltrasonics deals with sound waves in solids, liquids and gases at frequencies too high to be heard by the average person. Specialists areas include medical ultrasonics (including medical ultrasonography), sonochemistry, nondestructive testing, material characterisation and underwater acoustics (sonar).\n\nUnderwater acoustics is the scientific study of sound in water. It is concerned with both natural and man-made sound and its generation underwater; how it propagates, and the perception of the sound by animals. Applications include sonar to locate submerged objects such as submarines, underwater communication by animals, observation of sea temperatures for climate change monitoring, and marine biology.\n\nAcoustic engineers working on vibration study the motions and interactions of mechanical systems with their environments, including measurement, analysis and control. This might include: ground vibrations from railways and construction; vibration isolation to reduce noise getting into recording studios; studying the effects of vibration on humans (vibration white finger); vibration control to protect a bridge from earthquakes, or modelling the propagation of structure-borne sound through buildings.\n\nAlthough the way in which sound interacts with its surroundings is often extremely complex, there are a few ideal sound wave behaviours that are fundamental to understanding acoustical design. Complex sound wave behaviors include absorption, reverberation, diffraction, and refraction. Absorption is the loss of energy that occurs when a sound wave reflects off of a surface. Just as light waves reflect off of surfaces, sound waves also reflect off of surfaces, and every reflection results in a loss of energy. Absorption refers both to the sound that transmits through and the energy that is dissipated by a material. Reverberation is the persistence of sound that is caused by repeated boundary reflections after the source of the sound stops. This principle is particularly important in enclosed spaces. In addition to reflecting off of surfaces, sound waves also bend around surfaces in the path of the waves. This bending is known as diffraction. Refraction is another kind of sound wave bending. This type of bending, however, is caused by changes in the medium through which the wave is passing and not the presence of obstacles in the path of a sound wave. Temperature gradients, for example, cause bending in sound waves. Acoustical engineers apply these fundamental concepts, along with complex mathematical analysis, to control sound for a variety of applications.\n\n\n\n"}
{"id": "1356031", "url": "https://en.wikipedia.org/wiki?curid=1356031", "title": "Agribusiness", "text": "Agribusiness\n\nAgribusiness is the business of agricultural production. The term was coined in 1957 by Goldberg and Davis. It includes agrichemicals, breeding, crop production (farming and contract farming), distribution, farm machinery, processing, and seed supply, as well as marketing and retail sales. All agents of the food and fiber value chain and those institutions that influence it are part of the agribusiness system.\n\nWithin the agriculture industry, \"agribusiness\" is used simply as a portmanteau of agriculture and business, referring to the range of activities and disciplines encompassed by modern food production. There are academic degrees in and departments of agribusiness, agribusiness trade associations, agribusiness publications, and so forth, worldwide. \n\nIn the context of agribusiness management in academia, each individual element of agriculture production and distribution may be described as agribusinesses. However, the term \"agribusiness\" most often emphasizes the \"interdependence\" of these various sectors within the production chain.\n\nAmong critics of large-scale, industrialized, vertically integrated food production, the term \"agribusiness\" is used negatively, synonymous with \"corporate farming\". As such, it is often contrasted with smaller family-owned farms. Although recent trends have seen Agribusiness firms with an ethical drive, such as Caspian Farming who's goal is to push towards low impact farming and sustainable development.\n\nExamples of agribusinesses include seed and agrichemical producers like Dow AgroSciences, DuPont, Monsanto, and Syngenta; AB Agri (part of Associated British Foods) animal feeds, biofuels, and micro-ingredients, ADM, grain transport and processing; John Deere, farm machinery producer; Ocean Spray, farmer's cooperative; and Purina Farms, agritourism farm.\n\nAs concern over global warming intensifies, biofuels derived from crops are gaining increased public and scientific attention. This is driven by factors such as oil price spikes, the need for increased energy security, concern over greenhouse gas emissions from fossil fuels, and support from government subsidies. In Europe and in the US, increased research and production of biofuels have been mandated by law.\n\nStudies of agribusiness often come from the academic fields of agricultural economics and management studies, sometimes called agribusiness management. To promote more development of food economies, many government agencies support the research and publication of economic studies and reports exploring agribusiness and agribusiness practices. Some of these studies are on foods produced for export and are derived from agencies focused on food exports. These agencies include the Foreign Agricultural Service (FAS) of the U.S. Department of Agriculture, Agriculture and Agri-Food Canada (AAFC), Austrade, and New Zealand Trade and Enterprise (NZTE). \n\nThe Federation of International Trade Associations publishes studies and reports by FAS and AAFC, as well as other non-governmental organizations on its website.\n\nRay A. Goldberg coined the term agribusiness together with coauthor John H. Davis. They provided a rigorous economic framework for the field in their book A Concept of Agribusiness (Boston: Division of Research, Graduate School of Business Administration, Harvard University, 1957). That seminal work traces a complex value-added chain that begins with the farmer's purchase of seed and livestock and ends with a product fit for the consumer's table. Agribusiness boundary expansion is driven by a variety of transaction costs.\n\nManuel Alvarado Ledesma (CEMA University, Argentina) and Peter D. Goldsmith (University of Illinois) explain the implications of weak institutions on agribusiness investment. According to them weak institutions lead to policy development and enforcement grounded in the moment, rather than based on precedent and deliberative processes over time.\n\n\n"}
{"id": "27278390", "url": "https://en.wikipedia.org/wiki?curid=27278390", "title": "Alternate lighting of surfaces", "text": "Alternate lighting of surfaces\n\nAlternate lighting of surfaces (ALiS) is type of plasma display technology jointly developed by Fujitsu and Hitachi in 1999. Alternate lighting of surfaces uses an interlaced scanning method rather than a progressive one. This technique allows native lower resolution plasma display panels to display at higher resolutions. This technique also helps in prolonging panel life and power consumption reductions. \n"}
{"id": "9458114", "url": "https://en.wikipedia.org/wiki?curid=9458114", "title": "Australian Information Industry Association", "text": "Australian Information Industry Association\n\nThe Australian Information Industry Association (AIIA) is the peak body representing the information and communications technology (ICT) industry in Australia.\n\nAIIA’s membership captures all sectors of the ICT industry, ranging from hardware and software services to multinational companies and local small-to-medium-sized enterprises. \n\nAIIA sets the strategic direction of the ICT industry, influences public policy, engages industry stakeholders and provides member companies with business productivity tools, advisory services and market intelligence.\n\nThe AIIA has been chaired by John Grant.\n\n"}
{"id": "5158441", "url": "https://en.wikipedia.org/wiki?curid=5158441", "title": "Automatic milking", "text": "Automatic milking\n\nAutomatic milking is the milking of dairy animals, especially of dairy cattle, without human labour. Automatic milking systems (AMS), also called voluntary milking systems (VMS), were developed in the late 20th century. They have been commercially available since the early 1990s. The core of such systems that allows complete automation of the milking process is a type of agricultural robot. Automated milking is therefore also called robotic milking. Common systems rely on the use of computers and special herd management software.\n\nThe milking process is the collection of tasks specifically devoted to extracting milk from an animal (rather than the broader field of dairy animal husbandry). This process may be broken down into several sub-tasks: collecting animals before milking, routing animals into the parlour, inspection and cleaning of teats, attachment of milking equipment to teats, and often massaging the back of the udder to relieve any held back milk, extraction of milk, removal of milking equipment, routing of animals out of the parlour.\n\nMaintaining milk yield during the lactation period (approximately 300 days) requires consistent milking intervals, usually twice daily and with maximum time spacing between milkings. In fact all activities must be scheduled around the milking process on the dairy farm. Such a milking routine imposes restrictions on time management and personal life of an individual farmer, as the farmer is committed to milking in the early morning and in the evening for seven days a week regardless of personal health, family responsibilities or social schedule. This time restriction is exacerbated for lone farmers and farm families if extra labour cannot easily or economically be obtained, and is a factor in the decline in small-scale dairy farming. Techniques such as once-a-day milking and voluntary milking (see below) have been investigated to reduce these time constraints.\n\nTo alleviate the labour involved in milking, much of the milking process has been automated during the 20th century: many farmers use semi-automatic or automatic cow traffic control (powered gates, etc.), the milking machine (a basic form was developed in the late 19th century) has entirely automated milk extraction, and automatic cluster removal is available to remove milking equipment after milking. Automatic teat spraying systems are available, however, there is some debate over the cleaning effectiveness of these.\n\nThe final manual labour tasks remaining in the milking process were cleaning and inspection of teats and attachment of milking equipment (milking cups) to teats. Automatic cleaning and attachment of milking cups is a complex task, requiring accurate detection of teat position and a dextrous mechanical manipulator. These tasks have been automated successfully in the voluntary milking system (VMS), or automatic milking system (AMS).\n\nSince the 1970s, much research effort has been expended in investigating methods to alleviate time management constraints in conventional dairy farming, culminating in the development of the automated voluntary milking system. There is a video of the historical development of the milking robot at Silsoe Research Institute.\n\nVoluntary milking allows the cow to decide her own milking time and interval, rather than being milked as part of a group at set milking times. AMS requires complete automation of the milking process as the cow may elect to be milked at any time during a 24-hour period.\n\nThe milking unit comprises a milking machine, a teat position sensor (usually a laser), a robotic arm for automatic teat-cup application and removal, and a gate system for controlling cow traffic. The cows may be permanently housed in a barn, and spend most of their time resting or feeding in the free-stall area. If cows are to be grazed as well, using a selection gate to allow only those cows that have been milked to the outside pastures has been advised by some AMS manufacturers.\n\nWhen the cow elects to enter the milking unit (due to highly palatable feed that she finds in the milking box), a cow ID sensor reads an identification tag (transponder) on the cow and passes the cow ID to the control system. If the cow has been milked too recently, the automatic gate system sends the cow out of the unit. If the cow may be milked, automatic teat cleaning, milking cup application, milking, and teatspraying takes place. As an incentive to attend the milking unit, concentrated feedstuffs needs to be fed to the cow in the milking unit.\nThe barn may be arranged such that access to the main feeding area can only be obtained by passing the milking unit. This layout is referred to as \"forced cow traffic\". Alternatively, the barn may be set up such that the cow always has access to feed, water, and a comfortable place to lie down, and is only motivated to visit the milking system by the palatable feed available there. This is referred to as \"free cow traffic\".\n\nThe innovative core of the AMS system is the robotic manipulator in the milking unit. This robotic arm automates the tasks of teat cleaning and milking attachment and removes the final elements of manual labour from the milking process. Careful design of the robot arm and associated sensors and controls allows robust unsupervised performance, such that the farmer is only required to attend the cows for condition inspection and when a cow has not attended for milking.\n\nTypical capacity for an AMS is 50–70 cows per milking unit. AMS usually achieve milking frequencies between 2 and 3 times per day, so a single milking unit handling 60 cows and milking each cow 3 times per day has a capacity of 7.5 cows per hour. This low capacity is convenient for lower-cost design of the robot arm and associated control system, as a window of several minutes is available for each cow and high-speed operation is not required.\n\nAMS units have been available commercially since the early 1990s, and have proved relatively successful in implementing the voluntary milking method. Many of the research and developments have taken place in the Netherlands. The most farms with AMS are located in the Netherlands, and Denmark.\n\nA new variation on the theme of robotic milking includes a similar robotic arm system, but coupled with a rotary platform, improving the number of cows that can be handled per robot arm. A mobile variation of robotic milking, adapted to tie-stall configuration (stanchion barns), is used in Canada. In this configuration, the AMS travels in the centre isle of the barn approaching cows from behind to milk them in their stalls. \n\n\n\n\n\n"}
{"id": "54464581", "url": "https://en.wikipedia.org/wiki?curid=54464581", "title": "Awei", "text": "Awei\n\nShenzhen Yale Electronics Co., known by its brand name Awei, is a Chinese manufacturer of headphones and portable speakers.\n"}
{"id": "7731824", "url": "https://en.wikipedia.org/wiki?curid=7731824", "title": "Centre for Ultrahigh Bandwidth Devices for Optical Systems", "text": "Centre for Ultrahigh Bandwidth Devices for Optical Systems\n\nThe Centre for Ultrahigh bandwidth Devices for Optical Systems (or CUDOS) is a collaboration of Australian and international researchers in optical science and photonics technology. CUDOS is an Australian Research Council Centre of Excellence and was formally launched in 2003.\n\nCUDOS commenced operation in 2003 as one of the first Australian Research Council Centres of Excellence, and began with 4 research programs across 7 groups. \n\nIt continued through 2007 when ARC renewed funding for another 3 years. \n\nThe latest incarnation is based on new ARC funding from 2011-2017, with Australian and international researchers collaborating on 6 new Flagship projects. \n\nCUDOS is a research consortium between 8 groups at 6 Australian Universities: The University of Sydney (CUDOS headquarters), Australian National University, Macquarie University, University of Technology Sydney, RMIT University and Monash University.\n\nThe Research Director is Professor Ben Eggleton, with Professor Yuri Kivshar as Deputy Director and Professor Martijn de Sterke as Associate Director.\n\nCUDOS aims to be the world-leader in research in on-chip photonics, for all-optical signal processing. The centre conducts research to create a world-best on-chip photonic platform for information transfer and processing technologies. CUDOS aims to translate the intellectual capital, which the researchers create to build a community of professionals which can drive wealth creation in Australia.\n\nCUDOS brings together a powerful team of Australian and international researchers in optical science and photonics technology, and has played a pivotal role in demonstrating ground-breaking integrated photonic signal processors that can massively increase the information capacity of the Internet.\n\nThe centre currently has six Flagship Projects.\n\nFunctional Metamaterials and Metadevices: Metamaterials are synthesised on the sub-wavelength scale to have optical properties (refractive index, dispersion) that can differ dramatically from those of bulk materials: perfect lenses, cloaking, and negative refractive index materials are examples. CUDOS aims to develop metamaterials that will enable entirely new ways to control photons.\n\nOn-chip Nanoplasmonics: The refractive index of metals is very high, so the wavelength of the optical modes is very short. CUDOS is developing novel techniques to fabricate nano-structured composites of metals and optically transmitting materials. They are investigating novel modes of light propagation in these materials and use them to create ultracompact devices like transmission lines and antennae. The vision of this project is to develop three- and two-dimensional nanoplasmonic structures that can allow unprecedented control of light at the sub wavelength scale.\n\nHybrid integration: As metamaterials, nanoplasmonic materials and new kinds of nonlinear optical materials are developed, they need to be integrated with existing optical platforms of silicon or chalcogenide, so that light can pass from one material to another on the same ‘chip’. This project aims to develop novel designs for integrating such hybrid materials, and novel fabrication techniques.\n\nMid-Infrared Photonics: The mid-infrared region of the spectrum (3 – 10 µm) has enormous potential for highly efficient sensing of molecules significant in agriculture, natural resource management, homeland security, and others. CUDOS developed photonic platforms and novel sources for this region.\n\nNonlinear Quantum Photonics: This research focuses on highly compact approaches based on nonlinear optics to generate single photons. The aim is to achieve this on a chip and create a fully integrated flexible platform to generate these single photons and use them to perform quantum-based processing operations.\n\nTerabit per second Photonics: All-optical processing has the potential to replace electronics in many areas of ultrahigh bandwidth communications systems. CUDOS is developing all-optical processors, using nonlinear optics, and investigating new approaches to enable much higher volumes of data to be carried per unit of optical bandwidth.\n\nCUDOS research has been working towards optical circuitry which could result in much faster speeds for data transmission.\n\n"}
{"id": "718833", "url": "https://en.wikipedia.org/wiki?curid=718833", "title": "Chute (gravity)", "text": "Chute (gravity)\n\nA chute is a vertical or inclined plane, channel, or passage through which objects are moved by means of gravity.\n\nA chute, also known as a race, flume, cat, or river canyon, is a steep-sided passage through which water flows rapidly.\n\nAkin to these, man-made chutes, such as the timber slide and log flume, were used in the logging industry to facilitate the downstream transportation of timber along rivers. These are no longer in common use. Man-made chutes may also be a feature of spillways on some dams. Some types of water supply and irrigation systems are gravity fed, hence chutes. These include aqueducts, puquios, and acequias.\n\nChutes are in common use in tall buildings to allow the rapid transport of items from the upper floors to a central location on one of the lower floors or basement. Chutes may be round, square or rectangular at the top and/or the bottom.\n\n\n\nAn elevator is not a chute since it does not operate by means of gravity.\n\nGoust, a hamlet in southwestern France, is notable for its mountainside chute that is used to transport coffins.\n\nChutes are also found in:\n"}
{"id": "28685458", "url": "https://en.wikipedia.org/wiki?curid=28685458", "title": "Construction collaboration technology", "text": "Construction collaboration technology\n\nConstruction collaboration technology refers to software applications used to enable effective sharing of project-related information between geographically dispersed members of a construction project team, often through use of a web-based software as a service platform.\n\nThe terms \"construction collaboration\" and \"construction collaboration software\" were coined in Australia by Aconex in 2001. It was later adopted in 2003 in the UK when seven UK-based vendors joined together to form the Network for Construction Collaboration Technology Providers (NCCTP), to promote the benefits and use of collaborative technologies in the architecture, engineering, construction (AEC) and related industries.\n\nThe phrase was taken on in the UK as it was preferred to the then commonly used term 'project extranet' which was felt might exclude use of the platforms for multi-project programmes of work, or for post-construction collaboration - e.g.: for facilities management. It also supported progressive moves within the UK construction industry to promote more collaborative or integrated approaches following the 1994 Latham and 1998 Egan Reports. For example, Sir John Egan's follow-up report, \"Accelerating Change\" in 2002, recommended:\n\nOther descriptions such as 'construction project management' or 'construction document management' were seen as confusing or misleading, being associated more with scheduling tools (e.g.: Microsoft Project) or with generic electronic document management systems (e.g.: Documentum) that could not easily handle AEC-oriented requirements for dispersed teams.\n\nEssentially, construction collaboration technologies are deployed to support the requirements of a multi-disciplinary construction project team. This is typically drawn from multiple companies, all based in different locations with their own IT systems, and is brought together – usually temporarily – to plan, design, construct and, in some cases, to operate and maintain the resulting built asset. It is common for construction collaboration technology to be cloud based, or hosted as a centralised database. These platforms enable information to be shared and accessed in real-time by all team members.\n\nConstruction collaboration technologies replace localised sets of data held by individual team members or companies. A centralised repository or data store is created that can be accessed by all authorised team members, usually using a lowest common denominator technology: a computer equipped with an internet browser and a telecommunications link to the internet. The platforms' functionality also reflects the industry's extensive use of graphical information - most notably design drawings - and the need to be able to access, view, mark-up and comment on designs.\n\nThe core characteristics of construction collaboration technologies can be summarised as:\n\nReflecting the need to encourage take-up and active use of their platforms, the leading UK construction collaboration technology vendors all adopted a similar charging structure. Rather than charging companies per-user or per-seat licenses, the applications were typically licensed per-project, with customers paying a single subscription (typically monthly or quarterly) for the duration of the planning, design and construction process, and allowing use by all companies in the project's supply chain.\n\nThe founder members of the NCCTP were: 4Projects (Since Acquired by Viewpoint Construction Software), BIW Technologies, BuildOnline, Cadweb, Causeway Technologies and Sarcophagus. Business Collaborator and Aconex joined shortly afterwards. The NCCTP was initially managed by CIRIA before becoming a membership forum within Constructing Excellence in August 2007.\n\n"}
{"id": "41199531", "url": "https://en.wikipedia.org/wiki?curid=41199531", "title": "Current sense monitor", "text": "Current sense monitor\n\nA Current Sense Monitor is a type of monitor. It uses a high side voltage and reforms it into a proportional output current. It has a range of 20 Volts to 2.5 Volts. It is used for portable battery products.\n"}
{"id": "92943", "url": "https://en.wikipedia.org/wiki?curid=92943", "title": "Digital-to-analog converter", "text": "Digital-to-analog converter\n\nIn electronics, a digital-to-analog converter (DAC, D/A, D2A, or D-to-A) is a system that converts a digital signal into an analog signal. An analog-to-digital converter (ADC) performs the reverse function.\n\nThere are several DAC architectures; the suitability of a DAC for a particular application is determined by figures of merit including: resolution, maximum sampling frequency and others. Digital-to-analog conversion can degrade a signal, so a DAC should be specified that has insignificant errors in terms of the application. \n\nDACs are commonly used in music players to convert digital data streams into analog audio signals. They are also used in televisions and mobile phones to convert digital video data into analog video signals which connect to the screen drivers to display monochrome or color images. These two applications use DACs at opposite ends of the frequency/resolution trade-off. The audio DAC is a low-frequency, high-resolution type while the video DAC is a high-frequency low- to medium-resolution type. \n\nDue to the complexity and the need for precisely matched components, all but the most specialized DACs are implemented as integrated circuits (ICs). Discrete DACs would typically be extremely high speed low resolution power hungry types, as used in military radar systems. Very high speed test equipment, especially sampling oscilloscopes, may also use discrete DACs.\n\nA DAC converts an abstract finite-precision number (usually a fixed-point binary number) into a physical quantity (e.g., a voltage or a pressure). In particular, DACs are often used to convert finite-precision time series data to a continually varying physical signal.\n\nAn \"ideal\" DAC converts the abstract numbers into a conceptual sequence of impulses that are then processed by a reconstruction filter using some form of interpolation to fill in data between the impulses. A conventional \"practical\" DAC converts the numbers into a piecewise constant function made up of a sequence of rectangular functions that is modeled with the zero-order hold. Other DAC methods (such as those based on delta-sigma modulation) produce a pulse-density modulated output that can be similarly filtered to produce a smoothly varying signal.\n\nAs per the Nyquist–Shannon sampling theorem, a DAC can reconstruct the original signal from the sampled data provided that its bandwidth meets certain requirements (e.g., a baseband signal with bandwidth less than the Nyquist frequency). Digital sampling introduces quantization error that manifests as low-level noise in the reconstructed signal.\n\nDACs and ADCs are part of an enabling technology that has contributed greatly to the digital revolution. To illustrate, consider a typical long-distance telephone call. The caller's voice is converted into an analog electrical signal by a microphone, then the analog signal is converted to a digital stream by an ADC. The digital stream is then divided into network packets where it may be sent along with other digital data, not necessarily audio. The packets are then received at the destination, but each packet may take a completely different route and may not even arrive at the destination in the correct time order. The digital voice data is then extracted from the packets and assembled into a digital data stream. A DAC converts this back into an analog electrical signal, which drives an audio amplifier, which in turn drives a loudspeaker, which finally produces sound.\n\nMost modern audio signals are stored in digital form (for example MP3s and CDs) and, in order to be heard through speakers, they must be converted into an analog signal. DACs are therefore found in CD players, digital music players, and PC sound cards.\n\nSpecialist standalone DACs can also be found in high-end hi-fi systems. These normally take the digital output of a compatible CD player or dedicated transport (which is basically a CD player with no internal DAC) and convert the signal into an analog line-level output that can then be fed into an amplifier to drive speakers.\n\nSimilar digital-to-analog converters can be found in digital speakers such as USB speakers, and in sound cards.\n\nIn voice over IP applications, the source must first be digitized for transmission, so it undergoes conversion via an ADC, and is then reconstructed into analog using a DAC on the receiving party's end.\n\nVideo sampling tends to work on a completely different scale altogether thanks to the highly nonlinear response both of cathode ray tubes (for which the vast majority of digital video foundation work was targeted) and the human eye, using a \"gamma curve\" to provide an appearance of evenly distributed brightness steps across the display's full dynamic range - hence the need to use RAMDACs in computer video applications with deep enough colour resolution to make engineering a hardcoded value into the DAC for each output level of each channel impractical (e.g. an Atari ST or Sega Genesis would require 24 such values; a 24-bit video card would need 768...). Given this inherent distortion, it is not unusual for a television or video projector to truthfully claim a linear contrast ratio (difference between darkest and brightest output levels) of 1000:1 or greater, equivalent to 10 bits of audio precision even though it may only accept signals with 8-bit precision and use an LCD panel that only represents 6 or 7 bits per channel.\n\nVideo signals from a digital source, such as a computer, must be converted to analog form if they are to be displayed on an analog monitor. As of 2007, analog inputs were more commonly used than digital, but this changed as flat panel displays with DVI and/or HDMI connections became more widespread. A video DAC is, however, incorporated in any digital video player with analog outputs. The DAC is usually integrated with some memory (RAM), which contains conversion tables for gamma correction, contrast and brightness, to make a device called a RAMDAC.\n\nA device that is distantly related to the DAC is the digitally controlled potentiometer, used to control an analog signal digitally.\n\nA one-bit mechanical actuator assumes two positions: one when on, another when off. The motion of several one-bit actuators can be combined and weighted with a whiffletree mechanism to produce finer steps. The IBM Selectric typewriter uses such a system.\n\nThe most common types of electronic DACs are:\n\nDACs are very important to system performance. The most important characteristics of these devices are:\n\nOther measurements, such as phase distortion and jitter, can also be very important for some applications, some of which (e.g. wireless data transmission, composite video) may even \"rely\" on accurate production of phase-adjusted signals.\n\nLinear PCM audio sampling usually works on the basis of each bit of resolution being equivalent to 6 decibels of amplitude (a 2x increase in volume or precision).\n\nNon-linear PCM encodings (A-law / μ-law, ADPCM, NICAM) attempt to improve their effective dynamic ranges by a variety of methods - logarithmic step sizes between the output signal strengths represented by each data bit (trading greater quantisation distortion of loud signals for better performance of quiet signals)\n\n\n\n"}
{"id": "24436510", "url": "https://en.wikipedia.org/wiki?curid=24436510", "title": "Digital Media Sandbox Consortium", "text": "Digital Media Sandbox Consortium\n\nDigital Media Sandbox Consortium (DMSC) is a collaboration of institutions of learning and corporations, which promote competition and initiatives involving academic technology, service learning and engaged research. Founded in 2006, DMSC launched the \"DMSC Governor's Challenge Digital Media Tournament\" the following year.\n\n\nTennessee Board of Regents System\n\nUniversity of Tennessee System\n\nTennessee Private Colleges\n\nNorth Carolina\n\nVirginia\n\n"}
{"id": "274816", "url": "https://en.wikipedia.org/wiki?curid=274816", "title": "Distributed control system", "text": "Distributed control system\n\nA distributed control system (DCS) is a computerised control system for a process or plant usually with a large number of control loops, in which autonomous controllers are distributed throughout the system, but there is central operator supervisory control. This is in contrast to systems that use centralized controllers; either discrete controllers located at a central control room or within a central computer. The DCS concept increases reliability and reduces installation costs by localising control functions near the process plant, with remote monitoring and supervision. \n\nDistributed control systems first emerged in large, high value, safety critical process industries, and were attractive because the DCS manufacturer would supply both the local control level and central supervisory equipment as an integrated package, thus reducing design integration risk. Today the functionality of SCADA and DCS systems are very similar, but DCS tends to be used on large continuous process plants where high reliability and security is important, and the control room is not geographically remote. \n\nThe key attribute of a DCS is its reliability due to the distribution of the control processing around nodes in the system. This mitigates a single processor failure. If a processor fails, it will only affect one section of the plant process, as opposed to a failure of a central computer which would affect the whole process. This distribution of computing power local to the field Input/Output (I/O) connection racks also ensures fast controller processing times by removing possible network and central processing delays. \n\nThe accompanying diagram is a general model which shows functional manufacturing levels using computerised control. \n\nReferring to the diagram;\n\n\nLevels 1 and 2 are the functional levels of a traditional DCS, in which all equipment are part of an integrated system from a single manufacturer. \n\nLevels 3 and 4 are not strictly process control in the traditional sense, but where production control and scheduling takes place.\n\nThe processor nodes and operator graphical displays are connected over proprietary or industry standard networks, and network reliability is increased by dual redundancy cabling over diverse routes. This distributed topology also reduces the amount of field cabling by siting the I/O modules and their associated processors close to the process plant. \n\nThe processors receive information from input modules, process the information and decide control actions to be signalled by the output modules. The field inputs and outputs can be analog signals e.g. 4–20 mA DC current loop or 2 state signals that switch either \"on\" or \"off\", such as relay contacts or a semiconductor switch. \n\nDCSs are connected to sensors and actuators and use setpoint control to control the flow of material through the plant. A typical application is a PID controller fed by a flow meter and using a control valve as the final control element. The DCS sends the setpoint required by the process to the controller which instructs a valve to operate so that the process reaches and stays at the desired setpoint. (see 4–20 mA schematic for example).\n\nLarge oil refineries and chemical plants have several thousand I/O points and employ very large DCS. Processes are not limited to fluidic flow through pipes, however, and can also include things like paper machines and their associated quality controls, variable speed drives and motor control centers, cement kilns, mining operations, ore processing facilities, and many others. \n\nDCSs in very high reliability applications can have dual redundant processors with \"hot\" switch over on fault, to enhance the reliability of the control system. \n\nAlthough 4–20 mA has been the main field signalling standard, modern DCS systems can also support fieldbus digital protocols, such as Foundation Fieldbus, profibus, HART, Modbus, PC Link etc., and other digital communication protocols such as modbus.\n\nModern DCSs also support neural networks and fuzzy logic applications. Recent research focuses on the synthesis of optimal distributed controllers, which optimizes a certain H-infinity or the H 2 control criterion.\n\nDistributed control systems (DCS) are dedicated systems used in manufacturing processes that are continuous or batch-oriented. \n\nProcesses where a DCS might be used include:\n\n\nProcess control of large industrial plants has evolved through many stages. Initially, control would be from panels local to the process plant. However this required a large manpower resource to attend to these dispersed panels, and there was no overall view of the process. The next logical development was the transmission of all plant measurements to a permanently-manned central control room. Effectively this was the centralisation of all the localised panels, with the advantages of lower manning levels and easier overview of the process. Often the controllers were behind the control room panels, and all automatic and manual control outputs were transmitted back to plant. However, whilst providing a central control focus, this arrangement was inflexible as each control loop had its own controller hardware, and continual operator movement within the control room was required to view different parts of the process. \n\nWith the coming of electronic processors and graphic displays it became possible to replace these discrete controllers with computer-based algorithms, hosted on a network of input/output racks with their own control processors. These could be distributed around plant, and communicate with the graphic display in the control room or rooms. The distributed control system was born. \n\nThe introduction of DCSs allowed easy interconnection and re-configuration of plant controls such as cascaded loops and interlocks, and easy interfacing with other production computer systems. It enabled sophisticated alarm handling, introduced automatic event logging, removed the need for physical records such as chart recorders, allowed the control racks to be networked and thereby located locally to plant to reduce cabling runs, and provided high level overviews of plant status and production levels.\n\nEarly minicomputers were used in the control of industrial processes since the beginning of the 1960s. The IBM 1800, for example, was an early computer that had input/output hardware to gather process signals in a plant for conversion from field contact levels (for digital points) and analog signals to the digital domain.\n\nThe first industrial control computer system was built 1959 at the Texaco Port Arthur, Texas, refinery with an RW-300 of the Ramo-Wooldridge Company.\n\nIn 1975, both Honeywell and Japanese electrical engineering firm Yokogawa introduced their own independently produced DCS's - TDC 2000 and CENTUM systems, respectively. US-based Bristol also introduced their UCS 3000 universal controller in 1975. In 1978 Valmet introduced their own DCS system called Damatic (latest generation named Valmet DNA). In 1980, Bailey (now part of ABB) introduced the NETWORK 90 system, Fisher Controls (now part of Emerson Electric) introduced the PROVoX system, Fischer & Porter Company (now also part of ABB) introduced DCI-4000 (DCI stands for Distributed Control Instrumentation).\n\nThe DCS largely came about due to the increased availability of microcomputers and the proliferation of microprocessors in the world of process control. Computers had already been applied to process automation for some time in the form of both direct digital control (DDC) and setpoint control. In the early 1970s Taylor Instrument Company, (now part of ABB) developed the 1010 system, Foxboro the FOX1 system, Fisher Controls the DC system and Bailey Controls the 1055 systems. All of these were DDC applications implemented within minicomputers (DEC PDP-11, Varian Data Machines, MODCOMP etc.) and connected to proprietary Input/Output hardware. Sophisticated (for the time) continuous as well as batch control was implemented in this way. A more conservative approach was setpoint control, where process computers supervised clusters of analog process controllers. A workstation provided visibility into the process using text and crude character graphics. Availability of a fully functional graphical user interface was a way away.\n\nCentral to the DCS model was the inclusion of control function blocks. Function blocks evolved from early, more primitive DDC concepts of \"Table Driven\" software. One of the first embodiments of object-oriented software, function blocks were self-contained \"blocks\" of code that emulated analog hardware control components and performed tasks that were essential to process control, such as execution of PID algorithms. Function blocks continue to endure as the predominant method of control for DCS suppliers, and are supported by key technologies such as Foundation Fieldbus today.\n\nMidac Systems, of Sydney, Australia, developed an objected-oriented distributed direct digital control system in 1982. The central system ran 11 microprocessors sharing tasks and common memory and connected to a serial communication network of distributed controllers each running two Z80s. The system was installed at the University of Melbourne.\n\nDigital communication between distributed controllers, workstations and other computing elements (peer to peer access) was one of the primary advantages of the DCS. Attention was duly focused on the networks, which provided the all-important lines of communication that, for process applications, had to incorporate specific functions such as determinism and redundancy. As a result, many suppliers embraced the IEEE 802.4 networking standard. This decision set the stage for the wave of migrations necessary when information technology moved into process automation and IEEE 802.3 rather than IEEE 802.4 prevailed as the control LAN.\n\nIn the 1980s, users began to look at DCSs as more than just basic process control. A very early example of a Direct Digital Control DCS was completed by the Australian business Midac in 1981–82 using R-Tec Australian designed hardware. The system installed at the University of Melbourne used a serial communications network, connecting campus buildings back to a control room \"front end\". Each remote unit ran two Z80 microprocessors, while the front end ran eleven Z80s in a parallel processing configuration with paged common memory to share tasks and that could run up to 20,000 concurrent control objects.\n\nIt was believed that if openness could be achieved and greater amounts of data could be shared throughout the enterprise that even greater things could be achieved. The first attempts to increase the openness of DCSs resulted in the adoption of the predominant operating system of the day: \"UNIX\". UNIX and its companion networking technology TCP-IP were developed by the US Department of Defense for openness, which was precisely the issue the process industries were looking to resolve.\n\nAs a result, suppliers also began to adopt Ethernet-based networks with their own proprietary protocol layers. The full TCP/IP standard was not implemented, but the use of Ethernet made it possible to implement the first instances of object management and global data access technology. The 1980s also witnessed the first PLCs integrated into the DCS infrastructure. Plant-wide historians also emerged to capitalize on the extended reach of automation systems. The first DCS supplier to adopt UNIX and Ethernet networking technologies was Foxboro, who introduced the I/A Series system in 1987.\n\nThe drive toward openness in the 1980s gained momentum through the 1990s with the increased adoption of commercial off-the-shelf (COTS) components and IT standards. Probably the biggest transition undertaken during this time was the move from the UNIX operating system to the Windows environment. While the realm of the real time operating system (RTOS) for control applications remains dominated by real time commercial variants of UNIX or proprietary operating systems, everything above real-time control has made the transition to Windows.\n\nThe introduction of Microsoft at the desktop and server layers resulted in the development of technologies such as OLE for process control (OPC), which is now a de facto industry connectivity standard. Internet technology also began to make its mark in automation and the world, with most DCS HMI supporting Internet connectivity. The 1990s were also known for the \"Fieldbus Wars\", where rival organizations competed to define what would become the IEC fieldbus standard for digital communication with field instrumentation instead of 4–20 milliamp analog communications. The first fieldbus installations occurred in the 1990s. Towards the end of the decade, the technology began to develop significant momentum, with the market consolidated around Ethernet I/P, Foundation Fieldbus and Profibus PA for process automation applications. Some suppliers built new systems from the ground up to maximize functionality with fieldbus, such as Rockwell PlantPAx System, Honeywell with Experion & Plantscape SCADA systems, ABB with System 800xA, Emerson Process Management with the Emerson Process Management DeltaV control system, Siemens with the SPPA-T3000 or Simatic PCS 7, Forbes Marshall with the Microcon+ control system and Azbil Corporation with the Harmonas-DEO system. Fieldbus technics have been used to integrate machine, drives, quality and condition monitoring applications to one DCS with Valmet DNA system.\n\nThe impact of COTS, however, was most pronounced at the hardware layer. For years, the primary business of DCS suppliers had been the supply of large amounts of hardware, particularly I/O and controllers. The initial proliferation of DCSs required the installation of prodigious amounts of this hardware, most of it manufactured from the bottom up by DCS suppliers. Standard computer components from manufacturers such as Intel and Motorola, however, made it cost prohibitive for DCS suppliers to continue making their own components, workstations, and networking hardware.\n\nAs the suppliers made the transition to COTS components, they also discovered that the hardware market was shrinking fast. COTS not only resulted in lower manufacturing costs for the supplier, but also steadily decreasing prices for the end users, who were also becoming increasingly vocal over what they perceived to be unduly high hardware costs. Some suppliers that were previously stronger in the PLC business, such as Rockwell Automation and Siemens, were able to leverage their expertise in manufacturing control hardware to enter the DCS marketplace with cost effective offerings, while the stability/scalability/reliability and functionality of these emerging systems are still improving. The traditional DCS suppliers introduced new generation DCS System based on the latest Communication and IEC Standards, which resulting in a trend of combining the traditional concepts/functionalities for PLC and DCS into a one for all solution—named \"Process Automation System\" (PAS). The gaps among the various systems remain at the areas such as: the database integrity, pre-engineering functionality, system maturity, communication transparency and reliability. While it is expected the cost ratio is relatively the same (the more powerful the systems are, the more expensive they will be), the reality of the automation business is often operating strategically case by case. The current next evolution step is called Collaborative Process Automation Systems.\n\nTo compound the issue, suppliers were also realizing that the hardware market was becoming saturated. The life cycle of hardware components such as I/O and wiring is also typically in the range of 15 to over 20 years, making for a challenging replacement market. Many of the older systems that were installed in the 1970s and 1980s are still in use today, and there is a considerable installed base of systems in the market that are approaching the end of their useful life. Developed industrial economies in North America, Europe, and Japan already had many thousands of DCSs installed, and with few if any new plants being built, the market for new hardware was shifting rapidly to smaller, albeit faster growing regions such as China, Latin America, and Eastern Europe.\n\nBecause of the shrinking hardware business, suppliers began to make the challenging transition from a hardware-based business model to one based on software and value-added services. It is a transition that is still being made today. The applications portfolio offered by suppliers expanded considerably in the '90s to include areas such as production management, model-based control, real-time optimization, plant asset management (PAM), Real-time performance management (RPM) tools, alarm management, and many others. To obtain the true value from these applications, however, often requires a considerable service content, which the suppliers also provide.\n\nThe latest developments in DCS include the following new technologies:\n\n\nIncreasingly, and ironically, DCS are becoming centralised at plant level, with the ability to log into the remote equipment. This enables operator to control both at enterprise level ( macro ) and at the equipment level (micro) both within and outside the plant as physical location due to interconnectivity primarily due to wireless and remote access has shrunk.\n\nAs wireless protocols are developed and refined, DCS increasingly includes wireless communication. DCS controllers are now often equipped with embedded servers and provide on-the-go web access. Whether DCS will lead IIOT or borrow key elements from remains to be established.\n\nMany vendors provide the option of a mobile HMI, ready for both Android and iOS. With these interfaces, the threat of security breaches and possible damage to plant and process are now very real.\n\n"}
{"id": "82898", "url": "https://en.wikipedia.org/wiki?curid=82898", "title": "Dolby Digital", "text": "Dolby Digital\n\nDolby Digital is the name for audio compression technologies developed by Dolby Laboratories. Originally named Dolby Stereo Digital until 1994, except for Dolby TrueHD, the audio compression is lossy. The first use of Dolby Digital was to provide digital sound in cinemas from 35mm film prints; today, it is now also used for other applications such as TV broadcast, radio broadcast via satellite, DVDs, Blu-ray discs and game consoles.\n\n\"Batman Returns\" was the first film to use Dolby Digital technology when it premiered in theaters in the summer of 1992. Dolby Digital cinema soundtracks are optically recorded on a 35 mm release print using sequential data blocks placed between every perforation hole on the sound track side of the film. A constant bit rate of 320 kbit/s is used. A charge-coupled device (CCD) scanner in the image projector picks up a scanned video image of this area, and a processor correlates the image area and extracts the digital data as an AC-3 bitstream. The data is then decoded into a 5.1 channel audio source. All film prints with Dolby Digital data also have Dolby Stereo analogue soundtracks using Dolby SR noise reduction and such prints are known as Dolby SR-D prints. The analogue soundtrack provides a fall-back option in case of damage to the data area or failure of the digital decoding; it also provides compatibility with projectors not equipped with digital soundheads. Almost all current release cinema prints are of this type and may also include SDDS data and a timecode track to synchronize CD-ROMs carrying DTS soundtracks.\n\nThe simplest way of converting existing projectors is to add a so-called \"penthouse\" digital soundhead above the projector head. However, for new projectors it made sense to use dual analogue/digital soundheads in the normal optical soundhead position under the projector head. To allow for the dual-soundhead arrangement the data is recorded 26 frames ahead of the picture. If a penthouse soundhead is used, the data must be delayed in the processor for the required amount of time, around 2 seconds. This delay can be adjusted in steps of the time between perforations, (approximately 10.4 ms).\n\n, Dolby Digital in film sound mixing is being gradually replaced with Dolby Surround 7.1, with the more advanced Dolby Atmos technology also gaining in popularity. While majority of movie theaters currently utilize Dolby Digital, virtually all films released today are mixed in Dolby Surround 7.1 and Dolby Atmos.\n\nDolby Digital has similar technologies, included in Dolby Digital EX, Dolby Digital Live, Dolby Digital Plus, Dolby Digital Surround EX, Dolby Digital Recording, Dolby Digital Cinema, Dolby Digital Stereo Creator and Dolby Digital 5.1 Creator.\n\nDolby Digital is the common version containing up to six discrete channels of sound. The most elaborate mode in common use involves five channels for normal-range speakers () (right, center, left, right surround, left surround) and one channel ( allotted audio) for the subwoofer driven low-frequency effects. Mono and stereo modes are also supported. AC-3 supports audio sample-rates up to 48 kHz.\n\nThis format has different names:\nIn 1991, a limited experimental release of \"\" in Dolby Digital played in 3 US theatres. In 1992, \"Batman Returns\" is the first movie to be released in Dolby Digital. In 1995, the LaserDisc version of \"Clear and Present Danger\" featured the first home theater Dolby Digital mix, quickly followed by \"True Lies\", \"Stargate\", \"Forrest Gump\", and \"Interview with the Vampire\" among others.\n\nDolby Digital EX is similar in practice to Dolby's earlier Pro-Logic format, which utilized matrix technology to add a center surround channel and single rear surround channel to stereo soundtracks. EX adds an extension to the standard 5.1 channel Dolby Digital codec in the form of matrixed rear channels, creating 6.1 or 7.1 channel output.\n\nIt provides an economical and backwards-compatible means for 5.1 soundtracks to carry a sixth, center back surround channel for improved localization of effects. The extra surround channel is matrix encoded onto the discrete \"left surround\" and \"right surround\" channels of the 5.1 mix, much like the front center channel on Dolby Pro Logic encoded stereo soundtracks. The result can be played without loss of information on standard 5.1 systems, or played in 6.1 or 7.1 on systems with Surround EX decoding and added speakers.\nDolby Digital Surround EX has since been used for the \"Star Wars\" prequels on the DVD versions and also the remastered original \"Star Wars\" trilogy. A number of DVDs have a Dolby Digital Surround EX audio option.\n\nThe cinema version of \"Dolby Digital EX\" was introduced in 1999, when Dolby and Skywalker Sound, a division of Lucasfilm Ltd., codeveloped \"Dolby Digital Surround EX\"™ for the release of \"\". Dolby Digital Surround EX has since been used for the \"Star Wars\" prequels on the DVD versions and also the remastered original \"Star Wars\" trilogy.\nDolby Digital Live (DDL) is a real-time encoding technology for interactive media such as video games. It converts any audio signals on a PC or game console into a 5.1-channel 16-bit/48 kHz Dolby Digital format at 640 kbit/s and transports it via a single S/PDIF cable. A similar technology known as DTS Connect is available from competitor DTS. An important benefit of this technology is that it enables the use of digital multichannel sound with consumer sound cards, which are otherwise limited to digital PCM stereo or analog multichannel sound because S/PDIF over RCA, BNC, and TOSLINK can only support two-channel PCM, Dolby Digital multichannel audio, and DTS multichannel audio. HDMI was later introduced, and it can carry uncompressed multichannel PCM, lossless compressed multichannel audio, and lossy compressed digital audio. However, Dolby Digital Live is still useful with HDMI to allow transport of multichannel audio over HDMI to devices that are unable to handle uncompressed multichannel PCM.\n\nDolby Digital Live is available in sound cards using various manufacturers' audio chipsets. The SoundStorm, used for the Xbox game console and certain nForce2 motherboards, used an early form of this technology. DDL is available on motherboards with codecs such as Realtek's ALC882D, ALC888DD and ALC888H. Other examples include some C-Media PCI sound cards and Creative Labs' X-Fi and Z series sound cards, whose drivers have enabled support for DDL.\n\nNVIDIA later decided to drop DDL support in their motherboards due to the cost of involved royalties, leaving an empty space in this regard in the sound cards market.\nThen in June 2005 came Auzentech, which with its X-Mystique PCI card, provided the first consumer sound card with Dolby Digital Live support.\n\nInitially no Creative X-Fi based sound cards supported DDL (2005~2007) but a collaboration of Creative and Auzentech resulted in the development of the Auzentech Prelude, the first X-Fi card to support DDL. Originally planned to extend DDL support to all X-Fi based sound cards (except the 'Xtreme Audio' line which is incapable of DDL hardware implementation), the plan was dropped because Dolby licensing would have required a royalty payment for all X-Fi cards and, problematically, those already sold.\nIn 2008, Creative released the X-Fi Titanium series of sound cards which fully supports Dolby Digital Live while leaving all PCI versions of Creative X-Fi still lacking support for DDL.\n\nSince September 2008, all Creative X-Fi based sound cards support DDL (except the 'Xtreme Audio' and its based line such as Prodigy 7.1e, which is incapable of DDL in hardware). X-Fi's case differs.\n\nWhile they forgot about the plan, programmer Daniel Kawakami made a hot issue by applying Auzentech Prelude DDL module back to Creative X-Fi cards by disguising the hardware identity as Auzentech Prelude.\n\nCreative Labs alleged Kawakami violated their intellectual property and demanded he cease distributing his modified drivers.\nEventually Creative struck an agreement with Dolby Laboratories regarding the Dolby license royalty by arranging that the licensing cost be folded into the purchase price of the Creative X-Fi PCI cards rather than as a royalty paid by Creative themselves. Based on the agreement, in September 2008 Creative began selling the \"Dolby Digital Live\" packs enabling Dolby Digital Live on Creative's X-Fi PCI series of sound cards. It can be purchased and downloaded from Creative. Subsequently Creative added their \"DTS Connect\" pack to the DDL pack at no added cost.\n\nE-AC-3 (Dolby Digital Plus) is an enhanced coding system based on the AC-3 codec. It offers increased bitrates (up to 6.144 Mbit/s), support for even more audio channels (up to 15.1 discrete channels in the future), and improved coding techniques (only at low data rates) to reduce compression artifacts, enabling lower data rates than those supported by AC-3 (e.g. 5.1-channel audio at 256 kbit/s). It is not backward compatible with existing AC-3 hardware, though E-AC-3 codecs generally are capable of transcoding to AC-3 for equipment connected via S/PDIF. E-AC-3 decoders can also decode AC-3 bitstreams. The fourth generation Apple TV supports E-AC-3. The discontinued HD DVD system directly supported E-AC-3. Blu-ray Disc offers E-AC-3 as an option to graft added channels onto an otherwise 5.1 AC-3 stream, as well as for delivery of secondary audio content (e.g. director's commentary) that is intended to be mixed with the primary audio soundtrack in the Blu-ray Disc player.\n\nDolby AC-4 is an audio compression standard supporting multiple audio channels and/or audio objects. Support for 5.1 channel audio is mandatory and additional channels up to 7.1.4 are optional. AC-4 provides a 50% reduction in bit rate over AC-3/Dolby Digital Plus.\n\nDolby TrueHD, developed by Dolby Laboratories, is an advanced lossless audio codec based on Meridian Lossless Packing. Support for the codec was mandatory for HD DVD and is optional for Blu-ray Disc hardware. Dolby TrueHD supports 24-bit bit depths and sample rates up to 192 kHz. Maximum bitrate is 18 MBit/s while it supports up to 16 audio channels (HD DVD and Blu-ray Disc standards currently limit the maximum number of audio channels to eight). It supports metadata, including dialog normalization and Dynamic Range Control.\n\nAlthough commonly associated with the 5.1 channel configuration, Dolby Digital allows a number of different channel selections. The options are:\n\n\nThese configurations optionally include the extra low-frequency effects (LFE) channel. The last two with stereo surrounds optionally use Dolby Digital EX matrix encoding to add an extra Rear Surround channel.\n\nMany Dolby Digital decoders are equipped with downmixing to distribute encoded channels to speakers. This includes such functions as playing surround information through the front speakers if surround speakers are unavailable, and distributing the center channel to left and right if no center speaker is available. When outputting to separate equipment over a 2-channel connection, a Dolby Digital decoder can optionally encode the output using Dolby Surround to preserve surround information.\n\nThe '.1' in 5.1, 7.1 etc. refers to the LFE channel, which is also a discrete channel.\n\nDolby Digital audio is used on DVD-Video and other purely digital media, like home cinema. In this format, the AC-3 bitstream is interleaved with the video and control bitstreams.\n\nThe system is used in bandwidth-limited applications other than DVD-Video, such as digital TV. The AC-3 standard allows a maximum coded bit rate of 640 kbit/s. 35mm film prints use a fixed rate of 320 kbit/s, which is the same as the maximum bit rate for 2-channel MP3. DVD-Video discs are limited to 448 kbit/s, although many players can successfully play higher-rate bitstreams (which are non-compliant with the DVD specification). HD DVD limits AC-3 to 448 kbit/s. ATSC and digital cable standards limit AC-3 to 448 kbit/s. Blu-ray Disc, the PlayStation 3 and the Xbox game console can output an AC-3 signal at a full 640 kbit/s. Some Sony PlayStation 2 console games are able to output AC-3 standard audio as well, primarily during pre-rendered cutscenes.\n\nDolby is part of a group of organizations involved in the development of AAC (Advanced Audio Coding), part of MPEG specifications, and considered the successor to MP3.\n\nDolby Digital Plus (DD-Plus) and TrueHD are supported in HD DVD, as mandatory codecs, and in Blu-ray Disc, as optional codecs.\n\nThe data layout of AC-3 is described by simplified \"C-like\" language in official specifications. An AC-3 stream is a series of frames; The frame size code is used along with the sample rate code to determine the number of (2-byte)\nwords before the next syncword. Channel blocks can be either long, in which case the entire block is processed as single modified discrete cosine transform or short, in which case two half length transforms are performed on the block. Below is a simplified AC-3 header. A detailed description is in the ATSC \"Digital Audio Compression (AC-3) (E-AC-3) Standard\", section 5.4.\nA free ATSC A/52 stream decoder, liba52, is available under the GPL license.\n\nAudio codec AC3 is covered by patents (though these are now expired). Patents are used to ask to pay a commercial license to publish an application that decodes AC3. This leads some audio app developers to ban AC3 from their apps, although the open source VLC media player supports AC-3 audio without having paid for any kind of patent license.\n\nIn Dolby's 2005 original and amended S-1 filings with the SEC, Dolby acknowledged that \"Patents relating to our Dolby Digital technologies expire between 2008 and 2017.\"\n\nThe last patent covering AC-3 expired March 20, 2017, so it is now generally free to use.\n\n\n"}
{"id": "2704993", "url": "https://en.wikipedia.org/wiki?curid=2704993", "title": "Electronic speed control", "text": "Electronic speed control\n\nAn electronic speed control or ESC is an electronic circuit that controls and regulates the speed of an electric motor. It may also provide reversing of the motor and dynamic braking.\nMiniature electronic speed controls are used in electrically powered radio controlled models. Full-size electric vehicles also have systems to control the speed of their drive motors. \nAn electronic speed control follows a speed reference signal (derived from a throttle lever, joystick, or other manual input) and varies the switching rate of a network of field effect transistors (FETs) . By adjusting the duty cycle or switching frequency of the transistors, the speed of the motor is changed. The rapid switching of the transistors is what causes the motor itself to emit its characteristic high-pitched whine, especially noticeable at lower speeds. \n\nDifferent types of speed controls are required for brushed DC motors and brushless DC motors. A brushed motor can have its speed controlled by varying the voltage on its armature. (Industrially, motors with electromagnet field windings instead of permanent magnets can also have their speed controlled by adjusting the strength of the motor field current.) A brushless motor requires a different operating principle. The speed of the motor is varied by adjusting the timing of pulses of current delivered to the several windings of the motor. \n\nBrushless ESC systems basically create three-phase AC power, as in a variable frequency drive , to run brushless motors. Brushless motors are popular with radio controlled airplane hobbyists because of their efficiency, power, longevity and light weight in comparison to traditional brushed motors. Brushless AC motor controllers are much more complicated than brushed motor controllers.\n\nThe correct phase varies with the motor rotation, which is to be taken into account by the ESC: Usually, back EMF from the motor is used to detect this rotation, but variations exist that use magnetic (Hall effect) or optical detectors. Computer-programmable speed controls generally have user-specified options which allow setting low voltage cut-off limits, timing, acceleration, braking and direction of rotation. Reversing the motor's direction may also be accomplished by switching any two of the three leads from the ESC to the motor.\n\nESCs are normally rated according to maximum current, for example, 25 amperes or 25 A. Generally the higher the rating, the larger and heavier the ESC tends to be which is a factor when calculating mass and balance in airplanes. Many modern ESCs support nickel metal hydride, lithium ion polymer and lithium iron phosphate batteries with a range of input and cut-off voltages. The type of battery and number of cells connected is an important consideration when choosing a battery eliminator circuit (BEC), whether built into the controller or as a stand-alone unit. A higher number of cells connected will result in a reduced power rating and therefore a lower number of servos supported by an integrated BEC, if it uses a linear voltage regulator. A well designed BEC using a switching regulator should not have a similar limitation.\n\nLarge, high-current ESCs are used in electric cars, such as the Nissan Leaf, Tesla Roadster (2008), Model S, Model X, Model 3, and the Chevrolet Bolt. The energy draw is usually measured in kilowatts (the Nissan Leaf, for instance, uses an 80 kilowatt motor that produces 210 foot-pounds of torque). Most mass-produced electric cars use AC motors, which allow the ESC to capture energy when the car coasts, using the motor as a generator and slowing the car down. The captured energy is used to charge the batteries and thus extend the driving range of the car (this is known as regenerative braking). In some vehicles, such as those produced by Tesla, this can be used to slow down so effectively that the car's conventional brakes are only needed at very low speeds (the motor braking effect diminishes as the speed is reduced). In others, such as the Nissan Leaf, there is only a slight \"drag\" effect when coasting, and the ESC modulates the energy capture in tandem with the conventional brakes to bring the car to a stop.\nSome custom-built electric cars (usually built by enthusiasts using the chassis, body and transmission of an existing vehicle) use DC motors instead of AC, because of their lower cost and simpler wiring. However, DC motors cannot be used for regenerative braking, so these vehicles cannot travel as far on the same battery, all other factors being equal. DC motors also cannot safely run at RPMs as high as AC motors can, so many custom-built electric cars retain a multi-speed transmission of some kind. Since electric motors have full torque from zero RPM, the vehicle can still start off in a high gear, but starting in a lower gear allows for quicker acceleration, lower current draw and less wear and tear on the motor. This is a limited view of the system; in fact the current and the proportional torque is limited by the batteries and / or the power devices used to control the motor so that a transmission could be beneficial to increase torque at low vehicle speeds just like a gasoline fueled one. However it is usually easier to increase the motor size so that the maximum torque is acceptable since the weight of a transmission can be equal to the motor. \n\nESCs used in mass-produced electric cars with AC motors usually have reversing capability, allowing the motor to run in both directions. The car has only one gear ratio, and the motor simply runs in the opposite direction to make the car go in reverse. Some custom-built electric cars with DC motors also have this feature, using an electrical switch to reverse the direction of the motor, but others run the motor in the same direction all the time and use a traditional manual or automatic transmission to reverse direction (usually this is easier, since the vehicle used for the conversion already has the transmission, and the electric motor is simply installed in place of the original engine).\n\nA motor used in an electric bicycle application requires high initial torque and therefore uses Hall sensor commutation for speed measurement. Electric bicycle controllers generally use brake application sensors, pedal rotation sensors and provide potentiometer-adjustable motor speed, closed-loop speed control for precise speed regulation, protection logic for over-voltage, over-current, and thermal protection. Sometimes pedal torque sensors are used to enable motor assist proportional to applied torque and sometimes support is provided for regenerative braking but infrequent braking and the low mass of bicycles limits recovered energy. An implementation is described in an for a 200 W, 24 V Brushless DC (BLDC) motor.\n\nP.A.S or PAS may appear within the list of components of electric conversion kits for bicycles which implies \"Pedal Assistance Sensor\" or sometimes \"Pulse Pedal Assistance Sensor\". Pulse usually relates to a magnet and sensor which measures the rotational velocity of the crank. Pedal pressure sensors under the feet are possible but not common.\n\nAn ESC can be a stand-alone unit which plugs into the receiver's throttle control channel or incorporated into the receiver itself, as is the case in most toy-grade R/C vehicles. Some R/C manufacturers that install proprietary hobby-grade electronics in their entry-level vehicles, vessels or aircraft use onboard electronics that combine the two on a single circuit board.\nElectronic speed controls for model RC vehicles may incorporate a battery eliminator circuit to regulate voltage for the receiver, removing the need for separate receiver batteries. The regulator may be linear or switched mode. ESCs, in a broader sense, are PWM controllers for electric motors. The ESC generally accepts a nominal 50 Hz PWM servo input signal whose pulse width varies from 1 ms to 2 ms. When supplied with a 1 ms width pulse at 50 Hz, the ESC responds by turning off the motor attached to its output. A 1.5 ms pulse-width input signal drives the motor at approximately half-speed. When presented with 2.0 ms input signal, the motor runs at full speed.\n\nESCs designed for sport use in cars generally have reversing capability; newer sport controls can have the reversing ability overridden so that it can not be used in a race. Controls designed specifically for racing and even some sport controls have the added advantage of dynamic braking capability. The ESC forces the motor to act as a generator by placing an electrical load across the armature. This in turn makes the armature harder to turn, thus slowing or stopping the model. Some controllers add the benefit of regenerative braking.\n\nESCs designed for radio-control helicopters do not require a braking feature (since the one way bearing would render it useless anyhow) nor do they require reverse direction (although it can be helpful since the motor wires can often be difficult to access and change once installed).\n\nMany high-end helicopter ESCs provide a \"Governor mode\" which fixes the motor RPM to a set speed, greatly aiding CCPM-based flight. It is also used in quadcopters.\n\nESCs designed for radio-control airplanes usually contain a few safety features. If the power coming from the battery is insufficient to continue running the electric motor the ESC will reduce or cut off power to the motor while allowing continued use of ailerons, rudder and elevator function. This allows the pilot to retain control of the airplane to glide or fly on low power to safety.\n\nESCs designed for boats are by necessity waterproof. The watertight structure is significantly different from that of non-marine type ESCs, with a more packed air trapping enclosure. Thus arises the need to cool the motor and ESC effectively to prevent rapid failure. Most marine-grade ESCs are cooled by circulated water run by the motor, or negative propeller vacuum near the drive shaft output. Like car ESCs, boat ESCs have braking and reverse capability.\n\nElectronic Speed Controllers (ESC) are an essential component of modern quadcopters (and all multirotors) that offer high power, high frequency, high resolution 3-phase AC power to the motors in an extremely compact miniature package. These craft depend entirely on the variable speed of the motors driving the propellers. This wide variation and fine RPM control in motor/prop speed gives all of the control necessary for a quadcopter (and all multirotors) to fly.\n\nQuadcopter ESCs usually can use a faster update rate compared to the standard 50 Hz signal used in most other RC applications. A variety of ESC protocols beyond PWM are utilized for modern-day multirotors, including, Oneshot42, Oneshot125, Multishot, and DShot. DShot is a digital protocol that offers a certain advantages over classical analog control, such as higher resolution, CRC checksums, and a lack of oscillator drift (removing the need for calibration). Modern day ESCs protocols can communicate at speeds of 37.5KHz or greater, with DSHOT2400 frame only taking 6.5μs.\n\nMost modern ESC contain a microcontroller interpreting the input signal and appropriately controlling the motor using a built-in program, or firmware. In some cases it is possible to change the factory built-in firmware for an alternate, publicly available, open source firmware. This is done generally to adapt the ESC to a particular application. Some ESCs are factory built with the capability of user upgradable firmware. Others require soldering to connect a programmer.\n\n"}
{"id": "44212980", "url": "https://en.wikipedia.org/wiki?curid=44212980", "title": "Floating landing platform", "text": "Floating landing platform\n\nA floating landing platform is a large marine floating structure used to land launch vehicle booster stages. \n\nThis is a fairly new phenomena after 2013 as all early orbital launch vehicle stages were expended, with booster stages returning through the atmosphere to certain destruction on contact with either land or sea. Since that time, two private companies are developing reusable launch vehicle technology and using, or planning to use, floating landing platforms.\n\nAfter attempts to land orbital rocket booster stages by parachute failed in the late 2000s, SpaceX began to develop reusable technology in the early 2010s. By mid-decade, Blue Origin was following, with a commitment to land the booster of a large launch vehicle on a moving ship.\n\nIn the 2010s, Space Exploration Technologies (SpaceX) contracted with a Louisiana shipyard to build a floating landing platform for reusable orbital launch vehicles. The platform had an approximately landing pad surface and was capable of precision positioning with diesel-powered azimuth thrusters so the platform can hold its position for launch vehicle landing. This platform was first deployed in January 2015 when SpaceX attempted a controlled descent flight test to land the first stage of Falcon 9 flight 14 on a solid surface after it was used to loft a contracted payload toward Earth orbit. The platform utilizes GPS position information to navigate and hold its precise position. The rocket landing leg span is and must not only land within the -wide barge deck, but must also deal with ocean swells and GPS errors.\nSpaceX CEO Elon Musk first displayed a photograph of the newly designated \"autonomous spaceport drone ship\" in November 2014. The ship is designed to hold position to within , even under storm conditions.\n\nOn 8 April 2016, the first stage of the rocket that launched the Dragon CRS-8 spacecraft, successfully landed on the drone ship named \"Of Course I Still Love You,\" the first successful landing of a rocket booster on a floating platform.\nBy early 2018, SpaceX had two operational drone ships and had a third under construction. By September 2018, sea platform landings had become routine for the SpaceX Falcon launch vehicles, with over 23 attempted and 17 successful recoveries.\n\n, Blue Origin is intending to make the first stage boosters of New Glenn be reusable, and recover launched boosters on the Atlantic Ocean, downrange of their Florida launch site, via a stabilized ship that is underway, acting as a moving floating landing platform. The hydrodynamically-stabilized ship is projected to increase the likelihood of successful recovery in rough seas. The first recovery from an orbital launch by Blue is expected around 2020. \n\nIn October 2018, the ship was disclosed to be the former \"Stena Freighter\", built in 2004 as a roll-on/roll-off cargo ship. The ship is currently undergoing refit in 2018–2019 in Pensacola, Florida.\n"}
{"id": "45832", "url": "https://en.wikipedia.org/wiki?curid=45832", "title": "Gyrocompass", "text": "Gyrocompass\n\nA gyrocompass is a type of non-magnetic compass which is based on a fast-spinning disc and the rotation of the Earth (or another planetary body if used elsewhere in the universe) to find geographical direction automatically. The use of a gyrocompass can be considered to be one out of seven fundamental ways to determine the heading of a vehicle. Although one important component of a gyrocompass is a gyroscope, these are not the same devices; a gyrocompass is built to use the effect of gyroscopic precession, which is a distinctive aspect of the general gyroscopic effect. Gyrocompasses are widely used for navigation on ships, because they have two significant advantages over magnetic compasses:\n\nAircraft commonly use gyroscopic instruments (but not a gyrocompass) for navigation and altitude monitoring; for details, see Flight instruments and Gyroscopic autopilot.\n\nA gyroscope, not to be confused with gyrocompass, is a spinning wheel mounted on a set of gimbals so that its axis is free to orient itself in any way. When it is spun up to speed with its axis pointing in some direction, due to the law of conservation of angular momentum, such a wheel will normally maintain its original orientation to a fixed point in outer space (not to a fixed point on Earth). Since our planet rotates, it appears to a stationary observer on Earth that a gyroscope's axis is completing a full rotation once every 24 hours. Such a rotating gyroscope is used for navigation in some cases, for example on aircraft, where it is known as heading indicator or directional gyro, but cannot ordinarily be used for long-term marine navigation. The crucial additional ingredient needed to turn a gyroscope into a gyrocompass, so it would automatically position to true north, is some mechanism that results in an application of torque whenever the compass's axis is not pointing north.\n\nOne method uses friction to apply the needed torque: the gyroscope in a gyrocompass is not completely free to reorient itself; if for instance a device connected to the axis is immersed in a viscous fluid, then that fluid will resist reorientation of the axis. This friction force caused by the fluid results in a torque acting on the axis, causing the axis to turn in a direction orthogonal to the torque (that is, to precess) along a line of longitude. Once the axis points toward the celestial pole, it will appear to be stationary and won't experience any more frictional forces. This is because true north (or true south) is the only direction for which the gyroscope can remain on the surface of the earth and not be required to change. This axis orientation is considered to be a point of minimum potential energy.\n\nAnother, more practical, method is to use weights to force the axis of the compass to remain horizontal (perpendicular to the direction of the center of the Earth), but otherwise allow it to rotate freely within the horizontal plane. In this case, gravity will apply a torque forcing the compass's axis toward true north. Because the weights will confine the compass's axis to be horizontal with respect to the Earth's surface, the axis can never align with the Earth's axis (except on the Equator) and must realign itself as the Earth rotates. But with respect to the Earth's surface, the compass will appear to be stationary and pointing along the Earth's surface toward the true North Pole.\n\nSince the gyrocompass's north-seeking function depends on the rotation around the axis of the Earth that causes torque-induced gyroscopic precession, it will not orient itself correctly to true north if it is moved very fast in an east to west direction, thus negating the Earth's rotation. However, aircraft commonly use heading indicators or directional gyros, which are not gyrocompasses and do not align themselves to north via precession, but are periodically aligned manually to magnetic north.\n\nWe consider a gyrocompass as a gyroscope which is free to rotate about one of its symmetry axes, also the whole rotating gyroscope is free to rotate on the horizontal plane about the local vertical. Therefore there are two independent local rotations. In addition to these rotations we consider the rotation of the Earth about its north-south (NS) axis, and we model the planet as a perfect sphere. We neglect friction and also the rotation of the Earth about the Sun.\n\nIn this case a non-rotating observer located at the center of the Earth can be approximated as being an inertial frame. We establish cartesian coordinates formula_1 for such an observer (whom we name as 1-O), and the barycenter of the gyroscope is located at a distance formula_2 from the center of the Earth.\n\nLet us consider another (non-inertial) observer (the 2-O) located at the center of the Earth but rotating about the NS-axis by formula_3. We establish coordinates attached to this observer as\n\nso that the unit formula_5 versor formula_6 is mapped to the point formula_7. For the 2-O neither the Earth nor the barycenter of the gyroscope is moving. The rotation of 2-O relative to 1-O is performed with angular velocity formula_8. We suppose that the formula_9 axis denotes points with zero longitude (the prime, or Greenwich, meridian).\n\nWe now rotate about the formula_10 axis, so that the formula_11-axis has the longitude of the barycenter. In this case we have\n\nWith the next rotation (about the axis formula_13 of an angle formula_14, the co-latitude) we bring the formula_15 axis along the local zenith (formula_16-axis) of the barycenter. This can be achieved by the following orthogonal matrix (with unit determinant)\n\nso that the formula_18 versor formula_19 is mapped to the point formula_20.\n\nWe now choose another coordinate basis whose origin is located at the barycenter of the gyroscope. This can be performed by the following translation along the zenith axis\n\nso that the origin of the new system, formula_22 is located at the point formula_23, and formula_2 is the radius of the Earth. Now the formula_25-axis points towards the south direction.\n\nNow we rotate about the zenith formula_26-axis so that the new coordinate system is attached to the structure of the gyroscope, so that for an observer at rest in this coordinate system, the gyrocompass is only rotating about its own axis of symmetry. In this case we find\n\nThe axis of symmetry of the gyrocompass is now along the formula_28-axis.\n\nThe last rotation is a rotation on the axis of symmetry of the gyroscope as in\n\nSince the height of the gyroscope's barycenter does not change (and the origin of the coordinate system is located at this same point), its gravitational potential energy is constant. Therefore its Lagrangian formula_30 corresponds to its kinetic energy formula_31 only. We have\n\nwhere formula_33 is the mass of the gyroscope, formula_34 is the squared inertial speed of the origin of the coordinates of the final coordinate system (i.e. the center of mass). This constant term does not affect the dynamics of the gyroscope and it can be neglected. On the other hand, the tensor of inertia is given by\n\nand\n\nTherefore we find\n\nThe Lagrangian can be rewritten as\n\nwhere\n\nis the part of the Lagrangian responsible for the dynamics of the system. Then, since formula_40, we find\n\nSince the angular momentum formula_42 of the gyrocompass is given by formula_43, we see that the constant formula_44 is the component of the angular momentum about the axis of symmetry. Furthermore, we find the equation of motion for the variable formula_45 as\n\nor\n\nAt the poles we find formula_48, and the equations of motion become\n\nThis simple solution implies that the gyroscope is uniformly rotating with constant angular velocity in both the vertical and symmetrical axis.\n\nLet us suppose now that formula_50 and that formula_51, that is the axis of the gyroscope is approximately along the north-south line, and let us find the parameter space (if it exists) for which the system admits stable small oscillations about this same line. If this situation occurs, the gyroscope will always be approximately aligned along the north-south line, giving direction. In this case we find\n\nLet us consider the case that\n\nand, further, we allow for fast gyro-rotations, that is\n\nTherefore, for fast spinning rotations, formula_55 implies formula_56. In this case, the equations of motion further simplify to\n\nTherefore we find small oscillations about the north-south line, as formula_58, where the angular velocity of this harmonic motion of the axis of symmetry of the gyrocompass about the north-south line is given by\n\nwhich corresponds to a period for the oscillations given by\n\nTherefore formula_61 is proportional to the geometric mean of the Earth and spinning angular velocities. In order to have small oscillations we have required formula_62, so that the North is located along the right-hand-rule direction of the spinning axis, that is along the negative direction of the formula_63-axis, the axis of symmetry. As a side result, on measuring formula_64 (and knowing formula_65), one can deduce the local co-latitude formula_66.\n\nThe first, not yet practical, form of gyrocompass was patented in 1885 by Marinus Gerardus van den Bos. A usable gyrocompass was invented in 1906 in Germany by Hermann Anschütz-Kaempfe, and after successful tests in 1908 became widely used in the German Imperial Navy. Hermann Anschütz-Kaempfe founded the company Anschütz & Co., which started in Kiel / Germany the mass production of gyro compasses. The company is today Raytheon Anschütz GmbH. The gyrocompass was an important invention for nautical navigation because it allowed accurate determination of a vessel’s location at all times regardless of the vessel’s motion, the weather and the amount of steel used in the construction of the ship. \n\nIn the United States, Elmer Ambrose Sperry produced a workable gyrocompass system (1908: patent #1,242,065), and founded the Sperry Gyroscope Company. The unit was adopted by the U.S. Navy (1911), and played a major role in World War I. The Navy also began using Sperry's \"Metal Mike\": the first gyroscope-guided autopilot steering system. In the following decades, these and other Sperry devices were adopted by steamships such as the RMS Queen Mary, airplanes, and the warships of World War II. After his death in 1930, the Navy named the USS \"Sperry\" after him.\n\nMeanwhile, in 1913, C. Plath (a Hamburg, Germany-based manufacturer of navigational equipment including sextants and magnetic compasses) developed the first gyrocompass to be installed on a commercial vessel. C. Plath sold many gyrocompasses to the Weems’ School for Navigation in Annapolis, MD, and soon the founders of each organization formed an alliance and became Weems & Plath.\nBefore the success of the gyrocompass, several attempts had been made in Europe to use a gyroscope instead. By 1880, William Thomson (Lord Kelvin) tried to propose a gyrostat (tope) to the British Navy. In 1889, Arthur Krebs adapted an electric motor to the Dumoulin-Froment marine gyroscope, for the French Navy. That gave the \"Gymnote\" submarine the ability to keep a straight line while underwater for several hours, and it allowed her to force a naval block in 1890.\n\nIn 1923 Max Schuler published his paper containing his observation that if a gyrocompass possessed Schuler tuning such that it had an oscillation period of 84.4 minutes (which is the orbital period of a notional satellite orbiting around the Earth at sea level), then it could be rendered insensitive to lateral motion and maintain directional stability.\n\nA gyrocompass is subject to certain errors. These include steaming error, where rapid changes in course, speed and latitude cause deviation before the gyro can adjust itself. On most modern ships the GPS or other navigational aids feed data to the gyrocompass allowing a small computer to apply a correction.\nAlternatively a design based on a strapdown architecture (including a triad of fibre optic gyroscopes, ring laser gyroscopes or hemispherical resonator gyroscopes and a triad of accelerometers) will eliminate these errors, as they do not depend upon mechanical parts to determinate rate of rotation.\n\n\n\n"}
{"id": "55147178", "url": "https://en.wikipedia.org/wiki?curid=55147178", "title": "Herbert Merrett", "text": "Herbert Merrett\n\nSir Herbert Henry Merrett (1886 – 3 October 1959) was a Welsh industrialist.\n\nHerbert Merrett grew up and was educated in Cardiff. He began his career in the Cardiff Docks with Cory Brothers, initially a junior, though be later became the company's general chairman and director when it merged with Powell Duffryn Ltd in 1947 as a result of nationalisation. Herbert was a director of Powell Duffryn from 1940, having previously been director of Blaenclydach Colliery (c.1923-1937), Troedyrhiw Coal (c.1923-1933) Company and D.R. Llewellyn & Sons (c. 1923-1933) and chairman of Graigola Merthyr Company (c.1923-1933). His other positions include chairman and managing director of Crynant Colliery (c.1933-1940) and director of North's Navigation Collieries (c.1940-1950). By 1930, he also was managing director of his own company, Gueret, Llewellyn & Merrett Ltd.\n\nIn 1932, Herbert published a book entitled \"I fight for coal\", describing his early experiences in the Welsh coal industry. Herbert was Justice of the Peace for Glamorganshire and High Sheriff 1934-35. In 1935-36, he served as president of Cardiff Incorporated Chamber of Commerce. He later became president of the British Coal Exporters' Federation 1946-49. In 1950 he was knighted and in that same year received an award from France of Chevalier de la Légion d'Honneur, having established associations within Europe.\n\nMerrett was an avid footballer in his youth and played for the amateur team the Cardiff Corinthians. He became a qualified referee and was chairman of the Cardiff City Association Football Club 1939-1957, barring one short break. He was also president of the Glamorgan County Cricket Club.\n\nMerrett was born in Canton parish, Cardiff on 18 December 1886, the son of Lewis and Elizabeth Merrett. In 1911, he married Marion Linda Higgins who bore him two daughter and a son. Merrett's grandson is Alan Chalmers, who ran security at Wimbledon for 40 years.\n\nMerrett died on 3 October 1959.\n\n"}
{"id": "12286511", "url": "https://en.wikipedia.org/wiki?curid=12286511", "title": "Hex (climbing)", "text": "Hex (climbing)\n\nA hex is an item of rock climbing equipment used to protect climbers from falls. They are intended to be wedged into a crack or other opening in the rock, and do not require a hammer to place. They were developed as an alternative to pitons, which are hammered into cracks and are more prone to damage the rock. Most commonly, a carabiner will be used to join the hex to the climbing rope by means of a loop of webbing, cord or a cable which is part of the hex.\n\nHexes are a type of nut, a hollow eccentric hexagonal prism with tapered ends, usually threaded with webbing, a swaged cable, or a cord. They are manufactured by several firms, with a range of sizes varying from about wide. Climbers select a range of sizes to use on a specific climb based on the characteristics of the cracks in the rock encountered on that particular climb. Sides may be straight or curved although the functioning principles remain the same no matter which shape is selected; the lack of sharp corners on curved models may make them easier to remove from the rock.\n\nHexes may be placed either as passive or active protection. When placed passively they work like chock stones in flared cracks, like other climbing nuts, just larger and with a different shape. Active protection is achieved by orienting the webbing so that a pull causes a camming action against the rock similar to Tricams, allowing for placement in parallel cracks. They are often preferred by alpine mountaineers over spring-loaded camming devices because of their lack of moving parts and overall lower weight for the same size crack.\n\nThe original hexes were invented by Yvon Chouinard and Tom Frost, and called \"Hexentrics\". The \"polycentric\" hexentric was designed by Swedish/Norwegian climber Tomas Carlstrom and given to Chouinard Equipment in 1973. They applied for a U. S. patent in 1974 and it was granted on April 6, 1976. Hexes were produced by Chouinard Equipment, Ltd until 1989, when it was sold as a design to Black Diamond Equipment. They are now produced and sold in much the same design today.\n"}
{"id": "50256460", "url": "https://en.wikipedia.org/wiki?curid=50256460", "title": "History of the Kinetograph, Kinetoscope, and Kinetophonograph", "text": "History of the Kinetograph, Kinetoscope, and Kinetophonograph\n\nHistory of the Kinetograph, Kinetoscope, and Kinetophonograph is a book written by siblings William Kennedy Dickson and Antonia Dickson about the history of film. The brother Dickson wrote from his experiences working for Thomas Edison at his \"Black Maria\" studio in West Orange, New Jersey; Edison himself prefaced the book. Emphasis is placed on the eponymous devices: the kinetograph, the kinetoscope, and the kinetophonograph. Dickson helped to develop these devices, which facilitate the capturing and exhibition of motion pictures.\n\nConsidered the first book of history on the subject of film, it was published in 1895 as a monograph. The Museum of Modern Art acquired the book in 1940 and later reprinted it in 1970 and 2000. The book has been received positively by literary critics and film scholars, who saw it as a valuable primary source and early look at the film industry. \n\n\"History of the Kinetograph, Kinetoscope, and Kinetophonograph\" is a collection of essays on the history of film, written by the motion picture pioneer William Kennedy Dickson and his sister Antonia Dickson. The brother Dickson had lived in England before moving to the United States in 1879. In 1883, at age 23, Dickson earned the employment of Thomas Edison at his Machine Works company in New York City. In 1888, Edison commissioned Dickson for the development of what would become the kinetoscope, an early means of playing back motion picture film. Dickson moved later to Edison's \"Black Maria\" film production studio in West Orange, New Jersey; the bulk of \"History\" recounts his experiences working at this studio.\n\nThe 55-page monograph contains 54 illustrations rendered by William. The mechanics of primordial motion picture cameras and exhibition are explained, with eponymous emphasis given to the kinetograph, the kinetoscope, and the kinetophonograph. Dickson worked with Edison on the development of these devices, which respectively capture pictures on film, play films back, and combine picture with sound. Antonia and William give credit to the other architects of film and their works, as well as the performers and subjects who star in those works. A preface penned by Edison appears at the book's start.\n\n\"History\" was published first in 1895. An early version of the book's contents appeared in the June 1894 issue of \"The Century Magazine\". That year, the siblings had published a biography of Edison. For the time of its publishing, the book served as not only a history of film but also as advertisement and a directory pertaining to the subjects in the films addressed in it. The book fell out of knowledge since until the Museum of Modern Art acquired a copy in 1940. The museum's Film Library division gave it a reprinting thirty years later, in 1970; Arno Press published this reprinting in association with the New York Times Company. The book received another reprinting in 2000, this time published by the museum itself.\n\nThe book is considered by scholars of the medium the first published history on the subject of film, with the art critic Nancy Mowll Mathews calling it unprecedented. The historian Lewis Jacobs called the book important as a primary source for the history of early film. From the authors' combined tone, Jacobs perceived the Dicksons' excitement at having the privilege to observe popular film actors and to have worked in his industry at its infancy. Jacobs defined the book as \"intimate\" and its closing words as an \"eloquent prediction\" on the expansion of kinetograph technology. Jacobs recommended the book for film scholars and historians for its wisdom and its emphasis on the beginning of film as both a technology and as an art form. The literary critic Laura Marcus found the book's scientific approach in tension with its purpose to market but conceded that the eponymous devices had a psychic nature intrinsically. Marcus noted the harmony of the Dicksons' view of film as both uncanny and natural and of their opinion of the \"Black Maria\" as both a nursery and a laboratory like that depicted in \"Frankenstein\". Despite the book's promotional tone, Marcus called its imagery and motifs influential in shaping subsequent publications about film.\n\nStéphanie Côté of the \"Journal of Film Preservation\" recommended the book for its importance as well but noted the difficulty of the brother Dickson's writing. Côté called the sister Dickson's style similarly flamboyant. The film critic David Thomson regarded the book's prose as \"plummy\" from the brother Dickson's admiration for the film industry. Mathews called Dickson's front cover illustration mediocre but convincing.\n\n"}
{"id": "41989305", "url": "https://en.wikipedia.org/wiki?curid=41989305", "title": "In vivo bioreactor", "text": "In vivo bioreactor\n\nThe in vivo bioreactor (IVB) is a regenerative medicine paradigm where bone is grown in vivo. The IVB has basic elements:\n\nAn example of the implementation of the IVB approach was in the engineering of autologous bone by injecting calcium alginate in a sub-periosteal location. The periosteum is a membrane that covers the long bones, jawbone, ribs and the skull. This membrane contains an endogenous population of pluripotent cells called the periosteal cells, which are a type of mesenchymal stem cells (MSC), which reside in the cambium layer, i.e., the side facing the bone. A key step in the procedure is the elevation of the periosteum without damaging the cambium surface and to ensure this a new technique called hydraulic elevation was developed.\n\nThe choice of the sub-periosteum site is used because stimulation of the cambium layer using transforming growth factor–beta resulted in enhanced chondrogenesis, i.e., formation of cartilage. In development the formation of bone can either occur via a Cartilage template initially formed by the MSCs that then gets ossified through a process called endochondral ossification or directly from MSC differentiation to bone via a process termed \"intra-membranous ossification\". Upon exposure of the periosteal cells to calcium from the alginate gel, these cells become bone cells and start producing bone matrix through the intra-membranous ossification process, recapitulating all steps of bone matrix deposition. The extension of the IVB paradigm to engineering autologous hyaline cartilage was also recently demonstrated. In this case, agarose is injected and this triggers local hypoxia, which then results in the differentiation of the periosteal MSCs into articular chondrocytes, i.e. cells similar to those found in the joint cartilage. Since this processes occurs in a relative short period of less than two weeks and cartilage can remodel into bone, this approach might provide some advantages in treatment of both cartilage and bone loss. The IVB concept needs to be however realized in humans and this is currently being undertaken.\n\n"}
{"id": "2088757", "url": "https://en.wikipedia.org/wiki?curid=2088757", "title": "Insteon", "text": "Insteon\n\nInsteon is a home automation (domotics) technology that enables light switches, lights, thermostats, leak sensors, remote controls, motion sensors, and other electrically powered devices to interoperate through power lines, radio frequency (RF) communications, or both. It employs a dual-mesh networking topology in which all devices are peers and each device independently transmits, receives, and repeats messages. Like other home automation systems, it has been associated with the Internet of Things.\n\nInsteon-based products were launched in 2005 by Smartlabs, the company which holds the trademark for \"Insteon\". A Smartlabs subsidiary, also named Insteon, was created to market the technology.\n\nAccording to a press release on June 13, 2017, SmartLabs and its Insteon technology has been acquired by Richmond Capital Partners with Rob Lilleness of Universal Electronics assuming the role of chairman and CEO. As of Nov 6, 2017, the Smarthome building appears to have been vacated.\n\nEvery message received by an Insteon compatible device undergoes error detection and correction and is then retransmitted to improve reliability. All devices retransmit the same message simultaneously so that message transmissions are synchronous to the powerline frequency, thus preserving the integrity of the message while strengthening the signal on the powerline and erasing RF dead zones. Insteon powerline messaging uses phase-shift keying. Insteon RF messaging uses frequency-shift keying.\n\nEach message contains a two-bit \"hops\" field that is initialized to 3 by the originating node and decremented each time a node in the network repeats the message. Individual Insteon messages can also carry up to 14 bytes of arbitrary user data for custom applications.\n\nInsteon is an integrated dual-mesh (formerly referred to as \"dual-band\") network that combines wireless radio frequency (RF) and a building's existing electrical wiring. The electrical wiring becomes a backup transmission medium in the event of RF/wireless interference. Conversely, RF/wireless becomes a backup transmission medium in the event of powerline interference. As a peer-to-peer network, devices do not require network supervision, thus allowing optional operation without central controllers and routing tables.\n\nInsteon devices can function without a central controller. Additionally, they may be managed by a central controller to implement functions such as control via smartphones and tablets, control scheduling, event handling, and problem reporting via email or text messaging. A computer can be used as a central controller by connecting it to an Insteon USB/serial PowerLinc modem, which serves as a communication bridge between the computer and the Insteon device network.\n\nInsteon network security is maintained via \"linking control\" to ensure that users cannot create links that would allow them to control a neighbors’ Insteon devices, and via encryption within extended Insteon messages for applications such as door locks and security applications, should those applications choose to implement encryption.\n\nInsteon enforces linking control by requiring users to have physical possession of devices, or knowledge of their unique Insteon IDs in order to create links. Firmware in Insteon devices prohibits them from identifying themselves to other devices unless a user either physically presses a button on the device during the installation process or explicitly addresses the device via a central controller. Linking to a device by sending Insteon messages (e.g., from a central controller) requires knowledge of the address of the target Insteon device. As these addresses are unique for each device and assigned at the factory (and displayed on a printed label attached to each device), users must have physical access to the device to read the device address from the label and manually enter it when prompted during installation.\n\nThe security of Insteon RF devices was criticized in a DEF CON presentation in 2015.\n\nInsteon devices are configured by applying a sequence of \"taps\" (button presses) to a pushbutton on each device to establish direct device-to-device links. Alternatively, a central controller may be used to configure devices.\n\nEach Insteon device has its own unique identifier code, similar to a MAC address, and the technology allows directly linked devices to manage their identifiers.\n\nOlder Insteon chip sets manufactured by Smartlabs can transmit, receive, and respond to (but not repeat) X10 power line messages, thus enabling X10 networks to interoperate with Insteon.\n\nIn 2014, Insteon released a home automation system compatible with the touch-enabled Metro interface, with devices appearing as \"live tiles\", and later added voice control via Microsoft Cortana.\n\nIn 2015, voice control was added via compatibility with Amazon Echo. That same year, Logitech announced the Harmony Hub-based remote would support Insteon devices when deployed with an Insteon Hub. Also in 2015, Insteon announced an initiative to integrate the Google-owned Nest learning thermostat with the Insteon Hub.\n\nInsteon was one of two launch partners for Apple's HomeKit platform, with the HomeKit-enabled Insteon Hub Pro. In 2015, Insteon announced support for the Apple Watch, allowing watch owners to control their home with an Insteon Hub.\n\n\n\n\n\n\n\n\n\n"}
{"id": "15491314", "url": "https://en.wikipedia.org/wiki?curid=15491314", "title": "International Tunneling and Underground Space Association", "text": "International Tunneling and Underground Space Association\n\nThe International Tunneling and Underground Space Association is an organization founded in 1974, comprising currently 71 member nations and 310 affiliate members, aiming to encourage the use of the subsurface for the benefit of public, environment and sustainable development, and to promote advances in planning, design, construction, maintenance and safety of tunnels and underground space.\n\n"}
{"id": "3733061", "url": "https://en.wikipedia.org/wiki?curid=3733061", "title": "Intervalometer", "text": "Intervalometer\n\nAn intervalometer is a device that counts intervals of time. Such devices are commonly used to signal, in accurate time intervals, the operation of some other device. For instance, an intervalometer might activate something every 30 seconds. Other names for such a device include interval meter and interval timer.\n\nIn photography, intervalometers are used to trigger exposures. This is often done for a time-lapse series. It may also be used to take, or begin taking, picture(s) after a set delay.\n\nExamples of intervalometer use in aerial photography include delaying the start of picture taking by an unattended camera until some time after takeoff and separating multiple exposures in time, and thus distance as the vehicle containing the camera travels, to obtain the 3D effect (stereoscopy). To obtain the 3D effect each image should have about 60% of the surface in common with either the preceding or following image. The interval is calculated as a function of the altitude and speed of the vehicle; shorter intervals for low altitude and high speed.\n\nOften the purpose of a photographic intervalometer is to reduce the resources required either to take the pictures or post-process them as similar images could be obtained by having the camera continuously take pictures as rapidly as possible. Using an intervalometer permits restricting the pictures taken to only those with the desired content. This reduces the requirements for resources such as power and storage media (e.g. film or memory card space).\n\nMost digital single-lens reflex (DSLR) cameras are limited to 30 second or shorter exposures. An intervalometer can be used to take long (>30 seconds) or very long exposures (minutes or hours) using the \"Bulb\" setting. Long and very long exposures taken at night can be combined to create time-lapse animations, including star trails. Astrophotographers can use processing techniques with such exposures to create images of deep-sky objects in the night sky, like nebulae and galaxies.\n\nMost modern cameras include the most basic intervalometer functionality, the \"self-timer\". This delays the shutter release for a short time, allowing the photographer to get into the picture, for example.\n\nIn the past, intervalometers were external devices which interfaced to a camera to trigger taking a picture, or series of pictures, at a set time. These sometimes used existing remote shutter features on cameras. Later, standalone products commonly referred to as intervalometers have added capabilities far beyond the basics of just measuring, and signaling, a time interval. One of the first features that was added to intervalometers was the ability to use an external event to signal the start of the time interval(s). The ability to sense an external event is such a common feature of intervalometer products that many people do not distinguish between the sensing of the event and the measuring of time intervals.\n\nWhat is meant when someone refers to an \"intervalometer\" must be determined from context. Some possibilities are: time-lapse capability (strictly an intervalometer function), sensing of a remote event, a time delay longer than what most consider the \"self-timer\" range, etc. Strictly speaking, an intervalometer only measures, and/or signals, time intervals.\n\nAlmost all digital cameras have the basic hardware capability required for intervalometer functions: knowing the current and elapsed times. The implementation of more advanced functions is a matter of what the manufacturer chooses to implement in the camera's firmware. Functions beyond the self-timer are beginning to be seen in some digital cameras, and are used in some cases to distinguish models within a camera line.\n\nThe ALE-39 countermeasures system uses intervalometers manufactured by Ledex Inc. (now part of Johnson Electric) of Dayton, Ohio. The ALE-39 can fire flares in a synchronized pattern, very rapidly and with great reliability. The intervalometer used in the ALE-39 is essentially a solenoid-actuated rotary switch driven by a separate programmer which gives timing intervals and channel enabling to either of one or two channels. Intervalometers that contain internal interval clocks include the Lau-68, Suu-13 and similar electromechanically sequenced switches. Safety is provided to unfired outputs by maintaining a ground connection to all except the output being selected for firing; i.e., providing an electrical pulse to the firing squib.\n\nBomber aircraft can release all of their bombs at one time (\"salvo\") or drop individual bombs at intervals. If the bombardier selects the latter, he can program an intervalometer to control the pace at which the bombs are released. This, of course, determines how far apart they will land in the target area.\n\nOne of the more common types of intervalometers is the timer that is used to turn lights on and off at set times, or that of an automatic sprinkler system. These are commonly used by people when they leave their home for an extended period of time to make it appear the home is occupied. There are also a large number of commercial and industrial applications for even such basic intervalometers.\n"}
{"id": "23761317", "url": "https://en.wikipedia.org/wiki?curid=23761317", "title": "K-box", "text": "K-box\n\nK-box was the brand name of a small loudspeaker powered by an internal amplifier sold by Kerchoonz.com in 2009.\n\nWhen placed onto solid flat surfaces, walls or windows the K-box turns the surface into speakers or sound board. It amplifies and provide stereo sound that is supplied by any audio source that has 3.5mm audio output. The K-box can be utilized with contemporary electronics, like a computer, iPod, iPhone, television, or portable gaming device. The K-box is about the size of a standard cell (mobile) phone.\nThe K-box uses a hybrid technology of standard speaker, delivering mid to high frequencies and a Gel Audio driver (developed by SFX Technologies) delivering low end Bass frequencies. When placed on flat solid surfaces, this technology enhances the bass response via high-intensity vibrations. in order to generate sound and thus improve the volume and audio quality. The K-box gel audio technology has been explained as “hydro-gel” suspension system used to improve the dynamics in comparison to similar speaker products which typically work on transduction.\n\nThe K-box is powered by a built-in rechargeable battery with claims of 20 hours of playtime per charge It is recharged via computer USB or 5v adaptor.\n\nSTV's Colin Kelly \"Gadget Guru\" presenter for the television program \"The Hour\" mentions K-box in his television review as among the best sounds from one of these \"boombox\" products for taking to the beach. According to the show, the product is made in Scotland.\n\nPortable (115 x 55 x 20mm)\nUp to 20 hours battery life (recharge via USB)\nMaximum spl is 95 Db, depending on location and surface on which it is placed\nIt has a given frequency response of 40H-20 kHz\n\nThe K-box received an award as runner-up as The Most Valuable Music Therapy Product of 2009 by The Therapy Times in July 2009.\n\n"}
{"id": "1958790", "url": "https://en.wikipedia.org/wiki?curid=1958790", "title": "Landfill gas", "text": "Landfill gas\n\nLandfill gas is a complex mix of different gases created by the action of microorganisms within a landfill. Landfill gas is approximately forty to sixty percent methane, with the remainder being mostly carbon dioxide. Trace amounts of other volatile organic compounds (VOCs) comprise the remainder (<1%). These trace gases include a large array of species, mainly simple hydrocarbons. \n\nLandfill gases have an influence on climate change. The major components are CO and methane, both of which are greenhouse gas. Methane in the atmosphere is a far more potent greenhouse gas, with each molecule having twenty five times the effect of a molecule of carbon dioxide. Methane itself however accounts for less composition of the atmosphere than does carbon dioxide .. Landfills are the third largest source of methane in the US.\n\nLandfill gases are the result of three processes:\nThe first two depend strongly on the nature of the waste. The dominant process in most landfills is the third process whereby anaerobic bacteria decompose organic waste to produce biogas, which consists of methane and carbon dioxide together with traces of other compounds. Despite the heterogeneity of waste, the evolution of gases follows well defined kinetic pattern. Formation of methane and CO commences about six months after depositing the landfill material. The evolution of gas reaches a maximum at about 20 years, then declines over the course of decades.\n\nBecause gases produced by landfills are both valuable and sometimes hazardous, monitoring techniques have been developed. Flame ionization detectors can be used to measure methane levels as well as total VOC levels. Surface monitoring and sub-surface monitoring as well as monitoring of the ambient air is carried out. In the U.S., under the Clean Air Act of 1990, it is required that many large landfills install gas collection and control systems, which means that at the very least the facilities must collect and flare the gas.\n\nU.S. Federal regulations under Subtitle D of RCRA formed in October 1979 regulate the siting, design, construction, operation, monitoring, and closure of MSW landfills. Subtitle D now requires controls on the migration of methane in landfill gas. Monitoring requirements must be met at landfills during their operation, and for an additional 30 years after. The landfills affected by Subtitle D of RCRA are required to control gas by establishing a way to check for methane emissions periodically and therefore prevent off-site migration. Landfill owners and operators must make sure the concentration of methane gas does not exceed 25% of the LEL for methane in the facilities' structures and the LEL for methane at the facility boundary.\n\nA United States Environmental Protection Agency (EPA) report indicates that as of 2016, counts of operational municipal solid waste landfills range between 1,900 and 2,000. In a nation-wide study done by the Environmental Research and Education Foundation in 2013, only 1,540 operational municipal solid waste landfills were counted throughout the United States. Decomposing waste in these landfills produce landfill gas, which is a mixture of about half methane and half carbon dioxide. Landfills are the third largest source of methane emissions in the United States, with municipal solid waste landfills representing 95 percent of this fraction.\n\nThe gases produced within a landfill can be collected and used in various ways. The landfill gas can be utilized directly on site by a boiler or any type of combustion system, providing heat. Electricity can also be generated on site through the use of microturbines, steam turbines, or fuel cells. The landfill gas can also be sold off site and sent into natural gas pipelines. This approach requires the gas to be processed into pipeline quality, e.g., by removing various contaminants and components. The efficiency of gas collection at landfills directly impacts the amount of energy that can be recovered - closed landfills (those no longer accepting waste) collect gas more efficiently than open landfills (those that are still accepting waste). A comparison of collection efficiency at closed and open landfills found about a 17 percentage point difference between the two.\n\nLandfill gas can also be used to evaporate leachate, another byproduct of the landfill process. This application displaces another fuel that was previously used for the same thing. \nIn the U.S., the number of landfill gas projects increased from 399 in 2005, to 594 in 2012 according to the Environmental Protection Agency. These projects are popular because they control energy costs and reduce greenhouse gas emissions. These projects collect the methane gas and treat it, so it can be used for electricity or upgraded to pipeline-grade gas. (Methane gas has twenty-one times the global warming potential of carbon dioxide). For example, in the U.S., Waste Management uses landfill gas as an energy source at 110 landfill gas-to-energy facilities. This energy production offsets almost two million tons of coal per year, creating energy equivalent to that needed by four hundred thousand homes. These projects also reduce greenhouse gas emissions into the atmosphere.\n\nThe EPA, which estimates that hundreds of landfills could support gas to energy projects, has also established the Landfill Methane Outreach Program. This program was developed to reduce methane emissions from landfills in a cost-effective manner by encouraging the development of environmentally and economically beneficial landfill gas-to-energy projects.\n\nCapture and use of landfill gas can be expensive. Some environmental groups claim that the projects do not produce \"renewable power\" because trash (their source) is not renewable. The Sierra Club opposes government subsidies for such projects. The Natural Resources Defense Council (NRDC) argues that government incentives should be directed more towards solar, wind, and energy-efficiency efforts.\n\nWhen landfill gas permeates through a soil cover, a fraction of the methane in the gas is oxidized microbially to CO.\n\nLandfill gas emissions can lead to environmental, hygiene and security problems in the landfill. Several accidents have occurred, for example at Loscoe, England in 1986, where migrating landfill gas accumulated and partially destroyed a property. An accident causing two deaths occurred from an explosion in a house adjacent to Skellingsted landfill in Denmark in 1991. Due to the risk presented by landfill gas, there is a clear need to monitor gas produced by landfills. In addition to the risk of fire and explosion, gas migration in the subsurface can result in contact of landfill gas with groundwater. This, in turn, can result in contamination of groundwater by organic compounds present in nearly all landfill gas.\n\nAlthough usually evolved only in trace amounts, landfills do release some aromatics and chlorocarbons.\n\nLandfill gas migration, due to pressure differentials and diffusion, can occur. This can create an explosion hazard if the gas reaches sufficiently high concentrations in adjacent buildings.\n\n\n"}
{"id": "961611", "url": "https://en.wikipedia.org/wiki?curid=961611", "title": "Lip balm", "text": "Lip balm\n\nLip balm or lip salve is a wax-like substance applied topically to the lips of the mouth to moisturize and relieve chapped or dry lips, angular cheilitis, stomatitis, or cold sores. Lip balm often contains beeswax or carnauba wax, camphor, cetyl alcohol, lanolin, paraffin, and petrolatum, among other ingredients. Some varieties contain dyes, flavor, fragrance, phenol, salicylic acid, and sunscreens.\n\nThe primary purpose of lip balm is to provide an occlusive layer on the lip surface to seal moisture in lips and protect them from external exposure. Dry air, cold temperatures, and wind all have a drying effect on skin by drawing moisture away from the body. Lips are particularly vulnerable because the skin is so thin, and thus they are often the first to present signs of dryness. Occlusive materials like waxes and petroleum jelly prevent moisture loss and maintain lip comfort while flavorants, colorants, sunscreens, and various medicaments can provide additional, specific benefits.\n\nLip balm can be applied where a finger is used to apply it to the lips, or in a lipstick-style tube from which it can be applied directly.\n\nLip balm was first marketed in the 1880s by Charles Browne Fleet, though its origins may be traced to earwax. More than 40 years prior to the commercial introduction of lip balm by Fleet, Lydia Maria Child recommended earwax as a treatment for cracked lips in her highly-popular book, Child observed that, \"Those who are troubled with cracked lips have found this earwax remedy successful when others have failed. It is one of those sorts of cures, which are very likely to be laughed at; but I know of its having produced very beneficial results.\"\n\n\nSome physicians have suggested that certain types of lip balm can be addictive or contain ingredients that actually cause drying. Lip balm manufacturers sometimes state in their FAQs that there is nothing addictive in their products or that all ingredients are listed and approved by the FDA. Snopes found the claim that there are substances in Carmex that are irritants necessitating reapplication, such as ground glass, to be false.\n"}
{"id": "16103661", "url": "https://en.wikipedia.org/wiki?curid=16103661", "title": "List of GPS satellites", "text": "List of GPS satellites\n\nAs of February 2016, 72 Global Positioning System navigation satellites have been launched, 31 of which are operational, 9 in reserve, 30 have been retired and 2 were lost at launch. The constellation requires a minimum of 24 operational satellites, and the official target count is 33.\n\nSVNs are \"space vehicle numbers\" which are serial numbers assigned to each GPS satellite. PRNs are the \"pseudo-random noise\" sequences, or Gold codes, that each satellite transmits to differentiate itself from other satellites in the active constellation.\n\nRefer to GPS Constellation Status for the most up-to-date information.\nNumbers in parentheses refer to non-operational satellites.\n\nOnce launched, GPS satellites do not change their plane assignment but slot assignments are somewhat arbitrary and are subject to change.\n\nThe following table is for the purpose of making it possible to determine the block associated with a PRN by looking at one column in one table rather than having to search through all rows of three tables. Thus this table can be used to quickly and easily determine the number of satellites in orbit and health associated with each block.\n\nThis section is for the purpose of making it possible to determine the PRN associated with a SVN at a particular epoch. For example, SVN 049 had been assigned PRNs 01, 24, 27, and 30 at different times of its lifespan, whereas PRN 01 had been assigned to SVNs 032, 037, 049, 035, and 063 at different epochs. This information can be found in the IGS ANTEX file, which uses the convention \"GNN\" and \"GNNN\" for PRNs and SVNs, respectively. For example, SVN 049 is described as:\n\nwhereas for PRN 01 the following excerpt is relevant:\n\nA table extracted out of the ANTEX file is made available by the Bernese GNSS Software.\n\n\n"}
{"id": "31724371", "url": "https://en.wikipedia.org/wiki?curid=31724371", "title": "List of networking test equipment vendors", "text": "List of networking test equipment vendors\n\nNetworking Test Equipment is a category of Electronic test equipment.\n\n"}
{"id": "5652077", "url": "https://en.wikipedia.org/wiki?curid=5652077", "title": "Logic level", "text": "Logic level\n\nIn digital circuits, a logic level is one of a finite number of states that a digital signal can inhabit. Logic levels are usually represented by the voltage difference between the signal and ground, although other standards exist. The range of voltage levels that represents each state depends on the logic family being used.\n\nIn binary logic the two levels are logical high and logical low, which generally correspond to binary numbers 1 and 0 respectively. Signals with one of these two levels can be used in boolean algebra for digital circuit design or analysis.\n\nThe use of either the higher or the lower voltage level to represent either logic state is arbitrary. The two options are active high and active low. Active-high and active-low states can be mixed at will: for example, a read only memory integrated circuit may have a chip-select signal that is active-low, but the data and address bits are conventionally active-high. Occasionally a logic design is simplified by inverting the choice of active level (see De Morgan's theorem).\n\nThe name of an active-low signal is historically written with a bar above it to distinguish it from an active-high signal. For example, the name Q, read \"Q bar\" or \"Q not\", represents an active-low signal. The conventions commonly used are:\n\nMany control signals in electronics are active-low signals (usually reset lines, chip-select lines and so on). Logic families such as TTL can sink more current than they can source, so fanout and noise immunity increase. It also allows for wired-OR logic if the logic gates are open-collector/open-drain with a pull-up resistor. Examples of this are the I²C bus and the Controller Area Network (CAN),and the PCI Local Bus. RS232 signaling, as used on some serial ports, uses active-low signals.\n\nSome signals have a meaning in both states and notation may indicate such. For example, it is common to have a read/write line designated R/W, indicating that the signal is high in case of a read and low in case of a write.\n\nThe two logical states are usually represented by two different voltages, but two different currents are used in some logic families. High and low thresholds are specified for each logic family. When below the low threshold, the signal is \"low\". When above the high threshold, the signal is \"high\". Intermediate levels are undefined, resulting in highly implementation-specific circuit behavior.\n\nIt is usual to allow some tolerance in the voltage levels used; for example, 0 to 2 volts might represent logic 0, and 3 to 5 volts logic 1. A voltage of 2 to 3 volts would be invalid and occur only in a fault condition or during a logic level transition. However, few logic circuits can detect such a condition, and most devices will interpret the signal simply as high or low in an undefined or device-specific manner. Some logic devices incorporate Schmitt trigger inputs, whose behavior is much better defined in the threshold region and have increased resilience to small variations in the input voltage. The problem of the circuit designer is to avoid circumstances that produce intermediate levels, so that the circuit behaves predictably.\n\nNearly all digital circuits use a consistent logic level for all internal signals. That level, however, varies from one system to another. Interconnecting any two logic families often required special techniques such as additional pull-up resistors or purpose-built interface circuits known as level shifters. A level shifter connects one digital circuit that uses one logic level to another digital circuit that uses another logic level. Often two level shifters are used, one at each system: A line driver converts from internal logic levels to standard interface line levels; a line receiver converts from interface levels to internal voltage levels.\n\nFor example, TTL levels are different from those of CMOS. Generally a TTL output does not rise high enough to be reliably recognized as a logic 1 by a CMOS input, especially if it is only connected to a high-input-impedance CMOS input that does not source significant current. This problem was solved by the invention of the 74HCT family of devices that uses CMOS technology but TTL input logic levels. These devices only work with a 5 V power supply.\n\nIn three-state logic, an output device can be in one of three possible states: 0, 1, or Z, with the last meaning high impedance. This is not a logic level, but means that the output is not controlling the state of the connected circuit.\n\n4-level logic adds a fourth state, X (\"don't care\"), meaning the value of the signal is unimportant and undefined. It means that an input is undefined, or an output signal may be chosen for implementation convenience (see ).\n\nIEEE 1164 defines 9 logic states for use in electronic design automation. The standard includes strong and weakly driven signals, high impedance and unknown and unitialized states.\n\n\n"}
{"id": "1764207", "url": "https://en.wikipedia.org/wiki?curid=1764207", "title": "Loongson", "text": "Loongson\n\nLoongson () is a family of general-purpose MIPS64 CPUs developed at the \"Institute of Computing Technology\" (ICT), Chinese Academy of Sciences (CAS) in China. The chief architect is Professor . It was formerly called Godson.\n\nLoongson is the result of a public–private partnership. \"BLX IC Design Corporation\" was founded in 2002 by ICT and Jiangsu Zhongyi Group. Based in Beijing, BLX focuses on designing the 64-bit Loongson general-purpose and embedded processors, together with developing software tools and reference platforms.\n\nSTMicroelectronics fabricates and markets Loongson chips for BLX, which is fabless.\n\nThe current Loongson instruction set is a MIPS64, but the internal microarchitecture is independently developed by ICT. Early implementations of the family lacked four instructions patented by MIPS Technologies to avoid legal issues.\n\nIn 2007, a deal was reached by MIPS Technologies and ICT. STMicroelectronics bought a MIPS license for Loongson, and thus the processor can be promoted as \"MIPS-based\" or \"MIPS-compatible\" instead of \"MIPS-like\".\n\nIn June 2009, ICT licensed the MIPS32 and MIPS64 architectures directly from \"MIPS Technologies\".\n\nIn August 2011, Loongson Technology Corp. Ltd. licensed the MIPS32 and MIPS64 architectures from MIPS Technologies, Inc. for continued development of MIPS-based Loongson CPU cores.\n\nThe first revision of the Loongson architecture, the Loongson1 (Godson-232 core) is a pure 32-bit CPU running at a clock speed of 266 MHz. It is fabricated with 0.18 micron CMOS process, has 8 KB of data cache, 8 KB of instruction cache and a 64-bit floating-point unit, capable of 200 double-precision MFLOPS. It is intended for embedded applications, such as point of sale (POS) systems, where a high performance 64-bit architecture is not needed.\n\nThe Loongson 2 adds 64-bit ability to the Loongson architecture. Initially running at 500 MHz, later revisions to Godson 2E were produced that run up to 1 GHz. The Godson 2F, released to market in early 2008, ran at 1.2 GHz.\n\n\n\n\n\nThe 65 nm Loongson 3 (Godson-3) is able to run at a clock speed near 1 GHz, with 4 CPU cores (~15 W) first and 8 cores later (40 W). In April 2010, Loongson 3A was released with DDR2/3 DRAM support. In 2017, Loongson released latest version of 3A cpu, 3A3000. As one of the domestic CPU of China, Loongson 3A3000 is being commercialized, and in the recently exhibition in Nanjing (2017), based on the Loongson 3A3000 motherboard developers computer quietly debut. 3A3000 is Loongson latest CPU model (2017), it is using a LoongISA instruction set and designed with quad-core 64-bit and clocked at 1.5 GHz, power consumption is only 30W. For the performance, because the frequency is only 1.5Ghz, 3A3000 single-threaded performance is significantly behind to intel or AMD products. 3A3000 is only about one-third of Intel i5 4460 (3.2G).\n\nThere are two versions of the Loongson-3B (Godson-3B), the first featuring a 32 nm 6-core processor, and the second version having a 28 nm 8-core processor. Each version can be clocked from 1.2 GHz to 1.5 GHz. Loongson-3B has exceptional energy efficiency in terms of performance per watt - executing 192 GFLOPS using 40 watts. Each CPU core has 64 KB L1 cache and 128 KB L2 cache. All the cores share a common 8 MB L3 cache, which helps to reduce the cache miss rate.\n\nICT has launched a Loongson-3B-based six-core desktop solution. Technical specifications:\n\nThis desktop solution uses an optimized version of Fedora 13, with a lot of software ported and available, such as Kingsoft (WPS) office suite. The manufacturer states that the user experience of the desktop solution has been significantly improved over its Loongson-3A based predecessor. Results of a benchmark test, conducted in April 2014, are available\n\nThe Loongson 3 adds over 200 new instructions over Loongson 2. Their addition has the specific benefit of speeding up Intel x86 CPU emulation at a cost of 5% of the total die area. The new instructions help with emulation performance, for example QEMU (the only known example). The new instructions reduce the impact of executing x86/CISC-style instructions in the MIPS pipeline. With added improvements in QEMU from ICT, Loongson-3 achieves an average of 70% the performance of executing native binaries when running x86 binaries from nine benchmarks.\n\nUnlike processors from Intel, Advanced Micro Devices or VIA Technologies, Loongson does not directly support the x86 instruction set. The processor's main operating system is Linux, while in theory any OS with MIPS support should also work. For example, Windows CE was ported to a Loongson-based system with minimal effort. In 2010, Lemote ported an Android distribution to the Loongson platform.\n\nMany operating systems work on Loongson:\n\n\n\nThe GNU Compiler Collection (GCC) is the main compiler for software development on the Loongson platform.\n\nICT also ported Open64 to the Loongson II platform.\n\nOpen source applications on Linux Platform can be ported with little effort. Most common open-source applications (including OpenOffice.org, Mozilla Firefox, Pidgin, and MPlayer) and applications written for the Java platform are supported. For .NET applications, an unofficial port of the Mono Common Language Runtime is available online.\n\nIn March 2006, a €100 Loongson II computer design called \"Longmeng\" (Dragon Dream) was announced by Lemote.\n\nIn June 2006 at Computex'2006, YellowSheepRiver announced the \"Municator YSR-639\", a small form factor computer based on the 400 MHz Loongson 2.\n\nCurrently, Loongson boxes that come with a 667 MHz Godson 2E processor or an 800 MHz Godson 2F processor are sold in China at CNY 1599 (US$200) or CNY 1800 respectively without monitor, mouse, or keyboard.\n\n, two manufacturers have announced Loongson 2F products for sale outside China.\n\n\nLoongson insiders revealed a new model based on the Loongson 3A quad-core laptop has been developed and is expected to launch in August 2011. With a similar design to the MacBook Pro from Apple Inc., it will carry a Linux operating system by default.\n\nIn September 2011, Lemote announced the Yeeloong-8133 13.3\" laptop featuring 900 MHz, quad-core Loongson-3A/2GQ CPU.\n\nOn 26 December 2007, China revealed its first Loongson based supercomputer with performance 1 teraFLOPS of peak performance, and about 350 GFLOPS measured by LINPACK in Hefei, designated as KD-50-I. This supercomputer was designed by a joint team led by Chen Guoliang at the computer science technology department of the University of Science and Technology of China and ICT (the secondary contractor). KD-50-I is the first Chinese built supercomputer to utilize domestic Chinese CPUs, with a total of more than 336 Loongson-2F CPUs, and nodes are interconnected by Ethernet. The size of the computer was roughly equivalent to a household refrigerator and the cost was less than RMB 800,000 (approximately US$120,000, EUR 80,000).\n\nOn 20 April 2010, USTC announced successful development of Loongson 3A based KD-60-1. The new supercomputer is a cluster of standard blade servers with a total of over 80 quad-core Loongson processors, providing theoretical peak performance of 1 TFLOPS and reduces power consumption by 56% compared to the KD-50-I system that has similar performance.\n\nOn 26 December 2012, USTC announced successful development of Loongson 3B based KD-90-1. The new supercomputer is a cluster of standard blade servers with a total of over 10 octo-core Loongson processors, providing theoretical peak performance of 1 TFLOPS, and reduces power consumption by 62% compared to the KD-60 system that has similar performance.\n\nThe high-performance Dawning 6000, which has a projected speed of over one quadrillion operations per second, will incorporate the Loongson processor as its core. Dawning 6000 is currently jointly developed by the Institute of Computing Technology under the Chinese Academy of Sciences and the Dawning Information Industry Company. Li Guojie, chairman of Dawning Information Industry Company and director and academician of the Institute of Computing Technology, said research and development of the Dawning 6000 is expected to be completed in two years.\n\nTopstar has also released a pair of Mini-ATX based motherboards, the TEB-6040M and TEB-5040.\n\nDevelopment of the first Loongson chip was started in 2001.\n\nOn 25 June 2008, Hu Weiwu (chief designer of Loongson processors) gave a keynote speech at ISCA 2008, held in Beijing. The topic of the speech was \"Research and Development of Godson processors\".\n\n2010 January, Jiangsu province plans to buy 1.5 million Loongson PCs.\n\n\nhttp://www.c114.net/news/212/a1025509.html\n\n"}
{"id": "58971829", "url": "https://en.wikipedia.org/wiki?curid=58971829", "title": "Microneedle drug delivery", "text": "Microneedle drug delivery\n\nMicroneedles are microscopic applicators used to deliver vaccines or other drugs through transdermal application. Microneedles are constructed through various methods usually involving photolithographic processes or micromolding. These methods involve etching microscopic structure into resin or silicon in order to cast microneedles. Microneedles are made from a variety of material ranging from silicon, titanium, stainless steel, and polymers. Some microneedles are made of a drug to be delivered to the body but are shaped into a needle so they will penetrate the skin. The microneedles range in size, shape, and function but are all used as an alternative to other delivery methods like the conventional hypodermic needle or other injection apparatus. \n\nMicroneedles are usually applied through small arrays. The arrays used are a collection of microneedles, ranging from only a few microneedles to several hundred, applied to an applicator sometimes a patch or other solid stamping device. The arrays are applied to the skin of patients and are given time to allow for the effective administration of drugs. Microneedles are an easier method for physicians as they require less training to apply and because they are not as hazardous as other needles, making the administration of drugs to patients safer and less painful while also avoiding some of the drawbacks of using other forms of drug delivery, such as risk of infection, production of hazardous waster, or cost. \n\nMicroneedles were first mentioned in a 1998 paper on transdermal drug delivery demonstrating that microneedles could penetrate human skin. Subsequent research into microneedle drug delivery has explored the medical and cosmetic applications of this technology through its design. This early paper sought to explore the possibility of using microneedles in the future for vaccination. Since then researchers have studied microneedle delivery of insulin, vaccines, anti-inflammatories, and other pharmaceuticals. In dermatology, microneedles are used for scarring treatment with skin rollers. \n\nThe major goal of any microneedle design is to penetrate the outmost layer of the skin the stratum corneum (10-15μm). \n\nResearch has shown that there is a limit on the type of drugs that can be delivered through microneedles. Only compounds with a relatively low molecular weight, like the common allergen nickel (130 Da), can penetrate the skin. Compounds that weigh more than 500 Da cannot penetrate the skin. \n\nThis type of array is designed as a two part system; the microneedle array is first applied to the skin to create microscopic wells just deep enough to penetrate the outermost layer of skin, and then the drug is applied via transdermal patch. Solid microneedles are already used by dermatologists in collagen induction therapy, a method which uses repeated puncturing of the skin with microneedles. \n\nHollow microneedles are similar to solid microneedles in material. They contain reservoirs that deliver the drug directly into the site. Since the delivery of the drug is dependent on the flow rate of the microneedle, there is a possibility that this type of array could become clogged by excessive swelling or flawed design. This design also increases the likelihood of buckling under the pressure of application, and therefore failing to deliver any drugs. \n\nJust like solid microneedles, coated microneedles are usually designed from polymers or metals. In this method the drug is applied directly to the microneedle array instead of being applied through other patches or applicators. Coated microneedles are often covered in other surfactants or thickening agents to assure that the drug is delivered properly. Some of the chemicals used on coated microneedles are known irritants. While there is risk of local inflammation to the area where the array was, the array can be removed immediately with no harm to the patient.\n\nIn a more recent adaptation of the microneedle design, dissolvable microneedles encapsulate the drug in a nontoxic polymer which dissolves once inside the skin. This polymer would allow the drug to be delivered into the skin and could be broken down once inside the body. Pharmaceutical companies and researchers have begun to study and implement polymers such as Fibroin, a silk-based protein that can be molded into structures like microneedles and dissolved once in the body. \n\nThere are many advantages to the use of microneedles, the most prominent being the improved comfort of patients. Needle phobia can affect both adults and children, and sometimes can lead to fainting. The benefit of microneedle arrays is that they reduce anxiety that patients have when confronted with a hypodermic needle. In addition to improving psychological and emotional comfort, microneedles have been shown to be substantially less painful than conventional injections. Some studies recorded children's views on blood sampling with microneedle and found patients were more willing when prompted with a less painful procedure than traditional sampling with needles. Microneedles are beneficial to physicians as well since they produce less hazardous waste than needles and are generally easier to use. Microneedles are also less expensive than needles as they require less material and the material used is cheaper than the materials in hypodermic needles. \n\nMicroneedles present a new opportunity for home and community based healthcare. One of the biggest drawbacks of traditional needles is the hazardous waste that they produce, making disposal a serious concern for doctors and hospitals. For patients who require regular administration of medication at home, disposal can become an environmental concern is needles are placed in the trash. Microneedles would provide those who are limited in their ability to seek hospital care with the ability to safely administer drugs in the comfort of their homes. \n\nOne of the benefits of microneedles are their lower rates of microbial invasion into delivery sites. Traditional injection methods can leave puncture wounds for up to 48 hours post-treatment. This leaves a large window of opportunity for harmful bacteria to enter into the skin. Microneedles only damage the skin to a depth of 10-15μm making it difficult for bacteria to enter the bloodstream, and giving the body a smaller wound to repair. Further research is required to determine the types of bacteria able to breach the shallow puncture site of microneedles. \n\nThere are some concerns about how physicians can be sure that all of the drug or vaccine has entered the skin when microneedles are applied. Hollow and coated microneedles both possess the risk that the drug will not properly enter the skin and will not be effective. Both of these types of microneedles can leak onto a person's skin either by damage of the microneedle or incorrect application by the physician. This is why it is essential that physicians are trained how to properly apply the arrays. \n\nAnother concern is that incorrectly applied arrays could leave foreign material in the body. Although there is a lower risk of infection associated with microneedles, the arrays are more fragile than a typical hypodermic needle due to their small size and thus have a chance of breaking off and remaining in the skin. Some of the material used to construct the microneedles, such as titanium, cannot be absorbed by the body and any fragments of the needles would cause irritation. \n\nThere is a limited amount of literature available on the subject of microneedle drug delivery as current research is still exploring how to make effective needles.\n"}
{"id": "8528504", "url": "https://en.wikipedia.org/wiki?curid=8528504", "title": "Murata Electronics (Finland)", "text": "Murata Electronics (Finland)\n\nMurata Electronics Oy is a Finnish company (previously called VTI Technologies Oy) that designs and manufactures silicon-based capacitive sensors for the measurement of acceleration, pressure, inclination, shock, vibration and angular rate. All sensors are based on the company’s proprietary 3D MEMS technology. In 2012, VTI Technologies was acquired by the Japanese Murata Manufacturing group, and changed its name to Murata Electronics Oy.\n\nProduct range includes MEMS accelerometers, inclinometers, gyroscopes and pressure sensor elements. Typical uses of these products in automotive applications include electronic stability control (ESC), hill-start assist (HSA), antilock braking system (ABS), car alarms and navigation. Medical applications include use in pacemakers and implantable defibrillators (ICD). They are also used in instrumentation for transport, and in consumer electronics.\n"}
{"id": "38784571", "url": "https://en.wikipedia.org/wiki?curid=38784571", "title": "NYU Tandon Online", "text": "NYU Tandon Online\n\nNYU Tandon Online, formerly known as NYU-ePoly, is the online learning department for higher education at New York University Tandon School of Engineering. which is one of the schools of engineering, technology, management and applied sciences in the United States.\n\nCurrently, the School of Engineering offers 8 graduate level programs online, including 6 master's degrees, and 2 advanced certificates.Nasir Memon joined the department as the head in 2017 and introduced the units first non-credit course offering, A Bridge to Tandon. NYU Tandon Online's master's degree programs offer the same curriculum and credentials as their on-campus counterparts. The unit piloted active learning in online courses, engaging students in interactive, high-quality learning modules paired with the university's pedagogical approaches. The platform has been recognized by the Alfred P. Sloan Foundation, Online Learning Consortium & by US News and World Report.\n\nThe “e-Poly” story began back in 2006, as the online learning wing of the then Polytechnic University. The department was initially designed for its on-campus students to blend and accelerate their time to degree completion by taking some courses online. The first program offered entirely online was the Cybersecurity master's degree. By 2009, e-Poly program offerings included 3 master’s degrees and 2 advanced certificates. In 2009, e-Poly was renamed NYU-ePoly, in recognition of the university’s affiliation with NYU. Following the School's name change in October, 2015, the department was renamed NYU Tandon Online. As growth remains constant, today NYU Tandon Online offers over 65 online courses in diverse technology and management fields.\n\nNYU Tandon Online offers online graduate programs in conjunction with various Departments at NYU Tandon School of Engineering. Among them, Cybersecurity, Bioinformatics, Electrical Engineering, Management of Technology, Industrial Engineering, and Computer Engineering. The masters programs offered by NYU Tandon Online follow the same curriculum and approach as their on-campus counterparts. In addition to the 6 Masters programs, it also offers 2 Advanced Certificate programs and the Bridge Program offered in 4 different pathways towards a master's program.\n\nNYU Tandon Online offered its first non-credit offering in 2016. A bridge to NYU Tandon, a computer science bridge for non-computer scientists. The $1,500 program provides individuals, the equivalent of three traditional courses qualifying them for admissions into the Computer Science/Cybersecurity/Bioinformatics/Computer Engineering master's degree programs. The program is specially designed for students who wish to apply to the MS in Computer Science/MS in Cybersecurity/MS in Bioinformatics or MS in Computer Engineering programs at the NYU Tandon School of Engineering but lack the prerequisite computer science training.\n\nApart from the regular masters and certificate programs, NYU Tandon Online also allows individuals to take up to 9 credits at NYU Tandon School of Engineering, without formally applying for admissions. This option is especially beneficial for students who want to learn an advanced topic for personal and professional development.\n\nNYU Tandon School of Engineering is accredited by the Middle States Commission on Higher Education and New York State Education Department to award master’s and graduate certificate degrees.\n\nNYU Tandon Online is associated with many corporations, societies and companies. NYU Tandon Online is also associated with the National Security Agency through their National Centers of Academic Excellence in Cyber Operations Program. Under the direction of NYU Enterprise Learning, the unit partnered with Scientific American to provide interactive short courses called Professional Learning.\n\n\n\n"}
{"id": "44904604", "url": "https://en.wikipedia.org/wiki?curid=44904604", "title": "NetCentrics", "text": "NetCentrics\n\nNetCentrics Corporation, based in Herndon, Virginia, is a contractor to the federal government. NetCentrics is a leading provider of enterprise IT, cloud and cybersecurity services to the federal government. The company was founded in 1995 by Bob Dixon and Bob Dougherty, the company's first CEO. In July 2017, Cynthia Barreda succeeded Dougherty as the current CEO.\n\nNetCentrics was recognized by Comparably in 2017 and again in 2018, receiving a total of 13 awards including Best Companies in DC, Best CEO, Best Company for Diversity, and Best Company Outlook. In 2018, Barreda was recognized by DCA Live as a Federal Entrepreneur and NetCentrics was recognized as one of the fastest growing cybersecurity companies in the DMV. \n"}
{"id": "38556048", "url": "https://en.wikipedia.org/wiki?curid=38556048", "title": "Network function virtualization", "text": "Network function virtualization\n\nNetwork functions virtualization (also network function virtualization or NFV) is a network architecture concept that uses the technologies of IT virtualization to virtualize entire classes of network node functions into building blocks that may connect, or chain together, to create communication services.\n\nNFV relies upon, but differs from, traditional server-virtualization techniques, such as those used in enterprise IT. A virtualized network function, or VNF, may consist of one or more virtual machines running different software and processes, on top of standard high-volume servers, switches and storage devices, or even cloud computing infrastructure, instead of having custom hardware appliances for each network function.\n\nFor example, a virtual session border controller could be deployed to protect a network without the typical cost and complexity of obtaining and installing physical network protection units. Other examples of NFV include virtualized load balancers, firewalls, intrusion detection devices and WAN accelerators.\n\nProduct development within the telecommunication industry has traditionally followed rigorous standards for stability, protocol adherence and quality, reflected by the use of the term carrier grade to designate equipment demonstrating this reliability. While this model worked well in the past, it inevitably led to long product cycles, a slow pace of development and reliance on proprietary or specific hardware, e.g., bespoke application-specific integrated circuits (ASICs). The rise of significant competition in communication services from fast-moving organizations operating at large scale on the public Internet (such as Google Talk, Skype, Netflix) has spurred service providers to look for ways to disrupt the status quo.\n\nIn October 2012, a specification group, \"Network Functions Virtualisation\", \npublished a white paper at a conference in Darmstadt, Germany, on software-defined networking (SDN) and OpenFlow. The group, part of the European Telecommunications Standards Institute (ETSI), was made up of representatives from the telecommunication industry from Europe and beyond.\n\nSince the publication of the white paper, the group has produced several more in-depth materials, including a standard terminology definition and use cases for NFV that act as references for vendors and operators considering to adopt Network Virtualization.\n\nThe NFV framework consists of three main components:\n\nThe building block for both the NFVI and the NFV-MANO is the NFV platform. In the NFVI role, it consists of both virtual and physical processing and storage resources, and virtualization software. In its NFV-MANO role it consists of VNF and NFVI managers and virtualization software operating on a hardware controller. The NFV platform implements carrier-grade features used to manage and monitor the platform components, recover from failures and provide effective security – all required for the public carrier network.\n\nA service provider that follows the NFV design implements one or more virtualized network functions, or \"VNFs\". A VNF by itself does not automatically provide a usable product or service to the provider's customers. To build more complex services, the notion of \"service chaining\" is used, where multiple VNFs are used in sequence to deliver a service.\n\nAnother aspect of implementing NFV is the \"orchestration\" process. To build highly reliable and scalable services, NFV requires that the network be able to instantiate VNF instances, monitor them, repair them, and (most important for a service provider business) bill for the services rendered. These attributes, referred to as carrier-grade features, are allocated to an orchestration layer in order to provide high availability and security, and low operation and maintenance costs. Importantly, the orchestration layer must be able to manage VNFs irrespective of the underlying technology within the VNF. For example, an orchestration layer must be able to manage an SBC VNF from vendor X running on VMware vSphere just as well as an IMS VNF from vendor Y running on KVM.\n\nThe initial perception of NFV was that virtualized capability should be implemented in data centers. This approach works in many – but not all – cases. NFV presumes and emphasizes the widest possible flexibility as to the physical location of the virtualized functions.\n\nIdeally, therefore, virtualized functions should be located where they are the most effective and least expensive. That means a service provider should be free to locate NFV in all possible locations, from the data center to the network node to the customer premises. This approach, known as distributed NFV, has been emphasized from the beginning as NFV was being developed and standardized, and is prominent in the recently released NFV ISG documents.\n\nFor some cases there are clear advantages for a service provider to locate this virtualized functionality at the customer premises. These advantages range from economics to performance to the feasibility of the functions being virtualized.\n\nThe first ETSI NFV ISG-approved public multi-vendor proof of concept (PoC) of D-NFV was conducted by Cyan, Inc., RAD, Fortinet and Certes Networks in Chicago in June, 2014, and was sponsored by CenturyLink. It was based on RAD’s dedicated customer-edge D-NFV equipment running Fortinet’s Next Generation Firewall (NGFW) and Certes Networks’ virtual encryption/decryption engine as Virtual Network Functions (VNFs) with Cyan’s Blue Planet system orchestrating the entire ecosystem. RAD's D-NFV solution, a Layer 2/Layer 3 network termination unit (NTU) equipped with a D-NFV X86 server module that functions as a virtualization engine at the customer edge, became commercially available by the end of that month. During 2014 RAD also had organized a D-NFV Alliance, an ecosystem of vendors and international systems integrators specializing in new NFV applications.\n\nWhen designing and developing the software that provides the VNFs, vendors may structure that software into software components (implementation view of a software architecture) and package those components into one or more images (deployment view of a software architecture). These vendor-defined software components are called VNF Components (VNFCs). VNFs are implemented with one or more VNFCs and it is assumed, without loss of generality, that VNFC instances map 1:1 to VM Images.\n\nVNFCs should in general be able to scale up and/or scale out. By being able to allocate flexible (virtual) CPUs to each of the VNFC instances, the network management layer can scale up (i.e., scale \"vertically\") the VNFC to provide the throughput/performance and scalability expectations over a single system or a single platform. Similarly, the network management layer can scale out (i.e., \"scale horizontally\") a VNFC by activating multiple instances of such VNFC over multiple platforms and therefore reach out to the performance and architecture specifications whilst not compromising the other VNFC function stabilities.\n\nEarly adopters of such architecture blueprints have already implemented the NFV modularity principles.\n\nSDN, or software-defined networking, is a concept related to NFV, but they refer to different domains.. \nNetwork functions virtualization (NFV) and Deep Packet Inspection (DPI) could efficiently complement the SDN functions . \n\nIn essence, software-defined networking (SDN) is an approach to build data networking equipment and software that separates and abstracts elements of these systems. It does this by decoupling the control plane and data plane from each other, such that the control plane resides centrally and the forwarding components remain distributed. The control plane interacts both northbound and southbound. In the northbound direction the control plane provides a common abstracted view of the network to higher-level applications and programs using APIs. In the southbound direction the control plane programs the forwarding behavior of the data plane, using device level APIs of the physical network equipment distributed around the network.\n\nThus, NFV is not dependent on SDN or SDN concepts. It is entirely possible to implement a virtualized network function (VNF) as a standalone entity using existing networking and orchestration paradigms. However, there are inherent benefits in leveraging SDN concepts to implement and manage an NFV infrastructure, particularly when looking at the management and orchestration of VNFs, and that's why multivendor platforms are being defined that incorporate SDN and NFV in concerted ecosystems.\n\nAn NFV infrastructure needs a central orchestration and management system that takes operator requests associated with a VNF, translates them into the appropriate processing, storage and network configuration needed to bring the VNF into operation. Once in operation, the VNF potentially must be monitored for capacity and utilization, and adapted if necessary.\n\nAll these functions can be accomplished using SDN concepts and NFV could be considered one of the primary SDN use cases in service provider environments. It is also apparent that many SDN use-cases could incorporate concepts introduced in the NFV initiative. Examples include where the centralized controller is controlling a distributed forwarding function that could in fact be also virtualized on existing processing or routing equipment.\n\nNFV has proven a popular standard even in its infancy. Its immediate applications are numerous, such as virtualization of mobile base stations, platform as a service (PaaS), content delivery networks (CDN), fixed access and home environments. The potential benefits of NFV is anticipated to be significant. Virtualization of network functions deployed on general purpose standardized hardware is expected to reduce capital and operational expenditures, and service and product introduction times. Many major network equipment vendors have announced support for NFV. This has coincided with NFV announcements from major software suppliers who provide the NFV platforms used by equipment suppliers to build their NFV products.\n\nHowever, to realize the anticipated benefits of virtualization, network equipment vendors are improving IT virtualization technology to incorporate carrier-grade attributes required to achieve high availability, scalability, performance, and effective network management capabilities. To minimize the total cost of ownership (TCO), carrier-grade features must be implemented as efficiently as possible. This requires that NFV solutions make efficient use of redundant resources to achieve five-nines availability (99.999%), and of computing resource without compromising performance predictability.\n\nThe NFV platform is the foundation for achieving efficient carrier-grade NFV solutions. It is a software platform running on standard multi-core hardware and built using open source software that incorporates carrier-grade features. The NFV platform software is responsible for dynamically reassigning VNFs due to failures and changes in traffic load, and therefore plays an important role in achieving high availability. There are numerous initiatives underway to specify, align and promote NFV carrier-grade capabilities such as ETSI NFV Proof of Concept, ATIS Open Platform for NFV Project, Carrier Network Virtualization Awards and various supplier ecosystems.\n\nThe vSwitch, a key component of NFV platforms, is responsible for providing connectivity both VM-to-VM (between VMs) and between VMs and the outside network. Its performance determines both the bandwidth of the VNFs and the cost-efficiency of NFV solutions. The standard Open vSwitch's (OVS) performance has shortcomings that must be resolved to meet the needs of NFVI solutions. Significant performance improvements are being reported by NFV suppliers for both OVS and Accelerated Open vSwitch (AVS) versions.\n\nVirtualization is also changing the way availability is specified, measured and achieved in NFV solutions. As VNFs replace traditional function-dedicated equipment, there is a shift from equipment-based availability to a service-based, end-to-end, layered approach. Virtualizing network functions breaks the explicit coupling with specific equipment, therefore availability is defined by the availability of VNF services. Because NFV technology can virtualize a wide range of network function types, each with their own service availability expectations, NFV platforms should support a wide range of fault tolerance options. This flexibility enables CSPs to optimize their NFV solutions to meet any VNF availability requirement.\n\nETSI has already indicated that an important part of controlling the NFV environment be done through automation and orchestration. There is a separate stream MANO within NFV outlining how flexibility should be controlled.\n\nRecent performance study on NFV focused on the throughput, latency and jitter of virtualized network functions (VNFs), as well as NFV scalability in terms of the number of VNFs a single physical server can support.\n\n"}
{"id": "1384140", "url": "https://en.wikipedia.org/wiki?curid=1384140", "title": "Passenger name record", "text": "Passenger name record\n\nIn the airline and travel industries, a passenger name record (PNR) is a record in the database of a computer reservation system (CRS) that consists of the personal information for a passenger and also contains the itinerary for the passenger, or a group of passengers travelling together. The concept of a PNR was first introduced by airlines that needed to exchange reservation information in case passengers required flights of multiple airlines to reach their destination (“interlining”). For this purpose, IATA and ATA defined standards for interline messaging of PNR and other data through the \"ATA/IATA Reservations Interline Message Procedures - Passenger\" (AIRIMP). There is no general industry standard for the layout and content of a PNR. In practice, each CRS or hosting system has its own proprietary standards, although common industry needs, including the need to map PNR data easily to AIRIMP messages, has resulted in many general similarities in data content and format between all of the major systems.\n\nWhen a passenger books an itinerary, the travel agent or travel website user will create a PNR in the CRS, which may be an airline's database or typically one of the global distribution systems (GDSs), such as Amadeus, Sabre, or Travelport (Apollo, Galileo, and Worldspan). If a booking is made directly with of the airline's CRS, the PNR is called the Master PNR for the passenger and the associated itinerary. The PNR is identified in the CRS by a record locator. If portions of the travel itinerary are not to be provided by the holder of the Master PNR, then copies of the PNR information are sent to the CRSs of the airlines that will be providing transportation. These CRSs will open copies of the Master PNR in their own database to manage the portion of the itinerary for which they are responsible. Many airlines have their CRS hosted by one of the GDSs, which allows sharing of the PNR. The record locators of the copied PNRs are communicated back to the CRS that holds the Master PNR, so all records remain tied together. This enables exchanging updates of the PNR when the status of the trip changes in any of the CRSs.\n\nAlthough PNRs were originally introduced for air travel airlines systems, they can also be used for bookings of hotels, car rental, airport transfers, and train trips. PNRs are now routinely shared with government agencies, such as customs and security or law enforcement agencies.\n\nFrom a technical point, there are five parts of a PNR required before the booking can be completed. They are:\n\nOther information, such as a timestamp and the agency's pseudo-city code, will go into the booking automatically. All entered information will be retained in the \"history\" of the booking.\n\nOnce the booking has been completed to this level, the CRS will issue a unique all alpha or alpha-numeric record locator, which will remain the same regardless of any further changes made (except if a multi-person PNR is split). Each airline will create their own booking record with a unique record locator, which, depending on service level agreement between the CRS and the airline(s) involved, will be transmitted to the CRS and stored in the booking. If an airline uses the same CRS as the travel agency, the record locator will be the same for both. \n\nA considerable amount of other information is often desired by both the airlines and the travel agent to ensure efficient travel, including:\n\n\nIn more recent times, many governments now require the airline to provide further information included to assist investigators tracing criminals or terrorists.\nThese include:\n\nThe components of a PNR are identified internally in a CRS by a one character code. This code is often used when creating a PNR via direct entry into a terminal window (as opposed to using a graphical interface). The following codes are standard across all CRSs based on the original PARS system:\n\nThe majority of airlines and travel agencies choose to host their PNR databases with a computer reservations system (CRS) or global distribution system (GDS) company such as Sabre, Galileo, Worldspan and Amadeus.\n\nSome privacy organizations are concerned at the amount of personal data that a PNR might contain. While the minimum data for completing a booking is quite small, a PNR will typically contain much more information of a sensitive nature.\nThis will include the passenger’s full name, date of birth, home and work address, telephone number, e-mail address, credit card details, IP address if booked online, as well as the names and personal information of emergency contacts. The PNR data in part can be accessed through the airline website by using a 6-character alphanumeric record locator code. This code can also be found in boarding pass barcodes.\n\nDesigned to “facilitate easy global sharing of PNR data,” the CRS-GDS companies “function both as data warehouses and data aggregators, and have a relationship to travel data analogous to that of credit bureaus to financial data.”. A canceled or completed trip does not erase the record since “copies of the PNRs are ‘purged’ from live to archival storage systems, and can be retained indefinitely by CRSs, airlines, and travel agencies.” Further, CRS-GDS companies maintain web sites that allow almost unrestricted access to PNR data – often, the information is accessible by just the reservation number printed on the ticket.\n\nAdditionally, “[t]hrough billing, meeting, and discount eligibility codes, PNRs contain detailed information on patterns of association between travelers. PNRs can contain religious meal preferences and special service requests that describe details of physical and medical conditions (e.g., “Uses wheelchair, can control bowels and bladder”) – categories of information that have special protected status in the European Union and some other countries as “sensitive” personal data.”\n\nDespite the sensitive character of the information they contain, PNRs are generally not recognized as deserving the same privacy protection afforded to medical and financial records. Instead, they are treated as a form of commercial transaction data.\n\nAfter years of debate, the European Parliament approved on 14 April 2016 the PNR directive. This directive 2016/681 will oblige airlines to collect their passengers' data and hand them over to the EU member states.\n\nThe Belgian Chamber of Representatives approved on 22 December 2016 a draft law to transpose the European directive in a Belgian legislative framework. This law goes further and aims the recording of train passengers' data as well.\n\nSee United States–European Union Agreement on Passenger Name Records.\n\nOn January 16, 2004, the Article 29 Working Party released their \"Opinion 1/2004 (WP85)\" on the level of PNR protection ensured in Australia for the transmission of Passenger Name Record data from airlines.\n\nIn 2010 the European Commission's Directorate-General for Justice, Freedom and Security was split in two. The resulting bodies were the Directorate-General for Justice (European Commission) and the Directorate-General for Home Affairs (European Commission).\n\nOn the 4th of May 2011, Stefano Manservisi, Director-General at the Directorate-General for Home Affairs (European Commission) wrote to the European Data Protection Supervisor (EDPS) with regards to a PNR sharing agreement with Australia, a close ally of the US and signatory to the UKUSA Agreement on signals intelligence.\n\nThe EDPS responded on the 5th of May in \"Letter 0420 D845\":\n\nThe Article 29 Working Party document \"Opinion 1/2005 on the level of protection ensured in Canada for the transmission of Passenger Name Record and Advance Passenger Information from airlines (WP 103)\", 19 January 2005, offers information on the nature of agreements with Canada.\n\n\n"}
{"id": "9668598", "url": "https://en.wikipedia.org/wiki?curid=9668598", "title": "Planetarium Negara", "text": "Planetarium Negara\n\n<mapframe latitude=\"3.13927\" longitude=\"101.68893\" zoom=\"17\" width=\"200\" height=\"200\" align=\"right\" />\n\nThe National Planetarium () is the national planetarium of Malaysia. It is a blue-domed structure that is situated on top of a hill in the Lake Gardens at Jalan Perdana, Kuala Lumpur. It is about in area.\n\nThe National Planetarium started as the Planetarium Division in the Prime Minister's Department in 1989. The construction of the National Planetarium complex began in 1990 and was completed in 1993. A soft launch to the public began in May 1993 and it was officially opened by the Prime Minister of Malaysia, Mahathir bin Mohamad, on 7 February 1994. In July 1995, the Division was transferred to the Ministry of Science, Technology and the Environment, which is also the owner of this planetarium until now.\n\nOne of the major attractions of this planetarium includes a space theatre which screens space shows and large format film.\n\nIn the main hall are permanent exhibits related to space science. Among them is Arianne IV space engine, which is also one of the engines used to launch MEASAT 1, Malaysia's first satellite into space. A telescope is located in the observatory.\n\nThe National Planetarium extends to a space theme park where replicas of ancient observatories are sited. It is connected by an overhead pedestrian bridge to the National Museum of Malaysia.\n\nThe planetarium is opened daily from 9.00 am to 4.30 pm (except on Monday, Hari Raya Puasa holiday, Hari Raya Haji holiday and public holidays as stated by the Planetarium Negara). The planetarium shows will be aired every one hour starting at 10am and the last show will be at 4pm. The entrance to the gallery is free and the shows ticket is RM 12 for adult (per person) and RM 8 for children (per person).\n\nThe planetarium is accessible within walking distance west of Kuala Lumpur railway station.\n"}
{"id": "22025119", "url": "https://en.wikipedia.org/wiki?curid=22025119", "title": "Plunger lift", "text": "Plunger lift\n\nA plunger lift is an artificial lift method of deliquifying a natural gas well. A plunger is used to remove contaminants from productive natural gas wells, such as water (in liquid, mist, or ice forms), sand, oil and wax.\n\nThe basics of the plunger is to open and close the well shutoff valve at the optimum times, to bring up the plunger and the contaminants and maximize natural gas production. A well without a deliquification technique will stop flowing or slow down and become a non-productive well, long before a properly deliquified well.\n\nThe plunger lift has low energy cost, low environmental impact, low capital investment and low maintenance cost. Modern wellhead controllers offer a variety of criteria to control the plunger. The original controllers were just timers, with fixed open and close cycles.\n\nMeasuring the various pressures in the system allows intelligent and reactive control. The pressures often measured are casing, tubing, line, and differential (DP). The other items measured are plunger arrival times, flow rates, temperatures and status of various auxiliary equipment: oil tank level, compressor status.\n\n\n\n\n"}
{"id": "33433141", "url": "https://en.wikipedia.org/wiki?curid=33433141", "title": "Roasting pan", "text": "Roasting pan\n\nA roasting pan is a piece of cookware used for roasting meat in an oven, either with or without vegetables or other ingredients. A roasting pan may be used with a rack that sits inside the pan and lets the meat sit above the fat and juice drippings.\n\nA shallow roasting pan is normally used for roasting small cuts of meat, but large-size roasting pans are also used for cooking large poultry such as turkey or goose, or for larger cuts of meat. A deep roasting pan can hold vegetables and other ingredients that meat can sit on rather than a rack, letting the vegetables absorb the fat and juice from the meat while cooking. A deep roasting pan can also be used as a baking dish or basin, holding smaller baking dishes that must be surrounded by boiling water.\n\nRoasting pans are made in several materials that offer their own unique benefits:\n\nCovering or uncovering a roasting pan will depend upon the type of meat being roasted, or the recipe being followed. Generally, meat that is high in fat content, such as duck, has no need of being covered during roasting. A lean cut of meat, however, will benefit from being covered during a long roasting time, to retain juices and soften the meat (for example, in using a tajine for roasting meat).\n\nA roasting rack may be an accessory part of the roasting pan. A rack may be horizontal and lay flat on the roasting pan, or it may be a standing rack that sits vertically in the pan. A standing rack is most commonly used for roasting poultry. A variation of the roasting pan standing rack is the beer can chicken recipe.\n\n"}
{"id": "1683865", "url": "https://en.wikipedia.org/wiki?curid=1683865", "title": "Shaker furniture", "text": "Shaker furniture\n\nShaker furniture is a distinctive style of furniture developed by the United Society of Believers in Christ's Second Appearing, commonly known as Shakers, a religious sect that had guiding principles of simplicity, utility and honesty. Their beliefs were reflected in the well-made furniture of minimalist designs.\n\nShaker communities were largely self-sufficient: in their attempt to separate themselves from the outside world and to create a heaven-on-earth, members grew their own food, constructed their own buildings, and manufactured their own tools and household furnishings.—Metropolitan Museum of Art\n\nFurniture was made thoughtfully, with functional form and proportion. Rather than using ornamentation — such as inlays, carvings, metal pulls, or veneers — which was seen as prideful or deceitful, they developed \"creative solutions such as asymmetrical drawer arrangements and multipurpose forms to add visual interest.\" Furniture was made of cherry, maple or pine lumber, which was generally stained or painted with one of the colors which were dictated by the sect, typically blue, red, yellow or green. Drawer pulls for dressers or other furniture were made of wood.\n\nA core business for the New Lebanon Shaker community by the 1860s was the production of well-made \"ladder\" back or turned post chairs. The minimalist design and woven seats were fast and easy to produce. Furniture built and used by the New Lebanon \"believers\" is exhibited in the Shaker Retiring Room at the Metropolitan Museum of Art in New York City, which originated from the North Family Shakers' 1818 First Dwelling House. The furniture, acquired in the 1970s, and Shaker textiles are considered among the finest Shaker collections in the world.\n\nMany examples of Shaker furniture survive and are preserved today, including such popular forms as Shaker tables, chairs, rocking chairs (made in several sizes), and cabinets. Collections of Shaker furniture are maintained by many art and historical museums in the United States and the United Kingdom, as well as in numerous private collections including the Shaker tilting chair. The underlying principles of Shaker design have given inspiration to some of the finest designers of modern furniture. Shaker ladder back chairs, for instance, deeply influenced the work of an entire generation of postwar Danish designers. Also many ideals of furniture formed around the common Shaker furniture construction.\n\n\n\n\n"}
{"id": "3710585", "url": "https://en.wikipedia.org/wiki?curid=3710585", "title": "Software analyst", "text": "Software analyst\n\nIn a software development team, a software analyst is the person who studies the software application domain, prepares software requirements, and specification (Software Requirements Specification) documents. The software analyst is the seam between the software users and the software developers. They convey the demands of software users to the developers.\n\nA software analyst is expected to have the following skills:\n\n"}
{"id": "48125053", "url": "https://en.wikipedia.org/wiki?curid=48125053", "title": "Space (architecture)", "text": "Space (architecture)\n\nSpace is one of the elements of design of architecture, as space is continuously studied for its usage. Architectural designs are created by carving space out of space, creating space out of space, and designing spaces by dividing this space using various tools, such as geometry, colours, and shapes.It is an undefined expanse of land given to an architect to define.\n\n"}
{"id": "19373580", "url": "https://en.wikipedia.org/wiki?curid=19373580", "title": "Stand guidance system", "text": "Stand guidance system\n\nA stand guidance system is a system which gives information to a pilot attempting to park an aircraft at an airport stand, usually via visual methods, leading to the term Visual Docking Guidance System (VDGS) and also \"A-VDGS\" (the A standing for advanced) This allows them to remain clear of obstructions and ensures that jetways can reach the aircraft.\n\nAzimuth Guidance for Nose-In Stand is one of the most popular forms of stand guidance. It consists of two coloured lights mounted side by side.\n\nIf the pilot is on the stand centreline they will see two green lights. If they are off centreline, one of the lights will appear red and the pilot then steers towards the green one. AGNIS alone provides only azimuth guidance, it does not inform pilots when they should stop. It is relatively imprecise but cheap to implement and reliable.\n\nThe Parallax Aircraft Parking Aid is frequently combined with an AGNIS system, informing flight crews when to stop. The device features no electronics or moving parts; it consists simply of a large grey box (usually with one or more sides missing) with a large rectangular slot cut into the front.\n\nInside the box, towards the rear, is a white stick or fluorescent tube, which appears to move from one side of the slot to the other as the viewer moves closer, although it is in fact fixed and the effect is merely due to perspective (see parallax). Above and/or below this slot will be markings in white or yellow, indicating where different types of plane should stop.\n\nAs this system relies on the position of the viewer, it will not give accurate distance information to aircraft which have deviated significantly from the stand centreline.\n\nThis is a simple two phase traffic light with red and green lights, mounted to the side of the AGNIS lights. Typically these are round and mounted vertically, in order to avoid confusion to AGNIS lights which are also red and green but mounted side by side and usually square.\n\nUsed in combination with AGNIS at stands which can only accept smaller airliners, it features one or two mirrors, allowing the flightcrew to see ground markings in relation to their nose wheel, when it is within the area they need to stop. Typically two mirrors are used, angled differently to suit the various heights of cockpits from the ground.\n\nAdvanced Visual Docking Guidance Systems feature electronic displays which perform the functions of an AGNIS/PAPA installation, although with much greater accuracy. They may also provide collision avoidance from static objects.\nAn infrared high definition camera is scanning the entire area for possible objects, affecting the safety of the aircraft. \nThe usual distance is from 8–50 meters . The A-VGDS, is equipped with a low visibility function allowing aircraft to park in extremely poor visibility conditions .\n\nA-VDGS systems usually have emergency stop buttons located both on the stand and in the jetway/gate area, which causes the stop indication to appear immediately.\n\nRLG GIS-206 product line which utilizes cutting-edge laser technology to reduce error margins, enhance ease-of-use and installation. Accommodate all aircraft types without the error factors resulting from variations in aircraft height and gross weight. All RLG docking systems comply with ICAO Annex 14 recommendations. \n\nThe Aircraft Positioning and Information System (APIS++) is manufactured by FMT. The left side of the unit displays Azimuth guidance; providing the pilot with vital information when as far as 200 meters from the stand and while the aircraft is still perpendicular to the stand centreline. The right side of the unit gives the pilot precise information concerning closing rate as well as instructions when to stop.\n\nThe azimuth guidance, based on the Moiré Pattern works independently of the laser that calculates distance to stop and is unaffected by weather conditions.\n\nThe laser, which has a pulse frequency of 9.6 kHz and a resolution of 1 cm, ensures that the pilot can stop in the exact stopping position. The system can be connected with the passenger boarding bridge, apron management system or other airport systems via TCP/IP or Ethernet connections. It can also be equipped with a back-up traffic light system.\n\nThe green lights on the left and right of the display will begin to appear, from bottom to top, as the aircraft gets closer to the stop line. The green centreline on the display moves smoothly left and right to accurately indicate how away from the centreline an aircraft is.\n\nSafedock by Safegate, allows aircraft to park up to an accuracy of 10 cm using invisible infrared lasers to attain the aircraft's position and type for easier docking. It comes in a variety of models; the T1, the T2 and the T3. Red and yellow arrows indicate in which direction the pilot needs to manoeuvre to be on the centreline. Each model has an S variant (e.g. T1S), which also indicates the position of the aircraft relative to the centreline. In either variant, a central yellow column disappears from bottom to top as the aircraft approaches the stop point.\n\nMarshallers use a variety of arm signals to indicate where aircraft are in relation to both the stop line and the centreline. Signals used by marshallers can vary between countries and services (for example, some may be specific to aircraft carriers).\n\n"}
{"id": "32182026", "url": "https://en.wikipedia.org/wiki?curid=32182026", "title": "Switching circuit theory", "text": "Switching circuit theory\n\nSwitching circuit theory is the mathematical study of the properties of networks of idealized switches. Such networks may be strictly combinational logic, in which their output state is only a function of the present state of their inputs; or may also contain sequential elements, where the present state depends on the present state and past states; in that sense, sequential circuits are said to include \"memory\" of past states. An important class of sequential circuits are state machines. Switching circuit theory is applicable to the design of telephone systems, computers, and similar systems. Switching circuit theory provided the mathematical foundations and tools for digital system design in almost all areas of modern technology.\n\nFrom 1934 to 1936, NEC engineer Akira Nakashima published a series of papers showing that the two-valued Boolean algebra, which he discovered independently, can describe the operation of switching circuits. His work was later cited and elaborated on in Claude Shannon's seminal 1938 paper \"A Symbolic Analysis of Relay and Switching Circuits\". The principles of Boolean algebra are applied to switches, providing mathematical tools for analysis and synthesis of any switching system.\n\nIdeal switches are considered as having only two exclusive states, for example, open or closed. In some analysis, the state of a switch can be considered to have no influence on the output of the system and is designated as a \"don't care\" state. In complex networks it is necessary to also account for the finite switching time of physical switches; where two or more different paths in a network may affect the output, these delays may result in a \"logic hazard\" or \"race condition\" where the output state changes due to the different propagation times through the network.\n\n\n"}
{"id": "52344552", "url": "https://en.wikipedia.org/wiki?curid=52344552", "title": "The Biosphere Rules", "text": "The Biosphere Rules\n\nThe Biosphere Rules is a framework for implementing closed loop production in business. They emerged from a 2005 research project at IE Business School that identified the principles that facilitate circular processes in nature but interpreted for—and translated to—industrial production systems. The research indicated that adopting the principles allowed businesses to establish economically and environmentally sustainable closed-loop manufacturing systems.\n\nThe five principles that constitute the Biosphere Rules are briefly:\n\n1. Materials parsimony.\n\nMinimize the types of materials used in products with a focus on materials that are life-friendly and economically recyclable.\n\n2. Value cycle.\n\nRecover and reincarnate materials from end-of-use goods into new value-added products.\n\n3. Power autonomy.\n\nMaximize the power autonomy of products and processes so they can function on renewable energy.\n\n4. Sustainable product platforms.\n\nLeverage value cycles as product platforms for profitable scale, scope, and knowledge economies.\n\n5. Function over form.\n\nFulfill customers’ functional needs in ways that sustain the value cycle.\n\nThis biomimetic framework contends that these principles facilitate the transition of human manufacturing systems business towards a functioning circular economy.\n\nThe Biosphere Rules emerged from a research program established through a 2005 partnership between the renowned eco-designer William McDonough and the Center for Eco-Intelligent Management at IE Business School, a top-ranked European management institute. The research program identified the attributes of natural systems that allowed for the closed-loop production of organisms in the biosphere. These attributes were then used as a framework for analyzing concrete company examples in the program’s second phase. Case studies of first mover companies adopting closed loop or cradle-to-cradle design for product development found common elements that made closed-loop systems viable in a business context. Importantly, the identified elements were similar to the principles seen in natural systems, mapped onto human manufacturing approaches.\n\nThe principles were first published in the February 2008 issue the Harvard Business Review in an article entitled “The Biosphere Rules.” They were later expanded upon in a book entitled “Earth, Inc.: Using Nature’s Rules to Build Sustainable Profits” published in 2010 by the Harvard Business School Press.\n\nRule #1: Materials Parsimony\n\nThe Materials Parsimony rule deals with constraining the number of types of materials used in product design and manufacturing. This is not to be confused with the sustainable business strategy of eco-efficiency which seeks to reduce the amount of materials used in production.\n\nThe parsimony rule comes instead from applying a biomimicry perspective to materials in the biosphere. The Periodic Table of the Elements encompass the 88 naturally occurring elements from arsenic to xenon. Yet despite this diversity of options, the biosphere relies on four elements—carbon, hydrogen, oxygen, and nitrogen (CHON)—as the foundation of every living thing on earth. Adding trace amounts of sulfur, phosphorus and calcium we can account for the weight of 99 percent of every living thing on the planet. Parsimony in the biosphere makes it possible break down an organism like a rabbit locally and reassemble its constituent materials into a tree, mushroom or even another rabbit.\n\nThe functional benefit of materials parsimony for circular economy is that it dramatically simplifies the logistics and transaction costs of recycling, while producing scale economies through supply concentration. Applying the Materials Parsimony rule to industry requires simplifying of the number and types of materials used in products. While expecting business to use the same four elements as nature is currently impractical, businesses can dramatically reduce the number of materials employed in manufacture, with a small number doing the yeoman’s work. For special isolated applications, a smaller set of select materials could be reserved and designed to be easily separated from the bulk of the product. By making these choices, business could absorb a large percent of production within a circular materials economy.\n\nA first step towards materials parsimony that some companies are using is the materials sourcing strategy known as “green screening,” which seeks to exclude potentially hazardous materials from products. Limiting the “materials palette” to environmentally benign inputs at the design stage makes dealing with product and process waste much easier. It also can have the effect of dramatically reducing the number of materials in the product designer’s pallet. Beyond green screening, the emerging strategy of uni-materialization or monomateriality takes the material parsimony approach to its logical extreme by seeking to design products from a single material.\n\nRule #2: Value Cycling\n\nThe second principle - Value Cycling - refers to the actual cyclical reuse of materials from one high value use to another. It is called a “value cycle” to differentiate from the “value chain” model that is typical of current production approaches, whereby raw materials are converted into products and then into waste in a linear stepwise process.\n\nIn the biosphere, value cycling occurs at the atomic and molecular levels. The same materials that compose a leaf can be decomposed and incorporated into a new tree, a worm or other organisms. Technological limitations generally constrain human manufacturing system’s ability to mimic natural molecular cycling, but the approach can be applied analogously at different levels. Materials in products can be value cycled in “shallow loops” or “deep loops”.\n\nWith “shallow-loop” cycling, manufacturers adopt recycling at the “components level” through the remanufacture, refurbishment or reuse of product parts and components. In contrast, “deep-loop” recycling refers to the regeneration at the material level. Currently a limited number of materials can be deep-loop value cycled, such as metals, glass and some plastics. As green chemistry research results in new polymers that can be recycled at the materials level, greater adoption of deep-loop value cycling approaches can be implemented.\n\nRule #3: Power Autonomy\n\nEvery transformation of materials in nature - from turtle to tree - requires energy. In nature, the energy source is solar power captured biologically through photosynthesis. A tree’s captured energy can then be transferred throughout ecosystems through the trophic pyramid to herbivores and carnivores. Plants and animals also have inherent biological processes that absorb energy and store it in chemical form for later use. Solar energy therefore serves as the basis of the biosphere’s transformations of material.\n\nHuman manufacturing, in contrast, has relied primarily on fossil fuels like oil, gas and coal to extract and transform materials. Fossil fuels cannot be considered a sustainable energy solution for many reasons, including the fact that they are finite and their exploitation is reversing important planetary biogeochemical processes producing unwanted climatic and ecosystem impacts. The power autonomy rule applied to human manufacturing requires that products and production processes run on solar-motivated renewable energy sources as in the biosphere.\n\nThe power autonomy approach sees this occurring in a two step process. The first step is to increase the energy efficiency products and production processes. As energy efficiency increases new options for generation and energy storage open up because less total energy is needed. The renewable energy technologies and energy storage technologies can be adopted that move the business towards a state of autonomous power generation.\n\nPower autonomy can be seen from a fractal perspective in that it can occur at many levels. Power autonomy at the product level would be implemented by designing products that can capture and store their own energy. At the corporate level, a power autonomous company could be one that generates all its energy use from renewable sources. And power autonomy can be considered at the city or state level as in the concept of energy independence.\n\nRule #4: Sustainable Product Platforms\n\nIt is estimated that there 8.7 million species on the planet, yet all this diversity is built upon a single materials and energy platform consisting of a solar powered value cycle and a parsimonious materials pallet (CHON). Nature’s platform allows economies of scale, scope and knowledge that drive the proliferation of species into every habitable niche on the planet. Scale economies occur through reproduction and the duplicative growth of species population. Scope economies, on the other hand, come through speciation, the evolutionary emergence of new species, based on the same platform as individual species. Finally, knowledge economies arise through the accumulative encoding, refining and sharing of survival information genetically overtime in DNA.\n\nThe Sustainable Production Platform principle allows businesses to emulate nature’s platform approach in manufacturing and generate similar economies. A sustainable product platform consists of a parsimonious materials palette and associated processing technologies that are assembled into a power autonomous value cycle that is flexible enough to produce a wide variety of products. If managers only use this system to produce a single product, the realized increasing returns would come solely from scale economies (spreading the fixed costs over an increasing unit output). Henry Ford exploited economies of scale by efficiently mass-producing a single car, the Model-T. But by treating the materials-process combination as a fundamental design platform and leveraging it across an entire family of products, managers can foster the scale, scope, and knowledge economies that will continually optimize the value cycle and build larger and more durable returns for their business and society. In the 1920s, General Motors began exploiting an economy of scope by offering a variety of vehicle designs and brands built on a single automobile design platform.\n\nSustainable Product Platforms can be built across a spectrum of governance options. At one end manufacturers can vertically integrate all the stages of the value cycle retaining full ownership of their materials and product components. At the other extreme, value cycles can be completely open source where materials are value cycled through market forces. This is the case for most commodity recycling today such as steel and aluminum.\n\nRule #5: Function Over Form\n\nThroughout the geological record, there is evidence of nature’s constant evolutionary experimentation with life forms. Over billions of years, nature has produced innumerable species in an effort to take advantage of every ecological opportunity. But despite this evident diversity of forms, there are clear patterns in the ecological functions organisms fulfill. Classes of organisms – producers, predators, pollinators, parasites – serve specific ecosystem functions. The form and its function are encoded in the genes of species, which allow the distributed local manufacturing of organisms necessary to fulfill needed ecosystem functions within the context of a globally integrated system.\n\nIn contrast, human manufacturing has tended to emphasize the commercialization of a specific product form rather than the underlying function that the product is intended to serve. Implementing the Biosphere Rules requires that engineers to shift design thinking towards providing a desired function, using the capabilities of the sustainable product platform as a design constraint on form. This approach logically leads to servicization and product of service strategies.\n\nImplementing the Biosphere Rules at an established company require longterm strategic commitments and investment and there are many organizational and other barriers to adopting circular economy practices. The Biosphere Rules, however, were designed in a modular fashion that allows for stepwise implementation. A company can take action to review its input sourcing decisions and move towards greater materials parsimony without having to implement all the other rules simultaneously. Other rules can be implemented in a similar sequential manner which eases the disruption to existing systems and helps facilitate a smooth transition towards a circular business model. However, the ultimate goal is an integrated sustainable business system.\n\nUnruh, Gregory C. \"The biosphere rules.\" Harvard Business Review 86.2 (2008): 111-117.\n\nUnruh, Gregory. Earth, Inc.: Using nature's rules to build sustainable profits. Harvard Business Press, 2010.\n\nHarvard Business Review Webinar, \"Earth, Inc.: Using Nature’s Rules to Build Sustainable Profits\" https://hbr.org/webinar/2012/07/earth-inc-using-natures-rules\n"}
{"id": "2684415", "url": "https://en.wikipedia.org/wiki?curid=2684415", "title": "Through-the-lens metering", "text": "Through-the-lens metering\n\nIn photography, through-the-lens (TTL) metering refers to a feature of cameras whereby the intensity of light reflected from the scene is measured through the lens; as opposed to using a separate metering window or external hand-held light meter. In some cameras various TTL metering modes can be selected. This information can then be used to set the optimal film or image sensor exposure (average luminance), it can also be used to control the amount of light emitted by a flash unit connected to the camera.\n\nThrough-the-lens metering is most often associated with single-lens reflex (SLR) cameras.\n\nIn most film and digital SLRs, the light sensor(s) for exposure metering are incorporated into the pentaprism or pentamirror, the mechanism by which a SLR allows the viewfinder to see directly through the lens. As the mirror is flipped up, no light can reach there during exposure, the necessary amount of exposure needs to be determined before the actual exposure. Consequently, these light sensors could traditionally be used for ambient light TTL metering only. In newer SLRs as well as in almost all DSLRs, they can also be utilized for preflash TTL metering, where the metering is carried out before the mirror flips up using a small preflash of known intensity and the necessary amount of flash light is extrapolated from the reflected flash light measured by the metering cells in the roof of the camera and is then applied during the exposure without any possible real-time feedback.\n\nThere were a few particularly sophisticated film SLRs including the Olympus OM-2, the Pentax LX, the Nikon F3, and the Minolta 9000, where metering cells located at the bottom of the mirror box were used for ambient light metering, depending on model either instead or in addition to metering cells in the roof of the camera. Depending on model, the light was reflected down there either by a secondary mirror behind the half-transparent main mirror, a special reflective coating of the first shutter curtain, the surface of the film itself, or combinations thereof. One of the advantages of this approach is that the measuring result requires no adjustments when changing focusing screens or viewfinders. Also, some of the cameras using this configuration (f.e. the Minolta 9000) are virtually immune against measurement errors caused by light reaching the metering cells at larger angles, for example with shift/tilt lenses.\n\nMetering cells located at the bottom of the mirror box using light reflected off the film are also used in all film SLRs supporting the classical form of real-time TTL flash metering.\n\nSome early Pentax DSLRs could use this same configuration for TTL flash metering as well, but since the reflectance properties of image sensors differ significantly from those of film, this method proved to be unreliable in practice. Therefore, digital SLR cameras typically don't support any real-time TTL flash metering and must use preflash metering instead. The ambient and flash light metering is then carried out by a metering module located in the roof of the camera (see above).\n\nDigital SLRs supporting live view or video will use the read out of the image sensor itself for exposure metering in these modes. This also applies to Sony's SLT digital cameras, which use the image sensor for exposure metering all the time.\nUp to the time of this writing (2012), no digital SLR or SLT camera on the market supported any form of real-time TTL flash metering using the image sensor. However, it can be expected that such methods will be introduced as image sensor technology progresses, given the advantages of metering with real-time feedback and without preflash.\n\nTTL metering systems have been incorporated into other types of cameras as well. Most digital \"point-and-shoot cameras\" use TTL metering, performed by the imaging sensor itself.\n\nIn many advanced modern cameras multiple 'segments' are used to acquire the amount of light in different places of the picture. Depending on the mode the photographer has selected, this information is then used to correctly set the exposure. With a simple spot meter, a single spot on the picture is selected. The camera sets the exposure in order to get that particular spot properly exposed. On some modern SLR systems the spot metering area or zone can be coupled to the actual focusing area selected offering more flexibility and less need to use exposure lock systems. With multiple segment metering (also known as matrix or honeycomb metering), the values of the different segments are combined and weighted to set the correct exposure. Implementations of these metering modes vary between cameras and manufacturers, making it difficult to predict how a scene will be exposed when switching cameras.\n\nIn the 1970s Olympus marketed the OM-2 camera, which measured the exposure directly off the film (OTF). In OTF metering used by Olympus, metering was performed in one of two ways — or a combination of both — depending upon the shutter speed in use.\n\nIn the OM-2's Auto Dynamic Metering (ADM) system the first shutter curtain had the lens-facing side coated with a computer generated pattern of white blocks to emulate an average scene. As the mirror flipped-up the metering cell in the base of the mirror box measured the light reflected from the subject bouncing off this pattern of blocks. The timing of the release of the second curtain was adjusted in real time during the actual exposure. As the shutter speed increased, the actual light reflecting off the film surface was measured and the timing of the second curtain's release adjusted accordingly. This gave cameras equipped with this system the ability to adjust to changes in lighting during the actual exposure which was useful for specialist applications such as photomicrography and astronomical photography.\n\nLeica later used a variation of this system, as did Pentax with their Integrated Direct Metering (IDM) in the LX camera.\nA variation of this \"OTF\" system was used on early Olympus E-Series digital cameras to fine-tune the exposure just before the first curtain was released; for this to work, the first curtain was coated in a neutral grey colour.\n\nThe process of calculating the correct amount of flash light can also be done 'through the lens'. This is being done in a significantly different way than non-flash 'through the lens' metering. The actual metering itself happens in two different ways, depending on the medium. Digital TTL works differently than analog TTL.\n\nThe analog version of TTL works as follows: when the incoming light hits the film, a part of it is reflected towards a sensor. This sensor controls the flash. If enough light is captured, the flash is stopped. During early testing of this system by Minolta and Olympus it was found that not all brands and types of film reflect the light to the same amount although the actual difference between brands was less than half a stop. The one exception was Polaroid's instant slide film which had a black surface and could not be used in TTL flash mode. Nevertheless, for most applications analogue TTL flash exposure metering was more advanced and accurate than systems used previously and permitted far more flexibility - with bounced flash exposures in particular being more accurate than manually calculated equivalents.\n\nWith digital, this way of direct reflection metering is not possible any more since a CMOS or CCD chip, used to collect the light, is not reflective enough. There are a few older digital cameras which still use the analog technique, but these are getting rare. The Fujifilm S1 and S3 are the most known to use this technique.\n\nDigital TTL works as follows: Before the actual exposure one or more small flashes, called \"preflashes\", are emitted. The light returning through the lens is measured and this value is used to calculate the amount of light necessary for the actual exposure. Multiple pre-flashes can be used to improve the flash output. Canon refers to this technique as \"E-TTL\" and has later improved the system with \"E-TTL II\". The first form of digital TTL by Nikon, called \"D-TTL\", was used in a few early models. Since then, the superior \"i-TTL\" system has been used.\n\nWhen using front-curtain flash (when the flash fires immediately after the shutter opens), the preflashes and main flash appear as one to the human eye, since there is very little time between them. When using rear-curtain flash (when the flash fires at the end of the exposure) and a slow shutter speed, the distinction between the main flash and the preflashes is more obvious.\n\nSome cameras and flash units take more information into account when calculating the necessary flash output, including the distance of the subject to the lens. This improves the lighting when a subject is placed in front of a background. If the lens is focused on the subject, the flash will be controlled to allow for proper exposure on the subject, thus leaving the background underexposed. Alternatively, if the lens is focused on the background, the background will be properly exposed, leaving the subject in the foreground typically overexposed. This technique requires both a camera capable of calculating the distance information, as well as the lens being capable of communicating the focal distance to the body. Nikon refers to this technique as \"3D matrix metering\", although different camera manufacturers use different terms for this technique. Canon incorporated this technique in E-TTL II.\n\nMore advanced TTL flash techniques include off-camera flash lighting, where one or more flash units are located at different locations around the subject. In this case a 'commander' unit (which can be integrated in the camera body) is used to control all of the remote units. The commander unit usually controls the remote flashes by using flashes of visible or infrared light, although TTL-capable radio triggering systems are available. The photographer can normally vary the light ratios between the different flashes. The technique of using preflashes to get a proper exposure is still used in automatic flash modes.\n\nThe first camera to feature through-the-lens light metering was by Japanese company Nikon, with a prototype rangefinder camera, the SPX. The camera used Nikon 'S' type rangefinder lenses.\n\nJapanese company Pentax was the first manufacturer to show an early prototype 35 mm behind-the-lens metering SLR camera, which was named the Pentax Spotmatic. The camera was shown at the 1960 photokina show. The first TTL light metering SLR was the 1963 Topcon RE Super, which had the CdS metering cell placed behind the reflex mirror.\n\n\n"}
{"id": "52378237", "url": "https://en.wikipedia.org/wiki?curid=52378237", "title": "UNICAF", "text": "UNICAF\n\nFounded in 2012, (UNICAF; /ˈjuːnᵻkɑːf/) is an online platform in sub-Saharan Africa that is dedicated to making higher education of international standard accessible to African professionals and young school leavers.\n\nUNICAF was founded in 2012, with its name reflecting its origins, having initially evolved from the University of Nicosia, the largest, independent tertiary education institution in Cyprus. While initially focusing solely on Africa, it has been expanding its range to offer scholarships to students worldwide, provided they meet certain eligibility criteria. UNICAF has established a number of branch campuses and information centres in Africa, such as in Nigeria, Ghana, Kenya, Mauritius, Uganda, Somalia, Zambia and Malawi. Already more than 12,000 have taken advantage of the UNICAF scholarships and registered to one of the partnership programmes. The number of students is estimated to increase to 60,000 by the year 2020. The learning centres are set up to include digital libraries, computer labs, internet, generators and make students less prone to the studying in areas with partially inadequate infrastructures.\n\n“While education in Africa is rising and demand for skilled people also ascends, there are serious challenges to Africans reaching quality Higher Education (HE) levels that are recognised by the world's employers. The gap in the market needs to be addressed now... if the 2040 African labour force is to have global value.”\n\nApplicants have to meet the general eligibility requirements for the award of a scholarship and additionally specific academic qualifications must be met in order to qualify for the relevant degree programmes.\n\nCurrently, UNICAF offers a number of undergraduate and postgraduate programmes in a number of areas such as business, education, psychology and LLM Laws.\n\nUNICAF has a physical presence in nine African countries and a number of branch campuses such as the one in Malawi and Zambia to service south-eastern Africa. The campuses aim to provide a number of student services and face-to-face tutorials to support the online delivery model. Although founded by UNICAF, Unicaf University is treated as a partner university with scholarships provided for students taking the courses, whether online or in the form of blended learning. According to Dr Kevin Andrews, Vice-Chancellor, Unicaf University is currently in the process of opening additional campuses in several locations in Africa.\n\n"}
{"id": "56984527", "url": "https://en.wikipedia.org/wiki?curid=56984527", "title": "Unmanned aircraft in Singapore", "text": "Unmanned aircraft in Singapore\n\nAccording to the Civil Aviation Authority of Singapore (CAAS), an unmanned aircraft (UA), commonly known as a drone, is operated without a pilot on board. An unmanned aircraft system (UAS) comprises the UA and associated elements such as the remote control equipment.\n\nDue to Singapore’s busy airspace and densely populated urban environment, the UA must be operated safely and responsibly to avoid risks to aviation and public safety. The CAAS requires operators to understand and abide by regulations, including recreational or research uses of the UA. More information on the regulations can be found on Air Navigation Order, paragraph 80.\n\nThe Unmanned Aircraft (Public Safety and Security) Act provides clear guidelines for the safe use of unmanned aircraft.\n\nLaws were passed in Parliament in May 2015 to allay concerns over safety, security and privacy surrounding unmanned aerial vehicles (UAVs). The Unmanned Aircraft (Public Safety and Security) Bill outlines regulations for the safe flying of drones and enforcement action against errant users. For instance, permits are required to fly drones above 7 kg, or within a 5 km radius of an aerodrome.\n\nBefore conducting any outdoor activities, operators should ensure that the UA is flown within the permitted areas. The CAAS website provides a map delineating prohibited areas, danger and restricted areas, areas within 5 km of an airport or an airbase and protected areas.\n\nIn 2017, the Singapore National Day Parade was gazetted as a “special event” under the Public Order Act. The order, which was in effect for 24 hours on 9 August 2017, prohibited the unauthorised flying of unmanned aerial vehicles (AUV) such as drones in the area without a permit. The boundaries of the special event area included Marina Boulevard, Victoria Street, Middle Road, Beach Road and the Marina Barrage carpark.\n\nIt is also an offence to fly a UAV outside of the special event area in a manner that “disrupts, interferes with, delays or obstructs” with the National Day Parade. Offenders may be arrested and upon conviction, be liable to an imprisonment term for up to 12 months, a fine of up to $20,000, or to both. The UAV will be seized.\n\nThe 32nd Asean summit held at the Istana on 27 April 2018 and the Shangri-La Hotel on 28 April was declared an enhanced security special event under the Public Order Act by the Ministry of Home Affairs. It is an offence to bring or fly drones in the area or outside of the area that disrupts, interferes with, delays or obstructs the conduct of the event.\n\nThe Data Protection Provisions do not impose any obligation on an individual acting in his personal or domestic capacity. Organisations will need to consider whether the drones they deploy are likely to capture personal data of individuals, and may wish to evaluate whether any exception under the Personal Data Protection Act (PDPA) applies in respect of its particular circumstances.\n\nOrganisations using UAs for photography, video or audio recording activities that capture personal data should refer to the Personal Data Protection Commission’s (PDPC) advisory guidelines. Among other obligations, the Data Protection Provisions require organisations to\ninform individuals of the purposes for which their personal data will be collected, used and disclosed in order to obtain their consent. An organisation must therefore provide notification of the purposes for the collection, use or disclosure of personal data captured by its drones, in order to fulfil the obligation to obtain consent. The notifications should specify if photography, video and/or audio recording is occurring and should generally be placed so as to enable individuals to have sufficient awareness that drones are in operation in the general locale. For example, it may be appropriate to place a notice at points of entry to the area of operation, where individuals are able to read the notice prior to entry.\n\nUsers with UAs that contain short range devices that conform to the approved operating radio frequencies and corresponding power limits will be exempted from Info-communications Media Development Authority's (IMDA) licensing requirements. For UAs with radio frequencies or power limits that are not in the IMDA’s guidelines, equipment dealers have to apply for the relevant license and register their equipment with the IMDA. Under the Telecommunications (Dealers) Regulations, an equipment dealer may only sell IMDA-registered telecommunication equipment for use in Singapore.\n\nIn general, permits are not required for recreational or research uses of UA in Singapore, as long as the operation of the UA is in line with CAAS’ operating conditions.\n\nSituations where recreational or research uses of UA require a permit is where:\n\nCAAS defines recreational activities as “any pursuit or activity engaged in for enjoyment, relaxation or leisure”.\n\nActivities that are not considered recreational uses include:\n\n\nAccording to CAAS, any activity falling within the following categories are considered research in nature: \n\nRegardless of UA weight or location of UA operations, an operator permit and Class 1 Activity Permit is required for operations that are non-recreational or non-research in nature.\n\nExamples of applicable uses include:\n\nCAAS grants an operator permit to an applicant who is able to ensure safe operation of UA, taking into account the applicant’s organisational set-up, competency of the personnel especially those flying the UA, procedures to manage safety including the conduct of safety risk assessments, and the airworthiness of each of the aircraft. The permit is valid for up to one year.\n\nAn activity permit is granted by CAAS to an applicant for a single activity or a block of repeated activities to be carried out by a UA at a specific area of operation, and which are of specific operational profiles and conditions.\n\nA Class 1 Activity permit is required for purposes that are not recreational or research in nature; or if the UA used is over 7 kg in total mass (including payload). A Class 1 Activity Permit is not valid without a UA Operator Permit.\n\nA Class 2 Activity permit is required for UA activities for recreational or research purposes, and which meets any of these conditions:\n\nThe table below summarises the various permit fees:\n\nBesides permits from the CAAS, other permits may be required from various agencies depending on the nature of the usage. This includes:\n\nBefore the Act was introduced in 2015, then Transport Minister Lui Tuck Yew said in Parliament that there had been more than 20 reported incidents involving drones between April 2014 and May 2015, two of which involved the drones falling onto MRT tracks.\n\nIn 2016, the Singapore government received a report of a remote control aeroplane that damaged the roof of a housing block in Bishan. However, the operator had yet to be located, according to Transport Minister Khaw Boon Wan in a written parliamentary response.\n\nIn 2015, a 27-year-old man who flew a drone at the War Memorial Park on National Day was given a stern warning by the police. He was believed to have been trying to take photographs of the NDP fireworks.\n\nBetween June 2015 and May 2017, the CAAS recorded 103 violations. Mr Tan Kah Han, senior director for safety regulation and director for airworthiness and flight operations at the CAAS, said that such incidents typically involve flying within 5 km of aerodromes, which is not allowed, and flying within restricted and security-sensitive areas without a permit.\n\nIn 2017, there were 12 breaches involving UAs during the National Education and preview shows according to a police statement.\n\nOn 9 August 2017, a 53-year-old man was arrested on National Day for flying a drone at Marina Barrage, which was marked within the Special Events Area. Unauthorised flying of unmanned aerial vehicles is not allowed for 24 hours on National Day.\n\nExperts highlight the risks of increasingly sophisticated drone technology being \"accessible to the man on the street at brick-and-mortar shops or online\", such as high-definition video capabilities making it easier for surveillance to be carried out covertly.\n\nIn 2015, The Workers' Party’s Gerald Giam, a Non-Constituency Member of Parliament, proposed fitting drones above a certain weight and size with geo-fencing capabilities, to prevent them from entering prohibited spaces. Government Parliamentary Committee (Home Affairs and Law) member Desmond Choo felt that it was important to conduct education campaigns to educate both users and companies that bring in drones.\n\nDr Foong Shaohui, a Singapore University of Technology and Design assistant professor with an interest in robotics and unmanned systems, highlighted how without proper training, even a drone weighing just 1 kilograms can cause property damage and serious injuries.”\n\nMr Mohamed Faisal Mohamed Salleh, deputy director of Nanyang Technology University's Air Traffic Management Research Institute, said that it can be \"almost impossible\" to find the pilot after UA-related accidents. To counter this issue, he suggested the use of aircraft surveillance technology to trace the positions of all UAs, which could potentially involve the use of telco or wireless networks, or by creating an \"Electronic Road Pricing system in the sky\".\n\nWhile Mr Yue Keng Mun, from Temasek Polytechnic's School of Engineering, proposed having indoor or fenced-up outdoor spaces for operators to practice their flying skills, Mr Khaw responded in his parliamentary reply that \"promoting shared used of space in land-scarce Singapore was preferable to setting aside special flying parks.\"\n\nAccording to an article by The Straits Times in January 2018, more private condominiums are banning the use of drones within their estates, citing concerns over privacy and safety.\n\nIn 2017, Singapore joined the Unmanned Aircraft Systems (UAS) Advisory Group, a 15-member group set up by the United Nations’ civil aviation arm to draw up global rules and regulations for the safe use of UAs.\n\nIn 2018, one-north was designated as Singapore’s first drone estate, to provide companies and research institutions with an urban environment for test-bedding innovative unmanned aircraft systems. Under the drone estate initiative, approved operators and research users can carry out their trials and operations at one-north without compromising safety and security.\n\nThe Singapore Armed Forces have invested in new technologies such as drones that can home in and catch errant drones. Counter-drone systems are among the features of smart airbases of the future.\n\n\n"}
{"id": "1673648", "url": "https://en.wikipedia.org/wiki?curid=1673648", "title": "Women's Trade Union League", "text": "Women's Trade Union League\n\nThe Women's Trade Union League (WTUL) was a U.S. organization of both working class and more well-off women formed in 1903 to support the efforts of women to organize labor unions and to eliminate sweatshop conditions. The WTUL played an important role in supporting the massive strikes in the first two decades of the twentieth century that established the International Ladies' Garment Workers' Union and Amalgamated Clothing Workers of America and in campaigning for women's suffrage among men and women workers.\n\nThe roots of the WTUL come from a British organization of the same name founded thirty years earlier. The British League had originally supported the creation of a separate women's labor movement but, by the 1890s, merged its own aims with the mainstream British labor movement and functioned as an umbrella organization of women's trade unions. Its first American supporter was the socialist William English Walling who met with British WTUL leaders in 1902. He returned to the United States and began to generate support for a similar American organization.\n\nOrganized in 1903 at the American Federation of Labor convention, the WTUL spent much of its early years trying to cultivate ties with the AFL leadership. Its first president was Mary Morton Kehew, a labor and social reformer from Boston. By 1907, the WTUL saw its purpose as supporting the AFL and encouraging women's membership in the organization. In its constitution that year, the WTUL defined its purpose in assisting \"in organizing women into trade unions...such unions to be affiliated, where practicable, with the American Federation of Labor.\" In response, the AFL leadership generally ignored the League. When the WTUL decided to hold its annual conference at a different location than the AFL in 1905, Samuel Gompers was furious and refused to attend. Still, the League did push the AFL towards a pro-suffrage position and did manage to organize more women into the Federation than at any previous time.\n\nIt also drew on the earlier work of activists in the settlement house movement, such as Jane Addams and Florence Kelley, and budding unions in industries with a large number of women workers, such as garments and textiles. The WTUL leadership comprised both upper-class philanthropists and working-class women with experience organizing unions, including a significant portion of the most important female labor leaders of the day, including Mary Kenney O'Sullivan and Rose Schneiderman.\n\nBut the heyday of the League came between 1907 and 1922 under the presidency of Margaret Dreier Robins. During that period, the WTUL led the drive to organize women workers into unions, secured protective legislation, and educated the public on the problems and needs of working women.\n\nThe League supported a number of strikes in the first few years of its existence, including the 1907 telegrapher's strike organized by the Commercial Telegraphers Union of America. The WTUL played a critical role in supporting the Uprising of the 20,000, the New York City and Philadelphia shirtwaist workers' strike, by providing a headquarters for the strike, raising money for relief funds, soup kitchens and bail for picketers, providing witnesses and legal defense for arrested picketers, joining the strikers on the picket line, and organizing mass meetings and marches to publicize the shirtwaist workers' demands and the sweatshop conditions they were fighting. Some observers made light of the upper-class women members of the WTUL who picketed alongside garment workers, calling them the \"mink brigade\". These distinctions split strikers from their upper-class benefactors as well: a contingent of strikers challenged Alva Belmont concerning her reasons for supporting the strike.\n\nThe strike was, however, less than wholly successful: Italian workers crossed the picket lines in large numbers and the strikers lacked the resources to hold out longer than the employers. In addition, although activists within the WTUL, including William E. Walling and Lillian D. Wald, were also among the founders of the NAACP that year and fought the employers' plan to use African-American strikebreakers to defeat the strike, others in the black community actively encouraged black workers to cross the picket lines. Even so, the strike produced some limited gains for workers, while giving both the WTUL and women garment workers a practical education in organizing.\n\nThe WTUL played a similar role in the strike of mostly male cloakmakers in New York City and men clothing workers in Chicago in 1910, in the 1911 garment workers strike in Cleveland and in many other actions in Iowa, Massachusetts, Missouri and Wisconsin. By 1912, however, the WTUL began to distance itself from the labor movement, supporting strike action selectively when it approved of the leadership's strategy and criticizing the male-dominated leadership of the ILGWU that it saw as unrepresentative of women workers. The WTUL's semi-official relationship with the American Federation of Labor was also strained when the United Textile Workers, an AFL affiliate, insisted that it stop providing relief for Lawrence, Massachusetts textile workers who refused to return to work during the strike led by the Industrial Workers of the World; some WTUL leaders complied, while others refused, denouncing both the AFL and the WTUL for its acquiescence in strikebreaking activities.\n\nThe League had a closer relationship with the Amalgamated Clothing Workers of America, the union formed by the most militant locals of mostly immigrant workers in the men's clothing industry in Chicago, New York and other eastern urban centers, which was outside the AFL. The WTUL trained women as labor leaders and organizers at its school founded in Chicago in 1914 and played a key role in bringing Italian garment workers into the union in New York.\n\nAt this time the WTUL also began to work for legislative reforms, in particular the eight-hour day, the minimum wage and protective legislation. Because of the hostility of the United States Supreme Court toward economic legislation at the time, only legislation that singled out women and children for special protections survived challenges to its constitutionality. Ironically, Samuel Gompers and the conservative leadership of the AFL also viewed such legislation with hostility, but for a different reason: they believed by that point that legislation of this sort interfered with collective bargaining, both by usurping the role of unions in obtaining better wages and working conditions and in setting a precedent for governmental intrusion into the area.\n\nThe WTUL was also active in demanding safe working conditions, both before and after the Triangle Shirtwaist Factory fire in 1911 in which 146 workers were killed. That fire, which had been preceded by a similar fire in Newark, New Jersey in which twenty-five garment workers were killed, not only galvanized public opinion on the subject, but also exposed the fissures between the League's well-heeled supporters and its working class militants, such as Rose Schneiderman. As Schneiderman said in her speech at the memorial meeting held in the Metropolitan Opera House on April 2, 1911:\n\nThe WTUL also began to work actively for women's suffrage, in close coalition with the National American Woman Suffrage Association, in the years before passage of the Nineteenth Amendment to the United States Constitution in 1920. The WTUL saw suffrage as a way to gain protective legislation for women and to provide them with the dignity and other less tangible benefits that followed from political equality. Schneiderman coined an evocative phrase in campaigning for suffrage in 1912:\n\nHer phrase \"bread and roses\", recast as \"We want bread and roses too\", became the slogan of the largely immigrant, largely women workers of the 1912 Lawrence textile strike.\n\nThe WTUL was, on the other hand, mistrustful of the National Woman's Party, with its more individualistic, rights-oriented approach to woman's equality. The WTUL was strongly opposed to the Equal Rights Amendment drafted by the NWP after the passage of the Nineteenth Amendment on the ground that it would undo the protective legislation that the WTUL had fought so hard to obtain.\n\nThe WTUL focused increasingly on legislation in the 1920s and thereafter. Its leadership, in particular Schneiderman, were supporters of the New Deal and had a particularly close connection to the Roosevelt administration through Eleanor Roosevelt, a member of the WTUL since 1923. The WTUL dissolved in 1950.\n\nA related organization was the Women's Education and Industrial Union (WEIU), which employed female researchers such as Louise Marion Bosworth to research the working conditions of women.\n\n\n"}
