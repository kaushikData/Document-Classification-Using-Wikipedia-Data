{"id": "52918977", "url": "https://en.wikipedia.org/wiki?curid=52918977", "title": "AIS station", "text": "AIS station\n\nAIS receiver station receive telegrams from near by vessels via VHF data (about 162 MHz) and sending it to Automatic identification system to be recorded and used for vessel tracking and other purpose.\n"}
{"id": "17578531", "url": "https://en.wikipedia.org/wiki?curid=17578531", "title": "Aerobic granulation", "text": "Aerobic granulation\n\nThe biological treatment of wastewater in the sewage treatment plant is often accomplished using conventional activated sludge systems. These systems generally require large surface areas for treatment and biomass separation units due to the generally poor settling properties of the sludge. Aerobic granules are a type of sludge that can self-immobilize flocs and microorganisms into spherical and strong compact structures. The advantages of aerobic granular sludge are excellent settleability, high biomass retention, simultaneous nutrient removal and tolerance to toxicity. Recent studies show that aerobic granular sludge treatment could be a potentially good method to treat high strength wastewaters with nutrients, toxic substances.\n\nThe aerobic granular sludge usually is cultivated in SBR (sequencing batch reactor) and applied successfully as a wastewater treatment for high strength wastewater, toxic wastewater and domestic wastewater. Compared with conventional aerobic granular processes for COD removal, current research focuses more on simultaneous nutrient removal, particularly COD, phosphorus and nitrogen, under pressure conditions, such as high salinity or thermophilic condition.\n\nIn recent years, new technologies have been developed to improve settlability. The use of aerobic granular sludge technology is one of them.\n\nProponents of aerobic granular sludge technology claim \"it will play an important role as an innovative technology alternative to the present activated sludge process in industrial and municipal wastewater treatment in the near future\" and that it \"can be readily established and profitably used in activated sludge plants\". However, in 2011 it was characterised as \"not yet established as a large-scale application ... with limited and unpublished full-scale applications for municipal wastewater treatment.\"\n\nThe following definition differentiates an aerobic granule from a simple floc with relatively good settling properties and came out of discussions which took place at the \"1st IWA-Workshop Aerobic Granular Sludge\" in Munich (2004):\n\nGranular sludge biomass is developed in sequencing batch reactors (SBR) and without carrier materials. These systems fulfil most of the requirements for their formation as:\n\nGranular activated sludge is also developed in flow-through reactors using the Hybrid Activated Sludge (HYBACS®) process, comprising an attached-growth reactor with short retention time upstream of a suspended growth reactor. The attached bacteria in the first reactor, known as a SMART unit, are exposed to a constant high COD, triggering the expression of high concentrations of hydrolytic enzymes in the EPS layer around the bacteria (citation needed). The accelerated hydrolysis liberates soluble readily-degradable COD which promotes the formation of granular activated sludge.\n\nThe development of biomass in the form of aerobic granules is being studied for its application in the removal of organic matter, nitrogen and phosphorus compounds from wastewater.\nAerobic granules in an aerobic SBR present several advantages compared to conventional activated sludge process such as:\n\nThe HYBACS process has the additional benefit of being a flow-through process, thus avoiding the complexities of SBR systems. It is also readily applied to the upgrading of existing flow-through activated sludge processes, by installing the attached growth reactors upstream of the aeration tank. Upgrading to granular activated sludge process enables the capacity of an existing wastewater treatment plant to be doubled.\n\nSynthetic wastewater was used in most of the works carried out with aerobic granules. These works were mainly focussed on the study of granules formation, stability and nutrient removal efficiencies under different operational conditions and their potential use to remove toxic compounds. The potential of this technology to treat industrial wastewater is under study, some of the results:\n\n\nAerobic granulation technology for the application in wastewater treatment is widely developed at laboratory scales. The large-scale experience is growing rapidly and multiple institutions are making efforts to improve this technology:\n\n\nThe feasibility study showed that the aerobic granular sludge technology seems very promising (de Bruin et al., 2004. Based on total annual costs a GSBR (Granular sludge sequencing batch reactors) with pre-treatment and a GSBR with post-treatment proves to be more attractive than the reference activated sludge alternatives (6-16%).\nA sensitivity analysis shows that the GSBR technology is less sensitive to land price and more sensitive to rain water flow. Because of the high allowable volumetric load the footprint of the GSBR variants is only 25% compared to the references. However, the GSBR with only primary treatment cannot meet the present effluent standards for municipal wastewater, mainly because of exceeding the suspended solids effluent standard caused by washout of not well settleable biomass.\n\nAerobic granulation technology is already successfully applied for treatment of wastewater.\n\n\n\n\n"}
{"id": "2182188", "url": "https://en.wikipedia.org/wiki?curid=2182188", "title": "Air-operated valve", "text": "Air-operated valve\n\nAn air-operated valve is a type of power operated valve that uses air pressure against a piston or diaphragm to produce linear or circular movement to operate a valve.\nTypes are 2-way, 3-way and 4-way. The 2 way air-operated valves can be either normally closed or normally opened.\n\n"}
{"id": "17004398", "url": "https://en.wikipedia.org/wiki?curid=17004398", "title": "Alert on LAN", "text": "Alert on LAN\n\nAlert on LAN (AOL, sometimes AoL) is a 1998, IBM- and Intel-developed technology that allows for remote management and control of networked PCs. AOL requires a Wake on LAN adapter. \n\nThe main idea of AOL is to send warnings to remote administrators about different PC conditions using a LAN. These conditions include: \n\n\nAlert on LAN 2 (AOL2) extends AOL to allow active PC management, including:\n\n\n\n"}
{"id": "12744809", "url": "https://en.wikipedia.org/wiki?curid=12744809", "title": "Association of Women Surgeons", "text": "Association of Women Surgeons\n\nThe Association of Women Surgeons (AWS) is a non-profit educational and professional organization founded in 1981. With over 2,000 members in 21 countries, both women and men, AWS is one of the largest international organizations dedicated to supporting, enhancing the interaction, and facilitating the exchange of information between women surgeons at various stages in their careers. The organization's mission statement reads: \"To inspire, encourage, and enable women surgeons to achieve their personal and professional goals\".\n\nAWS actions aim to: ENGAGE current and future women surgeons to realize their professional and personal goals. EMPOWER women to succeed. EXCEL in those aspirations through mentorship, education and a networking community that promotes their contributions and achievements as students, surgeons and leaders.\n\nThe AWS was founded in 1981 when Dr. Patricia Numann posted a sign inviting any woman surgeon to a breakfast at the October meeting of the American College of Surgeons (ACS) in the San Francisco Hilton Hotel.\n\n\n\n\n\n\nThe association counts with five local chapters:\n\nIn addition, there are over 50 AWS Medical Student Chapters nationally and internationally. \nMany institutions also have institutional chapters for their women surgical residents.\n\n\n1981 – 1988 – Founder, Patricia Numann\n\n1988 – 1990 – Tamar Earnest\n\n1990 – 1992 – Mary McCarthy\n\n1992 – 1994 – Linda Phillips\n\n1994 – 1995 – Margaret Dunn\n\n1995 – 1996 – Joyce Majure\n\n1996 – 1997 – M. Margaret Kemeny\n\n1997 – 1998 – Leigh Neumayer\n\n1998 – 1999 – Beth Sutton\n\n1999 – 2000 – Dixie Mills\n\n2000 – 2001 – Kim Ephgrave (deceased)\n\n2001 – 2002 – Myriam Curet\n\n2002 – 2003 – Susan Kaiser\n\n2003 – 2004 – Vivian Gahtan\n\n2004 – 2005 – Susan Stuart\n\n2005 – 2006 – Hilary Sanfey\n\n2006 – 2007 – Patricia Bergen\n\n2007 – 2008 – Mary Hooks\n\n2008 – 2009 – AJ Copeland\n\n2009 – 2010 – Rosemary Kozar\n\n2010 – 2011 – Marilyn Marx\n\n2011 – 2012 – Betsy Tuttle-Newhall\n\n2012 – 2013 – Susan Pories\n\n2013 – 2014 – Danielle Walsh\n\n2014 – 2015 – Nancy Gantt\n\n2015 – 2016 – Amalia Cochran\n\n2016 – 2017 – Christine Laronga\n\n2017 – 2018 – Celeste Hollands\n\n2018 – 2019 – Sareh Parengi\n\nIn 1992, the AWS held its first summer meeting in Park City, Utah. The theme was Strategies for Success.\nStarting in 2004, the AWS has held an annual conference held in conjunction with the ACS Clinical Congress. The conference features a one day conference with scientific session, key note speakers or panels; a Presidential award dinner and a networking breakfast.\n2018 (Boston) – Theme: Dare to Be You\n2017 (San Diego) – Theme: Better Together\n\n2016 (Washington DC) – Theme: Sailing with Es: Engage, Empower, Excel\n\n2015 (Chicago) – Theme: Leaning In without Stressing Out\n\n2014 (San Francisco) – Theme: Thriving Amidst Change\n\n2013 (Washington DC) – Theme: On Point\n\n2012 (Chicago) – Theme: We Are the World\n\n2011 (San Francisco) – Theme: Celebrating 30 Years\n\n2010 (Washington DC)\n\n2009 (Chicago)\n\n2008 (San Francisco)\n\n2007 (New Orleans) – Theme: Women as Leaders\n\n2006 (Chicago) – Theme: Women as Surgeons: Past, Present and Future\n\n2005 (San Francisco) – Theme: Challenging Paradigms: Defining and Redefining Success\n\n2004 (New Orleans) – Theme: Transitions in a Surgical Career\n\n\nNina Starr Braunwald Award\n\nNamed in honor of Dr. Nina Starr Braunwald, the first woman cardiac surgeon and a Deputy NIH Director, this award was established with an endowment from Dr. Eugene Braunwald. This is the most prestigious award granted by AWS and is given in recognition of outstanding contributions to the advancement of women in surgery.\n\nLeigh Anne Neumayer, MD FACS (2017)\n\nDiana Farmer MD FACS (2016)\n\nRosemary Kozar MD FACS (2015)\n\nHilary Sanfey MB BCh FACS (2014)\n\nJoyce Majure MD FACS (2013)\n\nValerie Rusch MD FACS (2012)\n\nMargaret Dunn MD, MBA, FACS (2011)\n\nJulie Freischlag MD, FACS (2010)\n\nSusan Miller Briggs MD, MPH, FACS (2009)\n\nM. Margaret Kemeny MD, FACS (2008)\n\nNancy Ascher MD, FACS (2007)\n\nThe American College of Surgeons (2006)\n\nKarin Muraszko MD, FACS (2005)\n\nBarbara Lee Bass MD, FACS (2004)\n\nMary C. McCarthy MD, FACS (2003)\n\nSusan M. Love MD, FACS (2002)\n\nLinda G. Phillips MD, FACS (2001)\n\nAnna Marie Ledgerwood MD, FACS (2000)\n\nPatricia Numann MD, FACS (1998)\n\nLeslie J. Kohman MD, FACS (1997)\n\nPatricia K. Donahoe MD, FACS (1996)\n\nKathryn D. Anderson MD, FACS (1995)\n\nOlga Jonasson MD, FACS (1994)\n\nClaude Organ MD, FACS (1993)\n\nDistinguished Member Award\n\nThis award is given to the individual who as an AWS member exemplifies the ideals and mission of the organization. Since 2006, it has been named in honor of Dr. Olga Jonasson, the first woman to chair an academic Department of Surgery.\n\nKaren Brasel MD, MPH, FACS and Kazumi Kawase, MD, PhD (2017)\n\nSherry Wren MD FACS (2016)\n\nPatricia Turner MD FACS (2015)\n\nBarb Pettitt MD FACS (2014)\n\nHilary Sanfey MB BCh FACS (2013)\n\nSusan Kaiser MD FACS (2012)\n\nPauline Chen MD, FACS (2011)\n\nGayle Woodson MD, FACS (2010)\n\nKimberly Ephgrave MD, FACS (2009)\n\nDixie Mills MD, FACS (2008)\n\nMyriam Curet MD, FACS (2007)\n\nLeigh Neumayer MD, FACS (2006)\n\nPatricia Lowery MD, FACS (2005)\n\nLinda Brodsky MD, FACS (2004)\n\nAdrienne Krausz MD (2003)\n\nLena M. Napolitano MD, FACS (2002)\n\nMargaret Dunn MD, MBA, FACS (2002)\n\nMary McCarthy MD, FACS (2001)\n\nSylvia Ramos MD, FACS, PC (2001)\n\nJulie Ann Freischlag MD, FACS (2000)\n\nKaren S. Guice MD, FACS, MPP (1999)\n\nJoyce Majure MD, FACS (1998)\n\nGloria Sarto MD, PhD (1998)\n\nBarbara Lee Bass MD, FACS (1997)\n\nCarol Scott-Connor MD, PhD, FACS (1997)\n\nMaria Allo MD, FACS (1996)\n\nBarbara Kinder MD, FACS (1996)\n\nFrances K. Conley MD, FACS (1995)\n\nSally Abston MD, FACS (1994)\n\nTamar Earnest MD, FACS (1994)\n\nJessie Ternberg MD, PhD, FACS (1994)\n\nHughenna Gauntlett MD, FACS (1993)\n\nNina Starr Braunwald MD (1992)\n\nOlga Jonasson MD, FACS (1992)\n\nAlma Dea Morani MD, FACS (1992)\n\nPatricia Numann MD, FACS (1991)\n\nChristine Haycock MD, FACS (1990)\n\nPast Presidents’ Honorary Member Award\n\nThe Honorary Member Award is given to individuals supportive of the goals of AWS. It was named in honor of Dr. Claude Organ from 2007 to 2010. In 2011 it was re-named the AWS Past Presidents’ Honorary Member Award in acknowledgment of an endowment from all of the Past Presidents of AWS.\n\nKeith Lillemoe MD, FACS(2017)\n\nFabrizio Michelassi MD FACS (2016)\n\nJohn Atkinson MD FACS\n\nLinda Richetelli-Pepe (2014)\n\nF. William Blaisdell, MD (2014)\n\nMarsha Moses PhD & Keith Amos MD, FACS (posthumously)(2013)\n\nGeorge Sheldon MD, FACS (2012)\n\nMargaret Tarpley MLS & John Tarpley MD, FACS (2011)\n\nLasalle D. Leffall, Jr. MD, FACS (2010)\n\nJohn Davis MD, FACS (2009)\n\nCarlos Pellegrini MD, FACS (2009)\n\nMichael Zinner MD, FACS (2008)\n\nFrank R. Lewis, Jr. MD, FACS (2007)\n\nAveril Mansfield CBE, FRCS, FACS (2006)\n\nMorris Kerstein MD, FACS (2005)\n\nCourtney M. Townsend, Jr. MD, FACS (2004)\n\nAjit Sachdeva MD, FACS (2003)\n\nKaren Licitra (2002)\n\nMonica Maillet PhD (2001)\n\nJohn T. Preskitt, Sr. MD, FACS (2000)\n\nDavid Majure (1999)\n\nDebra A. DaRosa PhD (1998)\n\nJudith Keel (1997)\n\nLinn Meyer (1995)\n\nMarlys Witte MD (1994)\n\nJanet Bickel, MA (1992)\n\nEllen More PhD (1992)\n\nJudith Briles PhD (1990)\n\nOutstanding Woman Resident Award\n\nThe Outstanding Resident Award is given to a surgical trainee who demonstrates potential as a future leader in surgery. In 2010, this award was named in honor of Dr. Hilary Sanfey in acknowledgement of her generous endowment of this award.\n\nMeredith Barrett MD (2017)\n\nKristy Rialon, MD (2016)\n\nRobin Petroze MD (2015)\n\nSepideh Gholami, MD (2014)\n\nMeera Kotagal MD (2013)\n\nStephanie Chao, MD (2012)\n\nShimae Fitzgibbons, MD (2012)\n\nMaureen Tedesco MD (2011)\n\nElizabeth David MD (2010)\n\nVictoria Stager MD (2010)\n\nNancy Cho MD (2009)\n\nLesly Dossett MD (2009)\n\nTraci Hedrick MD (2009)\n\nAmy Vertrees MD (2009)\n\nJennifer Waljee MD (2009)\n\nJennifer LaFemina MD (2008)\n\nErika Adams Newman MD (2007)\n\nLouisa L. Pecchioni MD (2006)\n\nLeora Balsam MD (2005)\n\nGretchen P. Purcell MD, PhD (2004)\n\nPatricia Turner MD (2003)\n\nMelina Kibbe MD (2002)\n\nAnna Santos-Quinones MD (2001)\n\nCarol Sawmiller MD (2000)\n\nJanice Cormier MD, MPH (1999)\n\nPatricia Numann Outstanding Medical Student Award\n\nNamed in honor of the founder of AWS, this award was established to encourage and support women medical students pursuing a career in surgery. The winner is chosen based on her expressed interest in surgery, potential leadership qualities or research contributions to the field of surgery, her personal statement, and letters of recommendation.\n\nMary Shen (2017)\n\nJessica G.Y. Luc (2016)\n\nHillary Braun (2015)\n\nChristina Grassi (2014)\n\nJamie Anderson (2013)\n\nSophia McKinley (2012)\n\nRajshri Mainthia (2011)\n\nShelly Choo (2010)\n\nJuliet Emamaulee (2009)\n\nAminata Sallah (2008)\n\nRebecca Snyder (2007)\n\nAnn Vaughters (2006)\n\nErika Manning (2005)\n\nKelley Hutcheson (2004)\n\nSudha Jayaraman (2003)\n\nAWS Visiting Professors and Host Institutions\n\nThe AWS Visiting Professor Program provides medical schools with the opportunity to request top women surgeons as speakers and to receive funding from the AWS Foundation. Opportunities to lecture heighten the visibility of women surgeons while encouraging women medical students to pursue similar careers. The host institution is indicated in parentheses.\n\nGeeta Lal MD, FACS (2018)\n\nAnn Rogers MD, FACS (Houston Methodist Hospital 2017)\n\nMarie Crandall MD, MPH, FACS (East Carolina University 2016)\n\nSharon Stein MD< FACS, FASCRS (University of Alabama Birmingham 2016)\n\nAnees B. Chagpar, MD, MSc, MPA, FACS, FRCSC (Ohio State, 2015)\n\nCarla Pugh MD FACS (Oklahoma University, 2015)\n\nAmalia Cochran, MD, FACS, FCCM, (Washington University, 2014)\n\nSareh Parangi, MD, FACS , (Tulane University, 2014)\n\nBetsy Tuttle-Newhall, MD, FACS (The Cleveland Clinic, 2013)\n\nCarmen C. Solorzano, MD, FACS (Gundersen Health Systems – WI, 2013)\n\nCarol Scott-Conner MD, PhD, MBA, FACS (University of South Carolina, 2012)\n\nMartha A. Zeiger MD, FACS, FACE Massachusetts General Hospital, 2012)\n\nMary Hooks MD, MBA, FACS; (Emory University,2011)\n\nCatherine Wittgen MD, FACS; (University of Wisconsin, 2010)\n\nTherese Duane MD, FACS; (University of Arizona, 2010)\n\nKaren Brasel MD, MPH, FACS; (Louisiana State University, 2009)\n\nPamela Lipsett MD, FACS; (University of California, Davis, 2008)\n\nLinda Phillips MD, FACS; (New York Presbyterian Hospital, 2007)\n\nMary McCarthy MD, FACS; (University of Texas at Houston, 2006)\n\nKathryn Spanknebel MD, FACS; (Tufts New England Medical Center, 2005)\n\nNancy Harthun MD, FACS; (The Mayo Clinic, 2005)\n\nRosemary Duda MD, MPH, FACS; (University of Texas at Galveston, 2004)\n\nAmy Friedman MD, FACS; (Medical College of Georgia, 2004)\n\nMyriam Curet MD, FACS; (University of Texas at San Antonio, 2003)\n\nLeslie Kohman MD, FACS; (University of Hawaii, 2003)\n\nM. Margaret Kemeny MD, FACS; (Duke University Medical Center, 2002)\n\nSusan Mackinnon MD, FACS; (Northwestern University, 2002)\n\nJulie Ann Freischlag MD, FACS; (University of Louisville, 2001)\n\nAWS Grant Recipients\n\nThe Association of Women Surgeons and the AWS Foundation have provided funding for original surgical research since 1996. Since that time Ethicon Endo-Surgery, Inc. has provided support for these grants. In 2007 through 2011 Genomic Health also provided support allowing for two individual grants during those five years.\nSmita Sihag MD, MPH (2018) in partnership with Ethicon-Endo Surgery\n\nNasreen Vohra MD (2017) in partnership with Ethicon-Endo Surgery\n\nGenevieve M. Boland MD, PhD (2016) in partnership with Ethicon-Endo Surgery\n\nLeslie Tyree MD (2015 in partnership with Ethicon-Endo Surgery\n\nHeather Yeo MD(2014) in partnership with Ethicon-Endo Surgery\n\nAnna Leung, MD (2013) in partnership with Ethicon-Endo Surgery \n\nErin W. Gilbert MD (2012), in partnership with Ethicon-Endo Surgery\n\nKatie S. Nason MD, MPH (2011), in partnership with Ethicon-Endo Surgery\n\nNancy L. Cho MD (2011), in partnership with Genomic Health\n\nJacqueline S. Jeruss MD, PhD, FACS (2010), in partnership with Ethicon-Endo Surgery\n\nNanette R. Reed MD (2010), in partnership with Genomic Health\n\nAi-Xuan Holterman MD, FACS (2009), in partnership with Ethicon-Endo Surgery\n\nTari A. King MD, FACS (2009), in partnership with Genomic Health\n\nChristina Roland MD (2008), in partnership with Ethicon-Endo Surgery\n\nAnn M. Rogers MD, FACS (2008), in partnership with Genomic Health\n\nGeeta Lal MD, FACS (2007), in partnership with Ethicon-Endo Surgery\n\nKimberley Eden Steele MD, FACS (2007), in partnership with Genomic Health\n\nAnjali Kumar MD, MPH (2006), in partnership with Ethicon-Endo Surgery\n\nAsha Bale MD, FACS (2005), in partnership with Ethicon-Endo Surgery\n\nLori Wilson MD, FACS (2004), in partnership with Ethicon-Endo Surgery\n\nRebecca Aft MD, PhD, FACS (2003), in partnership with Ethicon-Endo Surgery\n\nKatie Nason MD, MPH (2002), in partnership with Ethicon-Endo Surgery\n\nCelia Divino MD, FACS (2001), in partnership with Ethicon-Endo Surgery\n\nCeleste Hollands MD, FACS (2000), in partnership with Ethicon-Endo Surgery\n\nDanielle Walsh MD, FACS (1999), in partnership with Ethicon-Endo Surgery\n\nAphrodite M. Henderson MD, FACS (1998), in partnership with Ethicon-Endo Surgery\n\nAlicia Silva MD, FACS (1997), in partnership with Ethicon-Endo Surgery\n\nKaren Horvath MD, FACS (1996), in partnership with Ethicon-Endo Surgery\n\nStarr Award AWS Poster Contest\n\nNamed in honor of Dr. Nina Starr Braunwald, annual awards are given to the winners of the Annual Medical Student and Resident Poster Competition. This program facilitates interaction and idea exchange between students interested in surgery, surgical residents, and faculty members of various institutions.\n\nFirst Place Resident Award Winners\n\nVernissia Tam (2017)\n\nJing Li Huang MD (2016)\n\nHolly Mewhort MD (2015)\n\nFariha Sheikh MD (2014)\n\nHolly Mewhort MD (2013 )\n\nSepideh Gholami MD (2012)\n\nIrena Gribovskaja-Rupp MD, (2011)\n\nElizabeth Malm-Buatsi MD (2010)\n\nJennifer Steiman MD (2009)\n\nMara Antonoff MD (2008)\n\nFirst Place Student Award Winners\n\nJosephine Coury (2017)\n\nVusala Snyder (2016)\n\nElizabeth Kudlaty (2015)\n\nMartha Henderson (2014)\n\nAvianne Bunnell (2013)\n\nJoelle Straehla (2012 )\n\nAnalise Thomas, (2011)\n\nEva Vertes (2010)\n\nStephanie Chang (2009)\n\nAmy Fielder (2008)\n\nClaude Organ Traveling Fellowship\n\nThe family and friends of the late Claude H. Organ, Jr, MD, FACS established an endowment through the American College of Surgeons Foundation to provide funding for an annual fellowship to be awarded to an outstanding surgeon who is a member of any one of the following organizations: Society of Black Academic Surgeons, the Association of Women Surgeons or the Surgical Section of the \nNational Medical Association.\n\nKakra Hughes MD, FACS (2017)\n\nStephanie Bonne MD, FACS (2016)\n\nKathie-Ann Joseph, MD, MPH, FACS, (2015)\n\nCatherine J. Hunter, MD, FACS (2014)\n\nAnees B. Chagpar, MD, MSc, MPA, FACS, FRCSC (2013)\n\nKeith D. Amos, MD, FACS (2012)\n\nCarla Pugh MD, FACS (2011)\n\nMelina Kibbe MD. FACS (2010)\n\nBridget Fahy MD, FACS (2009)\n\nPatricia Turner MD, FACS (2008)\n"}
{"id": "24488539", "url": "https://en.wikipedia.org/wiki?curid=24488539", "title": "Aviation communication", "text": "Aviation communication\n\nAviation communication refers to the conversing of two or more aircraft. Aircraft are constructed in such a way that make it very difficult to see beyond what is directly in front of them. As safety is a primary focus in aviation, communication methods such as wireless radio are an effective way for aircraft to communicate with the necessary personnel. Aviation is an international industry and as a result involves multiple languages. However, as deemed by the International Civil Aviation Organization (ICAO), English is the official language of aviation. The industry considers that some pilots may not be fluent English speakers and as a result pilots are obligated to participate in an English proficiency test.\n\nAviation communication is the means by which aircraft crews connect with other aircraft and people on the ground to relay information. Aviation communication is a crucial component pertaining to the successful functionality of aircraft movement both on the ground and in the air. Increased communication reduces the risk of an accident.\n\nDuring the early stages of aviation, it was assumed that skies were too big and empty that it was impossible that two planes would collide. However, in 1956 two planes famously crashed over the Grand Canyon, which sparked the creation of the Federal Aviation Administration (FAA). Aviation was roaring during the Jet Age and as a result, communication technologies needed to be developed. This was initially seen as a very difficult task: ground controls used visual aids to provide signals to pilots in the air. With the advent of portable radios small enough to be placed in planes, pilots were able to communicate with people on the ground. With later developments, pilots were then able to converse air-to-ground and air-to-air. Today, aviation communication relies heavily on the use of many systems. Planes are outfitted with the newest radio and GPS systems, as well as Internet and video capabilities.\n\nEnglish is the main language used by the aviation industry; the use of aviation English is regulated by the International Civil Aviation Organization (ICAO).\n\nFlight was considered a foreign concept until the Wright Brothers successfully completed the world's first human flight in 1903. The industry grew rapidly and ground crews initially relied on coloured paddles, signal flares, hand signs, and other visual aids to communicate with incoming and outgoing aircraft. Although these methods were effective for ground crews, they offered no way for pilots to communicate back. As wireless telegraphy technologies developed alongside the growth of aviation during the first decade of the twentieth century, wireless telegraph systems were used to send messages in Morse code, first from ground-to-air and later air-to-ground. With this technology, planes were able to call in accurate artillery fire and act as forward observers in warfare.\n\nIn 1911, wireless telegraphy was put into operational use in the Italo-Turkish War. In 1912, the Royal Flying Corps had begun experimenting with \"wireless telegraphy\" in aircraft. Lieutenant B.T James was a leading pioneer of wireless radio in aircraft. In the spring of 1913, James had begun to experiment with radios in a B.E.2A. James managed to successfully increase the efficiency of wireless radio before he was shot down and killed by anti-aircraft fire on July 13, 1915.\n\nNonetheless, wireless communication systems in aircraft remained experimental and would take years to successfully develop a practical prototype. The early radios were heavy in weight and were considered an unreliable piece of equipment; additionally there were still major issues with ground forces using radio because signals were easily intercepted and targeted by opposing forces. At the beginning of World War I, aircraft were not typically equipped with wireless equipment. Instead, soldiers used large panel cut outs to distinguish friendly forces. These cut outs could also be used as a directional device to help pilots navigate back to friendly and familiar airfields.\n\nIn April 1915, Captain J.M. Furnival was the first person to hear a voice from the ground from Major Prince who said, \"If you can hear me now, it will be the first time speech has ever been communicated to an aeroplane in flight.\" In June 1915, the world's first air-to-ground voice transmission took place at Brooklands, England over about 20 miles. Ground-to-air was initially by Morse code, but it is believed 2-way voice communications were available and installed by July 1915. By early 1916, the Marconi Company (England) started production of air-to-ground radio transmitters/receivers which were used in the war over France.\n\nIn 1917, AT&T invented the first American air-to-ground radio transmitter. They tested this device at Langley Field in Virginia and found it was a viable technology. In May 1917, General George Squier of the U.S. Army Signal Corps contacted AT&T to develop an air-to-ground radio with a range of 2,000 yards. By July 4 of that same year, AT&T technicians achieved two-way communication between pilots and ground personnel. This allowed ground personnel to communicate directly with pilots using their voices instead of Morse code. Though few of these devices saw service in the war, they proved this was a viable and valuable technology worthy of refinement and advancement.\n\nFollowing World War I new technology was developed to increase the range and performance of the radios being used to communicate with planes in the air. In December 1919 a year after the end of World War I, Hugh Trenchard, 1st Viscount Trenchard, a senior officer in the Royal Flying Corps (RFC) later Royal Air Force (RAF), produced a report on the permanent organisation and operations of the RAF in peacetime in which he argued that if the air force officer was not to be a chauffeur, and nothing more, then navigation, meteorology, photography and wireless were necessities.\n\nIt was not until 1930 however that airborne radios were reliable enough and had enough power to make them viable to be standard in all planes and it was this year that the International Commission for Aerial Navigation agreed that all aircraft carrying 10 or more passengers should carry wireless equipment. Prior to this, only military aircraft designated for scout missions required radios. The operating distance of radios increased much slower than the distance planes were able to travel. This resulted in a planes messages having to bounce from airfield to airfield in order to get to its intended recipient. As the speed of planes increased this resulted in a plane reaching its destination before the message that it was on its way arrived at the airfield.\n\nOn 15 November 1938, the Army Airways Communications System (AACS) was established, this system was a point-to-point communications system used by the Army Air Corps. It allowed army air fields to remain in contact with planes throughout their entire flight. It could also be used to disseminate weather reports and orders to military aircraft and act as an air traffic control for arrivals and departures at military airfields. As technology increased, systems such as the AACS expanded and spread across the globe as other militaries and civilian services developed their own system of air control.\n\nThe development of radar in the mid-1930s proved a great advance in air-to-ground communication. Radar could be used to track planes in the air and determine distance, direction, speed and even type of aircraft. This allowed for better air traffic control as well as navigation aides for pilots. Radar also proved to be a valuable tool in targeting for bombers. Radar stations on the coast of Britain could aim two radar beams from separate locations on the coast towards Germany. By aligning the two radar beams to intersect over the desired target, a town or factory for example, an aircraft could then follow one radar signal until it intersected with the other where it would then know to drop bombs.\n\nThe Royal Air Force used the R1155/T1154 receiver/transmitter combination in most of its larger aircraft, particularly the Avro Lancaster and Short Sunderland. Single seat aircraft such as the Spitfire and Hurricane were equipped mostly with the TR1143 set. Other systems employed were Eureka and the S-Phone, which enabled Special Operations Executive agents working behind enemy lines to communicate with friendly aircraft and coordinate landings and the dropping of agents and supplies.\n\nCommunication error can occur between pilots and between pilots and air traffic controllers. Communication error contains: \n\nThe more information needing transfer, the more chance for error. Unclear pronunciation could happen with non-English speakers. Sometimes lack of self-confidence and motivation affects expression in communication. Misunderstanding happens with both native speakers and non-native speakers through communication, so a standard aviation language is important to improve this situation.\n\nSources of communication error come from: phonology (speech rate, stress, intonation, pauses), syntax (language word patterns, sentence structure), semantics, and pragmatics (language in context). Even though English is the international aviation language, native English speakers still play a role in misunderstanding and situational awareness. Both the ICAO and the Federal Aviation Administration use alternative phrases, which is confusing to both native and non-native English speakers.\n\nThe biggest problem regarding non-native English speakers' transmissions is speech rate. In order to understand alternative and unfamiliar accents, people's rate of comprehension and response slows down. Accents also affect transmissions because of the different pronunciations across languages. Some of the earlier miscommunication issues included the limitation of language-based warning systems in aircraft and insufficient English proficiency.\n\nAccording to US department of transportation's report, errors between pilots and controllers include:\n\n\nGenerally, miscommunication is caused by mis-hearing by the pilots for 28%, pilot not responding for 20%, controller mis-hearing for 15% and 10% that controllers do not respond. Also, a professional research shows that 30% of the information will be lost during the miscommunication. Moreover, miscommunication exists in personnel with different background of linguistics is shown to be one of the major problem in miscommunication to cause aviation accidents. Avoiding or minimizing miscommunication could be achieved by standardized debriefing or an interview process, and following a checklist to supplement written data.\n\nThe International Civil Aviation Organization established English as the international aviation language in 1951 to improve consistency, accuracy, and effectiveness of pilot - air traffic control communication. It requires that all pilots on international flights and air traffic controllers serving international airports and routes must be able to communicate in English effectively, as well as in their native language. The goal was to achieve standards that would eliminate communication error, language, and comprehension difficulties, all of which have been a major cause of operational airspace incidents. Miscommunication between pilots and air traffic control is a prominent factor in fatal airplane crashes, airspace incidents, runway incursion, and mid-air collisions.\n\nAviation English is the highly specialized language and sequences used by pilots, air traffic control, and other aviation personnel and it focuses on a particular pronunciation, vocabulary, grammatical structure, and discourse styles that are used in specific aviation-related contexts. The language used by pilots and air traffic controllers during radiotelephony communication can be categorized into two types: standard phraseology, and plain language repertoire. Standard phraseology is the specialized phrasing commonly used by the aviation community to effectively communicate, and plain language is a more normal language used in everyday life.\n\nMany non-native English speaking pilots and air traffic controllers learn English during their flight training and use it in a highly practical level while safely operating an aircraft and maintaining the safety of airspace, which can be highly stressful.\n\nICAO also established the Language Proficiency Requirements to try to rectify multiple issues regarding accents, terminology, and interpretation in communication. The intention of the LPRs is to \"ensure that the language proficiency of pilots and air traffic controllers is sufficient to reduce miscommunication as much as possible and to allow pilots and controllers to recognize and solve potential miscommunication when it does occur\" and \"that all speakers have sufficient language proficiency to handle non-routine situations.\" The structure of the LPR has six levels, pronunciation, structure, vocabulary, fluency, comprehension, and interactions. The implemented universal aviation English proficiency scale ranged from Level 1 to Level 6.\n\nBeginning in March 2008, ICAO set out the requirement that all pilots flying international routes and air traffic control serving international airports and routes must be a Level 4 or above and will be continually reassessed every three years. The criteria to achieve Level 4 are as follows:\n\nEnglish is the aviation language used by ICAO. Usually, human factors that affect communications include two aspects: direct, meaning the error caused by the language itself, which is the problem for non English speakers, and also indirect, with the gender, age, and experience impacting the communication in aviation.\nAs a result, both pilots and ATCs need to have enough English ability to accomplish their tasks. Through education to help improve aviation English, participants need not only focus on the textbook, but need experience in an actual environment such as lab experience to help speakers to improve their English fluency and avoid misunderstanding which helps non-English speakers to communicate normally.\n\n"}
{"id": "160573", "url": "https://en.wikipedia.org/wiki?curid=160573", "title": "Axle", "text": "Axle\n\nAn axle is a central shaft for a rotating wheel or gear. On wheeled vehicles, the axle may be fixed to the wheels, rotating with them, or fixed to the vehicle, with the wheels rotating around the axle. In the former case, bearings or bushings are provided at the mounting points where the axle is supported. In the latter case, a bearing or bushing sits inside a central hole in the wheel to allow the wheel or gear to rotate around the axle. Sometimes, especially on bicycles, the latter type axle is referred to as a \"spindle\".\n\nOn cars and trucks, several senses of the word \"axle\" occur in casual usage, referring to the shaft itself, its housing, or simply any transverse pair of wheels. Strictly speaking, a shaft which rotates with the wheel, being either bolted or splined in fixed relation to it, is called an \"axle\" or \"axle shaft\". However, in looser usage, an entire assembly including the surrounding axle housing (typically a casting) is also called an \"axle\".\n\nAn even broader (somewhat figurative) sense of the word refers to every pair of parallel wheels on opposite sides of a vehicle, regardless of their mechanical connection to each other and to the vehicle frame or body. Thus, transverse pairs of wheels in an independent suspension may be called an \"axle\" in some contexts. This very loose definition of \"axle\" is often used in assessing toll roads or vehicle taxes, and is taken as a rough proxy for the overall weight-bearing capacity of a vehicle, and its potential for causing wear or damage to roadway surfaces.\n\nAxles are an integral component of most practical wheeled vehicles. In a live-axle suspension system, the axles serve to transmit driving torque to the wheel, as well as to maintain the position of the wheels relative to each other and to the vehicle body. The axles in this system must also bear the weight of the vehicle plus any cargo. A non-driving axle, such as the front beam axle in heavy duty trucks and some two-wheel drive light trucks and vans, will have no shaft, and serves only as a suspension and steering component. Conversely, many front-wheel drive cars have a solid rear beam axle.\n\nIn other types of suspension systems, the axles serve only to transmit driving torque to the wheels; the position and angle of the wheel hubs is an independent function of the suspension system. This is typical of the independent suspensions found on most newer cars and SUVs, and on the front of many light trucks. These systems still have differentials, but will not have attached axle housing tubes. They may be attached to the vehicle frame or body, or integral in a transaxle. The axle shafts (usually constant-velocity type) then transmit driving torque to the wheels. Like a full floating axle system, the drive shafts in a front-wheel drive independent suspension system do not support any vehicle weight.\n\nA straight axle is a single rigid shaft connecting a wheel on the left side of the vehicle to a wheel on the right side. The axis of rotation fixed by the axle is common to both wheels. Such a design can keep the wheel positions steady under heavy stress, and can therefore support heavy loads. Straight axles are used on trains (that is, locomotives and railway wagons), for the rear axles of commercial trucks, and on heavy duty off-road vehicles. The axle can optionally be protected and further reinforced by enclosing the length of the axle in a housing.\n\nIn split-axle designs, the wheel on each side is attached to a separate shaft. Modern passenger cars have split drive axles. In some designs, this allows independent suspension of the left and right wheels, and therefore a smoother ride. Even when the suspension is not independent, split axles permit the use of a differential, allowing the left and right drive wheels to be driven at different speeds as the automobile turns, improving traction and extending tire life.\n\nA tandem axle is a group of two or more axles situated close together. Truck designs use such a configuration to provide a greater weight capacity than a single axle. Semi trailers usually have a tandem axle at the rear.\n\nAxles are typically made from SAE grade 41xx steel or SAE grade 10xx steel. SAE grade 41xx steel is commonly known as \"chrome-molybdenum steel\" (or \"chrome-moly\") while SAE grade 10xx steel is known as \"carbon steel\". The primary differences between the two are that chrome-moly steel is significantly more resistant to bending or breaking, and is very difficult to weld with tools normally found outside a professional welding shop.\n\nAn axle that is driven by the engine or prime mover is called a \"drive axle\".\n\nModern front-wheel drive cars typically combine the transmission (gearbox and differential) and front axle into a single unit called a \"transaxle\". The drive axle is a split axle with a differential and universal joints between the two half axles. Each half axle connects to the wheel by use of a constant velocity (CV) joint which allows the wheel assembly to move freely vertically as well as to pivot when making turns.\n\nIn rear-wheel drive cars and trucks, the engine turns a driveshaft (also called a \"propellor shaft\" or \"tailshaft\") which transmits rotational force to a drive axle at the rear of the vehicle. The drive axle may be a live axle, but modern rear wheel drive automobiles generally use a split axle with a differential. In this case, one half-axle or half-shaft connects the differential with the left rear wheel, a second half-shaft does the same with the right rear wheel; thus the two half-axles and the differential constitute the rear axle.\n\nSome simple vehicle designs, such as leisure go-karts, may have a single driven wheel where the drive axle is a split axle with only one of the two shafts driven by the engine, or else have both wheels connected to one shaft without a differential (kart racing). However, other go-karts have two rear drive wheels too.\n\nA dead axle, also called a \"lazy axle\", is not part of the drivetrain, but is instead free-rotating. The rear axle of a front-wheel drive car is usually a dead axle. Many trucks and trailers use dead axles for strictly load-bearing purposes. A dead axle located immediately in front of a drive axle is called a \"pusher axle\". A tag axle is a dead axle situated behind a drive axle. Dead axles are also found on semi trailers, farm equipment, and certain heavy construction machinery serving the same function. On some vehicles (such as motorcoaches), the tag axle may be steerable. In some designs the wheels on a lazy axle only come into contact with ground when the load is significant, thus saving unnecessary tire wear.\n\nSome dump trucks and trailers may be configured with a lift axle (also known as an \"airlift axle\" or \"drop axle\"), which may be mechanically raised or lowered. The axle is lowered to increase the weight capacity, or to distribute the weight of the cargo over more wheels, for example to cross a weight restricted bridge. When not needed, the axle is lifted off the ground to save wear on the tires and axle, and to increase traction in the remaining wheels. Lifting an axle also alleviates lateral scrubbing of the additional axle in very tight turns, allowing the vehicle to turn more readily. In some situations removal of pressure from the additional axle is necessary for the vehicle to complete a turn at all.\n\nSeveral manufacturers offer computer-controlled airlift, so that the dead axles are automatically lowered when the main axle reaches its weight limit. The dead axles can still be lifted by the press of a button if needed, for better maneuverability.\n\nLift axles were in use in the early 1940s. Initially, the axle was lifted by a mechanical device. Soon hydraulics replaced the mechanical lift system. One of the early manufacturers was Zetterbergs, located in Östervåla, Sweden. Their brand was Zeta-lyften.\n\nThe liftable tandem drive axle was invented in 1957 by the Finnish truck manufacturer Vanajan Autotehdas, a company sharing history with Sisu Auto.\n\nA full-floating axle carries the vehicle's weight on the axle casing, not the halfshafts; they serve only to transmit torque from the differential to the wheels. They \"float\" inside an assembly that carries the vehicle's weight. Thus the only stress it must endure is torque (not lateral bending force). Full-floating axle shafts are retained by a flange bolted to the hub, while the hub and bearings are retained on the spindle by a large nut. In contrast, a semi-floating design carries the weight of the vehicle on the axle shaft itself; there is a single bearing at the end of the axle housing that carries the load from the axle and that the axle rotates through.\n\nThe full-floating design is typically used in most 3/4- and 1-ton light trucks, medium duty trucks and heavy-duty trucks, as well as most agricultural applications, such as large tractors and self-propelled agricultural machinery. There are a few exceptions, such as many Land-Rover vehicles and in American stock car racing since the early 1960s. The overall assembly can carry more weight than a semi-floating or non-floating axle assembly, because the hubs have two bearings riding on a fixed spindle. A full-floating axle can be identified by a protruding hub to which the axle shaft flange is bolted. \n\nThe semi-floating axle setup is commonly used on half-ton and lighter 4x4 trucks in the rear. This setup allows the axle shaft to be the means of propulsion, and also support the weight of the vehicle. The main difference between the full- and semi-floating axle setups is the number of bearings. The semi-floating axle features only one bearing, while the full-floating assembly has bearings in both wheel hubs. The other difference is about the axle removal. To remove the semi-floating axle, one has to remove a wheel first. And, if such axle breaks, the wheel is most likely to come off the vehicle. The semi-floating design is found under most 1/2-ton and lighter trucks and SUVs and rear-wheel-drive passenger cars, usually being smaller or less expensive models.\n\nA benefit of a full-floating axle is that even if an axle shaft (used to transmit torque or power) breaks, the wheel will not come off, preventing serious accidents.\n\n"}
{"id": "15383036", "url": "https://en.wikipedia.org/wiki?curid=15383036", "title": "Bionic contact lens", "text": "Bionic contact lens\n\nBionic contact lenses are devices that, it is proposed by the manufacturers and developers, could provide a virtual display that could have a variety of uses from assisting the visually impaired to video gaming. The device will have the form of a conventional contact lens with added bionics technology in the form of augmented reality, with functional electronic circuits and infrared lights to create a virtual display allowing the viewer to see a computer-generated display superimposed on the world outside.\n\nAn antenna on the lens could pick up a radio frequency.\n\nIn 2016, work on Interscatter from the University of Washington has shown the first Wi-Fi enabled contact lens prototype that can communicate directly with mobile devices such as smart phones at data rates between 2–11Mbit/s.\n\nExperimental versions of these devices have been demonstrated, such as one developed by Sandia National Laboratories. The lens is expected to have more electronics and capabilities on the areas where the eye does not see. Radio frequency power transmission and solar cells are expected in future developments. Recent work augmented the contact lens with Wi-Fi connectivity.\n\nIn 2011, a functioning prototype with a wireless antenna and a single-pixel display was developed.\n\nPrevious prototypes proved that it is possible to create a biologically safe electronic lens that does not obstruct a person’s view. Engineers have tested the finished lenses on rabbits for up to 20 minutes and the animals showed no problems.\n\n"}
{"id": "9764270", "url": "https://en.wikipedia.org/wiki?curid=9764270", "title": "Box crib", "text": "Box crib\n\nA box crib or cribbing is a temporary wooden structure used to support heavy objects during construction, relocation, vehicle extrication and urban search and rescue. It is commonly used to secure overturned motor vehicles, and debris within collapsed buildings. Cribbing is often used in conjunction with other stabilization equipment, such as pneumatic or hydraulic shoring. Cribbing is also used in sub-surface mining as a roof support. Cribbing has largely been replaced by hydraulic shoring in modern mining applications.\n\nSome forms of cribbing can also be used on movie sets and production sites for stabilizing dolly tracks, platforms, and various temporary structures when quick setup times are needed.\n\nThe stability of a crib is affected by a variety of factors: the material used (often a soft wood which gives audible warnings before failure), the number of contact points between the crib and the supported surface, the ratio of the footprint of the crib to its height, and the area of contact made between the crib and the ground and supported surface.\n\nCribbing is usually accomplished with blocks of wood, often 4\"x4\" or 6\"x6\" and 18\"-24\" long. Soft woods, like spruce and pine are often preferred because they crack slowly and make loud noises before completely failing, whereas stiffer woods may fail explosively and without warning.\n\nCribbing may also be made out of plastic, which unlike wood is not susceptible to rot or corrosion from fluids the cribbing may come in contact with (e.g. oil, gasoline, hydraulic fluid).\n\nCribbing equipment is normally of three varieties: rectangular blocks, wedges (also called shims), and \"step chocks\" (large wooden chocks constructed of wood of different lengths). Blocks are the bread and butter of cribbing and will be used in most cribbing evolutions. Shims are used to snug up contact between the crib and supported object or change the direction of the crib (tilt). Step chocks are often used as a quick solution for stabilizing vehicles on all fours or to quickly increase the height of a crib.\n\nCribbing structures are often categorized by shape. Different shapes of cribbing structures are chosen depending on the area available and the point being cribbed to.\n\nA box crib is the simplest, most stable and most common method of \"cribbing\". It is constructed by arranging sets (2 or more) of matched blocks in a regular log-cabin style to form a rising square or rectangular frame. The more blocks on each level, the greater the number of support points and therefore the greater the strength of the crib tower. In trench rescue training materials three basic types of box cribbing are the 4-point, 9-point and full crib. The four point type has two timbers on each level thus four points of contact. Three timbers on each layer makes nine points of contact. The full crib type has each layer filled with timbers. Each point of contact carries a maximum load depending on the size and species of the timbers.\n\nA triangle or A' crib is similar to the box crib, except it resembles a triangular prism instead of rectangular.\n\nA parallelogram crib resembles a diamond prism.\n\nA \"'tilted tower\" crib is a box crib in which pairs of shims are used to change the direction of the rising structure, resulting in a curve. Curving a crib must be done in moderation and is only advisable for structures at sharp angles to the ground.\n\nCribbing is often performed as part of lifting/lowering operations. Expressions such as \"\"lift an inch, crib an inch\" and \"pack as you jack\"\" are used to remind operators of the importance of cribbing to secure and protect the load. The use of cribbing also allows for the use of a lifting device with a limited working range (such as a jack, lifting air bags or hydraulic rescue tools) as the load can be raised to the maximum range of the device, then lowered a short distance onto the cribbing, allowing another platform of cribbing to be built to raise the device and repeat the process.\n\nCribbing can be used to secure a load following a building collapse, vehicular accident or other event where the regular stability of a structure or vehicle has been reduced.\n\nCribbing is often used in the repair of heavy equipment or farm equipment. Subassemblies that have been disconnected are supported by cribbing until reassembly. In such work, cribbing is often safer than jack stands because it is less likely to tip over. And if the metal parts bite into the wood, this helps to keep sliding from happening. These factors are especially true in field repairs, because most ground is not perfectly level and hard, like a concrete garage floor would be. Even on concrete, cribbing is often preferred. \n\nIn heavy industry, cribbing is a part of the rigger's trade. \n\nCribbing is used when moving buildings in whole, or in part. Cribbing is used to raise the structure and allow the carrying vehicle to be positioned underneath, at which point the structure is then lowered onto the vehicle and the cribbing towers are removed. At the other end of the operation, the process is reversed, and the structure is lowered into its new position.\n\n\n"}
{"id": "50577861", "url": "https://en.wikipedia.org/wiki?curid=50577861", "title": "Bread dildo", "text": "Bread dildo\n\nThe bread dildo (Ancient Greek: ὀλισβοκόλλιξ, olisbokollix) is a dildo prepared using bread, allegedly made in the Greco-Roman era around 2,000 years ago. Alternately, it may be a metaphorical joke based on the shape of a loaf of bread.\n\nThe Ancient Greek term \"kollix\" refers to bread, \"olisbos\" refers to a dildo, and the term \"olisbokollix\" is found as a hapax legomenon in the Ancient Greek lexicon of Hesychius \"written in the fifth century A.D.\"\n\nOikonomides claims to identify three different red-figure paintings as depictions of \"bread dildos\":\n\n\n"}
{"id": "347161", "url": "https://en.wikipedia.org/wiki?curid=347161", "title": "Bullroarer", "text": "Bullroarer\n\nThe bullroarer, rhombus, or turndun, is an ancient ritual musical instrument and a device historically used for communicating over great distances. It dates to the Paleolithic period, being found in Ukraine dating from 18,000 BC. Anthropologist Michael Boyd, a bullroarer expert, documents a number found in Europe, Asia, the Indian sub-continent, Africa, the Americas, and Australia.\n\nIn ancient Greece it was a sacred instrument used in the Dionysian Mysteries and is still used in rituals worldwide.\n\nAlong with the didgeridoo, it was a prominent musical technology among the Australian Aborigines, used in ceremonies across the continent.\n\nA bullroarer consists of a weighted airfoil (a rectangular thin slat of wood about (6 in) to (24 in) long and about (0.5 in) to (2 in) wide) attached to a long cord. Typically, the wood slat is trimmed down to a sharp edge around the edges, and serrations along the length of the wooden slat may or may not be used, depending on the cultural traditions of the region in question.\nThe cord is given a slight initial twist, and the roarer is then swung in a large circle in a horizontal plane, or in a smaller circle in a vertical plane. The aerodynamics of the roarer will keep it spinning about its axis even after the initial twist has unwound. The cord winds fully first in one direction and then the other, alternating. \n\nIt makes a characteristic roaring vibrato sound with notable sound modulations occurring from the rotation of the roarer along its longitudinal axis, and the choice of whether a shorter or longer length of cord is used to spin the bullroarer. By modifying the expansiveness of its circuit and the speed given it, and by changing the plane in which the bullroarer is whirled from horizontal to vertical or vice versa, the modulation of the sound produced can be controlled, making the coding of information possible. \n\n\nThe low-frequency component of the sound travels extremely long distances, clearly audible over many miles on a quiet night.\n\nVarious cultures have used bullroarers as musical, ritual, and religious instruments and long-range communication devices for at least 19,000 years. For example, due to their eerie sound, some people used bullroarers in the southern United States in the late 1800s and early 1900s to play pranks on superstitious people.\n\nBecause of its pitch modulation, the bullroarer has been incorrectly used as an example of something exhibiting the Doppler effect. Such an explanation could be correct for an observer, but not for the user. For an observer other than the user, the bullroarer's blade alternately approaches, and recedes from, him or her, leading to the instrument's pitch rising and falling respectively. Such an explanation cannot be correct if the observer is also the user, because for him or her, the blade remains nearly equidistant. Any pitch modulation that is heard identically by a user and a different observer must be a property of the instrument, rather than from the Doppler effect.\n\nThe greatest pitch variation are caused by the spinning blade's winding up or loosening the cord. When the twist in one direction gets tight enough, the blade spin will slow and then it will reverse its spin and unwind rapidly, and will continue that direction of spin until the cord twist tightens again. At that time, the blade will reverse its spin direction again. During the reversals the blade's rotational speed about its long axis rises and falls. This variation in its own rapid rate of spin is what produces the pitch variation.\n\nThis instrument has been used by numerous early and traditional cultures in both the northern and southern hemispheres but in the popular consciousness it is perhaps best known for its use by Australian Aborigines (it is from one of their languages that the name \"turndun\" comes).\n\nHenry Cowell composed a composition for two violins, viola, two celli, and two bullroarers. A bullroarer featured in the Kate Bush \"Before The Dawn\" concerts in London 2014.\n\nBullroarers have been used in initiation ceremonies and in burials to ward off evil spirits, bad tidings, and especially women and children. \nBullroarers are considered secret men's business by all or almost all Aboriginal tribal groups, and hence forbidden for women, children, non-initiated men, or outsiders to even hear. Fison and Howitt documented this in \"Kamilaroi and Kurnai\" (page 198). Anyone caught breaching the imposed secrecy was to be punished by death.\n\nThey are used in men's initiation ceremonies, and the sound they produce is considered in some indigenous cultures to represent the sound of the Rainbow Serpent. In the cultures of southeastern Australia, the sound of the bullroarer is the voice of Daramulan, and a successful bullroarer can only be made if it has been cut from a tree containing his spirit.\n\nIn 1987, Midnight Oil included a recording of a bullroarer on their album \"Diesel and Dust\" (at the beginning of the song \"Bullroarer\") inadvertently causing offense to the Aboriginal people of Central Australia from whom the recording was taken.\n\nThe bullroarer can also be used as a tool in Aboriginal art.\n\nBullroarers have sometimes been referred to as \"wife-callers\" by Australian Aborigines.\n\nA bullroarer is used by Paul Hogan in the 1988 film \"Crocodile Dundee II\". John Antill included one in the orchestration of his ballet \"Corroboree\" (1946). See: Corroboree.\n\nIn Ancient Greece, bullroarers were especially used in the ceremonies of the cult of Cybele. A bullroarer was known as a \"rhombos\" (literally meaning \"whirling\" or \"rumbling\"), both to describe its sonic character and its typical shape, the rhombus. (\"Rhombos\" also sometimes referred to the rhoptron, a buzzing drum).\n\nIn Britain and Ireland, the bullroarer—under a number of different names and styles—is used chiefly for amusement, although formerly it may have been used for ceremonial purposes. In parts of Scotland it was known as a \"thunder-spell\" and was thought to protect against being struck by lightning. In the Elizabeth Goudge novel \"Gentian Hill\" (1949), set in Devon in the early 19th century, a bullroarer figures as a toy cherished by Sol, an elderly farm labourer, who being mute, uses it occasionally to express strong emotion; however, the sound it makes is perceived as being both eerie and unlucky by two other characters, who have an uneasy sense that ominous spirits of the air (\"Them\") are being invoked by its whirring whistle.\n\nScandinavian Stone Age cultures used the bullroarer. In 1991, the archeologists Hein B. Bjerck and Martinius Hauglid found a 6.4 cm-long piece of slate that turned out to be a 5000-year-old bullroarer (called a \"brummer\" in Scandinavia). It was found in Tuv in northern Norway, a place that was inhabited in the Stone Age.\n\nThe Dogon use bullroarers to announce the beginning of ceremonies conducted during the \"Sigui\" festival held every sixty years over a seven-year period. The sound has been identified as the voice of an ancestor from whom all Dogon are descended.\n\nThe pūrerehua is a traditional Māori bullroarer. Its name comes from the Māori word for moth. Made from wood, stone or bone and attached to a long string, the instruments were traditionally used for healing or making rain.\n\nAlmost all the native tribes in North America used bullroarers in religious and healing ceremonies and as toys. There are many styles.\n\nNorth Alaskan Inupiat bullroarers are known as \"imigluktaaq\" or \"imigluktaun\" and described as toy noise-maker of bone or wood and braided sinew (wolf-scare).\n\nThe inland Pomo tribes of California used bullroarers as a central part of the xalimatoto or Thunder ceremony. Four male tribe members, accompanied by a drummer, would spin bullroarers made from cottonwood, imitating the sound of a thunder storm.\n\nShamans of the Amazon basin, for example in Tupi, Kamayurá and Bororo culture used bullroarers as musical instrument for rituals. In Tupian languages, the bullroarer is known as \"hori hori.\"\n\n\n"}
{"id": "14367101", "url": "https://en.wikipedia.org/wiki?curid=14367101", "title": "Candied fruit", "text": "Candied fruit\n\nCandied fruit, also known as crystallized fruit or glacé fruit, has existed since the 14th century. Whole fruit, smaller pieces of fruit, or pieces of peel, are placed in heated sugar syrup, which absorbs the moisture from within the fruit and eventually preserves it. Depending on size and type of fruit, this process of preservation can take from several days to several months. This process allows the fruit to retain its quality for a year.\n\nThe continual process of drenching the fruit in syrup causes the fruit to become saturated with sugar, preventing the growth of spoilage microorganisms due to the unfavourable osmotic pressure this creates.\n\nFruits that are commonly candied include dates, cherries, pineapple, and a root, ginger. The principal candied peels are orange and citron; these with candied lemon peel are the usual ingredients of mixed chopped peel (which may also include glacé cherries).\n\nRecipes vary from region to region, but the general principle is to boil the fruit, steep it in increasingly strong sugar solutions for a number of weeks, and then dry off any remaining water.\n\nFood preservation methods using sugar (palm syrup and honey) were known to the ancient cultures of China and Mesopotamia. However, the precursors of modern candying were the Arabs, who served candied citrus and roses at the important moments of their banquets . With the Arab domination of parts of southern Europe, candied fruit made its way to the West. The first documents that demonstrate the use of candied fruit in Europe date back to the 16th Century . In Italy, they became a key ingredient of some of the most famous sweets of its culinary tradition: among these, the Milanese panettone, the Cassata Siciliana and the Cannoli.\n\nCandied fruits such as cherries are commonly used in fruitcakes or pancakes.\n\n"}
{"id": "264268", "url": "https://en.wikipedia.org/wiki?curid=264268", "title": "Carpet", "text": "Carpet\n\nA carpet is a textile floor covering typically consisting of an upper layer of pile attached to a backing. The pile was traditionally made from wool, but, since the 20th century, synthetic fibers such as polypropylene, nylon or polyester are often used, as these fibers are less expensive than wool. The pile usually consists of twisted tufts which are typically heat-treated to maintain their structure. The term \"carpet\" is often used interchangeably with the term \"rug\", although the term \"carpet\" can be applied to a floor covering that covers an entire house, whereas a \"rug\" is generally no bigger than a single room, and traditionally does not even span from one wall to another, and is typically not even attached as part of the floor.\n\nCarpets are used for a variety of purposes, including insulating a person's feet from a cold tile or concrete floor, making a room more comfortable as a place to sit on the floor (e.g., when playing with children or as a prayer rug), reducing sound from walking (particularly in apartment buildings) and adding decoration or colour to a room. Carpets can be made in any colour by using differently dyed fibers. Carpets can have many different types of patterns and motifs used to decorate the surface. In the 2000s, carpets are used in industrial and commercial establishments such as retail stores and hotels and in private homes. In the 2010s, a huge range of carpets and rugs are available at many price and quality levels, ranging from inexpensive, synthetic carpets that are mass-produced in factories and used in commercial buildings to costly hand-knotted wool rugs which are used in private homes of wealthy families.\n\nCarpets can be produced on a loom quite similar to woven fabric, made using needle felts, knotted by hand (in oriental rugs), made with their pile injected into a backing material (called tufting), flatwoven, made by hooking wool or cotton through the meshes of a sturdy fabric or embroidered. Carpet is commonly made in widths of and in the USA, 4 m and 5 m in Europe. Since the 20th century, where necessary for wall-to-wall carpet, different widths of carpet can be seamed together with a seaming iron and seam tape (formerly it was sewn together) and fixed to a floor over a cushioned underlay (pad) using nails, tack strips (known in the UK as gripper rods), adhesives, or occasionally decorative metal stair rods. Wall-to-wall carpet is distinguished from rugs or mats, which are loose-laid floor coverings, as wall-to-wall carpet is fixed to the floor and covers a much larger area.\n\nThe GoodWeave labelling scheme used throughout Europe and North America assures that child labour has not been used: importers pay for the labels, and the revenue collected is used to monitor centres of production and educate previously exploited children.\n\nIt is assumed that the word \"carpet\" entered into English (English: carpet) in the 13th century (through Medieval Latin carpita, meaning \"thick woolen cloth\") [11] as a consequence of the trade in rugs through the port cities of the Armenian kingdom of Cilicia. The \"Online Etymology Dictionary\" states that the term \"carpet\" was first used in English in the late 13th century, with the meaning \"coarse cloth\", and by the mid-14th century, \"tablecloth, [or] bedspread\". The meaning of the term \"carpet\" shifted in the 15th century to refer to floor coverings. The term \"carpet\" comes from Old French \"carpite\". It is assumed that the word \"carpet\" entered into French (French: carpette) in the 13th century (through Medieval Latin carpita, meaning \"thick woolen cloth\") [11] as a consequence of the trade in rugs through the port cities of the Armenian kingdom of Cilicia. One derivation of the term states that the French term came from the Old Italian \"carpita\", from the verb \"carpire\" meaning \"to pluck\". The \"Online Etymology Dictionary\" states that the term comes \"...from Old French carpite \"heavy decorated cloth, carpet,\" from Medieval Latin or Old Italian carpita \"thick woolen cloth,\" probably from Latin carpere \"to card, pluck,\" probably so called because it was made from unraveled, shred[d]ed, \"plucked\" fabric\". \n\nThe term \"carpet\" is often used interchangeably with the term \"rug\". Some sources define a carpet as stretching from wall to wall. Another definition treats rugs as of lower quality or of smaller size, with carpets quite often having finished ends. A third common definition is that a carpet is permanently fixed in place while a rug is simply laid out on the floor. Historically, the term \"carpet\" was also applied to table and wall coverings, as carpets were not commonly used on the floor in European interiors until the 15th century.\n\nThe term \"rug\" was first used in English in the 1550s, with the meaning \"coarse fabric\". The term is of \"...Scandinavian origin; compare Norwegian dialectal rugga \"coarse coverlet,\" from Old Norse rogg \"shaggy tuft,\" from Proto-Germanic *rawwa-, perhaps related to rag (n.) and rough (adj.).\" The meaning of \"rug\" \"...evolved to \"coverlet, wrap\" (1590s), then \"mat for the floor\" (1808)\".\n\nThe carpet is produced on a loom quite similar to woven fabric. The pile can be plush or Berber. Plush carpet is a cut pile and Berber carpet is a loop pile. There are new styles of carpet combining the two styles called cut and loop carpeting. Normally many colored yarns are used and this process is capable of producing intricate patterns from predetermined designs (although some limitations apply to certain weaving methods with regard to accuracy of pattern within the carpet). These carpets are usually the most expensive due to the relatively slow speed of the manufacturing process. These are very famous in Iran, India, Pakistan, and Arabia.\n\nThese carpets are more technologically advanced. Needle felts are produced by intermingling and felting individual synthetic fibers using barbed and forked needles forming an extremely durable carpet. These carpets are normally found in commercial settings such as hotels and restaurants where there is frequent traffic.\n\nOn a knotted pile carpet (formally, a \"supplementary weft cut-loop pile\" carpet), the structural weft threads alternate with a supplementary weft that rises at right angles to the surface of the weave. This supplementary weft is attached to the warp by one of three knot types (see below), such as shag carpet which was popular in the 1970s, to form the pile or nap of the carpet. Knotting by hand is most prevalent in oriental rugs and carpets. Kashmir carpets are also hand-knotted. Pile carpets, like flat carpets, can be woven on a loom. Both vertical and horizontal looms have been used in the production of European and oriental carpets. The warp threads are set up on the frame of the loom before weaving begins. A number of weavers may work together on the same carpet. A row of knots is completed and cut. The knots are secured with (usually one to four) rows of weft. The warp in woven carpet is usually cotton and the weft is jute.\n\nThere are several styles of knotting, but the two main types of knot are the symmetrical (also called Turkish or Ghiordes) and asymmetrical (also called Persian or Senna). Contemporary centres of knotted carpet production are: Lahore and Peshawar (Pakistan), Kashmir (India), Mirzapur, Bhadohi, Tabriz (Iran), Afghanistan, Armenia, Azerbaijan, Turkey, Northern Africa, Nepal, Spain, Turkmenistan, and Tibet. The importance of carpets in the culture of Turkmenistan is such that the national flag features a vertical red stripe near the hoist side, containing five carpet guls (designs used in producing rugs). Kashmir is known for handknotted carpets of silk or wool. The GoodWeave labelling scheme used throughout Europe and North America assures that child labour has not been used: importers pay for the labels, and the revenue collected is used to monitor centres of production and educate previously exploited children.\n\nThese are carpets that have their pile injected into a backing material, which is itself then bonded to a secondary backing made of a woven hessian weave or a man made alternative to provide stability. The pile is often sheared in order to achieve different textures. This is the most common method of manufacturing of domestic carpets for floor covering purposes in the world.\n\nA flatweave carpet is created by interlocking warp (vertical) and weft (horizontal) threads. Types of oriental flatwoven carpet include kilim, soumak, plain weave, and tapestry weave. Types of European flatwoven carpets include Venetian, Dutch, damask, list, haircloth, and ingrain (aka double cloth, two-ply, triple cloth, or three-ply).\n\nA hooked rug is a simple type of rug handmade by pulling strips of cloth such as wool or cotton through the meshes of a sturdy fabric such as burlap. This type of rug is now generally made as a handicraft. The process of creating a hooked rug is called Rug hooking\n\nUnlike woven carpets, embroidery carpets' are not formed on a loom. Their pattern is established by the application of stitches to a cloth (often linen) base. The tent stitch and the cross stitch are two of the most common. Embroidered carpets were traditionally made by royal and aristocratic women in the home, but there has been some commercial manufacture since steel needles were introduced (earlier needles were made of bone) and linen weaving improved in the 16th century. Mary, Queen of Scots, is known to have been an avid embroiderer. 16th century designs usually involve scrolling vines and regional flowers (for example, the Bradford carpet). They often incorporate animal heraldry and the coat of arms of the maker. Production continued through the 19th century. Victorian embroidered carpet compositions include highly illusionistic, 3-dimensional flowers. Patterns for tiled carpets made of a number of squares, called Berlin wool work, were introduced in Germany in 1804, and became extremely popular in England in the 1830s. Embroidered carpets can also include other features such as a pattern of shapes, or they can even tell a story.\n\nCarpet can be formulated from many single or blended natural and synthetic fibres. Fibres are chosen for durability, appearance, ease of manufacture, and cost. In terms of scale of production, the dominant yarn constructions are polyamides (nylons) and polypropylene with an estimated 90% of the commercial market.\n\nSince the 20th century, nylon is one of the most common materials for the construction of carpets. Both nylon 6 and nylon 6-6 are used. Nylon can be dyed topically or dyed in a molten state (solution dying). Nylon can be printed easily and has excellent wear characteristics. Due to nylon's excellent wear-resistance, it is widely used in industrial and commercial carpeting. In carpets, nylon tends to stain easily because of the dye sites which exist on the fibre. These dye sites need to be filled in order to give nylon carpet any type of stain resistance. As nylon is petroleum-based it varies in price with the price of oil.\n\nPolypropylene, a polyolefin stiffer than the cheaper polyethylene, is used to produce carpet yarns because it is still less expensive than the other materials used for carpets. It is difficult to dye and does not wear as well as wool or nylon. Polypropylene, sometimes referred to simply as \"olefin\", is commonly used to construct berber carpets. Large looped olefin berber carpets are usually only suited for light domestic use and tend to mat down quickly. Berber carpets with smaller loops tend to be more resilient and retain their new appearance longer than large looped berber styles. Commercial grade level-loop carpets have very small loops, and commercial grade cut-pile styles can be well constructed. When made with polypropylene, commercial grade styles wear very well, making them very suitable for areas with heavy foot traffic such as offices. Polypropylene carpets are known to have good stain resistance, but not against oil- based agents. If a stain does set, it can be difficult to clean. Commercial grade carpets can be glued directly to the floor or installed over a 1/4\" thick, 8-pound density padding. Outdoor grass carpets are usually made from polypropylene.\n\nWool has excellent durability, can be dyed easily and is fairly abundant. When blended with synthetic fibres such as nylon the durability of wool is increased. Blended wool yarns are extensively used in production of modern carpet, with the most common blend being 80% wool to 20% synthetic fibre, giving rise to the term \"80/20\". Wool is relatively expensive and consequently it only comprises a small portion of the market.\n\nThe polyester known as \"PET\" (polyethylene terephthalate) is used in carpet manufacturing in both spun and filament constructions. After the price of raw materials for many types of carpet rose in the early 2000s, polyester became more competitive. Polyester has good physical properties and is inherently stain-resistant because it is hydrophobic, and, unlike nylon, does not have dye sites. Colour is infused in a molten state (solution dyeing). Polyester has the disadvantage that it tends to crush or mat down easily. It is typically used in mid- to low-priced carpeting.\n\nAnother polyester, \"PTT\" (Polytrimethylene terephthalate), also called Sorona or 3GT (Dupont) or Corterra (Shell), is a variant of PET. Lurgi Zimmer PTT was first patented in 1941, but it was not produced until the 1990s, when Shell Chemicals developed the low-cost method of producing high-quality 1,3 propanediol (PDO), the starting raw material for PTT Corterra Polymers. DuPont subsequently commercialized a biological process for making 1,3-propanediol from corn syrup, imparting significant renewable content on the corresponding Sorona polyester carpet fibers. These carpet fibers have resiliency comparable to nylon.\n\nAcrylic is a synthetic material first created by the Dupont Corporation in 1941 but has gone through various changes since it was first introduced. In the past, acrylic carpet used to fuzz or \"pill\" easily. This happened when the fibres degraded over time and short strands broke away with contact or friction. Over the years, new types of acrylics have been developed to alleviate some of these problems, although the issues have not been completely removed. Acrylic is fairly difficult to dye but is colourfast, washable, and has the feel and appearance of wool, making it a good rug fabric.\n\nThe knotted pile carpet probably originated in the Caspian Sea area (Northern Iran) or the Armenian Highland. Although there is evidence of goats and sheep being sheared for wool and hair which was spun and woven as far back at the 7th millennium, the earliest surviving pile carpet is the \"Pazyryk carpet\", which dates from the 5th-4th century BC. It was excavated by Sergei Ivanovich Rudenko in 1949 from a Pazyryk burial mound in the Altai Mountains in Siberia. This richly coloured carpet is 200 x 183 cm (6'6\" x 6'0\") and framed by a border of griffins.\n\nAlthough claimed by many cultures, this square tufted carpet, almost perfectly intact, is considered by many experts to be of Caucasian, specifically Armenian, origin. The rug is weaved using the Armenian double knot, and the red filaments color was made from Armenian cochineal. The eminent authority of ancient carpets, Ulrich Schurmann, says of it, \"From all the evidence available I am convinced that the Pazyryk rug was a funeral accessory and most likely a masterpiece of Armenian workmanship\". Gantzhorn concurs with this thesis. At the ruins of Persepolis in Iran where various nations are depicted as bearing tribute, the horse design from the Pazyryk carpet is the same as the relief depicting part of the Armenian delegation.\nThe historian Herodotus writing in the 5th century BC also informs us that the inhabitants of the Caucasus wove beautiful rugs with brilliant colors which would never fade.\n\nThere has recently been a surge in demand for Afghan carpets, although many Afghan carpet manufacturers market their products under the name of a different country. The carpets are made in Afghanistan, as well as by Afghan refugees who reside in Pakistan and Iran. Famous Afghan rugs include the \"Shindand\" or \"Adraskan\" (named after local Afghan villages), woven in the Herat area in western Afghanistan.\n\nAfghan carpets are commonly known as Afghan rugs. Afghan carpets are a unique and well recognized handmade material design that originates from Afghanistan. They often exhibit intricate detailing, mainly using traditional tribal designs originating from the Turkmen, Kazakh, Baloch, and Uzbeks. The hand-made rugs come in many patterns and colors, yet the traditional and most common example of Afghan carpet is the octagon-shaped elephant-foot (Bukhara). The rugs with this print are most commonly red in color. Many dyes, such as vegetable dyes, are used to impart rich color.\n\nThe historian Herodotus writing in the 5th century BC also informs us that the inhabitants of the Caucasus wove beautiful rugs with brilliant colors which would never fade.[23] Various rug fragments have been excavated in Armenia dating back to the 7th century BC or earlier. The oldest, single, surviving knotted carpet in existence is the Pazyryk carpet, excavated from a frozen tomb in Siberia, dated from the 5th to the 3rd century BC, now in the Hermitage Museum in St. Petersburg. This square tufted carpet, almost perfectly intact, is considered by many experts to be of Caucasian, specifically Armenian, origin. The eminent authority of ancient carpets, Ulrich Schurmann, says of it, \"From all the evidence available I am convinced that the Pazyryk rug was a funeral accessory and most likely a masterpiece of Armenian workmanship\". Gantzhorn concurs with this thesis. At the ruins of Persepolis in Iran where various nations are depicted as bearing tribute, the horse design from the Pazyryk carpet is the same as the relief depicting part of the Armenian delegation.\nArmenian carpets were renowned by foreigners who travelled to Artsakh; the Arab geographer and historian Al-Masudi noted that, among other works of art, he had never seen such carpets elsewhere in his life.\n\nArt historian Hravard Hakobyan notes that \"Artsakh carpets occupy a special place in the history of Armenian carpet-making.\" Common themes and patterns found on Armenian carpets were the depiction of dragons and eagles. They were diverse in style, rich in colour and ornamental motifs, and were even separated in categories depending on what sort of animals were depicted on them, such as \"artsvagorgs\" (eagle-carpets), \"vishapagorgs\" (dragon-carpets) and \"otsagorgs\" (serpent-carpets). The rug mentioned in the Kaptavan inscriptions is composed of three arches, \"covered with vegatative ornaments\", and bears an artistic resemblance to the illuminated manuscripts produced in Artsakh.\n\nThe art of carpet weaving was in addition intimately connected to the making of curtains as evidenced in a passage by Kirakos Gandzaketsi, a 13th-century Armenian historian from Artsakh, who praised Arzu-Khatun, the wife of regional prince Vakhtang Khachenatsi, and her daughters for their expertise and skill in weaving.\n\nThe Gultapin excavations discovered several carpet weaving tools which date back to the 4th-3rd millennium BC. According to Iranica Online \"\"The main weaving zone was in the eastern Transcaucasus south of the mountains that bisect the region diagonally, the area now comprised in the Azerbaijan SSR; it is the homeland of a Turkic population known today as Azeri. Other ethnic groups also practiced weaving, some of them in other parts of the Caucasus, but they were of lesser importance\".\" Azerbaijan was one of the most important centers of carpet weaving and as a result of that, several different schools have evolved. While traditionally schools are divided into four main branches, each region has its own version of the carpets. The Schools are divided into four main branches: Kuba-Shirvan, Ganja-Kazakh carpet-weaving school, The Baku carpet school, Karabakh school of carpet weaving.\n\nAs opposed to most antique rug manufactory practices, Chinese carpets were woven almost exclusively for internal consumption. China has a long history of exporting traditional goods; however, it was not until the first half of the 19th century that the Chinese began to export their rugs. Once in contact with western influences, there was a large change in production: Chinese manufactories began to produce art-deco rugs with commercial look and price point. The centuries-old Chinese textile industry is rich in history. While most antique carpets are classified according to a specific region or manufactory, scholars attribute the age of any specific Chinese rug to the ruling emperor of the time. The earliest surviving examples of the craft were produced during the time of Ch'ung Chen, the last emperor of the Chen Dynasty.\n\nCarpet weaving may have been introduced into the area as far back as the eleventh century with the coming of the first Muslim conquerors, the Ghaznavids and the Ghauris, from the West. It can with more certainty be traced to the beginning of the Mughal Dynasty in the early sixteenth century, when the last successor of Timur, Babar, extended his rule from Kabul to India to found the Mughal Empire. Under the patronage of the Mughals, Indian craftsmen adopted Persian techniques and designs. Carpets woven in the Punjab made use of motifs and decorative styles found in Mughal architecture.\n\nAkbar, a Mogul emperor, is accredited to introducing the art of carpet weaving to India during his reign. The Mughal emperors patronized Persian carpets for their royal courts and palaces. During this period, he brought Persian craftsmen from their homeland and established them in India. Initially, the carpets woven showed the classic Persian style of fine knotting. Gradually it blended with Indian art. Thus the carpets produced became typical of the Indian origin and gradually the industry began to diversify and spread all over the subcontinent. During the Mughal period, the carpets made on the Indian subcontinent became so famous that demand for them spread abroad. These carpets had distinctive designs and boasted a high density of knots. Carpets made for the Mughal emperors, including Jahangir and Shah Jahan, were of the finest quality. Under Shah Jahan's reign, Mughal carpet weaving took on a new aesthetic and entered its classical phase. Indian carpets are well known for their designs with attention to detail and presentation of realistic attributes. The carpet industry in India flourished more in its northern part with major centres found in Kashmir, Jaipur, Agra and Bhadohi.\n\nIndian carpets are known for their high density of knotting. Hand-knotted carpets are a speciality and widely in demand in the West. The carpet industry in India has been successful in establishing social business models that help underprivileged sections of the society. Notable examples of social entrepreneurship ventures are Jaipur rugs, Fabindia.\n\nAnother category of Indian rugs which, though quite popular in most of the western countries, have not received much press, is hand-woven rugs of Khairabad (Citapore rugs). Khairabad small town in Citapore (now spelled as \"Sitapur\") district of India had been ruled by Raja Mehmoodabad. Khairabad (Mehmoodabad Estate) was part of Oudh province which had been ruled by shi'i Muslims having Persian linkages. Citapore rugs made in Khairabad and neighbouring areas are all hand-woven and distinct from tufted and knotted rugs. Flat weave is the basic weaving technique of Citapore rugs and generally cotton is the main weaving material here but jute, rayon and chenille are also popular. \"IKEA\" and \"Agocha\" have been major buyers of rugs from this area.\n\nThe art of weaving developed in South Asia at a time when few other civilizations employed it. Excavations at Harappa and Mohenjo-Daro– ancient cities of the Indus Valley Civilization– have established that the inhabitants used spindles and spun a wide variety of weaving materials. Some historians consider that the Indus Valley civilization first developed the use of woven textiles. As of the late 1990s, hand-knotted carpets were among Pakistan's leading export products and their manufacture is the second largest cottage and small industry. Pakistani craftsmen have the capacity to produce any type of carpet using all the popular motifs of gulls, medallions, paisleys, traceries, and geometric designs in various combinations. At the time of independence, manufacturing of carpets was set up in Sangla Hill, a small town of District Sheikhupura. Chaudary Mukhtar Ahmad Member son of Maher Ganda introduced and taught this art to locals and immigrants. He is considered founder of this industry in Pakistan. Sangla Hill is now a focal point in Carpet Industry in Pakistan. Almost all the exporters and manufacturers who are running their business at Lahore, Faisalabad and Karachi have their area offices in Sangla Hill.\n\nThe Persian carpet is a part of Persian (Iranian) art and culture. Carpet-weaving in Persia dates back to the Bronze Age. The earliest surviving corpus of Persian carpets come from the Safavid dynasty (1501–1736) in the 16th century. However, painted depictions prove a longer history of production. There is much variety among classical Persian carpets of the 16th and 17th century. Common motifs include scrolling vine networks, arabesques, palmettes, cloud bands, medallions, and overlapping geometric compartments rather than animals and humans. This is because Islam, the dominant religion in that part of the world, forbids their depiction. Still, some show figures engaged either in the hunt or feasting scenes. The majority of these carpets are wool, but several silk examples produced in Kashan survive.\n\nIran is also the world's largest producer and exporter of handmade carpets, producing three quarters of the world's total output and having a share of 30% of world's export markets. Iran is also the maker of the largest handmade carpet in history, measuring 60,546 square feet (equal to over 5600 sqaure meters).\n\nScandinavian rugs are among the most popular of all weaves in modern design. Preferred by influential modernist thinkers, designers, and advocates for a new aesthetic in the mid-twentieth century, Scandinavian rugs have become very widespread in many different avenues of contemporary interior design. With a long history of adaptation and evolution, the tradition of Scandinavian rug-making is among the most storied of all European rug-making traditions.\n\nTurkish carpets (also known as Anatolian), whether hand knotted or flat woven, are among the most well known and established hand crafted art works in the world. Historically: religious, cultural, environmental, sociopolitical and socioeconomic conditions created widespread utilitarian need and have provided artistic inspiration among the many tribal peoples and ethnic groups in Central Asia and Turkey. Turks; nomadic or pastoral, agrarian or town dwellers, living in tents or in sumptuous houses in large cities, have protected themselves from the extremes of the cold weather by covering the floors, and sometimes walls and doorways, with carpets and rugs. The carpets are always hand made of wool or sometimes cotton, with occasional additions of silk. These carpets are natural barriers against the cold. Turkish pile rugs and kilims are also frequently used as tent decorations, grain bags, camel and donkey bags, ground cushions, oven covers, sofa covers, bed and cushion covers, blankets, curtains, eating blankets, table top spreads, prayer rugs and for ceremonial occasions.\n\nThe oldest records of flat woven kilims come from Çatalhöyük Neolithic pottery, circa 7000 B.C. One of the oldest settlements ever to have been discovered, Çatalhöyük is located south east of Konya in the middle of the Anatolian region. The excavations to date (only 3% of the town) not only found carbonized fabric but also fragments of kilims painted on the walls of some of the dwellings. The majority of them represent geometric and stylized forms that are similar or identical to other historical and contemporary designs.\n\nThe knotted rug is believed to have reached Asia Minor and the Middle East with the expansion of various nomadic tribes peoples during the latter period of the great Turkic migration of the 8th and 9th centuries. Famously depicted in European paintings of The Renaissance, beautiful Anatolian rugs were often used from then until modern times, to indicate the high economic and social status of the owner.\n\nWomen learn their weaving skills at an early age, taking months or even years to complete the beautiful pile rugs and flat woven kilims that were created for their use in every aspect of daily life. As is true in most weaving cultures, traditionally and nearly exclusively, it is women and girls who are both artisan and weaver.\n\nTürkmen carpet (also called \"Bukhara Uzbekistan\") is a type of handmade floor-covering textile traditionally originating in Central Asia. It is useful to distinguish between the original Turkmen tribal rugs and the rugs produced in large numbers for export in the 2000s, mainly in Pakistan and Iran. The original Turkmen rugs were produced by the Turkmen tribes who are the main ethnic group in Turkmenistan and are also found in Afghanistan and Iran. They are used for various purposes, including tent rugs, door hangings and bags of various sizes.\n\nOriental carpets began to appear in Europe after the Crusades in the 11th century, due to contact by Crusaders with Eastern traders. Until the mid-18th century they were mostly used on walls and tables. Except in royal or ecclesiastical settings they were considered too precious to cover the floor. Starting in the 13th century oriental carpets begin to appear in paintings (notably from Italy, Flanders, England, France, and the Netherlands). Carpets of Indo-Persian design were introduced to Europe via the Dutch, British, and French East India Companies of the 17th and 18th century and in the Polish–Lithuanian Commonwealth by Armenian merchants (Polish carpets or Polonaise carpets).\n\nAlthough isolated instances of carpet production pre-date the Muslim invasion of Spain, the Hispano-Moresque examples are the earliest significant body of European-made carpets. Documentary evidence shows production beginning in Spain as early as the 10th century AD. The earliest extant Spanish carpet, the so-called Synagogue carpet in the Museum of Islamic Art, Berlin, is a unique survival dated to the 14th century. The earliest group of Hispano-Moresque carpets, Admiral carpets (also known as armorial carpets), has an all-over geometric, repeat pattern punctuated by blazons of noble, Christian Spanish families. The variety of this design was analyzed most thoroughly by May Beattie. Many of the 15th-century, Spanish carpets rely heavily on designs originally developed on the Anatolian Peninsula. Carpet production continued after the Reconquest of Spain and eventual expulsion of the Muslim population in the 15th century. 16th-century Renaissance Spanish carpet design is a derivative of silk textile design. Two of the most popular motifs are wreaths and pomegranates.\n\nDuring the Moorish (Muslim) period production took place in Alcaraz in the province of Murcia, as well as being recorded in other towns. Carpet production after the Christian reconquest continued in Alcaraz while Cuenca, first recorded as a weaving centre in the 12th century, became increasingly important, and was dominant in the 17th and early 18th century. Carpets of completely different French based designs began to be woven in a royal workshop, the Royal Tapestry Factory \"(Real Fábrica de Tapices de Santa Bárbara)\" in Madrid in the 18th century. Cuenca was closed down by the royal degree of Carlos IV in the late 18th century to stop it competing with the new workshop. Madrid continued as a weaving centre through to the 20th century, producing brightly coloured carpets most of whose designs are strongly influenced by French carpet design, and which are frequently signed (on occasions with the monogram MD; also sometimes with the name Stuyck) and dated in the outer stripe. After the Spanish civil war General Franco revived the carpet weaving industry in workshops named after him, weaving designs that are influenced by earlier Spanish carpets, usually in a very limited range of colours.\n\nPirot carpet (Serbian: Пиротски ћилим, Pirotski ćilim) refers to a variety of flat tapestry-woven carpets or rugs traditionally produced in Pirot, a town in southeastern Serbia. Pirot kilims with some 122 ornaments and 96 different types have been protected by geographical indication in 2002. They are one of the most important traditional handicrafts in Serbia. In the late 19th century and up to the Second World War, Pirot kilims have been frequently used as insignia of Serbian and Yugoslav royalty. This tradition was revived in 2011 when Pirot kilims were reintroduced for state ceremonies in Serbia. Carpet weaving in Pirot dates back to the Middle Ages. One of the first mentions of the Pirot kilim in written sources date to 1565, when it was said that the šajkaši boats on the Danube and Drava were covered with Pirot kilims. Pirot was once the most important rug-making centre in the Balkans. Pirot is located on the historical main highway which linked central Europe with Constantinople.Pirot was also known as Şarköy in Turkish. The Pirot carpet varieties are also found in Bulgaria and Turkey, and in many other international collections. One of the chief qualities are the colour effects achieved through the choice and arrangement of colours.\n\nIn the beginning of the 19th century plant dyes were replaced by aniline colourings. \"The best product of the country is the Pirot carpet, worth about ten shillings a square metre. The designs are extremely pretty, and the rugs, without being so heavy as the Persian, or so ragged and scant in the web and woof as Caramanian, wear for ever. The manufacture of these is almost entirely confined to Pirot. From Pirots old Turkish signification as Şarköy stems the traditional trade name of the rugs as Şarköy-kilims. Stemming from the homonym to the today's Turkish settlement of Şarköy in Thracia, which had no established rug making tradition, Şarköys are often falsely ascribed to originate from Turkey. Also in the rug selling industry, Şarköy are mostly labeled as being of oriental or Turkish origin as to easier sell them to non familiar customers as they prefer rug with putative oriental origin. In fact, Şarköys have been established from the 17th century in the region of the Western Balkan or Stara Planina mountains in the towns of Pirot, Berkowiza, Lom, Chiprovtsi and Samokow. Later they have been also produced in Knjaževac and Caribrod.\n\nThe Chiprovtsi carpet (Чипровци килим) is a type of handmade carpet with two absolutely identical sides, part of Bulgarian national heritage, traditions, arts and crafts. Its name is derived from the town of Chiprovtsi where their production started in the 17th century. The carpet weaving industry played a key role in the revival of Chiprovtsi in the 1720s after the devastation of the failed 1688 Chiprovtsi Uprising against Ottoman rule. The western traveller Ami Boué, who visited Chiprovtsi in 1836–1838, reported that \"mainly young girls, under shelters or in corridors, engage in carpet weaving. They earn only five francs a month and the payment was even lower before\". By 1868, the annual production of carpets in Chiprovtsi had surpassed 14,000 square metres. In 1896, almost 1,400 women from Chiprovtsi and the region were engaged in carpet weaving. In 1920, the locals founded the \"Manual Labour\" carpet-weaving cooperative society, the first of its kind in the country. At present. the carpet (\"kilim\") industry remains dominant in the town. Carpets have been crafted according to traditional designs, but in recent years it is up to the customers to decide the pattern of the carpet they have ordered. The production of a single carpet takes about 50 days; primarily women engage in carpet weaving. Work is entirely manual and all used materials are natural; the primary material is wool, coloured using plant or mineral dyes. The local carpets have been prized at exhibitions in London, Paris, Liège and Brussels. In recent decades, however, the Chiprovtsi carpet industry has been in decline as it had lost its firm foreign markets. As a result, the town and the municipality have been experiencing a demographic crisis.\n\nIn 1608 Henry IV initiated the French production of \"Turkish style\" carpets under the direction of Pierre DuPont. This production was soon moved to the Savonnerie factory in Chaillot just west of Paris. The earliest, well-known group produced by the Savonnerie, then under the direction of Simon Lourdet, are the carpets that were produced in the early years of Louis XIV's reign. They are densely ornamented with flowers, sometimes in vases or baskets, against dark blue or brown grounds in deep borders. The designs are based on Netherlandish and Flemish textiles and paintings. The most famous Savonnerie carpets are the series made for the Grande Galerie and the Galerie d'Apollon in the Palais du Louvre between c. 1665-1685. These 105 masterpieces, made under the artistic direction of Charles Le Brun, were never installed, as Louis XIV moved the court to Versailles in 1688. Their design combines rich acanthus leaves, architectural framing, and mythological scenes (inspired by Cesare Ripa's Iconologie) with emblems of Louis XIV's royal power.\n\nPierre-Josse Perrot is the best-known of the mid-eighteenth-century carpet designers. His many surviving works and drawings display graceful rococo s-scrolls, central rosettes, shells, acanthus leaves, and floral swags. The Savonnerie manufactory was moved to the Gobelins in Paris in 1826. The Beauvais manufactory, better known for their tapestry, also made knotted pile carpets from 1780 to 1792. Carpet production in small, privately owned workshops in the town of Aubusson began in 1743. Carpets produced in France employ the symmetrical knot.\n\nKnotted pile carpet weaving technology probably came to England in the early 16th century with Flemish Calvinists fleeing religious persecution. Because many of these weavers settled in South-eastern England in Norwich the 14 extant 16th and 17th century carpets are sometimes referred to as \"Norwich carpets.\" These works are either adaptations of Anatolian or Indo-Persian designs or employ Elizabethan-Jacobean scrolling vines and blossoms. All but one are dated or bear a coat of arms. Like the French, English weavers used the symmetrical knot. There are documented and surviving examples of carpets from three 18th-century manufactories: Exeter (1756–1761, owned by Claude Passavant, 3 extant carpets), Moorfields (1752–1806, owned by Thomas Moore, 5 extant carpets), and Axminster (1755–1835, owned by Thomas Whitty, numerous extant carpets). Exeter and Moorfields were both staffed with renegade weavers from the French Savonnerie and, therefore, employ the weaving structure of that factory and Perrot-inspired designs. Neoclassical designer Robert Adam supplied designs for both Moorfields and Axminster carpets based on Roman floor mosaics and coffered ceilings. Some of the most well-known rugs of his design were made for Syon House, Osterley House, Harewood House, Saltram House, and Newby Hall.\n\nAxminster carpet has three main types of broadloom carpet construction in use today (machine woven, tufted & hand knotted). Machine woven carpet is an investment that will last 20 or 30 years and woven Axminster and Wilton carpets are still extremely popular in areas where longevity and design flexibility are a big part of the purchasing decision. Hotels and leisure venues almost always choose these types and many homes use woven Axminsters as design statements.\nMachine-woven carpets like Axminster and Wilton are made by massive looms that weave together ‘bobbins’ of carpet yarn and backing. The finished result, which can be intricately patterned, creates a floor that provides supreme underfoot luxury with high performance. Tufted carpets are also popular in the home. They are relatively speedy to make - a pre-woven backing has yarns tufted into it. Needles push the yarn through the backing and which is then held in place with underlying \"loopers\". Tufted carpets can be twist pile, velvet, or loop pile. Twist pile carpets are produced when one or more fibres are twisted in the tufting process, so that in the finished carpet they appear to be bound together. Velvet pile carpets tend to have a shorter pile and a tighter construction, giving the finished article a smooth, velvety appearance. Loop pile carpets are renowned for being hard wearing and lend carpets great texture. The traditional domain of rugs from far away continents, hand knotted squares and rugs use the expertise of weavers to produce work of the finest quality. Traditional rugs often feature a deliberate ‘mistake’ on behalf of the weaver to guarantee their authenticity.\n\nSix of Axminster carpets are known as the \"Lansdowne\" group. These have a tripartite design with reeded circles and baskets of flowers in the central panel flanked by diamond lozenges in the side panels. Axminster Rococo designs often have a brown ground and include birds copied from popular, contemporary engravings. Even now a large percentage of the 55,000 population town still seek employment in this industry. The town of Wilton, Wiltshire is also known for its carpet weaving, which dates back to the 18th century.\n\nThe Brussels Loom was introduced into England towards the middle of the eighteenth century marked the beginning of a new era in carpet-weaving. It was the first loom on which a pile carpet could be woven mechanically, the pile consisting of rows of loops, formed over wires inserted weftwise during weaving and subsequently withdrawn. Brussels was the first type of carpet to be woven in a loom incorporating the jacquard pattern selecting mechanism and in 1849 power was applied to the loom by Biglow in the U.S.A.\n\nLater when bladed wires were developed the pile loops were severed on withdrawal of the blade wires to produce a carpet known as Wilton, after this development the loom became known as the Wilton loom, and in modern usage the designation Wilton applies to both cut-pile and loop-pile carpets made in this loom. The latter now variously described as Brussels-Wilton, round wire Wilton, loop-pile Wilton, and round wired jacquard. The methods of manufacture, including the principles of designing, preparatory processes, and weaving, are the same in most respects for both Brussels and Wilton qualities. The chief difference between them is that whereas Brussels loop-pile is secured satisfactorily by the insertion of two picks of weft to each wire (2-shot), the Wilton cut-pile is woven more often with three picks of weft to each wire (3-shot) to ensure that the tufts are firmly secured in the carpet backing.\n\nBrussels carpets have a smooth slightly ribbed surface and their patterning is well defined, a characteristic feature of the carpet. Closeness of pile rather than height contributes to their neat appearance and hard wearing properties, although they do not simulate the luxury of cut-pile carpets. Brussels Wilton Carpets were initially produced on 27-inch (3/4) looms and were sewn together by hand. The looms could incorporate up to 5 frames all with different colours thus enabling figured or pattern carpets to be manufactured. With judicial and very skilful planting of colours in the frames the number of colours could be increased to about twenty, thus enabling very complex designs to be produced. Due to the additional costs in labour these carpets were normally only produced for the bespoke market.\n\nAfter the first World War the carpets started to be produced for the general market using popular designs and colourways but they always remained at the luxury end of the general market. The growing middle class of the twentieth century aspired to acquire a Wilton carpet for their 'best' room. Despite the impact of industrialization, the areas where Brussels Wilton carpets were produced remained centred mainly in the Midlands around the towns of Wilton and Kidderminster and in West Yorkshire where the firm of John Crossley and Sons in Halifax became synonymous with carpet manufacture. There were smaller areas of manufacture in Scotland and Durham. With the development of different manufacturing methods and looms capable of the mass production of carpets, the public began change their décor, including carpets, on a regular basis, which increased the demand for carpets. The last quarter of the 20th century saw the rapid decline of the labour-intensive Brussels Wilton carpets. Very few of the original ¾ Wilton looms still exist and the few that do are either in museums or used by small manufacturers that continue to produce bespoke (custom-made) luxury carpets for the elite and to replace carpets in historic buildings in the UK and abroad.\n\nCarpet is commonly made in widths of and in the US, 4 m and 5 m in Europe. Where necessary different widths can be seamed together with a seaming iron and seam tape (formerly it was sewn together) and it is fixed to a floor over a cushioned underlay (pad) using nails, tack strips (known in the UK as gripper rods), adhesives, or occasionally decorative metal stair rods, thus distinguishing it from rugs or mats, which are loose-laid floor coverings. For environmental reasons, the use of wool, natural bindings, natural padding, and formaldehyde-free glues is becoming more common. These options are almost always at a premium cost.\n\nIn the UK, some carpets are still manufactured for yachts, hotels, pubs and clubs in a narrow width of and then sewn to size. Carpeting which covers an entire room area is loosely referred to as 'wall-to-wall', but carpet can be installed over any portion thereof with use of appropriate transition moldings where the carpet meets other types of floor coverings. Carpeting is more than just a single item; it is, in fact, a system comprising the carpet itself, the carpet backing (often made of latex), the cushioning underlay, and a method of installation.\nCarpet tiles are also available, typically square. These are usually only used in commercial settings and are affixed using a special pressure-sensitive glue, which holds them into place while allowing easy removal (in an office environment, for example) or to allow rearrangement in order to spread wear.\n\n\"Carpet binding\" is a term used for any material being applied to the edge of a carpet to make a rug. Carpet binding is usually cotton or nylon, but also comes in many other materials such as leather. Non-synthetic binding is frequently used with bamboo, grass and wool rugs, but is often used with carpet made from other materials.\n\nThere are many stories about magic carpets, legendary flying carpets that can be used to transport people who are on it instantaneously or quickly to their destination. Disney's Aladdin depicts a magic carpet found by Aladdin and Abu in the Cave of Wonders while trying to find Genie's lamp. Aladdin and Jasmine ride on him to go on a ride around the world. The term \"[m]agic carpet [is] first attested [in] 1816. From the 16th century to the 19th century, the term \"carpet\" was used \"...as an adjective often with a tinge of contempt, when used of men (as in carpet-knight, 1570s)\", which meant a man who was associated with \"...luxury, ladies' boudoirs, and drawing rooms\". \"Rolling out the red carpet\" is an expression which means to welcome a guest lavishly and handsomely. In some cases, an actual red carpet is used for VIPs and celebrities to walk on, such as at the Cannes Film Festival and when foreign dignitaries are welcomed to a country.\n\nIn 1820s British servant slang, to \"carpet\" someone means to call them for a reprimand. To be \"called on the carpet\" means to be summoned for a serious reason, typically a scolding reprimand; this usage dates from 1900. A stronger variant of this expression, to be \"hauled on the carpet\", implies an even sterner reprimand. Carpet bombing is a type of bombing from airplanes which developed in the 20th century in which an entire city is bombed (rather than precise strikes on military targets). The slang expression \"laugh at the carpet\" means to vomit on the floor (especially a carpeted floor). The expression \"on the carpet\" refers to a matter which is under discussion or consideration. The term \"carpet muncher\" is a derogatory slang term for a lesbian; this expression is first attested in 1992.\n\nThe term carpet bag, which literally refers to a suitcase made from a piece of carpet, is used in several figurative contexts. The term gained a popular usage after the American Civil War to refer to carpetbaggers, Northerners who moved to the South after the war, especially during the Reconstruction era (1865–1877). Carpetbaggers allegedly politically manipulated and controlled former Confederate states for financial and power gains. In modern usage in the U.S., the term is sometimes used derisively to refer to a politician who runs for public office in an area where he or she does not have deep community ties, or has lived only for a short time. In the United Kingdom, the term was adopted to refer informally to those who join a mutual organization, such as a building society, in order to force it to demutualize, that is, to convert into a joint stock company, solely for personal financial gain.\n\n\"Cutting the rug\" is a slang term for dancing which originated in 1942. The use of the term \"rug\" as an informal term for a \"toupee\" (man's wig) is theater slang from 1940. The term \"sweep [something] under the rug\" or \"sweep [something] under the carpet\" figuratively refers to situations where a person or organization is hiding something embarrassing or negative; this use was first recorded in 1953. The figurative expression \"pull the rug out from under (someone)\", meaning to \"suddenly deprive of important support\" is first attested to in 1936, in American English. A related figurative expression used centuries earlier was \"cut the grass under (one's) feet\", which is attested to in the 1580s. A \"rugrat\" or \"rug-rat\" is a slang term for a baby or child, first attested in 1968. The expression \"snug as a bug in a rug\" means\n\"wrapped up tight, warm, and comfortable\". To \"lie like a rug\" means \"to tell lies shamelessly\". The expression \"pull the rug out (from under someone)\" means \"to make someone or someone's plans fall through\" or \"to upset someone's plans\".\n\n"}
{"id": "1726070", "url": "https://en.wikipedia.org/wiki?curid=1726070", "title": "Churn drill", "text": "Churn drill\n\nThe churn drill is a large drilling machine that bores large diameter holes in the ground. In mining, they were used to drill into the soft carbonate rocks of lead and zinc hosted regions to extract bulk samples of the ore.\n\nChurn drills were invented as early as 221 BC in Qin Dynasty China, capable of reaching a depth of 1500 m. Churn drills in ancient China were built of wood and labor-intensive, but were able to go through solid rock. The churn drill was transmitted to Europe during the 12th century. A churn drill using steam power, based on \"the ancient Chinese method of lifting and dropping a rod tipped with a bit,\" was first built in 1835 by Isaac Singer in the United States, according to \"The History of Grinding\". In America, they were common in the Tri-State areas during the lead and zinc mining in Missouri, Oklahoma, and Kansas.\n\nThere is an example of one of these machines at the Northern Life Museum in Fort Smith, Northwest Territories, Canada. It was used in 1929-1930 at the Pine Point lead and zinc mine in the Northwest Territories.\n\n"}
{"id": "2133700", "url": "https://en.wikipedia.org/wiki?curid=2133700", "title": "Coulomb blockade", "text": "Coulomb blockade\n\nCoulomb blockade can be observed by making a device very small, like a quantum dot. When the device is small enough, electrons inside the device will create a strong Coulomb repulsion preventing other electrons to flow. Thus, the device will no longer follow Ohm's law and the current-voltage relation of the Coulomb blockade looks like a staircase.\n\nEven when Coulomb blockade can be used to demonstrate the quantization of the electric charge, it remains a classical effect and its main description does not require quantum mechanics. However, when few electrons are involved and an external static magnetic field is applied, Coulomb blockade provides the ground for a spin blockade (like Pauli spin blockade) and valley blockade, which include quantum mechanical effects due to spin and orbital interactions respectively between the electrons.\n\nThe devices can be composed of either metallic or superconducting electrodes. If the electrodes are superconducting, Cooper pairs (with a charge of minus two elementary charges formula_1) carry the current. In the case that the electrodes are metallic or \"normal-conducting\", i.e. neither superconducting nor semiconducting, electrons (with a charge of formula_2) carry the current.\n\nThe following section is for the case of tunnel junctions with an insulating barrier between two normal conducting electrodes (NIN junctions).\n\nThe tunnel junction is, in its simplest form, a thin insulating barrier between two conducting electrodes. According to the laws of classical electrodynamics, no current can flow through an insulating barrier. According to the laws of quantum mechanics, however, there is a nonvanishing (larger than zero) probability for an electron on one side of the barrier to reach the other side (see quantum tunnelling). When a bias voltage is applied, this means that there will be a current, and, neglecting additional effects, the tunnelling current will be proportional to the bias voltage. In electrical terms, the tunnel junction behaves as a resistor with a constant resistance, also known as an ohmic resistor. The resistance depends exponentially on the barrier thickness. Typically, the barrier thickness is on the order of one to several nanometers.\n\nAn arrangement of two conductors with an insulating layer in between not only has a resistance, but also a finite capacitance. The insulator is also called dielectric in this context, the tunnel junction behaves as a capacitor.\n\nDue to the discreteness of electrical charge, current through a tunnel junction is a series of events in which exactly one electron passes (\"tunnels\") through the tunnel barrier (we neglect cotunneling, in which two electrons tunnel simultaneously). The tunnel junction capacitor is charged with one elementary charge by the tunnelling electron, causing a voltage build up formula_3, where formula_4 is the capacitance of the junction. If the capacitance is very small, the voltage build up can be large enough to prevent another electron from tunnelling. The electric current is then suppressed at low bias voltages and the resistance of the device is no longer constant. The increase of the differential resistance around zero bias is called the Coulomb blockade.\n\nIn order for the Coulomb blockade to be observable, the temperature has to be low enough so that the characteristic charging energy (the energy that is required to charge the junction with one elementary charge) is larger than the thermal energy of the charge carriers. In the past, for capacitances above 1 femtofarad (10 farad), this implied that the temperature has to be below about 1 kelvin. This temperature range is routinely reached for example by 3He refrigerators. Thanks to small sized quantum dots of only few nanometers, Coulomb blockade has been observed next above liquid helium temperature, up to room temperature.\n\nTo make a tunnel junction in plate condenser geometry with a capacitance of 1 femtofarad, using an oxide layer of electric permittivity 10 and thickness one nanometer, one has to create electrodes with dimensions of approximately 100 by 100 nanometers. This range of dimensions is routinely reached for example by electron beam lithography and appropriate pattern transfer technologies, like the Niemeyer-Dolan technique, also known as shadow evaporation technique. The integration of quantum dot fabrication with standard industrial technology has been achieved for silicon. CMOS process for obtaining massive production of single electron quantum dot transistors with channel size down to 20 nm x 20 nm has been implemented.\n\nThe simplest device in which the effect of Coulomb blockade can be observed is the so-called single-electron transistor. It consists of two electrodes known as the \"drain\" and the \"source\", connected through tunnel junctions to one common electrode with a low self-capacitance, known as the \"island\". The electrical potential of the island can be tuned by a third electrode, known as the \"gate\", which is capacitively coupled to the island. \n\nIn the blocking state no accessible energy levels are within tunneling range of an electron (in red) on the source contact. All energy levels on the island electrode with lower energies are occupied.\n\nWhen a positive voltage is applied to the gate electrode the energy levels of the island electrode are lowered. The electron (green 1.) can tunnel onto the island (2.), occupying a previously vacant energy level. From there it can tunnel onto the drain electrode (3.) where it inelastically scatters and reaches the drain electrode Fermi level (4.).\n\nThe energy levels of the island electrode are evenly spaced with a separation of formula_5 This gives rise to a self-capacitance formula_4 of the island, defined as\nTo achieve the Coulomb blockade, three criteria have to be met:\n\nA typical Coulomb blockade thermometer (CBT) is made from an array of metallic islands, connected to each other through a thin insulating layer. A tunnel junction forms between the islands, and as voltage is applied, electrons may tunnel across this junction. The tunneling rates and hence the conductance vary according to the charging energy of the islands as well as the thermal energy of the system.\n\nCoulomb blockade thermometer is a primary thermometer based on electric conductance characteristics of tunnel junction arrays. The parameter V=5.439NkT/e, the full width at half\nminimum of the measured differential conductance dip over an array of N junctions together with the physical constants provide the absolute temperature.\n\n\"Main article: Ionic Coulomb blockade\"\n\nIonic Coulomb blockade (ICB) is the special case of CB, appearing in the electro-diffusive transport of charged ions through sub-nanometer artificial nanopores or biological ion channels. ICB is widely similar to its electronic counterpart in quantum dots, but presents some specific features defined by possibly different valence z of charge carriers (permeating ions vs electrons) and by the different origin of transport engine (classical electrodiffusion vs quantum tunnelling).\n\nIn the case of ICB, Coulomb gap formula_13 is defined by dielectric self-energy of incoming ion inside the pore/channel formula_14and hence formula_15 depends on ion valence \"z\". ICB appears strong formula_16, even at the room temperature, for ions with formula_17, e.g. for <chem>Ca^2+</chem> ions.\n\nICB has been recently experimentally observed in sub-nanometer <chem>MoS2</chem> pores.\n\nIn biological ion channels ICB typically manifests itself in such valence selectivity phenomena as formula_18 conduction bands (vs fixed charge formula_19) and concentration-dependent divalent blockade of sodium current.\n\n\n"}
{"id": "23899755", "url": "https://en.wikipedia.org/wiki?curid=23899755", "title": "Demeton", "text": "Demeton\n\nDemeton was a phosphorothioate insecticide with the chemical formula CHOPS. While it was previously used as an insecticide, it is now largely obsolete due to its relatively high toxicity to humans. The chemical structure of demeton is closely related to military nerve agents such as VX, and a derivative with one of the ethoxy groups replaced by methyl was investigated by both the US and Soviet chemical weapons programs under the names \"V.sub.X\" and \"GD-7\".\n\n"}
{"id": "24728239", "url": "https://en.wikipedia.org/wiki?curid=24728239", "title": "Doron Swade", "text": "Doron Swade\n\nDoron Swade MBE is a museum curator and author, specialising in the history of computing. He is especially known for his work on the computer pioneer Charles Babbage and his Difference Engine.\n\nSwade was originally from South Africa. He has studied electronics engineering, history, machine intelligence, philosophy of science and physics at a number of universities including the University of Cape Town, University of Cambridge, and University College London (UCL). He holds a BSc in physics and electronics engineering, an MSc in control engineering, and a PhD in the history of computing from UCL.\n\nHe has been a curator at the Science Museum in London, England, and the Computer History Museum in Silicon Valley, California, United States. At the Science Museum, he curated the computing and electronics collections and rose to be Assistant Director and Head of Collections. His major project at the museum was to organise the construction of Charles Babbage's Difference Engine, in collaboration with Dr Allan Bromley who studied Babbage's original drawings at the Science Museum.\n\nIn 1989, Swade was a co-founder of the Computer Conservation Society, a specialist group of the British Computer Society (BCS), with regular meetings at the Science Museum. He is a Fellow of the BCS and a Chartered Engineer.\n\nSwade is a Visiting Professor in the History of Computing at the University of Portsmouth, UK. He is also an Honorary Research Fellow in Computer Science at Royal Holloway, University of London.\n\nHe appeared in the \"In Our Time\" program on Ada Lovelace, a collaborator with Charles Babbage, broadcast on BBC Radio 4 in 2008.\n\nSwade was awarded an MBE for services to the history of computing in the UK New Year Honours 2009 list.\n\nSince 2010 he has been involved with the Plan 28 project to understand whether Babbage's Analytical Engine was a feasible computer based on Babbage's work, and to build a simulation. \nSwade has written the following books:\n\n\nCharles Babbage, 'Irascible Genius,' and the First Computer\nhttp://www.sigcis.org/?q=node/42\n\n"}
{"id": "39381659", "url": "https://en.wikipedia.org/wiki?curid=39381659", "title": "Executive Order 12999", "text": "Executive Order 12999\n\nExecutive Order 12999 is a United States Presidential Executive Order signed on April 17, 1996, by President Bill Clinton which\npermits U.S. federal agencies to transfer excess computers and related peripherals to educational and nonprofit 501(c)(3) organizations.\n\nThis order extended Executive Order 12821, which was signed by President George H.W. Bush on November 16, 1992.\n\nThe Executive Order has three core elements for the development of American education and educational organizations.\n\n"}
{"id": "20234388", "url": "https://en.wikipedia.org/wiki?curid=20234388", "title": "Fail-safes in nanotechnology", "text": "Fail-safes in nanotechnology\n\nFail-safes in nanotechnology are devices or features integrated with nanotechnology which, in the event of failure, respond in a way that will cause no harm, or at least a minimum of harm, to other devices or personnel. Fail-safe principles are governed by national standards and engineering practices, and are widely used in conventional engineering design. It is possible to scale down macro-scale fail-safe principles and devices for similar applications at the nano-scale. The use of fail-safes in nanotechnology applications supports social acceptance of those applications by reducing the risks to users; , there are both theoretical and practical ways to implement fail-safe designs in nanotechnology.\n\nA predominant challenge to the social acceptance of nanotechnology is concerned with the medical use of nanostructures in the human body. While any structure for medical use would be developed to be bio-compatible and harmless, sound engineering design must take into account all possibilities of failure. Thus, the design would include ways to manipulate the structures in the body in the event of failure.\nMany researchers are looking into creating nano-scale robots (“nanobots”), for the purpose of undertaking tasks where only robots on the nano scale can be used, such as inside the human body. These robots would have the ability to construct other nanostructures or perform medical procedures, and will be introduced into the body via an injection. The robots’ shells and circuits would be made of ferrous nanoparticles so that a magnetic field could be used to prevent or manipulate their movement. In case of failure or malfunction, a small EMP or an MRI could be used to deactivate the nanobots. Both techniques induce an electromagnetic field, corrupting the memory and shorting out the circuitry of any electronic device within range.\n\nResearchers are pursuing the building of nanostructures using amino acids. Nanostructures that are created using amino acids are constructed using only synthetic types of amino acids, which tags these structures with unique molecules. These engineered amino acids essentially form synthetic proteins that differ from the naturally occurring proteins in the human body. This difference in the engineered amino acids makes these proteins easy to isolate and target. In case of failure or malfunction, it is possible to identify these proteins using the specifically targeted molecules, which act as a flag to indicate the location of the target. Then, another mechanism would be used to isolate them and deactivate them.\n\nDNA within our bodies naturally breaks down, replicates itself, and rebuilds itself every time a cell divides. These processes are all controlled and completed by various enzymes. DNA molecules are composed of corresponding base pair nucleotides in a double-helix formation, which makes these processes very efficient, accurate, and predictable. Due to the ease with which DNA molecules can be fashioned, many publications in the academic society are geared towards creating nanostructures using DNA. With a DNA-based nano-device, synthetic proteins could be created, designed to deactivate a nano-device. These synthetic proteins would be injected into the body to break down the DNA and render a nano-device harmless in the event of a malfunction.\n\nBiological proteins within the human body serve three main functions: they are structural building blocks, enzymes, and facilitate cellular signaling. Synthetic proteins could be developed as a form of indicator and attached to a DNA-based nano-device. This indicator would then be used for the purpose of monitoring nano-devices in the human body. If all DNA-based nano-devices were closely monitored in the human body, they could be controlled quickly in the event of a malfunction.\n\nIn nanotechnology, particularly in nanobots, the need for a sound programming architecture is very important due to a potentially higher risk of damage in the event of a malfunction. A two-layer approach can be used to control nano-devices: (1) by providing a preprogrammed fail-safe functionality in case of anticipated failures; and (2) a remote-controlled override for use in unforeseen situations. The “remote”-controlled nano-device would require a specialist in the room, to guide the nanobot throughout the procedure.\nMany researchers are developing methods that use bacteria to deliver drugs. These bacteria can be “programmed” to perform a specific task, and can be directed to go to targeted locations in the body. However, the bacteria may damage healthy organs or fail to deliver the medicine to the sick organ in the case of a malfunction. In such cases, a fail-safe mechanism is required to neutralize the bacteria and prevent damage. An antibiotic is generally suitable as the fail-safe agent.\n"}
{"id": "29955122", "url": "https://en.wikipedia.org/wiki?curid=29955122", "title": "Fast-teks", "text": "Fast-teks\n\nFast-teks On-Site Computer Services is an information technologies based company founded in 2003 in Tampa, Florida. The company has since expanded to over 265 franchises in the United States and Canada. It specializes in on-site computer repairs, computer maintenance, and several general computer based support for consumers and businesses.\n\nFast-teks offers several services for business and residential customers. The company offers support for all major computer brands, internet installation, virus and spyware removal, wireless networking setup, data recovery and technology consulting. Fast-teks also has a specialized information technology management service for businesses. The service acts as an external IT department, handling all of the business' computer maintenance and technical support.\n\nFast-teks corporate office has registered partnerships with Dell Computers, AVG Technologies, CompUSA, and TigerDirect.\n\nFast-teks started franchising the company in 2004, only one year after starting the business. By 2006 the company had 57 franchises in the United States. In 2010, that number had more than quadrupled with over 250 franchises worldwide. Fast-teks was ranked #2 in the Tech Category and #186 overall in Entrepreneur's 2010 edition of the Franchise 500.\n\n"}
{"id": "28126621", "url": "https://en.wikipedia.org/wiki?curid=28126621", "title": "Fiama Di Wills", "text": "Fiama Di Wills\n\nFiama Di Wills is an Indian personal care brand that offers shampoos, conditioner, bathing bars and shower gels. It is owned by ITC Limited, an Indian conglomerate with a market capitalization of USD 40 billion and a turnover of USD 8 billion.\n\nLaunched on September 15, 2007, the brand was the second to roll out of ITC’s personal care stable.\n"}
{"id": "10792995", "url": "https://en.wikipedia.org/wiki?curid=10792995", "title": "Frequency synthesizer", "text": "Frequency synthesizer\n\nA frequency synthesizer is an electronic circuit that generates a range of frequencies from a single reference frequency. Frequency synthesizers are used in many modern devices such as radio receivers, televisions, mobile telephones, radiotelephones, walkie-talkies, CB radios, cable television converter boxes satellite receivers, and GPS systems. A frequency synthesizer may use the techniques of frequency multiplication, frequency division, direct digital synthesis, frequency mixing, and phase-locked loops to generate its frequencies. The stability and accuracy of the frequency synthesizer's output are related to the stability and accuracy of its reference frequency input. Consequently, synthesizers use stable and accurate reference frequencies, such as those provided by crystal oscillators.\n\nThree types of synthesizer can be distinguished. The first and second type are routinely found as stand-alone architecture: direct analog synthesis (also called a mix-filter-divide architecture as found in the 1960s HP 5100A) and the more modern direct digital synthesizer (DDS) (table-look-up). The third type are routinely used as communication system IC building-blocks: indirect digital (PLL) synthesizers including integer-N and fractional-N.\n\nIt is in some ways similar to a DDS, but it has architectural differences. One of its big advantages is to allow a much finer resolution than other types of synthesizers with a given reference frequency.\n\nPrior to widespread use of synthesizers, in order to pick up stations on different frequencies, radio and television receivers relied on manual tuning of a local oscillator, which used a resonant circuit composed of an inductor and capacitor, or sometimes resonant transmission lines; to determine the frequency. The receiver was adjusted to different frequencies by either a variable capacitor, or a switch which chose the proper tuned circuit for the desired channel, such as with the turret tuner commonly used in television receivers prior to the 1980s. However the resonant frequency of a tuned circuit is not very stable; variations in temperature and aging of components caused frequency drift, causing the receiver to drift off the station frequency. Automatic frequency control (AFC) solves some of the drift problem, but manual retuning was often necessary. Since transmitter frequencies are stabilized, an accurate source of fixed, stable frequencies in the receiver would solve the problem.\n\nQuartz crystal resonators are many orders of magnitude more stable than LC circuits and when used to control the frequency of the local oscillator offer adequate stability to keep a receiver in tune. However the resonant frequency of a crystal is determined by its dimensions and cannot be varied to tune the receiver to different frequencies. One solution is to employ many crystals, one for each frequency desired, and switch the correct one into the circuit. This \"brute force\" technique is practical when only a handful of frequencies are required, but quickly becomes costly and impractical in many applications. For example, the FM radio band in many countries supports 100 individual channel frequencies from about 88 MHz to 108 MHz; the ability to tune in each channel would require 100 crystals. Cable television can support even more frequencies or channels over a much wider band. A large number of crystals increases cost and requires greater space.\n\nThe solution to this was the development of circuits which could generate multiple frequencies from a \"reference frequency\" produced by a crystal oscillator. This is called a frequency synthesizer. The new \"synthesized\" frequencies would have the frequency stability of the master crystal oscillator, since they were derived from it. \n\nMany techniques have been devised over the years for synthesizing frequencies. Some approaches include phase locked loops, double mix, triple mix, harmonic, double mix divide, and direct digital synthesis (DDS). The choice of approach depends on several factors, such as cost, complexity, frequency step size, switching rate, phase noise, and spurious output.\n\nCoherent techniques generate frequencies derived from a single, stable master oscillator. In most applications, a crystal oscillator is common, but other resonators and frequency sources can be used. Incoherent techniques derive frequencies from a set of several stable oscillators. The vast majority of synthesizers in commercial applications use coherent techniques due to simplicity and low cost.\n\nSynthesizers used in commercial radio receivers are largely based on phase-locked loops or PLLs. Many types of frequency synthesizer are available as integrated circuits, reducing cost and size. High end receivers and electronic test equipment use more sophisticated techniques, often in combination.\n\nA well-thought-out \"design procedure\" is considered to be the first significant step to a successful synthesizer project. In the system design of a frequency synthesizer, states Manassewitsch, there are as many \"best\" design procedures as there are experienced synthesizer designers. System analysis of a frequency synthesizer involves output frequency range (or frequency bandwidth or tuning range), frequency increments (or resolution or frequency tuning), frequency stability (or phase stability, compare spurious outputs), phase noise performance (e.g., spectral purity), switching time (compare settling time and rise time), and size, power consumption, and cost. James A. Crawford says that these are mutually contradictive requirements.\n\nInfluential early books on frequency synthesis techniques include those by Floyd M. Gardner (his 1966 \"Phaselock techniques\") and by Venceslav F. Kroupa (his 1973 \"Frequency Synthesis\").\n\nMathematical techniques analogous to mechanical gear-ratio relationships can be employed in frequency synthesis when the frequency synthesis factor is a ratio of integers. This method allows for effective planning of distribution and suppression of spectral spurs.\n\nVariable-frequency synthesizers, including DDS, are routinely designed using Modulo-N arithmetic to represent phase.\n\nA phase locked loop is a feedback control system. It compares the phases of two input signals and produces an error signal that is proportional to the difference between their phases. The error signal is then low pass filtered and used to drive a voltage-controlled oscillator (VCO) which creates an output frequency. The output frequency is fed through a frequency divider back to the input of the system, producing a negative feedback loop. If the output frequency drifts, the phase error signal will increase, driving the frequency in the opposite direction so as to reduce the error. Thus the output is \"locked\" to the frequency at the other input. This other input is called the reference and is usually derived from a crystal oscillator, which is very stable in frequency. The block diagram below shows the basic elements and arrangement of a PLL based frequency synthesizer.\n\nThe key to the ability of a frequency synthesizer to generate multiple frequencies is the divider placed between the output and the feedback input. This is usually in the form of a digital counter, with the output signal acting as a clock signal. The counter is preset to some initial count value, and counts down at each cycle of the clock signal. When it reaches zero, the counter output changes state and the count value is reloaded. This circuit is straightforward to implement using flip-flops, and because it is digital in nature, is very easy to interface to other digital components or a microprocessor. This allows the frequency output by the synthesizer to be easily controlled by a digital system.\n\nSuppose the reference signal is 100 kHz, and the divider can be preset to any value between 1 and 100. The error signal produced by the comparator will only be zero when the output of the divider is also 100 kHz. For this to be the case, the VCO must run at a frequency which is 100 kHz x the divider count value. Thus it will produce an output of 100 kHz for a count of 1, 200 kHz for a count of 2, 1 MHz for a count of 10 and so on. Note that only whole multiples of the reference frequency can be obtained with the simplest integer N dividers. Fractional N dividers are readily available.\n\nIn practice this type of frequency synthesizer cannot operate over a very wide range of frequencies, because the comparator will have a limited bandwidth and may suffer from aliasing problems. This would lead to false locking situations, or an inability to lock at all. In addition, it is hard to make a high frequency VCO that operates over a very wide range. This is due to several factors, but the primary restriction is the limited capacitance range of varactor diodes. However, in most systems where a synthesizer is used, we are not after a huge range, but rather a finite number over some defined range, such as a number of radio channels in a specific band.\n\nMany radio applications require frequencies that are higher than can be directly input to the digital counter. To overcome this, the entire counter could be constructed using high-speed logic such as ECL, or more commonly, using a fast initial division stage called a \"prescaler\" which reduces the frequency to a manageable level. Since the prescaler is part of the overall division ratio, a fixed prescaler can cause problems designing a system with narrow channel spacings – typically encountered in radio applications. This can be overcome using a dual-modulus prescaler.\n\nFurther practical aspects concern the amount of time the system can switch from channel to channel, time to lock when first switched on, and how much noise there is in the output. All of these are a function of the \"loop filter\" of the system, which is a low-pass filter placed between the output of the frequency comparator and the input of the VCO. Usually the output of a frequency comparator is in the form of short error pulses, but the input of the VCO must be a smooth noise-free DC voltage. (Any noise on this signal naturally causes frequency modulation of the VCO.) Heavy filtering will make the VCO slow to respond to changes, causing drift and slow response time, but light filtering will produce noise and other problems with harmonics. Thus the design of the filter is critical to the performance of the system and in fact the main area that a designer will concentrate on when building a synthesizer system.\n\nMany PLL frequency synthesizers can also generate frequency modulation (FM). The modulating signal is added to the output of the loop filter, directly varying the frequency of the VCO and the synthesizer output. The modulation will also appear at the phase comparator output, reduced in amplitude by any frequency division. Any spectral components in the modulating signal too low to be blocked by the loop filter end up back at the VCO input with opposite polarity to the modulating signal, thus canceling them out. (The loop effectively sees these components as VCO noise to be tracked out.) Modulation components above the loop filter cutoff frequency cannot return to the VCO input so they remain in the VCO output.\n\nThis scheme therefore cannot directly handle low frequency (or DC) modulating signals but this is not a problem in the many AC-coupled video and audio FM transmitters that use this method. Such signals may also be placed on a subcarrier above the cutoff frequency of the PLL loop filter.\n\n\n\n\n"}
{"id": "4424638", "url": "https://en.wikipedia.org/wiki?curid=4424638", "title": "Gittings Studios", "text": "Gittings Studios\n\nGittings Studios is a photographic studio founded in 1928 by Paul Gittings, Sr.. Gittings bought the Bachrach Studios in the southern region of the United States during the Great Depression. The Gittings laboratory was a pioneer of color dye transfer prints. In the 1960s, the Gittings laboratories were the first to use machines from Kodak that would develop into the \"one hour processing.\" Due to this progress, the Gittings laboratory lost its almost-monopoly hold on the color film processing industry.\n\nPaul Linwood Gittings, Sr. was instrumental in organizing, funding and establishing the International Photography Hall of Fame and Museum in Oklahoma City, Oklahoma.\n\nIn the 1950s and 1960s, Mr. Gittings also operated a studio within Neiman Marcus.\n\nIn his autobiography entitled \"Color Portraiture,\" he writes, \"The photographer doesn't sell photographs; he sells sentiment and flattery. From the day that he [the photographer] understands the philosophy of the product he sells and bends his efforts to that end alone, he will prosper.\"\nand\n\"Finally, if I could choose those words of wisdom which have served me best, during a long photographic lifetime, I would mention first: 'The only difference between the difficult and the impossible is that the impossible takes more time.' Sometimes the hundredth attempt is the one that brings success.\"\nThe company has trained and inspired generations of portrait photographers.\nThe organization was sold in 1987 to Paul Skipworth, then again in 1998 to Greg Lorfing. Under Lorfing's direction, Gittings has worked in 15 countries in Europe, Asia and the Middle East, gaining worldwide recognition as a leading studio for legal portraiture.\n\n"}
{"id": "12860795", "url": "https://en.wikipedia.org/wiki?curid=12860795", "title": "Grease fitting", "text": "Grease fitting\n\nA grease fitting, grease nipple, Zerk fitting, or Alemite fitting is a metal fitting used in mechanical systems to feed lubricants, usually lubricating grease, into a bearing under moderate to high pressure using a grease gun. \n\nGrease fittings are permanently installed by either a (taper) thread or straight push-fit ('hammer in') arrangement, leaving a nipple connection that a grease gun attaches to. The pressure supplied by the grease gun forces a small captive bearing ball in the fitting to move back against the force of its retaining spring. The arrangement is thus essentially a valve that opens under pressure to allow lubricant to pass through a channel and be forced into the voids of the bearing. When the pressure ceases, the ball returns to its closed position. The ball excludes dirt and functions as a check valve to prevent grease escaping back out of the fitting. The ball is almost flush with the surface of the fitting so that it can be wiped clean to reduce the amount of debris carried with the grease into the bearing. \n\nThe common convex shape of the fitting allows the concave tip of the grease gun to seal against the fitting easily from many angles, yet with a sufficiently tight seal to force the pressured grease to move the ball and enter the fitting, rather than simply oozing past this temporary annular (ring-shaped) seal. \n\nA less common design has a few names including 'button head', 'threepenny bit', 'flat' and 'tat head'. These are a larger nipple head with a flat sealing surface which overhangs the body of the nipple allowing the grease gun end to be hooked onto the nipple with a sliding action which wipes the nipple surface as it's attached. The grease gun hose is usually at a sharp angle (e.g. 70-90°) to the axis of the nipple but can be approached from any azimuthal angle. It is not possible for grease pressure to force this type of connector away from the nipple.\n\nGrease fittings are commonly made from zinc-plated steel, stainless steel, or brass.\n\nThe patent for the Zerk fitting was granted to Oscar U. Zerk in January 1929, and the assignee was the Alemite Manufacturing Corporation (thus the eponymous names for the fittings). Alemite was already marketing ball check valves to accept grease supplied under pressure from a grease gun, such as for car and truck chassis lubrication points, both for OEM installations and for aftermarket upgrade kits which would screw in as replacements for stock grease cups. But Zerk's fitting was an improved style, less vulnerable to dirt and more forgiving of angled approach. Today, many companies make these grease fittings. \n\nBefore Zerk fittings existed, bearings were lubricated in various other ways that tended to be more maintenance-intensive and often provided less effective lubrication. For example, a typical machinery bearing of the 19th and early 20th centuries was a plain bearing with a cross-drilled hole to receive oil or grease, with no clever fitting at its mouth, or at best a cap or cup. Often lubricant was delivered under no more pressure than gravity or a finger push might provide. For example, oil was gravity-fed into the hole, or grease was pushed in. Grease guns to feed the grease with higher pressure existed, but their pre-Zerk fittings were not as good for making clean, sealed contact easily, and they were less widely used than today. \n\nThe oil hole or grease hole was usually covered with a cap of some kind to keep dirt out, from a simple plug or screw to a spring-loaded hinged cap. Sometimes a cup was mounted, acting as a small reservoir. The standard mode of maintenance was to have each machine's operator, or a dedicated oiler, go around adding a small squirt of oil or blob of grease to each and every hole on a frequent basis. \n\nTypical frequencies for oiling were one oil squirt (to every bearing) for each day of use, or in some cases each week. Grease is essentially oil held in a viscous gel or cream whose viscosity is used to hold the oil over time in places where oil alone cannot be supplied constantly enough, and would quickly drip away without the viscous suspension. This trait makes it better for bearings that cannot practically receive new oil on a near-constant basis. Thus a typical frequency for greasing might be monthly, yearly, or every several years. \n\nIf the lubrication schedule was faithfully and conscientiously adhered to, the lubrication quality could be very good. Large marine engines and stationary engines to power whole factories were likeliest to get such top-quality care. But locomotives, rail cars, agricultural implements, and automobiles were less certain of good care, and lubrication quality without constant lubing tended to be intermittent, from mediocre to bad. Attempts to improve on the simple oil hole concept included putting a small reservoir of oil above the hole, which slowly dripped oil into it. An example is the Lunkenheimer oiler in the nearby image. Such oilers often incorporated one or more of the following features: \n\nFor greased bearings, the low pressures of lubrication before the grease fitting could often result in a failure of the grease to travel all the way down into all the voids of the bearing. The grease fitting improved this penetration, yielding more effective lubrication.\n\nSince the 1920s, the ever-growing proliferation of sealed bearings throughout the manufacturing industries has made the use of grease fittings less common. Sealed bearings are lubricated for life at the factory, and are sealed such that the lubricant is not lost or dirtied. Grease fittings are far from obsolete, however, and much new machinery is built with them every year (e.g., tractors, lawnmowers, industrial plant equipment, and still a few car and truck parts). As long as maintenance is even minimally attended to (via occasional lube jobs where new grease is pumped into the bearing), this type of bearing and lubrication setup is cost-effective, simple, and long-lasting. \n\nNorton Commando motorcycles (1967-1975) had a 'grease' nipple fitted to the swing arm assembly but heavy gear box oil was stipulated by the factory. The subsequent confusion by owners often resulted in grease being used (causing rapid wear of the pivot bushes).\n"}
{"id": "15632708", "url": "https://en.wikipedia.org/wiki?curid=15632708", "title": "History of the roller coaster", "text": "History of the roller coaster\n\nRoller coaster amusement rides have origins back to ice slides constructed in 18th-century Russia. Early technology featured sleds or wheeled carts that were sent down hills of snow reinforced by wooden supports. The technology evolved in the 19th century to feature railroad track using wheeled cars that were securely locked to the track. Newer innovations emerged in the early 20th century with side friction and underfriction technologies to allow for greater speeds and sharper turns. By the mid-to-late 20th century, these elements intensified with the introduction of steel roller coaster designs and the ability for them to invert riders.\n\nThe world's oldest roller coasters descended from the \"Russian Mountains,\" which were specially constructed hills of snow located in the gardens of palaces around the Russian capital, Saint Petersburg, in the 18th century. This attraction was called a \"Katalnaya Gorka\" or \"sliding mountain\" in Russian. The slides were built to a height of between and , had a 50 degree drop, and were reinforced by wooden supports. Sometimes wheeled carts were used instead of sleds. These slides became popular with the Russian upper class, and with Catherine II of Russia herself, who had such mountains built in the gardens of the Oranienbaum Palace near St. Petersburg, with a pavilion next to it for drinking tea after the sliding. \"Russian mountains\" remains the term for roller coasters in many languages, such as Spanish (\"\"), Italian (\"\"), and French (\"\"). Ironically, the Russian term for roller coaster, \"\" (amerikanskie gorki), translates literally as \"American mountains.\"\nRussian soldiers occupying Paris from 1815 through 1816, after the defeat of Napoleon at Waterloo, may have introduced the Russian amusement of sledding down steep hills. In July 1817, a French banker named Nicolas Beaujon opened the Parc Beaujon, an amusement park on the Champs Elysees. Its most famous feature was the \"Promenades Aériennes\" or \"Aerial Strolls.\" It featured wheeled cars securely locked to the track, guide rails to keep them on course, and higher speeds. The three-wheel carts were towed to the top of a tower, and then released to descend two curving tracks on either side. King Louis XVIII of France came to see the park, but it is not recorded if he tried the ride. Before long there were seven similar rides in Paris: \"Les Montagnes françaises\" (The French Mountains), \"le Delta\", \"les Montagnes de Belleville\" (The Mountains of Belleville), \"les Montagnes américaines\" (the American Mountains), \"Les Montages lilliputiennes\", (The miniature mountains), \"Les Montagnes susses\" (The Swiss mountains) and \"Les Montagnes égyptiennes\" (The Egyptian mountains).\n\nIn the beginning, these attractions were primarily for the upper classes. In 1845 a new amusement park opened in Copenhagen, Tivoli, which was designed for the middle class. These new parks featured roller coasters as permanent attractions. The first permanent loop track was probably also built in Paris from an English design in 1846, with a single-person wheeled sled running through a 13-foot (4 m) diameter vertical loop. These early single loop designs were called Centrifugal Railways. In 1887, a French entrepreneur, Joseph Oller, the owner of the Moulin Rouge music hall, built \"Les Montagnes Russes à Belleville\" (\"The Russian Mountains of Belleville\") a permanent roller coaster with a length of two hundred meters in the form of a double-eight, later enlarged to four figure-eight-shaped loops.\n\nIn the 1850s, a mining company in Summit Hill, Pennsylvania, constructed the Mauch Chunk gravity railroad, a brakeman-controlled, 8.7-mile (14 km) downhill track used to deliver coal to Mauch Chunk (now known as Jim Thorpe), Pennsylvania. By 1872, the \"Gravity Road\" (as it became known) was selling rides to thrill seekers. Railway companies used similar tracks to provide amusement on days when ridership was low.\n\nUsing this idea as a basis, LaMarcus Adna Thompson began work on a gravity Switchback Railway that opened at Coney Island in Brooklyn, New York in 1884. Passengers climbed to the top of a platform and rode a bench-like car down the track up to the top of another tower where the vehicle was switched to a return track and the passengers took the return trip. This track design was soon replaced with an oval complete circuit. In 1885, Phillip Hinkle introduced the first complete-circuit coaster with a lift hill, the \"Gravity Pleasure Road\", which became the most popular attraction at Coney Island. Not to be outdone, in 1886 LaMarcus Adna Thompson patented his design for a roller coaster that included dark tunnels with painted scenery. \"Scenic Railways\" were soon found in amusement parks across the county, with Frederick Ingersoll's construction company building many of them in the first two decades of the 20th century.\n\nAs it grew in popularity, experimentation in coaster dynamics took off. In the 1880s the concept of a vertical loop was again explored by Lina Beecher, and in 1895 the concept came into fruition with the \"Flip Flap Railway\", located at Sea Lion Park in Brooklyn, and shortly afterward with \"Loop the Loop\" at Olentangy Park near Columbus, Ohio as well as similar coasters in Atlantic City and Coney Island. The rides were incredibly dangerous, and many passengers suffered whiplash. Both were soon dismantled, and looping coasters had to wait for over a half century before making a reappearance.\n\nBy 1919, the first underfriction roller coaster had been developed by John Miller. Soon, roller coasters spread to amusement parks all around the world. Perhaps the best known historical roller coaster, \"The Cyclone\", was opened at Coney Island in 1927. Like \"The Cyclone\", all early roller coasters were made of wood. Many old wooden roller coasters are still operational, at parks such as Kennywood near Pittsburgh, Pennsylvania and Pleasure Beach Blackpool, England. The oldest operating roller coaster is \"Leap-The-Dips\" at Lakemont Park in Pennsylvania, a side friction roller coaster built in 1902. The oldest wooden roller coaster in the United Kingdom is the \"Scenic Railway\" at Dreamland Amusement Park in Margate, Kent and features a system where the brakeman rides the car with wheels. It was severely damaged by fire on April 7, 2008, but was subsequently restored and reopened to the public in 2015. \"Scenic Railway\" at Melbourne's Luna Park built in 1912, is the world's oldest continually-operating roller coaster, and it also still features a system where the brakeman rides the car with wheels. One of only 13 remaining examples of John Miller's work worldwide is the wooden roller coaster at Lagoon in Utah. The coaster opened in 1921 and is the 6th oldest coaster in the world.\n\nThe Great Depression marked the end of the golden age of roller coasters, as amusement parks generally went into a decline that resulted in less demand for new coasters. This lasted until 1972, when \"The Racer\" opened at Kings Island amusement park located in what was then a part of Deerfield Township in Warren County, Ohio. Designed by John C. Allen, the instant success of \"The Racer\" helped to ignite a renaissance for roller coasters, reviving worldwide interest throughout the industry.\n\nIn 1959, the Disneyland theme park introduced a new design breakthrough in roller coasters with the \"Matterhorn Bobsleds\". This was the first roller coaster to use a tubular steel track. Unlike conventional wooden rails, which are generally formed using steel strips mounted on laminated wood, tubular steel can be bent in any direction, which allows designers to incorporate loops, corkscrews, and many other maneuvers into their designs. Most modern roller coasters are made of steel, although wooden roller coasters are still being built along with hybrids of steel and wood.\n\nIn 1975 the first modern-day roller coaster to perform an inverting element opened: Corkscrew, located at Knott's Berry Farm in Buena Park, California. In 1976 the vertical loop made a permanent comeback with the Great American Revolution at Magic Mountain in Valencia, California.\n\nThe roller coasters mentioned here are significant for their role in the amusement industry. They were notable for specific reasons, including:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "31633696", "url": "https://en.wikipedia.org/wiki?curid=31633696", "title": "IEC 60601", "text": "IEC 60601\n\nIEC 60601 is a series of technical standards for the safety and essential performance of medical electrical equipment, published by the International Electrotechnical Commission. First published in 1977 and regularly updated and restructured, as of 2011 it consists of a general standard, about 10 collateral standards, and about 60 particular standards.\n\nThe general standard \"IEC 60601-1 - Medical electrical equipment - Part 1: General requirements for basic safety and essential performance\" - gives general requirements of the series of standards. 60601 is a widely accepted benchmark for medical electrical equipment and compliance with IEC60601-1 has become a requirement for the commercialisation of electrical medical equipment in many countries. Many companies view compliance with IEC 60601-1 as a requirement for most markets. This standard does not assure effectiveness of a medical device. In the US, evidence of effectiveness is required by the FDA and confirmed through either a Premarket Approval (PMA) or similarity to a predicate device via a 510(k) Premarket Notification.\n\nNational deviations of this series of standards exist which include country specific requirements; see e.g. UL or AAMI for US specifics.\n\nThe European EN and Canadian CSA versions of the standard are identical to the IEC standard. \n\nIn 2005, the third edition of IEC 60601-1 was published. It was the result of a comprehensive review of the second edition (dating from 1988). Some key changes are: the outline and the numbering scheme of the clauses and subclauses were changed, risk management was made much more relevant and the concept of essential performance was added. Currently (2012), the applicability of the second and third edition is somewhat overlapping depending on the products under consideration and the country/area of application. IEC 60601-1-11 (2010) must now be incorporated into the design and verification of a wide range of home use and point of care medical devices along with other applicable standards in the IEC 60601 3rd edition series. IEC 60601-1 merged to medical device directive 93/42/EEC which covers all IEC standard of electromedical & electrical safety so it is clear that EC cover all Previous IEC standard to medical device directive 93/42/EEC\n\nThe mandatory date for implementation of the EN European version of the standard is June 1, 2012. The US FDA requires the use of the standard on June 30, 2013, while Health Canada recently extended the required date from June 2012 to April 2013. The North American agencies will only require these standards for new device submissions, while the EU will take the more severe approach of requiring all applicable devices being placed on the market to consider the home healthcare standard.\n\nRequirements of 60601-1 may be overridden or bypassed by specific language in the standards for a particular product.\nCollateral standards (numbered 60601-1-X) define the requirements for certain aspects of safety and performance, e.g. Electromagnetic Compatibility (IEC 60601-1-2) or Protection for diagnostic use of X-rays (IEC 60601-1-3).\nParticular standards (numbered 60601-2-X) define the requirements for specific products or specific measurements built into products, e.g. MR scanners (IEC 60601-2-33) or Electroencephalograms (IEC 60601-2-26).\nCollaterals and Particulars may have their own revisions which are different from the General Standard.\n\nA list of the collateral and particular standards currently in force follows: (last updated 15 September 2016)\nFor example, \"IEC 60601-1-9\" for Environmentally Conscious Design of Medical Electrical Equipment published July 2007 is a collateral standard to IEC 60601-1 and has been developed drawing on extensive practical experience at Philips Medical Systems and Siemens Medical Solutions. The Part 9 standard asks manufacturers of medical devices to consider the environmental impacts of their devices throughout the product's entire life cycle and to minimize these where possible. The standard also requires that the manufacturer provide information to the user on how to use the product in the most environmentally sensitive way.\nThe USA, Canada, Japan, Australia and New Zealand have not yet set transition dates for their national versions of this latest edition 60601-1, but the national versions published to date do contain the requirement to also conform with IEC 60601-1-9. However, the European version (EN 60601-1:2006) requires compliance with the new IEC 60601-1-9 collateral standard by September 2009.\n\nAccording to the recent publication of the US national version of the collateral standard for products intended for home use, ANSI/AAMI HA60601-1-11, the application of the standard does not apply to the nursing home environment. In the United States, nursing facilities are considered to be environments providing professional healthcare. The American version of this collateral standard also places greater emphasis on a requirement that states that “inspection of the usability engineering file reinforce that the usability engineering process is necessary for validation of the instructions for use.” Devices typically mandated to use the new standard include oxygen concentrators, body-worn nerve and muscle stimulators, beds, sleep apnea monitors, and associated battery chargers prescribed for use at home. Although In Vitro Diagnostic devices such as blood glucose meters are being used by patients at home, the standard does not apply, as these devices remain under the jurisdiction of the more lenient IEC 61010 series.\n\nThe 60601 certification process has been criticized for its complexity, cost, and the business risk it raises. This has been more particularly a concern during the transition to the third edition due to the indefinite adoption schedule of the new revision.\n\n\n"}
{"id": "2422506", "url": "https://en.wikipedia.org/wiki?curid=2422506", "title": "JAUS", "text": "JAUS\n\nJoint Architecture for Unmanned Systems (JAUS), formerly known as Joint Architecture for Unmanned Ground Systems (JAUGS), was originally an initiative started in 1998 by the United States Department of Defense to develop an open architecture for the domain of unmanned systems.\n\nIn order to ensure that the component architecture is applicable to the entire domain of current and future unmanned systems, it is built on five principles: vehicle platform independence, mission isolation, computer hardware independence, technology independence, and operator use independence.\n\nThe JAUS Reference Architecture, which is no longer being maintained, is a component based message passing architecture that defines a data format and methods of communication between computing nodes. The architecture dictates a hierarchical system built up of subsystems, nodes and components, and contains a strictly defined message set to support interoperability. Significant portions of the architecture, including the definitions for subsystem, node and component, have been loosely defined in order to accommodate for the five principles on which it is based.\n\nThe architecture has migrated from the JAUS Working Group, which was composed of individuals from the government, industry and academia, to the Society of Automotive Engineers, Aerospace Division, Avionics Systems Division. The AS4, Unmanned Systems Technical Committee now maintains and advances the set of standards. The following standards have been migrated from the JAUS Reference Architecture to a services based framework:\n\n\n\nOthers currently in draft include:\n\nAnother standard that evolved from the JAUS efforts is the “JAUS Service Interface Definition Language” or JSIDL. JSIDL standardizes the language for defining JAUS compliant interfaces. The specification is contained in the SAE document AS5684.\n\nJAUS was officially used by the United States Department of Defense in its UGV Interoperability Profile (IOP). The IOP specifies rules for the use of standard JAUS messages as well as custom extensions to the standard message set.\n\nJAUS Tool Set\n"}
{"id": "50042361", "url": "https://en.wikipedia.org/wiki?curid=50042361", "title": "Katie Moussouris", "text": "Katie Moussouris\n\nKatie Moussouris is an American computer security researcher who is best known for her ongoing work advocating responsible security research. She created the bug bounty program at Microsoft. Formerly the Chief Policy Officer at HackerOne, a vulnerability disclosure company based in San Francisco, California, she is the founder and CEO of Luta Security. Moussouris was directly involved in creating the U.S. Department of Defense's first bug bounty program for hackers. \n\nMoussouris joined Symantec in October 2004 when they acquired @stake. She founded and managed Symantec Vulnerability Research.\n\nIn May 2007, Moussouris left Symantec to join Microsoft as a security strategist. She founded the Microsoft Vulnerability Research (MSVR) program, announced at BlackHat 2008. The program has coordinated the response to several significant vulnerabilities, including Dan Kaminsky's DNS flaw, and has also actively looked for bugs in third-party software affecting Microsoft customers (subsequent examples of this include Google's Project Zero).\n\nFrom September 2010 until May 2014, Moussouris was the Senior Security Strategist Lead at Microsoft, where she ran the Security Community Outreach and Strategy team for Microsoft as part of the Microsoft Security Response Center (MSRC) team. She instigated the Microsoft BlueHat Prize for Advancement of Exploit Mitigations, which awarded over $260,000 in prizes to researchers at BlackHat USA 2012. The grand prize of $200,000 was at the time the largest cash payout being offered by a software vendor. She also created Microsoft's first bug bounty program, which paid over $253,000 and received 18 vulnerabilities over the course of her tenure.\n\nMoussouris has helped edit the ISO/IEC 29147 document since around 2008. In April 2016, ISO made the standard freely available at no charge after a request from Moussouris and the CERT Coordination Center's Art Manion.\n\nIn May 2014, Moussouris was named the Chief Policy Officer at HackerOne, a vulnerability disclosure company based in San Francisco, California. In this role, Moussouris was responsible for the company's vulnerability disclosure philosophy, and worked to promote and legitimize security research among organizations, legislators and policy makers.\n\nWhile still at Microsoft, Moussouris began discussing a bug bounty program with the federal government; she continued these talks when she moved to HackerOne. In March 2016, Moussouris was directly involved in creating the Department of Defense's \"Hack the Pentagon\" pilot program, organized and vetted by HackerOne. It was the first bug bounty program in the history of the US federal government. Moussouris followed up the Pentagon program with \"Hack the Air Force\". HackerOne and Luta Security are partnering to deliver up to 20 bug bounty challenges over three years to the Defense Department.\n\nIn April 2016, Moussouris founded Luta Security, a consultancy to help organizations and governments work collaboratively with hackers through bug bounty programs.\n\nDuring 2015-2016 and 2016-2017, Katie Moussouris served as a Cybersecurity Fellow at New America, a U.S. based think tank.\n\nIn 2014, SC Magazine named Moussouris to its Women in IT Security list. She was also named as one of \"10 Women in Information Security That Everyone Should Know,\" and the \"One To Watch\" among the 2011 Women of Influence awards. In 2018 she was featured among \"America's Top 50 Women In Tech\" by Forbes\n\n\nIn 2018, Moussouris testified in front of the U.S. Senate Subcommittee on Consumer Protection, Product Safety, Insurance, and Data Security about security research for defensive purposes.\n\n\nIn September 2015, Moussouris filed a discrimination class-action lawsuit against Microsoft in federal court in Seattle. She alleged that Microsoft hiring practices upheld a practice of sex discrimination against women in technical and engineering roles with respect to performance evaluations, pay, promotions, and other terms and conditions of employment.\n\n"}
{"id": "28443302", "url": "https://en.wikipedia.org/wiki?curid=28443302", "title": "Knowledge-based configuration", "text": "Knowledge-based configuration\n\nKnowledge-based configuration, or also referred to as product configuration or product customization, is an activity of customising a product to meet the needs of a particular customer. The product in question may consist of mechanical parts, services, and software. Knowledge-based configuration is a major application area for artificial intelligence (AI), and it is based on modelling of the configurations in a manner that allows the utilisation of AI techniques for searching for a valid configuration to meet the needs of a particular customer.\n\nKnowledge-based configuration (of complex products and services) has a long history as an artificial intelligence application area, see, e.g. Informally, configuration can be defined as a \"special case of design activity, where the artifact being configured is assembled from instances of a fixed set of well-defined component types which can be composed conforming to a set of constraints\". Such constraints are representing technical restrictions, restrictions related to economic aspects, and conditions related to production processes. The result of a configuration process is a product configuration (concrete configuration), i.e., a list of instances and in some cases also connections between these instances. Examples of such configurations are computers to be delivered or financial service portfolio offers (e.g., a combination of loan and corresponding risk insurance).\n\nConfiguration systems or also referred to as configurators or mass customization toolkits, are one of the most successfully applied Artificial Intelligence technologies. Examples are the automotive industry, the telecommunication industry, the computer industry, and power electric transformers. Starting with rule-based approaches such as R1/XCON, model-based representations of knowledge (in contrast to rule-based representations) have been developed which strictly separate product domain knowledge from the problem solving one - examples thereof are the constraint satisfaction problem, the Boolean satisfiability problem, and different answer set programming (ASP) representations. There are two commonly cited conceptualizations of configuration knowledge. The most important concepts in these are components, ports, resources and functions. This separation of product domain knowledge and problem solving knowledge increased the effectiveness of configuration application development and maintenance, since changes in the product domain knowledge do not affect search strategies and vice versa.\n\nConfigurators are also often considered as \"open innovation toolkits\", i.e., tools which support customers in the product identification phase. In this context customers are innovators who articulate their requirements leading to new innovative products. \"Mass Confusion\" – the overwhelming of customers by a large number of possible solution alternatives (choices) – is a phenomenon which often comes with the application of configuration technologies. This phenomenon motivated the creation of personalized configuration environments taking into account a customer’s knowledge and preferences.\n\nCore configuration, i.e., guiding the user and checking the consistency of user requirements with the knowledge base, solution presentation and translation of configuration results into bill of materials (BOM) are major tasks to be supported by a configurator. Configuration knowledge bases are often built using proprietary languages.\nIn most cases knowledge bases are developed by knowledge engineers who elicit product, marketing and sales knowledge from domain experts. Configuration knowledge bases are composed of a formal description of the structure of the product and further constraints restricting the possible feature and component combinations.\n\nRecently, knowledge based configuration has been extended to service and software configuration. Modeling software configuration has been based on two main approaches: feature modeling, and component-connectors. Kumbang domain ontology combines the previous approaches building on the tradition of knowledge based configuration.\n\n\n\n\n"}
{"id": "173629", "url": "https://en.wikipedia.org/wiki?curid=173629", "title": "Law &amp; Order", "text": "Law &amp; Order\n\nLaw & Order is an American police procedural and legal drama television series created by Dick Wolf, launching the \"Law & Order\" franchise. Airing its entire run on NBC, \"Law & Order\" premiered on September 13, 1990 and completed its twentieth and final season on May 24, 2010.\n\nSet and filmed in New York City, the series follows a two-part approach: the first half-hour is the investigation of a crime (usually murder) and apprehension of a suspect by New York City Police Department detectives; the second half is the prosecution of the defendant by the Manhattan District Attorney's Office. Plots are often based on real cases that recently made headlines, although the motivation for the crime and the perpetrator may be different.\n\nThe show has been noted for its revolving cast over the years. Among the longest-running main cast members were Steven Hill as District Attorney Adam Schiff (seasons 1–10), Jerry Orbach as Detective Lennie Briscoe (seasons 3–14), S. Epatha Merkerson as Lieutenant Anita Van Buren (seasons 4–20), Sam Waterston as Executive Assistant District Attorney Jack McCoy (seasons 5–20; later District Attorney) and Jesse L. Martin as Detective Ed Green (seasons 10–18).\n\n\"Law & Order\"s twenty seasons ties with \"Gunsmoke\" (1955–75) and spin-off \"\" (1999–present) for the longest-running live-action scripted American primetime series. The success of the series has led to the creation of additional shows, making \"Law & Order\" a franchise, with also a television film, several video games, and international adaptations of the series. It has won and has been nominated for numerous awards over the years, including a number of Emmy Awards.\n\nOn May 14, 2010, NBC announced that it had canceled \"Law & Order\" and would air its final episode on May 24, 2010. Immediately following the show's cancellation, Wolf attempted to find a new home for the series. Those attempts failed, and in July 2010, Wolf declared that the series had now \"moved to the history books\".\n\nIn 1988, Dick Wolf developed a concept for a new television series that would depict a relatively optimistic picture of the American criminal justice system. He initially toyed with the idea of calling it \"Night & Day\" but then hit upon the title \"Law & Order\". The first half of each episode would follow two detectives (a senior and a junior detective) and their commanding officer as they investigate a violent crime.\n\nThe second half of the episode would follow the District Attorney's Office and the courts as two prosecutors, with advice from the District Attorney himself, attempt to convict the accused. Through this, \"Law & Order\" would be able to investigate some of the larger issues of the day by focusing on stories that were based on real cases making headlines.\n\nWolf took the idea to then-president of Universal Television Kerry McCluggage, who pointed out the similarity to a 1963 series titled \"Arrest and Trial\", which lasted one season. The two watched the pilot of that series, in which a police officer (Ben Gazzara) arrested a man for armed robbery in the first half, and the defense attorney, played by Chuck Connors gets the perpetrator off as the wrong guy in the second half; this was the formula of the show every week.\n\nWolf decided that, while his detectives would occasionally also be fallible, he wanted a fresh approach to the genre, to go from police procedural to prosecution with a greater degree of realism. In addition, the prosecution would be the hero, a reversal of the usual formula in lawyer dramas.\n\nInitially, Fox ordered thirteen episodes based on the concept alone, with no pilot. Then-network head Barry Diller reversed the decision. Although he loved the idea, he didn't believe it was a \"Fox show\". Wolf then went to CBS, which ordered a pilot, \"Everybody's Favorite Bagman\", written by Wolf about corrupt city officials involved with the mob. The network liked the pilot but did not order it because there were no breakout stars.\n\nIn the summer of 1989, NBC's top executives, Brandon Tartikoff and Warren Littlefield, screened the pilot and liked it; but they were concerned the intensity of the series could not be repeated week after week. However, by 1990, NBC executives had enough confidence that the innovative show could appeal to a wide audience that they ordered the series for a full season.\n\nThe series was shot on location in New York City and is known for its extensive use of local color. In later seasons, New York City mayors Rudy Giuliani and Michael Bloomberg, attorney William Kunstler and Bronx Congressman José Serrano all appeared on the show as themselves.\n\nLocal personalities also had recurring cameos as fictional characters, such as Donna Hanover and Fran Lebowitz as judges. On September 14, 2004, in New York City, a road leading to Pier 62 at Chelsea Piers (where the series was mostly shot) was renamed \"Law & Order Way\" in tribute to the series.\n\nThe music for \"Law & Order\" was composed by veteran composer Mike Post, and was deliberately designed to be minimal to match the abbreviated style of the series. Post wrote the theme song using electric piano, guitar, and clarinet. In addition, scene changes were accompanied by a tone generated by Post. He refers to the tone as \"The Clang\", while \"Entertainment Weekly\" critic Ken Tucker has referred to the sound as the \"ominous \"chung CHUNG\"\", actor Dann Florek (in a promo) as the \"doink doink\", and Richard Belzer as \"the Dick Wolf Cash Register Sound.\"\n\nThe tone moves the viewer from scene to scene, jumping forward in time with all the importance and immediacy of a judge's gavel – which is exactly what Post was aiming for when he created it. \"The Clang\" is an amalgamation of nearly a dozen sounds, including an actual gavel, a jail door slamming, and five hundred Japanese monks walking across a hardwood floor. The sound has become so associated with the \"Law & Order\" brand that it was also carried over to other series of the franchise.\n\nThe UK-aired Channel Five versions of seasons 7–16 of \"Law & Order\" and Seasons 1–9 of \"\" feature the song \"I'm Not Driving Anymore\" by Rob Dougan in the opening credits with \"Urban Warfare\" by Paul Dinletir being used for Seasons 10–11 after that Seasons 16–20 of Law & Order and Seasons 12+ of SVU used the US theme. Another Rob Dougan track, \"There's Only Me\", was used as the theme for seasons 1–6 of \"\", with Urban Warfare again being used for Seasons 7–8 and the US theme being used for Seasons 9–10.\n\nFor the 1988 pilot, George Dzundza and Chris Noth were cast as the original detectives, Sergeant Max Greevey and Detective Mike Logan. The producers felt that Dzundza would be a perfect senior police officer as he was someone the producers felt they could see themselves riding along with in a police cruiser. Noth and Michael Madsen were candidates for the role of Logan. Madsen initially was considered the perfect choice for the role, but, in a final reading, it was felt that Madsen's acting mannerisms were repetitive, and Noth received the role instead. Rounding out the police cast, Dann Florek was cast as Captain Donald Cragen.\n\nOn the prosecutor's side, Michael Moriarty was Dick Wolf's choice to play Executive Assistant District Attorney Benjamin \"Ben\" Stone. The network, however, preferred James Naughton, but, in the end, Wolf's choice would prevail, and Moriarty received the role. As his ADA, Richard Brooks and Eriq La Salle were being considered for the role of Paul Robinette. The network favored La Salle but, once again, the producers' choice prevailed, and Brooks received the role. As their boss, Roy Thinnes was cast as District Attorney Alfred Wentworth.\n\nNearly two years passed between the pilot and production of the series. The producers held options on Dzundza, Noth, Moriarty and Brooks. Each was paid holding money for the additional year and brought back. Florek also returned. Thinnes, however, was starring in \"Dark Shadows\" and declined to return. In his place, the producers tapped Steven Hill to play District Attorney Adam Schiff, a character loosely based on real-life New York County District Attorney Robert Morgenthau. Hill brought prestige and experience to the show and, as such, the producers allowed Hill to give insight on the direction he thought the character should go.\n\nDzundza was disappointed when he realized that the show would be more of an ensemble show rather than a show starring him. Though the cast liked his performance, they increasingly felt uncomfortable around Dzundza, who was also under stress due to the constant commute between New York City and his home in Los Angeles. Dzundza quit after only one season on the show, and Sergeant Greevey was written off as being killed in the line of duty.\n\nHe was replaced by Paul Sorvino as Sergeant Phil Cerreta, who was considered more even tempered than either Max Greevey or Mike Logan. Sorvino was initially excited about the role, but would leave midway through the next season, citing the exhausting schedule demanded by the filming of the show, a need to broaden his horizons, and the desire to preserve his vocal cords for singing opera as reasons for leaving the show. Sergeant Cerreta was written off as having been shot in the line of duty and transferring to a desk job at another precinct.\n\nTo replace Sorvino on the series, Wolf cast Jerry Orbach (who had previously guest starred as a defense attorney in the Season 2 episode \"The Wages of Love\") in the role of Detective Leonard W. \"Lennie\" Briscoe. Orbach's characterization of the world-weary, wisecracking Detective Briscoe was based on a similar NYPD character he portrayed in the 1981 film \"Prince Of The City,\" which Wolf had personally requested Orbach to replicate for the show.\n\nIntroduced on a recurring basis during Season 2 was Carolyn McCormick as Dr. Elizabeth Olivet, a police psychologist brought in on a case-by-case basis. NBC had been pushing for the producers to add female characters to the all-male cast. She was added to the opening credits as \"also starring\" in Season 3 and 4 but, despite the attempts of the producers to include her in as many episodes as possible, it was found to be difficult to incorporate her into the show due to the format leaning heavily on the police and prosecutors. She was removed from the credits in Season 5.\n\nMcCormick stayed with the show on a recurring basis, but believed that the character had become less profound and complex, and that her role had been reduced mostly to \"psychobabble.\" She left to star on \"Cracker\" after Season 7. After the cancellation of \"Cracker,\" she returned beginning in Season 13 and appeared occasionally until Season 20.\n\nBy the end of Season 3, NBC executives still felt the show did not have enough female characters. On the orders of then-network president Warren Littlefield, new female characters had to be added to the cast or the show would face possible cancellation on its relegated Friday night time slot. Wolf realized that, since there were only six characters on the show, someone had to be dismissed. He chose to dismiss Florek and Brooks from the regular roster, and later said it was the hardest two phone calls he had ever made. Though producers initially claimed the firings, especially that of Brooks, who was said not to get along with Moriarty, were for other reasons, Wolf confirmed that the firings were on the orders of Littlefield.\n\nTo replace Florek, S. Epatha Merkerson was cast as new squad leader Lieutenant Anita Van Buren. (Merkerson had previously guest starred as a mother of a gunshot victim in the Season 1 episode \"Mushrooms.\") To replace Brooks, Jill Hennessy was cast as Assistant District Attorney Claire Kincaid. Though no initial explanation was given on the show for the departures of Florek's or Brooks's characters, they would both later return in guest appearances, with Captain Cragen having been reassigned to the Internal Affairs Bureau and ADA Robinette having become a defense attorney. Florek also returned to direct a few episodes, and his character was eventually added to the cast of \"\".\n\nMeanwhile, Moriarty's behavior both on and off the set became problematic for Wolf. After a public statement in which Moriarty called Attorney General Janet Reno a \"psychopathic Nazi\" for her efforts to censor television violence, Moriarty engaged in a verbal confrontation with Reno at a dinner in Washington, D.C. Wolf asked Moriarty to tone down his comments, and Moriarty responded by quitting the show the next week. The final storyline for Ben Stone involved him resigning over guilt after a woman he compelled to testify against a Russian mobster was murdered by his cohorts. To replace Moriarty, Sam Waterston was Wolf's first choice for the role of Executive Assistant District Attorney John J. \"Jack\" McCoy Jr.; Waterston's character was markedly different from Moriarty's in that Jack McCoy was conceived as more emotionally stable and having more sex appeal.\n\nWolf dismissed Noth when his contract expired at the end of Season 5, because he felt that Lennie Briscoe and Mike Logan had become too similar to each other and the writers were having difficulty in writing their dialogue together. Furthermore, Noth had been disgruntled with the show since the dismissals of Florek and Brooks, and remained embittered against Wolf, who he felt was not a friend to his actors. The final story line for Detective Logan involved him being banished to work on Staten Island in a domestic violence crimes unit as punishment for punching a city council member who had orchestrated the murder of a gay colleague and had managed to get acquitted of the charges. (The made-for-television film \",\" in which Noth starred, centered around Logan attempting to get back into the department's good graces.) Noth was replaced by Benjamin Bratt as Detective Reynaldo \"Rey\" Curtis, who was hired in an attempt to find an actor even sexier than Noth to join the cast.\n\nHennessy chose not to renew her three-year contract at the end of Season 6 to pursue other projects, and Claire Kincaid was written off as being killed in a drunk driving accident. She was replaced by Carey Lowell as Assistant District Attorney Jamie Ross. Lowell remained with the show until the end of Season 8, when she left to spend more time with her daughter. (Jamie Ross was written off as leaving the D.A.'s office for similar reasons.) Lowell (who later returned for a couple guest appearances) was replaced by Angie Harmon as Assistant District Attorney Abigail \"Abbie\" Carmichael, who was conceived as being much louder and outspoken than any of her predecessors. Harmon auditioned with 85 other women, including Vanessa Williams, for the role, and was picked after Wolf heard her Texas accent.\n\nBeginning in Season 8 (1997), J. K. Simmons had the recurring role of Dr. Emil Skoda, a psychiatrist who worked with the Police Department.. He appeared in 41 episodes until 2004. He then reappeared for three episodes in the final season.\n\nBratt left the series at the end of Season 9, stating it was an amicable departure and he expected to eventually return for guest appearances. (He ultimately returned for the Season 20 episode \"Fed.\") Detective Curtis was written off as leaving the force in order to take care of his wife, who was suffering from multiple sclerosis, in her final days. He was replaced by Jesse L. Martin as Detective Ed Green, who was conceived of as more of a loose cannon in the mold of Mike Logan than Rey Curtis was. (Briscoe was described as being a recovering alcoholic, as Cragen had been; Green was described as being a recovering compulsive gambler.) In 2000, Hill announced he was leaving the series after Season 10. Hill, who was the last remaining member of the original cast, said his departure was mutual with the producers. He was replaced by Dianne Wiest as Interim District Attorney Nora Lewin, and Adam Schiff was written out off-screen as departing to work with Jewish charities and human-rights organizations in Europe.\n\nThe following year, Harmon left the show after three seasons (with Abbie Carmichael written off as being called on to serve the U.S. Attorney's office) and was replaced by Elisabeth Röhm as Assistant District Attorney Serena Southerlyn. The year after that, Wiest left the show after two seasons and was replaced by retiring U.S. Senator Fred Thompson as District Attorney Arthur Branch, whose character was conceived of as being much more right-leaning than his predecessors in the DA's office, and was a direct reaction to the September 11 attacks. No mention was made on the show of what happened to Nora Lewin, though producers said her character was only supposed to be an interim DA.\n\nAfter 12 years on \"Law & Order,\" Orbach announced in March 2004 that he was leaving the show at the end of Season 14 for the spin-off \".\" Lennie Briscoe was written off as retiring from the NYPD and later taking a position as an investigator for the DA's office. He was replaced at the 27th Precinct by Detective Joe Fontana, played by Dennis Farina. At the time, Orbach would not state the reason for his departure, but it was eventually revealed that he had been battling prostate cancer (for over 10 years) and that his role on \"Trial by Jury\" was designed to be less taxing on him than his role on the original series was. However, Orbach died from his cancer on December 28, 2004 and was featured in only the first two episodes of \"Trial by Jury.\" (His character was subsequently written off as having also died off-screen, though this was not revealed on the original series until the Season 18 episode \"Burn Card.\")\n\nSeason 15 would see the departure of Röhm mid-season. Röhm's final scene on the show, in the episode \"Ain't No Love\", sparked controversy within the fanbase, as ADA Southerlyn asked Arthur Branch if she was being fired because she was a lesbian, a fact the scripts had never even hinted at until then. Wolf said Röhm's departure was unexpected, and she exited the show in January 2005. Her replacement was Annie Parisse as Assistant District Attorney Alexandra Borgia. For a few seasons, she had often argued opposing points to McCoy and Branch, and he thought she would be better as a defender rather than a prosecutor.\n\nLater that season, Martin departed early to film \"Rent.\" Ed Green was temporarily written off as being shot in the line of duty and being replaced during his recovery by Detective Nick Falco, played by Michael Imperioli, who had previously guest starred as a murder suspect in the Season 6 episode \"Atonement.\" Parisse left the series at the end of Season 16 (with ADA Borgia written off as being murdered), and Farina announced shortly afterward that he too was leaving \"Law & Order\" to pursue other projects. (Detective Fontana was written off as having retired off-screen.)\n\nBy this point, NBC executives believed the series was beginning to show its age, as the ratings had been declining since Orbach's departure. Farina had never been popular with fans when he replaced Orbach, and it was felt that the cast just did not seem to mesh well together anymore. In an effort to revitalize the show, Wolf replaced Parisse with Alana de la Garza as Assistant District Attorney Consuela \"Connie\" Rubirosa, while Martin's character was promoted to senior detective and partnered with Detective Nina Cassady, played by Milena Govich, who had worked with Wolf on the short-lived series \"Conviction\" and served as the show's first female detective of the main cast.\n\nHowever, Govich proved to be even more unpopular with fans than her predecessor was, and she left the show after one season, with the explanation being that Detective Cassady's assignment to the precinct had been temporary and had been transferred out. She was replaced by Jeremy Sisto, who had previously guest starred as a defense attorney in the Season 17 episode \"The Family Hour\", as Detective Cyrus Lupo. Around the same time, Thompson announced he would leave the show to seek the 2008 Republican presidential nomination. (No explanation was given within the show regarding Arthur Branch's off-screen departure.) Waterston's character was promoted to Interim District Attorney (later made full District Attorney in Season 20) and his former position was filled in by Executive Assistant District Attorney Michael Cutter, played by Linus Roache.\n\nMartin later announced that he would leave the show for the second and last time near the end of Season 18 to pursue other endeavors, and Detective Green was written off as resigning from the force due to burnout. He was replaced by Anthony Anderson as Detective Kevin Bernard. In 2010, Merkerson announced that she would leave the show at the end of Season 20, with Lieutenant Van Buren given a season-long story arc involving her battling cervical cancer. However, the cancellation of the show rendered this moot.\n\n\"Law & Order\" episodes are typically segmented into two parts, roughly at the halfway point; the first part follows police and detective work, and the second follows the legal and courtroom proceedings of the case. The show dwells little on the characters' back-stories or social lives, focusing mainly on their lives at work.\n\nFor most of \"Law & Order\"s run, the cold open or lead-in of the show began with the discovery of a crime, usually a murder.\nThe scene typically began with a slice of everyday life in New York City. Some civilians would then discover the crime victim, or sometimes the crime would occur in a public place and they would be witnesses or a victim of a crime. The only exceptions to this are in the early seasons, mostly Seasons 1 & 2, the crime would usually be discovered by a pair of uniformed officers on patrol, or in later seasons when the cold open was replaced with rapid cuts of the victim's final moments, similar to \"\".\n\nThe police are represented in the show by the New York City Police Department 27th Precinct homicide department. In the show it is common that the detectives also investigate cases other than homicide or attempted homicides, like kidnappings and rape, the latter especially in the first nine seasons of the show, before \"\" premiered. However, in the real world these cases are handled by other units and divisions.\n\nThe viewers are introduced to two homicide detectives, a senior detective (usually a veteran cop) and a junior detective (usually a young but capable detective), who report directly to their boss at their precinct (either a Lieutenant or a Captain). During the preliminary crime scene examination, the featured detectives make their first observations and will come up with theories followed by a witticism or two, before the title sequence begins.\n\nThe detectives often have few or no good clues—they might not even know the victim's identity—and must usually chase several dead ends before finding a likely suspect(s).\n\nThey start their investigations at the crime scene by talking to any witnesses while the Crime Scene Unit (CSU) technicians start processing the scene by collecting forensic evidence. The medical examiner (M.E.)'s office will also be shown to collect the body from the crime scene.\n\nLater the medical examiner will perform an autopsy on the victim, offering more clues to the victim's cause and time of death (sometimes obtaining the victim's identity from dental records or fingerprints) which the detectives will read about in the M.E.'s autopsy report and by talking to the M.E. who performed it. When the detectives know the victim's identity they will inform their relatives or loved ones of their death and attempt to get more information on the victim's life and possible suspects.\n\nThe detectives continue their investigation by interviewing witnesses and possible suspects, all the while tracing the victim's last known movements and victim's state of mind (by talking to the victim's family, friends and co-workers). Sometimes they will have someone they suspect of the crime and will trace their last known movements and state of mind by talking to the people in the suspect's life, until they are either ruled out or dead certain of their guilt. They also visit the crime laboratory to submit and view evidence (e.g. fingerprints, DNA and ballistics, etc.), they may also look into any background information such as financial details and criminal history on both the victim and lead suspect. In some instances, psychologists and/or psychiatrists are called in for insight into the criminal's behavior or \"modus operandi\". All the while, the detectives report to their commanding officer, keeping them informed and being advised on how best to proceed next.\n\nWhen the detectives are certain they have the right suspect(s), the police will take the case to their boss, who decides if there is enough for a search and/or arrest warrant (though sometimes the commanding officer will consult with the New York City District Attorney's office to see if the case is strong enough) and whether or not any backup (such as uniformed officers or an armed tactical team) is needed. The detectives will then arrest the suspects(s), with the police sometimes having to chase the accused through the streets of New York. The scene then shifts to the interrogation room where the detectives interrogate the suspect(s) until they ask for a lawyer, their defense attorney shows up and asks the suspect not to talk anymore, or the Assistant District Attorney from the D.A.'s office decides they have enough to press charges.\n\nTowards the middle of a show, the police will begin to work with the prosecutors to make the arrest, though sometimes the Assistant District Attorney will appear earlier to arrange a plea-for-information deal or to decide if the detectives have enough evidence for search or arrest warrants before arresting the suspects. An arraignment court scene will follow, in which the defendants plead (usually not guilty) and bail conditions are set.\n\nThe matter is then taken over by a pair of representatives from the New York County District Attorney's Office, an Executive Assistant District Attorney and an Assistant District Attorney. They discuss deals, prepare the witnesses and evidence, and conduct the People's case in the trial. The District Attorneys work together and with the Medical Examiner's office, the crime laboratory (including fingerprint analysts, DNA profilers and ballistics analysts), and psychologists or psychiatrists (if the defendant uses an insanity plea), all of whom may be needed to testify in court for the prosecution. The police may also reappear to testify in court or to arrest another suspect, but most investigation in the second segment is done by the D.A.'s office, in consultation with the District Attorney for advice on the case, as the D.A., being an elected official, sometimes brings political considerations to bear concerning decisions to prosecute various alleged offenders. If the case is very weak then the police would re-investigate.\n\nUnlike many other legal dramas (e.g., \"The Defenders\", \"Matlock\", \"Perry Mason\" and \"L.A. Law\"), the court proceedings are shown from the prosecution's point of view, with the regular characters trying to prove the defendant's guilt, not innocence. After the arraignment of defendants, the D.A.s proceed to trial preparation, including legal research and plea negotiations. Some episodes include legal proceedings beyond the testimony of witnesses, including motion hearings, (often concerning admissibility of evidence); jury selection; and allocutions, usually as a result of plea bargains. Many episodes employ motions to suppress evidence as a plot device, and most of these end with evidence or statements being suppressed, often on a technicality. This usually begins with the service of the motion to the D.A. team, follows with argument and case citations of precedent before a judge in court, and concludes with visual reaction of the winning or losing attorney.\n\nMany episodes use outlandish defense scenarios such as diminished responsibility (e.g. \"Genetics\"/\"Television\"/\"God\"/\"the devil made me do it\" and intoxication defence) and temporary insanity (e.g. \"Black Rage\"/\"White Rage\"/\"Sports Rage\"). Some episodes revolve around moral and ethical debates including the right to die (euthanasia), the right to life (abortion), and the right to bear arms (gun control). Episodes usually end with the verdict being read by the jury foreperson and a shot of both the winning and losing parties. The scene then shifts to the District Attorney's office, where the team is leaving the office to go home while contemplating either the true guilt of the accused, the defense scenarios that were used, or the moral or ethical issue that was central to the episode.\n\nOften the plot of an initial portion of an episode resembles a recognizable aspect of an actual case. In early seasons, the details of these cases often closely followed the real stories, such as the season one episode \"Subterranean Homeboy Blues\", which had a woman shooting two attempted muggers, paralleling the Bernhard Goetz case. Another early episode, \"Out of the Half-Light\", focused on a racially charged rape case that mimicked the Tawana Brawley case. This \"ripped from the headlines\" style is reflected in the opening credits sequence that evolves from newspaper halftones to high-resolution photos. Another first-season episode, \"Poison Ivy\", was based on the Edmund Perry case where an NYPD officer fatally shot a black honor student who was committing a crime in front of the officer upon returning to the city after recently graduating from an Ivy League prep school. Later seasons would take real-life cases as inspiration but diverge more from the facts. Often this would be done by increasing the severity of the crime in question, usually by adding a murder. As a result, the plot would tend to veer significantly from the actual events that may have inspired the episode. Promotional advertisements of episodes with close real-life case parallels regularly use the \"ripped from the headlines\" phrase, although a textual disclaimer, within the actual episode, emphasizes that the story and characters are fictional. This format lends itself to exploring different outcomes or motives that similar events could have had under other circumstances.\n\nSome real-life crime victims have felt used and exploited, with one lawyer, Ravi Batra, going so far as to sue the show in 2004 for libel.\n\n\"Law & Order\" premiered September 13, 1990, and aired on NBC, with 456 episodes having been produced.\n\nThe show premiered September 13, 1990, and ended on May 24, 2010. 456 episodes were aired and produced. The show ran for twenty seasons on NBC. It was NBC's longest running crime drama, and tied for longest running primetime scripted drama with \"Gunsmoke\". The first two seasons were broadcast Tuesdays at 10 p.m. From season 3 through 16 the show aired Wednesday at 10 p.m. For season 17 it moved to Fridays at 10 p.m. For seasons 18 and 19 the show shifted back to Wednesdays at 10 p.m. For season 20 the show was broadcast Fridays at 8 p.m., while in the spring it moved to Mondays at 10 p.m., where it broadcast its series finale on May 24, 2010.\n\nRepeats of \"Law & Order\" were first broadcast weekdays on \"A&E\" beginning in the mid 1990s and are credited with drawing a new audience to the current weekly NBC episodes.\nAs of January 1, 2014, the series is being telecast on SundanceTV, TNT, WE tv, and WGN America.\n\nOn May 14, 2010, NBC officially canceled \"Law & Order\", opting instead to pick up \"Law & Order: Los Angeles\" for a first season and renew \"\" for a twelfth. Creator Dick Wolf continued to pressure the series' producer NBC Universal to make a deal with TNT, which held syndication rights to the show, for a twenty-first season if an acceptable license fee could be bargained. Talks between the two started up after upfronts. However, TNT said in a statement it was not interested in picking the show up for a new season.\n\nAfter TNT discussions fell through, cable network AMC also considered reviving \"Law & Order\"; however, attempts to revive it failed, and according to creator Dick Wolf, the series \"moved into the history books\".\n\nIn February 2015, NBC considered bringing the series back for a 10-episode limited series.\n\nThe longevity and success of \"Law & Order\" has spawned five American television series (\"\", \"\", \"\", \"\", and \"Law & Order True Crime\"), as well as a television film (\"\"), all of which use the name \"Law & Order\". Although there were fears initially that the failure of such shows could hurt the original series, it was felt the brand name was needed because of the commercial desirability such a brand name creates. To differentiate it from other series in the franchise, \"Law & Order\" is often referred to as \"The Mother Ship\" by producers and critics.\n\nThe series (and its spin-offs) shared a universe with the series \"\", with the two sharing several crossover episodes.\n\nThe original series has also been adapted for British television as \"\", with the setting changed to London. Similarly, \"Law & Order: Criminal Intent\" has been adapted for French and Russian television under the respective titles \"Paris enquêtes criminelles\" and \"\", and \"Law & Order: Special Victims Unit\" has also had a Russian version, \"\".\n\n\"Law & Order\" has been nominated for numerous awards in the television industry over the span of its run. Among its wins are the 1997 Emmy Award for Outstanding Drama Series, Screen Actors Guild Awards for Outstanding Male Actor in a Drama Series for Sam Waterston in 1999 and Jerry Orbach in 2005 (awarded after his death), and numerous Edgar Awards for Best Episode in a Television Series Teleplay.\n\nIn 2002, \"Law & Order\" was ranked No. 24 on TV Guide's 50 Greatest TV Shows of All Time. The show also placed No. 27 on \"Entertainment Weekly\" \"New TV Classics\" list.\n\nIn 2013, \"TV Guide\" ranked \"Law & Order\" #14 on their list of the 60 Greatest Shows of All Time.\n\nUniversal Studios Home Entertainment has released fourteen seasons on DVD in Region 1, along with the complete series. Law & Order: The Complete Series boxed set features all 20 seasons. Each season is individually packaged (in tray-stack style), with all new cover-art (including new cover art for the seasons that have been released). The set also includes a 50-page full-color book titled \"The Episode Guide\". Along with episode names and synopsis, there is trivia, facts about the making of the show, liner notes, and over 80 full-color photos. In Region 2, Universal Playback has released the first seven seasons on DVD in the UK. In Region 4, Universal Pictures has released the first eight seasons on DVD in Australia and New Zealand. also in Region 4, seasons 9-20 are now available on DVD in Australia and New Zealand.\n\n\n\n"}
{"id": "28429007", "url": "https://en.wikipedia.org/wiki?curid=28429007", "title": "List of Siemens products", "text": "List of Siemens products\n\nProducts produced by Siemens AG\n\n\n\n\"(See also Siemens Mobility)\"\n\n\n\n\n\n\n\n\nRadiography, Angiography, Fluoroscopy etc.\n\n\n\n\n\n"}
{"id": "309221", "url": "https://en.wikipedia.org/wiki?curid=309221", "title": "Lubrication", "text": "Lubrication\n\nLubrication is the process or technique of using a lubricant to reduce friction and/or wear in a contact between two surfaces. The study of lubrication is a discipline in the field of tribology.\n\nLubricants can be solids (such as Molybdenum disulfide MoS), solid/liquid dispersions (such as grease), liquids (such as oil or water), liquid-liquid dispersions or gases. \n\nFluid-lubricated systems are designed so that the applied load is partially or completely carried by hydrodynamic or hydrostatic pressure, which reduces solid body interactions (and consequently friction and wear). Depending on the degree of surface separation, different lubrication regimes can be distinguished.\n\nAdequate lubrication allows smooth, continuous operation of machine elements, reduces the rate of wear, and prevents excessive stresses or seizures at bearings. When lubrication breaks down, components can rub destructively against each other, causing heat, local welding, destructive damage and failure.\n\nAs the load increases on the contacting surfaces, three distinct situations can be observed with respect to the mode of lubrication, which are called lubrication regimes:\n\n\nBesides supporting the load the lubricant may have to perform other functions as well, for instance it may cool the contact areas and remove wear products. While carrying out these functions the lubricant is constantly replaced from the contact areas either by the relative movement (hydrodynamics) or by externally induced forces.\n\nLubrication is required for correct operation of mechanical systems such as pistons, pumps, cams, bearings, turbines, cutting tools etc. where without lubrication the pressure between the surfaces in close proximity would generate enough heat for rapid surface damage which in a coarsened condition may literally weld the surfaces together, causing seizure.\n\nIn some applications, such as piston engines, the film between the piston and the cylinder wall also seals the combustion chamber, preventing combustion gases from escaping into the crankcase.\n\n\n"}
{"id": "43934422", "url": "https://en.wikipedia.org/wiki?curid=43934422", "title": "Marshall Holloway", "text": "Marshall Holloway\n\nMarshall Glecker Holloway (November 23, 1912 – June 18, 1991) was an American physicist who worked at the Los Alamos Laboratory during and after World War II. He was its representative, and the deputy scientific director, at the Operation Crossroads nuclear tests at Bikini Atoll in the Pacific in July 1946. Holloway became the head of the Laboratory's W Division, responsible for new weapons development. In September 1952 he was charged with designing, building and testing a thermonuclear weapon, popularly known as a hydrogen bomb. This culminated in the Ivy Mike test in November of that year.\n\nMarshall Glecker Holloway was born in Oklahoma, on November 23, 1912, but his family moved to Florida when he was young. He graduated from Haines City High School, and entered the University of Florida, which awarded him a Bachelor of Science in Education in 1933, and a Master of Science degree in physics in 1935. He went on to Cornell University, where he wrote his Doctor of Philosophy thesis on the \"Range and Specific Ionization of Alpha Particles\".\n\nHolloway married Wilma Schamel, who worked in the Medical Office at Cornell as a medical technologist, on August 22, 1938. During a picnic at Taughannock Falls on June 3, 1940, she and a graduate student, Henry S. Birnbaum, drowned while trying to rescue two women in the water. The women were subsequently rescued by Jean Doe Bacher, the wife of physicist Robert Bacher, and Helen Hecht, a graduate student, but the bodies of Wilma and Birnbaum had to be retrieved with grappling hooks two days later.\n\nIn 1942, Holloway arrived at Purdue University on a secret assignment from the Manhattan Project. His task was to modify the cyclotron there to help the group there, which included L.D. P. King and Raemer Schreiber and some graduate students, measure the cross section of the fusion of a deuterium nucleus, when bombarded with a tritium nucleus to form a nucleus (alpha particle), and the cross section of a deuterium-tritium interaction to form . These calculations were for evaluating the feasibility of Edward Teller's thermonuclear \"Super bomb\", and the resulting reports would remain classified for many years.\n\nThe fusion cross section calculations were finished by September 1943, and the Purdue group moved to the Los Alamos Laboratory, where most of them, including Holloway, worked on the Water Boiler, an aqueous homogeneous reactor that was intended for use as a laboratory instrument to test critical mass calculations and the effect of various tamper materials. The Water Boiler group was headed by Donald W. Kerst from the University of Illinois, and the group designed and built the Water Boiler, which achieved its criticality in May 1944 under the control of Enrico Fermi, after one final addition of uranium enriched to 14% uranium 235. It was world third reactor but the first reactor to use enriched uranium as a fuel, using most of the world's supply at the time, and the first to use liquid nuclear fuel in the form of soluble uranium sulfate dissolved in water.\n\nHolloway studied the safety of the Little Boy bomb, particularly what would happen if the active material became immersed in water. He was also involved in experiments to measure the critical mass of plutonium. These proved hazardous, taking the lives of Harry Daghlian and Louis Slotin after the war. Holloway was part of Robert Bacher's \"pit team\" that assembled the Gadget for the Trinity nuclear test, and he helped Bacher fabricate the plutonium hemispheres of the Nagasaki Fat Man bomb.\n\nHolloway remained at Los Alamos after the war ended in 1945. He was its representative, and the deputy scientific director, at the Operation Crossroads nuclear tests at Bikini Atoll in the Pacific in July 1946, when atomic bombs were tested against an array of warships. Holloway became the head of the Laboratory's W Division, responsible for new weapons development.\n\nThe Los Alamos National Laboratory had continued research into fusion weapons for many years after Holloway's work in 1942 and 1943, and in 1951 the Atomic Energy Commission, which had replaced the Manhattan Project in 1947, ordered the Laboratory to proceed with designing, building and testing a thermonuclear weapon, popularly known as a hydrogen bomb. Laboratory director Norris Bradbury placed Holloway in charge of the hydrogen bomb program.\n\nAlthough Holloway had a well-earned reputation for his administrative ability, Bradbury's decision to put him in charge was not popular, especially with Edward Teller. The two men had clashed a number of times over a number of different issues. Holloway's appointment was therefore \"like waving a red flag in front of a bull\". Teller wrote that: \nTeller left the project on September 17, 1952, just a week after the announcement of Holloway's appointment. Nor was Teller the only one who chafed under Holloway's leadership style. Before the Ivy Mike test, Wallace Leland and Harold Agnew put a shark in Holloway's bed. \"He never said anything,\" Agnew recalled, \"but after that he was much more collegial.\"\n\nThe Ivy Mike test on November 1, 1952 was a complete success, but it was not a weapon so much as an experiment to verify the Teller and Stanislaw Ulam's design. Years of work was still required to produce a usable weapon.\n\nIn 1955, Holloway left the Los Alamos National Laboratory for the MIT Lincoln Laboratory, where he worked on air defense projects. In 1957 he became head of the Nuclear Products-ERCO Division of ACF Industries. He was vice president of Budd Company from 1967 to 1969, when he retired to live in Jupiter, Florida, Holloway and his wife Harriet subsequently moved to Winter Haven, Florida, where his son Jerry, a retired United States Air Force officer, lived. Holloway died there on June 18, 1991.\n\n"}
{"id": "26470773", "url": "https://en.wikipedia.org/wiki?curid=26470773", "title": "Metrics Reference Model", "text": "Metrics Reference Model\n\nThe metrics reference model (MRM) is the reference model created by the Consortium for Advanced Management-International (CAM-I) to be a single reference library of performance metrics. This library is useful for accelerating to development of and improving the content of any organization's business intelligence solution.\n\nThe MRM was created by the Business Intelligence Working Group of CAM-I. Substantial secondary research performed by the group revealed that there was no single, central location for performance metrics information. All published material focused on a single dimension of performance metrics (e.g., financial), or focused on how to use such metrics to affect the performance of an organization. Exhaustive lists of performance metrics that were generic to nearly any organization across all aspects of the business (e.g., customer, product/service, employee) were not found. Furthermore, without a single reference point, no research was located where the relationships between these performance metrics had been documented and described.\n\nMRM is a technology agnostic reference model based on the basic elements of a generic organization for which cost, process, and performance improvements may be desired.\n\nIn keeping with CAM-I's generic model of an organization, these elements include:\n\nThe figure below illustrates the flow between these components in a typical organization. \n\nFor each of the organizational elements, the MRM provides a comprehensive list of cost, process, and performance metrics of possible interest to many organizations. For example, some of the performance metrics provided for the employee organizational element include average customer review, customer retention rate, sales per customer/spending rate, average quantity per customer order, order frequency, average number of orders per customer, and average customer lifetime value.\n\nFor each performance metric, this information is also provided:\n\nThe benefit of the MRM is that each metric has already been identified, defined, and considered by a group of experienced business intelligence professionals. The idea of the MRM is for the business intelligence (BI) practitioner to follow these steps:\n\nHow the MRM may be used practically in the industry depends on whether an individual or an organization is considered.\n\nIndividual interest in the MRM:\n\n\nThe MRM to be used in a number of different ways, including:\n\n\n\n"}
{"id": "499797", "url": "https://en.wikipedia.org/wiki?curid=499797", "title": "MiniScribe", "text": "MiniScribe\n\nMiniScribe was a manufacturer of disk storage products, founded in Longmont, Colorado in 1980. MiniScribe designed and sold stepper motor-based hard disk drives with a large amount of onboard logic for the time. They eventually moved into higher-profile voice coil motor designs, and won major contracts with IBM. They were a relatively well-known brand through the early 1980s.\n\nThe company was started by Terry Johnson, who had a 20-year career in the hard drive business at such companies as IBM, Memorex and Storage Technology Corporation. MiniScribe became a major player when it won a series of contracts to supply IBM's PC division, and their subsequent rapid growth led to an initial public offering in late 1983, opening for trading in January 1984. However, slow sales of the IBM XT led IBM to dramatically scale back their orders late that year, forcing MiniScribe to lay off 26% of its staff and causing the value of the stock to plummet. Johnson left the company; at the time he stated that he had been planning to do this for some time and that his departure had \"absolutely nothing\" to do with IBM. Johnson would later comment that \"It's a very low inertia industry, you can blow your way into it and get blown out of it very quickly.\" Roger Gower, recently promoted to President, took over the CEO role as well.\n\nShortly thereafter the company was recapitalized with a $20 million investment from Hambrecht & Quist (H&Q), a venture capital firm. One of H&Q's officers was Quentin Thomas (\"QT\") Wiles, a turnaround specialist nicknamed \"Dr. Fix-It\". Wiles took over the CEO position from Gower, running the company remotely from his office in Sherman Oaks, Los Angeles, with a management team made up primarily of accountants. The company soon returned to profitability, with sales increasing from $114 million at the height of IBM's orders, to $603 million by 1988, when they were named the most well-managed company in the disk drive industry. Their major customer at this time was Compaq, but the company was also bidding for major contracts with Apple Computer and Digital Equipment Corporation (DEC).\n\nIn January 1987, the company officers conducted a quick inventory in order to estimate their accounts prior to an independent review by a third party accounting firm, Coopers & Lybrand. Their internal inventory count showed that there was a shortfall of between $2 and $4 million. This implied the cost to produce those drives that did sell was higher than initially thought, which, if properly booked against sales, would mean their operating margins would be unimpressive. Instead of reporting this, a number of the managers decided to cover it up with various means. They produced an inflated inventory count, and then broke into the accountant's lock boxes and replaced their independent count to match their newly inflated numbers. The team continued to roll these numbers forward through the quarters, compounding the problem.\n\nIn July 1987, Parker, director of far east operations, told Wiles that something was amiss. In August, Wiles travelled to Hong Kong and Singapore where he found a complete loss of control. The inventory count from that fall showed that the numbers had grown to $15 million, mostly in Colorado. A report was prepared to consider various solutions, but Wiles suggested that they continue hiding the problem, ordering all copies of the report be destroyed. This led to the company's most infamous cover-up; the managers rented a second warehouse in Colorado, where they personally packed 26,000 bricks into hard drive boxes and shipped them to Singapore in order to shore up the inventory count. After the count was complete, they recalled those serial numbers as defective units, but instead of writing them off, they checked them into inventory, along with other failed drives that had been returned.\n\nThe company continued to post impressive numbers, but there were troubling signs. Among them was that the company continued to post improving operating margins while the rest of the hard drive industry was suffering from rapidly falling margins due to ongoing downward price pressure. In 1988 the board of directors became concerned when the company's reported receivables grew dramatically, indicating a large amount of unpaid billables, while at the same time inventories were also growing, indicating unsold product. Those two numbers are normally at odds; increased receivables should indicate increased sales, which would normally result in decreased inventory. The directors began an internal investigation in October, while the company reported another record-setting quarter for that period in spite of failing to win either the Apple or DEC contracts.\n\nThe investigation revealed that Wiles set iron-clad sales forecasts and pushed these requirements down into the sales team, leaving no room for failure and setting bonuses on beating those figures. The sales team responded by \"touching up\" reporting documents as they moved back up the reporting chain. Wiles responded to these positive reports by setting even higher sales targets, leading to ever-increasing fraud to meet them. Wiles eventually left the company in February 1989, followed by most of the company's officers over the next months. The directors report was released in the summer, stating that Wiles and his management team had \"perpetrated a massive fraud\" in a \"company run amok\".\n\nAmong the other techniques used to improve the numbers were classic accounting tricks; failing to write off bad debts, shipping drives to warehouses and booking them as sales (channel stuffing), back-dating shipments to allow them to be booked in earlier reporting periods to meet goals, and deliberately shipping defective products repeatedly to different customers so one drive could be booked as multiple sales (leading employees to joke that the only thing being repaired were the worn-out cardboard boxes). When the company embarked on a round of layoffs just before the 1989 Christmas shutdown, including several of the employees who were involved in the brick scheme, they immediately called the Denver area newspapers, who broke the story. Following immediate investigations in Singapore and Colorado the fraud was confirmed.\n\nThe company declared bankruptcy on 1 January 1990, and was quickly liquidated with Maxtor acquiring its assets in a $46 million cash and stock deal in 1990. In a subsequent court case, Wiles claimed to have known nothing of these schemes, saying he was duped by the middle management. However, several of those middle managers testified that Wiles was very much aware of the fraud. The court also noted that Wiles sold a considerable number of shares in the company in April and May 1988, immediately prior to reporting a shortfall due to an \"elaborate scheme\" to manipulate inventory shortfalls. Wiles was eventually convicted of fraud and insider trading.\n\nFounder Terry Johnson later helped found CoData with the intent of building a 3.5\" hard drive with the capacity of then-standard 5.25\" drives. The company merged with Conner Peripherals in 1986 and as Conner Peripherals became a major vendor for a time. Johnson died piloting his small plane near Norman Wells in northern Canada on 24 July 2010.\n"}
{"id": "30966530", "url": "https://en.wikipedia.org/wiki?curid=30966530", "title": "National Data Repository", "text": "National Data Repository\n\nA National Data Repository (NDR) is a data bank that seeks to preserve and promote a country’s natural resources data, particularly data related to the petroleum exploration and production (E&P) sector.\n\nA National Data Repository is normally established by an entity that governs, controls and supports the exchange, capture, transference and distribution of E&P information, with the final target to provide the State with the tools and information to assure the growth, govern-ability, control, independence and sovereignty of the industry.\n\nThe two fundamental reasons for a country to establish an NDR are to preserve data generated inside the country by the industry, and to promote investments in the country by utilizing data to reduce the exploration, production, and transportation business risks.\n\nCountries take different approaches towards preserving and promoting their natural resources data. The approach varies according to a country’s natural resources policies, level of openness, and its attitude towards foreign investment.\n\nNDRs store a vast array of data related to a country’s natural resources. This includes wells, well log data, well reports, core samples, seismic surveys, post-stack seismic, field data/tapes, seismic (acquisition/processing) reports, production data, geological maps and reports, license data and geological models.\n\nSome NDRs are financed entirely by a country’s government. Others are industry-funded. Still some are hybrid systems, funded in part by industry and government.\nNDRs typically charge fees for data requests and for data loading. The cost differs significantly between countries. In some cases an annual membership is charged to oil companies to store and access the data in the NDR.\n\nEnergistics is the global energy standards resource center for the upstream oil and gas industry.\n\nEnergistics National Data Repository Work Group:\nThe standards body is Energistics.\n\nGlobal regulators of upstream oil and natural gas information, including seismic, drilling, production and reservoir data, formed the National Data Repository (NDR) Work Group in 2008 to collaborate on the development of data management standards and to assist emerging nations with hydrocarbon reserves to better collect, maintain and deliver oil and gas data to the public and to the industry.\n\nTen countries, led by the Netherlands, Norway and the United Kingdom, formed NDR to share best practices and to formalize the development and deployment of data management standards for regulatory agencies. The other countries involved in the NDR Work Group’s formation are Australia, Canada, India, Kenya, New Zealand, South Africa and the United States.\n\nAnnual NDR Conference: Approximately every 18 months Energistics organizes a National Data Repository Conference. The purpose is to provide government and regulatory agencies from around the world an opportunity to attend a series of workshops dedicated to developing data exchange standards, improving communications with the oil and gas industry and learning data management techniques for natural resources information.\n\nThe SEG is the custodian of the SEG standards which are used for the exchange, retention and release of seismic data. They are commonly used by National Data Repositories with the SEGD and SEGY being the field and processed exchange standards respectively.\n\nClick here to see a map of the NDRs around the world\n\n\n"}
{"id": "16620919", "url": "https://en.wikipedia.org/wiki?curid=16620919", "title": "Network Centric Product Support", "text": "Network Centric Product Support\n\nNetwork Centric Product Support (NCPS) is an early application of an Internet of Things (IoT) computer architecture developed to leverage new information technologies and global networks to assist in managing maintenance, support and supply chain of complex products made up of one or more complex systems, such as in a mobile aircraft fleet or fixed location assets such as in building systems. This is accomplished by establishing digital threads connecting the physical deployed subsystem with its design Digital Twins virtual model by embedding intelligence through networked micro-web servers that also function as a computer workstation within each subsystem component (i.e. Engine control unit on an aircraft) or other controller and enabling 2-way communications using existing Internet technologies and communications networks - thus allowing for the extension of a product lifecycle management (PLM) system into a mobile, deployed product at the subsystem level in real time. NCPS can be considered to be the support flip side of Network-centric warfare, as this approach goes beyond traditional logistics and aftermarket support functions by taking a complex adaptive system management approach and integrating field maintenance and logistics in a unified factory and field environment. Its evolution began out of insights gained by CDR Dave Loda (USNR) from Network Centric Warfare-based fleet battle experimentation at the US Naval Warfare Development Command (NWDC) in the late 1990s, who later lead commercial research efforts of NCPS in aviation at United Technologies Corporation. Interaction with the MIT Auto-ID Labs, EPCglobal, the Air Transport Association of America ATA Spec 100/iSpec 2200 and other consortium pioneering the emerging machine to machine Internet of Things (IoT) architecture contributed to the evolution of NCPS.\n\nSimply put, this architecture extends the existing World Wide Web infrastructure of networked web servers down into the product at its subsystem's controller level using a Systems Engineering \"system of systems\" nested approach. Its core is an embedded dual function webserver/computer workstation connected to the product controller's test ports (as used in retrofit applications, or integrated directly into the controller for new products), hence providing access to operational cycles, sensor and other information in a clustered, internet addressable node that allows for local or remote access, and the ability to host remotely reconfigurable software that can collect and process data from its mated subsystem controller onboard and pull in other computing resources throughout the network. It can then establish a localized wireless World Wide Web in and around the product that can be securely connected to by a mechanic with any web browser-equipped handheld independent of the greater World Wide Web, as well as seamlessly integrate into global networks when external wireless communications is available - thus creating data Digital Twins at the factory, connecting deployed product usage in the Product lifecycle with constantly updated digital threads. This allows for an integrated approach which enables both offline and online updates to occur. Legacy systems usually require a human to physically connect a laptop to the system controller or a telematic solution to manually collect data and carry it back to a location where it can be later transferred to the factory or to restricted webserver-based download sites for offboard analysis.\n\nThe architecture also enables communications with other micro-webservers in its Computer cluster (i.e. in an aircraft), or to higher level clusters (such as an internet portal managed fleet and flight operations managers), thus enabling access to data resources and personnel and factory engineers at remote office computers through the World Wide Web. As stated previously, the system operates asynchronously, in that it does not have to be always connected to the World Wide Web to function; rather it simply operates locally, then synchronizes two-way information relevant to the subsystem, acting as a Gateway (telecommunications) on board that connects with other gateways within the network, which can be airborne or on the ground, on an as-needed basis when communications is available. This can be accomplished through a Wireless LAN Network, satellite, cellular network or other wireless or wired communications capabilities.\n\nSecurity of the network is critical, and the architecture can utilize standard web security protocols, from Public-key cryptography to embedded hardware encryption devices.\n\nThe extension of the World Wide Web architecture into the product is important to understand, as all decisions for manufacturing of spare parts, scheduling for flights, and other factory OEM and airline operator functions, are driven primarily by what happens to the product in the field (rate of wear and impending failure, primarily). Predicting the rate of wear, and hence the impact on operations and forecasting for producing spare parts in the future, is critical for optimizing operations for all involved. Managing a complex system such as a fleet of aircraft, vehicles or fixed location products can be accomplished in this manner. For example, coupled with technologies such as RFID, the system could track parts from the factory to the aircraft on board, then continue to read the configuration of the subsystem’s replaceable tagged parts, map their configuration to hours run and duty cycles, then process/communicate the projected wear rate through the World Wide Web back to the operator or factory. In this way mechanical wear rates and future failures can be predicted more accurately and the forecasting of spare parts manufacturing and shipment can be significantly improved. This is called Prognostics Health Monitoring (PHM), which has become possible in recent years with the advent of electronic controllers, and is a recent evolutionary step in aircraft support and maintenance management that began as individual processes prior to World War II and solidified into a manual tracking system to support aircraft fleets in the Korean War. Support for the mechanic comes in local wireless access to technical information stored and remotely updated on board the micro-webserver component relevant to that product, such as service bulletins, factory updates, fault code driven, intelligent 3D computer game-like maintenance procedures, and social media applications for sharing of product issues and maintenance procedure improvements in the field to include collaborative 2-way voice, text and image communications. Note that this architecture can be utilized on any system that requires monitoring and trending, to include mobile medical applications for monitoring functionality of human systems when the subject is equipped with data sensors.\n\nThe original micro-webserver component (i. e. the onboard unit) that is key to enabling the NCPS architecture was first prototyped and demonstrated in 2001 by David Loda, Enzo Macchia, Sam Quadri and Bjorn Stickling at United Technologies' Pratt & Whitney division and initially tested on board a Fairchild-Dornier 328 (later AvCraft 328) regional jet in January 2002. It was introduced to the public and demonstrated at the Farnborough Air Show in July 2002 in prototype form and again in 2004 as a flight certified product offering marketed by United Technologies as the DTU and later FAST data management units for service in a number of aircraft and helicopter fleets.\n\nA similar complex systems approach in a completely different application is successfully embodied in the Eisenhower Interstate Highway System, though what is transported in NCPS is information, not cars and trucks. Network Centric Product Support, or net-centric product support, is an architectural concept, and merely connects the major avenues already existing in global communications and the internet down into the mobile product, extending maintenance and supply chain processes into an integrated product centric system with a real time feedback loop to the designers, factory and maintainers as to product performance and reliability. For example, to gain information about a particular engine on a mobile aircraft, it is most efficient to send the inquiry to the engine directly and host all information generated and relevant to that system there, as well synchronize in a twin remote database for access and queuing when the engine system is not in communications. Other examples where this can be applied include shipping containers, automobiles, spacecraft, appliances, human medical monitoring, or any other complex product with sensors and subsystems that require maintenance support and monitoring.\n\nMany organizations are beginning to see the value of a netcentric (also spelled \"net-centric\") approach to managing complex systems, including the Network Centric Operations Industry Consortium (NCOIC), which is an association of leading aerospace and defense contractors in the Network Centric Warfare arena. Network Centric thinking for aircraft operations, including Network Enabled Operations (NEO) demonstrations, also figure prominently in the commercial Next Generation Air Transportation System (NextGen) approach being made by the US Government to revamping air transportation management in the 21st Century.\n\n"}
{"id": "37375", "url": "https://en.wikipedia.org/wiki?curid=37375", "title": "Northrop Grumman RQ-4 Global Hawk", "text": "Northrop Grumman RQ-4 Global Hawk\n\nThe Northrop Grumman RQ-4 Global Hawk is an unmanned (UAV) surveillance aircraft. It was initially designed by Ryan Aeronautical (now part of Northrop Grumman), and known as Tier II+ during development. The Global Hawk performs duties similar to that of the Lockheed U-2. The RQ-4 provides a broad overview and systematic surveillance using high-resolution synthetic aperture radar (SAR) and long-range electro-optical/infrared (EO/IR) sensors with long loiter times over target areas. It can survey as much as of terrain a day, which is an area the size of South Korea or Iceland.\n\nThe Global Hawk is operated by the United States Air Force (USAF). It is used as a high-altitude platform covering the spectrum of intelligence collection capability to support forces in worldwide military operations. According to the USAF, the superior surveillance capabilities of the aircraft allow more precise weapons targeting and better protection of friendly forces. Cost overruns led to the original plan to acquire 63 aircraft being cut to 45, and to a 2013 proposal to mothball the 21 Block 30 signals intelligence variants. Each aircraft was to cost US$60.9 million in 2001, but this had risen to $222.7 million per aircraft (including development costs) by 2013. The U.S. Navy has developed the Global Hawk into the MQ-4C Triton maritime surveillance platform.\n\nThe Global Hawk took its first flight on 28 February 1998. The first seven aircraft were built under the Advanced Concept Technology Demonstration (ACTD) program, sponsored by DARPA, in order to evaluate the design and demonstrate its capabilities. Demand for the RQ-4's abilities was high in the Middle East; thus, the prototype aircraft were actively operated by the USAF in the War in Afghanistan. In an unusual move, the aircraft entered initial low-rate production while still in engineering and manufacturing development. Nine production Block 10 aircraft, sometimes referred to as RQ-4A, were produced; of these, two were sold to the US Navy and an additional two were deployed to Iraq to support operations there. The final Block 10 aircraft was delivered on 26 June 2006.\n\nIn order to increase the aircraft's capabilities, the airframe was redesigned, with the nose section and wings being stretched. The modified aircraft, designated RQ-4B Block 20, can carry up to 3,000 lb (1,360 kg) of internal payload. These changes were introduced with the first Block 20 aircraft, the 17th Global Hawk produced, which was rolled out in a ceremony on 25 August 2006. First flight of the Block 20 from the USAF Plant 42 in Palmdale, California to Edwards Air Force Base took place on 1 March 2007. Developmental testing of Block 20 took place in 2008.\n\nThe United States Navy took delivery of two of the Block 10 aircraft to evaluate their maritime surveillance capabilities, designated N-1 (BuNo 166509) and N-2 (BuNo 166510). The initial navalised example was tested at Edwards Air Force Base briefly, before moving to NAS Patuxent River in March 2006 for the Global Hawk Maritime Demonstration (GHMD) program, operated by Navy squadron VX-20. In July 2006, the GHMD aircraft flew in the Rim of the Pacific (RIMPAC) exercise for the first time; although it was in the vicinity of Hawaii, the aircraft was operated from NBVC Point Mugu, requiring flights of approximately each way to the area. Four flights were performed, resulting in over 24 hours of persistent maritime surveillance coordinated with and . For the GHMD program, the Global Hawk was tasked with maintaining maritime situational awareness, contact tracking, and imagery support of exercise operations. Images were transmitted to NAS Patuxent River for processing and then forwarded to the fleet off Hawaii.\n\nNorthrop Grumman entered a RQ-4B variant in the US Navy's Broad Area Maritime Surveillance (BAMS) UAV competition. On 22 April 2008, it was announced that Northrop Grumman's \"RQ-4N\" had won and that the Navy had awarded a US$1.16 billion contract. In September 2010, the RQ-4N was officially designated the \"MQ-4C\". The Navy MQ-4C differs from the Air Force RQ-4 mainly in its wing. While the Global Hawk remains at high altitude to conduct surveillance, the Triton climbs to 50,000 ft to see a wide area and can drop to 10,000 ft to get further identification of a target. The Triton's wings are specially designed to take the stresses of rapidly decreasing altitude. Though similar in appearance to the Global Hawk's wings, the Triton's internal wing structure is much stronger and has additional features including anti-icing capabilities and impact and lightning strike protection.\n\nDevelopment cost overruns placed the Global Hawk at risk of cancellation. In mid-2006, per-unit costs were 25% over baseline estimates, caused by both the need to correct design deficiencies as well as to increase its capabilities. This caused concern over a possible congressional termination of the program if its national security benefits could not be justified. However, in June 2006, the program was restructured. Completion of an operational assessment report by the USAF was delayed from August 2005-November 2007 due to manufacturing and development delays. The operational assessment report was released in March 2007 and production of the 54 air vehicles planned was extended by two years to 2015.\nIn February 2011, the USAF reduced its planned purchase of RQ-4 Block 40 aircraft from 22 to 11 in order to cut costs. In June 2011, the U.S. Defense Department's Director, Operational Test and Evaluation (DOT&E) found the RQ-4B \"not operationally effective\" due to reliability issues. In June 2011, the Global Hawk was certified by the Secretary of Defense as critical to national security following a breach of the Nunn-McCurdy Amendment; the Secretary stated: \"The Global Hawk is essential to national security; there are no alternatives to Global Hawk which provide acceptable capability at less cost; Global Hawk costs $220M less per year than the U-2 to operate on a comparable mission; the U-2 cannot simultaneously carry the same sensors as the Global Hawk; and if funding must be reduced, Global Hawk has a higher priority over other programs.\"\n\nOn 26 January 2012, the Pentagon announced plans to end Global Hawk Block 30 procurement as the type was found to be more expensive to operate and with less capable sensors than the existing U-2. Plans to increase procurement of the Block 40 variant were also announced. The Air Force's fiscal year 2013 budget request said it had resolved to divest itself of the Block 30 variant; however, the National Defense Authorization Act for Fiscal Year 2013 mandated operations of the Block 30 fleet through the end of 2014. The USAF plans to procure 45 RQ-4B Global Hawks as of 2013. Before retiring in 2014, ACC commander, General Mike Hostage said of the U-2's replacement by the drone that \"The combatant commanders are going to suffer for eight years and the best they’re going to get is 90 percent\".\n\nFrom 2010-2013, costs of flying the RQ-4 fell by more than 50%. In 2010, the cost per flight hour was $40,600, with contractor logistic support making up $25,000 per flight hour of this figure. By mid-2013, cost per flight hour dropped to $18,900, contractor logistic support having dropped to $11,000 per flight hour. This was in part due to higher usage, spreading logistics and support costs over a higher number of flight hours.\n\nThe German Air Force (\"Luftwaffe\") ordered a variant of the RQ-4B, to be equipped with a customized sensor suite, designated \"EuroHawk\". The aircraft was based on the RQ-4B Block 20/30/40 and was to be equipped with an EADS-built SIGINT package; it was intended to fulfill Germany's requirement to replace their aging Dassault-Breguet Atlantique electronic surveillance aircraft of the Marineflieger (\"German Naval Air Arm\"). The EADS sensor package is composed of six wing-mounted pods; reportedly these sensor pods could potentially be used on other platforms, including manned aircraft.\n\nThe EuroHawk was officially rolled out on 8 October 2009 and its first flight took place on 29 June 2010. It underwent several months of flight testing at Edwards Air Force Base. On 21 July 2011, the first EuroHawk arrived in Manching, Germany; after which it was scheduled to receive its SIGINT sensor package and undergo further testing and pilot training until the first quarter of 2012. The Luftwaffe planned to station the type with Reconnaissance Wing 51. In 2011 the German ministry of defence was aware of difficulties with the certification for use within the European airspace. During flight trials, problems with the EuroHawk's flight control system were found; the German certification process was also complicated by Northrop Grumman refusing to share technical data on the aircraft with which to perform evaluations.\n\nOn 13 May 2013, German media reported that the EuroHawk would not be certifiable under ICAO rules without an anti-collision system; thus preventing any operations within European airspace or the airspace of any ICAO member. The additional cost of certification was reported to be more than €600 million (US$780 million). On 15 May 2013, the German government announced the immediate termination of the program, attributing the cancellation to the certification issue. Reportedly, the additional cost to develop the EuroHawk to the standards needed for certification may not have guaranteed final approval for certification. German defense minister Thomas de Maizière stated EuroHawk was \"very important\" for Germany in 2012, then referred to the project as being \"a horror without end\" in his 2013 statement to the Bundestag. The total cost of the project before it was canceled was €562 million. Northrop Grumman and EADS have described reports of flight control problems and high costs for certification as \"inaccurate\"; they have stated their intention to provide an affordable plan to complete the first EuroHawk's flight testing and produce the remaining four aircraft.\n\nOn 8 August 2013, the EuroHawk set an endurance record by flying continuously in European airspace for 25.3 hours, reaching an altitude of . It was the longest flight by an unrefueled UAS weighing more than in European skies. On 5 October 2014, German Minister of Defence Ursula von der Leyen was reportedly considering reactivating the EuroHawk program to test its reconnaissance abilities over a long period at altitudes of up to . Attempting to test the recon system on Airbus aircraft and an Israeli drone as alternate platforms had proven unsuccessful. The Bundeswehr would use it to detect, decrypt, and potentially interfere with enemy communications signals. If tests prove successful, a carrier would be purchased, likely \"similar\" to the U.S. Global Hawk. Germany is considering installing the EuroHawk's SIGINT payloads onto the U.S. Navy MQ-4C Triton Global Hawk derivative, as the electronic and communications intelligence sensors would be more difficult to place on other substitute aircraft. It already has icing and lightning-strike protection, and was built with certification over civilian airspace in mind, meeting the STANAG 4671 requirements that had ended the EuroHawk program.\n\nIn January 2014, President Obama signed a budget that included a $10 million study on adapting the U-2's superior sensors for the RQ-4. In April 2015, Northrop Grumman reportedly installed the U-2's Optical Bar Camera (OBC) and Senior Year Electro-Optical Reconnaissance System (SYERS-2B/C) sensors onto the RQ-4 using a Universal Payload Adapter (UPA). Successful testing indicated that all RQ-4s could be similarly retrofitted.\n\nOn 14 July 2015, Northrop Grumman and the USAF signed an agreement to demonstrate an RQ-4B fitted with the U-2's OBC and SYERS-2C sensors; two Global Hawks are to be fitted with the UPA, involving the installation of 17 payload adapter fixtures and a new payload bay cover, as well as software and mission system changes for each sensor. The UPA can support of sensors and will create a canoe-shaped sensor bay on the fuselage's underside. The RQ-4's ability to operate these sensors will likely influence the U-2's planned retirement by 2019. In addition, Northrop also expects to receive a contract to integrate the UTC Aerospace Systems MS-177 mulitspectral sensor used on the E-8C JSTARS onto the RQ-4; the MS-177 will replace the SYERS-2 and includes modernized optronics and a gimbaled rotation device to increase field of view by 20 percent. The RQ-4B flew with the SYERS-2 on 18 February 2016.\n\nThe Global Hawk UAV system comprises the RQ-4 air vehicle, which is outfitted with various equipment such as sensor packages and communication systems; and a ground element consisting of a Launch and Recovery Element (LRE), and a Mission Control Element (MCE) with ground communications equipment. Each RQ-4 air vehicle is powered by an Allison Rolls-Royce AE3007H turbofan engine with thrust, and carries a payload of . The fuselage comprises an aluminum, semi-monocoque construction with V-tail; the wings are made of composite materials.\n\nThere have been several iterations of the Global Hawk with different features and capabilities. The first version to be used operationally was the RQ-4A Block 10, which performed imagery intelligence (IMINT) with a payload of a synthetic aperture radar (SAR) with electro-optical (EO) and infrared (IR) sensors; seven A-model Block 10s were delivered and all were retired by 2011. The RQ-4B Block 20 was the first of the B-model Global Hawks, which has a greater payload and employs upgraded SAR and EO/IR sensors; four Block 20s were converted into communications relays with the Battlefield Airborne Communications Node (BACN) payload. The RQ-4B Block 30 is capable of multi-intelligence (multi-INT) collecting with SAR and EO/IR sensors along with the Airborne Signals Intelligence Payload (ASIP), a wide-spectrum SIGINT sensor. The RQ-4B Block 40 is equipped with the multi-platform radar technology insertion program (MP-RTIP) active electronically scanned array (AESA) radar, which provides SAR and moving target indication (MTI) data for wide-area surveillance of stationary and moving targets.\n\nSince the RQ-4 is capable of conducting sorties lasting up to 30 hours long, scheduled maintenance has to be performed sooner than on other aircraft with less endurance. However, since it flies at higher altitudes than normal aircraft, it experiences less wear during flight.\n\nRaytheon's Integrated Sensor Suite (ISS) consists of a synthetic aperture radar (SAR), electro-optical (EO), and thermographic camera (IR) sensors. Either the EO or the IR sensors can operate simultaneously with the SAR. Each sensor provides wide area search imagery and a high-resolution spot mode. The SAR has a ground moving target indicator (GMTI) mode, which can provide a text message providing the moving target's position and velocity. Both SAR and EO/IR imagery are transmitted from the aircraft to the MCE as individual frames, and reassembled during ground processing. An onboard inertial navigation system, supplemented by Global Positioning System updates, comprises the navigational suite.\n\nGlobal Hawk is capable to operate autonomously and \"untethered\". A military satellite system (X Band Satellite Communication) is used for sending data from the aircraft to the MCE. The common data link can also be used for direct down link of imagery when the UAV is within line-of-sight of compatible ground stations. For dense flight areas the autonomous navigation is switched off and the RQ-4 is remote controlled via the satellite link by pilots on the ground who are supplied with the same instrument data and who carry the same responsibilities as pilots in manned planes.\n\nThe ground segment consists of a Mission Control Element (MCE) and Launch and Recovery Element (LRE), provided by Raytheon. The MCE is used for mission planning, command and control, and image processing and dissemination; an LRE for controlling launch and recovery; and associated ground support equipment. The LRE provides precision Differential GPS corrections for navigational accuracy during takeoff and landings, while precision coded GPS supplemented with an inertial navigation system is used during mission execution. By having separable elements in the ground segment, the MCE and the LRE can operate in geographically separate locations, and the MCE can be deployed with the supported command's primary exploitation site. Both ground segments are contained in military shelters with external antennas for line-of-sight and satellite communications with the air vehicles.\n\nThe Global Hawk carries the Hughes Integrated Surveillance & Reconnaissance (HISAR) sensor system. HISAR is a lower-cost derivative of the ASARS-2 package that Hughes developed for the U-2. It is also fitted in the US Army's RC-7B Airborne Reconnaissance Low Multifunction (ARLM) manned aircraft, and is being sold on the international market. HISAR integrates a SAR-MTI system, along with an optical and an thermography imager. All three sensors are controlled and their outputs filtered by a common processor and transmitted in real time at up to 50 Mbit/s to a ground station. The SAR-MTI system operates in the X band in various operational modes; such as the wide-area MTI mode with a radius of , combined SAR-MTI strip mode provides resolution over wide sections, and a SAR spot mode providing resolution over .\n\nIn July 2006, the USAF began testing the Global Hawk Block 30 upgrades in the Benefield Anechoic Facility at Edwards AFB; such as the Advanced Signals Intelligence Payload, an extremely sensitive SIGINT processor. In 2006, a specialist AESA radar system, the Multi-Platform Radar Technology Insertion Program, or MP-RTIP, began testing on the Scaled Composites Proteus; one modified Global Hawk shall carry the radar following validation. In 2010, Northrop spoke on the sensor capabilities of the new Block 40 aircraft, including MP-RTIP radar, emphasising surveillance over reconnaissance.\n\nOn 14 April 2014, a Block 40 Global Hawk completed the first Maritime Modes program risk reduction flight to enhance the Air Force's maritime surveillance capabilities. Maritime Modes is made up of a Maritime Moving Target Indicator and a Maritime Inverse synthetic aperture radar (MISAR) that function together to provide ISR information on vessels traveling on the water's surface. During the 11.5 hour flight off of the California coast, the MISAR collected data on over 100 items of interest. Maritime Modes is planned to be integrated with the RQ-4B's existing MP-RTIP radar to detect and produce synthetic aperture radar imagery of ground vehicles.\n\nIn November 2015, Northrop Grumman selected the Garmin International GSX 70 weather radar to be installed on Air Force Global Hawks. The GSX 70 is designed to provide operators with real-time weather information, offering horizontal scan angles of up to 120 degrees for better visibility into the strength and intensity of convective activity and a vertical scanning mode to analyze storm tops, gradients, and cell buildup activity. It also has a Turbulence Detection feature to identify turbulence in air containing precipitation and other airborne particulates and Ground Clutter Suppression that removes ground returns from the display so operators can focus on weather. Installation is expected to begin in early 2016.\n\nThe visible and infrared imagers share the same gimballed sensor package, and use common optics, providing a telescopic close-up capability. It can be optionally fitted with an auxiliary SIGINT package. To improve survivability, the Global Hawk is fitted with a Raytheon developed AN/ALR-89 self-protection suite consisting of the AN/AVR-3 Laser warning receiver, AN/APR-49 Radar warning receiver and a jamming system. An ALE-50 towed decoy also aids in the deception of enemy air defenses.\n\nFollowing the September 11th attacks, the normal acquisition process was bypassed almost immediately and early developmental Global Hawk models were employed in overseas contingency operations beginning in November 2001. Global Hawk ATCD prototypes were used in the War in Afghanistan and in the Iraq War. Since April 2010, they fly the Northern Route, from Beale Air Force Base over Canada to South-East Asia and back, reducing flight time and improving maintenance. While their data-collection capabilities have been praised, the program lost three prototype aircraft to accidents, more than one quarter of the aircraft used in the wars. The crashes were reported to be due to \"technical failures or poor maintenance\", with a failure rate per hour flown over 100 times higher than the F-16 fighter. Northrop Grumman stated that it was unfair to compare the failure rates of a mature design to that of a prototype aircraft. In June 2012, a media report described the Global Hawk, the MQ-1 Predator and the MQ-9 Reapers \"... the most accident-prone aircraft in the Air Force fleet.\" On 11 February 2010, the Global Hawks deployed in the Central Command AOR accrued 30,000 combat hours and 1,500 plus sorties.\n\nInitial operational capability was declared for the RQ-4 Block 30 in August 2011. The USAF did not plan to keep the RQ-4B Block 30 in service past 2014 due to the U-2 and other platforms being less expensive in the role; but Congress sought to keep it in service until December 2016. The USAF had 18 RQ-4 Block 30s by the time of the passage of the National Defense Authorization Act for Fiscal Year 2013, which directed a further three RQ-4s to be procured as part of Lot 11; The USAF felt that additional aircraft were \"excess to need\" and likely become backup or attrition reserve models. Despite the potential retirement of the Block 30 fleet due to low reliability, low mission readiness, and high costs, the USAF released a pre-solicitation notice on 12 September 2013 for Lot 12 aircraft. In planning the USAF's FY 2015 budget, the Pentagon reversed its previous decision, shifting $3 billion from the U-2 to the RQ-4 Block 30, which had become more competitive with the U-2 due to increased flying hours. Factors such as cost per flight hour (CPFH), information gathering rates, mission readiness, adverse weather operational capability, distance to targets, and onboard power still favored the U-2.\n\nAfter the 2011 Tōhoku earthquake and tsunami, RQ-4s flew 300 hours over the affected areas in Japan. There were also plans to survey the No. 4 reactor at the Fukushima Daiichi Nuclear Power Plant.\n\nBy November 2012, Northrop Grumman had delivered thirty-seven Global Hawks to the USAF. As of March 2014, 42 Global Hawks are in use around the world, with 32 in use by the USAF.\n\nThe USAF stated that U-2 pilot and altitude advantages allow better functionality in the stormy weather and airspace restrictions of the East Asia region and its altitude and sensor advantages allow it to see further into hostile territory. In October 2013, the U.S. secured basing rights to deploy RQ-4s from Japan, the first time that basing rights for the type had been secured in Northeast Asia. RQ-4s are stationed at Andersen Air Force Base in Guam, but bad weather often curtailed flights. Basing in Japan as opposed to Guam enhances spying capabilities against North Korea by eliminating range as a factor. Two RQ-4s moved from Anderson AFB to Misawa Air Base in mid-2014 in the type's first deployment to Japan; they were speculated to have focused on maritime patrol missions. The two RQ-4s successfully performed their missions from Misawa AB during a six-month deployment, with none cancelled due to poor weather. It was the first time that they had operated out of a civil-military airport, sharing airspace and runways with commercial aircraft safely without additional restrictions, usually taking off and landing during quieter periods of air traffic. Officials only stated that they had operated at \"various places around the Pacific.\"\n\nOn 19 September 2013, the RQ-4 Block 40 Global Hawk conducted its first wartime flight from Grand Forks Air Force Base.\n\nIn November 2013, an USAF RQ-4 deployed to the Philippines after Typhoon Haiyan to assist in relief efforts. It flew from Andersen Air Force Base in Guam to relay imagery of afflicted areas to response personnel and ground commanders.\n\nIn planning for the FY 2015 budget, the U-2 was to be retired in favor of the RQ-4, made possible by reductions of RQ-4 operating costs and would be the first time an unmanned aircraft would completely replace a manned aircraft. The Block 40 Global Hawk may have to be retired in FY 2016 if sequestration is not repealed. The U-2 will continue to fly through 2018 without replacement.\n\nIn May 2014, a U.S. Global Hawk conducted a surveillance mission over Nigeria as part of the search for the kidnapped Nigerian schoolgirls. The Global Hawk joined MC-12 manned aircraft in the search.\n\nThe Global Hawk has been used in Operation Inherent Resolve (OIR) against the Islamic State of Iraq and the Levant (ISIL). The aircraft provide real-time imagery and signals intelligence to identify friendly and enemy forces, do long-term target development, and track enemy equipment movement, enabling combatant commanders to act on better information and make key decisions. The BACN version allows ground troops to contact aircraft when they are in need of assistance, such as close air support. On 11 November 2015, an EQ-4 became the first Global Hawk aircraft to reach flying 500 sorties. All three EQ-4s in operation are supporting OIR. Upon landing, maintainers can complete ground maintenance and make the aircraft mission ready again within five hours; missions can last up to 30 hours, with each aircraft getting a \"day off\" in between combat flights. On 1 April 2017, an EQ-4 completed 1,000 continuous sorties, without incurring a single maintenance cancellation, while supporting OIR.\n\nOn 4 April 2016, it was reported that a USAF Global Hawk had completed its third flight over Germany under an initiative (the European Reassurance Initiative) to reassure NATO members concerned over Russian involvement in the conflict in Ukraine. Germany opened its airspace for up to five Global Hawk flights a month until the middle of October 2016. The Naval Air Station Sigonella, Sicily-based Global Hawk flies over Italian and French airspace and an air corridor through Germany with its sensors switched off on its way to its area of operations over the Baltic Sea.\n\nIn 2017, the USAF decided to began the process of training enlisted Airmen to fly the RQ-4 due to a shortage of pilots and an increased demand for the Global Hawks capabilities. The RQ-4 is currently the only aircraft enlisted pilots are flying.\n\nOn 16 August 2018, a Global Hawk, assigned to 12th Reconnaissance Squadron, took off from Beale AFB, California, and landed at Eielson Air Force Base, Alaska for Red Flag – Alaska. This was the first time an RQ-4 had landed in Alaska during a simulated combat training exercise.\n\nOn 24 April 2001, a Global Hawk flew non-stop from Edwards AFB to RAAF Base Edinburgh in Australia, making history by being the first pilotless aircraft to cross the Pacific Ocean. The flight took 22 hours, and set a world record for absolute distance flown by a UAV, .\n\nOn 22 March 2008, a Global Hawk set the endurance record for full-scale, operational unmanned aircraft UAVs by flying for 33.1 hours at altitudes up to 60,000 feet over Edwards AFB.\n\nFrom its first flight in 1998 to 9 September 2013, the combined Global Hawk fleet flew 100,000 hours. 88 percent of flights were conducted by USAF RQ-4s, while the remaining hours were flown by NASA Global Hawks, the EuroHawk, the Navy BAMS demonstrator, and the MQ-4C Triton. Approximately 75 percent of flights were in combat zones; RQ-4s flew in operations over Afghanistan, Iraq, and Libya; and supported disaster response efforts in Haiti, Japan, and California.\n\nFrom 10–16 September 2014, the RQ-4 fleet flew a total of 781 hours, the most hours flown by the type during a single week. 87 percent of flights were made by USAF RQ-4s, with the rest flown by the Navy BAMS-D and NASA hurricane research aircraft.\n\nThe longest Global Hawk combat sortie lasted 32.5 hours.\n\nIn December 2007, two Global Hawks were transferred from the USAF to NASA's Dryden Flight Research Center at Edwards AFB. Initial research activities beginning in the second quarter of 2009 supported NASA's high-altitude, long-duration Earth science missions. The two Global Hawks were the first and sixth aircraft built under the original DARPA Advanced Concept Technology Demonstration program, and were made available to NASA when the Air Force had no further need for them. Northrop Grumman is an operational partner with NASA and will use the aircraft to demonstrate new technologies and to develop new markets for the aircraft, including possible civilian uses.\n\nAccording to an article in the March 2010 issue of Scientific American (p. 25-27), NASA's Global Hawks were expected to begin scientific missions that month, and had been undergoing tests in late 2009. Initial science applications included measurements of the ozone layer and cross-Pacific transport of air pollutants and aerosols; the author of the Scientific American piece speculates that it could be used for Antarctic exploration while being based in Chile. In August–September 2010, one of the two Global Hawks was loaned for NASA's GRIP Mission (Genesis and Rapid Intensification Program). Its long-term on station capabilities and long range made it a suitable aircraft for monitoring the development of Atlantic basin Hurricanes. It was modified to equip weather sensors including Ku-band radar, lightning sensors and dropsondes. It successfully flew into Hurricane Earl off the United States East Coast on 2 September 2010.\n\nIn 2009, NATO announced that it expected to have a fleet of up to eight Global Hawks by 2012 to be equipped with MP-RTIP radar systems. NATO had budgeted US$1.4 billion (€1 billion) for the project, and a letter of intent was signed. NATO signed a contract for five Block 40 Global Hawks in May 2012. 12 NATO members are participating in the purchase. On 10 January 2014, Estonia revealed it wanted to participate in NATO Global Hawk usage. In July 2017, the USAF assigned the Mission Designation Series (MDS) of RQ-4D to the NATO AGS air vehicle.\n\nAustralia considered the purchase of a number of Global Hawks for maritime and land surveillance. The Global Hawk was to be assessed against the MQ-9 Mariner in trials in 2007. The Global Hawk aircraft would have operated in conjunction with manned Boeing P-8 Poseidon aircraft, as a replacement of aging AP-3C Orion aircraft. In the end, the Australian government decided not to proceed and canceled the order. In 2012, a procurement effort for seven UAVs by 2019 was initiated. In May 2013 the Australian government confirmed its interest in acquiring the MQ-4C Triton maritime surveillance variant.\n\nCanada has also been a potential customer, looking at the Global Hawk for maritime and land surveillance as either a replacement for its fleet of CP-140 Aurora patrol aircraft or to supplement manned patrols of remote Arctic and maritime environments, before withdrawing from the joint effort in August 2011. Spain has a similar requirement, and has existing contacts with Northrop Grumman.\n\nOn 24 August 2013, Japan announced that the Japan Air Self-Defense Force planned to operate one Global Hawk jointly with the U.S. by 2015. On 21 November 2014, the Japanese Ministry of Defense officially decided to procure the Global Hawk, which beat out the General Atomics Guardian ER; Japan has also been interested in the purchase of three aircraft.\n\nIn 2011, South Korea's Defense Acquisition Program Administration (DAPA) expressed interest in acquiring at least four RQ-4Bs to increase intelligence capabilities following the exchange of the Wartime Operational Control from the U.S. to the Republic of Korea. Officials debated on the topic of the Global Hawks and domestic UAV programs. In September 2011, the US and South Korea discussed aircraft deployments near its land border to view North Korea and the North Korea–China border. In January 2012, DAPA announced that it would not proceed with a purchase due to a price rise from US$442M to US$899M, and that other platforms such as the Global Observer or the Phantom Eye were being investigated. However, in December 2012, South Korea notified Congress of a possible Foreign Military Sale of 4 RQ-4 Block 30 (I) Global Hawks with the Enhanced Integrated Sensor Suite (EISS) at an estimated cost of $1.2 billion. On 5 July 2013, the Korean National Assembly advised the government to re-evaluate the RQ-4 purchase, again citing high costs. On 17 December 2014, Northrop Grumman was awarded a $657 million contract by South Korea for four RQ-4B Block 30 Global Hawks, the first pair to be delivered in 2018 and the second pair in 2019.\n\nThe New Zealand Defence Force is studying the Global Hawk, which has the range to conduct surveillance in the Southern Ocean around Antarctica, and in the Pacific Islands. The acquisition process has not moved beyond an expression of interest.\n\nThe Indian Navy has expressed interest in acquiring six to eight MQ-4C Maritime Surveillance Unmanned Aircraft Systems.\n\nIn September 2018, Transport Canada was looking into buying a former German Air Force EuroHawk for surveillance missions in the Arctic. The EuroHawk cannot currently fly and has no equipement inside such as GPS and navigation tool.\n\n\n\n\n\n\n\"This article contains material that originally came from the web article \"Unmanned Aerial Vehicles\" by Greg Goebel, which exists in the Public Domain.\"\n"}
{"id": "232315", "url": "https://en.wikipedia.org/wiki?curid=232315", "title": "Numerically controlled oscillator", "text": "Numerically controlled oscillator\n\nA numerically controlled oscillator (NCO) is a digital signal generator which creates a synchronous (i.e. clocked), discrete-time, discrete-valued representation of a waveform, usually sinusoidal. NCOs are often used in conjunction with a digital-to-analog converter (DAC) at the output to create a direct digital synthesizer (DDS).\n\nNumerically controlled oscillators offer several advantages over other types of oscillators in terms of agility, accuracy, stability and reliability. NCOs are used in many communications systems including digital up/down converters used in 3G wireless and software radio systems, digital PLLs, radar systems, drivers for optical or acoustic transmissions, and multilevel FSK/PSK modulators/demodulators.\n\nAn NCO generally consists of two parts: \n\nWhen clocked, the phase accumulator (PA) creates a modulo-2 sawtooth waveform which is then converted by the phase-to-amplitude converter (PAC) to a sampled sinusoid, where N is the number of bits carried in the phase accumulator. N sets the NCO frequency resolution and is normally much larger than the number of bits defining the memory space of the PAC look-up table. If the PAC capacity is 2, the PA output word must be truncated to M bits as shown in Figure 1. However, the truncated bits can be used for interpolation. The truncation of the phase output word does not affect the frequency accuracy but produces a time-varying periodic phase error which is a primary source of spurious products. Another spurious product generation mechanism is finite word length effects of the PAC output (amplitude) word.\n\nThe frequency accuracy relative to the clock frequency is limited only by the precision of the arithmetic used to compute the phase. NCOs are phase- and frequency-agile, and can be trivially modified to produce a phase-modulated or frequency-modulated output by summation at the appropriate node, or provide quadrature outputs as shown in the figure.\n\nA binary phase accumulator consists of an N-bit binary adder and a register configured as shown in Figure 1. Each clock cycle produces a new N-bit output consisting of the previous output obtained from the register summed with the frequency control word (FCW) which is constant for a given output frequency. The resulting output waveform is a staircase with step size formula_1, the integer value of the FCW. In some configurations, the phase output is taken from the output of the register which introduces a one clock cycle latency but allows the adder to operate at a higher clock rate. \nThe adder is designed to overflow when the sum of the absolute value of its operands exceeds its capacity (2−1). The overflow bit is discarded so the output word width is always equal to its input word width. The remainder formula_2, called the residual, is stored in the register and the cycle repeats, starting this time from formula_2 (see figure 2). Since a phase accumulator is a finite state machine, eventually the residual at some sample K must return to the initial value formula_4. The interval K is referred to as the grand repetition rate (GRR) given by \n\nwhere GCD is the greatest common divisor function. The GRR represents the true periodicity for a given formula_1 which for a high resolution NCO can be very long. Usually we are more interested in the \"operating frequency\" determined by the average overflow rate, given by\n\nThe \"frequency resolution\", defined as the smallest possible incremental change in frequency, is given by\n\nEquation (1) shows that the phase accumulator can be thought of as a programmable non-integer frequency divider of divide ratio formula_9.\n\nThe phase-amplitude converter creates the sample-domain waveform from the truncated phase output word received from the PA. The PAC can be a simple read only memory containing 2 contiguous samples of the desired output waveform which typically is a sinusoid. Often though, various tricks are employed to reduce the amount of memory required. This include various trigonometric expansions, trigonometric approximations and methods which take advantage of the quadrature symmetry exhibited by sinusoids. Alternatively, the PAC may consist of random access memory which can be filled as desired to create an arbitrary waveform generator.\n\nSpurious products are the result of harmonic or non-harmonic distortion in the creation of the output waveform due to non-linear numerical effects in the signal processing chain. Only numerical errors are covered here. For other distortion mechanisms created in the digital-to-analog converter see the corresponding section in the direct-digital synthesizer article.\n\nThe number of phase accumulator bits of an NCO (N) is usually between 16 and 64. If the PA output word were used directly to index the PAC look-up table an untenably high storage capacity in the ROM would be required. As such, the PA output word must be truncated to span a reasonable memory space. Truncation of the phase word causes phase modulation of the output sinusoid which introduces non-harmonic distortion in proportion to the number of bits truncated. The number of spurious products created by this distortion is given by:\n\nwhere W is the number of bits truncated.\n\nIn calculating the spurious-free dynamic range, we are interested in the spurious product with the largest amplitude relative to the carrier output level given by:\n\nwhere P is word width of the DAC. For W >4,\n\nAnother related spurious generation method is the slight modulation due to the GRR outlined above. The amplitude of these spurs is low for large N and their frequency is generally too low to be detectable but they may cause issues for some applications.\n\nAnother source of spurious products is the amplitude quantization of the sampled waveform contained in the PAC look up table(s). If the number of DAC bits is P, the\nAM spur level is approximately equal to −6.02 P − 1.76 dBc.\n\nPhase truncation spurs can be reduced substantially by the introduction of white gaussian noise prior to truncation. The so-called dither noise is summed into the lower W+1 bits of the PA output word to linearize the truncation operation. Often the improvement can be achieved without penalty because the DAC noise floor tends to dominate system performance. Amplitude truncation spurs can not be mitigated in this fashion. Introduction of noise into the static values held in the PAC ROMs would not eliminate the cyclicality of the truncation error terms and thus would not achieve the desired effect.\n\n"}
{"id": "15318665", "url": "https://en.wikipedia.org/wiki?curid=15318665", "title": "Patton Design", "text": "Patton Design\n\nPatton Design, Inc. is an industrial design, engineering, software and hardware, and prototyping consultancy based in Irvine, California.The firm was founded in 1983 by California State University at Long Beach alumnus Doug Patton.\n\nCompany staff appeared on the ABC television series \"American Inventor\" and represented the million dollar winner with the \"Anecia Survival Capsule\" child safety seat, the second place winner with \"The WordAce\" children's game, and the third place winner with \"The Catch Elite\" football receiver trainer.\n\nPatton Design is a patron member of the Industrial Designers Society of America and has won seventeen Industrial Design Excellence Awards.\n\n"}
{"id": "770546", "url": "https://en.wikipedia.org/wiki?curid=770546", "title": "Piping", "text": "Piping\n\nWithin industry, piping is a system of pipes used to convey fluids (liquids and gases) from one location to another. The engineering discipline of piping design studies the efficient transport of fluid.\n\nIndustrial process piping (and accompanying in-line components) can be manufactured from wood, fiberglass, glass, steel, aluminum, plastic, copper, and concrete. The in-line components, known as fittings, valves, and other devices, typically sense and control the pressure, flow rate and temperature of the transmitted fluid, and usually are included in the field of piping design (or piping engineering). Piping systems are documented in piping and instrumentation diagrams (P&IDs). If necessary, pipes can be cleaned by the tube cleaning process.\n\n\"Piping\" sometimes refers to piping design, the detailed specification of the physical piping layout within a process plant or commercial building. In earlier days, this was sometimes called drafting, technical drawing, engineering drawing, and design, but is today commonly performed by designers that have learned to use automated computer-aided drawing or computer-aided design (CAD) software.\n\nPlumbing is a piping system with which most people are familiar, as it constitutes the form of fluid transportation that is used to provide potable water and fuels to their homes and businesses. Plumbing pipes also remove waste in the form of sewage, and allow venting of sewage gases to the outdoors. Fire sprinkler systems also use piping, and may transport nonpotable or potable water, or other fire-suppression fluids.\n\nPiping also has many other industrial applications, which are crucial for moving raw and semi-processed fluids for refining into more useful products. Some of the more exotic materials used in pipe construction are Inconel, titanium, chrome-moly and various other steel alloys.\n\nGenerally, industrial piping engineering has three major sub-fields:\n\nProcess piping and power piping are typically checked by pipe stress engineers to verify that the routing, nozzle loads, hangers, and supports are properly placed and selected such that allowable pipe stress is not exceeded under different loads such as sustained loads, operating loads, pressure testing loads, etc., as stipulated by the ASME B31, EN 13480 or any other applicable codes and standards. It is necessary to evaluate the mechanical behavior of the piping under regular loads (internal pressure and thermal stresses) as well under occasional and intermittent loading cases such as earthquake, high wind or special vibration, and water hammer. This evaluation is usually performed with the assistance of a specialized (finite element) pipe stress analysis computer programs such as AutoPIPE, CAEPIPE, and CAESAR.\n\nIn cryogenic pipe supports, most steel become more brittle as the temperature decreases from normal operating conditions, so it is necessary to know the temperature distribution for cryogenic conditions. Steel structures will have areas of high stress that may be caused by sharp corners in the design, or inclusions in the material.\n\nThe material with which a pipe is manufactured often forms as the basis for choosing any pipe. Materials that are used for manufacturing pipes include:\n\n\nEarly wooden pipes were constructed out of logs that had a large hole bored lengthwise through the center. Later wooden pipes were constructed with staves and hoops similar to wooden barrel construction. Stave pipes have the advantage that they are easily transported as a compact pile of parts on a wagon and then assembled as a hollow structure at the job site. Wooden pipes were especially popular in mountain regions where transport of heavy iron or concrete pipes would have been difficult.\n\nWooden pipes were easier to maintain than metal, because the wood did not expand or contract with temperature changes as much as metal and so consequently expansion joints and bends were not required. The thickness of wood afforded some insulating properties to the pipes which helped prevent freezing as compared to metal pipes. Wood used for water pipes also does not rot very easily. Electrolysis doesn't affect wood pipes at all, since wood is a much better electrical insulator.\n\nIn the Western United States where redwood was used for pipe construction, it was found that redwood had \"peculiar properties\" that protected it from weathering, acids, insects, and fungus growths. Redwood pipes stayed smooth and clean indefinitely while iron pipe by comparison would rapidly begin to scale and corrode and could eventually plug itself up with the corrosion.\n\nThere are certain standard codes that need to be followed while designing or manufacturing any piping system. Organizations that promulgate piping standards include:\n\n\n"}
{"id": "56424423", "url": "https://en.wikipedia.org/wiki?curid=56424423", "title": "Q multiplier", "text": "Q multiplier\n\nIn electronics, a Q multiplier is a circuit added to a radio receiver to improve its selectivity and sensitivity. It is a regenerative amplifier adjusted to provide positive feedback within the receiver. This has the effect of narrowing the receiver's bandwidth, as if the Q factor of its tuned circuits had been increased. The Q multiplier was a common accessory in shortwave receivers of the vacuum tube era as either a factory installation or an add-on device. In use, the Q multiplier had to be adjusted to a point just short of oscillation to provide maximum sensitivity and rejection of interfering signals. \n\nA Q multiplier could also be adjusted to act as a notch filter, useful for reducing the interfering effect of signals on frequencies near to the desired signal. In some receiver designs, the Q multiplier was made to also serve as a beat frequency oscillator by adjusting it to oscillate. This could be used for reception of single sideband or Morse radiotelegraphy, but in that case the circuit no longer provided improved selectivity. \n\nThe principle of regeneration applied to radio receivers was developed by Edwin Armstrong, who patented a regenerative receiver in 1914. At least one console-model broadcast superheterodyne receiver used positive feedback to improve selectivity in a 1926 design. Q-multipliers were common on shortwave general-coverage and communications receivers of the 1950s. With the advent of crystal and ceramic intermediate frequency filters, the Q-multiplier was no longer popular. \n"}
{"id": "12387162", "url": "https://en.wikipedia.org/wiki?curid=12387162", "title": "R-390A", "text": "R-390A\n\nThe R-390A /URR is a general coverage HF radio communications receiver designed by Collins Radio Company for the US military.\n\nThe R-390A military shortwave radio receiver was the result of a project undertaken by the U.S. Army Signal Corps in 1954 to replace the existing R-390 receiver then in use. The R-390 had done its job so well that the Corps decided continued use of this type of receiver necessitated an improved, reduced-cost version. There are many references to the R390A in the open literature during this period; a picture of the receiver appeared in the May 1959 issue of the amateur radio magazine QST.\n\nTotal production of the R-390A (as determined by the high serial numbers noted) is over 55,000 units. Initial production started in 1955 and ran through approximately 1970, and then was restarted in 1984 by Fowler Industries for Avondale Shipyards. Manufacturers and their approximate production numbers are:\n\n\nCompanies which made spare modules, but not whole sets were Communications Systems Corp., Clavier Corp. and Hacking Labs.\n\nThe R-390A is a general coverage radio receiver capable of receiving amplitude modulated, code, and frequency shift keying signals. Its tuning range is from 500 kilohertz to 32 megahertz, in 32 one-megahertz bands. The circuit is the superheterodyne type, double conversion above 8 MHz, below which triple conversion is used. It employs 23 vacuum tubes (6AK6 x 3, 5654 x 2, 12AU7/5814A x 4, 26Z5W x 2, 3TF7 x 1, 6BA6/5749W x 6, 6C4/6100 x 3, 6DC6 x 1, 0A2WA x 1), a larger than normal count for most general-coverage receivers. The receiver weighs 85 pounds and can be operated on 120 volt or 240 volt supplies. It fits neatly into a 10.5 inch-tall standard 19 inch equipment rack.\n\nTuning of the R-390A's radio frequency and intermediate frequency front end is synchronized by means of an ingenious mechanical system of racks, gears, and cams. When the front panel tuning controls are rotated, this system raises and lowers ferrite slugs in and out of the receiver's tuning coils. This ensures that all front-end circuits are tracked, meaning all circuits are tuned to the correct frequency to maintain excellent selectivity and sensitivity. The receiver's construction is modular for easy servicing. Each major area of the receiver is contained in easily removable subassemblies, and these can be repaired or replaced as needs be. Though the R-390A is mechanically and electrically complex, alignment and servicing were designed to follow simplified procedures published by the Signal Corps.\n\nThe R-390A was deployed to most branches of the US military and remained in general use through the 1980s. The last major update to its documentation was in 1984. As the military procured newer receivers, many R-390As were released to surplus while others were destroyed. Some receivers were retained by the services, however, when they found that the R-390A's vacuum tube circuitry could easily survive an electromagnetic pulse. There are reports, possibly apocryphal, that R-390A receivers are still in use aboard U.S. Navy submarines since the receiver can withstand the strong radio frequency fields found aboard ship.\n\nMany of the R-390As that exist today are in the hands of vintage amateur radio collectors and amateur radio operators who contend that few modern solid state communications receivers can equal its performance. There is a wealth of information, both printed and electronic, devoted to R-390A restoration and maintenance, as the R-390A is widely considered an example of the best of vacuum tube technology.\n\n\n\n"}
{"id": "22373821", "url": "https://en.wikipedia.org/wiki?curid=22373821", "title": "Reciprocal frame", "text": "Reciprocal frame\n\nA reciprocal frame is a class of self-supporting structure made of three or more beams and which requires no center support to create roofs, bridges or similar structures.\n\nA reciprocal roof is assembled by first installing a temporary central support that holds the first rafter at the correct height. The first rafter is fitted between the wall and the temporary central support and then further rafters are added, each resting on the last. The final rafter fits on top of the previous rafter and under the very first one. The rafters are then tied with wire before the temporary support is removed. The failure of a single element may lead to the failure of the whole structure.\n\nThe reciprocal frame, also known as a Mandala roof, has been used since the twelfth century in Chinese and Japanese architecture although little or no trace of these ancient methods remain. More recently they were used by architects Kazuhiro Ishii (the Spinning House) and Yasufumi Kijima, and engineer Yoishi Kan\n(Kijima Stonemason Museum). \n\nVillard de Honnecourt produced sketches showing similar designs in the 13th century and similar structures were also used in the chapter house of Lincoln Cathedral. Josep Maria Jujol used this structure in both the Casa Bofarull and Casa Negre\n\nTwo reciprocal frame illustrations are shown on a page of illustrations from an 1813 book in the article Timber roof truss\n\nhttps://sites.google.com/site/charpentejuxtaposee/\n\n"}
{"id": "56188331", "url": "https://en.wikipedia.org/wiki?curid=56188331", "title": "Sasaki Associates", "text": "Sasaki Associates\n\nSasaki is a global design firm specializing in architecture, interior design, planning, urban design, landscape architecture, strategic planning, graphic design, and civil engineering.\n\nSasaki was founded in 1953 by acclaimed landscape architect, Hideo Sasaki in Watertown, Massachusetts, while he served as a professor and landscape architecture department chair at the Harvard Graduate School of Design. When Sasaki partnered with Peter Walker in 1957 the firm became Sasaki, Walker and Associates. In 1959, Walker opened up a new branch of the firm in San Francisco, and in 1964 the Watertown location was renamed Sasaki, Dawnson & DeMay.\n\nSasaki, Dawson and DeMay redesigned Copley Square in 1970. The firm's work includes Christopher Columbus Park (also called the Waterfront Park) and the plaza at the First Church of Christ.\n\nSasaki won an international design competition for the 2008 Beijing Olympics Green. The design included over 1,700 acres of woods and wetlands. Alan Ward was the lead landscape architect. Sasaki principal Dennis Pieprz also wanted the Olympic Green design to be useful in the long-term. The Olympic Forest Park is Beijing's largest public green space.\n\nLed by Sasaki Principals Michael Grove, Mark Dawson, and Tao Zhang, the firm opened its Shanghai office in 2012 where it worked on influential projects including the 798 Art Zone, the master plan for Suzhou Creek, and a vision plan for the Songzhuang art colony.\n"}
{"id": "87493", "url": "https://en.wikipedia.org/wiki?curid=87493", "title": "Snowshoe", "text": "Snowshoe\n\nA snowshoe is footwear for walking over snow. Snowshoes work by distributing the weight of the person over a larger area so that the person's foot does not sink completely into the snow, a quality called \"flotation\". Snowshoeing is a form of hiking.\n\nTraditional snowshoes have a hardwood frame with rawhide lacings. Some modern snowshoes are similar, but most are made of materials such as lightweight metal, plastic, and synthetic fabric. In addition to distributing the weight, snowshoes are generally raised at the toe for maneuverability. They must not accumulate snow, hence the latticework, and require bindings to attach them to the feet.\n\nIn the past, snowshoes were essential tools for fur traders, trappers and anyone whose life or living depended on the ability to get around in areas of deep and frequent snowfall, and they remain necessary equipment for forest rangers and others who must be able to get around areas inaccessible to motorized vehicles when the snow is deep. However, snowshoes are mainly used today for recreation, primarily by hikers and runners who like to continue their hobby in wintertime. Snowshoeing is easy to learn and in appropriate conditions is a relatively safe and inexpensive recreational activity. However, snowshoeing in icy, steep terrain can be more dangerous.\n\nBefore people built snowshoes, nature provided examples. Several animals, most notably the snowshoe hare, had evolved over the years with oversized feet enabling them to move more quickly through deep snow.\n\nThe origin and age of snowshoes are not precisely known, although historians believe they were invented from 4,000 to 6,000 years ago, probably starting in Central Asia. British archaeologist Jacqui Wood hypothesized that the equipment interpreted to be the frame of a backpack of the Chalcolithic mummy Ötzi was actually part of a snowshoe. Strabo wrote that the inhabitants of the Caucasus used to attach flat surfaces of leather under their feet and that its inhabitants used round wooden surfaces, something akin to blocks, instead. However, the \"traditional\" webbed snowshoe as we know it today had direct origins to North American indigenous people, e.g., the Huron, Cree, and so forth. Samuel de Champlain wrote, referencing the Huron and Algonquin First Nations, in his travel memoirs (V.III, pg. 164), \"Winter, when there is much snow, they (the Indians) make a kind of snowshoe that are two to three times larger than those in France, that they tie to their feet, and thus go on the snow, without sinking into it, otherwise they would not be able to hunt or go from one location to the other\".\n\nTwo groups of snowshoe pioneers diverged early on, setting patterns that can still be seen today. One group abandoned the snowshoe as it migrated north to what is now Scandinavia, eventually turning the design into the forerunners of the Nordic ski. The other went northeast, eventually crossing the Bering Strait into North America.\n\nIn 2016, Italian scientists reported \"the oldest snowshoe in the world\" discovered in the Dolomites and dated to between 3800 and 3700 B.C.\n\nThe indigenous people of North America developed the most advanced and diverse snowshoes prior to the 20th century. Nearly every North American aboriginal culture developed its own particular shape of shoe, the simplest and most primitive being those of the far north. The Inuit have two styles, one being triangular in shape and about in length, and the other almost circular, both reflecting the need for high flotation in deep, loose and powdery snow. However, contrary to popular perception, the Inuit did not use their snowshoes much since they did most of their foot travel in winter over sea ice or on the tundra, where snow does not pile up deeply.\n\nSouthward the shoe becomes gradually narrower and longer, the largest being the hunting snow-shoe of the Cree, which is nearly long and turned up at the toe. Even smaller models, developed most notably by the Iroquois, are narrower and shorter, reflecting the need for maneuverability in forested areas.\n\nThe Plains Indians wore snowshoes on their winter season bison hunts before horses were introduced. Despite their great diversity in form, snowshoes were, in fact, one of the few cultural elements common to all tribes that lived where the winters were snowy, in particular, the Northern regions.\n\nSnowshoes were slowly adopted by Europeans in what became Canada and the United States, with the French \"voyageurs\" and coureurs des bois well in advance of British settlers. According to the Encyclopædia Britannica, French Voyageurs were primarily 18th and 19th century French Canadian fur traders who explored the frontier waterways by canoe. The Europeans adopted the use of snowshoes through the French and Indian Wars. Superior French snowshoeing skill almost turned the French and Indian War, a conflict that saw two engagements named the Battle on Snowshoes, to their favor.\n\nHowever, the British were quick learners. The \"Oxford English Dictionary\" reports the term being used by the English as early as 1674. Sixteen years later, after a French-Indian raiding party attacked a British settlement near what is today Schenectady, New York, the British took to their own snowshoes and pursued the attackers for almost , ultimately recovering both people and goods taken by their attackers.\n\nThe \"teardrop\" snowshoes worn by lumberjacks are about long and broad in proportion, while the tracker's shoe is over long and very narrow. This form, the stereotypical snowshoe, resembles a tennis racquet, and indeed the French term is \"raquette à neige\".\n\nThis form was copied by the Canadian snowshoe clubs of the late 18th century. Founded for military training purposes, they became the earliest recreational users of snowshoes.\n\nThe snowshoe clubs such as the Montreal Snow Shoe Club (1840) shortened the teardrop to about long and broad, slightly turned up at the toe and terminating in a kind of tail behind. This is made very light for racing purposes, but much stouter for touring or hunting. The tail keeps the shoe straight while walking.\n\nAnother variant, the \"bearpaw\", ends in a curved heel instead of a tail. While many early enthusiasts found this more difficult to learn on, as they were thicker in the middle and rather cumbersome, they did have the advantage of being easier to pack and nimbler in tight spaces. Two forms of traditional bearpaw snowshoes developed: an eastern version used by \"spruce gummers\" consisting of an oval frame with wooden cross braces, and a western version with a rounded triangular frame and no wooden bracing.\n\nTraditional snowshoes are made of a single strip of some tough wood, usually white ash, curved round and fastened together at the ends and supported in the middle by a light cross-bar. The space within the frame is filled with a close webbing of dressed caribou or neat's-hide strips, leaving a small opening just behind the cross-bar for the toe of the moccasined foot. They are fastened to the moccasin by leather thongs, sometimes by buckles. Such shoes are still made and sold by native peoples.\n\nOutside of indigenous populations and some competitions such as Arctic Winter Games, very few of the old-fashioned snowshoes are actually used by enthusiasts anymore, although some value them for the artisanship involved in their construction. They are sometimes seen as decorations, mounted on walls or on mantels in ski lodges.\n\nEven though many enthusiasts prefer aluminum snowshoes, there is still a large group of snowshoe enthusiasts who prefer wooden snowshoes. Wooden frames do not freeze as readily. Many enthusiasts also prefer wood snowshoes because they are very quiet.\n\nWhile recreational use of snowshoes began with snowshoe clubs in Quebec, Canada (who held events where races and hikes were combined with fine food and drink), the manufacture of snowshoes for recreational purposes really began in the late 19th century, when serious recreational use became more widespread.\n\nIn the late 20th century the snowshoe underwent a radical redesign. It started in the 1950s when the Vermont-based Tubbs company created the Green Mountain Bearpaw, which combined the shortness of that style with an even narrower width than had previously been used (Pospisil 1979). This rapidly became one of the most popular snowshoes of its day.\n\nIn 1972, experimenting with new designs in Washington's Cascade Mountains, Gene and Bill Prater created the snowshoe known today. They began using aluminum tubing and replaced the lace with neoprene and nylon decking. To make them easier to use in mountaineering, the Praters developed a hinged binding and added cleats to the bottom of the shoe.\n\nThe Sherpa Snowshoe company started manufacturing these \"Western\" shoes and they proved very popular. Eastern snowshoers were a bit more skeptical at first, believing that the style was unnecessary in the east, until the Praters demonstrated their improved effectiveness on New Hampshire's Mount Washington.\n\nThese use an aluminum or stainless steel frame and take advantage of technical advances in plastics and injection molding to make a lighter and more durable shoe. They require little maintenance, and usually incorporate aggressive crampons.\n\nSome, such as the \"Denali\" model made by Mountain Safety Research, use no metal frame and can be fitted with optional detachable tail extenders. Newer models have heel-lifters, called \"ascenders\", that flip up to facilitate hill climbing.\n\nThe use of solid decking in place of the standard latticework of lacing came as a surprise to many enthusiasts, since it challenged a long-held belief that the lattice was necessary to prevent snow from accumulating on the shoe. In practice, however, it seems that very little snow comes through the openings in either type of shoe.\n\nNeoprene/nylon decks also displayed superior water resistance, neither stretching as rawhide will when wet nor requiring annual treatment with spar varnish, features that were immediately appreciated. Eventually they were replaced with even lighter materials such as polypropylene. This tendency also gave way to the creation of inflatable snowshoes made of different fabrics such as cordura and thermoplastic polyurethane (TPU).\n\nThese more athletic designs have helped the sport enjoy a renaissance after a period of eclipse when winter recreationists showed more interest in skiing. In the U.S., the number of snowshoers tripled during the 1990s.\n\nIn fact, ski resorts with available land are beginning to offer snowshoe trails to visitors, and some popular hiking areas are almost as busy in the colder months as they are on warm summer weekends.\n\nAs many winter recreationists rediscover snowshoeing, many more new models of snowshoe are becoming available. Ski areas and outdoor equipment stores are offering snowshoes for rent.\n\nSnowshoes today are divided into three types:\n\nSizes are often given in inches, even though snowshoes are nowhere near perfectly rectangular. Mountaineering shoes can be at least long by wide; a lighter pair of racing shoes can be slightly narrower and or shorter.\n\nRegardless of configuration, all wooden shoes are referred to as \"traditional\" and all shoes made of other materials are called \"modern.\"\n\nNotwithstanding these variations in planned use, larger users should plan on buying larger snowshoes. A common formula is that for every pound of body weight, there should be one square inch of snowshoe surface (14.5 cm²/kg) per snowshoe to adequately support the wearer. Users should also consider the weight of any gear they will be packing, especially if they expect to break trail. Those planning to travel into deep powder look for even larger shoes.\n\nMany manufacturers now include weight-based flotation ratings for their shoes, although there is no standard for setting this yet.\n\nAs is often the case with downhill skis, wood-frame snowshoes and suitable bindings are typically marketed and purchased separately rather than as a single piece. One common style is termed the \"H\" binding, as it consists of a strap around the heel crossing a strap around the toe and one at the instep, forming a rough version of the eponymous letter.\n\nOn modern shoes, there are two styles of binding: fixed-rotation (also known as \"limited-rotation\") bindings, and full-rotation (also known as \"pivot\") bindings. With either binding system, the heel is left free, and the difference is in how the ball of the foot is attached to the snowshoe.\n\nIn fixed-rotation bindings, the binding is attached to the snowshoe with an elastic strap that brings the tail of the snowshoe up with each step. The snowshoe therefore moves with the foot and the tail does not drag. Fixed-rotation bindings are preferred for racing. Full-rotation bindings allow the user's toes to pivot below the deck of the snowshoe. They allow the crampon cleats that are under the foot to be kicked into a slope for grip in climbing, but are relatively awkward for stepping sideways and backwards as the tail of the snowshoe can drag. Fixed-rotation bindings often cause snow to be kicked up the back of the wearer's legs; this does not tend to happen with full-rotation bindings.\n\nA series of straps, usually three, are used to fasten the foot to the snowshoe. Some styles of binding use a cup for the toe. It is important that a user be able to manipulate these straps easily, as removing or securing the foot often must be done outdoors in cold weather with bare hands, exposing him or her to the possibility of frostbite. When putting on snowshoes, left is distinguished from right by which way the loose ends of the binding straps point: always outward, to avoid stepping on them repeatedly.\n\nIn 1994, Bill Torres and a younger associate developed the step-in binding, designed to make it easier for snowshoers wearing hard-shelled plastic boots (serious mountaineers) to change from snowshoes to crampons and back again as needed.\n\nSnowshoers often use trekking poles as an accessory to help them keep their balance on the snow. Some manufacturers have begun making special snowshoeing models of their poles, with larger baskets more like those found on ski poles (which can also be used).\n\nOther than that, no other special accessories are required. Most types of footwear can be worn with snowshoes, although hiking boots are the preferred choice among most recreational users (except racers, who prefer running shoes). Ski boots, however, will only work with certain snowshoes such as the MSR Denali, otherwise requiring backcountry skiers to carry other footwear for the snowshoe portion of their trip.\n\nIf going into deep snow, snowshoers will often take along gaiters to keep snow from getting into their boots from above. Some manufacturers make their snowshoes with boot or toe covers to provide the same protection.\n\nA carrier of some type is also advisable, particularly if the trip will not take place entirely on snowshoes. Some backpack manufacturers have designed special packs with \"daisy chains,\" strips of looped nylon webbing on which the shoes can be secured. Snowshoe manufacturers, too, have begun including carriers and tote bags for their products, if for no other reason than to prevent the often-sharp cleats on the bottom from damaging surfaces they come in contact with.\n\nSince snowshoeing is commonly done in cold weather, users typically prepare for it by dressing in layers and carrying the appropriate equipment.\n\nSnowshoes function best when there is enough snow beneath them to pack a layer between them and the ground, usually at a depth of or more. However, contrary to popular belief, snowshoes perform poorly on very icy and steep terrain. Compared to crampons, snowshoes give relatively little grip on ice. It is common for novice snowshoers to climb up a steep slope to a summit and then have difficulty climbing back down, which tends to be more difficult than ascending. In icy conditions, summer hiking routes may require mountaineering skills and equipment, not snowshoes.\n\nIt is often said by snowshoers that if you can walk, you can snowshoe. This is true in optimal conditions, but snowshoeing properly requires some slight adjustments to walking. The method of walking is to lift the shoes slightly and slide the inner edges over each other, thus avoiding the unnatural and fatiguing \"straddle-gait\" that would otherwise be necessary. A snowshoer must be willing to roll his or her feet slightly as well. An exaggerated stride works best when starting out, particularly with larger or traditional shoes.\n\nWalking skills are easily transferable to straightforward snowshoe travel, but this is not always the case with turning around. While a snowshoer with space to do so can, and usually does, simply walk in a small semicircle, on a steep slope or in close quarters such as a boreal forest this may be impractical or impossible. It is thus necessary in such circumstances to execute a \"kick turn\" similar to the one employed on skis: lifting one foot high enough to keep the entire snowshoe in the air while keeping the other planted, putting the foot at a right angle to the other (or as close as possible for the situation and the snowshoer's physical comfort), then planting it on the snow and quickly repeating the action with the other foot. This is much easier to accomplish with poles.\n\nWhile the cleating and traction improvements to modern snowshoes have greatly enhanced snowshoers' climbing abilities, on very steep slopes it is still beneficial to make \"kick steps,\" kicking the toes of the shoes into the snow to create a kind of snow stairs for the next traveler to use.\n\nAlternatively, snowshoers can use two techniques borrowed from skis: the herringbone (walking uphill with the shoes spread outward at an angle to increase their support) and the sidestep.\n\nFor those snowshoers who use poles, it can be easier to rely on the poles to 'pull' oneself with regular stride, up the slope.\n\nOnce a trail has been broken up a mountain or hill, snowshoers often find a way to speed up the return trip that manages to also be fun and rests the leg muscles: \"glissading\" the trail, or sliding down on their buttocks. This does not damage the trail, and in fact helps pack the snow better for later users.\n\nIn situations where they must break trail downhill and thus cannot glissade, snowshoers sometimes run downhill in exaggerated steps, sliding slightly on the snow as they do, an option sometimes called \"step sliding.\" Also effective, are poles placed in front as you descend in a regular stride. If carrying poles and properly experienced, they can also employ skiing techniques such as telemarking.\n\nOn newly fallen snow it is necessary for a snowshoer to \"break\" a trail. This is tiring (it may require up to 50% more energy than simply following behind) even on level terrain, and frequently in groups this work is shared among all participants.\n\nA trail breaker can improve the quality of the ensuing route by using a technique, similar to the hiking rest step, called \"stamping\": pausing momentarily after each step before putting full weight on the foot. This helps smooth the snow underneath and compacts it even better for the next user.\n\nA well-broken trail is usually a rut in the snow about deep and wide. While it may appear after heavy use as if it is possible to \"bareboot\" or walk it without benefit of snowshoes, this practice is frowned upon by serious snowshoers as it leads to \"postholing,\" or roughening of the trail from places where boots have fallen through (initial appearances to the contrary, the snow in a broken trail is not sufficiently packed to support the more concentrated weight of a foot).\n\nIn soft conditions, following trails broken by backcountry skiers can be difficult on snowshoes. In addition, since snowshoes destroy ski tracks, many areas ask that snowshoers observe traditional backcountry courtesy and stay out of ski tracks. Ski trails are normally much narrower than a typical snowshoe trail, and less well packed because skis offer more flotation than snowshoes. If the snow is deep and soft, snowshoers may find themselves postholing right through the ski track. In most cases the ski track offers little advantage and putting in a separate snowshoe track allows both snowshoers and skiers to have a positive experience and avoids friction with skiers who often resent having their tracks obliterated and their skiing enjoyment greatly reduced.\n\nSnowshoeing expands the potential for exercise available in the wintertime. , at least 500 American schools, mostly but not exclusively in the Northeast have started offering snowshoe programs in their physical education classes to help combat obesity. It had the added benefit of being gentler on the feet than walking or running the equivalent routes, since snow cushions the foot's impact.\n\nFor the same reason, it is less detrimental to the environment, since the snow likewise buffers the earth against the impact of so many hikers and campers, cutting back on trail erosion and other effects of heavy use.\n\nSnowshoeing makes even familiar hikes different and new. If the snow is deep enough, obstacles such as large boulders and fallen logs can be more easily bypassed.\n\nImmoderate snowshoeing leads to serious lameness of the feet and ankles which Canadian \"voyageurs\" called \"mal de raquette\". Modern snowshoes are much lighter and more comfortable so that lameness caused by snowshoeing is now very rare.\n\nNonetheless, many snowshoers find that their legs, particularly their calf muscles, take some time to get used to snowshoeing again at the start of each winter. Frequently the first serious trip leaves them sore for several days afterwards.\n\nThe resurgence of interest in snowshoeing in the late 20th century was in some part due to snowboarders, who took to them as a way to reach backcountry powder bowls and other areas while they were still banned from most ski areas. Their similarities to snowboards, in shape and binding, led many of them to continue use even after snowboarders were allowed to use most ski slopes. Despite most ski areas now allowing snowboarders, there is a growing interest in backcountry and sidecountry snowboarding in the search for fresh powder. The recent development of splitboards has enabled snowboarders to access backcountry without the need for snowshoes.\n\nDownhill skiers, too, found snowshoes useful in reaching the same areas.\n\nAnother popular expedition, particularly among hikers, is the \"ski-shoe\" trip combining a cross-country ski portion on a level, wide trail with a snowshoe up a less skiable section, usually to a mountain summit.\n\nRunners have found that using light snowshoes allows them to continue exercising and racing during winter. Like their warm-weather counterparts, events cover all distances, from sprints of 100 m to the 100 km \"Iditashoe.\" There are even hurdle events.\n\nSnowshoe segments have become common in many multi-sport events and adventure races, including a required snowshoe segment in the winter quadrathlon. Some competitors in those events like Sally Edwards and Tom Sobal have emerged as stars.\n\nWhile snowshoe racing has probably been around as long as there have been snowshoes, as an organized sport it is relatively new. The United States Snowshoe Association was founded in 1977 to serve as a governing body for competitive snowshoeing. It is headquartered in Corinth, New York, which considers itself the \"Snowshoe Capital of the World\" as a result. Similar organizations, such as the European Snowshoe Committee and Japan's Chikyu Network, exist in other countries and there is an international competitive level as well.\n\nSnowshoe races are part of the Arctic Winter Games and the winter Special Olympics. However, they are not yet an Olympic event.\n\nThe rawhide webbing of traditional snowshoes, as noted above, needs regular waterproofing. Spar varnish is the preferred waterproofing for traditional snowshoes. A light sanding is preferred before 3 coats of spar varnish is applied. Modern snowshoes need no regular maintenance save a sharpening of cleats if desired.\n\nBoth kinds of snowshoe, however, can and do break. The most common damage suffered is to the frame, which can be splinted with a stick or piece of wood if necessary. Decking rarely gets broken, but if it is punctured and the hole looks as if it might continue to grow, the best solution is the patching kits made for tents.\n\nCable ties can serve many purposes in repairing snowshoes. They can splint frames in a pinch, replace a broken rivet, secure a tie or lace, and repair winter clothing as well.\n\n\n\n"}
{"id": "9154054", "url": "https://en.wikipedia.org/wiki?curid=9154054", "title": "Stationary engineer", "text": "Stationary engineer\n\nA stationary engineer, sometimes called an operating engineer, or a power engineer, operates industrial machinery and equipment that provides energy in various forms.\n\nPower engineers are trained in many areas, including mechanical, thermal, chemical, electrical, metallurgical, computer, and a wide range of safety skills. They typically work in factories, office buildings, hospitals, warehouses, power generation plants, industrial facilities, and residential and commercial buildings. \n\nPower engineers are responsible for the safe operation and maintenance of a wide range of equipment including boilers, steam turbines, gas turbines, pumps, gas compressors, generators, motors, air conditioning systems, heat exchangers, refrigeration equipment, heat recovery steam generators (HRSGs) that may be directly (duct burners) or indirectly fired (gas turbine exhaust heat collectors), hot water generators, and refrigeration machinery in addition to its associated auxiliary equipment (air compressors, natural gas compressors, electrical switchgear, pumps, etc.).\n\nPower engineers may hold various titles, such as boiler attendant or fitter. In Canada, Power Engineers are ranked by classes. Fifth class being the starting point and First class being the highest level attained. However in India there are only two classes – Ist class and 2nd class. This is a national standard in Canada and India but America isn't quite so organized in this regard. In the United States, stationary engineers must be licensed in several cities and states. The New York City Department of Buildings requires a Stationary Engineer's License to practice in the City of New York; to obtain the license one must pass a written and practical exam and have at least five years' experience working directly under a licensed stationary engineer, or one year if in possession of a Bachelor of Science degree in mechanical engineering. Holders of the Stationary Engineer's License primarily work in large power generation facilities, such as cogeneration power plants, peaking units, and large central heating and refrigeration plants (CHRPs). For the State of California, Stationary Engineers are the State of California Military Department's sole source of Airfield Lighting and Repair.\n\nThe stationary engineering trade emerged during the Industrial Revolution. The group includes railroad engineers and marine engineers. Famous people who began their working lives in this trade include George Stephenson and Henry Ford. The early steam engines developed by Thomas Savery and Thomas Newcomen which drew water from mines and the industrial steam engines perfected by James Watt and others employed the ancestors of today's engineers. Railroad engineers operated early steam locomotives and continue to operate trains today. The traditions and classification of the engineer were developed to the greatest extent by marine engineers who worked in the engine rooms of the great ocean liners in the 19th and 20th centuries. The use of the title \"engineer\" by Power engineers has been challenged in court by university-educated professional engineers; however, the stationary engineers have prevailed to date. The job of today's engineer has been greatly changed by computers and automation as well as the replacement of steam engines on ships and trains. Workers have adapted to the challenges of the changing job market. \n\nMany Power engineers are becoming more involved with the technical aspect of the job as it has shifted toward a dependence on building automation. Building and central plant operations are now relying heavily on direct digital controls; and as such the engineer is required to be much more computer literate to work with the BAS (Building Automation System).\n\n"}
{"id": "2613005", "url": "https://en.wikipedia.org/wiki?curid=2613005", "title": "Steam tractor", "text": "Steam tractor\n\nA steam tractor is a vehicle powered by a steam engine which is used for pulling.\n\nIn North America, the term \"steam tractor\" usually refers to a type of agricultural tractor powered by a steam engine, used extensively in the late 19th and early 20th centuries.\n\nIn Great Britain, the term \"steam tractor\" is more usually applied to the smallest models of traction engine - typically those weighing seven tons or less - used for hauling small loads on public roads. Although known as \"light steam tractors\", these engines are generally just smaller versions of the 'road locomotive'.\n\nThis article concentrates on the steam-powered agricultural vehicles intended for the direct-pulling of ploughs and other implements (as opposed to cable-hauling).\n\nOwing to differences in soil conditions, the development of steam-powered agricultural machines differed considerably on either side of the Atlantic.\n\nIn Great Britain, a number of traction engine builders attempted to produce a design of agricultural engine that could pull a plough directly, in place of a team of horses. However, the heavier and wetter soils found in Britain meant that these designs were not successful — being less economical to use than the team of horses they were intended to replace. These engines were also known as \"steam tractors\". Instead, farmers resorted to cable-hauled ploughing using ploughing engines.\n\nA distinctive example of a British-designed (agricultural) steam tractor is the Garrett \"Suffolk Punch\", a 1917 design intended to compete directly with internal combustion-powered alternatives. \n\nThe first steam tractors that were designed specifically for agricultural uses were portable engines built on skids or on wheels and transported to the work area using horses. Later models used the power of the steam engine itself to power a drive train to move the machine and were first known as \"traction drive\" engines which eventually was shortened to \"tractor\". These drive mechanisms were one of three types: chain, shaft, and open pinion. The open pinion became the most popular design due to its strength. Later improvements included power steering, differentials, compounded engines, and butt-strap boiler design.\n\nThe steam engine was gradually phased out by the mid-1920s as the less expensive, lighter, and faster-starting internal combustion (kerosene, petrol or distillate) tractors fully emerged after World War I.\n\nThese engines were used extensively in rural North America to aid in threshing, in which the owner/operator of a threshing machine or \"threshing rig\" would travel from farmstead to farmstead threshing grain. Oats were a common item to be threshed, but wheat and other grains were common as well. On a \"threshing day\", all the neighbors would gather at that day's farmstead to complete a massive job in one day through cooperation. The women and older girls were in charge of cooking the noon meal and bringing water to the men. The children had various jobs based upon their age and sex. These jobs included driving the bundle racks, pitching bundles into the threshing machine, supplying water for the steam engine, hauling away the freshly threshed grain and scooping it into the granary. Steam traction engines were often too expensive for a single farmer to purchase, so \"threshing rings\" were often formed. In a threshing ring, multiple farmers pooled their resources to purchase a steam engine. They also chose one person among them to go to a steam school, to learn how to run the engine properly. There were also threshing contractors, who owned their own engine and thresher, and went to different farms, hiring themselves out to thresh grain.\n\nThe immense pulling power of steam tractors allowed them to be used for ploughing as well. Certain steam tractors were better suited for ploughing than others, with the large Minneapolis Threshing machine Co., J.I. Case, Reeves & Co and Advance-Rumely engines being prime examples. Some of the largest steam tractors, such as the Case (known as \"Road Locomotives\"), were capable of pulling 30 or more plough bottoms, while most were powerful enough to pull between 6 and 20. Differing soil conditions highly affected the ploughing abilities of these tractors.\n\n\n(La Porte, Indiana)\n\n(Escanaba, Michigan, USA)\n\n\n"}
{"id": "9348405", "url": "https://en.wikipedia.org/wiki?curid=9348405", "title": "Technological innovation", "text": "Technological innovation\n\nTechnological innovation is an extended concept of innovation. While innovation is a rather-well defined concept, it has a broad meaning to many people, and especially numerous understanding in the academic and business world.\n\nInnovation, refers to adding extra steps of developing new services and products in the marketplace or in the public that fulfill undressed needs or solve problems that were not in the past. Technological Innovation, however focuses on the technological aspects of a product or service rather than covering the entire organization business model. It is important to clarify that Innovation is not only driven by technology.\n\nTechnological innovation is the process where an organization (or a group of people working outside a structured organization) embarks in a journey where the importance of technology as a source of innovation has been identified as a critical success factor for increased market competitiveness. The wording \"technological innovation\" is preferred to \"technology innovation\". \"Technology innovation\" gives a sense of working on technology for the sake of technology. \"Technological innovation\" better reflects the business consideration of improving business value by working on technological aspects of the product or services. Moreover, in a vast majority of products and services, there is not one unique technology at the heart of the system. It is the combination, the integration and interaction of different technologies that make the product or service successful.\n\nIf the process of technological innovation is formalized (typically within an organization: a company, a public entity, a think tank, a university, etc.) it can be referred as Technological Innovation Management (or Technology Innovation Management - TIM). The \"management\" aspect refers to the inputs, outputs and constraints a \"Manager\" or team of \"Managers\" are responsible to govern the process of technological innovation in a way that aligns with the company strategy. In a context where Technological Innovation is not to be guided along known paths within the organization, the wording and concept of Technological Innovation Leadership is preferred. In many occasion, especially in start-ups and new ventures, the Technological Innovation is performed in an unknown context. The boundaries and constraints of the Technology at work are not precisely know. Hence it requires leaders and not managers to give the vision and coach the team to explore the unknown part of the technology.\n\nTechnological Innovation:\n"}
{"id": "32052570", "url": "https://en.wikipedia.org/wiki?curid=32052570", "title": "Wiltshire cure", "text": "Wiltshire cure\n\nThe Wiltshire cure is a traditional English technique for curing bacon and ham. The technique originated in the 18th century in Calne, Wiltshire; it was developed by the Harris family. Originally it was a dry cure method that involved applying salt to the meat for 10–14 days. Storing the meat in cold rooms meant that less salt was needed. The Wiltshire cure has been a wet cure, soaking the meat in brine for 4–5 days, since the First World War. Smoking is not part of the process, although bacon is often smoked after being cured.\n\nIn 2010, several large British supermarket chains signed a voluntary code agreeing to use clearer labelling on pork products to avoid confusion over country of origin. For shops under this agreement, pork products sold in the UK that are labelled with \"Wiltshire Cure\" should only have been sourced from the UK.\n\n"}
