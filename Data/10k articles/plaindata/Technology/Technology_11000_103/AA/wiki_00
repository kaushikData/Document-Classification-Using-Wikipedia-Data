{"id": "29470181", "url": "https://en.wikipedia.org/wiki?curid=29470181", "title": "Archos 43", "text": "Archos 43\n\nARCHOS 43 Internet Tablet is a discontinued 4.3-inch tablet computer designed and developed by Archos in the ARCHOS Generation 8 Internet Tablets line. The Archos 43 runs version 2.2 (Froyo) of the Android operating system. It was released globally in early November 2010. It was met with mixed reviews, with its biggest overall complaint being its resistive touchscreen.\n\nOn the \"ARCHOS 43 Internet Tablet\" several applications are installed by default. Users are able to add applications through Appslib, an application marketplace.\n\n\n"}
{"id": "33530159", "url": "https://en.wikipedia.org/wiki?curid=33530159", "title": "Arlene Harris (inventor)", "text": "Arlene Harris (inventor)\n\nArlene Joy Harris (born June 6, 1948), also known as the \"First Lady of Wireless,\" is an entrepreneur and inventor, who holds numerous wireless communications patents. In May, 2007 she was the first woman inducted into the Wireless Hall of Fame.\n\nIn 1983 Harris co-founded Cellular Business Systems Inc. (CBSI), (sold to Cincinnati Bell, now Convergys) where she guided the development of the leading billing/CRM service bureau in the early cellular industry. She personally specified and directed the development of the first automated cellular service activation systems now used globally in retail locations to remotely and instantly activate cellular phones. While at CBSI Harris served as one of three FCC committee members challenged to develop intersystem roaming protocols. The committee was established to create methods for cellular companies to bill customers who visited their networks. The committee's work resulted in the Cellular Inter-carrier Billing Exchange Record (CIBER) used throughout the cellular industry.\n\nIn 1986 Harris launched Dyna LLC in Chicago, Illinois, and later relocated to Del Mar, California, as a home base to incubate and spin out new ideas and help young companies. \n\nHarris founded the software company Subscriber Computing, Inc. (merged with Corsair, now CyberSource) in 1986; her team built and delivered systems to the largest paging companies in the world and provided the first converged billing systems for cellular and the Internet to global leaders, including British Telecom and Hutchinson. In 1988, she led the Company's implementation of the first communications methods used to support access to cellular services by low and no credit consumers. The concept became known as \"prepaid\" cellular service and has grown to become one of the primary forms of subscriber relationship and payments in the cellular industry. \n\nIn 1986, Harris founded Cellular Pay Phone, Inc. (CPPI) where she developed her first patented invention, the first program-controlled end-to-end management system (created with OKI Electronics and Motorola). This offering made CPPI the first niche cellular reseller in history to create a special cellular phone and a tightly integrated system to support cellular with automated payments by credit card.\n\nFounded by Harris in 1994 and under her guidance, SOS Wireless Communications developed the first consumer oriented reseller of cellular service designed especially for safety. Like the payphone business SOS developed a custom phone and service for making outgoing calls for urgent communications. SOS customers were primarily older Americans who adopted cellular service to keep them safe while on the road.\n\nHarris acquired cellular carrier Accessible Wireless in 2001 to provide a home carrier service for offerings targeting typically low usage applications. Accessible and SOS both supported the founding of Harris' next company GreatCall in 2005.\n\nGreatCall is the first complete end-to-end value-added service provider in the cellular industry to focus on simplicity with primary emphasis on baby boomers and senior citizens. At GreatCall Harris led the development of the Jitterbug phone in partnership with Samsung to create a simple and personalized cell phone experience. The Jitterbug and service earned top honors as one of New York Times top 10 greatest technology ideas of 2006 (as judged by David Pogue), as a finalist in Yahoo's \"Last Gadget Standing\" competition at the Consumer Electronics Show in 2007 and as Reader's Digest \"Top 100 Products.\" Additionally, GreatCall won the wireless industry's coveted Andrew Seybold Choice Award for \"Best New Company\" at CTIA in 2007 and the American Society on Aging's Award for \"Best Small Business in 2008.\"\n\n"}
{"id": "40820811", "url": "https://en.wikipedia.org/wiki?curid=40820811", "title": "Boxcar averager", "text": "Boxcar averager\n\nA boxcar averager (alternative names are gated integrator and boxcar integrator) is an electronic test instrument that integrates the signal input voltage after a defined waiting time (trigger delay) over a specified period of time (gate width) and then averages over multiple integration results (samples) – for a mathematical description see boxcar function.\nThe main purpose of this measurement technique is to improve signal to noise ratio in pulsed experiments with often low duty cycle by the following three mechanisms: 1) signal integration acts as a first averaging step that strongly suppresses noise components with a frequency of the reciprocal gate width and higher, 2) time-domain based selection of signal parts that actually carry information of interest and neglect of all signal parts where only noise is present, and 3) averaging over a defined number of periods provides low-pass filtering and convenient adjustment of time resolution.\nSimilar to lock-in amplifiers, boxcar averagers are mostly used for the analysis of periodic signals. Whereas the lock-in can be understood as sophisticated band pass filters with adjustable center frequency and bandwidth, the boxcar averager allows to define the signal of interest and resulting time resolution mostly in the time domain.\n\nThe boxcar operation is defined by a trigger delay, a gate width and the number of trigger events (i.e., samples) that are averaged over in the buffer.\n\nThe principle of operation can be understood as a two-step process: signal integration over the desired gate width and averaging the integrated signal over a defined amount of periods/trigger events\n\nConsidering a simple implementation of the core circuitry looks like regular RC low-pass filter that can be gated by a switch S.\nProvided the filter time constant τ = RC is set to sufficiently large values with respect to the gate width, the output voltage is to a good approximation the integral of the input signal with a signal bandwidth of B = 1/(4RC). The output of this filter can then be subjected to another analog circuit for subsequent averaging. After each trigger event this sampling circuit has to be set back before receiving the next pulse. The time it takes for this reset is one of the major speed limitations for analog implementations, where maximum trigger rates of a few 10 kHz are typical even though the gate width itself can be as low as a few ten picoseconds and delay is set to zero.\n\nThe origin of the boxcar averager dates back to as early as 1950 where the technique helped to improve signal quality in experiments investigating nuclear magnetic resonances with pulsed schemes.). The first reported application of \"boxcar circuits to NMR was Holcomb and Norberg\". In their 1955 paper Holcomb and Norberg credit the invention of the “boxcar integrator” to a large extent to L. S. Kypta and H. W. Knoebel.\n\n"}
{"id": "17050857", "url": "https://en.wikipedia.org/wiki?curid=17050857", "title": "CBU-72", "text": "CBU-72\n\nThe CBU-72 was a 550-pound American fuel-air cluster bomb used by the United States Military until 1996. It was very effective against armored vehicles, aircraft parked in the open, bunkers, and minefields.\n\nThe CBU-72 consisted of three fuel-air explosive (FAE) submunitions. Each submunition weighed about 100 pounds and dispensed a cloud approximately 60 feet in diameter and 8 feet thick composed of its 75 pounds of ethylene oxide aerosol fuel across the target area, with air-burst fusing set for 30 feet. An embedded detonator ignited the cloud as it descended to the ground to produce a massive explosion.\nThe high-pressure of the rapidly expanding wave front flattened all objects within close proximity of the epicenter of the fuel cloud, as well as causing debilitating damage well beyond it. Like other FAE using ethylene oxide, in the event of non-ignition, it functions as a chemical weapon, due to the highly toxic nature of this gas.\n\nFirst-generation CBU-55 and CBU-72 fuel-air weapons were used in the Vietnam War. A second generation of FAE weapons were based on those, and were used by the United States in Iraq during Operation Desert Storm. A total of 254 CBU-72s were dropped by the United States Marine Corps, mostly from A-6Es. They were targeted against mine fields and personnel in trenches, but were more useful as a psychological weapon.\n\nAfter Desert Storm, the United States Navy and the Marines removed their remaining FAE weapons from service, and by 1996, they had been transferred for demilitarization. By the middle of 2001, only a few hundred remained in existence, awaiting demilitarization.\n\n"}
{"id": "2928094", "url": "https://en.wikipedia.org/wiki?curid=2928094", "title": "CXML", "text": "CXML\n\ncXML (commerce eXtensible Markup Language) is a protocol, created by Ariba in 1999, intended for communication of business documents between procurement applications, e-commerce hubs and suppliers. cXML is based on XML and provides formal XML schemas for standard business transactions, allowing programs to modify and validate documents without prior knowledge of their form.\n\nThe protocol does not include the full breadth of interactions some parties may wish to communicate. However, it can be expanded through the use of extrinsic elements and newly defined domains for various identifiers. This expansion is the limit of point-to-point configurations necessary for communication.\n\nThe current protocol includes documents for setup (company details and transaction profiles), catalogue content, application integration (including the widely used PunchOut feature), original, change and delete purchase orders and responses to all of these requests, order confirmation and ship notice documents (cXML analogues of EDI 855 and 856 transactions) and new invoice documents.\n\nPunchOut is a protocol for interactive sessions managed across the Internet, a communication from one application to another, achieved through a dialog of real-time, synchronous cXML messages, which support user interaction at a remote site. This protocol is most commonly used today in the form of Procurement PunchOut, which specifically supports interactions between a procurement application and a supplier’s eCommerce web site and possibly includes an intermediary for authentication and version matching. The buyer leaves or \"punches out\" of their company's system and goes to the supplier's web-based catalog to locate and add items to their shopping cart, while their application transparently maintains connection with the web site and gathers pertinent information. A vendor catalog, enhanced for this process, is known as a punchout catalog. PunchOut enables communication between the software and the web site so that relevant information about the transaction is delivered to the appropriate channels.\n\n\ncXML is published based on the input of many companies, and is controlled by Ariba. cXML is a protocol that is published for free on the Internet along with its DTD. It is open to all for their use without restrictions apart from publications of modifications and naming that new protocol. Essentially, everyone is free to use cXML with any and all modifications as long as they don't publish their own standard and call it \"cXML\". Beginning in February 1999, the cXML standard has been available for all to use. The details of its license agreement are found at http://cxml.org/license.html.\n\n"}
{"id": "23709340", "url": "https://en.wikipedia.org/wiki?curid=23709340", "title": "Circuit design language", "text": "Circuit design language\n\nCDL: Circuit Description Language is a kind of netlist, a description of an electronic circuit. It is usually automatically generated from a circuit schematic. It is used for electronic circuit simulation and layout versus schematic (LVS) checks. It is similar to SPICE netlists, but with some extensions.\n\nSeveral vendors such as Cadence Design Systems, Mentor Graphics, and Synopsys support CDL netlists, although their solutions may be proprietary and not readable by competing systems.\n"}
{"id": "7590761", "url": "https://en.wikipedia.org/wiki?curid=7590761", "title": "Curtain rod", "text": "Curtain rod\n\nA curtain rod, curtain rail, or traverse rod is a device used to suspend curtains, usually above windows or along the edges of showers or bathtubs, though also wherever curtains might be used. When found in bathrooms, curtain rods tend to be telescopic and self-fixing, while curtain rods in other areas of the home are often affixed with decorative brackets or finial.\n\nBathtubs can use L shaped or circle rod, mainly when only one wall and be used.\n\nCurtain rods can be made of many materials including- wood, metal and plastic. Curtain rods come in almost endless styles and designs. Not all curtain rods are simple straight poles; curved and hinged poles are available from numerous companies, allowing installation in bay windows and around curved walls and corners. Curtain rods can also be shaped like a crane or exhibit a swing arm design. Prices and quality of curtain rods are as varied as designs from inexpensive big box store products to high end specialty products made by companies catering to interior designers and architects.\n\n18 years following the invention of the curtain rod by Samuel R. Scrottron in 1892, the flat, telescoping curtain rod was invented by Charles W. Kirsch, of Sturgis, Michigan, in 1907. However, they were not in use until the 1920s. Kirsch also invented the traverse curtain rod in 1928.\n\n"}
{"id": "43383898", "url": "https://en.wikipedia.org/wiki?curid=43383898", "title": "Customer Service System", "text": "Customer Service System\n\nThe Customer Service System (CSS) of the BT Group (previously British Telecommunications) is the core operational support system for BT, bringing in 70% of income for the company (figures from 1997). BT rolled out CSS nationally in 1989 and provided an integrated system for telephony—order handling, repair handling and billing.\n\nBT Customer Service System (BT/CSS) was developed by Logica in 1984, costing £1bn to implement, representing the largest computer project undertaken in Europe and the largest integrated database in the world, at the time.\n\nIn 2005, the CSS databases deployed by BT handled 23 million customers, with 13 terabytes of data spread out over 28 mainframe images. The databases supported 230 million transactions per day generated by over 40,000 users.\n\nCSS is still used by BT but it is now controlled by Openreach. BT retail migrated all accounts from CSS to a new billing system known as Geneva/Oneview/Avalon. This was to support the migration to WLR3.\n"}
{"id": "40997", "url": "https://en.wikipedia.org/wiki?curid=40997", "title": "Data service unit", "text": "Data service unit\n\nA data service unit, sometimes called a digital service unit, is a piece of telecommunications circuit terminating equipment that transforms digital data between telephone company lines and local equipment. The device converts bipolar digital signals coming ultimately from a digital circuit and directly from a Channel service unit (CSU), into a format (e.g. RS- 530) compatible with the piece of data terminal equipment (DTE) (e.g. a router) to which the data is sent. The DSU also performs a similar process in reverse for data heading from the DTE toward the circuit. The telecommunications service a DSU supports can be a point-to-point or multipoint operation in a digital data network.\n\nA DSU is a two or more port device; one port is called the WAN (Wide Area Network) port and the other is called a DTE port. The purpose of the DSU is to transfer serial data synchronously between the WAN port and the DTE ports. If more than one DTE port is used, the DSU assigns the DTE data according to time slots (channels) on the WAN side.\n\nOn the WAN side, the DSU, via a CSU, interfaces with a digital carrier such as DS1 or DS3 or a low speed Digital Data Service. On the DTE side, the DSU provides control lines, timing lines and appropriate physical and electrical interface. To maintain the synchronous relationship between the ports, the DSU manages timing by slaving ports to the bit rate of another or to its internal clock. Typically, the DTE port provides timing to the data terminal equipment while the WAN port dictates the rate.\n\nDSUs usually include some maintenance capabilities. At minimum, they can loop data back at either the WAN or DTE ports, or at both. When only one port is looped back, the data received at that port is simultaneously sent back toward the port and passed in normal fashion to the other port. Most DSUs also allow various data patterns to be generated and monitored to measure error rate of the communication link. A DSU may be a separate piece of equipment, or may be combined in a CSU/DSU.\n"}
{"id": "17827804", "url": "https://en.wikipedia.org/wiki?curid=17827804", "title": "Directional well", "text": "Directional well\n\nA directional well is an oil industry term for an oil well with a borehole that deviates from a vertically straight line. This is normally done with the intention of hitting several target sands, for instance.\n\n \n"}
{"id": "58142292", "url": "https://en.wikipedia.org/wiki?curid=58142292", "title": "Drumeo", "text": "Drumeo\n\nDrumeo, an acronym for \"drum education online\", is an online drum learning platform known for offering drum lessons to students.\n\nDrumeo was launched in 2012 by Jared Falk and Rick Kettner with its physical location in Abbotsford, British Columbia, Canada.\n\nPrior to the launching of the platform, Jared had been teaching private drum lessons touring as musician for years. Rick, on the other hand, used to film simple video lessons for online drum message boards. Both later teamed up and created the online drum learning platform.\n\nIn 2016, Drumeo released the \"Drumeo P4 Practice Pad\" which offers four different playing surfaces, compared to the traditional one-surface practice pads. It was designed by New York drummer Pat Petrillo.\n\nDrumeo hosts a YouTube channel for drummers and offers video drum lessons through a subscription model called \"Drumeo Edge\". The platform also hosts notable drummers who come along to offer free live drum lessons to students from time to time.\n\nDrumeo has notable drummers that offer lessons. Among them include Bernard Purdie, Antonio Sanchez, Todd Sucherman, Mark Guiliana, Kenny Aronoff, and others. Among the notable students of the platform includes Petr Čech, Arsenal soccer goalie.\n\n\n"}
{"id": "1121231", "url": "https://en.wikipedia.org/wiki?curid=1121231", "title": "Electronic bill payment", "text": "Electronic bill payment\n\nElectronic bill payment is a feature of online, mobile and telephone banking, similar in its effect to a giro, allowing a customer of a financial institution to transfer money from their transaction or credit card account to a creditor or vendor such as a public utility, department store or an individual to be credited against a specific account. These payments are typically executed electronically as a direct deposit through a national payment system, operated by the banks or in conjunction with the government. Payment is typically initiated by the payer but can also be set up as a direct debit.\n\nIn addition to the bill payment facility, most banks will also offer various features with their electronic bill payment systems. These include the ability to schedule payments in advance to be made on a specified date (convenient for installments such as mortgage and support payments), to save the biller information for reuse at a future time and various options for searching the recent payment history. In many cases the payment data can also be downloaded and posted directly into the customer's accounting or personal finance software.\n\nAlthough this technology was available from the mid 1990s, uptake was initially slow until internet access by households increased. By 2000, adoption of electronic bill payment systems started to dramatically increase.\n\nFrom the consumer's point of view, electronic payment of bills is cheaper, faster, and more convenient than writing, posting and reconciling cheques. In addition, though limitations exist, a wider range of bank accounts or credit cards can be used for the electronic payment of bills.\n\nUsing electronic bill presentment and payment enables businesses to fast-track customer payments and get access to funds faster, which in turn results in cash flow improvement.\n\nFor banks the advantages of electronic bill payments are a reduction in processing costs minimizing paperwork and an increase in customer loyalty. In a 2003 study, the banks said that \"customers who pay online show more loyalty and are more receptive to other offers\".\n\n"}
{"id": "1747014", "url": "https://en.wikipedia.org/wiki?curid=1747014", "title": "Embryo space colonization", "text": "Embryo space colonization\n\nEmbryo space colonization is a theoretical interstellar space colonization concept that involves sending a robotic mission to a habitable terrestrial planet, dwarf planet, minor planet or natural satellite transporting frozen early-stage human embryos or the technological or biological means to create human embryos. The proposal circumvents the most severe technological problems of other mainstream interstellar colonization concepts. In contrast to the sleeper ship proposal, it does not require the more technically challenging 'freezing' of fully developed humans (see \"cryonics)\".\n\nEmbryo space colonization concepts involve various concepts of delivering the embryos from Earth to an extrasolar planet around another star system. \n\nRegardless of the \"cargo\" used in any embryo space colonization scenario, the basic concept is that upon arrival of the embryo-carrying spacecraft (EIS) at the target planet, fully robots would build the first settlement on the planet and start growing food. More ambitiously, the planet may be terraformed first. Thereafter the first embryos could be unfrozen (or created using biosequenced or natural sperm and egg cells as outlined above).\n\nIn any event, one of the technologies needed for the proposal are artificial uteri. The embryos would need to develop in such artificial uteri until a large enough population existed to procreate by natural biological means.\n\n\nLike every proposal for interstellar colonization, embryo space colonization depends on solutions to still-unsolved technological problems. Some of these are:\n\nFurther unknowns that affect the feasibility of embryo space colonization are:\n\n\n\n"}
{"id": "840147", "url": "https://en.wikipedia.org/wiki?curid=840147", "title": "Exhaust gas", "text": "Exhaust gas\n\nExhaust gas or flue gas is emitted as a result of the combustion of fuels such as natural gas, gasoline, petrol, biodiesel blends, diesel fuel, fuel oil, or coal. According to the type of engine, it is discharged into the atmosphere through an exhaust pipe, flue gas stack, or propelling nozzle. It often disperses downwind in a pattern called an \"exhaust plume\".\n\nIt is a major component of motor vehicle emissions (and from stationary internal combustion engines), which can also include:\n\nMotor vehicle emissions contribute to air pollution and are a major ingredient in the creation of smog in some large cities. A 2013 study by MIT indicates that 53,000 early deaths occur per year in the United States alone because of vehicle emissions. According to another study from the same university, traffic fumes alone cause the death of 5,000 people every year just in the United Kingdom.\n\nThe largest part of most combustion gas is nitrogen (N), water vapor (HO) (except with pure-carbon fuels), and carbon dioxide (CO) (except for fuels without carbon); these are not toxic or noxious (although carbon dioxide is a greenhouse gas that contributes to global warming). A relatively small part of combustion gas is undesirable, noxious, or toxic substances, such as carbon monoxide (CO) from incomplete combustion, hydrocarbons (properly indicated as CH, but typically shown simply as \"HC\" on emissions-test slips) from unburnt fuel, nitrogen oxides (NO) from excessive combustion temperatures, and particulate matter (mostly soot).\n\nExhaust gas temperature (EGT) is important to the functioning of the catalytic converter of an internal combustion engine. It may be measured by an exhaust gas temperature gauge. EGT is also a measure of engine health in gas-turbine engines (see below).\n\nDuring the first two minutes after starting the engine of a car that has not been operated for several hours, the amount of emissions can be very high. This occurs for two main reasons:\n\nComparable with the European emission standards EURO III as it was applied on October 2000\n\nIn 2000, the United States Environmental Protection Agency began to implement more stringent emissions standards for light duty vehicles. The requirements were phased in beginning with 2004 vehicles and all new cars and light trucks were required to meet the updated standards by the end of 2007.\n\nIn spark-ignition engines the gases resulting from combustion of the fuel and air mix are called exhaust gases. The composition varies from petrol to diesel engines, but is around these levels:\nThe 10% oxygen for \"diesel\" is likely if the engine was idling, e.g. in a test rig. It is much less if the engine is running under load.\n\nExhaust gas from an internal combustion engine whose fuel includes nitromethane will contain nitric acid vapour, which is corrosive, and when inhaled causes a muscular reaction making it impossible to breathe. People exposed to it should wear a gas mask.\n\n\nIn jet engines and rocket engines, exhaust from propelling nozzles which in some applications shows shock diamonds.\n\n\nIn steam engine terminology the exhaust is steam that is now so low in pressure that it can no longer do useful work.\n\nMono-nitrogen oxides NO and NO (NOx)(whether produced this way or naturally by lightning) react with ammonia, moisture, and other compounds to form nitric acid vapor and related particles. Small particles can penetrate deeply into sensitive lung tissue and damage it, causing premature death in extreme cases. Inhalation of NO species increases the risk of lung cancer and colorectal cancer. and inhalation of such particles may cause or worsen respiratory diseases such as emphysema and bronchitis and heart disease.\n\nIn a 2005 U.S. EPA study the largest emissions of NOx came from on road motor vehicles, with the second largest contributor being non-road equipment which is mostly gasoline and diesel stations.\n\nThe resulting nitric acid may be washed into soil, where it becomes nitrate, which is useful to growing plants.\n\nWhen oxides of nitrogen (NOx) and volatile organic compounds (VOCs) react in the presence of sunlight, ground level ozone is formed, a primary ingredient in smog. A 2005 U.S. EPA report gives road vehicles as the second largest source of VOCs in the U.S. at 26% and 19% are from non road equipment which is mostly gasoline and diesel stations. 27% of VOC emissions are from solvents which are used in the manufacturer of paints and paint thinners and other uses.\n\nOzone is beneficial in the upper atmosphere, but at ground level ozone irritates the respiratory system, causing coughing, choking, and reduced lung capacity. It also has many negative effects throughout the ecosystem.\n\nCarbon monoxide poisoning is the most common type of fatal air poisoning in many countries. Carbon monoxide is colorless, odorless and tasteless, but highly toxic. It combines with hemoglobin to produce carboxyhemoglobin, which is ineffective for delivering oxygen to bodily tissues. In 2011, 52% of carbon monoxide emissions were created by mobile vehicles in the U.S.\n\nChronic (long-term) exposure to benzene (CH) damages bone marrow. It can also cause excessive bleeding and depress the immune system, increasing the chance of infection. Benzene causes leukemia and is associated with other blood cancers and pre-cancers of the blood.\n\nThe health effects of inhaling airborne particulate matter have been widely studied in humans and animals and include asthma, lung cancer, cardiovascular issues, premature death. Because of the size of the particles, they can penetrate the deepest part of the lungs. A 2011 UK study estimates 90 deaths per year due to passenger vehicle PM. In a 2006 publication, the U.S. Federal Highway Administration (FHWA) state that in 2002 about 1 per-cent of all PM and 2 per-cent of all PM emissions came from the exhaust of on-road motor vehicles (mostly from diesel engines).\n\nCarbon dioxide is a greenhouse gas. Motor vehicle CO emissions are part of the anthropogenic contribution to the growth of CO concentrations in the atmosphere which according to the vast majority of the scientific community is causing climate change. Motor vehicles are calculated to generate about 20% of the European Union's man-made CO emissions, with passenger cars contributing about 12%. European emission standards limit the CO emissions of new passenger cars and light vehicles. The European Union average new car CO emissions figure dropped by 5.4% in the year to the first quarter of 2010, down to 145.6 g/km.\n\nVehicle exhaust contains much water vapour.\n\nThere has been research into ways that troops in deserts can recover drinkable water from their vehicles' exhaust gases.\n\nEmission standards focus on reducing pollutants contained in the exhaust gases from vehicles as well as from industrial flue gas stacks and other air pollution exhaust sources in various large-scale industrial facilities such as petroleum refineries, natural gas processing plants, petrochemical plants and chemical production plants. However, these are often referred to as flue gases. Catalytic converters in cars intend to break down the pollution of exhaust gases using a catalyst. Scrubbers in ships intend to remove the sulfur dioxide (SO) of marine exhaust gases. The regulations on marine sulfur dioxide emissions are tightening, however only a small number of special areas worldwide have been designated for low sulfur diesel fuel use only.\n\nOne of the advantages claimed for advanced steam technology engines is that they produce smaller quantities of toxic pollutants (e.g. oxides of nitrogen) than petrol and diesel engines of the same power. They produce larger quantities of carbon dioxide but less carbon monoxide due to more efficient combustion.\n\nResearchers from the University of California, Los Angeles School of Public Health say preliminary results of their statistical study of children listed in the California Cancer Registry born between 1998 and 2007 found that traffic pollution may be associated with a 5% to 15% increase in the likelihood of some cancers. A World Health Organization study found that diesel fumes cause an increase in lung cancer.\n\nThe California Air Resources Board (C.A.R.B.) found in studies that 50% or more of the air pollution (smog) in Southern California is due to car emissions. Concentrations of pollutants emitted from combustion engines may be particularly high around signalized intersections because of idling and accelerations. Computer models often miss this kind of detail. \n\n\n"}
{"id": "5074329", "url": "https://en.wikipedia.org/wiki?curid=5074329", "title": "Fibre Channel network protocols", "text": "Fibre Channel network protocols\n\nCommunication between devices in a fibre channel network uses different elements of Fibre Channel standards.\n\nAll Fibre Channel communication is done in units of four 10-bit codes. This group of 4 codes is called a transmission word.\n\nAn ordered set is a transmission word that includes some combination of control (K) codes and data (D) codes\n\nEach device has an Arbitrated Loop Physical Address (AL_PA). These addresses are defined by an 8-bit field but must have neutral disparity as defined in the 8B/10B coding scheme. That reduces the number of possible values from 256 to 134. The 134 possible values have been divided between the fabric, FC_AL ports, and other special purposes as follows:\n\nIn addition to the transfer of data, it is necessary for Fibre Channel communication to include some metadata. This allows for the setting up of links, sequence management, and other control functions. The meta-data falls into two types, primitives which consist of a 4 character transmission word and non-data frames which are more complex structures.\n\nAll primitives are four characters in length. They begin with the control character K28.5, followed by three data characters. In some primitives the three data characters are fixed, in others they can be varied to change the meaning or to act as parameters for the primitive. In some cases the last two parameter characters are identical.\n\nParameters are shown in the table below in the form of their hexadecimal 8-bit values. This is clearer than their full 10-bit (Dxx.x) form as shown in the Fibre Channel standards:\nNote 1: The first parameter byte of the EOF primitive can have one of four different values (8A, 95, AA, or B5). This is done so that the EOF primitive can rebalance the disparity of the whole frame. The remaining two parameter bytes define whether the frame is ending normally, terminating the transfer, or is to be aborted due to an error.\n\nNote 2: The Open selective replicate variant can be repeated a number of times in order to communicate with more than one destination port simultaneously. The Open broadcast replicate variant will allow communication with all ports simultaneously.\n\nNote 3: The SOF primitive contains a pair of control bytes (shown as cccc in the table) to designate the type of frame.\n\nThe Fibre Channel protocol transmits data in frames each of which can contain up to 2112 bytes of payload data. The structure of a frame is shown in this table:\n\nIn addition to data frames, there are non-data frames that are used for setup and messaging purposes. These fall into three categories: link control frames, link service frames, and extended link service frames. The following table lists the most common ones:\n\n"}
{"id": "41194", "url": "https://en.wikipedia.org/wiki?curid=41194", "title": "Frequency standard", "text": "Frequency standard\n\nA frequency standard is a stable oscillator used for frequency calibration or reference. A frequency standard generates a fundamental frequency with a high degree of accuracy and precision. Harmonics of this fundamental frequency are used to provide reference points.\n\nSince time is the reciprocal of frequency, it is relatively easy to derive a time standard from a frequency standard. A standard clock comprises a frequency standard, a device to count off the cycles of the oscillation emitted by the frequency standard, and a means of displaying or outputting the result.\n\nFrequency standards in a network or facility are sometimes administratively designated as \"primary\" or \"secondary\". The terms \"primary\" and \"secondary\", as used in this context, should not be confused with the respective technical meanings of these words in the discipline of precise time and frequency.\n\nA frequency reference is an instrument used for providing a stable frequency of some kind. There are different sorts of frequency references, acoustic ones such as tuning forks but also electrical ones that emit a signal of a certain frequency (a frequency standard).\n\nAmong the most stable frequency references in the world are cesium standards, including cesium fountains, and hydrogen masers. Cesium standards are widely recognized as having better long-term stability, whereas hydrogen masers can attain superior short-term performance; therefore, several national standards laboratories use ensembles of cesium standards and hydrogen masers in order to combine the best attributes of both.\n\nThe carrier of time signal transmitters, LORAN-C transmitters and of several long wave and medium wave broadcasting stations is derived from an atomic clock and can be therefore used as frequency standard.\n\n"}
{"id": "22769766", "url": "https://en.wikipedia.org/wiki?curid=22769766", "title": "Global information system", "text": "Global information system\n\nThere are a variety of definitions and understandings of a global information system (GIS, GLIS), such as \n\n\nCommon to this class of information systems is that the context is a global setting, either for its use or development process. This means that it highly relates to distributed systems / distributed computing where the distribution is global. The term also incorporates aspects of global software development and there outsourcing (when the outsourcing locations are globally distributed) and offshoring aspects. A specific aspect of global information systems is the case (domain) of global software development. A main research aspect in this field concerns the coordination of and collaboration between virtual teams. Further important aspects are the internationalization and language localization of system components.\n\nCritical tasks in designing global information systems are \n\nA variety of examples can be given. Basically every multi-lingual website can be seen as a global information system. However, mostly the term GLIS is used to refer to a specific system developed or used in a global context.\n\nSpecific examples are \n\n"}
{"id": "52095802", "url": "https://en.wikipedia.org/wiki?curid=52095802", "title": "Illuminated mannequin", "text": "Illuminated mannequin\n\nIlluminated mannequins are a home decor piece that were popular in the 1970s and 1980s in the United States. Although niche items of decor, their less known style was a symbol in 70s fashion and home furnishing.\n\nAn illuminated mannequin is a full or part body mannequin which illuminates from inside as an alternative to floor lamps.\n\nThe illuminated mannequin was originally manufactured and released by Adel Rootstein as a decorative counterpart to her widely successful Twiggy Mannequin. Rootstein became well-known thanks to the success of the Twiggy mannequin, and the illuminated mannequin followed as a lesser known counterpart in its wake.\n\nAdel Rootstein has gifted her home décor mannequins to musicians and actors which she is inspired by, as mentioned briefly in an interview with Stevie Nicks in 1983.\n\nThere have been various celebrity photographs taken in which the illuminated mannequins can be seen in the celebrities homes, most notably Stevie Nicks of Fleetwood Mac and Michael Jackson.\n\nThe illuminated mannequin is quoted as an early inspiration to Ralph Pucci of the Pucci Mannequins. Pucci’s mannequin display ‘THE ART OF THE MANNEQUIN’ at the Museum of Arts and Design (MAD) had an illuminated mannequin form amongst the many models in his display. From March 31 to August 30, 2015, thirty of Pucci’s most memorable mannequins were on display for museum goers. Finding the exhibition, “extremely gratifying,” Pucci was thrilled that the visual industry is getting a shining moment in a museum whose mission is to “document contemporary and historic innovation in craft, art, and design.”\n"}
{"id": "21599202", "url": "https://en.wikipedia.org/wiki?curid=21599202", "title": "International Federation for Learning, Education, and Training Systems Interoperability", "text": "International Federation for Learning, Education, and Training Systems Interoperability\n\nThe International Federation for Learning, Education, and Training Systems Interoperability (LETSI) is an international nonprofit organization focused on enabling technical interoperability for computer-based learning, education, and training systems. Comprising e-learning vendors, adopters, standards bodies, associations, and policy makers, LETSI's primary activity is to support the adoption of open software standards in learning systems. The LETSI community formed around an international planning effort for the next generation of the Sharable Content Object Reference Model (SCORM), which was originally created by the U.S. Advanced Distributed Learning Initiative. LETSI was founded in March, 2008 to serve the international SCORM community.\n\nIn 1997 the U.S. Department of Defense founded the Advanced Distributed Learning (ADL) Initiative, with the mission of improving \"access to education, training, and performance aids, tailored to individual needs, delivered cost effectively, anytime and anywhere.\" \n\nIn January 2000, the ADL released the first edition of the Sharable Content Object Reference Model (SCORM), a technical framework designed to facilitate interoperability of computer-based education and training materials. Though developed for use within the DoD, SCORM became widely adopted in commercial, educational, government, and international projects.\n\nRecognizing that SCORM had acquired an international constituency across a broad spectrum of markets, the ADL in 2005 determined that an \"international collaborative approach\" was required in order to meet the needs of the SCORM user base, and resolved that \"an international stewardship organisation shall be established and become fully functional within a three-year period.\"\n\nIn March 2007, representatives from Australia, Brazil, Canada, China, France, Germany, Hungary, Japan, Korea, Mexico, Norway, Singapore, Switzerland, Tunisia, the United Kingdom, and the United States assembled at the London Institute for Education to discuss the formation of LETSI. \n\nIn March 2008, the LETSI founding sponsors signed a Memorandum of Understanding in Seoul, Korea about their common goals and principles and approved LETSI’s Operating Procedures Pro Tem.\n\nIn August, 2008, LETSI held the first SCORM 2.0 conference, in Pensacola, Florida.\n\nLETSI adheres to an open development process. The public may view meeting minutes, committee decisions, and materials related to work in progress at no cost through the LETSI wiki. Nonmembers may contribute comments, case studies, and other inputs to LETSI through the wiki. Voting, however, requires membership, which involves an annual fee.\n\nOutputs of LETSI, such as a new SCORM reference model, are planned to be public and non-proprietary. \n\nLETSI currently operates under IEEE Industry Standards and Technology Organization (ISTO) interim procedures. The Sponsors' Executive Committee (SEC), which is composed of sponsors, serves as LETSI’s governing body.\n\nLETSI activities are organized into the following working groups, each responsible for a different area of the next SCORM:\n\n\nTo increase consistency of adoption and to decrease time-to-market for innovation, LETSI plans to develop enabling software components (for example, code for RESTful and SOAP-based web services) and other tools for developers. These tools will be available to developers free of charge.\n\n\n"}
{"id": "14858275", "url": "https://en.wikipedia.org/wiki?curid=14858275", "title": "Interposer", "text": "Interposer\n\nAn interposer is an electrical interface routing between one socket or connection to another. The purpose of an interposer is to spread a connection to a wider pitch or to reroute a connection to a different connection.\n\nInterposer comes from the Latin word \"interpōnere\", meaning \"to put up between\".\n\nA common example of an interposer is an integrated circuit die to BGA, such as in the Pentium II. This is done through various substrates, both rigid and flexible, most commonly FR4 for rigid, and polyimide for flexible. Silicon and glass are also evaluated as an integration method. Interposer stacks are also a widely accepted, cost-effective alternative to 3D ICs. There are already several products with interposer technology in the market, notably the AMD Fiji/Fury GPU, and the Xilinx Virtex-7 FPGA. In 2016, demonstrated their second generation 3D-NoC technology which combines small dies (“chiplets”), fabricated at the FDSOI 28 nm node, on a 65 nm CMOS interposer.\n\nAnother example of an interposer would be the adapter used to plug a SATA drive into a SAS backplane with redundant ports. While SATA drives can be connected to nearly all SAS backplanes without adapters, an interposer would allow providing path redundancy.\n\n"}
{"id": "42028254", "url": "https://en.wikipedia.org/wiki?curid=42028254", "title": "JPay", "text": "JPay\n\nJPay is a privately held corrections-related service provider based in the United States with its headquarters in Miramar, Florida. The company contracts with state Departments of Correction (DOC), county jails, and private federal prisons to provide technologies and services including money transfer, email, video visitation and parole and probation payments to approximately 1.5 million inmates throughout 35 states.\n\nJPay was started in 2002 by the company’s CEO and founder, Ryan Shapiro. Mr. Shapiro stepped down in 2016 and Errol Feldman was named the new CEO. In 2005, the company moved its headquarters from New York to Miami. In 2009, JPay’s services expanded to offer an inmate MP3 player, the JP3, and a library of music tracks for purchase. In 2011, JPay moved its headquarters from Miami to Miramar, Florida, to accommodate a larger call center.\n\nIn 2012, JPay launched a tablet, the JP4, designed for the corrections industry, which enables inmates to read and draft emails, play games, and listen to music. It also allows inmates to view and attach photos and videograms. The decision to permit use of the JP4, and the full extent of its functions, is made by the state corrections departments. JPay’s tablet has been distributed in seven DOC agencies, including North Dakota, Georgia, Florida, Louisiana, Virginia, Michigan and Washington.\n\nIn April 2015, JPay was acquired by Securus Technologies though terms of the agreement were kept secret.\n\nIn a 2012 Bloomberg Business Week article entitled \"JPay, the Apple of the Prison System,\" Ryan Shapiro, the company’s founder and CEO, stated, \"We're looking for products that an inmate would want to buy and a corrections facility would accept.\" He went on to state, \"We take outside applications, redevelop them for prisons specifically, and then deploy them…the prison doesn't pay for any of (our services); it's the end user who pays.\" And, as the JP4 was being launched, Shapiro was quoted saying, “Think about education, think about games; it's endless where we could go. We think it's as big, if not bigger, than the money-transfer business.\"\n\nIn a 2014 CNBC article, Ryan Shapiro stated \"Our goal is to become the nation's digital consumer app company for prisons.\"\n\nAn inmate’s friend or family member can use JPay’s money transfer service to deposit money to the inmate’s commissary or trust account. JPay offers electronic payment and deposit options which include credit and debit card payments via online, phone, and mobile app channels. The company has a relationship with MoneyGram to accept cash at MoneyGram’s U.S. agent locations, like Walmart and CVS/pharmacy. Additionally, the company processes money orders on behalf of its contracted agencies.\n\nJPay provides services that an inmate and an inmate's family and friends can use to communicate, such as video visitation, email, videogram, instant messaging, and a tablet computer (\"J8P5\").\n\nJPay provides payment services for offenders to make community corrections and court-ordered payments. As part of its parole and probationary services, JPay also offers a release card (JPay Progress Card), which is a prepaid, reloadable MasterCard . While all agencies contract to use JPay for money transfer services, they do not all utilize JPay’s full ranges of services.\n\nJPay has been one of the corporate supporters of the Creative Corrections Education Foundation, a nonprofit founded by a former Texas warden, which collects contributions from prison inmates and from corporate sponsors to fund scholarships for children of prison inmates. In 2014 the charity reported having provided $63,000 in scholarships over the previous two years.\n\nIn 2012, the Pennsylvania Department of Banking fined JPay $80,000 for violating the state’s Money Transmitter Act by failing to have the necessary state license since it began operating in the state in 2004. In total, JPay has been fined $408,500 across seven states for operating unlicensed businesses.\n\nIn February 2015, Valerie Buford sued the Indiana Department of Corrections after she learned that her brother, Leon Benson, had been placed in solitary confinement, had good conduct time deducted, and had JPay access revoked after she had reposted a videogram sent via JPay on a Facebook page campaigning for his release. Although access was later restored, Buford continued to argue that the actions had a chilling effect on her ability to communicate with Benson. Officials of the prison claimed the actions were to \"protect\" JPay's intellectual property; at the time, its terms of use stated that JPay held the copyright to any content that was sent through its systems, regardless of its original author. Following a May 2015 posting by the Electronic Frontier Foundation that criticized the company for attempting to abuse copyright law, JPay amended its terms of use to no longer contain this clause.\n\nSenator Cory Booker (Democrat, New Jersey) vowed to ask the Consumer Financial Protection Bureau to intercede on behalf of inmates against JPay's allegedly predatory practices with its prepaid debit release cards which are often the only form in which released prisoners are permitted to collect their prison earnings and remaining commissary balances. Allegedly, the company deducts non-optional fees which can exceed 40% of the funds owed to released inmates.\n\nIn approximately half of the states it operates in, contracts between the various state prison systems or individual prisons and JPay include an agreement to apportion part of the fees, collected from those sending money to the prisoners, back to the prison operators in exchange for giving JPay a monopoly over financial transfers to inmates. According to state mandated disclosures in Illinois, correctional institution operators there received approximately $48,000 for 2013 in what company founder Shapiro prefers to call \"commissions\". One prisoner, released in 2010 following a 28-year incarceration for murder, identified the potential hardship on the poorest families, leaving a mother of three with a husband in jail to decide: \"Do I send money to him so he can afford to stay in touch with the kids, or do I feed the kids?\" Meanwhile, JPay funds lavish parties for corrections institute bureaucrats during the American Correctional Association's annual convention, providing \"tequila, hand-rolled cigars\" and \"live mariachi band\" as well as sponsoring an award, including a trip, presented by the Association of State Correctional Administrators to former state corrections directors.\n\nAt the political level, despite many of JPay's contracts explicitly banning lobbying, Shapiro says the company's lawyers approved JPay's hiring registered state lobbyists and spending $20,000 lobbying Washington in attempts to take the federal prisons' financial transactions contract from Bank of America. A \"Master Contract\" between JPay and the National Association of State Procurement Officials and the Multi-State Corrections Procurement Alliance, valid until July 2015, set out kickback rates, to any state signing on, of 50¢ per inbound money transfer to prisoners, 5¢ per outbound email, $5 per MP3 device, $10 per JP5 Tablet device, and 5% of JPay's music download fees (which are 30% – 50% higher than iTunes).\n"}
{"id": "51113881", "url": "https://en.wikipedia.org/wiki?curid=51113881", "title": "Jennie Hwang", "text": "Jennie Hwang\n\nJennie Hwang (Jennie S. Hwang) is an international business woman, entrepreneur, engineer, scientist, author and international speaker, who has been National President of the Surface Mount Technology Association and heads H-Technologies Group. She is the first woman to receive a PhD from Case Western Reserve University in Materials Science and Engineering. A YWCA award in Cleveland is named for her.; the Award, now for 18 years running, recognizes outstanding women college students who study in STEM (Science, Technology, Engineering, Math) disciplines. \n\nShe is an inductee of the Women in Technology International Hall of Fame. and the Ohio Women's Hall of Fame; a recipient of YWCA Achievement Award; and Distinguished Alumni Awards (Case Western Reserve University Van Horn Award, Kent State University.) She is elected to the National Academy of Engineering(6) and named R&D Stars-to-Watch by the Industry Week (7). \n\nDr. Hwang was the commencement speaker for Kent State University, 2001 and the commencement speaker for Ohio University, 2007, where she encouraged graduates to enrich in global perspectives (8). \n\nHer formal education includes four academic degrees (Ph.D., M.S., M.A., B.S.) and Harvard Business School Executive Program & Columbia University Corporate Governance programs. She has held senior executive positions with Lockheed Martin Corporation, Sherwin Williams Company, SCM Corporation (Hanson PLC) and was the CEO of International Electronic Materials Corporation that she co-founded and was later acquired and the Interim CEO of Asahi America, Inc. \n\nShe has served on a number of boards/committees (public and private companies, government committees, non-profits and industry boards); she is the Chairman of Assessment Board of U.S. Army Research Laboratories commissioned by National Academies and U.S. Department of Defense. She is an invited featured-speaker/lecturer across the U.S. and in over twenty-eight countries in four continents. An author of 500+ publications and several books including two ground-breaking books on environmentally-friendly lead-free electronics, published by Electrochemical Publications, LTD, U.K. and McGraw-Hill, U.S.A., respectively(9). Further: www.JennieHwang.com\n\nWomen in Technology International Hall of Fame: \nhttp://www.witi.com/center/witimuseum/halloffame/143608/Dr.-Jennie-S.-Hwang-International-Businesswomen,-Worldwide-Speaker,-Prolific-Author,-Corp-Director-University-Trustee,-Comminuty-Leader/\n\n(6) National Academies: \nhttp://www.nap.edu/catalog.php?record_id=18254\n\n(7) Industry Week:\nhttp://www.industryweek.com/uncategorized/rd-stars-watch\n\n(8) Commencement Address:\nhttps://www.slideshare.net/JennieHwang1/commencementaddressohio-university200706jennie-hwang-65597295\n\n(9)Books:\nhttps://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=book+by+jennie+hwang\n\n(10) ethw.org/Oral-History:Jennie_S._Hwang\n\n\nNational Academies http://www.nap.edu/catalog.php?record_id=18254\n\n<ref>\n"}
{"id": "6257581", "url": "https://en.wikipedia.org/wiki?curid=6257581", "title": "Magnesium/Teflon/Viton", "text": "Magnesium/Teflon/Viton\n\nMagnesium/Teflon/Viton (MTV) is a pyrolant. Teflon and Viton are trademarks of DuPont for polytetrafluoroethylene, (CF), and fluoroelastomer, (CHCF)(CF(CF)CF).\n\nThermites based on magnesium/Teflon/Viton, aka MTV-compositions, have been in use since the 1950s as payloads in infrared decoy flare applications. See also Countermeasures. Derived from the acronym MTV is the expression \"MTV-Flare\" for pyrotechnic infrared decoy flares.\n\nWhereas in conventional visual pyrotechnic illuminants sodium nitrate, NaNO, is used as an oxidizer, in MTV compositions the polytetrafluoroethylene, (CF), acts as fluorine source. The very high reaction enthalpy, formula_1, upon combustion of magnesium with PTFE is based on the formation of magnesium fluoride, having a very high negative enthalpy of formation ( formula_2 = −1124 kJ mol):\n\nAs lots of carbon and much heat is released upon combustion of MTV the combustion flame can be described as a grey body of high emissivity.\n\nDepending on stoichiometry, MTV displays varying burn rates and yields different reaction products. With constant Viton-content the burn rate increases exponentially with increasing magnesium content. Nevertheless the burn rate of MTV, as is the case with many metallized pyrotechnic compositions is strongly dependent on the specific surface area of the metal fuel, that are particle morphology and dimensions. Generally magnesium powder having a high specific surface area will exhibit a higher burn rate than those having a smaller specific area. The main reactions products for MTV at Mg contents between 30 and 65 wt% magnesium fluoride, soot and vaporized magnesium.\n\nFor aerial decoy flares magnesium rich compositions are used with Mg contents between 55 and 65 wt%. At these stoichiometries only a part of the applied Mg reacts with the PTFE. The surplus Mg is vaporised and reacts with the atmospheric oxygen; likewise the thermally excited soot reacts with the atmospheric oxygen:\n\nPyrotechnic compositions based on magnesium/polytetrafluoroethylene with stoichiometries from 25 wt% to 90 wt% magnesium are, according to German explosive legislation, the Koenen test (steel sleeve test), and BAM impact test, explosive substances. Due to their sensitivity and their reaction behaviour these substances are categorized as group 1.1.2. MTV compositions explode at minimum confinement (also self confinement) at relative low amounts. MTV compositions are sensitive toward thermal ignition.\nIn addition MTV compositions in loose and pressed state are extraordinarily sensitive to friction and electrostatic discharges (ESD). Hence, suitable measures have to be taken to avoid ESD while processing and handling of MTV.\n\nSince aircraft and helicopters could (and still can) counter surface-to-air and air-to-air missiles with the substance, MTV was a classified issue until the mid-1980s. It was not until 1997 that the U.S. government released a formerly classified invention, (filing year 1957), that originally described the properties and applications of MTV.\n\nAlthough missile development has brought up seeker countermeasures against MTV flares there are still numerous missile systems fielded worldwide based on 1st Generation technology. Hence MTV flares are still not obsolete in fighting unknown threats. Together with advanced spectral flares (see countermeasures) they are part of the so-called \"cocktail solution\".\n\nE.-C. Koch, \"Metal-Fluorocarbon Based Energetic Materials\", Wiley-VCH, 2012, 360 pages \n"}
{"id": "3168401", "url": "https://en.wikipedia.org/wiki?curid=3168401", "title": "Magnetic flux leakage", "text": "Magnetic flux leakage\n\nMagnetic flux leakage (TFI or Transverse Field Inspection technology) is a magnetic method of nondestructive testing that is used to detect corrosion and pitting in steel structures, most commonly pipelines and storage tanks. The basic principle is that a powerful magnet is used to magnetize the steel. At areas where there is corrosion or missing metal, the magnetic field \"leaks\" from the steel. In an MFL (or Magnetic Flux Leakage) tool, a magnetic detector is placed between the poles of the magnet to detect the leakage field. Analysts interpret the chart recording of the leakage field to identify damaged areas and to estimate the depth of metal loss.\n\nThere are many methods of assessing the integrity of a pipeline. In-line-Inspection (ILI) tools are built to travel inside a pipeline and collect data as they go. The type of ILI we are interested in here, and the one that has been in use the longest for pipeline inspection, is the magnetic flux leakage inline inspection tool (MFL-ILI). MFL-ILIs detect and assess areas where the pipe wall may be damaged by corrosion. The more advanced versions are referred to as \"high-resolution\" because they have a large number of sensors. The high-resolution MFL-ILIs allow more reliable and accurate identification of anomalies in a pipeline, thus, minimizing the need for expensive verification excavations (i.e. digging up the pipe to verify what the problem is). Accurate assessment of pipeline anomalies can improve the decision making process within an Integrity Management Program and excavation programs can then focus on required repairs instead of calibration or exploratory digs. Utilizing the information from an MFL ILI inspection is not only cost effective but, as well, can also prove to be an extremely valuable building block of a Pipeline Integrity Management Program.\n\nThe reliable supply and transportation of product in a safe and cost-effective manner is a primary goal of most pipeline operating companies and managing the integrity of the pipeline is paramount in maintaining this objective. In-line-inspection programs are one of the most effective means of obtaining data that can be used as a fundamental base for an Integrity Management Program. There are many types of ILI tools that detect various pipeline defects, but high-resolution MFL tools are becoming more prevalent as its applications are surpassing those to which it was originally designed. Originally designed for detecting areas of metal loss, the modern High Resolution MFL tool is proving to be able to accurately assess the severity of corrosion features, define dents, wrinkles, buckles, and, in some cases, cracks. Having a device that can perform simultaneous tasks reliably is more efficient and ultimately provides cost saving benefits.\n\nBackground and origin of the term \"pig\":\nIn the field, a device that travels inside a pipeline to clean or inspect it is typically known as a pig. PIG is an acronym for \"Pipeline Inspection Gauge\". The acronym PIG came later as the nickname for \"pig\" originated from cleaning pigs (first designed pigs) that actually sounded like squealing or screeching pigs when they passed through the lines scraping, scrubbing and \"squeegeeing\" the internal surface. The name serves as common industry jargon for all pigs, both intelligent tools and cleaning tools. Pigs, in order to fit inside the pipeline, are cylindrical and are necessarily short in order to be able to negotiate bends in the pipeline. Many other short, cylindrical objects, such as propane storage tanks, are also known as pigs and it is likely that the name came from the shape of the devices. In some countries a pig is known as a \"Diablo\", literally translated to mean \"the Devil\" relating to the shuddering sound the tool would make as it passed beneath people's feet. The pigs are built to match the diameter of a pipeline and use the very product being carried to end users to transport them. Pigs have been used in pipelines for many years and have many uses. Some separate one product from another, some clean and some inspect. An MFL tool is known as an \"intelligent\" or \"smart\" inspection pig because it contains electronics and collects data real-time while travelling through the pipeline. Sophisticated electronics on board allow this tool to accurately detect features as small as 1 mm by 1 mm, dimensions of the wall of a pipeline as well as depth or thickness of wall (helps indicate potential wall loss).\n\nTypically, an MFL tool consists of two or more bodies. One body is the magnetizer with the magnets and sensors and the other bodies contain the electronics and batteries. The magnetizer body houses the sensors that are located between powerful \"rare-earth\" magnets. The magnets are mounted between the brushes and tool body to create a magnetic circuit along with the pipe wall. As the tool travels along the pipe, the sensors detect interruptions in the magnetic circuit. Interruptions are typically caused by metal loss and which in most cases is corrosion and the dimensions of the potential metal loss is denoted previously as \"feature.\" Other features may be manufacturing defects and not actual corrosion. The feature indication or \"reading\" includes its length by width by depth as well as the o'clock position of the anomaly/feature. Mechanical damage such as shovel gouges can also be detected. The metal loss in a magnetic circuit is analogous to a rock in a stream. Magnetism needs metal to flow and in the absence of it, the flow of magnetism will go around, over or under to maintain its relative path from one magnet to another, similar to the flow of water around a rock in a stream. The sensors detect the changes in the magnetic field in the three directions (axial, radial, or circumferential) to characterize the anomaly. The sensors are typically oriented axially which limits data to axial conditions along the length of the pipeline. Other designs of smart pigs can address other directional data readings or have completely different functions than that of a standard MFL tool. Oftentimes an operator will run a series of inspection tools to help verify or confirm MFL readings and vice versa. An MFL tool can take sensor readings based on either the distance the tool travels or on increments of time. The choice depends on many factors such as the length of the run, the speed that the tool intends to travel, and the number of stops or outages that the tool may experience.\n\nThe second body is called an Electronics Can. This section can be split into a number of bodies depending on the size of the tool. This can, as the name suggests, contains the electronics or \"brains\" of the smart pig. The Electronics Can also contains the batteries and is some cases an IMU (Inertial Measurement Unit) to tie location information to GPS coordinates. On the very rear of the tool are odometer wheels that travel along the inside of the pipeline to measure the distance and speed of the tool.\n\nAs a MFL tool navigates the pipeline a magnetic circuit is created between the pipewall and the tool. Brushes typically act as a transmitter of magnetic flux from the tool into the pipewall, and as the magnets are oriented in opposing directions, a flow of flux is created in an elliptical pattern. High Field MFL tools saturate the pipewall with magnetic flux until the pipewall can no longer hold any more flux. The remaining flux leaks out of the pipewall and strategically placed tri-axial Hall effect sensor heads can accurately measure the three-dimensional vector of the leakage field.\n\nGiven the fact that magnetic flux leakage is a vector quantity and that a hall sensor can only measure in one direction, three sensors must be oriented within a sensor head to accurately measure the axial, radial and circumferential components of an MFL signal. The axial component of the vector signal is measured by a sensor mounted orthogonal to the axis of the pipe, and the radial sensor is mounted to measure the strength of the flux that leaks out of the pipe. The circumferential component of the vector signal can be measured by mounting a sensor perpendicular to this field. Earlier MFL tools recorded only the axial component but high-resolution tools typically measure all three components. To determine if metal loss is occurring on the internal or external surface of a pipe, a separate eddy current sensor is utilized to indicate wall surface location of the anomaly. The unit of measure when sensing an MFL signal is the gauss or the tesla and generally speaking, the larger the change in the detected magnetic field, the larger the anomaly.\n\nThe primary purpose of a MFL tool is to detect corrosion in a pipeline. To more accurately predict the dimensions (length, width and depth) of a corrosion feature, extensive testing is performed before the tool enters an operational pipeline. Using a known collection of measured defects, tools can be trained and tested to accurately interpret MFL signals. Defects can be simulated using a variety of methods.\n\nCreating and therefore knowing the actual dimensions of a feature makes it relatively easy to make simple correlations of signals to actual anomalies found in a pipeline. When signals in an actual pipeline inspection have similar characteristics to the signals found during testing it is logical to assume that the features would be similar. The algorithms and neural nets designed for calculating the dimensions of a corrosion feature are complicated and often they are closely guarded trade secrets. An anomaly is often reported in a simplified fashion as a cubic feature with an estimated length, width and depth. In this way, the effective area of metal loss can be calculated and used in acknowledged formulas to predict the estimated burst pressure of the pipe due to the detected anomaly.\n\nAnother important factor in the ongoing improvement of sizing algorithms is customer feedback to the ILI vendors. Every anomaly in a pipeline is unique and it is impossible to replicate in the shop what exists in all cases in the field. Open lines of communication usually exist between the inspection companies and the pipeline operators as to what was reported and what was actually visually observed in an excavation.\n\nAfter an inspection, the collected data is downloaded and compiled so that an analyst is able to accurately interpret the collected signals. Most pipeline inspection companies have proprietary software designed to view their own tool's collected data. The three components of the MFL vector field are viewed independently and collectively to identify and classify corrosion features. Metal loss features have unique signals that analysts are trained to identify.\n\nHigh-resolution MFL tools collect data approximately every 2 mm along the axis of a pipe and this superior resolution allows for a comprehensive analysis of collected signals. Pipeline Integrity Management programs have specific intervals for inspecting pipeline segments and by employing high-resolution MFL tools an exceptional corrosion growth analysis can be conducted. This type of analysis proves extremely useful in forecasting the inspection intervals.\n\nAlthough primarily used to detect corrosion, MFL tools can also be used to detect features that they were not originally designed to identify. When an MFL tool encounters a geometric deformity such as a dent, wrinkle or buckle, a very distinct signal is created due to the plastic deformation of the pipe wall.\n\nThere are cases where large non-axial oriented cracks have been found in a pipeline that was inspected by a magnetic flux leakage tool. To an experienced MFL data analyst, a dent is easily recognizable by trademark \"horseshoe\" signal in the radial component of the vector field. What is not easily identifiable to an MFL tool is the signature that a crack leaves.\n\n\n\n"}
{"id": "13818236", "url": "https://en.wikipedia.org/wiki?curid=13818236", "title": "Micromanipulator", "text": "Micromanipulator\n\nA micromanipulator is a device which is used to physically interact with a sample under a microscope, where a level of precision of movement is necessary that cannot be achieved by the unaided human hand. It may typically consist of an input joystick, a mechanism for reducing the range of movement and an output section with the means of holding a microtool to hold, inject, cut or otherwise manipulate the object as required. The mechanism for reducing the movement usually requires the movement to be free of backlash. This is achieved by the use of kinematic constraints to allow each part of the mechanism to move only in one or more chosen degrees of freedom, which achieves a high precision and repeatability of movement, usually at the expense of some absolute accuracy.\n\nMovement reduction can be performed by mechanical levers, hydraulically using pistons of different diameters connected by tubing containing non-compressible fluid, electronically using stepper motors or linear actuators, or combinations of techniques in one instrument. Mechanisms with different ranges of movement or variable reduction ratio may be incorporated in one instrument to allow coarse and fine positioning.\n\nDepending on the application, users may require different scales of movement resolution, movement speed, range and accuracy. These are the critical variables integrated into manipulator design by manufacturers, which are typically presented to suit particular applications. \n\nMicromanipulators are usually used in conjunction with microscopes. Depending on the application, one or more micromanipulators may be fitted to a microscope stage or rigidly mounted to a bench next to a microscope. A typical application of micromanipulation is human intracytoplasmic sperm injection. Here, a spermatozoon measuring some 3 to 5 micrometres across is injected into an oocyte of approximately 100 micrometres in diameter, under the direct manual control of an embryologist. A disposable glass micropipette is fitted to a toolholder mounted on the output of the manipulator. The toolholder can be adjusted for different sized tools as well as the angle at which the tool is held.\n\nMicromanipulators are also used in applications such as microelectronics to position test probes onto small to medium scale integrated circuits and hybrid devices, and patch clamp experiments in biological research.\n\n"}
{"id": "4856755", "url": "https://en.wikipedia.org/wiki?curid=4856755", "title": "Mosaic notation program", "text": "Mosaic notation program\n\nMosaic (also called Composer's Mosaic) was a Macintosh scorewriter application for producing music notation, developed by Mark of the Unicorn. \n\nFirst released as Professional Composer among early Macintosh software in 1984, the application introduced a user interface similar to the word processor. The main features included entering musical notation, printing sheet music, and support for lyrics under the score with the font of choice. Notes could be selected from the user interface or entered from the keyboard. The user could also change or extend the tempo, key signature, meter, and other parameters.\n\nThe next major release, Professional Composer 2.0, supported writing on up to 40 staves and allowed the user to enter notes as short as 128th notes, with all operations mainly controlled by menus and dialog boxes. Version 2.0 also introduced several improvements for printing (such as automatically condensing parts with several rest measures), allowing production of professional quality scores. Although the application demanded knowledge of music theory to use its rich features, it offered only rudimentary playback capabilities. A \"Macworld\" review also criticized the high price (495 USD in February 1986) and the lack of automatic scrolling when staves were filled (only via scroll bars). \n\nVersion 2.2 (1988) corrected several bugs and improved compatibility with Mac Plus, SE and II. Version 2.3M was the last release of Professional Composer. \n\nMosaic entered the market in 1992 as the successor to Professional Composer. An early user review of version 1.01 criticized stability issues and problems with file importing from other applications. In version 1.58 released in 1998, the notation software removed all limits on page size, score length, number of staves, and number of voices per staff. Configuration options in different windows created a flexible but sometimes confusing user interface. Drag and drop features and ability to convert MIDI files into usable notation were counted among the strongest points of Mosaic.\n\nAfter 1998, no new versions of Mosaic were released by MOTU, and because it was not compatible with MacOS 10, it became orphaned technology and abandonware. Competing notation packages are Sibelius, Finale, and Dorico, however no direct conversion of file formats, such as via MusicXML, is possible. Mosaic users now have to rely on creating PDF files of Mosaic output under MacOS 9 and then having these read by OCR programs such as PDFtoMusic and PhotoScore by Neuratron.\n\n"}
{"id": "26835219", "url": "https://en.wikipedia.org/wiki?curid=26835219", "title": "Numerical Wind Tunnel (Japan)", "text": "Numerical Wind Tunnel (Japan)\n\nNumerical Wind Tunnel (数値風洞) was an early implementation of the vector parallel architecture developed in a joint project between National Aerospace Laboratory of Japan and Fujitsu. It was the first supercomputer with a sustained performance of close to 100 Gflop/s for a wide range of fluid dynamics application programs. It stood out at the top of the TOP500 during 1993-1996. With 140 cores, the Numerical Wind Tunnel reached a Rmax of 124.0 GFlop/s and a Rpeak of 235.8 GFlop/s in November 1993.\n\nIt consisted of parallel connected 166 vector processors with a gate delay as low as 60 ps in the Ga-As chips. The resulting cycle time was 9.5 ns. The processor had four independent pipelines each capable of executing two Multiply-Add instructions in parallel resulting in a peak speed of 1.7 Gflop/s per processor. Each processor board was equipped with 256 Megabytes of central memory.\n"}
{"id": "24664942", "url": "https://en.wikipedia.org/wiki?curid=24664942", "title": "PICMG 2.11", "text": "PICMG 2.11\n\nPICMG 2.11 is a specification by PICMG that defines the electrical and mechanical requirements relating to plug-in power modules in CompactPCI systems.\n\nAdopted : 10/1/1999\n\nCurrent Revision : 1.0\n"}
{"id": "44434270", "url": "https://en.wikipedia.org/wiki?curid=44434270", "title": "Photonic laser thruster", "text": "Photonic laser thruster\n\nA photonic laser thruster (PLT) is an amplified photonic propulsion thruster for space propulsion that works on the principle of a photon-pushed sail, generating thrust directly from the momentum of a photon from a laser reflected from a mirror. The thruster, invented by Young K. Bae differs from other solar sail and laser propulsion thrusters in that an amplification process is used, in which the incident beam is re-used by being reflected by a stationary mirror, with an amplification stage at each reflection.\nBecause of the recycling of energy, the photonic laser thruster has been demonstrated to be more energy efficient than other laser-pushed sail concepts.\n\nThe near-term usage of the photonic laser thruster the earth-orbit applications include propellant-free and thrust-plume-contamination-free spacecraft maneuvering for precision formation flying, large optical and RF synthetic aperture construction, and stationkeeping. The usage of the photonic laser thruster for main space propulsion would require scaling-up of the laser power and controlling laser diffraction over interplanetary and interstellar distances. Photonic laser thrusters have a very high specific impulse, and can permit spacecraft to reach much higher speeds that approach a fraction of the light speed, unlike conventional rockets, which are limited by the Tsiolkovsky rocket equation.\n\nThe use of light for propulsion has been researched since the beginning of the 20th century, with the analysis of a sail pushed by the pressure of sunlight by Friedrich Tsander. Photon propulsion has been discussed for decades as a propulsion that could enable interstellar flight.\n\nIn the traditional photonic propulsion, such as laser- or microwave-pushed lightsails, photons transfer their momentum to the sail by reflection. Since, for a sail moving slowly with respect to the speed of light, very little of the energy of the photon is lost on reflection, a theoretical way to increase the efficiency is by recycling photons, bouncing the reflected photons back to the sail by a mirror, resulting in repetitively cycling the photons between two reflective mirrors in an optical cavity.\n\nThe photonic thruster utilized this approach of recycling the reflected light, but with the addition of an active material amplifying the beam.\n\nThe Photonic Laser Thruster (PLT) was invented and developed by Young Bae and his team at Y.K. Bae Corp for use in nanometer precision spacecraft formation under a NASA Institute for Advanced Concepts (NIAC) grant in 2006, for forming space telescopes and radars. Initial proposed uses include high-precision and high-speed maneuver of small spacecraft, such as formation flying, orbit adjustments, and drag compensation. In this research Bae discovered that photonic laser thruster was stable against movements of laser mirrors and proved that it can be used for main propulsion. In this program he demonstrated a 100-fold improvement in conversion efficiency, reaching a photon thrust of 35 micronewtons by putting the laser-energizing medium between two mirrors as in typical lasers.\n\nIn August 2015, under another NASA program (NIAC, NASA Innovative Advanced Concepts) he demonstrated additional 100-fold improvement, achieving a photon thrust of 3.5 millinewtons. In addition, a small 1U CubeSat satellite was propelled and stopped in simulated zero-gravity. The laser power of the trapped photon beam exceeded 500 kW, which was powered by a 500 W laser. The concept is proposed for beaming thrust from a conventional heavy \"tanker\" vehicle to a more expensive, lightweight mission vehicle, similar to aerial refueling.\n\nThe limitations posed by the rocket equation can be overcome, as long as the reaction mass is not carried by the spacecraft. In the Beamed Laser Propulsion (BLP) concept, the photons are beamed from the photon source to the spacecraft as coherent light. Robert L. Forward pioneered interstellar propulsion concepts including photon propulsion and antimatter rocket propulsion. Specifically, Forward introduced beamed laser propulsion, aiming at the goal of achieving roundtrip manned interstellar travel.\n\nMarx, Redding and Simmons and McInnes calculated that the energy conversion efficiency of terrestrial laser-driven propulsion is approximately proportional to v/c at low speeds (v<0.1c), thus is small at low speeds (v«0.1c). However, at higher speeds (v>0.1c), owing to the favorable Doppler shift energy transfer, onboard photon propulsion becomes much more energy efficient.\n\nPhotons transfer their energy to the spacecraft by redshifting due to Doppler shift upon reflection, thus the higher the spacecraft speed, the higher the efficiency. The figure shows the energy transfer efficiency from photons to the spacecraft's kinetic energy as a function of β=v/c (the spacecraft velocity divided by the light velocity) in photon propulsion. As the spacecraft velocity approaches the light velocity (v≈c), the efficiency of photon propulsion approaches 100%, as if the spacecraft acts like a black hole in the moving direction. The lower solid curve in the figure represents the efficiency of conventional photon rocket or sail with photon recycling. The upper solid line represents schematically an example the efficiency of a recycling photon rocket, such as a PLT. At low β, the recycling rocket can have a high thrust amplification factor (in this example, ~3,000), however as β approaches 1, the amplification factor converges to 1 and the overhead of recycling is unneeded. Therefore, these rockets are projected to bridge the efficiency gap. The simplest recycling scheme is a Herriot cell with multi-bouncing laser beams between two high reflectance mirrors that do not form a resonant optical cavity as illustrated in Figure 4. This cell type approach was first proposed by Meyer, et al., followed by Simmons and McInnes. Mertzger and Landis proposed a multi-bounce lightsail craft, such that the beam is reflected back and forth between the lightsail and a source reflector. Advanced reflectors permit more than 1000 bounces, reducing power requirements by 1000x compared to single bounce proposals. Using 100 MW to 1 GW lasers, a sub-100 day Mars transit is possible. The first experimental attempt on photon thrust amplification in a non-resonant Herriot-cell type optical cavity was performed by Gray et al. who obtained amplified photon thrust of ~0.4 µN with a 300-W laser and a photon thrust amplification factor of ~2.6.\n\nThe passive resonator in the Fabry-Perot interferometer has been extensively used in high-sensitivity optical detection methods, such as the cavity ring-down spectroscopy. One experiment produced 20,000 photon bounces using mirrors with 0.99995 reflectance. in which even one nanometer perturbation in cavity length destroys the resonance and nulls the photon thrust. The injection of laser power into the cavity remains challenging.\n\nMeyer \"et al\". concluded that for interstellar flights, recycling photon propulsion vehicles are much more energy efficient than an onboard photon rocket, such as the nuclear photonic rocket. They proposed possible applications of photon recycling using passive resonant optical cavities (lasers outside the optical cavity), such as the Laser Elevator.\n\nAfter 2000 Bae began to investigate photon recycling for use in a nanometer accuracy formation flight, for a NASA-NIAC project called Photon Tether Formation Flight (PTFF). The goal was to sustain fixed-formation flight with a baseline distance between craft of over 10 km, for next generation NASA space missions. In 2006 Bae investigated active resonant optical cavities, in which the optical gain medium is located within the cavity, and coined the term \"photonic laser thruster\" (PLT) for such thrusters. Initially PLT was proposed. It was assumed that the cavity would be sensitive to the stability of the mirrors and other optical parameters as in passive cavities. Bae discovered that he could sustain the resonance with the mirror in his hand. This would be impossible if the cavity were highly sensitive to cavity perturbations such as moving and tilting. He concluded the gain medium in the cavity actively stabilizes the recycling photon beam, through negative feedback. This encouraged the idea that these thrusters could enable a range of applications well beyond PTFF. In December 2006, a proof-of-concept of PTFF was demonstrated in a laboratory environment. Figure 6 illustrates the first demonstration setup, consisting of a concave High Reflectance (HR) mirror, a Diode Pumped Solid State laser gain medium and a flat Output Coupler (OC) mirror. The photon thrust was determined by measuring the difference between the weight of the HR mirror with laser on and that with laser off with the use of a digital scale. Below is an infrared picture of the thruster in action. \n\nFigure 7 shows the experimental results of the photon thrust measured as a function of the laser power output through the OC mirror, P, with reflectance greater than 0.997 according to the manufacturer’s specification. Curve fitting the data resulted in the specific thrust of 20±1 µN/W, resulting in the apparent photon momentum multiplication factor of 2,990±150 and the true OC mirror reflectance used in this demonstration of 0.99967±0.00002. The maximum photon thrust demonstrated was 35 μN. When the demonstration setup was operating at thruster levels near or beyond 35 µN, the setup became unstable and the gain medium overheated. Therefore, it was realized that thermal management is critical to scaling such thrusters.\n\nThe Doppler shift limits the maximum obtainable velocity of the accelerating mirror and its accommodating spacecraft. Doppler shift effect on the active resonant cavity behavior is an extremely complicated issue. Optical gain in the laser cavity can only occur for a finite range of optical frequencies. The gain bandwidth is approximately the width of this frequency range. For example, the gain bandwidth of the YAG laser system with the laser wavelength in the order of 1,000 nm is in the order of 0.6 nm, ~ 0.06% of the wavelength. For an order of magnitude estimation, it can be assumed that a thruster utilizing the YAG laser system will be limited by the gain bandwidth to the first order, i.e., its theoretical maximum spacecraft velocity is ~ (180 km/s), 0.06% of the light velocity, c=3x10 m/s. To overcome this redshift limitation, at high operation velocities, wide bandwidth lasers should be used.\n\nGreater thrust requires higher power lasers. Lasers are often designed to maximize power outside the laser cavity, while PLT calls for maximizing laser power inside the cavity. The necessary laser operation parameters to maximize intracavity power (circulating power) in laser cavities are still poorly understood.\nBae and colleagues chose a thin disk laser and began to develop a PLT in 2013. Typically, a thin disk laser with a 0.2 mm disk thickness can be operated with a 99% output coupler, and provide an intracavity power 100 times larger than extracavity power. A schematic diagram of this design is shown in Figure 8. A thin disk gain medium is coated with HR with a reflectivity of up to 99.999% and attached to a heat sink. The recycling photons between the gain medium and the HR mirror located in the mission platform deliver amplified thrust beaming from the resource platform.\n\nIn 2014, Bae's group, working under a NASA program (NIAC, NASA Innovative Advanced Concepts), demonstrated intracavity power of 154 kW with a 0.6 cm diameter thin disk laser, which can be translated into a photon thrust of 1.03 mN. In April 2015, the group successfully measured photon thrust up to 1.1 mN with a digital scale. They were able to accelerate, slow and stop a 0.45 kg spacecraft-simulating platform along a 2m frictionless air track in a Class-1,000 cleanroom. In August 2015, under the NASA program the group successfully propelled a Cube satellite, and demonstrated and measured photon thrust up to 3.5 mN with the use of the newly developed NIST/Scientech/Navy radiation pressure sensor. An intracavity power of the photonic laser thruster over 500 kW was demonstrated with 500 W laser pumping.\n\nTo date the experimental tests of the photonic thruster are limited to laboratory-scale distances. The maximum range of operation for a photonic laser thruster, PLT, is yet to be established. Bohn demonstrated a 1 km-long laser resonator similar to the PLT cavity, which is an active cavity, in 1995 and proposed that such resonators could scale to 100 km. Recently, 4-km  Fabry–Pérot cavities, which are passive cavities, but share the same intracavity power multiplication principle with the PLT cavity, have been demonstrated in LIGO for gravitational wave detection with an intracavity multiplication factor of 280 and an intracavity laser power on the order of 100 kW. Based on these results and the state-of-the-art technologies in precision optics, the PLT cavity length over 1,000 km is promising. Further studies need to be performed to determine whether the PLT cavity could be scaled for astronomical distances.\n\nLikewise, use of the amplifying cavity has not yet been demonstrated for the case when the sail is moving at velocities approaching fractions of the light velocity. The Doppler shift of the moving sail will, as noted earlier, mean that the amplification medium will need to operate at a different wavelength for each pass of the photons through the system. The mode of the Fabry-Perot cavity will likewise be changing with time.\n\nAnother issue is intracavity laser beam aiming, aligning and tracking over long distances. Progress in directed energy weapons has improved these capabilities. The speed of light limits how quickly tracking information can travel between the two cavity ends over long distances.\n\nAt speeds higher than rocket speeds lack of brakes is a problem. Such a problem can be solved by installing another photonic laser thruster at the destination as in photonic railway. The braking of spacecraft with PLT in lab has been recently demonstrated.\n\nPLTs are studied for maneuvering spacecraft in near earth orbit, propellantless operation, thrust and power beaming for \"perpetual\" stationkeeping, and ultra-precision spacecraft formation flying, with or without tethers.\n\nNorman and Peck analyzed a group of spacecraft that can exploit relative positions and velocities so that differential gravity provides a force opposite to the photon thrust from a PLT. In such a scheme, with two orbiting platforms moving in formation, when the photon thrust changes, their positions relative to the center of mass change as well, until the \"virtual gravitational tug\" counterbalances it again. Existing concepts with conventional thrusters are not persistent, are mechanically constrained (e.g. with tethers), or lie within the orbital plane, limiting their aperture for earth-observation missions. This system in contrast could control of out-of-plane motions. Figure 9 shows examples of \"virtual tug\" satellite formations for forming large synthetic apertures in Low Earth Orbit.\n\nSimilarly, small orbital craft that need to correct for orbital drag could do this with significantly less propellant, by moving a larger resource vehicle into a similar orbit with low inter-vehicle velocity. The replacement of such a resource vehicle could be faster and more economical.\n\nBae proposed a 4-staged developmental map towards interstellar flight. This would involve the photonic railway, a permanent energy-efficient transportation infrastructure based on Photonic Laser Thruster (PLT) in combination with Forward's model of beamed-laser propulsion (BLP), a PLT-BLP hybrid. This would reduce the cost and duration of interstellar commutes via proposed spacetrains. The Stage-1 focuses on the near-earth space endeavors, such as orbit tuning, with Photonic Laser Thruster. The Stage-2 focuses on Interlunar photonic railway between the earth and the moon. The Stage-3 focuses on Interplanetary photonic railway between the earth and planets, moons and asteroids in the solar system. The Stage-4 focuses on Interstellar photonic railway between the planets and moons of the solar system and those of other star systems.\n\nIf the photonic laser thruster is scalable for the use in such main space propulsion, multiple photonic laser thrusters can be used to construct a photonic railway that has been proposed as a potential permanent transport infrastructure for interplanetary or interstellar commutes, allowing the transport craft themselves to carry very little or no fuel.\nThe photonic railway was investigated for a space applications involving planets, moons and asteroids both in the solar system and other star systems, such as mining and setting up permanent habitation, with regular interplanetary and interstellar travel.\n\nFor example, platforms could be built in Earth orbit and then used for constructing further platforms at one of the Lagrange points of a planet of interest. For Mars, solar power is still strong, thus solar pumped platforms could be operated near Mars. Planets farther from the sun might not support solar pumping, and the railway would involve two PLT-BLP systems near Earth.\n\nBae compared the energy need to speed spacecraft for conventional rockets and that for photonic railway or PLT-BLP, in terms of specific energy (J/kg) that is the energy required for propelling a unit mass to a given velocity. Fig. 11 shows examples of the specific energy (J/kg) as a function of the spacecraft velocity (km/s) related with the Mars photonic railway. Two curves represent the specific energies for conventional rockets with I = 500 s and 3,000 s, respectively. The upper straight solid line represents the specific energy for BLP and the lower straight solid line for photonic railway or PLT-BLP with M=1,000. Here M is the photon thrust amplification factor. It is interesting that BPL becomes more energy efficient than rockets with I=500 s, if the travel time needs to be shorter than 1 month. BPL becomes more energy efficient than rockets with I=3,000 s, if the travel time is shorter than a week. However, photonic railway or PLT-BPL with M=1,000 becomes more energy efficient than rockets with I=500 s, if the travel time needs to be shorter than 2 month. BPL becomes more energy efficient than rockets with I=3,000 s, if the travel time needs to be shorter than two weeks. Eventually, when the flight time needs to be 3 days, for example, both BLP and photonic railway or PLT-BLP are more energy efficient than rockets with I=3,000 s. This estimate demonstrates photonic railway is potentially the most energy efficient way to commute to planets in the solar system.\n\nOne interesting aspect of the Spacetrain on the photonic railway was realized that the continuous low level acceleration of up to 1 g will create an artificial gravity that may eliminate or minimize the health effects of weightlessness long term space travel in zero-gravity environment. This artificial gravity will play a crucial role in reducing or eliminating the health problems arising from the typical zero-g space travel environment.\n\nA simple PLT system could provide continuous and constant thrust in a straight line. However, travel around the solar system involves interacting with planets and the sun, so trajectories and travel time calculations are more complex. Fu-Yuen Hsiao has investigated the trajectories of spacecraft relying entirely on a PLT.\n\nBae's investigation concluded that the development of interplanetary and interstellar photonic railway will require development of the ways to utilize Photon Bose Einstein Condensation or diffraction-minimized laser beam propagation with non-diffracting beams, such as Bessel beam, or x-ray lasers and future advanced material science and technologies. Bae further concluded that the realization of the interstellar photonic railway would require that the PLT technology developments ride on the Moore’s law as the 20th century silicon devices did.\n\n"}
{"id": "32328672", "url": "https://en.wikipedia.org/wiki?curid=32328672", "title": "Piazza (web service)", "text": "Piazza (web service)\n\nPiazza is a Q&A web service. It can be described as \"mixture between a wiki and a forum\" that can be used with learning management systems. The word Piazza comes from the Italian word for plaza—a common city square where people can come together to share ideas and knowledge.\n\nPooja Sankar created the first prototype of Piazza in 2009. By February 2010, Piazza was used by approximately 600 Stanford students. In January 2011, Piazza opened to all institutions, reaching over 330 schools and tens of thousands of students by the summer of the same year.\n\nWhile the service originally operated under the name 'Piazzza', in June 2011, the third 'z' was dropped from Piazza's name.\n\n\nUsers can publicly (and anonymously, if the head instructor allows it) ask questions, answer questions, and post notes. Each question prompts a collective answer to which any user can contribute and an instructor answer, shown directly below, which can only be edited by instructors. Multiple students are allowed contribute to each answer like the Wikipedia entries, and each answer have a version history that shows what each student wrote. Users are allowed to attach external files to posts, use LaTeX formatting, view a post's edit history, add follow-up questions, and receive email notifications when new content is added. The interface consists of a dynamic list of posts on the left side of the screen, a central panel for viewing and contributing to individual posts, and an upper bar for account control. According to the company's data, the average Piazza question is answered within 14 minutes.\n\nIndividual Piazza classes are self-contained and can be locked with an access code. Anyone may create a class, but the head instructor retains full control over the class content, along with administrative abilities such as endorsing good answers and viewing more detailed statistics on class activity.\n\nThe Piazza team is based in Palo Alto, California.\n\nIn November 2011, Piazza launched iOS and Android mobile apps which would allow students share their ideas and get their queries answered.\n\n\n"}
{"id": "31639683", "url": "https://en.wikipedia.org/wiki?curid=31639683", "title": "Picramic acid", "text": "Picramic acid\n\nPicramic acid, also known as 2-amino-4,6-dinitrophenol, is an acid obtained by neutralizing an alcoholic solution of picric acid with ammonium hydroxide. Hydrogen sulfide is then added to the resulting solution, which turns red, yielding sulfur and red crystals. These are the ammonium salts of picramic acid, from which it can be extracted using acetic acid.\n\nPicramic acid is explosive and very toxic. It has a bitter taste.\n"}
{"id": "5754502", "url": "https://en.wikipedia.org/wiki?curid=5754502", "title": "Prefabricated building", "text": "Prefabricated building\n\nA prefabricated building, informally a prefab, is a building that is manufactured and constructed using prefabrication. It consists of factory-made components or units that are transported and assembled on-site to form the complete building.\n\nBuildings have been built in one place and reassembled in another throughout history. This was especially true for mobile activities, or for new settlements; one of the first buildings at Cape Ann in 1624 was probably partially prefabricated, and was rapidly disassembled and moved at least once. John Rollo described in 1801 earlier use of portable hospital buildings in the West Indies Possibly the first advertised prefab house was the \"Manning cottage\". A London carpenter, Henry Manning, constructed a house that was built in components, then shipped and assembled by British emigrants. This was published at the time (advertisement, South Australian Record, 1837) and a few still stand in Australia. One such is the Friends Meeting House, Adelaide. \nThe peak year for the importation of portable buildings to Australia was 1853, when several hundred arrived. These have been identified as coming from Liverpool, Boston and Singapore (with Chinese instructions for re-assembly). In Barbados the Chattel house was a form of prefabricated building which was developed by emancipated slaves who had limited rights to build upon land they did not own. As the buildings were moveable they were legally regarded as chattels.\n\nIn 1855 during the Crimean War, after Florence Nightingale wrote a letter to \"The Times\", Isambard Kingdom Brunel was commissioned to design a prefabricated modular hospital. In five months he designed the Renkioi Hospital: a 1,000 patient hospital, with innovations in sanitation, ventilation and a flushing toilet. Fabricator William Eassie constructed the required 16 units in Gloucester Docks, shipped directly to the Dardanelles. Only used from March 1856 to September 1857, it reduced the death rate from 42% to 3.5%.\n\nThe world's first prefabricated, pre-cast panelled apartment blocks were pioneered in Liverpool. A process was invented by city engineer John Alexander Brodie, whose inventive genius also had him inventing the football goal net. The tram stables at Walton in Liverpool followed in 1906. The idea was not extensively adopted in Britain, however was widely adopted elsewhere, particularly in Eastern Europe.\n\nPrefabricated homes were produced during the Gold Rush in the United States, when kits were produced to enable Californian prospectors to quickly construct accommodation. Homes were available in kit form by mail order in the United States in 1908.\n\nPrefabricated housing was popular during the Second World War due to the need for mass accommodation for military personnel. The United States used Quonset huts as military buildings, and in the United Kingdom prefabricated buildings used included Nissen huts and Bellman Hangars. 'Prefabs' were built after the war as a means of quickly and cheaply providing quality housing as a replacement for the housing destroyed during the Blitz. The proliferation of prefabricated housing across the country was a result of the Burt Committee and the Housing (Temporary Accommodation) Act 1944. Under the Ministry of Works Emergency Factory Made housing programme, a specification was drawn up and bid on by various private construction and manufacturing companies. After approval by the MoW, companies could bid on Council led development schemes, resulting in whole estates of prefabs constructed to provide accommodation for those made homeless by the War and ongoing slum clearance. Almost 160,000 had been built in the UK by 1948 at a cost of close to £216 million. The largest single prefab estate in Britain was at Belle Vale (South Liverpool), where more than 1,100 were built after World War 2. The estate was demolished in the 1960s amid much controversy as the prefabs were very popular with residents at the time.\n\nPrefabs were aimed at families, and typically had an entrance hall, two bedrooms (parents and children), a bathroom (a room with a bath)  — which was a novel innovation for many Britons at that time, a separate toilet, a living room and an equipped (not \"fitted\" in the modern sense) kitchen. Construction materials included steel, aluminium, timber or asbestos, depending on the type of dwelling. The aluminium \"Type B2\" prefab was produced as four pre-assembled sections which could be transported by lorry anywhere in the country.\n\nThe Universal House (pictured left & lounge diner right) was given to the Chiltern Open Air Museum after 40 years temporary use. The Mark 3 was manufactured by the Universal Housing Company Ltd, Rickmansworth.\n\nThe United States used prefabricated housing for troops during the war and for GIs returning home. \"Prefab\" classrooms were popular with UK schools increasing their rolls during the baby boom of the 1950s and 1960s.\n\nMany buildings were designed with a five-ten year life span, but have far exceeded this, with a number surviving today. In 2002, for example, the city of Bristol still had residents living in 700 examples. Many UK councils have been in the process of demolishing the last surviving examples of Second World War prefabs in order to comply with the British government's Decent Homes Standard, which came into effect in 2010. There has, however, been a recent revival in prefabricated methods of construction in order to compensate for the United Kingdom's current housing shortage.\n\nArchitects are incorporating modern designs into the prefabricated houses of today. Prefab housing should no longer be compared to a mobile home in terms of appearance, but to that of a complex modernist design. There has also been an increase in the use of \"green\" materials in the construction of these prefab houses. Consumers can easily select between different environmentally friendly finishes and wall systems. Since these homes are built in parts, it is easy for a home owner to add additional rooms or even solar panels to the roofs. Many prefab houses can be customized to the client's specific location and climate, making prefab homes much more flexible and modern than before.\n\nThere is a zeitgeist in architectural circles and the spirit of the age favors the small carbon footprint of \"prefab.\" Eminent amongst the new breed of off the shelf luxury modernist products is the perrinepod, which has found favor worldwide for its green credentials and three-day build time.\n\nMany eastern European countries had suffered physical damage during World War Two and their economies were in a very poor state. There was a need to reconstruct cities which had been severely damaged due to the war. For example, Warsaw, Poland had been practically razed to the ground under the planned destruction of Warsaw by German forces after the 1944 Warsaw Uprising. The centre of Dresden, Germany had been totally destroyed by the 1945 Allied bombardment. Stalingrad had been largely destroyed and only a small number of structures were left standing.\n\nPrefabricated buildings served as an inexpensive and quick way to alleviate the massive housing shortages associated with the wartime destruction and large-scale urbanization and rural flight.\n\nBroad Sustainable Buildings of Changsha, China has been continuously breaking records on building skyscrapers in record times which went viral in YouTube. Now they aim to build Sky City, a mega tall skyscraper in record time.\n\nMcDonald's uses prefabricated structures for their buildings, and set a record of constructing a building and opening for business within 13 hours (on pre-prepared ground works).\n\nIn the UK, the major supermarkets have each developed a modular unit system to shop building, based on the systems developed by German cost retailler Aldi and the Danish supermarket chain Netto.\n\n\n"}
{"id": "34438789", "url": "https://en.wikipedia.org/wiki?curid=34438789", "title": "PureCell System", "text": "PureCell System\n\nThe PureCell System is a stationary phosphoric acid fuel cell designed, manufactured and marketed by Doosan Fuel Cell America (formerly ClearEdge Power/UTC Power) of South Windsor, Connecticut. Intended for distributed generation and micro combined heat and power applications, it is considered a good match for commercial and industrial buildings such as hotels, hospitals, data centers, supermarkets and educational institutions. PureCell System says that its users will see lower energy costs, reduced emissions, 95% system efficiency, 10-year cell stack durability and 20-year product life.\nIt utilizes a combustion-free process with natural gas and converts heat exhaust into cooling and heating, turning potential waste into usable energy.\n\nOn August 17, 2011, it was announced that the PureCell Model 400 system fleet had reached 200,000 hours of field operation.\nIn September 2013, ClearEdge Power announced that its 400 kW stationary fuel cell surpassed 1,000,000 hours of field operation.\n\nClearEdge Power’s chief operating officer Joe Triompo said that PureCell System repeat customers include The Coca-Cola Company, Cox Communications, and Whole Foods Market.\n\nOther customers include First National Bank of Omaha, Price Chopper Supermarkets, the Condé Nast Building (officially, 4 Times Square), Shaw's, GS Power Co./Samsung, Becker + Becker, St. Helena Hospital, the World Trade Center, The Octagon, New Haven City Hall, South Windsor High School, 360 State Street, Albertsons, the University of Connecticut Diversey, Inc., and Eastern Connecticut State University.\n\n"}
{"id": "158715", "url": "https://en.wikipedia.org/wiki?curid=158715", "title": "Radiator", "text": "Radiator\n\nRadiators are heat exchangers used to transfer thermal energy from one medium to another for the purpose of cooling and heating. The majority of radiators are constructed to function in automobiles, buildings, and electronics. The radiator is always a source of heat to its environment, although this may be for either the purpose of heating this environment, or for cooling the fluid or coolant supplied to it, as for engine cooling. Despite the name, most radiators transfer the bulk of their heat via convection instead of thermal radiation. \n\nThe Roman hypocaust is an early example of a type of radiator for building space heating. Franz San Galli, a Prussian-born Russian businessman living in St. Petersburg, is credited with inventing the heating radiator around 1855, having received a radiator patent in 1857, but American Joseph Nason developed a primitive radiator in 1841 and received a number of U.S. patents for hot water and steam heating.\n\nHeat transfer from a radiator occurs by all the usual mechanisms: thermal radiation, convection into flowing air or liquid, and conduction into the air or liquid. A radiator may even transfer heat by phase change, for example, drying a pair of socks. In practice, the term \"radiator\" refers to any of a number of devices in which a liquid circulates through exposed pipes (often with fins or other means of increasing surface area). The term \"convector\" refers to a class of devices in which the source of heat is not directly exposed.\n\nTo increase the surface area available for heat exchange with the surroundings, a radiator will have multiple fins, in contact with the tube carrying liquid pumped through the radiator. Air (or other exterior fluid) in contact with the fins carries off heat. If air flow is obstructed by dirt or damage to the fins, that portion of the radiator is ineffective at heat transfer.\n\nRadiators are commonly used to heat buildings. In a central heating system, hot water or sometimes steam is generated in a central boiler and circulated by pumps through radiators within the building, where this heat is transferred to the surroundings.\n\nRadiators are used for cooling internal combustion engines, mainly in automobiles but also in piston-engined aircraft, railway locomotives, motorcycles, stationary generating plants and other places where such engines are used.\n\nTo cool down the engine, a coolant is passed through the engine block, where it absorbs heat from the engine. The hot coolant is then fed into the inlet tank of the radiator (located either on the top of the radiator, or along one side), from which it is distributed across the radiator core through tubes to another tank on the opposite end of the radiator. As the coolant passes through the radiator tubes on its way to the opposite tank, it transfers much of its heat to the tubes which, in turn, transfer the heat to the fins that are lodged between each row of tubes. The fins then release the heat to the ambient air. Fins are used to greatly increase the contact surface of the tubes to the air, thus increasing the exchange efficiency. The cooled coolant is fed back to the engine, and the cycle repeats. Normally, the radiator does not reduce the temperature of the coolant back to ambient air temperature, but it is still sufficiently cooled to keep the engine from overheating.\n\nThis coolant is usually water-based, with the addition of glycols to prevent freezing and other additives to limit corrosion, erosion and cavitation. However, the coolant may also be an oil. The first engines used thermosiphons to circulate the coolant; today, however, all but the smallest engines use pumps.\n\nUp to the 1980s, radiator cores were often made of copper (for fins) and brass (for tubes, headers, and side-plates, while tanks could also be made of brass or of plastic, often a polyamide). Starting in the 1970s, use of aluminium increased, eventually taking over the vast majority of vehicular radiator applications. The main inducements for aluminium are reduced weight and cost. \n\nSince air has a lower heat capacity and density than liquid coolants, a fairly large volume flow rate (relative to the coolant's) must be blown through the radiator core to capture the heat from the coolant. Radiators often have one or more fans that blow air through the radiator. To save fan power consumption in vehicles, radiators are often behind the grille at the front end of a vehicle. Ram air can give a portion or all of the necessary cooling air flow when the coolant temperature remains below the system's designed maximum temperature, and the fan remains disengaged.\n\nAs electronic devices become smaller, the problem of dispersing waste heat becomes more difficult. Tiny radiators known as heat sinks are used to convey heat from the electronic components into a cooling air stream. Heatsink do not use water, rather they conduct the heat from the source (high-performance heat sinks have copper to conduct better). Heat is transferred to the air by conduction and convection; a relatively small proportion of heat is transferred by radiation owing to the low temperature of semiconductor devices compared to their surroundings.\n\nRadiators are found as components of some spacecraft. These radiators work by radiating heat energy away as light (generally infrared given the temperatures at which spacecraft try to operate) because in the vacuum of space neither convection nor conduction can work to transfer heat away. On the International Space Station, these can be seen clearly as large white panels attached to the main truss. They can be found on both manned and unmanned craft.\n"}
{"id": "239138", "url": "https://en.wikipedia.org/wiki?curid=239138", "title": "Reliability (statistics)", "text": "Reliability (statistics)\n\nReliability in statistics and psychometrics is the overall consistency of a measure. A measure is said to have a high reliability if it produces similar results under consistent conditions. \"It is the characteristic of a set of test scores that relates to the amount of random error from the measurement process that might be embedded in the scores. Scores that are highly reliable are accurate, reproducible, and consistent from one testing occasion to another. That is, if the testing process were repeated with a group of test takers, essentially the same results would be obtained. Various kinds of reliability coefficients, with values ranging between 0.00 (much error) and 1.00 (no error), are usually used to indicate the amount of error in the scores.\" For example, measurements of people's height and weight are often extremely reliable.\n\nThere are several general classes of reliability estimates:\n\nReliability does not imply validity. That is, a reliable measure that is measuring something consistently is not necessarily measuring what you want to be measured. For example, while there are many reliable tests of specific abilities, not all of them would be valid for predicting, say, job performance.\n\nWhile reliability does not imply validity, reliability does place a limit on the overall validity of a test. A test that is not perfectly reliable cannot be perfectly valid, either as a means of measuring attributes of a person or as a means of predicting scores on a criterion. While a reliable test may provide useful valid information, a test that is not reliable cannot possibly be valid.\n\nFor example, if a set of weighing scales consistently measured the weight of an object as 500 grams over the true weight, then the scale would be very reliable, but it would not be valid (as the returned weight is not the true weight). For the scale to be valid, it should return the true weight of an object. This example demonstrates that a perfectly reliable measure is not necessarily valid, but that a valid measure necessarily must be reliable.\n\nIn practice, testing measures are never perfectly consistent. Theories of test reliability have been developed to estimate the effects of inconsistency on the accuracy of measurement. The basic starting point for almost all theories of test reliability is the idea that test scores reflect the influence of two sorts of factors:\n\n1. Factors that contribute to consistency: stable characteristics of the individual or the attribute that one is trying to measure\n\n2. Factors that contribute to inconsistency: features of the individual or the situation that can affect test scores but have nothing to do with the attribute being measured.\n\nThese factors include:\n\n\nThe goal of estimating reliability is to determine how much of the variability in test scores is due to errors in measurement and how much is due to variability in true scores.\n\nA true score is the replicable feature of the concept being measured. It is the part of the observed score that would recur across different measurement occasions in the absence of error.\n\nErrors of measurement are composed of both random error and systematic error. It represents the discrepancies between scores obtained on tests and the corresponding true scores.\n\nThis conceptual breakdown is typically represented by the simple equation:\n\nThe goal of reliability theory is to estimate errors in measurement and to suggest ways of improving tests so that errors are minimized.\n\nThe central assumption of reliability theory is that measurement errors are essentially random. This does not mean that errors arise from random processes. For any individual, an error in measurement is not a completely random event. However, across a large number of individuals, the causes of measurement error are assumed to be so varied that measure errors act as random variables.\n\nIf errors have the essential characteristics of random variables, then it is reasonable to assume that errors are equally likely to be positive or negative, and that they are not correlated with true scores or with errors on other tests.\n\nIt is assumed that:\n\n1. Mean error of measurement = 0\n\n2. True scores and errors are uncorrelated\n\n3. Errors on different measures are uncorrelated\n\nReliability theory shows that the variance of obtained scores is simply the sum of the variance of true scores plus the variance of errors of measurement.\n\nThis equation suggests that test scores vary as the result of two factors:\n\n1. Variability in true scores\n\n2. Variability due to errors of measurement.\n\nThe reliability coefficient formula_2 provides an index of the relative influence of true and error scores on attained test scores. In its general form, the reliability coefficient is defined as the ratio of \"true score\" variance to the total variance of test scores. Or, equivalently, one minus the ratio of the variation of the \"error score\" and the variation of the \"observed score\":\n\nUnfortunately, there is no way to directly observe or calculate the true score, so a variety of methods are used to estimate the reliability of a test.\n\nSome examples of the methods to estimate reliability include test-retest reliability, internal consistency reliability, and \"parallel-test reliability\". Each method comes at the problem of figuring out the source of error in the test somewhat differently.\n\nIt was well-known to classical test theorists that measurement precision is not uniform across the scale of measurement. Tests tend to distinguish better for test-takers with moderate trait levels and worse among high- and low-scoring test-takers. Item response theory extends the concept of reliability from a single index to a function called the \"information function\". The IRT information function is the inverse of the conditional observed score standard error at any given test score.\n\nThe goal of estimating reliability is to determine how much of the variability in test scores is due to errors in measurement and how much is due to variability in true scores.\n\nFour practical strategies have been developed that provide workable methods of estimating test reliability.\n\n1. Test-retest reliability method: directly assesses the degree to which test scores are consistent from one test administration to the next.\n\nIt involves:\n\n\nThe correlation between scores on the first test and the scores on the retest is used to estimate the reliability of the test using the Pearson product-moment correlation coefficient: see also item-total correlation.\n\n2. Parallel-forms method:\n\nThe key to this method is the development of alternate test forms that are equivalent in terms of content, response processes and statistical characteristics. For example, alternate forms exist for several tests of general intelligence, and these tests are generally seen equivalent.\n\nWith the parallel test model it is possible to develop two forms of a test that are equivalent in the sense that a person’s true score on form A would be identical to their true score on form B. If both forms of the test were administered to a number of people, differences between scores on form A and form B may be due to errors in measurement only.\n\nIt involves:\n\n\nThe correlation between scores on the two alternate forms is used to estimate the reliability of the test.\n\nThis method provides a partial solution to many of the problems inherent in the test-retest reliability method. For example, since the two forms of the test are different, carryover effect is less of a problem. Reactivity effects are also partially controlled; although taking the first test may change responses to the second test. However, it is reasonable to assume that the effect will not be as strong with alternate forms of the test as with two administrations of the same test.\n\nHowever, this technique has its disadvantages:\n\n\n3. Split-half method:\n\nThis method treats the two halves of a measure as alternate forms. It provides a simple solution to the problem that the parallel-forms method faces: the difficulty in developing alternate forms.\n\nIt involves:\n\n\nThe correlation between these two split halves is used in estimating the reliability of the test. This halves reliability estimate is then stepped up to the full test length using the Spearman–Brown prediction formula.\n\nThere are several ways of splitting a test to estimate reliability. For example, a 40-item vocabulary test could be split into two subtests, the first one made up of items 1 through 20 and the second made up of items 21 through 40. However, the responses from the first half may be systematically different from responses in the second half due to an increase in item difficulty and fatigue.\n\nIn splitting a test, the two halves would need to be as similar as possible, both in terms of their content and in terms of the probable state of the respondent. The simplest method is to adopt an odd-even split, in which the odd-numbered items form one half of the test and the even-numbered items form the other. This arrangement guarantees that each half will contain an equal number of items from the beginning, middle, and end of the original test.\n\n4. Internal consistency: assesses the consistency of results across items within a test. The most common internal consistency measure is Cronbach's alpha, which is usually interpreted as the mean of all possible split-half coefficients. Cronbach's alpha is a generalization of an earlier form of estimating internal consistency, Kuder–Richardson Formula 20. Although the most commonly used, there are some misconceptions regarding Cronbach's alpha.\n\nThese measures of reliability differ in their sensitivity to different sources of error and so need not be equal. Also, reliability is a property of the \"scores of a measure\" rather than the measure itself and are thus said to be \"sample dependent\". Reliability estimates from one sample might differ from those of a second sample (beyond what might be expected due to sampling variations) if the second sample is drawn from a different population because the true variability is different in this second population. (This is true of measures of all types—yardsticks might measure houses well yet have poor reliability when used to measure the lengths of insects.)\n\nReliability may be improved by clarity of expression (for written assessments), lengthening the measure, and other informal means. However, formal psychometric analysis, called item analysis, is considered the most effective way to increase reliability. This analysis consists of computation of item difficulties and item discrimination indices, the latter index involving computation of correlations between the items and sum of the item scores of the entire test. If items that are too difficult, too easy, and/or have near-zero or negative discrimination are replaced with better items, the reliability of the measure will increase.\n\n\n\n"}
{"id": "14001540", "url": "https://en.wikipedia.org/wiki?curid=14001540", "title": "Roger Tiley", "text": "Roger Tiley\n\nRoger Tiley is a Welsh documentary photographer. His work on documenting the coal mines of Wales and America has been used extensively in publications. Specifically during the UK miners' strike (1984-1985).\n\nTiley first worked in photography as an industrial photographer for Lucas Industries, a large car component company. He spent four years there and subsequently decided to specialise in documentary photography.\n\nHe studied at the School of Documentary Photography under Magnum photographer David Hurn.\n\nOn completion of the course in 1984, Tiley worked for a number of national newspapers and magazines, including \"The Times,\" \"The Sunday Times,\" \"The Guardian\" and \"The Observer.\" Much of his journalistic work was based on the miners strike in 1984/85.\n\nBecause of his work on the miners strike, he was commissioned, along with David Bailey, John Davies (photographer) and Paul Reas to produce \"The Valleys Project\", a collection of photographs reflecting life in the South Wales Valleys. They are meticulously annotated with context information and encompass every part of the miners' lives.\n\nSince the 1980s, Tiley has concentrated on working on commissions for exhibitions, archive collections, television and book publication. Over the past three decades, Tiley has had work exhibited and published regularly in Europe and the US and is the author of three books.\n\nHis commissions include photographing the Welsh descendants living in Pennsylvania and covering extensively the mining communities of West Virginia, Kentucky, Virginia, and Tennessee.\n\nHe is now a former lecturer in photography, media studies and graphic design at Gower College Swansea, Wales. He has delivered lectures on his photographic practice in the UK and the US. He is also involved in delivering photography workshops on landscape and documentary photography.\n\nTiley was awarded a commission to photograph the manufactured coast-scape in Wales. For the year-long project He travelled around the coastline of Wales to make photographs of the way the coast line has been adapted to cater for twenty-first-century needs. This included photographing industrial manufacturing plants, the tourism industry and the need for power generation. The work was exhibited in Europe and the UK.\n\nTiley was the only photographer allowed to photograph underground on the coal face of the last deep coal mine in the UK, Kellingley Colliery in North Yorkshire. This work was commissioned by Keo Films, London, who made a BBC documentary titled 'The Last Miners'. It was also published as a book by 2Ten Books.\n\nTiley has also produced and directed moving image.\n\nHis work has been featured on a number of television and radio features in the UK, Europe and the US.\n\nTiley worked on a project called \"Thirty Years Ago\", re-visiting the mining communities he photographed thirty years ago and making portraits of former miners, along with people involved in mining today. His work is closely linked to his upbringing in the south Wales valleys. He is very proud of his birthplace. To this day, he supports the miners and feels sad to see the demise of the industrial valleys.\n\n\n"}
{"id": "22360941", "url": "https://en.wikipedia.org/wiki?curid=22360941", "title": "S-200 (bioremediation)", "text": "S-200 (bioremediation)\n\nS-200 is a bioremediation product used to clean up oil spills. It is an oleophilic nitrogen-phosphorus nutrient that promotes the growth of micro-organisms that degrade hydrocarbons (such as oil and fuel). S-200 bonds to the hydrocarbon to eliminate the need to reapply in tidal or rain events. S-200 is identified as a bioremediation accelerator and as such, does not contain bacterial cultures, but rather contributes to the establishment of a robust microbial population. In the laboratory, considerable biodegradation to alkanes was seen over the course of treatment. Field trials have yielded inconsistent results.\n\nThe product was developed by International Environmental Products, a US company based around S-200 as its product.\n\nAfter laboratory and field trials, S-200 was the only bioremediation process selected by the Spanish Department of National Parks for the final cleanup of the Galician coastline following the Prestige oil tanker spill.\n\nThe effects of the product were studied on a beach affected by the spill] off the coast of Spain in 2002. A study concluded that it enhanced the biodegradation rate of specific compounds, but did not establish whether it had improved the visible aspect of the beach, detached stuck oil, or reduced weathered oil. Other compounds, such as uric acid and lecithin, may be more effective than S-200, but wash off in tidal areas or rain events and must be applied continuously. In 2006, other researchers summarized the findings of experiments on Prestige-affected coastal areas, concluding that oloephilic substances such as S-200 were of \"limited effectiveness.\n\nIn 2006, a field bioremediation assay was conducted by the Department of Microbiology, University of Barcelona on the use of S-200 ten months after the Prestige heavy fuel-oil spill on a beach of the Cantabrian coast in northern Spain. The field survey indicated that the product enhanced the biodegradation rate, particularly of high molecular weight n-alkanes, alkylcyclohexanes, and benzenes, and alkylated PAHs. The most significant molecular bioremediation indicators were the depletion of diasteranes and C-27 sterane components.\n\nHowever, the study was confined to analysis of specific compounds, and did not report whether the application of S-200 caused a decrease in the amount of weathered oil, the detachment of any oil that had been stuck, or any improvement to the visible appearance of the beach.\n\nIn 2006, other researchers summarized the findings of experiments on \"Prestige\"-affected coastal areas, concluding that oloephilic fertilizers such as S-200 were of \"limited effectiveness\".\n\nA 2007 test by researchers at the Technical University of Crete comparing a control to treatment by S-200 and treatment by uric acid and lecithin found that the hydrocarbon degradation in a period of 7 days was greater with the uric acid and lecithin treatment than it was with the S-200 treatment and the control.\n"}
{"id": "7102272", "url": "https://en.wikipedia.org/wiki?curid=7102272", "title": "Sample preparation equipment", "text": "Sample preparation equipment\n\nSample preparation equipment refers to equipment used for the preparation of physical specimens for subsequent microscopy or related disciplines - including failure analysis and quality control. The equipment includes the following types of machinery:\n\n\nEach of these system types incorporates a wealth of accessories and consumable items which fit the particular system for a specific application.\n\nArticle from MATERIALS WORLD Journal discussing the various sample preparation disciplines that allow for failure analysis of electronic materials and components\n\nArticle from ULTRA TEC Web-site discussing the backside sample preparation of a packaged electronic device that allow for (through silicon) backside analysis\n\nArticle discussing the applications of jet etch equipment\n"}
{"id": "2582836", "url": "https://en.wikipedia.org/wiki?curid=2582836", "title": "Search Engine Watch", "text": "Search Engine Watch\n\nSearch Engine Watch (SEW) provides news and information about search engines and search engine marketing.\n\nSearch Engine Watch was started by Danny Sullivan in 1996. In 1997, Sullivan sold it for an undisclosed amount to MecklerMedia (now WebMediaBrands). In 2005 the website and related Search Engine Strategies conference series were sold to Incisive Media for $43 million. On November 30, 2006, Danny Sullivan left Search Engine Watch, after his resignation announcement on August 29, 2006. Rebecca Lieb was named editor-in-chief the following month.\n\nIn 2015, Incisive Media sold SES, Search Engine Watch, and ClickZ to Blenheim Chalcot.\n\nGoogle's Matt Cutts has called Search Engine Watch \"a must read.\" Yahoo's Tim Mayer has said that it is the \"most authoritative source on search.\"\n\n"}
{"id": "17686546", "url": "https://en.wikipedia.org/wiki?curid=17686546", "title": "Slack action", "text": "Slack action\n\nIn railroading, slack action is the amount of free movement of one car before it transmits its motion to an adjoining coupled car. This free movement results from the fact that in railroad practice cars are loosely coupled, and the coupling is often combined with a shock-absorbing device, a \"draft gear,\" which, under stress, substantially increases the free movement as the train is started or stopped. Loose coupling is necessary to enable the train to bend around curves and is an aid in starting heavy trains, since the application of the locomotive power to the train operates on each car in the train successively, and the power is thus utilized to start only one car at a time.\n\nThe UK formerly used three link couplings which allowed a large amount of slack; these have since been replaced by buffers and chain couplers. While the couplings are held tight by buffers and shortened by a turnbuckle, other railways decided to replace them with automatic couplings, such as the Scharfenberg coupler and the Janney Coupler.\n\n"}
{"id": "21336483", "url": "https://en.wikipedia.org/wiki?curid=21336483", "title": "Smörgåsbord", "text": "Smörgåsbord\n\nSmörgåsbord () is a type of Scandinavian meal, originating in Sweden, served buffet-style with multiple hot and cold dishes of various foods on a table. \n\nSmörgåsbord became internationally known at the 1939 New York World's Fair when it was offered at the Swedish Pavilion's \"Three Crowns Restaurant\". It is typically a celebratory meal and guests can help themselves from a range of dishes laid out for their choice. In a restaurant the term refers to a buffet-style table laid out with many small dishes from which, for a fixed amount of money, one is allowed to choose as many as one wishes. In Pennsylvania, smorgasbords are popular Pennsylvania Dutch-style buffets that are often associated with Amish-made meals.\n\nIn Northern Europe, the term varies between 'cold table' and 'buffet': In Norway it is called koldtbord or kaldtbord and in Denmark det kolde bord (literally \"the cold table\"); in Germany kaltes Buffet (literally \"cold buffet\"); in The Netherlands koud buffet (\"cold buffet\"); in Iceland it is called hlaðborð (\"farmyard/courtyard table\"), in Estonia it is called külmlaud (\"cold table\") or rootsi laud (\"Swedish table\"), in Finland voileipäpöytä (\"butter-bread/sandwich table\") or ruotsalainen seisova pöytä (\"Swedish standing table/buffet\"). In Eastern Europe, each language has a term that literally means \"Swedish table\". Similarly, in Japan バイキング / ヴァイキング (\"baikingu\" / \"vaikingu\", i.e. \"Viking\") is a popular name used.\n\nThe Swedish word \"smörgåsbord\" consists of the words \"smörgås\" (sandwich, usually open-faced) and \"bord\" (table). \"Smörgås\" in turn consists of the words \"smör\" (butter, cognate with English \"smear\") and \"gås\". \"Gås\" literally means goose, but later referred to the small pieces of butter that formed and floated to the surface of cream while it was churned. These pieces reminded the old Swedish peasants of fat geese swimming to the surface. The small butter pieces were just the right size to be placed and flattened out on bread, so \"smörgås\" came to mean buttered bread. In Sweden, the term \"att breda smörgåsar\" (to spread butter on open-faced sandwiches) has been used since at least the 16th century.\n\nIn English and also in Scandinavian languages, the word \"smörgåsbord\" refers loosely to any buffet with a variety of dishes — not necessarily with any connection to the Swedish Christmas traditions discussed in this article. In an extended sense, the word is used to refer to any situation which invites patrons to select whatever they wish among lots of pleasant things, such as the smorgasbord of university courses, books in a bookstore, etc. In the Scandinavian languages, ö/ø and å are separate letters and are correctly transliterated as oe and aa, respectively, not as o and a. If the ö and å letters aren't available, the word can be correctly rendered as smoergaasbord.\n\nA traditional Swedish \"smörgåsbord\" consists of both hot and cold dishes. Bread, butter, and cheese are always part of the \"smörgåsbord\". It is customary to begin with the cold fish dishes which are generally various forms of herring, salmon, and eel. After eating the first portion, people usually continue with the second course (other cold dishes), and round off with hot dishes. Dessert may or may not be included in a smörgåsbord.\n\nA special Swedish type of \"smörgåsbord\" is the \"julbord\" (literally \"Yule/Christmas table\"). The classic Swedish julbord is central to traditional Swedish cuisine, often including bread dipped in ham broth and continuing with a variety of fish (salmon, herring, whitefish and eel), baked ham, meatballs, pork ribs, head cheese, sausages, potato, Janssons frestelse, boiled potatoes, cheeses, beetroot salad, various forms of boiled cabbage, kale and rice pudding.\n\nIt is customary to eat particular foods together; herring is typically eaten with boiled potatoes and hard-boiled eggs and is frequently accompanied by strong spirits like snaps, brännvin or akvavit with or without spices. Other traditional foods are smoked eel, rollmops, herring salad, baked herring and smoked salmon.\nOther dishes are pork sausages (\"fläskkorv\"), smoked pork and potato sausages (\"isterband\"), cabbage rolls (\"kåldolmar\"), baked beans, omelette with shrimps or mushrooms covered with béchamel sauce. Side dishes include beetroot salad in mayonnaise and warm stewed red, green or brown cabbage.\n\nLutfisk, lyed fish made of stockfish (dried ling or cod served with boiled potato and thick white sauce) and green peas that can be served with the warm dishes or as a separate fourth course. Lutfisk is often served as dinner the second day after the traditional Christmas Yule-table dinner. Julbord desserts include rice pudding (\"risgrynsgröt\"), sprinkled with cinnamon powder. Traditionally, an almond is hidden in the bowl of rice porridge, and whoever finds it receives a small prize or is recognized for having good luck.\nJulbord is served from early December until just before Christmas at restaurants and until Epiphany in some homes. It is tradition for most Swedish and Norwegian workplaces to hold an annual Julbord between November and January.\n\nIn Denmark a typical tradition resembling the Swedish \"julbord\" is \"Julefrokost\" (\"Christmas-lunch\"), which involves a wellstocked Danish smörgåsbord with cold as well as hot dishes, and plenty of beer and snaps. It is distinct from the Danish Christmas dinner, which is served on December 24, and is served as a lunchtime meal, usually for family and friends on December 25 or 26. It is also a tradition for most Danish workplaces to hold an annual Julefrokost some time during the months of November to January.\n\nThe members of the Swedish merchant and upper class in sixteenth-century Sweden and Finland served schnapps table (\"brännvinsbord\"), a small buffet presented on a side table offering a variety of hors d'oeuvres served prior to a meal before sitting at the dinner table. The most simple \" brännvinsbord \" was bread, butter, cheese, herring and several types of liqueurs; but smoked salmon, sausages and cold cuts were also served. The \" brännvinsbord \" was served as an appetizer for a gathering of people and eaten while standing before a dinner or supper, often two to five hours before dinner, sometimes with the men and women in separate rooms.\nThe \"smörgåsbord\" became popular in the mid-seventeenth century, when the food moved from the side table to the main table and service began containing both warm and cold dishes. \"Smörgåsbord\" was also served as an appetizer in hotels and later at railway stations, before the dining cars time for the guests. Restaurants in Stockholm at the 1912 Olympic Games stopped serving \"smörgåsbord \" as an appetizer and started serving them instead as a main course.\n\n\n"}
{"id": "18426062", "url": "https://en.wikipedia.org/wiki?curid=18426062", "title": "Structured ASIC platform", "text": "Structured ASIC platform\n\nStructured ASIC is an intermediate technology between ASIC and FPGA, offering high performance, a characteristic of ASIC, and low NRE cost, a characteristic of FPGA.\nUsing Structured ASIC allows products to be introduced quickly to market, to have lower cost and to be designed with ease.\n\nIn a FPGA, interconnects and logic blocks are programmable after fabrication, offering high flexibility of design and ease of debugging in prototyping.\nHowever, the capability of FPGAs to implement large circuits is limited, in both size and speed, due to complexity in programmable routing, and significant space occupied by programming elements, e.g. SRAMs, MUXes.\nOn the other hand, ASIC design flow is expensive.\nEvery different design needs a complete different set of masks.\nThe Structured ASIC is a solution between these two.\nIt has basically the same structure as a FPGA, but being mask-programmable instead of field-programmable, by configuring one or several via layers between metal layers.\nEvery SRAM configuration bit can be replaced by a choice of putting a via or not between metal contacts. \n\nA number of commercial vendors have introduced structured ASIC products. They have a wide range of configurability, from a single via layer to 6 metal and 6 via layers. Altera's Hardcopy-II, eASIC's Nextreme are examples of commercial structured ASICs.\n\n\nExternal Links: eda.ee.ucla.edu/EE201A-04Spring/ASICslides.ppt\n"}
{"id": "1582889", "url": "https://en.wikipedia.org/wiki?curid=1582889", "title": "Swords to ploughshares", "text": "Swords to ploughshares\n\nSwords to ploughshares (or Swords to plowshares) is a concept in which military weapons or technologies are converted for peaceful civilian applications.\n\nThe phrase originates from the Book of Isaiah:\n\nThe \"ploughshare\" is often used to symbolize creative tools that benefit humankind, as opposed to destructive tools of war, symbolized by the \"sword\", a similar sharp metal tool with an arguably opposite use.\n\nIn addition to the original Biblical Messianic intent, the expression \"beat swords into ploughshares\" has been used by disparate social and political groups.\n\nAn ongoing example as of 2013 is the dismantling of nuclear weapons and the use of their contents as fuel in civilian electric power stations, the Megatons to Megawatts Program. Nuclear fission development, originally accelerated for World War II weapons needs, has been applied to many civilian purposes since its use at Hiroshima and Nagasaki, including electricity and radiopharmaceutical production.\n\nThis analogy is used several times in the Old Testament or Tanakh, in both directions, such as in the following verses:\n\nAn expression of this concept can be seen in a bronze statue in the United Nations garden called \"Let Us Beat Swords into Plowshares\", a gift from the Soviet Union sculpted by Evgeniy Vuchetich, representing the figure of a man hammering a sword into the shape of a plowshare.\n\n\n\n\n\n\n\n"}
{"id": "39366623", "url": "https://en.wikipedia.org/wiki?curid=39366623", "title": "Textile History", "text": "Textile History\n\nTextile History is a peer-reviewed academic journal first published in 1968 and published by Maney Publishing on behalf of the Pasold Research Fund. It covers \"aspects of the cultural and social history of apparel and textiles, as well as issues arising from the exhibition, preservation and interpretation of historic textiles or clothing\".\n\nThe journal is indexed in publications including \"Arts and Humanities Citation Index\", \"British Humanities Index\" and \"Historical Abstracts\".\n"}
{"id": "4061041", "url": "https://en.wikipedia.org/wiki?curid=4061041", "title": "The Magic Box", "text": "The Magic Box\n\nThe Magic Box is a 1951 British, Technicolor, biographical drama film, directed by John Boulting. The film stars Robert Donat as William Friese-Greene, with a host of cameo appearances by actors including Peter Ustinov and Laurence Olivier. It was produced by Ronald Neame and distributed by British Lion Film Corporation. The film was a project of the Festival of Britain and adapted by Eric Ambler from the controversial biography by Ray Allister.\n\nThis biographical drama gives an account of the life of William Friese-Greene, who first designed and patented one of the earliest working cinematic cameras. Told in flashback, the film details Friese-Greene's tireless experiments with the \"moving image\", leading inexorably to a series of failures and disappointments, as others hog the credit for the protagonist's discoveries.\n\nIn 1921, William Friese-Greene, in dire financial straits and separated from his wife, but still working, attends a film conference in London. He is saddened that all those attending are businessmen interested only in moneymaking. He attempts to speak, but no-one is interested and he sits down. He thinks back to his early pioneering days.\n\nYoung \"Willie\" works as an assistant to photographer Maurice Guttenberg, who will not let him take portraits his way. He leaves and, with his new wife, a client of his former employer, he opens a studio. After a slow start, he does well and opens other studios, but he is more interested in developing moving pictures and colour films. He single-mindedly works on his ideas, spending more and more money, and is eventually declared bankrupt. With the coming of World War I, their sons (one under age) enlist in the army to relieve their parents of the burden of providing for them.\n\nIn partnership with a businessman, he develops his ideas, but the partnership sours and he's on his own, bankrupt, again. Nevertheless, he perseveres and, late one night, he projects the short film he has taken in Hyde Park that afternoon. Excited, he rushes out and drags in a passing policeman, portrayed by Laurence Olivier (credited as Larry Oliver), to witness the success of the film. The policeman is dumbfounded, not quite comprehending what he has just seen.\n\nBack at the conference, Friese-Greene again stands up to speak, but becomes incoherent and is forced to sit down. He collapses. A doctor is called, but it is too late. Examining the contents of his pockets in an attempt to identify him, the doctor comments that all the money he could find was just enough for a ticket to the cinema.\n\n\n\nThe film was completed and shown just before the end of the 1951 Festival of Britain, but it did not go on general release until 1952.\n\nThe film was nominated for two BAFTA Awards in 1952—BAFTA Award for Best Film and BAFTA Award for Best British Film.\n\n\n"}
{"id": "26696343", "url": "https://en.wikipedia.org/wiki?curid=26696343", "title": "Uniformology", "text": "Uniformology\n\nUniformology is a branch of the auxiliary sciences of history which studies uniforms - especially military uniforms - through ages and civilizations.\n"}
{"id": "44467027", "url": "https://en.wikipedia.org/wiki?curid=44467027", "title": "Volksflugzeug", "text": "Volksflugzeug\n\nThe Volksflugzeug (People’s Aircraft) was a grand Third Reich scheme for the mass-production of a small and simple airplane in the 1930s. It was one of the attempts of the Nazi regime to use consumer technologies as a propaganda tool.\nUnlike the Volkswagen car, the showpiece of the Nazi's attempt to appear to work for the good of the average German, as well as the less-known \"Volksempfänger\" radio, the \"Volkskühlschrank\" refrigerator and the \"Volksgasmaske\" gas mask, the Volksflugzeug project was contemplated but never fully realized.\n\nThe Volksflugzeug grand plan surfaced at different times and in different locations in Germany during Nazi rule. However, since it only had lukewarm official backing, it remained ill-defined and vague. Finally World War II dictated priorities ended abruptly most of the projects connected with the scheme.\n\nThe idea of a ‘People's Aircraft’ predated the Nazi regime. Its main source of inspiration was Henry Ford’s Ford T. After the industrial production of a cheap car that the ordinary citizen could afford was made possible, there was only one step in the minds of some towards building an aircraft on the same pattern. The idea of a ‘People's Plane’ was put forward to Henry Ford by William Bushnell Stout and Ford immediately became enthusiastic about it. The Ford Flivver was the resulting plane, a project that ended up not becoming as successful as expected. \nIn Germany itself, some projects in the second half of the 1920 decade were intended as early Volksflugzeug planes. Already in 1928 the Raab-Katzenstein RK-9 \"Grasmücke\" (Warbler) was a 2-seat biplane trainer fitted with a 3-cylinder Anzani motor that was planned as a 'Volksflugzeug'. The Etrich Sport-Taube, built in 1929 by the Etrich company, was also intended as a Volksflugzeug, but it soon faced difficulties regarding production in series and the project was given up.\nOther early People’s Aircraft projects were the Deicke ADM 11 \"Volksflugzeug\" built in 1933, the Gerner G.I and G.II biplanes, and the Messerschmitt M 17.\n\nEngineers such as Hanns Klemm saw that there was a future in light aircraft. This led Klemm to design the Daimler L15 while he worked at the Daimler Motor Company in Sindelfingen.\nKlemm had a clear goal: to build in series a Volksflugzeug, a plane that would be low in production costs, fitted with a low-priced motor and low in maintenance. Later, after leaving Daimler, Klemm established his own company Leichtflugzeugbau Klemm in Böblingen.\nThere was a competition for the construction of a single-seat low-cost Volksflugzeug in the early 1930s before the Nazi Party came to power. In addition to industrial and private companies, also academic groups took part in the contest. Thus the \"Akademische Fliegergruppe Berlin\" of the Berlin Technical High School produced the B 4, also known as „FF“, \"fast fertig\" (almost ready) and won the first prize. Following the Nazi takeover such Academic Flying Groups were disbanded through a decree issued on 11 April 1933 by the Reichskommissar responsible for aviation.\n\nThe prefix ‘Volks-’ became for National Socialism a kind of honor badge that was awarded to selected industrial products. The capacity to mass-produce such items flattered a totalitarian state which sought to present itself as a leader in technical progress as well as the forerunner of a more egalitarian world led by the Führer. \nIn the post war years such people's projects were often mentioned as “the good side” of the Nazi times.\n\nInitiative and eventual instructions regarding the “People’s Products” proposals came from the German Labour Front, led by Robert Ley, as well as from the Nazi Propaganda Ministry led by Joseph Goebbels. Political, ideological, as well as economical factors dictated their viability within the Nazi context. The “People’s Products” had a double function, on one hand they were key elements of the official propaganda, which promised an affluent and comfortable future society to citizens, mainly in order to make the actual economic hardships dictated by the German re-armament acceptable to the population. On the other hand, they constituted real plans and visions of a specifically Nazi consumer’s society.\n\nThe failure of the grand scheme became evident, however, when the economic goal of massive people’s consumption revealed itself incompatible with the simultaneous political goals of autarky, re-armament and territorial expansion.\nBy mid 1937 with the German re-armament in full gear, such small planes were labelled as 'weak-engined planes' (\"schwachmotorige Flugzeuge\") and Nazi engineer Hermann Schäfer would write in Berlin that:\nThe name „Volksflugzeug“ is only acceptable for a plane that wide layers of a \"Volk\" can afford to buy and upkeep. However, in the foreseeable time one can neither count on a plane of that type nor on the creation of the necessary conditions for it, such as take-off, landing and maintenance infrastructures.<ref name=\"LW-5/6\">\"Luftwelt\" Bd. 4 Nr. 5/6, May/June 1937. \"Schwachmotorige Flugzeuge\" p. 192</ref>\n\nThe vision behind the Volksflugzeug project was to manufacture a small plane in large quantities that would be affordable for the average Third Reich citizen.\n\nSome of the designs of the Volksflugzeug airplane were developed into viable small aircraft, but not as part of the wider Nazi scheme, such as the Messerschmitt M 33, a very little plane that would have been produced in a ‘build-it-yourself’ kit. Willy Messerschmitt's idea was to produce a plane so cheap that it would be affordable for anyone with an average salary. Designed during the pre-Nazi takeover of Germany years, the Messerschmitt M-33 never went past the project stage.\nOther craft, such as the Lehmann Falter, another very little airplane roughly based on the Mignet Pou du Ciel (), were seeking to improve or develop existing designs of very small planes, but remained projects as well.\n\nEngineer Wolf Hirth had pursued the idea of building a Volksflugzeug when he finally built and patented the Hirth Hi-20 MoSe (Motorsegler) in 1937. It was the first motorglider with a non-rigid motor that could change angle.\n\nThe Technical Department of the Nazi Ministry of Aviation, the \"Technisches Amt\" (LC, but more often referred to as the C-amt) in charge of all research and development, led by Ernst Udet after June 1936, withdrew its favor from smaller aircraft building companies. Industries such as Weser Flugzeugbau (former Rohrbach), Bücker, Ago, Klemm and Fieseler were told that there were no further projects for them to develop because future projects could be handled by the remaining aircraft industrial groups. Instead these small companies were advised to become involved in the production of a so-called Volksflugzeug powered by motors of . The recommendation was taken not only by Klemm, with its Kl 105, Siebel with the Si 202 Hummel and Fieseler with the Fi 253 Spatz, but apparently as well by Bücker with its Bü 134 Student, of which only one was built, even though the Bü 134 was powered by a Hirth HM 504 A motor which with fell a bit beyond the scheme.\n\nTwo of the further designs made by Bücker's Swedish engineer Anders J. Anderson, the Bü 180 and the Bü 182 Kornett, could have been the result of the Volksflugzeug proposal made by the LC II, the department of the \"Technisches Amt\" responsible for the development of new aircraft. The Bü 182 Kornett, the last plane designed by Anderson, of which only three were built, also found no support in the Air Ministry of the Reich, even though it combined technical progress and low-cost. The Bü 182 Kornett was a highly innovative model, fitted with a low-priced high-performance engine, that would have made a good trainer for the Luftwaffe.\n\nThe 1930s saw the development of certain light-weight and low-power aviation engines that were intended for small craft and that made Volksflugzeug-type planes feasible both from the low cost as well as from a technical perspective.\n\n\n\n"}
