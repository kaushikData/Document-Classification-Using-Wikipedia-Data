{"id": "7432103", "url": "https://en.wikipedia.org/wiki?curid=7432103", "title": "2,4,6-Trinitroaniline", "text": "2,4,6-Trinitroaniline\n\n2,4,6-Trinitroaniline, CHNO, abbreviated as TNA and also known as picramide, a nitrated amine. Materials in this group range from slight to strong oxidizing agents. If mixed with reducing agents, including hydrides, sulfides and nitrides, they may begin a vigorous reaction that culminates in a detonation. The aromatic nitro compounds may explode in the presence of a base such as sodium hydroxide or potassium hydroxide even in the presence of water or organic solvents. The explosive tendencies of aromatic nitro compounds are increased by the presence of multiple nitro groups. The appearance of trinitroaniline varies from yellow to orange to red depending on its purity and concentration. \n\nTrinitroaniline is only used in modern times in the small warheads of some explosive devices such as mortars. In World War II it was used by Imperial Japanese Navy as Type 97 \"bakuyaku\" (Model 1931 explosive) in some versions of gun projectiles instead of less stable burster \"schimose\". It was also used in the Yokosuka MXY-7 Ohka, a kamikaze antishipping human-guided rocket aircraft.\n\nTrinitroaniline is dangerously explosive. Symptoms of exposure to this compound may include skin and eye irritation, headache, drowsiness, weakness, cyanosis, and respiratory distress.\n\n"}
{"id": "57817972", "url": "https://en.wikipedia.org/wiki?curid=57817972", "title": "ADA (Smart antenna)", "text": "ADA (Smart antenna)\n\nADA, (an abbreviation of Adaptive Antenna) is an advanced smart antenna, using multichannel Controlled Reception Pattern Antenna (CPRA), designed for various platforms, including UAVs, planes and ships, manufactured by the MLM factory of the Israeli Aerospace Industries (IAI).\n\nThe ADA enables vehicles, missiles and other platforms equipped with the smart antenna to navigate even when heavy jamming is conducted in order to block all reception of GPS and other GNSS signals. Using advanced techniques even very faint \"good\" signals are being extracted from the superposition of incoming waves that include noise and strong signals produced by the enemy (or friendly) jammers. These signals are then used (just like a regular GPS receiver) to triangulate the position and determine the GPS tune.\n\nThe existence of ADA was publicly unveiled in February 2017, along with a report that the Israeli Defence Forces has already decided to install the ADAs on \"major platforms\" in service. It was also displayed in the Aero India air show that was held from 14th to the 18th that month in Bangalore. \n\nOn July 2017 MLM has signed a contract with American Honeywell to jointly develop a GPS anti-jam navigation system, based on ADA and Honywell's devices.\n\n"}
{"id": "44088995", "url": "https://en.wikipedia.org/wiki?curid=44088995", "title": "Acoustoelastography", "text": "Acoustoelastography\n\nAcoustoelastography is an ultrasound technique that relates ultrasonic wave amplitude changes to a tendon's mechanical properties.\n\nSee also the page on the acoustoelastic effect.\n"}
{"id": "3704899", "url": "https://en.wikipedia.org/wiki?curid=3704899", "title": "Architectural theory", "text": "Architectural theory\n\nArchitectural theory is the act of thinking, discussing, and writing about architecture. Architectural theory is taught in most architecture schools and is practiced by the world's leading architects. Some forms that architecture theory takes are the lecture or dialogue, the treatise or book, and the paper project or competition entry. Architectural theory is often didactic, and theorists tend to stay close to or work from within schools. It has existed in some form since antiquity, and as publishing became more common, architectural theory gained an increased richness. Books, magazines, and journals published an unprecedented number of works by architects and critics in the 20th century. As a result, styles and movements formed and dissolved much more quickly than the relatively enduring modes in earlier history. It is to be expected that the use of the internet will further the discourse on architecture in the 21st century.\n\nThere is little information or evidence about major architectural theory in antiquity, until the 1st century BCE, with the work of Vitruvius. This does not mean, however, that such works did not exist, given that many works never survived antiquity.\n\nVitruvius was a Roman writer, architect, and engineer active in the 1st century BCE. He was the most prominent architectural theorist in the Roman Empire known today, having written \"De architectura\" (known today as \"The Ten Books of Architecture\"), a treatise written in Latin and Greek on architecture, dedicated to the emperor Augustus. Probably written between 27 and 23 BCE, it is the only major contemporary source on classical architecture to have survived. Divided into ten sections or \"books\", it covers almost every aspect of Roman architecture, from town planning, materials, decorations, temples, water supplies, etc. It rigorously defines the classical orders of architecture. It also proposes the three fundamental laws that Architecture must obey, in order to be so considered: \"firmitas, utilitas, venustas\", translated in the 17th century by Sir Henry Wotton into the English slogan \"firmness, commodity and delight\" (meaning structural adequacy, functional adequacy, and beauty). The rediscovery of Vitruvius' work had a profound influence on architects of the Renaissance, adding archaeological underpinnings to the rise of the Renaissance style, which was already under way. Renaissance architects, such as Niccoli, Brunelleschi and Leon Battista Alberti, found in \"De Architectura\" their rationale for raising their branch of knowledge to a scientific discipline.\n\nThroughout the Middle Ages, architectural knowledge was passed by transcription, word of mouth and technically in master builders' lodges. Due to the laborious nature of transcription, few examples of architectural theory were penned in this time period. Most works from this period were theological, and were transcriptions of the bible, so the architectural theories were the notes on structures included therein. The Abbot Suger's \"Liber de rebus in administratione sua gestis\", was an architectural document that emerged with gothic architecture. Another was Villard de Honnecourt's portfolio of drawings from about the 1230s.\n\nIn Song Dynasty China, Li Jie published the \"Yingzao Fashi\" in 1103, which was an architectural treatise that codified elements of Chinese architecture.\n\nThe first great work of architectural theory of this period belongs to Leon Battista Alberti, \"De Re Aedificatoria\", which placed Vitruvius at the core of the most profound theoretical tradition of the modern ages. From Alberti, good architecture is validated through the Vitruvian triad, which defines its purpose. This triplet conserved all its validity until the 19th century. A major transition into the 17th century and ultimately to the phase of Enlightenment was secured through the advanced mathematical and optical research of the celebrated architect and geometer Girard Desargues, with an emphasis on his studies on conics, perspective and projective geometry.\n\nThe Age of the Enlightenment witnessed considerable development in architectural theory on the European continent. New archeological discoveries (such as those of Pompeii and Herculaneum) drove new interest in Classical art and architecture. Thus the term Neoclassicism, exemplified by the writings of Prussian art critic [Johann Joachim Winkelmann], arose to designate 18th-century architecture which looked to these new Classical precedents for inspiration in building design.\n\nMajor architectural theorists of the Enlightenment include Julien-David Leroy, Abbé Marc-Antoine Laugier, Giovanni Battista Piranesi, \nRobert Adam, James Stuart, Georg Friedrich Hegel\nand Nicholas Revett.\n\nA vibrant strain of Neoclassicism, inherited from Marc-Antoine Laugier's seminal Essai, provided the foundation for two generations of international activity around the core themes of classicism, primitivism and a \"return to Nature.\"\n\nReaction against the dominance of neo-classical architecture came to the fore in the 1820s with Augustus Pugin providing a moral and theoretical basis for Gothic Revival architecture, and in the 1840s John Ruskin developed this ethos.\n\nThe American sculptor Horatio Greenough published the essay \"American Architecture\" in August 1843 in which he rejected the imitation of old styles of buildings and outlined the functional relationship between architecture and decoration. These theories anticipated the development of Functionalism in modern architecture.\n\nTowards the end of the century, there occurred a blossoming of theoretical activity. In England, Ruskin's ideals underpinned the emergence of the Arts and Crafts movement exemplified by the writings of William Morris. This in turn formed the basis for Art Nouveau in the UK, exemplified by the work of Charles Rennie Mackintosh, and influenced the Vienna Secession. On the Continent, the theories of Viollet-le-Duc and Gottfried Semper provided the springboard for enormous vitality of thought dedicated to architectural innovation and the renovation of the notion of style. Semper in particular developed an international following, in Germany, England, Switzerland, Austria, Bohemia, France, Italy and the United States. The generation born during the middle-third of the 19th century was largely enthralled with the opportunities presented by Semper's combination of a breathtaking historical scope and a methodological granularity. In contrast to more recent, and thus \"modern\", thematically self-organized theoretical activities, this generation did not coalesce into a \"movement.\" They did, however, seem to converge on Semper's use of the concept of \"Realismus\", and they are thus labelled proponents of architectural realism. Among the most active Architectural Realists were: Georg Heuser, Rudolf Redtenbacher, Constantin Lipsius, Hans Auer, Paul Sédille, Lawrence Harvey, Otto Wagner and Richard Streiter.\n\nIn 1889 Camillo Sitte published \"Der Städtebau nach seinem künstlerischen Grundsätzen\" (translated as \"City Planning According to Artistic Principles\") which was not exactly a criticism of architectural form but an aesthetic criticism (inspired by medieval and Baroque town planning) of 19th-century urbanism. Mainly a theoretical work, it had an immediate impact on architecture, as the two disciplines of architecture and planning intertwined. Demand for it was so high that five editions appeared in German between 1889 and 1922 and a French translation came out in 1902. (No English edition came out until 1945.) For Sitte, the most important issue was not the architectural shape or form of a building but the quality of the urban spaces that buildings collectively enclose, the whole being more than the sum of its parts. The Modern Movement rejected these thoughts and Le Corbusier energetically dismissed the work. Nevertheless, Sitte's work was revisited by post-modern architects and theorists from the 1970s, especially following its republication in 1986 by Rizzoli, in an edition edited by Collins and Collins (now published by Dover). The book is often cited anachronistically today as a vehicle for the criticism of the Modern Movement.\n\nAlso on the topic of artistic notions with regard to urbanism was Louis Sullivan's \"The Tall Office Building Artistically Considered\" of 1896. In this essay, Sullivan penned his famous alliterative adage \"form ever follows function\"; a phrase that was to be later adopted as a central tenet of Modern architectural theory. While later architects adopted the abbreviated phrase \"form follows function\" as a polemic in service of functionalist doctrine, Sullivan wrote of function with regard to biological functions of the natural order. Another influential planning theorist of this time was Ebenezer Howard, who founded the garden city movement. This movement aimed to form communities with architecture in the Arts and Crafts style at Letchworth and Welwyn Garden City and popularised the style as domestic architecture.\n\nIn Vienna, the idea of a radically new modern architecture had many theorists and proponents. An early use of the term \"modern architecture\" in print occurred in the title of a book by Otto Wagner, who gave examples of his own work representative of the Vienna Secession with art nouveau illustrations, and didactic teachings to his students. Soon thereafter, Adolf Loos wrote \"Ornament and Crime\", and while his own style is usually seen in the context of the Jugendstil, his demand for \"the elimination of ornament\" joined the slogan \"form follows function\" as a principle of the architectural so-called Modern Movement that came to dominate the mid-20th century. Walter Gropius, Ludwig Mies van der Rohe and Le Corbusier provided the theoretical basis for the International Style with aims of using industrialised architecture to reshape society. Frank Lloyd Wright, while modern in rejecting historic revivalism, was idiosyncratic in his theory, which he conveyed in copious writing. Wright did not subscribe to the tenets of the International Style, but evolved what he hoped would be an American, in contrast to a European, progressive course. Wright's style, however, was highly personal, involving his particular views of man and nature. Wright was more poetic and firmly maintained the 19th-century view of the creative artist as unique genius. This limited the relevance of his theoretical propositions. Towards the end of the century postmodern architecture reacted against the austerity of High Modern (International Style) principles, viewed as narrowly normative and doctrinaire.\n\nIn contemporary architectural discourse theory has become more concerned with its position within culture generally, and thought in particular. This is why university courses on architecture theory may often spend just as much time discussing philosophy and cultural studies as buildings, and why advanced postgraduate research and doctoral dissertations focus on philosophical topics in connection with architectural humanities. Some architectural theorists aim at discussing philosophical themes, or engage in direct dialogues with philosophers, as in the case of Peter Eisenman's and Bernard Tschumi's interest in Derrida's thought, or Anthony Vidler's interest in the works of Freud and Lacan, in addition to an interest in Gaston Bachelard's \"Poetics of Space\" or texts by Gilles Deleuze. This has also been the case with educators in academia like Dalibor Vesely or Alberto-Perez Gomez, and in more recent years this philosophical orientation has been reinforced through the research of a new generation of theorists (E.G. Jeffrey Kipnis or Sanford Kwinter). Similarly, we can refer to contemporary architects who are interested in philosophy and cultural studies. Some are interested in phenomenology, like Christian Norberg-Schulz, or specialize as philosophers and historians of science, such as Nader El-Bizri. Others, like Manfredo Tafuri, are interested in new ontological definitions of architecture tracing a new notion of modernity in architecture. The notion that theory entails critique also stemmed from post-structural literary studies in the work of many other theorists and architects, such as Mark Wigley, among others. In their theories, architecture is compared to a language which can be invented and re-invented every time it is used. This theory influenced the so-called deconstructivist architecture. In contrast, network society innovators, especially Silicon Valley software developers, have embraced Christopher Alexander's emphasis on The Timeless Way of Building (1979) based on pattern languages that are optimized on-site as construction unfolds.\n\nSince 2000, architectural theory has also had to face the rapid rise of urbanism and globalization. By developing a new understanding of the city, many theorists developed new understandings of the urban conditions of our planet (E.G. Rem Koolhaas's \"Bigness\"). Interests in fragmentation and architecture as transient objects further affected such thinking (e.g. the concern for employing high technology), but also related to general concerns such as ecology, mass media, and economism.\n\nIn the past decade, there has been the emergence of the so-called \"Digital\" Architecture. Several currents and design methodologies are being developed simultaneously, some of which reinforce each other, whereas others work in opposition. One of these trends is Biomimicry, which is the process of examining nature, its models, systems, processes, and elements, to emulate or take inspiration from them in order to solve human problems. Architects also design organic-looking buildings in the attempt to develop a new formal language. Another trend is the exploration of those computational techniques that are influenced by algorithms relevant to biological processes and sometimes referred to as Digital morphogenesis. Trying to utilize Computational creativity in architecture, Genetic algorithms developed in computer science are used to evolve designs on a computer, and some of these are proposed and built as actual structures. Since these new architectural tendencies emerged, many theorists and architects have been working on these issues, developing theories and ideas such as Patrick Schumacher's Parametricism.\n\nNonetheless, there is no evidence for claiming that we are witnessing the birth of an entirely new type of architectural theory and practice. \nContemporary architecture's theoretical world is plural and multicolored. There are different dominant schools of architectural theory which are based on linguistic analysis, philosophy, post-structuralism, or cultural theory. For instance, there is emerging interest in the re-discovery of the post-modernist project (Sam Jacob), in the definition of new radical tendencies of architecture and its implication in the development of cities (Pier Vittorio Aureli), and in a new formalist approach to architecture through the appropriation of concepts from the Object Oriented philosophy (Peter Trummer or Tom Wiscombe). It is too early, however, to say whether any of these explorations will have widespread or lasting impact on architecture.\n\n\n\n\nAnti-architecture\n\n\n\n"}
{"id": "41113159", "url": "https://en.wikipedia.org/wiki?curid=41113159", "title": "Architecture and Building Research Institute", "text": "Architecture and Building Research Institute\n\nThe Architecture and Building Research Institute (ABRI; ) a leading national research agency in Taiwan under the supervision of the Ministry of the Interior of the Republic of China (Taiwan).\n\n\nThe agency is accessible within walking distance from Dapinglin MRT station of Taipei Metro.\n\n"}
{"id": "6679480", "url": "https://en.wikipedia.org/wiki?curid=6679480", "title": "Arms Export Control Act", "text": "Arms Export Control Act\n\nThe Arms Export Control Act of 1976 (Title II of , codified at ) gives the President of the United States the authority to control the import and export of defense articles and defense services. The H.R. 13680 legislation was passed by the 94th Congressional session and enacted into law by the 38th President of the United States Gerald R. Ford on June 30, 1976.\n\nThe Act of Congress requires international governments receiving weapons from the United States to use the armaments for legitimate self-defense. Consideration is given as to whether the exports \"would contribute to an arms race, aid in the development of weapons of mass destruction, support international terrorism, increase the possibility of outbreak or escalation of conflict, or prejudice the development of bilateral or multilateral arms control or nonproliferation agreements or other arrangements.\" The Act also places certain restrictions on American arms traders and manufacturers, prohibiting them from the sale of certain sensitive technologies to certain parties and requiring thorough documentation of such trades to trusted parties.\n\nWhen the President is aware of the possibility of violations of the AECA, the law requires a report to Congress on the potential violations.\n\nU.S. Immigration and Customs Enforcement (ICE) conducts an industry outreach program called the Project Shield America to prevent foreign adversaries, terrorists, and criminal networks from obtaining U.S. munitions and strategic technology.\n\nFrom 1963 to 1973, 128 nations received $2.5 trillion in weapons and services, the majority from the United States. Law only required that the Secretary of State report “significant” arms sales to Congress, semi-annually. In the early 1970s, legislators moved to become involved in deciding to whom arm sales could be made and under what circumstances. Concern over arms sales increased significantly in the summer of 1973, when news surfaced of a potential Nixon Administration sale of F-4 fighter-bombers to Saudi Arabia.\n\nIn 1973, Norvill Jones, a staffer of the Committee of Senate Foreign Relations, tried to interest members in establishing a procedure by which Congress could review large arms sales. Finding no takers, Jones mentioned the idea to Dr. Paula Stern, then a foreign policy aide to Senator Gaylord Nelson of Wisconsin, later the Chairwoman of the U.S. International Trade Commission (1984-1986). Nelson approved the idea and introduced an amendment for a one-house veto over significant arms sales. Stern and Nelson settled on a reporting \"tripwire\" of $25 million, the cost of a squadron of F-5E's. Nelson has credited Stern with conceiving the amendment and supplying the persistence needed to steer the attention of the Senate to arms sales. Nelson's proposed floor measure passed 44 to 43 in the Senate but a like House measure introduced by Representative Jonathan Bingham of New York was defeated.\n\nIn 1974, a renewed attempt passed as the Nelson-Bingham Act of 1974. It provided that when the U.S. government offered to sell any defense article or service costing $25 million or more, the President must inform both Houses of Congress of the details, giving Congress twenty days to adopt a \"veto\" resolution. The Nelson-Bingham initiative worked \"a profound transformation in arms export policy\" and had a \"significant impact on U.S. government policy, both on long-range planning and on several major individual sales.\"\n\nIn 1975, President Gerald Ford's arms sale to Jordan led Congress to examine how to strengthen the Nelson-Bingham Amendment. A significant number of sale notifications had been allowed to be kept classified by the executive. The time period of Congressional review had been found to be deficient and the dollar level of $25 million as a tripwire had prevented a number of smaller sales from avoiding scrutiny. Members of Congress also sought to expand the scope of their review beyond government-to-government sales. The 1976 Arms Control Act incorporated these and other changes.\n\nIn the 1990s, after a report from RSA Data Security, Inc., who were in a licensing dispute with regard to use of the RSA algorithm in PGP, the Customs Service started a criminal investigation of Phil Zimmermann, for allegedly violating the Arms Export Control Act. The US Government had long regarded cryptographic software as a munition, and thus subject to arms trafficking export controls. At that time, the boundary between permitted (\"low-strength\") cryptography and impermissible (\"high-strength\") cryptography placed PGP well on the too-strong-to-export side (this boundary has since been relaxed). The investigation lasted three years, but was finally dropped without filing charges.\n\nFrom FY 2004 to FY 2006 there had been 283 arrests, 198 indictments, and 166 convictions based on AECA violations.\n\nIn 2005 the Government Accounting Office (GAO) did a study on arms exports since 9/11. The study noted that the system itself had not been changed since 9/11 since the system was already designed to counter such threats. The study did report that the processing time for arms cases increased starting in 2003.\n\nIn 2006 Boeing was fined $15 million for unlicensed foreign sales involving a gyroscopic microchip or gyrochip with military applications.\n\nIn March 2007, ITT Corporation was fined for criminal violation of the act. The fines resulted from ITT's outsourcing program, in which they transferred night vision goggles and classified information about countermeasures against laser weapons, including light interference filters to engineers in Singapore, the People's Republic of China, and the United Kingdom. They were fined $100 million US dollars, although they were also given the option of spending half of that sum on research and development of new night vision technology. The United States government will assume rights to the resulting created intellectual property.\n\nIn January 2009, Congressman Dennis Kucinich sent a notice to Secretary of State, Dr. Condoleezza Rice, that Israel’s actions in Gaza since December 27, 2008 may constitute a violation of the requirements of the AECA. The AECA requires that each nation that receives a shipment of arms from the United States must certify that the weapons are used for internal security and legitimate self-defense, and that their use does not lead to an escalation of conflict. However, the AECA does not define \"internal security\" or \"legitimate self-defense.\" Kucinich said that Israel's actions in Gaza killed nearly 600 and injured over 2,500, including innocent civilians and children in residential areas and civilian institutions like schools. Kucinich said that this may have violated the AECA because they didn't further Israel's internal security or legitimate self-defense, but increased the possibility of an outbreak or escalation of conflict. The charges were denied by the IDF and no action has been taken under the act.\n\nIn July 2009 John Reece Roth, a former University of Tennessee professor, was convicted of violating the AECA and sentenced to 48 months in prison. Roth had a United States Air Force (USAF) contract to develop plasma technology to reduce drag on airplane wings. One application was for unmanned air vehicles (drones). Roth was accused of violating the law by sharing technical (not classified) data with Chinese and Iranian graduate students, and of having technical data on his laptop during a trip to China. Roth and others said that the AECA, as applied in his case, would violate academic freedom and force professors to discriminate against students on the basis of nationality.\n\n\n"}
{"id": "30867806", "url": "https://en.wikipedia.org/wiki?curid=30867806", "title": "BBC School Radio", "text": "BBC School Radio\n\nBBC School Radio is a division of the BBC providing audio learning resources for primary schools in the United Kingdom.\n\nThe first broadcast to schools was organized by the privately owned British Broadcasting Company and given by the composer Sir Walford Davies, Professor of Music at Gresham College. It was transmitted from Britain's second ever radio station, 2LO in London, on 4 April 1924.\n\nFollowing the dissolution of the British Broadcasting Company on 31 December 1926 and the transfer of its assets to the Crown-chartered British Broadcasting Corporation, the Carnegie United Kingdom Trust funded a project on broadcasting to schools based on feedback collected from teachers in Kent. In 1928 the Central Council for School Broadcasting (CCSB) was established; its first two Chairmen were Herbert Fisher and Eustace Percy, 1st Baron Percy of Newcastle. For each curriculum subject covered in the broadcasts there was a Subject Committee, staffed by teachers.\n\nFrom 1929-57, the first Director of School Broadcasting was Mary Somerville. By the 1930s, secondary schools were included in the target audience and broadcasts were added covering foreign-language learning. Older listeners were also tuning in.\n\nDuring the Second World War, School Radio gained a new importance. Any regional variations were consolidated into a single home service for children with a five-minute news broadcast that was designed to explain the confusing circumstances. By 1942, half of all British schools were listening.\n\nThe School Broadcasting Council for the United Kingdom had been set up in 1947, replacing the CCSB, and included Scotland and Wales. After the Newsom Report in 1963, more series were made about the transition from school to work. The 1960s to 1980s were regarded as a 'golden age' for British schools radio broadcasting, and by the early 1970s, around 90% of schools were using the School Radio service. The BBC produced around 80 series per year for School Radio, which amounted to around 16 hours per week. From 1983, older programmes were available on cassette from the Centre of Educational Technology in Mold, Flintshire.\n\nOriginally broadcast on the BBC Home Service, they transferred to BBC Radio 4 when that network replaced the Home Service in 1967. They were broadcast on all frequencies until the start of the 1973/74 school year when they were heard only on Radio 4's FM frequencies. Programming aired on weekdays from 10.00 (9.05 on Thursdays) to 12.00 and from 14.00 to 15.00. In September 1990 they were transferred to BBC Radio 5 and BBC Radio 3. They were aired on Radio 5 during the day, between 9.00 and 10.25, and were repeated overnight, along with additional programmes, between 01.00 and 03.10 on Radio 3. Following the replacement of the original Radio 5 by Radio 5 Live, the daytime schools transmissions also moved to Radio 3, airing between 14.00 and 15.00, much to the chagrin of many of that station's listeners. From Autumn 1996 all programmes were broadcast overnight on Radio 3 where they could be pre-recorded, before being finally switched to overnight transmission via the digital version of Radio 4. Since 2003 all school radio programmes have been available on the internet. The advent of podcasting has opened up a whole new avenue for the school radio service.\n\nTV broadcasting for schools began on 13 May 1957 (this had been hoped to begin in the late 1940s, but financial constraints prevented this). This was first broadcast by Associated-Rediffusion, not the BBC, though the BBC began schools television four months later. The BBC's television service had begun in 1936, and stopped for the war, to begin again in 1946. Schools television was shown on BBC1 until 1983 when it moved to BBC2.\n\nNo commercial broadcasters have ever produced educational radio programmes for schools.\n\nOn 4 April 1984, John Dunn presented a programme entitled \"Faith, Hope and Clarity\", about the sixty years of BBC Schools Radio, on Radio 4. In the same year, from 3 to 5 July, a three-day festival was held at Pebble Mill Studios in Birmingham. It was hosted by Duncan Goodhew, Sue Lawley, and Rolf Harris.\n\nIn 1939 the School Broadcasting Department moved to Bristol.\n\nPresenters have included -\n\nIn 2011 School Radio moved from its home on \"Wood Lane\" in London up to the newly built BBC Bridge House at \"MediaCityUK\", also home to BBC Bitesize, BBC Teach, and BBC Children's (CBeebies and CBBC). A small School Radio team is based in Scotland, producing programmes exclusively for Scotland's Curriculum for Excellence.\n\nBroadcasts took place in the middle of the night (starting at 3.00) on Radio 4 Digital from Tuesdays to Thursdays. Programmes could be recorded under the Educational Recording Agency copyright laws, but podcasts are freely available, online and on the BBC iPlayer Radio app.\n\nOn the 28th June 2018 School Radio ended it's run on BBC Radio, instead becoming an online streaming and podcast download exclusive service, with content uploaded throughout the year, rather than following strict broadcast schedules.\n\nContent is divided into twelve subjects:\n\nThe English section includes a selection of abridgements of classic stories told by celebrity voices, including \"The Wind in the Willows\", read by Bernard Cribbins, and \"The Tales of Hans Christian Andersen\", read by Anne-Marie Duff, Sir Derek Jacobi, David Tennant and Penelope Wilton, amongst others.\n\nPre-recorded programmes were previously available on CD or DVD from BBC Schools' Broadcast Recordings, but these were eventually phased out in favour of online podcast versions.\n\n\n\n"}
{"id": "39415583", "url": "https://en.wikipedia.org/wiki?curid=39415583", "title": "Beltri", "text": "Beltri\n\nThe BELTRI, or \"Balance Elevator-Lift for Tractioning Recumbent Individuals\" is an apparatus designed for the mobilization of bedridden patients. It consists of a metallic structure with pulleys and ropes, that uses a counterbalance to lift the weight of a patient without difficulty. It was first introduced in Austria in the 19th century by Dr. Carl Emmert and is now used throughout the world by many nurses and physicians.\n"}
{"id": "22532922", "url": "https://en.wikipedia.org/wiki?curid=22532922", "title": "Bticino", "text": "Bticino\n\nBticino S.p.A. is an Italian metalworking company that operates in the field of electrical low voltage equipment used for residential, employment and production. Bticino proposes solutions for the energy distribution, for the communication (intercoms and video intercoms) and for the control of light, sound, climate and security.\n\nFounded in 1936 by brothers Arnaldo, Luigi and Ermanno Bassani with the name of Ticino Electric Switches to produce metal smallwares of different applications. Since 1948, the company became Bassani SpA and specializes itself in the manufacture of electrical components used inside houses, in order to meet the growing demand resulting from post-conflict reconstruction.\n\nResponding to changes in electrical technology and the demands of living spaces, Bassani Ticino (name adopted in 1974) developed a set of devices to control the distribution of low voltage energy, designing a range of products – from the \"electric switchboard\" to the switches – which combine the technical and functional features related to the installation mode, with special attention to drawing.\n\nIn 1989, Bassani Ticino joined the French group Legrand, changing its name to BTicino.\nBticino was the first firm to conceive the switches as furniture (not only as industrial components), developing a specific treatment aimed at improving product quality and simplifying implementation.\n\nThe company became famous in the Italian market during the 50s; \"Domino\" and \"Sicura\" (“Safe”) were the names of the first series that got a big success.\nThe products of the '60s are characterized by ease of assembly and by modularity: they had spread the rectangular box, which represents the current standard.\nThe Magic Series (1961) is the first case of power socket with modular switches.\n\nThe issue of security becomes increasingly important as it increases the capillary diffusion of the electrical systems, which make it more likely an accidental contact with an electrical conductor in tension.\nIt is then built and patented a circuit breaker called a “residual current device” (1965); it’s high-power switching, with the addition of a differential transformer.\n\nThe company is also known over the years for the plaques that surround the normal switches inside the house.\nWith the Series Living (1985) and the subsequent Light and Living International (1996) are introduced devices of multifunctional control of individual components.\n\nIn 2001 Bticino launched the first MyHome Domotics system based on bus SCS technology.\nContinuing on the path toward automation, Axolute series (2005) first proposed the integration of video-intercom with domestic installation, while Axolute Nighter & Whice (2008) marks a further step towards aesthetics simplification of electrical equipment for residential use.\n\nIn 2006 it was made available the protocol OpenWebNet, that allows interaction with the Domotics plant through the use of appropriate Gateway.\n\nIn 2007 Bticino obtained a place on www.grandesuccesso.it, worldwide website dedicated to reward success and quality (as a cause of success itself).\n\nThe company is part of the International Group Legrand and operates both on the Italian market and on the Worldwide one with over 60 offices abroad.\nThe main products in the field of electrical equipment are traditional and domotics switches, plates, residual current devices, intercoms and video intercoms.\n\nThe main office and headquarter is in Varese, where there are 1500 employees, split between design, production, quality, test, marketing, administrative and commercial offices. Other important locations are Erba, where all the domotics products and video intercoms are developed and produced; Ospedaletto Lodigiano where is located the central warehouse; Bodio, where the plaques are made; Azzano San Paolo and Torre del Greco, where are developed and manufactured devices industrial and residual current device.\n\nBTicino is present on many markets: Europe (Belgium, Spain), Latin America (Brazil, Mexico, Costa Rica, Venezuela, Chile, Peru), Asia (Thailand, China), and the United States.\n\nThe strategy of the company is to bring quality, technology and Italian design values, especially introducing within the functional and installation performance the merits of aesthetics and comfort..\n\nThe company sponsored the 2008 UCI Road World Championships held in Varese.\n\n\n"}
{"id": "1303149", "url": "https://en.wikipedia.org/wiki?curid=1303149", "title": "Cirrus (interbank network)", "text": "Cirrus (interbank network)\n\nCirrus (stylized as cirrus) is a worldwide ATM network. It is a subsidiary of Mastercard and based in Purchase, New York. Founded in 1982, it links Mastercard and Maestro credit, debit and prepaid cards and Cirrus ATM cards to a global network of millions of ATMs.\n\nPrior to its acquisition by Mastercard in 1987, Cirrus System, LLC was owned by Bank of Montreal, BayBanks Inc., First Interstate Bancorp, Mellon Bank, NBD Bancorp Inc. and Norwest Corp.\n\nBy default, Mastercard and Maestro cards are linked to the Cirrus network, but very often all three logotypes will be shown. Canadian, American, Venezuelan, Chilean and Saudi Arabian ATMs use this network alongside their local networks and many banks have adopted Cirrus as their international interbank network alongside either a local network, the rival Plus ATM network owned by Visa, or both. In countries such as India and Bangladesh, the Cirrus network also serves as a local interbank network as well as an international network.\n\n"}
{"id": "32760944", "url": "https://en.wikipedia.org/wiki?curid=32760944", "title": "Cloud engineering", "text": "Cloud engineering\n\nCloud engineering is the application of engineering disciplines to cloud computing. It brings a systematic approach to concerns of commercialization, standardization, and governance of cloud computing applications. In practice, it leverages the methods and tools of engineering in conceiving, developing, operating and maintaining cloud computing systems and solutions. It is about the process of designing the systems necessary to leverage the power and economics of cloud resources to solve business problems.\n\nCloud engineering is a field of engineering that focuses on cloud services, such as \"software as a service\", \"platform as a service\", and \"infrastructure as a service\". It is a multidisciplinary method encompassing contributions from diverse areas such as \"systems engineering\", \"software engineering\", \"web engineering\", \"performance engineering\", \"information technology engineering\", \"security engineering\", platform engineering, service engineering, risk engineering, and quality engineering. The nature of commodity-like capabilities delivered by cloud services and the inherent challenges in this business model drive the need for cloud engineering as the core discipline.\n\nElements of Cloud Engineering include:\n\nThe professionals who work in the field of cloud engineering are primarily cloud architects and engineers. The key skills possessed by cloud engineering professionals are:\n\nThe demand for skills in advanced ICT (Information and Communication Technology) has rapidly expanded in recent years as business and society are being transformed by the emergence of Internet and Web as ubiquitous media for enabling knowledge-based global economy. This in turn has created a huge demand for networked-enabled parallel and distributed computing technologies that are changing the way we conduct science, operate business, and tackle challenging problems such as epidemic diseases and climate change.\n\nThere are many platforms available for cloud engineering.\n\nThe notion of cloud engineering in the context of cloud computing had been sparsely used in discussions, presentations and talks in various occasions in the middle of the 2000s. The term of cloud engineering was formally coined around 2007 and the concept of cloud engineering was officially introduced in April 2009. Various aspects and topics of this subject have been extensively covered in a number of industry events. Extensive research has been conducted on specific areas in cloud engineering, such as development support for cloud patterns, and cloud business continuity services. The first IEEE International Conference on Cloud Engineering (IC2E) took place on March 25–28, 2013 and the second conference was held on March 10–14, 2014.\n\n\n"}
{"id": "20054068", "url": "https://en.wikipedia.org/wiki?curid=20054068", "title": "Complementary assets", "text": "Complementary assets\n\nTwo assets are said to be complements when investment in one asset increases the marginal return on the other. On the contrary, assets are substitutes when investment in one reduces the marginal return on the other.\n\nIf the production process is described by the production function formula_1, where formula_2 and formula_3 are the amounts invested of the two assets, then it is possible to define formally the elasticity of substitution as \nIf formula_5 is higher than 1, the assets are substitutes; if lower, complements.\n\nIn the field of stratetgy, the concept is sometimes understood to apply to assets, infrastructure or capabilities needed to support the successful commercialization and marketing of a technological innovation, other than those assets fundamentally associated with that innovation. The term was first coined by David Teece. Key empirical studies on complementary assets were conducted by Frank T. Rothaermel.\n\nComplementary assets are broken down into three general types:\n\nComplementary assets, among other factors, are important for organizations wishing to commercialize and profit from an innovation.\n\nNew biotechnology firms often lack the complementary assets to commercialize their innovations and thus form collaborative partnerships with large incumbent firms who do possess the necessary complementary assets such as manufacturing capabilities, marketing channels, brand name, etc. (Rothaermel, 2001)\n\nRC Cola was the first firm to commercialize both diet cola and cola in a can. However, rivals Coca-Cola and Pepsi soon imitated this and beat RC Cola out of the market based on their superior marketing capabilities and brand name recognition, i.e. their complementary assets (Teece, 1986) .\n"}
{"id": "1338457", "url": "https://en.wikipedia.org/wiki?curid=1338457", "title": "Contact list", "text": "Contact list\n\nA contact list is a collection of screen names. It is a commonplace feature of instant messaging, Email clients, online games and mobile phones. It has various trademarked and proprietary names in different contexts.\n\nThe contact list is just a list. Its window shows screennames that represent actual other people. To communicate with someone on the list, the user can select a name and act upon it, for example open a new E-mail editing session, instant message, or telephone call. In some programs, if your contact list shows someone, their list will show yours. Contact lists for mobile operating systems are often shared among several mobile apps.\n\nSome text message clients allow you to change your display name at will while others only allow you to reformat your screen name (Add/remove spaces and capitalize letters). Generally, it makes no difference other than how it's displayed.\n\nWith most programs, the contact list can be minimized to keep it from getting in the way, and is accessed again by selecting its icon.\n\nThe style of the contact list is different with the different programs, but all contact lists have similar capabilities.\n\nSuch lists may be used to form social networks with more specific purposes. The list is not the network: to become a network, a list requires some additional information such as the status or category of the contact. Given this, contact networks for various purposes can be generated from the list. Salespeople have long maintained contact networks using a variety of means of contact including phone logs and notebooks. They do not confuse their list with their network, nor would they confuse a \"sales contact\" with a \"friend\" or person they had already worked with.\n\n"}
{"id": "36586200", "url": "https://en.wikipedia.org/wiki?curid=36586200", "title": "Cooper Research Technology", "text": "Cooper Research Technology\n\nCooper Research Technology Limited (also known as Cooper Technology) is a British manufacturer of high-performance civil engineering materials testing equipment, based in Ripley, Derbyshire, England. The company specialises in the design and manufacture of laboratory testing equipment used for the investigation of the mechanical properties of materials used in road construction. While at the University of Nottingham in the 1980s, founder Keith Cooper designed a method of measuring and assessing the mechanical properties of asphaltic materials with the equipment known as the Nottingham Asphalt Tester (NAT), later superseded by the UTM-NU (Servo-Pneumatic Universal Testing Machine). \n\nThe company over the years designed and developed a range of materials testing equipment which includes gyratory compactors, and roller compactors to prepare and compact specimens, in addition to servo hydraulic and servo pneumatic universal testers, triaxial testers, fatigue testing systems and wheel trackers to perform standards.\n\nThe company manufactures a range of equipment for testing asphalt, bitumen, aggregate, concrete, cement and soil, as well as general laboratory equipment. It also offers a United Kingdom Accreditation Service calibration service.\n\n"}
{"id": "39034966", "url": "https://en.wikipedia.org/wiki?curid=39034966", "title": "Exemptions for hydraulic fracturing under United States federal law", "text": "Exemptions for hydraulic fracturing under United States federal law\n\nThere are many exemptions for hydraulic fracturing under United States federal law: the oil and gas industries are exempt or excluded from certain sections of a number of the major federal environmental laws. These laws range from protecting clean water and air, to preventing the release of toxic substances and chemicals into the environment: the Clean Air Act, Clean Water Act, Safe Drinking Water Act, National Environmental Policy Act, Resource Conservation and Recovery Act, Emergency Planning and Community Right-to-Know Act, and the Comprehensive Environmental Response, Compensation, and Liability Act, commonly known as Superfund.\n\nHydraulic fracturing, also known as fracking, is a process used to extract oil and natural gas. The process to extract oil and natural gas begins with thousands of gallons of water, mixed with a slurry of chemicals, some of which are undisclosed. This liquid mixture is then forced into well casings under high pressure, and then is horizontally injected into bedrock to create cracks or fissures. The forced change in geologic structure allows gas molecules to escape, therefore allowing the natural gas to be harvested.\n\nHydraulic fracturing has changed the energy scene as a result of many technological advances. Fracking uses both historically-known vertical and horizontal drilling techniques which are used in tandem to extract oil and gas. This process can occur at depths over 10,000 feet deep.\n\nThe primary product of hydraulic fracturing is natural gas which consists mostly of methane.\n\nCongress passed the 1970 Clean Air Act to ensure that the general public was protected from harmful levels of criteria pollutants, established by the Environmental Protection Agency (EPA). The six regulated criteria pollutants include: particulate matter, lead, ozone, NO, carbon monoxide, and sulfur dioxide. All pollutant levels are calculated by associated health risks that would harm the most sensitive subgroup of people, which are considered to be inner city children. Any major pollution sources must abide by the National Emissions Standards for Hazardous Air Pollutants (NESHAP), by using the \"Maximum Achievable Control Technology\" (MACT) specified for their industry.\n\nIn 1977 and 1990, Congress amended the law to create provisions related to the Prevention of Significant Deterioration clauses as well as create programs related to controlling and preventing acid rain, respectively. However, under the act, major sources of hazardous air pollutants are required to obtain a \"Title V\" permit to ensure that the minimum standards are in place under the regulations, while area sources are not. Most oil and gas production sites are not required to obtain a Title V permit because their emissions threshold is just slightly below the categorical statutory definition. In addition to not having to obtain a Title V permit, the oil and gas exploration and production wells are exempt from the \"Aggregation Rule\" within the definition of \"major source\" as defined under the act, essentially to be unregulated under this federal statute.\n\nThe Clean Water Act is a result of the 1972 amendments to the Federal Water Pollution Control Act, which was passed to ultimately eliminate pollution discharge into any body of water in the United States. One of the major mechanisms for implementing this statute was to create a permitting process for all discharging methods that involved dumping pollutants into streams, lakes, rivers, wetlands, or creeks. The National Pollution Discharge Elimination System (NPDES) permitting requirements apply to all phases of the petroleum industry. Petroleum industry waste, including frac flowback and produced water, cannot be discharged to the waters of the United States, except under an NPDES or equivalent state permit.\n\nIn 1987, Congress amended the Act, requiring the EPA to develop a permitting program for storm water runoff, but the exploration, production, and processing of oil and gas was exempt.\n\nThe Energy Policy Act of 2005 expanded the exemption to include exemptions for runoff from gas and oil construction activities which include \"oil and gas exploration, production, process, or treatment operations and transmission facilities.\"\n\nIn 2006 EPA promulgated regulations that would not require oil and gas facilities to obtain storm water runoff permits, if the runoff is \"composed entirely of storm water\", which is defined as composed of \"precipitation runoff\" and \"not contaminated by contact with or that has not come into contact with, any overburden, raw material, intermediate products, finished product, byproduct or waste products located on the site of such operations.\" Any discharges containing other than precipitation runoff, such as petroleum or produced wastewater are still subject to criminal prosecution under the Clean Water Act. EPA's 2006 rule was vacated by the United States Court of Appeals for the Ninth Circuit.\n\nIn 1974, the Safe Drinking Water Act (SDWA) was passed to protect the quality of U.S. public drinking water and aims to protect above and below ground water sources that are or could potentially be used for human consumption. Section C of the SDWA requires the EPA to establish minimum regulations for state Underground Injection Control Programs. Under part C, Section 1421 of the SDWA, underground injection is \"the subsurface emplacement of fluids by well injection.\" The oil and gas industry makes extensive use of Class II injection wells, which are regulated under the SDWA. There are currently about 144,000 such wells with permits issued to SDWA standards. Most Class II injector wells are for enhanced oil recovery, such as waterfloods. About 20 percent of Class II wells are used in waste disposal, to dispose of produced water, usually brine, into deep formations below the base of fresh water.\n\nFrom the time of the passage of the 1974 SDWA, EPA declined to require Class II UIC permits for hydraulic fracturing. The agency maintained that it was not required to do so, because underground injection was not the \"principal function\" of the wells. The EPA also cited the \"endangerment clause\" in SWDA section 1421(b)(2), which directs the EPA to establish regulations which: \"… are essential to assure that underground sources of drinking water will not be endangered by such injection.\" The agency stated that it did not consider hydraulic fracturing to be an endangerment to underground drinking water sources.\n\nThe policy was overturned in 1997 by the United States Court of Appeals for the Eleventh Circuit, which ruled that \"hydraulic fracturing activities constitute underground injection according to Section C of the SDWA. This required the EPA and state underground injection control programs to regulate hydraulic fracturing under the SDWA.\n\nThe EPA responded with a study of potential and actual impacts of hydraulic fracturing of coalbed methane wells on drinking water. The draft report was issued for public comment in 2002, and the final report was published in 2004. Section 7.4 of the report \"concluded that the injection of hydraulic fracturing fluids into coalbed methane wells poses little or no threat to USDWs and does not justify additional study at this time.\" The exception was for frac fluids containing diesel fuel, which the EPA concluded could pose a threat.\n\nThe conclusions of the EPA report were incorporated into law the following year, by two amendments of the SDWA contained in the 2005 Energy Policy Act. The amendments added two exclusions to the definition of underground injection: \"\"(i) the underground injection of natural gas for purposes of storage; and (ii) the underground injection of fluids or propping agents (other than diesel fuels) pursuant to hydraulic fracturing operations related to oil, gas, or geothermal production activities. This provision became known to its critics as the \"Halliburton loophole\" named after the oil services firm Halliburton.\n\nThe National Environmental Policy Act (NEPA) of 1969 requires federal agencies to conduct an environmental assessment for all major actions potentially affecting the environment. If the assessment determines that the federal action may significantly alter the environment, then an environmental impact statement (EIS) is required.\n\nThe Energy Policy Act of 2005 created a rebuttable presumption that certain oil and gas related activities authorized by the U.S. Department of the Interior in managing public lands, and the U.S. Department of Agriculture in managing National Forest System lands are subject to a \"categorical exclusion\" under NEPA, and do not require an EIS, unless it can be demonstrated that they pose a risk to the environment, Congress specified five circumstances for which there would be such a rebuttable presumption that an additional EIS is not required:\n\nOther than for the exclusions listed above, federal agencies are required by NEPA to do Environmental Impact Statements to evaluate any oil and gas activities which have the potential to seriously affect the environment. Such EIS's are routinely done for specific areas by the U.S. Forest Service,\nthe Bureau of Land Management, and the Bureau of Ocean Energy Management.\n\nThe Resource Conservation and Recovery Act (RCRA) of 1976 was passed \"to protect human health and the environment from the potential hazards of waste disposal, to conserve energy and natural resources, to reduce the amount of waste generated, and to ensure that wastes are managed in an environmentally sound manner.\" Subtitle C of RCRA gives the EPA the authority to regulate the generation, transport, treatment, storage and disposal of all deemed hazardous waste.\n\nIn December 1978, the EPA issued its proposed RCRA regulations. For RCRA Subtitle C (hazardous waste management), the EPA defined six categories of \"special wastes,\" which were generated in high volumes and were believed to be less hazardous than the other wastes for which RCRA Subtitle C was designed. Among the special wastes were included cement kiln dust, fly ash, mining wastes, and wastes from oil, gas, and geothermal exploration and production. The oil, gas, and geothermal wastes included drilling fluids, produced waters, and other wastes associated with oil and natural gas exploration, development, or production. The EPA proposed that regulation of special wastes under Subtitle C, be deferred until further study.\n\nPrior to the completion of the EPA's regulatory determination, Congress enacted the Solid Waste Disposal Act in 1980 which exempted oil field wastes under section C of RCRA unless the EPA determined that the waste was hazardous.\n\nEach of the six special waste categories was the subject of separate EPA study. In July 1988, the EPA finished its study of oil, gas, and geothermal production wastes, in which it concluded that they did not warrant regulation under RCRA Subtitle C, but noted that they would continue to be regulated under Subtitle D (solid waste disposal). The EPA's decision was based on its determinations that oil, gas, and geothermal production was already regulated by the states, that Subtitle C did not have the regulatory flexibility to deal effectively with the wastes, and that the permitting requirements of Subtitle C would impose unreasonable delays on oil, gas, and geothermal extraction. However, the EPA report identified regulatory gaps for oil and gas wastes, for which it recommended additional rules under existing EPA regulatory authority, under RCRA Subtitle D, the Clean Water Act, and the Safe Water Drinking Act.\n\nFederal regulation of the storage of petroleum was established by the Oil Pollution Act of 1990.\n\nThe Emergency Planning and Community Right-to-Know Act, or EPCRA was passed by Congress in 1986 to help communities plan for emergencies that involve hazardous substance spills or releases. The Act requires federal, state, local governments and Indian tribes to inform the public of hazardous and toxic chemicals being used or stored at facilities, their use, and any release into the environment. The provisions of the EPCRA include emergency planning (Sections 301-303), and emergency release notification (Section 304).\n\nThe Toxics Release Inventory Reporting (Section 313) of \"EPCRA requires the EPA and States to collect data on releases and transfers of listed toxic chemicals.\" The facilities required to report releases and transfers under section 313 are those in certain industries on a Standard Industrial Classification list determined by the EPA. The EPA has steadily expanded the law's coverage by adding new industrial classifications to the list. As of 2014, the oil and gas industry has not been added to the list, and is therefore exempt from the EPCRA Section 313.\n\nThe Comprehensive Environmental Response, Compensation, and Liability Act, also known as Superfund was enacted in 1980 to clean up sites where toxic or hazardous substances have been dumped into the environment. The law can be retroactively implemented, and all potentially polluting parties can be held responsible for the costs. As of March 26, 2015, there have been a total of 1,709 Superfund sites, of which 386 (23%) have been remediated.\n\nUnder Section 9601(14) of CERCLA, hazardous waste definitions exclude crude petroleum, including crude oil, natural gas liquids, and any of their component fractions. Included in the exemption are refined petroleum products, such as gasoline and diesel fuel, insofar as their content of naturally occurring petroleum compounds. If any spills that would be otherwise classified under the Superfund contain only petroleum compounds, they are exempt from the cleanup process associated with CERCLA. The petroleum exemption does not extend to hazardous contaminants such as PCBs or pesticides, which are sometimes mixed with petroleum product. \"Moreover, if the petroleum product and an added hazardous substance are so commingled that, as a practical matter, they cannot be separated, then the entire oil spill is subject to CERCLA response authority.\" As of 1987, there were at least 153 CERCLA Superfund sites that included waste oil.\n\nDespite the petroleum exemption, the EPA has exercised its power under CERCLA to intervene where it considers oil and gas operations to pose \"imminent and substantial danger to the public health or welfare.\" Citing its CERCLA authority, the EPA has investigated instances of groundwater pollution it believed were related to oil and gas wells, including those at Pavillion, Wyoming, Dimock, Pennsylvania, and a Marcellus shale gas well in Bradford County, Pennsylvania.\n\nCongress addressed petroleum contamination in the 1986 Superfund Amendments and Reauthorization Act, which authorized the EPA to enforce the environmental cleanup of petroleum hydrocarbons released from underground storage tanks. The act also established the Leaking Underground Storage Tank Trust Fund, to fund cleanup of petroleum hydrocarbon released from underground storage tanks at places such as gasoline stations. Sites contaminated by petroleum from leaking underground storage tanks are much more widespread and numerous than CERCLA Superfund sites. The EPA notes that nearly every community has petroleum contamination beneath present or former gasoline stations. As of September 2014, the federally financed but mostly state-run leaking underground storage tank program has found 521,271 petroleum releases from underground storage tanks at 205,000 facilities, 86% of which have been remediated. In fiscal year 2014, 6,847 new leaking tanks were discovered. The program is financed by a federal 0.1-cent tax on petroleum products.\n\nThere have many debates surrounding the regulatory exemptions for hydraulic fracturing. It has been noted that if not for the exemption for hydraulic fracturing in the Energy Policy Act of 2005 or the RCRA exemption that exempts oil and gas waste from being designated as a hazardous waste, underground injection would have included fracking operations, and the EPA would have had the power to further regulate it as well as enforcing disclosure requirements.\n\nThe oil and gas industry supports the idea that states should control the regulatory specificities of fracking. Some contend that these exemptions are carefully analyzed. A 2004 EPA study concluded that fracking injection in coalbed methane wells \"posed little or no threat to drinking water;\" the study has since been contraverted. and some still contend that there is a lack of funded studies to show a large scale degree of fracking fluid polluted groundwater. Many oil and gas companies contend that the regulations currently in place are sufficient.\n\nThe June 2015 draft report of an ongoing U.S. Environmental Protection Agency study on fracking effects on drinking water listed a number of mechanisms by which fracking can degrade drinking water. The draft report noted among its major findings:\n"}
{"id": "9346011", "url": "https://en.wikipedia.org/wiki?curid=9346011", "title": "Fish wheel", "text": "Fish wheel\n\nA fish wheel, also known as a salmon wheel, is a device situated in rivers for catching fish which looks and operates like a watermill. However, in addition to paddles, a fish wheel is outfitted with wire baskets designed to catch and carry fish from the water and into a nearby holding tank. The current of the river presses against the submerged paddles and rotates the wheel, passing the baskets through the water where they intercept fish that are swimming or drifting. Naturally a strong current is most effective in spinning the wheel, so fish wheels are typically situated in shallow rivers with brisk currents, close to rapids, or waterfalls. The baskets are built at an outward-facing slant with an open end so the fish slide out of the opening and into the holding tank where they await collection. Yield is increased if fish swimming upstream are channeled toward the wheel by weirs.\n\nFish wheels were used on the Columbia River in Oregon by large commercial operations in the early twentieth century, until were banned by the U.S. government for their contribution to destroying the salmon population (see below). The wheel's prevalent use in catching salmon, (in particular, salmon species Chinook, Chum, Coho, Sockeye, and Pink) and other anadromous species of fish, has given fish wheels their second name as \"salmon\" wheels. Although salmon were prioritized by commercial fishers and Indigenous peoples (albeit for different reasons,) other fish such as steelhead trout (\"Oncorhynchus mykiss\"), ooligan (\"Thaleichthys pacificus\"), and lamprey (\"Lampetra tridentata\") were also considered valuable catch. While the fish wheel is best known for its presence on the Northwestern coast of North America, there is debate whether the technology arrived via Asian migrants who had come to labor in the gold fields, by Scottish and Russian migrants, or was a potentially Scandinavian invention sometime during the turn of the twentieth century. \nThe advent of fish wheel technology in the early twentieth century also drew interest from various First Nations communities of Northwestern North America, as well as dog-sledders. Ultimately, the efficacy of the wheel proved an excellent means of subsistence for hungry sled dogs and humans alike, and began to draw communities toward fertile rivers where they started using wheels to feed themselves. This changed routine hunting grounds for many communities including some Northern Athabaskan First Nations (such as Haida and Tlingit), who began to place more emphasis on fishing than hunting.\n\nSince this time, despite being a foreign technology, the fish wheel has become a culturally embedded tool for self-subsisting communities and Indigenous peoples of the Northwestern area of North America; the latter of whom have incorporated it in some ways with their traditional ecological knowledge. As well, the fish wheels of today are enjoying a sort of beneficial renaissance wherein strict rules and regulations from both Canada and the United States have been instituted to restrict them in commercial uses, and instead, are encouraged as a means to feed small off-grid communities, and in conservation efforts.\n\nThe implementation of fish wheels in the Pacific Northwest at the dawn of the twentieth century made salmon a lucrative commodity for new settlers, but they also significantly contributed to the destruction of various salmon populations along the coast. This not only implicated the environmental ecology of the area, but was also greatly problematic for the surrounding Indigenous communities, as salmon have long been a culturally embedded food and species for such First Nations peoples for many reasons that can be seen in their traditions of smoking salmon meat, to clothing used in rituals, and the prominent featuring of salmon in First Nations art. The unique life cycle of salmon—wherein the fish migrate from the ocean up rivers to spawn and die, and whose spawn repeats the cycle by returning to the ocean to mature—made for an interesting source of food, in particular because different species of salmon spawn at different times during the year, and in different rivers. Therefore, as a food not omnipresent throughout the year, the coming of new salmon journeying up river to spawn was a celebration. Despite the variance of cultural traditions between the many coastal tribes of the Northwest, this celebration, known as the \"First Salmon Ceremony,\" is a ceremony that all such communities share in common, and all rejoice in the return of the salmon. Such a dependence on the return of these fish made Indigenous communities of the coast sensitive to the healthy procreation of journeying salmon, and in this way, just as the return of salmon heralded a season of harvest, it also cautioned fishermen to reap only a selective number, so there was enough salmon left to spawn, and ultimately, return the following year. This model of seasonal adherence and moderation made for a dependably renewable food source, and a naturally sustainable relationship between people and salmon. With this in mind, Traditional Ecological Knowledge of Indigenous peoples is increasingly becoming an important topic of conversation in addressing policy-related issues of environmental sustainability. Industrialized fishing brought about by Euro-American settlers in the late nineteenth century not only greatly disturbed the food sovereignty and food security of local Indigenous communities, but was interpreted by these communities as both disrespectful to the salmon, and also their way of life. This, among many other things, contributed to tensions between Indigenous, and non-Indigenous communities of the Northwest.\n\nThe abundance of salmon in the Columbia River of Oregon state made the area popular to Euro-American traders and business-people in the 19th century, those whom quickly anchored a profitable business of trade with Indigenous communities, river boats, and steamships traveling along the Pacific coast. However, the landscape of trade changed drastically with the boom of the industrial revolution, and the arrival of the Northern Pacific Railroad to Oregon in 1883. The revolution also brought with it new technologies in food preservation—canning, in particular—and consequently, new types of entrepreneurs who saw opportunity in the Columbia River as grounds to establish salmon canneries. These companies placed their factories strategically at the beginning of the salmon's upstream migration, when the fish were not yet weakened and wounded from their journey. This, however, meant for captured salmon that had yet to spawn, which would prove greatly injurious to the population. By the end of the 1880s, thirty canning companies had been erected, and brought with them new harvesting techniques, including shore seining, gillnets, and fish wheels. The wheels, in particular, were tremendously effective in the churning waters of the Columbia. One fish wheel, for example, recorded a catch of 227,000 pounds of salmon in one day in 1894. By 1900, seventy-six fish wheels had been erected between the Cascades, Celilo Falls, and the Dalles Rapids. 1911 marked the highest year of harvest at forty-seven million pounds of fish, but also drew attention to a rapidly declining salmon population. The efficacy of fish wheels made them unpopular with other fishers on the Columbia, including Indigenous communities dispossessed of their traditional hunting grounds, downriver gillnetters, and even sport fishers who found the wheels ignoble. Contrariwise, the fish wheel operators pointed blame at the gillnetting fleets for being responsible for destroying the salmon population. The argument grew heated and drew the attention of conservationists and government officials who soon joined the conversation, and eventually legislation to limit salmon harvest was enacted, including restrictions on gillnets, and the prohibition of fish wheels, which were officially outlawed in Oregon by 1926 and in Washington in 1934.\n\nThe legal battle entitled United States v. Winans exemplifies the climate of Indigenous relationships with the state at the time, in regards to land and water dispossession by the encroaching establishment of such fishing industries. The United States had recently entered into several treaties with certain First Nations tribes of the Pacific Northwest wherein land occupied by the First Nations was taken by the state in exchange for monetary compensation and small land reserves (\"reservations\") where said tribal communities were guaranteed the security of practicing their cultural traditions—including hunting and fishing. With these treaties newly in place, fishing and canning companies were free to erect their industries on what was once Indigenous land. Brothers Lineas and Audubon Winans, for example, established a state-licensed fish wheel operation near Celilo Falls in the 1890s, which devastated the local salmon run that was otherwise of critical importance to tribes situated downstream, such as the Umatilla, Yakama and Nez Perce peoples. Likewise, under the protection of these newly-established treaties, the Winans brothers' operation also legally and forcibly prohibited passage to these Indigenous peoples to their traditional fishing grounds. The battle was fought by First Nations peoples against the state to reinstate rights to their lands whereupon the Washington State Court ruled for the Winans on the basis of their exclusive rights to private property. In response, the Indigenous community brought suit to enjoin the brothers' operation to cease using their fish wheel.\n\nIn 1949, a man running a fish wheel some twenty miles south of Fort Yukon sparked a small gold rush when he discovered pea-sized nuggets of gold caught in the baskets of the wheel, and a local radio station caught the news. Bush aircraft brought many prospectors who set up a camp along eight miles of the Yukon river, which included a small coffee shop and a clothing store, and set about panning the river bed. Unfortunately, no gold was found, and all but two of the initial nuggets were discovered to be brass. Those two that were gold were suspected to have belonged to the remains of an old prospector's cache, and were reported to have been only worth two dollars.\n\nIn Alaska and the Yukon Territory, the harvest of salmon is important for self-subsisting communities and individuals for both people and dog sled teams. The Alaska Department of Fish and Game will allocate permits for the use of fish wheels in such personal circumstances, but under strict rules and regulations, and only in specific areas of the Chitina and Copper rivers. Given these rivers traverse between the countries of Canada and The United States, state-sanctioned rules and regulations between Alaska and the Yukon are similar. Additionally, given the precarious situation of the local salmon population and its importance to Indigenous communities, community-driven initiatives like the Yukon River Panel offer critical suggestions to government policy-makers in both countries that consider the cultural relevance of salmon, and the importance in conserving their species. For example, one of many initiatives driven forward by this panel is a program for First Nations youth that involves, among many other traditional interactions with salmon, instruction in operating a fish wheel.\nThe Alaska Department of Fish and Game currently employs nine fish wheels situated along the Yukon River to help quantify the population of migrating salmon species, as does the Nisga'a Fisheries Board, with wheels in the Nass River of British Columbia. The Washington Department of Fish and Wildlife also uses fish wheels for salmon stock assessment, and does not permit the use of them for commercial gain. However, multiple studies have found that this method of live-capture, where the fish were kept for periods of time in holding tanks, then physically handled in marking procedures, initiated stress in released specimens, and ultimately impeded their ability to swim upstream. In an effort to mitigate the stress induced in these procedures, from 2001 to 2003, researchers tested the implementation of an event-triggered video-recording system on fish wheels in Alaska's Yukon River drainage. In these systems, captured fish would trigger a camera shutter which would document every fish that passed through the wheel thereby removing the need for human handling. In these experiments, this method also demonstrated an improvement in fish-counting accuracy, however the cost of the video-recording equipment makes implementation on a large scale restrictive.\n\n\n"}
{"id": "1473805", "url": "https://en.wikipedia.org/wiki?curid=1473805", "title": "Friends of Friendless Churches", "text": "Friends of Friendless Churches\n\nThe Friends of Friendless Churches is a registered charity formed in 1957 and active in England and Wales. It campaigns for and rescues redundant historic churches threatened by demolition, decay, or inappropriate conversion. To that end, as of September 2018, it owns 51 former churches or chapels, 26 of which are in England, and 25 in Wales.\n\nThe charity was formed in 1957 by Ivor Bulmer-Thomas, a writer, former MP and a high church Anglican, who became its first chairman; its executive committee included prominent politicians and architects. Initially the charity campaigned and obtained grants for the repair and restoration of churches within its remit. The 1968 Pastoral Measure established the Redundant Churches Fund (now called the Churches Conservation Trust), which it was thought would obviate the need for the Friends. However, the Church Commissioners turned down a number of buildings that the executive committee considered worthy of preservation, including Old St Matthew's Church, Lightcliffe, and St Peter's Church, Wickham Bishops. The charity therefore decided in 1972 to change its constitution, allowing it to acquire threatened buildings either by freehold or by lease. The tower of the church at Lightcliffe was the first property to be vested with the charity.\n\nThe charity raises money from a number of sources. Since 1999, it has been recognised in Wales as the equivalent of the Churches Conservation Trust (which only covers churches in England), and as a consequence receives full funding for taking Anglican churches into its care. Of this, 70% comes from the State via Cadw, and 30% from the Church in Wales. In England, grants are sometimes obtained from bodies such as English Heritage, as in the case of St Mary's Church, Mundon, but otherwise funds are raised by donations and local money-raising campaigns. Some of the churches have been supported by the formation of local groups of Friends, such as Caldecote Church Friends, and the Friends of St Andrew's, Wood Walton. Members of the public can make donations, become a member of the charity, or leave a legacy in their wills. In addition the charity administers two trusts, one of which, the Cottam Will Trust, was established by Rev S. E. Cottam for \"the advancement of religion of objects of beauty to be placed in ancient Gothic churches either in England or Wales\".\n\nAll the churches owned by the charity are listed buildings, and most are former Anglican churches, either from the Church of England or the Church in Wales, although three were private chapels, one, the Strict and Particular Baptist Chapel, Waddesdon, was a Nonconformist chapel, and another, St Mary of the Angels Church, Brownshill, was a Roman Catholic church. In the financial year ending 31 March 2010, the income of the charity was £475,367, and its expenditure was £509,034. The charity works closely with the Ancient Monuments Society. its patronage is vacant, following the death of the Marquess of Anglesey in 2013. The ecclesiastical patron is Rev Wyn Evans, Bishop of St David's and the president is the Marquess of Salisbury. In 2007 the charity achieved its 50th anniversary, in celebration of which they published a book entitled \"Saving Churches\", containing details of their history and accounts of their churches.\n\nThe list is split into two sections, one for England and the other for Wales. This division reflects the former management of the English churches (with one exception) by the Church of England, the Welsh churches by the Church in Wales, and the different funding arrangements in the two countries.\n\nKey\n\n\nThe distinctive characteristic of voluntary sources is that the donor receives nothing in return for the money given. It includes grants from government and other charitable sources, as well as public gifts, donations and legacies.<br>\nThis is the date of first construction of the existing building.\n\n"}
{"id": "33164829", "url": "https://en.wikipedia.org/wiki?curid=33164829", "title": "Golden LEAF Biomanufacturing Training and Education Center", "text": "Golden LEAF Biomanufacturing Training and Education Center\n\nThe Golden LEAF Biomanufacturing Training and Education Center (BTEC) is a multidisciplinary instructional center at North Carolina State University that provides education and training to develop skilled professionals for the biomanufacturing industry. Biomanufacturing refers to the use of living organisms or other biological material to produce commercially viable products. Examples include therapeutic proteins, monoclonal antibodies, and vaccines for medical use; amino acids and enzymes for food manufacturing; and biofuels and biochemicals for industrial applications. BTEC provides hands-on education and training in bioprocessing concepts and biomanufacturing methods that comply with cGMP (current Good Manufacturing Practice), a set regulations published by the United States Food and Drug Administration (FDA).\n\nBTEC reports administratively through the university's College of Engineering and is guided by an advisory board made up of representatives from the biomanufacturing industry and other organizations interested in biotechnology and biomanufacturing.\n\nIn 2003, North Carolina's Golden LEAF Foundation provided almost $39 million to build BTEC, as part of a larger grant to establish a statewide public-private partnership now called NCBioImpact. The state of North Carolina provided funds for process equipment and supports the operation of the facility. The NCBioImpact partnership now includes BTEC, BRITE (Biomanufacturing Research Institute and Technology Enterprise) at North Carolina Central University, North Carolina BioNetwork of the North Carolina Community College System, NCBIO (North Carolina Biosciences Organization), the North Carolina Biotechnology Center, and the Golden LEAF Foundation. It was created to provide workforce training and development for the biotechnology industry, thereby fostering the growth of this economic sector in the state. According to the North Carolina Biotechnology Center, North Carolina is home to 528 biotechnology companies that provide 57,000 jobs and $1.92 billion in taxes for state and local government. Employment in the industry has grown 4.1% from 2008 to 2010, when other industries shed thousands of jobs. In recent years some of the world's largest pharmaceutical companies, e.g. Novartis and Merck & Co, have located and/or expanded manufacturing operations in North Carolina.\n\nBTEC opened in fall 2007 and was the first facility dedicated to biomanufacturing training. BTEC is 82,500 gross square feet and contains 63,000-gross square feet of laboratories, which range from small or bench scale to large-scale suites that simulate a biomanufacturing pilot plant capable of producing biopharmaceutical products. Upstream processes utilize bacteria, yeast, animal cells, and insect cells. Equipment in these spaces includes the following:\n\n\nThe main BTEC facility is home to the North Carolina Community College System's BioNetwork Capstone Center, which operates an aseptic processing/filling suite and several bench-scale labs. In 2012, BTEC completed construction of additional laboratories in a nearby facility for cell culture, purification, and processing of active virus.\n\nBTEC delivers undergraduate and graduate courses to North Carolina State University students. Academic programs include the following:\n\n\nCurriculum was created with extensive input from industry professionals, and most courses include substantial hands-on laboratory work. Most BTEC courses are offered in a half-semester (eight-week) format, which enables students to complete a series of courses in one academic year.\n\nBTEC collaborates with industry partners to design, develop and deliver courses that provide professionals working for biomanufacturing companies, equipment vendors, or regulatory agencies with continuing education opportunities. Open-enrollment courses are offered throughout the year and are available to all interested parties. BTEC also regularly delivers courses customized to meet a client's specific needs for training.\n\nBTEC provides biomanufacturing training specified in contracts of grants to provide training for government agencies. In 2007, the FDA awarded BTEC a 5-year contract to develop and deliver biomanufacturing training for FDA inspectors. In 2010, BTEC received a grant for almost $900,000 from Biomedical Advanced Research and Development Authority (BARDA), part of the United States Department of Health and Human Services. With funding from this grant, a team of instructors from BTEC, Duke University, and industry provide a three-week course on influenza vaccine manufacturing. Trainees were selected by institutions participating in a U.S. government-sponsored program to build vaccine production capacity among countries with developing economies. Countries represented included Egypt, India, Indonesia, Mexico, Romania, Serbia, Russia, South Korea, Thailand, and Vietnam.\n\nWhen laboratories are not being used for training, BTEC uses them to perform a variety of services for scientists from industry, government, and academia. Projects involve technology development, process improvement/scale-up, analytical testing, and preparation of material for preclinical studies. These services allow scientists to advance their research projects toward commercialization. In turn, these advancements stimulate the North Carolina economy.\n\nBTEC is located on the Centennial Campus of North Carolina State University in Raleigh, North Carolina, United States. The campus is approximately 15 miles east of Research Triangle Park and Raleigh-Durham International Airport.\n\n"}
{"id": "27892242", "url": "https://en.wikipedia.org/wiki?curid=27892242", "title": "HDBaseT", "text": "HDBaseT\n\nHDBaseT, promoted and advanced by the HDBaseT Alliance, is a consumer electronic (CE) and commercial connectivity standard for transmission of uncompressed high-definition video (HD), audio, power, home networking, Ethernet, USB, and some control signals, over a common category cable (Cat5e or above) using the same 8P8C modular connectors used by Ethernet.\n\nThe HDBaseT Alliance, incorporated on June 14, 2010 by Samsung Electronics, Sony Pictures Entertainment, LG Electronics and Valens, was developed to promote the HDBaseT standard originally created by Valens. The HDBaseT 1.0 specification was also finalized in June 2010.\nExternal accessories, such as dongles, were on the market in 2010 for devices not yet embedded with HDBaseT.\nProducts were demonstrated at the 2013 Consumer Electronics Show.\n\nIn mid-2013, the HDBaseT Alliance issued Spec 2.0, an update to the original specification, which enriches the HDBaseT offering to the pro-AV market, and enables a multimedia home connectivity solution. Spec 2.0 specifies the HDBaseT network protocol, defining the required adaptations across all layers of the OSI model, to provide the optimized services for time sensitive applications, such as high throughput video and audio. Spec 2.0 maintains all the features of Spec 1.0, but also adds networking, switching, and control-point capabilities such as flexible and fully utilized mesh topology, distributed routing, and end-to-end error handling, enabling multipoint-to-multipoint connectivity and multistreaming. Spec 2.0 also embeds USB 2.0, enabling touch-screen functionality and keyboard-video-mouse (KVM) for no-latency transmission of ultra-high-definition audio and video. It also specifies either category cable or fiber as the transmission medium.\n\nThe IEEE has announced that HDBaseT has been accepted as the IEEE 1911 standard.\n\nAn Internet Protocol (IP) version of HDBaseT was demonstrated in 2017.\n\nHDBaseT is transmitted over category 5e cables or above up to 100 m (328 ft) long, with 8P8C modular connectors of the type commonly used for Ethernet local area network connections.\n\nHDBaseT transmits uncompressed ultra-high-definition video (up to 4K), audio, power over HDBaseT (PoH - up to 100W), Ethernet, USB, and a series of controls such as RS232 and IR.\n\nHDBaseT is complementary to standards such as HDMI, and it is an alternative to radio frequency, coaxial cable, composite video, S-Video, SCART, component video, D-Terminal, or VGA. HDBaseT connects and networks CE devices such as set-top boxes except Cisco and Scientific Atlantic boxes, DVD players, Blu-ray Disc players, personal computers (PCs), video game consoles, switches, matrices, projectors, and AV receivers to compatible digital audio devices, computer monitors, and digital televisions. \n\nHDBaseT delivers uncompressed ultra-HD video (up to 4K) to a network of devices or as a point-to-point connection. Uncompressed content supports all video sources, including legacy products, accurately renders gaming graphics and features such as electronic program guides, and does not degrade video quality or add latency.\nIt supports TV and PC video formats, including standard, enhanced, high-definition, ultra-HD (4K), and 3D video. Due to bitrate limitations of 10.2 Gbit/s instead of the required 18 Gbit/s in the HDMI 2.0 specification, HDBaseT 2.0 can only support uncompressed 4K at 30 Hz with 4:4:4 color coding, or 4K at 60 Hz with 4:2:0 color coding, but not the full 60 Hz with 4:4:4 color.. However, using either very light visually-lossless compression or color space conversion (CSC), it is possible to pass 4K/60/4:4:4 formats over an HDBaseT 2.0 link. The visually-lossless compression also provides support for HDR10 formats.\n\nAudio is a requirement for most consumer electronics devices. Audio is passed through the same media as video, so all standard formats are included. HDBaseT does not support the Audio Return Channel (ARC), a feature found on HDMI 1.4.\n\nHDBaseT supports the 100 Mbit/s version of Ethernet over twisted pair. This can provide Internet access, or enable televisions, stereos, computers and other CE devices to communicate with each other and access multimedia content, including video, pictures and music stored on the local network.\n\nSending power over the same LAN cable gives the option to forgo plugging devices into the wall for power. HDBaseT uses a variation of Power over Ethernet (PoE) standard called \"power over HDBaseT\" to provide up to 100W of power to CE devices, such as Blu-ray players, monitors and TVs, and eliminate the need for external power cables. HDBaseT can power remote TVs and other devices up to 100 watts. A 60\" TV connected via an HDBaseT-enabled Cat5e/6 cable requires no power source.\n\nHDBaseT delivers control signals starting from CEC that operates basic functionality such as power-on, power-off and play/stop, to RS232/USB and IR commands.\n\nFeature attributes of the specification are \"5Play feature set, the converged delivery of uncompressed ultra-high-definition digital video and audio, Ethernet, control signals, USB 2.0, and up to 100W of power through a single, 100m/328ft LAN cable.\"\n\nKey parameters of the specification are \"6Gbs tunneling of video & data, with native networking capabilities over 15m (50ft) of a single unshielded twisted-pair (UTP) cable.\"\n\nThis variant of the specification \"can distribute audio, video, Ethernet, controls, USB and power over one category cable for 100m/328ft.\" Depictions do suggest usage in a system with a central media cabinet and a distribution hub. Applications could be home entertainment and media delivery as a hotel service.\n\nThis setup claims to make \"use of a single category cable to meet all of industrial PC requirements, offering video & audio, Ethernet, controls, USB 2.0 and power over 100m/328ft.\" Further descriptions and depictions indicate the ability of daisy-chaining, usage of fiber optics for length extending, bi-directional tunneled USB connections and KVM-switch (keyboard/video/mouse) functionality that is for example usable for various types of terminal applications.\n\n\n"}
{"id": "3389161", "url": "https://en.wikipedia.org/wiki?curid=3389161", "title": "History of wood carving", "text": "History of wood carving\n\nWood carving is one of the oldest arts of humankind. Wooden spears from the Middle Paleolithic, such as the Clacton Spear, show that people have engaged in utilitarian woodwork for millennia. Indeed the beginnings of the craft go so far back that, at least where timber is present, the use of wood exists as a universal in human culture as both a means to create or enhance technology and as a medium for artistry. The North American Indian carves his wooden fish-hook or his pipe stem just as the Polynesian works patterns on his paddle. The native of Guyana decorates his cavassa grater with a well-conceived scheme of incised scrolls, while the native of Loango Bay distorts his spoon with a design of figures standing up in full relief carrying a hammock. Wood carving is also present in architecture. \n\nFigure-work seems to have been universal. To carve a figure/design in wood may be not only more difficult but also less satisfactory than sculpting with marble, owing to the tendency of wood to crack, to be damaged by insects, or to suffer from changes in the atmosphere. The texture of the material, too, often proves challenging to the expression of features, especially in the classic type of youthful face. On the other hand, magnificent examples exist of the more rugged features of age: the beetling brows, the furrows and lines neutralizing the defects of the grain of the wood. In ancient work the surface may not have been of such consequence, for figures as a rule being painted for protection and especially color.\n\nIt is not always realized at the present day to what extent color has even from the most ancient times been used to enhance the effect of wood-carving and sculpture. The modern colour prejudice against gold and other tints is perhaps because painted work has been vulgarized. The arrangement of a proper and harmonious scheme of colour is not the work of the house painter, but of the specially trained artist.\n\nIn the early 20th century, the \"Encyclopædia Britannica\" Eleventh Edition, on which much of this entry is based, commented, \"Of late years carving has gone out of fashion. The work is necessarily slow and requires substantial skill, making the works expensive. Other and cheaper methods of decoration have driven carving from its former place. Machine work has much to answer for, and the endeavor to popularize the craft by means of the village class has not always achieved its own end. The gradual disappearance of the individual artist, elbowed out as he has been, by the contractor, is fatal to the continuance of an art which can never flourish when done at so much a yard.\" This statement has proven untrue, as the continued survival of the art and craft of woodcarving can be demonstrated by the large number of who have carried on or advanced the tradition in different parts of the world.\n\nThe extreme dryness of the climate of Egypt accounts for the existence of a number of woodcarvings from this remote period. Some wood panels from the tomb of Hosul Egypt, at Sakkarah are of the III. dynasty. The carving consists of Egyptian hieroglyphs and figures in low relief, and the style is extremely delicate and fine. A stool shown on one of the panels has the legs shaped like the fore and hind limbs of an animal, a form common in Egypt for thousands of years.\n\nIn the Cairo museum may be seen the statue of a man from the period of the Great Pyramid of Giza, possibly 4000 B.C. The expression of the face and the realism of the carriage have never been surpassed by any Egyptian sculptor of this or any other period. The figure is carved out of a solid block of sycamore, and in accordance with the Egyptian custom the arms are joined on. The eyes are inlaid with pieces of opaque white quartz, with a line of bronze surrounding to imitate the lid; a small disk of transparent rock crystal forms the iris, while a tiny bit of polished ebony fixed behind the crystal imparts to it a lifelike sparkle. The IV., V. and VI. dynasties cover the finest period of Egyptian sculpture. The statues found in the tombs show a freedom of treatment which was never reached in later times. They are all portraits, which the artist strove his utmost to render exactly like his model. For these are not, like mere modern statues, simply works of art, but had primarily a religious signification (Maspero). As the spirits of the deceased might inhabit, these Ka statues, the features and proportions were closely copied.\n\nThere are to be found in the principal museums of Europe many Egyptian examples: mummy cases of human beings with the face alone carved, animal mummy cases, sometimes boxes, with the figure of a lizard, perhaps, carved in full Mummy relief standing on the lid. Sometimes the animal would be carved in the round and its hollowed body used as the case itself.\n\nOf furniture, folding seats like the modern camp stool, and chairs with legs terminating in the heads of beasts or the feet of animals, Furniture still exist. Beds supported by lions paws XI. and XII. dynasties, from Gebelein, now in the Cairo Museum), headrests, 6 or 8 in. high, shaped like a crutch on a foot, very like those used by the native of New Guinea today, are carved with scenes, etc., in outline. In the British Museum may be seen a tiny little coffer, 4 in. by 21/2 in., with very delicate figures carved in low relief. This little box stands on cabriole legs 3/4 of an inch long with claw feet, quite Louis Quinze in character. There are incense ladles, the handle representing a bouquet of lotus flowers, the bowl formed like the leaf of an aquatic plant with serrated edges from Gurnah during the XVIII. dynasty; mirror handles, representing a little pillar, or a lotus stalk, sometimes surmounted by a head of Hathor, the Egyptian Venus or of Bes, god of the toilet; pin-cushions, in the shape of a small round tortoise with holes in the back for toilet pins, which were also of wood with dog-head ends (XI. dynasty, Cairo Museum); and perfume boxes such as a fish, the two halves forming the bottom and top of the perfume or pomatum was removed by little wooden spoons, one shaped in the form of a cartouche emerging from a full-blown lotus, another shaped like the neck of a goose, a third consisting of a dog running with a fish in its closed mouth, the fish forming the bowl. The list might be prolonged, but enough has been said to show to what a pitch of refinement the art of wood-carving had reached thousands of years before the birth of Christ.\n\nOf the work of Assyria, Greece and Rome, little is actually known except from history or inference. It may be safely assumed that the Assyria craft kept pace with the varying taste and refinement of Greece and all the older civilizations. Important pieces of wooden Roman sculpture which once existed in Greece and other ancient countries are only known to us from the descriptions of Pausanias and other classic writers. Many examples of the wooden images of the gods, were preserved down to late historic times. The Palladium, or sacred figure of Pallas, which was guarded by the Vestal Virgins in Rome and was fabled to have been brought by Aeneas from the burning Troy, was one of these wooden figures.\n\nGreat works of art were created in wood during the entire Middle Ages, eg in Cathedrals, Abbeys and other Church connected sites. These works demonstrated both craftsmanship and artistry.\n\nWood-carving examples of the first eleven centuries of CE are rare due to the fact that wood do decay easily in 1,000 years. The carved panels of the main doors of St Sabina on the Aventine Hill, Rome, are very interesting specimens of early Christian relief sculpture in wood, dating, as the dresses show, from the 5th century. The doors are made up of a large number of small square panels, each minutely carved with a scene from the Old or New Testament. A very fine fragment of Byzantine art (11th or 12th centuries) is preserved in a monastery at Mount Athos in Macedonia. It consists of two panels (one above the other) of relief sculpture, surmounted by a semicircular arch of conventional foliage springing from columns ornamented with animals in foliage of spiral form. The capitals and bases are square, each face being carved with a figure. It is a wonderfully fine piece of work, conceived in the best decorative spirit.\n\nIn Scandinavian countries we find some very early work of excellent design, both Christian and Non-Christian in nature, as \"The Christening\" in that part of the world took place quite late in the first millenium CE. In the Christiania Museum there are some fine chairs. In the Copenhagen Museum there are panels from Iceland in the same style. The celebrated wooden doorways of Aal (1200 CE), Sauland, Flaa, Solder and other Norwegian churches (Christiania Museum) has dragons and intricate scroll work, a style which we still see carried on in the door-posts of the 15th century in the Nordiska museum, Stockholm, and in the Icelandic work of quite modern times. In these early days the leaf was not much developed in design. The carver depended almost entirely on the stalk, a style of work which has its counterpart in Burmese work of the 17th century.\n\nTowards the end of this epoch wood-carving reached its culminating point. The choir stalls, rood-screens, roofs, retables, of England, France and the Teutonic countries of Europe, have in execution, balance and proportion, never at any time been approached. In small designs, in detail, in minuteness, in mechanical accuracy, the carver of this time has had his rivals, but for greatness of architectural conception, for a just appreciation of decorative treatment, the designer of the 15th century stands alone.\nIt should always be borne in mind that color was the keynote of this scheme. The custom was practically universal, and enough traces remain to show how splendid was the effect of these old Gothic churches and cathedrals. The priests in their gorgeous vestments, the lights, the crucifix, the banners and incense, the frescoed or diapered walls, and that crowning glory of Gothic art, the stained glass, were all in harmony with these beautiful schemes of colored carved work. Red, blue, green, white and gilding were the tints as a rule used. Not only were the screens painted in colors, but the parts painted white were often further decorated with delicate lines and sprigs of foliage in conventional pattern. The plain surfaces of the panels were also adorned with saints, often on a background of delicate gesso diaper, colored or gilded (Southwold). Nothing could exceed the beauty of the triptychs or retables of Germany, Flanders or France; carved with scenes from the New Testament in high relief arranged under a delicate lacework of canopies and clustered pinnacles glistening with gold and brilliant colors. In Germany the effect was further enhanced by emphasizing parts of the gilding by means of a transparent varnish tinted with red or green, thus giving a special tone to the metallic luster.\n\nThe style of design used during this great period owes much of its interest to the now obsolete custom of directly employing the craftsman and his men, instead of the present-day habit of giving the work to a contractor. It is easy to trace how those bands of carvers traveled about from church to church. In one district the designer would employ a particular form and arrangement of vine leaf, while in another adjoining quite a different style repeatedly appears. The general scheme was of course planned by one mastermind, but the carrying out of each section, each part, each detail, was left to the individual workman. Hence that variety of treatment, that endless diversity, which gives a charm and interest to Gothic art, unknown in more symmetrical epochs. The Gothic craftsman appreciated the cardinal fact that in design beautiful detail does not necessarily ensure a beautiful composition, and subordinated the individual part to the general effect. He also often carved in situ, a practice seldom if ever followed in the present day. Here and there one comes across the work of long years ago still unfinished. A half-completed bench-end, a fragment of screen left plain, clearly show that sometimes at least the church was the workshop.\n\nGothic design roughly divides itself into two classes:\n\nThe lines of foliage treatment, so common in the bands of the 15th-century roodscreens and the panel work especially of Germany, serve to illustrate the widely different motives of the craftsmen of these two great epochs. Again, while the Renaissance designer as a rule made the two sides of the panel alike, the Gothic carver seldom repeated a single detail. While his main lines and grouping corresponded, his detail differed. Of numberless examples a 15th-century chest (Plate III. fig. 6) in the Kunstgewerbemuseum Berlin may be referred to. The arrangements of foliage, etc., on top, back and front, are typical of Gothic at its best.\n\nAs this section treats of woodcarving in Europe generally, and not of any one country alone, the dates just named must be of necessity only approximate. The 13th century was marked not only by great skill both in design and treatment, but also much devotional feeling. The craftsman seems to have not merely carved, but to have carved to the glory of God. At no time was work more delicately conceived or more beautifully cut. This early Gothic style certainly lent itself to fine finish, and in this respect was more suited to stone treatment than to wood. But the loving care bestowed on each detail seems to point to a religious devotion which is sometimes absent from later work. Very good examples of capitals (now, alas, divided down the center) are to be seen in Peterborough cathedral. Scrolls and foliage spring from groups of columns of four. Some Italian columns of the same date (Victoria and Albert Museum) should be compared, much to the advantage of the former. Exeter Cathedral boasts misericords unsurpassed for skilful workmanship; mermaids, dragons, elephants, masks, knights and other subjects introduced into foliage, form the designs. Salisbury cathedral is noted for its stall elbows, and the reredos in the south transept of Adisham, Kent, is another fine example testifying to the great skill of the 13th-century woodcarvers. A very interesting set of stalls, the early history of which is unknown, was placed in Barming church, Kent, about the year 1868. The book rest ends are carved with two scrolls and an animal standing between, and the ends of the stalls with figure sculpture:\n\nDuring this period foliage forms, though still conventional, more closely followed nature. The canopy work of the choir of Winchester contains exquisite carvings of oak and other leaves. The choir stalls of Ely and Chichester and the tomb of Edward III. in Westminster Abbey are all fine examples of this period. Exeter boasts a throne that of Bishop Stapledon (1308–1326) standing high, which remains unequaled for perfection of proportion and delicacy of detail. In France the stalls of St Benoit-sur-Loire, Lisieux, and Évreux are good 14th-century examples. But little Gothic work is now to be seen in the churches of this country. It is to the museums we have to look for traces of the old Gothic carvers. The two retables in Dijon Museum, the work of Jacques de Baerze (1301), a sculptor of Flanders, who carved for Philippe le Hardi, Duke of Burgundy, are masterpieces of design and workmanship. The tracery is of the very finest, chiefly gilt on backgrounds of diapered gesso.\n\nTowards the end of the 14th century carvers gave up natural foliage treatment to a great extent, and took to more conventional forms. The oak and the maple no longer inspired the designer, but the vine was constantly employed. A very large amount of 15th-century work remains to us, but the briefest reference only can be made to some of the more beautiful examples that help to make this period so great.\n\nThe rood screen, that wonderful feature of the medieval church, was now universal. It consisted of a tall screen of usually about thirty ft. high, on the top of which rested a loft, i.e. a platform rood about . in width guarded on either side by a gallery screen, and either on the top or in front of that, facing the nave, was placed the rood, i.e. a large crucifix with figures of St Mary and St John on either side. This rood screen sometimes spanned the church in one continuous length (Leeds, Kent), but often filled in the aisle and chancel arches in three separate divisions (Church Handborough, Oxon.). The loft was as a rule approached by a winding stair built in the thickness of the aisle wall. The lower part of the screen itself was solid paneled to a height of about and the upper part of this paneling was filled in with tracery (Carbrook, Norfolk), while the remaining flat surfaces of the panels were often pictured with saints on a background of delicate gesso diaper (Southwold, Suffolk). Towards the end of this period the employment of figures became less common as a means of decoration, and the panels were sometimes filled- entirely with carved foliage (Swimbridge, Devon). The upper part of the rood screen consisted of open arches with the heads filled in with pierced tracery, often enriched with crockets (Seaming, Norfolk), embattled transoms (Hedingham Castle, Essex), or floriated cusps (Eye, Suffolk). The mullions were constantly carved with foliage (Cheddar, Somerset), pinnacles (Causton, Norfolk), angels (Pilton, Devon), or decorated with canopy work in gesso (Southwold). But the feature of these beautiful screens was the loft with its gallery and vaulting. The loft floor rested on the top of the rood screen and was usually balanced and kept in position by means of a groined vaulting (Harberton, Devon) or a cove (Eddington, Somerset). The finest examples of vaulting are to be seen in Devon. The bosses at the intersections of the ribs and the carved tracery of the screen at Honiton stand unrivaled. Many screens still possess the beam which formed the edge of the loft floor and on which the gallery rested. It was here that the medieval roodscreen carver gave most play to his fancy, and carved the finest designs in foliage to be seen throughout the whole Gothic period. Although these massed moulds, crests and bands have the appearance of being carved out of one log, they were in practice invariably built up in parts, much of the foliage, etc., being pierced and placed in hollow moulds in order to increase the shadow. As a rule the arrangement consisted of a crest running along the top, with a smaller one depending from the lower edge, and three bands of foliage and vine between them (Feniton, Devon). The designs of vine leaves at Kenton, Bow and Dartmouth, all in Devon, illustrate three very beautiful treatments of this plant. At Swimbridge, Devon, there is a very elaborate combination; the usual plain beads which separate the bands are carved with twisted foliage also. At Abbots Kerswell and other places in the district round Totnes the carvers introduced birds in the foliage with the best effect. The variety of cresting used is very great. That at Winchcomb, Gloucester, consists of dragons combined with vine leaves and foliage. It illustrates how Gothic carvers sometimes repeated their patterns in as mechanical a way as the worst workmen of the present time. Little can be said of the galleries, so few remain to us. They were nearly all pulled down when the order to destroy the roods was issued in 1548. That they were decorated with carved saints under niches (Llananno, Wales), or painted figures (Strencham, Worcester), is certain from the examples that have survived the Reformation. At Atherington. Devon, the gallery front is decorated with the royal coat of arms, other heraldic devices, and with prayers. The Breton screen at St Fiacre-le-Faouet is a wonderful example of French work of this time, btit does not compare with the best English examples. Its flamboyant lines and its small tracery never obtained any foothold in England, though screens carved in this way (Colebrook, Devon) are sometimes to be found.\n\nThe rood was sometimes of such dimensions as to require some support in addition to the gallery on which it rested. A carved beam was used from which a chain connected the rood itself. At Cullompton, Devon, such a beam still exists, and is carved with foliage; an open cresting ornaments the under side and two angels support the ends. This particular rood stood on a base of rocks, skulls and bones, carved out of two solid logs averaging 18 in. wide and 21 in. high, and together measuring long; there are round holes along the top which were probably used for lights.\n\nNo country in Europe possesses roofs to equal those of England created in the 15th century. The great roof of Westminster Hall remains to the present day unique. In Norfolk and Suffolk roofs abound of the hammerbeam class; that at Woolpit, Suffolk, achieves the first rank of quality. Each bracket is carved with strongly designed foliage, the end of every beam terminates in an angel carrying a shield, and the purlins are crested, while each truss is supported by a canopied riche (containing a figure) resting on an angel corbel. Here, too, as at Ipswich and many other churches, there is a row of angels with outspread wings under the wall-plate. This idea of angels in the roof is a very beautiful one, and the effect is much enhanced by the coloring. The roof at St Nicholas, King's Lynn, is a magnificent example of tiebeam construction. The trusses are filled in with tracery at the sides and the centres more or less open, and the beams, which are crested and embattled, contain a row of angels on either side. In Devon, Cullompton possesses a very fine semicircular ceiling supported at intervals by ribs pierced with carving. Each compartment is divided up into small square panels, crossed by diagonal ribs of cresting, while every joint is ornamented with a boss carved in the decorative way peculiar to the Gothic craftsman. The nave roof of Manchester cathedral is nearly flat, and is also divided up into small compartments and bossed; the beams are supported by carved brackets resting on corbels with angels at each base.\n\nIn the 15th century, choir stalls with their canopies continued to increase in magnificence. Manchester cathedral (middle of 15th century) and Henry VII chapel in Westminster Abbey (early 16th) are good examples of the fashion of massing ~7~7 pinnacles and canopies; a custom which hardly compares with the more simple beauty of the 14th-century work of Ely cathedral. The stalls of Amiens cathedral were perhaps the finest in the world at the beginning of the 16th century. The cresting employed, though common on the Continent, is of a kind hardly known in England, consisting as it does of arches springing from arches, and decorated with crockets and finials. The tabernacle work over the end seats, with its pinnacles and flying buttresses, stretches up towards the roof in tapering lines of the utmost delicacy. The choir stalls (the work of Jörg Syrlin the Elder) in Ulm cathedral are among the finest produced by the German carver. The front panels are carved with foliage of splendid decorative boldness, strength and character; the stall ends were carved with foliage and sculpture along the top edge, as was sometimes the case in Bavaria and France as well as Germany.\n\nIn early times the choir alone possessed seats, the nave being left bare. Gradually benches were introduced, and during the 15th century became universal. The poppyhead form of B ornament now reached perfection and was constantly used enc for seats other than those of the choir. The name refers en a. to the carved finial which is so often used to complete the top of the bench end and is peculiarly English in character. In Devon and Cornwall it is rarely met with (Ilsington, Devon). In Somerset it is more common, while in the eastern counties thousands of examples remain. The quite simple fleur-de-lys form of poppyhead, suitable for the village, is seen in perfection at Trunch, Norfolk, and the very elaborate form when the poppyhead springs from a crocketed circle filled in with sculpture, at St Nicholas, King's Lynn. Often the foliage contained a face (St Margaret's, Cley, Norfolk), or the poppyhead consisted of figures or birds only (Thurston, Suffolk) or a figure standing on a dragon (Great Brincton, Northampton); occasionally the traditional form was departed from and the finial carved like a lemon in outline (Bury St Edmuncis) or a diamond (Tirley, Glos.). In Denmark an ornament in the form of a large circle sometimes takes the place of the English poppy-head. In the Copenhagen Museum there is a set of bench ends of the 15th century with such a decoration carved with coats of arms, interlacing strap-work, etc. But the old 15th-century bench end did not depend entirely on the poppy-head for its embellishment. The side was constantly enriched with elaborate tracery (Dennington, Norfolk) or with tracery and domestic scenes (North Cadbury, Somerset), or would consist of a mass of sculpture in perspective, with canopy work, buttresses and sculptured niches, while the top of the bench end would be crowned with figures carved in the round, of the finest craftsmanship. Such work at Amiens cathedral is a marvel alike of conception, design and execution. In the Kunstgewerbemuseum Berlin some beautiful stall ends are to be seen. Out of a dragons mouth grows a conventional tree arranged and balanced in excellent proportion. On another, stall end a tree is carved growing out of the mouth of a fool. This custom of making foliage grow out of the mouth or eyes is hardly defensible, and was by no means confined to any country or time. We have plenty of Renaissance examples of the same treatment.\n\nBefore the 15th century preaching had not become a regular institution in England, and pulpits were not so common. However, the value of the sermon began to be appreciated from the use to which the Lollards and other sects put this method of teaching doctrine, and pulpits became a necessity. A very beautiful one exists at Kenton, Devon. It is, as is generally the case, octagonal, and stands on a foot. Each angle is carved with an upright column of foliage between pinnacles, and the panels, which are painted with saints, are enriched with carved canopies and foliage; it is, however, much restored. The puipit at Trull, Somerset, is noted for its fine figure carving. A large figure standing under a canonv fills each of the nanelled sides. while many other smaller figures help to enrich the general effect. Examples of Gothic sounding boards are very rare; that, together with the pulpit, in the choir of Winchester is of the time of Prior Silkstede (1520), and is carved with his rebus, a skein of twisted silk.\n\nThe usual form of font cover during the hundred years before the Reformation was pyramidal, the ribs of the salient angles being Fo straight and cusped (Frindsbury, Kent) or of curved outline and cusped (St Mildred, Canterbury). There is a very charming one of this form at Colebrook, Devon. It is quite plain but for a little angel kneeling on the top, with its hands clasped in prayer. But the most beautiful form is the massed collection of pinnacles and canopy work, of which there is such a fine example at Sudbury, Suffolk. It was not uncommon to carve a dove on the topmost pinnacle (Castleacre, Norfolk), in allusion to the descent of the Holy Spirit. The finest font in England is undoubtedly that of Ijiford, Suffolk. It rises some . in height, arid when the panels were painted with saints and the exquisite tabernacle work colored and gilded, must have been a masterpiece of Gothic craftsmanship. A cord connecting the tops of these covers with the roof or with a carved beam standing out from the wall, something like a crane (Salle, Norfolk), was used to remove the cover on the occasion of baptism.\n\nMany lecterns of the Gothic period do not exist today. They usually had a double sloping desk which revolved round a central moulded post. The lectern at Swanscombe, Kent, has an eras, circle of good foliage ornamenting each face of the book rest, and sonic tracery work at either end. The box form is more common in France than in England, the pedestal of such a lectern being surrounded by a casing of three or more sides. A good example with six sides is in the church of Vance (France), and one of triangular form in the Muse of Bourges, while a four-sided box lectern is still in use in the church of Lenham, Kent. The Gothic prayer desk, used for private devotional purposes, is hardly known in England, but is not uncommon on the Continent. There is a beautiful specimen in the Muse, Bourges; the front and sides of the part for kneeling are carved with that small tracery of flowing character so common in France and Belgium during the latter part of the 15th century, and the back, which rises to a height of ., contains a little crucifix with traceried decoration above and below.\n\nA word should be said about the ciboria, so often found on the Oboria continent of Europe. In tapering arrangement of tabernacle work they rival the English font covers in delicacy of outline (Muse, Rouen).\n\nNumbers of doors are to be met with not only in churches but also in private houses. Lavenham, Suffolk, is rich in work of this latter class. In England the general custom was to carve the head of the door only with tracery (East Brent, Somerset), but in the Tudor period doors were some times covered entirely with linenfold paneling (St Albans Abbey). This form of decoration was exceedingly common on the Continent as well as in England. In France the doors towards the latter part of the 15th century were often square-headed, or perhaps had the corners rounded. These doors were usually divided into some six or eight oblong panels of more or less equal size. One of the doors of Bourges Cathedral is treated thus, the panels being filled in with very good tracery enriched with crockets and coats of arms. But a more restrained form of treatment is constantly employed, as at the church of St Godard, Rouen, where the upper panels only are carved with tracery and coats of arms and the lower adorned with simple linenfold design.\n\nTo Spain and the Teutonic countries of Europe we look for the most important object of church decoration, the retable; the Reformation accounting for the absence in England of any work of this iec kind. The magnificent altar-piece in Schleswig cathedral was carved by Hans Bruggerman, and consists, like many others, of a number of panels filled with figures standing some four or five deep. The figures in the foremost rows are carved entirely separate, and stand out by themselves, while the background is composed of figure work and architecture, etc., in diminishing perspective. The panels are grouped together under canopy work forming one harmonious whole. The genius of this great carver shows itself in the large variety of the facial expression of those wonderful figures all instinct with life and movement, In France few retables exist outside the museums. In the little church of Marissel, not far from Beauvais, there is a retable consisting of eleven panels, the crucifixion being, of course, the principal subject. And there is a beautiful example from Antwerp in the Muse Cluny, Paris; the pierced tracery work which decorates the upper part being a good example of the style composed of interlacing segments of circles so common on the Continent during late Gothic times and but seldom practised in England. ln Spain the cathedral of Valladolid was famous for its retable, and Alonso Cano and other sculptors frequently used wood for large statuary, which was painted in a very realistic way with the most startlingly lifelike effect. Denmark also possessed a school of able wood-carvers who imitated the great altar-pieces of Germany. A very large and well-carved example still exists in the cathedral of Roskilde. But besides these great altarpieces tiny little models were carved on a scale the minuteness of which staggers the beholder. Triptychs and shrines, etc., measuring but a few inches were filled in with tracery and figures that excite the utmost wonder. In the British Museum there is such a triptych (Flemish, I 511); the center panel, measuring an inch or two square, is crowded with figures in full relief and in diminishing perspective, after the custom of this period. This rests on a semicircular base which is carved with the Lord's Supper, and is further ornamented with figures and animals. The whole thing inclusive measures about 9 in. high, and, with the triptych wings open, 5 in. wide. The extraordinary delicacy and minuteness of detail of this microscopic work baffle description. There is another such a piece, also Flemish, in the Wallace collection, which rivals that just referred to in rni& applied talent. For, marvellous as these works of art are, they fail to satisfy. They make ones eyes ache, they worry one as to how the result could ever have been obtained, and after the first astonishment one must ever feel that the same work of art on a scale large enough for a cathedral could have been carved with half the labor.\n\nWith regard to paneling generally, there were, during the last fifty years of the period now under review, three styles of design followed by most European carvers, each of which attained great notoriety. Firstly, a developed form of small Panelling. tracery which was very common in France and the Netherlands. A square-headed panel would be filled in with small detail of flamboyant character, the perpendicular line or mullion being always subordinate, as in the German chasse (Muse Cluny), and in some cases absent, as the screen work of Évreux cathedral shows us. Secondly, the linenfold design. The great majority of examples are of a very conventional form, but at Bere Regis, Dorsetshire, the designs with tassels, and at St Sauvur, Caen, those with fringe work, readily justify the universal title applied to this very decorative treatment of large surfaces. At the beginning of the 16th century yet another pattern became the fashion. The main lines of the design consisted of flat hollow mouldings sometimes in the form of interlacing circles (Gatton, Surrey), at other times chiefly straight (Rochester cathedral), and the intervening spaces would be filled in with cusps or sprigs of foliage. It marks the last struggle of this great school of design to withstand the oncoming flood of the new art, the great Renaissance. From this time onward Gothic work, in spite of various attempts, has never again taken a place in domestic decoration. The lines of the tracery style, the pinnacle, and the crocket unequaled as they have always been in devotional expression are universally considered unsuited for decoration in the ordinary house.\n\nBut little reference can be made to the domestic side of the period which ended with the dawn of the 16th century, because so few remains exist. At Bayeux, Bourges, Reims and preeminently Rouen, we see by the figures of saints, bishops or virgins, how much the religious feeling of the Middle Ages entered into the domestic life. In England the carved corner post (which generally carried a bracket at the top to support the overhanging storey) calls for comment. In Ipswich, there are several such posts. On one house near the river, that celebrated subject, the fox preaching to geese, is carved in graphic allusion to the dissemination of false doctrine.\n\nOf mantelpieces, there is a good example in the Rouen Museum. The overhanging corners are supported by dragons and the plain mouldings have little bunches of foliage carved at either end, a custom as common in France during the 15th century as it was in England a century earlier; the screen. beam at Eastbourne parish church, for example.\n\nAs a rule, cabinets of the 15th century were rectangular in plan. In Germany and Austria the lower part was often enclosed, as well as the upper; the top, middle and lower rails being carved with geometrical design or with bands of foliage (Museum, Vienna). But it was also the custom to make these cupboards with the corners cut off, thus giving five sides to the piece of furniture. A very pretty instance, which is greatly enhanced by the metal work of the lock plates and hinges, is in the Muse Cluny, and there are other good specimens with the lower part open in the Victoria and Albert Museum, London.\n\nThe chest was a very important piece of furniture, and is often to be met with covered with the most elaborate carving (Orleans Museum). There is a splendid chest (14th century) in the Cluny Museum; the front is carved with twelve knights in armour standing under as many arches, and the spandrels are filled in with faces, dragons and so on. But it is to the 15th century that we look for the best work of this class; there is no finer example than that in the Kunstgewerbemuseum Berlin. The front is a very animated hunting scene most decoratively arranged in a scheme of foliage, and the top bears two coats of arms with helms, crests and mantling. But the more general custom in chest decoration was to employ tracery with or without figure work; Avignon Museum contains some typical examples of the latter class.\n\nA certain number of seats used for domestic purposes are of great interest. A good example of the long bench placed against the wall, with lofty panelled back and canopy over, is in the Musée Cluny, Paris. In the Museum at Rouen is a long seat of a movable kind with a low panelled back of pierced tracery, and in the Dijon Museum there is a good example of the typical chair of the period, with arms and high panelled and traceried back. There was a style of design admirably suited to the decoration of furniture when made of softwood such as pine. It somewhat resembled the excellent Scandinavian treatment of the 10th-12th centuries already referred to. A pattern of Gothic foliage, often of beautiful outline, would be simply grounded out to a shallow depth. The shadows, curves and twists only being emphasized by a few well-disposed cuts with a V tool; and of course the whole effect greatly improved by colour. A Swiss door of the 15th century in the Berlin Museum, and some German, Swiss and Tirolese work in the Victoria and Albert Museum, offer patterns that might well be imitated today by those who require simple decoration while avoiding the hackneyed Elizabethan forms.\n\nIt is hard to compare the figure work of England with that on the Continent owing to the disastrous effect of the Reformation. But when we examine the roofs of the Eastern counties, the bench ends of Somerset, or the misereres in many parts of the country, we can appreciate how largely wood sculpture was used for purposes of decoration. If as a rule the figure work was I not of a very high order, we have conspicuous exceptions in the stall elbows of Sherborne, and the pulpit of Trull, Somerset. Perhaps the oldest instance is the often mutilated and often restored effigy of Robert, Duke of Normandy, in Gloucester Cathedral (12th century), and carved, as was generally the case in England, in oak. At Clifton Reynes, Buckingham, there are two figures of the 13th century. They are both hollowed out from the back in order to facilitate seasoning the wood and to prevent cracking. During the 13th, 14th and 15th centuries there are numberless instances of figure carving of the most graphic description afforded in the misereres in many of our churches and cathedrals. But of figures carved in the round apart from their surroundings hardly an instance remains. At the little chapel of Cartmel Fell, in the wilds of Westmorland, there is a figure of Our Lord from a crucifix, some in length. The cross is gone, the arms are broken away, and the feet have been burned off. A second figure of Our Lord (originally in the church of Keynes Inferior) is in the museum of Caerleon, and a third, from a church in Lincolnshire, is now in a private collection. On the continent some of the finest figure work is to be found in the retables, some of which are in the Victoria and Albert Museum. A Tirolese panel of the 15th century carved in high relief, representing St John seated with his back to the onlooker, is a masterpiece of perspective and foreshortening, and the drapery folds are perfect. The same may be said of a small statue of the Virgin, carved in lime by a Swiss hand, and some work of the great Tilman Riemenschneider of Wurzburg (1460–1531) shows that stone sculptors of medieval times were not ashamed of wood.\n\nWith the beginning of the 16th century, the great Renaissance began to elbow its way in to the exclusion of Gothic design. But the process was not sudden, and much transition work has great merit. The rood screen at Hurst, Berkshire, the stall work of Cartmel Priory, Westmorland, and the bench ends of many of the churches in Somerset, give good illustrations. But the new style was unequal to the old in devotional feeling, except in classic buildings like St Paul's Cathedral, where the stalls of Grinling Gibbons better suit their own surroundings. The rest of this article will therefore be devoted in the main to domestic work, and the exact location of examples can only be given when not the property of private owners or where the public have access.\n\nDuring the 16th century the best work is undoubtedly to be found on the Continent. France, Germany and the Netherlands producing numberless examples not only of house decoration but of furniture as well. The wealth of the newly discovered American continent was only one factor which assisted in the civilizing influence of this time, and hand in hand with the spread of commerce came the desire for refinement. The custom of building houses chiefly in wood wherever timber was plentiful continued. Pilasters took the place of pinnacles, and vases or dolphins assisted the acanthus leaf to oust the older forms of design. House fronts of wood gave ample scope to the carver. That of Sir Paul Pinder (1600), formerly in Bishopsgate, but now preserved in the Victoria and Albert Museum, is a good example of decorative treatment without overloading. The brackets carved in the shape of monsters which support the projecting upper storey are typical of hundreds of dwellings, as for instance St Peters Hospital, Bristol. The panels, too, of Sir Paul Pinders house are good examples of that Jacobean form of medallion surrounded by scroll work which is at once as decorative as it is simple.\n\nIn England that familiar style known as Elizabethan and Jacobean prevailed throughout the 16th and 17th centuries. At the present time hardly a home in the land has not its old oak chest carved with the familiar half circle or scroll border along the top rail, or the arch pattern on the panels. The court cupboards, with their solid or open under parts and upper cornice supported by turned balusters of extravagant thickness, are to be seen wherever one goes. And chairs, real as well as spurious, with solid backs carved in the usual flat relief, are bought up with an avidity inseparable from fashion. Four-post bedsteads are harder to come by. The back is usually broken up into small panels and carved, the best effect being seen in those examples where the paneling or the framework only is decorated. The dining-hall tables often had six legs of great substance, which were turned somewhat after the shape of a covered cup, and were carved with foliage bearing a distant resemblance to the acanthus. Rooms were generally panelled with oak, sometimes divided at intervals by flat pilasters and the upper frieze carved with scroll work or dolphins. But the feature which distinguished the period was the fire mantle. It always must be the principal object in a room, and the Elizabethan carver fully appreciated this fact. By carving the chimney breast as a rule to the ceiling and covering the surrounding walls with more or less plain paneling, the designer, by thus concentrating the attention on one point, often produced results of a high order. Caryatid figures, pilasters and friezes were among the customary details employed to produce good effects. No finer example exists than that lately removed from the old palace at Bromley-by-Bow to the Victoria and Albert Museum. The mantelshelf is . from the ground and consists of a deep quadrant mould decorated with flat scroll work of good design. The supporting pilasters on either side are shaped and moulded in the customary Jacobean manner and are crowned by busts with Ionic capitals on the heads. Above the shelf the large center panel is deeply carved with the royal coat of arms with supporters and mantling, and on either side a semicircular arched niche contains a figure in classic dress. The Elizabethan carver often produced splendid staircases, sometimes carving the newel posts with heraldic figures bearing coats of arms, etc. The newels of a staircase at Highgate support different types of Cromwellian soldiers, carved with great vivacity and life. But in spite of excellent work, as for example the beautiful gallery at Hatfield, the carving of this period did not, so far as England was concerned, compare with other epochs, or with contemporary work in other parts of Europe. Much of the work is badly drawn and badly executed. It is true that good decorative effects were constantly obtained at the very minimum of cost, but it is difficult to discover much merit in work which really looks best when badly cut.\n\nIn France this flat and simple treatment was to a certain extent used. Doors were most suitably adorned in this way, and the split baluster so characteristic of Jacobean work is often to be met with. There are some very good cabinets in the museum at Lyngby, Denmark, illustrating these two methods of treatment in combination. But the Swiss and Austrians elaborated this style, greatly improving the effect by the addition of color. However, the best Continental designs adopted the typical acanthus foliage of Italy, while still retaining a certain amount of Gothic feeling in the strength of the lines and the cut of the detail. Panelling often long and narrow was commonly used for all sorts of domestic purposes, a feature being a medallion in the center with a simple arrangement of vase, dolphins, dragons, or birds and foliage filling in the spaces above and below.\n\nThe cabinets of the Netherlands and Belgium are excellent models of design. These pieces of furniture were usually arranged in two storeys with a fine moulded and carved cornice, mid division and plinth. The pilasters at the sides, and small raised panels carved only on the projecting part, would compose a very harmonious whole. A proportion of the French cabinets are decorated with caryatids not carved in the best taste, and, like other French woodwork of this period, are sometimes overloaded with sculpture. The doors of St Maclou, Rouen, fine as they are, would hardly to-day be held up as models for imitation. A noteworthy set of doors belong to the Oudenaarde Town Hall. The central door contains twelve and that on either side eight panels, each of which is carved with Renaissance foliage surrounding an unobtrusive figure. In the Palais de Justice we see that great scheme of decoration which takes up the whole of the fireplace end of the hall. Five large figures carved in the round are surrounded by small ones and with foliage and coats of arms.\n\nIn Italy, the birthplace of the Renaissance, there is much fine work of the 16th century. A very important school of design was promoted by Raphael, whose patterns were used or adapted by a large number of craftsmen. The shutters of Raphaels Stanze in the Vatican, and the choir stalls in the church of St Pietro de Cassinesi at Perugia, are among the most beautiful examples of this style of carving. The work is in slight relief, and carved in walnut with those graceful patterns which Raphael developed out of the newly discovered remains of ancient Roman wall painting from the palace of Nero and other places. In the Victoria and Albert Museum are many examples of Italian work: the door from a convent near Parma, with its three prominent masks and heavy gadroon moulds; a picture frame with a charming acanthus border and, egg and tongue moulds on either side; and various marriage chests in walnut covered with very elaborate schemes of carving. It is sometimes difficult to distinguish Spanish, or for that matter South of France work, from Italian, so much alike is the character. The Spaniards yield to none in good workmanship. Some Spanish panels of typical Italian design are in the Victoria and Albert Museum as well as cabinets of the purest Renaissance order. There is a wonderful Portuguese coffer (17th century) in this section. The top is deeply carved in little compartments with scenes from the life of Our Lord.\n\nIn England, the great school of Grinling Gibbons arose. Although he carved many beautiful mouldings of conventional form (Hampton Court Palace, Chatsworth, etc.), his name is usually associated with a very heavy form of decoration which was copied direct from nature. Great swags of drapery and foliage with fruit and dead birds, etc., would be carved in lime a foot thick. For technical skill these examples are unsurpassed; each grape would be undercut, the finer stalks and birds legs stand out quite separate, and as a consequence soon succumb to the energy of the housemaid's broom. Good work of this class is to be found at Petworth; Trinity College, Oxford; Trinity College, Cambridge; St Pauls cathedral; St James, Piccadilly; and many other London churches.\n\nDuring the reigns of Louis XIV. and XV. the principal merit of carved design, i.e. its appropriateness and suitability, gradually disappeared. Furniture was often carved in a way hardly legitimate. The legs, the rails of tables and chairs, the frames of cabinets, of looking-glasses, instead of being first made fcr construction and strength. and then decorated, were first designed to carry cherubs heads and rococo (i.e. rock and shell ornament), quite regardless of utility or convenience. A wealth of such mistaken design was also applied to state carriages, to say nothing of bedsteads and other furniture. However, the wall paneling of the mansions of the rich, and sometimes the paneling of furniture, was decorated with rococo design in its least illegitimate form. The main part of the wood surface would be left plain, while the center would be carved with a medallion surrounded by foliage, vases or trophies of torches and musical instruments, etc., or perhaps the upper part of the panel would be thus treated. France led the fashion, which was more or less followed all over Europe. In England gilt chairs in the style of Louis XV. were made in some quantities. But Thomas Chippendale, Ince and Mayhew, Sheraton, Johnson, Heppelwhite and other cabinet-makers did not as a rule use much carving in their designs. Scrolls, shells, ribbon, ears of corn, etc., in very fine relief, were, however, used in the embellishment of chairs, etc., and the claw and ball foot was employed as a termination to the cabriole legs of cabinets and other furniture.\n\nThe mantelpieces of the 18th century were, as a rule, carved in pine and painted white. Usually the shelves were narrow and supported by pilasters often of flat elliptic plan, sometimes by caryatids, and the frieze would consist of a raised center panel carved with a classic scene in relief, or with a mask alone, and on either side a swag of flowers, fruit and foliage.\nInterior doorways were often decorated with a broken pediment more or less ornate, and a swag of foliage commonly depended from either side over a background of scroll work. The outside porches so often seen in Queen Anne houses were of a character peculiar to the 18th century. A small platform or curved roof was supported by two large and heavy brackets carved with acanthus scroll work. The staircases were as a rule exceedingly good. Carved and pierced brackets were fixed to the open strings (i.e. the sides of the steps), giving a very pretty effect to the graceful balustrade of turned and twisted columns.\n\nRenaissance figure work calls for little comment. During the 16th century many good examples were produced those priestly statues in the museum of Sens for example. But the figure work used in the decoration of cabinets, etc., seldom rose above the ordinary level. In the 18th century cherubs heads were fashionable and statuettes were sometimes carved in boxwood as ornaments, but as a means of decorating houses wood sculpture ceased to be. The Swiss, however, have kept up their reputation for animal sculpture to the present day, and still turn out cleverly carved chamois and bears, etc.; as a rule the more sketchily cut the better the merit. Their more ambitious works, their groups of cows, etc., sometimes reach a high level of excellence.\n\nBetween the 17th and 18th century a florid woodcarving industry started in the Gardena valley, which is now located in the Italian province of South Tyrol. A network of people from that valley traveled on foot to all European cities, as far as to Lisbon and Saint Petersburg, to sell the products of hundreds of carvers. Finally in the 19th century in Gardena, mainly wooden toys and dolls known also as Dutch dolls or penny dolls, were carved by the millions of pieces. The Museum Gherdëina in Urtijëi displays a large collection of examples of woodcarcarvings from that region.\n\nGilded woodcarving in Portugal and Spain continued to be produced, and the style exported to their New World colonies, and the Philippines, Macao and Goa.\n\nOf the work of the 19th century onward little can be said in praise. Outside and beyond the present-day fashion for collecting old oak there seems to be no demand for carved decoration. In church work a certain number of carvers find occupation, as also for repairs or the production of imitations. But the carving one is accustomed to see in hotels or on board the modern ocean palace is in the main the work of the machine, often with finishing work done by human workers.\n\nNonetheless, the 1800s saw the teaching of woodcarving became formalized in several European countries. For example, the Austrian woodcarver Josef Moriggl (1841–1908) had a long career as a teacher, culminating in his appointment in 1893 as Professor at the Staats-Gewerbeschule (Craft School) in Innsbruck, where he served until his retirement in 1907.\n\nIn Gröden the institution of an art school in 1820 improved considerably the skills of the carvers. A new industrial branch developed with hundreds of artists and artisans dedicated to sculpture and manufacturing of statues and altars in wood exported to the whole world. Unfortunately the machine-carving industry, initiated in the 1950s and the Second Vatican Council, caused hundreds of carvers in Val Gardena to quit their craft. A worldwide trade of machine-carved figuerines and statues ensued.\n\nIn the early medieval period screens and other fittings were produced for the Coptic churches of Egypt by native Christian workmen. In the British Museum there is a set of ten small cedar panels from the church door of Sitt Miriam, Cairo (13th century). The six sculptured figure panels are carved in very low relief and the four foliage panels are quite Oriental in character, intricate and fine both in detail and furnish. In the Cairo Museum there is much work treated, after the familiar Arab style, while other designs are quite Byzantine in character. The figure work is not of a very high order.\n\nMuslim wood-carvers of Persia, Syria, Egypt and Spain are renowned for their skill, designed and executed the richest paneling and other decorations for wall linings, ceilings, pulpits and all kinds of fittings and furniture. The mosques and private houses of Cairo, Damascus and other Oriental Cities are full of the most elaborate and minutely delicate woodwork. A favorite style of ornament was to cover the surface with very intricate interlacing patterns, formed by finely moulded ribs; the various geometrical spaces between the ribs were then filled in with small pieces of wood carved with foliage in slight relief. The use of different woods such as ebony or box, inlaid so as to emphasize the design, combined with the ingenious richness of the patterns, give this class of woodwork an almost unrivaled splendour of effect. Carved ivory is also often used for the filling in of the spaces. The Arabs are past masters in the art of carving flat surfaces in this way. A gate in the mosque of the sultan Bargoug (Cairo, 14th century) well illustrates this appreciation of lines and surfaces. The pulpit or mimbar (15th century) from a Cairo mosque, now in the Victoria and Albert Museum, is also a good example in the same style, the small spaces in this case being filled in with ivory carved in flat relief.\n\nScreens made up of labyrinths of complicated joinery, consisting of multitudes of tiny balusters connecting hexagons, squares or other forms, with the flat surfaces constantly enriched with small carvings, are familiar to every one. In Cairo we also have examples in the mosque of Qous (12th century) of that finely arranged geometrical interlacing of curves with foliage terminations which distinguishes the Saracenic designer. Six panels in the Victoria and Albert Museum (13th century), and work on the tomb of the sultan Li Ghoury (16th century), show how deeply this form of decoration was ingrained in the Arab nature. Figure work and animals were sometimes introduced, in medieval fashion, as in the six panels just referred to, and at the hflpital du Moristan (13th century) and the mosque of El Nesfy Qeycoun (14th century). There is a magnificent panel on the door of Beyt-el-Emyr. This exquisite design is composed of vine leaves and, grapes of conventional treatment in low relief. The Arab designer was fond of breaking up his paneling in a way reminding one of a similar Jacobean custom. The main panel would be divided into a number of hexagonal, triangular or other shapes, and each small space filled in with conventional scroll work. Much of this simple flat design reminds one of that Byzantine method from which the Elizabethan carvers were inspired.\n\nThe Persian carvers closely followed Arab design. A pair of doors of the 14th century from Samarkand (Victoria and Albert Museum) are typical. Boxes, spoons and other small articles were often fretted with interlacing lines of Saracenic character, the delicacy and minuteness of the work requiring the utmost patience and skill. Many of the patterns remind one, of the sandalwood work of Madras, with the difference that the Persians v~ere satisfied with a much lower relief. Sometimes a very beautiful result was obtained by the sparing tise of fretted lattice pattern among foliage. A fine panel of the 14th century in the Victoria and Albert Museum shows how active was Arab influence even as far as Bokhara.\n\nThroughout the great Indian peninsula woodcarving of the most luxurious kind has been continuously produced for many centuries. The ancient Hindu temples were decorated with doors, ceilings and various fittings carved in teak and other woods with patterns of extreme richness and minute elaboration. The doors of the temple of Somnath, on the north-west coast, were famed for their magnificence and were highly valued as sacred relics. In 1024 they were taken to Ghazni by the Moslem conqueror, Sultan Mahmud, and are now lying at the fort at Agra. The gates which now exist are very fine specimens of ancient woodcarving, but are likely copies of the original, likely ancient, doors. Many doors, columns, galleries or even entire house-fronts are covered with the most intricate design bewildering to behold (Bhera, Shahpur). But this is not always the case, and the Oriental is at times more restrained in his methods. Architectural detail is to be seen with enrichment carved round the framing. Hindu treatment of the circle is often exceedingly good, and might perhaps less rarely inspire western design. Foliage, fruit and flowers are constantly adapted to a scheme of fret-cut decoration for doors or windows as well as the frames of chairs and the edges of tables. Southern Indian wood carvers are known to work often with sandalwood, always covered with design, where scenes or characters from Hindu mythology occupy space. Many of the gong stands of Burma show the highest skill; the arrangement of two figures bearing a pole from which a gong hangs is familiar.\n\nIn these countries the carver is unrivaled for deftness of hand. Grotesque and imitative work of the utmost perfection is produced, and many of the carvings of these countries, Japan in particular, are beautiful works of art, especially when the carver copies the lotus, lily or other aquatic plant. A favorite form of decoration consists of breaking up the architectural surfaces, such as ceilings, friezes, and columns, into framed squares and filling each panel with a circle, or diamond of conventional treatment with a spandrels in each corner. A very Chinese feature is the finial of the newel post, so constantly left more or less straight in profile and deeply carved with monsters and scrolls. A heavily enriched moulding bearing a strong resemblance to the gadroon pattern is commonly used to give emphasis to edges, and the dragon arranged in curves imitative of nature is frequently employed over a closely designed and subordinated background.\nThe general rule that in every country designers use much the same means whereby a pattern is obtained holds good in China. There are forms of band decoration here which closely resemble those of Gothic Europe, and a chair from Turkestan (3rd century) might almost be Elizabethan, so like are the details. Screens of grill form, often found in the historically Islamic countries, are common, and the deeply grounded, closely arranged patterns of Bombay also have their counterparts. The imperial dais in the Chien-Ching Hall, Pekin, is a masterpiece of intricate design. The back consists of one central panel of considerable height, with two of lesser degree on either side luxuriously carved. The whole is crowned with a very heavy crest of dragons and scroll work; the throne also is a wonderful example of carved treatment, and the doors of a cabinet in the same building show how rich an effect of foliage can be produced without the employment of stalk or scroll. One might almost say, he wastes his talent on such an ungrateful material as wood. In this material fans and other trifles are carved with a delicacy that courts disaster.\n\nIn Japan much of the Chinese type is apparent. The native carver is fond of massing foliage without the stalk to lead him. He appears to put in his foliage, fruit and flowers first and then to indicate a stalk here and there, thus reversing the order of the Western method. Such a treatment, especially when birds and beasts are introduced, has the highest decorative effect. But, as such close treatment is bound to do, it depends for success to some extent upon its scheme of color. A long panel in the Victoria and Albert Museum, depicting merchants with their packhorse, strongly resembles in its grouping and treatment Gothic work of the 15th century, as for example the panel of St Hubert in the museum at Chftlons. The strength and character of Japanese figure work is quite equal to the best Gothic sculpture of the 15th century.\n\nThere is a general similarity running through the carved design of most races of primitive culture, the chip form of ornament being almost universally employed. Decorated surfaces depending almost entirely upon the incised line also obtain all over the world, and may no doubt be accounted for by the extensive use of stone cutting tools. The carver shows the same tendency to over-exalt his art by crowding on too much design as the more civilized craftsman of other lands, while he also on occasion exercises a good deal of restraint by a harmonious balance of decoration and plain space. So far as his chip designs and those patterns more or less depending on the line are concerned, his work as a rule is good and suitable, but when he takes to figure work his attempts do not usually meet with success. Primitive carving, generally, shows that very similar stages of artistic development are passed through by men of every age and race.\n\nA very favorite style of chip pattern is that formed by small triangles and squares entirely covering a surface in the Cook Islands, the monotony being sometimes varied by a band of different arrangement in the middle of the article or at the top or bottom. So far as the cultivation of patience and accuracy is concerned, has no equal. The Fiji Islanders, employ chip designs rivaling those of Europe in variety. Upon occasion the aboriginal Marquesas carver appreciates the way in which plain surfaces contrast and emphasize decorated parts, and judiciously restricts his skill to bands of decoration or to special points. The Ijos of the lower Niger design their paddles in a masterly way, and show a fine sense of proportion between the plain and the decorated surface. Their designs, though slightly in relief, are of the chip nature. The method of decorating a subject with groups of incised lines, straight or curved, though often very effective and in every way suitable, is not a very advanced form of art and has decided limits. The natives of the Congo, now two nations, covered by the landmass of the Republic of the Congo and Democratic Republic of the Congo does good work of this kind.\n\nCarving in relief is common enough, idols being produced in many forms. The South African carves the handle of his spoon perhaps in the form of a giraffe, and in the round, with each leg cut separately and the four hoofs meeting at the bowl, hardly a comfortable form of handle to hold. The North American Indian shows a wider invention than some nations, the twist in various shapes being a favorite treatment say of pipe stems. The Papuan has quite a style of his own; he uses a scroll of the form familiar in Indian shawls, and in some cases the scroll entwines in a way which faintly suggests the guilloche. The native of New Guinea also employs the scroll for a motive, the flat treatment of which reminds one of a similar method in use in Scandinavian countries. The work of the New Zealander is greatly in advance of the average primitive type; he uses a very good scheme of scroll work for decorative purposes, the lines of the scrolls often being enriched with a small pattern in a way reminding one of the familiar Norman treatment, as for example the prows of his canoes. The Maori wood carver sometimes carves not only the barge boards of his house but the gables also, reptilian and grotesque figures being as a rule introduced; the main posts and rafters, too, of the inside receive attention. Unlike the Hindu he has a good idea of decorative proportion, and does not plan his scheme of design on too small a scale.\n"}
{"id": "24590788", "url": "https://en.wikipedia.org/wiki?curid=24590788", "title": "Hot-point probe", "text": "Hot-point probe\n\nA hot point probe is a method of determining quickly whether a semiconductor sample is n (negative) type or p (positive) type. A voltmeter or ammeter is attached to the sample, and a heat source, such as a soldering iron, is placed on one of the leads. The heat source will cause charge carriers (electrons in an n-type, electron holes in a p-type) to move away from the lead. The heat from the probe creates an increased number of higher energy carriers which then diffuse away from the contact point. This will cause a current/voltage difference. For example, if the heat source is placed on the positive lead of a voltmeter attached to an n-type semiconductor, a positive voltage reading will result as the area around the heat source/positive lead becomes positively charged.\n"}
{"id": "1505382", "url": "https://en.wikipedia.org/wiki?curid=1505382", "title": "Hybrid computer", "text": "Hybrid computer\n\nHybrid computers are computers that exhibit features of analog computers and digital computers. The digital component normally serves as the controller and provides logical and numerical operations, while the analog component often serves as a solver of differential equations and other mathematically complex equations. The first desktop hybrid computing system was the Hycomp 250, released by Packard Bell in 1961. Another early example was the HYDAC 2400, an integrated hybrid computer released by EAI in 1963. Late in the 20th century, hybrids dwindled with the increasing capabilities of digital computers including digital signal processors.\n\nIn general, analog computers are extraordinarily fast, since they are able to solve most mathematically complex equations at the rate at which a signal traverses the circuit, which is generally an appreciable fraction of the speed of light. On the other hand, the precision of analog computers is not good; they are limited to three, or at most, four digits of precision.\n\nDigital computers can be built to take the solution of equations to almost unlimited precision, but quite slowly compared to analog computers. Generally, complex mathematical equations are approximated using iterative methods which take huge numbers of iterations, depending on how good the initial \"guess\" at the final value is and how much precision is desired. (This initial guess is known as the numerical \"seed\".) For many real-time operations in the 20th century, such \"digital\" calculations were too slow to be of much use (e.g., for very high frequency phased array radars or for weather calculations), but the precision of an \"analog\" computer is insufficient.\n\nHybrid computers can be used to obtain a very good but relatively imprecise 'seed' value, using an analog computer front-end, which is then fed into a digital computer iterative process to achieve the final desired degree of precision. With a three or four digit, highly accurate numerical seed, the total digital computation time to reach the desired precision is dramatically reduced, since many fewer iterations are required. One of the main technical problems to be overcome in hybrid computers is minimizing digital-computer noise in analog computing elements and grounding systems.\n\nConsider that the nervous system in animals is a form of hybrid computer. Signals pass across the synapses from one nerve cell to the next as discrete (digital) packets of chemicals, which are then summed within the nerve cell in an analog fashion by building an electro-chemical potential until its threshold is reached, whereupon it discharges and sends out a series of digital packets to the next nerve cell. The advantages are at least threefold: noise within the system is minimized (and tends not to be additive), no common grounding system is required, and there is minimal degradation of the signal even if there are substantial differences in activity of the cells along a path (only the signal delays tend to vary). The individual nerve cells are analogous to analog computers; the synapses are analogous to digital computers.\n\nHybrid computers should be distinguished from hybrid systems. The latter may be no more than a digital computer equipped with an analog-to-digital converter at the input and/or a digital-to-analog converter at the output, to convert analog signals for ordinary digital signal processing, \"and conversely\", e.g., for driving physical control systems, such as servomechanisms.\n\nRecently in 2015, researchers at Columbia University published a paper on a small scale hybrid computer in 65 nm CMOS technology. This 4th-order VLSI hybrid computer contains 4 integrator blocks, 8 multiplier/gain-setting blocks, 8 fanout blocks for distributing current-mode signals, 2 ADCs, 2 DACs and 2 SRAMs blocks. Digital controllers are also implemented on the chip for executing the external instructions. A robot experiment in the paper demonstrates the use of the hybrid computing chip in today's emerging low-power embedded applications.\n\n"}
{"id": "7333367", "url": "https://en.wikipedia.org/wiki?curid=7333367", "title": "Industrial control system", "text": "Industrial control system\n\nIndustrial control system (ICS) is a general term that encompasses several types of control systems and associated instrumentation used for industrial process control.\n\nSuch systems can range from a few modular panel-mounted controllers to large interconnected and interactive distributed control systems with many thousands of field connections. All systems receive data received from remote sensors measuring process variables (PVs), compare these with desired set points (SPs) and derive command functions which are used to control a process through the final control elements (FCEs), such as control valves.\n\nThe larger systems are usually implemented by Supervisory Control and Data Acquisition (SCADA) systems, or distributed control systems (DCS), and programmable logic controllers (PLCs), though SCADA and PLC systems are scalable down to small systems with few control loops. Such systems are extensively used in industries such as chemical processing, pulp and paper manufacture, power generation, oil and gas processing and telecommunications.\n\nThe simplest control systems are based around small discrete controllers with a single control loop each. These are usually panel mounted which allows direct viewing of the front panel and provides means of manual intervention by the operator, either to manually control the process or to change control setpoints. Originally these would be pneumatic controllers, a few of which are still in use, but nearly all are now electronic.\n\nQuite complex systems can be created with networks of these controllers communicating using industry standard protocols. Networking allow the use of local or remote SCADA operator interfaces, and enables the cascading and interlocking of controllers. However, as the number of control loops increase for a system design there is a point where the use of a programmable logic controller (PLC) or distributed control system (DCS) is more manageable or cost-effective.\n\nA distributed control system (DCS) is a digital processor control system for a process or plant, wherein controller functions and field connection modules are distributed throughout the system. As the number of control loops grows, DCS becomes more cost effective than discrete controllers. Additionally a DCS provides supervisory viewing and management over large industrial processes. In a DCS, a hierarchy of controllers is connected by communication networks, allowing centralised control rooms and local on-plant monitoring and control.\n\nA DCS enables easy configuration of plant controls such as cascaded loops and interlocks, and easy interfacing with other computer systems such as production control. It also enables more sophisticated alarm handling, introduces automatic event logging, removes the need for physical records such as chart recorders and allows the control equipment to be networked and thereby located locally to equipment being controlled to reduce cabling.\n\nA DCS typically uses custom-designed processors as controllers, and uses either proprietary interconnections or standard protocols for communication. Input and output modules form the peripheral components of the system.\n\nThe processors receive information from input modules, process the information and decide control actions to be performed by the output modules. The input modules receive information from sensing instruments in the process (or field) and the output modules transmit instructions to the final control elements, such as control valves.\n\nThe field inputs and outputs can either be continuously changing analog signals e.g. current loop or 2 state signals that switch either \"on\" or \"off\", such as relay contacts or a semiconductor switch.\n\nDistributed control systems can normally also support Foundation Fieldbus, PROFIBUS, HART, Modbus and other digital communication buses that carry not only input and output signals but also advanced messages such as error diagnostics and status signals.\n\nSupervisory control and data acquisition (SCADA) is a control system architecture that uses computers, networked data communications and graphical user interfaces for high-level process supervisory management. The operator interfaces which enable monitoring and the issuing of process commands, such as controller set point changes, are handled through the SCADA supervisory computer system. However, the real-time control logic or controller calculations are performed by networked modules which connect to other peripheral devices such as programmable logic controllers and discrete PID controllers which interface to the process plant or machinery.\n\nThe SCADA concept was developed as a universal means of remote access to a variety of local control modules, which could be from different manufacturers allowing access through standard automation protocols. In practice, large SCADA systems have grown to become very similar to distributed control systems in function, but using multiple means of interfacing with the plant. They can control large-scale processes that can include multiple sites, and work over large distances. This is a commonly-used architecture industrial control systems, however there are concerns about SCADA systems being vulnerable to cyberwarfare or cyberterrorism attacks.\n\nThe SCADA software operates on a supervisory level as control actions are performed automatically by RTUs or PLCs. SCADA control functions are usually restricted to basic overriding or supervisory level intervention. A feedback control loop is directly controlled by the RTU or PLC, but the SCADA software monitors the overall performance of the loop. For example, a PLC may control the flow of cooling water through part of an industrial process to a set point level, but the SCADA system software will allow operators to change the set points for the flow. The SCADA also enables alarm conditions, such as loss of flow or high temperature, to be displayed and recorded.\n\nPLCs can range from small modular devices with tens of inputs and outputs (I/O) in a housing integral with the processor, to large rack-mounted modular devices with a count of thousands of I/O, and which are often networked to other PLC and SCADA systems. They can be designed for multiple arrangements of digital and analog inputs and outputs, extended temperature ranges, immunity to electrical noise, and resistance to vibration and impact. Programs to control machine operation are typically stored in battery-backed-up or non-volatile memory.\n\nProcess control of large industrial plants has evolved through many stages. Initially, control would be from panels local to the process plant. However this required a large manpower resource to attend to these dispersed panels, and there was no overall view of the process. The next logical development was the transmission of all plant measurements to a permanently-manned central control room. Effectively this was the centralisation of all the localised panels, with the advantages of lower manning levels and easier overview of the process. Often the controllers were behind the control room panels, and all automatic and manual control outputs were individually transmitted back to plant in the form of pneumatic or electrical signals.\n\nHowever, whilst providing a central control focus, this arrangement was inflexible as each control loop had its own controller hardware so system changes required reconfiguration of signals by re-piping or re-wiring. It also required continual operator movement within a large control room in order to monitor the whole process. With the coming of electronic processors, high speed electronic signalling networks and electronic graphic displays it became possible to replace these discrete controllers with computer-based algorithms, hosted on a network of input/output racks with their own control processors. These could be distributed around the plant and would communicate with the graphic displays in the control room. The concept of \"distributed control\" was realised.\n\nThe introduction of distributed control allowed flexible interconnection and re-configuration of plant controls such as cascaded loops and interlocks, and easy interfacing with other production computer systems. It enabled sophisticated alarm handling, introduced automatic event logging, removed the need for physical records such as chart recorders, allowed the control racks to be networked and thereby located locally to plant to reduce cabling runs, and provided high level overviews of plant status and production levels. For large control systems, the general commercial name \"Distributed Control System\" (DCS) was coined to refer to proprietary modular systems from many manufacturers which had high speed networking and a full suite of displays and control racks which all seamlessly integrated.\n\nWhilst the DCS was tailored to meet the needs of large industrial continuous processes, in industries where combinatoric and sequential logic was the primary requirement, the PLC (programmable logic controller) evolved out of a need to replace racks of relays and timers used for event-driven control. The old controls were difficult to re-configure and fault-find, and PLC control enabled networking of signals to a central control area with electronic displays. PLC were first developed for the automotive industry on vehicle production lines, where sequential logic was becoming very complex. It was soon adopted in a large number of other event-driven applications as varied as printing presses and water treatment plants.\n\nSCADA's history is rooted in distribution applications, such as power, natural gas, and water pipelines, where there is a need to gather remote data through potentially unreliable or intermittent low-bandwidth and high-latency links. SCADA systems use open-loop control with sites that are widely separated geographically. A SCADA system uses RTUs (remote terminal units, also referred to as remote telemetry units) to send supervisory data back to a control center. Most RTU systems always did have some limited capacity to handle local controls while the master station is not available. However, over the years RTU systems have grown more and more capable of handling local controls.\n\nThe boundaries between DCS and SCADA/PLC systems are blurring as time goes on. The technical limits that drove the designs of these various systems are no longer as much of an issue. Many PLC platforms can now perform quite well as a small DCS, using remote I/O and are sufficiently reliable that some SCADA systems actually manage closed loop control over long distances. With the increasing speed of today's processors, many DCS products have a full line of PLC-like subsystems that weren't offered when they were initially developed.\n\nThis led to the concept and realisation of a PAC - programmable automation controller - which is programmed in a modern programming language such as C or C++, - that is an amalgamation of these three concepts.\n\n\n\n"}
{"id": "18782488", "url": "https://en.wikipedia.org/wiki?curid=18782488", "title": "Institute of IT Professionals", "text": "Institute of IT Professionals\n\nThe Institute of IT Professionals (IITP) is a non-profit incorporated society in New Zealand. As New Zealand's ICT professional body, the IITP exists to promote education and ensure a high level of professional practice amongst ICT professionals. Before July 2012, IITP was known as the New Zealand Computer Society Inc (NZCS).\n\nThe objects of the Institute of IT Professionals, as provided in the Institute's constitution, are to:\n\n\nAll IITP members must formally agree to a Code of Ethics.\n\nThe IITP Code of Ethics is mostly concerned with non-discrimination, zeal, community, skills, competence, continuous development, consequences, and conflicts of interest, and contains the following 8 tenets:\n\n\nThe IITP has an estimated membership of approximately 3,500 individual members, plus around 120 Corporate Partners (businesses who have joined on behalf of their staff) resulting in an estimated representation of over 10,000 ICT professionals.\n\nIITP provides for multiple membership levels depending on a member's stage of career and requirements.\n\nProfessional membership is for those in the ICT profession who meet certain requirements in terms of experience and qualifications. \n\n\n\n\nThe Institute of IT Professionals is a single nationwide non-profit incorporated society.\n\nWithin the Institute are five branches based on geographic location, being Auckland, Wellington, Canterbury, Waikato/Bay of Plenty, and Otago/Southland. The IITP also encompasses a number of Specialist Groups in topics such as Software Testing and Computer Security. IITP branches and specialist groups are staffed by volunteers.\n\nThe Institute is governed by a National Council made up of the IITP President, Deputy President, and five Councillors, with each councillor being appointed by one of the branches of the Institute.\n\nThe Institute maintains a fully staffed operational head office in Wellington and is managed by a Chief Executive who also sits on Council in a non-voting capacity.\n\nIITP is regarded as the voice of the ICT profession in New Zealand and undertakes significant advocacy on behalf of the profession and wider sector.\n\nIITP is represented on most ICT-related advisory groups, panels and public ICT-related boards in New Zealand, and was a founding member of the Digital Development Council, a body set up by the New Zealand Government to help achieve New Zealand's digital potential.\n\nThe Institute is engaged with government (both ministerial and official level), industry and academia and works as a catalyst and conduit for these three important sub-sectors to work together in the interests of the overall ICT Sector, both in the area of ethics and professional practice as well as to solve issues such as the current ICT skills shortage and drop in tertiary ICT enrolments.\n\nIITP also takes an active interest in educational issues and in 2008 completed a detailed analysis of ICT-related NCEA \"Achievement Standards\" in secondary schools and outlined a number of significant and serious problems with these standards.\n\nThe Institute also promotes digital literacy.\n\nIn 2009 the Institute released an internationally aligned ICT professional certification in New Zealand, the Information Technology Certified Professional (ITCP) qualification.\n\nThe IITP runs numerous events throughout New Zealand, but predominantly in Auckland, Wellington, Christchurch, Hamilton and Dunedin.\n\nAs well as around 20 local events a month, the Institute began a monthly nationwide Innovators of ICT event series in August 2008, taking notable and successful entrepreneurs such as Rod Drury and Don Christie on a speaking trip to the five cities above to promote innovation and \"thinking outside the square\" to New Zealand's development and ICT community.\n\nThe Institute was founded as the New Zealand Data Processing and Computer Society Inc in October 1960 in Wellington, New Zealand and changed its name to New Zealand Computer Society Inc in 1967.\n\nThe IITP occasionally confers the title of Honorary Fellow of the IITP (HFIITP) on an individual who has made a significant contribution to the ICT sector in New Zealand over a period of time, or the Institute itself over many years.\n\nHFIITP recipients include former Minister of ICT Hon David Cunliffe and ICT entrepreneur Rod Drury. There are currently 25 Honorary Fellows.\n\nThe IITP is a full member of the International Federation for Information Processing (IFIP), an international umbrella organisation originally set up by UNESCO, and South East Asia Regional Computer Confederation (SEARCC).\n\nThe Institute also works with other professional bodies around the world, such as the Australian Computer Society and the British Computer Society.\n\n\n"}
{"id": "17599785", "url": "https://en.wikipedia.org/wiki?curid=17599785", "title": "Inte:Ligand", "text": "Inte:Ligand\n\nInte:Ligand Software Entwicklungs und Consulting GmbH supports scientists worldwide with innovative approaches for early drug discovery research by developing and applying computer-aided design (CAD) solutions. Molecular designers including chemists and modelers in the pharmaceutical, cosmetic, nutrition, animal health and crop protection life science industries use the solutions to design and identify novel bioactive molecules. Inte:Ligand develops the most innovative and user friendly modeling software platforms and provides expert consulting to inspire the innovative process of designing molecules, filtering ideas, and de-risking candidates assessed in early discovery projects.\n\nInte:Ligand was founded in Maria Enzersdorf, Lower Austria (Niederösterreich) by Prof. Thierry Langer, Gerhard Wolber and Prof. Hermann Stuppner in 2003. They established the company headquarters on Mariahilferstrasse in Vienna, Austria that same year. Sharon D Bryant joined the team in 2008 to leverage the company's core competencies worldwide.\n\nIn 2007 Inte:Ligand was the recipient of the NÖ Innovation prize (Innovationpreis) for the development of the simulation software LigandScout. As of 2017 there were more than 1500 literature, book chapters and review articles published related to InteLigand software technology in the areas of virtual screening, 3D-pharmacophore modeling, hit identification, medicinal chemistry decision support, activity profiling, docking, fragment-based compound design, protein-protein-interactions, drug-repurposing and molecular dynamics simulations.\n\nOther applications include the discovery of new Myeloperoxidase ligands, HIV reverse transcriptase inhibitors, applications in anti-viral bio-activity profiling, the development of models to predict HIV Protease activity, Cytochrome P450 activity prediction\nand simulation models for the activity on Factor Xa.\n\n\nOther companies and institutions providing drug discovery software:\n\n"}
{"id": "2166948", "url": "https://en.wikipedia.org/wiki?curid=2166948", "title": "Integrated access device", "text": "Integrated access device\n\nAn Integrated Access Device (or IAD) is a customer premises device that provides access to wide area networks and the Internet. Specifically, it aggregates multiple channels of information including voice and data across a single shared access link to a carrier or service provider PoP (Point of Presence). The access link may be a T1 line, a DSL connection, a cable (CATV) network, a broadband wireless link, or a metro-Ethernet connection.\n\nAt the PoP, the customer's aggregated information is typically directed into an Add-drop multiplexer or an MSPP (multiservice provisioning platform), which are complex and expensive devices that sit between customers and the core network. They manage traffic streams coming from customers and forward those streams to the PSTN (voice) or appropriate wide area networks (ATM, frame relay, or the Internet).\n\nAn IAD is sometimes installed by the service provider to which a customer wishes to connect. This allows the service provider to control the features of the access link and manage its operation during use. Competitive service providers are now offering access services over a variety of access technologies, including wireless optical (i.e., Terabeam) and metro-Ethernet networks. Old telco protocols and transport methods (T1 lines and time-division multiplexing) are replaced with access methods that are appropriate for the underlying transport. Because of this, the provider will usually specify an appropriate IAD or install an IAD.\n\nSIAD will aggregate its IP data traffic and GSM ATM traffic at the cellsite passing it along to the multi-service routers sitting in front of mobile switching center (MSC). \nIt will aggregate the cell site traffic and forward to the MSN.\n\nAn Integrated Access Device (or IAD) allows customers to continue using their legacy TDM voice equipment (PBX, Channel Bank, etc...) without the need to invest in new Voice over Internet Protocol (VoIP) capable equipment. This also allows customers in locations where legacy TDM services are no longer available to delay expensive equipment upgrades required to move to a pure Voice over IP (VoIP) solution. \n\nThere are other cost saving benefits of moving to an Integrated Access Device (or IAD) since a Primary Rate Interface (PRI) which is typically required for Private Branch Exchange (PBX) equipment requires 23 voice (bearer) channels and 1 signaling (data) channel. By purchasing an IAD instead and connecting to a VoIP service provider, customers can purchase only the number of voice channels that are actually required at their location. Since the VoIP service allows internet access to be used for both voice and data, moving to a VoIP solution allows customers to remove the local access charges associated with the T1 local loop that was required to deliver the legacy PRI service. This allows customers to realize a significant cost savings by removing unused voice channels and access costs. Some companies estimate a 30%–75% cost savings can be realized by moving to VoIP. \n\nAnother benefit in moving to a VoIP solution over TDM is that additional security can be added to the voice service since MultiProtocol Label Switching (MPLS or Virtual Private Network (VPN) connections can be used for voice access. This is especially important in the legal and healthcare industry where confidential information is routinely shared during phone calls. \n\nThe only caveats to sharing Voice and data on the same connection are that proper Quality of Service (QoS) must be added to the circuit to ensure that the voice traffic takes priority over the data traffic and that enough bandwidth is available for the number of sessions (AKA concurrent call paths, channels, etc..) of voice service that are needed at the site. A good rule of thumb (and to make for easy math) for a home phone quality (G.711) VoIP codec is to assume 100 kbit/s per session multiplied by the number of sessions required at the location. For example 10 sessions of VoIP would require 1 Mbit/s of available bandwidth and 1 Mbit/s of premium QOS to ensure a quality call experience.\n"}
{"id": "3011783", "url": "https://en.wikipedia.org/wiki?curid=3011783", "title": "Internationale Funkausstellung Berlin", "text": "Internationale Funkausstellung Berlin\n\nThe IFA or Internationale Funkausstellung Berlin (International radio exhibition Berlin, a.k.a. 'Berlin Radio Show') is one of the oldest industrial exhibitions in Germany. Between 1924 and 1939 it was an annual event, but as from 1950 it was organized on a two yearly basis until 2005. Since then it has become an annual event again, held in September. Today it is one of world's leading trade shows for consumer electronics and home appliances.\n\nIt offers the opportunity to exhibitors to present their latest products and developments to the general public. As a result of daily reporting in almost all the German media, the radio exhibition achieves a large spreading of the information – and advertising messages and has also become international. In the course of its history, a large number of world innovations were first seen at the exhibition.\n\nGerman physicist and inventor Manfred von Ardenne gave a public demonstration of a television system using a cathode ray tube for both transmission (using flying-spot image scans, not a camera) and reception, at the 1931 show.\n\nIn 1933 The Volksempfänger (VE 301 W), a Nazi-sponsored radio receiver design, was introduced. Ordered by Dr. Joseph Goebbels, designed by Otto Griessing, sold by Gustav Seibt, it was presented at the tenth Berliner Funkausstellung on 18 August 1933, its price fixed at 76 Reichsmark (RM). 100,000 units were sold during the exhibition.\n\nIn 1938 the DKE 38 (\"Deutscher Kleinempfänger 38\", i.e. German miniature receiver 1938) followed, the price fixed at 35 RM.\n\nAEG, founded in 1883 by Emil Rathenau, showed the first practical audio tape recorder, the Magnetophon K1, at the August 1935 show.\n\nIn 1939 the exhibition was called \"Grosse Deutsche Funk- und Fernseh-Ausstellung\" (Great German Radio and Television Exhibition). The \"Einheits-Fernseh-Empfänger E1\", a TV set designed to be affordable for everybody, was introduced. Plans for large-scale manufacture were thwarted by the outbreak of World War II. Color TV was also introduced (a prototype), based on an invention by Werner Flechsig (cf. shadow mask).\n\nMultinational Dutch electronics corporation Philips introduced the compact audio cassette medium for audio storage and the first cassette recorder (the Philips EL3300), developed by ir. Lou Ottens and his team at the Philips factory in Hasselt, at the 1963 show, on Friday 30 August.\n\n"}
{"id": "239497", "url": "https://en.wikipedia.org/wiki?curid=239497", "title": "Knowledge base", "text": "Knowledge base\n\nA knowledge base (KB) is a technology used to store complex structured and unstructured information used by a computer system. The initial use of the term was in connection with expert systems which were the first knowledge-based systems.\n\nThe original use of the term knowledge-base was to describe one of the two sub-systems of a knowledge-based system. A knowledge-based system consists of a knowledge-base that represents facts about the world and an inference engine that can reason about those facts and use rules and other forms of logic to deduce new facts or highlight inconsistencies.\n\nThe term \"knowledge-base\" was coined to distinguish this form of knowledge store from the more common and widely used term \"database\". At the time (the 1970s) virtually all large Management Information Systems stored their data in some type of hierarchical or relational database. At this point in the history of Information Technology the distinction between a database and a knowledge base was clear and unambiguous.\n\nA database had the following properties:\n\n\nThe first knowledge-based systems had data needs that were the opposite of these database requirements. An expert system requires structured data. Not just tables with numbers and strings, but pointers to other objects that in turn have additional pointers. The ideal representation for a knowledge base is an object model (often called an ontology in artificial intelligence literature) with classes, subclasses and instances.\n\nEarly expert systems also had little need for multiple users or the complexity that comes with requiring transactional properties on data. The data for the early expert systems was used to arrive at a specific answer, such as a medical diagnosis, the design of a molecule, or a response to an emergency. Once the solution to the problem was known there was not a critical demand to store large amounts of data back to a permanent memory store. A more precise statement would be that given the technologies available researchers compromised and did without these capabilities because they realized they were beyond what could be expected and they could develop useful solutions to non-trivial problems without them. Even from the beginning, the more astute researchers realized the potential benefits of being able to store, analyze, and reuse knowledge. For example, see the discussion of Corporate Memory in the earliest work of the Knowledge-Based Software Assistant program by Cordell Green et al.\n\nThe volume requirements were also different for a knowledge-base compared to a conventional database. The knowledge-base needed to know facts about the world. For example, to represent the statement that \"All humans are mortal\". A database typically could not represent this general knowledge but instead would need to store information about thousands of tables that represented information about specific humans. Representing that all humans are mortal and being able to reason about any given human that they are mortal is the work of a knowledge-base. Representing that George, Mary, Sam, Jenna, Mike... and hundreds of thousands of other customers are all humans with specific ages, sex, address, etc. is the work for a database.\n\nAs expert systems moved from being prototypes to systems deployed in corporate environments the requirements for their data storage rapidly started to overlap with the standard database requirements for multiple, distributed users with support for transactions. Initially, the demand could be seen in two different but competitive markets. From the AI and Object-Oriented communities, object-oriented databases such as Versant emerged. These were systems designed from the ground up to have support for object-oriented capabilities but also to support standard database services as well. On the other hand, the large database vendors such as Oracle added capabilities to their products that provided support for knowledge-base requirements such as class-subclass relations and rules.\n\nThe next evolution for the term knowledge-base was the Internet. With the rise of the Internet, documents, hypertext, and multimedia support were now critical for any corporate database. It was no longer enough to support large tables of data or relatively small objects that lived primarily in computer memory. Support for corporate web sites required persistence and transactions for documents. This created a whole new discipline known as Web Content Management. The other driver for document support was the rise of knowledge management vendors such as Lotus Notes. Knowledge Management actually predated the Internet but with the Internet there was great synergy between the two areas. Knowledge management products adopted the term \"knowledge-base\" to describe their repositories but the meaning had a subtle difference. In the case of previous knowledge-based systems the knowledge was primarily for the use of an automated system, to reason about and draw conclusions about the world. With knowledge management products the knowledge was primarily meant for humans, for example to serve as a repository of manuals, procedures, policies, best practices, reusable designs and code, etc. In both cases the distinctions between the uses and kinds of systems were ill-defined. As the technology scaled up it was rare to find a system that could really be cleanly classified as knowledge-based in the sense of an expert system that performed automated reasoning and knowledge-based in the sense of knowledge management that provided knowledge in the form of documents and media that could be leveraged by us humans.\n\n"}
{"id": "41713698", "url": "https://en.wikipedia.org/wiki?curid=41713698", "title": "Leslie Dewan", "text": "Leslie Dewan\n\nLeslie Dewan (born November 27, 1984) is an American entrepreneur and the co-founder and Chief Executive Officer of Transatomic Power. Dr. Dewan is a member of the board of MIT and was named a Young Global Leader by the World Economic Forum.\n\nDewan is a 2002 graduate of The Winsor School in Boston, Massachusetts. \nShe received S.B. degrees from the Massachusetts Institute of Technology in mechanical engineering and nuclear engineering in 2007.\nShe received her Ph.D. in nuclear engineering from MIT in 2013. \nWhile at MIT, Dewan was awarded a Department of Energy Computational Science Graduate Fellowship and an MIT Presidential Fellowship.\n\nDewan co-founded Transatomic Power in Cambridge, Massachusetts in 2011 and is the Chief Executive Officer.\nTransatomic Power is designing and developing a molten salt reactor (Generation IV reactor) that hopes to generate clean and low-cost nuclear power.\nIn December 2012, Forbes magazine selected Dewan for their 30 Under 30 in Energy.\nIn September 2013, MIT Technology Review recognized Dewan as one of “35 Innovators Under 35”.\nIn December 2013, TIME magazine selected Dewan as one of \"30 People Under 30 Changing the World\".\n\nDewan appeared in the documentary Uranium – Twisting the Dragon's Tail, and an episode of Nova entitled \"The Nuclear Option\" in 2017. She is expected to appear in the forthcoming documentary \"The Limitless Generation\".\n\n"}
{"id": "30911092", "url": "https://en.wikipedia.org/wiki?curid=30911092", "title": "List &amp; Label", "text": "List &amp; Label\n\nList & Label is a reporting tool for developers to enable the design and print of reports in Microsoft Windows applications. It supports data sources and development environments such as Microsoft Visual Studio, Embarcadero RAD Studio. \nReports are designed once in the interactive Designer and can then be exported to formats such as PDF, Excel, XHTML and RTF. For the custom preview format, a free viewer application is available. The Designer can be used by end-users royalty free with all paid editions of List & Label.\n\nThe product was first released in 1992 by combit. The current version is 24, released in October 2018.\n\nThe Designer enables users to graphically layout the report. It offers report objects such as tables, charts, crosstabs, gauges, HTML, formatted text, barcodes, matrix codes, lines, rectangles and images. Object properties can either have a fixed value or a conditional formatting. The Designer can be extended by the developer – custom objects, functions and menu actions are supported. The application can interact with the report via the programmable object model of the report. HTML5 viewer is available.\n\nDepending on the programming language, the product offers automatic support for data sources:\nAdditionally, the product offers support for unbound data and can be extended to support other data sources via interfaces.\n\n\nList & Label can be used in Windows development environments. While it competes with other products such as Crystal Reports, SQL Server Reporting Services, ActiveReports, and most notably on the Microsoft .NET platform, there are few competing products for other programming languages (e.g. Progress, Alaska Xbase++, Visual DataFlex).\n\n\n\n"}
{"id": "28715355", "url": "https://en.wikipedia.org/wiki?curid=28715355", "title": "Longitudinal Video Recording", "text": "Longitudinal Video Recording\n\nLongitudinal Video Recording or LVR was a consumer VCR system and videotape standard.\n\nLVR differed from other VCR technologies in that instead of running a tape slowly past a pair of rapidly moving recording heads and laying the tracks obliquely across the tape in a helical scan, the LVR ran a tape quickly past a stationary recording head which would step across the tape width and lay down a series of parallel tracks along the tape's length. The head used fixed scanning but moved down the tape to the next track once the end of the quickly moving tape (240 ips with 300 tracks to a ½” tape) had been reached.\n\nWork had begun on longitudinal video recording as early as 1950 by the electronics division of entertainer Bing Crosby's production company, Bing Crosby Enterprises (BCE), who had pioneered the use of magnetic tape recording for his radio show in the 1940s. BCE gave the world's first demonstration of a videotape recording in Los Angeles on November 11, 1951. Developed by John T. Mullin and Wayne R. Johnson since 1950, the device gave what were described as \"blurred and indistinct\" images, using a modified Ampex 200 tape recorder and standard quarter-inch (0.6 cm) audio tape moving at 360 inches (9.1 m) per second. A year later, an improved version, using one-inch (2.6 cm) magnetic tape, was shown to the press, who reportedly expressed amazement at the quality of the images, although they had a \"persistent grainy quality that looked like a worn motion picture\". Overall, the picture quality was still considered inferior to the best kinescope recordings on film. Bing Crosby Enterprises hoped to have a commercial version available in 1954, but none came forth. BCE demonstrated a color model in February 1955, using a longitudinal recording on half-inch (1.3 cm) tape, essentially similar to what RCA had demonstrated in 1953. CBS, RCA's competitor, was about to order BCE machines when Ampex introduced the superior Quadruplex system (see below).\n\nFrom then it became clear that 'rotating-head' helical scan recorders were the way forward with superior sound and picture quality. However the expense of rotating-head machines ($2000 for a machine in 1956) meant that work continued on developing a consumer stationary-head machine for some time with efforts from Akai, GEC and the BBC. The BBC abandoned their VERA system in 1958 in favor of the Ampex quadruplex system, though further research continued with the BBC Research department demonstrating an experimental digital LVR machine in June 1974 which recorded colour television on 42 tracks on a one-inch tape moving at 120 ips.\n\nThe first home VCRs to become widely available were the Sony U-Matic system in 1971 and Philips VCR system, released in 1972. However, the first system to be successful with consumers was Sony's Betamax in 1975. This was quickly followed by the competing VHS (Video Home System) format from JVC, and later by Video 2000 from Philips. \nBASF had announced a commercial LVR as early as 1974, however it wasn't demonstrated until Autumn 1978 at the Berlin Radio Show. Toshiba first demonstrated their prototype LVR at the Consumer Electronics Show in Chicago in June 1979. Although using the same principle, the tapes developed by BASF and Toshiba were incompatible, with BASF moving a length of tape back and forth over the head, whilst Toshiba used a continuous loop.\n\nOne of the advantages touted for the system was the low production cost of both the tapes and the hardware but consumers never had the chance to try it. Problems with the poor picture quality caused by a very brief interruption to the picture as the head moved down led to delays in production. In addition, the uncertainty as European rival Blaupunkt began developing their own system and the quality issues let VHS and Betamax become the dominant formats and the LVR never went to market.\n\n"}
{"id": "21610483", "url": "https://en.wikipedia.org/wiki?curid=21610483", "title": "NASA Orbital Debris Observatory", "text": "NASA Orbital Debris Observatory\n\nNASA Orbital Debris Observatory (NODO) was an astronomical observatory located in the Lincoln National Forest near Cloudcroft, New Mexico approximately northeast of Alamogordo. From 1995 to 2002 it hosted two telescopes funded and operated by NASA that were dedicated to detecting orbital debris. The facility was initially called the Cloudcroft Electro-Optical Research Facility when it was completed in 1962, and was also known as the Cloudcroft Observatory. It is now privately owned.\n\n\n\n"}
{"id": "19375335", "url": "https://en.wikipedia.org/wiki?curid=19375335", "title": "Near-field magnetic induction communication", "text": "Near-field magnetic induction communication\n\nA Near-Field Magnetic Induction communication system is a short range wireless physical layer that communicates by coupling a tight, low-power, non-propagating magnetic field between devices. The concept is for a transmitter coil in one device to modulate a magnetic field which is measured by means of a receiver coil in another device.\n\nNear-field magnetic induction (NFMI) communication systems differ from other wireless communications in that most conventional wireless RF systems use an antenna to generate, transmit, and propagate an electromagnetic wave. In these types of systems all of the transmission energy is designed to radiate into free space. This type of transmission is referred to as \"far-field.\"\n\nAccording to Maxwell's equation for a radiating wire, the power density of far-field transmissions attenuates or rolls off at a rate proportional to the inverse of the range to the second power (1/r) or −20 dB per decade. This slow attenuation over distance allows far-field transmissions to communicate effectively over a long range. The properties that make long range communication possible are a disadvantage for short range communication systems.\n\nThe NFMI system uses a short range (less than 2 meters).\n\nThe standard modulation schemes used in typical RF communications (amplitude modulation, phase modulation, and frequency modulation) can be used in near-field magnetic induction system\n\nNFMI systems are designed to contain transmission energy within the localized magnetic field. This magnetic field energy resonates around the communication system, but does not radiate into free space. This type of transmission is referred to as \"near-field.\" The power density of near-field transmissions is extremely restrictive and attenuates or rolls off at a rate proportional to the inverse of the range to the sixth power (1/r) or −60 dB per decade.\n\nIn current commercial implementations of near-field communications, the most commonly used carrier frequency is 13.56 MHz and has a wavelength (λ) of 22.1 meters. The crossover point between near-field and far-field occurs at approximately λ/2π. At this frequency the crossover occurs at 3.52 meters, at which point the propagating energy from the NFMI system conforms to the same propagation rules as any far-field system; rolling off at −20 dB per decade. At this distance the propagated energy levels are −40 dB to −60 dB (10,000 to 1,000,000 times) lower than an equivalent intentional far-field system.\n\nNear-field magnetic induction technology has been in use nearly exclusively by the company FreeLinc. Using NFMI to create a secure wireless communication between two-way radio accessories. This is done by creating a magnetic communication \"bubble\" around headsets, speaker-microphones and radios. This magnetic bubble has a radius of approximately 1.5 meters, is immune from radio frequency (RF) interference and virtually secure from eavesdropping. An eavesdropper would have to be standing next to the radio, within the magnetic bubble, to intercept wireless transmissions to and from a microphone or headset.\n\n\nhttp://www.freelinc.com/technology/\n\nhttps://www.nxp.com/products/wireless-connectivity/miglo:NFMI-RADIO-SOLUTIONS\n"}
{"id": "6214840", "url": "https://en.wikipedia.org/wiki?curid=6214840", "title": "Nuclear graphite", "text": "Nuclear graphite\n\nNuclear graphite is any grade of graphite, usually synthetic graphite, specifically manufactured for use as a moderator or reflector within a nuclear reactor. Graphite is an important material for the construction of both historical and modern nuclear reactors, due to its extreme purity and its ability to withstand extremely high temperatures.\n\nThe potential for creating a nuclear chain reaction in uranium became apparent in 1939 following the nuclear fission experiments of Otto Hahn and Fritz Strassman, and the interpretation of these results by Lise Meitner and Otto Frisch. The exciting possibilities that this presented rapidly spread throughout the world physics community. In order for the fission process to chain react, the neutrons created by uranium fission must be slowed down by interacting with a neutron moderator (an element with a low atomic weight, that will \"bounce\", when hit by a neutron) before they will be captured by other uranium atoms. It became well known by late 1939 that the two most promising moderators were heavy water and graphite.\n\nIn February 1940, using funds that were allocated partly as a result of the Einstein-Szilard letter to President Roosevelt, Leo Szilard purchased several tons of graphite from the Speer Carbon Company and from the National Carbon Company (the National Carbon Division of the Union Carbide and Carbon Corporation in Cleveland Ohio) for use in Enrico Fermi's first fission experiments, the so-called exponential pile. Fermi writes that \"The results of this experiment was [sic] somewhat discouraging\" presumably due to the absorption of neutrons by some unknown impurity. So, in December 1940 Fermi and Szilard met with Herbert G. MacPherson and V. C. Hamister at National Carbon to discuss the possible existence of impurities in graphite. During this conversation it became clear that minute quantities of boron impurities were the source of the problem.\n\nAs a result of this meeting, over the next two years, MacPherson and Hamister developed thermal and gas extraction purification techniques at National Carbon for the production of boron-free graphite. The resulting product was designated AGOT Graphite (\"Atcheson Graphite Ordinary Temperature\") by National Carbon, and it was \"the first true nuclear grade graphite\".\n\nDuring this period, Fermi and Szilard purchased graphite from several manufacturers with various degrees of neutron absorption cross section: AGX graphite from National Carbon Company with 6.68 mb (millibarns) cross section, US graphite from United States Graphite Company with 6.38 mb cross section, Speer graphite from the Speer Carbon Company with 5.51 mb cross section, and when it became available, AGOT graphite from National Carbon, with 4.97 mb cross section. (See also Haag [2005].) By November 1942 National Carbon had shipped 250 tons of AGOT graphite to the University of Chicago where it became the primary source of graphite to be used in the construction of Fermi's Chicago Pile-1, the first nuclear reactor to generate a sustained chain reaction (December 2, 1942). AGOT graphite was used to build the X-10 graphite reactor in Oak Ridge TN (early 1943) and the first reactors at the Hanford Site in Washington (mid 1943), for the production of plutonium during and after World War II. The AGOT process and its later refinements became standard techniques in the manufacture of nuclear graphite.\n\nThe neutron cross section of graphite was also investigated during the second world war in Germany by Walter Bothe, P. Jensen, and Werner Heisenberg. The purest graphite available to them was a product from the Siemens Plania company, which exhibited a neutron absorption cross section of about 6.4 mb to 7.5 mb (Haag 2005). Heisenberg therefore decided that graphite would be unsuitable as a moderator in a reactor design using natural uranium, due to this apparently high rate of neutron absorption. Consequently, the German effort to create a chain reaction involved attempts to use heavy water, an expensive and scarce alternative, made all the more difficult to acquire as a consequence of the Norwegian heavy water sabotage by Norwegian and Allied forces. Writing as late as 1947, Heisenberg still did not understand that the only problem with graphite was the boron impurity.\n\nIn December 1942 Eugene Wigner suggested that neutron bombardment might introduce dislocations and other damage in the molecular structure of materials such as the graphite moderator in a nuclear reactor (the Wigner effect). The resulting buildup of energy in the material became a matter of concern The possibility was suggested that graphite bars might fuse together as chemical bonds at the surface of the bars are opened and closed again. Even the possibility that the graphite parts might very quickly break into small pieces could not be ruled out. However, the first power-producing reactors (X-10 Graphite Reactor and Hanford B Reactor) had to be built without such knowledge. Cyclotrons, which were the only fast neutron sources available, would take several months to produce neutron irradiation equivalent to one day in a Hanford reactor.\n\nThis was the starting point for large-scale research programmes to investigate the property changes due to fast particle radiation and to predict their influence on the safety and the lifetime of graphite reactors to be built. Influences of fast neutron radiation on strength, electrical and thermal conductivity, thermal expansivity, dimensional stability, on the storage of internal energy (Wigner energy), and on many other properties have been observed many times and in many countries after the first results emerged from the X-10 reactor in 1944.\n\nAlthough catastrophic behaviour such as fusion or crumbling of graphite pieces has never occurred, large changes in many properties do result from fast neutron irradiation which need to be taken into account when graphite components of nuclear reactors are designed. Although not all effects are well understood yet, more than 100 graphite reactors have successfully operated in the last 60 years. A few severe accidents in graphite reactors can in no case be attributed to insufficient information (at the time of design) regarding the properties of the graphite in use. Recently, the collection of new material property data has improved knowledge significantly. \n\nReactor-grade graphite must be free of neutron absorbing materials, especially boron, which has a large neutron capture cross section. Boron sources in graphite include the raw materials, the packing materials used in baking the product, and even the choice of soap (for example, borax) used to launder the clothing worn by workers in the machine shop. Boron concentration in thermally purified graphite (such as AGOT graphite) can be less than 0.4 ppm and in chemically purified nuclear graphite it is less than 0.06 ppm.\n\nThis describes the behavior of nuclear graphite, specifically when exposed to fast neutron irradiation.\n\nSpecific phenomena addressed:\n\nNuclear graphite for the UK Magnox reactors was manufactured from petroleum coke mixed with coal-based binder pitch heated and extruded into billets, and then baked at 1,000 °C for several days. To reduce porosity and increase density, the billets were impregnated with coal tar at high temperature and pressure before a final bake at 2,800 °C. Individual billets were then machined into the final required shapes.\n\nThe manufacturing process is designed to ensure uniformity in material properties. Despite this care, recent research using stochastic finite element analysis has shown that tiny spatial variations in material properties may play a significant role in how a graphite component ages. A study carried out in 2016 provides data for the spatial variation of properties such as density and Young's modulus within a typical billet. This information has been used to calibrate random fields for probabilistic simulation.\n\nThere have been two major accidents in graphite-moderated reactors, the Windscale fire and the Chernobyl disaster.\n\nIn the Windscale fire, an untested annealing process for the graphite was used, causing overheating in unmonitored areas of the core and leading directly to the ignition of the fire. The material that ignited was not the graphite moderator itself, but rather the canisters of metallic uranium fuel within the reactor. When the fire was extinguished, it was found that the only areas of the graphite moderator to have incurred thermal damage were those that had been close to the burning fuel canisters.\n\nIn the Chernobyl disaster, the moderator was not responsible for the primary event. Instead, a massive power excursion during a mishandled test caused the catastrophic failure of the reactor vessel and a near-total loss of coolant supply. The result was that the fuel rods rapidly melted and flowed together while in an extremely-high-power state, causing a small portion of the core to reach a state of runaway prompt criticality and leading to a massive energy release, resulting in the explosion of the reactor core and the destruction of the reactor building. The massive energy release during the primary event superheated the graphite moderator, and the disruption of the reactor vessel and building allowed the superheated graphite to come into contact with atmospheric oxygen. As a result, the graphite moderator caught fire, sending a plume of highly radioactive fallout into the atmosphere and over a very widespread area.\n\n"}
{"id": "50398290", "url": "https://en.wikipedia.org/wiki?curid=50398290", "title": "Paul F. Forman Team Engineering Excellence Award", "text": "Paul F. Forman Team Engineering Excellence Award\n\nThe Paul F. Forman Team Engineering Excellence Award was first introduced as the \"Engineering Excellence Award\" by the Optical Society in 1989 and was awarded individually, or shared among individuals. In 2007 it was named in honor of Paul F. Forman. This award recognizes technical achievements in optical engineering as well as contributions to society such as engineering education. It award is sponsored by Zygo Corporation, Canon Inc, Optical Solutions Group at Synopsys, Cambridge Research & Instrumentation, and several individual contributors.\n"}
{"id": "2277296", "url": "https://en.wikipedia.org/wiki?curid=2277296", "title": "Phlegmatized explosive", "text": "Phlegmatized explosive\n\nA phlegmatized explosive is an explosive that has had an agent added to stabilize or desensitize it. Sometimes this is desirable either to improve the handling properties of an explosive (e.g. when munitions are filled in factories) or to reduce its sensitivity, brisance or detonation velocity. TNT explosive can itself be used to phlegmatize more sensitive explosives such as RDX (to form Cyclotol), HMX (to form Octol) or PETN (to form Pentolite). Other typical phlegmatizing agents include paraffin wax (5% used in OKFOL and Composition H6), paper or even water (used in water gel explosives). Such agents are nearly always flammable themselves (therefore adding fuel to the blast) or will at least boil off easily. Typically, a small amount of phlegmatizing agent is used e.g. Composition B, which has 1% paraffin wax added, or the Russian RGO hand grenade which contains 90 grams of \"A-IX-1\" explosive, comprising 96% RDX and 4% paraffin wax by weight. Another example of use is the VS-50 antipersonnel mine, which contains an explosive filling of 43 grams of RDX, again phlegmatized by combining it with 10% paraffin wax by weight.\n\nExplosive compounds may exist in material states that limit their application. For instance, nitroglycerin is normally an oily liquid. Phlegmatization of nitroglycerin allows it to be formed as a solid, commonly known as dynamite. It also allows the liquid, which is very sensitive to shock, to be handled more vigorously.\n"}
{"id": "1429193", "url": "https://en.wikipedia.org/wiki?curid=1429193", "title": "Policy appliances", "text": "Policy appliances\n\nPolicy appliances are technical control and logging mechanisms to enforce or reconcile policy rules (information use rules) and to ensure accountability in information systems. Policy appliances can be used to enforce policy or other systems constraints within and among trusted systems. \n\nThe emerging global information society consists of many heterogeneous but interconnected systems that are governed or managed according to different policies, rules, or principles that meet local information management needs. For example, systems may be subject to different international, national or other political subdivision information disclosure or privacy laws; or different information management or security policies among or between government agencies, government and private sector information systems, or producers and consumers of proprietary information or intellectual property, etc. \n\nThis interconnected network of systems (for which the Internet as we currently know it serves as the transport layer) increasingly requires dynamic agreement (negotiation) and technical mediation as to which policies will govern information as it flows between or among systems (that is, what use policies will govern what information goes where, under what constraints, and who has access to it for what purposes, etc.). The alternative to developing these mediating mechanisms to provide automated policy negotiation and enforcement across interconnection between disparate systems is the increased \"balkanization\" or fragmentation of the Internet. \n\nBecause no single policy can govern all systems or information needs, methods of reconciling differences between systems and then enforcing and monitoring agreed policies are necessary in order to share useful information and keep systems interconnected. Current static methods based on all-or-nothing access control are insufficient to meet variable information production and consumption needs, particularly when there are potentially competing policies (for example, the conflict between disclosure and privacy laws) that are contextually dependent. Access control mechanisms that simply control who has access between systems result in stove-piped information silos, \"walled gardens\", and increased network fragmentation. \"Policy appliance\" is a general term to describe dynamic, contextually-aware control mechanisms currently being researched and developed to enforce use policies between systems.\n\nAlthough policy development and enforcement itself is a political or cultural process, not a technological one, technical systems architecture can be used to determine what policy opportunities exist by controlling the terms under which information is exchanged, or applications behave, across systems. In order to maintain the open transport, end-to-end principles embedded in the current Internet design – that is, to avoid hard-coding policy solutions in the transport layer or using strict access control regimes to segment the network – policy appliances are required to mediate between systems to facilitate information sharing, data exchange, and management process interoperability.\n\nPolicy appliances -- a generic term referring to any form of middleware that manages policy rules -- can mediate between data owners or producers, data aggregators, and data users, and among heterogeneous institutional systems or networks, to enforce, reconcile, and monitor agreed information management policies and laws across system (or between jurisdictions) with divergent information policies or needs. Policy appliances can interact with smart data (data that carries with it contextual relevant terms for its own use), intelligent agents (queries that are self-credentialed, authenticating, or contextually adaptive), or context-aware applications to control information flows, protect security and confidentiality, and maintain privacy. \n\nPolicy appliances support policy-based information management processes by enabling rules-based processing, selective disclosure, and accountability and oversight. \n\nExamples of policy appliance technologies for rules-based processing include analytic filters, contextual search, semantic programs, labeling and wrapper tools, and DRM, among others; policy appliance technologies for selective disclosure include anonymization, content personalization, subscription and publishing tools, among others; and, policy appliance technologies for accountability and oversight include authentication, authorization, immutable and non-repudiable logging, and audit tools, among others.\n\nControl and accountability over policy appliances between competing systems is becoming a key determinant in policy implementation and enforcement, and will continue to be subject to ongoing international and national political, corporate and bureaucratic struggle. Transparency, together with immutable and non-repudiable logs, are necessary to ensure accountability and compliance for both political, operational and civil liberties policy needs. Increasingly, international and national information policy and law will need to rely on technical means of enforcement and accountability through policy appliances. \n\n\"See also\", Technology, Security, and Privacy: The Fear of Frankenstein, the Mythology of Privacy, and the Lessons of King Ludd, 7 Yale J. L. & Tech. 123; 9 Intl. J. Comm. L. & Pol'y 8 (2004) at 56-58 (discussing “privacy appliances” to enforce rules and provide accountability). The concept of privacy appliances originated with the DARPA Total Information Awareness project. \"See\" Presentation by Dr. John Poindexter, Director, Information Awareness Office (IAO), DARPA, at DARPA-Tech 2002 Conference, Anaheim, CA (Aug. 2, 2002); ISAT 2002 Study, Security with Privacy (Dec. 13, 2002); and IAO Report to Congress regarding the Terrorism Information Awareness Program at A-13 (May 20, 2003) in response to Consolidated Appropriations Resolution, 2003, No.108-7, Division M, §111(b) [signed Feb. 20, 2003].\n"}
{"id": "38079557", "url": "https://en.wikipedia.org/wiki?curid=38079557", "title": "Pressure reference system", "text": "Pressure reference system\n\nPressure reference system (PRS) is an enhancement of the inertial reference system and attitude and heading reference system designed to provide position angles measurements which are stable in time and do not suffer from long term drift caused by the sensor imperfections. The measurement system uses behavior of the International Standard Atmosphere where atmospheric pressure descends with increasing altitude and two pairs of measurement units. Each pair measures pressure at two different positions that are mechanically connected with known distance between units, e.g. the units are mounted at the tips of the wing. In horizontal flight, there is no pressure difference measured by the measurement system which means the position angle is zero. In case the airplane turns, the tips of the wings mutually change their positions, one is going up and the second one is going down, and the pressure sensors in every unit measure different values which are translated into a position angle.\n\nThe strapdown inertial navigation system uses double integration of the accelerations measured by an inertial measurement unit (IMU). This process sums the sensors outputs together with all the sensor and measurement errors. The precision and long-term stability of the INS system depends on the quality of sensors used within the IMU. The sensor quality can be evaluated by Allan Variance technique. A precise IMU uses laser gyroscopes and precise accelerometers which are expensive. The INS is a sole system with no other inputs. Nowadays the trend of the modern navigation is to integrate signals from IMU together with data provided by Global Positioning System (GPS). This approach gives long term stability to the INS output by suppressing sensor error influence on the calculation of the airplane position. The measurement system becomes attitude and heading reference system which can relax requirement on the sensor precision because the long-term stability is assured by GPS. The sensors used within AHRS are used only for position angles determination and so just one numerical integration of the angular rate measurements is required. The AHRS system is cheaper and a lot of universities and companies are developing AHRS systems based on microelectromechanical systems (MEMS) sensors. The MEMS sensors do not have performance required for navigation purposes. It is shown in an experimental research report, where the output of the navigation solution drifts away after 2 seconds. The AHRS units based on MEMS inertial sensors usually also use a vector magnetometer, a GPS receiver, and a data fusion algorithm to cope with MEMS inertial sensors errors. Next to the sensor imperfections there are also environmental parameters which influence the computed values (position angles):\nAll these influences cause drifts in the computed output data which can confuse pilot who performs the flight.\n\nThe concept of the PRS was defined by Pavel Paces in his PhD thesis where results measured under laboratory conditions were also published. Three arrangements of the PRS were evaluated:\nWhile the first method gives only ambiguous results the second method works well as it can be replaced by two altimeters. Disadvantage of the second method is high measurement uncertainty of both values. This is being solved by the extension of the reference volumes used even in absolute pressure sensors.\n"}
{"id": "14669487", "url": "https://en.wikipedia.org/wiki?curid=14669487", "title": "Productive nanosystems", "text": "Productive nanosystems\n\nThe Technology Roadmap for Productive Nanosystems defines \"productive nanosystems\" as functional nanometer-scale systems that make atomically-specified structures and devices under programmatic control, i.e., they perform Atomically precise manufacturing. Such devices are currently only hypothetical, and productive nanosystems represents a more advanced approach among several to perform Atomically Precise Manufacturing. A workshop on Integrated Nanosystems for Atomically Precise Manufacturing was held by the Dept. of Energy in 2015. \n\nPresent-day technologies are limited in various ways. Large atomically precise structures (that is, virtually defect-free) do not exist. Complex 3D nanoscale structures exist in the form of folded linear molecules such as DNA origami and proteins. It is also possible to build very small atomically precise structures using scanning probe microscopy to construct molecules such as FeCO and Triangulene, or to perform hydrogen depassivation lithography. But it is not yet possible to combine components in a systematic way to build larger, more complex systems.\n\nPrinciples of physics and examples from nature both suggest that it will be possible to extend atomically precise fabrication to more complex products of larger size, involving a wider range of materials. An example of progress in this direction would be Christian Schafmeister's work on bis-peptides.\n\nMihail Roco, one of the architects of the USA's National Nanotechnology Initiative, has proposed four states of nanotechnology that seem to parallel the technical progress of the Industrial Revolution, of which productive nanosystems is the most advanced.\n\n1. Passive nanostructures - nanoparticles and nanotubes that provide added strength, electrical and thermal conductivity, toughness, hydrophilic/phobic and/or other properties that emerge from their nanoscale structure.\n\n2. Active nanodevices - nanostructures that change states in order to transform energy, information, and/or to perform useful functions. There is some debate about whether or not state-of-the art integrated circuits qualify here, since they operate despite emergent nanoscale properties, not because of them. Therefore, the argument goes, they don't qualify as \"novel\" nanoscale properties, even though the devices themselves are between one and a hundred nanometers. \n\n3. Complex nanomachines - the assembly of different nanodevices into a nanosystem to accomplish a complex function. Some would argue that Zettl's machines fit in this category; others argue that modern microprocessors and FPGAs also fit. \n\n4. Systems of nanosystems/Productive nanosystems - these will be complex nanosystems that produce atomically precise parts for other nanosystems, not necessarily using novel nanoscale-emergent properties, but well-understood fundamentals of manufacturing. Because of the discrete (i.e. atomic) nature of matter and the possibility of exponential growth, this stage is seen as the basis of another industrial revolution. There are currently many different approaches to building productive nanosystems: including top-down approaches like Patterned atomic layer epitaxy and Diamondoid Mechanosynthesis. There are also bottom-up approaches like DNA Origami and Bis-peptide Synthesis.\n\nA fifth step, info/bio/nano convergence, was added later by Roco. This is the convergence of the three most revolutionary technologies, since every living thing is made up of atoms and information.\n"}
{"id": "2548926", "url": "https://en.wikipedia.org/wiki?curid=2548926", "title": "Rock-cut architecture", "text": "Rock-cut architecture\n\nRock-cut architecture is the creation of structures, buildings, and sculptures by excavating solid rock where it naturally occurs. Rock-cut architecture is designed and made by man from the start to finish. In India and China, the terms 'cave' and 'cavern' are often applied to this form of man-made architecture. However, caves and caverns, that began in natural form, are not considered to be 'rock-cut architecture' even if extensively modified. Although rock-cut structures differ from traditionally built structures in many ways, many rock-cut structures are made to replicate the facade or interior of traditional architectural forms. Interiors were usually carved out by starting at the roof of the planned space and then working downward. This technique prevents stones falling on workers below. The three main uses of rock-cut architecture were temples (like those in India), tombs (like those in Petra, Jordan) and cave dwellings (like those in Cappadocia, Turkey).\n\nSome rock-cut architecture, mostly for tombs, is excavated entirely in chambers under the surface of relatively level rock. If the excavation is instead made into the side of a cliff or steep slope, there can be an impressive facade, as found in Lycian tombs, Petra, Ajanta and elsewhere. The most laborious and impressive rock-cut architecture is the excavation of tall free-standing monolithic structures entirely below the surface level of the surrounding rock, in a large excavated hole around the structure. Ellora in India and Lalibela in Ethiopia (built by the Zagwe dynasty) provide the most spectacular and famous examples of such structures.\n\nRock-cut architecture, though intensely laborious with ancient tools and methods, was presumably combined with quarrying the rock for use elsewhere; the huge amounts of stone removed have normally vanished from the site. Rock-cut architecture is also said to be cut, hewn, etc., \"from the living rock\". Another term sometimes associated with rock-cut architecture is monolithic architecture, which is rather applied to free-standing structures made of a single piece of material. Monolithic architecture is often rock-cut architecture (e.g. Ellora Kailasanathar Temple) but monolithic structures might be also cast of artificial material, e.g. concrete.\n\nThe Gommateshwara statue (Bahubali), the largest monolithic statue in the world, at Shravanabelagola, Karnataka, India, was built in 983 AD and was carved from a large single block of granite. In many parts of the world there are also rock reliefs, relief sculptures carved into rock faces, outside caves or at other sites.\n\nAncient monuments of rock-cut architecture are widespread in several regions of world. Alteration of naturally formed caverns, although distinct from completely carved structures in the strict sense, date back to the neolithic period on several Mediterranean islands e.g. Malta (Hypogeum of Ħal-Saflieni), Sardinia (Anghelu Ruju, built between 3,000 and 1,500 BCE) and others.\nLarge-scale rock-cut structures were built in Ancient Egypt. Among these monuments was the Great Temple of Ramesses II, known as Abu Simbel, located along the Nile in Nubia, near the borders of Sudan about 300 kilometers from Aswan in Egypt. It dates from about the 19th Dynasty (ca. 1280 BCE), and consists of a monumentally scaled facade carved out of the cliff and a set of interior chambers that form its sanctuary. \n\nIn the 8th century, the Phrygians started some of the earliest rock-cut monuments, such as the Midas monument (700 BCE), dedicated to the famous Phrygian king Midas.\n\nIn the 5th century BCE, the Lycians, who inhabited southern Anatolia (now Turkey) built hundreds of rock-cut tombs of a similar type, but smaller in scale. Excellent examples are to be found near Dalyan, a town in Muğla Province, along the sheer cliffs that faces a river. Since these served as tombs rather than as religious sites, the interiors were usually small and unassuming. The ancient Etruscans of central Italy also left an important legacy of rock-cut architecture, mostly tombs, as those near the cities of Tarquinia and Vulci.\n\nThe creation of rock-cut tombs in ancient Israel began in the 8th-century BCE and continued through the Byzantine period. The Tomb of Absalom was constructed in the 1st century AD in the Kidron Valley of Jerusalem.\nRock-cut architecture occupies a particularly important place in the history of Indian Architecture. The earliest instances of Indian rock-cut architecture, the Barabar caves, date from about the 3rd to the 2nd century BCE. They were built by the Buddhist monks and consisted mostly of multi-storey buildings carved into the mountain face to contain living and sleeping quarters, kitchens, and monastic spaces. Some of these monastic caves had shrines in them to the Buddha, bodhisattvas and saints. As time progressed, the interiors became more elaborate and systemitized; surfaces were often decorated with paintings, such as those at Ajanta. At the beginning of the 7th century Hindu rock-cut temples began to be constructed at Ellora. Unlike most previous examples of rock-cut architecture which consisted of a facade plus an interior, these temples were complete three-dimensional buildings created by carving away the hillside. They required several generations of planning and coordination to complete. Other major examples of rock-cut architecture in India are at Ajanta and Pataleshwar.\nThe Nabataeans in their city of Petra, now in Jordan, extended the Western Asian tradition, carving their temples and tombs into the yellowish-orange rock that defines the canyons and gullies of the region. These structures, dating from 1st century BCE to about 2nd century CE, are particularly important in the history of architecture given their experimental forms. Here too, because the structures served as tombs, the interiors were rather perfunctory. In Petra one even finds a theater where the seats are cut out of the rock.\nThe technological skills associated with making these complex structures moved into China along the trade routes. The Longmen Grottoes, the Mogao Caves, and the Yungang Grottoes consist of hundreds of caves many with statues of Buddha in them. Most were built between 460–525 AD. There are extensive rock-cut buildings, including houses and churches in Cappadocia, Turkey. They were built over a span of hundreds of years prior to the 5th century CE. Emphasis here was more on the interiors than the exteriors.\n\nAnother extensive site of rock-cut architecture is in Lalibela, a town in northern Ethiopia. The area contains numerous Orthodox churches in three dimensions, as at Ellora, that were carved out of the rock. These structures, which date from the 12th and 13th centuries CE and which are the last significant examples of this architectural form, ranks as among the most magnificent examples of rock-cut architecture in the world, with both interior and exterior brought to fruition.\n\nAncient rock cut tombs, temples and monasteries often have been adorned with frescoes and reliefs. The high resistance of natural cliff, skilled use of plaster and constant microclimate often have helped to preserve this art in better condition than in conventional buildings. Such exceptional examples are the ancient and early medieval frescoes in such locations as Bamyan Caves in Afghanistan with the most ancient known oil paintings in the world from 8th century AD, Ajanta Caves in India with well preserved tempera paintings from 2nd century BCE, Christian frescoes on Churches of Göreme, Turkey and numerous other monuments in Asia, Europe and Africa.\n\n\n"}
{"id": "25606638", "url": "https://en.wikipedia.org/wiki?curid=25606638", "title": "Search neutrality", "text": "Search neutrality\n\nSearch neutrality is a principle that search engines should have no editorial policies other than that their results be comprehensive, impartial and based solely on relevance. This means that when a user queries a search engine, the engine should return the most relevant results found in the provider's domain (those sites which the engine has knowledge of), without manipulating the order of the results (except to rank them by relevance), excluding results, or in any other way manipulating the results to a certain bias. \nSearch neutrality is related to network neutrality in that they both aim to keep any one organization from limiting or altering a user's access to services on the Internet. Search neutrality aims to keep the organic search results (results returned because of their relevance to the search terms, as opposed to results sponsored by advertising) of a search engine free from any manipulation, while network neutrality aims to keep those who provide and govern access to the Internet from limiting the availability of resources to access any given content.\n\nThe term \"search neutrality\" in context of the internet appears as early as March 2009 in an academic paper by Andrew Odlyzko titled, \"Network Neutrality, Search Neutrality, and the Never-ending Conflict between Efficiency and Fairness in Markets\". In this paper, Odlykzo predicts that if net neutrality were to be accepted as a legal or regulatory principle, then the questions surrounding search neutrality would be the next controversies. Indeed, in December 2009 the New York Times published an opinion letter by Foundem co-founder and lead complainant in an anti-trust complaint against Google, Adam Raff, which likely brought the term to the broader public. According to Raff in his opinion letter, search neutrality ought to be \"the principle that search engines should have no editorial policies other than that their results be comprehensive, impartial and based solely on relevance\". On October 11, 2009, Adam and his wife Shivaun launched SearchNeutrality.org, an initiative dedicated to promoting investigations against Google's search engine practices . There, the Raffs note that they chose to frame their issue with Google as \"search neutrality\" in order to benefit from the focus and interest on net neutrality.\n\nIn contrast to net neutrality, answers to such questions, as \"what is search neutrality?\" or \"what are appropriate legislative or regulatory principles to protect search neutrality?\", appear to have less consensus. The idea that neutrality means equal treatment, regardless of the content, comes from debates on net neutrality . Neutrality in search is complicated by the fact that search engines, by design and in implementation, are not intended to be neutral or impartial. Rather, search engines and other information retrieval applications are designed to collect and store information (indexing), receive a query from a user, search for and filter relevant information based on that query (searching/filtering), and then present the user with only a subset of those results, which are ranked from most relevant to least relevant (ranking). \"Relevance\" is a form of bias used to favor some results and rank those favored results. Relevance is defined in the search engine so that a user is satisfied with the results and is therefore subject to the user's preferences. And because relevance is so subjective, putting search neutrality into practice has been so contentious.\n\nSearch neutrality became a concern after search engines, most notably Google, were accused of search bias by other companies. Competitors and companies claim search engines systematically favor some sites (and some kind of sites) over others in their lists of results, disrupting the objective results users believe they are getting. \n\nThe call for search neutrality goes beyond traditional search engines. Sites like Amazon.com and Facebook are also accused of skewing results. Amazon’s search results are influenced by companies that pay to rank higher in their search results while Facebook filters their newsfeed lists to conduct social experiments.\n\nIn order to find information on the Web, most users make use of search engines, which crawl the web, index it and show a list of results ordered by relevance. The use of search engines to access information through the web has become a key factor for online businesses companies, which depend on the flow of users visiting their pages. One of these companies is Foundem. Foundem provides a \"vertical search\" service to compare products available on online markets for the U.K. Many people see these \"vertical search\" sites as spam. Beginning in 2006 and for three and a half years following, Foundem’s traffic and business dropped significantly due to what they assert to be a penalty deliberately applied by Google. It is unclear, however, whether their claim of a penalty was self-imposed via their use of iframe HTML tags to embed the content from other websites. At the time at which Foundem claims the penalties were imposed, it was unclear whether web crawlers crawled beyond the main page of a website using iframe tags without some extra modifications. The former SEO director OMD UK, Jaamit Durrani, among others, offered this alternative explanation, stating that “Two of the major issues that Foundem had in summer was content in iFrames and content requiring javascript to load – both of which I looked at in August, and they were definitely in place. Both are huge barriers to search visibility in my book. They have been fixed somewhere between then and the lifting of the supposed ‘penalty’. I don’t think that’s a coincidence.” \n\nMost of Foundem’s accusations claim that Google deliberately applies penalties to other vertical search engines because they represent competition. Foundem is backed by a Microsoft proxy group, the 'Initiative for Competitive Online Marketplace'.\n\nThe following table details Foundem's chronology of events as found on their website:\n\nGoogle's large market share (85%) has made them a target for search neutrality litigation via antitrust laws. In February 2010, Google released an article on the Google Public Policy blog expressing their concern for fair competition, when other companies at the UK joined Foundem's cause (eJustice.fr, and Microsoft's Ciao! from Bing) also claiming being unfairly penalized by Google.\n\nAfter two years of looking into claims that Google “manipulated its search algorithms to harm vertical websites and unfairly promote its own competing vertical properties,” the Federal Trade Commission (FTC) voted unanimously to end the antitrust portion of its investigation without filing a formal complaint against Google. The FTC concluded that Google’s “practice of favoring its own content in the presentation of search results” did not violate U.S. antitrust laws. The FTC further determined that even though competitors might be negatively impacted by Google's changing algorithms, Google did not change its algorithms to hurt competitors, but as a product improvement to benefit consumers.\n\nThere are a number of arguments for and against search neutrality.\n\n\n\nGoogle’s \"Universal Search\" system has been identified as one of the least neutral search engine practices, and following the implementation of Universal Search websites, such as MapQuest, the company experienced a massive decline in web traffic. This decline has been attributed to Google linking to its own services rather than the services offered at external websites. Despite these claims, Google actually displays Google content on the first page, while rival search engines do not considerably less often than Microsoft's Bing which displays Microsoft content when rivals do not. Bing displays Microsoft content in first place more than twice as often as Google shows Google content in first place. This indicates that as far as there is any 'bias', Google is less biased than its principal competitor.\n"}
{"id": "29640242", "url": "https://en.wikipedia.org/wiki?curid=29640242", "title": "Smart thermostat", "text": "Smart thermostat\n\nSmart thermostats are devices that can be used with home automation and are responsible for controlling a home's heating and/or air conditioning. They perform the same functions as a Programmable thermostat as they allow the user to control the temperature of their home throughout the day using a schedule, such as setting a different temperature at night.\n\nLike a connected thermostat, they are connected to the Internet. They allow users to adjust heating settings from other internet-connected devices, such as smartphones. This allows users to easily adjust the temperature remotely. This ease of use is essential for ensuring energy savings: studies have shown that households with programmable thermostats actually have higher energy consumption than those with simple thermostats, because residents program them incorrectly or disable them completely.\n\nSmart thermostats also record internal/external temperatures, time the HVAC system has been running and can even notify you if your air filter needs to be replaced. This information is typically displayed later on an internet-connected device.\n\n Some smart thermostats, such as the Nest thermostat, can learn when the house is likely to be occupied, and when it is likely to be empty. This allows automatic pre-heating or pre-cooling so the temperature is comfortable when a resident arrives. If the residents or lifestyles change, these smart thermostats will gradually adjust the schedule, maintaining energy savings and comfort.\n\nMotion detectors can determine if someone is home. One smart thermostat that uses motion detectors is the Ecobee4.\n\nA wireless network can be used to sense when someone is out of range, thus determining if they're in or nearby their home. This technique, called geofencing, is used by the Honeywell T6 Smart Thermostat.\n\nA Connected thermostat is one that can be controlled through an internet connection, but will not provide analytic information. In recent years WI-FI thermostats have risen in popularity, they combine the technology of thermometers and WI-FI. So now you can have a thermometer in your home that is displayed on your phone that uses Wi-Fi technology. This technology is being developed right now so it will be available for thermostats in machinery and cars. Google are involved in this push towards technology since they acquired a Wi-Fi temperature company called Nest.\n\nRather than controlling the temperature of the whole house, zoned systems can control individual rooms. This can increase energy savings, for example by heating or cooling only a Home-office and not the bedrooms and other areas that are empty during the day.\n\nThe most advanced smart thermostats combine both: they are able to learn when each room of a house is normally occupied and automatically schedule the heating for that room appropriately. Currently, the only such system available for home use is Heat Genius.\n\n"}
{"id": "48804473", "url": "https://en.wikipedia.org/wiki?curid=48804473", "title": "Snauwaert", "text": "Snauwaert\n\nSnauwaert is an Italian based tennis racquet brand and manufacturer of other tennis equipment. It was founded in 1928 by the brothers-in-law Valler Snauwaert and Eugeen Depla. Famous tennis players that used Snauwaert include Vitas Gerulaitis, Evonne Goolagong Cawley, Miloslav Mečíř, Mikael Pernfors, Tomáš Šmíd and Brian Gottfried.\n\nThe Snauwaert Ergonom was one of the most unusual racquets ever produced, featuring a rotating head that allegedly stayed in line with the path of the ball longer than a conventional racquet head.\n"}
{"id": "13930879", "url": "https://en.wikipedia.org/wiki?curid=13930879", "title": "Soft launch", "text": "Soft launch\n\nA soft launch, also known as a soft opening, is a preview release of a product or service to a limited audience prior to the general public. Soft-launching a product is sometimes used to gather data or feedback regarding its acceptance in the marketplace, prior to making it widely available during an official release or grand opening. A company may also choose a soft launch to test the functionality of a product, allowing adjustments to be made before a wider release and marketing efforts are implemented.\n\nWhen implementing a soft launch strategy, a company releases a product with little or no marketing. A soft launch permits a company to react to customer demands quickly and effectively, introducing new features which will ultimately make the product successful. For companies with a limited marketing budget, a soft launch can allow them to focus on product development rather than marketing.\n\nSoft launches can be used with websites to help roll out new features, test or tweak a design (or possible redesign) allowing a design which users dislike to be quietly retired. Changes can also be made to increase the functionality of the website and respond to user requests. Gmail, for example, was soft launched in 2005 and fully launched one year later.\n\nIn the instance of hardware products, a limited release soft launch can be used to test the market prior to a wide scale release. It also means companies are allowed to make last minute changes to the product after the soft launch. In many instances, soft launches of hardware are done in major metropolitan areas where the company has access to a wide variety of demographic groups.\n\nSoft launches are also used for software, with a small release being made to a limited group of individuals for beta testing. Software can be extensively tested by the releasing company, but ultimately it needs to be used to determine how effective it is. Major flaws in the design may emerge during beta testing and can be corrected before the product is released into a major market.\n\nSome software is soft launched on the Internet, which allows for easy software updates. Early beta testers can grow attached to the program and will continue to download new versions as they are released. Thus, companies often build up a loyal customer base, which spreads the word to other potential customers.\n\nBefore committing to a hard launch in the United States, developers creating English mobile applications may choose to launch unofficially in less populous English-speaking countries such as Australia, Canada, and New Zealand, to refine the app by analyzing usage and spending habits, which are thought to be similar to those in the United States. This may also reduce the chances of the American press noticing the app. Canada has the additional benefit of having similar time zones to the United States.\n\nWhen a brick and mortar business wishes to open prior to its grand opening (to test its capacity and train its staff), this may be referred to as a soft launch or soft opening.\n\nWhen a business wishes to close but still allow customers to shop, this is referred to as a \"soft close\".\n\nThe term test event is often used in sports, especially in the UK, to refer to events held in a newly constructed sports venue before its official opening. For example, a newly built venue in the UK is required to host two events at reduced capacity, with the second event using a larger capacity than the first, before being granted a safety certificate that allows it to hold events at full capacity.\n"}
{"id": "42811967", "url": "https://en.wikipedia.org/wiki?curid=42811967", "title": "Speed alliance", "text": "Speed alliance\n\nSpeed-alliance is a product of Hahn Air Systems GmbH. It is used by high-speed railway companies to sell their seats via Global Distribution System (GDS) under the two character code 5W. The railway connections are displayed in the airline display of the GDS to be sold by travel agencies worldwide. \nIn addition, travel agencies can combine the train connections with flights of the partner airlines of Hahn Air Lines GmbH on one HR-169 ticket. Hahn Air Lines GmbH is a sister company of Hahn Air Systems GmbH. \n"}
{"id": "616993", "url": "https://en.wikipedia.org/wiki?curid=616993", "title": "Steamroller", "text": "Steamroller\n\nA steamroller (or steam roller) is a form of road roller – a type of heavy construction machinery used for leveling surfaces, such as roads or airfields – that is powered by a steam engine. The levelling/flattening action is achieved through a combination of the size and weight of the vehicle and the \"rolls\": the smooth wheels and the large cylinder or drum fitted in place of treaded road wheels.\n\nThe majority of steam rollers are outwardly similar to traction engines as many traction engine manufacturers later produced rollers based on their existing designs, and the patents owned by certain roller manufacturers tended to influence the general arrangements used by others. The key difference between the two vehicles is that on a roller the main roll replaces the front wheels and axle that would be fitted to a traction engine.\n\nIn many parts of the world, the term \"steam roller\" is still used regardless of the method of propulsion. This typically only applies to the largest examples (used for road-making).\n\nBefore about 1850, the word steamroller meant a fixed machine for rolling and curving steel plates for boilers and ships.\nFrom then on, it also meant a vehicle.\nAn early steamroller was demonstrated by Louis Lemoine in France in 1860 and in Britain in 1863 by William Clark and partner W.F. Batho.\n\nThe company Aveling & Porter was the first to successfully sell the product commercially and subsequently became the largest manufacturer in the world. In 1866 they produced a prototype roller with 3 foot-wide rollers fitted to the rear of a standard 12 nominal horsepower traction engine. This experimental machine was described by local papers as 'the world's first steamroller' and it caused a public spectacle. \n\nIn 1867 the steam road roller was patented and the company began production of the first practical steam roller - the new machine's rollers were mounted at the front instead of the back and it weighed in excess of 30 tons. It was tested on the Military Road in Chatham, Star Hill in Rochester and in Hyde Park, London and the machine proved a huge success. Within a year, they were being exported around the world, including to France, India and the United States. A New York City chief engineer said of one of these, that \"in one day's rolling at a cost of 10 dollars, as much work was accomplished as in two days' rolling with a 7 ton roller drawn by eight horses at a cost of 20 dollars a day.\"\n\nAveling & Porter refined their product continuously over the following decades, introducing fully steerable front rollers and compound steam engines at the 1881 Royal Agricultural Show.\n\nThe majority of rollers were of the same basic 3-roll configuration, gear-driven, with two large smooth wheels (rolls) at the back and a single wide roll at the front. (Actually, the wide roll usually comprised two narrower rolls on the same axle, to make steering easier.) However, there was also a distinctive variant, the \"tandem\", which had two wide rolls, one front, one rear. Those made by Robey & Co. used their standard steam wagon engine and pistol boiler fitted in a girder frame with rolls and a chain drive to produce a quick-reversing roller suitable for modern road surfaces such as tarmacadam and bituminous asphalt. A number of Robey & Co. tandem rollers were modified to make a further variant, the tri-tandem, which was a tandem with a third roll, mounted directly behind the rear one. Robey supplied the parts, but the modification was undertaken by Goodes of Royston. Ten tandem and two tri-tandem Robey rollers survive in preservation, and one of the tri-tandems is known to have been used to construct parts of the M1 motorway.\n\nA variation of the basic configuration was the \"convertible\": an engine which could be either a steam roller or a traction engine and could be changed from one form to the other in a relatively short time – \"i.e.\", less than half a day. Convertible engines were liked by local authorities, since the same machine could be used for haulage in the winter and road-mending in the summer.\n\nAlthough most steam roller designs are derived from traction engines, and were manufactured by the same companies, there are a number of features that set them apart.\n\nThe most obvious difference is in the wheels. Traction engines were generally built with large fabricated spoked steel wheels with wide rims. Those intended for road use would have continuous solid rubber tyres bolted around the rims, to improve traction on tarmac. Engines intended for agricultural use would have a series of strakes bolted diagonally across the rims, like the tread on a modern pneumatic tractor tyre, and the wheels were typically wider to spread the load more evenly.\n\nSteam rollers, on the other hand, had smooth rear wheels and a roller at the front. The roller consisted of a pair of adjacent wide cylinders supported at both ends. This replaced the separate wheels and axle of a traction engine.\n\nIn the conventional arrangement, the front roller is mounted centrally, forward of the chimney. In order to allow enough clearance from the boiler (and hence a larger front roll), the smokebox is extended forward substantially at the top to incorporate a support plate on which to mount the bearing for the roller assembly. This gives the distinctive, hooded look to the front of a steam roller. It also necessitates a different design of smokebox door – it has to hinge up or down, rather than opening sideways, due to the limited access available. Access to the boiler tubes for cleaning is limited and the brush usually has to be inserted through the small gap between the top of the roll and the fork.\n\nThe front and rear rolls were usually fitted with scraper bars. As the vehicle moved along, these removed any surface material that had become stuck to the roll, to prevent a build-up of material and ensure a flat finish was maintained.\n\nSome steam rollers were fitted with a scarifier mounted on the tender box at the rear. They could be swung down to road level and used to rip up the old surface before a road was remade.\n\nAnother accessory was a tar sprayer – a bar mounted on the back of the roller. This was not a common fixture.\n\nBritain was a large exporter of steam rollers to the world over the years, with the firm of Aveling and Porter probably being the most famous and the most prolific.\n\nMany other traction engine manufacturers built steam rollers, but after Aveling and Porter, the most popular were Marshall, Sons & Co., John Fowler & Co., and Wallis & Steevens.\n\nIn America, the Buffalo-Springfield Roller Company was a large builder. J. I. Case made a roller variant of their famed farm engines, but had a small market share. Other nations had makers including the Czechs, Swiss, Swedes, Germans and Dutch which produced steam rollers.\n\nIn the UK, a number of companies owned fleets of steam rollers and contracted them out to local authorities. Many were still in use into the 1960s, and part of the M1 motorway was made with the help of steam rollers. A few steam rollers were still being used for road maintenance in the early 1970s, and this may go some way to explaining why diesel-powered rollers are still colloquially known as \"steam\" rollers to this day.\n\nMany steam rollers are preserved in working order, and can be seen in operation during special live steam festivals, where operating scale models may also be displayed. At some of the UK steam fairs and rallies, demonstrations of road building using the old techniques, tools and machines are re-enacted by 'Road Gangs' in authentic dress; steam rollers feature prominently in these demonstrations. The annual Great Dorset Steam Fair has a section dedicated to road-making machinery, including a line-up of working steam rollers.\n\nBritish steeplejack and engineering enthusiast Fred Dibnah was known as a national institution in Great Britain for the conservation of steam rollers and traction engines. The first engine he restored to working order was an Aveling & Porter steam roller, registration no. DM3079. Built in 1912, it was a 10-ton slide-valve, single-cylinder, 4-shaft, road roller. Originally named \"Allison\", after his first wife, Fred renamed the engine \"Betsy\" (his mother's name) following his divorce – Fred's view being \"wives may change but your mother remains your mother!\" This roller was featured in many of Fred's early television programmes. It may still be seen at steam rallies in Britain and was in steam at the Great Dorset Steam Fair in 2011.\n\nUnlike the often-lethal movie roles by their diesel-powered equivalents, the film appearances by steam rollers are relatively benign:\n\nThe 1934 short film \"Mickey's Steam Roller\" involves a steamroller hijacking by Mickey Mouse's nephews which leads to devastating but non-lethal results.\n\nA steam roller was part of the supporting cast in the 1953 British film \"The Titfield Thunderbolt\", which encountered a GWR tank locomotive in the film. In the 1971 film \"Dad's Army\", the Walmington-on-Sea platoon is sent on an exercise for Home Guard training. On the way, an incident that disables Jones's van results in Capt. Mainwaring commandeering a passing steam roller to tow the van to the exercise. Unfortunately, on arrival at the training camp, Mainwaring and Jones discover that neither knows how to stop the roller, and they end up flattening their tents and equipment.\n\nIn \"'Allo 'Allo!\", in one episode (Series 3 episode 4, \"Flight Of Fancy\"), Colonel Kurt Von Strohm and Captain Hans Geering drive a steamroller to infiltrate an old railway parade but find they cannot stop and accidentally run over and crush Herr Flick's Gestapo staff car.\n\nIn 1997’s Austin Powers, the titular character drives a steamroller over a security guard, killing him, but not before an overly long comical sequence where he sees the impending danger and remains 'helpless' for several seconds until his demise.\n\nSeveral characters in children's popular fiction have been based on steam rollers. \"George the Steamroller\" appears in the Rev W. Awdry's Railway Series books and also in the derived television series, \"Thomas the Tank Engine and Friends\". A member of \"The Pack\", \"Buster the Steamroller\", has also appeared in the TV series.\n\n\"Roley\" is one of the main vehicle characters in the children's books and television series, \"Bob the Builder\". He is a green roller with a cab, enclosed power unit and no chimney, and so is obviously diesel-powered – nevertheless, his official title is \"Roley the Steamroller\". This is an example of how use of the term \"steam roller\", to describe a modern road roller, still persists in the English language.\n\nSteam rollers have had an influence on popular music, for example, the group Buffalo Springfield named themselves after (the manufacturer of) a steam roller parked outside the house. The song \"Steamroller Blues\" was written and performed by James Taylor in 1970 and subsequently became a favourite of live concerts by Elvis Presley. Verdi's Il Trovatore Anvil Chorus is associated with steamrolling and power in some parts of the world. The name of American music group Mannheim Steamroller is the result of blending the word \"steamroller\" with \"Mannheim roller\", an 18th-century German musical technique characterized by a crescendo passage having a rising melodic line over an ostinato bass line, popularized by the Mannheim school of composition. See also Steam (Peter Gabriel song).\n\nAlso, the name of ex-Yugoslav and Croatian rock band Parni Valjak means \"steamroller\".\n\n\n"}
{"id": "54698311", "url": "https://en.wikipedia.org/wiki?curid=54698311", "title": "Video line selector", "text": "Video line selector\n\nA video line selector is an electronic circuit or device for picking a line from an analog video signal. The input of the circuit is connected to an analog video source, the output triggers an oscilloscope, so display the selected line on the oscilloscope or similar device.\n\nVideo line selectors are circuits or units of other devices, fitted to the demand of the unit or a separate device for use in workshops, production and laboratories. They contain analog and digital circuits and an internal or external DC power supply. There's a video signal input, sometimes an output to prevent reflexions of the video signal and the cause of shadows of the video picture, also a trigger output. There is also an input or adjust for the line number(s) to be picked out and as an option an automatic or manual setting to fit other video standards and non-interlaced video. Video line selectors do not need all the picture signal, just the synchronisation signals are needed. Sometimes inputs for H- and V-sync were installed, only.\n\nThe video signal input is 75 Ω terminated or connected to the video output for a monitor. The amplified video signal is connected to the inputs of the H- und V-sync detector circuits. The H-sync detector outputs the horizontal synchronisation pulse filtered from the video signal. This is the line synchronisation and makes the lines fit vertically. The V-sync detector filters the vertical synchronisation and makes the picture fit the same position on the screen than the previous one.\n\nBoth synchronisation output pulses are fed to a digital synchron counter. The V-sync resets the counter. The H-sync is being counted. On every frame picture, the counter is being reset and the lines were counted. Most often interlaced video was used, spitting up a picture in the odd numbered lines, followed by the even numbered lines in a half picture each. (→deninterlacing).\n\nInterlace video requires a V-sync detector which detects first a second scan of the interlaced frame. \nSome reset the counter and toggle an interlace bit, others ignore the sync after the odd numbered lines and continue counting.\nBroadcast television systems were based on a nearly identical monochrome video signal with minor changes all over the world, which a number of lines can be covered by 10 bit counter (2 < lines < 2 → 512 < 576 < 1024). The digital comparator, feed by the line number preset and the counter detects the logical equivalence as match of the binary numbers, which is the output pulse of the video line selector. When fed to the trigger input of an oscilloscope, the signal of the selected video line is displayed on the oscilloscope when the test probe is fed by the video signal. A precision timer can trigger a pixel or dot of the line.\n\nIn order to simplify the digital part of the circuit, it is possible to load the preset line number into the counter and have it count descending. When the counter reaches zero, the trigger output is set. A 10 inputs NOR gate is more sufficient than a 10 bits digital comparator, but evaluating several lines per picture is no longer possible. Decreasing the line number by one, the carry bit of the counter can be used as trigger output, replacing a 10 inputs NOR-Gate.\n\nVideo line selecting was used in laboratory, production, and workshop: (selection only)\nMonitoring television picture content:\nFor modifying television signals:\nAs precise optical sensor:\n\n\n"}
{"id": "22726023", "url": "https://en.wikipedia.org/wiki?curid=22726023", "title": "Vidyo", "text": "Vidyo\n\nVidyo, Inc., founded as Layered Media, is a privately held, venture-funded company that provides software-based collaboration technology and product-based visual communication products. The company's VidyoConferencing products are the first in the videoconferencing industry to take advantage of the H.264 standard for video compression, Scalable Video Coding (SVC).\n\nThe firm's implementation of this technology enables video communication across general purpose IP networks.\n\nVidyo was founded by Ofer Shapiro, who had developed the first IP video conferencing bridge and programmable gatekeeper technology at Radvision. He left Radvision in early 2004 to develop improved video conferencing networks. Packet loss and latency that accompanies general purpose IP networks posed significant challenges to the existing systems. Costly dedicated networks and expensive Multipoint Control Units (MCU), which exacerbated delay due to transcoding or forced all endpoints to conform to the least common denominator endpoint quality, were the only products the industry had to offer at that time. He realized that to take advantage of state of the art H.264 compression technology, the fundamental technology behind video conferencing systems had to change Shapiro’s efforts resulted in a paradigm shift for the video conferencing industry - a new system architecture. It was based upon Scalable Video Coding (SVC) which allowed for error resiliency which was absent in monolithic encoding schemes that were common throughout the industry. Implementation of SVC for only some system components offered little value. The full benefit of SVC required re-designing both the endpoint (client) and MCU (server), a costly and time consuming proposition for incumbent providers of video conferencing systems. Unencumbered by a legacy product line, Shapiro developed a new architecture in which the MCU was replaced with a low cost router and all of the encoding and decoding was done at the end points.\n\nAt the end of 2004, Shapiro obtained support from Avery More in early 2005; and secured seed funding by October 2005, led by on Bayless of Sevin Rosen Funds. Operating in stealth mode through the beginning of 2007, Shapiro’s team created a video conferencing product with HD quality video, that effectively addressed the interactivity inhibiting delay common in legacy systems, and enabled each end point to send and receive at its highest quality levels, irrespective of the other end points and the network. . Layered Media licensed its technology to its first OEM customers early in 2007.\n\nIn June 2007 Layered Media secured series B funding, with Rho Ventures joining in to lead the round. Shortly after Layered Media changed its name to Vidyo, Inc.\n\nBy the beginning of 2008, Vidyo emerged from stealth mode with a product offering for the enterprise market and an annual licensing price model which lowered the start up cost barrier for organizations looking to implement a video conferencing solution.\n\nIn the first quarter of 2009, Vidyo raised another round of funding, led by Menlo Ventures joining Rho Ventures, Sevin Rosen Funds, and Star Ventures.\n\nIn early April 2010, the company announced another $25 million from their Series C round of financing, bringing the total amount of capital raised by the company to $63 million since its founding in 2005. All existing investors, Menlo Ventures, Rho Ventures, Sevin Rosen Funds and Star Ventures, participated in the round, which was led by Four Rivers Group.\n\nIn May 2010, Vidyo launched its Software Development Kit (SDK), enabling developers to build multipoint video conferencing applications into Android and Moblin-based smart phones and tablets running Intel Atom Processor Z6xx series-based platforms (formerly Moorestown) and on ARM processor-based platforms.\n\nIn June 2010, at Infocomm in Las Vegas, Vidyo demonstrated the first videoconferencing system to attain 1440p (decode), at 2560 x 1440p resolution, in an HD multiparty video conference via general purpose IP networks.\n\nA few days later, the company announced a partnership with HP to expand the HP Halo portfolio to include conference room and desktop endpoints that run on enterprise networks.\n\nIn March 2016, Vidyo integration was added to KioWare Kiosk System Software, creating a video conferencing kiosk solution.\n\nVideo-chat service for Nintendo's Wii U game console was co-developed with Vidyo, Nintendo European Research & Development and Nintendo Software and Nintendo Software Technology. VidyoRouter is an appliance that performs the packet-switching function. The VidyoPortal is a Web-based environment used to access and manage the VidyoConferencing system and accounts from anywhere with internet access. VidyoDesktop is a software-based endpoint, managed via VidyoPortal, able to support HD quality video. HD-200 is a High Definition room system endpoint. The HD-100 is an entry level High Definition room system endpoint. Both of these products are managed by the VidyoPortal and interoperate with VidyoDesktop users.\nVidyoGateway is an appliance used to connect legacy video conferencing systems with the VidyoConferencing network. The VidyoCampus Program is used by colleges and universities to deploy the system to every desktop throughout their collaboration community. VidyoHealth is a scalable high-definition telemedicine product that uses the public Internet and existing general purpose IP networks at medical facilities for doctor-patient and doctor-doctor interactions.\n\n"}
{"id": "58656620", "url": "https://en.wikipedia.org/wiki?curid=58656620", "title": "Women in Tech Africa", "text": "Women in Tech Africa\n\nWomen in Tech Africa (WiTA) is an organization founded by Ethel D Cofie. with a focus on entrepreneurship expansion and multiplying the numbers of females in technology especially in Africa. Over the years, WiTA has strategically focused on enabling women to drive Africa’s growth story and create impact on personal life through technology. Currently, its target audience comprises aspiring female tech entrepreneurs between the ages of 18-40. Women in Tech Africa is the largest group on the continent with membership across 30 countries globally with physical chapters in Ghana, Malawi, Zimbabwe, Somaliland, Germany, Ireland, Kenya, Tanzania and Mauritius.\n\n\nThis digital festival brings women in tech globally together to celebrate success and impact through leadership training, peer learning, and workshops.  The focus topics are related to technology, entrepreneurship, work-life balance, and leadership. The audience for these events generally comprises women tech entrepreneurs, girls aspiring for STEM careers and C level executive women. These events focus not only on women in Africa but African women all across the globe. Currently, WiTW received an endorsement from the Graca Machel Trust.\n\nIn collaboration with the MTN Foundation, Women in Tech Africa organized the MTN Girl Code project in 2017. This project sought to increase the number of women participating in the MTN App Challenge (an MTN initiative run annually), improve the number of ladies in the coding ecosystem in Ghana and encourage the number of female in the African Startup eco-system. Trainings were in business validation, mobile app and IoT development and game and animation development.\n\nIn Collaboration with ATBN (UK) and Comic Relief, #HerFuture Africa, a female entrepreneurship empowerment project aimed at empowering and equipping young women with the necessary tools to address the needs of their respective societies. Participants were trained in design thinking, ideation, project plans, and business model canvas and solution generation. This project saw the rise of Ivy Barley, the founder of Developers in Vogue.\n\nIn collaboration with the Technical Centre for Agricultural and Rural Cooperation (CTA) AgriHack Talent Initiative, this initiative aimed at accelerating entrepreneurship for improved livelihoods in the African, Caribbean and Pacific countries. The pitch AgriHack competition was open to founders of ICT solutions that address needs in the agriculture sector.\n\n"}
